[{"text": "A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings Chris Sweeney and Maryam Naja\ufb01an Massachusetts Institute of Technology Cambridge , MA , USA fcsweeney , naja\ufb01an g@mit.edu", "entities": [[9, 11, "TaskName", "Word Embeddings"], [20, 21, "DatasetName", "Cambridge"]]}, {"text": "Abstract Word embedding models have gained a lot of traction in the Natural Language Processing community , however , they suffer from unintended demographic biases .", "entities": []}, {"text": "Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test ( WEAT ) .", "entities": []}, {"text": "While these approaches offer great geometric insights into unintended biases in the embedding vector space , they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications .", "entities": []}, {"text": "In this work , we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias .", "entities": []}, {"text": "Our metric ( Relative Negative Sentiment Bias , RNSB ) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups .", "entities": [[13, 15, "TaskName", "word embeddings"]]}, {"text": "We show that our framework and metric enable useful analysis into the bias in word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "1 Introduction Word embeddings have established themselves as an integral part of Natural Language Processing ( NLP ) applications .", "entities": [[2, 4, "TaskName", "Word embeddings"]]}, {"text": "Unfortunately word embeddings have also introduced unintended biases that could cause downstream NLP systems to be unfair .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "Recent studies have shown that word embeddings exhibit unintended gender and stereotype biases inherent in the training corpus .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "Bias can be de\ufb01ned as an unfair expression of prejudice for or against a person , a group , or an idea .", "entities": []}, {"text": "Bias is a broad term , which covers a range of problems particularly relevant in natural language systems such as , discriminatory gender bias ( Bolukbasi et al . , 2016a ; Zhao et", "entities": []}, {"text": "al . , 2017 ) , bias against regionally accented speech ( Naja\ufb01an et al . , 2016 , 2017 ) , personal or political view bias ( Iyyer et al . , 2014 ; Recasens et al . , 2013 ) , and many other examples .", "entities": []}, {"text": "In Figure 1 : 2 - D PCA embeddings for positive / negative sentiment words and a set of national origin identity terms .", "entities": [[7, 8, "MethodName", "PCA"]]}, {"text": "Geometrically , it is dif\ufb01cult to parse how these embeddings can lead to discrimination .", "entities": []}, {"text": "our work , we restrict our de\ufb01nition of bias to unequal distributions of negative sentiment among demographic identity terms in word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "One could also look at unequal distributions of positive sentiment , but for this work we restrict ourselves to the negative case .", "entities": []}, {"text": "Sentiment analysis makes up a large portion of current NLP systems .", "entities": [[0, 2, "TaskName", "Sentiment analysis"]]}, {"text": "Therefore , preventing negative sentiment from mixing with sensitive attributes ( i.e. race , gender , religion ) in word embeddings is needed to prevent discrimination in ML models using the embeddings .", "entities": [[19, 21, "TaskName", "word embeddings"]]}, {"text": "As studied in ( Packer et al . , 2018 ) , unintentionally biased word embeddings can have adverse consequences when deployed in applications , such as movie sentiment analyzers or messaging apps .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "Negative sentiment can be unfairly entangled in the word embeddings , and detecting this unintended bias is a dif\ufb01cult problem .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "We need clear signals to evaluate which groups are discriminated against due to the bias in an embedding model .", "entities": []}, {"text": "That way we can pinpoint where to mitigate those biases .", "entities": []}, {"text": "To demonstrate this need for clear signals", "entities": []}, {"text": "of bias in word embeddings , we look at Figure 1 .", "entities": [[3, 5, "TaskName", "word embeddings"]]}, {"text": "Figure 1 shows a 2D word embedding projection of positive sentiment ( green ) and negative sentiment ( red ) words .", "entities": []}, {"text": "It would be unfair for any given demographic identity word vector ( blue ) to be more semantically related to negative terms than the other identities .", "entities": []}, {"text": "However , many identity terms exist closer to negative words than other identity terms in the vector space .", "entities": []}, {"text": "This bias may affect a downstream ML model , but the vector space has no absolute interpretable meaning , especially when it comes to whether this embedding model will lead to a unfairly discriminative algorithm .", "entities": []}, {"text": "Our framework enables transparent insights into word embedding bias by instead viewing the output of a simple logistic regression algorithm trained on an unbiased positive / negative word sentiment dataset initialized with biased word vectors .", "entities": [[17, 19, "MethodName", "logistic regression"]]}, {"text": "We use this framework to create a clear metric for unintended demographic bias in word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "2 Prior Work Researchers have found a variety of ways in which dangerous unintended bias can show up in NLP applications ( Blodgett and O\u2019Connor , 2017 ; Hovy and Spruit , 2016 ; Tatman , 2017 ) .", "entities": []}, {"text": "Mitigating such biases is a dif\ufb01cult problem , and researchers have created many ways to make fairer NLP applications .", "entities": []}, {"text": "Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text ( Bolukbasi et al . , 2016b , a ; Zhao et", "entities": []}, {"text": "al . , 2017 ; Zhang et al . , 2018 ) , or inequality of sentiment or toxicity for various protected groups ( Caliskan - Islam et al . , 2016 ; Bakarov , 2018 ; Dixon et al . ; Garg et al . , 2018 ; Kiritchenko and Mohammad , 2018 ) .", "entities": []}, {"text": "More speci\ufb01cally , word embeddings has been an area of focus for evaluating unintended bias .", "entities": [[3, 5, "TaskName", "word embeddings"]]}, {"text": "( Bolukbasi et al . , 2016b ) de\ufb01nes a useful metric for identifying gender bias and ( Caliskan - Islam et al . , 2016 ) de\ufb01nes a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text .", "entities": []}, {"text": "Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vswoman ( Bolukbasi et al . , 2016a ) , orEuropean American names vs. African American names ( Caliskan - Islam et al . , 2016 ) .", "entities": []}, {"text": "Though geometrically intuitive , these tests do not have a direct relation to discrimination in general .", "entities": []}, {"text": "Ourframework and RNSB metric enable a clear evaluation of discrimination with respect to word embedding bias for a whole class of demographics .", "entities": []}, {"text": "3 Methods We present our framework for understanding and evaluating unintentional demographic bias in word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "We \ufb01rst describe the \ufb02ow of our framework .", "entities": []}, {"text": "Then , we address which datasets / models were chosen for our approach .", "entities": []}, {"text": "Finally , we show how our framework can enable analysis and new metrics like RNSB .", "entities": []}, {"text": "3.1 Framework Figure 2 : We isolate unintended bias to the word embeddings by training a logistic regression classi\ufb01er on a unbiased positive / negative word sentiment dataset ( initialized with the biased word embeddings ) .", "entities": [[11, 13, "TaskName", "word embeddings"], [16, 18, "MethodName", "logistic regression"], [33, 35, "TaskName", "word embeddings"]]}, {"text": "We measure word embedding bias by analyzing the predicted probability of negative sentiment for identity terms .", "entities": []}, {"text": "Our framework enables the evaluation of unintended bias in word embeddings through the results of negative sentiment predictions .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "Our framework has a simple layout .", "entities": []}, {"text": "Figure 2 shows the \ufb02ow of our system .", "entities": []}, {"text": "We \ufb01rst use the embedding model we are trying to evaluate to initialize vectors for an unbiased positive / negative word sentiment dataset .", "entities": []}, {"text": "Using this dataset , we train a logistic classi\ufb01cation algorithm to predict the probability of any word being a negative sentiment word .", "entities": []}, {"text": "After training , we take a set of neutral identity terms from a protected group ( i.e. national origin ) and predict the probability of negative sentiment for each word in the set .", "entities": []}, {"text": "Neutral identity terms that are unfairly entangled with negative sentiment in the word embeddings will be classi\ufb01ed like their neighboring sentiment words from the sentiment dataset .", "entities": [[12, 14, "TaskName", "word embeddings"]]}, {"text": "We leverage this set of negative sentiment probabilities to summarize unintended demographic bias using RNSB .", "entities": []}, {"text": "3.2 Models and Data We evaluate three pretrained embedding models : GloVe ( Pennington et al . , 2014 ) , Word2vec ( Mikolov et al . , 2013 ) ( trained on the large Google News corpus ) , and ConceptNet ( Speer et al . , 2017 ) .", "entities": [[11, 12, "MethodName", "GloVe"], [35, 36, "DatasetName", "Google"], [41, 42, "DatasetName", "ConceptNet"]]}, {"text": "GloVe and Word2vec embeddings have been shown to contain unintended bias in ( Bolukbasi et al . , 2016a ; Caliskan - Islam et al . , 2016 ) .", "entities": [[0, 1, "MethodName", "GloVe"]]}, {"text": "ConceptNet has been shown to be less biased than these models ( Speer , 2017 ) due to the mixture of curated corpora used for training .", "entities": [[0, 1, "DatasetName", "ConceptNet"]]}, {"text": "As part of our pipeline , we also use a labeled positive / negative sentiment training set ( Hu and Liu , 2004 ) .", "entities": []}, {"text": "This dataset has been shown to be a trustworthy lexicon for negative and positive sentiment words ( Pang et al . , 2008 ; Liu , 2012 ; Wilson et al . , 2005 ) .", "entities": []}, {"text": "We trust these labels to be unbiased so that we may isolate the unintended biases entering our system to the word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "Finally , we use a simple logistic regression algorithm to predict negative sentiment .", "entities": [[6, 8, "MethodName", "logistic regression"]]}, {"text": "Although the choice of ML model can have an impact on fairness for sentiment applications as shown in ( Kiritchenko and Mohammad , 2018 ) , we choose a simple ML model to limit the possible unintended biases introduced downstream from our word embeddings .", "entities": [[42, 44, "TaskName", "word embeddings"]]}, {"text": "3.3 Bias Analysis : RNSB We now present our metric for unintended demographic bias , RNSB .", "entities": []}, {"text": "For gold standard labeled positive / negative sentiment words , ( xi;yi ) , in training set , S , wherexiis a word vector from a possibly biased word embedding model , we \ufb01nd the minimizer , f\u0003(xi )", "entities": []}, {"text": "= \u001b(wTxi ) , for the logistic loss , l , and learned weights , w. minw2RdnX i=0l(yi;wTxi )", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "+", "entities": []}, {"text": "\u0015kwk2;\u0015 > 0", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "Then for a set , K = fk1;:::;k tg , oftdemographic identity word vectors from a particular protected group ( i.e. national origin , religion , etc . ) , we de\ufb01ne a set , P , containing the predicted negative sentiment probability via minimizer , f\u0003 , normalized to be one probability mass .", "entities": [[5, 7, "HyperparameterName", "K ="]]}, {"text": "P= ( f\u0003(k1)Pt i=1f\u0003(ki);:::;f\u0003(kt)Pt i=1f\u0003(ki ) )", "entities": []}, {"text": "Thus , our metric , RNSB ( P ) , is de\ufb01ned as the KL divergence of PfromU , whereUis the uniformdistribution for telements .", "entities": []}, {"text": "RNSB ( P ) = DKL(PkU )", "entities": []}, {"text": "We choose our set of neutral identity terms based on the most populous demographics for each protected group .", "entities": []}, {"text": "However , due to the simplicity of this method , one can easily adapt it to include identity terms that suit the application in need of analysis .", "entities": []}, {"text": "Since neutral identity terms are inherently not associated with sentiment , it is unfair to have identity term with differing levels of negative sentiment .", "entities": []}, {"text": "This type of discrimination can show up in many downstream sentiment analysis applications .", "entities": [[10, 12, "TaskName", "sentiment analysis"]]}, {"text": "Thus , we want no differences between negative sentiment predictions of various identity terms .", "entities": []}, {"text": "Mathematically , this can be represented as a uniform distribution of negative sentiment probability for identity terms from a protected group .", "entities": []}, {"text": "Our RNSB metric captures the distance , via KL divergence , between the current distribution of negative sentiment and the fair uniform distribution .", "entities": []}, {"text": "So the more fair a word embedding model with respect to sentiment bias , the lower the RNSB metric .", "entities": []}, {"text": "4 Results and Discussion We evaluate our framework and metric on two cases studies : National Origin Discrimination and Religious Discrimination .", "entities": []}, {"text": "For each case study , we create a set of the most frequent identity terms from the protected groups in the Wikipedia word corpus and analyze bias with respect to these terms via our framework .", "entities": []}, {"text": "First , we compare the RNSB metric for 3 pretrained word embeddings , showing that our metric is consistent with other word embedding analysis like WEAT ( Caliskan - Islam et al . , 2016 ) .", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "We then show that our framework enables an insightful view into word embedding bias .", "entities": []}, {"text": "4.1 RNSB Metric on Word Embeddings We vary the word embeddings used in our framework and calculate the RNSB metric for each embedding .", "entities": [[4, 6, "TaskName", "Word Embeddings"], [9, 11, "TaskName", "word embeddings"]]}, {"text": "The results are displayed in Table 1 .", "entities": []}, {"text": "For both case studies , the bias is largest in GloVe , as shown by the largest RNSB metric .", "entities": [[10, 11, "MethodName", "GloVe"]]}, {"text": "As mentioned earlier , ConceptNet is a state of the art model that mixes models like GloVe and Word2vec , creating fairer word embeddings .", "entities": [[4, 5, "DatasetName", "ConceptNet"], [16, 17, "MethodName", "GloVe"], [22, 24, "TaskName", "word embeddings"]]}, {"text": "Through the RNSB metric , one can see that the unintended demographic", "entities": []}, {"text": "Figure 3 : Histograms showing relative negative sentiment probability between national origin identity terms .", "entities": []}, {"text": "The top left graph is GloVe , the top right is ConceptNet .", "entities": [[5, 6, "MethodName", "GloVe"], [11, 12, "DatasetName", "ConceptNet"]]}, {"text": "The bottom histogram is the uniform distribution of negative sentiment in a perfect fair scenario .", "entities": []}, {"text": "bias of these word embeddings is an order of magnitude lower than GloVe or Word2vec .", "entities": [[3, 5, "TaskName", "word embeddings"], [12, 13, "MethodName", "GloVe"]]}, {"text": "Although the RNSB metric is not directly comparable to WEAT scores , these results are still consistent with some of the bias predicted by ( Caliskan - Islam et al . , 2016 ) .", "entities": []}, {"text": "The WEAT score shows that word embeddings like Word2vec and GloVe are biased with respect to national origin because European - American names are more correlated with positive sentiment than AfricanAmerican names .", "entities": [[5, 7, "TaskName", "word embeddings"], [10, 11, "MethodName", "GloVe"]]}, {"text": "RNSB captures the same types of biases , but has a clear and larger scope , measuring discrimination with respect to more than two demographics within a protected group .", "entities": []}, {"text": "Case Study GloVe Word2Vec ConceptNet National Origin Identity 0.6225 0.1945 0.0102 Religion Identity 0.3692 0.1026 0.0291 Table 1 : Table showing our RNSB metric for various word embeddings on two case studies .", "entities": [[2, 3, "MethodName", "GloVe"], [4, 5, "DatasetName", "ConceptNet"], [26, 28, "TaskName", "word embeddings"]]}, {"text": "Our metric effectively predicts the unintended demographic bias in the presented word embeddings with respect to negative sentiment .", "entities": [[11, 13, "TaskName", "word embeddings"]]}, {"text": "4.2 Analyzing Unintended Demographic Bias in Word Embeddings Using the probability distribution of negative sentiment for the identity terms in a protected group , we can gain insights into the relative risks for discrimination between various demographics .", "entities": [[6, 8, "TaskName", "Word Embeddings"]]}, {"text": "Figure 3 shows three histograms .", "entities": []}, {"text": "The bottom histogram is the uniform distribution .", "entities": []}, {"text": "As described earlier , zero unintended demographic bias with respect to our de\ufb01nition is achieved when all the identity terms within a protected group have equal negative sentiment .", "entities": []}, {"text": "The top two histograms show the negative sentiment probability for each identity normalized across all terms to be a probability distribution .", "entities": []}, {"text": "The left histogram is computed using the GloVe word embeddings , and the righthistogram is computed using the fairer ConceptNet embeddings .", "entities": [[7, 8, "MethodName", "GloVe"], [8, 10, "TaskName", "word embeddings"], [19, 20, "DatasetName", "ConceptNet"]]}, {"text": "One can see that certain demographics have very high negative sentiment predictions , while others have very low predictions .", "entities": []}, {"text": "The ConceptNet distribution seems to equalize much of this disparity .", "entities": [[1, 2, "DatasetName", "ConceptNet"]]}, {"text": "This type of analysis is very insightful as it enables one to see which identities are more at risk for discrimination .", "entities": []}, {"text": "A more direct way to measure how certain groups receive similar unfair treatment is to compute a correlation matrix between the vectors containing negative sentiment predictions for each identity term .", "entities": []}, {"text": "We compute this matrix for the same two cases : GloVe word embeddings ( top ) and ConceptNet word embeddings ( bottom ) shown in Figure 4 .", "entities": [[10, 11, "MethodName", "GloVe"], [11, 13, "TaskName", "word embeddings"], [17, 18, "DatasetName", "ConceptNet"], [18, 20, "TaskName", "word embeddings"]]}, {"text": "The GloVe word embedding correlation matrix contains a lot of dark low correlations between identities , as a lot of identities contain small amounts of negative sentiment .", "entities": [[1, 2, "MethodName", "GloVe"]]}, {"text": "But this visual brings out that certain groups like Indian , Mexican , and Russian have a high correlation , indicating that they could be treated similarly unfairly in a downstream ML algorithm .", "entities": []}, {"text": "This is a useful insight that could allow a practitioner to change to embedding training corpora to create fairer models .", "entities": []}, {"text": "For the ConceptNet word embeddings , we see a much more colorful heat map , indicating there are higher correlations between more identity terms .", "entities": [[2, 3, "DatasetName", "ConceptNet"], [3, 5, "TaskName", "word embeddings"]]}, {"text": "This hints that ConceptNet contains less targeted discrimination via negative sentiment .", "entities": [[3, 4, "DatasetName", "ConceptNet"]]}, {"text": "This visual also brings out slight differences in negative sentiment prediction .", "entities": []}, {"text": "Identity terms like Scottish have lower correlations across the board , manifesting that this identity has slightly less negative sentiment than the rest of the identities .", "entities": []}, {"text": "This is important to analyze to get a broader context for how various identities could receive different amounts of discrimination stemming from the word embedding bias .", "entities": []}, {"text": "( a ) GloVe Fairness Correlation Heatmap ( b ) ConceptNet Fairness Correlation Heatmap Figure 4 : National origin correlation matrix for negative sentiment prediction using GloVe ( a ) and ConceptNet ( b ) word embeddings .", "entities": [[3, 4, "MethodName", "GloVe"], [4, 5, "TaskName", "Fairness"], [6, 7, "MethodName", "Heatmap"], [10, 11, "DatasetName", "ConceptNet"], [11, 12, "TaskName", "Fairness"], [13, 14, "MethodName", "Heatmap"], [26, 27, "MethodName", "GloVe"], [31, 32, "DatasetName", "ConceptNet"], [35, 37, "TaskName", "word embeddings"]]}, {"text": "We can use these \ufb01gures to analyze how certain groups could be similarly discriminated against via their negative sentiment correlation .", "entities": []}, {"text": "5", "entities": []}, {"text": "Discussion We showed how our framework can be used in the religious and national origin case studies .", "entities": []}, {"text": "In practice , our framework should be used to measure bias among demographics of interest for the NLP application in question .", "entities": []}, {"text": "Our RNSB metric is a useful signal a practitioner can use to choose the embedding model with the least amount of risk for discrimination in their application , or even to evaluate what types of unintended biases exists in their training corpora .", "entities": []}, {"text": "We used our framework to evaluate unintended bias with respect to sentiment , but there exists many other types of unintended demographic bias to create clear signals for in word embeddings.6 Conclusion We presented a transparent framework for evaluating unintended demographic bias in word embeddings .", "entities": [[43, 45, "TaskName", "word embeddings"]]}, {"text": "For this work our scope was limited to unfair biases with respect to negative sentiment .", "entities": []}, {"text": "In our framework , we train a classi\ufb01er on an unbiased positive / negative word sentiment dataset initialized with biased word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "This way , we can observe the unfairness in the word embeddings at the ML prediction level .", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "This allows us to observe clearer signals of bias in our metric , Relative Negative Sentiment Bias ( RNSB ) .", "entities": []}, {"text": "Previous metrics and analysis into unintended bias in word embeddings rely on vector space arguments for only two demographics at a time , which does not lend itself well to evaluating real world discrimination .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "Our metric has a direct connection to discrimination and can evaluate any number of demographics in a protected group .", "entities": []}, {"text": "Finally , our framework and metric reveal transparent analysis of the unintended bias hidden in word embeddings .", "entities": [[15, 17, "TaskName", "word embeddings"]]}, {"text": "Acknowledgments This work was made possible in part through support of the United States Agency for International Development .", "entities": []}, {"text": "The opinions expressed herein are those of the authors and do not necessarily re\ufb02ect the views of the United States Agency for International Development or the US Government .", "entities": []}, {"text": "References Amir Bakarov .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A survey of word embeddings evaluation methods .", "entities": [[3, 5, "TaskName", "word embeddings"]]}, {"text": "arXiv preprint arXiv:1801.09536 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Su Lin Blodgett and Brendan O\u2019Connor .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Racial disparity in natural language processing : A case study of social media african - american english .", "entities": []}, {"text": "FATML .", "entities": []}, {"text": "Tolga Bolukbasi , Kai - Wei Chang , James Zou , Venkatesh Saligrama , and Adam Kalai .", "entities": [[15, 16, "MethodName", "Adam"]]}, {"text": "2016a .", "entities": []}, {"text": "Quantifying and reducing stereotypes in word embeddings .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "ICML .", "entities": []}, {"text": "Tolga Bolukbasi , Kai - Wei Chang , James Y Zou , Venkatesh Saligrama , and Adam T Kalai .", "entities": [[16, 17, "MethodName", "Adam"]]}, {"text": "2016b .", "entities": []}, {"text": "Man is to computer programmer as woman is to homemaker ?", "entities": []}, {"text": "debiasing word embeddings .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In NIPS , pages 4349\u20134357 .", "entities": []}, {"text": "Aylin Caliskan - Islam , Joanna J Bryson , and Arvind Narayanan .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semantics derived automatically from language corpora necessarily contain human biases .", "entities": []}, {"text": "Science , pages 1\u201314 .", "entities": []}, {"text": "Lucas Dixon , John Li , Jeffrey Sorensen , Nithum Thain , and Lucy Vasserman .", "entities": []}, {"text": "Measuring and mitigating unintended bias in text classi\ufb01cation .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "Nikhil Garg , Londa Schiebinger , Dan Jurafsky , and James Zou .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Word embeddings quantify 100 years of gender and ethnic stereotypes .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}, {"text": "Proceedings of the National Academy of Sciences , 115(16 ) .", "entities": []}, {"text": "Dirk Hovy and Shannon L. Spruit . 2016 .", "entities": []}, {"text": "The social impact of natural language processing .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 591\u2013598 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Minqing Hu and Bing Liu .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Mining and summarizing customer reviews .", "entities": []}, {"text": "In ACM , pages 168\u2013177 .", "entities": [[1, 2, "DatasetName", "ACM"]]}, {"text": "Mohit Iyyer , Peter Enns , Jordan Boyd - Graber , and Philip Resnik .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Political ideology detection using recursive neural networks .", "entities": []}, {"text": "In ACL , volume 1 , pages 1113\u20131122 .", "entities": []}, {"text": "Svetlana Kiritchenko and Saif M Mohammad .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Examining gender and race bias in two hundred sentiment analysis systems .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}, {"text": "Proceedings of the 7thJoint Conference on Lexical and Computational Se - mantics(*SEM ) , New Orleans , USA .", "entities": []}, {"text": "Bing Liu .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Sentiment analysis and opinion mining .", "entities": [[0, 2, "TaskName", "Sentiment analysis"], [3, 5, "TaskName", "opinion mining"]]}, {"text": "Synthesis lectures on human language technologies , 5(1):1\u2013167 .", "entities": []}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , and Jeff Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In NIPS , pages 3111\u20133119 .", "entities": []}, {"text": "Maryam Naja\ufb01an , Wei - Ning Hsu , Ahmed Ali , and James Glass . 2017 .", "entities": []}, {"text": "Automatic speech recognition of arabic multi - genre broadcast media .", "entities": [[0, 3, "TaskName", "Automatic speech recognition"]]}, {"text": "In ASRU , pages 353\u2013359 .", "entities": []}, {"text": "Maryam Naja\ufb01an , Saeid Safavi , John HL Hansen , and Martin Russell . 2016 .", "entities": []}, {"text": "Improving speech recognition using limited accent diverse british english training data with deep neural networks .", "entities": [[1, 3, "TaskName", "speech recognition"]]}, {"text": "In MLSP , pages 1 \u2013 6 .", "entities": []}, {"text": "Ben Packer , Yoni Halpern , Mario Guajardo - Cspedes , and Margaret Mitchell .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Text embedding models contain bias .", "entities": []}, {"text": "here \u2019s why that matters .", "entities": []}, {"text": "Google Developers .", "entities": [[0, 1, "DatasetName", "Google"]]}, {"text": "Bo Pang , Lillian Lee , et al . 2008 .", "entities": []}, {"text": "Opinion mining and sentiment analysis .", "entities": [[0, 2, "TaskName", "Opinion mining"], [3, 5, "TaskName", "sentiment analysis"]]}, {"text": "FTIR , 2(1\u20132):1\u2013135 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In EMNLP , pages 1532\u20131543 .", "entities": []}, {"text": "Marta Recasens , Cristian Danescu - Niculescu - Mizil , and Dan Jurafsky .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Linguistic models for analyzing and detecting biased language .", "entities": []}, {"text": "In ACL , volume 1 , pages 1650\u20131659.Robyn", "entities": []}, {"text": "Speer .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Conceptnet numberbatch 17.04 : better , less - stereotyped word vectors .", "entities": [[0, 1, "DatasetName", "Conceptnet"]]}, {"text": "ConceptNet .", "entities": [[0, 1, "DatasetName", "ConceptNet"]]}, {"text": "Robyn Speer , Joshua Chin , and Catherine Havasi . 2017 .", "entities": []}, {"text": "Conceptnet 5.5 : An open multilingual graph of general knowledge .", "entities": [[0, 1, "DatasetName", "Conceptnet"], [8, 10, "TaskName", "general knowledge"]]}, {"text": "In AAAI , pages 4444\u20134451 .", "entities": []}, {"text": "Rachael Tatman . 2017 .", "entities": []}, {"text": "Gender and dialect bias in youtube \u2019s automatic captions .", "entities": []}, {"text": "In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing , pages 53\u201359 .", "entities": [[8, 9, "DatasetName", "Ethics"]]}, {"text": "Theresa Wilson , Janyce Wiebe , and Paul Hoffmann .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Recognizing contextual polarity in phraselevel sentiment analysis .", "entities": [[5, 7, "TaskName", "sentiment analysis"]]}, {"text": "In EMNLP , pages 347 \u2013 354 .", "entities": []}, {"text": "Brian Hu Zhang , Blake Lemoine , and Margaret Mitchell .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Mitigating unwanted biases with adversarial learning .", "entities": []}, {"text": "AIES .", "entities": []}, {"text": "Jieyu Zhao , Tianlu Wang , Mark Yatskar , Vicente Ordonez , and Kai - Wei Chang .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Men also like shopping : Reducing gender bias ampli\ufb01cation using corpus - level constraints .", "entities": []}, {"text": "EMNLP .", "entities": []}]