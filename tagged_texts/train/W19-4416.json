[{"text": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 159\u2013167 Florence , Italy , August 2 , 2019 .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics159The LAIX Systems in the BEA-2019 GEC Shared Task Ruobing LiyChuan", "entities": []}, {"text": "Wang Yefei Zha Yonghong Yu Shiman Guo Qiang Wang Yang LiuyHui Liny LAIX Inc. yfruobing.li , yang.liu , hui.lin g@liulishuo.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "In this paper , we describe two systems we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task .", "entities": []}, {"text": "We investigate competitive classi\ufb01cation models with bi - directional recurrent neural networks ( Bi - RNN ) and neural machine translation ( NMT ) models .", "entities": [[19, 21, "TaskName", "machine translation"]]}, {"text": "For different tracks , we use ensemble systems to selectively combine the NMT models , the classi\ufb01cation models , and some rules , and demonstrate that an ensemble solution can effectively improve GEC performance over single systems .", "entities": []}, {"text": "Our GEC systems ranked the \ufb01rst in the Unrestricted Track , and the third in both the Restricted Track and the Low Resource Track .", "entities": [[8, 9, "DatasetName", "Unrestricted"], [17, 18, "DatasetName", "Restricted"]]}, {"text": "1 Introduction Grammatical error correction ( GEC ) is the task of automatically correcting grammatical errors in text .", "entities": [[2, 5, "TaskName", "Grammatical error correction"]]}, {"text": "With the increasing number of language learners , GEC has gained more and more attention from educationists and researchers in the past decade .", "entities": []}, {"text": "The following is a GEC example : I [ fall !", "entities": []}, {"text": "fell ] asleep at 11 p.m. last [ nigh!night ] .", "entities": []}, {"text": "Here fallneeds to be corrected to its past tense form and nigh is a spelling mistake .", "entities": []}, {"text": "GEC is considered as a mapping task from incorrect sentences to correct sentences .", "entities": []}, {"text": "Incorrect sentences can be seen as being produced by adding noises to correct sentences .", "entities": []}, {"text": "The added noise does not happen randomly , but occurs when people learn or use the language according to a certain error distribution and language usage bias .", "entities": []}, {"text": "Initially , people used rule - based approaches to solve GEC problems ( Naber and Mi\u0142kowski , 2005 ) .", "entities": []}, {"text": "Rules are relatively easy to make but with poor generalization .", "entities": []}, {"text": "Later researchers began to treat GEC as a classi\ufb01cation task .", "entities": []}, {"text": "According to the grammatical information around the target word , classi\ufb01erscan be constructed to predict the true grammatical role of the target word .", "entities": []}, {"text": "One drawback of the classi\ufb01cation methods for GEC is that training different classi\ufb01ers for different error types may be resource - intensive and inef\ufb01cient since there are many grammatical error types .", "entities": []}, {"text": "Recently , translation methods have become the focus of research , and there is a clear trend that state - of - the - art GEC systems are being shifted from traditional NLP methods to NMT based methods .", "entities": []}, {"text": "In recent years , GEC performance has seen signi\ufb01cant improvement in some public GEC test sets ( Ge et al . , 2018 ) .", "entities": []}, {"text": "In CoNLL-2013 ( Ng et al . , 2013 ) and CoNLL-2014 ( Ng et al . , 2014 ) GEC Shared Task , machine learning based GEC methods emerged with relatively good performance .", "entities": []}, {"text": "Classi\ufb01cation methods achieved the best result in CoNLL-2013 ( Rozovskaya et al . , 2013 ) .", "entities": []}, {"text": "After that , statistical machine translation ( SMT ) methods began to show better performance in CoNLL2014 ( Felice et al . , 2014 ) .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "( Chollampatt et al . , 2016 ) was the \ufb01rst study to obtain the state - ofthe - art result with neural networks .", "entities": []}, {"text": "Then after ( Junczys - Dowmunt and Grundkiewicz , 2016 ) , machine translation methods became the mainstream in GEC solutions .", "entities": [[12, 14, "TaskName", "machine translation"]]}, {"text": "In addition , an RNNbased context model achieved better results than previous traditional classi\ufb01cation models ( Wang et al . , 2017 ) .", "entities": []}, {"text": "Using a CNN - based sequenceto - sequence architecture ( Gehring et al . , 2017 ) , ( Chollampatt and Ng , 2018 ) proposed the \ufb01rst end - to - end NMT model and reported the state - ofthe - art result .", "entities": []}, {"text": "As Transformer ( Vaswani et al . , 2017 ) plays an increasingly important role in sequence modeling , Transformer - based end - to - end NMT models began to lead the current GEC research ( Junczys - Dowmunt et al . , 2018 ;", "entities": [[1, 2, "MethodName", "Transformer"], [19, 20, "MethodName", "Transformer"]]}, {"text": "Grundkiewicz and Junczys - Dowmunt , 2018 ; Ge et al . , 2018 ; Zhao et al . , 2019 ) .", "entities": []}, {"text": "It is worth mentioning that ( Lichtarge et al . , 2019 ) used Wikipedia ed-", "entities": []}, {"text": "160its history corpus , which is huge but noisy , and gained a result very close to the state - of - the - art result .", "entities": []}, {"text": "Learning a GEC translation model from noisy data is a worthy future direction as the GEC parallel corpus is expensive to obtain .", "entities": []}, {"text": "This paper describes our two systems for the three tracks in the BEA-2019 GEC Shared Task ( Bryant et al . , 2019 ) .", "entities": []}, {"text": "We use two popular NMT models and two improved versions of neural classi\ufb01cation models to train the basic models .", "entities": []}, {"text": "Ensemble strategies are then used to combine outcomes from different models .", "entities": []}, {"text": "Our two systems for the three tracks are described in next section .", "entities": []}, {"text": "In Section 3 , we evaluate the systems on the development data and show the \ufb01nal results on the test data .", "entities": []}, {"text": "Section 4 concludes the paper and summarizes the future work .", "entities": []}, {"text": "2 System Overview 2.1 Restricted and Unrestricted Track We submitted the same system output for the Restricted and Unrestricted tasks .", "entities": [[4, 5, "DatasetName", "Restricted"], [6, 7, "DatasetName", "Unrestricted"], [16, 17, "DatasetName", "Restricted"], [18, 19, "DatasetName", "Unrestricted"]]}, {"text": "The system uses several ensemble methods to combine the CNNbased and Transformer - based translation models , described in details below .", "entities": [[11, 12, "MethodName", "Transformer"]]}, {"text": "2.1.1 CNN - based translation ensemble systems We found that CNN - based systems obtained the best results for some error types , likely due to some characteristics derived from CNN .", "entities": []}, {"text": "We trained four CNN - based ensemble systems , using the model architecture in ( Chollampatt and Ng , 2018 ) , but without reranking .", "entities": []}, {"text": "Four best combinations to build the ensemble systems were selected .", "entities": []}, {"text": "Unlike ( Chollampatt and Ng , 2018 ) , we did not use fastText ( Bojanowski et al . , 2017 ) to initialize word embeddings because we found no improvement on the development set by doing that .", "entities": [[13, 14, "MethodName", "fastText"], [24, 26, "TaskName", "word embeddings"]]}, {"text": "We tuned parameters for the system , such as batch size , word embedding dimension , etc . 2.1.2 Transformer - based translation systems Transformer is currently considered to be one of the most powerful models for sequence modeling .", "entities": [[9, 11, "HyperparameterName", "batch size"], [12, 15, "HyperparameterName", "word embedding dimension"], [19, 20, "MethodName", "Transformer"], [24, 25, "MethodName", "Transformer"]]}, {"text": "For GEC , some of the best recent results reported on CoNLL-2014 test set are obtained by Transformer - based translation models .", "entities": [[17, 18, "MethodName", "Transformer"]]}, {"text": "We trained eight Transformer - based translation models in a low resource translation paradigm ( JunczysDowmunt et al . , 2018 ) .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "We tuned parameters fordomain and error adaptation .", "entities": []}, {"text": "We also compared the results using 2 GPUs and 4 GPUs as the authors reported the difference in their Github repository1 .", "entities": []}, {"text": "2.1.3 Ensemble methods We expect to combine these models trained above into a more powerful system through effective ensemble methods .", "entities": []}, {"text": "Our ensemble work mainly focuses on rule - based solutions .", "entities": []}, {"text": "We will introduce two main modules \ufb01rst .", "entities": []}, {"text": "Con\ufb01dence Table", "entities": []}, {"text": "We can obtain the precision andF0:5metric on each error type through sentence alignment and error type classi\ufb01cation by Errant ( Bryant et al . , 2017 ) .", "entities": []}, {"text": "Errant provides performance statistics based on 55 error types and is also the tool used to evaluate this GEC shared task , thus we use the result of operation and error type span - level ( Bryant et al . , 2017 ) for a model or system as the con\ufb01dence table .", "entities": []}, {"text": "Con\ufb02ict Solver We often encounter GEC error con\ufb02icts when combining multiple models or systems .", "entities": []}, {"text": "For example , We love played soccer .", "entities": []}, {"text": "One system corrects played toplaying , while another system may correct played toto play .", "entities": []}, {"text": "When two different corrections occur in the same place , we need to consider which one to choose .", "entities": []}, {"text": "We solve this problem in a uni\ufb01ed pipeline , which can also be seen as an ensemble way : ( 1 ) We sort each group of con\ufb02icting corrections proposed by all the systems in a reverse order of location index and con\ufb01dence .", "entities": []}, {"text": "( 2 ) We apply three sub - strategies : \u000fWhen combining outcomes from different systems , we treat the precision in a con\ufb01dence table as the con\ufb01dence .", "entities": []}, {"text": "Each correction has its con\ufb01dence obtained by looking up the precision of the corresponding type of the correction in the table .", "entities": []}, {"text": "If two con\ufb02icting corrections are the same , we merge them and add \u000b to the con\ufb01dence of the correction ; otherwise , the correction with a lower con\ufb01dence will be discarded .", "entities": []}, {"text": "\u000fAfter combining outcomes , if the con\ufb01dence of a correction is lower than \f , the correction is discarded .", "entities": []}, {"text": "\u000f", "entities": []}, {"text": "is used to distinguish when it is more important to focus on the precision or F0:5of 1https://github.com/grammatical/ neural - naacl2018", "entities": []}, {"text": "161 Figure 1 : The architecture of the ensemble system in Restricted and Unrestricted Tracks .", "entities": [[11, 12, "DatasetName", "Restricted"], [13, 14, "DatasetName", "Unrestricted"]]}, {"text": "a correction .", "entities": []}, {"text": "When we move to the \ufb01nal ensemble with con\ufb01dence tables of existing systems , if the con\ufb01dence is larger than", "entities": []}, {"text": ", we select the correction proposed by the system that has the best F0:5on the type of this correction .", "entities": []}, {"text": "Otherwise , the correction by a system with the best precision is selected .", "entities": []}, {"text": "In Figure 1 , means the outcome is obtained by combining two systems represented by intersecting lines of two different colours .", "entities": []}, {"text": "If there are multiple on a line , it means the ensemble is over all of these on this line .", "entities": []}, {"text": "Figure 1 displays three types of ensemble methods based on all of the CNN - based and Transformer - based translation models .", "entities": [[17, 18, "MethodName", "Transformer"]]}, {"text": "\u000fCombine each CNN - based ensemble model with each of the selected \ufb01ve of the Transformer - based models .", "entities": [[15, 16, "MethodName", "Transformer"]]}, {"text": "This is noted as \u2018 ensemble - by-2 \u2019 .", "entities": []}, {"text": "\u000fPerform ensemble over all of the ensemble models relating to either CNN ensemble 1 or CNN ensemble 2 , noted as EoE ( Ensemble over Ensemble ) 1 and 2 . \u000fEnsemble each CNN ensemble model with some selected combinations of Transformer - based models to produce 16 strong ensemble system outcomes , represented as \u2018 Hybrid Ensemble \u2019 in Figure 1 .", "entities": [[41, 42, "MethodName", "Transformer"]]}, {"text": "It is where multiple lines of the same color are merged into one line in Figure 1 .", "entities": []}, {"text": "After getting all of the ensemble outcomes , we will do the \ufb01nal ensemble step : select the best con\ufb01dence for each type from each single or ensemble system to form the strongest \ufb01nal outcome .", "entities": []}, {"text": "In this ensemble step , we use the last aforementioned sub - strategy , and discard the error types with very low con\ufb01dence to boost the \ufb01nal performance .", "entities": []}, {"text": "2.2 Low Resource Track For the Low Resource Track we developed different individual systems and used an ensemble method to combine them .", "entities": []}, {"text": "For the translation model , we did not obtain very strong performance because the training data is limited .", "entities": []}, {"text": "We also explored the noisy Wikipedia edit history corpus for the Transformer - based translation model .", "entities": [[11, 12, "MethodName", "Transformer"]]}, {"text": "However , we noticed that , for some error types with clear de\ufb01nitions , the classi\ufb01ers trained on a large amount of native corpus have good performance .", "entities": []}, {"text": "In addition , we made some grammatical rules to correct errors and adopted an off - the - shelf spelling checker ( Kelly , 2006 ) .", "entities": []}, {"text": "Finally , we leverage a sim-", "entities": []}, {"text": "162ple ensemble method to combine all of the classi\ufb01ers , rules , spelling checker and translation models .", "entities": []}, {"text": "Note that for the Restricted and Unrestricted tracks , we did not observe any gain from the classi\ufb01cation models or the rule - based methods , therefore only the translation systems were used for those tracks .", "entities": [[4, 5, "DatasetName", "Restricted"], [6, 7, "DatasetName", "Unrestricted"]]}, {"text": "2.2.1 Classi\ufb01cation model After an analysis of the development sets , we decided to build classi\ufb01ers for eight common error types .", "entities": []}, {"text": "Based on ( Wang et al . , 2017 ) , we developed two classi\ufb01cation model structures for the eight error types .", "entities": []}, {"text": "( A ) Bi - GRU context model Figure 2 shows the bi - directional GRU context model we use to determine the right grammatical category for a target word .", "entities": [[5, 6, "MethodName", "GRU"], [15, 16, "MethodName", "GRU"]]}, {"text": "The concatenated left and right source states of the target word form the contextual semantic vector representation .", "entities": []}, {"text": "This is used as a query to calculate the attention weight at .", "entities": []}, {"text": "An attention vector Ctis then computed as the weighted average , according to at , over all the source states .", "entities": []}, {"text": "Ctis then fed through a fully connected layer and softmax layer to produce the predictive distribution .", "entities": [[9, 10, "MethodName", "softmax"]]}, {"text": "Figure 2 : Bi - GRU Context model structure .", "entities": [[5, 6, "MethodName", "GRU"]]}, {"text": "We use this to train models for the following error types : Subject - verb agreement , Article , Plural or singular noun , Verb form , Preposition substitution , Missing comma and Period comma substitution .", "entities": []}, {"text": "Labels for each task were extracted automatically from the native corpus through part - ofspeech tagging tools.(B )", "entities": []}, {"text": "Pointer context model The classi\ufb01ers above use the same classi\ufb01cation labels for different target words .", "entities": []}, {"text": "We also need a classi\ufb01cation model to deal with the problem as in the Word form task , where each word has a different set of predictive labels ( as shown for word \u2018 gone \u2019 in Figure 3 ) .", "entities": []}, {"text": "Inspired by the Pointer network model ( Vinyals et al . , 2015 ) , we proposed the pointer context model .", "entities": [[0, 1, "DatasetName", "Inspired"], [3, 5, "MethodName", "Pointer network"]]}, {"text": "Figure 3 shows the pointer context model that takes the target word \u2019s confusion set as the label candidates .", "entities": []}, {"text": "The computation path is the same as the Bi - GRU model structure .", "entities": [[10, 11, "MethodName", "GRU"]]}, {"text": "We concatenate the target word \u2019s char - based embedding and Ctto obtainC1 t , and then use it as the query to compute dot product", "entities": []}, {"text": "a1 twith each of the word embeddings in the confusion set .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "a1 t is then fed through a softmax layer to produce the predictive distribution .", "entities": [[7, 8, "MethodName", "softmax"]]}, {"text": "This model is very effective at dealing with varying number of candidates as seen in the Word form task .", "entities": []}, {"text": "Figure 3 : Pointer Context model structure .", "entities": []}, {"text": "2.2.2 NMT model We use the same Transformer - based translation model mentioned in Subsection 3.2.2 .", "entities": [[7, 8, "MethodName", "Transformer"]]}, {"text": "Due to the limitation of the corpus , we leverage the Wiked ( Grundkiewicz and Junczys - Dowmunt , 2014 ) as our training corpus for the NMT model .", "entities": []}, {"text": "2.2.3 Rules and spell checker", "entities": []}, {"text": "We have implemented the following GEC rules .", "entities": []}, {"text": "( 1)\u2018a \u2019 and \u2018 an \u2019 substitution .", "entities": []}, {"text": "For this problem , we made rules based on the \ufb01rst phoneme of the following word .", "entities": []}, {"text": "( 2)Comma deletion .", "entities": []}, {"text": "After a prepositional", "entities": []}, {"text": "163Track FCE Lang-8 NUCLEW&I+ LOCNESSCommon CrawlWikedWiki dumps Restricted Track", "entities": [[1, 2, "DatasetName", "FCE"], [7, 8, "DatasetName", "Restricted"]]}, {"text": "Yes", "entities": []}, {"text": "Yes", "entities": []}, {"text": "Yes Yes Yes - Unrestricted Track", "entities": [[4, 5, "DatasetName", "Unrestricted"]]}, {"text": "Yes", "entities": []}, {"text": "Yes", "entities": []}, {"text": "Yes", "entities": []}, {"text": "Yes Yes -", "entities": []}, {"text": "Low Resource Track - - - -", "entities": []}, {"text": "Yes Yes Yes Table 1 : Corpus used for training in corresponding track .", "entities": []}, {"text": "Seed Batch sizeWord embedding dimensionNumber of input channelsNumber of output channelsNumber of layersF0:5 5001 32 128 256 256 7 0.3370 5002 32 128 256 256 7 0.3219 5003 32 128 256 256 7 0.3370 5004 32 128 256 256 7 0.3411 5005 32 128 256 256 7 0.3449 5012 32 256 512 512 10 0.3339 5102 32 128 512 512 7 0.3329 7011 16 256 512 512 7 0.3328 7205 32 256 512 512 14 0.3328 Table 2 : Results of tuned single CNN - based translation models on the development set .", "entities": []}, {"text": "phrase at the beginning of a sentence , we add a comma .", "entities": []}, {"text": "For example , \u201c Despite our differences we collaborate well . \u201d", "entities": []}, {"text": "A comma should be added after Despite our differences .", "entities": []}, {"text": "( 3)Orthography mistakes .", "entities": []}, {"text": "We obtain statistics of named entities that require initial capitalization and make a white list using the Wikipedia corpus .", "entities": []}, {"text": "If a word is on the white list , we will force the conversion to the initial capitalization form .", "entities": []}, {"text": "In addition , we use Pyenchant as our spell checker ( Kelly , 2006 ) .", "entities": []}, {"text": "The top candidate is considered to be the correction .", "entities": []}, {"text": "2.2.4", "entities": []}, {"text": "Ensemble We use the con\ufb02ict solver described above to do the ensemble for all of the outputs of the classi\ufb01ers , rules , spell checker and NMT model .", "entities": []}, {"text": "3 Experiments 3.1 Data Sets Table 1 lists the data sets used in Restricted Track and Unrestricted Track , including FCE ( Yannakoudakis et al . , 2011 ) , Lang-82(Mizumoto et al . , 2012 ) , NUCLE ( Ng et al . , 2014 ) , W&I+LOCNESS", "entities": [[13, 14, "DatasetName", "Restricted"], [16, 17, "DatasetName", "Unrestricted"], [20, 21, "DatasetName", "FCE"]]}, {"text": "( Bryant et al . , 2019 ) and Common Crawl .", "entities": [[9, 11, "DatasetName", "Common Crawl"]]}, {"text": "We use Common Crawl to pretrain the decoder parameters for the Transformer - based translation model .", "entities": [[2, 4, "DatasetName", "Common Crawl"], [11, 12, "MethodName", "Transformer"]]}, {"text": "FCE , Lang-8 , NUCLE and W&I are used to train all of the translation models .", "entities": [[0, 1, "DatasetName", "FCE"]]}, {"text": "2https://lang-8.comIt is worth noting that we did data augmentation for W&I to train all of the translation models .", "entities": [[7, 9, "TaskName", "data augmentation"]]}, {"text": "The data sets used in Low Resource Track include Wiked , Wikipedia Dumps and Common Crawl .", "entities": [[14, 16, "DatasetName", "Common Crawl"]]}, {"text": "All of the classi\ufb01ers are trained on Wikipedia Dumps and the translation model is trained on Wiked corpus .", "entities": []}, {"text": "For Wiked corpus , we did some data cleaning work .", "entities": []}, {"text": "We discarded some noisy sentences that include error types such as U : OTHER , R : OTHER , R : NOUN , etc .", "entities": []}, {"text": "The development set from W&I+LOCNESS are used in all the tracks .", "entities": []}, {"text": "Following the data pre - processing pipeline used to generate the data provided by the shared task , we tokenize all of the data using spaCy3 . 3.2 Restricted and Unrestricted Track 3.2.1 CNN - based translation ensemble models We added the W&I corpus eight times to the training corpus for domain adaptation .", "entities": [[28, 29, "DatasetName", "Restricted"], [30, 31, "DatasetName", "Unrestricted"], [51, 53, "TaskName", "domain adaptation"]]}, {"text": "Table 2 shows the performance of the single CNN - based translation models .", "entities": []}, {"text": "All the parameters in Table 2 are tuned over the W&I+LOCNESS development set .", "entities": []}, {"text": "Table 3 shows the results of the four CNN - based ensemble systems .", "entities": []}, {"text": "We use ensembles in the same way as ( Chollampatt and Ng , 2018 ) .", "entities": []}, {"text": "The above results prove that the ensemble method has yielded a very large improvement in this task .", "entities": []}, {"text": "3https://spacy.io", "entities": []}, {"text": "164Ensemble index Combination Precision RecallF0:5 1 5012,5102,7011,7205 0.5076 0.2195 0.4021 2 5001,5002,5003,5004 0.5003 0.1951 0.3811 3 5005,5012,5102,7205 0.5156 0.2150 0.4029 4 5005,5012,7011,7205 0.5152 0.2159 0.4034", "entities": [[3, 4, "MetricName", "Precision"]]}, {"text": "Table", "entities": []}, {"text": "3 : Results of CNN - based ensemble systems on the development set .", "entities": []}, {"text": "Model index Error weightCopy number of W&I trainsetGPU numberPrecision RecallF0:5 1", "entities": [[2, 3, "MetricName", "Error"]]}, {"text": "3 10 2 0.4585 0.3525 0.4325 2 3 10 4 0.4602 0.3514 0.4333 3 3 8 2 0.4592 0.3575 0.4345 4 3 8 4 0.4641 0.3548 0.4372 5 3 15 2 0.4494 0.3479 0.4247 6 3 15 4 0.4648 0.3467 0.4352 7 2 10 2 0.4715 0.3303 0.4343 8 2 10 4 0.4868 0.3412 0.4485 Table 4 : Results of Transformer - based translation models on the development set .", "entities": [[60, 61, "MethodName", "Transformer"]]}, {"text": "3.2.2 Transformer - based translation models We trained eight Transformer - based translation models in different combinations of error adaptation , domain adaptation , and GPU set .", "entities": [[1, 2, "MethodName", "Transformer"], [9, 10, "MethodName", "Transformer"], [21, 23, "TaskName", "domain adaptation"]]}, {"text": "In Table 4 , we notice that a smaller error weight yields higher precision and a slight decrease in recall .", "entities": []}, {"text": "We set the copy number as 8 , 10 and 15 , and \ufb01nd that domain adaptation has no signi\ufb01cant effect on the results .", "entities": [[15, 17, "TaskName", "domain adaptation"]]}, {"text": "4 GPU is obviously better than 2 GPU sets , which is probably because of the larger batch size accumulation for gradient calculation .", "entities": [[17, 19, "HyperparameterName", "batch size"]]}, {"text": "3.2.3 Ensemble methods As described in Section 2.1.3 , we need to ensemble all of the CNN - based and Transformer - based translation models .", "entities": [[20, 21, "MethodName", "Transformer"]]}, {"text": "We have already introduced the con\ufb01guration of the single models in Section 3.2.1 and Section 3.2.2 .", "entities": []}, {"text": "Next we will describe the con\ufb01guration of the ensemble system .", "entities": []}, {"text": "For the three ensemble types : Ensemble - by-2 , EoE and Hybrid Ensemble , as shown in Figure 1 , we used different parameters in the con\ufb02ict solver .", "entities": []}, {"text": "We did a small - scale grid search for the parameters in Table 5 .", "entities": []}, {"text": "When combining two models that are not strong , we expect a higher recall so \f was not high .", "entities": []}, {"text": "For EoE and hybrid ensemble , we expect a higher precision so that they can provide high quality single type performance .", "entities": []}, {"text": "Corrections proposed by multiple models are given higher weights ( controlled by \u000b ) .", "entities": []}, {"text": "If the con\ufb01dence of a correctionEnsemble method", "entities": []}, {"text": "Ensemble - by-2 0.2 0.4 EoE 0.15 0.8 Hybrid ensemble 0.15 0.62 Final ensemble 0.0 0.5 0.52 Table 5 : Parameters in the con\ufb02ict solver for the ensemble methods in Restricted and Unrestricted Track .", "entities": [[30, 31, "DatasetName", "Restricted"], [32, 33, "DatasetName", "Unrestricted"]]}, {"text": "\ufb01nally reaches \f , the correction will be adopted .", "entities": []}, {"text": "In the \ufb01nal ensemble , we select the best performance on each type from each single system or ensemble system and discard the corrections with low precision ( controlled by \f ) .", "entities": []}, {"text": "To get higher F0:5 , in the case where the precision is greater than a prede\ufb01ned threshold ( controlled by", "entities": []}, {"text": ") , we will choose the model with the highest F0:5for the corresponding error type .", "entities": []}, {"text": "The \ufb01nal outcome of the data set is then fed through the translation models and ensemble systems again to do a second pass correction .", "entities": []}, {"text": "3.2.4 Results Table 6 summarizes some results on the development set and gives the of\ufb01cial test result .", "entities": []}, {"text": "We can see that the individual CNN or Transformer - based translation models perform reasonably well , and the ensemble methods consistently outperform the individual systems .", "entities": [[8, 9, "MethodName", "Transformer"]]}, {"text": "The second pass correction further improves the performance , and the last post - processing step boosts both recall and F0:5 .", "entities": []}, {"text": "165Step Precision RecallF0:5 Best CNN - based ensemble model 0.5152 0.2159 0.4034 Best Transformer - based translation model 0.4868 0.3412 0.4485 Best ensemble - by-2 0.5281 0.3434 0.4768 Best hybrid ensemble 0.5885 0.3278 0.5078 + Combine best performance 0.6283 0.3269 0.5305 + Second pass 0.6272 0.3412 0.5372 Submission system ( + Post - processing , Dev set ) 0.6243 0.3457 0.5376 Submission system ( Test set )", "entities": [[1, 2, "MetricName", "Precision"], [13, 14, "MethodName", "Transformer"]]}, {"text": "0.7317 0.4950 0.6678 Table 6 : Results of Restricted and Unrestricted Track .", "entities": [[8, 9, "DatasetName", "Restricted"], [10, 11, "DatasetName", "Unrestricted"]]}, {"text": "Ensemble method", "entities": []}, {"text": "Ensemble for all 0.15 0.3 Final ensemble 0.0 0.25 0.3 Table 7 : Parameters for the ensemble method in Low Resource Track .", "entities": []}, {"text": "Table 6 also shows that there is a big gap between the performance on the development set and test set , partly because the \ufb01nal test set uses a combination of \ufb01ve annotators .", "entities": []}, {"text": "3.3 Low Resource Track 3.3.1 Classi\ufb01cation models We trained classi\ufb01ers for seven error types : Subject - verb agreement , Article , Plural or singular noun , Verb form , Preposition substitution , Missing comma and Period comma substitution and Word form .", "entities": []}, {"text": "As mentioned in Subsection 2.2.1 , Word form model is trained using the Pointer Context model .", "entities": []}, {"text": "The other error types are trained using Bi - GRU Context model .", "entities": [[9, 10, "MethodName", "GRU"]]}, {"text": "3.3.2 NMT model A Transformer - based translation model is trained on the \ufb01ltered Wiked corpus .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "The model architecture follows that in ( Junczys - Dowmunt et al . , 2018 ) .", "entities": []}, {"text": "Although the performance of the NMT model is not strong , it provides good performance equivalent to the classi\ufb01ers for some error types .", "entities": []}, {"text": "3.3.3 Ensemble", "entities": []}, {"text": "We use one con\ufb02ict solver to combine the outputs from all of the systems in this task .", "entities": []}, {"text": "Parameters for this ensemble system are shown in Table 7 . 3.3.4 Results Table 8 shows results for different systems ( for classi\ufb01cation models , different error type classi\ufb01ers ) on the development set , and the overall re - Model Precision RecallF0:5 Rule 0.4497 0.0216 0.0905 Spelling 0.3188 0.0363 0.1248 Article 0.4367 0.0134 0.0597 Missing comma 0.4729 0.0503 0.1763 Period comma substitution0.4561 0.0070 0.0328 Plural or singular noun0.3203 0.0121 0.0524 Preposition substitution0.3713 0.0101 0.0454 Subject - verb agreement0.3981 0.0115 0.0517 Verb form 0.4135 0.0074 0.0344 Word form 0.4506 0.0294 0.1164 NMT 0.1279 0.1480 0.1315 Submission system ( Dev set)0.4970 0.1686 0.3577 Submission system ( Test set)0.6201 0.3125 0.5181 Table 8 : Results of Low Resource Track .", "entities": [[41, 42, "MetricName", "Precision"]]}, {"text": "sults on the test set .", "entities": []}, {"text": "We can see that the base systems are not very strong , and the ensemble system signi\ufb01cantly improves the performance .", "entities": []}, {"text": "The difference between the development set and test set can still be observed in this task .", "entities": []}, {"text": "4 Conclusions and Future Work We have presented two different systems for the three GEC tracks .", "entities": []}, {"text": "When there is a suf\ufb01cient parallel learner corpus , such as in Restricted Track and Unrestricted Track , the NMT ensemble model is the best choice to implement a GEC system .", "entities": [[12, 13, "DatasetName", "Restricted"], [15, 16, "DatasetName", "Unrestricted"]]}, {"text": "We have evaluated two kinds of NMT models : CNN - based and Transformer - based translation models .", "entities": [[13, 14, "MethodName", "Transformer"]]}, {"text": "We have also explored different ensemble strategies from multiple base mod-", "entities": []}, {"text": "166els to maximize the overall system performance .", "entities": []}, {"text": "Finally we reached the result of F0:5=0.6678 on the of\ufb01cial test set in Restricted Track andUnrestricted Track , ranking the third in the Restricted track4 .", "entities": [[13, 14, "DatasetName", "Restricted"], [23, 24, "DatasetName", "Restricted"]]}, {"text": "It is worth noting that there is a huge gap between the results on the development set and the test set , which suggests that there might be an unneglectable mismatch between the development set and the test set .", "entities": []}, {"text": "Indeed , the development set is annotated by one annotator , while the test set is annotated by \ufb01ve , as announced of\ufb01cially .", "entities": []}, {"text": "ForLow", "entities": []}, {"text": "Resource Track , there is a lack of parallel learner corpus , and thus we rely less on the translation models .", "entities": []}, {"text": "We have built eight classi\ufb01ers trained on Wikipedia dumps according to different error types and an NMT model trained on the Wikipedia edits history corpus .", "entities": []}, {"text": "By a simple ensemble method , we reached F0:5=0.5181 , placing our system in the third place in Low Resource Track .", "entities": []}, {"text": "Although GEC has reached the human level performance on some GEC test sets , there is still room for improvement .", "entities": []}, {"text": "In a low resource setup , how to deal with the huge but noisy data is worth exploring .", "entities": []}, {"text": "( Lichtarge et al . , 2019 ) gave a good solution on this topic , but more work needs to be done .", "entities": []}, {"text": "Second , we will investigate methods such as the reinforcement learning based method ( Wu et al . , 2018 ) to address the mismatch between the training objectives and evaluation methods in GEC .", "entities": []}, {"text": "References Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Enriching word vectors with subword information .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics .", "entities": []}, {"text": "Christopher Bryant , Mariano Felice , and Ted Briscoe . 2017 .", "entities": []}, {"text": "Automatic annotation and evaluation of error types for grammatical error correction .", "entities": [[8, 11, "TaskName", "grammatical error correction"]]}, {"text": "Christopher Bryant , Mariano Felice , \u00d8istein E. Andersen , and Ted Briscoe . 2019 .", "entities": []}, {"text": "The BEA-2019 Shared Task on Grammatical Error Correction .", "entities": [[5, 8, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the 14th Workshop on Innovative Use of NLP for Building Educational Applications .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shamil Chollampatt and Hwee Tou Ng . 2018 .", "entities": []}, {"text": "A multilayer convolutional encoder - decoder neural network for grammatical error correction .", "entities": [[9, 12, "TaskName", "grammatical error correction"]]}, {"text": "In Proceedings of the 32nd AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "4https://www.cl.cam.ac.uk/research/nl/ bea2019st/#resultsShamil Chollampatt , Kaveh Taghipour , and Hwee Tou Ng . 2016 .", "entities": []}, {"text": "Neural network translation models for grammatical error correction .", "entities": [[5, 8, "TaskName", "grammatical error correction"]]}, {"text": "In Proceedings of the 25th International Joint Conference on Artifcial Intelligence .", "entities": []}, {"text": "Mariano Felice , Zheng Yuan , \u00d8istein E. Andersen , Helen Yannakoudakis , and Ekaterina Kochmar .", "entities": [[10, 11, "DatasetName", "Helen"]]}, {"text": "2014 .", "entities": []}, {"text": "Grammatical error correction using hybrid systems and type \ufb01ltering .", "entities": [[0, 3, "TaskName", "Grammatical error correction"]]}, {"text": "Eighteenth Conference on Computational Natural Language Learning .", "entities": []}, {"text": "Tao Ge , Furu Wei , and Ming Zhou .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Reaching human - level performance in automatic grammatical error correction : An empirical study .", "entities": [[7, 10, "TaskName", "grammatical error correction"]]}, {"text": "Microsoft Research Technical Report .", "entities": []}, {"text": "Jonas Gehring , Michael Auli , David Grangier , Denis Yarats , and Yann N. Dauphin . 2017 .", "entities": []}, {"text": "Convolutional sequence to sequence learning .", "entities": [[1, 4, "MethodName", "sequence to sequence"]]}, {"text": "Proceedings of the 34th International Conference on Machine Learning .", "entities": []}, {"text": "Roman Grundkiewicz and Marcin Junczys - Dowmunt .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The wiked error corpus : A corpus of corrective wikipedia edits and its application to grammatical error correction .", "entities": [[15, 18, "TaskName", "grammatical error correction"]]}, {"text": "In Advances in Natural Language Processing \u2013 Lecture Notes in Computer Science , volume 8686 , pages 478\u2013490 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Roman Grundkiewicz and Marcin Junczys - Dowmunt .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Near human - level performance in grammatical error correction with hybrid machine translation .", "entities": [[6, 9, "TaskName", "grammatical error correction"], [11, 13, "TaskName", "machine translation"]]}, {"text": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics .", "entities": []}, {"text": "Marcin Junczys - Dowmunt and Roman Grundkiewicz . 2016 .", "entities": []}, {"text": "Phrase - based machine translation is state - ofthe - art for automatic grammatical error correction .", "entities": [[3, 5, "TaskName", "machine translation"], [13, 16, "TaskName", "grammatical error correction"]]}, {"text": "The 2016 Conference on Empirical Methods on Natural Language Processing .", "entities": []}, {"text": "Marcin Junczys - Dowmunt , Roman Grundkiewicz , Shubha Guha , and Kenneth Hea\ufb01eld .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Approaching neural grammatical error correction as a low - resource machine translation task .", "entities": [[2, 5, "TaskName", "grammatical error correction"], [10, 12, "TaskName", "machine translation"]]}, {"text": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics .", "entities": []}, {"text": "Ryan Kelly .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Pyenchant .", "entities": []}, {"text": "Jared Lichtarge , Christopher Alberti , Shankar Kumar , Noam Shazeer , and Niki Parmar .", "entities": [[7, 8, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Weakly supervised grammatical error correction using iterative decoding .", "entities": [[2, 5, "TaskName", "grammatical error correction"]]}, {"text": "Tomoya Mizumoto , Yuta Hayashibe , Mamoru Komachi , Masaaki Nagata , and Yu Matsumoto .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "The effect of learner corpus size in grammatical error correction of esl writings .", "entities": [[7, 10, "TaskName", "grammatical error correction"]]}, {"text": "In Proceedings of COLING 2012 .", "entities": []}, {"text": "Daniel Naber and Marcin Mi\u0142kowski .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "LanguageTool .", "entities": []}, {"text": "167Hwee Tou Ng , Siew Mei Wu , Ted Briscoe , Christian Hadiwinoto , Raymond Hendy Susanto , and Christopher Bryant .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The conll-2014 shared task on grammatical error correction .", "entities": [[1, 4, "DatasetName", "conll-2014 shared task"], [5, 8, "TaskName", "grammatical error correction"]]}, {"text": "Eighteenth Conference on Computational Natural Language Learning .", "entities": []}, {"text": "Hwee Tou Ng , Siew Mei Wu , Yuanbin Wu , Christian Hadiwinoto , and Joel Tetreault .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "The conll2013 shared task on grammatical error correction .", "entities": [[5, 8, "TaskName", "grammatical error correction"]]}, {"text": "Seventeenth Conference on Computational Natural Language Learning .", "entities": []}, {"text": "Alla Rozovskaya , Kai - Wei Chang , Mark Sammons , and Dan Roth .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "The university of illinois system in the conll-2013 shared task .", "entities": []}, {"text": "Seventeenth Conference on Computational Natural Language Learning .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "31st Conference on Neural Information Processing Systems .", "entities": []}, {"text": "Oriol Vinyals , Meire Fortunato , and Navdeep Jaitly . 2015 .", "entities": []}, {"text": "Pointer networks .", "entities": []}, {"text": "Proceedings of the 28th International Conference on Neural Information Processing Systems .", "entities": []}, {"text": "Chuan Wang , RuoBing Li , and Hui Lin .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Deep context model for grammatical error correction .", "entities": [[4, 7, "TaskName", "grammatical error correction"]]}, {"text": "Proceedings of the Seventh ISCA workshop on Speech and Language Technology in Education 2017 .", "entities": []}, {"text": "Lijun Wu , Fei Tian , Tao Qin , Jianhuang Lai , and TieYan Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A study of reinforcement learning for neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "Helen Yannakoudakis , Ted Briscoe , and Ben Medlock .", "entities": [[0, 1, "DatasetName", "Helen"]]}, {"text": "2011 .", "entities": []}, {"text": "A new dataset and method for automatically grading esol texts .", "entities": []}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies .", "entities": []}, {"text": "Wei Zhao , Liang Wang , Kewei Shen , Ruoyu Jia , and Jingming Liu . 2019 .", "entities": []}, {"text": "Improving grammatical error correction via pre - training a copy - augmented architecture with unlabeled data .", "entities": [[1, 4, "TaskName", "grammatical error correction"]]}, {"text": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics .", "entities": []}]