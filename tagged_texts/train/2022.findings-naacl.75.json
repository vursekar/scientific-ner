[{"text": "Findings of the Association for Computational Linguistics : NAACL 2022 , pages 1010 - 1018 July 10 - 15 , 2022 \u00a9 2022 Association for Computational Linguistics Restoring Hebrew Diacritics Without a Dictionary Elazar Gershuni Technion \u2013 Israel Institute of Technology Haifa , Israel elazarg@gmail.comYuval Pinter Department of Computer Science Ben - Gurion University of the Negev Beer Sheva , Israel uvp@cs.bgu.ac.il", "entities": []}, {"text": "Abstract We demonstrate that it is feasible to accurately diacritize Hebrew script without any human - curated resources other than plain diacritized text .", "entities": []}, {"text": "We present NAKDIMON , a two - layer character - level LSTM , that performs on par with much more complicated curationdependent systems , across a diverse array of modern Hebrew sources .", "entities": [[11, 12, "MethodName", "LSTM"]]}, {"text": "The model is accompanied by a training set and a test set , collected from diverse sources .", "entities": []}, {"text": "1 Introduction The vast majority of modern Hebrew texts are written in a letter - only version of the Hebrew script , one which omits the diacritics present in the full diacritized , or dotted variant.1Since most vowels are encoded via diacritics , the pronunciation of words in the text is left underspecified , and a considerable mass of tokens becomes ambiguous .", "entities": []}, {"text": "This ambiguity forces readers and learners to infer the intended reading using syntactic and semantic context , as well as common sense ( Bentin and Frost , 1987 ; Abu - Rabia , 2001 ) .", "entities": []}, {"text": "In NLP systems , recovering such signals is difficult , and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text ( Shacham and Wintner , 2007 ; Goldberg and Elhadad , 2010 ; Tsarfaty et al . , 2019 ) .", "entities": []}, {"text": "As an example , the sentence in Table 1 ( a ) will be resolved by a typical reader as ( b ) in most reasonable contexts , knowing that the word \u201c softly \u201d may characterize landings .", "entities": []}, {"text": "In contrast , an automatic system processing Hebrew text may not be as sensitive to this kind of grammatical knowledge and instead interpret the undotted token as the more 1Also known as pointed text , or via the Hebrew term for the diacritic marks , nikkud / niqqud .(a)/\u05b9\u05d3\u05e0\u05b5f\u05d4\u05de\u05d8\u05d5\u05e1\u05e0\u05d7\u05ea\u05d1\u05e8\u05db\u05d5\u05ea\u05d3\u05e8\u05b9\u05d5", "entities": []}, {"text": "hamatos naxat ? ? ? ?", "entities": []}, {"text": "\u2018 The plane landed ( unspecified ) \u2019 ( b)/\u05b9\u05d3\u05e0\u05b5f\u05d4\u05b7\u05ea\u05b7\u05e4 / \ufb3b\ufb35\u05ea\u05d3\u05e8\u05b9\u05d52\u05e0 / \u05d7\u05b7\u05ea\ufb31\u05b0\u05e8 u\u05d4\u05b7\ufb3e\u05b8\u05d8\ufb4b\u05e1\u05e1\u05ea\u05b7\u05de\u05b7\u05e7\u05e0   hamatos naxat b - rakut \u2018 The plane landed softly \u2019 ( c)/\u05b9\u05d3\u05e0\u05b5f\u05e1\u05ea\u05b7\u05de\u05b7\u05e7 / \u05db\ufb4b\u05ea\u05d3\u05e8\u05b9\u05d52\u05e0 / \u05d7\u05b7\u05ea\ufb31\u05b0\u05e8 u\u05d4\u05b7\ufb3e\u05b8\u05d8\ufb4b\u05e1\u05e1\u05ea\u05b7\u05de\u05b7\u05e7\u05e0   hamatos naxat braxot \u2018 The plane landed congratulations \u2019 Table 1 : An example of an undotted Hebrew text ( a ) ( written right to left ) which can be interpreted in at least two different ways ( b , c ) , dotted and pronounced differently , but only ( b ) makes grammatical sense .", "entities": []}, {"text": "frequent word in ( c ) , harming downstream performance .", "entities": []}, {"text": "One possible way to overcome this problem is by adding diacritics to undotted text , or dotting , implemented using data - driven algorithms trained on dotted text .", "entities": []}, {"text": "Obtaining such data is not trivial , even given correct pronunciation : the standard Tiberian diacritic system contains several sets of identicallyvocalized forms , so while most Hebrew speakers easily read dotted text , they are unable to produce it .", "entities": []}, {"text": "Moreover , the process of manually adding diacritics in either handwritten script or through digital input devices is mechanically cumbersome .", "entities": []}, {"text": "Thus , the overwhelming majority of modern Hebrew text is undotted , and manually dotting it requires expertise .", "entities": []}, {"text": "The resulting scarcity of available dotted text in modern Hebrew contrasts with Biblical and Rabbinical texts which , while dotted , manifest a very different language register .", "entities": []}, {"text": "This state of affairs allows individuals and companies to offer dotting as paid services , either by experts or automatically , e.g. the Morfix engine by Melingo.2 Such usage practices also force a disconnect in the NLP pipeline , requiring an API call into an external 2https://nakdan.morfix.co.il/1010", "entities": []}, {"text": "service whose parameters can not be updated .", "entities": []}, {"text": "Existing computational approaches to dotting are manifested as complex , multi - resourced systems which perform morphological analysis on the undotted text and look undotted words up in handcrafted dictionaries as part of the dotting process .", "entities": [[16, 18, "TaskName", "morphological analysis"]]}, {"text": "Dicta \u2019s Nakdan ( Shmidman et al . , 2020 ) , the current state - of - the - art , applies such methods in addition to applying multiple neural networks over different levels of the text , requiring manual annotation not only for dotting but also for morphology .", "entities": []}, {"text": "Among the resources it uses are a diacritized corpus of 3 M tokens and a POS - tagged corpus of 300 K tokens .", "entities": []}, {"text": "Training the model takes several weeks.3 In this work , we set out to simplify the dotting task as much as possible to standard modules .", "entities": []}, {"text": "We introduce a large corpus of semi - automatically dotted Hebrew , collected from various sources , and use it to train an RNN - based model .", "entities": []}, {"text": "Our system , NAKDIMON , accepts the undotted character sequence as its input , consults no external resources or lexical components , and produces diacritics for each character , resulting in dotted text whose quality is comparable to that of the commercial Morfix , on both character - level and word - level accuracy .", "entities": [[53, 54, "MetricName", "accuracy"]]}, {"text": "Our model is easy to integrate within larger systems that perform end - to - end Hebrew processing tasks , as opposed to the existing proprietary dotters .", "entities": []}, {"text": "To our knowledge , this is the first attempt at a \u201c light \u201d model for Hebrew dotting since early HMM - based systems ( Kontorovich , 2001 ; Gal , 2002 ) .", "entities": []}, {"text": "We introduce a novel test set for Modern Hebrew dotting , derived from larger and more diverse sources than existing datasets .", "entities": []}, {"text": "In experiments over our dataset , we show that our system is particularly useful in the main use case of modern dotting , which is to convey the desired pronunciation to a reader , and that the errors it makes should be more easily detectable by non - professionals than Dicta\u2019s.4 2", "entities": []}, {"text": "Task and Datasets 2.1 Dotting as Sequence Labeling The input to the dotting task consists of a sequence of characters .", "entities": []}, {"text": "Each of the characters is assigned three values , from three separate diacritic categories : one category for the dot distinguishing 3Private communication .", "entities": []}, {"text": "4The system is available at https://nakdimon.org , and the source code is a available at https://github . com / elazarg / nakdimon .shin ( /\u05b9\u05d3\u05e0\u05b5f\ufb2a\u05d3\u05e8\u05b9\u05d5 ) from sin(/\u05b9\u05d3\u05e0\u05b5f\ufb2b\u05d3\u05e8\u05b9\u05d5 , ) two consonants sharing a base character /\u05b9\u05d3\u05e0\u05b5f\u05e9\u05d3\u05e8\u05b9\u05d5 ; another for the presence of dagesh /mappiq , a central dot affecting pronunciation of some consonants , e.g. /\u05b9\u05d3\u05e0\u05b5f\ufb44\u05d3\u05e8\u05b9\u05d5 / p/ from /\u05b9\u05d3\u05e0\u05b5f\u05b9\u05e0\u05b9\u05e8\u05b5\u05d6 / \u05d3\u05e8\u05b9\u05d5j\u05e4\u05e0\u05b4\u05b9 / f/ , but also present elsewhere ; and one for all other diacritic marks , which mostly determine vocalization , e.g. /\u05b9\u05d3\u05e0\u05b5f\u05e1\u05ea\u05b7\u05de\u05b7\u05e7 / \u05d3\u05e8\u05b9\u05d52\u05d3 / da/ vs. /\u05b9\u05d3\u05e0\u05b5f\u05dc\u05b9\u05d2\u05b5\u05e1 / \u05d3\u05e8\u05b9\u05d52\u05d3 / de/. Diacritics of different categories may co - occur on single letters , e.g. /\u05b9\u05d3\u05e0\u05b5f\u05e1\u05ea\u05b7\u05de\u05b7\u05e7 / \u05d3\u05e8\u05b9\u05d53 / \u05d4\u05e1\u05b5\u05d2\u05b7\u05d3\u05ea\u05b9\u05d3\u05e0\u05b4\u05d4\u05e1\u05e0\u05b4\u05d4\u05e1 , or may be absent altogether .", "entities": []}, {"text": "Full script Hebrew script written without intention of dotting typically employs a compensatory variant known colloquially as full script ( ktiv male , /\u05b9\u05d3\u05e0\u05b5f\u05db\u05ea\u05d9\u05d1\u05de\u05dc\u05d0\u05d3\u05e8\u05b9\u05d5 , ) which adds instances of the letters /\u05b9\u05d3\u05e0\u05b5f\u05d9\u05d3\u05e8\u05b9\u05d5and /\u05b9\u05d3\u05e0\u05b5f\u05d5\u05d3\u05e8\u05b9\u05d5in", "entities": []}, {"text": "some places where they can aid pronunciation , but are incompatible with the rules for dotted script .", "entities": []}, {"text": "In our formulation of dotting as a sequence tagging problem , and in collecting our test set from raw text , these added letters may conflict with the dotting standard .", "entities": []}, {"text": "For the sake of input integrity , and unlike some other systems , we opt not to remove these characters , but instead employ a dotting policy consistent with full script .", "entities": []}, {"text": "See Appendix A for further details .", "entities": []}, {"text": "2.2 Training corpora Dotted modern Hebrew text is scarce , since speakers usually read and write undotted text , with the occasional diacritic added for disambiguation when context does not suffice .", "entities": []}, {"text": "As we are unaware of legally - obtainable dotted modern corpora , we use a combination of dotted pre - modern texts as well as automatically and semi - automatically dotted modern sources to train N AKDIMON : The PRE - MODERN portion is obtained from two main sources : A combination of late pre - modern text from Project Ben - Yehuda , mostly texts from the late 19th century and the early 20th century;5 rabbinical texts from the medieval period , the most important of which is Mishneh Torah ( obtained from Project Mamre);6and 23 short stories from the short story project.7This portion contains roughly 1.81 M Hebrew tokens , most of which are dotted , with a varying level of accuracy , varying dotting styles , and varying degree of similarity to Modern Hebrew .", "entities": [[41, 42, "MethodName", "MODERN"], [123, 124, "MetricName", "accuracy"]]}, {"text": "The AUTOMATIC portion contains 547 short stories taken from the short story project .", "entities": []}, {"text": "The stories are dotted using Dicta without manual validation .", "entities": []}, {"text": "5https://benyehuda.org 6https://mechon-mamre.org 7https://shortstoryproject.com/he/1011", "entities": []}, {"text": "Genre Sources # Docs # Tokens Wiki Dicta test set 22 5,862", "entities": []}, {"text": "News", "entities": []}, {"text": "Yanshuf 78 11,323 \u2020Literary Books , forums 129 73,770 *", "entities": []}, {"text": "Official gov.il 24 20,181 *", "entities": []}, {"text": "News /", "entities": []}, {"text": "Mag Online outlets 137 92,151 * User - gen . Blogs , forums 63 60,673 * Wiki he.wikipedia 40 62,723 Total 493 326,683 Table 2 : Data sources for our MODERN Hebrew training set .", "entities": [[30, 31, "MethodName", "MODERN"]]}, {"text": "Rows marked with * were automatically dotted via the Dicta API and corrected manually .", "entities": []}, {"text": "Rows with \u2020 were dotted at low quality , requiring manual correction .", "entities": []}, {"text": "The rest were available with professional dotting .", "entities": []}, {"text": "The corpus contains roughly 1.27 M Hebrew tokens .", "entities": []}, {"text": "Lastly , the MODERN portion contains manually collected text in Modern Hebrew , mostly from undotted sources , which we dot using Dicta and follow up by manually fixing errors , either using Dicta \u2019s API or via automated scripts which catch common mistakes .", "entities": [[3, 4, "MethodName", "MODERN"]]}, {"text": "We made an effort to collect a diverse set of sources : news , opinion columns , paragraphs from books , short stories , Wikipedia articles , governmental publications , blog posts and forums expressing various domains and voices , and more .", "entities": []}, {"text": "Our MODERN corpus contains roughly 326 K Hebrew tokens , and is much more consistent and similar to the expectation of a native Hebrew speaker than the PRE - MODERN or the AUTOMATIC corpora , and more accurately dotted than the AUTOMATIC corpus .", "entities": [[1, 2, "MethodName", "MODERN"], [29, 30, "MethodName", "MODERN"]]}, {"text": "The sources and statistics of this dataset are presented in Table 2 . 2.3 New test set Shmidman et al .", "entities": []}, {"text": "( 2020 ) provide a benchmark dataset for dotting modern Hebrew documents .", "entities": []}, {"text": "However , it is relatively small and non - diverse : all 22 documents in the dataset originate in a single source , namely Hebrew Wikipedia articles .", "entities": []}, {"text": "Therefore , we created a new test set8from a larger variety of texts , including high - quality Wikipedia articles and edited news stories , as well as user - generated blog posts .", "entities": []}, {"text": "This set consists of ten documents from each of eleven sources ( 5x Dicta \u2019s test set ) , and totals 20,474 Hebrew tokens , roughly 3.5x Dicta \u2019s .", "entities": []}, {"text": "We use the same technique and style for dotting this corpus as we do for the MODERN corpus ( \u00a7 2.2 ) , but the documents were 8https://github.com/elazarg/hebrew _ diacritized / tree / master / test_moderncollected in different ways .", "entities": [[16, 17, "MethodName", "MODERN"]]}, {"text": "3 Nakdimon NAKDIMON embeds the input characters and passes them through a two - layer BiLSTM ( Hochreiter and Schmidhuber , 1997 ) .", "entities": [[15, 16, "MethodName", "BiLSTM"]]}, {"text": "The LSTM output is fed into a single linear layer , which then feeds three linear layers , one for each diacritic category ( see \u00a7 2 ) .", "entities": [[1, 2, "MethodName", "LSTM"], [8, 10, "MethodName", "linear layer"]]}, {"text": "Each character then receives a prediction for each category independently and all predicted marks are added to it as output .", "entities": []}, {"text": "Decoding is performed greedily , with no validation of readability or any other dependence between character - level decisions .", "entities": []}, {"text": "The input is pre - processed by removing all but Hebrew characters , spaces and punctuation ; digits are converted to a dedicated symbol , as are Latin characters .", "entities": []}, {"text": "All existing diacritic marks are stripped , and each document is split into chunks bounded at whitespace , ignoring sentence boundaries .", "entities": []}, {"text": "We train NAKDIMON first over PRE - MODERN , then over the AUTOMATIC corpus , and then by over the MODERN corpus .", "entities": [[7, 8, "MethodName", "MODERN"], [20, 21, "MethodName", "MODERN"]]}, {"text": "During training , the loss is the sum of the cross - entropy loss from all three categories .", "entities": [[4, 5, "MetricName", "loss"], [13, 14, "MetricName", "loss"]]}, {"text": "Trivial decisions , such as the label for the shin / sindiacritic for any non- /\u05b9\u05d3\u05e0\u05b5f\u05e9\u05d3\u05e8\u05b9\u05d5letter , are masked .", "entities": []}, {"text": "Tuning experiments are detailed in Appendix B ; an evaluation of a preliminary version of NAKDI MON over the Dicta test set is in Appendix C , and Hyperparameters are detailed in Appendix D. 4 Experiments We compare the performance of NAKDIMON on our new test set ( \u00a7 2.3 ) against Dicta,9Snopi,10 and Morfix ( Kamir et al . , 2002 ) .", "entities": []}, {"text": "as well as a MAJORITY baseline which returns the most common dotting for each word seen in our full training set", "entities": []}, {"text": ".", "entities": []}, {"text": "Metrics We report four metrics : decision accuracy ( DEC)is computed over the entire set of individual possible decisions : dagesh /mappiq for letters that allow it , sin / shin dot for the letter /\u05b9\u05d3\u05e0\u05b5f\u05e9\u05d3\u05e8\u05b9\u05d5 , and all other diacritics for letters that allow them ; character accuracy ( CHA)is the portion of characters in the text that end up in their intended final form ( which may combine two or three decisions , e.g. dagesh + vowel ) ; word accuracy ( WOR ) is the portion of words with no mistakes ; and vocalization 9Version 4.0 , wordlist version 43 .", "entities": [[7, 8, "MetricName", "accuracy"], [48, 49, "MetricName", "accuracy"], [82, 83, "MetricName", "accuracy"]]}, {"text": "10http://nakdan.com/Nakdan.aspx1012", "entities": []}, {"text": "System DEC CHA WOR VOC MAJORITY 93.79 90.01 84.87 86.19 SNOPI 91.29 85.84 76.45 78.91 MORFIX 96.84 94.92 90.38 92.39 DICTA 97.95 96.77", "entities": []}, {"text": "94.11 94.92 NAKDIMON 97.91 96.37 89.75 91.64 Table 3 : Document - level macro % accuracy .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "accuracy ( VOC)is the portion of words where any dotting errors do not cause incorrect pronunciation among mainstream Israeli Hebrew speakers.11 4.1 Results We provide document - level macro - averaged accuracy percentage results for a single run over our test set in Table 3 .", "entities": [[0, 1, "MetricName", "accuracy"], [31, 32, "MetricName", "accuracy"]]}, {"text": "All systems , except Snopi , substantially outperform the majority - dotting baseline on all metrics .", "entities": []}, {"text": "NAKDIMON outperforms Morfix on character - level metrics but not on word - level metrics , mostly since Morfix ignores certain words altogether , incurring errors on multiple characters .", "entities": []}, {"text": "We note the substantial improvement our model achieves on the VOC metric compared to the WOR metric : 18.43 % of word - level errors are attributable to vocalization - agnostic dotting , compared to 13.80 % for Dicta and 10.41 % for Snopi ( but 20.91 % for Morfix ) .", "entities": []}, {"text": "Considering that the central use case for dotting modern Hebrew text is to facilitate pronunciation to learners and for reading , and that undotted homograph ambiguity typically comes with pronunciation differences , we believe this measure to be no less important than WOR .", "entities": []}, {"text": "Results on Dicta \u2019s test set ( Shmidman et al . , 2020 ) are presented in Appendix C. 4.2 Error analysis In Table 4 we present examples of words dotted incorrectly , or correctly , only by NAKDIMON , compared with Morfix and Dicta .", "entities": [[20, 21, "MetricName", "Error"]]}, {"text": "The largest category forNAKDIMON -only errors ( \u223c18 % of 90 sampled ) are ones where a fused preposition+determiner character is dotted to only include the preposition , perhaps due to its inability to detect the explicit determiner clitic /\u05b9\u05d3\u05e0\u05b5f\u05d4\u05d3\u05e8\u05b9\u05d5in", "entities": []}, {"text": "neighboring words , on which the complex systems apply morphological segmentation .", "entities": []}, {"text": "In other cases ( \u223c15 % ) , NAKDIMON creates 11These are : the sin / shin dot , vowel distinctions across the a / e / i / o / u / null sets , and dagesh in the /\u05b9\u05d3\u05e0\u05b5f\u05d1\u05d3\u05e8\u05b9\u05d5 / \u05b9\u05d3\u05e0\u05b5 / f\u05b9\u05e0\u05b9\u05e8\u05b5\u05d6 / \u05d3\u05e8\u05b9\u05d5j\u05db\u05e0\u05b4\u05b9 / \u05b9\u05d3\u05e0\u05b5 / f\u05b9\u05e0\u05b9\u05e8\u05b5\u05d6 / \u05d3\u05e8\u05b9\u05d5j\u05e4\u05e0\u05b4\u05b9characters .", "entities": []}, {"text": "We do not distinguish between kamatz gadol /kamatz katan , and schwa is assumed to always be null .", "entities": []}, {"text": "Context Correct Incorrect ./\u05b9\u05d3\u05e0\u05b5f\u05de\u05b5\u05de / \u05d3\u05e8\u05b9\u05d5f\u05b7\u05db / \u05dc\u05d4\u05e1\u05ea\u05db\u05dc\u05dc\u05d4\u05d1\u05e2\u05d9\u05e0\u05d9\u05d9\u05dc\u05b7\u05e0\u05b4 ff\u05d5\u05e6\u05e8\u05d9\u05dc\u05b7\u05e0\u05b4 ... \u05b9\u05d3\u05e0\u05b5 / f\u05de\u05b5\u05de / \u05d3\u05e8\u05b9\u05d5f\u05e7\u05b4\u05e8\u05b4\u05d4 / \u05d9\u05dc\u05b7\u05e0\u05b41\u05d4\u05b7\u05ea\u05b7\u05e4 / \u05d91\u05b9\u05d3\u05e0\u05b5 / \ufb31\u05b8\u05e2\u05b5\u05d9\u05e0 f\u05de\u05b5\u05de / \u05d3\u05e8\u05b9\u05d5f\u05e7\u05b4\u05e8\u05b4\u05d4 / \u05d9\u05dc\u05b7\u05e0\u05b41\u05d4\u05b7\u05ea\u05b7\u05e4 / \u05d91\ufb31\u05b0\u05e2\u05b5\u05d9\u05e0 ... \u2018 and we need to look her in the eyes ( /in eyes ) . \u2019 . . ./\u05b9\u05d3\u05e0\u05b5f\u05b7\u05db", "entities": []}, {"text": "/ \u05d1\u05e1\u05d1\u05dc\u05e0\u05d5\u05ea\u05d3\u05e8\u05b9\u05d5 ff\u05d9\u05e2\u05e0\u05d5\u05dc\u05dc\u05b7\u05e0\u05b4 ... \u05b9\u05d3\u05e0\u05b5 / f\u05b7\u05db / \u05d3\u05e8\u05b9\u05d5ff\u05b9\u05d3\u05e0\u05b5 / \u05dc\u05b8\u05b7\u05e6\u05b5\u05d4\u05e1\u05dc\u05b7\u05e0\u05b4 f\u05b7\u05db / \u05d3\u05e8\u05b9\u05d5ff\u05dc\u05b0\u05b7\u05e6\u05b5\u05d4\u05e1\u05dc\u05b7\u05e0\u05b4 ... \u2018 you.sg.f ( /unreadable ) will be answered patiently . . . \u2019 . .", "entities": []}, {"text": "./\u05b9\u05d3\u05e0\u05b5f\u05de\u05b5\u05de / \u05d3\u05e8\u05b9\u05d5f\u05e0 / \u05d4\u05e8\u05d0\u05e9\u05d5\u05e0\u05d9\u05dc\u05b7\u05e0\u05b4 u\u05e0f\u05de\u05e9\u05ea\u05de\u05e9\u05d9\u05d4\u05d0\u05d9\u05d9\u05e4\u05d5\u05dc\u05b7\u05e0\u05b4 ... \u05b9\u05d3\u05e0\u05b5 / f\u05e0 / \u05d3\u05e8\u05b9\u05d5u\u05e0f\u05b7\u05e6\u05b5\u05d4\u05e1 / \u05d9\u05e4\ufb4b\u05dc\u05b7\u05e0\u05b41\u05b9\u05d3\u05e0\u05b5 / \u05d4\u05b8\u05d0\u05b7\u05d9 f\u05e0 / \u05d3\u05e8\u05b9\u05d5u\u05e0f\u05d4\u05b8\u05d0\u05b4\u05d9\u05d9\ufb44\ufb4b\u05dc\u05b7\u05e0\u05b4 ... \u2018 the first iPhone ( / ee - pon ) users . . . \u2019", "entities": []}, {"text": "Table 4 : Examples of words dotted incorrectly ( top ) or correctly ( bottom ) only by N AKDIMON .", "entities": []}, {"text": "unreadable vocalization sequences , as it has no lexical component and is decoded greedily .", "entities": []}, {"text": "These types of errors are more friendly to the typical use cases of a dotting system , as they are likely to stand out to a reader .", "entities": []}, {"text": "In contrast , a large portion of cases where only NAKDIMON was correct ( \u223c13 % of 152 ) are foreign names and terms .", "entities": []}, {"text": "This may be the result of such words not yet appearing in dictionaries , or not being easily separable from an adjoining clitic , while character - level information can capture pronunciation patterns from similar words ( e.g. /\u05b9\u05d3\u05e0\u05b5f\u05e0 / \u05d3\u05e8\u05b9\u05d5u\u05e0f\u05d8\u05b6\u05dc\u05b6\u05e4\ufb4b\u05dc\u05b7\u05e0\u05b4 \u2018 telephone \u2019 , for the example /\u05b9\u05d3\u05e0\u05b5f\u05e0 / \u05d3\u05e8\u05b9\u05d5u\u05e0f\u05d4\u05d0\u05d9\u05d9\u05e4\u05d5\u05dc\u05b7\u05e0\u05b4 . )", "entities": []}, {"text": "OOVs To further quantify the strengths of NAKDIMON \u2019s architecture and training abilities , we evaluate the systems \u2019 results pertaining only to those words in the test set which do not appear in our training sets .", "entities": []}, {"text": "We follow common practice by calling them OOVs ( \u201c out of vocabulary \u201d ) , but emphasize that NAKDIMON does not consult an explicit vocabulary , and the other systems are not evaluated against their own vocabularies ( which are unknown to us ) .", "entities": []}, {"text": "We find that NAKDIMON \u2019s performance on this subset is substantially worse compared with the other systems than on the full set : 15 percentage points below Dicta and seven below Morfix on the VOC metric ( see full results in Appendix C ) .", "entities": []}, {"text": "These results might be counter - intuitive considering the proven utility of character - level models in OOV contexts ( e.g. , Plank et al . , 2016 ) , and so we offer several possible explanations : First , many \u201c OOVs \u201d consist in fact of known words coupled with an unseen combination of prefix clitics and/or suffix possessive markers , which other systems explicitly remove using morphological analyzers before dotting .", "entities": []}, {"text": "Second , mirroring the last finding from the overall analysis , some \u201c OOVs \u201d are proper names which appear in dictionaries but are absent from the training set , due to corpus effects such as time and domain , or simply chance.1013", "entities": []}, {"text": "5 Related Work Existing work on diacritizing Hebrew is not common , and all efforts build on word - level features .", "entities": []}, {"text": "Kontorovich ( 2001 ) trains an HMM on a vocalized and morphologically - tagged portion of the Hebrew Bible containing 30,743 words , and evaluates the result on a test set containing 2,852 words , achieving 81 % WOR accuracy .", "entities": [[39, 40, "MetricName", "accuracy"]]}, {"text": "Note that Biblical Hebrew is very different from Modern Hebrew in both vocabulary , grammatical structure , and diacritization , and also has many words with unique diacritization .", "entities": []}, {"text": "In our system , we exclude the Bible altogether from the training set , as its inclusion actively hurts performance on the validation set , which consists of Modern Hebrew .", "entities": []}, {"text": "Tomer ( 2012 ) designs a diacritization system for Hebrew verbs consisting of a combination of a verb inflection system , a syllable boundary detector , and an SVM model for classifying verb inflection paradigms .", "entities": [[28, 29, "MethodName", "SVM"]]}, {"text": "The focus on verbs in a type - level setup makes this work incomparable to ours or to others in this survey .", "entities": []}, {"text": "In Arabic , diacritization serves a comparable purpose to that in Hebrew , but not exclusively : most diacritic marks differentiate consonantal phonemes from each other , e.g. /char48 / char2e / b/ vs. /char10 / char48 / t/ ( which only the sin / shin dot does in Hebrew ) , whereas vocalization marks are in a one - to - one relationship with their phonetic realizations , e.g. only the fatha as in /char0b / char48 / char2e / ba/ encodes the /a/ vowel .", "entities": []}, {"text": "Dictionary - less Arabic diacritization has been attempted using a 3 - layer Bi - LSTM ( Belinkov and Glass , 2015 ) .", "entities": [[15, 16, "MethodName", "LSTM"]]}, {"text": "Abandah et al .", "entities": []}, {"text": "( 2015 ) use a Bi - LSTM where characters are assigned either one or more diacritic symbols .", "entities": [[7, 8, "MethodName", "LSTM"]]}, {"text": "Our system differs from theirs by virtue of separating the diacritization categories .", "entities": []}, {"text": "Mubarak et al .", "entities": []}, {"text": "( 2019 ) tackled Arabic diacritization as a sequence - to - sequence problem , tasking the model with reproducing the characters as well as the marks .", "entities": []}, {"text": "Zalmout and Habash ( 2017 ) have made the case against RNN - only systems , arguing for the importance of morphological analyzers in Arabic NLPsystems .", "entities": []}, {"text": "We concede that well - curated systems may perform better than uncurated ones , particularly on low - resource languages such as Hebrew , but we note that they are difficult to train for individual use cases and are burdensome to incorporate within larger systems .", "entities": []}, {"text": "Diacritics restoration in Latin - based scripts , applicable mostly to European languages , forms a substantially different problem from the one in Hebrew given the highly lexicalized nature of diacritic usage in these languages and the very low rate of characters requiring diacritics .", "entities": []}, {"text": "The state - of - theart systems in such languages employ transformer models in a sequence - to - sequence setup ( N \u00b4 aplava et", "entities": []}, {"text": "al . , 2021 ; Stankevi \u02c7cius", "entities": []}, {"text": "et", "entities": []}, {"text": "al . , 2022 ) , supplanting character - RNN sequence prediction architectures reminiscent of ours ( N \u00b4 aplava et al . , 2018 ) .", "entities": []}, {"text": "Indeed , the authors of this latter work note the only nonEuropean in their dataset , Vietnamese , as a special outlier .", "entities": []}, {"text": "6 Conclusion Learning directly from plain diacritized text can go a long way , even with relatively limited resources .", "entities": []}, {"text": "NAKDIMON demonstrates that a simple architecture for diacritizing Hebrew text as a sequence tagging problem can achieve performance on par with much more complex systems .", "entities": []}, {"text": "We also introduce and release a corpus of dotted Hebrew text , as well as a source - balanced test set .", "entities": []}, {"text": "In the future , we wish to evaluate the utility of dotting as a feature for downstream tasks such as question answering , machine translation , and speech generation , taking advantage of the fact that our simplified model can be easily integrated in an end - to - end Hebrew processing system .", "entities": [[20, 22, "TaskName", "question answering"], [23, 25, "TaskName", "machine translation"]]}, {"text": "Ethical Considerations We collected the data for our training set and test sets from open online sources , while making sure their terms allow research application and privacy is not impugned .", "entities": []}, {"text": "NAKDIMON \u2019s architecture does not encourage memorization of training data and the system is not trained for generating text .", "entities": []}, {"text": "We consider a main use case for our system to be assisting Hebrew learners in reading .", "entities": []}, {"text": "We therefore expect NAKDIMON to facilitate life in Israel for immigrants still struggling with Hebrew , among other underprivileged groups .", "entities": []}, {"text": "Automatic dotting can increase inclusion in Hebrew - prominent societies for literacy - challenged individuals , and derivative1014", "entities": []}, {"text": "improvements in text - to - speech applications can assist those with impaired vision .", "entities": []}, {"text": "Lastly , dotting can help researchers with limited understanding of Hebrew access resources in the language .", "entities": []}, {"text": "Hebrew is a gendered language .", "entities": []}, {"text": "Orthographically , in many cases the lack of dots masks gender ambiguity , allowing both masculine and feminine readings for a given word ( e.g. /\u05b9\u05d3\u05e0\u05b5f\u05e1\u05ea\u05b7\u05de\u05b7\u05e7 / \u05dc\u05b7\u05d7\u05b0\ufb4a\u05b0\u05d3\u05e8\u05b9\u05d53\ufb2a/ \u05b9\u05d3\u05e0\u05b5 / f\u05e1\u05ea\u05b7\u05de\u05b7\u05e7 / \u05dc\u05b7\u05d7\u05b0\ufb4a\u05b8\u05d3\u05e8\u05b9\u05d53\ufb2a \u2018 you.fem sent \u2019 / \u2018 you.masc sent \u2019 ) .", "entities": []}, {"text": "While wellperforming automatic dotting can help alleviate these ambiguities and reduce the amount of potentially prejudiced readings , we recognize the large body of work on gender bias in NLP ( Blodgett et al . , 2020 ) , including in Hebrew NLP ( Moryossef et al . , 2019 ) , and the findings that an imbalanced training set may result in an even more skewed distribution of gender bias in applications ( Zhao et al . , 2017 ) .", "entities": []}, {"text": "We believe our unlexicalized approach is more robust to such bias compared with other systems , and have already started quantifying and addressing these issues as we find them in ongoing work .", "entities": []}, {"text": "In the meantime , we offer this paragraph as a disclaimer .", "entities": []}, {"text": "Acknowledgments We would like to thank Avi Shmidman for details about Dicta \u2019s Nakdan and other suggestions .", "entities": []}, {"text": "We thank Sara Gershuni for lengthy and fruitful discussions , and for her linguistic insights and advice .", "entities": []}, {"text": "We thank Yoav Goldberg , Reut Tsarfaty , Ian Stewart , Sarah Wiegreffe , Kyle Gorman and many anonymous reviewers for their comments and suggestions in discussions and on earlier drafts .", "entities": []}, {"text": "References Gheith Abandah , Alex Graves , Balkees Al - Shagoor , Alaa Arabiyat , Fuad Jamour , and Majid Al - Taee . 2015 .", "entities": []}, {"text": "Automatic diacritization of Arabic text using recurrent neural networks .", "entities": []}, {"text": "International Journal on Document Analysis and Recognition ( IJDAR ) , 18:183\u2013197 .", "entities": []}, {"text": "Salim Abu - Rabia .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "The role of vowels in reading semitic scripts : Data from Arabic and Hebrew .", "entities": []}, {"text": "Reading and Writing , 14(1 - 2):39\u201359 .", "entities": []}, {"text": "Yonatan Belinkov and James Glass . 2015 .", "entities": []}, {"text": "Arabic diacritization with recurrent neural networks .", "entities": []}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 2281 \u2013 2285 , Lisbon , Portugal . Association for Computational Linguistics .", "entities": []}, {"text": "Shlomo Bentin and Ram Frost . 1987 .", "entities": []}, {"text": "Processing lexical ambiguity and visual word recognition in a deep orthography .", "entities": []}, {"text": "Memory & Cognition , 15(1):13\u201323 .", "entities": []}, {"text": "Su Lin Blodgett , Solon Barocas , Hal Daum \u00b4 e III , and Hanna Wallach .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Language ( technology ) is power : A critical survey of \u201c bias \u201d in NLP .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5454 \u2013 5476 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ya\u2019akov Gal . 2002 .", "entities": []}, {"text": "An HMM approach to vowel restoration in Arabic and Hebrew .", "entities": []}, {"text": "In Proceedings of the ACL-02 Workshop on Computational Approaches to Semitic Languages , Philadelphia , Pennsylvania , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Yoav Goldberg and Michael Elhadad .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Easy - first dependency parsing of modern Hebrew .", "entities": [[3, 5, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the NAACL HLT 2010", "entities": []}, {"text": "First Workshop on Statistical Parsing of Morphologically - Rich Languages , pages 103\u2013107 , Los Angeles , CA , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Sepp Hochreiter and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural computation , 9(8):1735 \u2013 1780 .", "entities": []}, {"text": "Dror Kamir , Naama Soreq , and Yoni Neeman .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "A comprehensive NLP system for modern standard Arabic and modern Hebrew .", "entities": []}, {"text": "In Proceedings of the ACL-02 Workshop on Computational Approaches to Semitic Languages , Philadelphia , Pennsylvania , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Diederik P Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Leonid Kontorovich .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Problems in Semitic NLP : Hebrew vocalization using HMMs .", "entities": []}, {"text": "In Problems in Semitic NLP , NIPS Workshop on Machine Learning Methods for Text and Images .", "entities": []}, {"text": "Amit Moryossef , Roee Aharoni , and Yoav Goldberg .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Filling gender & number gaps in neural machine translation with black - box context injection .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the First Workshop on Gender Bias in Natural Language Processing , pages 49\u201354 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Hamdy Mubarak , Ahmed Abdelali , Hassan Sajjad , Younes Samih , and Kareem Darwish .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Highly effective Arabic diacritization using sequence to sequence modeling .", "entities": [[5, 8, "MethodName", "sequence to sequence"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 2390\u20132395 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jakub N \u00b4 aplava , Milan Straka , and Jana Strakov \u00b4 a. 2021 .", "entities": []}, {"text": "Diacritics restoration using bert with analysis on czech language .", "entities": []}, {"text": "arXiv preprint arXiv:2105.11408 .1015", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jakub N \u00b4 aplava , Milan Straka , Pavel Stra \u02c7n\u00b4ak , and Jan Haji\u02c7c .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Diacritics restoration using neural networks .", "entities": []}, {"text": "In Proceedings of the Eleventh International Conference on Language Resources and Evaluation ( LREC 2018 ) , Miyazaki , Japan .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Barbara Plank , Anders S\u00f8gaard , and Yoav Goldberg . 2016 .", "entities": []}, {"text": "Multilingual part - of - speech tagging with bidirectional long short - term memory models and auxiliary loss .", "entities": [[1, 7, "TaskName", "part - of - speech tagging"], [9, 14, "MethodName", "long short - term memory"], [17, 18, "MetricName", "loss"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 412\u2013418 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Danny Shacham and Shuly Wintner . 2007 .", "entities": []}, {"text": "Morphological disambiguation of Hebrew : A case study in classifier combination .", "entities": [[0, 2, "TaskName", "Morphological disambiguation"]]}, {"text": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP - CoNLL ) , pages 439 \u2013 447 , Prague , Czech Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Avi Shmidman , Shaltiel Shmidman , Moshe Koppel , and Yoav Goldberg . 2020 .", "entities": []}, {"text": "Nakdan : Professional Hebrew diacritizer .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 197\u2013203 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Leslie N Smith .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Cyclical learning rates for training neural networks .", "entities": []}, {"text": "In 2017 IEEE Winter Conference on Applications of Computer Vision ( WACV ) , pages 464\u2013472 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Lukas Stankevi \u02c7cius , Mantas Luko \u02c7sevi\u02c7cius , Jurgita Kapo \u02c7ci\u00afut\u02d9e - Dzikien \u02d9e , Monika Briedien \u02d9e , and Tomas Krilavi \u02c7cius .", "entities": []}, {"text": "2022 .", "entities": []}, {"text": "Correcting diacritics and typos with ByT5 transformer model .", "entities": []}, {"text": "arXiv e", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "- prints .", "entities": []}, {"text": "Eran Tomer .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Automatic Hebrew Text Vocalization .", "entities": []}, {"text": "Ben - Gurion University of the Negev , Faculty of Natural Sciences , Department of Computer Science .", "entities": []}, {"text": "Reut Tsarfaty , Shoval Sadde , Stav Klein , and Amit Seker .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "What \u2019s wrong with Hebrew NLP ?", "entities": []}, {"text": "and how to make it right .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ): System Demonstrations , pages 259\u2013264 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008 .", "entities": []}, {"text": "Nasser Zalmout and Nizar Habash . 2017 .", "entities": []}, {"text": "Do n\u2019t throw those morphological analyzers away just yet : Neural morphological disambiguation for Arabic .", "entities": [[11, 13, "TaskName", "morphological disambiguation"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 704\u2013713,Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Jieyu Zhao , Tianlu Wang , Mark Yatskar , Vicente Ordonez , and Kai - Wei Chang .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Men also like shopping : Reducing gender bias amplification using corpus - level constraints .", "entities": []}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2979\u20132989 , Copenhagen , Denmark . Association for Computational Linguistics.1016", "entities": []}, {"text": "Dicta \u2013 reported / reproduced New test set ( \u00a7 2.3 ) OOV System DEC CHA WOR VOC DEC CHA WOR VOC WOR VOC Baselines MAJMOD 84.93 75.94 68.10 69.63 88.04 81.22 76.14 77.10 N / A N / A MAJALL 91.67 86.29 79.43 81.19 93.79 90.01 84.87 86.19 N / A N / A Lexicalized SNOPI 87.81 78.96 / 79.92 66.41 / 66.57 70.35 91.29 85.84 76.45 78.91 40.83 42.39 MORFIX 94.91 90.32 / 91.29 80.90 / 82.24 86.48 96.84 94.92 90.38 92.39 63.91 69.20 DICTA 97.53 95.12 / 95.71 88.23 / 89.23 90.66 97.95 96.77 94.11 94.92 76.21 77.66 Unlexicalized NAKDIMON 095.78 92.59 79.00 83.01 94.59 91.70 84.94 87.54 47.05 50.96 NAKDIMON 97.91 96.37 89.75 91.64 57.46 62.06 Table 5 : Document - level macro % accuracy on the test set from Shmidman et al .", "entities": [[128, 129, "MetricName", "accuracy"]]}, {"text": "( 2020 ) and on our new test set .", "entities": []}, {"text": "We can not report our full NAKDIMON \u2019s performance on the former , as we use the test set for parts of its training .", "entities": []}, {"text": "MAJALLis reported as MAJORITY in the main text ; MAJMODonly considers text in the MODERN portion of our training set .", "entities": [[14, 15, "MethodName", "MODERN"]]}, {"text": "A Full Script Reconciliation We apply the following resolution tactics for added letters in undotted text : ( a ) We almost never remove or add letters to the original text ( unless it is completely undiacritizable ) .", "entities": []}, {"text": "( b ) We keep dagesh in letters that follow a shuruk which replaces a kubuts , and similarly for yod ( hirik male replacing hirik haser ) .", "entities": []}, {"text": "( c ) When we have double vavor doubleyod , the second letter is usually left undotted , except when it is impossible to have the correct vocalization this way .", "entities": []}, {"text": "Resolving ktiv haser discrepancies from Morfix outputs is done by adding missing vowel letters , or removing superfluous vowel letters , in such a way that would not count as an error if it is correct according to Academy regulations .", "entities": []}, {"text": "B Development Experiments We tried to further improve NAKDIMON by initializing its parameters from a language model trained to predict masked characters in a large undotted Wikipedia corpus ( 440 MB , 30 % mask rate ) , but were only able to achieve an improvement of 0.07 % .", "entities": []}, {"text": "Attempted architectural modifications , including substituting a Transformer ( Vaswani et al . , 2017 ) for the LSTM ; adding a CRF layer to the decoding process ; and adding a residual connection between the character LSTM layers , yielded no substantial benefits in these experiments .", "entities": [[7, 8, "MethodName", "Transformer"], [18, 19, "MethodName", "LSTM"], [22, 23, "MethodName", "CRF"], [32, 34, "MethodName", "residual connection"], [37, 38, "MethodName", "LSTM"]]}, {"text": "Similarly , varying the number of LSTM layers between 2 and 5 ( keeping the total number of parameters roughly constant , close to the 5,313,223 parameters of our final model ) had little to no impact on the accuracy on the validation set .", "entities": [[6, 7, "MethodName", "LSTM"], [16, 19, "HyperparameterName", "number of parameters"], [39, 40, "MetricName", "accuracy"]]}, {"text": "Figure 1 : WOR error rate on validation set as a function of training set size vs. Dicta , over five runs .", "entities": []}, {"text": "Other metrics show similar trends .", "entities": []}, {"text": "Figure 1 shows the favorable effect of training NAKDIMON over an increasing amount of MOD ERN text .", "entities": [[14, 15, "DatasetName", "MOD"]]}, {"text": "C Dicta Test Set We present results for the Dicta test set in Table 5 .", "entities": []}, {"text": "In order to provide fair comparison and to preempt overfitting on this test data , we ran this test in a preliminary setup on a variant of NAKDIMON which was not tuned or otherwise unfairly trained .", "entities": []}, {"text": "This system , NAKDIMON 0 , differs from our final variant in three main aspects : it is not trained on the Dicta portion of our training corpus ( \u00a7 2.2 ) , it is not trained on the AUTOMATIC corpus , and it employs a residual connection between the two character Bi - LSTM layers .", "entities": [[4, 5, "DatasetName", "0"], [46, 48, "MethodName", "residual connection"], [54, 55, "MethodName", "LSTM"]]}, {"text": "Testing on the Dicta test set required some minimal evaluation adaptations resulting from encoding constraints ( for example , we do not distinguish between kamatz katan andkamatz gadol ) .", "entities": []}, {"text": "Thus , we copy the results reported in1017", "entities": []}, {"text": "Shmidman et al .", "entities": []}, {"text": "( 2020 ) as well as our replication .", "entities": []}, {"text": "We see that the untuned NAKDIMON 0performs on par with the proprietary Morfix , which uses word - level dictionary data , consistent with our main results on our novel test set .", "entities": []}, {"text": "D Hyperparameters We tuned hyperparameters and architecture over a held - out validation set of 40 documents with 27,681 tokens , on which Dicta performs at 91.56 % WOR accuracy .", "entities": [[29, 30, "MetricName", "accuracy"]]}, {"text": "In our chosen setup , we train NAKDIMON over PRE - MODERN for a single epoch , followed by two epochs over the AUTOMATIC corpus , and then by three epochs over the MODERN corpus .", "entities": [[11, 12, "MethodName", "MODERN"], [33, 34, "MethodName", "MODERN"]]}, {"text": "We optimize using Adam ( Kingma and Ba , 2014 ) .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "For the PRE - MODERN corpus we use a cyclical learning rate schedule ( Smith , 2017 ) , varying linearly from 3\u00b710\u22123through 8\u00b710\u22123and down to 10\u22124 , which we found to be more useful than a constant learning rate .", "entities": [[4, 5, "MethodName", "MODERN"], [10, 12, "HyperparameterName", "learning rate"], [38, 40, "HyperparameterName", "learning rate"]]}, {"text": "For each of AUTOMATIC and MODERN corpora we use epoch - wise decreasing learning rate : ( 3\u00b710\u22123,10\u22123)and(10\u22123,10\u22123,3\u00b710\u22124)respectively .", "entities": [[5, 6, "MethodName", "MODERN"], [13, 15, "HyperparameterName", "learning rate"]]}, {"text": "We set maximum chunk size to 80 characters , and use batch size of 128 .", "entities": [[11, 13, "HyperparameterName", "batch size"]]}, {"text": "We set both character embedding and LSTM hidden dimensions to 400 , and apply a dropout rate of 0.1.1018", "entities": [[6, 7, "MethodName", "LSTM"]]}]