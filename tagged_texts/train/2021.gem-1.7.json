[{"text": "Proceedings of the 1st Workshop on Natural Language Generation , Evaluation , and Metrics ( GEM 2021 ) , pages 68\u201372 August 5\u20136 , 2021 .", "entities": [[15, 16, "DatasetName", "GEM"]]}, {"text": "\u00a9 2021 Association for Computational Linguistics68GOT : Testing for Originality in Natural Language Generation Jennifer Brooks Department of Computer Science", "entities": []}, {"text": "The George Washington University Washington , DC jtbrooks@gwu.eduAbdou Youssef Department of Computer Science The George Washington University Washington , DC ayoussef@gwu.edu", "entities": [[1, 3, "DatasetName", "George Washington"], [14, 16, "DatasetName", "George Washington"]]}, {"text": "Abstract We propose an approach to automatically test for originality in generation tasks where no standard automatic measures exist .", "entities": []}, {"text": "Our proposal addresses original uses of language , not necessarily original ideas .", "entities": []}, {"text": "We provide an algorithm for our approach and a run - time analysis .", "entities": []}, {"text": "The algorithm , which \ufb01nds all of the original fragments in a ground - truth corpus and can reveal whether a generated fragment copies an original without attribution , has a run - time complexity of \u0012(nlogn)wherenis", "entities": []}, {"text": "the number of sentences in the ground truth .", "entities": []}, {"text": "1 Introduction This research addresses an ethical consideration for Natural Language Generation , namely , plagiarism .", "entities": []}, {"text": "The Oxford English Dictionary de\ufb01nes original ( adjective ) as \u201c present or existing from the beginning ; \ufb01rst or earliest \u201d and \u201c created directly and personally by a particular artist ; not a copy or imitation \u201d .", "entities": []}, {"text": "But , if we apply the de\ufb01nitions of \u201c original \u201d to language , then there are two ways in which a piece of generated text may be original .", "entities": []}, {"text": "For one , the text may express an \u201c original idea \u201d , such as Einstein did in 1905 with \u201c E = mc2 \u201d .", "entities": []}, {"text": "On the other hand , a non - original idea may be expressed in an original way , via , for example , \ufb01gurative language .", "entities": []}, {"text": "Our proposed approach addresses original uses of language .", "entities": []}, {"text": "It does not necessarily address original ideas .", "entities": []}, {"text": "How do we protect intellectual property when it comes to language generators that are trained on a world - wide - web of data ?", "entities": []}, {"text": "Our language generators have to be held accountable .", "entities": []}, {"text": "They should also be protected .", "entities": []}, {"text": "What if a language generator generates an original analogy ?", "entities": []}, {"text": "What if it writes a poem that is so great that it ends up in the history books ?", "entities": []}, {"text": "Multiple language generators may be trained on the same ground truth ( e.g. , Wikipedia ) with thesame embedding vectors ( e.g. , BERT ( Devlin et al . , 2018 ) and GPT ( Vaswani et al . , 2017 ; Radford et", "entities": [[23, 24, "MethodName", "BERT"], [33, 34, "MethodName", "GPT"]]}, {"text": "al . , 2018 ) )", "entities": []}, {"text": "and the same technologies ( deep neural networks , LSTM cells ( Hochreiter and Schmidhuber , 1997 ) , transformers ( Vaswani et al . , 2017 ) ) .", "entities": [[9, 10, "MethodName", "LSTM"]]}, {"text": "It will become a question of \u201d Whose generator said it \ufb01rst ? \u201d", "entities": []}, {"text": "With automatic language generation , we need a way to automatically measure , store , and reference original ideas and language .", "entities": []}, {"text": "We propose one possible solution to these originality - related problems .", "entities": []}, {"text": "For the purposes of our analyses , we de\ufb01ne ground truth as the set of sentences that are compared with the generated sentences .", "entities": []}, {"text": "The ground truth may be larger than the training set , but should include the training set .", "entities": []}, {"text": "The gound truth would also , ideally , grow .", "entities": []}, {"text": "For example , the ground truth could start out as the training set , but as new sentences are generated with a trained model , then the new sentences may be added to the ground truth .", "entities": []}, {"text": "We also claim that generated sentences should only be added to the ground truth if they are original or include citations where appropriate .", "entities": []}, {"text": "2 Background Our criteria and basis for evaluating measurements of originality are : 1.Can we tell whether a generated sentence is an original use of language ?", "entities": []}, {"text": "2.Can we tell whether the sentence contains a fragment from the ground truth that is a candidate for protection as intellectual property ?", "entities": []}, {"text": "Therefore , when measuring generation originality by comparing the generated sentence with the sentences in the ground truth , then the answers to numbers 1 and 2 above are binary .", "entities": []}, {"text": "Either the generated sentence is an original use of language or it is not .", "entities": []}, {"text": "Either the generation is at risk of plagiarism or", "entities": []}, {"text": "69it is not .", "entities": []}, {"text": "However , if we consider that the ground truth may not be representative of all the sentences that have ever been generated , then there is a measure of uncertainty that may be added to the binary outcome .", "entities": []}, {"text": "There are no standard automatic measures for novelty and originality in stylized language generation ( Mou and Vechtomova , 2020 ) .", "entities": []}, {"text": "High perplexity ( PPL ) and a low BLEU ( Papineni et", "entities": [[1, 2, "MetricName", "perplexity"], [8, 9, "MetricName", "BLEU"]]}, {"text": "al . , 2002 ) score may suggest novelty , but they are not suf\ufb01cient for testing for originality .", "entities": []}, {"text": "High PPL and a low BLEU score may be achieved when there is little overlap between the generated language and the ground truth , but nonesense and off - topic sentences are rewarded .", "entities": [[5, 7, "MetricName", "BLEU score"]]}, {"text": "While nonesense sentences may be novel , they may be grammatically incorrect , and sentences that are grammatically correct will likely have some overlap with fragments ( n - grams ) in the ground truth , such as using phrases like \u201c she said that \u201d .", "entities": []}, {"text": "So , we want a generation originality test that does n\u2019t penalize n - gram overlap .", "entities": []}, {"text": "( An original use of language may combine common n - grams in a new way . )", "entities": []}, {"text": "We also want a generation originality test that \ufb02ags potential plagiarism of original fragments in the ground truth , which neither BLEU nor PPL does .", "entities": [[21, 22, "MetricName", "BLEU"]]}, {"text": "We propose a generation originality test ( GOT ) that addresses original uses of language .", "entities": []}, {"text": "It does not necessarily address original ideas .", "entities": []}, {"text": "GOT is equally appropriate for stylized text generation , where novelty is desirable , and for other generation tasks where there is not an imposed style but the generation is open - ended , including summarization tasks .", "entities": [[6, 8, "TaskName", "text generation"], [35, 36, "TaskName", "summarization"]]}, {"text": "3 Proposed Approach Our proposed generation originality test ( GOT ) determines whether : 1.any fragment in a generated sentence equals an \u201c original \u201d fragment in the ground truth , in which case the generation may be in violation of a copyright law , if no citation of the original source is included ; or , 2.the generated sentence is \u201c original \u201d , per De\ufb01nition 1 , below .", "entities": []}, {"text": "De\ufb01nition 1 ( Original Sentence ) .A sentence , whether generated or in the ground truth , of n tokens is original if there exists an original k - gram within the sentence for some k \u0014n .", "entities": []}, {"text": "The originality of k - grams is de\ufb01ned next .", "entities": []}, {"text": "The de\ufb01nition of originality of a fragment ( or k - gram ) depends on whether we are referring to a generated fragment or to a fragment in the ground truth .", "entities": []}, {"text": "Generated fragments are tested against the ground truth .", "entities": []}, {"text": "If the generated fragment does not appear in the ground truth , then the generated fragment is considered original .", "entities": []}, {"text": "If it appears once in the ground truth , then it is considered not original and so a citation may be needed .", "entities": []}, {"text": "See Table 1 for a summary of the criterion for each type of fragment to be true .", "entities": []}, {"text": "In Table 1 , Cequals the number of times that fragment appears in the ground truth .", "entities": []}, {"text": "Type Criterion Ground Truth FragmentOriginal C= 1", "entities": []}, {"text": "Not Original C\u00152 Generated FragmentOriginal C= 0", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "Not Original , Citation NeededC= 1", "entities": []}, {"text": "Not Original , No Citation NeededC\u00152 Table 1 : Criterion per fragment type , where Cis the number of times the fragment appears in the ground truth .", "entities": []}, {"text": "Note , Cis always with respect to counts in the ground truth , even when evaluating generated fragments .", "entities": []}, {"text": "Ground - truth fragments that appear once and only once in the ground truth are considered original.1Likewise , fragments that appear more than once in the ground truth are considered \u201c not original \u201d .", "entities": []}, {"text": "For example , \u201c lengthened shadow \u201d appeared twice in our ground truth and so it is not considered an original phrase in the ground truth .", "entities": []}, {"text": "Combining non - original fragments to generate a new idea or analogy , however , could be considered an original use of language .", "entities": []}, {"text": "For example , \u201c the writer is the lengthened shadow of a man \u201d contains the fragments \u201c the writer is \u201d and \u201c the lengthened shadow \u201d and \u201c of a man \u201d which are not original fragments in our ground truth .", "entities": []}, {"text": "However , the way in which they are combined in this example creates an original use of language \u2013 in this case , a metaphor .", "entities": []}, {"text": "( Examples of fragments that appeared many times in our 1For simplicity of explanation , we qualify a fragment as \u201c original \u201d , and therefore a candidate for protection of intellectual property , if it appears \u201c once and only once \u201d in the ground truth .", "entities": []}, {"text": "However , with very large datasets , it may be necessary to relax the criteria from \u201c once and only once \u201d to a relatively small number of occurrences , in order to consider a fragment a candidate for protection of intellectual property .", "entities": []}, {"text": "70training set are \u201c it is \u201d and \u201c human life \u201d . )", "entities": []}, {"text": "Here is one possible use of GOT .", "entities": []}, {"text": "If a generated sentence contains a fragment that appears once and only once in the ground truth ( after duplicate sentences are removed from the ground truth ) , then the generated sentence may be discarded because it contains a fragment from the ground truth that is a candidate for protection as intellectual property .", "entities": []}, {"text": "In other words , the sentence may be in violation of a copyright law .", "entities": []}, {"text": "Otherwise , the sentence could include a citation of the source for the original fragment .", "entities": []}, {"text": "The de\ufb01nition of ground - truth original fragments actually calls for more nuance , which we will elaborate and explain how to compute next .", "entities": []}, {"text": "We maintain a count per fragment that is incremented each time the fragment appears in a new sentence in a new document or by a different author ( if the author can be determined in both instances ) in the ground truth .", "entities": []}, {"text": "In other words , if a fragment in the ground truth is repeated in the same document , or by the same author across documents , then the count for that fragment is incremented only once .", "entities": []}, {"text": "( Therefore , an author , if known , should also be stored for each fragment , at least until the count for that fragment is greater than 1 .", "entities": []}, {"text": "When the count for a fragment is greater than 1 , then it has already been determined that the fragment was seen a second time in a different document by a different known , or unknown , author . )", "entities": []}, {"text": "The count for a fragment will be 1 if it occurs just once in the ground truth , or if all of its occurrences are in the same document or by the same author ; otherwise , the count will be greater than 1 .", "entities": []}, {"text": "Now , a ground - truth fragment is said to be original if and only if its count is 1 .", "entities": []}, {"text": "See Algorithm 1 for psuedo - code to test for originality and \ufb01nd all original fragements in a dataset .", "entities": []}, {"text": "To examine fragments , we use a window length ofwlvarying between 2 and the sentence length , wherewlis the number of words in the fragment .", "entities": []}, {"text": "If the \ufb01rst or last word in the window is a determinant ( e.g. , \u2018 a \u2019 or \u2018 the \u2019 ) , any use of the verbs to be andto have ( \u2018 is \u2019 , \u2018 are \u2019 , \u2018 am \u2019 , \u2018 was \u2019 , \u2018 were \u2019 , \u2018 has \u2019 , \u2018 had \u2019 , \u2018 have \u2019 ) , punctuation mark , or preposition / subordinating conjunction ( e.g. , \u2018 to \u2019 , \u2018 of \u2019 , or \u2018 from \u2019 ) , the window is moved one step to the right .", "entities": []}, {"text": "( Shortening the window to get rid of the determinant , special verb , special character , or preposition would result in a window size already covered in the previous step . )", "entities": []}, {"text": "All words and characters are allowed in the other positions of the window , so , for example , a comma or preposition may appear in the middle of a window of size 3 or more .", "entities": []}, {"text": "3.1 Runtime Complexity The following complexity analysis is with respect to Algorithm 1 .", "entities": []}, {"text": "We are representing FandOwith balanced binary search trees ( e.g. , red - black tree ( Guibas and Sedgewick , 1978 ; OKASAKI , 1999 ) ) where the comparator is lexicographic ordering .", "entities": []}, {"text": "Searching , insertion and deletion in such trees take \u0012(logn)comparisons .", "entities": []}, {"text": "Since the length of fragments is assumed to be constant on average , then each comparison takes constant time , implying that each search / insert / delete operation in OandFtake \u0012(logn)time .", "entities": []}, {"text": "Given our representation of FandOwith balanced binary search trees , consider the following time complexity analysis : \u2022Letn= number of sentences in the dataset .", "entities": []}, {"text": "The \ufb01rst for - loop ( line 1 ) iterates ntimes .", "entities": []}, {"text": "\u2022Letc= the average length ( i.e. , number of tokens ) of a sentence in our ground truth .", "entities": []}, {"text": "We found thatc= 25 , a fairly small constant .", "entities": []}, {"text": "Therefore , the two for - loops in Steps 4 and 5 iterate on average a constant number of times .", "entities": []}, {"text": "\u2022The binary search in F(line 10 ) has a runtime complexity of \u0012(logn ) .", "entities": []}, {"text": "\u2022Depending on the result of the binary search ofF(line 10 ) there may be an insertion to F ( line 14 ) which has a runtime complexity of \u0012(logn ) .", "entities": []}, {"text": "\u2022Then the number of calculations in lines 1 - 20 is the following function of n:2c2nlogn .", "entities": []}, {"text": "\u2022The code segment of lines 21 - 26 takes \u0012(n ) time because the number of wl - token fragments in the ground truth dataset ( of nsentences where each sentence consists of ctokens on average ) is at most cn .", "entities": []}, {"text": "\u2022Therefore , the runtime complexity is : \u0012(nlogn ) .", "entities": []}, {"text": "This algorithm would be executed before generation tasks , but may also be executed whenever the 2If the \ufb01rst or last word in the window is a determinant ( e.g. , \u2018 a \u2019 or \u2018 the \u2019 ) , special verb ( \u2018 is \u2019 , \u2018 are \u2019 , \u2018 am \u2019 , \u2018 was \u2019 , \u2018 were \u2019 , \u2018 has \u2019 , \u2018 had \u2019 , \u2018 have \u2019 ) , punctuation mark , or preposition / subordinating conjunction ( e.g. , \u2018 to \u2019 , \u2018 of \u2019 , or \u2018 from \u2019 ) , the window is moved one step to the right .", "entities": []}, {"text": "71Algorithm 1 Find Original Fragments in the Ground Truth Require : Input S , the sentences in the ground truth to evaluate Require : Input F , list of fragments already discovered , may be empty set ; Require : Input CountPerFrag ( f ) , for all f2F Require : O , list of original fragments .Count per o2O should always be 1 1 : for each s2S do 2 : l = number of tokens in sentence s 3 : sentParts = set of tokens in s 4 : for each wlin range 2toldo .", "entities": []}, {"text": "wl = length of window 5 : for each iin range 0tol\u0000wl+", "entities": []}, {"text": "1do .assume zero - based indexing 6 : ifsentParts [ i]orsentParts [ i+wl\u00001 ] = special token2then 7 : Continue to next i 8 : else 9 : frag = sentParts", "entities": []}, {"text": "[ i : i+wl ] 10 : iffrag2F then .binary search of F 11 : CountPerFrag", "entities": []}, {"text": "[ frag ] = CountPerFrag [ frag ] + 1 12 : Break from for - loop in line 5 13 : else .frag was not found in F 14 :", "entities": []}, {"text": "Add frag toF 15 : CountPerFrag [ frag ] = 1 16 : end if 17 : end if 18 : end for 19 : end for 20 : end for 21 : Set Oto the empty set ; 22 : for each frag inFdo 23 : ifCountPerFrag [ frag ]", "entities": []}, {"text": "= = 1 then 24 : Add frag toO ; 25 : end if 26 : end for reference set changes or is updated ( for example , based on generated language ) .", "entities": []}, {"text": "4 Example : Results on One Application To see how GOT performed on a generation task , we applied it to a metaphor generator that we built , based on an RNN ( Elman , 1990 ) architecture with LSTM cells ( Hochreiter and Schmidhuber , 1997 ) for training a language model on the language of metaphors , using only metaphors and their topics as input .", "entities": [[39, 40, "MethodName", "LSTM"]]}, {"text": "( A topic was inserted at the beginning of each input sentence . )", "entities": []}, {"text": "The model was trained to predict the next word in the sentences from our ground truth \u2014 a set of 22,113 quotes , where each quote contains at least one metaphor and is labeled with a topic .", "entities": []}, {"text": "There are 1,684 unique topics ( e.g. , \u201c animals \u201d , \u201c fear \u201d , \u201c \ufb01shing \u201d , \u201c grandparents \u201d , \u201c happiness \u201d , \u201d motives \u201d , \u201c politics \u201d , and more examples listed in Table 2 ) and the dataset is currently available to the public online as part of \u201c Dr. Mardy \u2019s Dictionary of Metaphorical Quotations \u201d ( Grothe , 2008 ) .", "entities": []}, {"text": "To the trained language model , we apply an inference engine that uses weighted random choice with a \u201c constraining factor \u201d to encourage language coherence and originality in the output , and pat - terns of metaphors to encourage the generation of grammatically correct metaphors ( Brooks and Youssef , 2020 ) .", "entities": []}, {"text": "The constraining factor , c(for c\u00151 ) , causes the inference engine to select \u2014 with a probability of1 c \u2014 the most likely word to appear next .", "entities": []}, {"text": "Otherwise , and with a probability of 1\u00001 c , the inference engine will make a weighted random selection .", "entities": []}, {"text": "Selecting the most likely next word encourages language coherencey in the output , while weighted random selection encourages originality .", "entities": []}, {"text": "( We found that a constraining factor of 3 or 4 worked best with our model . )", "entities": []}, {"text": "A generated sentence failed the GOT if a fragment of at least 2 words appeared as an \u201c original \u201d fragment in the training set ; that is , if the fragment appeared just once in the ground truth .", "entities": []}, {"text": "Using our metaphor generator , we generated 500 metaphors from randomly chosen topics .", "entities": []}, {"text": "Applying GOT on each of the 500 generated metaphors , we found that only 32 repeated an \u201c original \u201d fragment from the training set .", "entities": []}, {"text": "From this experiment , we conclude that out of the 500 generated metaphors , 468 of them , or just over 93 % , can be considered original .", "entities": []}, {"text": "( Table 2 provides examples from our metaphor generator on randomly generated topics . )", "entities": []}, {"text": "72Topic Generated Metaphor tears The arrested waters shone and danced .", "entities": []}, {"text": "fathers Expectations are premeditated resentments .", "entities": []}, {"text": "character Today is the companion of genius .", "entities": []}, {"text": "friends Assumptions are the termites of relationships .", "entities": []}, {"text": "writers The writer is the lengthened shadow of a man .", "entities": []}, {"text": "world", "entities": []}, {"text": "This world is the rainbow of us .", "entities": []}, {"text": "truth", "entities": []}, {"text": "The brain is the eden of a star .", "entities": []}, {"text": "innocence", "entities": []}, {"text": "The cure for silence is the salt of speech .", "entities": []}, {"text": "imagination Success is the only deadline .", "entities": []}, {"text": "Table 2 : Examples of Generated Metaphors 5 Conclusion Our approach to originality testing includes two contributions : \u2022An automatic test , where no standard existed , for originality in generated language \u2022An automatic test , where no standard existed , for identifying where generators are in violation of copying an original use of language without attribution The \ufb01rst contribution tells us whether a generation is an original use of language .", "entities": []}, {"text": "The second contribution tells us whether a generation is , at least , not at risk of committing plagiarism .", "entities": []}, {"text": "For example , the sentence \u201c A bird built a nest \u201d is not an original use of language ; however , it is at least probably not in violation of plagiarism since it does not contain a fragment that is so rare that it should be protected as an original use of language .", "entities": []}, {"text": "References Jennifer Brooks and Abdou Youssef .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Discriminative pattern mining for natural language metaphor generation .", "entities": []}, {"text": "In Proceedings of the Discriminative Pattern Mining Workshop .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "BERT : pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "CoRR , abs/1810.04805 .", "entities": []}, {"text": "Jeffrey L. Elman .", "entities": []}, {"text": "1990 .", "entities": []}, {"text": "Finding structure in time .", "entities": []}, {"text": "COGNITIVE SCIENCE , 14(2):179\u2013211 .", "entities": []}, {"text": "Mardy Grothe . 2008 .", "entities": []}, {"text": "I", "entities": []}, {"text": "Never Metaphor I Did n\u2019t Like : A Comprehensive Compilation of History \u2019s Greatest Analogies , Metaphors , and Similes .", "entities": []}, {"text": "Harper Collins .", "entities": []}, {"text": "Leo J. Guibas and Robert Sedgewick .", "entities": []}, {"text": "1978 .", "entities": []}, {"text": "A dichromatic framework for balanced trees .", "entities": []}, {"text": "In 19th Annual Symposium on Foundations of Computer Science ( sfcs 1978 ) , pages 8\u201321 .", "entities": []}, {"text": "S. Hochreiter and J. Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long shortterm memory .", "entities": []}, {"text": "Neural Computation , 9:1735\u20131780 .", "entities": []}, {"text": "Lili Mou and Olga Vechtomova . 2020 .", "entities": []}, {"text": "Stylized text generation : Approaches and applications .", "entities": [[1, 3, "TaskName", "text generation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics : Tutorial Abstracts , pages 19\u201322 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "CHRIS OKASAKI .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Red - black trees in a functional setting .", "entities": []}, {"text": "Journal of Functional Programming , 9(4):471\u2013477 .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and Wei jing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "pages 311\u2013318 .", "entities": []}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In I. Guyon , U. V .", "entities": []}, {"text": "Luxburg , S. Bengio , H. Wallach , R. Fergus , S. Vishwanathan , and R. Garnett , editors , Advances in Neural Information Processing Systems 30 , pages 5998\u20136008 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}]