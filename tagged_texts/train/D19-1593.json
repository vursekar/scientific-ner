[{"text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 5838\u20135845 , Hong Kong , China , November 3\u20137 , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics5838Higher - order Comparisons of Sentence Encoder Representations Mostafa Abdouy\u0003Artur", "entities": []}, {"text": "Kulmizev|Felix Hill}Daniel M. Low\u007fAnders S\u00f8gaardy yDepartment of Computer Science , University of Copenhagen |Department of Linguistics and Philology , Uppsala University } DeepMind\u007fProgram in Speech and Hearing Bioscience and Technology , Harvard Medical School - MIT Abstract Representational Similarity Analysis ( RSA ) is a technique developed by neuroscientists for comparing activity patterns of different measurement modalities ( e.g. , fMRI , electrophysiology , behavior ) .", "entities": []}, {"text": "As a framework , RSA has several advantages over existing approaches to interpretation of language encoders based on probing or diagnostic classi\ufb01cation : namely , it does not require large training samples , is not prone to over\ufb01tting , and it enables a more transparent comparison between the representational geometries of different models and modalities .", "entities": []}, {"text": "We demonstrate the utility of RSA by establishing a previously unknown correspondence between widely - employed pretrained language encoders and human processing dif\ufb01culty via eye - tracking data , showcasing its potential in the interpretability toolbox for neural models .", "entities": []}, {"text": "1 Introduction Examining the parallels between human and machine learning is a natural way for us to better understand the former and track our progress in the latter .", "entities": []}, {"text": "The \u201c black box \u201d aspect of neural networks has recently inspired a large body of work related to interpretability , i.e. understanding of representations that such models learn .", "entities": []}, {"text": "In NLP , this push has been largely motivated by linguistic questions , such as : what linguistic properties are captured by neural networks ?", "entities": []}, {"text": "andto what extent do decisions made by neural models re\ufb02ect established linguistic theories ?", "entities": []}, {"text": "Given the relative recency of such questions , much work in the domain so far has been focused on the context of models in isolation ( e.g. what does model Xlearn about linguistic phenomenon Y ? )", "entities": []}, {"text": "In order to more broadly understand models \u2019 representational tendencies , however , it is vital that such questions be formed not only with other models in mind , but also other rep - resentational methods and modalities ( e.g. behavioral data , fMRI measurements , etc . ) .", "entities": []}, {"text": "In context of the latter concern , the present - day interpretability toolkit has not yet been able to afford a practical way of reconciling this .", "entities": []}, {"text": "In this work , we employ Representational Similarity Analysis ( RSA ) as a simple method of interpreting neural models \u2019 representational spaces as they relate to other models and modalities .", "entities": []}, {"text": "In particular , we conduct an experiment wherein we investigate the correspondence between human processing dif\ufb01culty ( as re\ufb02ected by gaze \ufb01xation measurements ) and the representations induced by popular pretrained language models .", "entities": [[30, 33, "TaskName", "pretrained language models"]]}, {"text": "In our experiments , we hypothesize that there exists an overlap between the sentences which are dif\ufb01cult for humans to process and those for which per - layer encoder representations are least correlated .", "entities": []}, {"text": "Our intuition is that such sentences may exhibit factors such as low - frequency vocabulary , lexical ambiguity , and syntactic complexity ( e.g. multiple embedded clauses ) , etc . that are uncommon in both standard language and , relatedly , the corpora employed in training large - scale language models .", "entities": []}, {"text": "In the case of a human reader , encountering such a sentence may result in a number of processing delays , e.g. longer aggregate gaze duration .", "entities": []}, {"text": "In the case of a sentence encoder , an uncommon sentence may lead to a degradation of representations in the encoder \u2019s layers , wherein a lower layer might learn to encode vastly different information than a higher one .", "entities": []}, {"text": "Similarly , different models \u2019 representations may emphasize different aspects of these more complex sentences and therefore diverge from each other .", "entities": []}, {"text": "With this in mind , our hypothesis is that sentences which are dif\ufb01cult for humans to process are likely to have divergent representations within models \u2019 internal layers and between different models \u2019 layers .", "entities": []}, {"text": "5839Understanding and analysing language encoders In recent years , some prominent efforts towards interpreting neural networks for NLP have included : developing suites that evaluate network representations through performance on downstream tasks ( Conneau et al . , 2017a ;", "entities": []}, {"text": "Wang et al . , 2018 ; McCann et al . , 2018 ) ; analyzing network predictions on carefully curated datasets ( Linzen et al . , 2016 ; Marvin and Linzen , 2018 ; Gulordava et al . , 2018 ; Loula et al . , 2018 ; Dasgupta et al . , 2018 ; Tenney et al . , 2018 ) ; and employing diagnostic classi\ufb01ers to assess whether certain classes of information are encoded in a model \u2019s ( intermediate ) representations ( Adi et al . , 2016 ; Chrupa\u0142a et al . , 2017 ; Hupkes et al . , 2017 ; Belinkov et al . , 2017 ) .", "entities": []}, {"text": "While these approaches provide valuable insights into how neural networks process a large variety of phenomena , they rely on decoding accuracy as a probe for encoded linguistic information .", "entities": [[21, 22, "MetricName", "accuracy"]]}, {"text": "If properly biased , this means that they can detect whether information is encoded in a representation or not .", "entities": []}, {"text": "However , they do not allow for a direct comparison of representational structure between models .", "entities": []}, {"text": "Consider a toy dataset of \ufb01ve sentences of interest and three encodings derived from quite different processing models ; a hidden state of a trained neural language model , a tf - idf weighted bag - of - words representation , and measurements of \ufb01xation duration from an eyetracking device .", "entities": []}, {"text": "Probing methods do not allow us to quantify or visualise , for each of these encoding strategies , how the encoder \u2019s responses to the \ufb01ve sentences relate to each other .", "entities": []}, {"text": "Moreover , probing methods would not directly reveal whether the \ufb01xations from the eye - tracking device aligned more closely with the tf - idf representation or the states of the neural language model .", "entities": []}, {"text": "In short , while probing classi\ufb01er methods can establish if phenomena are separable based on the provided representations , they do not tell us about the overall geometry of the representational spaces .", "entities": []}, {"text": "RSA , on the other hand , provides a basis for higher - order comparisons between spaces of representations , and a way to visualise and quantify the extent to which they are isomorphic .", "entities": []}, {"text": "Indeed , RSA has seen a modest introduction within interpretable NLP in recent years .", "entities": []}, {"text": "For example , Chrupa\u0142a et al .", "entities": []}, {"text": "( 2017 ) employed RSA as a means of correlating encoder representations of speech , text , and images in a post - hoc analysis of amulti - task neural pipeline .", "entities": []}, {"text": "Similarly , Bouchacourt and Baroni ( 2018 ) used the framework to measure the similarity between input image embeddings and the representations of the same image by an agent in an language game setting .", "entities": [[28, 29, "DatasetName", "agent"]]}, {"text": "More recently , Chrupa\u0142a and Alishahi ( 2019 ) correlated activation patterns of sentence encoders with symbolic representations , such as syntax trees .", "entities": []}, {"text": "Lastly , similar to our work here , Abnar et al .", "entities": []}, {"text": "( 2019 ) proposed an extension to RSA that enables the comparison of a single model in the face of isolated , changing parameters , and employed this metric along with RSA to correlate NLP models \u2019 and human brains \u2019 respective representations of language .", "entities": []}, {"text": "We hope to position our work among this brief survey and further demonstrate the \ufb02exibility of RSA across several levels of abstraction .", "entities": []}, {"text": "2 Representational Similarity Analysis RSA was proposed by Kriegeskorte et al .", "entities": []}, {"text": "( 2008 ) as a method of relating the different representational modalities employed in neuroscienti\ufb01c studies .", "entities": []}, {"text": "Due to the lack of correspondence between the activity patterns of disparate measurement modalities ( e.g. brain activity via fMRI , behavioural responses ) , RSA aims to abstract away from the activity patterns themselves and instead compute representational dissimilarity matrices ( RDMs ) , which characterize the information carried by a given representation method through dissimilarity structure .", "entities": []}, {"text": "Given a set of representational methods ( e.g. , pretrained encoders )", "entities": []}, {"text": "Mand a set of experimental conditions ( sentences ) N , we can construct RDMs for each method in M.", "entities": []}, {"text": "Each cell in an RDM corresponds to the dissimilarity between the activity patterns associated with pairs of experimental conditionsni;nj2N , say , a pair of sentences .", "entities": []}, {"text": "When ni = nj , the dissimilarity between an experimental condition and itself is intuitively 0 , thus making theN\u0002NRDM symmetric along a diagonal of zeros ( Kriegeskorte et al . , 2008 ) .", "entities": [[15, 16, "DatasetName", "0"]]}, {"text": "The RDMs of the different representational methods in Mcan then be directly compared in a Representational Similarity Matrix ( RSM ) .", "entities": []}, {"text": "This comparison of RDMs is known as second - order analysis , which is broadly based on the idea of asecond - order isomorphism ( Shepard and Chipman , 1970 ) .", "entities": []}, {"text": "In such an analysis , the principal point of comparison is the match between the dissimilarity structure of the different representa-", "entities": []}, {"text": "5840 Figure 1 : An example of \ufb01rst- and second - order analyses , where N= # of experimental conditions , M= # of models , and H= # of activity patterns observed for a given model ( i.e. dimensionality ) .", "entities": []}, {"text": "The right - most side of the \ufb01gure depicts a representational similairty matrix ( RSM ) of correlations between RDMs .", "entities": []}, {"text": "tional methods .", "entities": []}, {"text": "Intuitively , this can be expressed through the notion of distance between distances , and is thus related to Earth Mover \u2019s Distance ( Rubner et al . , 2000).1Figure 1 shows an illustration of the \ufb01rst and second order analyses for pretrained language encoders .", "entities": []}, {"text": "Note that RSA is meaningfully different from , and complementary to , methods that employ saturating functions of representation distances ( e.g. decoding accuracy , mutual information ) , which suffer from ( a)a ceiling effect : being able to distinguish experimental phenomenon AfromB with with an accuracy of 100 % and experimental phenomenon CfromDwith an accuracy of 100 % does not mean that the distance between AandB is the same as that between CandD ; and ( b)discretization ( Nili et al . , 2014 ) .", "entities": [[23, 24, "MetricName", "accuracy"], [47, 48, "MetricName", "accuracy"], [56, 57, "MetricName", "accuracy"]]}, {"text": "We follow Kriegeskorte et al .", "entities": []}, {"text": "( 2008 ) in using the correlation distance of experimental condition pairs ni;nj2Nas a dissimilarity measure , where \u0016niis the mean of ni \u2019s elements,\u0001is the dot product , and kis thel2norm : corr(x ) = 1\u0000(ni\u0000\u0016ni)\u0001(nj\u0000\u0016nj ) k(ni\u0000\u0016nik2k(nj\u0000\u0016njk2 .", "entities": []}, {"text": "Compared to other measures , correlation distance is preferable as it normalizes both the mean and variance of activity patterns over experimental conditions .", "entities": []}, {"text": "Other popular measures include the Euclidean distance and the Malahanobis distance ( Kriegeskorte et al . , 2006 ) .", "entities": []}, {"text": "3 Fixation Duration and Encoder Disagreement Gaze \ufb01xation patterns have been shown to strongly re\ufb02ect the online cognitive processing demands of 1More precisely , our measure of dissimilarity between experimental conditions is analogous to ground distance and dissimilarity between RDMs to earth mover \u2019s distance.human readers ( Raney et al . , 2014 ; Ashby et al . , 2005 ) and to be dependent upon a number of linguistic factors ( Van Gompel , 2007 ) .", "entities": []}, {"text": "Speci\ufb01cally , it has been demonstrated that word frequency , syntactic complexity , and lexical ambiguity play a strong part in determining which sentences are dif\ufb01cult for humans to process ( Rayner and Duffy , 1986 ;", "entities": []}, {"text": "Duffy et al . , 1988 ; Levy , 2008 ) .", "entities": []}, {"text": "Using the RSA framework , we aim to explore how gaze \ufb01xation patterns and the linguistic factors associated with sentence processing dif\ufb01culty relate to the representational spaces of popular language encoders .", "entities": []}, {"text": "Namely , we hypothesize that , for a given sentence , disagreement between hidden layers corresponds to processing dif\ufb01culty .", "entities": []}, {"text": "Because layer disagreement for a sentence measures the extent to which two layers ( e.g. within BERT ) disagree with each other about the pairwise similarity of the sentence ( with other sentences in the corpus ) , a sentence with high layer disagreement will have unstable similarity relationships to other sentences in the corpus .", "entities": [[16, 17, "MethodName", "BERT"]]}, {"text": "This indicates that it has a degraded encoder representation .", "entities": []}, {"text": "Going further , we also hypothesize that models \u2019 representations of said sentences may be confounded , in part , by factors that are known to in\ufb02uence humans .", "entities": []}, {"text": "Eye - tracking data For our experiments", "entities": []}, {"text": ", we make use of the Dundee eye - tracking corpus ( Kennedy et al . , 2003 ) , the English part of which consists of eye - movement data recorded as 10 native participants read 2,368 sentences from 20 newspaper articles .", "entities": []}, {"text": "We consider the following \ufb01xation features : T OTAL FIXATION DURATION and FIRST PASS DURATION .", "entities": [[13, 14, "DatasetName", "PASS"]]}, {"text": "For each of the features , we \ufb01rst take the average of the measurements recorded for all 10 participants per word , then ob-", "entities": []}, {"text": "5841tain sentence - level annotations by summing the measurements of all words in a sentence and dividing by its length .", "entities": []}, {"text": "The result of this is two vectorsVtotfix andVfirstpass of length 2;368 , where each cell in the vector corresponds to a sentence \u2019s average total \ufb01xation and average \ufb01rst pass duration , respectively .", "entities": []}, {"text": "Syntactic complexity , word frequency , and lexical ambiguity We also consider the three following linguistic features which affect processing dif\ufb01culty .", "entities": []}, {"text": "For each of the following the result is also a vector of length 2;368where each cell corresponds to a sentence : a.the average word log frequency per sentence extracted from the British National Corpus ( Leech , 1992 ) , VlogFreq : .", "entities": []}, {"text": "b.the average number of senses per word per sentence extracted from WordNet ( Miller , 1995),VwordSense .", "entities": []}, {"text": "c. Yngve scores , a standard measure of syntactic complexity based on cognitive load ( Yngve , 1960 ) , VY ngve .", "entities": []}, {"text": "Pretrained encoders We conduct our analysis on pretrained BERT - large ( Devlin et al . , 2018 ) and ELMo ( Peters et al . , 2018 ) , two widely employed contextual sentence encoders .", "entities": [[8, 9, "MethodName", "BERT"], [20, 21, "MethodName", "ELMo"]]}, {"text": "To obtain a representation of a sentence from a given layer L , we perform mean - pooling over the time - steps which correspond to the words of a sentence , obtaining a vector representation of the sentence .", "entities": []}, {"text": "Meanpooling is a common approach for obtaining vector representations of sentences for downstream tasks ( Peters et al . , 2018 ; Conneau et al . , 2017b ) .", "entities": []}, {"text": "We refer to ELMo \u2019s lowest layer as E1 , BERT \u2019s 11th layer as B11 , etc .", "entities": [[3, 4, "MethodName", "ELMo"], [10, 11, "MethodName", "BERT"]]}, {"text": "RDMs We construct an RDM ( see \u00a7 2 ) for each contextual encoder \u2019s layers .", "entities": []}, {"text": "Each RDM is a 2;368 \u00022;368matrix which represents the dissimilarity structure of the layer , ( i.e. , each row vector in the matrix contains the dissimilarity of a given sentence to every other sentence ) .", "entities": []}, {"text": "We then compute the correlations between the two different RDMs .", "entities": []}, {"text": "For our evaluation of how well the representational geometry of a layer correlates to another , we employ Kendall \u2019s \u001c Aas suggested in Nili et", "entities": []}, {"text": "al . ( 2014 ) , computing the pairwise correlation for each two corresponding rows in two RDMs .", "entities": []}, {"text": "This second - order analysis gives us a pairwise relational similarity vector VCorr Li\u0000Ljoflength 2;368 , which has the correlations between two layersLiandLj \u2019s RDMs for each of the sentences .", "entities": []}, {"text": "Third - order analysis The \ufb01nal part of our analysis involves computing correlations ( Spearman \u2019s \u001a ) offVCorr Li\u0000Lj;VlogFreq;VY ngve;VwordSenseg with each of Vtotfix andVfirstpass .", "entities": []}, {"text": "The results from this are shown in Table 1 .", "entities": []}, {"text": "The top section of the table shows correlations when LiandLjare the three \ufb01nal adjacent layers in BERT and ELMo .", "entities": [[16, 17, "MethodName", "BERT"], [18, 19, "MethodName", "ELMo"]]}, {"text": "The middle section shows the results for top three BERT layer pairs LiandLjwhich maximize the correlation scores .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "The \ufb01nal section shows correlation with the linguistic features .", "entities": []}, {"text": "Finally , Figure 2 shows Spearman \u2019s \u001acorrelations between VCorr Li\u0000Ljand each ofVtotfix , andVY ngve for all combinations of the 24 BERT layers .", "entities": [[22, 23, "MethodName", "BERT"]]}, {"text": "4 Discussion Our results show highly signi\ufb01cant negative correlations between VCorr Li\u0000Ljand sentence gaze \ufb01xation times .", "entities": []}, {"text": "These \ufb01ndings con\ufb01rm the hypothesis that the sentences that are most challenging for humans to process , are the sentences ( a)the layers of BERT disagree most on among themselves ; and ( b)that ELMo and BERT disagree most on , indicating that there may be common factors which affect human processing dif\ufb01culty and result in disagreement between layers .", "entities": [[24, 25, "MethodName", "BERT"], [34, 35, "MethodName", "ELMo"], [36, 37, "MethodName", "BERT"]]}, {"text": "By Layer disagreement we refer to the expression 1\u0000VCorr Li\u0000Lj .", "entities": []}, {"text": "It is important to note that these encoders are trained with a language modelling objective , unlike models where reading behaviour is explicitly modelled ( Hahn and Keller , 2016 ) or predicted ( Matthies and S\u00f8gaard , 2013 ) .", "entities": [[12, 14, "TaskName", "language modelling"]]}, {"text": "Indeed , the similarities here emerge naturally as a function of the task being performed .", "entities": []}, {"text": "This can be seen as analogous to the case of similarities observed between neural networks trained to perform object recognition and spatio - temporal cortical dynamics ( Cichy et al . , 2016 ) .", "entities": [[18, 20, "TaskName", "object recognition"]]}, {"text": "Syntactic complexity Figure 2 shows that , for all combinations of BERT layers , total \ufb01xation time and Yngve scores have strong negative and positive correlations ( respectively ) with layer disagreement .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "Furthermore , we observe that disagreement between middle layers seems to show the strongest correlation with Yngve scores .", "entities": []}, {"text": "To con\ufb01rm this , we split the correlations into four groups : \u201c low \u201d ( i;j2[1;8 ] ) , \u201c middle \u201d ( i;j2", "entities": []}, {"text": "5842 Figure 2 : RSMs showing ( Spearman \u2019s \u001a ) correlation between disagreement among layers iandj(VCorr Li\u0000Lj ) andVtotfix ( left )", "entities": []}, {"text": "andVY ngve ( Right ) .", "entities": []}, {"text": "BERT layers are denoted with numbers from 1 ( topmost ) to 24 ( lowest ) .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "Layer Disagreement Total Fixation First Pass Duration E1 - B22 -0.46 -0.46 E2 - B23 -0.66 -0.67 E3 - B24 -0.22 -0.23 B11 - B12 -0.88 -0.87 B12 - B13 -0.87", "entities": []}, {"text": "-0.85 B10 - B21 -0.87 -0.86 Linguistic Features Log Freq .", "entities": []}, {"text": "-0.20 -0.19 Avg .", "entities": []}, {"text": "Senses per Word -0.007 * -0.004 * Yngve Score 0.66 0.66 Table 1 : Spearman \u2019s \u001abetweenVCorr Li\u0000Lj , VlogFreq : , VwordSense , VY ngve and each ofVtotfix andVfirstpass .", "entities": [[8, 9, "MetricName", "Score"]]}, {"text": "All correlations signi\ufb01cant with p<0:0001 after Bonferroni correction unless marked with * .", "entities": []}, {"text": "[ 9;16 ] ) , \u201c high \u201d ( i;j2[17;24 ] ) , and \u201c out \u201d ( ji\u0000 jj>7 ) , with the latter representing out - ofgroup correlations ( e.g. Corr L1\u0000L24 ) .", "entities": []}, {"text": "To account for correlations between disagreeing adjacent layers ( e.g.ji\u0000jj= 1 ) and Yngve scores being higher ( as a possible confounding factor ) , we also distinguish layers as either \u201c adjacent \u201d or \u201c non - adjacent \u201d .", "entities": []}, {"text": "Considering these two factors as three- and two - leveled independent variables respectively , we conduct a two - way analysis of variance .", "entities": []}, {"text": "The analysis reveals that the effect of group is signi\ufb01cant at F(3;275 ) = 78:47;p < 0:0001 , with \u201c low \u201d ( \u0016= 0.65,\u001b= 0.08 ) , \u201c middle \u201d ( \u0016= 0.84,\u001b= 0.03 ) , \u201c high \u201d ( \u0016= 0.80,\u001b= 0.05 ) , and \u201c out \u201d ( \u0016= 0.80,\u001b= 0.05 ) .", "entities": []}, {"text": "Neither the effect of adjacency nor its interaction with group proved to be signi\ufb01cant .", "entities": []}, {"text": "This can be seen as ( modest ) support for the \ufb01ndings of previous work ( Blevins et al . , 2018 ; Tenney et al . , 2019 ): namely , that the intermediate layers of neural language models encode themost syntax , and are therefore possibly more sensitive towards syntactic complexity .", "entities": []}, {"text": "A very similar pattern is observed for total \ufb01xation time .", "entities": []}, {"text": "When considered together with the correlation between VY ngve and \ufb01xation times , this indicates a tripartite af\ufb01nity between layer disagreement , syntactic complexity , and \ufb01xation .", "entities": []}, {"text": "Lexical Ambiguity and Word Frequency", "entities": []}, {"text": "Finally , we observe that VlogFreq : has a moderate correlation with both \ufb01xation time and layer disagreement and that VwordSense is nearly uncorrelated to both .", "entities": []}, {"text": "Detailed plots of the latter can be found in Appendix A. 5 Conclusion We presented a framework for analyzing neural network representations ( RSA ) that allowed us to relate human sentence processing data with language encoder representations .", "entities": []}, {"text": "In experiments conducted on two widely used encoders , our \ufb01ndings show that sentences which are dif\ufb01cult for humans to process have more divergent representations both intra - encoder and between different encoders .", "entities": []}, {"text": "Furthermore , we lend modest support to the intuition that a model \u2019s middle layers encode comparatively more syntax .", "entities": []}, {"text": "Our framework offers insight that is complimentary to decoding or probing approaches , and is particularly useful to compare representations from across modalities .", "entities": []}, {"text": "Acknowledgements We would like to thank Vinit Ravishankar , Matt Lamm , and the anonymous reviewers for their helpful comments .", "entities": []}, {"text": "Mostafa Abdou and Anders S\u00f8gaard are supported by a Google Focused Research Award and a Facebook Research Award .", "entities": [[9, 10, "DatasetName", "Google"]]}, {"text": "5843References Abnar , S. , Beinborn , L. , Choenni , R. , and Zuidema , W. ( 2019 ) .", "entities": []}, {"text": "Blackbox meets blackbox :", "entities": []}, {"text": "Representational similarity & stability analysis of neural language models and brains .", "entities": []}, {"text": "In Proceedings of the 2019 ACL Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 191 \u2013 203 , Florence , Italy .", "entities": [[22, 23, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Adi , Y . , Kermany , E. , Belinkov , Y . , Lavi , O. , and Goldberg , Y .", "entities": []}, {"text": "( 2016 ) .", "entities": []}, {"text": "Fine - grained analysis of sentence embeddings using auxiliary prediction tasks .", "entities": [[5, 7, "TaskName", "sentence embeddings"]]}, {"text": "arXiv preprint arXiv:1608.04207 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ashby , J. , Rayner , K. , and Clifton , C. ( 2005 ) .", "entities": []}, {"text": "Eye movements of highly skilled and average readers : Differential effects of frequency and predictability .", "entities": []}, {"text": "The Quarterly Journal of Experimental Psychology Section A , 58(6):1065\u20131086 .", "entities": []}, {"text": "Belinkov , Y . , M ` arquez , L. , Sajjad , H. , Durrani , N. , Dalvi , F. , and Glass , J. ( 2017 ) .", "entities": []}, {"text": "Evaluating layers of representation in neural machine translation on partof - speech and semantic tagging tasks .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Eighth International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , volume 1 , pages 1\u201310 .", "entities": []}, {"text": "Blevins , T. , Levy , O. , and Zettlemoyer , L. ( 2018 ) .", "entities": []}, {"text": "Deep rnns encode soft hierarchical syntax .", "entities": []}, {"text": "arXiv preprint arXiv:1805.04218 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Bouchacourt , D. and Baroni , M. ( 2018 ) .", "entities": []}, {"text": "How agents see things : On visual representations in an emergent language game .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 981\u2013985 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Chrupa\u0142a , G. and Alishahi , A. ( 2019 ) .", "entities": []}, {"text": "Correlating neural and symbolic representations of language .", "entities": []}, {"text": "arXiv preprint arXiv:1905.06401 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Chrupa\u0142a , G. , Gelderloos , L. , and Alishahi , A. ( 2017 ) .", "entities": []}, {"text": "Representations of language in a model of visually grounded speech signal .", "entities": []}, {"text": "arXiv preprint arXiv:1702.01991 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Cichy , R. M. , Khosla , A. , Pantazis , D. , Torralba , A. , and Oliva , A. ( 2016 ) .", "entities": []}, {"text": "Comparison of deep neural networks to spatio - temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence .", "entities": [[14, 16, "TaskName", "object recognition"]]}, {"text": "Scienti\ufb01c reports , 6:27755 .", "entities": []}, {"text": "Conneau , A. , Kiela , D. , Schwenk , H. , Barrault , L. , and Bordes , A. ( 2017a ) .", "entities": []}, {"text": "Supervised learning of universal sentence representations from natural language inference data .", "entities": [[7, 10, "TaskName", "natural language inference"]]}, {"text": "CoRR , abs/1705.02364 .", "entities": []}, {"text": "Conneau , A. , Kiela , D. , Schwenk , H. , Barrault , L. , and Bordes , A. ( 2017b ) .", "entities": []}, {"text": "Supervised learning of universal sentence representations from natural language inference data .", "entities": [[7, 10, "TaskName", "natural language inference"]]}, {"text": "arXiv preprint arXiv:1705.02364 .Dasgupta , I. , Guo , D. , Stuhlm \u00a8uller , A. , Gershman , S. J. , and Goodman , N. D. ( 2018 ) .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Evaluating compositionality in sentence embeddings .", "entities": [[3, 5, "TaskName", "sentence embeddings"]]}, {"text": "arXiv preprint arXiv:1802.04302 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Devlin , J. , Chang , M.-W. , Lee , K. , and Toutanova , K. ( 2018 ) .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Duffy , S. A. , Morris , R. K. , and Rayner , K. ( 1988 ) .", "entities": []}, {"text": "Lexical ambiguity and \ufb01xation times in reading .", "entities": []}, {"text": "Journal of memory and language , 27(4):429\u2013446 .", "entities": []}, {"text": "Gulordava , K. , Bojanowski , P. , Grave , E. , Linzen , T. , and Baroni , M. ( 2018 ) .", "entities": []}, {"text": "Colorless green recurrent networks dream hierarchically .", "entities": []}, {"text": "arXiv preprint arXiv:1803.11138 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hahn , M. and Keller , F. ( 2016 ) .", "entities": []}, {"text": "Modeling human reading with neural attention .", "entities": []}, {"text": "arXiv preprint arXiv:1608.05604 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hupkes , D. , Veldhoen , S. , and Zuidema , W. ( 2017 ) .", "entities": []}, {"text": "Visualisation and\u2019diagnostic classi\ufb01ers \u2019 reveal how recurrent and recursive neural networks process hierarchical structure .", "entities": []}, {"text": "arXiv preprint arXiv:1711.10203 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kennedy , A. , Hill , R. , and Pynte , J. ( 2003 ) .", "entities": []}, {"text": "The dundee corpus .", "entities": []}, {"text": "In Proceedings of the 12th European conference on eye movement .", "entities": []}, {"text": "Kriegeskorte , N. , Goebel , R. , and Bandettini , P. ( 2006 ) .", "entities": []}, {"text": "Information - based functional brain mapping .", "entities": []}, {"text": "Proceedings of the National Academy of Sciences , 103(10):3863\u20133868 .", "entities": []}, {"text": "Kriegeskorte , N. , Mur , M. , and Bandettini , P. A. ( 2008 ) .", "entities": []}, {"text": "Representational similarity analysisconnecting the branches of systems neuroscience .", "entities": []}, {"text": "Frontiers in systems neuroscience , 2:4 .", "entities": []}, {"text": "Leech , G. N. ( 1992 ) .", "entities": []}, {"text": "100 million words of english : the british national corpus ( bnc ) .", "entities": []}, {"text": "Levy , R. ( 2008 ) .", "entities": []}, {"text": "Expectation - based syntactic comprehension .", "entities": []}, {"text": "Cognition , 106(3):1126\u20131177 . Linzen , T. , Dupoux , E. , and Goldberg , Y .", "entities": []}, {"text": "( 2016 ) .", "entities": []}, {"text": "Assessing the ability of lstms to learn syntax - sensitive dependencies .", "entities": []}, {"text": "arXiv preprint arXiv:1611.01368 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Loula , J. , Baroni , M. , and Lake , B. M. ( 2018 ) .", "entities": []}, {"text": "Rearranging the familiar : Testing compositional generalization in recurrent networks .", "entities": []}, {"text": "arXiv preprint arXiv:1807.07545 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Marvin , R. and Linzen , T. ( 2018 ) .", "entities": []}, {"text": "Targeted syntactic evaluation of language models .", "entities": []}, {"text": "arXiv preprint arXiv:1808.09031 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matthies , F. and S\u00f8gaard , A. ( 2013 ) .", "entities": []}, {"text": "With blinkers on : Robust prediction of eye movements across readers .", "entities": []}, {"text": "InProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 803\u2013807 .", "entities": []}, {"text": "5844McCann , B. , Keskar , N. S. , Xiong , C. , and Socher , R. ( 2018 ) .", "entities": []}, {"text": "The natural language decathlon : Multitask learning as question answering .", "entities": [[8, 10, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1806.08730 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Miller , G. A. ( 1995 ) .", "entities": []}, {"text": "Wordnet : a lexical database for english .", "entities": []}, {"text": "Communications of the ACM , 38(11):39 \u2013 41 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Nili , H. , Wing\ufb01eld , C. , Walther , A. , Su , L. , MarslenWilson , W. , and Kriegeskorte , N. ( 2014 ) .", "entities": []}, {"text": "A toolbox for representational similarity analysis .", "entities": []}, {"text": "PLoS computational biology , 10(4):e1003553 .", "entities": [[0, 1, "DatasetName", "PLoS"]]}, {"text": "Peters , M. E. , Neumann , M. , Iyyer , M. , Gardner , M. , Clark , C. , Lee , K. , and Zettlemoyer , L. ( 2018 ) .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "arXiv preprint arXiv:1802.05365 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Raney , G. E. , Campbell , S. J. , and Bovee , J. C. ( 2014 ) .", "entities": []}, {"text": "Using eye movements to evaluate the cognitive processes involved in text comprehension .", "entities": []}, {"text": "Journal of visualized experiments : JoVE , ( 83 ) .", "entities": []}, {"text": "Rayner , K. and Duffy , S. A. ( 1986 ) .", "entities": []}, {"text": "Lexical complexity and \ufb01xation times in reading : Effects of word frequency , verb complexity , and lexical ambiguity .", "entities": []}, {"text": "Memory & cognition , 14(3):191\u2013201 .", "entities": []}, {"text": "Rubner , Y . , Tomasi , C. , and Guibas , L. ( 2000 ) .", "entities": []}, {"text": "The earth mover \u2019s distance as a metric for image retrieval .", "entities": [[9, 11, "TaskName", "image retrieval"]]}, {"text": "In IJCV .", "entities": []}, {"text": "Shepard , R. N. and Chipman , S. ( 1970 ) .", "entities": []}, {"text": "Second - order isomorphism of internal representations : Shapes of states .", "entities": []}, {"text": "Cognitive psychology , 1(1):1\u201317 .", "entities": []}, {"text": "Tenney , I. , Das , D. , and Pavlick , E. ( 2019 ) .", "entities": []}, {"text": "Bert rediscovers the classical nlp pipeline .", "entities": []}, {"text": "arXiv preprint arXiv:1905.05950 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tenney , I. , Xia , P. , Chen , B. , Wang , A. , Poliak , A. , McCoy , R. T. , Kim , N. , Van Durme , B. , Bowman , S. , Das , D. , et al . ( 2018 ) .", "entities": []}, {"text": "What do you learn from context ?", "entities": []}, {"text": "probing for sentence structure in contextualized word representations .", "entities": []}, {"text": "Van Gompel , R. P. ( 2007 ) .", "entities": []}, {"text": "Eye movements : A window on mind and brain .", "entities": []}, {"text": "Elsevier .", "entities": []}, {"text": "Wang , A. , Singh , A. , Michael , J. , Hill , F. , Levy , O. , and Bowman , S. R. ( 2018 ) .", "entities": []}, {"text": "Glue : A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[11, 14, "TaskName", "natural language understanding"]]}, {"text": "arXiv preprint arXiv:1804.07461 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yngve , V .", "entities": []}, {"text": "H. ( 1960 ) .", "entities": []}, {"text": "A model and an hypothesis for language structure .", "entities": []}, {"text": "Proceedings of the American philosophical society , 104(5):444\u2013466 .", "entities": []}, {"text": "A Correlation Heatmaps", "entities": []}, {"text": "5845 Figure 3 : RSM heatmaps showing ( Spearman \u2019s \u001a ) correlation between disagreement among layers iandj ( VCorr Li\u0000Lj ) and ( a)Vfirstpass ( top ) , ( b)VwordSense ( middle ) and , ( c)VlogFreq ( bottom ) .", "entities": []}]