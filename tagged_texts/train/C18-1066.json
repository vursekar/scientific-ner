[{"text": "Proceedings of the 27th International Conference on Computational Linguistics , pages 774\u2013784 Santa Fe , New Mexico , USA , August 20 - 26 , 2018.774A Position - aware Bidirectional Attention Network for Aspect - level Sentiment Analysis Shuqin Gu1 , Lipeng Zhang2 , Yuexian", "entities": [[36, 38, "TaskName", "Sentiment Analysis"]]}, {"text": "Hou1\u0003andYin Song1 1School of Computer Science and Technology , Tianjin University , Tianjin , China 2School of Computer Software , Tianjin University , Tianjin , China fshuqingu , lpzhang , yxhou , songyin g@tju.edu.cn Abstract Aspect - level sentiment analysis aims to distinguish the sentiment polarity of each speci\ufb01c aspect term in a given sentence .", "entities": [[38, 40, "TaskName", "sentiment analysis"]]}, {"text": "Both industry and academia have realized the importance of the relationship between aspect term and sentence , and made attempts to model the relationship by designing a series of attention models .", "entities": []}, {"text": "However , most existing methods usually neglect the fact that the position information is also crucial for identifying the sentiment polarity of the aspect term .", "entities": []}, {"text": "When an aspect term occurs in a sentence , its neighboring words should be given more attention than other words with long distance .", "entities": []}, {"text": "Therefore , we propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional GRU .", "entities": [[16, 18, "MethodName", "bidirectional GRU"]]}, {"text": "PBAN not only concentrates on the position information of aspect terms , but also mutually models the relation between aspect term and sentence by employing bidirectional attention mechanism .", "entities": []}, {"text": "The experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our proposed PBAN model .", "entities": []}, {"text": "1 Introduction Sentiment analysis , also known as opinion mining ( Liu , 2012 ; Pang et al . , 2008 ) , is a vital task in Natural Language Processing ( NLP ) .", "entities": [[2, 4, "TaskName", "Sentiment analysis"], [8, 10, "TaskName", "opinion mining"]]}, {"text": "It divides the text into two or more classes according to the affective states and the subjective information of the text , and has received plenty of attention from both industry and academia .", "entities": []}, {"text": "In this paper , we address the aspect - level sentiment analysis , which is a \ufb01ne - grained task in the \ufb01eld of sentiment analysis .", "entities": [[10, 12, "TaskName", "sentiment analysis"], [24, 26, "TaskName", "sentiment analysis"]]}, {"text": "For instance , given the mentioned aspect terms fmenu;server;specials g , and the sentence is \u201c The menu looked good , except for offering the Chilean Sea Bass , but the server does not offer up the specials that were written on the board outside . \u201d .", "entities": []}, {"text": "For aspect term menu , the sentiment polarity is positive , but for aspect term server , the polarity is negative while for specials , the polarity is neutral .", "entities": []}, {"text": "One important challenge in aspect - level sentiment analysis is how to model the semantic relationship between aspect terms and sentences .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}, {"text": "Traditional approaches have de\ufb01ned rich features about content and syntactic structures so as to capture the sentiment polarity ( Jiang et al . , 2011 ) .", "entities": []}, {"text": "However this kind of feature - based method is labor - intensive and highly depends on the quality of the features .", "entities": []}, {"text": "Compared with these methods , neural network architectures are capable of learning features without feature engineering , and have been widely used in a variety of NLP tasks such as machine translation ( Cho et al . , 2014 ) , question answering ( Andreas et al . , 2016 ) and text classi\ufb01cation ( Lai et al . , 2015 ) .", "entities": [[14, 16, "TaskName", "feature engineering"], [30, 32, "TaskName", "machine translation"], [41, 43, "TaskName", "question answering"]]}, {"text": "Recently , with the development of the neural networks , they are also applied to target - dependent sentiment analysis1 , such as Target - Dependent LSTM ( TD - LSTM )", "entities": [[26, 27, "MethodName", "LSTM"], [30, 31, "MethodName", "LSTM"]]}, {"text": "( Tang et al . , 2015 ) and Target - Connection LSTM ( TC - LSTM ) ( Tang et al . , 2015 ) .", "entities": [[12, 13, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"]]}, {"text": "However , these neural network - based methods can not effectively identify which words in the sentence are more important .", "entities": []}, {"text": "Fortunately , attention mechanisms are an effective way to solve this problem .", "entities": []}, {"text": "\u0003Corresponding author : Yuexian Hou .", "entities": []}, {"text": "This work is licenced under a Creative Commons Attribution 4.0 International Licence .", "entities": []}, {"text": "Licence details : http://creativecommons.org/licenses/by/4.0/ 1The aim of the target - dependent sentiment analysis is similar to aspect - level sentiment analysis .", "entities": [[11, 13, "TaskName", "sentiment analysis"], [19, 21, "TaskName", "sentiment analysis"]]}, {"text": "Given a sentence and target / aspect term , the task calls for inferring the sentiment polarity of the sentence towards the target / aspect term .", "entities": []}, {"text": "775Attention , which is widely applied to Computer Vision ( CV ) and NLP \ufb01elds , is an effective mechanism and has been demonstrated in image recognition ( Mnih et al . , 2014 ) ,", "entities": [[25, 27, "TaskName", "image recognition"]]}, {"text": "machine translation ( Bahdanau et al . , 2014 ; Luong et al . , 2015 ) and reading comprehension ( Hermann et al . , 2015 ; Cui et", "entities": [[0, 2, "TaskName", "machine translation"], [18, 20, "TaskName", "reading comprehension"]]}, {"text": "al . , 2016 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Therefore , some researchers have designed attention networks to address the aspect - level sentiment analysis and have obtained comparable results , such as AE - LSTM ( Wang et al . , 2016 ) , ATAE - LSTM ( Wang et al . , 2016 ) and IAN ( Ma et al . , 2017 ) .", "entities": [[14, 16, "TaskName", "sentiment analysis"], [24, 25, "MethodName", "AE"], [26, 27, "MethodName", "LSTM"], [38, 39, "MethodName", "LSTM"], [48, 49, "MethodName", "IAN"]]}, {"text": "However , these existing work ignores or does not explicitly model the position information of the aspect term in a sentence , which has been studied for improving performance in information retrieval ( IR ) .", "entities": [[30, 32, "TaskName", "information retrieval"]]}, {"text": "In ( Liu et al . , 2015 ) , the occurrence positions of the query terms were modeled via kernel functions and then integrated into traditional IR models to boost the retrieval performance .", "entities": []}, {"text": "By analyzing this aspect - level sentiment analysis task and the corresponding dataset , we \ufb01nd that when an aspect term occurs in a sentence , its neighboring words in the sentence should be given more attention than other words with long distance .", "entities": [[6, 8, "TaskName", "sentiment analysis"]]}, {"text": "Let us take \u201c It \u2019s a perfect place to have an amazing indian food . \u201d", "entities": []}, {"text": "as an example , when the aspect term is indian food , its corresponding sentiment polarity is positive .", "entities": []}, {"text": "Intuitively , we can see that the neighboring word of the indian food ( i.e. \u201c amazing \u201d ) has a greater contribution to judge the sentiment polarity of the aspect term than other words with long distance such as \u201c to \u201d and \u201c have \u201d .", "entities": []}, {"text": "Sometimes this intuitive idea of judging the sentiment polarity may be interpreted as a cognitive activity , which also can be rephrased in a quantum - like language model ( Niu et al . , 2017 ) .", "entities": []}, {"text": "To be speci\ufb01c , sentiment polarity may be interpreted as a quantum - like cognition state .", "entities": []}, {"text": "Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) ( Cho et al . , 2014 ) .", "entities": [[0, 1, "DatasetName", "Inspired"], [30, 31, "MethodName", "GRU"]]}, {"text": "In addition to utilizing the position information , PBAN also mutually models the relationship between the sentence and different words in the aspect term by adopting a bidirectional attention mechanism .", "entities": []}, {"text": "To be speci\ufb01c , our model consists of three components : 1 ) Obtaining position information of each word in corresponding sentence based on the current aspect term , then converting the position information into position embedding .", "entities": []}, {"text": "2 ) The PBAN composes of two Bi - GRU networks focusing on extracting the aspectlevel features and sentence - level features respectively .", "entities": [[9, 10, "MethodName", "GRU"]]}, {"text": "3 ) Using the bidirectional attention mechanism to model the mutual relation between aspect term and its corresponding sentence .", "entities": []}, {"text": "We evaluate our models on SemEval 2014 Datasets , and the results show that our models are more effective than other previous methods .", "entities": []}, {"text": "The main contributions of our work can be summarized as follows : ( 1 ) We attempt to explicitly investigate the effectiveness of the position information of aspect term for aspect - level sentiment analysis .", "entities": [[33, 35, "TaskName", "sentiment analysis"]]}, {"text": "( 2 ) We propose a position - aware bidirectional attention network ( PBAN ) based on Bi - GRU , which has been proved to be effective to improve the sentiment analysis performance .", "entities": [[19, 20, "MethodName", "GRU"], [31, 33, "TaskName", "sentiment analysis"]]}, {"text": "( 3 ) We apply a bidirectional attention mechanism , which can enhance the mutual relation between the aspect term and its corresponding sentence , and prevent the irrelevant words from getting more attention .", "entities": []}, {"text": "2 Model Overview In this section , we describe the proposed model position - aware bidirectional attention network ( PBAN ) for aspect - level sentiment analysis and PBAN is shown in Figure 1 .", "entities": [[25, 27, "TaskName", "sentiment analysis"]]}, {"text": "In this paper , the set of sentiment polarity of the aspect term is fpositive;negative;neutral g. 2.1 Position Representation As for how to model the position information of the aspect term in its corresponding sentence , inspired by the position encoding vectors used in ( Collobert et al . , 2011 ; Zeng et al . , 2014 ) , we de\ufb01ne a position index sequence whose length is equal to the length of corresponding sentence .", "entities": []}, {"text": "Suppose that if a word in the aspect term occurs in the sentence , then its position index will be marked as \u201c 0 \u201d , and the position index of other words will be represented as the relative distance to the current aspect term .", "entities": [[23, 24, "DatasetName", "0"]]}, {"text": "pi=8", "entities": []}, {"text": "> < > : ji\u0000jsj ; i < j s 0 ; j s\u0014i\u0014je ji\u0000jej ; i > j e(1 )", "entities": [[10, 11, "DatasetName", "0"]]}, {"text": "776 \u2026 Word   embedding Position embedding \u2026 \u2026", "entities": []}, {"text": "Nw 1w 1p NpBi - GRU 1h 2h \u2026 Nh Bi - GRU 1th 2th \u2026 Mth Mt 1tMean", "entities": [[5, 6, "MethodName", "GRU"], [12, 13, "MethodName", "GRU"]]}, {"text": "PoolAttention Mechanism 1\uf067 Rh 11\uf061 12\uf061 1N\uf061 1M\uf061 2M\uf061 MN\uf061 2\uf067 M\uf067 \u2026 \u2026 \u2026 \u2026 \u2a00 \u2026 \u2026 \u2a00dot productdot product 1s", "entities": []}, {"text": "2s", "entities": []}, {"text": "MsTerm embedding \uf0c4Figure 1 : The architecture of position - aware bidirectional attention network for aspect - level sentiment analysis ( PBAN).fw1;w2;:::;w Ngrepresents the word embedding in a sentence whose length is N , and ft1;t2;:::;t Mgrepresents the aspect term embedding whose length is M.fp1;p2;:::;p Ngis the position embedding of the aspect term , which is concatenated to the word embedding .", "entities": [[18, 20, "TaskName", "sentiment analysis"]]}, {"text": "fh1;h2;:::;hNgdenotes the hidden representation of inputs and fht 1;ht 2;:::;ht Mgindicates the hidden representation of aspect term .", "entities": []}, {"text": "where , jsandjedenote the starting and ending indices of the aspect term respectively , and pican be viewed as the relative distance of the i - thword in sentence to the aspect term .", "entities": []}, {"text": "For example , given a sentence \u201c not only was the food outstanding but the little perks were great . \u201d , and the aspect term isfood , then the position index sequence is represented as p=", "entities": []}, {"text": "[ 4;3;2;1;0;1;2;3;4;5;6;7 ] .", "entities": []}, {"text": "And its corresponding position embedding are obtained by looking up a position embedding matrix P2Rdp\u0002N , which is randomly initialized , and updated during the training process .", "entities": []}, {"text": "Here , dpdenotes the dimension of position embedding and Nindicates the length of the sentence .", "entities": []}, {"text": "After the position index is converted to the embedding , the position embedding can model the different weights of words with different distance .", "entities": []}, {"text": "From this example , it is obvious that the words with smaller relative distances ( such as \u201c outstanding \u201d ) play an more important role in judging the sentiment polarity of food .", "entities": []}, {"text": "We can \ufb01nd that this process is basically consistent with the way people judge the sentiment polarity of the aspect term .", "entities": []}, {"text": "Because we usually \ufb01rst observe the neighboring words of the aspect term , judging whether the neighboring words can show its sentiment polarity , after that we will focus on those words with long distance .", "entities": []}, {"text": "2.2 Word Representation Bidirectional LSTMs have been successfully applied to various NLP tasks ( Bahdanau et al . , 2014 ) , and it models the context dependency with the forward LSTM and the backward LSTM .", "entities": [[31, 32, "MethodName", "LSTM"], [35, 36, "MethodName", "LSTM"]]}, {"text": "The forward LSTM handles the sentence from left to right , and the backward LSTM processes it in the reverse order .", "entities": [[2, 3, "MethodName", "LSTM"], [14, 15, "MethodName", "LSTM"]]}, {"text": "Therefore , we can obtain two hidden representation , and then concatenate the forward hidden state and backward hidden state of each word .", "entities": []}, {"text": "In this paper , we choose to use bidirectional GRU since it performs similarly to bidirectional LSTM but has fewer parameters and lower computational complexity .", "entities": [[8, 10, "MethodName", "bidirectional GRU"], [15, 17, "MethodName", "bidirectional LSTM"]]}, {"text": "Concretely , we \ufb01rstly obtain the representation of each word in aspect term and sentence , and formalize the notations in our work .", "entities": []}, {"text": "We suppose that a sentence consists of Nwords [ w1;w2;:::;w N]and an aspect term contains Mwords [ t1;t2;:::;t M ] , then we get sentence embedding and aspect term embedding by looking up a word embedding matrix E2Rd\u0002vrespectively , where ddenotes the dimension of the embedding , and vindicates the vocabulary size .", "entities": [[24, 26, "TaskName", "sentence embedding"]]}, {"text": "Then we input aspect term embeddings into the left Bi - GRU to get the hidden contextual representa-", "entities": [[11, 12, "MethodName", "GRU"]]}, {"text": "777tion , which consists of forward hidden state\u0000 !", "entities": []}, {"text": "ht i2Rdhand backward hidden state \u0000", "entities": []}, {"text": "ht", "entities": []}, {"text": "i2Rdh , wheredh denotes the number of hidden units .", "entities": []}, {"text": "Finally , the hidden contextual representation of aspect term ht iis obtained by concatenating\u0000 !", "entities": []}, {"text": "ht iand \u0000 ht", "entities": []}, {"text": "i , i.e. ,ht i=", "entities": []}, {"text": "[ \u0000 !", "entities": []}, {"text": "ht", "entities": []}, {"text": "i ; \u0000 ht i]2R2dh .", "entities": []}, {"text": "For the right Bi - GRU structure , we take the concatenation of the position embedding and word embedding as the inputs , then we can obtain the \ufb01nal hidden contextual representation of the inputs , i.e. ,", "entities": [[5, 6, "MethodName", "GRU"]]}, {"text": "hi=", "entities": []}, {"text": "[ \u0000 ! hi ; \u0000hi]2R2dh .", "entities": []}, {"text": "2.3 Position - aware Bidirectional Attention Network Model ( PBAN ) As shown in Figure 1 , attention model in PBAN consists of two parts : including the aspect term to the position - aware sentence part and a position - aware sentence to the aspect term part .", "entities": []}, {"text": "For the former part , we can obtain the different hidden contextual representation of a sentence according to different word in aspect term .", "entities": []}, {"text": "For the later part , we can obtain the attention weights of the words in aspect term according to the position information , which is used for getting the \ufb01nal representation of a sentence .", "entities": []}, {"text": "Details will be described in follwing sections .", "entities": []}, {"text": "Aspect term to position - aware sentence attention : A sentence should be represented differently based on different words in aspect term , because different words may have different effects on the \ufb01nal representation of the sentence .", "entities": []}, {"text": "We \ufb01rstly get the hidden contextual representation of the aspect term by the left Bi - GRU , and get the hidden contextual representation of inputs ( i.e. , the concatenation of word embedding and position embedding ) by the right Bi - GRU structure .", "entities": [[16, 17, "MethodName", "GRU"], [43, 44, "MethodName", "GRU"]]}, {"text": "Here , we regard the position embedding as the part of the inputs , because it intuitively represents the relative distance of words in a sentence to the current aspect term as mentioned in section 2.1 .", "entities": []}, {"text": "Then we calculate the attention weights by adopting hidden contextual representation of aspect term and inputs , obtaining the attention weight distribution of sentence corresponding to each word in this aspect term .", "entities": []}, {"text": "It can be formulated as follows : si = NX j=1 \u000b ijhj ( 2 ) \u000b ij = exp(f(hj;ht i))PN k=1exp(f(hk;ht i))(3 ) f(hj;ht i )", "entities": []}, {"text": "= tanh ( hT jWmht i+bm ) ( 4 ) where \u000b ijindicates the attention weights from the word ht iin the aspect term to the j - thword in the inputs , andtanh is a non - liner activation function .", "entities": [[39, 41, "HyperparameterName", "activation function"]]}, {"text": "Wmis the weight matrix and bmis the bias .", "entities": []}, {"text": "Subsequently , \u000b ijis used to compute a weighted sum of the hidden representation si , producing a semantic vector that represents the input sequence .", "entities": []}, {"text": "Position - aware sentence attention to aspect term", "entities": []}, {"text": ": As we mentioned above , different words in aspect term will play different role in judging the sentiment polarity of aspect term .", "entities": []}, {"text": "Since we obtain the hidden contextual representation of the inputs by the right Bi - GRU , we utilize both the position and semantic information for calculating the attention weights of different words in aspect term .", "entities": [[15, 16, "MethodName", "GRU"]]}, {"text": "The process can be formulated as follows : hR = MX i=1", "entities": []}, {"text": "isi ( 5 )", "entities": []}, {"text": "i = exp(f(h;ht i))PM k=1exp(f(h;ht k))(6 ) f(h;ht i )", "entities": []}, {"text": "= tanh ( hTWnht i+bn ) ( 7 ) h=1 NNX i=1hi ( 8)", "entities": []}, {"text": "778where", "entities": []}, {"text": "istands for the attention weights from inputs to the words in aspect term , denoting which word in aspect term should be more focused .", "entities": []}, {"text": "his calculated by averagely pooling all Bi - GRU hidden states .", "entities": [[8, 9, "MethodName", "GRU"]]}, {"text": "Later , the sequence representation xis obtained by using a non - linear layer : x= tanh ( WRhR+bR ) ( 9 ) where WRandbRare weight matrix and bias respectively .", "entities": [[12, 14, "MethodName", "linear layer"]]}, {"text": "We feed xinto a linear layer , the length of whose output equals to the number of class labels kSk .", "entities": [[4, 6, "MethodName", "linear layer"]]}, {"text": "Finally , we add a softmax layer to compute the probability distribution for judging the sentiment polarities aspositive , negative orneutral : y = softmax ( Wsx+bs ) ( 10 ) where Wsandbsare the weight matrix and bias respectively for softmax layer .", "entities": [[5, 6, "MethodName", "softmax"], [24, 25, "MethodName", "softmax"], [40, 41, "MethodName", "softmax"]]}, {"text": "2.4 Model training The PBAN model can be trained in an end - to - end way in a supervised learning framework , the aim of the training is to optimize all the parameters so as to minimize the objective function ( loss function ) as much as possible .", "entities": [[42, 43, "MetricName", "loss"]]}, {"text": "In our work , let yibe the correct sentiment polarity , which is represented by one - hot vector , andbyidenotes the predicted sentiment polarity for the given sentence .", "entities": []}, {"text": "We regard the cross - entropy as the loss function , and the formula is as follows : loss=\u0000SX i=1yilog(byi )", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "+1 2\u0015k\u0012k2(11 ) where\u0015is the regularization factor and \u0012contains all the parameters .", "entities": []}, {"text": "Furthermore , in order to avoid over-\ufb01tting , we adopt the dropout strategy to enhance our PBAN model .", "entities": []}, {"text": "3 Experiments 3.1 Experiments Setting Parameters Setting : In our experiments , all word embedding are initialized by the pre - trained Glove vector2(Pennington et", "entities": []}, {"text": "al . , 2014 ) .", "entities": []}, {"text": "All the weight matrices are given the initial value by sampling from the uniform distribution U(\u00000:1;0:1 ) , and all the biases are set to zero .", "entities": []}, {"text": "The dimension of the word embedding and aspect term embedding are set to 300 , and the number of the hidden units are set to 200 .", "entities": []}, {"text": "The dimension of position embedding is set to 100 , which is randomly initialized and updated during the training process .", "entities": []}, {"text": "We use Tensor\ufb02ow ( Abadi et al . , 2016 ) to implement our proposed model and employ the Momentum as the training method , whose momentum parameter", "entities": []}, {"text": "is set to 0.9 , \u0015is set to 10\u00006 , and the initial learning rate is set to 0.01 .", "entities": [[13, 15, "HyperparameterName", "learning rate"]]}, {"text": "Dataset : To evaluate our proposed methods , we conduct experiments on the dataset of SemEval 2014 Task43 , the SemEval 2014 dataset consists of reviews in Restaurant and Laptop datasets .", "entities": []}, {"text": "Each review contains a list of aspect terms and corresponding polarities , which are labeled with fpositive;negative;neutral g. Particularly , each aspect term has its character index in the sentence , so that when different aspect term have the same word in a sentence , we can mark the relative position distance of a sentence according to the current aspect term without confusion .", "entities": []}, {"text": "Table 1 shows the training and test sample numbers in each sentiment polarity .", "entities": []}, {"text": "3.2 Model Comparison In order to evaluate the performance of our model , we compare our model with several baseline models , including LSTM ( Wang et al . , 2016 ) , AE - LSTM ( Wang et al . , 2016 ) , ATAE - LSTM ( Wang et al . , 2016 ) , IAN ( Ma et al . , 2017 ) and MemNet ( Tang et al . , 2016 ) .", "entities": [[23, 24, "MethodName", "LSTM"], [33, 34, "MethodName", "AE"], [35, 36, "MethodName", "LSTM"], [47, 48, "MethodName", "LSTM"], [57, 58, "MethodName", "IAN"]]}, {"text": "2Pre - trained word vectors of Glove can be obtained from http://nlp.stanford.edu/projects/glove/ 3The detail introduction of this dataset can be seen at : http://alt.qcri.org/semeval2014/task4/", "entities": []}, {"text": "779DatasetsPositive Negative Neutral Train Test Train Test Train Test Restaurant 2164 728 807 196 637 196 Laptop 994 341 870 128 464 169 Table 1 : Samples of SemEval 2014 Dataset .", "entities": []}, {"text": "LSTM :", "entities": [[0, 1, "MethodName", "LSTM"]]}, {"text": "LSTM takes the sentence as input so as to get the hidden representation of each word .", "entities": [[0, 1, "MethodName", "LSTM"]]}, {"text": "Then it regards the average value of all hidden states as the representation of sentence , and puts it into softmax layer to predict the probability of each sentiment polarity .", "entities": [[20, 21, "MethodName", "softmax"]]}, {"text": "However , it can not capture any information of aspect term in sentence ( Wang et al . , 2016 ) .", "entities": []}, {"text": "AE - LSTM : AE - LSTM \ufb01rst models the words in sentence via LSTM network and concatenate the aspect embedding to the hidden contextual representation for calculating the attention weights , which are employed to produce the \ufb01nal representation for the input sentence to judge the sentiment polarity ( Wang et al . , 2016 ) .", "entities": [[0, 1, "MethodName", "AE"], [2, 3, "MethodName", "LSTM"], [4, 5, "MethodName", "AE"], [6, 7, "MethodName", "LSTM"], [14, 15, "MethodName", "LSTM"]]}, {"text": "ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the aspect embedding to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .", "entities": [[2, 3, "MethodName", "LSTM"], [6, 7, "MethodName", "LSTM"], [8, 9, "MethodName", "AE"], [10, 11, "MethodName", "LSTM"]]}, {"text": "The other design of ATAE - LSTM is the same as AE - LSTM ( Wang et al . , 2016 ) .", "entities": [[6, 7, "MethodName", "LSTM"], [11, 12, "MethodName", "AE"], [13, 14, "MethodName", "LSTM"]]}, {"text": "IAN : IAN considers the separate modeling of aspect terms and sentences respectively .", "entities": [[0, 1, "MethodName", "IAN"], [2, 3, "MethodName", "IAN"]]}, {"text": "IAN is able to interactively learn attentions in the contexts and aspect terms , and generates the representations for aspect terms and contexts separately .", "entities": [[0, 1, "MethodName", "IAN"]]}, {"text": "Finally , it concatenates the aspect term representation and context representation for predicting the sentiment polarity of the aspect terms within its contexts ( Ma et al . , 2017 ) .", "entities": []}, {"text": "MemNet :", "entities": []}, {"text": "MemNet applies attention multiple times on the word embedding , so that more abstractive evidences could be selected from the external memory .", "entities": []}, {"text": "The output of the last attention layer is fed to a softmax layer for predictions ( Tang et al . , 2016 ) .", "entities": [[11, 12, "MethodName", "softmax"]]}, {"text": "DatasetsRestaurant Laptop Three - class Two - class Three - class Two - class LSTM 74.28 \u2014 66.45 \u2014 AE - LSTM 76.60 89.60 68.90 87.40 ATAE - LSTM 77.20 90.90 68.70 87.60 IAN 78.60 \u2014 72.10 \u2014 MemNet(9 ) 80.95 \u2014 72.21 \u2014 PBAN 81.16 91.67 74.12 87.81 Table 2 : Comparison with baselines .", "entities": [[14, 15, "MethodName", "LSTM"], [19, 20, "MethodName", "AE"], [21, 22, "MethodName", "LSTM"], [28, 29, "MethodName", "LSTM"], [33, 34, "MethodName", "IAN"]]}, {"text": "Accuracy on Three - class and Two - class prediction about Restaurant andLaptop dataset , and Two - class denotes fpositive;negative g. MemNet(9 ) indicates that MemNet with nine computational layers .", "entities": [[0, 1, "MetricName", "Accuracy"]]}, {"text": "Best scores are in bold .", "entities": []}, {"text": "Table 2 shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively .", "entities": []}, {"text": "We can observe that our proposed PBAN model achieves the best performance among all methods .", "entities": []}, {"text": "It is obvious that LSTM method gets the worst performance , because it treats aspect term and other words as the same , so that it can not take full advantage of the aspect term information and predicts the same polarity for different aspect terms in a sentence .", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "Furthermore , both AE - LSTM and ATAE - LSTM perform better than LSTM model , because they all consider the importance of the aspect term , and utilize the attention mechanism .", "entities": [[3, 4, "MethodName", "AE"], [5, 6, "MethodName", "LSTM"], [9, 10, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}, {"text": "Speci\ufb01cally , ATAELSTM outperforms AE - LSTM since it appends the aspect embedding to each word embedding and takes them as inputs , which helps the model obtain more semantic information related to aspect term .", "entities": [[4, 5, "MethodName", "AE"], [6, 7, "MethodName", "LSTM"]]}, {"text": "IAN realizes the importance of interaction between aspect term and context , and models aspect term and context using two connected attention networks .", "entities": [[0, 1, "MethodName", "IAN"]]}, {"text": "Thus , IAN performs better than ATAE - LSTM , and achieves an improvement of 1.40 points and 3.40 points on Restaurant andLaptop datasets in Three - class", "entities": [[2, 3, "MethodName", "IAN"], [8, 9, "MethodName", "LSTM"]]}, {"text": "780respectively .", "entities": []}, {"text": "MemNet(9 ) utilizes a more complex structure that containing nine computational layers , and it achieves better results compared to IAN since MemNet reads the useful information from external memory repeatedly .", "entities": [[20, 21, "MethodName", "IAN"]]}, {"text": "Although both IAN and MemNet models performance better than other methods , they all perform less competitive than our PBAN both on Restaurant andLaptop datasets .", "entities": [[2, 3, "MethodName", "IAN"]]}, {"text": "For IAN model , it interactively learns the attentions between the aspect term and its corresponding sentence , but this attention mechanism is coarse - grained and it does not fully consider the in\ufb02uence of different words in aspect term on the sentence .", "entities": [[1, 2, "MethodName", "IAN"]]}, {"text": "For MemNet model , although it utilizes the location information , it is mainly used for calculating the memory vectors .", "entities": []}, {"text": "Nevertheless , PBAN utilizes the character index of the aspect term ( provided in the raw dataset ) and adopts relative distance to represent the position sequence .", "entities": []}, {"text": "As we have mentioned in previous sections , an aspect term contains several words and different words in aspect term should have different contributions to the \ufb01nal representation of sentence .", "entities": []}, {"text": "In PBAN , the position information is regarded as the inputs of the Bi - GRU , so it can help calculate the weights of different words in aspect term and improve the \ufb01nal representation of the sentence .", "entities": [[15, 16, "MethodName", "GRU"]]}, {"text": "Moreover , when different aspect terms contain the same word , our proposed position information can effectively identify the current aspect term without confusion while MemNet can not .", "entities": []}, {"text": "Generally speaking , by integrating the position information and the bidirectional attention mechanism , PBAN achieves the state - of - the - art performances , and it can effectively judge the sentiment polarity of different aspect term in its corresponding sentence so as to improve the classi\ufb01cation accuracy .", "entities": [[48, 49, "MetricName", "accuracy"]]}, {"text": "3.3 Analysis of PBAN Model", "entities": []}, {"text": "In this section , we design a series of models to demonstrate the effectiveness of our PBAN model .", "entities": []}, {"text": "Firstly , we design an ATAE - Bi - GRU model , whose structure is similar with ATAE - LSTM .", "entities": [[9, 10, "MethodName", "GRU"], [19, 20, "MethodName", "LSTM"]]}, {"text": "The only difference between these two models is that ATAE - Bi - GRU uses the Bi - GRU structure rather than LSTM , and other design is the same as ATAE - LSTM .", "entities": [[13, 14, "MethodName", "GRU"], [18, 19, "MethodName", "GRU"], [22, 23, "MethodName", "LSTM"], [33, 34, "MethodName", "LSTM"]]}, {"text": "Next we design a BAN model without modeling position embedding , and it just utilizes the representation of aspect term and sentence .", "entities": []}, {"text": "In BAN , we still adopt bidirectional attention mechanism to model the relation between aspect term and sentence as PBAN does .", "entities": []}, {"text": "The only difference between BAN and PBAN is that BAN without taking the position embedding as a part of inputs .", "entities": []}, {"text": "Moreover , we also design a PAN model , whose structure is similar with the ATAE - BiGRU model .", "entities": [[17, 18, "MethodName", "BiGRU"]]}, {"text": "PAN takes the concatenation of the aspect term embedding and the word embedding as the inputs of the Bi - GRU structure to obtain the hidden contextual representation , and then PAN utilizes this representation and the position embedding of the aspect term to calculate the attention weights , so as to effectively judge the sentiment polarity of an aspect term .", "entities": [[20, 21, "MethodName", "GRU"]]}, {"text": "From Table 3 , we can \ufb01nd that PBAN achieves the best performance among these models .", "entities": []}, {"text": "Dataset Restaurant Laptop ATAE - LSTM 77.20 68.70 ATAE - Bi - GRU 77.68 69.47 PAN 78.07 71.13 IAN 78.60 72.10 BAN 78.74 72.61 PBAN 81.16 74.12 Table 3 : Analysis of PBAN model .", "entities": [[5, 6, "MethodName", "LSTM"], [12, 13, "MethodName", "GRU"], [18, 19, "MethodName", "IAN"]]}, {"text": "Because Bi - GRU structure has a big advantage over LSTM , it is obvious that ATAE - Bi - GRU model performs better than ATAE - LSTM model .", "entities": [[3, 4, "MethodName", "GRU"], [10, 11, "MethodName", "LSTM"], [20, 21, "MethodName", "GRU"], [27, 28, "MethodName", "LSTM"]]}, {"text": "For PAN model , it outperforms ATAE - LSTM and ATAE - BiGRU models , but it is worse than BAN model .", "entities": [[8, 9, "MethodName", "LSTM"], [12, 13, "MethodName", "BiGRU"]]}, {"text": "Compared with ATAE - Bi - GRU , the most difference is that PAN utilizes the position embedding to calculate the attention weights rather than the aspect term embedding like ATAE - Bi - GRU .", "entities": [[6, 7, "MethodName", "GRU"], [34, 35, "MethodName", "GRU"]]}, {"text": "Therefore , according to these three experimental results , we can prove the importance of the position information in aspect - level sentiment analysis task .", "entities": [[22, 24, "TaskName", "sentiment analysis"]]}, {"text": "As for BAN model , it outperforms IAN model while performs worse than PBAN model .", "entities": [[7, 8, "MethodName", "IAN"]]}, {"text": "Because", "entities": []}, {"text": "781 Aspect term Sentence Polarity pizza This   is   one   great   place   to   eat   pizza   more   out   but   not a   good   place   for   take -out   pizza.positive take - out    pizza", "entities": []}, {"text": "This   is   one   great   place   to   eat   pizza   more   out   but   not    a   good   place   for   take -out   pizza.negativeFigure 2 : Case Study : The visualized attention weights for sentence and aspect term by PBAN .", "entities": []}, {"text": "compared with IAN model , BAN model can learn more semantic relationship between aspect term and sentence via bidirectional attention mechanism .", "entities": [[2, 3, "MethodName", "IAN"]]}, {"text": "However , it ignores the position information of aspect term when compared with PBAN model .", "entities": []}, {"text": "As we expect , PBAN achieves the best performance among all these models .", "entities": []}, {"text": "This is because in addition to fully considering the position information of the aspect term in its corresponding sentence , PBAN also considers the mutual relationship between aspect term and sentence , which is mainly achieved by a bidirectional attention mechanism .", "entities": []}, {"text": "3.4 A Case Study To have an intuitive understanding of our proposed model , we visualize the attention weights on the aspect term and sentence in Figure 2 .", "entities": []}, {"text": "The color depth indicates the importance degree of the weight , the darker the more important .", "entities": []}, {"text": "In Figure 2 , the sentence is \u201c This is one great place to eat pizza more out but not a good place for take - out pizza . \u201d , the polarities are positive andnegative forpizza and take - out pizza respectively .", "entities": []}, {"text": "From Figure 2 , we can \ufb01nd that our model is more inclined to consider the neighboring words of the aspect term .", "entities": []}, {"text": "For example , when the current aspect term is pizza , obviously , its neighboring words such as \u201c great \u201d , \u201c place \u201d and\u201cmore \u201d get more attention and play a great role for judging sentiment polarity of pizza .", "entities": []}, {"text": "However , those words that are far from the current aspect term such as \u201c but \u201d , \u201c not \u201d and\u201ctake - out \u201d obtain less attention , which demonstrates the effectiveness of the position information .", "entities": []}, {"text": "For aspect term take - out pizza , it is obvious that the word \u201c take - out \u201d is more important to express the aspect term than the word \u201c pizza \u201d .", "entities": []}, {"text": "From Figure 2 , it is worth noting that some words such as \u201c good \u201d and\u201cplace \u201d get less attention even they are closer to the current aspect term than \u201c but \u201d and\u201cnot \u201d .", "entities": []}, {"text": "This is because different words in aspect term have different effect on a sentence , and we apply the bidirectional attention mechanism to choose more useful words .", "entities": []}, {"text": "For instance , in this case , PBAN should pay more attention on the word \u201c take - out \u201d .", "entities": []}, {"text": "Therefore , PBAN is capable of \ufb01guring out the important part in a sentence for judging the sentiment polarity by modeling the mutual relation between sentence and different words in aspect term .", "entities": []}, {"text": "4 Related Work In this section , we will brie\ufb02y review some research on sentiment analysis in recent years .", "entities": [[14, 16, "TaskName", "sentiment analysis"]]}, {"text": "The previous research can be divided into three directions : traditional machine learning methods , neural network methods and attention network methods .", "entities": []}, {"text": "4.1 Machine Learning for Sentiment Analysis Traditional machine learning approaches mainly involve text representation and feature extraction , such as bag - of - words models and sentiment lexicons features , then training a sentiment classi\ufb01er ( Prez - Rosas et al . , 2012 ) .", "entities": [[4, 6, "TaskName", "Sentiment Analysis"]]}, {"text": "Rao et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2010 ) demonstrated the utility of graph - based semi - supervised learning framework for building sentiment lexicons .", "entities": []}, {"text": "Kaji et al .", "entities": []}, {"text": "( 2007 ) explored to use structural clues that could extract polar sentences from HTML documents , and built lexicon from the extracted polar sentences .", "entities": []}, {"text": "However , these methods are labor - intensive , and usually results in high dimensional and high sparse phenomenon for the text representation .", "entities": []}, {"text": "7824.2 Neural Network for Target - dependent Sentiment Analysis Since a simple and effective method to learn distributed representation was proposed ( Mikolov et al . , 2013 ) , neural networks enhance target - dependent sentiment analysis signi\ufb01cantly .", "entities": [[7, 9, "TaskName", "Sentiment Analysis"], [36, 38, "TaskName", "sentiment analysis"]]}, {"text": "V o and Zhang ( 2015 ) split a tweet into a left context and a right context according to a given target , using distributed word representations and neural pooling functions to extract features .", "entities": []}, {"text": "Tang et al .", "entities": []}, {"text": "( 2015 ) proposed TD - LSTM and TC - LSTM , where target information is automatically taken into account .", "entities": [[6, 7, "MethodName", "LSTM"], [10, 11, "MethodName", "LSTM"]]}, {"text": "These two models integrated the connections between target words and context words so as to signi\ufb01cantly boost the classi\ufb01cation accuracy .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "Zhang et al .", "entities": []}, {"text": "( 2016 ) proposed two gated neural networks , one was used to capture tweet - level syntactic and semantic information , and the other was used to model the interactions between the left context and the right context of a given target .", "entities": []}, {"text": "With the gating mechanism , the target in\ufb02uenced the selection of sentiment signals over the context .", "entities": []}, {"text": "4.3 Attention Network for Aspect - level Sentiment Analysis With the successful application of the attention mechanism in machine translation and reading comprehension , it is also applied to aspect - level sentiment analysis in recent years .", "entities": [[7, 9, "TaskName", "Sentiment Analysis"], [18, 20, "TaskName", "machine translation"], [21, 23, "TaskName", "reading comprehension"], [32, 34, "TaskName", "sentiment analysis"]]}, {"text": "Wang et al .", "entities": []}, {"text": "( 2016 ) examined the latent relatedness of the aspect term and sentiment polarity for aspect - level sentiment analysis .", "entities": [[18, 20, "TaskName", "sentiment analysis"]]}, {"text": "They designed an attention - based LSTM to learn aspect term embedding , and let the aspect term embedding participate in calculating the attention weights .", "entities": [[6, 7, "MethodName", "LSTM"]]}, {"text": "Ma et al .", "entities": []}, {"text": "( 2017 ) proposed a new attention model IAN , which considered the separate modeling of aspect terms and could interactively learn attention in the contexts and aspect terms .", "entities": [[8, 9, "MethodName", "IAN"]]}, {"text": "Despite the effectiveness of these attention mechanisms , they are coarse - grained and it is still challenging to identify different sentiment polarity at a \ufb01ne - grained aspect level .", "entities": []}, {"text": "However , our PBAN model makes full use of the position information of the aspect term , and PBAN uses a \ufb01ne - grained bidirectional attention mechanism to model the mutual relationship between the sentence and each word in the aspect term , identifying the importance of the word in the aspect term to obtain a more effective sentence representation as described in Section 1 . 5", "entities": []}, {"text": "Conclusion In this paper , we have proposed a position - aware bidirectional network ( PBAN ) based on Bi - GRU for aspect - level sentiment analysis .", "entities": [[21, 22, "MethodName", "GRU"], [26, 28, "TaskName", "sentiment analysis"]]}, {"text": "The main idea of PBAN is to utilize the position embedding of aspect term for calculating the attention weights .", "entities": []}, {"text": "Moreover , PBAN adopts a bidirectional attention mechanism , which is not only capable of mutually modeling the relation between sentence and different words in aspect term , but also takes advantage of the position information to better judge the sentiment polarity of aspect term .", "entities": []}, {"text": "Experimental results on SemEval 2014 Datasets demonstrate that our proposed models can learn effective features and obtain superior performance over the baseline models .", "entities": []}, {"text": "Acknowledgements This work is funded in part by the national key research and development program of China ( 2017YFE0111900 ) , the Key Project of Tianjin Natural Science Foundation ( 15JCZDJC31100 ) , the National Natural Science Foundation of China ( Key Program , U1636203 ) , the National Natural Science Foundation of China ( U1736103 ) and MSCA - ITN - ETN - European Training Networks Project ( QUARTZ ) .", "entities": []}, {"text": "References Martn Abadi , Paul Barham , Jianmin Chen , Zhifeng Chen , Andy Davis , Jeffrey Dean , Matthieu Devin , Sanjay Ghemawat , Geoffrey Irving , and Michael Isard .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Tensor\ufb02ow : a system for large - scale machine learning .", "entities": []}, {"text": "Jacob Andreas , Marcus Rohrbach , Trevor Darrell , and Dan Klein . 2016 .", "entities": []}, {"text": "Learning to compose neural networks for question answering .", "entities": [[6, 8, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1601.01705 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "Computer Science .", "entities": []}, {"text": "783Kyunghyun Cho , Bart Van Merri \u00a8enboer , Caglar Gulcehre , Dzmitry Bahdanau , Fethi Bougares , Holger Schwenk , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Learning phrase representations using rnn encoder - decoder for statistical machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1406.1078 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ronan Collobert , Jason Weston , Michael Karlen , Koray Kavukcuoglu , and Pavel Kuksa . 2011 .", "entities": []}, {"text": "Natural language processing ( almost ) from scratch .", "entities": []}, {"text": "Journal of Machine Learning Research , 12(1):2493\u20132537 .", "entities": []}, {"text": "Yiming Cui , Zhipeng Chen , Si Wei , Shijin Wang , Ting Liu , and Guoping Hu . 2016 .", "entities": []}, {"text": "Attention - over - attention neural networks for reading comprehension .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}, {"text": "arXiv preprint arXiv:1607.04423 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Karl Moritz Hermann , Tom Koisk , Edward Grefenstette , Lasse Espeholt , Will Kay , Mustafa Suleyman , and Phil Blunsom .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Teaching machines to read and comprehend .", "entities": []}, {"text": "In International Conference on Neural Information Processing Systems , pages 1693\u20131701 .", "entities": []}, {"text": "Long Jiang , Mo Yu , Ming Zhou , Xiaohua Liu , and Tiejun Zhao . 2011 .", "entities": []}, {"text": "Target - dependent twitter sentiment classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies - Volume 1 , pages 151\u2013160 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nobuhiro Kaji and Masaru Kitsuregawa .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Building lexicon for sentiment analysis from massive collection of html documents .", "entities": [[3, 5, "TaskName", "sentiment analysis"]]}, {"text": "In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning .", "entities": []}, {"text": "Siwei Lai , Liheng Xu , Kang Liu , and Jun Zhao . 2015 .", "entities": []}, {"text": "Recurrent convolutional neural networks for text classi\ufb01cation .", "entities": []}, {"text": "In AAAI , volume 333 , pages 2267\u20132273 .", "entities": []}, {"text": "Baiyan Liu , Xiangdong An , and Jimmy Xiangji Huang . 2015 .", "entities": []}, {"text": "Using term location information to enhance probabilistic information retrieval .", "entities": [[7, 9, "TaskName", "information retrieval"]]}, {"text": "In International Acm Sigir Conference on Research Development in Information Retrieval , pages 883\u2013886 .", "entities": [[2, 3, "DatasetName", "Acm"], [9, 11, "TaskName", "Information Retrieval"]]}, {"text": "Bing Liu .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Sentiment analysis and opinion mining .", "entities": [[0, 2, "TaskName", "Sentiment analysis"], [3, 5, "TaskName", "opinion mining"]]}, {"text": "Synthesis lectures on human language technologies , 5(1):1\u2013167 .", "entities": []}, {"text": "Minh - Thang Luong , Hieu Pham , and Christopher D Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Effective approaches to attention - based neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1508.04025 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Dehong Ma , Sujian Li , Xiaodong Zhang , and Houfeng Wang .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Interactive attention networks for aspect - level sentiment classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1709.00893 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "Computer Science .", "entities": []}, {"text": "V olodymyr Mnih , Nicolas Heess , Alex Graves , et al . 2014 .", "entities": []}, {"text": "Recurrent models of visual attention .", "entities": [[0, 5, "MethodName", "Recurrent models of visual attention"]]}, {"text": "In Advances in neural information processing systems , pages 2204\u20132212 .", "entities": []}, {"text": "Xiaolei Niu , Yuexian Hou , and Panpan Wang . 2017 .", "entities": []}, {"text": "Bi - directional lstm with quantum attention mechanism for sentence modeling .", "entities": [[3, 4, "MethodName", "lstm"]]}, {"text": "In International Conference on Neural Information Processing , pages 178\u2013188 .", "entities": []}, {"text": "Bo Pang , Lillian Lee , et al . 2008 .", "entities": []}, {"text": "Opinion mining and sentiment analysis .", "entities": [[0, 2, "TaskName", "Opinion mining"], [3, 5, "TaskName", "sentiment analysis"]]}, {"text": "Foundations and Trends R", "entities": []}, {"text": "in Information Retrieval , 2(1\u20132):1\u2013135 .", "entities": [[1, 3, "TaskName", "Information Retrieval"]]}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 conference on empirical methods in natural language processing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Vernica Prez - Rosas , Carmen Banea , and Rada Mihalcea .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Learning sentiment lexicons in spanish .", "entities": []}, {"text": "In Eighth International Conference on Language Resources and Evaluation .", "entities": []}, {"text": "Delip Rao and Deepak Ravichandran .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Semi - supervised polarity lexicon induction .", "entities": []}, {"text": "In Eacl 2009 , Conference of the European Chapter of the Association for Computational Linguistics , Proceedings of the Conference , March 30 - April3 , 2009 , Athens , Greece , pages 675\u2013682 .", "entities": []}, {"text": "Duyu Tang , Bing Qin , Xiaocheng Feng , and Ting Liu . 2015 .", "entities": []}, {"text": "Effective lstms for target - dependent sentiment classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1512.01100 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Duyu Tang , Bing Qin , and Ting Liu . 2016 .", "entities": []}, {"text": "Aspect level sentiment classi\ufb01cation with deep memory network .", "entities": [[6, 8, "MethodName", "memory network"]]}, {"text": "arXiv preprint arXiv:1605.08900 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "784Duy - Tin V o and Yue Zhang .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Target - dependent twitter sentiment classi\ufb01cation with rich automatic features .", "entities": []}, {"text": "InIJCAI , pages 1347\u20131353 .", "entities": []}, {"text": "Yequan Wang , Minlie Huang , Xiaoyan Zhu , and Li Zhao .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Attention - based lstm for aspect - level sentiment classi\ufb01cation .", "entities": [[3, 4, "MethodName", "lstm"]]}, {"text": "In EMNLP , pages 606\u2013615 .", "entities": []}, {"text": "D. Zeng , K. Liu , S. Lai , G. Zhou , and J. Zhao .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Relation classi\ufb01cation via convolutional deep neural network .", "entities": []}, {"text": "Meishan Zhang , Yue Zhang , and Duy - Tin V o. 2016 .", "entities": []}, {"text": "Gated neural networks for targeted sentiment analysis .", "entities": [[5, 7, "TaskName", "sentiment analysis"]]}, {"text": "In AAAI , pages 3087\u20133093 .", "entities": []}]