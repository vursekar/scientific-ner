[{"text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 2567\u20132577 , Hong Kong , China , November 3\u20137 , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics2567PubMedQA : A Dataset for Biomedical Research Question Answering Qiao Jin University of Pittsburgh qiao.jin@pitt.eduBhuwan", "entities": [[11, 13, "TaskName", "Question Answering"]]}, {"text": "Dhingra Carnegie Mellon University bdhingra@cs.cmu.eduZhengping Liu University of Pittsburgh zliu@pitt.edu", "entities": []}, {"text": "William W. Cohen Google AI wcohen@google.comXinghua Lu University of Pittsburgh xinghua@pitt.edu", "entities": [[3, 4, "DatasetName", "Google"]]}, {"text": "Abstract We introduce PubMedQA , a novel biomedical question answering ( QA ) dataset collected from PubMed abstracts .", "entities": [[3, 4, "DatasetName", "PubMedQA"], [8, 10, "TaskName", "question answering"]]}, {"text": "The task of PubMedQA is to answer research questions with yes / no / maybe ( e.g. : Do preoperative statins reduce atrial \ufb01brillation after coronary artery bypass grafting ? )", "entities": [[3, 4, "DatasetName", "PubMedQA"]]}, {"text": "using the corresponding abstracts .", "entities": []}, {"text": "PubMedQA has 1k expert - annotated , 61.2k unlabeled and 211.3k arti\ufb01cially generated QA instances .", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}, {"text": "Each PubMedQA instance is composed of ( 1 ) a question which is either an existing research article title or derived from one , ( 2 ) a context which is the corresponding abstract without its conclusion , ( 3 ) a long answer , which is the conclusion of the abstract and , presumably , answers the research question , and ( 4 ) a yes / no / maybe answer which summarizes the conclusion .", "entities": [[1, 2, "DatasetName", "PubMedQA"]]}, {"text": "PubMedQA is the \ufb01rst QA dataset where reasoning over biomedical research texts , especially their quantitative contents , is required to answer the questions .", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}, {"text": "Our best performing model , multi - phase \ufb01ne - tuning of BioBERT with long answer bag - of - word statistics as additional supervision , achieves 68.1 % accuracy , compared to single human performance of 78.0 % accuracy and majority - baseline of 55.2 % accuracy , leaving much room for improvement .", "entities": [[29, 30, "MetricName", "accuracy"], [39, 40, "MetricName", "accuracy"], [47, 48, "MetricName", "accuracy"]]}, {"text": "PubMedQA is publicly available athttps://pubmedqa.github.io .", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}, {"text": "1 Introduction A long - term goal of natural language understanding is to build intelligent systems that can reason and infer over natural language .", "entities": [[8, 11, "TaskName", "natural language understanding"]]}, {"text": "The question answering ( QA ) task , in which models learn how to answer questions , is often used as a benchmark for quantitatively measuring the reasoning and inferring abilities of such intelligent systems .", "entities": [[1, 3, "TaskName", "question answering"]]}, {"text": "While many large - scale annotated general domain QA datasets have been introduced ( Rajpurkar et al . , 2016 ; Lai et al . , 2017 ; Ko \u02c7cisk`yQues tion : Do preoperative statins reduce atrial \ufb01brillation after coronary artery bypass grafting ?", "entities": []}, {"text": "Context : ( Objective ) Recent studies have demonstrated that statins have pleiotropic effects , including anti - in\ufb02ammatory effects and atrial \ufb01brillation ( AF ) preventive effects [ ... ] ( Methods ) 221 patients underwent CABG in our hospital from 2004 to 2007 .", "entities": []}, {"text": "14 patients with preoperative AF and 4 patients with concomitant valve surgery [ ... ] ( Results ) The overall incidence of postoperative AF was 26 % .", "entities": []}, {"text": "Postoperative AF was signi\ufb01cantly lower in the Statin group compared with the Non - statin group ( 16 % versus 33 % , p=0.005 ) .", "entities": []}, {"text": "Multivariate analysis demonstrated that independent predictors of AF", "entities": []}, {"text": "[ ... ] Long Answer : ( Conclusion ) Our study indicated that preoperative statin therapy seems to reduce AF development after CABG .", "entities": []}, {"text": "Answer : yes Figure 1 : An instance ( Sakamoto et al . , 2011 ) of PubMedQA dataset :", "entities": [[17, 18, "DatasetName", "PubMedQA"]]}, {"text": "Question is the original question title ; Context includes the structured abstract except its conclusive part , which serves as the Long Answer ; Human experts annotated the Answer yes .", "entities": []}, {"text": "Supporting fact for the answer is highlighted .", "entities": []}, {"text": "et", "entities": []}, {"text": "al . , 2018 ; Yang et al . , 2018 ; Kwiatkowski et al . , 2019 ) , the largest annotated biomedical QA dataset , BioASQ ( Tsatsaronis et al . , 2015 ) has less than 3k training instances , most of which are simple factual questions .", "entities": [[27, 28, "DatasetName", "BioASQ"]]}, {"text": "Some works proposed automatically constructed biomedical QA datasets ( Pampari et al . , 2018 ; Pappas et al . , 2018 ; Kim et al . , 2018 ) , which have much larger sizes .", "entities": []}, {"text": "However , questions of these datasets are mostly factoid , whose answers can be extracted in the contexts without much reasoning .", "entities": []}, {"text": "In this paper , we aim at building a biomedical QA dataset which ( 1 ) has substantial instances with some expert annotations and ( 2 ) requires reasoning over the contexts to answer the questions .", "entities": []}, {"text": "For this , we turn to the PubMed1 , a search engine providing access to over 25 million references of 1https://www.ncbi.nlm.nih.gov/pubmed/", "entities": []}, {"text": "2568biomedical articles .", "entities": []}, {"text": "We found that around 760k articles in PubMed use questions as their titles .", "entities": []}, {"text": "Among them , the abstracts of about 120k articles are written in a structured style \u2013 meaning they have subsections of \u201c Introduction \u201d , \u201c Results \u201d etc .", "entities": []}, {"text": "Conclusive parts of the abstracts , often in \u201c Conclusions \u201d , are the authors \u2019 answers to the question title .", "entities": []}, {"text": "Other abstract parts can be viewed as the contexts for giving such answers .", "entities": []}, {"text": "This pattern perfectly \ufb01ts the scheme of QA , but modeling it as abstractive QA , where models learn to generate the conclusions , will result in an extremely hard task due to the variability of writing styles .", "entities": []}, {"text": "Interestingly , more than half of the question titles of PubMed articles can be brie\ufb02y answered by yes / no / maybe , which is signi\ufb01cantly higher than the proportions of such questions in other datasets , e.g. : just 1 % in Natural Questions ( Kwiatkowski et al . , 2019 ) and 6 % in HotpotQA ( Yang et al . , 2018 ) .", "entities": [[43, 45, "DatasetName", "Natural Questions"], [57, 58, "DatasetName", "HotpotQA"]]}, {"text": "Instead of using conclusions to answer the questions , we explore answering them with yes / no / maybe and treat the conclusions as a long answer for additional supervision .", "entities": []}, {"text": "To this end , we present PubMedQA , a biomedical QA dataset for answering research questions using yes / no / maybe .", "entities": [[6, 7, "DatasetName", "PubMedQA"]]}, {"text": "We collected all PubMed articles with question titles , and manually labeled 1k of them for cross - validation and testing .", "entities": []}, {"text": "An example is shown in Fig .", "entities": []}, {"text": "1 . The rest of yes / no / answerable QA instances compose of the unlabeled subset which can be used for semisupervised learning .", "entities": []}, {"text": "Further , we automatically convert statement titles of 211.3k", "entities": []}, {"text": "PubMed articles to questions and label them with yes / no answers using a simple heuristic .", "entities": []}, {"text": "These arti\ufb01cially generated instances can be used for pre - training .", "entities": []}, {"text": "Unlike other QA datasets in which questions are asked by crowd - workers for existing contexts ( Rajpurkar et al . , 2016 ; Yang et al . , 2018 ; Ko \u02c7cisk`y et al . , 2018 ) , in PubMedQA contexts are generated to answer the questions and both are written by the same authors .", "entities": [[41, 42, "DatasetName", "PubMedQA"]]}, {"text": "This consistency assures that contexts are perfectly related to the questions , thus making PubMedQA an ideal benchmark for testing scienti\ufb01c reasoning abilities .", "entities": [[14, 15, "DatasetName", "PubMedQA"]]}, {"text": "As an attempt to solve PubMedQA and provide a strong baseline , we \ufb01ne - tune BioBERT ( Lee et al . , 2019 ) on different subsets in a multi - phase style with additional supervision of long answers .", "entities": [[5, 6, "DatasetName", "PubMedQA"]]}, {"text": "Though this model generates decent results and vastly outperforms other baselines , it \u2019s still muchworse than the single - human performance , leaving signi\ufb01cant room for future improvements .", "entities": []}, {"text": "2 Related Works Biomedical QA : Expert - annotated biomedical QA datasets are limited by scale due to the dif\ufb01culty of annotations .", "entities": []}, {"text": "In 2006 and 2007 , TREC2 held QA challenges on genomics corpus ( Hersh et al . , 2006 , 2007 ) , where the task is to retrieve relevant documents for 36 and 38 topic questions , respectively .", "entities": []}, {"text": "QA4MRE ( Pe \u02dcnas et al . , 2013 ) included a QA task about Alzheimer \u2019s disease ( Morante et al . , 2012 ) .", "entities": []}, {"text": "This dataset has 40 QA instances and the task is to answer a question related to a given document using one of \ufb01ve answer choices .", "entities": []}, {"text": "The QA task of BioASQ ( Tsatsaronis et al . , 2015 ) has phases of ( a ) retrieve question - related documents and ( b ) using related documents as contexts to answer yes / no , factoid , list or summary questions .", "entities": [[4, 5, "DatasetName", "BioASQ"]]}, {"text": "BioASQ 2019 has a training set of 2,747 QA instances and a test set of 500 instances .", "entities": [[0, 1, "DatasetName", "BioASQ"]]}, {"text": "Several large - scale automatically collected biomedical QA datasets have been introduced : emrQA ( Pampari et al . , 2018 ) is an extractive QA dataset for electronic medical records ( EHR ) built by re - purposing existing annotations on EHR corpora .", "entities": [[13, 14, "DatasetName", "emrQA"]]}, {"text": "BioRead ( Pappas et al . , 2018 ) and BMKC ( Kim et al . , 2018 ) both collect cloze - style QA instances by masking biomedical named entities in sentences of research articles and using other parts of the same article as context .", "entities": []}, {"text": "Yes / No QA : Datasets such as HotpotQA ( Yang et al . , 2018 ) , Natural Questions ( Kwiatkowski et al . , 2019 ) , ShARC ( Saeidi et al . , 2018 ) and BioASQ ( Tsatsaronis et al . , 2015 ) contain yes / no questions as well as other types of questions .", "entities": [[8, 9, "DatasetName", "HotpotQA"], [18, 20, "DatasetName", "Natural Questions"], [29, 30, "DatasetName", "ShARC"], [39, 40, "DatasetName", "BioASQ"]]}, {"text": "BoolQ ( Clark et al . , 2019 ) speci\ufb01cally focuses on naturally occurring yes / no questions , and those questions are shown to be surprisingly dif\ufb01cult to answer .", "entities": [[0, 1, "DatasetName", "BoolQ"]]}, {"text": "We add a \u201c maybe \u201d choice in PubMedQA to cover uncertain instances .", "entities": [[8, 9, "DatasetName", "PubMedQA"]]}, {"text": "Typical neural approaches to answering yes / no questions involve encoding both the question and context , and decoding the encoding to a class output , which is similar to the well - studied natural language inference ( NLI ) task .", "entities": [[34, 37, "TaskName", "natural language inference"]]}, {"text": "Recent breakthroughs of pre - trained language models like ELMo ( Peters et al . , 2018 ) and BERT ( Devlin et al . , 2018 ) show signi\ufb01cant performance im2https://trec.nist.gov/", "entities": [[9, 10, "MethodName", "ELMo"], [19, 20, "MethodName", "BERT"]]}, {"text": "2569 Ori .", "entities": []}, {"text": "Title - > QuestionLong Answer(Conclusion)Structured Context(Ori .", "entities": []}, {"text": "Abstractw / o conclusion)Generated yes / noPQA - Artificial ( 211.3k)Ori .", "entities": []}, {"text": "Question TitlePQA - Unlabeled ( 61.2k)Ori .", "entities": []}, {"text": "Question Title", "entities": []}, {"text": "Yes / no / maybePQA - Labeled ( 1k )", "entities": []}, {"text": "UnlabeledLong Answer(Conclusion)Structured Context(Ori .", "entities": []}, {"text": "Abstractw / o conclusion)Long Answer(Conclusion)Structured Context(Ori .", "entities": []}, {"text": "Abstractw / o conclusion)Figure 2 : Architecture of PubMedQA dataset .", "entities": [[8, 9, "DatasetName", "PubMedQA"]]}, {"text": "PubMedQA is split into three subsets , PQA - A(rti\ufb01cial ) , PQA - U(nlabeled ) and PQA - L(abeled ) .", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}, {"text": "provements on NLI tasks .", "entities": []}, {"text": "In this work , we use domain speci\ufb01c versions of them to set baseline performance on PubMedQA .", "entities": [[16, 17, "DatasetName", "PubMedQA"]]}, {"text": "3 PubMedQA Dataset 3.1 Data Collection PubMedQA is split into three subsets : labeled , unlabeled and arti\ufb01cially generated .", "entities": [[1, 2, "DatasetName", "PubMedQA"], [6, 7, "DatasetName", "PubMedQA"]]}, {"text": "They are denoted as PQA - L(abeled ) , PQA - U(nlabeled ) and PQA - A(rti\ufb01cial ) , respectively .", "entities": []}, {"text": "We show the architecture of PubMedQA dataset in Fig .", "entities": [[5, 6, "DatasetName", "PubMedQA"]]}, {"text": "2 . Statistic PQA - L PQA - U PQA - A Number of QA pairs 1.0k 61.2k 211.3k Prop . of yes ( % ) 55.2 \u2013 92.8 Prop . of no ( % ) 33.8 \u2013 7.2 Prop . of maybe ( % ) 11.0 \u2013 0.0 Avg . question length 14.4 15.0 16.3 Avg . context length", "entities": [[58, 60, "HyperparameterName", "context length"]]}, {"text": "238.9 237.3 238.0 Avg . long answer length 43.2 45.9 41.0 Table 1 : PubMedQA dataset statistics .", "entities": [[14, 15, "DatasetName", "PubMedQA"]]}, {"text": "Collection of PQA - L and PQA - U : PubMed articles which have i ) a question mark in the titles and ii ) a structured abstract with conclusive part are collected and denoted as pre - PQA - U.", "entities": []}, {"text": "Now each instance has 1 ) a question which is the original title 2 ) a context which is the structured abstract without the conclusive part and 3 ) a long answer which is the conclusive part of the abstract .", "entities": []}, {"text": "Two annotators3labeled 1k instances from prePQA - U with yes / no / maybe to build PQA - L using Algorithm 1 .", "entities": []}, {"text": "The annotator 1 does n\u2019t need to do much reasoning to annotate since the long answer is available .", "entities": []}, {"text": "We denote this reasoning - free setting .", "entities": []}, {"text": "However , the annotator 2 can not use the long answer , so reasoning over the context is required for 3Both are quali\ufb01ed M.D. candidates .", "entities": []}, {"text": "Algorithm 1 PQA - L data collection procedure Input : pre - PQA - U ReasoningFreeAnnotation   fg ReasoningRequiredAnnotation   fg GroundTruthLabel   fg while not \ufb01nished do Randomly sample an instance inst from pre - PQA - U ifinst is not yes / no / maybe answerable then Remove inst and continue to next iteration end if Annotator 1 annotates inst withl12fyes;no;maybeg using question , context and long answer Annotator 2 annotates inst withl22fyes;no;maybeg using question and context", "entities": []}, {"text": "ifl1 = l2then la l1 else Annotator 1 and Annotator 2 discuss for an agreement annotation la", "entities": []}, {"text": "if not9lathen Remove inst and continue to next iteration end if end if ReasoningFreeAnnotation [ inst ] l1 ReasoningRequiredAnnotation [ inst ] l2 GroundTruthLabel [ inst ] la end while annotation .", "entities": []}, {"text": "We denote such setting as reasoningrequired setting .", "entities": []}, {"text": "Note that the annotation process might assign wrong labels when both annotator 1 and annotator 2 make a same mistake , but considering human performance in x5.1 , such error rate could be as low as 1%4 . 500 randomly sampled PQA - L instances are used for 10 - fold cross validation and the rest 500 instances consist of PubMedQA test set .", "entities": [[60, 61, "DatasetName", "PubMedQA"]]}, {"text": "Further , we include the unlabeled instances in pre - PQA - U with yes / no / maybe answerable questions to build PQA - U. For this , we use a simple rule - based method which removes all questions started with interrogative words ( i.e. wh - words ) or involving selections from multiple entities .", "entities": []}, {"text": "This results in over 93 % agreement with annotator 1 in identifying the questions that can be answered by yes / no / maybe .", "entities": []}, {"text": "Collection of PQA - A : Motivated by the recent successes of large - scale pre - training from ELMo ( Peters et al . , 2018 ) and BERT ( Devlin et al . , 2018 ) , we use a simple heuristic to collect many noisily - labeled instances to build PQA - A for pretraining .", "entities": [[19, 20, "MethodName", "ELMo"], [29, 30, "MethodName", "BERT"]]}, {"text": "Towards this end , we use PubMed articles with 1 ) a statement title which has POS tagging structures of NP-(VBP / VBZ)5and 2 ) a structured abstract including a conclusive part .", "entities": []}, {"text": "The 4Roughly half of the products of two annotator error rates .", "entities": []}, {"text": "5Using Stanford CoreNLP parser ( Manning et al . , 2014 ) .", "entities": []}, {"text": "2570Original Statement Title Converted Question Label % Spontaneous electrocardiogram alterations predict ventricular \ufb01brillation in Brugada syndrome .", "entities": []}, {"text": "Dospontaneous electrocardiogram alterations predictventricular \ufb01brillation in Brugada syndrome?yes", "entities": []}, {"text": "92.8 Liver grafts from selected older donors do not have signi\ufb01cantly more ischaemia reperfusion injury .", "entities": []}, {"text": "Doliver grafts from selected older donors have signi\ufb01cantly more ischaemia reperfusion injury?no 7.2 Table 2 : Examples of automatically generated instances for PQA - A. Original statement titles are converted to questions and answers are automatically generated according to the negation status .", "entities": []}, {"text": "0.0 0.2 0.4 0.6 0.8 1.0 Frequency of the MeSH termsHumans Female Male Middle Aged Adult Aged Retrospective Studies Adolescent Aged , 80 and over Prospective Studies Treatment Outcome Young Adult Risk Factors Child Follow - Up Studies Time Factors Surveys and Questionnaires Child , Preschool Cross - Sectional Studies Pregnancy Sensitivity and Specificity Cohort Studies Prognosis Predictive Value of Tests Infant Figure 3 : MeSH topic distribution of PubMedQA .", "entities": [[69, 70, "DatasetName", "PubMedQA"]]}, {"text": "statement titles are converted to questions by simply moving or adding copulas ( \u201c is \u201d , \u201c are \u201d ) or auxiliary verbs ( \u201c does \u201d , \u201c do \u201d ) in the front and further revising for coherence ( e.g. : adding a question mark ) .", "entities": []}, {"text": "We generate the yes / no answer according to negation status of the VB .", "entities": []}, {"text": "Several examples are shown in Table 2 .", "entities": []}, {"text": "We collected 211.3k instances for PQA - A , of which 200k randomly sampled instances are for training and the rest 11.3k instances are for validation .", "entities": []}, {"text": "3.2 Characteristics We show the basic statistics of three PubMedQA subsets in Table 1 .", "entities": [[9, 10, "DatasetName", "PubMedQA"]]}, {"text": "Instance Topics : PubMed abstracts are manually annotated by medical librarians with Medical Subject Headings ( MeSH)6 , which is a controlled vocabulary designed to describe the topics of biomedical texts .", "entities": []}, {"text": "We use MeSH terms to represent abstract topics , and visualize their distribution in Fig .", "entities": []}, {"text": "3 .", "entities": []}, {"text": "Nearly all instances are human studies and they cover a wide variety of topics , including retrospective , prospective , and cohort studies , different age groups , and healthcare - related subjects like treatment outcome , prognosis and risk factors of diseases .", "entities": []}, {"text": "6https://www.nlm.nih.gov/mesh Question TypeReasoning TypeNumber interpretationalreadyin context?Figure 4 : Proportional relationships between corresponded question types , reasoning types , and whether the text interpretations of numbers exist in contexts .", "entities": []}, {"text": "Question and Reasoning Types : We sampled 200 examples from PQA - L and analyzed the types of questions and types of reasoning required to answer them , which is summarized in Table 3 .", "entities": []}, {"text": "Various types of questions have been asked , including causal effects , evaluations of therapies , relatedness , and whether a statement is true .", "entities": []}, {"text": "Besides , PubMedQA also covers several different reasoning types : most ( 57.5 % ) involve comparing multiple groups ( e.g. : experiment and control ) , and others require interpreting statistics of a single group or its subgroups .", "entities": [[2, 3, "DatasetName", "PubMedQA"]]}, {"text": "Reasoning over quantitative contents is required in nearly all ( 96.5 % ) of them , which is expected due to the nature of biomedical research .", "entities": []}, {"text": "75.5 % of contexts have text descriptions of the statistics while 21.0 % only have the numbers .", "entities": []}, {"text": "We use a Sankey diagram to show the proportional relationships between corresponded question type and reasoning type , as well as corresponded reasoning type and whether there are text interpretations of numbers in Fig .", "entities": []}, {"text": "4 . 3.3 Evaluation Settings The main metrics of PubMedQA are accuracy and macro - F1 on PQA - L test set using question and context as input .", "entities": [[9, 10, "DatasetName", "PubMedQA"], [11, 12, "MetricName", "accuracy"], [13, 16, "MetricName", "macro - F1"]]}, {"text": "We denote prediction using question and context as a reasoning - required setting , because under this setting answers are not directly", "entities": []}, {"text": "2571Question Type % Example Questions Does a factor in\ufb02uence the output ?", "entities": []}, {"text": "36.5 Does reducing spasticity translate into functional bene\ufb01t ?", "entities": []}, {"text": "Does ibuprofen increase perioperative blood loss during hip arthroplasty ?", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "Is a therapy good / necessary ?", "entities": []}, {"text": "26.0 Should circumcision be performed in childhood ?", "entities": []}, {"text": "Is external palliative radiotherapy for gallbladder carcinoma effective ?", "entities": []}, {"text": "Is astatement true ?", "entities": []}, {"text": "18.0 Sternal fracture in growing children : A rare and often overlooked fracture ?", "entities": []}, {"text": "Xanthogranulomatous cholecystitis : a premalignant condition ?", "entities": []}, {"text": "Is a factor related to the output ?", "entities": []}, {"text": "18.0", "entities": []}, {"text": "Can PRISM predict length of PICU stay ?", "entities": []}, {"text": "Is trabecular bone related to primary stability of miniscrews ?", "entities": []}, {"text": "Reasoning Type % Example Snippet in Context Inter - group comparison 57.5", "entities": []}, {"text": "[ ... ] Postoperative AF was signi\ufb01cantly lower in the Statin group compared with the Non - statin group ( 16 % versus 33 % , p=0.005 ) .", "entities": []}, {"text": "[ ... ] Interpreting subgroup statistics 16.5", "entities": []}, {"text": "[ ... ] 57 % of patients were of lower socioeconomic status and they had more health problems , less functioning , and more symptoms", "entities": []}, {"text": "[ ... ] Interpreting ( single ) group statistics 16.0", "entities": []}, {"text": "[ ... ] A total of 4 children aged 5 - 14 years with a sternal fracture were treated in 2 years , 2 children were hospitalized for pain management and [ ... ] Text Interpretations of Numbers % Example Snippet in Context Existing interpretations ofnumbers 75.5", "entities": []}, {"text": "[ ... ] Postoperative AF was signi\ufb01cantly lower in the Statin group compared with the Non - statin group ( 16 % versus 33 % , p=0.005 ) .", "entities": []}, {"text": "[ ... ] No interpretations ( numbers only ) 21.0", "entities": []}, {"text": "[ ... ] 30 - day mortality was 12.4 % in those aged < 70 years and 22 % in those>70 years ( p<0.001 ) .", "entities": []}, {"text": "[ ... ] No numbers ( texts only ) 3.5", "entities": []}, {"text": "[ ... ] The halofantrine therapeutic dose group showed loss and distortion of inner hair cells and inner phalangeal cells [ ... ] Table 3 : Summary of PubMedQA question types , reasoning types and whether there are text descriptions of the statistics in context .", "entities": [[9, 10, "MetricName", "loss"], [28, 29, "DatasetName", "PubMedQA"]]}, {"text": "Colored texts are matched key phrases ( sentences ) between types and examples .", "entities": []}, {"text": "expressed in the input and reasoning over the contexts is required to answer the question .", "entities": []}, {"text": "Additionally , long answers are available at training time , so generation or prediction of them can be used as an auxiliary task in this setting .", "entities": []}, {"text": "A parallel setting , where models can use question and long answer to predict yes / no / maybe answer , is denoted as reasoning - free setting since yes /", "entities": []}, {"text": "no / maybe are usually explicitly expressed in the long answers ( i.e. : conclusions of the abstracts ) .", "entities": []}, {"text": "Obviously , it \u2019s a much easier setting which can be exploited for bootstrapping PQA - U. 4 Methods 4.1 Fine - tuning BioBERT We \ufb01ne - tune BioBERT ( Lee et al . , 2019 ) on PubMedQA as a baseline .", "entities": [[38, 39, "DatasetName", "PubMedQA"]]}, {"text": "BioBERT is initialized with BERT ( Devlin et al . , 2018 ) and further pretrained on PubMed abstracts and PMC7articles .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "Expectedly , it vastly outperforms BERT in various biomedical NLP tasks .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "We denote the original transformer weights of BioBERT as \u00120 .", "entities": []}, {"text": "While \ufb01ne - tuning , we feed PubMedQA questions and contexts ( or long answers ) , separated 7https://www.ncbi.nlm.nih.gov/pmc/by the special [ SEP ] token , to BioBERT .", "entities": [[7, 8, "DatasetName", "PubMedQA"]]}, {"text": "The yes / no / maybe labels are predicted using the special[CLS ] embedding using a softmax function .", "entities": [[16, 17, "MethodName", "softmax"]]}, {"text": "Cross - entropy loss of predicted and true label distribution is denoted as LQA .", "entities": [[3, 4, "MetricName", "loss"]]}, {"text": "4.2 Long Answer as Additional Supervision Under reasoning - required setting , long answers are available in training but not inference phase .", "entities": []}, {"text": "We use them as an additional signal for training : similar to Ma et al .", "entities": []}, {"text": "( 2018 ) regularizing neural machine translation models with binary bag - of - word ( BoW ) statistics , we \ufb01ne - tune BioBERT with an auxiliary task of predicting the binary BoW statistics of the long answers , also using the special [ CLS ] embedding .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "We minimize binary crossentropy loss of this auxiliary task : LBoW=\u00001 NX ibilog^bi+ ( 1\u0000bi)log(1\u0000^bi ) wherebiand^biare ground - truth and predicted probability of whether token iis in the long answers ( i.e.:bi2f0;1gand^bi2[0;1 ] ) , andNis the BoW vocabulary size .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "The total loss is : L = LQA+ \f LBoW", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "2572 Figure 5 : Multi - phase \ufb01ne - tuning architecture .", "entities": []}, {"text": "Notations and equations are described in x4.3 .", "entities": []}, {"text": "In reasoning - free setting which we use for bootstrapping , the regularization coef\ufb01cient \f is set to 0 because long answers are directly used as input .", "entities": [[18, 19, "DatasetName", "0"]]}, {"text": "4.3 Multi - phase Fine - tuning Schedule Since PQA - A and PQA - U have different properties from the ultimate test set of PQA - L , BioBERT is \ufb01ne - tuned in a multi - phase style on different subsets .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "5 shows the architecture of this training schedule .", "entities": []}, {"text": "We use q , c , a , lto denote question , context , long answer and yes / no / maybe label of instances , respectively .", "entities": []}, {"text": "Their source subsets are indexed by the superscripts of A for PQA - A , U for PQA - U and L for PQA - L. Phase I Fine - tuning on PQA - A : PQA - A is automatically collected whose questions and labels are arti\ufb01cially generated .", "entities": []}, {"text": "As a result , questions of PQA - A might differ a lot from those of PQAU and PQA - L , and it only has yes / no labels with a very imbalanced distribution ( 92.8 % yes v.s. 7.2 % no ) .", "entities": []}, {"text": "Despite these drawbacks , PQA - A has substantial training instances so models could still bene\ufb01t from it as a pre - training step .", "entities": []}, {"text": "Thus , in Phase I of multi - phase \ufb01ne - tuning , we initialize BioBERT with \u00120 , and \ufb01ne - tune it on PQA - A using question and context as input : \u0012I argmin\u0012L(BioBERT \u0012(qA;cA);lA)(1 )", "entities": []}, {"text": "Phase II Fine - tuning on Bootstrapped PQA - U : To fully utilize the unlabeled instances in PQA - U , we exploit the easiness of reasoning - free setting to pseudo - label these instances with a bootstrapping strategy : \ufb01rst , we initialize BioBERT with \u00120 , and \ufb01ne - tune it on PQA - A using question and long answer ( reasoning - free ) , \u0012B1 argmin\u0012L(BioBERT \u0012(qA;aA);lA)(2)then", "entities": []}, {"text": "we further \ufb01ne - tune BioBERT \u0012B1on PQAL , also under the reasoning - free setting : \u0012B2 argmin\u0012L(BioBERT \u0012(qL;aL);lL)(3 ) We pseudo - label PQA - U instances using the most con\ufb01dent predictions of BioBERT \u0012B2for each class .", "entities": []}, {"text": "Con\ufb01dence is simply de\ufb01ned by the corresponding softmax probability and then we label a subset which has the same proportions of yes / no / maybe labels as those in the PQA - L : lU pseudo BioBERT \u0012B2(qU;aU ) ( 4 ) In phase II , we \ufb01ne - tune BioBERT \u0012Ion the bootstrapped PQA - U using question and context ( under reasoning - required setting ): \u0012II argmin\u0012L(BioBERT \u0012(qU;cU);lU pseudo ) ( 5 ) Final Phase Fine - tuning on PQA - L : In the \ufb01nal phase , we \ufb01ne - tune BioBERT \u0012IIon PQA - L : \u0012F argmin\u0012L(BioBERT \u0012(qL;cL);lL)(6 )", "entities": [[7, 8, "MethodName", "softmax"]]}, {"text": "Final predictions on instances of PQA - L validation and test sets are made using BioBERT \u0012F : lpred = BioBERT \u0012F(qL;cL ) 4.4", "entities": []}, {"text": "Compared Models Majority : The majority ( about 55 % ) of the instances have the label \u201c yes \u201d .", "entities": []}, {"text": "We use a trivial baseline denoted as Majority where we simply predict \u201c yes \u201d for all instances , regardless of the question and context .", "entities": []}, {"text": "Shallow Features :", "entities": []}, {"text": "For each instance , we include the following shallow features : 1 ) TF - IDF statistics of the question 2 ) TF - IDF statistics of the context / long answer and 3 ) sum of IDF of the overlapping non - stop words between the question and the context / long answer .", "entities": []}, {"text": "To allow multi - phase \ufb01ne - tuning , we apply a feed - forward neural network on the shallow features instead of using a logistic classi\ufb01er .", "entities": []}, {"text": "BiLSTM : We simply concatenate the question and context / long answer with learnable segment embeddings appended to the biomedical word2vec embeddings ( Pyysalo et al . , 2013 ) of each token .", "entities": [[0, 1, "MethodName", "BiLSTM"]]}, {"text": "The concatenated sentence is then fed to a biLSTM , and the \ufb01nal hidden states of the forward and backward network are used for classifying the yes / no / maybe label .", "entities": [[8, 9, "MethodName", "biLSTM"]]}, {"text": "2573ESIM with BioELMo : Following the state - ofthe - art recurrent architecture of NLI ( Peters et al . , 2018 ) , we use pre - trained biomedical contextualized embeddings BioELMo ( Jin et al . , 2019 ) for word representations .", "entities": []}, {"text": "Then we apply the ESIM model ( Chen et al . , 2016 ) , where a biLSTM is used to encode the question and context / long answer , followed by an attentional local inference layer and a biLSTM inference composition layer .", "entities": [[4, 5, "MethodName", "ESIM"], [17, 18, "MethodName", "biLSTM"], [39, 40, "MethodName", "biLSTM"]]}, {"text": "After pooling , a softmax output unit is applied for predicting the yes / no / maybe label .", "entities": [[4, 5, "MethodName", "softmax"]]}, {"text": "4.5 Compared Training Schedules Final Phase Only : Under this setting , we train models only on PQA - L.", "entities": []}, {"text": "It \u2019s an extremely low resources setting where there are only 450 training instances in each fold of cross - validation .", "entities": []}, {"text": "Phase I + Final Phase : Under this setting , we skip the training on bootstrapped PQA - U. Models are \ufb01rst \ufb01ne - tuned on PQA - A , and then \ufb01ne - tuned on PQA - L. Phase II + Final Phase : Under this setting , we skip the training on PQA - A. Models are \ufb01rst \ufb01ne - tuned on bootstrapped PQA - U , and then \ufb01netuned on PQA - L. Single - phase Training : Instead of training a model sequentially on different splits , under single - phase training setting we train the model on the combined training set of all PQA splits : PQAA , bootstrapped PQA - U and PQA - L. 5 Experiments 5.1 Human Performance Human performance is measured during the annotation : As shown in Algorithm 1 , annotations of annotator 1 and annotator 2 are used to calculate reasoning - free and reasoning - required human performance , respectively , against the discussed ground truth labels .", "entities": []}, {"text": "Human performance on the test set of PQA - L is shown in Table 4 .", "entities": []}, {"text": "We only test single - annotator performance due to limited resources .", "entities": []}, {"text": "Kwiatkowski et al .", "entities": []}, {"text": "( 2019 ) show that an ensemble of annotators perform signi\ufb01cantly better than single - annotator , so the results reported in Table 4 are the lower bounds of human performance .", "entities": []}, {"text": "Under reasoning - free setting where the annotator can see the conclusions , a single human achieves 90.4 % accuracy and 84.2 % macroF1 .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "Under reasoning - required setting , the task be - comes much harder , but it \u2019s still possible for humans to solve : a single annotator can get 78.0 % accuracy and 72.2 % macro - F1 .", "entities": [[31, 32, "MetricName", "accuracy"], [35, 38, "MetricName", "macro - F1"]]}, {"text": "Setting Accuracy ( % ) Macro - F1 ( % ) Reasoning - Free 90.40 84.18 Reasoning - Required 78.00 72.19 Table 4 : Human performance ( single - annotator ) .", "entities": [[1, 5, "MetricName", "Accuracy ( % )"], [5, 8, "MetricName", "Macro - F1"]]}, {"text": "5.2 Main Results We report the test set performance of different models and training schedules in Table 5 .", "entities": []}, {"text": "In general , multi - phase \ufb01ne - tuning of BioBERT with additional supervision outperforms other baselines by large margins , but the results are still much worse than just single - human performance .", "entities": []}, {"text": "Comparison of Models : A trend of BioBERT > ESIM w/ BioELMo > BiLSTM > shallow features > majority , conserves across different training schedules on both accuracy and macro - F1 .", "entities": [[9, 10, "MethodName", "ESIM"], [13, 14, "MethodName", "BiLSTM"], [27, 28, "MetricName", "accuracy"], [29, 32, "MetricName", "macro - F1"]]}, {"text": "Fine - tuned BioBERT is better than state - of - theart recurrent model of ESIM w/ BioELMo , probably because BioELMo weights are \ufb01xed while all BioBERT parameters can be \ufb01ne - tuned , which better bene\ufb01t from the pre - training settings .", "entities": [[15, 16, "MethodName", "ESIM"]]}, {"text": "Comparison of Training Schedules : Multiphase \ufb01ne - tuning setting gets 5 out of 9 modelwise best accuracy / macro - F1 .", "entities": [[17, 18, "MetricName", "accuracy"], [19, 22, "MetricName", "macro - F1"]]}, {"text": "Due to lack of annotated data , training only on the PQA - L ( \ufb01nal phase only ) generates similar results as the majority baseline .", "entities": []}, {"text": "In phase I + Final setting where models are pre - trained on PQA - A , we observe signi\ufb01cant improvements on accuracy and macro - F1 and some models even achieve their best accuracy under this setting .", "entities": [[22, 23, "MetricName", "accuracy"], [24, 27, "MetricName", "macro - F1"], [34, 35, "MetricName", "accuracy"]]}, {"text": "This indicates that a hard task with limited training instances can be at least partially solved by pre - training on a large automatically collected dataset when the tasks are similarly formatted .", "entities": []}, {"text": "Improvements are also observed in phase II + Final setting , though less signi\ufb01cant than those of phase I + Final .", "entities": []}, {"text": "As expected , multi - phase \ufb01netuning schedule is better than single - phase , due to different properties of the subsets .", "entities": []}, {"text": "Additional Supervision : Despite its simplicity , the auxiliary task of long answer BoW prediction clearly improves the performance : most results ( 28/40 ) are better with such additional supervision than without .", "entities": []}, {"text": "2574ModelFinal Phase Only Single - phase Phase I + Final Phase II + Final Multi - phase Acc F1 Acc F1 Acc F1 Acc F1", "entities": [[17, 18, "MetricName", "Acc"], [18, 19, "MetricName", "F1"], [19, 20, "MetricName", "Acc"], [20, 21, "MetricName", "F1"], [21, 22, "MetricName", "Acc"], [22, 23, "MetricName", "F1"], [23, 24, "MetricName", "Acc"], [24, 25, "MetricName", "F1"]]}, {"text": "Acc F1 Majority 55.20 23.71 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 Human ( single ) 78.00 72.19 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 w/o A.S. Shallow Features 53.88 36.12 57.58 31.47 57.48 37.24 56.28 40.88 53.50 39.33 BiLSTM 55.16 23.97 55.46 39.70 58.44 40.67 52.98 33.84 59.82 41.86 ESIM w/ BioELMo 53.90 32.40 61.28 42.99 61.96 43.32 60.34 44.38 62.08 45.75 BioBERT 56.98 28.50 66.44 47.25 66.90 46.16 66.08 50.84 67.66 52.41 w/ A.S. Shallow Features", "entities": [[0, 1, "MetricName", "Acc"], [1, 2, "MetricName", "F1"], [41, 42, "MethodName", "BiLSTM"], [52, 53, "MethodName", "ESIM"]]}, {"text": "53.60 35.92 57.30 30.45 55.82 35.09 56.46y40.76 55.06y40.67y", "entities": []}, {"text": "BiLSTM 55.22y23.86 55.96y40.26y61.06y41.18y54.12y34.11y58.86 41.06 ESIM w/ BioELMo 53.96y31.07 62.68y43.59y63.72y47.04y60.16", "entities": [[0, 1, "MethodName", "BiLSTM"], [4, 5, "MethodName", "ESIM"]]}, {"text": "45.81y63.72y47.90y BioBERT 57.28y28.70y66.66y46.70y67.24y46.21y66.44y51.41y68.08y52.72y Table 5 : Main results on PQA - L test set under reasoning - required setting .", "entities": []}, {"text": "A.S. : additional supervision.ywith A.S. is better than without A.S. Underlined numbers are model - wise best performance , and bolded numbers are global best performance .", "entities": []}, {"text": "All numbers are percentages .", "entities": []}, {"text": "Modelw / o A.S. w/ A.S. Acc F1 Acc F1", "entities": [[6, 7, "MetricName", "Acc"], [7, 8, "MetricName", "F1"], [8, 9, "MetricName", "Acc"], [9, 10, "MetricName", "F1"]]}, {"text": "Majority 92.76 48.12 \u2013 \u2013 Shallow Features 93.01 54.59 93.05 55.12 BiLSTM 94.59 73.40 94.45 71.81 ESIM w/ BioELMo 94.82 74.01 95.04 75.22 BioBERT 96.50 84.65 96.40 83.76 Table 6 : Results of Phase I ( eq . 1 ) .", "entities": [[11, 12, "MethodName", "BiLSTM"], [16, 17, "MethodName", "ESIM"]]}, {"text": "Experiments are on PQA - A under reasoning - required setting .", "entities": []}, {"text": "A.S. : additional supervision .", "entities": []}, {"text": "ModelEq . 2 Eq .", "entities": []}, {"text": "3 Acc F1 Acc F1 Majority 92.76 48.12 55.20 23.71 Human ( single ) \u2013 \u2013 90.40y84.18y Shallow Features 93.11 56.11 54.44 38.63 BiLSTM 95.97 83.70 71.46 50.93 ESIM w/ BioELMo 97.01 88.47 74.06 58.53 BioBERT 98.28 93.17 80.80 63.50 Table 7 : Bootstrapping results .", "entities": [[1, 2, "MetricName", "Acc"], [2, 3, "MetricName", "F1"], [3, 4, "MetricName", "Acc"], [4, 5, "MetricName", "F1"], [23, 24, "MethodName", "BiLSTM"], [28, 29, "MethodName", "ESIM"]]}, {"text": "Experiments are on PQA - A ( eq . 2 ) and PQA - L ( eq . 3 ) under reasoningfree setting.yReasoning - free human performance .", "entities": []}, {"text": "5.3 Intermediate Results", "entities": []}, {"text": "In this section we show the intermediate results of multi - phase \ufb01ne - tuning schedule .", "entities": []}, {"text": "Phase I : Results are shown in Table 6 .", "entities": []}, {"text": "Phase I is \ufb01ne - tuning on PQA - A using question and context .", "entities": []}, {"text": "Since PQA - A is imbalanced due to its collection process , a trivial majority baseline gets 92.76 % accuracy .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "Other models have better accuracy and especially macro - F1 than majority baseline .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 10, "MetricName", "macro - F1"]]}, {"text": "Finetuned BioBERT performs best .", "entities": []}, {"text": "Modelw / o A.S. w/ A.S. Acc F1 Acc F1", "entities": [[6, 7, "MetricName", "Acc"], [7, 8, "MetricName", "F1"], [8, 9, "MetricName", "Acc"], [9, 10, "MetricName", "F1"]]}, {"text": "Majority 55.10 23.68 \u2013 \u2013 Shallow Features 76.66 66.12 77.71 67.97 Majority 56.53 24.07 \u2013 \u2013 BiLSTM 85.33 81.32 85.68 81.87 Majority 55.10 23.68 \u2013 \u2013 ESIM w/ BioELMo 78.47 63.32 79.62 64.91 Majority 54.82 24.87 \u2013 \u2013 BioBERT 80.93 68.84 81.02 70.04 Table 8 : Phase II results ( eq . 5 ) .", "entities": [[16, 17, "MethodName", "BiLSTM"], [26, 27, "MethodName", "ESIM"]]}, {"text": "Experiments are on pseudo - labeled PQA - U under reasoning - required setting .", "entities": []}, {"text": "A.S. : additional supervision .", "entities": []}, {"text": "Bootstrapping : Results are shown in Table 7 .", "entities": []}, {"text": "Bootstrapping is a three - step process : \ufb01ne - tuning on PQA - A , then on PQA - L and pseudo - labeling PQA - U. All three steps are using question and long answer as input .", "entities": []}, {"text": "Expectedly , models perform better in this reasoning - free setting than they do in reasoning - required setting ( for PQA - A , Eq . 2 results in Table 7 are better than the performance in Table 6 ; for PQA - L , Eq . 3 results in Table 7 are better than the performance in Table 5 ) .", "entities": []}, {"text": "Phase II : Results are shown in Table 8 .", "entities": []}, {"text": "In Phase II , since each model is \ufb01ne - tuned on its own pseudo - labeled PQA - U instances , results are not comparable between models .", "entities": []}, {"text": "While the ablation study in Table 5 clearly shows that Phase II is helpful , performance in Phase II does n\u2019t necessarily correlate with \ufb01nal performance on PQA - L.", "entities": []}, {"text": "25756 Conclusion We present PubMedQA , a novel dataset aimed at biomedical research question answering using yes / no / maybe , where complex quantitative reasoning is required to solve the task .", "entities": [[4, 5, "DatasetName", "PubMedQA"], [13, 15, "TaskName", "question answering"]]}, {"text": "PubMedQA has substantial automatically collected instances as well as the largest size of expert annotated", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}, {"text": "yes / no / maybe questions in biomedical domain .", "entities": []}, {"text": "We provide a strong baseline using multi - phase \ufb01ne - tuning of BioBERT with long answer as additional supervision , but it \u2019s still much worse than just single human performance .", "entities": []}, {"text": "There are several interesting future directions to explore on PubMedQA , e.g. : ( 1 ) about 21 % of PubMedQA contexts contain no natural language descriptions of numbers , so how to properly handle these numbers is worth studying ; ( 2 ) we use binary BoW statistics prediction as a simple demonstration for additional supervision of long answers .", "entities": [[9, 10, "DatasetName", "PubMedQA"], [20, 21, "DatasetName", "PubMedQA"]]}, {"text": "Learning a harder but more informative auxiliary task of long answer generation might lead to further improvements .", "entities": [[10, 12, "TaskName", "answer generation"]]}, {"text": "Articles of PubMedQA are biased towards clinical study - related topics ( described in Appendix B ) , so PubMedQA has the potential to assist evidence - based medicine , which seeks to make clinical decisions based on evidence of high quality clinical studies .", "entities": [[2, 3, "DatasetName", "PubMedQA"], [19, 20, "DatasetName", "PubMedQA"]]}, {"text": "Generally , PubMedQA can serve as a benchmark for testing scienti\ufb01c reasoning abilities of machine reading comprehension models .", "entities": [[2, 3, "DatasetName", "PubMedQA"], [14, 17, "TaskName", "machine reading comprehension"]]}, {"text": "7 Acknowledgement We are grateful for the anonymous reviewers of EMNLP who gave us very valuable comments and suggestions .", "entities": []}, {"text": "References Qian Chen , Xiaodan Zhu , Zhenhua Ling , Si Wei , Hui Jiang , and Diana Inkpen .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Enhanced lstm for natural language inference .", "entities": [[1, 2, "MethodName", "lstm"], [3, 6, "TaskName", "natural language inference"]]}, {"text": "arXiv preprint arXiv:1609.06038 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Christopher Clark , Kenton Lee , Ming - Wei Chang , Tom Kwiatkowski , Michael Collins , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Boolq : Exploring the surprising dif\ufb01culty of natural yes / no questions .", "entities": [[0, 1, "DatasetName", "Boolq"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) .Jacob", "entities": []}, {"text": "Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "William Hersh , Aaron Cohen , Lynn Ruslen , and Phoebe Roberts .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Trec 2007 genomics track overview .", "entities": [[0, 1, "DatasetName", "Trec"]]}, {"text": "In TREC 2007 .", "entities": [[1, 2, "DatasetName", "TREC"]]}, {"text": "William Hersh , Aaron M. Cohen , Phoebe Roberts , and Hari Krishna Rekapalli .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Trec 2006 genomics track overview .", "entities": [[0, 1, "DatasetName", "Trec"]]}, {"text": "In TREC 2006 .", "entities": [[1, 2, "DatasetName", "TREC"]]}, {"text": "Qiao Jin , Bhuwan Dhingra , William W Cohen , and Xinghua Lu . 2019 .", "entities": []}, {"text": "Probing biomedical embeddings from language models .", "entities": []}, {"text": "arXiv preprint arXiv:1904.02181 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Seongsoon Kim , Donghyeon Park , Yonghwa Choi , Kyubum Lee , Byounggun Kim , Minji Jeon , Jihye Kim , Aik Choon Tan , and Jaewoo Kang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A pilot study of biomedical text comprehension using an attention - based deep neural reader : Design and experimental analysis .", "entities": []}, {"text": "JMIR medical informatics , 6(1):e2 . Tom\u00b4a\u02c7s", "entities": []}, {"text": "Ko \u02c7cisk`y , Jonathan Schwarz , Phil Blunsom , Chris Dyer , Karl Moritz Hermann , G \u00b4 aabor Melis , and Edward Grefenstette .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The narrativeqa reading comprehension challenge .", "entities": [[1, 2, "DatasetName", "narrativeqa"], [2, 4, "TaskName", "reading comprehension"]]}, {"text": "Transactions of the Association of Computational Linguistics , 6:317\u2013328 .", "entities": []}, {"text": "Tom Kwiatkowski , Jennimaria Palomaki , Olivia Rhinehart , Michael Collins , Ankur Parikh , Chris Alberti , Danielle Epstein , Illia Polosukhin , Matthew Kelcey , Jacob Devlin , et al . 2019 .", "entities": []}, {"text": "Natural questions : a benchmark for question answering research .", "entities": [[0, 2, "DatasetName", "Natural questions"], [6, 8, "TaskName", "question answering"]]}, {"text": "Guokun Lai , Qizhe Xie , Hanxiao Liu , Yiming Yang , and Eduard Hovy . 2017 .", "entities": []}, {"text": "Race : Large - scale reading comprehension dataset from examinations .", "entities": [[5, 7, "TaskName", "reading comprehension"]]}, {"text": "arXiv preprint arXiv:1704.04683 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jinhyuk Lee , Wonjin Yoon , Sungdong Kim , Donghyeon Kim , Sunkyu Kim , Chan Ho", "entities": []}, {"text": "So , and Jaewoo Kang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Biobert : pre - trained biomedical language representation model for biomedical text mining .", "entities": []}, {"text": "arXiv preprint arXiv:1901.08746 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shuming Ma , Xu Sun , Yizhong Wang , and Junyang Lin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Bag - of - words as target for neural machine translation .", "entities": [[9, 11, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1805.04871 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Christopher D. Manning , Mihai Surdeanu , John Bauer , Jenny Finkel , Steven J. Bethard , and David McClosky .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The Stanford CoreNLP natural language processing toolkit .", "entities": []}, {"text": "In Association for Computational Linguistics ( ACL ) System Demonstrations , pages 55\u201360 .", "entities": []}, {"text": "Roser Morante , Martin Krallinger , Alfonso Valencia , and Walter Daelemans .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Machine reading of biomedical texts about alzheimers disease .", "entities": []}, {"text": "In CLEF", "entities": []}, {"text": "25762012 Conference and Labs of the Evaluation ForumQuestion Answering For Machine Reading Evaluation ( QA4MRE ) , Rome / Forner , J.[edit . ] ; ea , pages 1\u201314 .", "entities": []}, {"text": "Anusri Pampari , Preethi Raghavan , Jennifer Liang , and Jian Peng .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "emrqa :", "entities": [[0, 1, "DatasetName", "emrqa"]]}, {"text": "A large corpus for question answering on electronic medical records .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1809.00732 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Dimitris Pappas , Ion Androutsopoulos , and Haris Papageorgiou .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Bioread :", "entities": []}, {"text": "A new dataset for biomedical reading comprehension .", "entities": [[5, 7, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of the Eleventh International Conference on Language Resources and Evaluation ( LREC-2018 ) .", "entities": []}, {"text": "Anselmo Pe \u02dcnas , Eduard Hovy , Pamela Forner , \u00b4 Alvaro Rodrigo , Richard Sutcliffe , and Roser Morante .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Qa4mre 2011 - 2013 : Overview of question answering for machine reading evaluation .", "entities": [[7, 9, "TaskName", "question answering"]]}, {"text": "In International Conference of the Cross - Language Evaluation Forum for European Languages , pages 303 \u2013 320 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Matthew E Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "arXiv preprint arXiv:1802.05365 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sampo Pyysalo , Filip Ginter , Hans Moen , Tapio Salakoski , and Sophia Ananiadou .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributional semantics resources for biomedical text processing .", "entities": []}, {"text": "Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Squad : 100,000 + questions for machine comprehension of text .", "entities": []}, {"text": "arXiv preprint arXiv:1606.05250 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Marzieh Saeidi , Max Bartolo , Patrick Lewis , Sameer Singh , Tim Rockt \u00a8aschel , Mike Sheldon , Guillaume Bouchard , and Sebastian Riedel .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Interpretation of natural language rules in conversational machine reading .", "entities": []}, {"text": "arXiv preprint arXiv:1809.01494 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hiroaki Sakamoto , Yasunori Watanabe , and Masataka Satou . 2011 .", "entities": []}, {"text": "Do preoperative statins reduce atrial \ufb01brillation after coronary artery bypass grafting ?", "entities": []}, {"text": "Annals of thoracic and cardiovascular surgery , 17(4):376\u2013382 .", "entities": []}, {"text": "George Tsatsaronis , Georgios Balikas , Prodromos Malakasiotis , Ioannis Partalas , Matthias Zschunke , Michael R Alvers , Dirk Weissenborn , Anastasia Krithara , Sergios Petridis , Dimitris Polychronopoulos , et al . 2015 .", "entities": []}, {"text": "An overview of the bioasq large - scale biomedical semantic indexing and question answering competition .", "entities": [[4, 5, "DatasetName", "bioasq"], [12, 14, "TaskName", "question answering"]]}, {"text": "BMC bioinformatics , 16(1):138 .", "entities": []}, {"text": "Zhilin Yang , Peng Qi , Saizheng Zhang , Yoshua Bengio , William W Cohen , Ruslan Salakhutdinov , and Christopher D Manning .", "entities": [[16, 17, "DatasetName", "Ruslan"]]}, {"text": "2018 .", "entities": []}, {"text": "Hotpotqa : A dataset for diverse , explainable multi - hop question answering .", "entities": [[0, 1, "DatasetName", "Hotpotqa"], [8, 13, "TaskName", "multi - hop question answering"]]}, {"text": "arXiv preprint arXiv:1809.09600 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "2577A Yes / no / maybe Answerability Not all naturally occuring question titles from PubMed are answerable by yes / no / maybe .", "entities": []}, {"text": "The \ufb01rst step of annotating PQA - L ( as shown in algorithm 1 ) from pre - PQA - U is to manually identify questions that can be answered using yes / no / maybe .", "entities": []}, {"text": "We labeled 1091 ( about 50.2 % ) of 2173 question titles as unanswerable .", "entities": []}, {"text": "For example , those questions can not be answered by yes / no / maybe : \u000f\u201cCritical Overview of HER2 Assessement in Bladder Cancer : What Is Missing for a Better Therapeutic Approach ? \u201d", "entities": []}, {"text": "( wh- question )", "entities": []}, {"text": "\u000f\u201cOtolaryngology externships and the match : Productive or futile ? \u201d ( multiple choices ) B Over - represented Topics Clinical study - related topics are over - represented in PubMedQA : we found proportions of MeSH terms like : \u000f\u201cPregnancy Outcome \u201d \u000f\u201cSocioeconomic Factors \u201d \u000f\u201cRisk Assessment \u201d \u000f\u201cSurvival Analysis \u201d \u000f\u201cProspective Studies \u201d \u000f\u201cCase - Control Studies \u201d \u000f\u201cReference Values \u201d are signi\ufb01cantly higher in the PubMedQA articles than those in 200k most recent general PubMed articles ( signi\ufb01cance is de\ufb01ned by p<0:05 in twoproportion z - test ) .", "entities": [[30, 31, "DatasetName", "PubMedQA"], [68, 69, "DatasetName", "PubMedQA"]]}, {"text": "C Annotation Criteria Strictly speaking , most yes / no / maybe research questions can be answered by \u201c maybe \u201d since there will always be some conditions where one statement is true and vice versa .", "entities": []}, {"text": "However , the task will be trivial in this case .", "entities": []}, {"text": "Instead , we annotate a question using \u201c yes \u201d if the experiments and results in the paper indicate it , so the answer is not universal but context - dependent .", "entities": []}, {"text": "Given a question like \u201c Do patients bene\ufb01t from drug X ? \u201d : certainly not all patients will bene\ufb01t from it , but if there is a signi\ufb01cant difference inan outcome between the experimental and control group , the answer will be \u201c yes \u201d .", "entities": []}, {"text": "If there is not , the answer will be \u201c no \u201d .", "entities": []}, {"text": "\u201c Maybe \u201d is annotated when ( 1 ) the paper discusses conditions where the answer is True and conditions where the answer is False or ( 2 ) more than one intervention / observation / etc . is asked , and the answer is True for some but False for the others ( e.g. : \u201c Do Disease A , Disease B and/or Disease C bene\ufb01t from drug X ? \u201d ) .", "entities": []}, {"text": "To model uncertainty of the answer , we do n\u2019t strictly follow the logic calculations where such questions can always be answered by either \u201c yes \u201d or \u201c no \u201d .", "entities": []}]