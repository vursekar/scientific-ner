[{"text": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Student Research Workshop , pages 92\u201396 Minneapolis , Minnesota , June 3 - 5 , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics92Deep Learning and Sociophonetics :", "entities": []}, {"text": "Automatic Coding of Rhoticity Using Neural Networks Sarah Gupta Dartmouth College sarah.gupta.19@dartmouth.eduAnthony DiPadova Dartmouth College anthony.f.dipadova.iii.19@dartmouth.edu", "entities": []}, {"text": "Abstract Automated extraction methods are widely available for vowels ( Rosenfelder et al . , 2014 ) , but automated methods for coding rhoticity have lagged far behind .", "entities": []}, {"text": "R - fulness versus rlessness ( in words like park , store , etc . ) is a classic and frequently cited variable ( Labov , 1966 ) , but it is still commonly coded by human analysts rather than automated methods .", "entities": []}, {"text": "Human - coding requires extensive resources and lacks replicability , making it dif\ufb01cult to compare large datasets across research groups ( Yaeger - Dror et al . , 2008 ; Heselwood et al . , 2008 ) .", "entities": []}, {"text": "Can reliable automated methods be developed to aid in coding rhoticity ?", "entities": []}, {"text": "In this study , we use Neural Networks / Deep Learning , training our model on 208 Boston - area speakers .", "entities": []}, {"text": "1 Introduction Despite advances in automation for phonetic alignment and extraction of vowel formants , there is still no reliable automated method for classifying r - dropping , that is , whether a given word is pronounced with an /r/ in words like park ( pahk ) , start ( staht ) , and so on .", "entities": []}, {"text": "R - dropping , also known as non - rhotic speech , is an important sociolinguistic variable in modern dialect research .", "entities": []}, {"text": "But unfortunately most researchers continue to depend on human judgments ( Nagy and Irwin , 2010 ; Becker , 2009 ; Nagy and Roberts , 2004 ) , which is an inconsistent and time - consuming method that lacks replicability .", "entities": []}, {"text": "Turning to the \ufb01eld of machine learning , our deep learning approach investigates a new way to distinguish rhotic versus non - rhotic pronunciations in recorded data .", "entities": []}, {"text": "This is the \ufb01rst study to use neural networks to classify rhotic versus non - rhotic speech .", "entities": []}, {"text": "Although human - coding requires extensive resources and lacks consistency and replicability ( Yaeger - Dror et al . , 2008 ; Heselwood et al . , 2008),making it dif\ufb01cult to compare large datasets across different research groups , it is the only method we have right now .", "entities": []}, {"text": "How soon will computers be able to quickly and reliably code rhoticity up to this standard ?", "entities": []}, {"text": "In terms of other machine learning approaches , McLarty , Jones , and Hall work on this challenge using Support Vector Machines ( SVMs ) ( Mclarty et al . , 2018 ) .", "entities": []}, {"text": "The present study uses Neural Networks / Deep Learning , one of the most effective and fastest - growing approaches in machine - learning .", "entities": []}, {"text": "To our knowledge , this is the \ufb01rst attempt to use neural networks for automatic coding of any sociophonetic variable .", "entities": []}, {"text": "This new method was developed using audio recordings from over 200 New England speakers from Boston , Maine , and central New Hampshire ( Stanford , forthcoming ) , and is here compared to other work on rhoticity ( Heselwood et al . , 2008 ; Mclarty et al . , 2018 ) .", "entities": []}, {"text": "In what ways can neural networks be effective tools in assisting the coding of rhoticity ?", "entities": []}, {"text": "To what level can they perform compared to traditional coding methods and other approaches ?", "entities": []}, {"text": "2 Background The phoneme /r/ has been particularly dif\ufb01cult to pin down because it may be articulated in different ways , yet still produce the same acoustic signal .", "entities": []}, {"text": "As most phoneticians have come to agree , F3 is one of the primary acoustic correlates of rhoticity ( Espy - Wilson et al . , 2000 ; Hagiwara , 1995 ; Thomas , 2011 ) .", "entities": []}, {"text": "The general consensus is that the F3 measurement for /r/ is lower than that of other non - rhotic vowels , but reliable standards for coding rhoticity are lacking .", "entities": []}, {"text": "In this paper , rhoticity will refer to post - vocalic realizations of the phoneme /r/ which do not occur before other vowels .", "entities": []}, {"text": "For example , rhotic tokens of interest would include park andfather but not", "entities": []}, {"text": "93marry .", "entities": []}, {"text": "British phonetician John Wells used the term \u201c rhotic \u201d , which has been subsequently considered in the \ufb01eld as one of the most de\ufb01ning traits of varieties of English ( Wells , 1982 ) .", "entities": []}, {"text": "Rhotic and non - rhotic dialects have been widely studied as they relate to sociolinguistic features of location , age , gender , and socioeconomic status .", "entities": []}, {"text": "However , we are still reliant on human analysts to make judgements of rhotic vs. non - rhotic speech , which can require a lot of time and money .", "entities": []}, {"text": "Despite advances in many areas of computational linguistics , there is still not an accurate way to determine rhoticity based on acoustic components alone ; a human must judge for themselves whether or not an /r/ has been dropped .", "entities": []}, {"text": "As expected , this is not highly replicable as different speakers may perceive things differently especially when it comes to dialects that are not so clear - cut ( Yaeger - Dror et al . , 2008 ) .", "entities": []}, {"text": "For this reason , an automated way to determine rhotic / non - rhotic tokens would be especially helpful in these contexts .", "entities": []}, {"text": "3 Other work 3.1 Heselwood , Plug , and Tickle Heselwood et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2008 ) extracted formant data from the spectrograms on the Bark scale \u2013 usually , formant data F2 / F3 is reported on the Hertz scale .", "entities": []}, {"text": "The Bark scale more closely correlates to human perception of sounds , that is , on a logarithmic scale rather than absolute .", "entities": []}, {"text": "After conversion , F2 was labeled Z2 and F3 was labeled Z3 , and a series of perceptual experiments were performed to ascertain rhoticity thresholds .", "entities": []}, {"text": "Note that it was conducted for the purposes of perceptual research rather than coding applications .", "entities": []}, {"text": "3.2 McLarty , Jones , and Hall Mclarty et al .", "entities": []}, {"text": "( 2018 ) trained a Support Vector Machine ( SVM ) on pre - vocalic /r/ and vowels , and their approach did quite well in classifying prevocalic /r / s.", "entities": [[5, 8, "MethodName", "Support Vector Machine"], [9, 10, "MethodName", "SVM"]]}, {"text": "They then took this pre - trained model and applied it to classifying postvocalic /r/ tokens , which classi\ufb01ed 84 % as vowels , and 15 % as /r/.", "entities": []}, {"text": "As they describe , this is likely because all postvocalic segments still contain vowel - like properties ; furthermore , their training set excluded postvocalic /r/", "entities": []}, {"text": "so the accuracy is expected to decrease .", "entities": [[2, 3, "MetricName", "accuracy"]]}, {"text": "However , their method did not perform as well in comparison to humans .", "entities": []}, {"text": "On tokens where there was no ground truth , humans only agreed with the SVM classi\ufb01cation about 55 % of the time.4 Methods In this initial study , we used Boston - area \ufb01eld recordings of 208 speakers , 100 tokens per speaker ( 107 women/101 men , born 1915 - 1997 ) .", "entities": [[14, 15, "MethodName", "SVM"]]}, {"text": "These on - the - street interviews ( 15 - 20 minutes each ) are typical sociolinguistic recordings in terms of speech styles ( word - list , sentences , reading passage , free speech ) and occasional background noise .", "entities": []}, {"text": "We chose to omit free speech because its token variability between speakers would present another challenging factor , leaving us with recordings where participants were reading ( word - list , sentences , passage ) .", "entities": []}, {"text": "Given word transcriptions , we used the Montreal Forced Aligner ( McAuliffe et al . , 2017 ) and modi\ufb01ed Praat scripts ( DiCanio , 2014 ; Koops , 2013 ) to align and extract vowel+(r ) sequences , e.g. , park , short .", "entities": []}, {"text": "However , note that because non - rhotic dialects are less common , and some of our recordings had background noise , it could be possible that alignments were not perfect for all of our tokens .", "entities": []}, {"text": "Two human analysts listened to recordings and judged each vowel+(r ) token as r - ful or r - less .", "entities": []}, {"text": "The human analysts agreed on 89.9 % of the tokens , similar to human agreement elsewhere ( Nagy and Irwin , 2010 ) .", "entities": []}, {"text": "Like other studies , we omitted tokens when the human analysts disagreed ( 10 % ) .", "entities": []}, {"text": "So overall , 1700 tokens were discarded because of speaker disagreement , and 6500 rhotic tokens and 5300 non - rhotic tokens remained for analysis .", "entities": []}, {"text": "4.1 Preliminary Investigations In early testing , we attempted classi\ufb01cation into rful , r - less , and unknown , but this did not provide strong results so we simpli\ufb01ed to a binary classi\ufb01cation .", "entities": []}, {"text": "From the beginning of this project , we knew we wanted to use a machine learning approach , so before using neural networks we tried some easier classi\ufb01ers .", "entities": []}, {"text": "However , we did not get encouraging results .", "entities": []}, {"text": "For example , our Random Forest Classi\ufb01er only gave about 54 % accuracy .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "When we tried simpler neural networks , these gave much more promising results to we chose to pursue this method .", "entities": []}, {"text": "4.2 Data Extraction and Model Speci\ufb01cations Following standard methods of Automatic Speech Recognition , we converted the audio to 12 MelFrequency - Cepstral - Coef\ufb01cients ( MFCCs ) .", "entities": [[10, 13, "TaskName", "Automatic Speech Recognition"]]}, {"text": "We used the 12 MFCCs , similar to Mclarty et", "entities": []}, {"text": "al ..", "entities": []}, {"text": "For each vowel+(r ) sequence , we normalized across", "entities": []}, {"text": "94the length to extract 100 time - points per token , as shown in \ufb01gure 1 .", "entities": []}, {"text": "In the training , MFCCs were Figure 1 : Model architecture .", "entities": []}, {"text": "more effective than traditional sociophonetic /r/ correlates F2 and F3 ( Thomas , 2011 ) .", "entities": []}, {"text": "These samples were used in the model architecture as shown in \ufb01gure 1 , where there are 100 samples for each vowel + /r/ sequence .", "entities": []}, {"text": "The Gated Recurrent Unit is shown in more detail in \ufb01gure 2 , where we can see the input from the previous timestep and layer , and how this is \ufb01ltered through gates using tanh andsigmoid activation functions .", "entities": [[1, 4, "MethodName", "Gated Recurrent Unit"]]}, {"text": "Figure 2 : Gated Recurrent Unit ( GRU ) architecture .", "entities": [[3, 6, "MethodName", "Gated Recurrent Unit"], [7, 8, "MethodName", "GRU"]]}, {"text": "Importantly , no work on coding rhoticity has made use of Recurrent Neural Networks , and we believe our methods are a promising step .", "entities": []}, {"text": "We used Gated Recurrent Units ( Cho et al . , 2014 ; Chung et al . , 2014 ) to train our system to classify vowel+(r ) tokens as r - ful or r - less .", "entities": []}, {"text": "Following standard methods in machine - learning , we split the data in order to train with 80 % of the data and test with 20 % .", "entities": []}, {"text": "We chose hyperparameters based on a grid search using 3 - fold cross validation ( only 3 dueto the small dataset ) .", "entities": []}, {"text": "We saved the test set to validate results .", "entities": []}, {"text": "The hidden layer size was 50 nodes , and dense layer size was 200 nodes .", "entities": [[1, 4, "HyperparameterName", "hidden layer size"]]}, {"text": "For regularization we used a kernel L2 regularization for the dense layer and we used both activation L2 and Recurrent L2 for the GRU layer .", "entities": [[6, 8, "HyperparameterName", "L2 regularization"], [23, 24, "MethodName", "GRU"]]}, {"text": "All of the alphas for this regularization are 0.01 .", "entities": []}, {"text": "The optimization method was RMSprop , and the learning rate was 0.001 .", "entities": [[4, 5, "MethodName", "RMSprop"], [8, 10, "HyperparameterName", "learning rate"]]}, {"text": "5 Results In \ufb01gure 3 , we see the Normalized Confusion Matrix , which summarizes our results by lining up true labels and predicted labels for our rhotic and non - rhotic tokens .", "entities": []}, {"text": "We consider this binary classi\ufb01cation either rhotic ( positive ) or non - rhotic ( negative ) .", "entities": []}, {"text": "In this way we can see the proportion of true positives ( predicted to be rhotic and indeed truly rhotic ) , false positive ( predicted to be rhotic but actually non - rhotic ) , true negative ( predicted to be non - rhotic and actually non - rhotic ) , and false negative ( predicted to be non - rhotic and actually rhotic ) .", "entities": []}, {"text": "In deciding which model to use , we tried a Figure 3 : Normalized Confusion Matrix .", "entities": []}, {"text": "few different con\ufb01gurations .", "entities": []}, {"text": "We used the sampled MFCCs ( as described earlier , \ufb01gure 1 ) as well as Bark measurements that were extracted also at 100 time - points across the vowel .", "entities": []}, {"text": "Because our MFCC data is multi - dimensional and time - dependent , we wanted to see how a Convolutional Neural Network would perform ( table 1 ) , but it turned out not to be as high in performance as our earlier model .", "entities": []}, {"text": "Figure 4 shows the Receiver Operating Characteristic ( ROC ) for our model ( created using scikitlearn ) , which is fairly good by machine learning standards .", "entities": []}, {"text": "The Area Under the Curve ( AUC , as noted in Table 1 ) is 0.892 , and as evident from the", "entities": [[6, 7, "MetricName", "AUC"]]}, {"text": "95graph , is much closer to 1 .", "entities": []}, {"text": "Our system had 81.1 % Figure 4 : Receiver Operating Characteristic .", "entities": []}, {"text": "accuracy with the human analysts in judging tokens as r - less or r - ful , scoring 0.829 for F - measure .", "entities": [[0, 1, "MetricName", "accuracy"], [20, 23, "MetricName", "F - measure"]]}, {"text": "Accuracy Precision Recall F1 AUC GRU - MFCC 0.811 0.829 0.830 0.829 0.892 GRU - Bark 0.806 0.844 0.856 0.850 0.869 CNN - MFCC 0.746 0.796 0.815 0.805 0.808 Table 1 : Metrics showing the performance of different models \u2013 our top performing model was using GRUs with MFCCs as input ( as described previously ) .", "entities": [[0, 1, "MetricName", "Accuracy"], [1, 2, "MetricName", "Precision"], [2, 3, "MetricName", "Recall"], [3, 4, "MetricName", "F1"], [4, 5, "MetricName", "AUC"], [5, 6, "MethodName", "GRU"], [13, 14, "MethodName", "GRU"]]}, {"text": "We also used the Heselwood et al . approach ( section 3.1 ) of classifying front or back vowels to see how accurately it would perform on the same test dataset .", "entities": []}, {"text": "This classi\ufb01cation gave an average speaker accuracy of 63.3 % and an average token accuracy of 62.1 % ( Table 2 ) , much lower than our best model \u2019s overall accuracy ( i.e. average across all tokens ) of 81.1 % ( Table 1 ) .", "entities": [[6, 7, "MetricName", "accuracy"], [14, 15, "MetricName", "accuracy"], [30, 32, "MetricName", "overall accuracy"]]}, {"text": "Average Speaker Accuracy 63.3 % Average Token Accuracy 62.1 % Table 2 : Heselwood et al . approach on test dataset ( using Bark thresholds Z2 and Z3 ) 6 Discussion", "entities": [[2, 3, "MetricName", "Accuracy"], [7, 8, "MetricName", "Accuracy"]]}, {"text": "The initial results of this study are promising .", "entities": []}, {"text": "Our results are quite strong , as shown by the metrics in Table 1 .", "entities": []}, {"text": "When testing the Heselwood et al . approach ( Table 2 ) , it only predicted correctly approximately 60 % of the time ; our model performs signi\ufb01cantly better , at an accuracy of 81.1 % ( Table 1 ) .", "entities": [[32, 33, "MetricName", "accuracy"]]}, {"text": "It seems that we are also slightly better at predicting rhotic tokens than non - rhotic ( Figure 3),which likely has to do with the fact that we have more rhotic tokens in total .", "entities": []}, {"text": "We aimed to reach human levels \u2013 considering that analyst agreement is 89.9 % for our dataset ( as mentioned above ) , our accuracy of 81.1 % is quite good .", "entities": [[24, 25, "MetricName", "accuracy"]]}, {"text": "However , these numbers are not strictly comparable as we discarded tokens that proved dif\ufb01cult for human analysts .", "entities": []}, {"text": "In future development of this method , we want to consider any sources of error on our part .", "entities": []}, {"text": "For example , some audio and text \ufb01les could be misaligned so we might consider hand - correcting these alignments .", "entities": []}, {"text": "However , the nature of the neural network could correct for this in that it learns to forget irrelevant or noisy data .", "entities": []}, {"text": "By gathering more data , we would expect that our accuracy would improve and eventually reach a plateau where additional speakers would not affect anything .", "entities": [[10, 11, "MetricName", "accuracy"]]}, {"text": "Additionally , a study that involves cross - corpus analysis could provide greater insight into how this model might be applicable on a larger scale , and how well our model actually performs .", "entities": []}, {"text": "Furthermore , if we had 3 analysts rather than 2 , we could have used a majority vote for classifying tokens , and would not have to discard tokens where rhoticity was ambiguous .", "entities": []}, {"text": "A shortcoming of this study is that it only involves speech that is elicited through reading \u2013 ideally future studies would involve free speech in order to use more natural speech .", "entities": []}, {"text": "R - dropping is a crucial sociolinguistic variable for English dialect research in the US Northeast , Great Britain , Australia , New Zealand , Singapore , and other locations .", "entities": []}, {"text": "Our neural network model takes a signi\ufb01cant step toward automation of this key variable .", "entities": []}, {"text": "In the future , we will continue optimizing and improving our model .", "entities": []}, {"text": "Other groups have studied automated methods for coding sociolinguistic variables ( Yuan and Liberman , 2011 ; Bailey , 2016 ) , and there are great ideas to be found in these works .", "entities": []}, {"text": "When automated methods for rhoticity reach the accuracy level of humans , along with consistency and full replicability , this will open the \ufb02oodgates to large amounts of /r/ data and greatly expand sociolinguistic knowledge of dialect variation around the world , ef\ufb01ciently allowing studies to be replicated across research groups .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "96References George Bailey .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Automatic Detection of Sociolinguistic Variation Using Forced Alignment .", "entities": []}, {"text": "University of Pennsylvania Working Papers in Linguistics : , 22(3 ) .", "entities": []}, {"text": "Kara Becker .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "/r/ and the construction of place identity on New York City \u2019s Lower East Side .", "entities": []}, {"text": "Journal of Sociolinguistics , 13(5):634 \u2013 658 .", "entities": []}, {"text": "Kyunghyun Cho , Bart Van Merrienboer , Caglar Gulcehre , Dzmitry Bahdanau , Fethi Bougares , Holger Schwenk , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) .", "entities": []}, {"text": "Junyoung Chung , Caglar Gulcehre , KyungHyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Empirical evaluation of Gated Recurrent Neural Networks on sequence modeling .", "entities": []}, {"text": "Neural Information Processing Systems 2014 , Deep Learning and Representation Learning Workshop .", "entities": [[9, 11, "TaskName", "Representation Learning"]]}, {"text": "Christian DiCanio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Combine intervals.praat .", "entities": []}, {"text": "Carol Y .", "entities": []}, {"text": "Espy - Wilson , Suzanne E. Boyce , Michel Jackson , Shrikanth Narayanan , and Abeer Alwan .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "Acoustic modeling of American English /r/.", "entities": []}, {"text": "The Journal of the Acoustical Society of America , 108(1):343 \u2013 356 .", "entities": []}, {"text": "Robert Hagiwara .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Acoustic Realizations of American /r/ as Produced by Women and Men .", "entities": []}, {"text": "Barry Heselwood , Leendert Plug , and Alison Tickle .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Assessing rhoticity using auditory , acoustic and psychoacoustic methods .", "entities": []}, {"text": "Proceedings of the 13th Methods in Dialectology . , pages 331 \u2013 340 . Chris Koops .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Praat script for extracting vowel formants .", "entities": []}, {"text": "William Labov .", "entities": []}, {"text": "1966 .", "entities": []}, {"text": "The Social Strati\ufb01cation of English in New York City .", "entities": []}, {"text": "CAL , pages 380 \u2013 403 .", "entities": []}, {"text": "Michael McAuliffe , Michaela Socolof , Sarah Mihuc , Michael Wagner , and Morgan Sonderegger . 2017 .", "entities": []}, {"text": "Montreal Forced Aligner : Trainable Text - Speech Alignment Using Kaldi .", "entities": []}, {"text": "Proceedings of the 18th Conference of the International Speech Communication Association .", "entities": []}, {"text": "Jason Mclarty , Taylor Jones , and Christopher Hall .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Corpus - Based Sociophonetic Approaches to Postvocalic R - lessness in African American Language .", "entities": []}, {"text": "American Speech , pages 1 \u2013 18 .", "entities": []}, {"text": "Naomi Nagy and Patricia Irwin .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Boston ( r ): Neighbo(r)s nea(r ) and fa(r ) .", "entities": []}, {"text": "Language Variation and Change , 22(02):241 \u2013 278 .", "entities": []}, {"text": "Naomi Nagy and Julie Roberts .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "New England : Phonology , pages 270 \u2013 281 .", "entities": []}, {"text": "De Gruyter Mouton .", "entities": []}, {"text": "Ingrid Rosenfelder , Josef Fruehwald , Keelan Evanini , Scott Seyfarth , Kyle Gorman , Hilary Prichard , and Jiahong Yuan .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "FA VE ( Forced Alignment and V owel Extraction ) .", "entities": [[0, 1, "MethodName", "FA"]]}, {"text": "James N Stanford .", "entities": []}, {"text": "forthcoming .", "entities": []}, {"text": "New England English : Large - scale acoustic sociophonetics and dialectology .", "entities": []}, {"text": "Oxford University Press .", "entities": []}, {"text": "Erik Thomas .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Sociophonetics : An introduction .", "entities": []}, {"text": "Palgrave Macmillan .", "entities": []}, {"text": "J. C. Wells .", "entities": []}, {"text": "1982 .", "entities": []}, {"text": "Accents of English .", "entities": []}, {"text": "Cambridge University Press .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Malcah Yaeger - Dror , Tyler Kendall , Paul Foulkes , Dominic Watt , Jillian Eddie , Philip Harrison , and Colleen Kavenagh .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "NWAV 37 .", "entities": []}, {"text": "Jiahong Yuan and Mark Liberman .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Automatic detection of g - dropping in American English using forced alignment .", "entities": []}]