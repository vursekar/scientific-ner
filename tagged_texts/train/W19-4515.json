[{"text": "Proceedings of the 6th Workshop on Argument Mining , pages 124\u2013135 Florence , Italy , August 1 , 2019 .", "entities": [[6, 8, "TaskName", "Argument Mining"], [11, 12, "MethodName", "Florence"]]}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics124Evaluation of Scienti\ufb01c Elements for Text Similarity in Biomedical Publications Mariana Neves , Daniel Butzke , Barbara Grune German Centre for the Protection of Laboratory Animals ( Bf3R ) , German Federal Institute for Risk Assessment ( BfR ) Diedersdorfer Weg 1 , 12277 , Berlin , Germany mariana.lara-neves@bfr.bund.de Abstract Rhetorical elements from scienti\ufb01c publications provide a more structured view of the document and allow algorithms to focus on particular parts of the text .", "entities": [[9, 11, "TaskName", "Text Similarity"]]}, {"text": "We surveyed the literature for previously proposed schemes for rhetorical elements and present an overview of its current state of the art .", "entities": []}, {"text": "We also searched for available tools using these schemes and applied four tools for our particular task of ranking biomedical abstracts based on text similarity .", "entities": [[23, 25, "TaskName", "text similarity"]]}, {"text": "Comparison of the tools with two strong baselines shows that the predictions provided by the ArguminSci tool can support our use case of mining alternative methods for animal experiments .", "entities": []}, {"text": "1 Introduction We aim to mine alternative methods to animal experiments from the biomedical literature .", "entities": []}, {"text": "These are methods that address any of the so - called 3R principles of replacement ( no animals at all or use of invertebrates over vertebrates ) , reduce ( use of less animals ) , or re\ufb01nement ( cause less harm to animals ) ( Gruber and Hartung , 2004 ; Doke and Dhawale , 2015 ) .", "entities": []}, {"text": "For such complex natural language processing ( NLP ) applications , it is necessary to rely on appropriate tools to precisely understand the text and better \ufb01nd the potential relevant documents .", "entities": []}, {"text": "The rhetorical elements , such as zones or particular entities , can support NLP algorithms by focusing on the relevant elements of the text ( Mann and Thompson , 1987 ) .", "entities": []}, {"text": "Given a certain document that describes an animal experiment for a certain research goal , hereafter called input document , we would like to \ufb01nd potential publications , hereafter called candidate documents , that describe an alternative method for the same research goal .", "entities": []}, {"text": "Thus , some of the scienti\ufb01c elements should be similar between input andcandidate documents , e.g. research goals and outcomes , while some others should be different , e.g. methods .", "entities": []}, {"text": "Finding an alternative method to animal experiment requires two tasks : ( a ) performing a text similarity task with respect to some aspects of the publication , and ( b ) precisely understanding the proposed method with respect to the 3R principles .", "entities": [[16, 18, "TaskName", "text similarity"]]}, {"text": "Therefore , the extraction of rhetorical elements has the potential to boost performance for these tasks .", "entities": []}, {"text": "Previous works have proposed many schemes for rhetorical elements in scienti\ufb01c publication , as reviewed in Webber et al .", "entities": []}, {"text": "( 2012 ) .", "entities": []}, {"text": "In a more recent survey , Nasar et al .", "entities": []}, {"text": "( 2018 ) present a good overview on both metadata and schemes for scienti\ufb01c articles .", "entities": []}, {"text": "On the one hand , many of these schemes are not supported by an annotated corpus for training suitable information extraction tools .", "entities": []}, {"text": "On the other hand , some tools based on these schemes are readily available for use .", "entities": []}, {"text": "We surveyed published schemes for rhetorical elements , whether focused on the biomedical domain or not , and we present a short overview on these .", "entities": []}, {"text": "For those schemes for which we could \ufb01nd available tools , the latter was used to process a collection of 562 biomedical abstracts .", "entities": []}, {"text": "We performed a comparison of the output ( rhetorical elements ) from the tools in the scope of a text similarity task on a manually annotated dataset .", "entities": [[19, 21, "TaskName", "text similarity"]]}, {"text": "In this work , we limited our evaluation for text similarity but did not address whether the proposed methods comply with the 3R principles .", "entities": [[9, 11, "TaskName", "text similarity"]]}, {"text": "In summary , the contributions of this work are the following : ( a ) a short survey on existing schemes and corpora for rhetorical elements in scienti\ufb01c publications ; ( b ) the identi\ufb01cation of the schemes for which available tools are readily available for use ; and ( c ) the evaluation of the available tools on a biomedical use case for text similarity .", "entities": [[64, 66, "TaskName", "text similarity"]]}, {"text": "The next section presents a survey on", "entities": []}, {"text": "125the available schemes , followed by the methodology that we propose to compare the tools in the scope of text similarity .", "entities": [[19, 21, "TaskName", "text similarity"]]}, {"text": "We present the results in Section 4 and our discussion in Section 5 . 2 Schemes for Rhetorical Elements We classi\ufb01ed the schemes according to the annotation level they address , either on the sentence , entity or relation - level .", "entities": []}, {"text": "We present a summary of all schemes that we found , but give a more detailed description for ( selected ) schemes for which an annotated corpus is available ( cf .", "entities": []}, {"text": "Table 1 ) .", "entities": []}, {"text": "2.1 Sentence - level Schemes Many schemes model scienti\ufb01c elements on the level of sentences or phrases , i.e. , for document zoning .", "entities": []}, {"text": "It consists of splitting the publications ( whether abstracts or full texts ) on zones according to its scienti\ufb01c content , e.g. introduction , methods , results .", "entities": []}, {"text": "Shimbo et al .", "entities": []}, {"text": "( 2003 ) proposed \ufb01ve categories and used structured abstracts from Medline while Hirohata et al .", "entities": []}, {"text": "( 2008 ) suggested four zoning categories .", "entities": []}, {"text": "Further , Mullen et al .", "entities": []}, {"text": "( 2005 ) proposed a schema in which labels are grouped in three groups .", "entities": []}, {"text": "Agarwal and Yu ( 2009 ) de\ufb01ned four categories ( IMRAD schema ) and manually annotated 148 articles , which was also used by Varga et al .", "entities": []}, {"text": "( 2012 ) for the annotation of more than 1,000 biomedical articles .", "entities": []}, {"text": "Ruch et al .", "entities": []}, {"text": "( 2007 ) also annotated and tried machine learning in biomedical abstracts .", "entities": []}, {"text": "However , none of the above data seems to be available for use , but we found many schemes with available corpora : AZ ( Teufel and Moens , 2002 ) .", "entities": []}, {"text": "The Argumentative Zoning ( AZ ) schema was \ufb01rst proposed by Teufel and Moens ( 2002 ) and an annotated corpus is freely available for download1 .", "entities": []}, {"text": "The schema is composed of seven rhetorical categories and the corresponding corpus contains 80 articles on computational linguistics .", "entities": []}, {"text": "Teufel et al .", "entities": []}, {"text": "( 2009 ) extended the schema to 11 categories ( the AZII schema ) , applied it to chemistry papers , and later compared it to the CoreSC schema ( Liakata et al . , 2010).2Later ,", "entities": []}, {"text": "Kova \u02c7cevi\u00b4c et", "entities": [[0, 1, "MethodName", "Kova"]]}, {"text": "al .", "entities": []}, {"text": "( 2012 ) annotated 110 articles in computational linguistics with a modi\ufb01ed version of the AZ labels .", "entities": []}, {"text": "Mizuta et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2006 ) also adapted the AZ schema to biomedicine by annotating 20 full - text articles .", "entities": []}, {"text": "1https://www.cl.cam.ac.uk/ \u02dcsht25 / AZ _ corpus.html 2However , the AZ - II corpus was not found .", "entities": []}, {"text": "Guo et al . ( 2010 ) compared three zoning schemes in abstracts , including a reduced version of the AZ schema composed of seven categories , and annotated 1,000 abstracts with these schemes.3 CoreSC ( Liakata et al . , 2010 ) .", "entities": []}, {"text": "This schema consists of three layers of labels and the corresponding ART corpus4is composed of 225 full texts .", "entities": []}, {"text": "The corpus and schema were used in Guo et al .", "entities": []}, {"text": "( 2010 ) ( just the \ufb01rst layer ) and in Liakata", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2012a ) for two life sciences applications , while Liakata et al .", "entities": []}, {"text": "( 2012b ) compared it to a schema for biomedical events and developed the the SAPIENTA software5 .", "entities": []}, {"text": "Dr. Inventor ( Ronzano and Saggion , 2015 ; Fisas et al . , 2015 ) .", "entities": []}, {"text": "The Dr. Inventor Framework proposes \ufb01ve categories and annotated 40 Computer Graphics papers , the so - called Dr. Inventor Rhetorically Annotated Corpus .", "entities": []}, {"text": "Later , they also annotated another layer for citation purposes ( Fisas et al . , 2016 ) .", "entities": []}, {"text": "An extension of this schema with argumentative components and relations was recently published ( Lauscher et al . , 2018b ) , along with a tool for the prediction of the scienti\ufb01c elements ( Lauscher et al . , 2018a ) .", "entities": []}, {"text": "MAZEA ( Dayrell et al . , 2012 ) .", "entities": []}, {"text": "This schema considers six categories and the corpus was annotated for 645 abstracts from Physical Sciences and Engineering and Life and Health Sciences.6A Web application is available for tagging abstracts .", "entities": []}, {"text": "PIBOSO ( Kim et al . , 2011 ) .", "entities": []}, {"text": "It was designed for the clinical domain and proposes six categories of a modi\ufb01ed version of the PICO criteria .", "entities": []}, {"text": "It was used for the ALTA - NICTA shared task7and recent works using this corpus include Hassanzadeh et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2014 ) and Jin and Szolovits ( 2018 ) .", "entities": []}, {"text": "The latter relies on deep learning methods and the implementation is readily available .", "entities": []}, {"text": "PubMed RCT ( Dernoncourt and Lee , 2017 ) .", "entities": [[0, 2, "DatasetName", "PubMed RCT"]]}, {"text": "It is a collection that includes two corpora of 20,000 and 200,000 medical abstracts annotated 3However , the URL informed in a later publication ( Guo et al . , 2013 ) no longer exists .", "entities": []}, {"text": "4https://www.aber.ac.uk/en/cs/ research / cb / projects / art / art - corpus/ 5http://www.sapientaproject.com/ software 6http://www.nilc.icmc.usp.br/ mazea - web / downloads.php 7https://www.kaggle.com/c/ alta - nicta - challenge2", "entities": []}, {"text": "126Tools Categories Corpora TopicSentence / PhraseAZ AIM , TEXTUAL , OWN , BACKGROUND , CONTRAST , BASIC , OTHER80 ( Teufel and Moens , 2002 ) and 20 ( Mizuta et al . , 2006)CL , bio CoreSC [ Level 1 ] Hypothesis , Motivation , Background , Goal , Object , Method , Experiment , Model , Observation , Result , Conclusion225 ( Liakata et al . , 2010 ) chem Dr. Inventor Approach , Challenge , Background , Outcomes , Future Work40 ( Ronzano and Saggion , 2015)CG MAZEA background , gap , purpose , method , result , conclusion645 abstracts ( Dayrell et al . , 2012)phy , eng , LS PIBOSO Population , Intervention , Background , Outcome , Study Design , Other1,000 abstracts ( Kim et al . , 2011)bio PubMedRCT background , objective , method , result , conclusion20,000 and 200,000 abstracts ( Dernoncourt and Lee , 2017)bio Wilbur FOCUS , POLARITY , CERTAINTY , EVIDENCE , DIRECTIONALITY10,000 sentences ( Shatkay et al . , 2008)bioEnt .", "entities": []}, {"text": "ScienceIE Task , Process , Material 500 ( Augenstein et al . , 2017)CSRelationG\u00b4abor USAGE , RESULT , MODEL , PART WHOLE , TOPIC , COMPARISON500 abstracts ( G \u00b4 abor et", "entities": [[0, 1, "DatasetName", "ScienceIE"]]}, {"text": "al . ,", "entities": []}, {"text": "2018)CL", "entities": []}, {"text": "SciDTB", "entities": []}, {"text": "[ Coarse level ] Attribution , Background , Cause - effect , Comparison , Condition , Contrast , Elaboration , Enablement , Evaluation , Explain , Joint , Manner - means , Progression , Same - unit , Summary , Temporal798 abstracts ( Yang and Li , 2018)CLHybridGreen", "entities": []}, {"text": "[ Levels 1 - 3 ] 1 . Causation , 1.1 One Group , 1.1.1 Agreement Arguments , 1.1.2 Eliminate Candidates , 1.1.3 Explanation - Based , 1.2 Two Group , 1.2.1 Difference , 1.2.2 Analogy ( Causal ) , 1.2.3 Explanation - Based , 2 . Other , 2.1 Classi\ufb01cation , 2.2 Con\ufb01rmationone ( Green , 2018 ) bio Table 1 : Summary of the selected schemes and corresponding categories , size of the annotated corpora , and topic of the latter .", "entities": []}, {"text": "Only the categories from the certain levels were shown for some schemes with various layers .", "entities": []}, {"text": "Numbers or the corpora refer to full - text documents , unless otherwise stated .", "entities": []}, {"text": "Regarding the topics , \u201c CL \u201d stands for computational linguistics , \u201c bio \u201d for biomedicine , \u201c chem \u201d for chemistry , \u201c CG \u201d for Computer Graphics , \u201c phy \u201d for Physics , \u201c eng \u201d for Engineering , \u201c LS \u201d for Life Sciences , and \u201c CS \u201d for Computer Science .", "entities": [[52, 53, "DatasetName", "CS"]]}, {"text": "127with \ufb01ve categories .", "entities": []}, {"text": "The corpus is freely available8as well as at least two tools for its detection , namely the one from Jin and Szolovits ( 2018 ) ( cf .", "entities": []}, {"text": "PIBOSO above ) and one based on AllenNLP ( Achakulvisut et al . , 2018 ) .", "entities": []}, {"text": "Wilbur ( Wilbur et al . , 2006 ) .", "entities": []}, {"text": "It consists of a schema developed for biomedical articles on \ufb01ve dimensions .", "entities": []}, {"text": "Later , the authors annotated 10,000 sentences from full - text publications ( Shatkay et al . , 2008 ) , which was made available after a detailed analysis ( Rzhetsky et al . , 2009).9The annotation are on the level of fragments , which usually correspond to either the sentences or phrases .", "entities": []}, {"text": "2.2 Entity - level Schemes Entity - level schemes aim at annotating the elements on the level of entities .", "entities": []}, {"text": "Gupta and Manning ( 2011 ) proposed a simple schema based on three concepts and labeled 474 abstracts of computational linguistics .", "entities": []}, {"text": "More recently , Jung ( 2017 ) de\ufb01ned \ufb01ve entity types and annotated 1,000 articles about information and communication technology ( ICT ) and chemical engineering .", "entities": []}, {"text": "Blake ( 2010 ) also proposed a schema based on various levels of evidence ( implicit and explicit claims ) and annotated 29 full - text biomedical articles .", "entities": []}, {"text": "However , none of the above data seems to be available but we found one schema with annotated corpus : ScienceIE ( Augenstein et al . , 2017 ) .", "entities": [[20, 21, "DatasetName", "ScienceIE"]]}, {"text": "This schema proposes three elements on the entity level as well as the annotation of keyphrases .", "entities": []}, {"text": "The corpus contains 500 articles about Computer Science , Material Sciences and Physics , which were split into training , development and test datasets and used for the a SemEval task in 2017 .", "entities": []}, {"text": "We found the implementation from two of the participants on the shared task , namely ( Prasad and Kan , 2017 ) and ( Eger et al . , 2017 ) .", "entities": []}, {"text": "2.3 Relation - level Schemes Previous work also considered schemes that consider relations between scienti\ufb01c elements .", "entities": []}, {"text": "Prasad et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2011 ) de\ufb01ned eight discourse relations in the Biomedical Discourse Relation Bank ( BioDRB ) and annotated 24 articles from the GENIA corpus , which was later used in a couple of works ( Ramesh and Yu , 2010 ; Polepalli Ramesh et al . , 8https://github.com/ Franck - Dernoncourt / pubmed - rct 9https://doi.org/10.1371/journal.pcbi .", "entities": [[22, 23, "DatasetName", "GENIA"]]}, {"text": "1000391.s0022012 ) .", "entities": []}, {"text": "Tateisi et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2013 ) de\ufb01ned 16 relations and annotated 30 articles , while Meyers et al .", "entities": []}, {"text": "( 2014 ) proposed \ufb01ve relations and sub - relations with which they annotated 200 biomedical articles .", "entities": []}, {"text": "However , none of the data above seems to be available , but we found corpora for the following two schemes : G\u00b4abor ( G \u00b4 abor et al . , 2016 )", "entities": []}, {"text": "It is a schema in the form of an ontology of 18 relations for the scienti\ufb01c literature , besides three more general relations .", "entities": [[9, 10, "MethodName", "ontology"]]}, {"text": "Six of these relations were recently addressed in the SemEval\u201918 Task 7 , for which annotated data is available ( G \u00b4 abor et al . , 2018 ) .", "entities": []}, {"text": "For sub - task 2 in SemEval\u201918 Task 7 , the code from the team that obtained the best scores in this task is available ( Luan et al . , 2018 ) .", "entities": []}, {"text": "SciDTB ( Yang and Li , 2018 ) .", "entities": []}, {"text": "It is a discourse treebank for scienti\ufb01c articles that includes 17 coarse - grained and 26 \ufb01ne - grained relation types .", "entities": []}, {"text": "They annotated 798 abstracts from the ACL Anthology that are available for download.10 2.4 Hybrid Schemes Hybrid schemes contain labels which cover more than one of the levels above .", "entities": []}, {"text": "Tateisi et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2016 ) created an ontology of entities and relations and annotated 400 abstracts about computational linguistic .", "entities": [[5, 6, "MethodName", "ontology"]]}, {"text": "However , we found only one hybrid schema for which annotated data is available : Green ( Green , 2018 ) .", "entities": []}, {"text": "It is schema of 15 arguments annotated for one single article from the biomedical domain .", "entities": []}, {"text": "The schema includes both entities and relations that are organized in a short taxonomy .", "entities": []}, {"text": "Both schema and the annotated article are available.11 3 Methods We evaluated tools that consider some of the schemes that we found ( cf .", "entities": []}, {"text": "Section 2 ) for the task of text similarity in the scope of our use case of mining alternative methods for animal experiments .", "entities": [[7, 9, "TaskName", "text similarity"]]}, {"text": "In this section we described the data and the tools that we used as well as the evaluation methodology .", "entities": []}, {"text": "3.1 Data We evaluated the selected schemes and tools for the task of text similarity .", "entities": [[13, 15, "TaskName", "text similarity"]]}, {"text": "For this purpose , we 10https://github.com/PKU-TANGENT/SciDTB 11https://github.com/greennl/BIO-Arg", "entities": []}, {"text": "128model our problem as the following : given an input document that describes an animal experiment , we would like to mine similar candidate documents that are potential alternatives to animal testing .", "entities": []}, {"text": "Our de\ufb01nition of similarity requires that both input and candidate documents should have similar research goal and comparable outcomes .", "entities": []}, {"text": "However , the methods in the input document should be substantial different from those in the candidate documents .", "entities": []}, {"text": "Therefore , we aim to compare input and candidate documents based on certain rhetorical elements as opposed to using the whole text .", "entities": []}, {"text": "Our evaluation datasets consist of seven input documents from Medline whose identi\ufb01ers ( PMIDs ) are 11489449 , 11932745 , 16192371 , 16850029 , 19735549 , 21494637 and 24204323 .", "entities": []}, {"text": "For each input document , we collected the top 200 documents ( titles and abstracts ) retrieved from PubMed \u2019s \u201c similar articles \u201d functionality .", "entities": []}, {"text": "On one hand , the candidate documents are already very similar to the input document .", "entities": []}, {"text": "On the other hand , the list of candidates returned by PubMed does not consider our de\ufb01nition of similarity .", "entities": []}, {"text": "In order to build a suitable test set for our use case , a biomedical researcher manually validated at least the top 100 documents with regards to three degrees of similarity : very similar , similar and not similar .", "entities": []}, {"text": "These three labels only consider the similarity of the research goals of each pair of abstracts ( input vs. candidate documents ) but do not address the 3R principles .", "entities": []}, {"text": "Some documents were ignored because either they were only partially similar or because no decision could be made only based on the title and the abstract .", "entities": []}, {"text": "After manual validation by the expert , our seven datasets encompass a total of 562 publications ( titles and abstracts ) .", "entities": []}, {"text": "Figure 1 illustrates the distribution of the labels for each input document .", "entities": []}, {"text": "Only four from the seven input documents had very similar publications ( from only 2 to 8 of them ) , while similar ones ( from only 4 to 19 ) could be found for all of them .", "entities": []}, {"text": "However , the non similar publications are still the largest part ( from 56 to 76 ) of the list .", "entities": []}, {"text": "The annotated data is available for download12 .", "entities": []}, {"text": "Some of the tools that we compared require some linguistic information not originally included in our documents , such as sentences and tokens .", "entities": []}, {"text": "We utilized syntok13for both sentence splitting and tokenization to build input data for one of 12https://github.com/mariananeves/ scientific - elements - text - similarity 13https://github.com/fnl/syntok Figure 1 : Number of documents according to the degree of similarity to the input document .", "entities": []}, {"text": "The number of the dataset ( 1 - 7 ) is shown before the PMID .", "entities": []}, {"text": "the tools , namely , Prasad and Kan ( 2017 ) .", "entities": []}, {"text": "3.2 Tools We found a few available tools that address some of schemes discussed in Section 2 .", "entities": []}, {"text": "However , we had dismiss some of them due to various problems .", "entities": []}, {"text": "We experienced many problems with the TensorFlow library while trying the tool14developed by ( Eger et al . , 2017 ) for the ScienceIE schema .", "entities": [[23, 24, "DatasetName", "ScienceIE"]]}, {"text": "The tool seems to require a version of the library that it is no longer available and we could not resolve this issue not even after contacting the tool \u2019s developers .", "entities": []}, {"text": "We also dismissed the tool15from Jin and Szolovits ( 2018 ) for the PIBOSO and PubMedRCT schemes .", "entities": []}, {"text": "The installation worked but we were not able to train it due to memory problems .", "entities": []}, {"text": "Finally , we did not try the tool16from Luan et al .", "entities": []}, {"text": "( 2018 ) since it addresses a relationbased schema ( G \u00b4 abor ) that requires pre - tagged entities .", "entities": []}, {"text": "Using named entities provided by other tools would probably add too much noise to the experiment .", "entities": []}, {"text": "Finally , we had to dismiss the SAPIENTA tool ( Liakata et al . , 2012b ) because it only allows uploading documents one by one to the Web application and we could not overcome this problem .", "entities": []}, {"text": "We describe below the four tools that we 14https://github.com/UKPLab/ semeval2017 - scienceie 15https://github.com/jind11/", "entities": [[9, 10, "DatasetName", "semeval2017"], [11, 12, "DatasetName", "scienceie"]]}, {"text": "HSLN - Joint - Sentence - Classification 16https://bitbucket.org/luanyi/ semeval2018 / src / master/", "entities": [[6, 7, "TaskName", "Classification"]]}, {"text": "129tried for the extraction of rhetorical elements .", "entities": []}, {"text": "Examples for the sentence - based ( zones ) and entitybased annotations are shown in Figure 2 .", "entities": []}, {"text": "We released in the GitHub repository the annotations extracted by the tools in the JSON format supported by the TextAE tool17 .", "entities": []}, {"text": "Achakulvisut et al.18(Achakulvisut et al . , 2018 ) ( PubMedRCT schema ) .", "entities": []}, {"text": "It addresses the PubMed RCT schema , thus provides predictions for \ufb01ve zoning labels , namely , \u201c Background \u201d , \u201c Objective \u201d , \u201c Method \u201d , \u201c Results \u201d and \u201c Conclusions \u201d .", "entities": [[3, 5, "DatasetName", "PubMed RCT"]]}, {"text": "We utilized the pre - trained models for Conditional Random Fields ( CRF ) as provided by the tool .", "entities": [[12, 13, "MethodName", "CRF"]]}, {"text": "Given that there is no publication , it is not clear what methods are behind the available models , but probably CRF .", "entities": [[21, 22, "MethodName", "CRF"]]}, {"text": "ArguminSci19(Lauscher et", "entities": []}, {"text": "al . , 2018a ) ( Dr. Inventor schema extended ) .", "entities": []}, {"text": "ArguminSci is available both for download as well as on - line ( Web application ) .", "entities": []}, {"text": "It provides predictions for \ufb01ve schemes but we considered only the \u201c Discourse Role Classi\ufb01cation ( DRC ) \u201d whose labels are \u201c Background \u201d , \u201c Challenge \u201d , \u201c Approach \u201d , \u201c Outcome \u201d and \u201c Future Work \u201d .", "entities": []}, {"text": "ArguminSci \u2019s models are based on bidirectional recurrent networks with long shortterm memory cells ( Bi - LSTMs ) and we utilized the command line version of the tool .", "entities": []}, {"text": "MAZEA tool20and schema ( Dayrell et al . , 2012 ) .", "entities": []}, {"text": "The tool addresses six categories , namely , \u201c Background \u201d , \u201c Gap \u201d , \u201c Purpose \u201d , \u201c Method \u201d , \u201c Result \u201d and \u201c Conclusion \u201d .", "entities": []}, {"text": "It is currently not available for download but only as a Web tool that requires to manually upload each document individually .", "entities": []}, {"text": "However , the developers kindly processed our documents locally and sent the predictions back to us .", "entities": []}, {"text": "The tool utilizes machine learning algorithms , such as Support Vector Machines ( SVM ) and Decision Trees .", "entities": [[13, 14, "MethodName", "SVM"]]}, {"text": "Prasad and Kan21(Prasad and Kan , 2017 ) ( ScienceIE schema ) .", "entities": [[9, 10, "DatasetName", "ScienceIE"]]}, {"text": "It addresses the three labels for entities from the ScienceIE schema , namely , \u201c Task \u201d , \u201c Process \u201d and \u201c Material \u201d .", "entities": [[9, 10, "DatasetName", "ScienceIE"]]}, {"text": "From the 17http://textae.pubannotation.org/ 18https://github.com/titipata/ detecting - scientific - claim 19https://github.com/anlausch/ ArguminSci 20http://www.nilc.icmc.usp.br/ mazea - web/ 21https://github.com/animeshprasad/ science_ierepository , we utilized the scripts for feature processing and the template to train the model with CRF++22 .", "entities": []}, {"text": "We had to correct the provided template in order to successfully train the system .", "entities": []}, {"text": "The entity recognition approach is based on various features and uses the CRF algorithm .", "entities": [[12, 13, "MethodName", "CRF"]]}, {"text": "3.3 Evaluation We evaluated the tools for the task of text similarity .", "entities": [[10, 12, "TaskName", "text similarity"]]}, {"text": "Therefore , we calculated the similarity between the input and candidate documents , either based on the whole text or on selected rhetorical elements as provided by the tools .", "entities": []}, {"text": "When utilizing the output from the various tools , we built a pseudo - document based either on the sentences or entities that we obtained .", "entities": []}, {"text": "For the zoning tools , we concatenated the sentences to form a single text , while we printed the entities ( one per line ) for the entity - based predictions .", "entities": []}, {"text": "Similarly , when evaluating combination of various labels , we concatenated the text from various labels into a single \ufb01le .", "entities": []}, {"text": "We performed text similarity using the TextFlow tool ( Mrabet et al . , 2017 ) and utilized these similarity scores to rank the candidate documents .", "entities": [[2, 4, "TaskName", "text similarity"]]}, {"text": "Subsequently , we evaluated the ranked list with regard the metrics of precision , recall and f - score at rank 10 , i.e. P@10 , R@10 and F@10 .", "entities": [[26, 27, "MetricName", "R@10"]]}, {"text": "P@10 is the rate of correct positive candidate documents in the top 10 highest ranked documents , i.e. P@10 = TP@10 10 .", "entities": []}, {"text": "The R@10 corresponds to the rate of positives candidate documents in the top 10 over the total of all positive instances , i.e. R@10 = TP@10 Num : Positive .", "entities": [[1, 2, "MetricName", "R@10"], [23, 24, "MetricName", "R@10"]]}, {"text": "Finally , the F@10 is the harmonic average of the P@10 and R@10 above , i.e. F@10 = 2\u0003P@10\u0003R@10 P@10 + R@10 .", "entities": [[12, 13, "MetricName", "R@10"], [21, 22, "MetricName", "R@10"]]}, {"text": "We considered as positive examples all those publications manually classi\ufb01ed by our expert as \u201c very similar \u201d or \u201c similar \u201d .", "entities": []}, {"text": "Given the few of these instances in our datasets , we decided to make no distinction between both categories .", "entities": []}, {"text": "As a result , the number of positive examples for the input documents in Figure 1 are 4 , 10 , 16 , 11 , 8 , 23 and 6 , respectively .", "entities": []}, {"text": "We evaluated at rank 10 due to the reason that only two datasets have more than 20 positive instances , while only two of them over 10 positive instances .", "entities": []}, {"text": "For datasets which contain more than 10 positive examples , we considered the number of positive instances to be equal to 10 in the equation of R@10 .", "entities": [[26, 27, "MetricName", "R@10"]]}, {"text": "For the \ufb01nal comparison between the various tools and baselines , we per22https://taku910.github.io/crfpp/", "entities": []}, {"text": "130 Figure 2 : Visualization in the TextAE tool of the annotations provided by two of the tools that we used .", "entities": []}, {"text": "formed an average of the metrics over the seven datasets .", "entities": []}, {"text": "We de\ufb01ned two baselines for comparison : ( i ) the original order of the candidate documents as returned by PubMed \u2019s \u201c similar articles \u201d functionality ; and ( ii ) string similarity based on the whole text ( title and abstract ) without any pre - processing on the text .", "entities": []}, {"text": "For the \ufb01rst baseline , we searched in PubMed for each of the seven PMIDs and downloaded the list of the top 100 similar articles ( stand of March 13th , 2019 ) .", "entities": []}, {"text": "Given that the current list of similar articles might include citations not present at the time when our corpus was annotated , we dismissed any document not included in our dataset when calculating the above metrics , i.e. , we did not consider them as false positives .", "entities": []}, {"text": "4 Results We compared the tools based on the metrics of P@10 , R@10 and F@10 that assess the performance of the various tools for the ranking task .", "entities": [[13, 14, "MetricName", "R@10"]]}, {"text": "We performed a total of 38 experiments which includes the four tools and baselines , as well as some combinations of selected labels from the tools .", "entities": []}, {"text": "The combination of labels were decided based on the performance of the single labels and on our understanding of which labels are more relevant for our use case .", "entities": []}, {"text": "Table 2 presents the results for our two baselines and the best results for each tool .", "entities": []}, {"text": "In the following we specify the labels that obtained the best results : \u000fAchakulvisut", "entities": []}, {"text": "et", "entities": []}, {"text": "al : the combination of all labels , i.e. \u201c Background - ConclusionsMethods - Objective - Results \u201d \u000fArguminSci : two combinations of labels were equally good : \u201c Background - Challenge - Tools P@10 R@10 F@10 PubMed 0.30 0.33 0.31 Title+Abstract 0.43 0.51 0.45 Achakulvisut et al 0.44 0.52 0.47 ArguminSci 0.47 0.56 0.50 MAZEA 0.4 0.47 0.42 Prasad and Kan", "entities": [[35, 36, "MetricName", "R@10"]]}, {"text": "0.44 0.54 0.47 Min score 0.14 0.16 0.15 Max score 0.83 1.0 0.90 Table 2 : Summary of the results from the two baselines ( two \ufb01rst rows ) and when using the selected tools .", "entities": []}, {"text": "The maximum scores represent the maximum value of P@10 , R@10 and F@10 that could have been obtained by any of the approaches .", "entities": [[10, 11, "MetricName", "R@10"]]}, {"text": "The minimum scores are the ones obtained when randomly selecting 10 candidates in each dataset , averaged over 1,000 experiments .", "entities": []}, {"text": "Outcome \u201d and \u201c Background - ChallengeOutcome - FutureWork \u201d .", "entities": []}, {"text": "\u000fMAZEA : the combination \u201c Method - Result \u201d .", "entities": []}, {"text": "\u000fPrasad and Kan : the combination \u201c ProcessMaterial \u201d .", "entities": []}, {"text": "For our datasets , all approaches using rhetorical tools obtained a better performance than the baseline from PubMed .", "entities": []}, {"text": "Further , three tools scored higher than our strong baseline that uses TextFlow over the whole text ( titles and abstracts ) .", "entities": []}, {"text": "Two of the tools ( Achakulvisut et al and ArguminSci ) address zoning elements while one of them ( Prasad and Kan ) returns entity - level annotations .", "entities": []}, {"text": "However , none of the tools scored close the maximum possible scores .", "entities": []}, {"text": "Given that we do not have at least 10 positive instances ( \u201c very similar \u201d or \u201c similar \u201d ) for some of our input documents , our maximum P@10 is of 0.83 instead of 1.0 .", "entities": []}, {"text": "The three zoning tools rely on labels that can", "entities": []}, {"text": "131Tools Labels P@10 R@10 F@10AchakulvisutBackground 0.28 0.32 0.30 Objective 0.33 0.41 0.35 Methods 0.31 0.40 0.34 Results 0.20 0.25 0.22 Conclusions 0.23 0.26 0.24ArguminSciBackground 0.23 0.25 0.24 Challenge 0.23 0.26 0.24 Approach 0.26 0.32 0.28 Outcome 0.41 0.50 0.44 Future Work 0.33 0.41 0.35MAZEABackground 0.24 0.28 0.25 Purpose 0.24 0.25 0.25 Method 0.30 0.37 0.32 Result 0.28 0.32 0.30 Conclusion 0.23 0.30 0.25PrasadProcess 0.37 0.48 0.40 Material 0.31 0.35 0.33 Task 0.28 0.36 0.31 Table 3 : Performance of the single labels in the reranking task .", "entities": [[3, 4, "MetricName", "R@10"]]}, {"text": "be mapped to one another , as shown by the order of their labels in Table 3 .", "entities": []}, {"text": "When examining the performance of single labels , only the \u201c Outcome \u201d label from ArguminSci tool could perform close our strong baseline .", "entities": []}, {"text": "The labels that we expected to be more relevant , i.e. the ones more related to the background and outcome sections and less with the methods section , did not always perform better in the ranking task .", "entities": []}, {"text": "For instance , the F@10 obtained by the label \u201c Approach \u201d from ArguminSci performed slightly better ( 0.28 ) than the \u201c Background \u201d ( 0.24 ) and \u201c Challenge \u201d ( 0.24 ) labels .", "entities": []}, {"text": "Similarly , the label \u201c Method \u201d from MAZEA performed better ( 0.32 ) than \u201c Background \u201d ( 0.25 ) and \u201c Purpose \u201d ( 0.25 ) sections .", "entities": []}, {"text": "We wonder whether the good performance of methods - related labels were actually due to mistakes in the classi\ufb01cation performed by the tools .", "entities": []}, {"text": "Our experiments showed that a combination of labels always performed better than the single ones , while some combinations of labels performed better than others ( cf .", "entities": []}, {"text": "Figure 3 ) .", "entities": []}, {"text": "We could not \ufb01nd any difference in the text similarity scores ( as computed by TextFlow ) when considering different order of the same labels in the concatenation of the text.5 Discussion We carried out a total of 38 experiments that involved diverse tools , single labels and combination of various labels .", "entities": [[8, 10, "TaskName", "text similarity"]]}, {"text": "We ran an error analysis to learn more about the false negatives and false positives that we obtained .", "entities": []}, {"text": "At least one positive document was missed by any of the tools , i.e. was not placed among the top 10 positions .", "entities": []}, {"text": "Many of the documents that we missed are certainly due to the limitation of considering only the top 10 highest ranked positions .", "entities": []}, {"text": "However , none of the experiments obtained a recall of 1.0 .", "entities": []}, {"text": "The highest recall that we obtained was 0.9 for the dataset 3 ( 16192371 ) using the ArguminSci tool and either the single label \u201c Outcome \u201d or the combination of labels \u201c Challenge - OutcomeFutureWork \u201d .", "entities": []}, {"text": "On one hand , \ufb01ve documents were missed by all experiments ( 38 times ) , namely , candidate documents \u201c 19155551 \u201d , \u201c 29133591 \u201d , \u201c 21362567 \u201d , \u201c 19667187 \u201d and \u201c 26047474 \u201d from datasets 3 , 5 , 6 , 6 , and 7 , respectively .", "entities": []}, {"text": "On the other hand , the candidate document \u201c 25174890 \u201d from dataset 6 was the least missed one : only by three experiments .", "entities": []}, {"text": "A total of 333 documents were wrongly classi\ufb01ed as positive , i.e. were placed among the top 10 ones , by any of the 38 experiments .", "entities": []}, {"text": "No candidate document was mistakenly classi\ufb01ed by all approaches , but the more frequent ones were : \u201c 21501651 \u201d ( 27 times ) and \u201c 23571276 \u201d ( 25 times ) , both from dataset 4 , and \u201c 11494364 \u201d ( 25 times ) from dataset 7 .", "entities": []}, {"text": "Our expert checked again the labels assigned to the top FPs and FNs above described and con\ufb01rmed that their labels are correct and that the documents have been wrongly classi\ufb01ed by the corresponding approaches .", "entities": []}, {"text": "Our experiments have shown that many of the tools can indeed support our use case , specially when compared to the original list provided by PubMed .", "entities": []}, {"text": "Regarding the integration of these tools into a work\ufb02ow , one of the tools is currently not available ( MAZEA ) , while all the others need some adaptations to be used in real - life applications .", "entities": []}, {"text": "With respect to the methods behind the tools , ArguminSci , which is based on LSTM , performed slightly better than the ones based on CRF ( Achakulvisut et al , Prasad and Kan ) and superior than the machine learning algorithms in MAZEA .", "entities": [[15, 16, "MethodName", "LSTM"], [25, 26, "MethodName", "CRF"]]}, {"text": "However , we did not evaluate the predictions made by the tools , but only their impact in a speci\ufb01c text similarity task .", "entities": [[20, 22, "TaskName", "text similarity"]]}, {"text": "132 Figure 3 : Comparison to the baselines of various combinations of labels as provided by the tools .", "entities": []}, {"text": "We expected that the best performing tools would be the ones that utilized corpora speci\ufb01cally built for the biomedical domain .", "entities": []}, {"text": "From the tools that we evaluated , only Achakulvisut et al and MAZEA were speci\ufb01cally trained on documents from the biomedical or health domains .", "entities": []}, {"text": "Nevertheless , ArguminSci , the best performing one , was trained on documents from computer graphics while and Prasad and Kan utilizes documents about computational linguistics .", "entities": []}, {"text": "We also investigated whether there was any impact of the document type in the corpora , i.e. either full texts or only abstracts , on the performance of the corresponding tools .", "entities": []}, {"text": "However , we did not observe any clear association between these two aspects .", "entities": []}, {"text": "While the best performing tool ( ArguminSci ) was trained on full texts , Achakulvisut et al utilizes only Medline abstracts .", "entities": []}, {"text": "Similar to ArguminSci , the tool from Prasad and Kan is also based on full text documents .", "entities": []}, {"text": "We carried out experiments with various tools but limited to a very speci\ufb01c use case .", "entities": []}, {"text": "Even though our datasets contains a reasonable number of documents ( 562 ) , the similarity of the candidate documents was computed with respect to only seven input documents , and datasets were annotated by only one annotator .", "entities": []}, {"text": "Further , we only considered titles and abstracts in our evaluation , while some tools were trained on full - text documents .", "entities": []}, {"text": "Previous work has already shown the differences of information and performance of NLP tools in biomedical abstracts and full texts ( Verspoor et al . , 2012 ; Mons et al . , 2004 ) .", "entities": []}, {"text": "Our future work will ad - dress many aspects : ( i ) use of full texts ; ( ii ) improvement of the datasets with additional annotators ; ( iii ) estimation of the compliance with the 3R principles by a candidate document , in addition to the calculation of similarity ; ( iv ) evaluation of the relation - based tool ( Luan et al . , 2018 ) and the one for which we experienced memory problems ( Jin and Szolovits , 2018 ) ; and ( v ) evaluation of other schemes ( e.g. Wilbur et al .", "entities": []}, {"text": "( 2006 ) ) for which an implementation is currently not available .", "entities": []}, {"text": "6 Conclusions We surveyed schemes that model scienti\ufb01c elements in publications and selected four schemes for which we could \ufb01nd an available tool .", "entities": []}, {"text": "We utilized the predictions from these tools for assessing the text similarity between documents and further ranking them in the scope of mining alternative methods to animal testing .", "entities": [[10, 12, "TaskName", "text similarity"]]}, {"text": "Our experiments show that a considerable improvement can be obtained when using ArguminSci , with respect to the original ranking returned by PubMed and to the strong baseline that we considered .", "entities": []}, {"text": "However , there is still much room for improvement given that the obtained scores are still far below the possible maximum values .", "entities": []}, {"text": "Acknowledgments We would like to thank Arnaldo Candido Junior and Sandra Maria Alu \u00b4 \u0131sio from the MAZEA tool for kindly processing our documents .", "entities": []}, {"text": "We also would like to thank Animesh Prasad and Min - Yen Kan for their support when using their tool .", "entities": []}, {"text": "133References Titipat Achakulvisut , Chandra Bhagavatula , Daniel E Acuna , and Konrad P Kording .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Claim extraction for scienti\ufb01c publications .", "entities": []}, {"text": "https://github.com/titipata/ detecting - scientific - claim .", "entities": []}, {"text": "Shashank Agarwal and Hong Yu . 2009 .", "entities": []}, {"text": "Automatically classifying sentences in full - text biomedical articles into introduction , methods , results and discussion .", "entities": []}, {"text": "Summit on Translat Bioinforma , 2009:6\u201310 .", "entities": []}, {"text": "Amias2009 - 6[PII ] .", "entities": []}, {"text": "Isabelle Augenstein , Mrinal Das , Sebastian Riedel , Lakshmi Vikraman , and Andrew McCallum . 2017 .", "entities": []}, {"text": "Semeval 2017 Task 10 : ScienceIE - Extracting Keyphrases and Relations from Scienti\ufb01c Publications .", "entities": [[5, 6, "DatasetName", "ScienceIE"]]}, {"text": "In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) , pages 546\u2013555 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Catherine Blake .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Beyond genes , proteins , and abstracts : Identifying scienti\ufb01c claims from full - text biomedical articles .", "entities": []}, {"text": "Journal of Biomedical Informatics , 43(2):173 \u2013 189 .", "entities": []}, {"text": "Carmen Dayrell , Arnaldo Candido Jr. , Gabriel Lima , Danilo Machado Jr. , Ann Copestake , Valria Feltrim , Stella Tagnin , and Sandra Aluisio .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Rhetorical move detection in english abstracts : Multi - label sentence classi\ufb01ers and their annotated corpora .", "entities": []}, {"text": "In Proceedings of the Eight International Conference on Language Resources and Evaluation ( LREC\u201912 ) , Istanbul , Turkey .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Franck Dernoncourt and Ji Young Lee . 2017 .", "entities": []}, {"text": "Pubmed 200k rct : a dataset for sequential sentence classi\ufb01cation in medical abstracts .", "entities": [[0, 1, "DatasetName", "Pubmed"]]}, {"text": "In Proceedings of the Eighth International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 308\u2013313 .", "entities": []}, {"text": "Asian Federation of Natural Language Processing .", "entities": []}, {"text": "Sonali K. Doke and Shashikant C. Dhawale .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Alternatives to animal testing : A review .", "entities": []}, {"text": "Saudi Pharmaceutical Journal , 23(3):223 \u2013 229 .", "entities": []}, {"text": "Steffen Eger , Erik - L \u02c6an Do Dinh , Ilia Kuznetsov , Masoud Kiaeeha , and Iryna Gurevych . 2017 .", "entities": []}, {"text": "Eelection at semeval-2017 task 10 : Ensemble of neural learners for keyphrase classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) , pages 942\u2013946 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Beatriz Fisas , Francesco Ronzano , and Horacio Saggion .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A multi - layered annotated corpus of scienti\ufb01c papers .", "entities": []}, {"text": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC 2016 ) , Paris , France .", "entities": []}, {"text": "European Language Resources Association ( ELRA).Beatriz Fisas , Horacio Saggion , and Francesco Ronzano . 2015 .", "entities": []}, {"text": "On the discoursive structure of computer graphics research papers .", "entities": []}, {"text": "In Proceedings of The 9th Linguistic Annotation Workshop , pages 42 \u2013 51 , Denver , Colorado , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Kata G \u00b4 abor , Davide Buscaldi , Anne - Kathrin Schumann , Behrang QasemiZadeh , Ha \u00a8\u0131fa Zargayouna , and Thierry Charnois .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Semeval-2018 Task 7 : Semantic relation extraction and classi\ufb01cation in scienti\ufb01c papers .", "entities": [[5, 7, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of International Workshop on Semantic Evaluation ( SemEval-2018 ) , New Orleans , LA , USA .", "entities": []}, {"text": "Kata G \u00b4 abor , Haifa Zargayouna , Davide Buscaldi , Isabelle Tellier , and Thierry Charnois . 2016 .", "entities": []}, {"text": "Semantic annotation of the acl anthology corpus for the automatic analysis of scienti\ufb01c literature .", "entities": []}, {"text": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC 2016 ) , Paris , France .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Nancy Green .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Proposed method for annotation of scienti\ufb01c arguments in terms of semantic relations and argument schemes .", "entities": []}, {"text": "In Proceedings of the 5th Workshop on Argument Mining , pages 105\u2013110 .", "entities": [[7, 9, "TaskName", "Argument Mining"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Franz P Gruber and Thomas Hartung .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Alternatives to animal experimentation in basic research .", "entities": []}, {"text": "ALTEX , 21 Suppl 1:331 .", "entities": []}, {"text": "Yufan Guo , Anna Korhonen , Maria Liakata , Ilona Silins Karolinska , Lin Sun , and Ulla Stenius .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Identifying the information structure of scienti\ufb01c abstracts : An investigation of three different schemes .", "entities": []}, {"text": "In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing , BioNLP \u2019 10 , pages 99\u2013107 , Stroudsburg , PA , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Yufan Guo , Ilona Silins , Ulla Stenius , and Anna Korhonen .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Active learning - based information structure analysis of full scienti\ufb01c articles and two applications for biomedical literature review .", "entities": [[0, 2, "TaskName", "Active learning"]]}, {"text": "Bioinformatics , 29(11):1440\u20131447 .", "entities": []}, {"text": "Sonal Gupta and Christopher D. Manning .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Analyzing the dynamics of research by extracting key aspects of scienti\ufb01c papers .", "entities": []}, {"text": "In In Proceedings of IJCNLP .", "entities": []}, {"text": "Hamed Hassanzadeh , Tudor Groza , and Jane Hunter .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Identifying scienti\ufb01c artefacts in biomedical literature : The evidence based medicine use case .", "entities": []}, {"text": "Journal of Biomedical Informatics , 49:159 \u2013 170 .", "entities": []}, {"text": "Kenji Hirohata , Naoaki Okazaki , Sophia Ananiadou , and Mitsuru Ishizuka . 2008 .", "entities": []}, {"text": "Identifying sections in scienti\ufb01c abstracts using conditional random \ufb01elds .", "entities": []}, {"text": "InIn Proc .", "entities": []}, {"text": "of the IJCNLP 2008 .", "entities": []}, {"text": "134Di Jin and Peter Szolovits .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Hierarchical neural networks for sequential sentence classi\ufb01cation in medical scienti\ufb01c abstracts .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3100\u20133109 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yuchul Jung .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A semantic annotation framework for scienti\ufb01c publications .", "entities": []}, {"text": "Quality & Quantity , 51(3):1009\u20131025 .", "entities": []}, {"text": "Su Nam Kim , David Martinez , Lawrence Cavedon , and Lars Yencken .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Automatic classi\ufb01cation of sentences to support evidence based medicine .", "entities": []}, {"text": "BMC Bioinformatics , 12(2):S5 .", "entities": []}, {"text": "Aleksandar Kova \u02c7cevi\u00b4c , Zora Konjovi \u00b4 c , Branko Milosavljevi \u00b4 c , and Goran Nenadic .", "entities": [[1, 2, "MethodName", "Kova"]]}, {"text": "2012 .", "entities": []}, {"text": "Mining methodologies from nlp publications : A case study in automatic terminology recognition .", "entities": []}, {"text": "Computer Speech & Language , 26(2):105 \u2013 126 .", "entities": []}, {"text": "Anne Lauscher , Goran Glava \u02c7s , and Kai Eckert .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Arguminsci :", "entities": []}, {"text": "A tool for analyzing argumentation and rhetorical aspects in scienti\ufb01c writing .", "entities": []}, {"text": "In Proceedings of the 5th Workshop on Argument Mining , pages 22\u201328 . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Argument Mining"]]}, {"text": "Anne Lauscher , Goran Glava \u02c7s , and Simone Paolo Ponzetto .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "An argument - annotated corpus of scienti\ufb01c publications .", "entities": []}, {"text": "In Proceedings of the 5th Workshop on Argument Mining , pages 40\u201346 . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Argument Mining"]]}, {"text": "Maria Liakata , Shyamasree Saha , Simon Dobnik , Colin Batchelor , and Dietrich Rebholz - Schuhmann .", "entities": []}, {"text": "2012a .", "entities": []}, {"text": "Automatic recognition of conceptualization zones in scienti\ufb01c articles and two life science applications .", "entities": []}, {"text": "Bioinformatics , 28(7):991\u20131000 .", "entities": []}, {"text": "Maria Liakata , Simone Teufel , Advaith Siddharthan , and Colin Batchelor .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Corpora for the conceptualisation and zoning of scienti\ufb01c papers .", "entities": []}, {"text": "In Proceedings of the Seventh International Conference on Language Resources and Evaluation ( LREC\u201910 ) , Valletta , Malta .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Maria Liakata , Paul Thompson , Anita de Waard , Raheel Nawaz , Henk Pander Maat , and Sophia Ananiadou . 2012b .", "entities": []}, {"text": "A three - way perspective on scienti\ufb01c discourse annotation for knowledge extraction .", "entities": []}, {"text": "InProceedings of the Workshop on Detecting Structure in Scholarly Discourse , ACL \u2019 12 , pages 37\u201346 , Stroudsburg , PA , USA .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yi Luan , Mari Ostendorf , and Hannaneh Hajishirzi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The uwnlp system at semeval-2018 task 7 : Neural relation extraction model with selectively incorporated concept embeddings .", "entities": [[9, 11, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of The 12th International Workshop on Semantic Evaluation , pages 788\u2013792 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "William C. Mann and Sandra A. Thompson .", "entities": []}, {"text": "1987 .", "entities": []}, {"text": "Rhetorical Structure Theory : Description and Construction of Text Structures .", "entities": []}, {"text": "Springer Netherlands , Dordrecht .", "entities": []}, {"text": "Adam Meyers , Giancarlo Lee , Angus Grieve - Smith , Yifan He , and Harriet Taber .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "2014 .", "entities": []}, {"text": "Annotating relations in scienti\ufb01c articles .", "entities": []}, {"text": "In Proceedings of the Ninth International Conference on Language Resources and Evaluation ( LREC\u201914 ) , Reykjavik , Iceland .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Yoko Mizuta , Anna Korhonen , Tony Mullen , and Nigel Collier . 2006 .", "entities": []}, {"text": "Zone analysis in biology articles as a basis for information extraction .", "entities": []}, {"text": "International Journal of Medical Informatics , 75(6):468 \u2013 487 .", "entities": []}, {"text": "Recent Advances in Natural Language Processing for Biomedical Applications Special Issue .", "entities": []}, {"text": "B. Mons , B. J. A. Schijvenaars , C. C. van der Eijk , E. M. van Mulligen , J. A. Kors , M. Weeber , M. J. Schuemie , and R. Jelier .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Distribution of information in biomedical abstracts and full - text publications .", "entities": []}, {"text": "Bioinformatics , 20(16):2597\u20132604 .", "entities": []}, {"text": "Yassine Mrabet , Halil Kilicoglu , and Dina DemnerFushman . 2017 .", "entities": []}, {"text": "Text\ufb02ow : A text similarity measure based on continuous sequences .", "entities": [[3, 5, "TaskName", "text similarity"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 763\u2013772 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tony Mullen , Yoko Mizuta , and Nigel Collier .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "A baseline feature set for learning rhetorical zones using full articles in the biomedical domain .", "entities": []}, {"text": "SIGKDD Explor .", "entities": []}, {"text": "Newsl . , 7(1):52\u201358 .", "entities": []}, {"text": "Zara Nasar , Syed Waqar Jaffry , and Muhammad Kamran Malik .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Information extraction from scienti\ufb01c articles : a survey .", "entities": []}, {"text": "Scientometrics .", "entities": []}, {"text": "Balaji Polepalli Ramesh , Rashmi Prasad , Tim Miller , Brian Harrington , and Hong Yu . 2012 .", "entities": []}, {"text": "Automatic discourse connective detection in biomedical text .", "entities": [[2, 4, "TaskName", "connective detection"]]}, {"text": "Journal of the American Medical Informatics Association , 19(5):800\u2013808 .", "entities": []}, {"text": "Animesh Prasad and Min - Yen Kan. 2017 .", "entities": []}, {"text": "Wing - nus at semeval-2017 task 10 : Keyphrase extraction and classi\ufb01cation as joint sequence labeling .", "entities": [[8, 10, "TaskName", "Keyphrase extraction"]]}, {"text": "In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) , pages 972\u2013976 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rashmi Prasad , Susan McRoy , Nadya Frid , Aravind Joshi , and Hong Yu . 2011 .", "entities": []}, {"text": "The biomedical discourse relation bank .", "entities": []}, {"text": "BMC Bioinformatics , 12(1):188 .", "entities": []}, {"text": "Balaji Polepalli Ramesh and Hong Yu . 2010 .", "entities": []}, {"text": "Identifying discourse connectives in biomedical text .", "entities": []}, {"text": "AMIA Annu Symp Proc , 2010:657\u2013661 .", "entities": []}, {"text": "Amia2010 sympproc 0657[PII ] .", "entities": []}, {"text": "135Francesco Ronzano and Horacio Saggion .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Dr. inventor framework : Extracting structured information from scienti\ufb01c publications .", "entities": []}, {"text": "In Discovery Science , pages 209\u2013220 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Patrick Ruch , Celia Boyer , Christine Chichester , Imad Tbahriti , Antoine Geissbhler , Paul Fabry , Julien Gobeill , Violaine Pillet , Dietrich RebholzSchuhmann , Christian Lovis , and Anne - Lise Veuthey .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Using argumentation to extract key sentences from biomedical abstracts .", "entities": []}, {"text": "International Journal of Medical Informatics , 76(2):195 \u2013 200 .", "entities": []}, {"text": "Connecting Medical Informatics and Bio - Informatics - MIE 2005 .", "entities": [[4, 5, "DatasetName", "Bio"]]}, {"text": "Andrey Rzhetsky , Hagit Shatkay , and W. John Wilbur .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "How to get the most out of your curation effort .", "entities": []}, {"text": "PLOS Computational Biology , 5(5):1\u201313 .", "entities": [[0, 1, "DatasetName", "PLOS"]]}, {"text": "Hagit Shatkay , Fengxia Pan , Andrey Rzhetsky , and W. John Wilbur . 2008 .", "entities": []}, {"text": "Multi - dimensional classi\ufb01cation of biomedical text :", "entities": []}, {"text": "Toward automated , practical provision of high - utility text to diverse users .", "entities": []}, {"text": "Bioinformatics , 24(18):2086\u20132093 .", "entities": []}, {"text": "Masashi Shimbo , Takahiro Yamasaki , and Yuji Matsumoto . 2003 .", "entities": []}, {"text": "Using sectioning information for text retrieval : a case study with the medline abstracts .", "entities": []}, {"text": "In Proceedings of Second International Workshop on Active Mining ( AM\u201903 ) , pages 32\u201341 .", "entities": []}, {"text": "Yuka Tateisi , Tomoko Ohta , Sampo Pyysalo , Yusuke Miyao , and Akiko Aizawa .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Typed entity and relation annotation on computer science papers .", "entities": []}, {"text": "InProceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC 2016 ) , Paris , France .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Yuka Tateisi , Yo Shidahara , Yusuke Miyao , and Akiko Aizawa .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Relation annotation for understanding research papers .", "entities": []}, {"text": "In LAW@ACL .", "entities": []}, {"text": "Simone Teufel and Marc Moens .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Summarizing scienti\ufb01c articles : Experiments with relevance and rhetorical status .", "entities": []}, {"text": "Comput .", "entities": []}, {"text": "Linguist . ,", "entities": []}, {"text": "28(4):409\u2013445 .", "entities": []}, {"text": "Simone Teufel , Advaith Siddharthan , and Colin Batchelor . 2009 .", "entities": []}, {"text": "Towards discipline - independent argumentative zoning : Evidence from chemistry and computational linguistics .", "entities": []}, {"text": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing : Volume 3 - Volume 3 , EMNLP \u2019 09 , pages 1493\u20131502 , Stroudsburg , PA , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Andrea Varga , Daniel Preotiuc - Pietro , and Fabio Ciravegna . 2012 .", "entities": []}, {"text": "Unsupervised document zone identi\ufb01cation using probabilistic graphical models .", "entities": []}, {"text": "InProceedings of the Eight International Conference on Language Resources and Evaluation ( LREC\u201912 ) , Istanbul , Turkey .", "entities": []}, {"text": "European Language Resources Association ( ELRA).Karin Verspoor , Kevin Bretonnel Cohen , Arrick Lanfranchi , Colin Warner , Helen L. Johnson , Christophe Roeder , Jinho D. Choi , Christopher Funk , Yuriy Malenkiy , Miriam Eckert , Nianwen Xue , William A. Baumgartner , Michael Bada , Martha Palmer , and Lawrence E. Hunter .", "entities": [[18, 19, "DatasetName", "Helen"]]}, {"text": "2012 .", "entities": []}, {"text": "A corpus of full - text journal articles is a robust evaluation tool for revealing differences in performance of biomedical natural language processing tools .", "entities": []}, {"text": "BMC Bioinformatics , 13(1):207 .", "entities": []}, {"text": "Bonnie Webber , Markus Egg , and Valia Kordoni .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Discourse structure and language technology .", "entities": []}, {"text": "Natural Language Engineering , 18(4):437490 .", "entities": []}, {"text": "W. John Wilbur , Andrey Rzhetsky , and Hagit Shatkay .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "New directions in biomedical text annotation : de\ufb01nitions , guidelines and corpus construction .", "entities": []}, {"text": "BMC Bioinformatics , 7(1):356 .", "entities": []}, {"text": "An Yang and Sujian Li .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SciDTB :", "entities": []}, {"text": "Discourse dependency treebank for scienti\ufb01c abstracts .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 444\u2013449 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}]