[{"text": "Proceedings of the 5th Conference on Machine Translation ( WMT ) , pages 300\u2013304 Online , November 19\u201320 , 2020 .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics300IIE \u2019s", "entities": []}, {"text": "Neural Machine Translation Systems for WMT20 Xiangpeng Wei1,2 , Ping Guo1,2 , Yunpeng Li1,2 , Xingsheng Zhang1,2 , Luxi Xing1,2 , Yue Hu1,2 1Institute of Information Engineering , Chinese Academy of Sciences , Beijing , China 2School of Cyber Security , University of Chinese Academy of Sciences , Beijing , China fweixiangpeng , huyue g@iie.ac.cn Abstract", "entities": [[1, 3, "TaskName", "Machine Translation"]]}, {"text": "In this paper we introduce the systems IIE submitted for the WMT20 shared task on German$French news translation .", "entities": []}, {"text": "Our systems are based on the Transformer architecture with some effective improvements .", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "Multiscale collaborative deep architecture , data selection , back translation , knowledge distillation , domain adaptation , model ensemble and re - ranking are employed and proven effective in our experiments .", "entities": [[11, 13, "MethodName", "knowledge distillation"], [14, 16, "TaskName", "domain adaptation"]]}, {"text": "Our German !", "entities": []}, {"text": "French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French!German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .", "entities": [[4, 5, "MetricName", "BLEU"], [20, 21, "MetricName", "BLEU"]]}, {"text": "1 Introduction We participate in the WMT20 shared news translation task in one language pair and two language directions , German ! French and French!German .", "entities": []}, {"text": "Our methods are based on techniques and approaches used in submissions from past years ( Deng et al . , 2018 ; Ng et al . , 2019 ; Sun et al . , 2019 ; Li et al . , 2019 ; Xia et al . , 2019 ) , including the use of subword models ( Sennrich et al . , 2016 ) , iterative back - translation , knowledge distillation , model ensembling and several techniques we proposed recently ( Wei et al . , 2020b , a ) .", "entities": [[15, 18, "DatasetName", "Deng et al"], [71, 73, "MethodName", "knowledge distillation"]]}, {"text": "For our submissions of two language directions , we adopt the deep transformer architectures ( 48layer ) based on multiscale collaboration mechanism ( Wei et al . , 2020b ) as our baseline , which outperformed the standard Transformer - Big as well as shallower models signi\ufb01cantly in terms of translation quality .", "entities": [[38, 39, "MethodName", "Transformer"]]}, {"text": "We also use an iterative back - translation approach ( Zhang et al . , 2018 ) with the controllable sampling to extend the back translation method by jointly training source - to - target and target - to - source NMT models .", "entities": []}, {"text": "Moreover , theknowledge distillation ( Freitag et al . , 2017 ) is employed to leverage the source - side monolingual data .", "entities": []}, {"text": "For our \ufb01nal models , we apply a domainspeci\ufb01c \ufb01ne - tuning process and model ensembling , and decode using noisy channel model re - ranking .", "entities": []}, {"text": "The paper is structured as follows : Section 2 describes the techniques we used , then section 3 shows the experimental settings and results .", "entities": []}, {"text": "Finally , we conclude our work in Section 4 .", "entities": []}, {"text": "2 Our Techniques 2.1 Multiscale Collaborative", "entities": []}, {"text": "Deep Models The structure of NMT models has evolved quickly , such as RNN - based ( Wu et al . , 2016 ) , CNN - based ( Gehring et al . , 2017 ) and attentionbased ( Vaswani et al . , 2017 ) systems .", "entities": []}, {"text": "Deep neural networks have revolutionized the state - of - the - art in various communities , from computer vision to natural language processing .", "entities": []}, {"text": "We adopt the deep transformer model proposed by our work ( Wei et al . , 2020b ) .", "entities": []}, {"text": "Instead of relying on the whole encoder stack to directly learn a desired representation , we let each encoder block learn a \ufb01ne - grained representation and enhance it by encoding spatial dependencies using a bottom - up network .", "entities": []}, {"text": "For coordination , we attend each block of the decoder to both the corresponding representation of the encoder and the contextual representation with spatial dependencies .", "entities": []}, {"text": "This not only shortens the path of error propagation , but also helps to prevent the lower level information from being forgotten or diluted .", "entities": []}, {"text": "In this section we describe the details ( as illustrated in \ufb01gure 1 ) of our deep architectures as below : Block - Scale Collaboration .", "entities": []}, {"text": "An intuitive extension of naive stacking of layers is to group few stacked layers into a block .", "entities": []}, {"text": "We suppose that the encoder and decoder of our model have the same number of blocks ( i.e. , N ) .", "entities": []}, {"text": "Each block of the encoder hasMn(n2f1;2;:::;Ng ) identical layers ,", "entities": []}, {"text": "301 BLOCK 1 LAYER BLOCK 2 LAYER SOURCE INPUTS BLOCK", "entities": []}, {"text": "N LAYER BLOCK 1", "entities": []}, {"text": "LAYER BLOCK 2 BLAYER TARGET INPUTS BLOCK N   BLAYER OUTPUT PROBABILITIES u1Mu2MuNM 0C NC 2C 1CFigure 1 : Illustration of Multiscale Collaborative", "entities": []}, {"text": "Deep NMT Model .", "entities": []}, {"text": "Nis the number of encoder and decoder blocks .", "entities": []}, {"text": "The n - th block of the encoder consists ofMnlayers , while each decoder block only contains one layer .", "entities": []}, {"text": "while each decoder block contains one layer .", "entities": []}, {"text": "Thus , we can adjust the value of each Mn\ufb02exibly to increase the depth of the encoder .", "entities": []}, {"text": "Formally , for the n - th block of the encoder : Bn e = BLOCK e(Bn\u00001 e ) ; ( 1 ) where BLOCK e(\u0001)is the block function , in which the layer functionF(\u0001)is iteratedMntimes , i.e. Bn e= Hn;Mne ; Hn;l e", "entities": []}, {"text": "= F(Hn;l\u00001 e;\u0002n;l e ) + Hn;l\u00001 e ; Hn;0 e= Bn\u00001 e;(2 ) wherel2f1;2;:::;M", "entities": []}, {"text": "ng , Hn;l eand\u0002n;l eare the representation and parameters of the l - th layer in then - th block , respectively .", "entities": []}, {"text": "The decoder works in a similar way but the layer function G(\u0001)is iterated only once in each block , Bn d = BLOCK d(Bn\u00001 d;Bn e )", "entities": []}, {"text": "= G(Bn\u00001 d;Bn", "entities": []}, {"text": "e;\u0002n d ) +", "entities": []}, {"text": "Bn\u00001 d:(3 )", "entities": []}, {"text": "Each block of the decoder attends to the corresponding encoder block .", "entities": []}, {"text": "Contextual Collaboration .", "entities": []}, {"text": "To model long - term spatial dependencies and reuse global representations , we de\ufb01ne a GRU cell Q(c;\u0016 x ) , which maps a hidden state cand an additional input \u0016 xinto a new hidden state :", "entities": [[15, 16, "MethodName", "GRU"]]}, {"text": "Cn = Q(Cn\u00001;Bn e);n2[1;N ] C0 = Ee;(4)whereEeis the embedding matrix of the source inputx .", "entities": []}, {"text": "The new state Cncan be fused with each layer of the subsequent blocks in both the encoder and the decoder .", "entities": []}, {"text": "Formally , Bn ein Eq.(1 ) can be re - calculated in the following way :", "entities": []}, {"text": "Bn e= Hn;Mne ; Hn;l e", "entities": []}, {"text": "= F(Hn;l\u00001 e;Cn\u00001;\u0002n;l e ) + Hn;l\u00001 e ; Hn;0 e= Bn\u00001 e:(5 )", "entities": []}, {"text": "Similarly , for decoder , we have Bn d = BLOCK d(Bn\u00001 d;Bn e )", "entities": []}, {"text": "= G(Bn\u00001 d;Bn e;Cn;\u0002n d ) +", "entities": []}, {"text": "Bn\u00001 d:(6 ) 2.2 Back - Translation with Controllable Sampling Back - translation ( BT ) is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system .", "entities": [[6, 7, "TaskName", "Translation"], [22, 24, "TaskName", "data augmentation"]]}, {"text": "Back - translation \ufb01rst trains an intermediate targetto - source system that is used to translate monolingual target data into additional synthetic parallel data .", "entities": []}, {"text": "This data is used in conjunction with human translated bitext data to train the desired source - totarget system .", "entities": []}, {"text": "In our work , we use an iterative back - translation approach to jointly train source - to - target and targetto - source NMT models .", "entities": []}, {"text": "The process can be summarized as below : \u000fstep 1 : we train both a source - to - target model ( M0 x!y ) and a target - to - source model ( M0 y!x ) using the human translated data .", "entities": []}, {"text": "\u000fstep 2 : we useMt x!yto translate source - side monolingual data to target language , and use Mt y!xto translate target - side monolingual data to source language , where tstarts from 0 .", "entities": [[33, 34, "DatasetName", "0"]]}, {"text": "\u000fstep 3 : we combine both the human translated data and pseudo data synthesized in step 2 to further optimize the two NMT models respectively .", "entities": []}, {"text": "\u000fRepeat steps 2 - 3 until the models converge .", "entities": []}, {"text": "In practice , we repeat 3 times for steps 2 - 3 .", "entities": []}, {"text": "We apply the controllable sampling strategy ( Wei et al . , 2020a ) to synthesize reasonable sentences which are at both high quality and diversity .", "entities": []}, {"text": "3022.3 Knowledge Distillation and Ensemble The early adoption of knowledge distillation ( KD ) ( Kim and Rush , 2016 ) is for model compression .", "entities": [[1, 3, "MethodName", "Knowledge Distillation"], [9, 11, "MethodName", "knowledge distillation"], [23, 25, "TaskName", "model compression"]]}, {"text": "We use the same method as in Sun et al .", "entities": []}, {"text": "( 2019 ) that adopts hybrid heterogeneous teacher : base transformer , deep transformer , big transformer and RNMT+ ( Chen et al . , 2018 ) .", "entities": []}, {"text": "For each individual model , we use the other two models as the teacher model to further improve the performance .", "entities": []}, {"text": "In addition , model ensemble is also used to boost the performance by combining the predictions of above four models at each decoding step .", "entities": []}, {"text": "2.4 Domain - speci\ufb01c Fine - tuning Fine - tuning with domain - speci\ufb01c data is a common and effective method to improve translation quality for a downstream task .", "entities": []}, {"text": "After completing training on the bitext and back - translated data , we train for an additional epoch on a smaller in - domain corpus .", "entities": []}, {"text": "We \ufb01rst select 100 K sentence - pairs from the bilingual as well as pseudo - generated data according to the \ufb01lter method in Deng et al .", "entities": [[24, 27, "DatasetName", "Deng et al"]]}, {"text": "( 2018 ) and continue to train the model on the \ufb01ltered data .", "entities": []}, {"text": "2.5 Reranking N - best reranking is a method of improving translation quality by scoring and selecting a candidate hypothesis from a list of n - best hypotheses generated by a source - to - target model .", "entities": []}, {"text": "For our submissions , we rerank the n - best hypotheses using two aspects as follows : logp(yjx )", "entities": []}, {"text": "+ \u00151logp(xjy ) + \u00152logp(y)(7 )", "entities": []}, {"text": "The weights \u00151and\u00152are determined by tuning them with a random search on a validation set and selecting the weights that give the best performance .", "entities": [[9, 11, "MethodName", "random search"]]}, {"text": "3 System Overview We submit constrained systems to both German to French and French to German translations , with the same techniques .", "entities": []}, {"text": "3.1 Dataset We use all available bilingual datasets and select 10 M bilingual data from WMT\u201920 corpora using the script filter interactive.py1 .", "entities": []}, {"text": "We share a vocabulary for the two languages and apply BPE for word segmentation with 32 K merge 1Scripts at : https://tinyurl.com/yx9fpoam .SystemGerman!French", "entities": [[10, 11, "MethodName", "BPE"]]}, {"text": "Dev Newstest19 MSC ( 48L ) 28.9 33.2 + Iterative BT 31.2 35.7 + KD & Ensemble 32.3 36.5 + Fine - tuning 32.9 37.2 + Reranking 33.8 38.4 WMT\u201920 submission 35.0 Table 1 : SacreBLEU scores on German ! French .", "entities": [[2, 3, "DatasetName", "MSC"], [35, 36, "MetricName", "SacreBLEU"]]}, {"text": "SystemFrench!German", "entities": []}, {"text": "Dev Newstest19 MSC ( 48L ) 22.8 31.7 + Iterative BT 24.2 34.0 + KD & Ensemble 25.1 34.7 + Fine - tuning 25.9 35.4 + Reranking 26.5 36.3 WMT\u201920 submission 36.6 Table 2 : SacreBLEU scores on French !", "entities": [[2, 3, "DatasetName", "MSC"], [35, 36, "MetricName", "SacreBLEU"]]}, {"text": "German .", "entities": []}, {"text": "operations .", "entities": []}, {"text": "For monolingual data , we use 18 M German sentences and 18 M French sentences from Newscrawl , and pre - process them in the same way as bilingual data .", "entities": []}, {"text": "We split 9k sentences from the \u201c dev08 - 14 \u201d as the validation set and use newstest 2019 as the test set .", "entities": []}, {"text": "3.2 Model Con\ufb01guration We use the PyTorch implementation of Transformer2 .", "entities": []}, {"text": "We choose the Transformer base setting , in which the encoder and decoder are of 48 and 6 layers , respectively .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "The dropout rate is \ufb01xed as 0.1 .", "entities": []}, {"text": "We set the batch size as 4096 and the parameter --update - freq as 16 .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}, {"text": "3.3 Results Results and ablations for De !", "entities": []}, {"text": "Fr Fr!De are shown in Table 1 and 2 , respectively .", "entities": []}, {"text": "We report case - sensitive SacreBLEU scores using SacreBLEU ( Post , 2018)3 , using international tokenization for German$French . German!French", "entities": [[5, 6, "MetricName", "SacreBLEU"], [8, 9, "MetricName", "SacreBLEU"]]}, {"text": "For De!Fr , iterative BT improves our baseline performance on newstest 2019 2https://github.com/pytorch/fairseq 3SacreBLEU signatures : BLEU+case.mixed+lang.de - fr+numrefs.1+smooth.exp+ test.wmt19+tok.13a+version.1.2.11 , BLEU+case.mixed+lang.fr - de+numrefs.1+smooth.exp+ test.wmt19+tok.13a+version.1.2.11", "entities": []}, {"text": "303by about 2.5 BLEU .", "entities": [[3, 4, "MetricName", "BLEU"]]}, {"text": "The addition of KD and model ensemble improves single model performance by 0.8 BLEU , but combining this with \ufb01ne - tuning and reranking gives us a total of 2 BLEU .", "entities": [[13, 14, "MetricName", "BLEU"], [30, 31, "MetricName", "BLEU"]]}, {"text": "Our \ufb01nal submission for WMT20 achieves 35.0 BLEU points for German!French translation ( ranked in the second place ) .", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "French!German", "entities": []}, {"text": "For Fr!De , we see similar improvements with iterative BT by about 2.3 BLEU .", "entities": [[13, 14, "MetricName", "BLEU"]]}, {"text": "KD , ensembling , and \ufb01ne - tuning add an additional 1.4 BLEU , with reranking contributing 0.9 BLEU .", "entities": [[12, 13, "MetricName", "BLEU"], [18, 19, "MetricName", "BLEU"]]}, {"text": "Our \ufb01nal submission for WMT20 achieves 36.6 BLEU points for French !", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "German translation ( ranked in the fourth among anonymous submissions ) .", "entities": []}, {"text": "4 Conclusion This paper describes CAS IIE \u2019s submission to the WMT20 German$French news translation task .", "entities": []}, {"text": "We investigate extremely deep models ( with 48 layers ) and exploit effective strategies to better utilize parallel data as well as monolingual data .", "entities": []}, {"text": "Finally , our German!French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French !", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .", "entities": [[4, 5, "MetricName", "BLEU"]]}, {"text": "References Mia Xu Chen , Orhan Firat , Ankur Bapna , Melvin Johnson , Wolfgang Macherey , George Foster , Llion Jones , Mike Schuster , Noam Shazeer , Niki Parmar , Ashish Vaswani , Jakob Uszkoreit , Lukasz Kaiser , Zhifeng Chen , Yonghui Wu , and Macduff Hughes .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The best of both worlds : Combining recent advances in neural machine translation .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 76\u201386 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Yongchao Deng , Shanbo Cheng , Jun Lu , Kai Song , Jingang Wang , Shenglan Wu , Liang Yao , Guchun Zhang , Haibo Zhang , Pei Zhang , Changfeng Zhu , and Boxing Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Alibaba \u2019s neural machine translation systems for wmt18 .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation , Volume 2 : Shared Task Papers , pages 372\u2013380 , Belgium , Brussels .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Markus Freitag , Yaser Al - Onaizan , and Baskaran Sankaran .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Ensemble distillation for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "CoRR , abs/1702.01802.Jonas Gehring , Michael Auli , David Grangier , Denis Yarats , and Yann N Dauphin . 2017 .", "entities": []}, {"text": "Convolutional sequence to sequence learning .", "entities": [[1, 4, "MethodName", "sequence to sequence"]]}, {"text": "In arXiv:1705.03122 .", "entities": []}, {"text": "Yoon Kim and Alexander M. Rush . 2016 .", "entities": []}, {"text": "Sequencelevel knowledge distillation .", "entities": [[1, 3, "MethodName", "knowledge distillation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1317\u20131327 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Bei Li , Yinqiao Li , Chen Xu , Ye Lin , Jiqiang Liu , Hui Liu , Ziyang Wang , Yuhao Zhang , Nuo Xu , Zeyang Wang , Kai Feng , Hexuan Chen , Tengbo Liu , Yanyang Li , Qiang Wang , Tong Xiao , and Jingbo Zhu . 2019 .", "entities": []}, {"text": "The niutrans machine translation systems for wmt19 .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Fourth Conference on Machine Translation ( Volume 2 : Shared Task Papers , Day 1 ) , pages 257\u2013266 , Florence , Italy .", "entities": [[7, 9, "TaskName", "Machine Translation"], [24, 25, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nathan Ng , Kyra Yee , Alexei Baevski , Myle Ott , Michael Auli , and Sergey Edunov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Facebook fair \u2019s wmt19 news translation task submission .", "entities": []}, {"text": "In Proceedings of the Fourth Conference on Machine Translation ( Volume 2 : Shared Task Papers , Day 1 ) , pages 314\u2013319 , Florence , Italy .", "entities": [[7, 9, "TaskName", "Machine Translation"], [24, 25, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Matt Post . 2018 .", "entities": []}, {"text": "A call for clarity in reporting BLEU scores .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 186 \u2013 191 , Brussels , Belgium .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Meng Sun , Bojian Jiang , Hao Xiong , Zhongjun He , Hua Wu , and Haifeng Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Baidu neural machine translation systems for wmt19 .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Fourth Conference on Machine Translation ( Volume 2 : Shared Task Papers , Day 1 ) , pages 374\u2013381 , Florence , Italy .", "entities": [[7, 9, "TaskName", "Machine Translation"], [24, 25, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 30 : Annual Conference on Neural Information Processing Systems 2017 , 4 - 9 December 2017 , Long Beach , CA , USA , pages 5998\u20136008 .", "entities": []}, {"text": "Xiangpeng Wei , Heng Yu , Yue Hu , Rongxiang Weng , Luxi Xing , and Weihua Luo . 2020a .", "entities": []}, {"text": "Uncertaintyaware semantic augmentation for neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , EMNLP 2020 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "304Xiangpeng Wei , Heng Yu , Yue Hu , Yue Zhang , Rongxiang Weng , and Weihua Luo .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Multiscale collaborative deep models for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 414\u2013426 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , and Klaus Macherey .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Google \u2019s neural machine translation system : Bridging the gap between human and machine translation .", "entities": [[0, 1, "DatasetName", "Google"], [3, 5, "TaskName", "machine translation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "In arXiv:1609.08144 .", "entities": []}, {"text": "Yingce Xia , Xu Tan , Fei Tian , Fei Gao , Di He , Weicong Chen , Yang Fan , Linyuan Gong , Yichong Leng , Renqian Luo , Yiren Wang , Lijun Wu , Jinhua Zhu , Tao Qin , and Tie - Yan Liu . 2019 .", "entities": []}, {"text": "Microsoft research asia \u2019s systems for wmt19 .", "entities": []}, {"text": "In Proceedings of the Fourth Conference on Machine Translation ( Volume 2 : Shared Task Papers , Day 1 ) , pages 424\u2013433 , Florence , Italy .", "entities": [[7, 9, "TaskName", "Machine Translation"], [24, 25, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zhirui Zhang , Shujie Liu , Mu Li , Ming Zhou , and Enhong Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Joint training for neural machine translation models with monolingual data .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Thirty - Second AAAI Conference on Arti\ufb01cial Intelligence , ( AAAI-18 ) , the 30th innovative Applications of Arti\ufb01cial Intelligence ( IAAI-18 ) , and the 8th AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence ( EAAI-18 ) , New Orleans , Louisiana , USA , February 2 - 7 , 2018 , pages 555\u2013562 .", "entities": []}, {"text": "AAAI Press .", "entities": []}]