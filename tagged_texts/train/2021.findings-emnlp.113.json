[{"text": "Findings of the Association for Computational Linguistics : EMNLP 2021 , pages 1304\u20131312 November 7\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics1304EDTC : A Corpus for Discourse - Level Topic Chain Parsing Longyin Zhang , Xin Tan , Fang Kong\u0003 , and Guodong Zhou School of Computer Science and Technology , Soochow University , China { lyzhang9,xtan9}@stu.suda.edu.cn { kongfang,gdzhou}@suda.edu.cn Abstract Discourse analysis has long been known to be fundamental in natural language processing .", "entities": []}, {"text": "In this research , we present our insight on discourse - level topic chain ( DTC ) parsing which aims at discovering new topics and investigating how these topics evolve over time within an article .", "entities": []}, {"text": "To address the lack of data , we contribute a new discourse corpus with DTC - style dependency graphs annotated upon news articles .", "entities": []}, {"text": "In particular , we ensure the high reliability of the corpus by utilizing a two - step annotation strategy to build the data and \ufb01ltering out the annotations with low con\ufb01dence scores .", "entities": []}, {"text": "Based on the annotated corpus , we introduce a simple yet robust system for automatic discourse - level topic chain parsing .", "entities": []}, {"text": "1 Introduction Topic information as a crucial auxiliary for text understanding has drawn great attention in recent decades ( Wu et al . , 2019 ; Wang et al . , 2020 ; Sahlgren , 2020 ) .", "entities": []}, {"text": "In the literature , previous studies on topic modeling usually extract topics by introducing latent variables for tokens for topic assigning ( Hofmann , 1999 ; Blei et al . , 2003 ; Yishu et al . , 2017 ) .", "entities": []}, {"text": "Similarly , researches on text - tilling achieve topic segments through lexical cohesion modeling ( Hearst , 1997 ; Purver et al . , 2006 ) .", "entities": []}, {"text": "Instead of lexical cohesion measuring , Rahimi et al .", "entities": []}, {"text": "( 2015 ) put their attention on evaluating the organization and cohesion of pieces of evidence and build topic chains on related text units .", "entities": []}, {"text": "Besides , recent studies on argument mining explore to build links or clusters for topic - dependent arguments ( Wachsmuth et al . , 2018 ; Shnarch et al . , 2018 ; Reimers et al . , 2019 ) .", "entities": [[5, 7, "TaskName", "argument mining"]]}, {"text": "Obviously , more and more researches show that there are certain structures among topic segments that deserve deeper exploration .", "entities": []}, {"text": "In this work , we aim to explore the cohesion of topic - related text segments .", "entities": []}, {"text": "Different from Rahimi \u0003Corresponding authoret al .", "entities": []}, {"text": "( 2015 ) , we show great interest in uncovering how \ufb01ne - grain topics emerge , evolve , and disappear in an article , which is referred to as discourselevel topic chain ( DTC ) parsing .", "entities": []}, {"text": "Since the DTC structure can provide relatively rich and low - noise information about certain topic aspects of articles , it is meaningful for various NLP tasks like summarization ( Perez - Beltrachini et al . , 2019 ) , document similarity measuring ( Gong et al . , 2018 ) , and response generation ( Dziri et", "entities": [[28, 29, "TaskName", "summarization"], [53, 55, "TaskName", "response generation"]]}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "In the literature , topic detection and tracking ( TDT ) ( Allan , 2002 ) is a research area most similar to DTC parsing which aims at identifying new events and tracking how they change over time .", "entities": []}, {"text": "However , the events in the TDT task refer to happenings at certain places and times which only compose a small subset of general topics .", "entities": []}, {"text": "Recently , Xi and Zhou ( 2017 ) manually annotate the \ufb01rst Chinese DTC corpus based on the theme - rheme theory ( Halliday and Matthiessen , 2004 ) .", "entities": []}, {"text": "By contrast , due to the lack of corpus , previous study on English DTC parsing usually uses unsupervised methods ( Kim and Oh , 2011 ) to explore the structure and trends of important topics hidden within news articles .", "entities": []}, {"text": "Obviously , one intractable problem facing DTC parsing is the lack of data .", "entities": []}, {"text": "This research is primarily motivated by ( Polanyi and Scha , 1984 ; Kim and Oh , 2011 ) on the topic chain concept , ( Xi and Zhou , 2017 ) on DTC corpus construction , and ( Reimers et al . , 2019 ) on topic - dependent argument linking .", "entities": []}, {"text": "And our contributions mainly include two aspects : ( i ) building anEnglish corpus of discourse - level topic chain ( EDTC ) through a two - step annotation method and ( ii ) lunching a simple but robust Bert - based baseline system for automatic DTC parsing .", "entities": []}, {"text": "Moreover , as implied in recent researches on discourse rhetorical structure ( DRS ) parsing ( Zhang et al . , 2020 ; Kobayashi et al . , 2021 ; Zhang et al . , 2021 ) , discourse parsing remains challenging due to the lack of data .", "entities": [[38, 40, "TaskName", "discourse parsing"]]}, {"text": "Under this circumstance , we annotate the", "entities": []}, {"text": "1305 \u0102\u0102\u0003 \u0102 u1 .", "entities": []}, {"text": "The novel   [ coronavirus ] , now called [ COVID-19 ] , had not previously detected before the   outbreak was reported in Wuhan , China in December 2019 .", "entities": []}, {"text": "u2 .", "entities": []}, {"text": "[ It ]   has killed around 800 people up to today , and there is stil l", "entities": []}, {"text": "no idea how to beat [ it ] .   u3 .", "entities": []}, {"text": "Up to today , [ WHO ]   has   convened several [ global expert networks ]   for mathematical   modeling , laboratory and clinical management , and infect ion prevention and control .", "entities": []}, {"text": "u4 .", "entities": []}, {"text": "And [ researchers in various countries ]   are stepping up their efforts to develop [ vaccines ]    against the [ virus ] .", "entities": []}, {"text": "u5 .", "entities": []}, {"text": "Some [ research institutions ]   say the [ virus ]   is at risk of mutation and that it is difficult to   eliminate in the short term .", "entities": []}, {"text": "\u0102   u", "entities": []}, {"text": "- k.", "entities": []}, {"text": "There is still no specific", "entities": []}, {"text": "[ medicine ]   against the [ coronavirus ] .international   response coronavirus \u0102\u0003 \u0102 U1 U2 U3   U - k U4 U5 Figure 1 : Example DTC structure .", "entities": []}, {"text": "TOs are marked with square brackets and TEs are marked with gray background .", "entities": []}, {"text": "DTC structures for the 385 Wall Street Journal ( WSJ ) articles in the RST - DT ( Carlson and Marcu , 2001 ) corpus aiming to build a bridge between discourse rhetorical structure and DTC structure for discourse researchers to utilize .", "entities": [[14, 17, "DatasetName", "RST - DT"]]}, {"text": "2 Corpus Annotation Before detailing the annotation process , we give a formal introduction to the \u201c topic \u201d mentioned in this paper .", "entities": []}, {"text": "In topic modeling , a topic is usually viewed as a probability distribution over a \ufb01xed vocabulary ( Liu et al . , 2016 ) .", "entities": []}, {"text": "In addition , previous studies on argument mining usually manually de\ufb01ne some coarse - grain topic categories for either topic - dependent argument classi\ufb01cation or clustering ( Reimers et al . , 2019 ) .", "entities": [[6, 8, "TaskName", "argument mining"]]}, {"text": "Different from previous work , topics in this study refer to \ufb01ne - grained topic categories that \ufb01t the context .", "entities": []}, {"text": "For example , given the sentence \u201c House prices are expected to be fragile . \u201d , the coarse - grained topic label of it could be \u201c economics \u201d and the \ufb01ne - grained label is \u201c house price \u201d .", "entities": []}, {"text": "Comparing the two kinds of labels , the \ufb01rst one seems more like the theme of an article which is useful in text - clustering or text - tilling , and the second one gives us more detailed description on the topic itself which is more practical in discourselevel topic chaining .", "entities": []}, {"text": "For better understanding of our annotation , we present some preliminary de\ufb01nitions as following : Discourse Topic Unit ( DTU ) refers to the elementary topic unit in our annotated DTC structure .", "entities": [[19, 20, "DatasetName", "DTU"]]}, {"text": "In the literature , Xi and Zhou ( 2017 ) hold the view that each sentence is composed of multiple DTUs with different sub - topics which they refer to as elementary discourse topic unit ( EDTU ) .", "entities": []}, {"text": "Different from them , we study macro DTC structures in this work where each sentence is taken as an independent DTU1 .", "entities": []}, {"text": "It is worth mentioning that not all the 1Although we built the corpus based on RST - DT , it remainsDTUs are topic - bearing , there are also some sentences with no topic meaning , e.g. , the sentence \u201c Oops ! \u201d .", "entities": [[15, 18, "DatasetName", "RST - DT"]]}, {"text": "Topic Object ( TO ) could be subject , object , or other noun or noun phrase in the DTU which can provide a certain basis for topic chain parsing .", "entities": [[19, 20, "DatasetName", "DTU"]]}, {"text": "Usually , each TO is closely related to the topic of its DTU , and each DTU maintains an independent TO set .", "entities": [[12, 13, "DatasetName", "DTU"], [16, 17, "DatasetName", "DTU"]]}, {"text": "Notably , the \u201c TO \u201d mentioned here is not directly equivalent to the \u201c entity \u201d in co - reference resolution , the judgment of TO requires a comprehensive consideration of document context .", "entities": []}, {"text": "For example , given the DTU \u201c Drexel Burnham Lambert Inc. is the adviser on the transaction . \u201d , if the surrounding context of the DTU is mainly about the company , then we choose \u201c Drexel Burnham Lambert Inc. \u201d as a TO ; if the context is mainly about the transaction , then we choose \u201c transaction \u201d as a TO , and we can also select both of them if necessary .", "entities": [[5, 6, "DatasetName", "DTU"], [26, 27, "DatasetName", "DTU"]]}, {"text": "It is worth mentioning that the TOs could also be implicit ones which require human judgments .", "entities": []}, {"text": "Topic Event ( TE ) refers to the main phrase which most clearly expresses an event occurrence or a description of the TOs in the DTU .", "entities": [[25, 26, "DatasetName", "DTU"]]}, {"text": "For the DTU u4 in Figure 1 , we select \u201c develop vaccines against the virus \u201d as the topic event of the DTU .", "entities": [[2, 3, "DatasetName", "DTU"], [23, 24, "DatasetName", "DTU"]]}, {"text": "With the above - mentioned de\ufb01nitions in mind , we argue that each DTU is composed of a set of TOs and a core TE .", "entities": [[13, 14, "DatasetName", "DTU"]]}, {"text": "Based on this concept , we give the following four annotation suggestions : \u2022Given two adjacent DTUs in a topic chain , their TO sets should have an intersection in the topic space .", "entities": []}, {"text": "For the two DTUs u3 and u4 in Figure 1 , although the two corresponding TO sets , { WHO , risky to directly take each elementary discourse unit ( EDU ) as a DTU since there are many competing hypotheses about what constitutes an EDU but without \u201c topic \u201d ( Carlson et al . , 2001 ) .", "entities": [[34, 35, "DatasetName", "DTU"]]}, {"text": "Previous work on topic - dependent argument mining usually take each independent sentence as an elementary unit , and this work is inspired by these researches .", "entities": [[6, 8, "TaskName", "argument mining"]]}, {"text": "1306global expert networks } and { researchers in various countries , vaccines } , have no vocabulary intersection , they are highly related in the topic space on \u201c international response \u201d .", "entities": []}, {"text": "In a sense , the relationship between TO sets is similar to that between mentions in co - reference resolution or tokens in lexical chains .", "entities": []}, {"text": "The difference is that DTC parsing requires not only the correlation between TO sets but also the topic transitivity between DTUs .", "entities": []}, {"text": "Therefore , for any two adjacent DTUs on a topic chain , the TE in the second DTU should evolve from the TEs in the established chain where the \ufb01rst DTU is located .", "entities": [[17, 18, "DatasetName", "DTU"], [30, 31, "DatasetName", "DTU"]]}, {"text": "\u2022Sometimes , a DTU may have topic relevance to multiple subsequent DTUs , we only opt for the closest andmost relevant one for annotation .", "entities": [[3, 4, "DatasetName", "DTU"]]}, {"text": "To achieve this , we follow two principles to build each arc in a topic chain : ( 1 ) For each DTU , we search its topic - related DTU from near to far ; ( 2 ) We label topic links for DTUs in order and the annotated DTC structure is dynamically optimized during the human annotation process .", "entities": [[22, 23, "DatasetName", "DTU"], [30, 31, "DatasetName", "DTU"]]}, {"text": "For example , when comparing the current DTU ( U - j ) with previous ones , we directly replace the previously annotated arc ( U - i , U - k ) with ( U - i , U - j ) if the topic relevancy between U - i and U - j obviously surpasses that between U - i and U - k. In other words , we do not require all topic chains to be labeled , but we try to ensure the accuracy of the annotated chains as much as possible .", "entities": [[7, 8, "DatasetName", "DTU"], [87, 88, "MetricName", "accuracy"]]}, {"text": "This labeling strategy can enhance the value of this small - scale corpus to some extent .", "entities": []}, {"text": "\u2022In news articles , many DTUs are organized in an overview - example format where similarities among the examples do exist but the evolution of topics is unseen .", "entities": []}, {"text": "In this study , we do not consider simple juxtapositions like this .", "entities": []}, {"text": "Taking wsj_2349 for example , \u201c u1 : The following issues were recently \ufb01led with the Securities and Exchange Commission : u2 : American Cyanamid Co. , offering of 1,250,000 common shares , via Merrill Lynch Capital Markets .", "entities": []}, {"text": "u3 : Limited Inc. , offering of up to $ 300 million of debt securities .", "entities": []}, {"text": "... u8 : Trans World Airlines Inc. , offering of ... \u201d .", "entities": []}, {"text": "There is a certain textual structure in between the DTUs from u2 to u8 ( e.g. , they share the multinuclear relation List in the RST theory ( Mann and Thompson , 1988 ) ) , but the topic transitivity is weak .", "entities": []}, {"text": "Therefore , we do not mark any topic chains among the DTUs.\u2022Due to the principle of saving words and avoiding repetitions , ellipsis and co - reference occur frequently .", "entities": []}, {"text": "Under this condition , we need to manually \ufb01ll in the ellipsis and clarify the co - reference for better annotation .", "entities": []}, {"text": "Here we take the example in Figure 1 to illustrate the annotation process .", "entities": []}, {"text": "Simply put , the annotation process is also the process of comparing the TO and TEs of the current DTU with that of the previous ones .", "entities": [[19, 20, "DatasetName", "DTU"]]}, {"text": "According to the annotation instructions , we do the comparison from near to far aiming to obtain the closest path for two adjacent DTUs on the chain .", "entities": []}, {"text": "For the DTU u1 , its TO set contains two topic objects , i.e. , \u201c coronavirus \u201d and \u201c COVID-19 \u201d , and its core topic event can be sketched as \u201c coronavirus outbreak in Wuhan \u201d .", "entities": [[2, 3, "DatasetName", "DTU"]]}, {"text": "Correspondingly , the TO set of u2 contains a pronoun object \u201c it \u201d , referring to \u201c coronavirus \u201d , and its core TE is manually detected as \u201c there is still no idea how to beat it \u201d .", "entities": []}, {"text": "Obviously , the two TO sets have an intersection ( i.e. , \u201c coronavirus \u201d ) and the TE in u2 does evolve from that in u1 .", "entities": []}, {"text": "Consequently , we mark a topic link between the two DTUs .", "entities": []}, {"text": "For u3 , both the TO set and TE do not meet our annotation requirements , so we neither link it to u1 nor u2 .", "entities": []}, {"text": "For u4 , the TO set is relevant to that of u3 as international institutions and the two TEs are also interrelated , we therefore build a link between them .", "entities": []}, {"text": "In this way , the overall vein of topic chains will be built after several rounds of comparison .", "entities": []}, {"text": "Notably , from the resulting graph we \ufb01nd that the topic chain with u1 , u2 , u5 , and u - k on it does provide rich and low - noise information about the evolution of COVID-19 , which re\ufb02ects the practical value of our annotated DTCs .", "entities": []}, {"text": "Subjective Differences in Manual Annotation .", "entities": []}, {"text": "A Chinese saying about Shakespeare is that \u201c There are a thousand Hamlets in a thousand people \u2019s eyes \u201d .", "entities": []}, {"text": "From the above annotation process we \ufb01nd that one intractable problem of DTC annotation is the high subjective differences between annotators .", "entities": []}, {"text": "More precisely , judging whether the temporary TE evolves from the previous one is really a very subjective problem , and it is hard to make a strict regulation for the annotators .", "entities": []}, {"text": "In this case , we tackle the issue from two aspects : ( i ) using a well pretrained topic model to assist manual annotation in a two - step fashion and ( ii ) calculating the con\ufb01dence scores of the annotations for data \ufb01ltrating .", "entities": []}, {"text": "Two - Step Annotation : The two - step method consists of two phases : \ufb01rst automatically building", "entities": []}, {"text": "1307 \u0102\u0102\u0003 \u0102 The [ Singapore and Kuala Lumpur stock   exchanges ]   are bracing for a turbulent separation ,   following Malaysian Finance Minister [ Daim   Zainuddin 's ]   long - awaited announcement that the   [ exchanges ]   will sever ties .", "entities": []}, {"text": "u1   On Friday , [ Datuk Daim ]   added spice to an   otherwise unremarkable address on Malaysia 's   proposed budget for 1990 by ordering the [ Kuala   Lumpur Stock Exchange ]   \" to take appropriate   action immediately \" to cut [ its ]   links with the   [ Stock Exchange of Singapore ] .", "entities": []}, {"text": "u2   The delisting of [ Malaysian - based companies ]    from the [ Singapore exchange ]   may not be a   smooth process , analysts say .", "entities": []}, {"text": "u3", "entities": []}, {"text": "Though [ the split ]   has long been expected , the   [ exchanges ]   are n't fully prepared to go their   separate ways .", "entities": []}, {"text": "u4   The [ finance minister 's order ]   was n't sparked by   a single event and does n't indicate a souring in   relations between the neighboring countries .", "entities": []}, {"text": "u5   Rather , the two [ exchanges ]   have been drifting   apart for some years , with a five - year - old   moratorium on new dual listings , separate and   different listing requirements .", "entities": []}, {"text": "u6   Wall street journal 0613 .", "entities": []}, {"text": "u7 < 1 2 >   < 2 3 >   <3 4 >   < 4 -1 >   < 5 -1 >   < 6 7 > 5 6 \u0102\u0102\u0003 \u0102<7 -1 > -1", "entities": []}, {"text": "Figure 2 : The two - step corpus annotation process .", "entities": []}, {"text": "The TOs and TEs are marked out for reference .", "entities": []}, {"text": "topic links between topic - related DTUs2and then manually re\ufb01ning the automatic annotations for DTC structures .", "entities": []}, {"text": "As depicted in Figure 2 , each DTU is preceded by an index pair ( i;j)according to which u - iand u - jare connected through a topic link .", "entities": [[7, 8, "DatasetName", "DTU"]]}, {"text": "And u- iis an ending unit when jequals -1 .", "entities": []}, {"text": "The solid arcs in the example refer to the topic links generated in the \ufb01rst stage .", "entities": []}, {"text": "On this basis , we bring in an auxiliary marker to re\ufb01ne the chain structures where \u201c \u0002 \u201d means that the initial topic arcs ( either machine - labeled or manually labeled links ) are unreasonable and should be deleted directly , and \u201c = \u201d means that the original arcs should be replaced with more proper topic links predicted by the human annotators , e.g. , the dashed arcs in the example .", "entities": []}, {"text": "In this way , we can dynamically optimize the DTC structures during the human annotation process thus determining the most relevant DTUs for annotation .", "entities": []}, {"text": "Our statistics show that around 37.4 % of the automatic annotations are retained in the corpus and 62.6 % of them are invalid and re - annotated by our annotators .", "entities": []}, {"text": "According to this , although there is a great dissimilarity between automatic and manually annotated structures , the topic links of the pre - trained model do provide a good 2Recently , Reimers et al .", "entities": []}, {"text": "( 2019 ) use superior contextualized language models for argument linking , which has proven to have great capabilities in aggregating arguments for unseen topics ( https://github.com/UKPLab ) .", "entities": []}, {"text": "To improve the reliability of the initial chains , we only keep the topic links with topic similarity higher than 0.9 in the \ufb01rst stage.length : # 1 : 715 2 : 442 3 : 266 4 : 159 5 : 92 6 + : 83 Table 1 : Distribution of chain lengths .", "entities": []}, {"text": "reference for better annotation consistency .", "entities": []}, {"text": "Annotation Con\ufb01dence : As stated before , considering the problem of subjective difference , it \u2019s really challenging to build a topic link between two DTUs because we \u2019re not sure if they \u2019re the most relevant .", "entities": []}, {"text": "Although it is hard to strictly regulate the annotators \u2019 subjectivities , it is feasible to calculate the reliability of each annotation item .", "entities": []}, {"text": "Therefore , we aim to ensure the quality of the corpus by \ufb01ltering out the annotations with low con\ufb01dence scores .", "entities": []}, {"text": "Speci\ufb01cally , given the annotation results of the pre - trained topic model , ( \u001c ; \u0013 ) , and that of three annotators , ( \u001c ; \u0017 ) ; ( \u001c ; \u0013 ) , and ( \u001c ; \u0017 ) , on the DTU \u001c , we set the con\ufb01dence of the pre - trained topic model to 0.5 and that of human annotators to 1 , then the con\ufb01dence score of each annotation on \u001c can be calculated as : ( \u001c ; \u0013)!(0:5 + 1)=3:5 , ( \u001c ; \u0017)!2=3:5 .", "entities": [[47, 48, "DatasetName", "DTU"]]}, {"text": "Based on the results , the annotation ( \u001c ; \u0017)with the highest con\ufb01dence score of 0.57 is determined as the result .", "entities": []}, {"text": "Following this way , we can greatly alleviate the \u201c subjectivity \u201d problem by retaining annotations with high con\ufb01dence .", "entities": []}, {"text": "According to our statistics , the averaged con\ufb01dence score of each DTU annotation is around 0.73 .", "entities": [[11, 12, "DatasetName", "DTU"]]}, {"text": "Data Details .", "entities": []}, {"text": "The annotated corpus contains 385 news articles ( 7962 DTUs ) from RST - DT ( Carlson and Marcu , 2001 ) .", "entities": [[12, 15, "DatasetName", "RST - DT"]]}, {"text": "We annotate 4122 topic links corresponding to 1757 topic chains in the corpus , and the chain length distribution is presented in Table 1 .", "entities": []}, {"text": "Obviously , the distribution of chain langths is uneven and most chains have less than 5 topic arcs .", "entities": []}, {"text": "For supervised learning , we have divided the dataset into three parts ( the test corpus is consist with that of RST - DT ) , as shown in Table 2 .", "entities": [[21, 24, "DatasetName", "RST - DT"]]}, {"text": "Based on the test corpus , we calculate the annotation consistency with an averaged Cohen \u2019s kappa value of 0.72 .", "entities": []}, {"text": "Concretely , we compare three groups of manual annotations on DTUs with each other for kappa value calculation and report the average score .", "entities": []}, {"text": "The data and codes are published at https://github . com / NLP - Discourse - SoochowU / DTCP .", "entities": []}, {"text": "3 Baseline Recent years have witnessed the great effects of pre - trained language models ( Devlin et al . , 2019 ;", "entities": []}, {"text": "1308Corpus Doc .", "entities": []}, {"text": "Sent .", "entities": []}, {"text": "Link Chain Train 313 6352 3260 1410 Dev .", "entities": []}, {"text": "34 740 403 164 Test 38 870 459 183 Table 2 : Statistic results for the datasets .", "entities": []}, {"text": "Yang et", "entities": []}, {"text": "al . , 2019 ; Cui et al . , 2020 ) on natural language understanding .", "entities": [[13, 16, "TaskName", "natural language understanding"]]}, {"text": "Following previous work , we introduce a Bert - based ( Devlin et al . , 2019 ) method in our baseline system .", "entities": []}, {"text": "Given a discourse with k-1 DTUs , we use the pretrained Bert3model to encode the entire discourse where each DTU is surrounded by the [ CLS ] and [ SEP ] tokens .", "entities": [[19, 20, "DatasetName", "DTU"]]}, {"text": "And we take the Bert output corresponding to [ CLS ] as our DTU representation .", "entities": [[13, 14, "DatasetName", "DTU"]]}, {"text": "Following previous work , we also \ufb01ne - tuned the pre - trained language model parameters during the training process .", "entities": []}, {"text": "For the convenience of calculation , a zero - initialized vector uzis added at the end of the DTU sequence for the tail DTUs of the topic chains or the isolated DTUs to point to , obtaining U = ( u1;:::;uk\u00001;uz ) .", "entities": [[18, 19, "DatasetName", "DTU"]]}, {"text": "For dependency parsing , we simply build a bi - linear function between Uand its duplicate to achieve it , as following : U \u000b = W \u000b U + b \u000b  U \f = W \f U + b \f  s= UT \u000b WU \f  where U \u000b andU \f are(D\u0002k)matrices representing Uand its duplicate , W2RD\u0002Ddenotes the parameters of the bilinear term , and s2Rk\u0002krefers to the scores for each DTU upon its candidate successor DTUs .", "entities": [[1, 3, "TaskName", "dependency parsing"], [72, 73, "DatasetName", "DTU"]]}, {"text": "The detailed system con\ufb01guration is presented in the Appendix .", "entities": []}, {"text": "We measure the micro - averaged F1 scores of both topic links and chains for performance , and we do not take those isolated DTUs into consideration to avoid the overestimation of performance .", "entities": [[6, 7, "MetricName", "F1"]]}, {"text": "For human performance , we asked 5 other researchers majoring in human language analysis to manually annotate the test set and took the averaged F1 scores as human performance .", "entities": [[24, 25, "MetricName", "F1"]]}, {"text": "Experimental results in Table 3 show that \ufb01ne - tuning the contextualized Bert model can achieve a great performance close to human level .", "entities": []}, {"text": "By observing the model outputs ( sampled in Appendix ) , we \ufb01nd that the automatically parsed chain structures are highly consistent with the manual annotations , which indicates the 3The pre - trained models are borrowed from https:// huggingface.co/transformers .Method", "entities": []}, {"text": "Link Chain Bert - base 89.5 78.9 Bert - large 91.7 82.1 Human - level 94.2 89.1 Table 3 : Baseline performance ( F1 ) .", "entities": [[23, 24, "MetricName", "F1"]]}, {"text": "high reliability of our corpus .", "entities": []}, {"text": "Notably , the obtained system has good generalization and robustness , and can be easily migrated to other NLP tasks for DTC structure incorporation .", "entities": []}, {"text": "4 Conclusion In this research , we explored how \ufb01ne - grain topics emerge , evolve , and disappear within an article .", "entities": []}, {"text": "To address the lack of data , we built an English DTC corpus through a two - step annotation method , and \ufb01ltered out the annotations with low con\ufb01dence scores to ensure the high reliability of the corpus .", "entities": []}, {"text": "During annotation , we found that each annotated topic chain does provide relatively low - noise information about a certain aspect of the article and the complete DTC structure can well describe the overall vein of topics in an article .", "entities": []}, {"text": "With this in mind , we introduced a simple and robust baseline system , and the parsing model we trained can be straightforwardly harnessed in downstream topic - sensitive NLP tasks to boost performance .", "entities": []}, {"text": "It is worth mentioning that we annotated the WSJ articles in the RST - DT corpus also aim to allow the discourse researchers to explore the potential correlation between RST- and DTC - style discourse analysis in future work .", "entities": [[12, 15, "DatasetName", "RST - DT"]]}, {"text": "Acknowledgements", "entities": []}, {"text": "The authors would like to thank Yuqing Xing , Jialong Xie , and the other annotators for their valuable discussion and advice on this research .", "entities": []}, {"text": "This work was supported by the National Key R&D Program of China under Grant No . 2020AAA0108600 , Projects 61876118 and 61976146 under the National Natural Science Foundation of China and the Priority Academic Program Development of Jiangsu Higher Education Institutions .", "entities": []}, {"text": "References James Allan .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Introduction to topic detection and tracking .", "entities": []}, {"text": "Topic Detection and Tracking .", "entities": []}, {"text": "The Information Retrieval Series , 12:1\u201316 .", "entities": [[1, 3, "TaskName", "Information Retrieval"]]}, {"text": "David M. Blei , Andrew Y .", "entities": []}, {"text": "Ng , and Michael I. Jordan .", "entities": []}, {"text": "13092003 .", "entities": []}, {"text": "Latent dirichlet allocation .", "entities": []}, {"text": "J. Mach .", "entities": []}, {"text": "Learn .", "entities": []}, {"text": "Res . , 3(null):993\u20131022 .", "entities": []}, {"text": "Lynn Carlson and Daniel Marcu .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Discourse tagging reference manual .", "entities": []}, {"text": "ISI Technical Report ISI - TR545 , 54:56 .", "entities": []}, {"text": "Lynn Carlson , Daniel Marcu , and Mary Ellen Okurovsky .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Building a discourse - tagged corpus in the framework of Rhetorical Structure Theory .", "entities": []}, {"text": "InProceedings of the Second SIGdial Workshop on Discourse and Dialogue .", "entities": []}, {"text": "Yiming Cui , Wanxiang Che , Ting Liu , Bing Qin , Shijin Wang , and Guoping Hu . 2020 .", "entities": []}, {"text": "Revisiting pre - trained models for Chinese natural language processing .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 657\u2013668 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nouha Dziri , Ehsan Kamalloo , Kory Mathewson , and Osmar Zaiane .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Augmenting neural response generation with context - aware topical attention .", "entities": [[2, 4, "TaskName", "response generation"]]}, {"text": "In Proceedings of the First Workshop on NLP for Conversational AI , pages 18\u201331 , Florence , Italy . Association for Computational Linguistics .", "entities": [[15, 16, "MethodName", "Florence"]]}, {"text": "Hongyu Gong , Tarek Sakakini , Suma Bhat , and JinJun Xiong .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Document similarity for texts of varying lengths via hidden topics .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2341\u20132351 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "MAK Halliday and CMIM Matthiessen .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "An Introduction to Functional Grammar .", "entities": []}, {"text": "Hodder Education .", "entities": []}, {"text": "Marti A. Hearst .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Text tiling : Segmenting text into multi - paragraph subtopic passages .", "entities": []}, {"text": "Computational Linguistics , 23(1):33\u201364 .", "entities": []}, {"text": "Thomas Hofmann . 1999 .", "entities": []}, {"text": "Probabilistic latent semantic indexing .", "entities": []}, {"text": "In Proceedings of SIGIR .", "entities": []}, {"text": "Dongwoo Kim and Alice Oh . 2011 .", "entities": []}, {"text": "Topic chains for understanding a news corpus .", "entities": []}, {"text": "Computational Linguistics and Intelligent Text Processing .", "entities": []}, {"text": "Lecture Notes in Computer Science , 6609:163\u2013176 .", "entities": []}, {"text": "Naoki Kobayashi , Tsutomu Hirao , Hidetaka Kamigaito , Manabu Okumura , and Masaaki Nagata . 2021 .", "entities": []}, {"text": "Improving neural RST parsing model with silver agreement subtrees .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Lan - guage Technologies , pages 1600\u20131612 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Lin Liu , Lin Tang , Wen Dong , Shaowen Yao , and Wei Zhou .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "An overview of topic modeling and its current applications in bioinformatics .", "entities": []}, {"text": "SpringerPlus , 5 .", "entities": []}, {"text": "William C Mann and Sandra A Thompson .", "entities": []}, {"text": "1988 .", "entities": []}, {"text": "Rhetorical structure theory : Toward a functional theory of text organization .", "entities": []}, {"text": "Text - Interdisciplinary Journal for the Study of Discourse , 8(3):243\u2013281 .", "entities": []}, {"text": "Laura Perez - Beltrachini , Yang Liu , and Mirella Lapata .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Generating summaries with topic templates and structured convolutional decoders .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5107\u20135116 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Livia Polanyi and Remko Scha .", "entities": []}, {"text": "1984 .", "entities": []}, {"text": "A syntactic approach to discourse semantics .", "entities": []}, {"text": "In 10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics , pages 413\u2013419 , Stanford , California , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Matthew Purver , Konrad P. K\u00f6rding , Thomas L. Grif\ufb01ths , and Joshua B. Tenenbaum .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Unsupervised topic modelling for multi - party spoken discourse .", "entities": []}, {"text": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics , pages 17\u201324 , Sydney , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Zahra Rahimi , Diane Litman , Elaine Wang , and Richard Correnti . 2015 .", "entities": []}, {"text": "Incorporating coherence of topics as a criterion in automatic response - to - text assessment of the organization of writing .", "entities": []}, {"text": "In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 20\u201330 , Denver , Colorado . Association for Computational Linguistics .", "entities": []}, {"text": "Nils Reimers , Benjamin Schiller , Tilman Beck , Johannes Daxenberger , Christian Stab , and Iryna Gurevych .", "entities": [[4, 5, "DatasetName", "Schiller"]]}, {"text": "2019 .", "entities": []}, {"text": "Classi\ufb01cation and clustering of arguments with contextualized word embeddings .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 567 \u2013 578 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Magnus Sahlgren .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Rethinking topic modelling : From document - space to term - space .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 2250\u20132259 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Eyal Shnarch , Carlos Alzate , Lena Dankin , Martin Gleize , Yufang Hou , Leshem Choshen , Ranit Aharonov , and Noam Slonim .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Will it blend ?", "entities": []}, {"text": "1310blending weak and strong labeled data in a neural network for argumentation mining .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 599\u2013605 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Henning Wachsmuth , Shahbaz Syed , and Benno Stein .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Retrieval of the best counterargument without prior topic knowledge .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 241\u2013251 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Weishi Wang , Steven C.H. Hoi , and Sha\ufb01q Joty .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Response selection for multi - party conversations with dynamic topic tracking .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 6581\u20136591 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chuhan Wu , Fangzhao Wu , Mingxiao An , Yongfeng Huang , and Xing Xie . 2019 .", "entities": []}, {"text": "Neural news recommendation with topic - aware news representation .", "entities": [[1, 3, "TaskName", "news recommendation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1154 \u2013 1159 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Xuefeng Xi and Guodong Zhou .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Building a chinese discourse topic corpus with a micro - topic scheme based on theme - rheme theory .", "entities": []}, {"text": "Big Data Analytics , 2:1\u201314 .", "entities": []}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Carbonell , Russ R Salakhutdinov , and Quoc V Le . 2019 .", "entities": []}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 32 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Miao Yishu , Grefenstette Edward , and Blunsom Phil . 2017 .", "entities": []}, {"text": "Discovering discrete latent topics with neural variational inference .", "entities": [[6, 8, "MethodName", "variational inference"]]}, {"text": "In Proceedings of the 34th International Conference on Machine Learning .", "entities": []}, {"text": "Longyin Zhang , Fang Kong , and Guodong Zhou .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Adversarial learning for discourse rhetorical structure parsing .", "entities": []}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 3946\u20133957 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Longyin Zhang , Yuqing Xing , Fang Kong , Peifeng Li , and Guodong Zhou .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A top - down neural architecture towards text - level parsing of discourse rhetorical structure .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 6386\u20136395 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Appendices A. Model Con\ufb01guration We used the 768D Bert - base and 1024D Bert - large model for DTU representation .", "entities": [[18, 19, "DatasetName", "DTU"]]}, {"text": "In order to prevent memory over\ufb02ow , we segment each article according to the maximum length of 64 , and encode the segmented text fragments in turn .", "entities": []}, {"text": "We manually set the dropout rate , learning rate , L2 regularization value by 0.2 , 1e-5 , and 1e-5 , respectively , according to their contributions to F1 - score , and the number of hyper - parameter search trials was around 15 .", "entities": [[7, 9, "HyperparameterName", "learning rate"], [10, 12, "HyperparameterName", "L2 regularization"], [28, 31, "MetricName", "F1 - score"]]}, {"text": "We trained the models iteratively on the training corpus for 20 rounds with the batch size set to 1 ( document ) , and we got the best model around the 18 - th round .", "entities": [[14, 16, "HyperparameterName", "batch size"]]}, {"text": "We implemented the codes based on the PyTorch framework , and all the experiments were conducted on the NVIDIA Tesla P40 GPUs with the random seed set to 2 .", "entities": []}, {"text": "The number of parameters in each model and the runtime time of each system are shown in the table below .", "entities": [[1, 4, "HyperparameterName", "number of parameters"]]}, {"text": "System Parameter scale Runtime Bert - base 111,553,025 270s Bert - large 337,671,937 541s Table 4 : The parameter scale and runtime ( seconds per round ) of our systems .", "entities": []}, {"text": "B. Instances of DTC Parsing Referring to our system outputs , we \ufb01nd that the automatically parsed DTC structures are highly consistent with human annotations .", "entities": []}, {"text": "Here , we present some automatic DTC structures constructed by the Bert - large - based system for reference .", "entities": []}, {"text": "B.1 .", "entities": []}, {"text": "[ u1 ] Moody \u2019s Investors Service said it reduced its rating on $ 165 million of subordinated debt of this Beverly Hills , Calif. , thrift , citing turmoil in the market for low - grade , high - yield securities .", "entities": []}, {"text": "[ u2 ] The agency said it reduced its rating on the thrift \u2019s subordinated debt to B-2 from Ba-2 and will keep the debt under review for possible further downgrade .", "entities": []}, {"text": "[ u3 ] Columbia Savings is a major holder of so - called junk bonds .", "entities": []}, {"text": "[ u4 ] New federal legislation requires that all thrifts divest themselves of such speculative securities over a period of years .", "entities": []}, {"text": "[ u5 ] Columbia Savings of\ufb01cials were n\u2019t available for comment on the downgrade .", "entities": []}, {"text": "[ u6 ] FRANKLIN SA VINGS ASSOCIATION ( Ottawa , Kan. ) \u2013 Moody \u2019s Investors Service", "entities": []}, {"text": "1311Inc . said it downgraded its rating to B-2 from Ba3 on less than $ 20 million of this thrift \u2019s senior subordinated notes .", "entities": []}, {"text": "[ u7 ] The rating concern said Franklin \u2019s \u201c troubled diversi\ufb01cation record in the securities business \u201d was one reason for the downgrade , citing the troubles at its L.F. Rothschild subsidiary and the possible sale of other subsidiaries .", "entities": []}, {"text": "\u201c They perhaps had concern that we were getting out of all these , \u201d said Franklin President Duane H. Hall .", "entities": []}, {"text": "\u201c I think it was a little premature on their part . \u201d", "entities": []}, {"text": "wsj_2375 u1 u2 u3 u4 u5 u6 u7   Figure 3 : Human annotated ( solid arcs ) and automatically generated ( dashed arcs ) DTC structures for B.1 .", "entities": []}, {"text": "B.2 .", "entities": []}, {"text": "[ u1 ] GAF , Part III is scheduled to begin today .", "entities": []}, {"text": "[ u2 ] After two mistrials , the stakes in the stock manipulation trial of GAF Corp. and its vice chairman , James T. Sherwin , have changed considerably .", "entities": []}, {"text": "[ u3 ] The \ufb01rst two GAF trials were watched closely on Wall Street because they were considered to be important tests of the government \u2019s ability to convince a jury of allegations stemming from its insider - trading investigations .", "entities": []}, {"text": "[ u4 ] In an eightcount indictment , the government charged GAF , a Wayne , N.J. , chemical maker , and Mr. Sherwin with illegally attempting to manipulate the common stock of Union Carbide Corp. in advance of GAF \u2019s planned sale of a large block of the stock in 1986 .", "entities": []}, {"text": "[ u5 ] The government \u2019s credibility in the GAF case depended heavily on its star witness , Boyd L. Jefferies , the former Los Angeles brokerage chief who was implicated by former arbitrager Ivan Boesky , and then pointed the \ufb01nger at Mr. Sherwin , takeover speculator Salim B. Lewis and corporate raider Paul Bilzerian .", "entities": []}, {"text": "[ u6 ] The GAF trials were viewed as previews of the government \u2019s strength in its cases against Mr. Lewis and Mr. Bilzerian .", "entities": []}, {"text": "[ u7 ] Mr. Jefferies \u2019s performance as a witness was expected to affect his sentencing .", "entities": []}, {"text": "[ u8 ] But GAF \u2019s bellwether role was short - lived .", "entities": []}, {"text": "[ u9 ] The \ufb01rst GAF trial ended in a mistrial after four weeks when U.S. District Judge Mary Johnson Lowe found that a prosecutor improperly , but unintentionally , withheld a document .", "entities": []}, {"text": "[ u10 ] After 93 hours of deliberation , the jurors in the second trial said they were hopelessly deadlocked , and another mistrial was declared on March 22 .", "entities": []}, {"text": "[ u11 ] Mean - while , a federal jury found Mr. Bilzerian guilty on securities fraud and other charges in June .", "entities": []}, {"text": "[ u12 ] A month later , Mr. Jefferies was spared a jail term by a federal judge who praised him for helping the government .", "entities": []}, {"text": "[ u13 ] In August , Mr. Lewis pleaded guilty to three felony counts .", "entities": []}, {"text": "wsj_1331 u1 u2 u3 u4 u5 u6 u7 u8 u9 u10 u11 u12 u13 Figure 4 : DTC structures for B.2 .", "entities": []}, {"text": "B.3 .", "entities": []}, {"text": "[ u1 ] MedChem Products Inc. said a U.S. District Court in Boston ruled that a challenge by MedChem to the validity of a U.S. patent held by Pharmacia Inc. was \u201c without merit . \u201d", "entities": []}, {"text": "[ u2 ] Pharmacia , based in Upsala , Sweden , had charged in a lawsuit against MedChem", "entities": []}, {"text": "that MedChem \u2019s AMVISC product line infringes on the Pharmacia patent .", "entities": []}, {"text": "[ u3 ] The patent is related to hyaluronic acid , a rooster - comb extract used in eye surgery .", "entities": []}, {"text": "[ u4 ] In its lawsuit , Pharmacia is seeking unspeci\ufb01ed damages and a preliminary injunction to block MedChem from selling the AMVISC products .", "entities": []}, {"text": "[ u5 ] A MedChem spokesman said the products contribute about a third of MedChem \u2019s sales and 10 % to 20 % of its earnings .", "entities": []}, {"text": "[ u6 ] In the year ended Aug. 31 , 1988 , MedChem earned $ 2.9 million , or 72 cents a share , on sales of $ 17.4 million .", "entities": []}, {"text": "[ u7 ] MedChem said the court \u2019s ruling was issued as part of a \u201c \ufb01rstphase trial \u201d in the patent - infringement proceedings and concerns only one of its defenses in the case .", "entities": []}, {"text": "[ u8 ] It said it is considering \u201c all of its options in light of the decision , including a possible appeal . \u201d", "entities": []}, {"text": "The medical - products company added that it plans to \u201c assert its other defenses \u201d against Pharmacia \u2019s lawsuit , including the claim that it has n\u2019t infringed on Pharmacia \u2019s patent .", "entities": []}, {"text": "[ u9 ] MedChem said that the court scheduled a conference for next Monday \u2013 to set a date for proceedings on Pharmacia \u2019s motion for a preliminary injunction .", "entities": []}, {"text": "wsj_2336 u1 u2 u3 u4 u5 u6 u7 u8 u9   Figure 5 : DTC structures for B.3 .", "entities": []}, {"text": "B.4 .", "entities": []}, {"text": "[ u1 ] ALBERTA ENERGY Co. , Calgary , said it \ufb01led a preliminary prospectus for an offering of common shares .", "entities": []}, {"text": "[ u2 ] The natural resources", "entities": []}, {"text": "1312development concern said proceeds will be used to repay long - term debt , which stood at 598 million Canadian dollars ( US$ 510.6 million ) at the end of 1988 .", "entities": []}, {"text": "[ u3 ] The company plans to raise between C$ 75 million and C$ 100 million from the offering , according to a spokeswoman at Richardson Greenshields of Canada Ltd. , lead underwriter .", "entities": []}, {"text": "[ u4 ] The shares will be priced in early November , she said .", "entities": []}, {"text": "wsj_1183", "entities": []}, {"text": "u1 u2 u3 u4   Figure 6 : DTC structures for B.4 .", "entities": []}, {"text": "B.5 .", "entities": []}, {"text": "[ u1 ] Three new issues begin trading on the New York Stock Exchange today , and one began trading on the Nasdaq / National Market System last week .", "entities": []}, {"text": "[ u2 ] On the Big Board , Crawford & Co. , Atlanta , ( CFD ) begins trading today .", "entities": []}, {"text": "[ u3 ] Crawford evaluates health care plans , manages medical and disability aspects of worker \u2019s compensation injuries and is involved in claims adjustments for insurance companies .", "entities": []}, {"text": "[ u4 ] Also beginning trading today on the Big Board are El Paso Re\ufb01nery Limited Partnership , El Paso , Texas , ( ELP ) and Franklin Multi - Income Trust , San Mateo , Calif. , ( FMI ) .", "entities": [[21, 22, "DatasetName", "Texas"]]}, {"text": "[ u5 ] El Paso owns and operates a petroleum re\ufb01nery .", "entities": []}, {"text": "[ u6 ] Franklin is a closed - end management investment company .", "entities": []}, {"text": "[ u7 ] On the Nasdaq overthe - counter system , Allied Capital Corp. , Washington , D.C. , ( ALII ) began trading last Thursday .", "entities": []}, {"text": "[ u8 ] Allied Capital is a closed - end management investment company that will operate as a business development concern .", "entities": []}, {"text": "wsj_0607 u1 u2 u3 u4 u5 u6 u7 u8   Figure 7 : DTC structures for B.5 .", "entities": []}]