[{"text": "Proceedings of the 22nd Conference on Computational Natural Language Learning ( CoNLL 2018 ) , pages 97\u2013107 Brussels , Belgium , October 31 - November 1 , 2018 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics97Pervasive Attention : 2D Convolutional Neural Networks for Sequence - to - Sequence Prediction Maha Elbayad1,2Laurent Besacier1Jakob Verbeek2 Univ .", "entities": []}, {"text": "Grenoble Alpes , CNRS , Grenoble INP , Inria , LIG , LJK , F-38000 Grenoble France 1firstname.lastname@univ-grenoble-alpes.fr 2firstname.lastname@inria.fr Abstract Current state - of - the - art machine translation systems are based on encoder - decoder architectures , that \ufb01rst encode the input sequence , and then generate an output sequence based on the input encoding .", "entities": [[28, 30, "TaskName", "machine translation"]]}, {"text": "Both are interfaced with an attention mechanism that recombines a \ufb01xed encoding of the source tokens based on the decoder state .", "entities": []}, {"text": "We propose an alternative approach which instead relies on a single 2D convolutional neural network across both sequences .", "entities": []}, {"text": "Each layer of our network recodes source tokens on the basis of the output sequence produced so far .", "entities": []}, {"text": "Attention - like properties are therefore pervasive throughout the network .", "entities": []}, {"text": "Our model yields excellent results , outperforming state - of - the - art encoderdecoder systems , while being conceptually simpler and having fewer parameters .", "entities": []}, {"text": "1 Introduction Deep neural networks have made a profound impact on natural language processing technology in general , and machine translation in particular ( Blunsom , 2013 ; Sutskever et al . , 2014 ; Cho et al . , 2014 ; Jean et al . , 2015 ; LeCun et al . , 2015 ) .", "entities": [[19, 21, "TaskName", "machine translation"]]}, {"text": "Machine translation ( MT ) can be seen as a sequenceto - sequence prediction problem , where the source and target sequences are of different and variable length .", "entities": [[0, 2, "TaskName", "Machine translation"]]}, {"text": "Current state - of - the - art approaches are based on encoder - decoder architectures ( Blunsom , 2013 ;", "entities": []}, {"text": "Sutskever et al . , 2014 ; Cho et al . , 2014 ; Bahdanau et al . , 2015 ) .", "entities": []}, {"text": "The encoder \u201c reads \u201d the variable - length source sequence and maps it into a vector representation .", "entities": []}, {"text": "The decoder takes this vector as input and \u201c writes \u201d the target sequence , updating its state each step with the most recent word that it generated .", "entities": []}, {"text": "The basic encoder - decoder model is generally equipped with an attention model ( Bahdanau et al . , 2015 ) , which repetitively re - accesses the source sequence during the decoding process .", "entities": []}, {"text": "Given the current state of the decoder , a probability distribution over the elements in the source sequence is computed , which is then used to select or aggregate features of these elements into a single \u201c context \u201d vector that is used by the decoder .", "entities": []}, {"text": "Rather than relying on the global representation of the source sequence , the attention mechanism allows the decoder to \u201c look back \u201d into the source sequence and focus on salient positions .", "entities": []}, {"text": "Besides this inductive bias , the attention mechanism bypasses the problem of vanishing gradients that most recurrent architectures encounter .", "entities": []}, {"text": "However , the current attention mechanisms have limited modeling abilities and are generally a simple weighted sum of the source representations ( Bahdanau et al . , 2015 ;", "entities": []}, {"text": "Luong et al . , 2015 ) , where the weights are the result of a shallow matching between source and target elements .", "entities": []}, {"text": "The attention module re - combines the same source token codes and is unable to re - encode or re - interpret the source sequence while decoding .", "entities": []}, {"text": "To address these limitations , we propose an alternative neural MT architecture , based on deep 2D convolutional neural networks ( CNNs ) .", "entities": []}, {"text": "The product space of the positions in source and target sequences de\ufb01nes the 2D grid over which the network is de\ufb01ned .", "entities": []}, {"text": "The convolutional \ufb01lters are masked to prohibit accessing information derived from future tokens in the target sequence , obtaining an autoregressive model akin to generative models for images and audio waveforms ( Oord et al . , 2016a , b ) .", "entities": []}, {"text": "See Figure 1 for an illustration .", "entities": []}, {"text": "This approach allows us to learn deep feature hierarchies based on a stack of 2D convolutional layers , and bene\ufb01t from parallel computation during training .", "entities": []}, {"text": "Every layer of our network computes features of the the source tokens , based on the target sequence produced so far , and uses these to predict the next output token .", "entities": []}, {"text": "Our model therefore has attention - like capabilities by construction , that are pervasive throughout the layers of the network ,", "entities": []}, {"text": "98 Alice a dit ` a Bob que Charlie < start >", "entities": []}, {"text": "Alice told Bob that Charlie toldTarget sequenceSource sequenceFigure 1 : Convolutional layers in our model use masked 3\u00023\ufb01lters so that features are only computed from previous output symbols .", "entities": []}, {"text": "Illustration of the receptive \ufb01elds after one ( dark blue ) and two layers ( light blue ) , together with the masked part of the \ufb01eld of view of a normal 3\u00023\ufb01lter ( gray ) .", "entities": []}, {"text": "rather than using an \u201c add - on \u201d attention model .", "entities": []}, {"text": "We validate our model with experiments on the IWSLT 2014 German - to - English ( De - En ) and English - to - German(En - De ) tasks .", "entities": []}, {"text": "We improve on state - of - the - art encoder - decoder models with attention , while being conceptually simpler and having fewer parameters .", "entities": []}, {"text": "In the next section we will discuss related work , before presenting our approach in detail in Section 3 .", "entities": []}, {"text": "We present our experimental evaluation results in Section 4 , and conclude in Section 5 . 2", "entities": []}, {"text": "Related work The predominant neural architectures in machine translation are recurrent encoder - decoder networks ( Graves , 2012 ; Sutskever et al . , 2014 ; Cho et", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "al . , 2014 ) .", "entities": []}, {"text": "The encoder is a recurrent neural network ( RNN ) based on gated recurrent units ( Hochreiter and Schmidhuber , 1997 ; Cho et", "entities": []}, {"text": "al . , 2014 ) to map the input sequence into a vector representation .", "entities": []}, {"text": "Often a bi - directional RNN ( Schuster and Paliwal , 1997 ) is used , which consists of two RNNs that process the input in opposite directions , and the \ufb01nal states of both RNNs are concatenated as the input encoding .", "entities": []}, {"text": "The decoder consists of a second RNN , which takes the input encoding , and sequentially samples the output sequence one to - ken at a time whilst updating its state .", "entities": []}, {"text": "While best known for their use in visual recognition models , ( Oord et al . , 2016a ; Salimans et al . , 2017 ; Reed et al . , 2017 ; Oord et al . , 2016c ) .", "entities": []}, {"text": "Recent works also introduced convolutional networks to natural language processing .", "entities": []}, {"text": "The \ufb01rst convolutional apporaches to encoding variablelength sequences consist of stacking word vectors , applying 1D convolutions then aggregating with a max - pooling operator over time ( Collobert and Weston , 2008 ; Kalchbrenner et al . , 2014 ; Kim , 2014 ) .", "entities": []}, {"text": "For sequence generation , the works of Ranzato et al . ( 2016 ) ; Bahdanau et al . ( 2017 ) ; Gehring et al .", "entities": []}, {"text": "( 2017a ) mix a convolutional encoder with an RNN decoder .", "entities": []}, {"text": "The \ufb01rst entirely convolutional encoder - decoder models where introduced by Kalchbrenner et al . ( 2016b ) , but they did not improve over state - of - the - art recurrent architectures .", "entities": []}, {"text": "Gehring et al .", "entities": []}, {"text": "( 2017b ) outperformed deep LSTMs for machine translation 1D CNNs with gated linear units ( Meng et al . , 2015 ;", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "Oord et al . , 2016c ; Dauphin et al . , 2017 ) in both the encoder and decoder modules .", "entities": []}, {"text": "Such CNN - based models differ from their RNN - based counterparts in that temporal connections are placed between layers of the network , rather than within layers .", "entities": []}, {"text": "See Figure 2 for a conceptual illustration .", "entities": []}, {"text": "This apparently small difference in connectivity has two important consequences .", "entities": []}, {"text": "First , it makes the \ufb01eld of view grow linearly across layers in the convolutional network , while it is unbounded within layers in the recurrent network .", "entities": []}, {"text": "Second , while the activations in the RNN can only be computed in a sequential manner , they can be computed in parallel across the temporal dimension in the convolutional case .", "entities": []}, {"text": "In all the recurrent or convolutional models mentioned above , each of the input and output sequences are processed separately as a onedimensional sequence by the encoder and decoder respectively .", "entities": []}, {"text": "Attention mechanisms ( Bahdanau et al . , 2015 ; Luong et al . , 2015 ; Xu et", "entities": []}, {"text": "al . , 2015 ) were introduced as an interface between the encoder and decoder modules .", "entities": []}, {"text": "During encoding , the attention model \ufb01nds which hidden states from the source code are the most salient for generating the next target token .", "entities": []}, {"text": "This is achieved by evaluating a \u201c context vector \u201d which , in its most basic form , is a weighted average of the source features .", "entities": []}, {"text": "The weights of the summation are predicted by a small neural network that scores these features condi-", "entities": []}, {"text": "99 < start >", "entities": []}, {"text": "The cat sat on theThe cat sat on the matFigure 2 : Illustration of decoder network topology with two hidden layers , nodes at bottom and top represent input and output respectively .", "entities": []}, {"text": "Horizontal connections are used for RNNs , diagonal connections for convolutional networks .", "entities": []}, {"text": "Vertical connections are used in both cases .", "entities": []}, {"text": "Parameters are shared across time - steps ( horizontally ) , but not across layers ( vertically ) .", "entities": []}, {"text": "tioning on the current decoder state .", "entities": []}, {"text": "Vaswani et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2017 ) propose an architecture relying entirely on attention .", "entities": []}, {"text": "Positional input coding together with self - attention ( Parikh et al . , 2016 ; Lin et al . , 2017 ) replaces recurrent and convolutional layers .", "entities": []}, {"text": "Huang et al .", "entities": []}, {"text": "( 2018 ) use an attentionlike gating mechanism to alleviate an assumption of monotonic alignment in the phrase - based translation model of Wang et al .", "entities": []}, {"text": "( 2017 ) .", "entities": []}, {"text": "Deng et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) treat the sentence alignment as a latent variable which they infer using a variational inference network during training to optimize a variational lower - bound on the log - likelihood .", "entities": [[16, 18, "MethodName", "variational inference"], [30, 33, "MetricName", "log - likelihood"]]}, {"text": "Beyond uni - dimensional encoding / decoding .", "entities": []}, {"text": "Kalchbrenner et al .", "entities": []}, {"text": "( 2016a ) proposed a 2D LSTM model similar to our 2D CNN for machine translation .", "entities": [[6, 7, "MethodName", "LSTM"], [14, 16, "TaskName", "machine translation"]]}, {"text": "Like our model , a 2D grid is de\ufb01ned across the input and output sequences , as in Figure 1 .", "entities": []}, {"text": "In their model , each cell takes input from its left and bottom neighbor .", "entities": []}, {"text": "In a second LSTM stream , each cell takes input from its left and top neighbor , as well as from the corresponding cell in the \ufb01rst stream .", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "They also observed that such a structure implements an implicit form of attention , by producing an input encoding that depends on the output sequence produced so far .", "entities": []}, {"text": "Wu et al .", "entities": []}, {"text": "( 2017 ) used a CNN over the 2D source - target representation as in our work , but only as a discriminator in an adversarial training setup .", "entities": []}, {"text": "They do not use masked convolutions , sincetheir CNN is used to predict if a given sourcetarget pair is a human or machine translation .", "entities": [[22, 24, "TaskName", "machine translation"]]}, {"text": "A standard encoder - decoder model with attention is used to generate translations .", "entities": []}, {"text": "3 Translation by 2D Convolution In this section we present our 2D CNN translation model in detail .", "entities": [[1, 2, "TaskName", "Translation"], [4, 5, "MethodName", "Convolution"]]}, {"text": "Input source - target tensor .", "entities": []}, {"text": "Given the source and target pair ( s;t)of lengthsjsjandjtjrespectively , we \ufb01rst embed the tokens in dsanddtdimensional spaces via look - up tables .", "entities": []}, {"text": "The word embeddingsfx1;:::;xjsjgandfy1;:::;yjtjgare then concatenated to form a 3D tensor X2 Rjtj\u0002jsj\u0002f0 , withf0 = dt+ds , where Xij= [ yixj ] : ( 1 ) This joint unigram encoding is the input to our convolutional network .", "entities": []}, {"text": "Convolutional layers .", "entities": []}, {"text": "We use the DenseNet ( Huang et al . , 2017 ) convolutional architecture , which is the state of the art for image classi\ufb01cation tasks .", "entities": [[3, 4, "MethodName", "DenseNet"]]}, {"text": "Layers are densely connected , meaning that each layer takes as input the activations of all the preceding layers , rather than just the last one , to produce its gfeature maps .", "entities": []}, {"text": "The parameter gis called the \u201c growth rate \u201d as it is the number of appended channels to the network \u2019s output at each layer .", "entities": []}, {"text": "The long - distance connections in the network improve gradient \ufb02ow to early network layers during training , which is bene\ufb01cial for deeper networks .", "entities": []}, {"text": "Each layer \ufb01rst batch - normalizes ( Ioffe and Szegedy , 2015 ) its input and apply a ReLU ( Nair and Hinton , 2010 ) non - linearity .", "entities": [[18, 19, "MethodName", "ReLU"]]}, {"text": "To reduce the computation cost , each layer \ufb01rst computes 4 g channels using a 1\u00021convolution from the f0 + ( l\u00001)ginput channels to layer l2f1;:::;Lg .", "entities": []}, {"text": "This is followed by a second batch - normalization and ReLU non - linearity .", "entities": [[10, 11, "MethodName", "ReLU"]]}, {"text": "The second convolution has(k\u0002dk 2e)kernels , i.e. masked as illustrated in Figure 1 , and generates the goutput features maps to which we apply dropout ( Srivastava et al . , 2014 ) .", "entities": [[2, 3, "MethodName", "convolution"]]}, {"text": "The architecture of the densely connected network is illustrated in Figure 3 .", "entities": []}, {"text": "We optionally use gated linear units ( Dauphin et al . , 2017 ) in both convolutions , these double the number of output channels , and we use half of them to gate the other half .", "entities": []}, {"text": "100 Input BN ReLU Conv(1 ) BN ReLU Conv(k ) Dropout Figure 3 : Architecture of the DenseNet at block level ( top ) , and within each block ( bottom ) .", "entities": [[3, 4, "MethodName", "ReLU"], [7, 8, "MethodName", "ReLU"], [10, 11, "MethodName", "Dropout"], [17, 18, "MethodName", "DenseNet"]]}, {"text": "Target sequence prediction .", "entities": []}, {"text": "Starting from the initialf0feature maps , each layer l2f1;:::;Lg of our DenseNet produces a tensor Hlof size jtj\u0002jsj\u0002fl , whereflis the number of output channels of that layer .", "entities": [[11, 12, "MethodName", "DenseNet"]]}, {"text": "To compute a distribution over the tokens in the output vocabulary , we need to collapse the second dimension of the tensor , which is given by the variable length of the input sequence , to retrieve a unique encoding for each target position .", "entities": []}, {"text": "The simplest aggregation approach is to apply max - pooling over the input sequence to obtain a tensorHpool2Rjtj\u0002fL , i.e. Hpool id= max j2f1;:::;jsjgHL ijd : ( 2 ) Alternatively , we can use average - pooling over the input sequence :", "entities": []}, {"text": "Hpool id=1p jsjX j2f1;:::;jsjgHL ijd : ( 3 ) The scaling with the inverse square - root of the source length acts as a variance stabilization term , which we \ufb01nd to be more effective in practice than a simple averaging .", "entities": []}, {"text": "The pooled features are then transformed to predictions over the output vocabulary V , by linearly mapping them with a matrix E2RjVj\u0002fLto the vocabulary dimension jVj , and then applying a soft - max .", "entities": []}, {"text": "Thus the probability distribution over Vfor thei - th output token is obtained as pi = SoftMax ( EHpool i ): ( 4 ) Alternatively , we can use Eto project to dimensiondt , and then multiply with the target word embedding matrix used to de\ufb01ne the input tensor .", "entities": [[16, 17, "MethodName", "SoftMax"]]}, {"text": "This reduces the number of parameters and generally improves the performance .", "entities": [[3, 6, "HyperparameterName", "number of parameters"]]}, {"text": "Implicit sentence alignment .", "entities": []}, {"text": "For a given output token position i , the max - pooling operator of Eq . ( 2 ) partitions the fLchannels by assigning them across the source tokens j. Let us de\ufb01ne Bij = fd2f1;:::;f Lgjj= arg max(HL ijd)g as the channels assigned to source token jfor output tokeni .", "entities": []}, {"text": "The energy that enters into the softmax to predict token w2 V for thei - th output position is given by eiw = X d2f1;:::;fLgEwdHpool id(5 )", "entities": [[6, 7, "MethodName", "softmax"]]}, {"text": "= X j2f1;:::;jsjgX d2BijEwdHL ijd : ( 6 ) The total contribution of the j - th input token is thus given by \u000b ij = X d2BijEwdHL ijd ; ( 7 ) where we dropped the dependence on wfor simplicity .", "entities": []}, {"text": "As we will show experimentally in the next section , visualizing the values \u000b ijfor the groundtruth output tokens , we can recover an implicit sentence alignment used by the model .", "entities": []}, {"text": "Self attention .", "entities": []}, {"text": "Besides pooling we can collapse the source dimension of the feature tensor with an attention mechanism .", "entities": []}, {"text": "This mechanism will generate a tensor Hattthat can be used instead of , or concatenated with , HPool .", "entities": []}, {"text": "We use the self - attention approach of Lin et al .", "entities": []}, {"text": "( 2017 ) , which for output token icomputes the attention vector \u001ai2Rjsjfrom the activations", "entities": []}, {"text": "HL i : \u001ai = SoftMax\u0000 HL iw+b 1jsj\u0001 ; ( 8) Hatt i = p jsj\u001a >", "entities": []}, {"text": "iHL", "entities": []}, {"text": "i ; ( 9 ) wherew2RfLandb2Rare parameters of the attention mechanism .", "entities": []}, {"text": "Scaling of attention vectors with the square - root of the source length was also used by Gehring et", "entities": []}, {"text": "al . ( 2017b ) , and we found it effective here as well as in the average - pooling case .", "entities": []}, {"text": "4 Experimental evaluation In this section , we present our experimental setup , followed by quantitative results , qualitative examples of implicit sentence alignments from our model , and a comparison to the state of the art .", "entities": []}, {"text": "1014.1 Experimental setup Data and pre - processing .", "entities": []}, {"text": "We experiment with the IWSLT 2014 bilingual dataset ( Cettolo et al . , 2014 ) , which contains transcripts of TED talks aligned at sentence level , and translate between German ( De ) and English ( En ) in both directions .", "entities": []}, {"text": "Following the setup of ( Edunov et al . , 2018 ) , sentences longer than 175 tokens and pairs with length ratio exceeding 1.5 were removed from the original data .", "entities": []}, {"text": "There are 160 + 7 K training sentence pairs , 7 K of which are separated and used for validation / development .", "entities": []}, {"text": "We report results on a test set of 6,578 pairs obtained by concatenating dev2010 and tst2010 - 2013 .", "entities": []}, {"text": "We tokenized and lowercased all data using the standard scripts from the Moses toolkit ( Koehn et al . , 2007 ) .", "entities": []}, {"text": "For open - vocabulary translation , we segment sequences using joint byte pair encoding ( Sennrich et al . , 2016 ) with 14 K merge operations on the concatenation of source and target languages .", "entities": [[11, 14, "MethodName", "byte pair encoding"]]}, {"text": "This results in a German and English vocabularies of around 12 K and 9 K types respectively .", "entities": []}, {"text": "Implementation details .", "entities": []}, {"text": "Unless stated otherwise , we use DenseNets with masked convolutional \ufb01lters of size 5\u00023 , as given by the light blue area in Figure 1 .", "entities": []}, {"text": "To train our models , we use maximum likelihood estimation ( MLE ) with Adam ( \f 1= 0:9 ; \f 2= 0:999;\u000f= 1e\u00008)starting with a learning rate of 5e\u00004that we scale by a factor of 0.8 if no improvement ( \u000e\u00140:01 ) is noticed on the validation loss after three evaluations , we evaluate every 8 K updates .", "entities": [[14, 15, "MethodName", "Adam"], [26, 28, "HyperparameterName", "learning rate"], [48, 49, "MetricName", "loss"]]}, {"text": "After training all models up to 40 epochs , the best performing model on the validation set is used for decoding the test set .", "entities": []}, {"text": "We use a beam - search of width 5 without any length or coverage penalty and measure translation quality using the BLEU metric ( Papineni et al . , 2002 ) .", "entities": [[21, 22, "MetricName", "BLEU"]]}, {"text": "Baselines .", "entities": []}, {"text": "For comparison with state - of - theart architectures , we implemented a bidirectional LSTM encoder - decoder model with dotproduct attention ( Bahdanau et al . , 2015 ;", "entities": [[13, 15, "MethodName", "bidirectional LSTM"]]}, {"text": "Luong et al . , 2015 ) using PyTorch ( Paszke et al . , 2017 ) , and used Facebook AI Research Sequence - toSequence Toolkit ( Gehring et al . , 2017b ) to train the ConvS2S and Transformer ( Vaswani et al . , 2017 ) models on our data .", "entities": [[40, 41, "MethodName", "Transformer"]]}, {"text": "For the Bi - LSTM encoder - decoder , the encoder is a single layer bidirectional LSTM with input embeddings of size 128 and a hidden state of sizeModel BLEU Flops \u0002105#params Average 31.57 \u00060.11", "entities": [[4, 5, "MethodName", "LSTM"], [15, 17, "MethodName", "bidirectional LSTM"], [29, 30, "MetricName", "BLEU"]]}, {"text": "3.63 7.18 M Max 33.70 \u00060.06 3.44 7.18 M Attn 32.07 \u00060.13 3.61 7.24 M Max , gated 33.66 \u00060.16 3.49 9.64 M [ Max , Attn ] 33.81 \u00060.03 3.51 7.24 M Table 1 : Our model ( L=24;g=32;ds = dt=128 ) with different pooling operators and using gated convolutional units .", "entities": []}, {"text": "256 ( 128 in each direction ) .", "entities": []}, {"text": "The decoder is a single layer LSTM with similar input size and a hidden size of 256 , the target input embeddings are also used in the pre - softmax projection .", "entities": [[6, 7, "MethodName", "LSTM"], [29, 30, "MethodName", "softmax"]]}, {"text": "For regularization , we apply a dropout of rate 0.2 to the inputs of both encoder and decoder and to the output of the decoder prior to softmax .", "entities": [[27, 28, "MethodName", "softmax"]]}, {"text": "As in ( Bahdanau et al . , 2015 ) , we refer to this model as RNNsearch .", "entities": []}, {"text": "The ConvS2S model we trained has embeddings of dimension 256 , a 16 - layers encoder and 12 - layers decoder .", "entities": []}, {"text": "Each convolution uses 3\u00021\ufb01lters and is followed by a gated linear unit with a total of 2\u0002256channels .", "entities": [[1, 2, "MethodName", "convolution"], [9, 12, "MethodName", "gated linear unit"]]}, {"text": "Residual connections link the input of a convolutional block to its output .", "entities": []}, {"text": "We \ufb01rst trained the default architecture for this dataset as suggested in FairSeq ( Gehring et al . , 2017b ) , which has only 4 layers in the encoder and 3 in the decoder , but achieved better results with the deeper version described above .", "entities": []}, {"text": "The model is trained with MLE using Nesterov accelerated gradient with a momentum of 0.99 and an initial learning rate of 0.25 decaying by a factor of 0.1 every epoch .", "entities": [[7, 10, "MethodName", "Nesterov accelerated gradient"], [18, 20, "HyperparameterName", "learning rate"]]}, {"text": "ConvS2S is also regularized with a dropout rate of 0.2 .", "entities": []}, {"text": "For the transformer model , use the settings of ( Vaswani et al . , 2017 ) .", "entities": []}, {"text": "We use token embeddings of dimension 512 , and the encoder and decoder have 6 layers and 8 attention heads .", "entities": []}, {"text": "For the inner layer in the per - position feed - forawrd network we usedff= 2048 .", "entities": [[15, 16, "DatasetName", "2048"]]}, {"text": "For MLE training we use Adam ( \f 1= 0:9 ; \f 2=", "entities": [[5, 6, "MethodName", "Adam"]]}, {"text": "0:98;\u000f= 1e\u00008)(Kingma and Ba , 2015 ) , and a learning rate starting from 1e\u00005that is increased during 4,000 warm - up steps then used a learning rate of 5e\u00004that follows an inverse - square - root schedule afterwards ( Vaswani et al . , 2017 ) .", "entities": [[10, 12, "HyperparameterName", "learning rate"], [26, 28, "HyperparameterName", "learning rate"]]}, {"text": "Similar to previous models we set the dropout rate to 0.2 .", "entities": []}, {"text": "4.2 Experimental results Architecture evaluation .", "entities": []}, {"text": "In this section we explore the impact of several parameters of our", "entities": []}, {"text": "102(a)L= 20 ; g= 32 ( b)L= 20 ; d= 128   ( c)d= 128 ; g= 32 Figure 4 : Impact of token embedding size , number of layers ( L ) , and growth rate ( g ) .", "entities": [[27, 30, "HyperparameterName", "number of layers"]]}, {"text": "model : the token embedding dimension , depth , growth rate and \ufb01lter sizes .", "entities": [[4, 6, "HyperparameterName", "embedding dimension"]]}, {"text": "We also evaluate different aggregation mechanisms across the source dimension : max - pooling , average - pooling , and attention .", "entities": []}, {"text": "In each chosen setting , we train \ufb01ve models with different initializations and report the mean and standard deviation of the BLEU scores .", "entities": [[21, 22, "MetricName", "BLEU"]]}, {"text": "We also state the number of parameters of each model and the computational cost of training , estimated in a similar way as Vaswani et al .", "entities": [[4, 7, "HyperparameterName", "number of parameters"]]}, {"text": "( 2017 ) , based on the wall clock time of training and the GPU single precision specs .", "entities": []}, {"text": "In Table 1 we see that using max - pooling instead average - pooling across the source dimension increases the performance with around 2 BLEU points .", "entities": [[24, 25, "MetricName", "BLEU"]]}, {"text": "Scaling the average representation withp jsjEq .", "entities": []}, {"text": "( 3 ) helped improving the performance but it is still largely outperformed by the max - pooling .", "entities": []}, {"text": "Adding gated linear units on top of each convolutional layer does not improve the BLEU scores , but increases the variance due to the additional parameters .", "entities": [[14, 15, "MetricName", "BLEU"]]}, {"text": "Stand - alone selfattention i.e. weighted average - pooling is slightly better than uniform average - pooling but it is still outperformed by max - pooling .", "entities": []}, {"text": "Concatenating the max - pooled features ( Eq . ( 2 ) ) with the representation obtained with self - attention ( Eq . ( 9 ) ) leads to a small but signi\ufb01cant increase in performance , from 33.70 to 33.81 .", "entities": []}, {"text": "In the remainder of our experiments we only use max - pooling for simplicity , unless stated otherwise .", "entities": []}, {"text": "In Figure 4 we consider the effect of the token embedding size , the growth rate of the network , and its depth .", "entities": []}, {"text": "The token embedding size togetherwith the growth rate gcontrol the number of features that are passed though the pooling operator along the source dimension , and that can be used used for token prediction .", "entities": []}, {"text": "Using the same embedding sized = dt = dson both source and target , the total number of features for token prediction produced by the network is fL= 2d+gL. In Figure 4 we see that for token embedding sizes between 128 to 256 lead to BLEU scores vary between 33.5 and 34 .", "entities": [[45, 46, "MetricName", "BLEU"]]}, {"text": "Smaller embedding sizes quickly degrade the performance to 32.2 for embeddings of size 64 .", "entities": []}, {"text": "The growth rate ( g ) has an important impact on performance , increasing it from 8 to 32 increases the BLEU scrore by more than 2.5 point .", "entities": [[21, 22, "MetricName", "BLEU"]]}, {"text": "Beyond g= 32 performance saturates and we observe only a small improvement .", "entities": []}, {"text": "For a good trade - off between performance and computational cost we choose g= 32 for the remaining experiments .", "entities": []}, {"text": "The depth of the network also has an important impact on performance , increasing the BLEU score by about 2 points when increasing the depth from 8 to 24 layers .", "entities": [[15, 17, "MetricName", "BLEU score"]]}, {"text": "Beyond this point performance drops due to over-\ufb01tting , which means we should either increase the dropout rate or add another level of regularization before considering deeper networks .", "entities": []}, {"text": "The receptive \ufb01eld of our model is controlled by its depth and the \ufb01lter size .", "entities": []}, {"text": "In Table 2 , we note that narrower receptive \ufb01elds are better than larger ones with less layers at equivalent complextities e.g. comparing ( k= 3;L= 20 ) to ( k= 5;L= 12 ) , and ( k= 5;L= 16 ) with ( k=7;L=12 ) .", "entities": []}, {"text": "Comparison to the state of the art .", "entities": []}, {"text": "We compare our results to the state of the art in Ta-", "entities": []}, {"text": "103k L BLEU Flops \u0002105#params 3 16 32.99 \u00060.08 2.47 4.32 M 3 20 33.18 \u00060.19 3.03 4.92 M 5 8 31.79 \u00060.09 0.63 3.88 M 5 12 32.87 \u00060.07 2.61 4.59 M 5 16 33.34 \u00060.12 3.55 5.37 M 5 20 33.62 \u00060.07 3.01 6.23 M 5 24 33.70 \u00060.06 3.44 7.18 M 5 28 33.46 \u00060.23 5.35 8.21 M 7 12 32.58 \u00060.12 2.76 5.76 M Table 2 : Performance of our model ( g= 32;ds= dt= 128 ) for different \ufb01lter sizes kand", "entities": [[2, 3, "MetricName", "BLEU"]]}, {"text": "depthsL and \ufb01lter sizes k. ble 3 for both directions German - English ( De - En ) and English - German ( En - De ) .", "entities": []}, {"text": "We refer to our model as Pervasive Attention .", "entities": []}, {"text": "Unless stated otherwise , the parameters of all models are trained using maximum likelihood estimation ( MLE ) .", "entities": []}, {"text": "For some models we additionally report results obtained with sequence level estimation ( SLE , e.g. using reinforcement learning approaches ) , typically aiming directly to optimize the BLEU measure rather than the likelihood of correct translation .", "entities": [[28, 29, "MetricName", "BLEU"]]}, {"text": "First of all we \ufb01nd that all results obtained using byte - pair encodings ( BPE ) are superior to wordbased results .", "entities": [[15, 16, "MethodName", "BPE"]]}, {"text": "Our model has about the same number of parameters as RNNsearch , yet improves performance by almost 3 BLEU points .", "entities": [[6, 9, "HyperparameterName", "number of parameters"], [18, 19, "MetricName", "BLEU"]]}, {"text": "It is also better than the recent work of Deng et al .", "entities": [[9, 12, "DatasetName", "Deng et al"]]}, {"text": "( 2018 ) on recurrent architectures with variational attention .", "entities": []}, {"text": "Our model outperforms both the recent transformer approach of Vaswani et al .", "entities": []}, {"text": "( 2017 ) and the convolutional model of Gehring et", "entities": []}, {"text": "al . ( 2017b ) in both translation directions , while having about 3 to 8 times fewer parameters .", "entities": []}, {"text": "Our model has an equivalent training cost to the transformer ( as implemented in fairseq ) while the convs2s implementation is well optimized with fast running 1dconvolutions leading to shorter training times .", "entities": []}, {"text": "Performance across sequence lengths .", "entities": []}, {"text": "In Figure 5 we consider translation quality as a function of sentence length , and compare our model to RNNsearch , ConvS2S and Transformer .", "entities": [[23, 24, "MethodName", "Transformer"]]}, {"text": "Our model gives the best results across all sentence lengths , except for the longest ones where ConvS2S and Transformer are better .", "entities": [[19, 20, "MethodName", "Transformer"]]}, {"text": "Overall , our model combines the strong performance of RNNsearch on short sentences with good perforFigure 5 : BLEU scores across sentence lengths .", "entities": [[18, 19, "MetricName", "BLEU"]]}, {"text": "mance of ConvS2S and Transformer on longer ones .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "Implicit sentence alignments .", "entities": []}, {"text": "Following the method described in Section 3 , we illustrate in Figure 6 the implicit sentence alignments the maxpooling operator produces in our model .", "entities": []}, {"text": "For reference we also show the alignment produced by our model using self - attention .", "entities": []}, {"text": "We see that with both max - pooling and attention qualitatively similar implicit sentence alignments emerge .", "entities": []}, {"text": "Notice in the \ufb01rst example how the max - pool model , when writing I \u2019ve been working , looks at arbeite but also at seitwhich indicates the past tense of the former .", "entities": []}, {"text": "Also notice some cases of non - monotonic alignment .", "entities": []}, {"text": "In the \ufb01rst example for some time occurs at the end of the English sentence , but seit einiger zeit appears earlier in the German source .", "entities": []}, {"text": "For the second example there is non - monotonic alignment around the negation at the start of the sentence .", "entities": []}, {"text": "The \ufb01rst example illustrates the ability of the model to translate proper names by breaking them down into BPE units .", "entities": [[18, 19, "MethodName", "BPE"]]}, {"text": "In the second example the German word Karriereweg is broken into the four BPE units karri , er , e , weg .", "entities": [[13, 14, "MethodName", "BPE"]]}, {"text": "The \ufb01rst and the fourth are mainly used to produce the English a carreer , while for the subsequent path the model looks at weg .", "entities": []}, {"text": "Finally , we can observe an interesting pattern in the alignment map for several phrases across the three examples .", "entities": []}, {"text": "A rough lower triangular pattern is observed for the English phrases for some time , and it \u2019s fantastic , and it \u2019s not , a little step , andin that direction .", "entities": []}, {"text": "In all these cases the phrase seems to be decoded as a unit , where features are \ufb01rst taken across the entire corresponding source", "entities": []}, {"text": "104 ( a ) Max - pooling   ( b ) Self - attention ( c ) Max - pooling   ( d ) Self - attention ( e ) Max - pooling   ( f ) Self - attention Figure 6 : Implicit BPE token - level alignments produced by our Pervasive Attention model .", "entities": [[44, 45, "MethodName", "BPE"]]}, {"text": "For the maxpooling aggregation we visualize \u000b obtained with Eq .", "entities": []}, {"text": "( 7 ) and for self - attention the weights \u001aof Eq .", "entities": []}, {"text": "( 8) .", "entities": []}, {"text": "105Word - based De - EnFlops ( \u0002105 ) # prms En - De # prms Conv - LSTM ( MLE ) ( Bahdanau et al . , 2017 ) 27.56 Bi - GRU ( MLE+SLE ) ( Bahdanau et al . , 2017 ) 28.53 Conv - LSTM ( deep+pos ) ( Gehring et al . , 2017a ) 30.4 NPMT + language model ( Huang et al . , 2018 ) 30.08 25.36 BPE - based RNNsearch * ( Bahdanau et al . , 2015 ) 31.02 1.79 6 M 25.92 7 M Varational attention ( Deng et al . , 2018 ) 33.10 Transformer * * ( Vaswani et al . , 2017 ) 32.83 3.53 59 M 27.68 61 M ConvS2S * * ( MLE ) ( Gehring et al . , 2017b ) 32.31 1.35 21 M 26.73 22 M ConvS2S ( MLE+SLE ) ( Edunov et al . , 2018 ) 32.84 Pervasive Attention ( this paper ) 33.81 \u00060.03 3.51 7 M 27.77\u00060.1 7 M Table 3 : Comparison to state - of - the art results on IWSLT German - English translation .", "entities": [[18, 19, "MethodName", "LSTM"], [33, 34, "MethodName", "GRU"], [48, 49, "MethodName", "LSTM"], [75, 76, "MethodName", "BPE"], [98, 101, "DatasetName", "Deng et al"], [106, 107, "MethodName", "Transformer"]]}, {"text": "( * ): results obtained using our implementation .", "entities": []}, {"text": "( * * ): results obtained using FairSeq ( Gehring et al . , 2017b ) .", "entities": []}, {"text": "phrase , and progressively from the part of the source phrase that remains to be decoded .", "entities": []}, {"text": "5 Conclusion We presented a novel neural machine translation architecture that departs from the encoder - decoder paradigm .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "Our model jointly encodes the source and target sequence into a deep feature hierarchy in which the source tokens are embedded in the context of a partial target sequence .", "entities": []}, {"text": "Max - pooling over this joint - encoding along the source dimension is used to map the features to a prediction for the next target token .", "entities": []}, {"text": "The model is implemented as 2D CNN based on DenseNet , with masked convolutions to ensure a proper autoregressive factorization of the conditional probabilities .", "entities": [[9, 10, "MethodName", "DenseNet"]]}, {"text": "Since each layer of our model re - encodes the input tokens in the context of the target sequence generated so far , the model has attention - like properties in every layer of the network by construction .", "entities": []}, {"text": "Adding an explicit self - attention module therefore has a very limited , but positive , effect .", "entities": []}, {"text": "Nevertheless , the max - pooling operator in our model generates implicit sentence alignments that are qualitatively similar to the ones generated by attention mechanisms .", "entities": []}, {"text": "We evaluate our model on the IWSLT\u201914 dataset , translation German to English and vice - versa .", "entities": []}, {"text": "We obtain excellent BLEU scores that compare favorably with the state of the art , while using a conceptually simpler model with fewer parameters .", "entities": [[3, 4, "MetricName", "BLEU"]]}, {"text": "We hope that our alternative joint source - target encoding sparks interest in other alternatives to the encoder - decoder model .", "entities": []}, {"text": "In the future , we plan toexplore hybrid approaches in which the input to our joint encoding model is not provided by tokenembedding vectors , but the output of 1D source and target embedding networks , e.g. ( bi-)LSTM or 1D convolutional .", "entities": []}, {"text": "We also want to explore how our model can be used to translate across multiple language pairs .", "entities": []}, {"text": "Our PyTorch - based implementation is available at https://github.com/elbayadm/ attn2d .", "entities": []}, {"text": "References D. Bahdanau , P. Brakel , K. Xu , A. Goyal , R. Lowe , J. Pineau , A. Courville , and Y .", "entities": []}, {"text": "Bengio .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "An actor - critic algorithm for sequence prediction .", "entities": []}, {"text": "In ICLR .", "entities": []}, {"text": "D. Bahdanau , K. Cho , and Y .", "entities": []}, {"text": "Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In ICLR .", "entities": []}, {"text": "N. Kalchbrenner P. Blunsom .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Recurrent continuous translation models .", "entities": []}, {"text": "In ACL .", "entities": []}, {"text": "M. Cettolo , J. Niehues , S. St\u00fcker , L. Bentivogli , and M. Federico .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Report on the 11th IWSLT evaluation campaign .", "entities": []}, {"text": "In IWSLT .", "entities": []}, {"text": "K. Cho , B. van Merrienboer , \u00c7. G\u00fcl\u00e7ehre , D. Bahdanau , F. Bougares , H. Schwenk , and Y .", "entities": []}, {"text": "Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Learning phrase representations using RNN encoder - decoder for statistical machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "InEMNLP .", "entities": []}, {"text": "R. Collobert and J. Weston .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "A uni\ufb01ed architecture for natural language processing :", "entities": []}, {"text": "Deep neural networks with multitask learning .", "entities": []}, {"text": "In ICML .", "entities": []}, {"text": "106Y .", "entities": []}, {"text": "Dauphin , A. Fan , M. Auli , and D. Grangier . 2017 .", "entities": []}, {"text": "Language modeling with gated convolutional networks .", "entities": []}, {"text": "In ICML .", "entities": []}, {"text": "Y .", "entities": []}, {"text": "Deng , Y .", "entities": []}, {"text": "Kim , J. Chiu , D. Guo , and A. Rush .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Latent alignment and variational attention .", "entities": []}, {"text": "arXiv preprint arXiv:1807.03756 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "S. Edunov , M. Ott , M. Auli , D. Grangier , and M. Ranzato .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Classical structured prediction losses for sequence to sequence learning .", "entities": [[1, 3, "TaskName", "structured prediction"], [5, 8, "MethodName", "sequence to sequence"]]}, {"text": "In NAACL .", "entities": []}, {"text": "J. Gehring , M. Auli , D. Grangier , and Y .", "entities": []}, {"text": "Dauphin . 2017a .", "entities": []}, {"text": "A convolutional encoder model for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In ACL .", "entities": []}, {"text": "J. Gehring , M. Auli , D. Grangier , D. Yarats , and Y .", "entities": []}, {"text": "Dauphin . 2017b .", "entities": []}, {"text": "Convolutional sequence to sequence learning .", "entities": [[1, 4, "MethodName", "sequence to sequence"]]}, {"text": "In ICML .", "entities": []}, {"text": "A. Graves .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Sequence transduction with recurrent neural networks .", "entities": []}, {"text": "arXiv preprint arXiv:1211.3711 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "S. Hochreiter and J. Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long shortterm memory .", "entities": []}, {"text": "Neural Computation , 9(8):1735 \u2013 1780 .", "entities": []}, {"text": "G. Huang , Z. Liu , L. van der Maaten , and K. Weinberger . 2017 .", "entities": []}, {"text": "Densely connected convolutional networks .", "entities": []}, {"text": "In CVPR .", "entities": []}, {"text": "P. Huang , C. Wang , S. Huang , D. Zhou , and L. Deng . 2018 .", "entities": []}, {"text": "Towards neural phrase - based machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In ICLR .", "entities": []}, {"text": "S. Ioffe and C. Szegedy .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Batch normalization : Accelerating deep network training by reducing internal covariate shift .", "entities": [[0, 2, "MethodName", "Batch normalization"]]}, {"text": "In ICML .", "entities": []}, {"text": "S. Jean , K. Cho , R. Memisevic , and Y . Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "On using very large target vocabulary for neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In ACL .", "entities": []}, {"text": "N. Kalchbrenner , I. Danihelka , and A. Graves .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "Grid long short - term memory .", "entities": [[1, 6, "MethodName", "long short - term memory"]]}, {"text": "In ICLR .", "entities": []}, {"text": "N. Kalchbrenner , L. Espeholt , K. Simonyan , A. van den Oord , A. Graves , and K. Kavukcuoglu .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "Neural machine translation in linear time .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "arXiv , arXiv:1610.10099 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "N. Kalchbrenner , E. Grefenstette , and P. Blunsom .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A convolutional neural network for modelling sentences .", "entities": []}, {"text": "In ACL .", "entities": []}, {"text": "Y .", "entities": []}, {"text": "Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classi\ufb01cation .", "entities": []}, {"text": "In ACL .", "entities": []}, {"text": "D. Kingma and J. Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In ICLR .", "entities": []}, {"text": "P. Koehn , H. Hoang , A. Birch , C. Callison - Burch , M. Federico , N. Bertoldi , B. Cowan , W. Shen , C. Moran , R. Zens , C. Dyer , O. Bojar , A. Constantin , and E. Herbst .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Moses : Open source toolkit for statistical machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In ACL.Y .", "entities": []}, {"text": "LeCun , Y .", "entities": []}, {"text": "Bengio , and G. Hinton .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Deep learning .", "entities": []}, {"text": "Nature , 52:436\u2013444 .", "entities": []}, {"text": "Z. Lin , M. Feng , C. dos Santos , M. Yu , B. Xiang , B. Zhou , and Y .", "entities": []}, {"text": "Bengio .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A structured selfattentive sentence embedding .", "entities": [[3, 5, "TaskName", "sentence embedding"]]}, {"text": "In ICLR .", "entities": []}, {"text": "T. Luong , H. Pham , and C. Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Effective approaches to attention - based neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In EMNLP .", "entities": []}, {"text": "F. Meng , Z. Lu , M. Wang , H. Li , W. Jiang , and Q. Liu . 2015 .", "entities": []}, {"text": "Encoding source language with convolutional neural network for machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In ACL .", "entities": []}, {"text": "V .", "entities": []}, {"text": "Nair and G. Hinton . 2010 .", "entities": []}, {"text": "Recti\ufb01ed linear units improve restricted Boltzmann machines .", "entities": []}, {"text": "In ICML .", "entities": []}, {"text": "A. van den Oord , S. Dieleman , H. Zen , K. Simonyan , O. Vinyals , A. Graves , N. Kalchbrenner , A. Senior , and K. Kavukcuoglu .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "Wavenet : a generative model for raw audio .", "entities": [[0, 1, "MethodName", "Wavenet"]]}, {"text": "In ISCA Speech Syntesis Workshop .", "entities": []}, {"text": "A. van den Oord , N. Kalchbrenner , and K. Kavukcuoglu .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "Pixel recurrent neural networks .", "entities": []}, {"text": "In ICML .", "entities": []}, {"text": "A. van den Oord , N. Kalchbrenner , O. Vinyals , L. Espeholt , A. Graves , and K. Kavukcuoglu .", "entities": []}, {"text": "2016c .", "entities": []}, {"text": "Conditional image generation with PixelCNN decoders .", "entities": [[0, 3, "TaskName", "Conditional image generation"], [4, 5, "MethodName", "PixelCNN"]]}, {"text": "InNIPS .", "entities": []}, {"text": "K. Papineni , S. Roukos , T. Ward , and W.-J. Zhu . 2002 .", "entities": []}, {"text": "BLEU : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "BLEU"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In ACL .", "entities": []}, {"text": "A. Parikh , O. T\u00e4ckstr\u00f6m , D. Das , and J. Uszkoreit .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A decomposable attention model for natural language inference .", "entities": [[5, 8, "TaskName", "natural language inference"]]}, {"text": "In EMNLP .", "entities": []}, {"text": "A. Paszke , S. Gross , S. Chintala , G. Chanan , E. Yang , Z. DeVito , Z. Lin , A. Desmaison L. Antiga , and A. Lerer .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Automatic differentiation in pytorch .", "entities": []}, {"text": "In NIPS - W .", "entities": []}, {"text": "M. Ranzato , S. Chopra , M. Auli , and W. Zaremba .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Sequence level training with recurrent neural networks .", "entities": []}, {"text": "In ICLR .", "entities": []}, {"text": "S. Reed , A. van den Oord , N. Kalchbrenner , S. G\u00f3mez Colmenarejo , Z. Wang , D. Belov , and N. de Freitas .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Parallel multiscale autoregressive density estimation .", "entities": [[3, 5, "TaskName", "density estimation"]]}, {"text": "In ICML .", "entities": []}, {"text": "T. Salimans , A. Karpathy , X. Chen , and D. Kingma . 2017 .", "entities": []}, {"text": "PixelCNN++ : Improving the PixelCNN with discretized logistic mixture likelihood and other modi\ufb01cations .", "entities": [[4, 5, "MethodName", "PixelCNN"]]}, {"text": "In ICLR .", "entities": []}, {"text": "M. Schuster and K. Paliwal .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Bidirectional recurrent neural networks .", "entities": []}, {"text": "Signal Processing , 45(11):2673\u20132681 .", "entities": []}, {"text": "R. Sennrich , B. Haddow , and A. Birch .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In ACL .", "entities": []}, {"text": "107N. Srivastava , G. Hinton , A. Krizhevsky , I. Sutskever , and R. Salakhutdinov .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Dropout : A simple way to prevent neural networks from over\ufb01tting .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "JMLR .", "entities": []}, {"text": "I. Sutskever , O. Vinyals , and Q. Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "In NIPS .", "entities": []}, {"text": "A. Vaswani , N. Shazeer , N. Parmar , J. Uszkoreit , L. Jones , A. Gomez , L. Kaiser , and I. Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In NIPS .", "entities": []}, {"text": "C. Wang , Y .", "entities": []}, {"text": "Wang , P.-S. Huang , A. Mohamed , D. Zhou , and L. Deng . 2017 .", "entities": []}, {"text": "Sequence modeling via segmentations .", "entities": []}, {"text": "In ICML .", "entities": []}, {"text": "L. Wu , Y .", "entities": []}, {"text": "Xia , L. Zhao , F. Tian , T. Qin , J. Lai , and T.-Y .", "entities": []}, {"text": "Liu . 2017 .", "entities": []}, {"text": "Adversarial neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "arXiv , arXiv:1704.06933 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "K. Xu , J. Ba , R. Kiros , K. Cho , A. Courville , R. Salakhutdinov , R. Zemel , and Y .", "entities": []}, {"text": "Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Show , attend and tell : Neural image caption generation with visual attention .", "entities": [[11, 13, "MethodName", "visual attention"]]}, {"text": "In ICML .", "entities": []}]