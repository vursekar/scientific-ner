[{"text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 3115\u20133124 July 5 - 10 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics3115Hyperbolic Capsule Networks for Multi - Label Classi\ufb01cation Boli Chen , Xin Huang , Lin Xiao , Liping Jing Beijing Key Lab of Traf\ufb01c Data Analysis and Mining Beijing Jiaotong University , Beijing , China f18120345 , 18120367 , 17112079 , lpjing g@bjtu.edu.cn", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "Although deep neural networks are effective at extracting high - level features , classi\ufb01cation methods usually encode an input into a vector representation via simple feature aggregation operations ( e.g. pooling ) .", "entities": []}, {"text": "Such operations limit the performance .", "entities": []}, {"text": "For instance , a multi - label document may contain several concepts .", "entities": []}, {"text": "In this case , one vector can not suf\ufb01ciently capture its salient and discriminative content .", "entities": []}, {"text": "Thus , we propose Hyperbolic Capsule Networks ( H YPER CAPS ) for Multi - Label Classi\ufb01cation ( MLC ) , which have two merits .", "entities": []}, {"text": "First , hyperbolic capsules are designed to capture \ufb01ne - grained document information for each label , which has the ability to characterize complicated structures among labels and documents .", "entities": []}, {"text": "Second , Hyperbolic Dynamic Routing ( HDR ) is introduced to aggregate hyperbolic capsules in a label - aware manner , so that the label - level discriminative information can be preserved along the depth of neural networks .", "entities": []}, {"text": "To ef\ufb01ciently handle large - scale MLC datasets , we additionally present a new routing method to adaptively adjust the capsule number during routing .", "entities": []}, {"text": "Extensive experiments are conducted on four benchmark datasets .", "entities": []}, {"text": "Compared with the state - of - the - art methods , H YPERCAPS signi\ufb01cantly improves the performance of MLC especially on tail labels .", "entities": []}, {"text": "1 Introduction The main difference between Multi - Class Classi\ufb01cation ( MCC ) and Multi - Label Classi\ufb01cation ( MLC ) is that datasets in MCC have only serval mutually exclusive classes , while datasets in MLC contain much more correlated labels .", "entities": []}, {"text": "MLC allows label co - occurrence in one document , which indicates that the labels are not disjointed .", "entities": []}, {"text": "In addition , a large fraction of the labels are the infrequently occurring tail labels ( Bhatia et al . , 2015 ) , which is also referred as the power - law label distribution .", "entities": []}, {"text": "Figure 1 : The power - law label distribution of EURLEX57Kwith Y - axis on log - scale .", "entities": []}, {"text": "Division is based on average number of training instances .", "entities": []}, {"text": "Figure 1 illustrates the label distribution of EURLEX57K(Chalkidis et al . , 2019 ) .", "entities": []}, {"text": "A multi - label document usually has serval head and tail labels , and hence contain several concepts about both its head and tail labels simultaneously .", "entities": []}, {"text": "Recent works for text classi\ufb01cation , such as CNN - K IM(Kim , 2014 ) and FASTTEXT ( Joulin et al . , 2017 ) , focus on encoding a document into a \ufb01xed - length vector as the distributed document representation ( Le and Mikolov , 2014 ) .", "entities": [[16, 17, "MethodName", "FASTTEXT"]]}, {"text": "These encoding based deep learning methods use simple operations ( e.g.pooling ) to aggregate features extracted by neural networks and construct the document vector representation .", "entities": []}, {"text": "A Fully - Connected ( FC ) layer is usually applied upon the document vector to predict the probability of each label .", "entities": []}, {"text": "And each row in its weight matrix can be interpreted as a label vector representation ( Du et al . , 2019b ) .", "entities": []}, {"text": "In this way , the label probability can be predicted by computing the dot product between label and document vectors , which is proportional to the scalar projection of the label vector onto the document vector as shown in Figure 2 .", "entities": []}, {"text": "For example , label \u201d movie \u201d should have the largest scalar projection onto a document about \u201d movie \u201d .", "entities": []}, {"text": "However , even", "entities": []}, {"text": "3116 Figure 2 : Illustration of the FC layer in the encoding based methods .", "entities": []}, {"text": "the learned label representation of \u201d music \u201d can be distinguished from \u201d movie \u201d , it may also have a large scalar projection onto the document .", "entities": []}, {"text": "Moreover , multi - label documents always contain several concepts about multiple labels , such as a document about \u201d sport movie \u201d .", "entities": []}, {"text": "Whereas the document vector representation is identical to all the labels , and training instances for tail labels are inadequate compared to head labels .", "entities": []}, {"text": "The imbalance between head and tail labels makes it hard for the FClayer to make prediction , especially on tail labels .", "entities": []}, {"text": "In this case , one vector can not suf\ufb01ciently capture its salient and discriminative content .", "entities": []}, {"text": "Therefore , the performance of constructing the document vector representation via simple aggregation operations is limited for MLC .", "entities": []}, {"text": "Capsule networks ( Sabour et al . , 2017 ; Yang et al . , 2018a ) has recently proposed to use dynamic routing in place of pooling and achieved better performance for classi\ufb01cation tasks .", "entities": []}, {"text": "In fact , capsules are \ufb01ne - grained features compared to the distributed document representation , and dynamic routing is a label - aware feature aggregation procedure .", "entities": []}, {"text": "( Zhao et al . , 2019 ) improves the scalability of capsule networks for MLC .", "entities": []}, {"text": "However , they only useCNN to construct capsules , which capture local contextual information ( Wang et al . , 2016 ) .", "entities": []}, {"text": "Effectively learning the document information about multiple labels is crucial for MLC .", "entities": []}, {"text": "Thus we propose to connect CNN andRNN in parallel to capture both local and global contextual information , which would be complementary to each other .", "entities": []}, {"text": "Nevertheless , Euclidean capsules necessitate designing a non - linear squashing function .", "entities": []}, {"text": "Inspired by the hyperbolic representation learning methods which demonstrate that the hyper - bolic space has more representation capacity than the Euclidean space ( Nickel and Kiela , 2017 ; Ganea et al . , 2018a ) , Hyperbolic Capsule Networks ( HYPER CAPS ) is proposed .", "entities": [[0, 1, "DatasetName", "Inspired"], [4, 6, "TaskName", "representation learning"]]}, {"text": "Capsules are constrained in the hyperbolic space which does not require the squashing function .", "entities": []}, {"text": "Hyperbolic Dynamic Routing ( HDR ) is introduced to aggregate hyperbolic capsules in a label - aware manner .", "entities": []}, {"text": "Moreover , in order to \ufb01t the large label set of MLC and improve the scalability of HYPER CAPS , adaptive routing is presented to adjust the number of capsules participated in the routing procedure .", "entities": []}, {"text": "The main contributions of our work are therefore summarized as follows : \u2022We propose to connect CNN andRNN in parallel to simultaneously extract local and global contextual information , which would be complementary to each other .", "entities": []}, {"text": "\u2022HYPER CAPS with HDR are formulated to aggregate features in a label - aware manner , and hyperbolic capsules bene\ufb01ts from the representation capacity of the hyperbolic space .", "entities": []}, {"text": "\u2022Adaptive routing is furthermore presented to improve the scalability of HYPER CAPSand \ufb01t the large label set of MLC .", "entities": []}, {"text": "\u2022Extensive experiments on four benchmark MLC datasets demonstrate the effectiveness of HYPER CAPS , especially on tail labels .", "entities": []}, {"text": "2 Preliminaries In order to make neural networks work in the hyperbolic space , formalism of the M\u00a8obius gyrovector space is adopted ( Ganea et al . , 2018b ) .", "entities": []}, {"text": "Ann - dimensional Poincar \u00b4 e ballBnis", "entities": []}, {"text": "aRiemannian manifold de\ufb01ned asBn = fx2Rnjkxk < 1 g , with its tangent space around p2Bndenoted asTpBnand", "entities": []}, {"text": "the conformal factor as\u0015p:=2 1\u0000kpk2 .", "entities": []}, {"text": "The exponential map expp : TpBn !", "entities": []}, {"text": "Bnfor w2TpBnnf0gis consequently de\ufb01ned as expp(w )", "entities": []}, {"text": "= p\b(tanh(\u0015p 2kwk)w kwk):(1 )", "entities": []}, {"text": "To work with hyperbolic capsules , M \u00a8obius operations in the Poincar \u00b4 e ball also need to be formulated .", "entities": []}, {"text": "M\u00a8obius addition foru;v2Bnis de\ufb01ned as u\bv=(1 + 2hu;vi+kvk2)u+(1\u0000kuk2)v 1 + 2hu;vi+kuk2kvk2;(2 ) whereh\u0001;\u0001idenotes the Euclidean inner product .", "entities": []}, {"text": "3117Thus M\u00a8obius summation can be formulated as n", "entities": []}, {"text": "M i = mpi = pm\b\u0001\u0001\u0001\b pn;pi2Bn : ( 3 ) M\u00a8obius scalar multiplication fork2Rand p2Bnnf0gis de\ufb01ned as k p= tanh(ktanh\u00001(kpk))p kpk:(4 ) Andk p=0whenp=02Bn .", "entities": []}, {"text": "The de\ufb01nition of M\u00a8obius matrix - vector multiplication forM2Rm\u0002nandp2 Bnwhen Mp6=0is as follows M p= tanh(kMpk kpktanh\u00001(kpk))Mp kMpk:(5 ) AndM", "entities": []}, {"text": "p=0whenMp = 0 .", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "HDR is developed based on these operations .", "entities": []}, {"text": "3 Local and Global Hyperbolic Capsules Neural networks are generally used as effective feature extractors for text classi\ufb01cation .", "entities": []}, {"text": "Kernels ofCNN can be used to capture local n - gram contextual information at different positions of a text sequence , while hidden states of RNN can represent global long - term dependencies of the text ( Wang et al . , 2016 ) .", "entities": []}, {"text": "Hence , we propose to obtain the combination of local and global hyperbolic capsules by connecting CNN andRNN in parallel , which would be complementary to each other .", "entities": []}, {"text": "Given a text sequence of a document with T word tokens x=", "entities": []}, {"text": "[ x1;:::;x T ] , pre - trained wdimensional word embeddings ( e.g. GLOVE ( Pennington et al . , 2014 ) ) are used to compose word vector representations E=", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "[ e1;:::;eT]2RT\u0002w , upon which CNN andRNN connected in parallel are used to construct local and global hyperbolic capsules in the Poincar \u00b4 e ball .", "entities": []}, {"text": "Figure 3 illustrates the framework for H YPER CAPS .", "entities": []}, {"text": "3.1 Local Hyperbolic Capsule Layer N - gram kernels K2Rk\u0002wwith different window sizekare applied on the local region of the word representations Et : t+k\u000012Rk\u0002wto construct the local features as lt='(K\u000eEt : t+k\u00001 ) ; ( 6 ) where\u000edenotes the element - wise multiplication and'is a non - linearity ( e.g. ReLU ) .", "entities": [[51, 52, "MethodName", "ReLU"]]}, {"text": "For simplicity , the bias term is omitted .", "entities": []}, {"text": "With totallydchannels , the local hyperbolic capsules at position tcan be constructed as lt= exp0([l(1 ) t;:::;l(d ) t])2Bd : ( 7 ) Therefore , a k - gram kernel with 1stride can constructT\u0000k+1local hyperbolic capsules .", "entities": []}, {"text": "The local hyperbolic capsule set is denoted as fu1;:::;uLg .", "entities": []}, {"text": "3.2 Global Hyperbolic Capsule Layer Bidirectional GRU ( Chung et al . , 2014 ) is adopted to incorporate forward and backward global contextual information and construct the global hyperbolic capsules .", "entities": [[5, 7, "MethodName", "Bidirectional GRU"]]}, {"text": "Forward and backward hidden states at time - step tare obtained by \u0000 !", "entities": []}, {"text": "ht = GRU ( \u0000\u0000!ht\u00001;et ) ;   \u0000ht = GRU ( \u0000\u0000ht+1;et):(8 )", "entities": [[2, 3, "MethodName", "GRU"], [10, 11, "MethodName", "GRU"]]}, {"text": "Each of the total 2Thidden states can be taken as a global hyperbolic capsule using the exponential map , i.e.\u0000 ! gt= exp0(\u0000 ! ht ) , and equally for the backward capsules .", "entities": []}, {"text": "The global hyperbolic capsule set is denoted asfu1;:::;uGg .", "entities": []}, {"text": "3.3 Hyperbolic Compression Layer As discussed in ( Zhao et al . , 2019 ) , the routing procedure is computational expensive for a large number of capsules .", "entities": []}, {"text": "Compressing capsules into a smaller amount can not only relieve the computational complexity , but also merge similar capsules and remove outliers .", "entities": []}, {"text": "Therefore , hyperbolic compression layer is introduced .", "entities": []}, {"text": "Each compressed local hyperbolic capsule is calculated as a weighted M\u00a8obius summation over all the local hyperbolic capsules .", "entities": []}, {"text": "For instance , ul= M uk2fu1;:::;uLgrk uk2Bd ; ( 9 ) whererkis a learnable weight parameter .", "entities": []}, {"text": "And likewise for compressing global hyperbolic capsules .", "entities": []}, {"text": "Let setfu1;:::;uPgdenote the compressed local and global hyperbolic capsules together , which are then aggregated in a label - aware manner via HDR .", "entities": []}, {"text": "4 Hyperbolic Dynamic Routing The purpose of Hyperbolic Dynamic Routing ( HDR ) is to iteratively aggregate local and global hyperbolic capsules into label - aware hyperbolic capsules , whose activations stand for probabilities of the labels .", "entities": []}, {"text": "3118 Figure 3 : Illustration of H YPER CAPSframework .", "entities": []}, {"text": "4.1 Label - Aware Hyperbolic Capsules With the acquirement of the compressed local and global hyperbolic capsule set fu1;:::;uPg in layer ` , letfv1;:::;vQgdenote the label - aware hyperbolic capsule set in the next layer ` +1 , where Qequals to the number of labels .", "entities": []}, {"text": "Following ( Sabour et al . , 2017 ) , the compressed hyperbolic capsules are \ufb01rstly transformed into a set of prediction capsules f^ujj1 ; : : : ; ^ujjPgfor the j - th label - aware capsule , each of them is calculated by ^ujji = Wij ui2Bd ; ( 10 ) where Wijis a learnable parameter .", "entities": []}, {"text": "Thenvjis calculated as a weighted M \u00a8obius summation over all the prediction capsules by vj= M ^ujji2f^ujj1;:::;^ujjPgcij ^ujji ; ( 11 ) wherecijdenotes the coupling coef\ufb01cient that indicates the connection strength between ^ujjiand vj .", "entities": []}, {"text": "The coupling coef\ufb01cient cijis iteratively updated during the HDR procedure and computed by the routing softmax cij = exp(bij)P kexp(bik ) ; ( 12 ) where the logits bijare the log prior probabilities between capsule iandj , which are initialized as 0 .", "entities": [[15, 16, "MethodName", "softmax"], [41, 42, "DatasetName", "0"]]}, {"text": "Once the label - aware hyperbolic capsules are produced , each bijis then updated by bij = bij+K(dB(vj;^ujji ) ) ; ( 13)where dB(\u0001;\u0001)denotes the Poincar \u00b4 e distance , which can be written as dB(u;v ) = cosh\u00001(1 +1 2\u0015u\u0015vku\u0000vk2):(14 )", "entities": []}, {"text": "AndKis aEpanechnikov kernel function ( Wand and Jones , 1994 ) with K= (", "entities": []}, {"text": "\u0000x ; x2[0 ;", "entities": []}, {"text": ") 0 ; x\u0015", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "( 15 ) where", "entities": []}, {"text": "is the maximum Poincar \u00b4 e distance between two points in the Poincar \u00b4 e ball , which is dB(p;0 ) withkpk= 1\u0000\u000f(\u000f= 10\u00005 ) to avoid numerical errors .", "entities": []}, {"text": "HDR is summarized in Algorithm 1 .", "entities": []}, {"text": "Different from the routing procedure described in ( Sabour et al . , 2017 ) , HDR does not require the squashing function since all the hyperbolic capsules are constrained in the Poincar \u00b4 e ball .", "entities": []}, {"text": "4.2 Adaptive Routing The large amount of labels in MLC is one major source of the computational complexity for the routing procedure .", "entities": []}, {"text": "Since most of the labels are unrelated to a document , calculating the label - aware hyperbolic capsules for all the unrelated labels is redundant .", "entities": []}, {"text": "Therefore , encoding based adaptive routing layer is used to ef\ufb01ciently decide the candidate labels for the document .", "entities": []}, {"text": "The adaptive routing layer produces the candidate probability of each label by c=\u001b(Wc1 TX ei2Eei+bc ) ; ( 16 )", "entities": []}, {"text": "3119Table 1 : Statistics of the datasets : Ntrain andNtestare the numbers of training and test instances , Wtrain and Wtestare their average word numbers , Lis the average label number per instance , Iis the average number of training instances per label , # Hand#Tare the numbers of head and tail labels , HandTare their average number of training instances respectively .", "entities": []}, {"text": "Dataset NtrainNtestWtrainWtestL", "entities": []}, {"text": "I # H H # T T AAPD 49,356 6,484 163.34 164.14 2.41 2,199.03 17 5,002.23 37 911.08 RCV1 23,149 781,265 259.47 269.23 3.21 715.50 27 2,209.44 76 184.76 ZHIHU 2,699,969 299,997 38.14 35.56 2.32 3,165.92 442 7,144.31 1,557 2,036.54 EUR - L EX57 K 51,000 6,000 726.46 725.37 5.06 53.45 711 273.72 3,560 9.46 Algorithm 1 Hyperbolic Dynamic Routing 1 : procedure HDR ( ^ujji , r , ` ) 2 : Initialize8i;j : bij 0 3 : forriterations do 4 : for all capsule iin layer`and capsule j in layer`+ 1 : cij softmax(bij ) .Eq .", "entities": [[18, 19, "DatasetName", "RCV1"], [76, 77, "DatasetName", "0"]]}, {"text": "12 5 : for all capsule jin layer ( ` + 1 ): vj Micij ^ujji 6 : for all capsule iin layer`and capsule j in layer`+ 1 : bij bij+K(dB(vj;^ujji ) ) 7 : returnvj where\u001bdenotes the Sigmoid function .", "entities": []}, {"text": "Wcand the bias bcare learnable parameters updated by minimizing the binary cross - entropy loss ( Liu et al . , 2017 ) Lc=\u0000QP j=1\u0000 yjlog(cj )", "entities": [[14, 15, "MetricName", "loss"]]}, {"text": "+ ( 1\u0000yj)log(1\u0000cj)\u0001 ; ( 17 ) wherecj2[0;1]is thej - th element in candyj2 f0;1gdenotes the ground truth about label j. The adaptive routing layer selects the candidate labels during test .", "entities": []}, {"text": "Label - aware hyperbolic capsules are then constructed via HDR to predict probabilities of these candidate labels .", "entities": []}, {"text": "During the training process , negative sampling is used to improve the the scalability of HYPER CAPS .", "entities": []}, {"text": "LetN+denote the true label set and N\u0000denote the set of randomly selected negative labels , the loss function is derived as Lf=\u0000\u0000P j2N+log(aj ) + P j2N\u0000log(1\u0000aj)\u0001 ; ( 18 ) whereaj=\u001b(dB(vj;0))is activations of the j - th label - aware capsules , which is proportional to the distance from the origin of the Poincar \u00b4 e ball.5 Experiments The proposed HYPER CAPS is evaluated on four benchmark datasets with various label number from 54 to 4271 .", "entities": [[16, 17, "MetricName", "loss"]]}, {"text": "We compare with the state - of - the - art methods in terms of widely used metrics .", "entities": []}, {"text": "Performance on tail labels is also compared to demonstrate the superiority of HYPER CAPS forMLC .", "entities": []}, {"text": "An ablation test is also carried out to analyse the contribution of each component of H YPER CAPS .", "entities": []}, {"text": "5.1 Experimental Setup Datasets Experiments are carried out on four publicly available MLC datasets , including the small - scale AAPD ( Yang et al . , 2018b ) and RCV1 ( Lewis et al . , 2004 ) , the large - scale ZHIHU1and EUR - L EX57K(Chalkidis et al . , 2019 ) .", "entities": [[30, 31, "DatasetName", "RCV1"]]}, {"text": "Labels are divided into head and tail sets according to their number of training instances , i.e.labels have less than average number of training instances are divided into the tail label set .", "entities": []}, {"text": "Their statistics can be found in Table 1 .", "entities": []}, {"text": "Evaluation metrics We use the rank - based evaluation metrics which have been widely adopted for MLC tasks ( Bhatia et al . , 2015 ; Liu et al . , 2017 ) , i.e. Precision@k ( P@k for short ) and nDCG@k , which are respectively de\ufb01ned as P@k = 1 kX j2rank k(a)yj ; ( 19 ) nDCG@k = P j2rank k(a)yj = log(j+ 1 ) Pmin(k;kyk0 ) j=1 1 = log(j+ 1);(20 ) whereyj2f0;1gdenotes the the ground truth about labelj , rank k(a)denotes the indices of the candidate label - aware hyperbolic capsules with k largest activations in descending order , and kyk0 is the true label number for the document instance .", "entities": []}, {"text": "1https://www.biendata.com/competition/ zhihu / data/ .", "entities": []}, {"text": "3120Table 2 : Results on all the labels in P@k andnDCG@k , bold face indicates the best of each line .", "entities": []}, {"text": "Dataset Metric F ASTTEXT SLEEC XML - CNN SGM R EGGNN NLP - C AP HYPER CAPS P@1 75.33 75.85 76.31 77.90 79.92 81.75 85.37 P@3 53.83 54.36 54.41 55.76 57.31 59.63 61.89 AAPD P@5 37.57 37.89 37.83 38.58 39.50 41.97 42.51 nDCG@3 71.22 71.54 72.12 73.73 75.77 78.40 81.64 nDCG@5 75.78 75.98 76.39 78.05 80.03 83.70 85.87 P@1 95.40 95.35 96.86 95.37 96.53 97.05 97.10 P@3 79.96 79.51 81.11 81.36 81.69 81.27 82.04 RCV1 P@5 55.64 55.06 56.07 53.06 56.23 56.33 57.06 nDCG@3 90.95 90.45 92.22 91.76 92.28 92.47 93.03 nDCG@5 91.68 90.97 92.63 90.69 92.67 93.11 93.66 P@1", "entities": [[14, 15, "DatasetName", "AP"], [17, 18, "MetricName", "P@1"], [58, 59, "MetricName", "P@1"], [74, 75, "DatasetName", "RCV1"], [99, 100, "MetricName", "P@1"]]}, {"text": "49.40 50.22 49.68 50.32 50.67 53.73 56.50 P@3 31.50 32.21 32.27 31.83 32.43 33.83 35.77 ZHIHU P@5 23.23 23.81 24.17 23.95 24.23 25.10 26.27 nDCG@3 46.52 47.57 46.65 46.90 47.97 48.89 50.61 nDCG@5 49.16 50.34 49.60 50.47 50.70 51.19 52.89 P@1 86.18 89.43 85.33 89.11 90.46 90.83 91.42 P@3 73.18 76.73 74.40 78.03 79.29 80.72 82.18 EUR - L EX57KP@5 60.15 63.59 61.21 65.02 65.83 69.14 70.53 nDCG@3 77.42 80.98 78.59 82.30 83.45 84.13 86.05 nDCG@5 73.21 76.96 74.36 78.50 79.40 81.91 83.28", "entities": [[40, 41, "MetricName", "P@1"]]}, {"text": "The \ufb01nal results are averaged over all the test instances .", "entities": []}, {"text": "Baselines To demonstrate the effectiveness of HYPER CAPSon the benchmark datasets , six comparative text classi\ufb01cation methods are chosen as the baselines .", "entities": [[8, 10, "DatasetName", "the benchmark"]]}, {"text": "FASTTEXT ( Joulin et al . , 2017 ) is a representative encoding - based method which use average pooling to construct document representations and MLP to make the predictions .", "entities": [[0, 1, "MethodName", "FASTTEXT"], [18, 20, "MethodName", "average pooling"], [25, 26, "DatasetName", "MLP"]]}, {"text": "SLEEC ( Bhatia et al . , 2015 ) is a typical label - embedding method for MLC , which uses k - nearest neighbors search to predict the labels .", "entities": [[21, 25, "MethodName", "k - nearest neighbors"]]}, {"text": "XML - CNN ( Liu et al . , 2017 ) employs CNN as local n - gram feature extractors and a dynamic pooling technique as aggregation method .", "entities": []}, {"text": "SGM ( Yang et al . , 2018b ) applies the seq2seq model with attention mechanism , which takes the global contextual information .", "entities": [[11, 12, "MethodName", "seq2seq"]]}, {"text": "REGGNN ( Xu et al . , 2019 ) uses a combination of CNN and LSTM with a dynamic gate that controls the information from these two parts .", "entities": [[15, 16, "MethodName", "LSTM"]]}, {"text": "NLP - C AP(Zhao et al . , 2019 ) is a capsule - based approach for MLC , which reformulates the routing algorithm .", "entities": []}, {"text": "NLPCAPuse only CNN to construct capsules , and it applies the squashing function onto capsules .", "entities": []}, {"text": "Implementation Details", "entities": []}, {"text": "All the words are converted to lower case and padding is used to handle the various lengths of the text sequences .", "entities": []}, {"text": "Maximum length of AAPD , RCV1 andEUR - L EX57 K is set to 500 , while maximum length of ZHIHU is50 .", "entities": [[5, 6, "DatasetName", "RCV1"]]}, {"text": "To compose the word vector representations , pre - trained 300 - dimensional GLOVE ( Pennington et al . , 2014 ) word embeddings are used for AAPD , RCV1 andEUR - L EX57 K , while ZHIHU uses its speci\ufb01ed 256 - dimensional word embeddings .", "entities": [[22, 24, "TaskName", "word embeddings"], [29, 30, "DatasetName", "RCV1"], [44, 46, "TaskName", "word embeddings"]]}, {"text": "The dimension of the Poincar \u00b4 e ball is set to 32 with a radius 1\u0000\u000f(\u000f= 10\u00005 ) to avoid numerical errors .", "entities": []}, {"text": "Multiple one - dimensional convolutional kernels ( with window sizes of 2 , 4 , 8) are applied in the local hyperbolic capsule layer .", "entities": []}, {"text": "The number of compressed local and global hyperbolic capsules is 128 .", "entities": []}, {"text": "Adaptive routing layer is not applied on the small - scale datasets AAPD andRCV1 .", "entities": []}, {"text": "The maximum candidate label number is set to 200 for the large - scale datasets ZHIHU andEUR - L EX57K. For the baselines , hyperparameters recommended by their authors are adopted .", "entities": []}, {"text": "5.2 Experimental Results The proposed HYPER CAPS is evaluated on the four benchmark datasets by comparing with the six baselines in terms of P@k andnDCG@k with k= 1;3;5 .", "entities": []}, {"text": "Results on all the labels averaged over the test instances are shown in Table 2 .", "entities": []}, {"text": "nDCG@1 is omitted since it gives the same value as P@1 .", "entities": [[10, 11, "MetricName", "P@1"]]}, {"text": "It is notable that HYPER CAPSobtains competitive results on the four datasets .", "entities": []}, {"text": "The encoding - based FASTTEXT is generally inferior to the other baselines as it applies the average pooling on word vector representations , which ig-", "entities": [[4, 5, "MethodName", "FASTTEXT"], [16, 18, "MethodName", "average pooling"]]}, {"text": "3121 ( a ) AAPD   ( b ) RCV1   ( c ) Z HIHU   ( d ) EUR - L EX57 K Figure 4 : Results on tail labels in nDCG@k .", "entities": [[9, 10, "DatasetName", "RCV1"]]}, {"text": "Figure 5 : Results of ablation test on EUR - L EX57Kin P@k .", "entities": []}, {"text": "L denotes local capsules , G denotes global capsules , H denotes HDR .", "entities": []}, {"text": "nores word order for the construction of document representations .", "entities": []}, {"text": "The typical MLC method SLEEC takes advantage of label correlations by embedding the label co - occurrence graph .", "entities": []}, {"text": "However , SLEEC uses TF - IDF vectors to represent documents , thus word order is also ignored .", "entities": []}, {"text": "XML - CNN uses a dynamic pooling technique to aggregate the local contextual features extracted by CNN , while SGM uses attention mechanism to aggregate the global contextual features extracted by LSTM .REGGNN is generally superior to both of them as it combines the local and global contextual information dynamically and takes label correlations into consideration using a regularized loss .", "entities": [[31, 32, "MethodName", "LSTM"], [59, 60, "MetricName", "loss"]]}, {"text": "However , the two capsulebased methods NLP - C APandHYPER CAPSconsistently outperform all the other methods owing to dynamic routing , which aggregates the \ufb01ne - grained capsule features in a label - aware manner .", "entities": []}, {"text": "Moreover , NLP - C APonly uses CNN to extract the local contextual information , while HYPER CAPS bene\ufb01ts from the parallel combination of local and global contextual information .", "entities": []}, {"text": "In addi - tion , NLP - C APapplies the non - linear squashing function for capsules in the Euclidean space , while HDR is designed for hyperbolic capsules , which take advantage of the representation capacity of the hyperbolic space .", "entities": []}, {"text": "Therefore , HYPER CAPSoutperforms NLP - C APas expected .", "entities": []}, {"text": "This result further con\ufb01rms that the proposed HYPER CAPSwith HDR is effective to learn the label - aware hyperbolic capsules for MLC .", "entities": []}, {"text": "5.3 Performance on Tail Labels InMLC , tail labels have low occurring frequency and hence are hard to predict compared to head labels .", "entities": []}, {"text": "The performance on tail labels of the four benchmark datasets is evaluated in terms of nDCG@k withk= 1;3;5 .", "entities": []}, {"text": "Figure 4 shows the results of the \ufb01ve deep learning based MLC methods , i.e. XML - CNN , SGM , REGGNN , NLP - C APand HYPER CAPS.nDCG@1 is smaller than nDCG@3 onAAPD , RCV1 andZHIHU since most of their test instances contain less than three tail labels .", "entities": [[35, 36, "DatasetName", "RCV1"]]}, {"text": "It is remarkable that HYPER CAPSoutperforms all the other methods on tail labels .", "entities": []}, {"text": "REGGNN takes advantage of the local and global contextual information and label correlations , thus it outperforms XML - CNN andSGM .", "entities": []}, {"text": "The two capsule - based methods NLP - C APand HYPER CAPSare both superior to the other methods , which indicates that the label - aware dynamic routing is effective for the prediction on tail labels .", "entities": []}, {"text": "In addition , the fact that HYPER CAPSsigni\ufb01cantly improves the prediction performance compared to NLP - C APimplies that the representation capacity of the hyperbolic space and the combination of local and global contextual information are helpful for learning on tail labels .", "entities": []}, {"text": "The results demonstrate the superiority of the proposed HYPER CAPS on tail labels for MLC .", "entities": []}, {"text": "31225.4 Ablation Test An ablation test would be informative to analyze the effect of varying different components of the proposed HYPER CAPS , which can be taken apart as local Euclidean capsules only ( denoted as L ) , global Euclidean capsules only ( denoted as G ) , a combination of the local and global Euclidean capsules ( denoted as L + G ) , and a combination of the local and global hyperbolic capsules ( denoted as L + G + H ) .", "entities": []}, {"text": "Euclidean capsules ( in L , G and L + G ) are aggregated via the origin dynamic routing ( Sabour et al . , 2017 ) , while hyperbolic capsules ( in L + G + H ) are aggregated via our HDR .", "entities": []}, {"text": "Figure 5 shows the results on EUR - L EX57 K in terms of P@k withk= 1;3;5 .", "entities": []}, {"text": "In order to make the comparison fair , the number of total compressed capsules is equally set to 256 for all the four models .", "entities": []}, {"text": "Adaptive routing is also applied with the maximum candidate label number set equally to 200 .", "entities": []}, {"text": "Generally , the proposed combination of local and global contextual information contributes to the effectiveness of the model ( L + G ) .", "entities": []}, {"text": "Therefore , it is practical to combine the local and global contextual information via dynamic routing .", "entities": []}, {"text": "HDR furthermore improves the performance by making use of the representation capacity of the hyperbolic space .", "entities": []}, {"text": "Overall , each of the components bene\ufb01ts the performance of H YPER CAPSfor MLC .", "entities": []}, {"text": "In summary , extensive experiments are carried out on four MLC benchmark datasets with various scales .", "entities": []}, {"text": "The results demonstrate that the proposed HYPER CAPScan achieve competitive performance compared with the baselines .", "entities": []}, {"text": "In particular , effectiveness of HYPER CAPS is shown on tail labels .", "entities": []}, {"text": "The ablation test furthermore con\ufb01rms that the combination of local and global contextual information is practical and HYPER CAPSbene\ufb01ts from the representation capacity of the hyperbolic space .", "entities": []}, {"text": "6 Related Work 6.1 Multi - Label Classi\ufb01cation Multi - label classi\ufb01cation ( MLC ) aims at assigning multiple relevant labels to one document .", "entities": []}, {"text": "The MLC label set is large compared to Multi - class classi\ufb01cation ( MCC ) .", "entities": []}, {"text": "Besides , the correlations of labels ( e.g.hierarchical label structures ( Banerjee et al . , 2019 ) ) and the existence of tail labels make MLC a hard task ( Bhatia et al . , 2015 ) .", "entities": []}, {"text": "As data sparsity and scalability issues arise with the large number of labels , XML - CNN ( Liu et al . , 2017 ) employs CNN as ef\ufb01cient feature extractor , whereas it ignores label correlations , which are often used to deal with tail labels .", "entities": []}, {"text": "The traditional MLC method SLEEC ( Bhatia et al . , 2015 ) makes use of label correlations by embedding the label co - occurrence graph .", "entities": []}, {"text": "The seq2seq model SGM ( Yang et al . , 2018b ) uses the attention mechanism to consider the label correlations , while REGGNN ( Xu et", "entities": [[1, 2, "MethodName", "seq2seq"]]}, {"text": "al . , 2019 ) applies a regularized loss speci\ufb01ed for label co - occurrence .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "REGGNN additionally chooses to dynamically combine the local and global contextual information to construct document representations .", "entities": []}, {"text": "6.2 Capsule Networks Capsule networks are recently proposed to address the representation limitations of CNN andRNN .", "entities": []}, {"text": "The concept of capsule is \ufb01rst introduced by ( Hinton et al . , 2011 ) .", "entities": []}, {"text": "( Sabour et al . , 2017 ) replaces the scalar output features of CNN with vector capsules and pooling with dynamic routing .", "entities": []}, {"text": "( Hinton et al . , 2018 ) proposes the EM algorithm based routing procedure between capsule layers .", "entities": [[10, 11, "MetricName", "EM"]]}, {"text": "( Gong et al . , 2018 ) proposes to regard dynamic routing as an information aggregation procedure , which is more effective than pooling .", "entities": []}, {"text": "( Yang et al . , 2018a ) and ( Du et al . , 2019a ) investigate capsule networks for text classi\ufb01cation .", "entities": []}, {"text": "( Zhao et al . , 2019 ) then presents a capsule compression method and reformulates the routing procedure to \ufb01t for MLC .", "entities": []}, {"text": "Our work is different from the predecessors as we design the Hyperbolic Dynamic Routing ( HDR ) to aggregate the parallel combination of local and global contextual information in form of hyperbolic capsules , which are constrained in the hyperbolic space without the requirement of non - linear squashing function .", "entities": []}, {"text": "In addition , adaptive routing is proposed to improve the scalability for large number of labels .", "entities": []}, {"text": "6.3 Hyperbolic Deep Learning Recent research on representation learning ( Nickel and Kiela , 2017 ) indicates that hyperbolic space is superior to Euclidean space in terms of representation capacity , especially in low dimension .", "entities": [[7, 9, "TaskName", "representation learning"]]}, {"text": "( Ganea et al . , 2018b ) generalizes operations for neural networks in the Poincar \u00b4 e ball using formalism of M \u00a8obius gyrovector space .", "entities": []}, {"text": "Some works lately demonstrate the superiority of the hyperbolic space for serval natural language processing tasks , such as textual entailment ( Ganea et al . , 2018a ) , machine translation ( Gulcehre et al . , 2019 ) and word embedding ( Tifrea et al . , 2019 ) .", "entities": [[30, 32, "TaskName", "machine translation"]]}, {"text": "Our work presents", "entities": []}, {"text": "3123the Hyperbolic Capsule Networks ( HYPER CAPS ) for MLC . 7 Conclusion We present the Hyperbolic Capsule Networks ( HYPER CAPS ) with Hyperbolic Dynamic Routing ( HDR ) and adaptive routing for Multi - Label Classi\ufb01cation ( MLC ) .", "entities": []}, {"text": "The proposed HYPER CAPS takes advantage of the parallel combination of \ufb01negrained local and global contextual information and label - aware feature aggregation method HDR to dynamically construct label - aware hyperbolic capsules for tail and head labels .", "entities": []}, {"text": "Adaptive routing is additionally applied to improve the scalability of HYPER CAPSby controlling the number of capsules during the routing procedure .", "entities": []}, {"text": "Extensive experiments are carried out on four benchmark datasets .", "entities": []}, {"text": "Results compared with the state - of - the - art methods demonstrate the superiority of HYPER CAPS , especially on tail labels .", "entities": []}, {"text": "As recent works explore the superiority of hyperbolic space to Euclidean space for serval natural language processing tasks , we intend to couple with the hyperbolic neural networks ( Ganea et al . , 2018b ) and the hyperbolic word embedding method such as POINCAR \u00b4 EGLOVE ( Tifrea et al . , 2019 ) in the future .", "entities": []}, {"text": "Acknowledgments This work was supported in part by the National Natural Science Foundation of China under Grant 61822601 , 61773050 , and 61632004 ; the Beijing Natural Science Foundation under Grant Z180006 ; National Key Research and Development Program ( 2017YFC1703506 ) ; the Fundamental Research Funds for the Central Universities ( 2019JBZ110 ) .", "entities": []}, {"text": "We thank the anonymous reviewers for their valuable feedback .", "entities": []}, {"text": "References Siddhartha Banerjee , Cem Akkaya , Francisco PerezSorrosal , and Kostas Tsioutsiouliklis .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Hierarchical transfer learning for multi - label text classi\ufb01cation .", "entities": [[1, 3, "TaskName", "transfer learning"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6295\u20136300 .", "entities": []}, {"text": "Kush Bhatia , Himanshu Jain , Purushottam Kar , Manik Varma , and Prateek Jain . 2015 .", "entities": []}, {"text": "Sparse local embeddings for extreme multi - label classi\ufb01cation .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 28 , pages 730\u2013738.Ilias Chalkidis , Emmanouil Fergadiotis , Prodromos Malakasiotis , and Ion Androutsopoulos .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Large - scale multi - label text classi\ufb01cation on EU legislation .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6314\u20136322 .", "entities": []}, {"text": "Junyoung Chung , Caglar Gulcehre , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Empirical evaluation of gated recurrent neural networks on sequence modeling .", "entities": []}, {"text": "In NIPS 2014 Workshop on Deep Learning .", "entities": []}, {"text": "Chunning Du , Haifeng Sun , Jingyu Wang , Qi Qi , Jianxin Liao , Chun Wang , and Bing Ma . 2019a .", "entities": []}, {"text": "Investigating capsule network and semantic feature on hyperplanes for text classi\ufb01cation .", "entities": [[1, 3, "MethodName", "capsule network"]]}, {"text": "pages 456\u2013465 .", "entities": []}, {"text": "Cunxiao Du , Zhaozheng Chin , Fuli Feng , Lei Zhu , Tian Gan , and Liqiang Nie . 2019b .", "entities": []}, {"text": "Explicit interaction model towards text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the Thirty - Third AAAI Conference on Arti\ufb01cial Intelligence , pages 6359\u20136366 .", "entities": []}, {"text": "Octavian Ganea , Gary Becigneul , and Thomas Hofmann .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Hyperbolic entailment cones for learning hierarchical embeddings .", "entities": []}, {"text": "In Proceedings of the 35th International Conference on Machine Learning , pages 1646\u20131655 .", "entities": []}, {"text": "Octavian Ganea , Gary Becigneul , and Thomas Hofmann . 2018b .", "entities": []}, {"text": "Hyperbolic neural networks .", "entities": []}, {"text": "In Advances in neural information processing systems 31 , pages 5345\u20135355 .", "entities": []}, {"text": "Jingjing Gong , Xipeng Qiu , Shaojing Wang , and Xuanjing Huang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Information aggregation via dynamic routing for sequence encoding .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 2742\u20132752 .", "entities": []}, {"text": "Caglar Gulcehre , Misha Denil , Mateusz Malinowski , Ali Razavi , Razvan Pascanu , Karl Moritz Hermann , Peter Battaglia , Victor Bapst , David Raposo , Adam Santoro , and Nando de Freitas .", "entities": [[28, 29, "MethodName", "Adam"]]}, {"text": "2019 .", "entities": []}, {"text": "Hyperbolic attention networks .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Geoffrey E Hinton , Alex Krizhevsky , and Sida D Wang .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Transforming auto - encoders .", "entities": []}, {"text": "In International Conference on Arti\ufb01cial Neural Networks , pages 44 \u2013 51 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Geoffrey E Hinton , Sara Sabour , and Nicholas Frosst .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Matrix capsules with EM routing .", "entities": [[3, 4, "MetricName", "EM"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Armand Joulin , Edouard Grave , Piotr Bojanowski , and Tomas Mikolov .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Bag of tricks for ef\ufb01cient text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers , pages 427\u2013431 .", "entities": []}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing , pages 1746\u20131751 .", "entities": []}, {"text": "3124Quoc Le and Tomas Mikolov .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Distributed representations of sentences and documents .", "entities": []}, {"text": "In Proceedings of the 31st International Conference on Machine Learning , pages 1188\u20131196 .", "entities": []}, {"text": "David D Lewis , Yiming Yang , Tony G Rose , and Fan Li . 2004 .", "entities": []}, {"text": "Rcv1 :", "entities": [[0, 1, "DatasetName", "Rcv1"]]}, {"text": "A new benchmark collection for text categorization research .", "entities": [[5, 7, "TaskName", "text categorization"]]}, {"text": "Journal of machine learning research , 5(Apr):361\u2013397 .", "entities": []}, {"text": "Jingzhou Liu , Wei - Cheng Chang , Yuexin Wu , and Yiming Yang . 2017 .", "entities": []}, {"text": "Deep learning for extreme multilabel text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 115\u2013124 .", "entities": [[6, 7, "DatasetName", "ACM"], [14, 16, "TaskName", "Information Retrieval"]]}, {"text": "Maximillian Nickel and Douwe Kiela . 2017 .", "entities": []}, {"text": "Poincar \u00b4 e embeddings for learning hierarchical representations .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 30 , pages 6338\u20136347 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing , pages 1532\u20131543 .", "entities": []}, {"text": "Sara Sabour , Nicholas Frosst , and Geoffrey E Hinton . 2017 .", "entities": []}, {"text": "Dynamic routing between capsules .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 30 , pages 3856\u20133866 .", "entities": []}, {"text": "Alexandru Tifrea , Gary Becigneul , and OctavianEugen Ganea .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Poincare glove : Hyperbolic word embeddings .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Matt P Wand and M Chris Jones .", "entities": []}, {"text": "1994 .", "entities": []}, {"text": "Kernel smoothing .", "entities": []}, {"text": "Chapman and Hall / CRC .", "entities": [[4, 5, "DatasetName", "CRC"]]}, {"text": "Xingyou Wang , Weijie Jiang , and Zhiyong Luo . 2016 .", "entities": []}, {"text": "Combination of convolutional and recurrent neural network for sentiment analysis of short texts .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 26th International Conference on Computational Linguistics , pages 2428\u20132437 .", "entities": []}, {"text": "Yunlai Xu , Xiangying Ran , Wei Sun , Xiangyang Luo , and Chongjun Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Gated neural network with regularized loss for multi - label text classi\ufb01cation .", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "In 2019 International Joint Conference on Neural Networks , pages 1\u20138 .", "entities": []}, {"text": "Min Yang , Wei Zhao , Jianbo Ye , Zeyang Lei , Zhou Zhao , and Soufei Zhang .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Investigating capsule networks with dynamic routing for text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3110\u20133119 .", "entities": []}, {"text": "Pengcheng Yang , Xu Sun , Wei Li , Shuming Ma , Wei Wu , and Houfeng Wang . 2018b .", "entities": []}, {"text": "SGM :", "entities": []}, {"text": "Sequence generation model for multi - label classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 3915\u20133926.Wei Zhao , Haiyun Peng , Steffen Eger , Erik Cambria , and Min Yang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Towards scalable and reliable capsule networks for challenging NLP applications .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1549\u20131559 .", "entities": []}, {"text": "Appendix A Label Distributions ( a ) AAPD ( b ) RCV1 ( c ) Z HIHU Figure 6 : Label distributions of the other three benchmark datasets .", "entities": [[11, 12, "DatasetName", "RCV1"]]}, {"text": "Y - axes of Z HIHU is on log - scale Figure 1 and Figure 6 show the label distributions of the four benchmark datasets .", "entities": []}, {"text": "Head and tail labels are divided based on the average number of training instances ( listed in Table 1 ) , i.e.labels have less than average number of training instances are tail labels .", "entities": []}, {"text": "We observe that this division generally follows the Pareto Principle , as nearly 80 % of labels are divided into the tail label set .", "entities": []}]