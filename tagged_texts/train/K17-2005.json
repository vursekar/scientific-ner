[{"text": "Proceedings of the CoNLL SIGMORPHON 2017 Shared Task : Universal Morphological Rein\ufb02ection , pages 58\u201365 , Vancouver , Canada , August 3\u20134 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics Morphological In\ufb02ection Generation with Multi - space Variational Encoder - Decoders Chunting Zhou and Graham Neubig Language Technologies Institute Carnegie Mellon University ctzhou,gneubig@cs.cmu.edu", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "This paper describes the CMU submission to shared task 1 of SIGMORPHON 2017 .", "entities": []}, {"text": "The system is based on the multi - space variational encoder - decoder ( MSVED ) method of Zhou and Neubig ( 2017 ) , which employs both continuous and discrete latent variables for the variational encoder - decoder and is trained in a semi - supervised fashion .", "entities": []}, {"text": "We discuss some language - speci\ufb01c errors and present result analysis .", "entities": []}, {"text": "1 Introduction In morphologically rich languages , different af\ufb01xes ( i.e. pre\ufb01xes , in\ufb01xes , suf\ufb01xes ) can be combined with the lemma to re\ufb02ect various syntactic and semantic features of a word .", "entities": [[22, 23, "DatasetName", "lemma"]]}, {"text": "In many areas of natural language processing ( NLP ) it is important that systems are able to correctly analyze and generate different morphological forms , including previously unseen forms .", "entities": []}, {"text": "The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation ( Chahuneau et al . , 2013 ) and information retrieval ( Darwish and Oard , 2007 ) .", "entities": [[16, 18, "TaskName", "machine translation"], [27, 29, "TaskName", "information retrieval"]]}, {"text": "Accordingly , learning morphological rein\ufb02ection patterns from labeled data is an important challenge .", "entities": []}, {"text": "The Universal Morphological Rein\ufb02ection task at SIGMORPHON 2017 ( Cotterell and Sch \u00a8utze , 2017 ) is an evaluation campaign aimed at systems that tackle the task of morphological in\ufb02ection .", "entities": []}, {"text": "It extends the SIGMORPHON 2016 Morphological Rein\ufb02ection by conducting tasks in 52 languages instead of 10 Cotterell et al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": ".", "entities": []}, {"text": "In our system submission , we utilize multispace variational encoder - decoders ( MSVEDs ) , which are a varitional encoder - decoder with both continuous and discrete latent variables ( Zhou andNeubig , 2017 ) .", "entities": []}, {"text": "The continuous latent variable is expected to re\ufb02ect the lemma form of a word and the discrete variables are used to induce the desired labels of the in\ufb02ected word .", "entities": [[9, 10, "DatasetName", "lemma"]]}, {"text": "The whole model is trained in a semi - supervised fashion .", "entities": []}, {"text": "For the supervised part we are reducing the reconstruction error of generating the in\ufb02ected word given the lemma and corresponding tags .", "entities": [[17, 18, "DatasetName", "lemma"]]}, {"text": "For the unsupervised part , we introduce the discrete latent variables representing the morphological tags , and train an auto - encoder over unlabeled corpora .", "entities": []}, {"text": "Thus , the training objective includes both the variational lower bound on the marginal log likelihood of the observed parallel training data and the monolingual data .", "entities": []}, {"text": "There are two tasks in SIGMORPHON 2017 , which are morphology in\ufb02ection ( task 1 ) and paradigm completion ( task 2 ) respectively .", "entities": []}, {"text": "We participated in task 1 , in\ufb02ection generation , in which the goal is to output the in\ufb02ected form of a lemma given a set of desired morphological tags.1 Experimental results found that our model works relatively well on the shared task 1 without extensive tuning of hyper - parameters and languagespeci\ufb01c features .", "entities": [[21, 22, "DatasetName", "lemma"]]}, {"text": "2 Methods In this section we will detail the multi - space variational encoder - decoder model .", "entities": []}, {"text": "Notation :", "entities": []}, {"text": "In morphological rein\ufb02ection , the source sequence x(s)consists of the characters in an in\ufb02ected word ( e.g. , \u201c played \u201d ) , while the associated labels y(t)describe some linguistic features ( e.g. ,y(t ) pos = Verb , y(t ) tense = Past ) that we 1We considered participation in task 2 , but while the training data in the second task provides all in\ufb02ection forms for each lemma , the number of different lemmas is rather smaller , which resulted in our model quickly over\ufb01tting to the training data when training the neural model .", "entities": [[70, 71, "DatasetName", "lemma"]]}, {"text": "Therefore , we only took part in the \ufb01rst task this time.58", "entities": []}, {"text": "hope to realize in the target .", "entities": []}, {"text": "The target sequence x(t)is", "entities": []}, {"text": "therefore the characters of the re - in\ufb02ected form of the source word ( e.g. , \u201c played \u201d ) that satisfy the linguistic features speci\ufb01ed by y(t ) .", "entities": []}, {"text": "For this task , each discrete variable y(t ) khas a set of possible labels ( e.g. pos = V , pos = ADJ , etc ) and follows a multinomial distribution .", "entities": []}, {"text": "2.1 Preliminaries : Variational Autoencoder The variational autoencoder ( Kingma and Welling , 2014 ) is an ef\ufb01cient way to handle ( continuous ) latent variables in neural models .", "entities": [[3, 5, "MethodName", "Variational Autoencoder"], [6, 8, "MethodName", "variational autoencoder"]]}, {"text": "We describe it brie\ufb02y here , and interested readers can refer to Doersch ( 2016 ) for details .", "entities": []}, {"text": "The V AE learns a generative model of the probability p(x)of observed data x.", "entities": [[2, 3, "MethodName", "AE"]]}, {"text": "The generative process consists of \ufb01rst generating a continuous latent variable zconditioned on the observed data x , which is termed as the recognition model q(z|x)(encoder ) and then use this latent variable to reconstruct the observation xknown as the reconstruction ( decoder ) model p(x|z ) .", "entities": []}, {"text": "V AE uses the variational inference to approximate the intractable posterior by learning a parametric posterior distribution for all observations .", "entities": [[1, 2, "MethodName", "AE"], [4, 6, "MethodName", "variational inference"]]}, {"text": "Th learning objective function is the variational lower bound on the marginal log likelihood of data : logp\u03b8(x)\u2265Ez\u223cq\u03c6(z|x)[logp\u03b8(x|z)]\u2212 KL(q\u03c6(z|x)||p(z ) )", "entities": []}, {"text": "( 1 ) To optimize the parameters with gradient descent , Kingma and Welling ( 2014 ) introduce a reparameterization trick that allows for training using simple backpropagation w.r.t .", "entities": []}, {"text": "the Gaussian latent variablesz .", "entities": []}, {"text": "2.2 Multi - space Variational Encoder - Decoders There are two cases to discuss when employing the variational encoder - decoder framework for labeled sequence transduction .", "entities": []}, {"text": "First , when the labels of the in\ufb02ected words are known as is the format of the training data in the shared task , we do n\u2019t need to bother introduction the discrete latent variables for the in\ufb02ected labels .", "entities": []}, {"text": "We maximize the variational lower bound on the conditional log likelihood of observing x(t)andy(t)asfollows : logp\u03b8(x(t),y(t)|x(s ) ) \u2265Ez\u223cq\u03c6(z|x(s))logp\u03b8(x(t),y(t),z|x(s ) ) q\u03c6(z|x(s ) )", "entities": []}, {"text": "= Ez\u223cq\u03c6(z|x(s))[logp\u03b8(x(t)|y(t),z ) + logp\u03c0(y(t))]\u2212 KL(q\u03c6(z|x(s))||p(z ) )", "entities": []}, {"text": "= Ll(x(t),y(t)|x(s ) ) ( 2 ) which is a simple extension to the vanilla variational auto - enocders .", "entities": []}, {"text": "Second , in the case of unsupervised learning or when the labels of the in\ufb02ected word is not observed , we only observe a word or a pair of words and we would like to maximize the log likelihood of the observed data by marginalizing over possible morphological labels , which is consisted to the supervised case above .", "entities": []}, {"text": "In this scenario , we can introduce the discrete latent variables for the in\ufb02ected labels which are used to infer the labels for the target word .", "entities": []}, {"text": "Then when decoding the word , we condition both on the continuous and discrete latent variables .", "entities": []}, {"text": "For the variational encoder - decoder ( MSVED ) , the variational lower bound on the conditional log likelihood is affected by the recognition model , and thus is computed as : logp\u03b8(x(t)|x(s ) ) \u2265E(y(t),z)\u223cq\u03c6(y(t),z|x(s),x(t))logp\u03b8(x(t),y(t),z|x(s ) ) q\u03c6(y(t),z|x(s),x(t ) )", "entities": []}, {"text": "= Ey(t)\u223cq\u03c6(y(t)|x(t))[Ez\u223cq\u03c6(z|x(s))[logp\u03b8(x(t)|y(t),z ) ] \u2212KL(q\u03c6(z|x(s))||p(z ) ) + logp\u03c0(y(t ) ) \u2212logq\u03c6(y(t)|x(t ) ) ]", "entities": []}, {"text": "= Lu(x(t)|x(s ) ) ( 3 ) While the unsupervised objective is trained by maximizing the following variational lower bound U(x)on the objective for unlabeled data : logp\u03b8(x)\u2265E(y , z)\u223cq\u03c6(y , z|x)logp\u03b8(x , y , z ) q\u03c6(y , z|x )", "entities": []}, {"text": "= Ey\u223cq\u03c6(y|x)[Ez\u223cq\u03c6(z|x)[logp\u03b8(x|z , y ) ] \u2212KL(q\u03c6(z|x)||p(z ) )", "entities": []}, {"text": "+ logp\u03c0(y ) \u2212logq\u03c6(y|x ) ]", "entities": []}, {"text": "= U(x ) ( 4 ) Note that when labels are not observed , the inference model q\u03c6(y|x)has the form of a discriminative classi\ufb01er , thus we can use observed labels as the supervision signal to learn a better classi\ufb01er .", "entities": []}, {"text": "In this case we also minimize the following cross entropy as the classi\ufb01cation loss : D(x , y ) = E(x , y)\u223cpl(x , y)[\u2212logq\u03c6(y|x ) ]", "entities": [[13, 14, "MetricName", "loss"]]}, {"text": "( 5)59", "entities": []}, {"text": "wherepl(x , y)is the distribution of labeled data .", "entities": []}, {"text": "To sum up , the semi - supervised model ( Semisup ) is trained to maximize the variational lower bounds and minimize the classi\ufb01cation crossentropy error of 5 . L(x(s),x(t),y(t),x )", "entities": []}, {"text": "= \u03b1\u00b7U(x ) + Lu(x(s)|x(t ) )", "entities": []}, {"text": "+ Ll(x(t),y(t)|x(s))\u2212D(x(t),y(t ) ) ( 6 ) The weight\u03b1controls the relative weight between the loss from unlabeled data and labeled data .", "entities": [[14, 15, "MetricName", "loss"]]}, {"text": "3 Learning MSVED 3.1 Learning Discrete Latent Variables One challenge in training our model is that discrete random variables in a stochastic computation graph prevent the gradient from being backpropagated due to their non - differentiability , and marginalizing over all label combinations is also infeasible in our case .", "entities": []}, {"text": "To alleviate this problem , we use the recently proposed Gumbel - Softmax trick ( Maddison et al . , 2014 ; Gumbel and Lieblein , 1954 ) to create a differentiable estimator for categorical variables .", "entities": [[12, 13, "MethodName", "Softmax"]]}, {"text": "In experiments , we start with a relatively large temperature and decrease it gradually .", "entities": []}, {"text": "3.2 Learning Continuous Latent Variables We observe that with the vanilla implementation the KL cost quickly decreases to near zero , setting q\u03c6(z|x)equal to standard normal distribution .", "entities": []}, {"text": "In this case , the RNN decoder can easily degenerate into an RNN language model .", "entities": []}, {"text": "Hence , the latent variables are ignored by the decoder and can not encode any useful information .", "entities": []}, {"text": "The latent variablezlearns an undesirable distribution that coincides with the imposed prior distribution but has no contribution to the decoder .", "entities": []}, {"text": "To force the decoder to use the latent variables , we take the following two approaches which are similar to Bowman et al .", "entities": []}, {"text": "( 2016 ) .", "entities": []}, {"text": "KL - Divergence Annealing : We add a coef\ufb01cient \u03bbto the KL cost and gradually anneal it from zero to a prede\ufb01ned threshold \u03bbm .", "entities": []}, {"text": "At the early stage of training , we set \u03bbto be zero and let the model \ufb01rst \ufb01gure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost .", "entities": []}, {"text": "This technique can also be seen in ( Ko \u02c7cisk`y et al . , 2016 ; Miao and Blunsom , 2016 ) .", "entities": []}, {"text": "Input Dropout in the Decoder : Besides annealing the KL cost , we also randomly drop out the kalb \u2303 (x)\u00b5(x) \u270f \u21e0 N(0,1)z < w > kk\u00e4+yT1yT2yT3yT4 ..........", "entities": [[1, 2, "MethodName", "Dropout"]]}, {"text": "kalb \u2303 (x)\u00b5(x) \u270f \u21e0 N(0,1)z < w > kkaMultinomial Sampling+ ...... y12{pos : V , N , ADJ} .. y22{def : DEF , INDEF}y32{num : DU , SG , PL} .........", "entities": []}, {"text": "Source WordRein\ufb02ected Form Source WordSource WordSupervised   Variational Encoder Decoder Unsupervised   Variational Auto - encodery1y2y3y4\u00b7\u00b7\u00b7Figure 1 : Model architecture for labeled and unlabeled data .", "entities": []}, {"text": "For the encoder - decoder model , only one direction from the source to target is given .", "entities": []}, {"text": "The classi\ufb01cation model is not illustrated in the diagram .", "entities": []}, {"text": "input token with a probability of \u03b2at each time step of the decoder during learning .", "entities": []}, {"text": "The previous ground - truth token embedding is replaced with a zero vector when dropped .", "entities": []}, {"text": "In this way , the RNN decoder could not fully rely on the ground - truth previous token , which ensures that the decoder uses information encoded in the latent variables .", "entities": []}, {"text": "4 Architecture for Morphological Rein\ufb02ection The overall model architecture is shown in Fig .", "entities": []}, {"text": "1 .", "entities": []}, {"text": "Each character and each label is associated with a continuous vector .", "entities": []}, {"text": "We employ Gated Recurrent Units ( GRUs ) for the encoder and decoder .", "entities": []}, {"text": "We use only single directional GRUs as the encoder for the input word x(s).uis the hidden representation of x(s)which is the last hidden state of GRUs .", "entities": []}, {"text": "and is used as the input for the inference model on z.", "entities": []}, {"text": "We represent \u00b5(u)and\u03c32(u)as MLPs and sample zfromN(\u00b5(u),diag(\u03c32(u ) ) ) , using z=\u00b5+\u03c3 \u25e6 /epsilon1 , where / epsilon1\u223c N ( 0,I ) .", "entities": []}, {"text": "Similarly , we can obtain the hidden representation of x(t)and use this as input to the inference model on each label y(t ) i , which is also an MLP following a softmax layer to generate the categorical probabilities of target labels .", "entities": [[29, 30, "DatasetName", "MLP"], [32, 33, "MethodName", "softmax"]]}, {"text": "Other experimental setups : We apply temperature annealing in the Gumble - Softmax with the scheme max(0.5,exp(\u22123e\u22125\u00b7t))every 2000 updates where tis the update steps .", "entities": [[12, 13, "MethodName", "Softmax"]]}, {"text": "We observe60", "entities": []}, {"text": "Language Dev Test Language Dev Test Language Dev Test Latin 66.2 66.2 Navajo 84.9 84.2 English 93.3 94.6 Icelandic 71.7 68.1 French 84.9 82.4 Lower - Sorbian 93.9 91.3", "entities": []}, {"text": "Irish 72.7 71.9 Armenian 85.3", "entities": []}, {"text": "82.3 Italian 94.2 92.6", "entities": []}, {"text": "Finnish 73.4 74.9 Latvian 85.6 87.5 Basque 95.0 97.0 Hungarian 74.5 73.6", "entities": []}, {"text": "Scottish - Gaelic 86.0 68.0 Estonian 95.1 93.7", "entities": []}, {"text": "Faroese 74.8 74.5 Bulgarian 86.3 86.7 Quechua 95.5 95.5 Russian 75.8 76.4 Macedonian 86.6 86.1", "entities": []}, {"text": "Khaling 96.2 94.8 Norwegian - Nynorsk 77.8 73.8 Northern - Sami 86.7 85.8 Hebrew 96.3 97.5", "entities": []}, {"text": "Polish 78.3 78.1 Slovene 87.3 87.8 Portuguese 96.4 96.4 German 79.3 78.7 Danish 88.1 85.4", "entities": []}, {"text": "Catalan 96.9 96.5 Swedish 80.2 80.6 Arabic 88.6 85.9 Urdu 98.4 97.9 Romanian 80.3 78.6 Sorani 89.6 87.8 Persian 98.6 98.7 Lithuanian 80.6 81.6 Slovak 89.6 87.9 Bengali 99.0 99.0 Serbo - Croatian 81.1 79.6 Turkish 90.4 90.3 Welsh 99.0 99.0 Norwegian - Bokmal 81.2 82", "entities": [[9, 10, "DatasetName", "Urdu"]]}, {"text": "Dutch 91.2 88.9 Haida 99.0 97.0 Czech 83.1 81.9 Albanian 91.9 91.3 Hindi 99.9 99.6 Kurmanji 83.4 83.8 Georgian 92.5 92.3 Ukrainian 84.5 84.0 Spanish 92.5 92.8 Average 87.18 86.21 Table 1 : Results of the ensemble system on the development ang test sets of 52 languages .", "entities": []}, {"text": "Language Src Word Tgt Labels Gold Tgt Ours", "entities": []}, {"text": "Latintrygon Pos = N;Case = ABL;Num = PL tr \u00afyg\u00afonibus tryg \u02dconibus largio Mood = SBJV;Num = PL;Per=2;Tense = PST;Asp = PRF;Pos = V larg \u00afiviss \u02dcetis largiss \u00afetis compenso Mood = SBJV;Num = SG;Per=3;Tense = PST;Asp = PFV;Pos = V compens", "entities": []}, {"text": "\u00afaverit compenserit", "entities": []}, {"text": "Icelandich\u00b4aspil Pos = N;Def = DEF;Case = GEN;Num = PL h \u00b4 aspilanna h \u00b4 asplanna gallabuxur Pos = N;Def = INDF;Case = GEN;Num = SG gallabuxna gallab \u00a8olur lest Pos = N;Def = DEF;Case = GEN;Num = SG L lestarinnar lestsins Table 2 : Examples of incorrect in\ufb02ection generation words on the dev data .", "entities": []}, {"text": "Language Settings Dev Acc .", "entities": [[3, 4, "MetricName", "Acc"]]}, {"text": "( Single Model . )", "entities": []}, {"text": "Icelandicvanilla Encoder - Decoder + attention , w/o data augmentation 81.0 our model w/o data augmentation and Wiki 78.6 our model ( full ) 71.7 Latinvanilla Encoder - Decoder + attention , w/o data augmentation 74.6 our model w/o data augmentation and Wiki 66.6 our model ( full )", "entities": [[8, 10, "TaskName", "data augmentation"], [14, 16, "TaskName", "data augmentation"], [33, 35, "TaskName", "data augmentation"], [39, 41, "TaskName", "data augmentation"]]}, {"text": "66.2 Persianvanilla Encoder - Decoder + attention , w/o data augmentation 99.6 our model w/o data augmentation and Wiki 99.6 our model ( full ) 98.6 Arabicvanilla Encoder - Decoder + attention , w/o data augmentation 90.7 our model w/o data augmentation and Wiki 91.3 our model ( full ) 88.6 Table 3 : Ablation experiments on the effects of data augmentation and WikiData .", "entities": [[9, 11, "TaskName", "data augmentation"], [15, 17, "TaskName", "data augmentation"], [34, 36, "TaskName", "data augmentation"], [40, 42, "TaskName", "data augmentation"], [60, 62, "TaskName", "data augmentation"]]}, {"text": "that our model is not sensitive to the temperature in this task .", "entities": []}, {"text": "All hyperparameters are tuned on the validation set , and include the following : For KL cost annealing , \u03bbmis set to be 0.2 for all language settings .", "entities": []}, {"text": "For character drop - out at the decoder , weempirically set \u03b2to be 0.4 for all languages .", "entities": []}, {"text": "We set the dimension of character embeddings to be 300 , tag label embeddings to be 200 , RNN hidden state to be 256 , and latent variable zto be 150 or 100 .", "entities": []}, {"text": "We set \u03b1the weight for the unsupervised61", "entities": []}, {"text": "loss to be 0.8 .", "entities": [[0, 1, "MetricName", "loss"]]}, {"text": "We train the model with Adadelta ( Zeiler , 2012 ) and use early - stop with a patience of 5 .", "entities": [[5, 6, "MethodName", "Adadelta"]]}, {"text": "Our system is an ensemble of \ufb01ve models and the probability vector at each time step is obtained by averaging the output probabilities from each model 5 Experiments 5.1 Data pre - processing Creating morphosyntactic tag maps : In our model , we treat the inference model on discrete labels in the form of discriminator , thus we need to know which label belongs to which morphosyntactic dimension .", "entities": []}, {"text": "For example , Vis a label of Part - of - speech - tagging .", "entities": [[7, 10, "DatasetName", "Part - of"]]}, {"text": "To obtain such mapping from a speci\ufb01c label to the morphosyntactic dimension , we leverage the Universal Morphological Feature Schema ( Sylak - Glassman , 2016 ) and also add the missing schema from the training data to create the key - value pairs of morphosysntactic dimension and label .", "entities": []}, {"text": "Then we reformat the labels provided in the data set into the key - value pairs to train a classi\ufb01er for each morphosyntactic dimension .", "entities": []}, {"text": "Data Augmentation : We augment the data set in the similar way as Kann and Sch \u00a8utze ( 2016 ) .", "entities": [[0, 2, "TaskName", "Data Augmentation"]]}, {"text": "By doing so , the training data is not limited to the form of lemma to in\ufb02ected word but can also be any word pairs that share the same lemma .", "entities": [[14, 15, "DatasetName", "lemma"], [29, 30, "DatasetName", "lemma"]]}, {"text": "This helps our model generalize better and learn the latent continuous representations more effectively .", "entities": []}, {"text": "The size of training data set after augmentation scales with a factor of 2 to 20 times compared with the original one .", "entities": []}, {"text": "Monolingual WikiData : We process the Wikipedia corpus provided by the shared task organizer as our unsupervised training data together with words in the training data .", "entities": []}, {"text": "For each language , we \ufb01rst get the character vocabulary of the corresponding training data and only keep words in the Wiki corpus for which characters are all in the character set we obtained .", "entities": []}, {"text": "All words that occur less than 20 times are eliminated .", "entities": []}, {"text": "We also limit the number of words used during training to be the 50000 most frequent words .", "entities": []}, {"text": "5.2 Results and Analysis The results on the dev and test data of the 52 languages are presented in 1 .", "entities": []}, {"text": "We obtain a generation accuracy above 80 % over more than 25 % languages and an average of 87.2 % for both devand test data .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "The generation accuracy is almost consistent on the dev and test data except that the test data accuracy of Scottish - Gaelic drops by near 21 % .", "entities": [[2, 3, "MetricName", "accuracy"], [17, 18, "MetricName", "accuracy"]]}, {"text": "We \ufb01nd that only a medium volume of training data is provided for Scottish - Gaelic .", "entities": []}, {"text": "This may be the reason why the model trained for ScottishGaelic can not generalize as well as other languages .", "entities": []}, {"text": "We do not tune the hyper - parameters for each language manually .", "entities": []}, {"text": "However , we test on different dimensions for the continuous latent variables .", "entities": []}, {"text": "The dimension size we have used included 100 and 150 .", "entities": []}, {"text": "And we observe signi\ufb01cant improvement by using a larger dimension size of latent variables over a portion of languages including Faroese , Lithuanian , Navajo , Scottish - gaelic , Northern - sami , Slovene , Sorani , Slovak .", "entities": []}, {"text": "However , we also observe that for some languages including Finnish , German , French , etc , the performance drops sign\ufb01cantly after increasing the size of continuous latent variable dimension .", "entities": []}, {"text": "This indicates that for different languages , the continuous space required to encode the lemma and in\ufb02ected information varies from language to language .", "entities": [[14, 15, "DatasetName", "lemma"]]}, {"text": "We will further investigate this in the future work .", "entities": []}, {"text": "5.3 Effect of Data Augmentation and Using Wiki Data While our performance was reasonable , it was not as good as that presented in our previous work ( Zhou and Neubig , 2017 ) , nor was it competitive with the highest - scoring models on the shared task .", "entities": [[3, 5, "TaskName", "Data Augmentation"]]}, {"text": "In order to examine the reason for this , we performed several ablations , the results of which are presented in Tab .", "entities": []}, {"text": "3", "entities": []}, {"text": "First , we \ufb01rst examined the effects of data augmentation and Wiki Data for semi - supervised learning on the performance of our model .", "entities": [[8, 10, "TaskName", "data augmentation"]]}, {"text": "By removing the augmented data from the training set , we observe a large gain in the generation accuracy .", "entities": [[18, 19, "MetricName", "accuracy"]]}, {"text": "Besides , we \ufb01nd that Wiki Data for semisupervised learning does n\u2019t help much to increase the model \u2019s performance .", "entities": []}, {"text": "The reasons for this will be examined further in the following section .", "entities": []}, {"text": "We additionally reimplemented a vanilla encoder - decoder model with attention that concatenates the input characters and target word tags together with a special token in the middle as the new input sequence to the encoder ( Kann and Sch \u00a8utze , 2016 ) .", "entities": []}, {"text": "The results show that the vanilla encoder - decoder works better than our62", "entities": []}, {"text": "Dimension Label Train Data WikiData Difference CaseNone 0.58 0.35 -0.22 ACC 0.14 0.51 0.38 NOM 0.14 0.12 -0.02 GEN 0.14 0.01 -0.13 PossessionNone 0.86 0.31 -0.55 PSSD 0.14 0.69 0.55 Language - Speci\ufb01c - FeaturesNone 0.90 0.42 -0.48 LGSPEC1 0.10", "entities": [[10, 11, "MetricName", "ACC"]]}, {"text": "0.58 0.48", "entities": []}, {"text": "MoodNone 0.68 0.10 -0.58 IND 0.20 0.62 0.42 IMP 0.02 0.03 0.01 SBJV 0.10 0.25 0.15 De\ufb01nitenessNone 0.57 0.60 0.03 DEF 0.22 0.34 0.12 NDEF 0.21 0.06 -0.15 GenderNone 0.53 0.52 -0.01 FEM 0.23 0.27 0.04 MASC 0.23 0.20", "entities": [[36, 37, "DatasetName", "MASC"]]}, {"text": "-0.03 PolitenessNone 0.85 0.58", "entities": []}, {"text": "-0.28 INFM 0.14 0.42 0.28 NumberNone 0.01 0.16 0.15 DU 0.22 0.34 0.12 SG 0.47 0.31 -0.15 PL 0.30 0.18", "entities": []}, {"text": "-0.11 PersonNone 0.58 0.74 0.15 1 0.06 0.02 -0.05 3 0.18 0.17 -0.01 2 0.17 0.08 -0.09 TenseNone 0.90 0.51 -0.40 PST 0.10 0.49 0.40 AspectNone 0.80 0.21 -0.59 PRF 0.10 0.41 0.31 IPFV 0.10 0.38 0.28 Part - of - SpeechNone 0.00 0.03 0.03 V+V .PTCP 0.01 0.29 0.28 V+V .MSDR 0.00 0.15 0.14 N 0.43 0.36 -0.07", "entities": [[37, 40, "DatasetName", "Part - of"]]}, {"text": "ADJ", "entities": []}, {"text": "0.14", "entities": []}, {"text": "0.14", "entities": []}, {"text": "-0.00 V 0.42 0.03 -0.39 V oiceNone 0.57 0.40 -0.18 PASS 0.16 0.39 0.22 ACT 0.27 0.22 -0.05 Table 4 : The distribution of morphosyntactic tags for Arabic on Wikipedia and the shared task training data respectively .", "entities": [[10, 11, "DatasetName", "PASS"]]}, {"text": "The linguistic tag classi\ufb01er has an average accuracy of 93.36 % on the Dev data .", "entities": [[6, 8, "MetricName", "average accuracy"]]}, {"text": "model in some cases .", "entities": []}, {"text": "We suspect that since task 1 is purely an in\ufb02ection task and because semi - supervised learning did not provide a particularly large bene\ufb01t , a simpler model that utilizes attention may be suf\ufb01cient .", "entities": []}, {"text": "This is in contrast to our previous \ufb01ndings , where semi - supervised learning was highly effective , and the proposed model out - performed the simpler attention - based baseline .", "entities": []}, {"text": "5.4 Analysis on the Distribution of Linguistic Tags of Wiki Data and Training Data One potential reason for the lack of effectiveness of semi - supervised training is that the semi - supervised data that we used for training was not appropriate for the task at hand , or that we were not able to use it in the most effective way .", "entities": []}, {"text": "In order to do so , we analyze the distribution of linguistic tags for words from the training data in the shared task and the Wiki Data provided by the organizer , with the hypothesis that if the distribution of tags for the Wiki Data is very different from the training and test data for the shared task , our predictions may be biased away from the testing distribution by incorporating the unsupervised Wiki data .", "entities": []}, {"text": "To perform this examination , we use the tag classi\ufb01er trained in our model to predict the labels for each word in the Wiki Data.63", "entities": []}, {"text": "Dimension Label Train Data WikiData Difference MoodNone 0.79 0.13 -0.66 IMP 0.03 0.69 0.66 SBJV 0.18 0.18", "entities": []}, {"text": "-0.00 PolitenessNone 0.52 0.30 -0.22 COL 0.48 0.70 0.22 NumberNone 0.04 0.67 0.62 SG 0.48 0.19", "entities": []}, {"text": "-0.30 PL 0.47 0.15 -0.32 PersonNone 0.04 0.28 0.24 1 0.31 0.23 -0.08 3 0.31 0.22 -0.09 2 0.34 0.27 -0.07 FinitenessNone 0.98 0.33 -0.66 NFIN 0.02 0.67 0.66 TenseNone 0.13 0.07 -0.07 FUT 0.04 0.42 0.38 PST 0.46 0.14 -0.32 PRS 0.36 0.37 0.01 AspectNone 0.39 0.37 -0.01 PROG 0.18 0.07 -0.11 PRF 0.17 0.03 -0.14 IPFV 0.18 0.09 -0.08 PFV 0.09 0.44 0.35 Part - of - SpeechNone 0.00 0.44 0.44 V+V .PTCP 0.03 0.18 0.15 V 0.97 0.38 -0.59 Table 5 : The distribution of morphosyntactic tags for Persian on Wikipedia and the shared task training data respectively .", "entities": [[65, 68, "DatasetName", "Part - of"]]}, {"text": "The linguistic tag classi\ufb01er has an average accuracy of 95.26 % on the Dev data .", "entities": [[6, 8, "MetricName", "average accuracy"]]}, {"text": "The percentages of each label within each morphosyntactic dimension for Arabic and Persian are listed in Tab . 4 and Tab .", "entities": []}, {"text": "5 .", "entities": []}, {"text": "We found that the distribution of the linguistic tags for the Wiki Data and the training data in the shared task are not always consistent .", "entities": []}, {"text": "For example , in Arabic , the distributions of predicted tags with respect to case , possession , part - of - speech , and several other classes differ signi\ufb01cantly from the original training data .", "entities": [[18, 21, "DatasetName", "part - of"]]}, {"text": "Such difference suggests that either the words in the unlabeled Wiki Data have very different characteristics than our training set , or our tag classi\ufb01er is not functioning properly to identify the tags .", "entities": []}, {"text": "Either case would be detrimental to semi - supervised learning .", "entities": []}, {"text": "The problem is even more stark for Persian : in Persian the only labeled words in the training data are verbs , so all nonverb words in the Wiki Data will receive an incorrect analysis , which is obviously not conducive to learning anything useful .", "entities": []}, {"text": "As a recommendation for the future , when performing semi - supervised learning for morphology where the labeled data only represents a subset of the phenomena in the language , it is likely necessary to \ufb01rst identify which of the available unlabeled data is appropriate for semi - supervised learning before applyingsuch methods .", "entities": []}, {"text": "5.5 Case Study on In\ufb02ected Words In Tab . 1 , we notice that the performance on Latin is relatively poor compared with other languages .", "entities": []}, {"text": "Latin is a highly in\ufb02ected languages with three distinct genders , seven noun cases , four verb conjugations , four verb principal parts , six tenses , three persons , three moods , two voices , two aspects and two numbers .", "entities": []}, {"text": "In addition to this , we found that the data set size after augmentation was only enlarged 2 times .", "entities": []}, {"text": "We examine some errors made by our system on two worst performed languages Latin and Icelandic in Tab .", "entities": []}, {"text": "2 . As shown in the table , we found that the in\ufb02ections of Latin and Icelandic have more suf\ufb01x variations from the lemma .", "entities": [[23, 24, "DatasetName", "lemma"]]}, {"text": "We guess our model still lacks the ability to capture more complicated in\ufb02ections for such languages .", "entities": []}, {"text": "We might consider adding the dependencies between different in\ufb02ections for multiple target labels in our future work .", "entities": []}, {"text": "6 Conclusion and Future Work In this work , we further examine the method proposed in ( Zhou and Neubig , 2017 ) for the shared task of SIGMORPHON 2017 on 52 languages and64", "entities": []}, {"text": "demonstrate the effectiveness of this approach .", "entities": []}, {"text": "We will further improve our model \u2019s sophistication by investigating strategies for choosing appropriate semi - supervised data , and examining the model \u2019s performance on languages with a high in\ufb02ection level .", "entities": []}, {"text": "Acknowledgments This work has been supported in part by an Amazon Academic Research Award .", "entities": []}, {"text": "We thank Matthew Honnibal for pointing out that the data distribution of Wikipedia corpus might be biased .", "entities": []}, {"text": "References Samuel R Bowman , Luke Vilnis , Oriol Vinyals , Andrew M Dai , Rafal Jozefowicz , and Samy Bengio .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Generating sentences from a continuous space .", "entities": []}, {"text": "Proceedings of CoNLL .", "entities": []}, {"text": "Victor Chahuneau , Eva Schlinger , Noah A Smith , and Chris Dyer . 2013 .", "entities": []}, {"text": "Translating into morphologically rich languages with synthetic phrases .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ryan Cotterell , Christo Kirov , John Sylak - Glassman , David Yarowsky , Jason Eisner , and Mans Hulden .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "The sigmorphon 2016 shared task \u2014 morphological rein\ufb02ection .", "entities": []}, {"text": "In Proceedings of the 2016 Meeting of SIGMORPHON .", "entities": []}, {"text": "Association for Computational Linguistics , Berlin , Germany .", "entities": []}, {"text": "Ryan Cotterell and Hinrich Sch \u00a8utze . 2017 .", "entities": []}, {"text": "Joint semantic synthesis and morphological analysis of the derived word .", "entities": [[4, 6, "TaskName", "morphological analysis"]]}, {"text": "arXiv preprint arXiv:1701.00946 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kareem Darwish and Douglas W Oard .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Adapting morphology for arabic information retrieval .", "entities": [[4, 6, "TaskName", "information retrieval"]]}, {"text": "In Arabic Computational Morphology , Springer , pages 245\u2013262 .", "entities": []}, {"text": "Carl Doersch . 2016 .", "entities": []}, {"text": "Tutorial on variational autoencoders .", "entities": [[3, 4, "MethodName", "autoencoders"]]}, {"text": "arXiv preprint arXiv:1606.05908 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Emil Julius Gumbel and Julius Lieblein .", "entities": []}, {"text": "1954 .", "entities": []}, {"text": "Statistical theory of extreme values and some practical applications : a series of lectures .", "entities": []}, {"text": "US Government Printing Of\ufb01ce Washington .", "entities": []}, {"text": "Katharina Kann and Hinrich Sch \u00a8utze . 2016 .", "entities": []}, {"text": "Med : The lmu system for the sigmorphon 2016 shared task on morphological rein\ufb02ection .", "entities": [[3, 4, "MethodName", "lmu"]]}, {"text": "In In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics , Phonology , and Morphology .", "entities": []}, {"text": "Berlin , Germany .", "entities": []}, {"text": "D.P. Kingma and M. Welling .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Auto - encoding variational bayes .", "entities": []}, {"text": "In The International Conference on Learning Representations .Tom\u00b4a\u02c7s", "entities": []}, {"text": "Ko \u02c7cisk`y , G \u00b4 abor Melis , Edward Grefenstette , Chris Dyer , Wang Ling , Phil Blunsom , and Karl Moritz Hermann . 2016 .", "entities": []}, {"text": "Semantic parsing with semi - supervised sequential autoencoders .", "entities": [[0, 2, "TaskName", "Semantic parsing"], [7, 8, "MethodName", "autoencoders"]]}, {"text": "the 2016 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) .", "entities": []}, {"text": "Chris J Maddison , Daniel Tarlow , and Tom Minka .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A * sampling .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems .", "entities": []}, {"text": "pages 3086\u20133094 .", "entities": []}, {"text": "Yishu Miao and Phil Blunsom .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Language as a latent variable : Discrete generative models for sentence compression .", "entities": [[10, 12, "DatasetName", "sentence compression"]]}, {"text": "the 2016 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) .", "entities": []}, {"text": "John Sylak - Glassman .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "The composition and use of the universal morphological feature schema ( unimorph schema ) .", "entities": []}, {"text": "Matthew D Zeiler .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Adadelta : an adaptive learning rate method .", "entities": [[0, 1, "MethodName", "Adadelta"], [4, 6, "HyperparameterName", "learning rate"]]}, {"text": "arXiv preprint arXiv:1212.5701 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Chunting Zhou and Graham Neubig .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Multispace variational encoder - decoders for semisupervised labeled sequence transduction .", "entities": []}, {"text": "In The 55th Annual Meeting of the Association for Computational Linguistics ( ACL ) .", "entities": []}, {"text": "Vancouver , Canada . https://arxiv.org/abs/1704.01691.65", "entities": []}]