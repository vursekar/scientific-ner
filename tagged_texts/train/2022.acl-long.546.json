[{"text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 7930 - 7944 May 22 - 27 , 2022 c", "entities": []}, {"text": "2022 Association for Computational Linguistics Learning to Generalize to More : Continuous Semantic Augmentation for Neural Machine Translation Xiangpeng Wei\u2020 , Heng Yu\u2020 , Yue Hu\u2021,\u00a7 , Rongxiang Weng\u2020 , Weihua Luo\u2020 , Rong Jin\u2020 \u2020Machine Intelligence Technology Lab , Alibaba DAMO Academy , Hangzhou , China \u2021Institute of Information Engineering , Chinese Academy of Sciences , Beijing , China \u00a7 School of Cyber Security , University of Chinese Academy of Sciences , Beijing , China pemywei@gmail.com https://github.com/pemywei/csanmt Abstract The principal task in supervised neural machine translation ( NMT ) is to learn to generate target sentences conditioned on the source inputs from a set of parallel sentence pairs , and thus produce a model capable of generalizing to unseen instances .", "entities": [[16, 18, "TaskName", "Machine Translation"], [85, 87, "TaskName", "machine translation"]]}, {"text": "However , it is commonly observed that the generalization performance of the model is highly influenced by the amount of parallel data used in training .", "entities": []}, {"text": "Although data augmentation is widely used to enrich the training data , conventional methods with discrete manipulations fail to generate diverse and faithful training samples .", "entities": [[1, 3, "TaskName", "data augmentation"]]}, {"text": "In this paper , we present a novel data augmentation paradigm termed Continuous Semantic Augmentation ( CSANMT ) , which augments each training instance with an adjacency semantic region that could cover adequate variants of literal expression under the same meaning .", "entities": [[8, 10, "TaskName", "data augmentation"]]}, {"text": "We conduct extensive experiments on both rich - resource and low - resource settings involving various language pairs , including WMT14 English \u2192{German , French } , NIST Chinese \u2192English and multiple low - resource IWSLT translation tasks .", "entities": [[20, 21, "DatasetName", "WMT14"]]}, {"text": "The provided empirical evidences show that CSANMT sets a new level of performance among existing augmentation techniques , improving on the state - of - theart by a large margin.1 1 Introduction Neural machine translation ( NMT ) is one of the core topics in natural language processing , which aims to generate sequences of words in the target language conditioned on the source inputs ( Sutskever et al . , 2014 ; Cho et", "entities": [[33, 35, "TaskName", "machine translation"]]}, {"text": "al . , 2014 ; Wu et", "entities": []}, {"text": "al . , 2016 ; Vaswani et", "entities": []}, {"text": "al . , 2017 ) .", "entities": []}, {"text": "In the common supervised setting , the training objective is to learn a transformation from the source space to the target spaceX 7\u2192 Y : f(y|x ; \u0398)with the usage of parallel data .", "entities": []}, {"text": "In this way , NMT models are expected to 1The core codes are contained in Appendix E.be capable of generalizing to unseen instances with the help of large scale training data , which poses a big challenge for scenarios with limited resources .", "entities": []}, {"text": "To address this problem , various methods have been developed to leverage abundant unlabeled data for augmenting limited labeled data ( Sennrich et al . , 2016a ; Cheng et al . , 2016 ; He et al . , 2016 ; Hoang et al . , 2018 ; Edunov et al . , 2018 ; He et al . , 2020 ; Song et al . , 2019 ) .", "entities": []}, {"text": "For example , backtranslation ( BT ) ( Sennrich et al . , 2016a ) makes use of the monolingual data on the target side to synthesize large scale pseudo parallel data , which is further combined with the real parallel corpus in machine translation task .", "entities": [[43, 45, "TaskName", "machine translation"]]}, {"text": "Another line of research is to introduce adversarial inputs to improve the generalization of NMT models towards small perturbations ( Iyyer et al . , 2015 ; Fadaee et al . , 2017 ; Wang et al . , 2018 ; Cheng et al . , 2018 ; Gao et al . , 2019 ) .", "entities": []}, {"text": "While these methods lead to significant boosts in translation quality , we argue that augmenting the observed training data in the discrete space inherently has two major limitations .", "entities": []}, {"text": "First , augmented training instances in discrete space are lack diversity .", "entities": []}, {"text": "We still take BT as an example , it typically uses beam search ( Sennrich et al . , 2016a ) or greedy search ( Lample et al . , 2018a , c ) to generate synthetic source sentences for each target monolingual sentence .", "entities": []}, {"text": "The above two search strategies are approximate algorithms to identify the maximum a - posteriori ( MAP ) output ( Edunov et al . , 2018 ) , and thus favor the most frequent one in case of ambiguity .", "entities": [[16, 17, "DatasetName", "MAP"]]}, {"text": "Edunov et al .", "entities": []}, {"text": "( 2018 ) proposed a sampling strategy from the output distribution to alleviate this issue , but this method typically yields synthesized data with low quality .", "entities": []}, {"text": "While some extensions ( Wang et al . , 2018 ; Imamura et al . , 2018 ; Khayrallah et al . , 2020 ; Nguyen et al . , 2020 ) augment each training instance with multiple literal forms , they still fail to cover adequate variants under the same meaning .", "entities": []}, {"text": "Second , it is difficult for augmented texts in dis-7930", "entities": []}, {"text": "crete space to preserve their original meanings .", "entities": []}, {"text": "In the context of natural language processing , discrete manipulations such as adds , drops , reorders , and/or replaces words in the original sentences often result in significant changes in semantics .", "entities": []}, {"text": "To address this issue , Gao et al .", "entities": []}, {"text": "( 2019 ) and Cheng et al .", "entities": []}, {"text": "( 2020 ) instead replace words with other words that are predicted using language model under the same context , by interpolating their embeddings .", "entities": []}, {"text": "Although being effective , these techniques are limited to word - level manipulation and are unable to perform the whole sentence transformation , such as producing another sentence by rephrasing the original one so that they have the same meaning .", "entities": []}, {"text": "In this paper , we propose Continuous Semantic Augmentation ( CSANMT ) , a novel data augmentation paradigm for NMT , to alleviate both limitations mentioned above .", "entities": [[15, 17, "TaskName", "data augmentation"]]}, {"text": "The principle of CSANMT is to produce diverse training data from a semantically - preserved continuous space .", "entities": []}, {"text": "Specifically , ( 1 ) we first train a semantic encoder via a tangential contrast , which encourages each training instance to support an adjacency semantic region in continuous space and treats the tangent points of the region as the critical states of semantic equivalence .", "entities": []}, {"text": "This is motivated by the intriguing observation made by recent work showing that the vectors in continuous space can easily cover adequate variants under the same meaning ( Wei et al . , 2020a ) .", "entities": []}, {"text": "( 2 ) We then introduce a Mixed Gaussian Recurrent Chain ( MGRC ) algorithm to sample a cluster of vectors from the adjacency semantic region .", "entities": []}, {"text": "( 3 ) Each of the sampled vectors is finally incorporated into the decoder by developing a broadcasting integration network , which is agnostic to model architectures .", "entities": []}, {"text": "As a consequence , transforming discrete sentences into the continuous space can effectively augment the training data space and thus improve the generalization capability of NMT models .", "entities": []}, {"text": "We evaluate our framework on a variety of machine translation tasks , including WMT14 EnglishGerman / French , NIST Chinese - English and multiple IWSLT tasks .", "entities": [[8, 10, "TaskName", "machine translation"], [13, 14, "DatasetName", "WMT14"]]}, {"text": "Specifically , CSANMT sets the new state of the art among existing augmentation techniques on the WMT14 English - German task with 30.94 BLEU score .", "entities": [[16, 17, "DatasetName", "WMT14"], [23, 25, "MetricName", "BLEU score"]]}, {"text": "In addition , our approach could achieve comparable performance with the baseline model with the usage of only 25 % of training data .", "entities": []}, {"text": "This reveals that CSANMT has great potential to achieve good results with very few data .", "entities": []}, {"text": "Furthermore , CSANMT demonstrates consistentimprovements over strong baselines in low resource scenarios , such as IWSLT14 English - German and IWSLT17 English - French .", "entities": []}, {"text": "2 Framework Problem Definition Supposing XandYare two data spaces that cover all possible sequences of words in source and target languages , respectively .", "entities": []}, {"text": "We denote ( x , y)\u2208(X , Y)as a pair of two sentences with the same meaning , where x={x1 , x2 , ... , x T}is the source sentence with Ttokens , and y={y1 , y2 , ... , y T\u2032}is the target sentence with T\u2032tokens .", "entities": []}, {"text": "A sequence - tosequence model is usually applied to neural machine translation , which aims to learn a transformation from the source space to the target space X 7\u2192 Y : f(y|x ; \u0398)with the usage of parallel data .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "Formally , given a set of observed sentence pairs C={(x(n),y(n))}N n=1 , the training objective is to maximize the log - likelihood : Jmle(\u0398 ) = E(x , y)\u223cC\u0000 logP(y|x ; \u0398)\u0001 .", "entities": [[19, 22, "MetricName", "log - likelihood"]]}, {"text": "( 1 ) The log - probability is typically decomposed as : logP(y|x ; \u0398 ) = PT\u2032 t=1logP(yt|y < t , x ; \u0398 ) , where \u0398is a set of trainable parameters and y < tis a partial sequence before time - step t.", "entities": [[14, 15, "HyperparameterName", "\u0398"], [24, 25, "HyperparameterName", "\u0398"]]}, {"text": "However , there is a major problem in the common supervised setting for neural machine translation , that is the number of training instances is very limited because of the cost in acquiring parallel data .", "entities": [[14, 16, "TaskName", "machine translation"]]}, {"text": "This makes it difficult to learn an NMT model generalized well to unseen instances .", "entities": []}, {"text": "Traditional data augmentation methods generate more training samples by applying discrete manipulations to unlabeled ( or labeled ) data , such as back - translation or randomly replacing a word with another one , which usually suffer from the problems of semantic deviation and the lack of diversity .", "entities": [[1, 3, "TaskName", "data augmentation"]]}, {"text": "2.1 Continuous Semantic Augmentation We propose a novel data augmentation paradigm for neural machine translation , termed continuous semantic augmentation ( CSANMT ) , to better generalize the model \u2019s capability to unseen instances .", "entities": [[8, 10, "TaskName", "data augmentation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "We adopt the Transformer ( Vaswani et al . , 2017 ) model as a backbone , and the framework is shown in Figure 1 .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "In this architecture , an extra semantic encoder translates the source xand the target sentence yto real - value vectors rx = \u03c8(x ; \u0398\u2032)and ry = \u03c8(y ; \u0398\u2032)respectively , where \u03c8 ( \u00b7 ; \u0398\u2032)is the forward function of the semantic encoder parameterized by \u0398\u2032(parameters other than \u0398).7931", "entities": []}, {"text": "Multi - Head AttentionFeed Forward Position EncodingAdd & Norm Masked   Multi - Head AttentionMulti - Head Attention Position EncodingFeed Forward Add & Norm Add & NormSoftmaxOuput Probabilities Add & Norm Add & Norm EncoderDecoder Broadcasting Integration Semantic EncoderFigure 1 : The framework of the C SANMT .", "entities": []}, {"text": "Definition 1 .", "entities": []}, {"text": "There is a universal semantic space among the source and the target languages for neural machine translation , which is established by a semantic encoder .", "entities": [[15, 17, "TaskName", "machine translation"]]}, {"text": "It defines a forward function \u03c8 ( \u00b7 ; \u0398\u2032)to map discrete sentences into continuous vectors , that satisfies : \u2200(x , y)\u2208(X , Y ) :", "entities": []}, {"text": "rx = ry .", "entities": []}, {"text": "Besides , an adjacency semantic region \u03bd(rx , ry)in the semantic space describes adequate variants of literal expression centered around each observed sentence pair ( x , y ) .", "entities": []}, {"text": "In our scenario , we first sample a series of vectors ( denoted by R ) from the adjacency semantic region to augment the current training instance , that is R={\u02c6r(1),\u02c6r(2 ) , ... , \u02c6r(K)},where \u02c6 r(k)\u223c \u03bd(rx , ry).Kis the hyperparameter that determines the number of sampled vectors .", "entities": []}, {"text": "Each sample \u02c6r(k)is then integrated into the generation process through a broadcasting integration network : \u02c6ot = W1\u02c6r(k)+W2ot+b , ( 2 ) where otis the output of the self - attention module at position t.", "entities": []}, {"text": "Finally , the training objective in Eq .", "entities": []}, {"text": "( 1 ) can be improved as Jmle(\u0398 ) = E(x , y)\u223cC,\u02c6r(k)\u2208R\u0000 logP(y|x,\u02c6r(k ) ; \u0398)\u0001\u0001 .(3 )", "entities": []}, {"text": "By augmenting the training instance ( x , y)with diverse samples from the adjacency semantic region , the model is expected to generalize to more unseen instances .", "entities": []}, {"text": "To this end , we must consider such two problems : ( 1 ) How to optimize the semantic encoder so that it produces a meaningful adjacency semantic region for each observed training pair .", "entities": []}, {"text": "Figure 2 : The diagram of formulating the adjacency semantic region for the sentence pair ( x(i),y(i ) ) .", "entities": []}, {"text": "( 2)How to obtain samples from the adjacency semantic region in an efficient and effective way .", "entities": []}, {"text": "In the rest part of this section , we introduce the resolutions of these two problems , respectively .", "entities": []}, {"text": "Tangential Contrastive Learning We start from analyzing the geometric interpretation of adjacency semantic regions .", "entities": [[1, 3, "MethodName", "Contrastive Learning"]]}, {"text": "The schematic diagram is illustrated in Figure 2 .", "entities": []}, {"text": "Let ( x(i),y(i))and(x(j),y(j ) ) are two instances randomly sampled from the training corpora .", "entities": []}, {"text": "For ( x(i),y(i ) ) , the adjacency semantic region \u03bd(rx(i ) , ry(i))is defined as the union of two closed balls that are centered by rx(i)and ry(i ) , respectively .", "entities": []}, {"text": "The radius of both balls is d=\u2225rx(i)\u2212ry(i)\u22252 , which is also considered as a slack variable for determining semantic equivalence .", "entities": []}, {"text": "The underlying interpretation is that vectors whose distances from rx(i)(orry(i ) ) do not exceed d , are semantically - equivalent to both rx(i)and ry(i ) .", "entities": []}, {"text": "To make \u03bd(rx(i ) , ry(i))conform to the interpretation , we employ a similar method as in ( Zheng et al . , 2019 ; Wei et al . , 2021 ) to optimize the semantic encoder with the tangential contrast .", "entities": []}, {"text": "Specifically , we construct negative samples by applying the convex interpolation between the current instance and other ones in the same training batch for instance comparison .", "entities": []}, {"text": "And the tangent points ( i.e. , the points on the boundary ) are considered as the critical states of semantic equivalence .", "entities": []}, {"text": "The training objective is formulated as : Jctl(\u0398\u2032 ) = E(x(i),y(i))\u223cB\u0012 loges\u0000 rx(i),ry(i)\u0001 es\u0000 rx(i),ry(i)\u0001", "entities": []}, {"text": "+ \u03be\u0013 , \u03be=|B|X j&j\u0338=i\u0010 es\u0000 ry(i),ry\u2032(j)\u0001", "entities": []}, {"text": "+ es\u0000 rx(i),rx\u2032(j)\u0001\u0011 , ( 4 ) where Bindicates a batch of sentence pairs randomly selected from the training corpora C , and s(\u00b7)is the score function that computes the cosine similarity between two vectors .", "entities": []}, {"text": "The negative samplesrx\u2032(j)andry\u2032(j)are designed as the following7932", "entities": []}, {"text": "Figure 3 : The geometric diagram of the proposed MGRC sampling .", "entities": []}, {"text": "rxandryare the representations of the source sentence xand the target sentence y , respectively .", "entities": []}, {"text": "To construct the augmented sample , a straightforward idea is that : ( 1 ) transform the norm or the direction of \u02dcr= ry\u2212rx , formulated as \u03c9\u2299\u02dcr(e.g . , the black dashed arrow ) , in which each element \u03c9i\u2208[\u22121,1 ] , and ( 2 ) combine rx(orry ) and the transformation \u03c9\u2299\u02dcras \u02c6rx = rx+\u03c9\u2299\u02dcr(i.e . , the red dashed arrow ) .", "entities": []}, {"text": "interpolation : rx\u2032(j)=rx(i)+\u03bbx(rx(j)\u2212rx(i ) ) , \u03bbx\u2208(d d\u2032x,1 ] , ry\u2032(j)=ry(i)+\u03bby(ry(j)\u2212ry(i ) ) , \u03bby\u2208(d d\u2032y,1],(5 ) where d\u2032 x=\u2225rx(i)\u2212rx(j)\u2225andd\u2032 y=\u2225ry(i)\u2212", "entities": []}, {"text": "ry(j)\u2225.", "entities": []}, {"text": "The two equations in Eq .", "entities": []}, {"text": "( 5 ) set up when d\u2032 xandd\u2032 yare larger than drespectively , or else rx\u2032(j)=rx(j)andry\u2032(j)=ry(j ) .", "entities": []}, {"text": "According to this design , an adjacency semantic region for the i - th training instance can be fully established by interpolating various instances in the same training batch .", "entities": []}, {"text": "We follow Wei et al .", "entities": []}, {"text": "( 2021 ) to adaptively adjust the value of \u03bbx(or\u03bby ) during the training process , and refer to the original paper for details .", "entities": []}, {"text": "MGRC Sampling To obtain augmented data from the adjacency semantic region for the training instance ( x , y ) , we introduce a Mixed Gaussian Recurrent Chain ( denoted by MGRC ) algorithm to design an efficient and effective sampling strategy .", "entities": []}, {"text": "As illustrated in Figure 3 , we first transform the bias vector \u02dcr = ry\u2212rxaccording to a predefined scale vector \u03c9 , that is \u03c9\u2299\u02dcr , where \u2299 is the element - wise product operation .", "entities": []}, {"text": "Then , we construct a novel sample \u02c6r = r+\u03c9\u2299\u02dcrfor augmenting the current instance , in which ris either rx orry .", "entities": []}, {"text": "As a consequence , the goal of the sampling strategy turns into find a set of scale vectors , i.e. { \u03c9(1 ) , \u03c9(2 ) , ... , \u03c9(K ) } .", "entities": []}, {"text": "Intuitively , we can assume that\u03c9follows a distribution with universal or Gaussian forms , despite the latter demonstrates better results in our experience .", "entities": []}, {"text": "Formally , we design aAlgorithm 1", "entities": []}, {"text": "MGRC Sampling Input : The representations of the training instance ( x , y ) , i.e.rxandry .", "entities": []}, {"text": "Output : A set of augmented samples R = { \u02c6r(1),\u02c6r(2 ) , ... , \u02c6r(K ) } 1 : Normalizing the importance of each element in \u02dcr = ry\u2212 rx : Wr=|\u02dcr|\u2212min(|\u02dcr| ) max(|\u02dcr|)\u2212min(|\u02dcr| ) 2 : Setk= 1,\u03c9(1)\u223c N(0,diag(W2 r)),\u02c6r(1)=r+\u03c9(1)\u2299 ( ry\u2212rx ) 3 : Initialize the set of samples as R={\u02c6r(1 ) } .", "entities": []}, {"text": "4 : while k\u2264(K\u22121)do 5 : k\u2190k+ 1 6 : Calculate the current scale vector : \u03c9(k)\u223c p(\u03c9|\u03c9(1 ) , \u03c9(2 ) , ... , \u03c9(k\u22121)according to Eq .", "entities": []}, {"text": "( 6 ) .", "entities": []}, {"text": "7 : Calculate the current sample : \u02c6r(k)=r+\u03c9(k)\u2299(ry\u2212 rx ) .", "entities": []}, {"text": "8 : R \u2190 RS{\u02c6r(k ) } .", "entities": []}, {"text": "9 : end while mixed Gaussian distribution as follow : \u03c9(k)\u223cp(\u03c9|\u03c9(1 ) , \u03c9(2 ) , ... , \u03c9(k\u22121 ) ) , p = \u03b7N\u0000 0,diag(W2 r)\u0001 + ( 1.0\u2212\u03b7)N\u00121 k\u22121k\u22121X i=1\u03c9(i),1\u0013 .(6 )", "entities": []}, {"text": "This framework unifies the recurrent chain and the rejection sampling mechanism .", "entities": []}, {"text": "Concretely , we first normalize the importance of each dimension in \u02dcras Wr=|\u02dcr|\u2212min(|\u02dcr| ) max(|\u02dcr|)\u2212min(|\u02dcr| ) , the operation |\u00b7|takes the absolute value of each element in the vector , which means the larger the value of an element is the more informative it is .", "entities": []}, {"text": "Thus N(0,diag(W2 r))limits the range of sampling to a subspace of the adjacency semantic region , and rejects to conduct sampling from the uninformative dimensions .", "entities": []}, {"text": "Moreover , N(1 k\u22121Pk\u22121 i=1\u03c9(i),1)simulates a recurrent chain that generates a sequence of reasonable vectors where the current one is dependent on the prior vectors .", "entities": []}, {"text": "The reason for this design is that we expect that pin Eq .", "entities": []}, {"text": "( 6 ) can become a stationary distribution with the increase of the number of samples , which describes the fact that the diversity of each training instance is not infinite .", "entities": [[13, 16, "HyperparameterName", "number of samples"]]}, {"text": "\u03b7is a hyperparameter to balance the importance of the above two Gaussian forms .", "entities": []}, {"text": "For a clearer presentation , Algorithm 1 summarizes the sampling process .", "entities": []}, {"text": "2.2 Training and Inference The training objective in our approach is a combination of Jmle(\u0398)in Eq .", "entities": []}, {"text": "( 3 ) and Jctl(\u0398\u2032)in Eq .", "entities": []}, {"text": "( 4 ) .", "entities": []}, {"text": "In practice , we introduce a two - phase training procedure with mini - batch losses .", "entities": []}, {"text": "Firstly , we train the semantic encoder from scratch using the task - specific data , i.e. \u0398\u2032\u2217= argmax\u0398\u2032Jctl(\u0398\u2032).7933", "entities": []}, {"text": "Method # Params .", "entities": [[2, 3, "MetricName", "Params"]]}, {"text": "Valid .", "entities": []}, {"text": "MT02 MT03 MT04 MT05 MT08 Avg .", "entities": []}, {"text": "Transformer , base ( our implementation ) 84 M 45.09 45.63 45.07 46.59 45.84 36.18 43.86 Back - translation ( Sennrich et al . , 2016a)\u221784 M 46.71 47.22 46.86 47.36 46.65 36.69 44.96 SwitchOut ( Wang et al . , 2018)\u221784 M 46.13 46.72 45.69 47.08 46.19 36.47 44.43 SemAug ( Wei et al . , 2020a )", "entities": [[0, 1, "MethodName", "Transformer"]]}, {"text": "86 M - - - 49.15 49.21 40.94 AdvAug ( Cheng et al . , 2020 ) - 49.26 49.03 47.96 48.86 49.88 39.63 47.07 CSANMT , base 96 M 50.46 49.65 48.84 49.80 50.40 41.63 48.06 Table 1 : BLEU scores", "entities": [[40, 41, "MetricName", "BLEU"]]}, {"text": "[ % ] on Zh \u2192En translation .", "entities": []}, {"text": "\u201c Params . \u201d denotes the number of parameters ( M = million ) .", "entities": [[1, 2, "MetricName", "Params"], [6, 9, "HyperparameterName", "number of parameters"]]}, {"text": "\u201c \u2217 \u201d indicates the results obtained by our implementation , we construct multiple pseudo sources for each target during back - translation but rather introducing extra monolingual corpora as in ( Wei et al . , 2020a ) for fairer comparisons .", "entities": []}, {"text": "Secondly , we optimize the encoder - decoder model by maximizing the log - likelihood , i.e. \u0398\u2217= argmax\u0398Jmle(\u0398 ) , and fine - tune the semantic encoder with a small learning rate at the same time .", "entities": [[12, 15, "MetricName", "log - likelihood"], [31, 33, "HyperparameterName", "learning rate"]]}, {"text": "During inference , the sequence of target words is generated auto - regressively , which is almost the same as the vanilla Transformer ( Vaswani et", "entities": [[22, 23, "MethodName", "Transformer"]]}, {"text": "al . , 2017 ) .", "entities": []}, {"text": "A major difference is that our method involves the semantic vector of the input sequence for generation : y\u2217 t= argmaxytP(\u00b7|y < t , x , rx ; \u0398 ) , where rx = \u03c8(x ; \u0398\u2032 ) .", "entities": [[28, 29, "HyperparameterName", "\u0398"]]}, {"text": "This module is plug - in - use as well as is agnostic to model architectures .", "entities": []}, {"text": "3 Experiments We first apply CSANMT to NIST Chinese - English ( Zh\u2192En ) , WMT14 English - German ( En \u2192De ) and English - French ( En \u2192Fr ) tasks , and conduct extensive analyses for better understanding the proposed method .", "entities": [[15, 16, "DatasetName", "WMT14"]]}, {"text": "And then we generalize the capability of our method to low - resource IWSLT tasks .", "entities": []}, {"text": "3.1 Settings Datasets .", "entities": []}, {"text": "For the Zh \u2192En task , the LDC corpus is taken into consideration , which consists of 1.25 M sentence pairs with 27.9 M Chinese words and 34.5 M English words , respectively .", "entities": []}, {"text": "The NIST 2006 dataset is used as the validation set for selecting the best model , and NIST 2002 ( MT02 ) , 2003 ( MT03 ) , 2004 ( MT04 ) , 2005 ( MT05 ) , 2008 ( MT08 ) are used as the test sets .", "entities": []}, {"text": "For the En \u2192De task , we employ the popular WMT14 dataset , which consists of approximately 4.5 M sentence pairs for training .", "entities": [[10, 11, "DatasetName", "WMT14"]]}, {"text": "We select newstest2013 as the validation set and newstest2014 as the test set .", "entities": []}, {"text": "For the En \u2192Fr task , we use the significantly larger WMT14 dataset consisting of 36 M sentence pairs .", "entities": [[11, 12, "DatasetName", "WMT14"]]}, {"text": "The combination of { newstest2012 , 2013 } was used for model selection and the experimental results were reported on newstest2014 .", "entities": [[11, 13, "TaskName", "model selection"]]}, {"text": "RefertoAppendix A for more details .", "entities": []}, {"text": "Training Details .", "entities": []}, {"text": "We implement our approach on top of the Transformer ( Vaswani et al . , 2017 ) .", "entities": [[8, 9, "MethodName", "Transformer"]]}, {"text": "The semantic encoder is a 4 - layer transformer encoder with the same hidden size as the backbone model .", "entities": []}, {"text": "Following sentence - bert ( Reimers and Gurevych , 2019 ) , we average the outputs of all positions as the sequence - level representation .", "entities": []}, {"text": "The learning rate for finetuning the semantic encoder at the second training stage is set as1e\u22125 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "All experiments are performed on 8 V100 GPUs .", "entities": []}, {"text": "We accumulate the gradient of 8 iterations and update the models with a batch of about 65 K tokens .", "entities": []}, {"text": "The hyperparameters Kand \u03b7inMGRC sampling are tuned on the validation set with the range of K\u2208 { 10,20,40,80}and \u03b7\u2208 { 0.15,0.30,0.45,0.6,0.75,0.90 } .", "entities": []}, {"text": "We use the default setup of K= 40 for all three tasks , \u03b7= 0.6 for both Zh \u2192En and En \u2192De while \u03b7= 0.45for En\u2192Fr .", "entities": []}, {"text": "For evaluation , the beam size and length penalty are set to 4 and 0.6 for the En \u2192De as well as En\u2192Fr , while 5 and 1.0 for the Zh \u2192En task .", "entities": []}, {"text": "3.2 Main Results Results of Zh \u2192En .", "entities": []}, {"text": "Table 1 shows the results on the Chinese - to - English translation task .", "entities": []}, {"text": "From the results , we can conclude that our approach outperforms existing augmentation strategies such as back - translation ( Sennrich et al . , 2016a ; Wei et al . , 2020a ) and switchout ( Wang et al . , 2018 ) by a large margin ( up to 3.63 BLEU ) , which verifies that augmentation in continuous space is more effective than methods with discrete manipulations .", "entities": [[52, 53, "MetricName", "BLEU"]]}, {"text": "Compared to the approaches that replace words in the embedding space ( Cheng et al . , 2020 ) , our approach also demonstrates superior performance , which reveals that sentence - level augmentation with continuous semantics works better on generalizing to unseen instances .", "entities": []}, {"text": "Moreover , compared to the vanilla Transformer , our approach consistently7934", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "ModelWMT 2014", "entities": []}, {"text": "En \u2192De", "entities": []}, {"text": "WMT", "entities": []}, {"text": "2014", "entities": []}, {"text": "En \u2192Fr # Params .", "entities": [[3, 4, "MetricName", "Params"]]}, {"text": "BLEU SacreBLEU # Params .", "entities": [[0, 1, "MetricName", "BLEU"], [1, 2, "MetricName", "SacreBLEU"], [3, 4, "MetricName", "Params"]]}, {"text": "BLEU SacreBLEU Transformer , base ( our implementation )", "entities": [[0, 1, "MetricName", "BLEU"], [1, 2, "MetricName", "SacreBLEU"], [2, 3, "MethodName", "Transformer"]]}, {"text": "62 M 27.67 26.8 67 M 40.53 38.5 Transformer , big ( our implementation ) 213 M 28.79 27.7 222 M 42.36 40.3 Back - Translation ( Sennrich et al . , 2016a)\u2217213 M 29.25 28.2 222 M 41.73 39.7 SwitchOut ( Wang et al . , 2018)\u2217213 M 29.18 28.1 222 M 41.62 39.6 SemAug ( Wei et al . , 2020a ) 221 M 30.29 - 230 M 42.92 AdvAug ( Cheng et al . , 2020)\u202065 M 29.57 - - - Data Diversification ( Nguyen et al . , 2020)\u20201260 M 30.70 -\u20201332 M 43.70 CSANMT , base 74 M 30.16 29.2 80 M 42.40 40.3 CSANMT , big 265 M 30.94 29.8 274 M 43.68 41.6 Table 2 : BLEU scores", "entities": [[8, 9, "MethodName", "Transformer"], [25, 26, "TaskName", "Translation"], [123, 124, "MetricName", "BLEU"]]}, {"text": "[ % ] on the WMT14 En \u2192De and En \u2192Fr tasks .", "entities": [[5, 6, "DatasetName", "WMT14"]]}, {"text": "\u201c \u2217 \u201d indicates the results obtained by our implementation , which is the same in Table 1 .", "entities": []}, {"text": "\u201c \u2020 \u201d denote estimate values .", "entities": []}, {"text": "We further compare against the baselines with increased amounts of parameters , and investigate the performance of CSANMT equipped with much stronger baselines ( e.g. deep and scale Transformers ( Ott et al . , 2018 ; Wang et al . , 2019 ; Wei et al . , 2020b ) ) in Sec .", "entities": []}, {"text": "3.3 .", "entities": []}, {"text": "( a ) NIST Zh \u2192En   ( b )", "entities": []}, {"text": "WMT14 En \u2192De   ( c )", "entities": [[0, 1, "DatasetName", "WMT14"]]}, {"text": "WMT14 En \u2192Fr   ( d ) Effect of \u03b7 .", "entities": [[0, 1, "DatasetName", "WMT14"]]}, {"text": "Figure 4 : Effects of Kand\u03b7on validation sets .", "entities": []}, {"text": "( a ) , ( b ) and ( c ) depict the BLEU curves with different values of K on Zh\u2192En , En \u2192De and En \u2192Fr , respectively .", "entities": [[13, 14, "MetricName", "BLEU"]]}, {"text": "( d ) demonstrates the performances of \u03b7with different values .", "entities": []}, {"text": "achieves promising improvements on five test sets .", "entities": []}, {"text": "Results of En \u2192De and En \u2192Fr .", "entities": []}, {"text": "From Table 2 , our approach consistently performs better than existing methods ( Sennrich et al . , 2016a ; Wang et al . , 2018 ; Wei et al . , 2020a ; Cheng et al . , 2020 ) , yielding significant gains ( 0.65 \u223c1.76 BLEU ) on the En \u2192De and En \u2192Fr tasks .", "entities": [[48, 49, "MetricName", "BLEU"]]}, {"text": "An exception is that Nguyen et al .", "entities": []}, {"text": "( 2020 ) achieved comparable results with ours via multiple forward and backward NMT models , thus data diversification intuitively demonstrates lower training efficiency .", "entities": []}, {"text": "Moreover , we observe that CSANMT gives 30.16 BLEU on the En \u2192De task with the base setting , significantly outperforming the vanilla Transformer by 2.49 BLEU points .", "entities": [[8, 9, "MetricName", "BLEU"], [23, 24, "MethodName", "Transformer"], [26, 27, "MetricName", "BLEU"]]}, {"text": "Our approach yields a further improvement of 0.68 BLEU by equipped with the wider architecture , demonstrating superiority over the standard Transformer by 2.15 BLEU .", "entities": [[8, 9, "MetricName", "BLEU"], [21, 22, "MethodName", "Transformer"], [24, 25, "MetricName", "BLEU"]]}, {"text": "Similar observations can be drawn for the En \u2192Fr task .", "entities": []}, {"text": "3.3 Analysis Effects of Kand\u03b7 .", "entities": []}, {"text": "Figure 4 illustrates how the hyper - parameters Kand\u03b7inMGRC sampling affect the translation quality .", "entities": []}, {"text": "From Figures 4(a)-4(c),we can observe that gradually increasing the number of samples significantly improves BLEU scores , which demonstrates large gaps between K= 10 andK= 40 .", "entities": [[9, 12, "HyperparameterName", "number of samples"], [14, 15, "MetricName", "BLEU"]]}, {"text": "However , assigning larger values ( e.g. , 80 ) toKdoes not result in further improvements among all three tasks .", "entities": []}, {"text": "We conjecture that the reasons are two folds : ( 1 ) it is fact that the diversity of each training instance is not infinite and thus MGRC gets saturated is inevitable with Kincreasing .", "entities": []}, {"text": "( 2 ) MGRC sampling with a scaled item ( i.e. , Wr ) may degenerate to traverse in the same place .", "entities": []}, {"text": "This prompts us to design more sophisticated algorithms in future work .", "entities": []}, {"text": "In our experiments , we default set K= 40 to achieve a balance between the training efficiency and translation quality .", "entities": []}, {"text": "Figure 4(d ) shows the effect of \u03b7on validation sets , which balances the importance of two Gaussian forms during the sampling process .", "entities": []}, {"text": "The setting of\u03b7= 0.6achieves the best results on both the Zh\u2192En and En \u2192De tasks , and \u03b7= 0.45consistently outperforms other values on the En \u2192Fr task .", "entities": []}, {"text": "Lexical diversity and semantic faithfulness .", "entities": []}, {"text": "We demonstrate both the lexical diversity ( measured by TTR = num . of types", "entities": []}, {"text": "num . of tokens ) of various trans-7935", "entities": []}, {"text": "0.00 0.25 0.50 0.75 1.00 Ratio of the Training Data1620242832BLEU ( % )", "entities": []}, {"text": "Ours Back - translation + Mono .", "entities": []}, {"text": "Back - translation BaselineFigure 5 : Comparison between discrete and continuous augmentations with different ratios of the training data .", "entities": []}, {"text": "Model BLEU Dec. speed Transformer - base 27.67 reference Default 4 - layer semantic encoder 30.16 0.95 \u00d7 Remove the extra semantic encoder 28.71 1.0 \u00d7 Take PTMs as the semantic encoder 31.10 0.62 \u00d7 Table 3 : Effect of the semantic encoder variants .", "entities": [[1, 2, "MetricName", "BLEU"], [4, 5, "MethodName", "Transformer"]]}, {"text": "lations and the semantic faithfulness of machine translated ones ( measured by BLEURT with considering human translations as the references ) in Table 4 .", "entities": []}, {"text": "It is clear that CSANMT substantially bridge the gap of the lexical diversity between translations produced by human and machine .", "entities": []}, {"text": "Meanwhile , CSANMT shows a better capability on preserving the semantics of the generated translations than Transformer .", "entities": [[16, 17, "MethodName", "Transformer"]]}, {"text": "We intuitively attribute the significantly increases of BLEU scores on all datasets to these two factors .", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "We also have studied the robustness of CSANMT towards noisy inputs and the translationese effect , see Appendix D for details .", "entities": []}, {"text": "Effect of the semantic encoder .", "entities": []}, {"text": "We introduce two variants of the semantic encoder to investigate its performance on En \u2192De validation set .", "entities": []}, {"text": "Specifically , ( 1 ) we remove the extra semantic encoder and construct the sentence - level representations by averaging the sequence of outputs of the vanilla sentence encoder .", "entities": []}, {"text": "( 2 ) We replace the default 4 - layer semantic encoder with a large pre - trained model ( PTM ) ( i.e. , XLM - R ( Conneau et al . , 2020 ) ) .", "entities": [[25, 26, "MethodName", "XLM"]]}, {"text": "The results are reported in Table 3 .", "entities": []}, {"text": "Comparing line 2 with line 3 , we can conclude that an extra semantic encoder is necessary for constructing the universal continuous space among different languages .", "entities": []}, {"text": "Moreover , when the large PTM is incorporated , our approach yields further improvements , but it causes massive computational overhead .", "entities": []}, {"text": "Comparison between discrete and continuous augmentations .", "entities": []}, {"text": "To conduct detailed compar - TTR BLEURT Score", "entities": [[7, 8, "MetricName", "Score"]]}, {"text": "Zh De Fr Zh De Fr Human 7.58 % 22.08 % 13.98 % - - Trans .", "entities": []}, {"text": "6.95 % 20.32 % 11.76 % 0.570 0.635 0.696 CSANMT 7.13 % 21.26 % 12.91 % 0.581 0.684 0.739 Table 4 : TTR ( Type - Token - Ratio ) ( Templin , 1957 ) and BLEURT scores of Zh \u2192En and En \u2192X translations produced by Human , vanilla Transformer ( written as Trans . ) , and CSANMT .", "entities": [[50, 51, "MethodName", "Transformer"]]}, {"text": "\u201c Human \u201d translations mean the references contained in the standard test sets .", "entities": []}, {"text": "Refer to Appendix D for the results on robustness test sets .", "entities": []}, {"text": "# Objective Sampling BLEU 1 Default tangential CTL Default M GRC 30.16 2 Default tangential CTL M GRC w/o recurrent chain 29.64 3 Default tangential CTL M GRC w/ uniform dist .", "entities": [[3, 4, "MetricName", "BLEU"]]}, {"text": "29.78 4 Variational Inference Gaussian sampling 28.07 5 Cosine similarity Default M GRC 28.19 Table 5 : Effect of MGRC sampling and tangential contrastive learning on En \u2192De validation set .", "entities": [[2, 4, "MethodName", "Variational Inference"], [23, 25, "MethodName", "contrastive learning"]]}, {"text": "isons between different augmentation methods , we asymptotically increase the training data to analyze the performance of them on the En \u2192De translation .", "entities": []}, {"text": "As in Figure 5 , our approach significantly outperforms the back - translation method on each subset , whether or not extra monolingual data ( Sennrich et al . , 2016a ) is introduced .", "entities": []}, {"text": "These results demonstrate the stronger ability of our approach than discrete augmentation methods on generalizing to unseen instances with the same set of observed data points .", "entities": []}, {"text": "Encouragingly , our approach achieves comparable performance with the baseline model with only 25 % of training data , which indicates that our approach has great potential to achieve good results with very few data .", "entities": []}, {"text": "Effect of MGRC sampling and tangential contrastive learning .", "entities": [[6, 8, "MethodName", "contrastive learning"]]}, {"text": "To better understand the effectiveness of the MGRC sampling and the tangential contrastive learning , we conduct detailed ablation studies in Table 5 .", "entities": [[12, 14, "MethodName", "contrastive learning"]]}, {"text": "The details of four variants with different objectives or sampling strategies are shown in Appendix C .", "entities": []}, {"text": "From the results , we can observe that both removing the recurrent dependence and replacing the Gaussian forms with uniform distributions make the translation quality decline , but the former demonstrates more drops .", "entities": []}, {"text": "We also have tried the training objectives with other forms , such as variational inference and cosine similarity , to optimize the semantic encoder .", "entities": [[13, 15, "MethodName", "variational inference"]]}, {"text": "However , the BLEU score drops significantly .", "entities": [[3, 5, "MetricName", "BLEU score"]]}, {"text": "Training Cost and Convergence .", "entities": []}, {"text": "Figure 67936", "entities": []}, {"text": "0.0 2.5 5.0 7.5 10.0 Iterations ( x10000)1620242832BLEU ( % ) CSANMT Back - Translation TransformerFigure 6 : BLEU curves over iterations on the WMT14 English \u2192German test set .", "entities": [[14, 15, "TaskName", "Translation"], [18, 19, "MetricName", "BLEU"], [24, 25, "DatasetName", "WMT14"]]}, {"text": "Note that back - translation is initialized from the vanilla Transformer .", "entities": [[10, 11, "MethodName", "Transformer"]]}, {"text": "30.445.650.355.665.973.679.680.5 30.953.866.568.274.679.282.284 3045607590 < 1[1,10)[10,50)[50,200)[200,500)[500,1000)[1000,10000)>=10000Word Accuracy ( % ) Word FrequencyTransformerCSANMT Figure 7 : Comparison between the vanilla Transformer andCSANMT on prediction accuracy of words with different frequencies .", "entities": [[5, 9, "MetricName", "Accuracy ( % )"], [18, 19, "MethodName", "Transformer"], [22, 23, "MetricName", "accuracy"]]}, {"text": "shows the evolution of BLEU scores during training .", "entities": [[4, 5, "MetricName", "BLEU"]]}, {"text": "It is obvious that our method performs consistently better than both the vanilla Transformer and the back - translation method at each iteration ( except for the first 10 K warm - up iterations , where the former one has access to less unique training data than the latter two due to the Ktimes over - sampling ) .", "entities": [[13, 14, "MethodName", "Transformer"]]}, {"text": "For the vanilla Transformer , the BLEU score reaches its peak at about 52 K iterations .", "entities": [[3, 4, "MethodName", "Transformer"], [6, 8, "MetricName", "BLEU score"]]}, {"text": "In comparison , both CSANMT and the back - translation method require 75 K updates for convergence .", "entities": []}, {"text": "In other words , CSANMT spends 44 % more training costs than the vanilla Transformer , due to the longer time to make the NMT model converge with augmented training instances .", "entities": [[14, 15, "MethodName", "Transformer"]]}, {"text": "This is the same as the back - translation method .", "entities": []}, {"text": "Word prediction accuracy .", "entities": [[2, 3, "MetricName", "accuracy"]]}, {"text": "Figure 7 illustrates the prediction accuracy of both frequent and rare words .", "entities": [[5, 6, "MetricName", "accuracy"]]}, {"text": "As expected , CSANMT generalizes to rare words better than the vanilla Transformer , and the gap of word prediction accuracy is as large as 16 % .", "entities": [[12, 13, "MethodName", "Transformer"], [20, 21, "MetricName", "accuracy"]]}, {"text": "This indicates that the NMT model alleviates the probability under - estimation of rare words via continuous semantic augmentation .", "entities": []}, {"text": "Effects of Additional Parameters and StrongModel # Params .", "entities": [[7, 8, "MetricName", "Params"]]}, {"text": "En \u2192De", "entities": []}, {"text": "En \u2192Fr Transformer ( Vaswani et al . , 2017)\u2020213 M 28.40 41.80 Transformer ( our impl . )", "entities": [[2, 3, "MethodName", "Transformer"], [13, 14, "MethodName", "Transformer"]]}, {"text": "213 M 28.79 42.36 Transformer ( our impl . , 10 layers ) 265 M 29.08 42.49 CSANMT 265 M 30.94 43.68 Scale Trans .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "( Ott et al . , 2018)\u2020210 M 29.30 43.20 DEEP(Wang et al . , 2019 ) 350 M 30.26 43.24 MSC(Wei et al . , 2020b)\u2020512 M 30.56 Our C SANMT with Scale Trans .", "entities": []}, {"text": "( Ott et al . , 2018 ) 263 M 31.37 44.12 DEEP(Wang et al . , 2019 ) 405 M 31.35 MSC(Wei et al . , 2020b )", "entities": []}, {"text": "566 M 31.49 Table 6 : BLEU scores", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "[ % ] on WMT14 testsets for the English - German ( En \u2192De ) and English - French ( En\u2192Fr ) tasks .", "entities": [[4, 5, "DatasetName", "WMT14"]]}, {"text": "Superscript\u2020denotes the numbers are reported from the paper , others are based on our runs .", "entities": []}, {"text": "\u201c - \u201d means omitted results because of the limitations of GPU resources .", "entities": []}, {"text": "\u201c 10 layers \u201d means that we construct the Transformer with a 10 - layer encoder , thus it has the same amount of parameters as our model .", "entities": [[9, 10, "MethodName", "Transformer"]]}, {"text": "Baselines .", "entities": []}, {"text": "In contrast to the vanilla Transformer , CSANMT involves with approximate 20 % additional parameters .", "entities": [[5, 6, "MethodName", "Transformer"]]}, {"text": "In this section , we further compare against the baselines with increased amounts of parameters , and investigate the performance of CSANMT equipped with much stronger baselines ( e.g. deep and scale Transformers ( Ott et al . , 2018 ;", "entities": []}, {"text": "Wang et al . , 2019 ; Wei et al . , 2020b ) ) .", "entities": []}, {"text": "From the results on WMT14 testsets in Table 6 , we can observe that CSANMT still outperforms the vanilla Transformer ( by more than 1.2 BLEU ) under the same amount of parameters , which shows that the additional parameters are not the key to the improvement .", "entities": [[4, 5, "DatasetName", "WMT14"], [19, 20, "MethodName", "Transformer"], [25, 26, "MetricName", "BLEU"]]}, {"text": "Moreover , CSANMT yields at least 0.9 BLEU gains equipped with much stronger baselines .", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "For example , the scale Transformer ( Ott et al . , 2018 ) , which originally gives 29.3 BLEU in the En \u2192De task , now gives 31.37 BLEU with our continuous semantic augmentation strategy .", "entities": [[5, 6, "MethodName", "Transformer"], [19, 20, "MetricName", "BLEU"], [29, 30, "MetricName", "BLEU"]]}, {"text": "It is important to mention that our method can help models to achieve further improvement , even if they are strong enough .", "entities": []}, {"text": "3.4 Low - Resource Machine Translation We further generalize the capability of the proposed CSANMT to various low - resource machine translation tasks , including IWSLT14 English - German and IWSLT17 English - French .", "entities": [[4, 6, "TaskName", "Machine Translation"], [20, 22, "TaskName", "machine translation"]]}, {"text": "The details of the datasets and model configurations can be found in Appendix B .", "entities": []}, {"text": "Table 7 shows the results of different models .", "entities": []}, {"text": "Compared to the vanilla Transformer , the proposed CSANMT improve the BLEU scores of the two tasks by 2.7 and 2.9 points , respectively.7937", "entities": [[4, 5, "MethodName", "Transformer"], [11, 12, "MetricName", "BLEU"]]}, {"text": "Model English - German English - French Transformer 28.64 35.8 Back - translation 29.45 36.3 CSANMT 31.29 38.6 Table 7 : BLEU scores", "entities": [[7, 8, "MethodName", "Transformer"], [21, 22, "MetricName", "BLEU"]]}, {"text": "[ % ] on the IWSLT tasks .", "entities": []}, {"text": "For fairer comparison , all the models are implemented by ourselves using the same backbone , and the extra monolingual corpora is not introduced into back - translation .", "entities": []}, {"text": "This result indicates that the claiming of the continuous semantic augmentation enriching the training corpora with very limited observed instances .", "entities": []}, {"text": "4 Related Work Data Augmentation ( DA ) ( Edunov et al . , 2018 ; Kobayashi , 2018 ; Gao et", "entities": [[3, 5, "TaskName", "Data Augmentation"]]}, {"text": "al . , 2019 ; Khayrallah et al . , 2020 ; Pham et al . , 2021 ) has been widely used in neural machine translation .", "entities": [[25, 27, "TaskName", "machine translation"]]}, {"text": "The most popular one is the family of back - translation ( Sennrich et al . , 2016a ; Nguyen et", "entities": []}, {"text": "al . , 2020 ) , which utilizes a target - to - source model to translate monolingual target sentences back into the source language .", "entities": []}, {"text": "Besides , constructing adversarial training instances with diverse literal forms via word replacing or embedding interpolating ( Wang et al . , 2018 ; Cheng et al . , 2020 ) is beneficial to improve the generalization performance of NMT models .", "entities": []}, {"text": "Vicinal Risk Minimization ( VRM ) ( Chapelle et al . , 2000 ) is another principle of data augmentation , in which DA is formalized as extracting additional pseudo samples from the vicinal distribution of observed instances .", "entities": [[18, 20, "TaskName", "data augmentation"]]}, {"text": "Typically the vicinity of a training example is defined using datasetdependent heuristics , such as color ( scale , mixup ) augmentation ( Simonyan and Zisserman , 2014 ; Krizhevsky et al . , 2012 ; Zhang et al . , 2018 ) in computer vision and adversarial augmentation with manifold neighborhoods ( Ng et al . , 2020 ; Cheng et al . , 2021 ) in NLP .", "entities": [[19, 20, "MethodName", "mixup"]]}, {"text": "Our approach relates to VRM that involves with an adjacency semantic region as the vicinity manifold for each training instance .", "entities": []}, {"text": "Sentence Representation Learning is a well investigated area with dozens of methods ( Kiros et al . , 2015 ; Cer et al . , 2018 ;", "entities": [[1, 3, "TaskName", "Representation Learning"]]}, {"text": "Yang et al . , 2018 ) .", "entities": []}, {"text": "In recent years , the methods built on large pre - trained models ( Devlin et al . , 2019 ; Conneau et al . , 2020 ) have been widely used for learning sentence level representations ( Reimers and Gurevych , 2019 ; Huang et al . , 2019 ; Yang et al . , 2019 ) .", "entities": []}, {"text": "Our work is also related to the methods that aims at learning the uni - versal representation ( Zhang et al . , 2016 ; Schwenk and Douze , 2017 ; Yang et al . , 2021 ) for multiple semantically - equivalent sentences in NMT .", "entities": []}, {"text": "In this context , contrastive learning has become a popular paradigm in NLP ( Kong et al . , 2020 ; Clark et al . , 2020 ; Gao et al . , 2021 ) .", "entities": [[4, 6, "MethodName", "contrastive learning"]]}, {"text": "The most related work are Wei et al .", "entities": []}, {"text": "( 2021 ) and Chi et al .", "entities": []}, {"text": "( 2021 ) , which suggested transforming cross - lingual sentences into a shared vector by contrastive objectives .", "entities": []}, {"text": "5 Conclusion We propose a novel data augmentation paradigm CSANMT , which involves with an adjacency semantic region as the vicinity manifold for each training instance .", "entities": [[6, 8, "TaskName", "data augmentation"]]}, {"text": "This method is expected to make more unseen instances under generalization with very limited training data .", "entities": []}, {"text": "The main components of CSANMT consists of the tangential contrastive learning and the Mixed Gaussian Recurrent Chain ( MGRC ) sampling .", "entities": [[9, 11, "MethodName", "contrastive learning"]]}, {"text": "Experiments on both rich- and low - resource machine translation tasks demonstrate the effectiveness of our method .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In the future work , we would like to further study the vicinal risk minimization with the combination of multi - lingual aligned scenarios and large - scale monolingual data , and development it as a pure data augmentator merged into the vanilla Transformer .", "entities": [[43, 44, "MethodName", "Transformer"]]}, {"text": "Acknowledgments We would like to thank all of the anonymous reviewers ( during ARR Oct. and ARR Dec. ) for the helpful comments .", "entities": []}, {"text": "We also thank Baosong Yang and Dayiheng Liu for their instructive suggestions and invaluable help .", "entities": []}, {"text": "References Mikel Artetxe , Gorka Labaka , Eneko Agirre , and Kyunghyun Cho . 2018 .", "entities": []}, {"text": "Unsupervised neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Daniel Cer , Yinfei Yang , Sheng - yi Kong , Nan Hua , Nicole Limtiaco , Rhomni St. John , Noah Constant , Mario Guajardo - Cespedes , Steve Yuan , Chris Tar , Yun - Hsuan Sung , Brian Strope , and Ray Kurzweil .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Universal sentence encoder .", "entities": []}, {"text": "CoRR , abs/1803.11175 .", "entities": []}, {"text": "Olivier Chapelle , Jason Weston , L\u00e9on Bottou , and Vladimir Vapnik .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "Vicinal risk minimization .", "entities": []}, {"text": "InAdvances in Neural Information Processing Systems , volume 13 .", "entities": []}, {"text": "MIT Press.7938", "entities": []}, {"text": "Yong Cheng , Lu Jiang , and Wolfgang Macherey .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Robust neural machine translation with doubly adversarial inputs .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4324\u20134333 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yong Cheng , Lu Jiang , Wolfgang Macherey , and Jacob Eisenstein .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "AdvAug :", "entities": []}, {"text": "Robust adversarial augmentation for neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5961\u20135970 . Association for Computational Linguistics .", "entities": []}, {"text": "Yong Cheng , Zhaopeng Tu , Fandong Meng , Junjie Zhai , and Yang Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Towards robust neural machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1756\u20131766 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yong Cheng , Wei Wang , Lu Jiang , and Wolfgang Macherey .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Self - supervised and supervised joint training for resource - rich machine translation .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "InInternational Conference on Machine Learning , pages 1825\u20131835 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Yong Cheng , Wei Xu , Zhongjun He , Wei He , Hua Wu , Maosong Sun , and Yang Liu . 2016 .", "entities": []}, {"text": "Semi - supervised learning for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , pages 1965\u20131974 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zewen Chi , Li Dong , Furu Wei , Nan Yang , Saksham Singhal , Wenhui Wang , Xia Song , Xian - Ling Mao , Heyan Huang , and Ming Zhou .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "InfoXLM : An information - theoretic framework for cross - lingual language model pre - training .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 3576\u20133588 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kyunghyun Cho , Bart van Merri\u00ebnboer , Caglar Gulcehre , Dzmitry Bahdanau , Fethi Bougares , Holger Schwenk , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Learning phrase representations using RNN encoder \u2013 decoder for statistical machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1724 \u2013 1734 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kevin Clark , Minh - Thang Luong , Quoc V .", "entities": []}, {"text": "Le , and Christopher D. Manning .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "ELECTRA : pretraining text encoders as discriminators rather than generators .", "entities": [[0, 1, "MethodName", "ELECTRA"]]}, {"text": "In 8th International Conference on Learning Representations , ICLR 2020 .", "entities": []}, {"text": "Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , Francisco Guzm\u00e1n , Edouard Grave , Myle Ott , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unsupervisedcross - lingual representation learning at scale .", "entities": [[3, 5, "TaskName", "representation learning"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8440\u20138451 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sergey Edunov , Myle Ott , Michael Auli , and David Grangier .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Understanding back - translation at scale .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 489\u2013500 . Association for Computational Linguistics .", "entities": []}, {"text": "Sergey Edunov , Myle Ott , Marc\u2019Aurelio Ranzato , and Michael Auli .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "On the evaluation of machine translation systems trained with back - translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 2836 \u2013 2846 . Association for Computational Linguistics .", "entities": []}, {"text": "Marzieh Fadaee , Arianna Bisazza , and Christof Monz . 2017 .", "entities": []}, {"text": "Data augmentation for low - resource neural machine translation .", "entities": [[0, 2, "TaskName", "Data augmentation"], [3, 9, "TaskName", "low - resource neural machine translation"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 567\u2013573 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Fei Gao , Jinhua Zhu , Lijun Wu , Yingce Xia , Tao Qin , Xueqi Cheng , Wengang Zhou , and Tie - Yan Liu . 2019 .", "entities": []}, {"text": "Soft contextual data augmentation for neural machine translation .", "entities": [[2, 4, "TaskName", "data augmentation"], [6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5539\u20135544 . Association for Computational Linguistics .", "entities": []}, {"text": "Tianyu Gao , Xingcheng Yao , and Danqi Chen . 2021 .", "entities": []}, {"text": "SimCSE : Simple contrastive learning of sentence embeddings .", "entities": [[0, 1, "MethodName", "SimCSE"], [3, 5, "MethodName", "contrastive learning"], [6, 8, "TaskName", "sentence embeddings"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894\u20136910 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Di He , Yingce Xia , Tao Qin , Liwei Wang , Nenghai Yu , Tie - Yan Liu , and Wei - Ying Ma . 2016 .", "entities": []}, {"text": "Dual learning for machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In Advances in Neural Information Processing Systems 29 , NIPS2016 , December 5 - 10 , 2016 , Barcelona , Spain , pages 820\u2013828 .", "entities": []}, {"text": "Junxian He , Jiatao Gu , Jiajun Shen , and Marc\u2019Aurelio Ranzato . 2020 .", "entities": []}, {"text": "Revisiting self - training for neural sequence generation .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Vu Cong Duy Hoang , Philipp Koehn , Gholamreza Haffari , and Trevor Cohn .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Iterative backtranslation for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2nd Workshop on Neural Machine7939", "entities": []}, {"text": "Translation and Generation , pages 18\u201324 .", "entities": [[0, 1, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Haoyang Huang , Yaobo Liang , Nan Duan , Ming Gong , Linjun Shou , Daxin Jiang , and Ming Zhou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unicoder :", "entities": []}, {"text": "A universal language encoder by pretraining with multiple cross - lingual tasks .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2485\u20132494 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kenji Imamura , Atsushi Fujita , and Eiichiro Sumita .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Enhancement of encoder and attention using target monolingual corpora in neural machine translation .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation , pages 55\u201363 .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mohit Iyyer , Varun Manjunatha , Jordan Boyd - Graber , and Hal Daum\u00e9 III .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Deep unordered composition rivals syntactic methods for text classification .", "entities": [[7, 9, "TaskName", "text classification"]]}, {"text": "InProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing , pages 1681\u20131691 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Huda Khayrallah , Brian Thompson , Matt Post , and Philipp Koehn .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Simulated multiple reference training improves low - resource machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 82\u201389 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ryan Kiros , Yukun Zhu , Russ R Salakhutdinov , Richard Zemel , Raquel Urtasun , Antonio Torralba , and Sanja Fidler .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Skip - thought vectors .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 3294 \u2013 3302 .", "entities": []}, {"text": "Sosuke Kobayashi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Contextual augmentation : Data augmentation by words with paradigmatic relations .", "entities": [[3, 5, "TaskName", "Data augmentation"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 452\u2013457 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Philipp Koehn , Hieu Hoang , Alexandra Birch , Chris Callison - Burch , Marcello Federico , Nicola Bertoldi , Brooke Cowan , Wade Shen , Christine Moran , Richard Zens , Chris Dyer , Ond \u02c7rej Bojar , Alexandra Constantin , and Evan Herbst .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Moses : Open source toolkit for statistical machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions , pages 177\u2013180 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Lingpeng Kong , Cyprien de Masson d\u2019Autume , Lei Yu , Wang Ling , Zihang Dai , and Dani Yogatama .", "entities": []}, {"text": "2020.A mutual information maximization perspective of language representation learning .", "entities": [[7, 9, "TaskName", "representation learning"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Alex Krizhevsky , Ilya Sutskever , and Geoffrey E Hinton .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Imagenet classification with deep convolutional neural networks .", "entities": [[0, 1, "DatasetName", "Imagenet"]]}, {"text": "In Advances in Neural Information Processing Systems , volume 25 .", "entities": []}, {"text": "Guillaume Lample , Alexis Conneau , Ludovic Denoyer , and Marc\u2019Aurelio Ranzato .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Unsupervised machine translation using monolingual corpora only .", "entities": [[0, 3, "TaskName", "Unsupervised machine translation"]]}, {"text": "InInternational Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Guillaume Lample , Alexis Conneau , Ludovic Denoyer , and Marc\u2019Aurelio Ranzato .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "Unsupervised machine translation using monolingual corpora only .", "entities": [[0, 3, "TaskName", "Unsupervised machine translation"]]}, {"text": "InInternational Conference on Learning Representations .", "entities": []}, {"text": "Guillaume Lample , Myle Ott , Alexis Conneau , Ludovic Denoyer , and Marc\u2019Aurelio Ranzato .", "entities": []}, {"text": "2018c .", "entities": []}, {"text": "Phrasebased & neural unsupervised machine translation .", "entities": [[3, 6, "TaskName", "unsupervised machine translation"]]}, {"text": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 5039\u20135049 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nathan Ng , Kyunghyun Cho , and Marzyeh Ghassemi .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "SSMBA : Self - supervised manifold based data augmentation for improving out - of - domain robustness .", "entities": [[7, 9, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1268\u20131283 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xuan - Phi Nguyen , Shafiq Joty , Kui Wu , and Ai Ti Aw . 2020 .", "entities": []}, {"text": "Data diversification : A simple strategy for neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In Advances in Neural Information Processing Systems , volume 33 , pages 10018\u201310029 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Myle Ott , Sergey Edunov , David Grangier , and Michael Auli .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Scaling neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 1\u20139 .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311\u2013318 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hieu Pham , Xinyi Wang , Yiming Yang , and Graham Neubig .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Meta back - translation .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Matt Post . 2018 .", "entities": []}, {"text": "A call for clarity in reporting BLEU scores .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 186 \u2013 191 , Belgium , Brussels .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics.7940", "entities": []}, {"text": "Marc\u2019Aurelio Ranzato , Sumit Chopra , Michael Auli , and Wojciech Zaremba .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Sequence level training with recurrent neural networks .", "entities": []}, {"text": "In 4th International Conference on Learning Representations , ICLR 2016 , San Juan , Puerto Rico , May 2 - 4 , 2016 , Conference Track Proceedings .", "entities": []}, {"text": "Nils Reimers and Iryna Gurevych .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SentenceBERT : Sentence embeddings using Siamese BERTnetworks .", "entities": [[2, 4, "TaskName", "Sentence embeddings"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3982\u20133992 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Holger Schwenk and Matthijs Douze . 2017 .", "entities": []}, {"text": "Learning joint multilingual sentence representations with neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2nd Workshop on Representation Learning for NLP , pages 157\u2013167 , Vancouver , Canada . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Representation Learning"]]}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016a .", "entities": []}, {"text": "Improving neural machine translation models with monolingual data .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 86\u201396 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016b .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , pages 1715\u20131725 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Karen Simonyan and Andrew Zisserman .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Very deep convolutional networks for large - scale image recognition .", "entities": [[8, 10, "TaskName", "image recognition"]]}, {"text": "arXiv preprint arXiv:1409.1556 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kaitao Song , Xu Tan , Tao Qin , Jianfeng Lu , and Tie - Yan Liu . 2019 .", "entities": []}, {"text": "Mass :", "entities": []}, {"text": "Masked sequence to sequence pretraining for language generation .", "entities": [[1, 4, "MethodName", "sequence to sequence"]]}, {"text": "In International Conference on Machine Learning , pages 5926\u20135936 .", "entities": []}, {"text": "Ilya Sutskever , Oriol Vinyals , and Quoc V Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "InAdvances in Neural Information Processing Systems 27 , NIPS 2014 , December 8 - 13 2014 , Montreal , Quebec , Canada , pages 3104\u20133112 .", "entities": []}, {"text": "Mildred C Templin .", "entities": []}, {"text": "1957 .", "entities": []}, {"text": "Certain language skills in children ; their development and interrelationships .", "entities": []}, {"text": "Huihsin Tseng , Pichuan Chang , Galen Andrew , Daniel Jurafsky , and Christopher Manning .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "A conditional random field word segmenter for sighan bakeoff 2005 .", "entities": [[1, 4, "MethodName", "conditional random field"]]}, {"text": "In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 30 , NIPS 2017 4 - 9 December 2017 , Long Beach , CA , USA , pages 5998\u20136008.Vered V olansky , Noam Ordan , and Shuly Wintner .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "On the features of translationese .", "entities": []}, {"text": "Digital Scholarship in the Humanities , 30(1):98\u2013118 .", "entities": []}, {"text": "Qiang Wang , Bei Li , Tong Xiao , Jingbo Zhu , Changliang Li , Derek F. Wong , and Lidia S. Chao . 2019 .", "entities": []}, {"text": "Learning deep transformer models for machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1810\u20131822 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xinyi Wang , Hieu Pham , Zihang Dai , and Graham Neubig .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SwitchOut : an efficient data augmentation algorithm for neural machine translation .", "entities": [[4, 6, "TaskName", "data augmentation"], [9, 11, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 856\u2013861 . Association for Computational Linguistics .", "entities": []}, {"text": "Xiangpeng Wei , Rongxiang Weng , Yue Hu , Luxi Xing , Heng Yu , and Weihua Luo . 2021 .", "entities": []}, {"text": "On learning universal representations across languages .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Xiangpeng Wei , Heng Yu , Yue Hu , Rongxiang Weng , Luxi Xing , and Weihua Luo . 2020a .", "entities": []}, {"text": "Uncertaintyaware semantic augmentation for neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 2724\u20132735 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xiangpeng Wei , Heng Yu , Yue Hu , Yue Zhang , Rongxiang Weng , and Weihua Luo .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Multiscale collaborative deep models for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 414\u2013426 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , et al . 2016 .", "entities": []}, {"text": "Google \u2019s neural machine translation system : Bridging the gap between human and machine translation .", "entities": [[0, 1, "DatasetName", "Google"], [3, 5, "TaskName", "machine translation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1609.08144 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ziang Xie , Sida I Wang , Jiwei Li , Daniel L\u00e9vy , Aiming Nie , Dan Jurafsky , and Andrew Y Ng . 2017 .", "entities": []}, {"text": "Data noising as smoothing in neural network language models .", "entities": []}, {"text": "arXiv preprint arXiv:1703.02573 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mingming Yang , Rui Wang , Kehai Chen , Masao Utiyama , Eiichiro Sumita , Min Zhang , and Tiejun Zhao .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Sentence - level agreement for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3076\u20133082 . Association for Computational Linguistics .", "entities": []}, {"text": "Yinfei Yang , Steve Yuan , Daniel Cer , Sheng - yi Kong , Noah Constant , Petr Pilar , Heming Ge , Yun - Hsuan Sung , Brian Strope , and Ray Kurzweil .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning semantic textual similarity from conversations .", "entities": [[1, 4, "TaskName", "semantic textual similarity"]]}, {"text": "InProceedings of The Third Workshop on Representation Learning for NLP , pages 164\u2013174 .", "entities": [[6, 8, "TaskName", "Representation Learning"]]}, {"text": "Association for Computational Linguistics.7941", "entities": []}, {"text": "Ziyi Yang , Yinfei Yang , Daniel Cer , Jax Law , and Eric Darve . 2021 .", "entities": []}, {"text": "Universal sentence representation learning with conditional masked language model .", "entities": [[2, 4, "TaskName", "representation learning"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6216 \u2013 6228 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Biao Zhang , Deyi Xiong , Jinsong Su , Hong Duan , and Min Zhang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Variational neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 521\u2013530 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hongyi Zhang , Moustapha Cisse , Yann N. Dauphin , and David Lopez - Paz . 2018 .", "entities": []}, {"text": "mixup :", "entities": [[0, 1, "MethodName", "mixup"]]}, {"text": "Beyond empirical risk minimization .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Wenzhao Zheng , Zhaodong Chen , Jiwen Lu , and Jie Zhou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Hardness - aware deep metric learning .", "entities": [[4, 6, "TaskName", "metric learning"]]}, {"text": "InProceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition ( CVPR ) .", "entities": []}, {"text": "Jinhua Zhu , Yingce Xia , Lijun Wu , Di He , Tao Qin , Wengang Zhou , Houqiang Li , and Tieyan Liu . 2020 .", "entities": []}, {"text": "Incorporating bert into neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "A Details of Rich - Resource Datasets For the Zh \u2192En task , the LDC corpus2is taken into consideration , which consists of 1.25 M sentence pairs with 27.9 M Chinese words and 34.5 M English words , respectively .", "entities": []}, {"text": "The NIST 2006 dataset is used as the validation set for selecting the best model , and NIST 2002 ( MT02 ) , 2003 ( MT03 ) , 2004 ( MT04 ) , 2005 ( MT05 ) , 2008 ( MT08 ) are used as the test sets .", "entities": []}, {"text": "We created shared BPE ( bytepair - encoding ( Sennrich et al . , 2016b ) ) codes with 60 K merge operations to build two vocabularies comprising 47 K Chinese sub - words and 30 K English sub - words .", "entities": [[3, 4, "MethodName", "BPE"]]}, {"text": "For the En \u2192De task , we employ the popular WMT14 dataset , which consists of approximately 4.5 M sentence pairs for training .", "entities": [[10, 11, "DatasetName", "WMT14"]]}, {"text": "We select newstest2013 as the validation set and newstest2014 as the test set .", "entities": []}, {"text": "All sentences had been jointly byte - pair - encoded with 32 K merge operations , which results in a shared source - target vocabulary of about 37 K tokens .", "entities": []}, {"text": "For the En \u2192Fr task , we use the significantly larger WMT14 dataset consisting of 36 M sentence pairs .", "entities": [[11, 12, "DatasetName", "WMT14"]]}, {"text": "The combination of { newstest2012 , 2013 } was used for model selection and the experimental results were reported on newstest2014 .", "entities": [[11, 13, "TaskName", "model selection"]]}, {"text": "2LDC2002E18 , LDC2003E07 , LDC2003E14 , the Hansards portion of LDC2004T07 - 08 and LDC2005T06.We use the Stanford segmenter ( Tseng et al . , 2005 ) for Chinese word segmentation and apply the script tokenizer.pl of Moses ( Koehn et al . , 2007 ) for English , German and French tokenization .", "entities": [[28, 31, "TaskName", "Chinese word segmentation"]]}, {"text": "We measure the performance with the 4gram BLEU score ( Papineni et al . , 2002 ) .", "entities": [[7, 9, "MetricName", "BLEU score"]]}, {"text": "Both the case - sensitive tokenized BLEU ( compued by multi-bleu.pl ) and the detokenized sacrebleu3(Post , 2018 ) are reported on the En \u2192De and En\u2192Fr tasks .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "The case - insensitive BLEU is reported on the Zh \u2192En task .", "entities": [[4, 5, "MetricName", "BLEU"]]}, {"text": "B Low - Resource Machine Translation For the low - resource scenario , we choose the IWSLT14 English - German ( En \u2192De ) and IWSLT17 English - French ( En \u2192Fr ) tasks .", "entities": [[4, 6, "TaskName", "Machine Translation"]]}, {"text": "Datasets .", "entities": []}, {"text": "For IWSLT14", "entities": []}, {"text": "En \u2192De , there are 160k sentence pairs for training and 7584 sentence pairs for validation .", "entities": []}, {"text": "As in previous work ( Ranzato et al . , 2016 ; Zhu et al . , 2020 ) , the concatenation of dev2010 , dev2012 , test2010 , test2011 and test2012 is used as the test set .", "entities": []}, {"text": "For IWSLT17 En \u2192Fr , there are236ksentence pairs for training and 10263 for validation .", "entities": []}, {"text": "The concatenation of test2010 , test2011 , test2012 , test2013 , test2014 and test2015 is used as the test set .", "entities": []}, {"text": "We use a joint source and target vocabulary with 10kbyte - pair - encoding ( BPE ) types ( Sennrich et al . , 2016b ) for above two tasks .", "entities": [[15, 16, "MethodName", "BPE"]]}, {"text": "Model Settings .", "entities": []}, {"text": "The model configuration is transformer_iwslt , representing a 6 - layer model with embedding size 512 and FFN layer dimension 1024 .", "entities": []}, {"text": "We train all models using the Adam optimizer with adaptive learning rate schedule ( warm - up step with 4 K ) as in ( Vaswani et al . , 2017 ) .", "entities": [[6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "optimizer"], [10, 12, "HyperparameterName", "learning rate"]]}, {"text": "During inference , we use beam search with a beam size of 5and length penalty of 1.0 .", "entities": []}, {"text": "C Variants with Different Objectives or Sampling Strategies Table 8 describes the details of four variants ( introduced in Table 5 , from row 2 to row 5 ) with different objectives or sampling strategies : ( 1 ) default tangential CTL in Eq .", "entities": []}, {"text": "( 4 ) + MGRC w/o recurrent dependence , ( 2 ) default tangential CTL in Eq .", "entities": []}, {"text": "( 4 ) + MGRC w uniform distribution , ( 3 ) variational inference ( Zhang et al . , 2016 ) + Gaussian sampling , and ( 4 ) cosine similarity + default M GRC sampling .", "entities": [[12, 14, "MethodName", "variational inference"]]}, {"text": "3https://github.com/mjpost/sacrebleu7942", "entities": []}, {"text": "Variants Training Objective for the Semantic Encoder Sampling Strategy for Obtaining Augmented Samples 1 E(x(i),y(i))\u223cB   loges\u0000 rx(i),ry(i)\u0001 es\u0000 rx(i),ry(i)\u0001", "entities": []}, {"text": "+ P|B| j&j\u0338=i\u0010 es\u0000 ry(i),ry\u2032(j)\u0001", "entities": []}, {"text": "+ es\u0000 rx(i),rx\u2032(j)\u0001\u0011 !", "entities": []}, {"text": "\u03c9(k)\u223c\u03b7N\u0000 0,diag(W2", "entities": []}, {"text": "r)\u0001", "entities": []}, {"text": "+ ( 1.0\u2212\u03b7)N\u0000 0,1\u0001 2 ditto \u03c9(k)\u223c\u03b7U\u0000 \u2212 W r , Wr\u0001 + ( 1.0\u2212\u03b7)U\u0000\u00afa\u22121,1\u2212\u00afa\u0001 where \u00afa=1 k\u22121Pk\u22121 i=1\u03c9(i ) 3 E(x(i),y(i))\u223cB\u0010 \u2212KL\u0000 p(rx(i))\u2225q(rx(i ) , ry(i))\u0001\u0011 \u02c6rx=\u00b5+\u03f5\u2299\u03c3 where p(rx(i))\u223c N(\u00b5 , \u03c32)andq(rx(i ) , ry(i))\u223c N(\u00b5\u2032 , \u03c3\u20322 ) where \u03f5is a standard Gaussian noise 4 E(x(i),y(i))\u223cB\u0010rT x(i)ry(i ) \u2225rx(i)\u2225\u00b7\u2225ry(i)\u2225\u0011 \u03c9(k)\u223c\u03b7N\u0000 0,diag(W2", "entities": []}, {"text": "r)\u0001", "entities": []}, {"text": "+", "entities": []}, {"text": "( 1.0\u2212\u03b7)N\u0010 1 k\u22121Pk\u22121 i=1\u03c9(i),1\u0011 Table 8 : The variants of the training objective for the semantic encoder as well as the sampling strategy for obtaining augmented samples .", "entities": []}, {"text": "ModelNoisy Inputs Translationese Original WS WD WR X\u2192Y\u2217X\u2217\u2192Y X\u2217\u2217\u2192Y\u2217 Transformer ( our implementation ) 27.67 15.33 18.59 16.98 32.82 28.56 39.04 Back - Translation ( our implementation ) 29.25 17.20 20.44 18.71 33.07 29.73 39.86 CSANMT 30.16 20.14 23.76 21.66 34.62 30.70 41.64 Table 9 : BLEU scores", "entities": [[9, 10, "MethodName", "Transformer"], [23, 24, "TaskName", "Translation"], [46, 47, "MetricName", "BLEU"]]}, {"text": "[ % ] on noisy inputs and the translationese effect , in the WMT14 En \u2192De setup .", "entities": [[13, 14, "DatasetName", "WMT14"]]}, {"text": "D Robustness on Noisy Inputs and Translationese", "entities": []}, {"text": "In this section , we study the robustness of our CSANMT towards both noisy inputs and the translationese effect ( V olansky et", "entities": []}, {"text": "al . , 2013 ) on newstest2014 for the WMT14 English - German task .", "entities": [[9, 10, "DatasetName", "WMT14"]]}, {"text": "Noisy Inputs .", "entities": []}, {"text": "Inspired by ( Gao et al . , 2019 ) , we construct noisy test sets via several strategies described as follows : \u2022Original : the original testset without any manipulations ; \u2022WS : word swap , randomly swap words in nearby positions within a window size 3 ( Artetxe et al . , 2018 ; Lample et al . , 2018b ) ; \u2022WD : word dropout , randomly drop words with a ratio of 15 % ( Iyyer et al . , 2015 ; Lample et al . , 2018b ) ; \u2022WR : word replace , randomly replace word tokens with a placeholder token ( e.g. , [ UNK ] )", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "( Xie et al . , 2017 ) or with a relevant ( measured by the similarity of word embeddings ) alternative ( Cheng et al . , 2019 ) .", "entities": [[18, 20, "TaskName", "word embeddings"]]}, {"text": "The replacement ratio also is 15 % .", "entities": []}, {"text": "Translationese Effect .", "entities": []}, {"text": "Edunov et al .", "entities": []}, {"text": "( 2020 ) pointed out that back - translation ( BT ) suffers from the translationese effect .", "entities": []}, {"text": "that is BT only shows significant improvements for test examples where thesource itself is a translation , or translationese , while is ineffective to translate natural text .", "entities": []}, {"text": "To test the effect of our method on translationese , we follow the same settings and testsets4provided by Edunov et al .", "entities": []}, {"text": "( 2020 ): \u2022natural source \u2192 translationese target ( X\u2192Y\u2217 ) ; \u2022translationese source \u2192 natural target ( X\u2217\u2192Y ) ; \u2022round - trip translationese source \u2192translationese target ( X\u2217\u2217\u2192Y\u2217 ) , where X\u2192Y\u2217\u2192X\u2217\u2217. Results .", "entities": []}, {"text": "As shown in Table 9 , our approach shows better robustness over two baseline methods across various artificial noises .", "entities": []}, {"text": "Moreover , CSANMT consistently outperforms the baseline in all three translationese scenarios , the same is true for back - translation .", "entities": []}, {"text": "However , Edunov et al .", "entities": []}, {"text": "( 2020 ) shows that BT improves only in the X\u2217\u2192Yscenario .", "entities": []}, {"text": "Our explanation for the inconsistency is that BT without monolingual data in our setting benefits from the natural parallel data to deal with the translationese sources .", "entities": []}, {"text": "4https://github.com/facebookresearch/ evaluation - of - nmt - bt7943", "entities": []}, {"text": "E Codes of tangential contrastive learning and M GRC sampling E.1 Tangential Contrastive Learning # src_embedding : [ batch_size , 1 , hidden_size ] # trg_embedding : [ batch_size , 1 , hidden_size ] def get_ctl_loss(src_embedding , trg_embedding , dynamic_coefficient ): batch_size = tf.shape(src_embedding)[0 ] def get_ctl_logits(query , keys ): # expand_query : [ batch_size , batch_size , hidden_size ] # expand_keys : [ batch_size , batch_size , hidden_size ] # the current ref is the positive key , while others in the training batch are negative ones expand_query = tf.tile(query , [ 1 , batch_size , 1 ] ) expand_keys = tf.tile(tf.transpose(keys , [ 1,0,2 ] ) , [ batch_size , 1 , 1 ] ) # distances between queries and positive keys d_pos = tf.sqrt(tf.reduce_sum(tf.pow(query - keys , 2.0 ) , axis=-1 ) )", "entities": [[4, 6, "MethodName", "contrastive learning"], [12, 14, "MethodName", "Contrastive Learning"]]}, {"text": "#", "entities": []}, {"text": "[ batch_size , 1 ] d_pos = tf.tile(d_pos , [ 1 , batch_size ] ) #", "entities": []}, {"text": "[ batch_size , batch_size ] d_neg = tf.sqrt(tf.reduce_sum(tf.pow(expand_query - expand_keys , 2.0 ) , axis=-1 ) )", "entities": []}, {"text": "#", "entities": []}, {"text": "[ batch_size , batch_size ] lambda_coefficient = ( d_pos / d_neg ) *", "entities": []}, {"text": "* dynamic_coefficient hardness_masks = tf.cast(tf.greater(d_neg , d_pos ) , dtype = tf.float32 ) hard_keys =( expand_query + tf.expand_dims(lambda_coefficient , axis=2 ) *", "entities": []}, {"text": "( expand_keys - expand_query ) )", "entities": []}, {"text": "* \\ tf.expand_dims(hardness_masks , axis=2 )", "entities": []}, {"text": "+ expand_keys * tf.expand_dims(1.0 - hardness_masks , axis=2 ) logits = tf.matmul(query , hard_keys , transpose_b = True ) #", "entities": []}, {"text": "[ batch_size , 1 , batch_size ] return logits logits_src_trg = get_ctl_logits(src_embedding , trg_embedding ) logits_trg_src = get_ctl_logits(trg_embedding , src_embedding ) + tf.expand_dims(tf.matrix_band_part(tf.ones([batch_size , batch_size ] ) , 0 , 0 ) * -1e9 , axis=1 ) logits = tf.concat([logits_src_trg , logits_trg_src ] , axis=2 ) #", "entities": [[28, 29, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}, {"text": "[ batch_size , 1 , 2 * batch_size ] labels = tf.expand_dims(tf.range(batch_size , dtype = tf.int32 ) , axis=1 ) labels = tf.one_hot(labels , depth=2 * batch_size , on_value=1.0 , off_value=0.0 ) cross_entropy_fn = tf.nn.softmax_cross_entropy_with_logits loss = tf.reduce_mean(cross_entropy_fn(logits = logits , labels = labels ) )", "entities": [[35, 36, "MetricName", "loss"]]}, {"text": "return loss E.2 M GRC Sampling # src_embedding : [ batch_size , hidden_size ] # trg_embedding : [ batch_size , hidden_size ] # default : K=20 and eta = 0.6 def mgmc_sampling(src_embedding , trg_embedding , K , eta ): batch_size = tf.shape(src_embedding)[0 ] def get_samples(x_vector , y_vector ): bias_vector = y_vector - x_vector W_r", "entities": [[1, 2, "MetricName", "loss"]]}, {"text": "= ( tf.abs(bias_vector ) - tf.reduce_min(tf.abs(bias_vector ) , axis=1 , keep_dims = True ) ) / \\", "entities": []}, {"text": "( tf.reduce_max(tf.abs(bias_vector ) , 1 , keep_dims = True ) - tf.reduce_min(tf.abs(bias_vector ) , 1 , keep_dims = True ) )", "entities": []}, {"text": "# initializing the set of samples R =", "entities": []}, {"text": "[ ] omega = tf.random_normal(tf.shape(bias_vector ) , 0 , W_r ) sample = x_vector", "entities": [[7, 8, "DatasetName", "0"]]}, {"text": "+ tf.multiply(omega , bias_vector ) R.append(sample ) for i in xrange(1 , K ): chain =", "entities": []}, {"text": "[ tf.expand_dims(item , axis=1 ) for item in R[:i ] ] average_omega = tf.reduce_mean(tf.concat(chain , axis=1 ) , axis=1 ) omega = eta * tf.random_normal(tf.shape(bias_vector ) , 0 , W_r ) + \\ ( 1.0 - eta ) * tf.random_normal(tf.shape(bias_vector ) , average_omega , 1.0 ) sample = x_vector", "entities": [[27, 28, "DatasetName", "0"]]}, {"text": "+ tf.multiply(omega , bias_vector ) R.append(sample ) return R x_sample = get_samples(src_embedding , trg_embedding ) y_sample = get_samples(trg_embedding , src_embedding ) return x_sample.extend(y_sample)7944", "entities": []}]