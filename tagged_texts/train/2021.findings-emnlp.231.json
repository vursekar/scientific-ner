[{"text": "Findings of the Association for Computational Linguistics : EMNLP 2021 , pages 2713\u20132718", "entities": []}, {"text": "November 7\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics2713Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text Xiao - Yu Guo Yuan - Fang Li Faculty of Information Technology , Monash University , Melbourne , Australia { xiaoyu.guo , yuanfang.li , gholamreza.haffari}@monash.eduGholamreza Haffari Abstract Numerical reasoning skills are essential for complex question answering ( CQA ) over text .", "entities": [[15, 17, "TaskName", "Question Answering"], [32, 33, "DatasetName", "Monash"], [53, 55, "TaskName", "question answering"]]}, {"text": "It requires opertaions including counting , comparison , addition and subtraction .", "entities": []}, {"text": "A successful approach to CQA on text , Neural Module Networks ( NMNs ) , follows the programmer - interpreter paradigm and leverages specialised modules to perform compositional reasoning .", "entities": []}, {"text": "However , the NMNs framework does not consider the relationship between numbers and entities in both questions and paragraphs .", "entities": []}, {"text": "We propose effective techniques to improve NMNs \u2019 numerical reasoning capabilities by making the interpreter questionaware and capturing the relationship between entities and numbers .", "entities": []}, {"text": "On the same subset of the DROP dataset for CQA on text , experimental results show that our additions outperform the original NMNs by 3.0 points for the overall F1 score .", "entities": [[6, 7, "DatasetName", "DROP"], [29, 31, "MetricName", "F1 score"]]}, {"text": "1 Introduction Complex Question Answering ( CQA ) is a challenging task , requiring a model to perform compositional and numerical reasoning .", "entities": [[3, 5, "TaskName", "Question Answering"]]}, {"text": "Originally proposed for the visual question answering ( VQA ) task , Neural Module Networks ( NMNs ) ( Andreas et al . , 2016 ) have recently been adopted to tackle the CQA problem over text ( Gupta et al . , 2020 ) .", "entities": [[4, 7, "DatasetName", "visual question answering"], [8, 9, "TaskName", "VQA"]]}, {"text": "The NMNs is an end - to - end differentiable model in the programmer - interpreter paradigm ( Guo et al . , 2020 ; Hua et al . , 2020a , b ) .", "entities": []}, {"text": "Briefly , the programmer learns to map each question into a program , i.e. a sequence of neural modules , and the interpreter then \u201c executes \u201d the program , operationalized by modules , on the paragraph to yield the answer for different types of complex questions .", "entities": []}, {"text": "NMNs achieves the best performance on a subset of the challenging DROP dataset ( Dua et al . , 2019 ) and is interpertable by nature .", "entities": [[11, 12, "DatasetName", "DROP"]]}, {"text": "However , NMNs \u2019 performance advantage is not consistent , as it underperforms in some types of questions that require numerical reasoning .", "entities": []}, {"text": "For instance , for date - compare questions , MTMSN ( Huet al . , 2019 ) achieves an F1 score of 85.21 , whereas NMNs \u2019 performance is 82.6 .", "entities": [[19, 21, "MetricName", "F1 score"]]}, {"text": "Similarly , for count questions , the F1 score is 61.6 for MTMSN and 55.7 for NMNs .", "entities": [[7, 9, "MetricName", "F1 score"]]}, {"text": "This performance gap stems from two deficiencies of NMNs , which we describe below with the help of two examples in Figure 1 .", "entities": []}, {"text": "Firstly , NMNs \u2019 interpreter is oblivious to the question when executing number - related modules .", "entities": []}, {"text": "For executing number - related modules , the interpreter only receives the paragraph as input , but not the question .", "entities": []}, {"text": "Such a lack of direct interactions with the question impairs model performance : the entities in the question , which may also occur in the paragraph , can help locate significant and relevant numbers to produce the final answer .", "entities": []}, {"text": "In the first example in Figure 1 , if the interpreter is aware of the correct event mentioned in the question ( i.e. \u201c the Constituent Assembly being elected \u201d ) , it can easily find the same event in the paragraph and further locate its date ( \u201c 12 November \u201d ) precisely .", "entities": []}, {"text": "Without this knowledge , the original NMNs found the wrong event ( i.e. \u201c dissolved the Constituent Assembly \u201d ) , thus the wrong date ( \u201c January 1918 \u201d ) , leading to an incorrect answer .", "entities": []}, {"text": "Secondly , NMNs disregards the relative positioning of entities and their related numbers in the paragraph .", "entities": []}, {"text": "Although NMNs can learn separate distributions over numbers extracted from a paragraph , it does not have an effective mechanism to identify the number that connects to a given entity .", "entities": []}, {"text": "Such an ability to recognise the association among numbers and entities is vital for learning numerical reasoning skills : the operation between numbers is meaningful only when they refer to the same entity or the same type of entities .", "entities": []}, {"text": "The second example in Figure 1 illustrates the positioning of entities and their related numbers .", "entities": []}, {"text": "With only a constraint on a window around an entity , the NMNs \u2019 interpreter tends to identify the nearest number as the related one to a given entity ( \u201c August 1996 to December 1997 \u201d for entity \u201c PUK and KDP later co - operated \u201d ) , resulting in wrong predictions .", "entities": []}, {"text": "1All F1 and EM numbers in this paper are percentages .", "entities": [[1, 2, "MetricName", "F1"], [3, 4, "MetricName", "EM"]]}, {"text": "2714 Question Paragraph NMNs   AnswerOur Answer Which event happened   first , the Constituent   Assembly being elected ,   or the elimination of   hierarchy in the army ? \u2026", "entities": []}, {"text": "On 12 November , a Constituent Assembly was elected .", "entities": []}, {"text": "In these elections , 26   mandatory delegates were proposed   by the Bolshevik Central Comm ittee and 58 were   proposed by the Socialist Revoluti onaries .", "entities": []}, {"text": "Of these mandatory c andidates , only one   Bolshevik and seven Socialist Re volutionary delegates were wome n. ...", "entities": []}, {"text": "The Bolsheviks   dissolved the Constituent Assembly in January 1918 , when it came into conflict with the   Soviets .", "entities": []}, {"text": "On 16 December 1917 , the government ventured to elimi nate hierarchy in the   army , removing all titles , ranks , and uniform decorations .", "entities": []}, {"text": "\u2026 hierarchy   in the army ( Incorrect)Constituent   Assembly was elected ( Correct ) What happened first :   the U.S. -mediated   Washington Agreement   or PUK and KDP later   co - operated ?", "entities": []}, {"text": "In September 1998 , Barzani and Talabani signed the U.S. -mediated Washington Agreement   establishing a formal peace treaty .", "entities": []}, {"text": "In the agreement , \u2026 , includ ing the PUK and KDP .", "entities": []}, {"text": "The   KDP estimated that 58,000 of its supporters had been expelled f rom PUK -controlled   regions from October 1996 to October 1997 .", "entities": []}, {"text": "The PUK says 49,000 of its supporters were   expelled from KDP -controlled regions from August 1996 to December 1997 .", "entities": []}, {"text": "The PUK and   KDP later co -operated with American forces during the 2003 invasion of Iraq , \u2026 PUK and   KDP later   cooperated ( Incorrect)the U.S. mediated   Washington   Agreement ( Correct)Figure 1 : Two examples in the DROP ( Dua et al . , 2019 ) dataset that demonstrate the deficienties of NMNs .", "entities": [[42, 43, "DatasetName", "DROP"]]}, {"text": "Tokens pertinent to our discussion are highlighted in red , and their relevant numbers are highlighted in orange .", "entities": []}, {"text": "Solid blue lines are predictions of our model , while dotted blue lines show the predictions of NMNs .", "entities": []}, {"text": "We propose three simple and effective mechanisms to improve NMNs \u2019 numerical reasoning capabilities .", "entities": []}, {"text": "Firstly , we improve the interpreter to make it questionaware .", "entities": []}, {"text": "By explicitly conditioning the execution on the question , the interpreter can exploit the information contained in the question .", "entities": []}, {"text": "Secondly , we propose an intuitive constraint to better relate numbers and their corresponding entities in the paragraph .", "entities": []}, {"text": "Finally , we strengthen the auxiliary loss to increase attention values of entities in closer vicinity within a sentence .", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "Experimental results show that our modifications significantly improve NMNs \u2019 numerical reasoning performance by up to 3.0 absolute F1 points .", "entities": [[18, 19, "MetricName", "F1"]]}, {"text": "With minor modification , these mechanisms are simple enough to be applied to other modular approaches .", "entities": []}, {"text": "2 Related Work Complex Question Answering focuses on questions that require capabilities beyond multi - hop reasoning .", "entities": [[4, 6, "TaskName", "Question Answering"]]}, {"text": "These capabilities include numerical , logical and discrete reasoning .", "entities": []}, {"text": "A number of neural models were recently proposed to address the CQA task , such as BiDAF ( Seo et al . , 2017 ) , QANet ( Y u et al . , 2018 ) , NMNs ( Gupta et al . , 2020 ) and NumNet ( Ran et al . , 2019 ) , which achieved high performance on benchmark datasets such as DROP ( Dua et al . , 2019 ) .", "entities": [[66, 67, "DatasetName", "DROP"]]}, {"text": "Numerical Reasoning is an essential capability for the CQA task , which is a challenging problem since the numbers and computation procedures are separately extracted and generated from raw text .", "entities": []}, {"text": "Dua et al .", "entities": []}, {"text": "( 2019 ) modified the output layer of QANet ( Y u et al . , 2018 ) and proposed a number - aware model NAQANet that can deal with numerical questions for which the answer can not be directly extracted from the paragraph .", "entities": []}, {"text": "In addition to NAQANet , NumNet ( Ran et al . , 2019 ) leveraged Graph Neural Network ( GNN ) to design a number - aware deep learning model .", "entities": []}, {"text": "Also leveraging GNN , Chen et al .", "entities": []}, {"text": "( 2020a ) distinguished number types more precisely by adding the connection with entities and obtained better performance .", "entities": []}, {"text": "Chen et al .", "entities": []}, {"text": "( 2020b ) searched possible programs exhaustively based on answer numbers and employed these programs as weak supervision to train the whole model .", "entities": []}, {"text": "Using dependency parsing of questions , Saha et al .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "( 2021 ) focused on the numerical part and obtained excellent results on different kinds of numerical reasoning questions .", "entities": []}, {"text": "Neural Module Networks ( NMNs ) ( Gupta et al . , 2020 ) adopts the programmer - interpreter paradigm and is a fully end - to - end differentiable model , in which the programmer ( responsible for composing programs ) and the interpreter ( responsible for soft execution ) are jointly learned .", "entities": []}, {"text": "Specialised modules , such as findandfind - num , are predefined to perform different types of reasoning over text and numbers .", "entities": []}, {"text": "Compared with those techniques that employ GNNs ( Ran et al . , 2019 ; Y u et al . , 2018 ) , NMNs is highly interpretable while achieving competitive performance .", "entities": []}, {"text": "More details can be found in Appendix A. 3 Proposed Model In this section , we will discuss the deficiencies of NMNs described in Section 1 and propose three techniques to overcome these problems .", "entities": []}, {"text": "Considering the importance of questions while executing programs , we incorporate a question - to - paragraph alignment matrix to form a question - aware interpreter in Section 3.1 .", "entities": []}, {"text": "In Section 3.2 , the correspondence between numbers and their related entities is enhanced with a simple and effective constraint on number - related", "entities": []}, {"text": "2715modules .", "entities": []}, {"text": "In Section 3.3 , we strengthen the auxiliary loss function in NMNs to further concentrate attention in the same sentence .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "3.1 Question - aware Interpreter The interpreter in the NMNs framework is responsible for executing specialised modules given the context ( i.e. paragraph ) .", "entities": []}, {"text": "For number - related modules such as\u201cfind - num \u201d , the question is not taken into account , which limits NMNs \u2019 performance on numerical reasoning , as information in the question is not taken into account .", "entities": []}, {"text": "As an example , let us take a clear look at the \u201c find - num \u201d module in NMNs .", "entities": []}, {"text": "find - num ( P)!T2 .", "entities": []}, {"text": "This module takes as input the distribution over paragraph tokens , and produces output an distribution over the numbers :", "entities": []}, {"text": "Sn ij = PiTWnPnj ; ( 1 ) An i = softmax ( Sn i ) ; ( 2 ) T = X iPi\u0001An i ; ( 3 )", "entities": [[11, 12, "MethodName", "softmax"]]}, {"text": "where inputPand", "entities": []}, {"text": "outputTare", "entities": []}, {"text": "distributions over paragraph tokens and numbers respectively , Pis the paragraph token representations , iis the index of the ithparagraph token , njis the index of the jthnumber token , and Wnis a learnable matrix .", "entities": []}, {"text": "Note that when computing the similarity matrix between the paragraph token Piand the number token Pnjin Equation 1 , there is no interaction with the question .", "entities": []}, {"text": "When the correct number types or related entities can be easily found in the question , incorporating the question in \u201c find - num \u201d can help narrow down the search of numbers in the paragraph .", "entities": []}, {"text": "The first example in Figure 1 shows that the NMNs fails to locate the correct number as the wrong event is recognized , without interacting with the question .", "entities": []}, {"text": "Inspired by this idea , we propose the question - toparagraph alignment modification to number - related modules .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "Specifically , the definition of \u201c find - num \u201d is modified as follows : find - num ( P , Q)!Tn , where the additional input Qobtained from the programmer represents the distribution over question tokens , and the new output is represented byTn .", "entities": []}, {"text": "Additional computational steps ( Equation 4 to 7 below ) are added after Equation 3 : 2We follow Gupta et al .", "entities": []}, {"text": "( 2020 ) and use same variables , annotations in equations for consistency .", "entities": []}, {"text": "Sn0 kj = QkTWnPnj ; ( 4 ) An0", "entities": []}, {"text": "k = softmax ( Sn0 k ) ; ( 5 ) T0 = X kQk\u0001An0 k ; ( 6 ) Tn=\u0015\u0001T+(1\u0000\u0015)\u0001T0 ; ( 7 ) where Qis the question token representations", "entities": [[0, 2, "HyperparameterName", "k ="], [2, 3, "MethodName", "softmax"]]}, {"text": "and k is the index of the kthquestion token .", "entities": []}, {"text": "As can be seen from the above equations , the input of the improved \u201c find - num \u201d module is extended to include not only paragraph but also question token distributions instead of only the paragraph .", "entities": []}, {"text": "More precisely , T0is another alignment matrix between all question tokens and number tokens , using the same form of Bi - linear attention computation as T. Finally , the new distribution Tnis produced by the weighted sum ofTandT0with an additional hyperparameters\u0015. Here we fix \u0015=0:5so that NMNs treats the paragraph and the question equally .", "entities": []}, {"text": "Other numberrelated modules are also revised in a similar way , e.g. \u201c find - date \u201d , \u201c compare - num - lt - than \u201d , \u201c find - max - num \u201d .", "entities": []}, {"text": "3.2 Number - Entity Positional Constraint It is highly likely for a paragraph to contain multiple numbers and entities , as shown in Figure 1 .", "entities": []}, {"text": "For such paragraphs , the original NMNs allows all numbers to interact with all entities in the computation of number - related modules such as \u201c find - num \u201d .", "entities": []}, {"text": "This is detrimental to performance as , intuitively , a number far away from an entity is less likely to be related to the entity .", "entities": []}, {"text": "As the second example in Figure 1 shows , NMNs connects \u201c December 1997 \u201d to the entity \u201c PUK and KDP \u201d since \u201c 2003 \u201d is far away from it , resulting in wrong predictions eventually .", "entities": []}, {"text": "To tackle this issue , we add another computational component , the relation matrix Un , into numberrelated modules .", "entities": []}, {"text": "Taking the \u201c find - num \u201d module as an example , the following step is added before Equation 2 when computing Sn ij : Sn ij = Un ij\u000eSn ij ; ( 8) where\u000eis element - wise multiplication .", "entities": []}, {"text": "In the above equation , the value of Sn ijis updated with the relation matrix Un , which constrains the relationship between theithparagraph token and jthnumber token .", "entities": []}, {"text": "More specifically , let stbe the token index set for the tthsentence in the paragraph .", "entities": []}, {"text": "Thus , if both the ith paragraph token and the jthnumber token belong to", "entities": []}, {"text": "2716the same sentence , element Un ij , in rowiand column j , is set to 1 , otherwise 0 : Un ij=\u001a1;(i2st)^(nj2st ) 0 ; otherwise(9 )", "entities": [[19, 20, "DatasetName", "0"], [24, 25, "DatasetName", "0"]]}, {"text": "By adding this matrix , the module only keeps the attention values of tokens in close vicinity within a sentence , and learns to find the related numbers that directly interact with entities .", "entities": []}, {"text": "Similarly , this relation matrix Unis also applied to other number - related modules to improve performance .", "entities": []}, {"text": "3.3 Auxiliary Loss Function Gupta et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) employed an auxiliary loss to constrain the relative positioning of output tokens with respect to input tokens in the \u201c find - num \u201d , \u201c find - date \u201d and\u201crelocate \u201d modules .", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "For instance , the auxiliary loss for the \u201c find - num \u201d module is as follows : Hn loss=\u0000mX i=1log(NtX j=01nj2[i\u0006W]An ij ) ; ( 10 ) where An ijis from Equation 2 .", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "The loss enables the model to concentrate the attention mass of output tokens within a window of size W(e.g . W=10 ) .", "entities": [[1, 2, "MetricName", "loss"]]}, {"text": "However , these loss functions still allow irrelevant numbers to have spuriously high attention values .", "entities": [[3, 4, "MetricName", "loss"]]}, {"text": "Taking the second line in Figure 1 as an example , based on the loss computation procedures , the number \u201c December 1997 \u201d will be also \u201c found \u201d and connected to the entity \u201c PUK and KDP \u201d in NMNs .", "entities": [[14, 15, "MetricName", "loss"]]}, {"text": "Obviously , this irrelevant year information should not be taken into consideration .", "entities": []}, {"text": "Therefore , we propose to strengthen the auxiliary loss to further concentrate attention mass to those tokens within the same sentence : Hn loss=\u0000mX i=1log(NtX j=01(nj2st)^(i2st)An ij);(11 ) where thestis the token index set for the tthsentence in the paragraph .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "In this way , the year \u201c 2003 \u201d is the only consideration for the previous example .", "entities": []}, {"text": "4 Experiments 4.1 Dataset and Settings We evaluate model performance on the same subset of the DROP dataset used by the original NMNs ( Gupta et al . , 2020 ) , which contains approx .", "entities": [[16, 17, "DatasetName", "DROP"]]}, {"text": "19,500 QA pairs for training , 440 for validation and 1,700 for testing .", "entities": []}, {"text": "The training procedures and hyper - parameter settings are the same as the original NMNs ( Gupta et al . , 2020 ) .", "entities": []}, {"text": "We report F1 and Exact Match ( EM ) scores following the literature ( Dua et al . , 2019 ; Gupta et", "entities": [[2, 3, "MetricName", "F1"], [4, 6, "MetricName", "Exact Match"], [7, 8, "MetricName", "EM"]]}, {"text": "al . , 2020).4.2 Results Table 1 shows the main results , where \u201c original \u201d represents the performance of the original NMNs ( Gupta et al . , 2020 ) .", "entities": []}, {"text": "Row 4 , \u201c + qai+nepc+aux \u201d , is our full model , which includes the question - aware interpreter ( + qai ) , the number - entity positional constraint ( + nepc ) , and the improved auxiliary loss ( + aux ) .", "entities": [[40, 41, "MetricName", "loss"]]}, {"text": "It can be observed that compared to \u201c original \u201d , our full model achieves significantly higher performance with F1 of 80.4 and EM of 76.6 , representing an increase of 3.0 and 2.6 absolute points respectively .", "entities": [[19, 20, "MetricName", "F1"], [23, 24, "MetricName", "EM"]]}, {"text": "Besides , our significant test shows p\u00140:01 .", "entities": []}, {"text": "Methods F1 EM original(Gupta et al . , 2020 ) 77.4 74.0 ours + qai 79.0 74.9 + qai+nepc 79.9 76.0 + qai+nepc+aux 80.4 76.6 Table 1 : Comparison between different models .", "entities": [[1, 2, "MetricName", "F1"], [2, 3, "MetricName", "EM"]]}, {"text": "We also conduct an ablation study to discuss the contribution of individual technique .", "entities": []}, {"text": "The second line , \u201c + qai \u201d , is the results with the question - aware interpreter employed only .", "entities": []}, {"text": "For this variant , the F1 and EM scores improve on the original baseline by 1.6 and 0.9 points respectively .", "entities": [[5, 6, "MetricName", "F1"], [7, 8, "MetricName", "EM"]]}, {"text": "With the addition of the number - entity positional constraint , \u201c + nepc \u201d , results show an improvement of 2.5 and 2.0 points for F1 and EM when comparing with \u201c original \u201d .", "entities": [[26, 27, "MetricName", "F1"], [28, 29, "MetricName", "EM"]]}, {"text": "These results show that all of the three techniques are effective in improving numerical reasoning skills for NMNs .", "entities": []}, {"text": "We also report performance by subsets of different question types in Table 2 .", "entities": []}, {"text": "Except for the numbercompare type , our model improves on the original NMNs across all other types of questions significantly , by at least 3.2 absolute points for F1 .", "entities": [[28, 29, "MetricName", "F1"]]}, {"text": "In addition , our model outperforms aforementioned MTMSN ( Hu et al . , 2019 ) on all question types as well .", "entities": []}, {"text": "Question Type MTMSN original ours date - compare 85.2 82.6 86.0 date - difference 72.5 75.4 78.6 number - compare 85.1 92.7 90.1 extract - number 80.7 86.1 90.1 count 61.6 55.7 61.8 extract - argument 66.6 69.7 73.2 Table 2 : Performance ( F1 ) by question types .", "entities": [[44, 45, "MetricName", "F1"]]}, {"text": "27175 Conclusion Neural Moudule Networks ( NMNs ) represent an interpretable state - of - the - art approach to complex question answering over text .", "entities": [[21, 23, "TaskName", "question answering"]]}, {"text": "In this paper , we further improve NMNs \u2019 numerical reasoning capabilities , by making the interpreter question - aware and placing stronger constraints on the relative positioning of entities and their related numbers .", "entities": []}, {"text": "Experimental results show that our approach significantly improves NMNs \u2019 numerical reasoning ability , with an increase in F1 of 3.0 absolute points .", "entities": [[18, 19, "MetricName", "F1"]]}, {"text": "Acknowledgements This research was supported in part by the Future Fellowship FT190100039 from the Australian Research Council .", "entities": []}, {"text": "The computational resources for this work were provided by the Multi - modal Australian ScienceS Imaging and Visualisation Environment ( MASSIVE ) ( www.massive.org.au ) .", "entities": [[20, 21, "DatasetName", "MASSIVE"]]}, {"text": "We would like to thank the anonymous reviewers for their useful comments to improve the manuscript .", "entities": []}, {"text": "References Jacob Andreas , Marcus Rohrbach , Trevor Darrell , and Dan Klein .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Neural module networks .", "entities": []}, {"text": "In Proceedings of CVPR , pages 39\u201348 .", "entities": []}, {"text": "Kunlong Chen , Weidi Xu , Xingyi Cheng , Zou Xiaochuan , Y uyu Zhang , Le Song , Taifeng Wang , Y uan Qi , and Wei Chu . 2020a .", "entities": []}, {"text": "Question directed graph attention network for numerical reasoning over text .", "entities": [[2, 5, "MethodName", "graph attention network"]]}, {"text": "In Proceedings of EMNLP , pages 6759\u20136768 .", "entities": []}, {"text": "Xinyun Chen , Chen Liang , Adams Wei Y u , Denny Zhou , Dawn Song , and Quoc V .", "entities": []}, {"text": "Le . 2020b .", "entities": []}, {"text": "Neural symbolic reader : Scalable integration of distributed and symbolic representations for reading comprehension .", "entities": [[12, 14, "TaskName", "reading comprehension"]]}, {"text": "InProceedings of ICLR .", "entities": []}, {"text": "Dheeru Dua , Yizhong Wang , Pradeep Dasigi , Gabriel Stanovsky , Sameer Singh , and Matt Gardner .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "DROP :", "entities": [[0, 1, "DatasetName", "DROP"]]}, {"text": "A reading comprehension benchmark requiring discrete reasoning over paragraphs .", "entities": [[1, 3, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of HLT - NAACL , pages 2368\u20132378 .", "entities": []}, {"text": "Xiaoyu Guo , Y uan - Fang Li , and Gholamreza Haffari .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Understanding unnatural questions improves reasoning over text .", "entities": []}, {"text": "In Proceedings of COLING , pages 4949\u20134955 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Nitish Gupta , Kevin Lin , Dan Roth , Sameer Singh , and Matt Gardner .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Neural module networks for reasoning over text .", "entities": []}, {"text": "In Proceedings of ICLR .", "entities": []}, {"text": "Minghao Hu , Y uxing Peng , Zhen Huang , and Dongsheng Li . 2019 .", "entities": []}, {"text": "A multi - type multi - span network for readingcomprehension that requires discrete reasoning .", "entities": []}, {"text": "In Proceedings of EMNLP - IJCNLP , pages 1596\u20131606 .", "entities": []}, {"text": "Y uncheng Hua , Y uan - Fang Li , Gholamreza Haffari , Guilin Qi , and Tongtong Wu . 2020a .", "entities": []}, {"text": "Few - shot complex knowledge base question answering via meta reinforcement learning .", "entities": [[4, 8, "TaskName", "knowledge base question answering"]]}, {"text": "In Proceedings of EMNLP , pages 5827\u20135837 .", "entities": []}, {"text": "Y uncheng Hua , Y uan - Fang Li , Gholamreza Haffari , Guilin Qi , and Wei Wu .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Retrieve , program , repeat : Complex knowledge base question answering via alternate meta - learning .", "entities": [[7, 11, "TaskName", "knowledge base question answering"], [13, 16, "TaskName", "meta - learning"]]}, {"text": "In Proceedings of IJCAI , pages 3679\u20133686 .", "entities": []}, {"text": "Qiu Ran , Yankai Lin , Peng Li , Jie Zhou , and Zhiyuan Liu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "NumNet : Machine reading comprehension with numerical reasoning .", "entities": [[2, 5, "TaskName", "Machine reading comprehension"]]}, {"text": "In Proceedings of EMNLP - IJCNLP , pages 2474\u20132484 .", "entities": []}, {"text": "Amrita Saha , Shafiq R. Joty , and Steven C. H. Hoi . 2021 .", "entities": []}, {"text": "Weakly supervised neuro - symbolic module networks for numerical reasoning .", "entities": []}, {"text": "CoRR , abs/2101.11802 .", "entities": []}, {"text": "V ersion 1 .", "entities": []}, {"text": "Min Joon Seo , Aniruddha Kembhavi , Ali Farhadi , and Hannaneh Hajishirzi . 2017 .", "entities": []}, {"text": "Bidirectional attention flow for machine comprehension .", "entities": []}, {"text": "In Proceedings of ICLR .", "entities": []}, {"text": "Adams Wei Y u , David Dohan , Minh - Thang Luong , Rui Zhao , Kai Chen , Mohammad Norouzi , and Quoc V .", "entities": []}, {"text": "Le .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Qanet :", "entities": []}, {"text": "Combining local convolution with global self - attention for reading comprehension .", "entities": [[2, 3, "MethodName", "convolution"], [9, 11, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of ICLR .", "entities": []}, {"text": "2718A NMNs model overview In order to solve the complex question answering problem , Gupta et al .", "entities": [[10, 12, "TaskName", "question answering"]]}, {"text": "( 2020 ) proposed a Neural Module Networks ( NMNs ) model .", "entities": []}, {"text": "Consisting of a programmer and an interpreter , NMNs can be more interpretable as shown in Figure 2 .", "entities": []}, {"text": "Figure 2 : Architecture of the NMNs model .", "entities": []}, {"text": "As Figure 2 shows , NMNs takes the question and the paragraph as inputs .", "entities": []}, {"text": "The programmer firstly maps the question into corresponding \u201c discrete \u201d modules in order .", "entities": []}, {"text": "Then , the interpreter executes these generated modules against the corresponding paragraph to produce the final answer .", "entities": []}, {"text": "Moreover , all modules are differentiable so that the whole NMNs can be trained in an end - to - end way .", "entities": []}, {"text": "B Settings for Experiments We mainly use PyTorch and AllenNLP deep learning platforms to implement our model .", "entities": []}, {"text": "After 40 - epoch training on Ubuntu 16.04 with one V100 GPU Card ( 16 GB memory ) , it takes around 24 hours to converge .", "entities": []}, {"text": "And all reported results are produced based on the saved checkpoint .", "entities": []}, {"text": "Name V alue batch size 4 epochs 40 hard em epochs 5 learning rate 1e-5 drop out rate 0.2 max question length 50 max paragraph length 459 max decode step 14 Table 3 : Hyper - parameter settings .", "entities": [[3, 5, "HyperparameterName", "batch size"], [12, 14, "HyperparameterName", "learning rate"]]}, {"text": "For hyper - parameters in our model , we do n\u2019t conduct experiments on their search trials since weemploy the same settings as Gupta et al .", "entities": []}, {"text": "( 2020 ) did , which can be found in Table 3 .", "entities": []}, {"text": "Note that they are also the configuration to obtain the best performance .", "entities": []}, {"text": "For the added parameter \u0015in Equation 7 , we leverage an empirical value \u0015=0:5without any fine - tuning .", "entities": []}, {"text": "Due to the page limitation , we did n\u2019t include more baselines , such as NAQANet ( Dua et al . , 2019 ) .", "entities": []}, {"text": "After running on the same split of DROP dataset , the F1 and EM scores by NAQANet are 62.1 % and 57.9 % respectively , which are substantially lower than our results in Table 1 , by over 17 % for both scores .", "entities": [[7, 8, "DatasetName", "DROP"], [11, 12, "MetricName", "F1"], [13, 14, "MetricName", "EM"]]}, {"text": "And we did apply these components in Section 3 to other modules , such as the \u201c extract - argument \u201d module ( extracts spans or tokens from paragraphs ) , and also obtained better results ( 0.5 % F1 increase ) .", "entities": [[39, 40, "MetricName", "F1"]]}, {"text": "Besides , for different question types , their statistics on the test set can be found in Table 4 .", "entities": []}, {"text": "Question Type Percentage date - compare 18.6 % date - difference 17.9 % number - compare 19.3 % extract - number 13.5 % count 17.6 % extract - argument 12.8 % Table 4 : Percentage by question types .", "entities": []}, {"text": "Current NMNs ( Gupta et al . , 2020 ) does not support other arithmetic datasets , since some arithmetic operations , including addition , are not supported .", "entities": []}, {"text": "Extending related arithmetic modules is one of our future work , based on which the NMNs could be trained on other datsets .", "entities": []}]