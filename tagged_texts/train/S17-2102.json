[{"text": "Proceedings of the 11th International Workshop on Semantic Evaluations ( SemEval-2017 ) , pages 621\u2013625 , Vancouver , Canada , August 3 - 4 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics NNEMBs at SemEval-2017 Task 4 : Neural Twitter Sentiment Classi\ufb01cation : a Simple Ensemble Method with Different Embeddings Yichun Yin Peking University yichunyin@pku.edu.cnYangqiu Song HKUST yqsong@cse.ust.hkMing Zhang", "entities": []}, {"text": "Peking University mzhang cs@pku.edu.cn", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "Recently , neural twitter sentiment classi\ufb01cation has become one of state - of - thearts , which requires less feature engineering work compared with traditional methods .", "entities": [[19, 21, "TaskName", "feature engineering"]]}, {"text": "In this paper , we propose a simple and effective ensemble method to further boost the performances of neural models .", "entities": []}, {"text": "We collect several word embedding sets which are publicly released ( often are learned on different corpus ) or constructed by running Skip - gram on released largescale corpus .", "entities": []}, {"text": "We make an assumption that different word embeddings cover different words and encode different semantic knowledge , thus using them together can improve the generalizations and performances of neural models .", "entities": [[6, 8, "TaskName", "word embeddings"]]}, {"text": "In the SemEval 2017 , our method ranks 1st in Accuracy , 5th in AverageR.", "entities": [[10, 11, "MetricName", "Accuracy"]]}, {"text": "Meanwhile , the additional comparisons demonstrate the superiority of our model over these ones based on only one word embedding set .", "entities": []}, {"text": "We release our code1for the method replicability .", "entities": []}, {"text": "1 Introduction Twitter sentiment classi\ufb01cation has attracted a lot of attention ( Dong et al . , 2015 ; Nakov et al . , 2016 ;", "entities": []}, {"text": "Rosenthal et al . , 2017 ) , which aims to classify a tweet into three sentiment categories : negative , neutral , and positive .", "entities": []}, {"text": "Tweet text has several features : written by the informal language , hash - tags and emoticons indicate sentiments , and sometimes is sarcasm , which make decisions of tweet sentiment hard for machines .", "entities": []}, {"text": "With releases of annotated datasets , more researchers prefer to use the 1https://github.com/zwjyyc/NNEMBstwitter sentiment classi\ufb01cation as one testbed to evaluate their proposed models .", "entities": []}, {"text": "Traditional methods ( Mohammad et al . , 2013 ) for twitter sentiment classi\ufb01cation use a variety of hand - crafted features including surface - form , semantic and sentiment lexicons .", "entities": []}, {"text": "The performances of these methods often depend on the quality of feature engineering work , and building a state - ofthe - art system is dif\ufb01cult for novices .", "entities": [[11, 13, "TaskName", "feature engineering"]]}, {"text": "Moreover , these designed features are presented by the onehot representation which can not capture the semantic relativeness of different features and proposes a problem of feature sparsity .", "entities": []}, {"text": "To address this , Tang et al .", "entities": []}, {"text": "( 2014 ) induced sentiment - speci\ufb01c low - dimensional , real - valued embedding features for twitter classi\ufb01cation , which encode both semantics and sentiments of words .", "entities": []}, {"text": "In the experiments , the embedding features and hand - crafted features obtain similar results , and also they are complementary for each other in the system .", "entities": []}, {"text": "With the developments of neural networks in natural language processing , neural sentiment classi\ufb01cation ( Severyn and Moschitti , 2015 ; Deriu et al . , 2016 ) has attracted a lot of attention recently and become the state - of - the - arts .", "entities": []}, {"text": "These methods \ufb01rst learn word embeddings from large - scale twitter corpus , then tune neural networks by the tweets which have distant labels , and \ufb01nally \ufb01ne - tune the proposed models by the annotated datasets .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "Learning word embeddings using in - domain data is an effective way to boost model performances ( Mikolov et al . , 2013 ; Yin et al . , 2016 ) .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "However , collecting large - scale twitter corpus is often time - consuming .", "entities": []}, {"text": "In this paper , we use the different word embedding sets to boost the performances of our neural networks , which only include released different word embeddings sets and the word embedding set derived from the released Yelp large - scale datasets by Skip - gram ( Mikolov et al . , 2013 ) .", "entities": [[25, 27, "TaskName", "word embeddings"]]}, {"text": "A simple and effective ensemble621", "entities": []}, {"text": "Figure 1 : Overview of our method .", "entities": []}, {"text": "method is proposed , which takes different word embedding sets as input to train neural networks and predicts labels of testing tweets by merging all output of neural models .", "entities": []}, {"text": "Our ensemble method show its effectiveness in SemEval 2017 , though most of used word embedding sets are not learned from twitter corpus , which can be explained that different embedding sets has different vocabularies and encode different parts of sentiment knowledge .", "entities": []}, {"text": "Moreover , we conduct additional experiments to analyze our model .", "entities": []}, {"text": "2 Method In this section , we describe the details of our method , which is illustrated in Figure 1 .", "entities": []}, {"text": "We feed different word embedding sets into neural networks and train these neural networks separately .", "entities": []}, {"text": "When predicting the labels of tweets in testing set , we sum label probabilities of all neural network to make \ufb01nal decisions .", "entities": []}, {"text": "2.1 Neural Network We have many choices of neural networks ( e.g. , LSTM , RNN and GRU ) for our method , here we consider RCNN ( Lei et al . , 2016 ) in our method .", "entities": [[13, 14, "MethodName", "LSTM"], [17, 18, "MethodName", "GRU"]]}, {"text": "RCNN has non - consecutive convolution and adaptive gated decay , which aims to capture longerrange , non - consecutive patterns in a weighted manner .", "entities": [[5, 6, "MethodName", "convolution"]]}, {"text": "Given a sequence of words which are denoted as{xi}l i=1 , the corresponding word embeddings { xi}l i=1are derived using the embedding matrix", "entities": [[13, 15, "TaskName", "word embeddings"]]}, {"text": "E. Then , RCNN obtains their corresponding hidden vectors{hi}l i=1using the convolution operation and gating mechanism .", "entities": [[11, 12, "MethodName", "convolution"]]}, {"text": "After obtaining hidden vectors , RCNN uses a pooling operation to get \ufb01xed - sized vector presentation , which is fedinto softmax layer to \ufb01nish the prediction .", "entities": [[21, 22, "MethodName", "softmax"]]}, {"text": "The ngram convolution operation and gating decay are described as follows :", "entities": [[2, 3, "MethodName", "convolution"]]}, {"text": "\u03bbt = \u03c3(W\u03bbxt+U\u03bbht\u22121+b\u03bb ) , c(1 ) t = \u03bbt\u2299c(1 ) t\u22121 + ( 1\u2212\u03bbt)\u2299(W1xt ) , c(2 ) t = \u03bbt\u2299c(2 ) t\u22121 + ( 1\u2212\u03bbt)\u2299(c(1 ) t\u22121+W2xt ) , \u00b7 \u00b7 \u00b7 , c(n ) t = \u03bbt\u2299c(n ) t\u22121 + ( 1\u2212\u03bbt)\u2299(c(n\u22121 ) t\u22121+Wnxt ) , ht= tanh ( c(n ) t+b ) , where W\u03bb , U\u03bb , b\u03bb , bandW\u2217are learnable parameters , \u03c3is sigmoid function which rescales the value into ( 0 , 1 ) , \u2299is dot product , \u03bbtis gating value determining how much information of xtand previous patterns is added into the hidden vector , c(i ) trefer to the vector for accumulated previous patterns which are ended with xtincludei consecutive tokens .", "entities": [[77, 78, "DatasetName", "0"]]}, {"text": "When \u03bbt= 0 , the convolution becomes a standard n - gram convolution .", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "MethodName", "convolution"], [12, 13, "MethodName", "convolution"]]}, {"text": "We also can build a deep RCNN by adding several convolution layer on top of hidden vectors derived from the bottom convolution layer .", "entities": [[10, 11, "MethodName", "convolution"], [21, 22, "MethodName", "convolution"]]}, {"text": "Here we consider the RCNN with dconvolution layers , which outputs{hd i}l i=1 .", "entities": []}, {"text": "Then , a last pooling operation is conducted on hidden vectors to obtain text representation r.", "entities": []}, {"text": "Finally , text representation is fed into a softmax layer .", "entities": [[8, 9, "MethodName", "softmax"]]}, {"text": "The softmax layer outputs the probability distribution over |Y|categories for the distributed representation , which is de\ufb01ned as : p(r ) = softmax ( Wclass kr ) .", "entities": [[1, 2, "MethodName", "softmax"], [22, 23, "MethodName", "softmax"]]}, {"text": "The cross - entropy objective function is used to optimize the RCNN model .", "entities": []}, {"text": "2.2 Prediction We learn different RCNN models with different embedding sets as input .", "entities": []}, {"text": "Formally , we havesembedding sets which are denoted as { E1,E2,\u00b7\u00b7\u00b7,Es } , and feed them into sRCNN models , then learn RCNN models separately .", "entities": []}, {"text": "We predict sentiment label of testing tweet based on these learned RCNN models , which are described by following functions:622", "entities": []}, {"text": "Sets Corpus Scale Algorithm Dimension Source V ocab gloveG General 840B GloVec 300 R 2.2 M gloveT Twitter 27B", "entities": [[9, 10, "DatasetName", "General"]]}, {"text": "GloVec 200 R 1.2", "entities": []}, {"text": "M word2vecGN", "entities": []}, {"text": "Google News 100B", "entities": [[0, 1, "DatasetName", "Google"]]}, {"text": "Word2Vec 300 R 3.0 M word2vecY Yelp Reviews 0.3B", "entities": []}, {"text": "Word2Vec 300 S 0.2 M Ensemble - - - - - 5.4 M Table 1 : Statistics of the embedding sets .", "entities": []}, {"text": "Rmeans the embedding set is publicly released and Smeans the embedding set is self - contained .", "entities": []}, {"text": "GloVec ( Mikolov et al . , 2013 ) and Word2Vec ( Pennington et al . , 2014 ) are most popular embedding algorithms .", "entities": []}, {"text": "Scale means the size of tokens in corpus , M and B refer to million and billion respectively .", "entities": []}, {"text": "The embedding set word2vecY are trained by Word2Vec with default settings and Yelp reviews are available at https://www.yelp.com/dataset challenge .", "entities": []}, {"text": "Dataset # num # category ratio Previous SemEvals 50032 1.5/4.7/3.8 SemEval 2017 Test 12284 3.2/4.8/2.0 Table 2 : Statistics of datasets .", "entities": []}, {"text": "p1 = RCNN 1({xi}l i=1,E1 ) , p2 = RCNN 2({xi}l i=1,E2 ) , ... , ps = RCNNs({xi}l i=1,Es ) , p / prime=/summationdisplay 1\u2264i\u2264spi .", "entities": []}, {"text": "y = argmax1\u2264i\u2264|Y|p / prime i , whereyis the predicted label .", "entities": []}, {"text": "3 Experiment 3.1 Datasets and Settings We use 4 embedding sets which are described in Table 1 .", "entities": []}, {"text": "Meanwhile , we crawl and merge all annotated datasets of previous SemEvals , and split them into training , development , and testing sets with ratio 8:1:1 , which are shown in Table 2 together with testing set of SemEval 2017 .", "entities": []}, {"text": "From the table , we can see that testing set of SemEval 2017 has big differences on the category ratio ( negative : neutral : positive ) , compared with the previous SemEval datasets .", "entities": []}, {"text": "For the model settings , all RCNN models have same con\ufb01gurations but different word embedding sets .", "entities": []}, {"text": "We set dimensions of hidden vectors to 250 and depthsdto 2 .", "entities": []}, {"text": "To avoid model over-\ufb01tting , we use dropout and regularization as follows : ( 1 ) the regularization parameter is set to 1e-5 ; ( 2 ) thedropout rate is set to 0.3 , which is applied in the \ufb01nal text representation .", "entities": []}, {"text": "All parameters are learned by Adam optimizer ( Kingma and Ba , 2014 ) with the learning rate 0.001 .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [16, 18, "HyperparameterName", "learning rate"]]}, {"text": "Note that , all word embedding sets are \ufb01xed when training .", "entities": []}, {"text": "All models are tuned by the development set in Training .", "entities": []}, {"text": "3.2 Results and Analysis In this section , we \ufb01rst report the results on datasets of previous SemEvals , which are described in Table 3 .", "entities": []}, {"text": "Then , we report the performances of our method on SemEval 2017 in Table 4 .", "entities": []}, {"text": "From the Table 3 , we observe that gloveT performs worst though it is trained on in - domain twitter dataset and the word2vecY performs best though it is derived from yelp reviews .", "entities": []}, {"text": "As far as we known , Yelp data is constructed by carefully \ufb01ltering and is high - quality .", "entities": []}, {"text": "Thus , we can include that the quality of corpus is also important as the size of corpus and domain in twitter sentiment classi\ufb01cation .", "entities": []}, {"text": "Additionally , we can infer that word2vecGN outperforms others in recall of negative category , word2vecY performs best in recall of neutral category , and gloveT is best in recall of positive category .", "entities": []}, {"text": "Different embedding sets propose different characteristics .", "entities": []}, {"text": "Additionally , the ensemble method obtains a signi\ufb01cant improvement of 4 % .", "entities": []}, {"text": "In the Table 4 , we compare our method with best and median systems in SemEval 2017 , and report the results of individual embedding sets .", "entities": []}, {"text": "Our method outperforms other systems in accuracy , but performs worse in R Average , especially in R Negative .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "Compared with the median system , our method has improvements of about 5 % in both accuracy and R Average .", "entities": [[16, 17, "MetricName", "accuracy"]]}, {"text": "Different from the results in Table 3 , the word2vecY performs623", "entities": []}, {"text": "Embeddings Accuracy RNegative RNeutral RPositive RAverage gloveG 70.6 66.3 66.4 77.7 70.1 gloveT 68.3 66.8 61.2 77.9 68.7 word2vecGN 70.5 70.2 68.3 73.5 70.7 word2vecY 72.2 65.0 71.3 76.2 70.8 Ensemble 74.6 72.1 71.2 80.0 74.5", "entities": [[1, 2, "MetricName", "Accuracy"]]}, {"text": "Table 3 : Results on datasets of previous SemEval .", "entities": []}, {"text": "R * means recall value .", "entities": []}, {"text": "Embeddings Accuracy RNegative RNeutral RPositive RAverage Best system 65.1 82.9 51.2 70.2 68.1 Median system 61.6", "entities": [[1, 2, "MetricName", "Accuracy"]]}, {"text": "53.1 65.0 67.4 61.8 gloveG 62.9 63.0 61.3 66.7 63.7 gloveT 63.7 70.5 57.4 68.2 65.4 word2vecGN 62.9 68.4 60.0 60.8 63.1 word2vecY 61.6 59.1 63.8 60.5 61.1 Our 66.4 69.8 64.0 66.8 66.9 Table 4 : Results on SemEval 2017 .", "entities": []}, {"text": "The median system is the system of rank 19th among 38 teams .", "entities": []}, {"text": "worse among these embedding sets , while the gloveT obtains best performances .", "entities": []}, {"text": "Additionally , we can observe that gloveT performs best both in RNegative and R Positive , and word2vecY performs best in R Neutral .", "entities": []}, {"text": "Compared with the embedding baselines , our ensemble method obtains improvements of 2.7 % and 1.5 % in accuracy and RAverage respectively , which demonstrates the effectiveness of the proposed method .", "entities": [[18, 19, "MetricName", "accuracy"]]}, {"text": "3.3 Error Analysis In this section , we analyze the incorrect predictions of our system in SemEval 2017 .", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "We summarize four kinds of errors in our system .", "entities": []}, {"text": "The \ufb01rst one is that some decisions need domain knowledge , which our method only can learn from the labeled datasets .", "entities": []}, {"text": "The instances are as follows : Messi \u2019s 100 international goals for Barcelona # fcblive https://t.co/fMkglvusL1 [ via @thereisagenius ] .", "entities": []}, {"text": "Predicted label : neutral , golden label : positive # Trudeau gives your cash to # Terrorist # Hamas - in\ufb02uenced group - # UNRWA - @CandiceMalcolm", "entities": []}, {"text": "https://t.co/5i5o2qwRWl Predicted label : neutral , golden label : negative Messis 9 goals in CL are more than 20 of the 32 teams in the competition have scored in total , and he s tied with \ufb01ve other sides # fcblive Predicted label : neutral , golden label : positive", "entities": []}, {"text": "The second one is emoticons in tweet , as most of word embedding sets do not include emoticon embeddings and emoticons are always with sentiFigure 2 : Emoticon instances .", "entities": []}, {"text": "ments .", "entities": []}, {"text": "The instances are described in Figure 2", "entities": []}, {"text": "The third one is that sentiments are not consistent in sentences .", "entities": []}, {"text": "For example , the \ufb01rst half part is positive , while the second half part is negative , in this case , our system would predict \u2019 positive \u2019 or \u2019 negative \u2019 , the golden label is neutral .", "entities": []}, {"text": "@jimmyfallon 1 .", "entities": []}, {"text": "Emily 2 . Michel 3 .", "entities": []}, {"text": "Kirk 4 .", "entities": []}, {"text": "TJ .", "entities": []}, {"text": "Love the quirky ones and Emily coz", "entities": []}, {"text": "she \u2019s such a BIATCH !", "entities": []}, {"text": "# gilmoregirlstop4 .", "entities": []}, {"text": "predicted label positive , golden label :", "entities": []}, {"text": "neutral The fourth one is the sarcasm , such as : # Hamas leader : # Trump may be a # Jew https://t.co/jGFZTvj2pF .", "entities": []}, {"text": "predicted label positive , golden label : negative 4 Conclusion We propose a simple and effective ensemble method to boost the neural twitter sentiment classi\ufb01cation .", "entities": []}, {"text": "By using different embedding sets , the system can cover more words and encode more sentiment information .", "entities": []}, {"text": "The results on datasets of previous SemEval and SemEval 2017 show the effectiveness of our method .", "entities": []}, {"text": "Moreover , error analysis is conducted to propose the main challenges for our method .", "entities": []}, {"text": "We release our code for system duplicability.624", "entities": []}, {"text": "References Jan Deriu , Maurice Gonzenbach , Fatih Uzdilli , Aur\u00b4elien Lucchi , Valeria De Luca , and Martin Jaggi .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Swisscheese at semeval-2016 task 4 : Sentiment classi\ufb01cation using an ensemble of convolutional neural networks with distant supervision .", "entities": []}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation , SemEval@NAACL - HLT 2016 , San Diego , CA , USA , June 16 - 17 , 2016 .", "entities": []}, {"text": "pages 1124\u20131128 .", "entities": []}, {"text": "http://aclweb.org/anthology/S/S16/S16-1173.pdf .", "entities": []}, {"text": "Li Dong , Furu Wei , Yichun Yin , Ming Zhou , and Ke Xu . 2015 .", "entities": []}, {"text": "Splusplus :", "entities": []}, {"text": "A feature - rich two - stage classi\ufb01er for sentiment analysis of tweets .", "entities": [[9, 11, "TaskName", "sentiment analysis"]]}, {"text": "SemEval2015 page 515 .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "CoRR abs/1412.6980 .", "entities": []}, {"text": "http://arxiv.org/abs/1412.6980 .", "entities": []}, {"text": "Tao Lei , Hrishikesh Joshi , Regina Barzilay , Tommi S. Jaakkola , Kateryna Tymoshenko , Alessandro Moschitti , and Llu \u00b4 \u0131s M ` arquez .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semi - supervised question retrieval with gated convolutions .", "entities": []}, {"text": "In NAACL HLT 2016 , The 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , San Diego California , USA , June 12 - 17 , 2016 .", "entities": []}, {"text": "pages 1279\u20131289 . http://aclweb.org/anthology/N/N16/N16-1153.pdf .", "entities": []}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Gregory S. Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 26 : 27th Annual Conference on Neural Information Processing Systems 2013 .", "entities": []}, {"text": "Proceedings of a meeting held December 5 - 8 , 2013 , Lake Tahoe , Nevada , United States . .", "entities": []}, {"text": "pages 3111\u20133119 . http://papers.nips.cc/paper/5021distributed-representations-of-words-and-phrasesand-their-compositionality .", "entities": []}, {"text": "Saif Mohammad , Svetlana Kiritchenko , and Xiaodan Zhu . 2013 .", "entities": []}, {"text": "Nrc - canada : Building the state - of - theart in sentiment analysis of tweets .", "entities": [[12, 14, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 7th International Workshop on Semantic Evaluation , SemEval@NAACL - HLT 2013 , Atlanta , Georgia , USA , June 14 - 15 , 2013 .", "entities": []}, {"text": "pages 321\u2013327 . http://aclweb.org/anthology/S/S13/S13-2053.pdf .", "entities": []}, {"text": "Preslav Nakov , Alan Ritter , Sara Rosenthal , Fabrizio Sebastiani , and Veselin Stoyanov .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semeval2016 task 4 : Sentiment analysis in twitter .", "entities": [[4, 6, "TaskName", "Sentiment analysis"]]}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation , SemEval@NAACL - HLT 2016 , San Diego , CA , USA , June 16 - 17 , 2016 .", "entities": []}, {"text": "pages 1\u201318 .", "entities": []}, {"text": "http://aclweb.org/anthology/S/S16/S16-1001.pdf .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher D. Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing , EMNLP 2014 , October 25 - 29,2014 , Doha , Qatar , A meeting of SIGDAT , a Special Interest Group of the ACL .", "entities": []}, {"text": "pages 1532\u20131543 .", "entities": []}, {"text": "http://aclweb.org/anthology/D/D14/D14-1162.pdf .", "entities": []}, {"text": "Sara Rosenthal , Noura Farra , and Preslav Nakov . 2017 .", "entities": []}, {"text": "SemEval-2017 task 4 : Sentiment analysis in Twitter .", "entities": [[4, 6, "TaskName", "Sentiment analysis"]]}, {"text": "In Proceedings of the 11th International Workshop on Semantic Evaluation .", "entities": []}, {"text": "Association for Computational Linguistics , Vancouver , Canada , SemEval \u2019 17 .", "entities": []}, {"text": "Aliaksei Severyn and Alessandro Moschitti . 2015 .", "entities": []}, {"text": "UNITN : training deep convolutional neural network for twitter sentiment classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation , SemEval@NAACL - HLT 2015 , Denver , Colorado , USA , June 4 - 5 , 2015 .", "entities": []}, {"text": "pages 464\u2013469 .", "entities": []}, {"text": "http://aclweb.org/anthology/S/S15/S15-2079.pdf . Duyu Tang , Furu Wei , Nan Yang , Ming Zhou , Ting Liu , and Bing Qin . 2014 .", "entities": []}, {"text": "Learning sentimentspeci\ufb01c word embedding for twitter sentiment classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) .", "entities": []}, {"text": "Association for Computational Linguistics , Baltimore , Maryland , pages 1555\u20131565 .", "entities": []}, {"text": "http://www.aclweb.org/anthology/P141146 .", "entities": []}, {"text": "Yichun Yin , Furu Wei , Li Dong , Kaimeng Xu , Ming Zhang , and Ming Zhou .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Unsupervised word and dependency path embeddings for aspect term extraction .", "entities": [[8, 10, "TaskName", "term extraction"]]}, {"text": "In Proceedings of the Twenty - Fifth International Joint Conference on Arti\ufb01cial Intelligence , IJCAI 2016 , New York , NY , USA , 9 - 15 July 2016 .", "entities": []}, {"text": "pages 2979\u20132985 .", "entities": []}, {"text": "http://www.ijcai.org/Abstract/16/423.625", "entities": []}]