[{"text": "Proceedings of the Fifth Workshop on NLP for Similar Languages , Varieties and Dialects , pages 156\u2013163 Santa Fe , New Mexico , USA , August 20 , 2018.156Comparing CRF and LSTM performance on the task of morphosyntactic tagging of non - standard varieties of South Slavic languages Nikola Ljube", "entities": [[29, 30, "MethodName", "CRF"], [31, 32, "MethodName", "LSTM"]]}, {"text": "\u02c7si\u00b4c Dept . of Knowledge Technologies Jo\u02c7zef Stefan Institute Jamova cesta 39 1000 Ljubljana , Slovenia nikola.ljubesic@ijs.si", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "This paper presents two systems taking part in the Morphosyntactic Tagging of Tweets shared task on Slovene , Croatian and Serbian data , organized inside the VarDial Evaluation Campaign .", "entities": []}, {"text": "While one system relies on the traditional method for sequence labeling ( conditional random \ufb01elds ) , the other relies on its neural alternative ( bidirectional long short - term memory ) .", "entities": [[26, 31, "MethodName", "long short - term memory"]]}, {"text": "We investigate the similarities and differences of these two approaches , showing that both methods yield very good and quite similar results , with the neural model outperforming the traditional one more as the level of non - standardness of the text increases .", "entities": []}, {"text": "Through an error analysis we show that the neural system is better at long - range dependencies , while the traditional system excels and slightly outperforms the neural system at the local ones .", "entities": []}, {"text": "We present in the paper new state - of - the - art results in morphosyntactic annotation of non - standard text for Slovene , Croatian and Serbian .", "entities": []}, {"text": "1 Introduction In this paper we present two systems taking part in the MTT ( Morphosyntactic Tagging of Tweets ) shared task , part of the VarDial Evaluation Campaign ( Zampieri et al . , 2018 ) .", "entities": []}, {"text": "In the task , general - domain and indomain datasets with tokens manually annotated with morphosyntactic descriptions ( MSDs ) , are given , together with large web - based datasets , for three South Slavic languages : Slovene , Croatian and Serbian .", "entities": []}, {"text": "The challenge of the task is to exploit similarity of standard vs. non - standard variants , as well as the overall proximity of the three languages in question .", "entities": []}, {"text": "While the \ufb01rst system , JANES , relies on the traditional method for sequence labeling , namely conditional random \ufb01elds ( CRF ) , the second system , JSI , relies on the currently hugely popular neural networks , more precisely bidirectional long short - term memories ( BiLSTM ) .", "entities": [[21, 22, "MethodName", "CRF"], [48, 49, "MethodName", "BiLSTM"]]}, {"text": "The contributions of this paper are the following : ( 1 ) a direct comparison of CRFs and BiLSTMs on a series of datasets , where CRFs are equipped with carefully engineered features , not generic ones , and ( 2 ) a new state - of - the - art in tagging non - standard varieties of the three languages in question .", "entities": []}, {"text": "2 System Descriptions 2.1 Datasets Distributed inside the Shared Task Before we describe our two systems participating in the task , we quickly quantify the available resources through token number in Table 1 as these heavily in\ufb02uence our decisions in the system setup .", "entities": []}, {"text": "Thetwitter .", "entities": []}, {"text": "* datasets come from the Janes - Tag manually annotated dataset of Slovene computermediated communication ( Erjavec et al . , 2017 ) and the ReLDI - NormTagNER- * manually annotated datasets of Croatian ( Ljube \u02c7si\u00b4c et", "entities": []}, {"text": "al . , 2017b ) and", "entities": []}, {"text": "Serbian ( Ljube \u02c7si\u00b4c et", "entities": []}, {"text": "al . , 2017c ) tweets .", "entities": []}, {"text": "They are all similar in size , with cca . 40 thousand tokens available for training , 8 thousand for development and 20 thousand for testing .", "entities": []}, {"text": "This work is licensed under a Creative Commons Attribution 4.0 International Licence .", "entities": []}, {"text": "Licence details : http://creativecommons.org/licenses/by/4.0/.", "entities": []}, {"text": "157Thestandard.train datasets mostly cover the general domain .", "entities": []}, {"text": "While the Slovene and Croatian datasets are similar in size with around 500 thousand tokens , the Serbian dataset is signi\ufb01cantly smaller with only 87 thousand tokens .", "entities": []}, {"text": "Theweb.auto datasets are large web - based datasets , slWac for Slovene ( Erjavec et al . , 2015 ) , hrWaC for Croatian and srWaC for Serbian ( Ljube \u02c7si\u00b4c and Klubi \u02c7cka , 2014 ) .", "entities": []}, {"text": "These are automatically annotated with state - of - the - art taggers of standard language for Slovene ( Ljube \u02c7si\u00b4c and Erjavec , 2016 ) and Croatian and Serbian", "entities": []}, {"text": "( Ljube \u02c7si\u00b4c et", "entities": []}, {"text": "al . , 2016 ) .", "entities": []}, {"text": "twitter.train twitter.dev twitter.test standard.train web.auto Slovene 37,756 7,056 19,296 586,248 895,875,492 Croatian 45,609 8,886 21,412 506,460 1,397,757,548 Serbian 45,708 9,581 23,327 86,765 554,627,647 Table 1 : Size of datasets distributed through the MTT shared task .", "entities": []}, {"text": "Sizes are in number of tokens .", "entities": []}, {"text": "2.2", "entities": []}, {"text": "The JANES System The JANES system ( the name of the system comes from the Slovene national project JANES inside which the system was developed1 ) is based on conditional random \ufb01elds ( CRFs ) ( Lafferty et al . , 2001 ) , exploiting the following handcrafted features : \u000flowercased focus token ( token for which features are being extracted ) \u000flowercased tokens in a window of f\u00003;\u00002;\u00001;1;2;3gform the focus token \u000ffocus token suf\ufb01xes of length f1;2;3;4 g \u000ffeatures encoding whether the focus token starts with http ( link ) , # ( hashtag ) or @(mention ) \u000fBrown cluster binary paths for the focus token , with the path length of f2;4;6;8", "entities": []}, {"text": "g These features were proven to yield optimal results in our previous work on tagging non - standard Slovene ( Ljube \u02c7si\u00b4c et", "entities": []}, {"text": "al . , 2017a ) .", "entities": []}, {"text": "The Brown clusters , the output of a method for context - dependent hierarchical word clustering ( Brown et al . , 1992 ) , were calculated from the web data that were made available through the shared task , namely the slWaC web corpus of Slovene ( Erjavec et al . , 2015 ) and the hrWaC and srWaC corpora of Croatian and Serbian ( Ljube \u02c7si\u00b4c and Klubi \u02c7cka , 2014 ) .", "entities": []}, {"text": "We have used default parameters for calculating Brown clusters , except for the minimum occurrence parameter which was set to 5 .", "entities": []}, {"text": "The web text was previously lowercased and punctuations and newlines were removed from it .", "entities": []}, {"text": "For training the tagger , we exploited ( 1 ) the proximity of the Croatian and Serbian language , and ( 2 ) the fact that we have much more standard training data and much less Twitter training data .", "entities": []}, {"text": "We sampled our \ufb01nal training data for each language in the following manner : \u000ffor Slovene : we added to the Slovene standard training data ten times the available non - standard data , thereby reaching a similar amount of standard and non - standard data in our training set ; from previous work we know that for CRFs oversampling in - domain data is the simplest and most effective method in merging out - domain and in - domain training data ( Horsmann and Zesch , 2015 ;", "entities": []}, {"text": "Ljube \u02c7si\u00b4c et al . , 2017a ) \u000ffor Croatian : we merged the Croatian and the Serbian standard language training datasets , added to it ten copies of the Croatian Twitter training dataset and two copies of the Serbian training dataset , thereby putting emphasis on the Croatian training data , which is expected to be closer to the Croatian test data 1http://nl.ijs.si/janes/english/", "entities": []}, {"text": "158\u000ffor Serbian : we merged the Croatian standard training data , two copies of the Serbian standard training data ( as these are more than \ufb01ve times smaller than the Croatian ones ) , ten copies of nonstandard Croatian training data , and four copies of non - standard Serbian training data , with the rationale that most non - standard elements in Croatian are present in non - standard Serbian as well , but with lower frequency ; by oversampling non - standard Croatian in the Serbian dataset we emphasize the non - standard elements in the Croatian non - standard training data as the Serbian non - standard data is much closer to the standard language ( Mili \u02c7cevi\u00b4c and Ljube \u02c7si\u00b4c , 2016 )", "entities": []}, {"text": "The system was implemented in CRFSuite ( Okazaki , 2007 ) , using the passive aggressive optimizer and 10 epochs , a setting which proved to yield best results in previous experiments ( Ljube \u02c7si\u00b4c and Erjavec , 2016 ) .", "entities": [[16, 17, "HyperparameterName", "optimizer"]]}, {"text": "2.3 The JSI System The JSI system ( the name comes from the name of our current employer , the Jo \u02c7zef Stefan Institute ) is an adaptation of the BiLSTM tagger written in pytorch2 , with some added modi\ufb01cations .", "entities": [[30, 31, "MethodName", "BiLSTM"]]}, {"text": "The architecture of the submitted system is the following : \u000fa character - level subnetwork , consisting of a character embedding layer of 16 dimensions and a BiLSTM layer with 25 units \u000fthe main network \u2013 concatenating the character - level representation of a word from the subnetwork described above ( 25 * 2 , i.e. , 50 dimensions ) , and the word embedding layer ( 100 dimensions ) \u2013 feeding this concatenated 150 - dimensional character- and word - level representation into a BiLSTM layer with 100 units \u2013 the per - token BiLSTM output being fed to a fully - connected layer with 256 units and a \ufb01nal softmax layer for prediction While developing this architecture , we investigated the impact of various setups on the Slovene dataset .", "entities": [[27, 28, "MethodName", "BiLSTM"], [85, 86, "MethodName", "BiLSTM"], [95, 96, "MethodName", "BiLSTM"], [111, 112, "MethodName", "softmax"]]}, {"text": "The results of experimenting with ( 1 ) different pretrained word embeddings , ( 2 ) the impact of adding different character - level representations , ( 3 ) \ufb01ne - tuning the model on in - domain data and ( 4 ) pretraining the character - level encoder on a in\ufb02ectional lexicon , are shown in Table 2 .", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "We performed our experiments on each of the above mentioned issues subsequently , always propagating to the next experiment set the setup achieving best results in the previous one .", "entities": []}, {"text": "The setup we start with consists only of the main network , without the character - level subnetwork .", "entities": []}, {"text": "2.3.1 Word Embeddings The \ufb01rst group of results considers different ways of pretraining word embeddings .", "entities": [[1, 3, "TaskName", "Word Embeddings"], [13, 15, "TaskName", "word embeddings"]]}, {"text": "The word embeddings were always pretrained on the web data available for each language .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "We considered only two tools for pretraining word embeddings : word2vec ( Mikolov et al . , 2013 ) and fasttext ( Bojanowski et al . , 2017 ) , and two architectures , CBOW and Skipgram .", "entities": [[7, 9, "TaskName", "word embeddings"], [20, 21, "MethodName", "fasttext"]]}, {"text": "The results ( word2vec cbow vs.word2vec skipgram ) show for Skipgram to be signi\ufb01cantly better suited for this task , which is in line with previous results ( Reimers and Gurevych , 2017 ) .", "entities": []}, {"text": "Comparing word2vec and fasttext ( word2vec skipgram vs.fasttext skipgram ) , fasttext shows a slightly better performance , but the difference gets more obvious ( almost half a point in token accuracy ) once fasttext is used to generate representations for the words not present in the pretrained word embeddings ( fasttext skipgram generated )", "entities": [[3, 4, "MethodName", "fasttext"], [11, 12, "MethodName", "fasttext"], [31, 32, "MetricName", "accuracy"], [34, 35, "MethodName", "fasttext"], [48, 50, "TaskName", "word embeddings"], [51, 52, "MethodName", "fasttext"]]}, {"text": ".3 2https://github.com/neulab/dynet-benchmark/blob/master/pytorch/ bilstm-tagger-withchar.py 3We would expect the positive impact of generating embeddings for out - of - vocabulary words to diminish once characterlevel representations are added to the model .", "entities": []}, {"text": "However , we did not investigate this .", "entities": []}, {"text": "159setup token accuracy with stdev word2vec cbow 0.8407 \u00060.0025 word2vec skipgram 0.8550 \u00060.0041 fasttext skipgram 0.8578 \u00060.0041 fasttext skipgram generated 0 .", "entities": [[2, 3, "MetricName", "accuracy"], [13, 14, "MethodName", "fasttext"], [17, 18, "MethodName", "fasttext"], [20, 21, "DatasetName", "0"]]}, {"text": "8596\u00060.0031 added character - level encoding 0.8780 \u00060.0030 added bidirectional encoding 0.8790\u00060.0032 additionally tuned on in - domain data 0.8836\u00060.0026 pretrained character - level encoder on web data 0.8855\u00060.0015 Table 2 : Initial experiments on the JSI system , performed on the Slovene dataset .", "entities": []}, {"text": "The standard deviation is calculated from ten evaluations performed during the last epoch .", "entities": []}, {"text": "2.3.2 Character - level Representations The second group of experiments considers the impact of adding character - level representations of each token to the word representation via a dedicated character - level BiLSTM .", "entities": [[32, 33, "MethodName", "BiLSTM"]]}, {"text": "Adding the character - level representation has shown the biggest impact among all the experiments , with \u00182accuracy points increase , and a minor difference between encoding the character sequence with a single - direction or a bi - directional LSTM .", "entities": [[40, 41, "MethodName", "LSTM"]]}, {"text": "2.3.3 Fine - tuning on the In - Domain Dataset", "entities": []}, {"text": "The third experiment considers the impact of not training the network on a simple merge of all the available relevant training data , but also \ufb01ne - tuning the network exclusively on in - domain data .", "entities": []}, {"text": "Running three epochs on the concatenation of all datasets , and then additional two epochs only on the in - domain Twitter data , consistently improved the results for around half an accuracy point .", "entities": [[32, 33, "MetricName", "accuracy"]]}, {"text": "This method is somewhat similar to the oversampling method applied on the JANES system .", "entities": []}, {"text": "It is , however , more elegant as it gives greater control over the amount and order of data fed into the system .", "entities": []}, {"text": "2.3.4 Pretraining the Character - level BiLSTM Finally , in the last set of experiments we investigated whether there is positive impact if the characterlevel encoder was pretrained on a in\ufb02ectional - lexicon - like resource .", "entities": [[6, 7, "MethodName", "BiLSTM"]]}, {"text": "In this shared task the web data were automatically tagged with a CRF tagger relying on a lexicon ( Ljube \u02c7si\u00b4c et", "entities": [[12, 13, "MethodName", "CRF"]]}, {"text": "al . , 2016 ; Ljube \u02c7si\u00b4c and Erjavec , 2016 ) , therefore we transformed the automatically - tagged web data into a lexicon by ( 1 ) picking only token - tag pairs occurring at least 100 times in the web data and ( 2 ) selecting only the most frequent token - tag pair per token .", "entities": []}, {"text": "With the second criterion we lost some information on homonymous words , but also got rid of a lot of wrong automatic annotations of frequent words .", "entities": []}, {"text": "The results on pretraining the character - level encoder show that the improvement lies below half an accuracy point , but this improvement showed to be consistent across all the three languages.4 3 Results In this section we report the results of the \ufb01nal setups of the JANES and the JSI system and compare it to the HunPos baseline ( Hal \u00b4 acsy et al . , 2007 ) de\ufb01ned by the shared task organizers .", "entities": [[17, 18, "MetricName", "accuracy"]]}, {"text": "Additionally , we report the results of the JANES system using an in\ufb02ectional lexicon for the speci\ufb01c language , namely Sloleks for Slovene ( Dobrovoljc et al . , 2015 ) , hrLex for Croatian ( Ljube \u02c7si\u00b4c et", "entities": []}, {"text": "al . , 2016a ) and srLex for Serbian ( Ljube \u02c7si\u00b4c et al . , 2016b ) .", "entities": []}, {"text": "We call this system JANES - lex .", "entities": []}, {"text": "We compare to this system as it is very straightforward to add information from an in\ufb02ectional lexicon as additional features to a CRF - based system .", "entities": [[22, 23, "MethodName", "CRF"]]}, {"text": "4On Croatian data we ran an additional experiment not with the noisy web data , but the manually constructed in\ufb02ectional lexicon hrLex ( Ljube \u02c7si\u00b4c et al . , 2016a ) , improving additionally for almost half an accuracy point .", "entities": [[38, 39, "MetricName", "accuracy"]]}, {"text": "However , in this shared task we decided not to use resources that were not shared by the organizers as we ( correctly ) assumed that other teams will not use additional resources neither and that we would lower the comparability of the obtained results .", "entities": []}, {"text": "160Slovene Croatian Serbian HunPos baseline 0.832 0.834 0.832 JANES 0.871 0.893 0.900 JANES - lex 0.877 0.897 0.901 JSI 0.883 0.890 0.900 JSI - simpler 0.891 0.898 0.903 Table 3 : Results of the two systems , their two adaptations and the baseline on the test data .", "entities": []}, {"text": "Reported metric is token - level accuracy .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "We also report a modi\ufb01cation of the JSI system that we implemented after the shared task was already concluded .", "entities": []}, {"text": "Namely , we removed the fully connected layer between the main BiLSTM and the softmax layer , which is actually the most frequent setup for sequence labeling .", "entities": [[11, 12, "MethodName", "BiLSTM"], [14, 15, "MethodName", "softmax"]]}, {"text": "The removed layer in the JSI system is a residue from the tagger we based our implementation on5 .", "entities": []}, {"text": "We call the simpli\ufb01ed tagger JSI - simpler .", "entities": []}, {"text": "The results of the two taggers and the two variants are given in Table 3 .", "entities": []}, {"text": "The reported results are those obtained on the test data .", "entities": []}, {"text": "We can \ufb01rst observe that ( 1 ) all the systems outperform the HunPos baseline by a wide margin and that ( 2 ) the results of the four remaining systems are rather close .", "entities": []}, {"text": "The largest difference that can be observed between the four systems are 2 accuracy points on Slovene between the basic CRF implementation ( JANES ) and the simpli\ufb01ed BiLSTM implementation ( JSIsimpler ) .", "entities": [[13, 14, "MetricName", "accuracy"], [20, 21, "MethodName", "CRF"], [28, 29, "MethodName", "BiLSTM"]]}, {"text": "The same difference is not to be observed on the other two languages , with the same systems having a difference of 0.5 points on Croatian and 0.3 points on Serbian .", "entities": []}, {"text": "The reason for the larger difference on Slovene data lies in the fact that the Slovene data is least standard ( 17 % tokens being nonstandard ) , followed by Croatian ( 13 % non - standard tokens ) , with Serbian data deviating the least from the norm ( 10 % non - standard tokens ) ( Mili \u02c7cevi\u00b4c et", "entities": []}, {"text": "al . , 2017 ) as more complex modeling techniques pay off more as the language deviates stronger from the norm .", "entities": []}, {"text": "Adding lexicon information to the JANES system ( JANES vs. JANES - lex ) improves the results on all three languages , but just slightly , between 0.1 % and 0.6 % .", "entities": []}, {"text": "Previous work on the problem ( Ljube \u02c7si\u00b4c et", "entities": []}, {"text": "al . , 2017a ) has shown that Brown clusters already provide to a large extent the information that was traditionally obtained through in\ufb02ectional lexicons .", "entities": []}, {"text": "Comparing the JANES and JSI results by using the McNemar \u2019s statistical test ( McNemar , 1947 ) , the difference on Slovene is statistically signi\ufb01cant at the p < 0:001level , with an absolute difference in 1.2 points and an relative error reduction of 9.3 % .", "entities": []}, {"text": "The differences on the remaining two languages are not statistically signi\ufb01cant .", "entities": []}, {"text": "When comparing the JSI and JSI - simpler results , it becomes obvious that the additional layer in the JSI system actually deteriorates the results .", "entities": []}, {"text": "On all the three languages , the differences are statistically signi\ufb01cant , on Slovene and Croatian on the p < 0:001level , while on Serbian it is on the p < 0:05level .", "entities": []}, {"text": "The level of signi\ufb01cance of difference between the JANES and JSI - simpler systems is identical to that of between JSI and JSI - simpler .", "entities": []}, {"text": "The most interesting observation from the \ufb01nal evaluation of the submitted and modi\ufb01ed systems is that the difference between the traditional CRFs and the ( probably over - hyped ? )", "entities": []}, {"text": "BiLSTMs is actually quite small , with relative error reductions being 15 % on Slovene , 5 % on Croatian and only 3 % on Serbian .", "entities": []}, {"text": "These results , as well as some preliminary results on standard test sets , suggest that there would be no signi\ufb01cant difference in the results between CRFs and BiLSTMs on standard training and test data .", "entities": []}, {"text": "5https://github.com/neulab/dynet-benchmark/blob/master/pytorch/ bilstm-tagger-withchar.py", "entities": []}, {"text": "161JANES JSI - simpler pred true freq pred true freq", "entities": []}, {"text": "Xf", "entities": []}, {"text": "Npmsn 74", "entities": []}, {"text": "Xf", "entities": []}, {"text": "Npmsn 55", "entities": []}, {"text": "Cc Qo 59", "entities": []}, {"text": "Cc Qo 55 Ncmsan Ncmsn", "entities": []}, {"text": "46", "entities": []}, {"text": "Npmsn Xf 40", "entities": []}, {"text": "Ncmsn Ncmsan 34 Ncmsan Ncmsn", "entities": []}, {"text": "34 Ncmsn Npmsn 31", "entities": []}, {"text": "Rgp Cs 23 Xf", "entities": []}, {"text": "Npmsan 23", "entities": []}, {"text": "Xf Ncmsn 24", "entities": []}, {"text": "Rgp Cs 23 Sl Sa 23", "entities": []}, {"text": "Npmsn Xf 23", "entities": []}, {"text": "Ncmsn Ncmsan 23", "entities": []}, {"text": "Xf Ncmsn 22", "entities": []}, {"text": "Agpnsny Rgp 20 Cs Rgp 20 Sa Sl 19 Table 4 : The ten most frequent confusion pairs for the JANES and the JSI - simpler systems .", "entities": []}, {"text": "4 Error analysis In this section we perform an analysis of confusion matrices of the JANES and the JSI - simpler system .", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "We perform the analysis on the output of the system on the Croatian test set .", "entities": []}, {"text": "We analyze and compare the 10 most frequent confusions for each system , which covers roughly 20 % of all errors done by each of the systems .", "entities": []}, {"text": "The confusion pairs are given in Table 4 .", "entities": []}, {"text": "Both systems make similar most frequent mistakes , some of which are typical for morphosyntactic tagging of standard varieties of South Slavic languages , other being more speci\ufb01c for the Twitter variety .", "entities": []}, {"text": "The typical mistakes on the standard language include confusing nominative masculinum common nouns ( Ncmsn ) for accusative masculinum common nouns ( Ncmsan ) and vice versa , confusing the word \u201c i \u201d ( English \u201c and \u201d ) in its coordinating conjunction ( Cc ) and particle ( Qo ) usage , confusing adverbs ( Rgp ) for adjectives ( Agpnsny for instance ) and confusing the word \u201c kada \u201d ( English \u201c when \u201d ) in its subordinative conjunction ( Cs ) and adverbial ( Rgp ) usage .", "entities": []}, {"text": "The errors that are more due to the speci\ufb01city of the Twitter variety are confusing proper names ( Np . * ) or common nouns ( Nc . * ) for foreign residuals ( mostly foreign words or foreign sequences of words , Xf ) and vice versa .", "entities": []}, {"text": "When comparing the most frequent errors between the two systems , the JANES CRF - based system seems to have more problems with the traditional discrimination between different context - dependent cases of nouns , which points to the direction that BiLSTMs are better at modeling long - range dependencies as discriminating between the nominative and the accusative case often requires a very wide context .", "entities": [[13, 14, "MethodName", "CRF"]]}, {"text": "On the other hand , what the BiLSTM system seems to be worse at is discriminating between different cases for prepositions , which heavily depends on the following adjective or noun .", "entities": [[7, 8, "MethodName", "BiLSTM"]]}, {"text": "While confusing an accusative preposition ( Sa ) for a locative one ( Sl ) the BiLSTM system did 23 times , this happened to the CRF system 17 times .", "entities": [[16, 17, "MethodName", "BiLSTM"], [26, 27, "MethodName", "CRF"]]}, {"text": "In the opposite direction , the BiLSTM system did 19 mistakes while the CRF system did one mistake less , namely 18 of them .", "entities": [[6, 7, "MethodName", "BiLSTM"], [13, 14, "MethodName", "CRF"]]}, {"text": "While it is clear why CRFs excel at predicting prepositional cases correctly as this dependence is in the scope of the local features , it seems that the BiLSTMs trade more mistakes in the local context for less mistakes in a wider one .", "entities": []}, {"text": "5 Conclusion In this paper we have compared two popular sequence labeling techniques : conditional random \ufb01elds ( CRFs ) and bidirectional long short - term memories ( BiLSTMs ) on the task of morphosyntactic annotation of tweets written in three closely related South Slavic languages : Slovene , Croatian and Serbian .", "entities": []}, {"text": "We have shown that CRFs with well de\ufb01ned features come very close to the performance of the stronger BiLSTM models , the difference between those two being bigger as the data are more nonstandard .", "entities": [[18, 19, "MethodName", "BiLSTM"]]}, {"text": "The relative error reduction between those two systems lies between 15 % for Slovene , for which the Twitter variety deviates the most from the standard , and 3 % for Serbian , for which the Twitter", "entities": []}, {"text": "162variety deviates the least .", "entities": []}, {"text": "For the CRF system , we have shown that using contextual , suf\ufb01xal and distributional features gives very good results .", "entities": [[2, 3, "MethodName", "CRF"]]}, {"text": "The latter make an in\ufb02ectional lexicon mostly obsolete , with just minor improvements in accuracy if features from large in\ufb02ectional lexicons are added .", "entities": [[14, 15, "MetricName", "accuracy"]]}, {"text": "For the BiLSTM system , we have shown that encoding a character - level representation of a word is the single most useful intervention , with minor improvements obtained through proper word embedding pretraining , \ufb01ne - tuning on in - domain data and pretraining the character - level encoder on pairs of words and MSD tags from a large automatically tagged web corpus .", "entities": [[2, 3, "MethodName", "BiLSTM"], [55, 56, "DatasetName", "MSD"]]}, {"text": "With an error analysis we have shown that the types of error performed by each of the systems are actually very similar , most of them still being typical tagger errors for languages with a rich in\ufb02ectional morphology .", "entities": []}, {"text": "However , there is evidence that BiLSTMs resolve long - range dependencies much better , such as discriminating between masculinum nouns in nominative and accusative singular , but yielding slightly more mistakes in the close - range dependencies such as the case of prepositions .", "entities": []}, {"text": "Acknowledgements The work presented in this paper has been funded by the Slovenian Research Agency national basic research project \u201c Resources , methods and tools for the understanding , identi\ufb01cation and classi\ufb01cation of various forms of socially unacceptable discourse in the information society \u201d ( ARRS J7 - 8280 , 20172020 ) , and by the Slovenian research infrastructure CLARIN.SI .", "entities": []}, {"text": "References Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Enriching word vectors with subword information .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 5:135\u2013146 .", "entities": []}, {"text": "Peter F. Brown , Peter V .", "entities": []}, {"text": "Desouza , Robert L. Mercer , Vincent J. Della Pietra , and Jenifer C. Lai .", "entities": []}, {"text": "1992 .", "entities": []}, {"text": "Class - based n - gram models of natural language .", "entities": []}, {"text": "Computational linguistics , 18(4):467\u2013479 .", "entities": []}, {"text": "Kaja Dobrovoljc , Simon Krek , Peter Holozan , Toma \u02c7z Erjavec , and Miro Romih .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Morphological lexicon sloleks 1.2 .", "entities": []}, {"text": "Slovenian language resource repository CLARIN.SI .", "entities": []}, {"text": "Toma \u02c7z Erjavec , Nikola Ljube \u02c7si\u00b4c , and Nata \u02c7sa Logar .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "The slWaC Corpus of the Slovene Web . Informatica , 39(1):35\u201342 .", "entities": []}, {"text": "Toma \u02c7z Erjavec , Darja Fi \u02c7ser , Jaka \u02c7Cibej , \u02c7Spela Arhar Holdt , Nikola Ljube \u02c7si\u00b4c , and Katja Zupan . 2017 .", "entities": []}, {"text": "CMC training corpus Janes - Tag 2.0 .", "entities": []}, {"text": "Slovenian language resource repository CLARIN.SI .", "entities": []}, {"text": "P\u00b4eter Hal \u00b4 acsy , Andr \u00b4 as Kornai , and Csaba Oravecz .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "HunPos :", "entities": []}, {"text": "An open source trigram tagger .", "entities": []}, {"text": "In Proceedings of the 45th Annual Meeting of the ACL , pages 209\u2013212 , Stroudsburg . Association for Computational Linguistics .", "entities": []}, {"text": "Tobias Horsmann and Torsten Zesch . 2015 .", "entities": []}, {"text": "Effectiveness of Domain Adaptation Approaches for Social Media PoS Tagging .", "entities": [[2, 4, "TaskName", "Domain Adaptation"]]}, {"text": "CLiC it , page 166 .", "entities": [[0, 1, "DatasetName", "CLiC"]]}, {"text": "John D. Lafferty , Andrew McCallum , and Fernando C. N. Pereira . 2001 .", "entities": []}, {"text": "Conditional random \ufb01elds : Probabilistic models for segmenting and labeling sequence data .", "entities": []}, {"text": "In Proceedings of the Eighteenth International Conference on Machine Learning , ICML \u2019 01 , pages 282\u2013289 , San Francisco , CA , USA .", "entities": []}, {"text": "Morgan Kaufmann Publishers Inc.", "entities": []}, {"text": "Nikola Ljube \u02c7si\u00b4c and Filip Klubi \u02c7cka .", "entities": []}, {"text": "2014.fbs , hr , srgwac - web corpora of bosnian , croatian and serbian .", "entities": []}, {"text": "In Proceedings of the 9th Web as Corpus Workshop ( WaC-9 ) , pages 29\u201335 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nikola Ljube \u02c7si\u00b4c , Filip Klubi \u02c7cka , and Damir Boras .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "In\ufb02ectional lexicon hrLex 1.2 .", "entities": []}, {"text": "Slovenian language resource repository CLARIN.SI .", "entities": []}, {"text": "Nikola Ljube \u02c7si\u00b4c , Filip Klubi \u02c7cka , and Damir Boras .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "In\ufb02ectional lexicon srLex 1.2 .", "entities": []}, {"text": "Slovenian language resource repository CLARIN.SI .", "entities": []}, {"text": "Nikola Ljube \u02c7si\u00b4c , Toma \u02c7z Erjavec , and Darja Fi \u02c7ser .", "entities": []}, {"text": "2017a .", "entities": []}, {"text": "Adapting a state - of - the - art tagger for south slavic languages to non - standard text .", "entities": []}, {"text": "In Proceedings of the 6th Workshop on Balto - Slavic Natural Language Processing , pages 60\u201368 .", "entities": []}, {"text": "163Nikola Ljube \u02c7si\u00b4c , Toma \u02c7z Erjavec , Maja Mili \u02c7cevi\u00b4c , and Tanja Samard \u02c7zi\u00b4c .", "entities": []}, {"text": "2017b .", "entities": []}, {"text": "Croatian Twitter training corpus ReLDI - NormTagNER - hr 2.0 .", "entities": []}, {"text": "Slovenian language resource repository CLARIN.SI .", "entities": []}, {"text": "Nikola Ljube \u02c7si\u00b4c , Toma \u02c7z Erjavec , Maja Mili \u02c7cevi\u00b4c , and Tanja Samard \u02c7zi\u00b4c .", "entities": []}, {"text": "2017c .", "entities": []}, {"text": "Serbian Twitter training corpus ReLDI - NormTagNER - sr 2.0 .", "entities": []}, {"text": "Slovenian language resource repository CLARIN.SI .", "entities": []}, {"text": "Nikola Ljube \u02c7si\u00b4c and Toma \u02c7z Erjavec .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Corpus vs. Lexicon Supervision in Morphosyntactic Tagging : the Case of Slovene .", "entities": []}, {"text": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC 2016 ) , Paris , France .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Nikola Ljube \u02c7si\u00b4c , Filip Klubi \u02c7cka , \u02c7Zeljko Agi \u00b4 c , and Ivo - Pavao Jazbec .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "New In\ufb02ectional Lexicons and Training Corpora for Improved Morphosyntactic Annotation of Croatian and Serbian .", "entities": []}, {"text": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC 2016 ) , Paris , France .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Quinn McNemar . 1947 .", "entities": []}, {"text": "Note on the sampling error of the difference between correlated proportions or percentages .", "entities": []}, {"text": "Psychometrika , 12(2):153\u2013157 .", "entities": []}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "CoRR , abs/1301.3781 .", "entities": []}, {"text": "Maja Mili \u02c7cevi\u00b4c and Nikola Ljube \u02c7si\u00b4c .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Tviterasi , tvitera \u02c7si or twittera \u02c7si ?", "entities": []}, {"text": "Producing and analysing a normalised dataset of Croatian and Serbian tweets .", "entities": []}, {"text": "Sloven \u02c7s\u02c7cina 2.0 : empirical , applied and interdisciplinary research , 4(2):156\u2013188 .", "entities": []}, {"text": "Maja Mili \u02c7cevi\u00b4c , Nikola Ljube \u02c7si\u00b4c , and Darja Fi \u02c7ser . 2017 .", "entities": []}, {"text": "Birds of a feather do n\u2019t quite tweet together : An analysis of spelling variation in Slovene , Croatian and Serbian Twitterese .", "entities": []}, {"text": "In Darja Fi \u02c7ser and Michael Beisswenger , editors , Investigating Computer - mediated Communication : Corpus - based Approaches to Language in the Digital World , pages 14\u201343 .", "entities": []}, {"text": "Ljubljana University Press , Faculty of Arts .", "entities": []}, {"text": "Naoaki Okazaki .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Crfsuite : a fast implementation of conditional random \ufb01elds ( crfs ) .", "entities": []}, {"text": "Nils Reimers and Iryna Gurevych . 2017 .", "entities": []}, {"text": "Optimal hyperparameters for deep lstm - networks for sequence labeling tasks .", "entities": [[4, 5, "MethodName", "lstm"]]}, {"text": "CoRR , abs/1707.06799 .", "entities": []}, {"text": "Marcos Zampieri , Shervin Malmasi , Preslav Nakov , Ahmed Ali , Suwon Shon , James Glass , Yves Scherrer , Tanja Samard \u02c7zi\u00b4c , Nikola Ljube \u02c7si\u00b4c , J\u00a8org Tiedemann , Chris van der Lee , Stefan Grondelaers , Nelleke Oostdijk , Antal van den Bosch , Ritesh Kumar , Bornini Lahiri , and Mayank Jain .", "entities": [[49, 50, "DatasetName", "Kumar"]]}, {"text": "2018 .", "entities": []}, {"text": "Language Identi\ufb01cation and Morphosyntactic Tagging : The Second VarDial Evaluation Campaign .", "entities": []}, {"text": "In Proceedings of the Fifth Workshop on NLP for Similar Languages , Varieties and Dialects ( VarDial ) , Santa Fe , USA .", "entities": []}]