[{"text": "Proceedings of the Sixth Conference on Machine Translation ( WMT ) , pages 130\u2013135 November 10\u201311 , 2021 .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "\u00a9 2021 Association for Computational Linguistics130Ensembling of Distilled Models from Multi - task Teachers for Constrained Resource Language Pairs Amr Hendy1 , Esraa A. Gad1 , Mohamed Abdelghaffar1 , Jailan S. ElMosalami1 , Mohamed A\ufb01fy1 , Ahmed Y. Taw\ufb01k1and Hany Hassan Awadalla2 1Microsoft Egypt Development Center , Cairo , Egypt 2Microsoft Corporation , Redmond , WA , USA { amrhendy,v-egad,mohamed.abdelghaar,v-jailanel}@microsoft.com { mafify,atawfik,hanyh}@microsoft.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "This paper describes our submission to the constrained track of WMT21 shared news translation task .", "entities": []}, {"text": "We focus on the three relatively low resource language pairs Bengali $ Hindi , English $ Hausa and Xhosa $ Zulu .", "entities": []}, {"text": "To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective employing both parallel and monolingual data .", "entities": []}, {"text": "In addition , we augment the data using back translation .", "entities": []}, {"text": "We also train a bilingual model incorporating back translation and knowledge distillation then combine the two models using sequence - to - sequence mapping .", "entities": [[10, 12, "MethodName", "knowledge distillation"]]}, {"text": "We see around 70 % relative gain in BLEU point for En$Haand around 25 % relative improvements for Bn$HiandXh$Zucompared to bilingual baselines .", "entities": [[8, 9, "MetricName", "BLEU"]]}, {"text": "1 Introduction Neural machine translation ( NMT ) witnessed a lot of success in the past few years especially for high resource languages ( Vaswani et al . , 2017 ) .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "Improving the quality of low resource languages is still challenging .", "entities": []}, {"text": "Some of the popular techniques are adding high resource helper languages as in multilingual neural machine translation ( MNMT ) ( Dong et al . , 2015 ; Firat et al . , 2016 ;", "entities": [[15, 17, "TaskName", "machine translation"]]}, {"text": "Ha et al . , 2016 ; Johnson et al . , 2017 ; Arivazhagan et", "entities": []}, {"text": "al . , 2019 ) , using monolingual data including pre - training ( Liu et al . , 2020 ) , multi - task learning ( Wang et al . , 2020 ) , back translation ( Sennrich et al . , 2016 ) or any combination of these methods ( Barrault et al . , 2020 ) and system combination of multiple systems ( Liu et al . , 2018 ) .", "entities": [[22, 26, "TaskName", "multi - task learning"]]}, {"text": "This paper describes the Microsoft Egypt Development Center ( EgDC ) submission to the WMT21 shared news translation task for three low resource language pairs ( six directions ) , Bengali $ Hindi ( Bn$Hi ) , English $ Hausa ( En$Ha ) and Xhosa $ Zulu ( Xh$Zu ) .", "entities": []}, {"text": "We focus on the constrained track because it is easier to comparedifferent systems and it is always possible to improve performance by adding more data .", "entities": []}, {"text": "The main features of our approach are as follows : \u2022Using a recently proposed multitask and multilingual learning framework to bene\ufb01t from monolingual data in both the source and target languages ( Wang et al . , 2020 ) .", "entities": []}, {"text": "\u2022Using knowledge distillation ( Freitag et al . , 2017 ) to create bilingual baselines from the original multilingual model and combining it with the multilingual model .", "entities": [[1, 3, "MethodName", "knowledge distillation"]]}, {"text": "The paper is organized as follows .", "entities": []}, {"text": "Section 2 gives an overview of the data used in the constrained scenario , followed by section 3 that gives a detailed description of our approach .", "entities": []}, {"text": "Section 4 presents our experimental evaluation .", "entities": []}, {"text": "Finally , our \ufb01ndings are summarized in Section 5 .", "entities": []}, {"text": "2 Data Following the constrained track , we use bitext data provided in WMT21 for the following pairs : Bengali$Hindi , English $ Hausa , Xhosa $ Zulu and English $ German .", "entities": []}, {"text": "Statistics of the parallel data used for the three pairs in addition to the German helper are shown in Table 1 .", "entities": []}, {"text": "We also use monolingual data for all previously mentioned languages provided in WMT21 for techniques such as multi - task training and back - translation .", "entities": []}, {"text": "Statistics of the monolingual data used for the 6 languages in addition to the German helper are shown in Table 2 .", "entities": []}, {"text": "For very low resource languages , Hausa , Xhosa and Zulu , we use all the available monolingual data , e.g. NewsCrawl + CommonCrawl + Extended CommonCrawl for Hausa , and Extended CommonCrawl for both Xhosa and Zulu .", "entities": []}, {"text": "For relatively high resource languages , Bengali , Hindi , English and German , we only use a subset of the provided data mostly from NewsCrawl due to its high - quality .", "entities": []}, {"text": "In addition to the NewsCrawl monolingual subset , we add a sampled subset from CommonCrawl to", "entities": []}, {"text": "131Language pair # of sentences Bengali $ Hindi 3.36 M English $ Hausa 750 K Xhosa$Zulu 94 K English $ German 84.8 M Table 1 : Bitext data used for bilingual and multilingual systems .", "entities": []}, {"text": "For each language pair , we use all available sources released in WMT21", "entities": []}, {"text": "Language # of sentences Raw Cleaned Bengali 53.8 M 53.3 M English 75 M 73.5 M German 111.2 M 109.9 M Hausa 10.8 M 6.2 M Hindi 60.2 M 59.8 M Xhosa 1.6 M 950 K Zulu 2 M 1.4 M Table 2 : Monolingual data used for multi - task training and back - translation avoid biasing into the news domain especially for Bengali $ Hindi and Xhosa $ Zulu whose target evaluation domain come from Wikipedia content .", "entities": []}, {"text": "2.1 Data Filtering For Bengali , English , Hindi and German , we apply fastText1language identi\ufb01cation on the monolingual data to remove sentences which are not predicted as the expected language .", "entities": []}, {"text": "We do the same for Hausa , Xhosa and Zulu using Polyglot2because fastText does not cover these three languages .", "entities": [[12, 13, "MethodName", "fastText"]]}, {"text": "The resulting size of the monolingual data of each language is shown in Table 2 . 3 System Architecture", "entities": []}, {"text": "The \ufb01nal MT system in each direction is an ensemble of two NMT models comprising a bilingual model ( one for each of the six primary directions ) and a multilingual model trained to provide translations for 8 directions ( the six primary directions plus English $ German ) .", "entities": []}, {"text": "The multilingual system uses a recently proposed multitask framework for training ( Wang et al . , 2020 ) .", "entities": []}, {"text": "We describe the individual systems in Subsection 3.1 .", "entities": []}, {"text": "This is followed by presenting our system combination techniques in Subsection 3.2 .", "entities": []}, {"text": "Finally we present the architecture of the submitted system highlighting our 1https://fasttext.cc/docs/en/language-identi\ufb01cation.html 2https://github.com/aboSamoor/polyglotdesign decisions in Subsection 3.3 .", "entities": []}, {"text": "3.1 Individual Systems This subsection describes the individual systems and their training leading to the proposed system combination strategy in the following subsection .", "entities": []}, {"text": "We \ufb01rst build bilingual models for the six primary directions using the data shown in Table 1 except the English $ German .", "entities": []}, {"text": "These serve as baselines to compare to the developed systems .", "entities": []}, {"text": "The models use a transformer base architecture comprising 6 encoder and 6 decoder layers and a 24 K joint vocabulary built for Bengali $ Hindi , a 8 K joint vocabulary built for English $ Hausa and a 4 K joint vocabulary built for Xhosa $ Zulu using sentencepiece ( Kudo and Richardson , 2018 ) to learn these subword units to tokenize the sentences .", "entities": [[48, 49, "MethodName", "sentencepiece"]]}, {"text": "In addition to the baseline bilingual models , we use knowledge distilled ( KD ) data and back - translated ( BT ) data generated from a multilingual model to build another set of bilingual models for each of the six primary directions .", "entities": []}, {"text": "This multilingual model is described below .", "entities": []}, {"text": "The purpose of these models is to participate in the ensemble along with the multilingual models .", "entities": []}, {"text": "The latter bilingual models follow the same transformer base architecture and joint vocabulary used in the baseline bilingual models .", "entities": []}, {"text": "The multilingual model combines the 8 translation directions shown in Table 1 .", "entities": []}, {"text": "These are the six primary directions plus English $ German as a helper .", "entities": []}, {"text": "The latter is mainly used to improve generation on the English centric directions .", "entities": []}, {"text": "The model uses a 64 K joint vocabulary constructed using sentencepiece ( Kudo and Richardson , 2018 ) from a subset of the monolingual data of each language as described in Section 2 .", "entities": [[10, 11, "MethodName", "sentencepiece"]]}, {"text": "The transformer model has 12 encoder and 6 decoder layers .", "entities": []}, {"text": "In addition , a multitask objective is used during training to make use of monolingual data .", "entities": []}, {"text": "The objective comprises the usual parallel data likelihood referred to as MT , a masked language model ( MLM ) at the encoder and a denoising auto - encoder ( DAE ) ( similar to mBART ( Liu et al . , 2020 ) ) at the decoder side .", "entities": [[18, 19, "DatasetName", "MLM"], [25, 26, "TaskName", "denoising"], [35, 36, "MethodName", "mBART"]]}, {"text": "The latter two objectives help leverage monolingual data for both the encoder and the decoder sides .", "entities": []}, {"text": "The three objectives are combined using different proportions according to a schedule during the training .", "entities": []}, {"text": "Please refer to ( Wang et al . , 2020 ) for details .", "entities": []}, {"text": "132To summarize we build the following models : \u2022Bilingual models trained using parallel data in Table 1 for the 6 primary directions .", "entities": []}, {"text": "These are mainly used as baselines .", "entities": []}, {"text": "\u2022Multilingual models trained using a multitask objective using parallel and monolingual data and comprising 8 directions .", "entities": []}, {"text": "\u2022Bilingual models trained using KD and BT data generated using our best multilingual model .", "entities": []}, {"text": "These are combined with the best multilingual model as described in 3.2 .", "entities": []}, {"text": "3.2 System Combination System combination or ensembling is known to improve the performance over individual systems .", "entities": []}, {"text": "There are many ways to create an ensemble ( Liu et al . , 2018 ; Dabre et al . , 2019 ) .", "entities": []}, {"text": "For example , individual models obtained from different checkpoints during the same training or by training models sharing the same vocab and architecture using different data or simply different random seeds can be combined using model averaging techniques .", "entities": [[30, 31, "DatasetName", "seeds"]]}, {"text": "Here , we opt to combine different models since it generally leads to better performance because different models tend to be more complementary .", "entities": []}, {"text": "To this end , we propose a simple and effective method to combine completely different architectures .", "entities": []}, {"text": "The proposed method could be also used in conjunction with checkpoint and model averaging for further gains , but we have n\u2019t tried this in our experiments due to time limitations .", "entities": []}, {"text": "The basic idea of our combination is very simple .", "entities": []}, {"text": "Assume we have the translation pair x!ywhere", "entities": []}, {"text": "yis the reference translation .", "entities": []}, {"text": "The output of model 1is the pair x!y1and the output of model 2is", "entities": []}, {"text": "the pair x!y2 .", "entities": []}, {"text": "This can be generalized to multiple systems but we limited our combination to only two models .", "entities": []}, {"text": "We train a new model that takes the set of hypotheses ( possibly augmented by the source sentence ) from the two models to generate the target sentence .", "entities": []}, {"text": "Thus this model combines the outputs of two models in the ensemble to produce a translation closer to the original target sentence i.e. < HY P > y", "entities": []}, {"text": "1 < HY P >", "entities": []}, {"text": "y 2!y .", "entities": []}, {"text": "We also experimented with adding the source to the input i.e. < SRC", "entities": []}, {"text": "> x < HY P >", "entities": []}, {"text": "y 1 < HY P > y 2!y which led to around 0.3 BLEU improvement for Ha!En , but we have n\u2019t tried on other pairs due to time limitation .", "entities": [[13, 14, "MetricName", "BLEU"]]}, {"text": "All combination models use 6layers encoder and decoder and a 64 K vocabulary similar to the multilingual system .", "entities": []}, {"text": "These combination models use the full bitext and dev data provided in WMT21 as shown in Table 1 .", "entities": []}, {"text": "The system combination is outlined in Figure 1 .", "entities": []}, {"text": "This ensembling technique can be thought of as providing both system combination and post - editing capabilities .", "entities": []}, {"text": "3.3 Overall System Our overall system is depicted in Figure 2 .", "entities": []}, {"text": "The \ufb01rst module shows the data input where language identi\ufb01cation ( LID ) is used to \ufb01lter the monolingual data .", "entities": []}, {"text": "As mentioned in Section 2.1 we use fastText and polyglot for LID depending on the language .", "entities": [[7, 8, "MethodName", "fastText"]]}, {"text": "We \ufb01rst build bilingual baselines which are not shown in the \ufb01gure .", "entities": []}, {"text": "Then as shown in the second module , we build 4 multilingual systems using different task objectives as follows : MT ; MT + MLM ; MT + DAE and MT + MLM + DAE trained on the 8 directions shown in Table 1 following the temperature - based strategy in ( Arivazhagan et al . , 2019 ) to balance the training data in different resource languages using T = 5 .", "entities": [[24, 25, "DatasetName", "MLM"], [32, 33, "DatasetName", "MLM"]]}, {"text": "We pick the best system and use it to back translate the selected monolingual data .", "entities": []}, {"text": "For most pairs , as detailed in Section 4 , we \ufb01nd that MT + DAE andMT + MLM", "entities": [[18, 19, "DatasetName", "MLM"]]}, {"text": "+ DAE are quite close .", "entities": []}, {"text": "Therefore , we use the MT + DAE to do back translation for all submitted 6 pairs .", "entities": []}, {"text": "We use beam search with beam size = 5 when generating the synthetic back - translated data .", "entities": []}, {"text": "Once we get the back - translated data ( called BT1 ) we add it to our parallel and monolingual data and build a new multilingual model called MT + DAE + BT1 .", "entities": []}, {"text": "We tag the back - translated data with < BT > tag at beginning of each source sentence", "entities": []}, {"text": "so the model can differentiate between the genuine parallel and backtranslated data quality .", "entities": []}, {"text": "The resulting model is used to regenerate the back - translated data ( called BT2 ) and to knowledge distill the bitext ( called KD ) .", "entities": []}, {"text": "The latter two data sets are augmented and used to build a bilingual system ( called MT + KD + BT2 ) .", "entities": []}, {"text": "We upsample the KD data set and the upsampling ratio is selected based on parameter sweeping and validating the resulting improvement on the validation set .", "entities": []}, {"text": "Finally , the latter bilingual model is combined with our \ufb01nal multilingual model using the method in Section 3.2 to create our submission .", "entities": []}, {"text": "4 Experimental Results In this section , we describe the results of our intermediate and \ufb01nal systems .", "entities": []}, {"text": "We report Sacre-", "entities": []}, {"text": "133 Figure 1 : The system combination component used for our experiments .", "entities": []}, {"text": "BLEU ( Post , 2018 ) on the validation set released in WMT21 , and both SacreBLEU and COMET ( Rei et al . , 2020 ) using the available implementation3on the of\ufb01cial test set released in WMT21 .", "entities": [[0, 1, "MetricName", "BLEU"], [16, 17, "MetricName", "SacreBLEU"]]}, {"text": "The results for the six submitted language pairs are in Tables 3 - 5 .", "entities": []}, {"text": "The \ufb01rst row in each table shows the bilingual baseline which performs relatively poor due to the limited amount of parallel data for each pair .", "entities": []}, {"text": "This is followed by the four multilingual systems with different objectives .", "entities": []}, {"text": "It is clear that adding a monolingual objective brings nice improvements for all language pairs .", "entities": []}, {"text": "The MT + DAE andMT + MLM + DAE perform closely for all language pairs indicating that target monolingual data is most important .", "entities": [[6, 7, "DatasetName", "MLM"]]}, {"text": "The next two rows show the results of adding back - translated data to the multilingual model and a bilingual baseline using back - translated and knowledge distilled data generated from the best multilingual model .", "entities": []}, {"text": "As expected adding back translation brings signi\ufb01cant improvement to all language pairs .", "entities": []}, {"text": "Also using the multilingual model to create data for a bilingual model shows excellent results that outperform the multilingual model .", "entities": []}, {"text": "Finally , the ensemble , as expected , performs better than the individual models .", "entities": []}, {"text": "The signi\ufb01cant difference between reported improvements in Ha$Enand other directions shows the effectiveness of adding De$Enparallel and monolingual data that helps English centric directions more than other directions .", "entities": []}, {"text": "We evaluated the \ufb01nal submitted systems on the of\ufb01cial test set released in WMT21 as shown in Table 6 . 3https://github.com/Unbabel/COMETSystem Ha - En En - Ha bilingual baseline 14.10 13.78 multi .", "entities": []}, {"text": "MT 14.32 13.16 + MLM 16.18 13.94 + DAE 18.05 14.91 + MLM + DAE 17.35 15.03 multi .", "entities": [[4, 5, "DatasetName", "MLM"], [12, 13, "DatasetName", "MLM"]]}, {"text": "MT + DAE + BT 1 21.11 20.24 bilingual MT + KD + BT 224.43 20.68 ensemble 24.90 21.00 Table 3 : Results of Ha - En and En - Ha systems .", "entities": []}, {"text": "We report SacreBLEU scores on the validation set provided in", "entities": [[2, 3, "MetricName", "SacreBLEU"]]}, {"text": "WMT21 System Bn - Hi Hi - Bn bilingual baseline 18.60 10.90 multi .", "entities": []}, {"text": "MT 18.21 10.02 + MLM 18.82 10.67 + DAE 18.64 10.40 + MLM + DAE 19.20 11.27 multi .", "entities": [[4, 5, "DatasetName", "MLM"], [12, 13, "DatasetName", "MLM"]]}, {"text": "MT + DAE + BT 1 20.18 12.29 bilingual MT + KD + BT 221.03 12.90 ensemble 21.20 13.30 Table 4 : Results of Bn - Hi and Hi - Bn systems .", "entities": []}, {"text": "We report SacreBLEU scores on the validation set provided in WMT21 5 Summary", "entities": [[2, 3, "MetricName", "SacreBLEU"]]}, {"text": "This paper describes our submission to the constrained track of WMT21 .", "entities": []}, {"text": "We focus on the three relatively low resource language pairs Bn$Hi , En$HaandXh$Zu .", "entities": []}, {"text": "To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective recently proposed in ( Wang et al . , 2020 ) .", "entities": []}, {"text": "In addition ,", "entities": []}, {"text": "134 Figure 2 : The overall system \ufb02ow used for our experiments System Xh - Zu Zu - Xh bilingual baseline 8.00 7.60 multi .", "entities": []}, {"text": "MT 7.53 7.47 + MLM 7.23 7.02 + DAE 8.53 8.24 + MLM + DAE 8.20 7.80 multi .", "entities": [[4, 5, "DatasetName", "MLM"], [12, 13, "DatasetName", "MLM"]]}, {"text": "MT + DAE + BT 1 9.06 8.86 bilingual MT + KD + BT 2 9.80 9.17 ensemble 10.00 9.30 Table 5 : Results of Xh - Zu and Zu - Xh systems .", "entities": []}, {"text": "We report SacreBLEU scores on the validation set provided in WMT21 Translation direction BLEU COMET Ha!En 17.13 0.149 En!Ha 16.13 0.086 Bn!Hi", "entities": [[2, 3, "MetricName", "SacreBLEU"], [11, 12, "TaskName", "Translation"], [13, 14, "MetricName", "BLEU"]]}, {"text": "21.08", "entities": []}, {"text": "0.532 Hi!Bn 10.93 0.411 Xh!Zu 9.94 0.180 Zu!Xh 9.25 0.299 Table 6 : Results of the submitted systems .", "entities": []}, {"text": "We report SacreBLEU and COMET scores on the of\ufb01cial test set provided in WMT21 .", "entities": [[2, 3, "MetricName", "SacreBLEU"]]}, {"text": "For COMET , we use the recommended model \u201c wmt20 - comet - da \u201d .", "entities": []}, {"text": "we augment the data using back translation .", "entities": []}, {"text": "We also use the resulting multilingual model to create abilingual model incorporating back translation and knowledge distillation .", "entities": [[15, 17, "MethodName", "knowledge distillation"]]}, {"text": "Finally , we combine the two models , using a \ufb02exible sequence - to - sequence approach , to yield our submitted systems .", "entities": []}, {"text": "We see large gains up to 8 - 10 BLEU points for En$Ha and nice improvements of up to 2 - 3 BLEU points forBn$HiandXh$Zu .", "entities": [[9, 10, "MetricName", "BLEU"], [22, 23, "MetricName", "BLEU"]]}, {"text": "References Naveen Arivazhagan , Ankur Bapna , Orhan Firat , Dmitry Lepikhin , Melvin Johnson , Maxim Krikun , Mia Xu Chen , Yuan Cao , George Foster , Colin Cherry , Wolfgang Macherey , Zhifeng Chen , and Yonghui Wu . 2019 .", "entities": []}, {"text": "Massively multilingual neural machine translation in the wild : Findings and challenges .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "Lo\u00efc Barrault , Magdalena Biesialska , Ond \u02c7rej Bojar , Marta R. Costa - juss\u00e0 , Christian Federmann , Yvette Graham , Roman Grundkiewicz , Barry Haddow , Matthias Huck , Eric Joanis , Tom Kocmi , Philipp Koehn , Chi - kiu Lo , Nikola Ljube\u0161i \u00b4 c , Christof Monz , Makoto Morishita , Masaaki Nagata , Toshiaki Nakazawa , Santanu Pal , Matt Post , and Marcos Zampieri .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Findings of the 2020 conference on machine translation ( WMT20 ) .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Fifth Conference on Machine Translation , pages 1\u201355 , Online .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Raj Dabre , Fabien Cromieres , and Sadao Kurohashi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Enabling multi - source neural machine trans-", "entities": []}, {"text": "135lation by concatenating source sentences in multiple languages .", "entities": []}, {"text": "Daxiang Dong , Hua Wu , W. He , Dianhai Yu , and Haifeng Wang . 2015 .", "entities": []}, {"text": "Multi - task learning for multiple language translation .", "entities": [[0, 4, "TaskName", "Multi - task learning"]]}, {"text": "In ACL .", "entities": []}, {"text": "Orhan Firat , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Multi - way , multilingual neural machine translation with a shared attention mechanism .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 866\u2013875 , San Diego , California . Association for Computational Linguistics .", "entities": []}, {"text": "Markus Freitag , Yaser Al - Onaizan , and Baskaran Sankaran .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Ensemble distillation for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "Thanh - Le Ha , Jan Niehues , and Alexander Waibel .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Toward multilingual neural machine translation with universal encoder and decoder .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "Melvin Johnson , Mike Schuster , Quoc V .", "entities": []}, {"text": "Le , Maxim Krikun , Yonghui Wu , Zhifeng Chen , Nikhil Thorat , Fernanda Vi\u00e9gas , Martin Wattenberg , Greg Corrado , Macduff Hughes , and Jeffrey Dean . 2017 .", "entities": []}, {"text": "Google \u2019s multilingual neural machine translation system : Enabling zero - shot translation .", "entities": [[0, 1, "DatasetName", "Google"], [4, 6, "TaskName", "machine translation"]]}, {"text": "Taku Kudo and John Richardson .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Sentencepiece :", "entities": [[0, 1, "MethodName", "Sentencepiece"]]}, {"text": "A simple and language independent subword tokenizer and detokenizer for neural text processing .", "entities": []}, {"text": "Yinhan Liu , Jiatao Gu , Naman Goyal , Xian Li , Sergey Edunov , Marjan Ghazvininejad , Mike Lewis , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multilingual denoising pre - training for neural machine translation", "entities": [[1, 2, "TaskName", "denoising"], [7, 9, "TaskName", "machine translation"]]}, {"text": ".", "entities": []}, {"text": "Yuchen Liu , Long Zhou , Yining Wang , Yang Zhao , Jiajun Zhang , and Chengqing Zong .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A comparable study on model averaging , ensembling and reranking in nmt .", "entities": []}, {"text": "In Natural Language Processing and Chinese Computing , pages 299\u2013308 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Matt Post . 2018 .", "entities": []}, {"text": "A call for clarity in reporting bleu scores .", "entities": [[6, 7, "MetricName", "bleu"]]}, {"text": "Ricardo Rei , Craig Stewart , Ana C Farinha , and Alon Lavie .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "COMET :", "entities": []}, {"text": "A neural framework for MT evaluation .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 2685\u20132702 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Improving neural machine translation models with monolingual data .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 86\u201396 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "Yiren Wang , ChengXiang Zhai , and Hany Hassan . 2020 .", "entities": []}, {"text": "Multi - task learning for multilingual neural machine translation .", "entities": [[0, 4, "TaskName", "Multi - task learning"], [7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1022\u20131034 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}]