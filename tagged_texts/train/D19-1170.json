[{"text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 1596\u20131606 , Hong Kong , China , November 3\u20137 , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics1596A", "entities": []}, {"text": "Multi - Type Multi - Span Network forReading Comprehension that Requires Discrete ReasoningMinghao Hu , Yuxing Peng , Zhen Huang , Dongsheng LiNational University of Defense Technology , Changsha , China{huminghao09,pengyuxing , huangzhen , dsli}@nudt.edu.cnAbstractRapid progress has been made in the \ufb01eld ofreading comprehension and question answer - ing , where several systems have achieved hu - man parity in some simpli\ufb01ed settings .", "entities": []}, {"text": "How - ever , the performance of these models de - grades signi\ufb01cantly when they are appliedto more realistic scenarios , where answersare involved with various types , multiple textstrings are correct answers , or discrete rea - soning abilities are required .", "entities": []}, {"text": "In this paper , we introduce the Multi - Type Multi - Span Net - work ( MTMSN ) , a neural reading comprehen - sion model that combines a multi - type answerpredictor designed to support various answertypes ( e.g. , span , count , negation , and arith - metic expression ) with a multi - span extractionmethod for dynamically producing one or mul - tiple text spans .", "entities": []}, {"text": "In addition , an arithmeticexpression reranking mechanism is proposedto rank expression candidates for further con-\ufb01rming the prediction .", "entities": []}, {"text": "Experiments show thatour model achieves 79.9 F1 on the DROP hid - den test set , creating new state - of - the - art re - sults .", "entities": [[6, 7, "MetricName", "F1"], [9, 10, "DatasetName", "DROP"]]}, {"text": "Source code1is released to facilitate fu - ture work.1 IntroductionThis paper considers the reading comprehensiontask in which somediscrete - reasoningabilities areneeded to correctly answer questions .", "entities": []}, {"text": "Speci\ufb01-cally , we focus on a new reading comprehensiondataset called DROP ( Dua et al . ,2019 ) , whichrequires Discrete Reasoning Over the content ofParagraphs to obtain the \ufb01nal answer .", "entities": [[10, 11, "DatasetName", "DROP"]]}, {"text": "Unlike pre - vious benchmarks such as CNN / DM ( Hermannet al . ,2015 ) and SQuAD ( Rajpurkar et al . ,2016)that have been well solved ( Chen et al . ,2016;Devlin et al . ,2019 ) , DROP is substantially morechallenging in three ways .", "entities": [[18, 19, "DatasetName", "SQuAD"], [44, 45, "DatasetName", "DROP"]]}, {"text": "First , the answers to1https://github.com/huminghao16/MTMSNthe questions involve a wide range of types suchas numbers , dates , or text strings .", "entities": []}, {"text": "Therefore , var - ious kinds of prediction strategies are required tosuccessfully \ufb01nd the answers .", "entities": []}, {"text": "Second , rather thanrestricting the answer to be a span of text , DROPloosens the constraint so that answers may be a setof multiple text strings .", "entities": []}, {"text": "Third , for questions thatrequire discrete reasoning , a system must have amore comprehensive understanding of the contextand be able to perform numerical operations suchas addition , counting , or sorting .", "entities": []}, {"text": "Existing approaches , when applied to this morerealistic scenario , have three problems .", "entities": []}, {"text": "First , to produce various answer types , Dua et al.(2019 ) extend previous one - type answer predic - tion ( Seo et al . ,2017 ) to multi - type predictionthat supports span extraction , counting , and ad - dition / subtraction .", "entities": []}, {"text": "However , they have not fullyconsidered all potential types .", "entities": []}, {"text": "Take the question\u201cWhat percent are not non - families ? \u201d and the pas - sage snippet \u201c 39.9 % were non - families \u201d as an ex - ample , anegationoperation is required to infer theanswer .", "entities": []}, {"text": "Second , previous reading comprehensionmodels ( Wang et al . ,2017;Yu et al . ,2018;Huet al . ,2018 ) are designed to produce one singlespan as the answer .", "entities": []}, {"text": "But for some questions such as\u201cWhich ancestral groups are smaller than 11%?\u201d,there may exist several spans as correct answers(e.g . , \u201c Italian \u201d , \u201c English \u201d , and \u201c Polish \u201d ) , whichcan not be well handled by these works .", "entities": []}, {"text": "Third , to support numerical reasoning , prior work ( Duaet al . ,2019 ) learns to predict signed numbers forobtaining an arithmetic expression that can be ex - ecuted by a symbolic system .", "entities": []}, {"text": "Nevertheless , theprediction of each signed number is isolated , andthe expression \u2019s context information has not beenconsidered .", "entities": []}, {"text": "As a result , obviously - wrong expres - sions , such as all predicted signs are either minusor zero , are likely produced .", "entities": []}, {"text": "To address the above issues , we introduce", "entities": []}, {"text": "1597the Multi - Type Multi - Span Network ( MTMSN),a neural reading comprehension model for pre - dicting various types of answers as well asdynamically extracting one or multiple spans .", "entities": [[11, 13, "TaskName", "reading comprehension"]]}, {"text": "MTMSN utilizes a series of pre - trained Trans - former blocks ( Devlin et al . ,2019 ) to obtain adeep bidirectional context representation .", "entities": []}, {"text": "On topof it , a multi - type answer predictor is proposedto not only support previous prediction strategiessuch as span , count number , and arithmetic ex - pression , but also add a new type of logical nega - tion .", "entities": []}, {"text": "This results in a wider range of coverageof answer types , which turns out to be crucial toperformance .", "entities": []}, {"text": "Besides , rather than always produc - ing one single span , we present a multi - span ex - traction method to produce multiple answers .", "entities": []}, {"text": "Themodel \ufb01rst predicts the number of answers , andthen extracts non - overlapped spans to the speci\ufb01camount .", "entities": []}, {"text": "In this way , the model can learn to dy - namically extract one or multiple spans , thus beingbene\ufb01cial for multi - answer cases .", "entities": []}, {"text": "In addition , wepropose an arithmetic expression reranking mech - anism to rank expression candidates that are de - coded by beam search , so that their context infor - mation can be considered during reranking to fur - ther con\ufb01rm the prediction .", "entities": []}, {"text": "Our MTMSN model outperforms all existingapproaches on the DROP hidden test set by achiev - ing 79.9 F1 score , a 32.9 % absolute gain over priorbest work at the time of submission .", "entities": [[8, 9, "DatasetName", "DROP"], [17, 19, "MetricName", "F1 score"]]}, {"text": "To make a faircomparison , we also construct a baseline that usesthe same BERT - based encoder .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "Again , MTMSNsurpasses it by obtaining a 13.2 F1 increase on thedevelopment set .", "entities": [[8, 9, "MetricName", "F1"]]}, {"text": "We also provide an in - depth ab - lation study to show the effectiveness of our pro - posed methods , analyze performance breakdownby different answer types , and give some qualita - tive examples as well as error analysis.2 Task DescriptionIn the reading comprehension task that requiresdiscrete reasoning , a passage and a question aregiven .", "entities": [[44, 46, "TaskName", "reading comprehension"]]}, {"text": "The goal is to predict an answer to the ques - tion by reading and understanding the passage .", "entities": []}, {"text": "Unlike previous dataset such as SQuAD ( Ra - jpurkar et al . ,2016 ) where the answer is limitedto be a single span of text , DROP loosens the con - straint so that the answer involves various typessuch as number , date , or span of text ( Figure1).Moreover , the answer can be multiple text stringsinstead of single continuous span ( A2 ) .", "entities": [[5, 6, "DatasetName", "SQuAD"], [28, 29, "DatasetName", "DROP"]]}, {"text": "To suc - Passage : As of the census of 2000 , there were 218,590people , 79,667 households , ... 22.5 % were of Germanpeople , 13.1 % Irish people , 9.8 % Italian people , ... Q1 : Which group from the census is larger : German orIrish?A1 : GermanQ2 : Which ancestral groups are at least 10%?A2 : German , IrishQ3 : How many more people are there than households?A3 : 138,923Q4 : How many percent were not German?A4 : 77.5Figure 1 : Question - answer pairs along with a passagefrom the DROP dataset.cessfully \ufb01nd the answer , some discrete reasoningabilities , such as sorting ( A1 ) , subtraction ( A3),and negation ( A4 ) , are required.3 Our ApproachFigure2gives an overview of our model that aimsto combine neural reading comprehension withnumerical reasoning .", "entities": [[94, 95, "DatasetName", "DROP"], [132, 134, "TaskName", "reading comprehension"]]}, {"text": "Our model uses BERT ( De - vlin et al . ,2019 ) as encoder : we map word em - beddings into contextualized representations usingpre - trained Transformer blocks ( Vaswani et al . ,2017)(\u00a73.1 ) .", "entities": [[3, 4, "MethodName", "BERT"], [29, 30, "MethodName", "Transformer"]]}, {"text": "Based on the representations , weemploy a multi - type answer predictor that is ableto produce four answer types : ( 1 ) span from thetext ; ( 2 ) arithmetic expression ; ( 3 ) count number;(4 ) negation on numbers ( \u00a7 3.2 ) .", "entities": []}, {"text": "FollowingDuaet al.(2019 ) , we \ufb01rst predict the answer type ofa given passage - question pair , and then adopt in - dividual prediction strategies .", "entities": []}, {"text": "To support multi - span extraction ( \u00a7 3.3 ) , the model explicitly pre - dicts the number of answer spans .", "entities": []}, {"text": "It then outputsnon - overlapped spans until the speci\ufb01c amount isreached .", "entities": []}, {"text": "Moreover , we do not directly use thearithmetic expression that possesses the maximumprobability , but instead re - rank several expressioncandidates that are decoded by beam search to fur - ther con\ufb01rm the prediction ( \u00a7 3.4 ) .", "entities": []}, {"text": "Finally , themodel is trained under weakly - supervised signalsto maximize the marginal likelihood over all pos - sible annotations ( \u00a7 3.5).3.1 BERT - Based EncoderTo obtain a universal representation for both thequestion and the passage , we utilize BERT ( De - vlin et al . ,2019 ) , a pre - trained deep bidirectionalTransformer model that achieves state - of - the - artperformance across various tasks , as the encoder .", "entities": [[23, 24, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"]]}, {"text": "Speci\ufb01cally , we \ufb01rst tokenize the question and", "entities": []}, {"text": "1598 Embeddings ( WordPiece , position , and segment)Transformer BlockSpanAnswer TypeAdd / SubCountNegation SpanSpan ... SpanSpant=2 ... Beam searchExp1Exp2Exp3NMSx Lu1 ... sign1psign2psign3psignpNu2u3uNarith1parith2parith3p CLSTok1ToknSEPTok1SEPTokm ...... startpendpspanpMulti - Span ExtractionArithmetic Expression RerankingMulti - Type Answer PredictorFigure 2 : An illustration of MTMSN architecture .", "entities": [[3, 4, "MethodName", "WordPiece"]]}, {"text": "The multi - type answer predictor supports four kinds of answertypes including span , addition / subtraction , count , and negation .", "entities": []}, {"text": "A multi - span extraction method is proposed todynamically produce one or several spans .", "entities": []}, {"text": "The arithmetic expression reranking mechanism aims to rank expressioncandidates that are decoded by beam search for further validating the prediction.the passage using the WordPiece vocabulary ( Wuet al . ,2016 ) , and then generate the input sequenceby concatenating a[CLS]token , the tokenizedquestion , a[SEP]token , the tokenized passage , and a \ufb01nal[SEP]token .", "entities": [[23, 24, "MethodName", "WordPiece"]]}, {"text": "For each token in thesequence , its input representation is the element - wise addition of WordPiece embeddings , posi - tional embeddings , and segment embeddings ( De - vlin et al . ,2019 ) .", "entities": [[16, 17, "MethodName", "WordPiece"]]}, {"text": "As a result , a list of input embed - dingsH02RT \u21e5 Dcan be obtained , whereDisthe hidden size andTis the sequence length .", "entities": []}, {"text": "A se - ries ofLpre - trained Transformer blocks are thenused to project the input embeddings into contex - tualized representationsHias : Hi= TransformerBlock(Hi\u00001),8i2[1,L]Here , we omit a detailed introduction of the blockarchitecture and refer readers toVaswani et al.(2017 ) for more details.3.2 Multi - Type Answer PredictorRather than restricting the answer to always be aspan of text , the discrete - reasoning reading com - prehension task involves different answer types(e.g . , number , date , span of text ) .", "entities": [[7, 8, "MethodName", "Transformer"]]}, {"text": "FollowingDuaet al.(2019 ) , we design a multi - type answer pre - dictor to selectively produce different kinds of an - swers such as span , count number , and arithmeticexpression .", "entities": []}, {"text": "To further increase answer coverage , we propose adding a new answer type to sup - port logical negation .", "entities": []}, {"text": "Moreover , unlike prior workthat separately predicts passage spans and ques - tion spans , our approach directly extracts spansfrom the input sequence .", "entities": []}, {"text": "Answer type predictionInspired by the Aug - mented QANet model ( Dua et al . ,2019 ) , we usethe contextualized token representations from thelast four blocks ( HL\u00003 , ... , HL ) as the inputs toour answer predictor , which are denoted asM0,M1,M2,M3 , respectively .", "entities": []}, {"text": "To predict the answertype , we \ufb01rst split the representationM2into aquestion representationQ2and a passage repre - sentationP2according to the index of intermedi - ate[SEP]token .", "entities": []}, {"text": "Then the model computes twovectorshQ2andhP2that summarize the questionand passage information respectively: \u21b5 Q= softmax(WQQ2),hQ2= \u21b5 QQ2wherehP2is computed in a similar way overP2.Next , we calculate a probability distribution torepresent the choices of different answer types as : ptype= softmax(FFN([hQ2;hP2;hCLS]))Here , hCLSis the \ufb01rst vector in the \ufb01nal con - textualized representationM3 , andFFNdenotesa feed - forward network consisting of two linearprojections with a GeLU activation ( Hendrycksand Gimpel,2016 ) followed by a layer normaliza - tion ( Lei Ba et al . ,2016 ) in between . SpanTo extract the answer either from the pas - sage or from the question , we combine the gatingmechanism ofWang et al.(2017 ) with the standarddecoding strategy ofSeo et al.(2017 ) to predictthe starting and ending positions across the en - tire sequence .", "entities": [[64, 65, "MethodName", "GeLU"]]}, {"text": "Speci\ufb01cally , we \ufb01rst compute threevectors , namelygQ0,gQ1,gQ2 , that summarize", "entities": []}, {"text": "1599the question information among different levels ofquestion representations:\u0000Q= softmax(FFN(Q2),gQ2=\u0000QQ2wheregQ0andgQ1are computed overQ0andQ1respectively , in a similar way as described above .", "entities": []}, {"text": "Then we compute the probabilities of the start - ing and ending indices of the answer span from theinput sequence as:\u00afMstart=[M2;M0;gQ2 \u2326 M2;gQ0 \u2326 M0],\u00afMend=[M2;M1;gQ2 \u2326 M2;gQ1 \u2326 M1],pstart= softmax(WS\u00afMstart),pend= softmax(WE\u00afMend)where \u2326 denotes the outer product between thevectorgand each token representation inM.Arithmetic expressionIn order to model theprocess of performing addition or subtractionamong multiple numbers mentioned in the pas - sage , we assign a three - way categorical variable(plus , minus , or zero ) for each number to indicateits sign , similar toDua et al.(2019 ) .", "entities": []}, {"text": "As a result , an arithmetic expression that has a number as the\ufb01nal answer can be obtained and easily evaluated .", "entities": []}, {"text": "Speci\ufb01cally , for each number mentioned in thepassage , we gather its corresponding representa - tion from the concatenation ofM2andM3 , even - tually yieldingU=(u1 , . . .", "entities": []}, {"text": ", uN)2RN \u21e5 2 \u21e4 DwhereNnumbers exist .", "entities": []}, {"text": "Then the probabilities ofthei - th number being assigned a plus , minus orzero is computed as : psigni= softmax(FFN([ui;hQ2;hP2;hCLS]))CountWe consider the ability of counting en - tities and model it as a multi - class classi\ufb01cationproblem .", "entities": []}, {"text": "To achieve this , the model \ufb01rst producesa vectorhUthat summarizes the important infor - mation among all mentioned numbers , and thencomputes a counting probability distribution as: \u21b5 U= softmax(WUU),hU= \u21b5 UU , pcount= softmax(FFN([hU;hQ2;hP2;hCLS]))NegationOne obvious but important linguisticphenomenon that prior work fails to capture isnegation .", "entities": []}, {"text": "We \ufb01nd there are many cases in DROPthat require to perform logical negation on num - bers .", "entities": []}, {"text": "The question ( Q4 ) in Figure1gives a qual - itative example of this phenomenon .", "entities": []}, {"text": "To modelthis phenomenon , we assign a two - way categori - cal variable for each number to indicate whethera negation operation should be performed .", "entities": []}, {"text": "Thenwe compute the probabilities of logical negationon thei - th number as : pnegationi= softmax(FFN([ui;hQ2;hP2;hCLS]))3.3 Multi - Span ExtractionAlthough existing reading comprehension tasksfocus exclusively on \ufb01nding one span of text asthe \ufb01nal answer , DROP loosens the restriction sothat the answer to the question may be severaltext spans .", "entities": [[20, 22, "TaskName", "reading comprehension"], [34, 35, "DatasetName", "DROP"]]}, {"text": "Therefore , speci\ufb01c adaption should bemade to extend previous single - span extraction tomulti - span scenario .", "entities": []}, {"text": "To do this , we propose directly predicting thenumber of spans and model it as a classi\ufb01cationproblem .", "entities": []}, {"text": "This is achieved by computing a proba - bility distribution on span amount aspspan= softmax(FFN([hQ2;hP2;hCLS]))To extract non - overlapped spans to the speci\ufb01camount , we adopt the non - maximum suppression(NMS ) algorithm ( Rosenfeld and Thurston,1971)that is widely used in computer vision for pruningredundant bounding boxes , as shown in Algorithm1 .", "entities": []}, {"text": "Concretely , the model \ufb01rst proposes a set oftop - KspansSaccording to the descending orderof the span score , which is computed aspstartkpendlfor the span(k , l ) .", "entities": []}, {"text": "It also predicts the amount ofextracted spanstfrompspan , and initializes a newset\u02dcS. Next , we add the spansithat possesses themaximum span score to the set\u02dcS , and remove itfromS. We also delete any remaining spansjthatoverlaps withsi , where the degree of overlap ismeasured using the text - level F1 function .", "entities": [[48, 49, "MetricName", "F1"]]}, {"text": "Thisprocess is repeated for remaining spans inS , untilSis empty or the size of\u02dcSreachest.3.4 Arithmetic Expression RerankingAs discussed in\u00a73.2 , we model the phenomenonof discrete reasoning on numbers by learning topredict a plus , minus , or zero for each number inthe passage .", "entities": []}, {"text": "In this way , an arithmetic expres - sion composed of signed numbers can be obtained , where the \ufb01nal answer can be deduced by per - forming simple arithmetic computation .", "entities": []}, {"text": "However , since the sign of each number is only determinedby the number representation and some coarse - grained global representations , the context infor - mation of the expression itself has not been con - sidered .", "entities": []}, {"text": "As a result , the model may predict some", "entities": []}, {"text": "1600Algorithm 1Multi - span extractionInput : pstart;pend;pspan1 : Generate the setSby extracting top - Kspans2 : SortSin descending order of span scores3 : t= arg maxpspan+14 : Initialize\u02dcS={}5 : whileS6={}and|\u02dcS|<tdo6 : forsiinSdo7 : Add spansito\u02dcS8 : Remove spansifromS9 : forsjinSdo10 : iff1(si , sj)>0then11 : Remove spansjfromS12 : return\u02dcSobviously wrong expressions ( e.g. , the signs thathave maximum probabilities are either minus orzero , resulting in a large negative value ) .", "entities": []}, {"text": "There - fore , in order to further validate the prediction , itis necessary to rank several highly con\ufb01dent ex - pression candidates using the representation sum - marized from the expression \u2019s context .", "entities": []}, {"text": "Speci\ufb01cally , we use beam search to producetop - ranked arithmetic expressions , which are sentback to the network for reranking .", "entities": []}, {"text": "Since each ex - pression consists of several signed numbers , weconstruct an expression representation by takingboth the numbers and the signs into account .", "entities": []}, {"text": "Foreach number in the expression , we gather its cor - responding vector from the representationU. Asfor the signs , we initialize an embedding matrixE2R3 \u21e5 2 \u21e4 D , and \ufb01nd the sign embeddings foreach signed number .", "entities": []}, {"text": "In this way , given thei - th ex - pression that containsMsigned numbers at most , we can obtain number vectorsVi2RM \u21e5 2 \u21e4 Daswell as sign embeddingsCi2RM \u21e5 2 \u21e4 D. Then theexpression representation along with the rerankingprobability can be calculated as: \u21b5 Vi= softmax(WV(Vi+Ci)),hVi= \u21b5 Vi(Vi+Ci),parithi= softmax(FFN([hVi;hQ2;hP2;hCLS]))3.5 Training and InferenceSince DROP does not indicate the answer", "entities": [[54, 55, "DatasetName", "DROP"]]}, {"text": "typebut only provides the answer string , we thereforeadopt the weakly supervised annotation scheme , as suggested inBerant et al.(2013);Dua et al.(2019 ) .", "entities": []}, {"text": "We \ufb01nd all possible annotations that pointto the gold answer , including matching spans , arithmetic expressions , correct count numbers , negation operations , and the number of spans .", "entities": []}, {"text": "Weuse simple rules to search over all mentioned num - bers to \ufb01nd potential negations .", "entities": []}, {"text": "That is , if 100minus a number is equal to the answer , then anegation occurs on this number .", "entities": []}, {"text": "Besides , we onlysearch the addition / subtraction of three numbers atmost due to the exponential search space .", "entities": []}, {"text": "To train our model , we propose using a two - step training method composed of an inferencestep and a training step .", "entities": []}, {"text": "In the \ufb01rst step , we usethe model to predict the probabilities of sign as - signments for numbers .", "entities": []}, {"text": "If there exists any an - notation of arithmetic expressions , we run beamsearch to produce expression candidates and la - bel them as either correct or wrong , which arelater used for supervising the reranking compo - nent .", "entities": []}, {"text": "In the second step , we adopt the marginallikelihood objective function ( Clark and Gardner,2018 ) , which sums over the probabilities of allpossible annotations including the above labeledexpressions .", "entities": []}, {"text": "Notice that there are two objectivefunctions for the multi - span component : one is adistantly - supervised loss that maximizes the prob - abilities of all matching spans , and the other is aclassi\ufb01cation loss that maximizes the probabilityon span amount .", "entities": [[18, 19, "MetricName", "loss"], [35, 36, "MetricName", "loss"]]}, {"text": "At test time , the model \ufb01rst chooses the answertype and then performs speci\ufb01c prediction strate - gies .", "entities": []}, {"text": "For the span type , we use Algorithm1fordecoding .", "entities": []}, {"text": "If the type is addition / subtraction , arith - metic expression candidates will be proposed andfurther reranked .", "entities": []}, {"text": "The expression with the maxi - mum product of cumulative sign probability andreranking probability is chosen .", "entities": []}, {"text": "As for the count - ing type , we choose the number that has the max - imum counting probability .", "entities": []}, {"text": "Finally , if the type isnegation , we \ufb01nd the number that possesses thelargest negation probability , and then output theanswer as 100 minus this number.4 Experiments4.1", "entities": []}, {"text": "Implementation DetailsDatasetWe consider the reading comprehensionbenchmark that requires Discrete Reasoning OverParagraphs ( DROP ) ( Dua et al . ,2019 ) to trainand evaluate our model .", "entities": [[12, 13, "DatasetName", "DROP"]]}, {"text": "DROP contains crowd - sourced , adversarially - created , 96.6 K question - answer pairs , with 77.4 K for training , 9.5 K forvalidation , and another 9.6 K hidden examples fortesting .", "entities": [[0, 1, "DatasetName", "DROP"]]}, {"text": "Passages are extracted from Wikipediaarticles and the answer to each question involvesvarious types such as number , date , or text string .", "entities": []}, {"text": "Some answers may even be a set of multiple spansof text in the passage .", "entities": []}, {"text": "To \ufb01nd the answers , a com-", "entities": []}, {"text": "1601ModelDev TestEM F1 EM F1Heuristic Baseline ( Dua et al . ,2019)4.28 8.07 4.18 8.59Semantic Role Labeling ( Carreras and M`arquez,2004)11.03 13.67 10.87 13.35BiDAF ( Seo et al . ,2017)26.06 28.85 24.75 27.49QANet+ELMo ( Yu et al . ,2018)27.71 30.33 27.08 29.67BERTBASE(Devlin et al . ,2019)30.10 33.36 29.45 32.70NAQANet ( Dua et al . ,2019)46.20 49.24 44.07 47.01NABERTBASE55.82 58.75 - -NABERTLARGE64.61 67.35 - -MTMSNBASE68.17 72.81 - -MTMSNLARGE76.68 80.54 75.85 79.88Human Performance ( Dua et al . ,2019)- - 92.38 95.98Table 1 : The performance of MTMSN and other competing approaches on DROP dev and test set .", "entities": [[2, 3, "MetricName", "F1"], [3, 4, "MetricName", "EM"], [97, 98, "DatasetName", "DROP"]]}, {"text": "ModelBASE LARGEEM F1 EM F1MTMSN68.2 72.8 76.7 80.5w /", "entities": [[2, 3, "MetricName", "F1"], [3, 4, "MetricName", "EM"]]}, {"text": "o Add / Sub46.7", "entities": []}, {"text": "51.3 53.8 58.0w / o Count62.5 66.4 71.8 75.6w / o Negation59.4", "entities": []}, {"text": "63.6 67.2 70.9w / o", "entities": []}, {"text": "Multi - Span67.5", "entities": []}, {"text": "70.7 75.6 78.4w / o Reranking66.9 71.2 74.9 78.7Table 2 : Ablation tests of base and large models on theDROP dev set.prehensive understanding of the context as well", "entities": []}, {"text": "asthe ability of numerical reasoning are required .", "entities": []}, {"text": "Model settingsWe build our model upon twopublicly available uncased versions of BERT : BERTBASEand BERTLARGE2 , and refer readerstoDevlin et al.(2019 ) for details on model sizes .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "We use Adam optimizer with a learning rate of 3e-5 and warmup over the \ufb01rst 5 % steps to train .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [6, 8, "HyperparameterName", "learning rate"]]}, {"text": "Themaximum number of epochs is set to 10 for basemodels and 5 for large models , while the batch sizeis 12 or 24 respectively .", "entities": [[1, 4, "HyperparameterName", "number of epochs"]]}, {"text": "A dropout probability of0.1 is used unless stated otherwise .", "entities": []}, {"text": "The number ofcounting class is set to 10 , and the maximum num - ber of spans is 8 .", "entities": []}, {"text": "The beam size is 3 by default , while the maximum amount of signed numbersMis set to 4 .", "entities": []}, {"text": "All texts are tokenized using Word-2BERTBASEis the original version while BERTLARGEis the model augmented with n - gram masking andsynthetic self - training : https://github.com / google - research / bert . ModelEM", "entities": []}, {"text": "F1MTMSN76.7 80.5w / o Q / P Vectors75.1", "entities": []}, {"text": "79.2w / o CLS Vector74.0 78.4Q / P Vectors Using Last Hidden76.5 80.2w / o Gated Span Prediction75.8 79.7Combine Add / Sub with Negation75.5", "entities": []}, {"text": "79.4Table 3 : Ablation tests of different architecture choicesusing MTMSNLARGE.Piece vocabulary ( Wu et al . ,2016 ) , and truncatedto sequences no longer than 512 tokens .", "entities": []}, {"text": "BaselinesFollowing the implementation ofAugmented QANet ( NAQANet ) ( Dua et al . ,2019 ) , we introduce a similar baseline called Aug - mented BERT ( NABERT ) .", "entities": [[26, 27, "MethodName", "BERT"]]}, {"text": "The main differenceis that we replace the encoder of QANet ( Yuet al . ,2018 ) with the pre - trained Transformerblocks ( Devlin et al . ,2019 ) .", "entities": []}, {"text": "Moreover , it also sup - ports the prediction of various answer types suchas span , arithmetic expression , and count number.4.2 Main ResultsTwo metrics , namely Exact Match ( EM ) and F1score , are utilized to evaluate models .", "entities": [[27, 29, "MetricName", "Exact Match"], [30, 31, "MetricName", "EM"]]}, {"text": "We use theof\ufb01cial script to compute these scores .", "entities": []}, {"text": "Since thetest set is hidden , we only submit the best singlemodel to obtain test results .", "entities": []}, {"text": "Table1shows the performance of our modeland other competitive approaches on the develop-", "entities": []}, {"text": "1602Type(%)NABERT MTMSNEM F1 EM F1Date1.6 55.7 60.8 55.7 69.0Number61.9 63.8 64.0 80.9 81.1Single Span31.7 75.9 80.6 77.5 82.8Multi Span4.8 0 22.7 25.1 62.8Table 4 : Performance breakdown of NABERTLARGEand MTMSNLARGEby gold answer types.ment and test sets .", "entities": [[2, 3, "MetricName", "F1"], [3, 4, "MetricName", "EM"], [19, 20, "DatasetName", "0"]]}, {"text": "MTMSN outperforms all ex - isting approaches by a large margin , and createsnew state - of - the - art results by achieving an EMscore of 75.85 and a F1 score of 79.88 on the testset .", "entities": [[30, 32, "MetricName", "F1 score"]]}, {"text": "Since our best model utilizes BERTLARGEas encoder , we therefore compare MTMSNLARGEwith the NABERTLARGEbaseline .", "entities": []}, {"text": "As we can see , our model obtains 12.07/13.19 absolute gain ofEM / F1 over the baseline , demonstrating the effec - tiveness of our approach .", "entities": [[13, 14, "MetricName", "F1"]]}, {"text": "However , as the humanachieves 95.98 F1 on the test set , our results sug - gest that there is still room for improvement.4.3 Ablation StudyComponent ablationTo analyze the effect ofthe proposed components , we conduct ablationstudies on the development set .", "entities": [[6, 7, "MetricName", "F1"]]}, {"text": "As illustrated inTable2 , the use of addition and subtraction is ex - tremely crucial : the EM / F1 performance of boththe base and large models drop drastically by morethan 20 points if it is removed .", "entities": [[17, 18, "MetricName", "EM"], [19, 20, "MetricName", "F1"]]}, {"text": "Predicting countnumbers is also an important component that con - tributes nearly 5 % gain on both metrics .", "entities": []}, {"text": "More - over , enhancing the model with the negation typesigni\ufb01cantly increases the F1 by roughly 9 percenton both models .", "entities": [[13, 14, "MetricName", "F1"]]}, {"text": "In brief , the above results showthat multi - type answer prediction is vitally impor - tant for handling different forms of answers , es - pecially in cases where discrete reasoning abilitiesare required .", "entities": []}, {"text": "We also report the performance after remov - ing the multi - span extraction method .", "entities": []}, {"text": "The resultsreveal that it has a more negative impact on theF1 score .", "entities": []}, {"text": "We interpret this phenomenon as fol - lows : producing multiple spans that are partiallymatched with ground - truth answers is much easierthan generating an exactly - matched set of multipleanswers .", "entities": []}, {"text": "Hence for multi - span scenarios , the gainof our method on F1 is relatively easier to obtainthan the one on EM .", "entities": [[12, 13, "MetricName", "F1"], [21, 22, "MetricName", "EM"]]}, {"text": "Finally , to ablate arithmeticexpression reranking , we simply use the arithmeticexpression that has the maximum cumulative signTypeNABERT MTMSN(% ) EM F1 ( % ) EM", "entities": [[20, 21, "MetricName", "EM"], [21, 22, "MetricName", "F1"], [25, 26, "MetricName", "EM"]]}, {"text": "F1Span43.0 67.9 74.2 42.7 72.2 81.0Add / Sub43.6 62.0 62.1 32.4 78.1 78.2Count13.4 62.4 62.4 13.4 70.4 70.4Negation0 0 0 11.5 96.3 96.3Table 5 : Performance breakdown of NABERTLARGEand", "entities": [[18, 19, "DatasetName", "0"], [19, 20, "DatasetName", "0"]]}, {"text": "MTMSNLARGEby predicted answer types.probability instead .", "entities": []}, {"text": "We \ufb01nd that our rerankingmechanism gives 1.8 % gain on both metrics forthe large model .", "entities": []}, {"text": "This con\ufb01rms that validating ex - pression candidates with their context informationis bene\ufb01cial for \ufb01ltering out highly - con\ufb01dent butwrong predictions .", "entities": []}, {"text": "Architecture ablationWe further conduct a de - tailed ablation in Table3to evaluate our architec - ture designs .", "entities": []}, {"text": "First , we investigate the effects ofsome \u201c global vectors \u201d used in our model .", "entities": []}, {"text": "Speci\ufb01-cally , we \ufb01nd that removing the question and pas - sage vectors from all involved computation leadsto 1.3 % drop on F1 .", "entities": [[22, 23, "MetricName", "F1"]]}, {"text": "Ablating the representationof[CLS]token leads to even worse results .", "entities": []}, {"text": "Wealso try to use the last hidden representation ( de - noted asM3 ) to calculate question and passagevectors , but \ufb01nd that does not work .", "entities": []}, {"text": "Next , we re - move the gating mechanism used during span pre - diction , and observe a nearly 0.8 % decline on bothmetrics .", "entities": []}, {"text": "Finally , we share parameters between thearithmetic expression component and the negationcomponent , and \ufb01nd the performance drops by1.1 % on F1.4.4 Analysis and DiscussionPerformance breakdownWe now provide aquantitative analysis by showing performancebreakdown on the development set .", "entities": []}, {"text": "Table4showsthat our gains mainly come from the most frequentnumber type , which requires various types of sym - bolic , discrete reasoning operations .", "entities": []}, {"text": "Moreover , signi\ufb01cant improvements are also obtained in themulti - span category , where the F1 score increasesby more than 40 points .", "entities": [[15, 17, "MetricName", "F1 score"]]}, {"text": "This result further provesthe validity of our multi - span extraction method .", "entities": []}, {"text": "We also give the performance statistics thatare categorized according to the predicted answertypes in Table5 .", "entities": []}, {"text": "As shown in the Table , the mainimprovements are due to the addition / subtractionand negation types .", "entities": []}, {"text": "We conjecture that there aretwo reasons for these improvements .", "entities": []}, {"text": "First , our", "entities": []}, {"text": "1603 Figure 3 : EM / F1 scores of MTMSNLARGEwith differ - ent maximum numbers of spans .", "entities": [[4, 5, "MetricName", "EM"], [6, 7, "MetricName", "F1"]]}, {"text": "Figure 4 : EM / F1 scores of MTMSNLARGEwith differ - ent beam sizes and amounts of signed numbers ( M).proposed expression reranking mechanism helpsvalidate candidate expressions .", "entities": [[3, 4, "MetricName", "EM"], [5, 6, "MetricName", "F1"]]}, {"text": "Second , a new in - ductive bias that enables the model to perform log - ical negation has been introduced .", "entities": []}, {"text": "The impressiveperformance on the negation type con\ufb01rms ourjudgement , and suggests that the model is able to\ufb01nd most of negation operations .", "entities": []}, {"text": "In addition , wealso observe promising gains brought by the spanand count types .", "entities": []}, {"text": "We think the gains are mainlydue to the multi - span extraction method as well asarchitecture designs .", "entities": []}, {"text": "Effect of maximum number of spansTo inves - tigate the effect of maximum number of spans onmulti - span extraction , we conduct an experimenton the dev set and show the curves in Figure3.We vary the value from 2 to 12 , increased by 2,and also include the extreme value 1 .", "entities": []}, {"text": "Accordingto the Figure , the best results are obtained at 8.A higher value could potentially increase the an - swer recall but damage the precision by makingmore predictions , and a smaller value may forcethe model to produce limited number of answers , resulting in high precision but low recall .", "entities": []}, {"text": "There - fore , a value of 8 turns out to be a good trade - offbetween recall and precision .", "entities": []}, {"text": "Moreover , when thevalue decreases to 1 , the multi - span extraction de - grades to previous single - span scenario , and theperformance drops signi\ufb01cantly .", "entities": []}, {"text": "Con\ufb01gurationSkipped Kept Ratio ( % ) F1Span33752 43657 56.4 38.9+|6384 71025 91.7 59.2+|+\u007f4282 73127 94.4 63.6+|+\u007f+~1595 75814", "entities": []}, {"text": "97.9 72.8Table 6 : Annotation statistics under different combi - nations of answer types in the DROP train set .", "entities": [[16, 17, "DatasetName", "DROP"]]}, {"text": "\u201c Kept\u201dand \u201c Skipped \u201d mean the number of examples with orwithout annotation , respectively.|refers to Add / Sub,\u007fdenotes Count , and ~ indicates Negation .", "entities": []}, {"text": "F1 scoresare benchmarked using MTMSNBASEon the dev set .", "entities": [[0, 1, "MetricName", "F1"]]}, {"text": "Effect of beam size andMWe further investi - gate the effect of beam size and maximum amountof signed numbers in Figure4 .", "entities": []}, {"text": "As we can see , a beam size of 3 leads to the best performance , likely because a larger beam size might confusethe model as too many candidates are ranked , onthe other hand , a small size could be not suf\ufb01-cient to cover the correct expression .", "entities": []}, {"text": "In addition , we \ufb01nd that the performance constantly decreasesas the maximum thresholdMincreases , suggest - ing that most of expressions only contain two orthree signed numbers , and setting a larger thresh - old could bring in additional distractions .", "entities": []}, {"text": "Annotation statisticsWe list the annotationstatistics on the DROP train set in Table6 .", "entities": [[7, 8, "DatasetName", "DROP"]]}, {"text": "Aswe can see , only annotating matching spans resultsin a labeled ratio of 56.4 % , indicating that DROPincludes various answer types beyond text spans .", "entities": []}, {"text": "By further considering the arithmetic expression , the ratio increase sharply to 91.7 % , suggestingmore than 35 % answers need to be inferred withnumeral reasoning .", "entities": []}, {"text": "Continuing adding countingleads to a percentage of 94.4 % , and a \ufb01nal 97.9%coverage is achieved by additionally taking nega - tion into account .", "entities": []}, {"text": "More importantly , the F1 scoreconstantly increases as more answer types are con - sidered .", "entities": [[4, 5, "MetricName", "F1"]]}, {"text": "This result is consistent with our observa - tions in ablation study .", "entities": []}, {"text": "Error analysisFinally , to better understand theremaining challenges , we randomly sample 100incorrectly predicted examples based on EM andcategorize them into 7 classes .", "entities": [[0, 1, "MetricName", "Error"], [17, 18, "MetricName", "EM"]]}, {"text": "38 % of errors areincorrect arithmetic computations , 18 % requiresorting over multiple entities , 13 % are due to mis - takes on multi - span extraction , 10 % are single - span extraction problems , 8 % involve miscount - ing , another 8 % are wrong predictions on spannumber , the rest ( 5 % ) are due to various reasons", "entities": []}, {"text": "1604such as incorrect preprocessing , negation error , and so on .", "entities": []}, {"text": "See Appendix for some examples ofthe above error cases.5", "entities": []}, {"text": "Related WorkReading comprehension benchmarksPromis - ing advancements have been made for readingcomprehension due to the creation of many largedatasets .", "entities": []}, {"text": "While early research used cloze - styletests ( Hermann et al . ,2015;Hill et al . ,2016 ) , mostof recent works ( Rajpurkar et al . ,2016;Joshi et al . ,2017 ) are designed to extract answers from thepassage .", "entities": []}, {"text": "Despite their success , these datasets onlyrequire shallow pattern matching and simple logi - cal reasoning , thus being well solved ( Chen et al . ,2016;Devlin et al . ,2019 ) .", "entities": []}, {"text": "Recently , Dua et al.(2019 ) released a new benchmark named DROPthat demands discrete reasoning as well as deeperparagraph understanding to \ufb01nd the answers .", "entities": []}, {"text": "Sax - ton et al.(2019 ) introduced a dataset consistingof different types of mathematics problems to fo - cuses on mathematical computation .", "entities": []}, {"text": "We choose towork on DROP to test both the numerical reason - ing and linguistic comprehension abilities .", "entities": [[4, 5, "DatasetName", "DROP"]]}, {"text": "Neural reading modelsPrevious neural read - ing models , such as BiDAF ( Seo et al . ,2017),R - Net ( Wang et al . ,2017 ) , QANet ( Yu et al . ,2018 ) , Reinforced Mreader ( Hu et al . ,2018 ) , areusually designed to extract a continuous spanof text as the answer .", "entities": []}, {"text": "Dua et al.(2019 ) en - hanced prior single - type prediction to support var - ious answer types such as span , count number , and addition / subtraction .", "entities": [[10, 12, "TaskName", "type prediction"]]}, {"text": "Different from theseapproaches , our model additionally supports anew negation type to increase answer coverage , and learns to dynamically extract one or multiplespans .", "entities": []}, {"text": "Morevoer , answer reranking has been wellstudied in several prior works ( Cui et", "entities": []}, {"text": "al . ,2016;Wang et", "entities": []}, {"text": "al . ,2018a , b , c;Hu et al . ,2019 ) .", "entities": []}, {"text": "We fol - low this line of work , but propose ranking arith - metic expressions instead of candidate answers .", "entities": []}, {"text": "End - to - end symbolic reasoningCombiningneural methods with symbolic reasoning was con - sidered byGraves et al.(2014);Sukhbaatar et al.(2015 ) , where neural networks augmented withexternal memory are trained to execute simple pro - grams .", "entities": []}, {"text": "Later works on program induction ( Reedand De Freitas,2016;Neelakantan et", "entities": [[3, 5, "TaskName", "program induction"]]}, {"text": "al . ,2016;Liang et al . ,2017 ) extended this idea by usingseveral built - in logic operations along with a key - value memory to learn different types of compo - sitional programs such as addition or sorting .", "entities": []}, {"text": "Incontrast to these works , MTMSN does not modelvarious types of reasoning with a universal mem - ory mechanism but instead deals each type withindividual predicting strategies .", "entities": []}, {"text": "Visual question answeringIn computer vi - sion community , the most similar work to ourapproach is Neural Module Networks ( Andreaset al . ,2016b ) , where a dependency parser is usedto lay out a neural network composed of severalpre - de\ufb01ned modules .", "entities": []}, {"text": "Later , Andreas et al.(2016a)proposed dynamically choosing an optimal lay - out structure from a list of layout candidates thatare produced by off - the - shelf parsers .", "entities": []}, {"text": "Hu et al.(2017 ) introduced an end - to - end module networkthat learns to predict instance - speci\ufb01c networklayouts without the aid of a parser .", "entities": []}, {"text": "Compared tothese approaches , MTMSN has a static networklayout that can not be changed during training andevaluation , where pre - de\ufb01ned \u201c modules \u201d are usedto handle different types of answers.6 ConclusionWe introduce MTMSN , a multi - type multi - spannetwork for reading comprehension that requiresdiscrete reasoning over the content of paragraphs .", "entities": [[44, 46, "TaskName", "reading comprehension"]]}, {"text": "We enhance a multi - type answer predictor to sup - port logical negation , propose a multi - span extrac - tion method for producing multiple answers , anddesign an arithmetic expression reranking mecha - nism to further con\ufb01rm the prediction .", "entities": []}, {"text": "Our modelachieves 79.9 F1 on the DROP hidden test set , cre - ating new state - of - the - art results .", "entities": [[3, 4, "MetricName", "F1"], [6, 7, "DatasetName", "DROP"]]}, {"text": "As future work , we would like to consider handling additionaltypes such as sorting or multiplication / division .", "entities": []}, {"text": "We also plan to explore more advanced methodsfor performing complex numerical reasoning .", "entities": []}, {"text": "AcknowledgmentsWe would like to thank the anonymous reviewersfor their thoughtful comments and insightful feed - back .", "entities": []}, {"text": "This work was supported by the NationalKey Research and Development Program of China(2016YFB100101).ReferencesJacob Andreas , Marcus Rohrbach , Trevor Darrell , andDan Klein .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "Learning to compose neural net-", "entities": []}, {"text": "1605works for question answering .", "entities": [[2, 4, "TaskName", "question answering"]]}, {"text": "InProceedings ofNAACL.Jacob Andreas , Marcus Rohrbach , Trevor Darrell , andDan Klein .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "Neural module networks .", "entities": []}, {"text": "InProceedings of CVPR.Jonathan Berant , Andrew Chou , Roy Frostig , and PercyLiang .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Semantic parsing on freebase fromquestion - answer pairs .", "entities": [[0, 2, "TaskName", "Semantic parsing"]]}, {"text": "InProceedings of EMNLP.Xavier Carreras and Llu\u00b4\u0131s M`arquez .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Introduc - tion to the conll-2004 shared task : Semantic role la - beling .", "entities": []}, {"text": "InProceedings of CONLL.Danqi Chen , Jason Bolton , and Christopher D Man - ning .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A thorough examination of thecnn / daily mail reading comprehension task.arXivpreprint arXiv:1606.02858.Christopher", "entities": [[8, 10, "TaskName", "reading comprehension"]]}, {"text": "Clark and Matt Gardner .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Simpleand effective multi - paragraph reading comprehen - sion .", "entities": []}, {"text": "InProceedings of ACL.Yiming Cui , Zhipeng Chen , Si Wei , Shijin Wang , Ting Liu , and Guoping Hu . 2016 .", "entities": []}, {"text": "Attention - over - attention neural networks for reading comprehen - sion.arXiv preprint arXiv:1607.04423.Jacob Devlin , Ming - Wei Chang , Kenton Lee , andKristina Toutanova .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bert : Pre - training of deepbidirectional transformers for language understand - ing .", "entities": []}, {"text": "InProceedings of NAACL.Dheeru Dua , Yizhong Wang , Pradeep Dasigi , GabrielStanovsky , Sameer Singh , and Matt Gardner .", "entities": []}, {"text": "2019.Drop : A reading comprehension benchmark requir - ing discrete reasoning over paragraphs .", "entities": [[3, 5, "TaskName", "reading comprehension"]]}, {"text": "InProceed - ings of NAACL.Alex Graves , Greg Wayne , and Ivo Danihelka.2014 .", "entities": []}, {"text": "Neural turing machines.arXiv preprintarXiv:1410.5401.Dan Hendrycks and Kevin Gimpel .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Bridgingnonlinearities and stochastic regularizers with gaus - sian error linear units .", "entities": []}, {"text": "CoRR , abs/1606.08415.Karl Moritz Hermann , Tomas Kocisky , EdwardGrefenstette , Lasse Espeholt , Will Kay , Mustafa Su - leyman , and Phil Blunsom .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Teaching ma - chines to read and comprehend .", "entities": []}, {"text": "InProceedings ofNIPS.Felix Hill , Antoine Bordes , Sumit Chopra , and JasonWeston .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "The goldilocks principle : Readingchildrens books with explicit memory representa - tions .", "entities": []}, {"text": "InProceedings of ICLR.Minghao Hu , Yuxing Peng , Zhen Huang , and Dong - sheng Li .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Retrieve , read , rerank : Towardsend - to - end multi - document reading comprehension .", "entities": [[14, 16, "TaskName", "reading comprehension"]]}, {"text": "InProceedings of ACL.Minghao Hu , Yuxing Peng , Zhen Huang , Xipeng Qiu , Furu Wei , and Ming Zhou .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Reinforcedmnemonic reader for machine reading comprehen - sion .", "entities": []}, {"text": "InProceedings of IJCAI.Ronghang Hu , Jacob Andreas , Marcus Rohrbach , Trevor Darrell , and Kate Saenko .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Learningto reason : End - to - end module networks for visualquestion answering .", "entities": []}, {"text": "InProceedings of ICCV.Mandar Joshi , Eunsol Choi , Daniel S Weld , and LukeZettlemoyer .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Triviaqa :", "entities": [[0, 1, "DatasetName", "Triviaqa"]]}, {"text": "A large scale distantlysupervised challenge dataset for reading comprehen - sion .", "entities": []}, {"text": "InProceedings of ACL.Jimmy Lei Ba , Jamie Ryan Kiros , and Geoffrey E Hin - ton . 2016 .", "entities": []}, {"text": "Layer normalization.arXiv preprintarXiv:1607.06450.Chen Liang , Jonathan Berant , Quoc Le , Kenneth DForbus , and Ni Lao .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Neural symbolic ma - chines : Learning semantic parsers on freebase withweak supervision .", "entities": []}, {"text": "InProceedings of ACL.Arvind Neelakantan , Quoc V Le , and Ilya Sutskever.2016 .", "entities": []}, {"text": "Neural programmer : Inducing latent pro - grams with gradient descent .", "entities": []}, {"text": "InProceedings ofICLR.Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , andPercy Liang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Squad : 100,000 + questions formachine comprehension of text .", "entities": []}, {"text": "InProceedings ofEMNLP.Scott Reed and Nando De Freitas .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Neuralprogrammer - interpreters .", "entities": []}, {"text": "InProceedings of ICLR.Azriel Rosenfeld and Mark Thurston .", "entities": []}, {"text": "1971 .", "entities": []}, {"text": "Edge andcurve detection for visual scene analysis .", "entities": []}, {"text": "IEEETransactions on computers , ( 5):562\u2013569.David Saxton , Edward Grefenstette , Felix Hill , andPushmeet Kohli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Analysing mathematicalreasoning abilities of neural models .", "entities": []}, {"text": "InProceedingsof ICLR.Minjoon Seo , Aniruddha Kembhavi , Ali Farhadi , andHannaneh Hajishirzi .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Bidirectional attention\ufb02ow for machine comprehension .", "entities": []}, {"text": "InProceedings ofICLR.Sainbayar Sukhbaatar , Jason Weston , Rob Fergus , et al.2015 .", "entities": []}, {"text": "End - to - end memory networks .", "entities": []}, {"text": "InProceed - ings of NIPS.Ashish Vaswani , Noam Shazeer , Niki Parmar , JakobUszkoreit , Llion Jones , Aidan N Gomez , \u0141ukaszKaiser , and Illia Polosukhin .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Attention is allyou need .", "entities": []}, {"text": "InProceedings of NIPS.Shuohang Wang , Mo Yu , Jing Jiang , Wei Zhang , Xiaoxiao Guo , Shiyu Chang , Zhiguo Wang , TimKlinger , Gerald Tesauro , and Murray Campbell.2018a .", "entities": []}, {"text": "Evidence aggregation for answer re - rankingin open - domain question answering .", "entities": [[7, 12, "TaskName", "open - domain question answering"]]}, {"text": "InProceedingsof ICLR.Wenhui Wang , Nan Yang , Furu Wei , Baobao Chang , and Ming Zhou .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Gated self - matching net - works for reading comprehension and question an - swering .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}, {"text": "InProceedings of ACL .", "entities": []}, {"text": "1606Yizhong Wang , Kai Liu , Jing Liu , Wei He , YajuanLyu , Hua Wu , Sujian Li , and Haifeng Wang .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "Multi - passage machine reading comprehension withcross - passage answer veri\ufb01cation .", "entities": [[3, 6, "TaskName", "machine reading comprehension"]]}, {"text": "InProceedingsof ACL.Zhen Wang , Jiachen Liu , Xinyan Xiao , Yajuan Lyu , and Tian Wu .", "entities": []}, {"text": "2018c .", "entities": []}, {"text": "Joint training of candidateextraction and answer selection for reading compre - hension .", "entities": [[5, 7, "TaskName", "answer selection"]]}, {"text": "InProceedings of ACL.Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc VLe , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , KlausMacherey , et al . 2016 .", "entities": []}, {"text": "Google \u2019s neural ma - chine translation system : Bridging the gap betweenhuman and machine translation.arXiv preprintarXiv:1609.08144.Adams Wei Yu , David Dohan , Quoc Le , Thang Luong , Rui Zhao , and Kai Chen .", "entities": [[0, 1, "DatasetName", "Google"]]}, {"text": "2018 .", "entities": []}, {"text": "Fast and accuratereading comprehension by combining self - attentionand convolution .", "entities": [[9, 10, "MethodName", "convolution"]]}, {"text": "InProceedings of ICLR .", "entities": []}]