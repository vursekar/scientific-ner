[{"text": "Proceedings of The Third Workshop on Computational Approaches to Code - Switching , pages 110\u2013114 Melbourne , Australia , July 19 , 2018 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics110Bilingual Character Representation for Ef\ufb01ciently Addressing Out - of - Vocabulary Words in Code - Switching Named Entity Recognition Genta Indra Winata , Chien - Sheng Wu , Andrea Madotto , Pascale Fung Center for Arti\ufb01cial Intelligence Research ( CAiRE ) Department of Electronic and Computer Engineering Hong Kong University of Science and Technology , Clear Water Bay , Hong Kong fgiwinata , cwuak , eeandreamad g@ust.hk , pascale@ece.ust.hk", "entities": [[20, 23, "TaskName", "Named Entity Recognition"]]}, {"text": "Abstract We propose an LSTM - based model with hierarchical architecture on named entity recognition from code - switching Twitter data .", "entities": [[4, 5, "MethodName", "LSTM"], [12, 15, "TaskName", "named entity recognition"]]}, {"text": "Our model uses bilingual character representation and transfer learning to address out - of - vocabulary words .", "entities": [[7, 9, "TaskName", "transfer learning"]]}, {"text": "In order to mitigate data noise , we propose to use token replacement and normalization .", "entities": []}, {"text": "In the 3rd Workshop on Computational Approaches to Linguistic Code - Switching Shared Task , we achieved second place with 62.76 % harmonic mean F1 - score for English - Spanish language pair without using any gazetteer and knowledge - based information .", "entities": [[24, 27, "MetricName", "F1 - score"]]}, {"text": "1 Introduction Named Entity Recognition ( NER ) predicts which word tokens refer to location , people , organization , time , and other entities from a word sequence .", "entities": [[2, 5, "TaskName", "Named Entity Recognition"], [6, 7, "TaskName", "NER"]]}, {"text": "Deep neural network models have successfully achieved the state - of - the - art performance in NER tasks ( Cohen ; Chiu and Nichols , 2016 ; Lample et al . , 2016 ;", "entities": [[17, 18, "TaskName", "NER"]]}, {"text": "Shen et al . , 2017 ) using monolingual corpus .", "entities": []}, {"text": "However , learning from code - switching tweets data is very challenging due to several reasons : ( 1 ) words may have different semantics in different context and language , for instance , the word \u201c cola \u201d can be associated with product or \u201c queue \u201d in Spanish ( 2 ) data from social media are noisy , with many inconsistencies such as spelling mistakes , repetitions , and informalities which eventually points to Out - ofV ocabulary ( OOV ) words issue ( 3 ) entities may appear in different language other than the matrix language .", "entities": []}, {"text": "For example \u201c todos los Domingos en Westland Mall \u201d where \u201c Westland Mall \u201d is an English named entity .", "entities": [[8, 9, "DatasetName", "Mall"], [13, 14, "DatasetName", "Mall"]]}, {"text": "Our contributions are two - fold : ( 1 ) bilingual character bidirectional RNN is used to capture character - level information and tackle OOV words issue ( 2 ) we apply transfer learning from monolingual pre - trained word vectors to adapt the model with different domains in a bilingual setting .", "entities": [[32, 34, "TaskName", "transfer learning"]]}, {"text": "In our model , we use LSTM to capture long - range dependencies of the word sequence and character sequence in bilingual character RNN .", "entities": [[6, 7, "MethodName", "LSTM"]]}, {"text": "In our experiments , we show the ef\ufb01ciency of our model in handling OOV words and bilingual word context .", "entities": []}, {"text": "2 Related Work Convolutional Neural Network ( CNN ) was used in NER task as word decoder by Collobert et al .", "entities": [[12, 13, "TaskName", "NER"]]}, {"text": "( 2011 ) and a few years later , Huang et al .", "entities": []}, {"text": "( 2015 ) introduced Bidirectional Long - Short Term Memory ( BiLSTM ) ( Sundermeyer et al . , 2012 ) .", "entities": [[11, 12, "MethodName", "BiLSTM"]]}, {"text": "Character - level features were explored by using neural architecture and replaced hand - crafted features ( Dyer et al . , 2015 ; Lample et al . , 2016 ; Chiu and Nichols , 2016 ; Limsopatham and Collier , 2016 ) .", "entities": []}, {"text": "Lample et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2016 ) also showed Conditional Random Field ( CRF ) ( Lafferty et al . , 2001 ) decoders to improve the results and used Stack memory - based LSTMs for their work in sequence chunking .", "entities": [[5, 8, "MethodName", "Conditional Random Field"], [9, 10, "MethodName", "CRF"], [36, 37, "TaskName", "chunking"]]}, {"text": "Aguilar et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2017 ) proposed multi - task learning by combining Part - of - Speech tagging task with NER and using gazetteers to provide language - speci\ufb01c knowledge .", "entities": [[4, 8, "TaskName", "multi - task learning"], [10, 16, "TaskName", "Part - of - Speech tagging"], [18, 19, "TaskName", "NER"]]}, {"text": "Characterlevel embeddings were used to handle the OOV words problem in NLP tasks such as NER ( Lample et al . , 2016 ) , POS tagging , and language modeling ( Ling et", "entities": [[15, 16, "TaskName", "NER"]]}, {"text": "al . , 2015 ) .", "entities": []}, {"text": "3 Methodology 3.1 Dataset For our experiment , we use English - Spanish ( ENG - SPA ) Tweets data from Twitter provided by", "entities": []}, {"text": "111Table 1 : OOV words rates on ENG - SPA dataset before and after preprocessing Train Dev Test All Entity All Entity All Corpus - - 18.91 % 31.84 % 49.39 % FastText ( eng ) ( Mikolov et", "entities": [[32, 33, "MethodName", "FastText"]]}, {"text": "al . , 2018 ) 62.62 % 16.76 % 19.12 % 3.91 % 54.59 % + FastText ( spa ) ( Grave et al . , 2018 ) 49.76 % 12.38 % 11.98 % 3.91 % 39.45 % + token replacement 12.43 % 12.35 % 7.18 % 3.91 % 9.60 % + token normalization 7.94 % 8.38 % 5.01 % 1.67 % 6.08 % Aguilar", "entities": [[16, 17, "MethodName", "FastText"]]}, {"text": "et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "There are nine different named - entity labels .", "entities": []}, {"text": "The labels use IOB format ( Inside , Outside , Beginning ) where every token is labeled as B - label in the beginning and follows withI - label if it is inside a named entity , or O otherwise .", "entities": []}, {"text": "For example \u201c Kendrick Lamar \u201d is represented as B - PER I - PER .", "entities": []}, {"text": "Table 2 and Table 3 show the statistics of the dataset .", "entities": []}, {"text": "Table 2 : Data Statistics for ENG - SPA Tweets Train Dev Test # Words 616,069 9,583 183,011 Table 3 : Entity Statistics for ENG - SPA Tweets Entities Train Dev # Person 4701 75 # Location 2810 10 # Product 1369 16 # Title 824 22 # Organization 811 9 # Group 718 4 # Time 577 6 # Event 232 4 # Other 324 6 \u201c Person \u201d , \u201c Location \u201d , and \u201c Product \u201d are the most frequent entities in the dataset , and the least common ones are \u201c Time \u201d , \u201c Event \u201d , and \u201c Other \u201d categories .", "entities": []}, {"text": "\u2018 Other \u201d category is the least trivial among all because it is not well clustered like others .", "entities": []}, {"text": "3.2 Feature Representation In this section , we describe word - level and character - level features used in our model .", "entities": []}, {"text": "Word Representation : Words are encoded into continuous representation .", "entities": []}, {"text": "The vocabulary is built from training data .", "entities": []}, {"text": "The Twitter data are very noisy , there are many spelling mistakes , irregular ways to use a word and repeating characters .", "entities": []}, {"text": "We apply several strategies to overcome the issue .", "entities": []}, {"text": "We use 300 - dimensional English ( Mikolov et al . , 2018 ) and Spanish ( Grave et al . , 2018 )", "entities": []}, {"text": "FastText pre - trained word vectors which comprise two million words vocabulary each and they are trained using Common Crawl and Wikipedia .", "entities": [[0, 1, "MethodName", "FastText"], [18, 20, "DatasetName", "Common Crawl"]]}, {"text": "To create the shared vocabulary , we concatenate English and Spanish word vectors .", "entities": []}, {"text": "For preprocessing , we propose the following steps : 1.Token replacement : Replace user hashtags ( # user ) and mentions ( @user ) with \u201c USR \u201d , and URL ( https://domain.com ) with \u201c URL \u201d .", "entities": []}, {"text": "2.Token normalization : Concatenate Spanish and English FastText word vector vocabulary .", "entities": [[7, 8, "MethodName", "FastText"]]}, {"text": "Normalize OOV words by using one out of these heuristics and check if the word exists in the vocabulary sequentially ( a ) Capitalize the \ufb01rst character ( b ) Lowercase the word ( c ) Step ( b ) and remove repeating characters , such as \u201c hellooooo \u201d into\u201chello \u201d or\u201clolololol \u201d into\u201clol \u201d ( d ) Step ( a ) and ( c ) altogether Then , the effectiveness of the preprocessing and transfer learning in handling OOV words are analyzed .", "entities": [[76, 78, "TaskName", "transfer learning"]]}, {"text": "The statistics is showed in Table 1 .", "entities": []}, {"text": "It is clear that using FastText word vectors reduce the OOV words rate especially when we concatenate the vocabulary of both languages .", "entities": [[5, 6, "MethodName", "FastText"]]}, {"text": "Furthermore , the preprocessing strategies dramatically decrease the number of unknown words .", "entities": []}, {"text": "Character Representation : We concatenate all possible characters for English and Spanish , including numbers and special characters .", "entities": []}, {"text": "English and Spanish have most of the characters in common , but , with some additional unique Spanish characters .", "entities": []}, {"text": "All cases are kept as they are .", "entities": []}, {"text": "1123.3 Model Description In this section , we describe our model architecture and hyper - parameters setting .", "entities": []}, {"text": "Bilingual Char - RNN :", "entities": []}, {"text": "This is one of the approaches to learn character - level embeddings without needing of any lexical hand - crafted features .", "entities": []}, {"text": "We use an RNN for representing the word with character - level information ( Lample et al . , 2016 ) .", "entities": []}, {"text": "Figure 1 shows the model architecture .", "entities": []}, {"text": "The inputs are characters extracted from a word and every character is embedded with ddimension vector .", "entities": []}, {"text": "Then , we use it as the input for a Bidirectional LSTM as character encoder , wherein every time step , a character is input to the network .", "entities": [[10, 12, "MethodName", "Bidirectional LSTM"]]}, {"text": "Consideratas the hidden states for word t. at= ( a1 1 ; a2 t ; : : : ; aV t ) where V is the character length .", "entities": []}, {"text": "The representation of the word is obtained by taking aV twhich is the last hidden state .", "entities": []}, {"text": "P r   u           \u00e9 b   a      l       oEmbeddingBiLSTM encoderCharacter   representation a8 a7 a6 a5 a4 a3 a1 a2 a3 a4 a5 a6 a7a2 a8a1", "entities": []}, {"text": "Figure 1 : Bilingual Char - RNN architecture Main Architecture : Figure 2 presents the overall architecture of the system .", "entities": []}, {"text": "The input layers receive word and character - level representations from English and Spanish pre - trained FastText word vectors and Bilingual Char - RNN .", "entities": [[17, 18, "MethodName", "FastText"]]}, {"text": "Consider Xas the input sequence : X= ( x1 ; x2 ; : : : ; x N ) where N is the length of the sequence .", "entities": []}, {"text": "We \ufb01x the word embedding parameters .", "entities": []}, {"text": "Then , we concatenate both vectors to get a richer word representation ut .", "entities": []}, {"text": "Afterwards , we pass the vectors to bidirectional LSTM .", "entities": [[7, 9, "MethodName", "bidirectional LSTM"]]}, {"text": "ut = xt\bat \u0000 !", "entities": []}, {"text": "ht=\u0000 \u0000\u0000\u0000 !", "entities": []}, {"text": "LSTM ( ut;\u0000\u0000!ht\u00001 ) , \u0000ht= \u0000\u0000\u0000 \u0000LSTM", "entities": [[0, 1, "MethodName", "LSTM"]]}, {"text": "( ut ; \u0000\u0000ht\u00001 )", "entities": []}, {"text": "O Word todos los Domingos en Westland MallChar", "entities": []}, {"text": "lang1 lang2O", "entities": []}, {"text": "I - LOC O c1 c2 c3 c4 c5 c6 h6 h5 h4 h3 h2 h1 h1 h2 h3 h4 h5 h6BiLSTM decoderoutput", "entities": [[6, 7, "DatasetName", "c3"], [7, 8, "DatasetName", "c4"]]}, {"text": "Embedding Bilingual   Char - RNNB - LOC B - TIMEFigure 2 : Main architecture ct=\u0000 !", "entities": []}, {"text": "ht\b \u0000ht where\bdenotes the concatenation operator .", "entities": []}, {"text": "Dropout is applied to the recurrent layer .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "At each time step we make a prediction for the entity of the current token .", "entities": []}, {"text": "A softmax function is used to calculate the probability distribution of all possible named - entity tags .", "entities": [[1, 2, "MethodName", "softmax"]]}, {"text": "yt = ect PT j=1ecj , where j= 1 , .. , T where ytis the probability distribution of tags at word tand T is the maximum time step .", "entities": []}, {"text": "Since there is a variable number of sequence length , we padded the sequence and applied mask when calculating cross - entropy loss function .", "entities": [[22, 23, "MetricName", "loss"]]}, {"text": "Our model does not use any gazetteer and knowledge - based information , and it can be easily adapted to another language pair .", "entities": []}, {"text": "3.4 Post - processing We found an issue during the prediction where some words are labeled with O , in between B - label andI - label tags .", "entities": []}, {"text": "Our solution is to insert I - label tag if the tag is surrounded byB - label andI - label tags with the same entity category .", "entities": []}, {"text": "Another problem we found that many I - label tags are paired with B - label in different categories .", "entities": []}, {"text": "So , we replace B - label category tag with corresponding I - label category tag .", "entities": []}, {"text": "This step improves the result of the pre-", "entities": []}, {"text": "113Table 4 : Results on ENG - SPA Dataset ( zresult(s ) from the shared task organizer ( Aguilar et al . , 2018 ) ywithout token normalization ) Model FeaturesF1 DevF1 Test BaselinezWord - 53.2802 % BiLSTMyWord + Char - RNN 46.9643 % 53.4759 % BiLSTM FastText ( eng ) 57.7174 % 59.9098 % BiLSTM FastText ( eng - spa ) 57.4177 % 60.2426 % BiLSTM + Char - RNN 65.2217 % 61.9621 % + post 65.3865 % 62.7608 % Competitorsz IIT BHU ( 1stplace ) - - 63.7628 % ( +1.0020 % ) FAIR ( 3rdplace ) - - 62.6671 % ( - 0.0937 % ) diction on the development set .", "entities": [[46, 47, "MethodName", "BiLSTM"], [47, 48, "MethodName", "FastText"], [55, 56, "MethodName", "BiLSTM"], [56, 57, "MethodName", "FastText"], [66, 67, "MethodName", "BiLSTM"]]}, {"text": "Figure 3 shows the examples .", "entities": []}, {"text": "B - PER O I - PER B - PER I - PER I - PER B - PER I - LOC B - LOC I - LOC I - LOC I - LOCFirst scenario Second scenario Figure 3 : Post - processing examples 3.5 Experimental Setup We trained our LSTM models with a hidden size of 200 .", "entities": [[50, 51, "MethodName", "LSTM"]]}, {"text": "We used batch size equals to 64 .", "entities": [[2, 4, "HyperparameterName", "batch size"]]}, {"text": "The sentences were sorted by length in descending order .", "entities": []}, {"text": "Our embedding size is 300 for word and 150 for characters .", "entities": []}, {"text": "Dropout ( Srivastava et al . , 2014 ) of 0.4 was applied to all LSTMs .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "Adam Optimizer was chosen with an initial learning rate of 0.01 .", "entities": [[0, 1, "MethodName", "Adam"], [1, 2, "HyperparameterName", "Optimizer"], [7, 9, "HyperparameterName", "learning rate"]]}, {"text": "We applied time - based decay ofp 2decay rate and stop after two consecutive epochs without improvement .", "entities": []}, {"text": "We tuned our model with the development set and evaluated our best model with the test set using harmonic mean F1 - score metric with the script provided by Aguilar et al .", "entities": [[20, 23, "MetricName", "F1 - score"]]}, {"text": "( 2018 ) .", "entities": []}, {"text": "4 Results Table 4 shows the results for ENG - SPA tweets .", "entities": []}, {"text": "Adding pre - trained word vectors and characterlevel features improved the performance .", "entities": []}, {"text": "Interestingly , our initial attempts at adding character - level features did not improve the overall performance , until we apply dropout to the Char - RNN .", "entities": []}, {"text": "The performance of the model improves signi\ufb01cantly after transfer learning with FastText word vectors whileit also reduces the number of OOV words in the development and test set .", "entities": [[8, 10, "TaskName", "transfer learning"], [11, 12, "MethodName", "FastText"]]}, {"text": "The margin between ours and \ufb01rst place model is small , approximately 1 % .", "entities": []}, {"text": "We try to use sub - words representation from Spanish FastText ( Grave et al . , 2018 ) , however , it does not improve the result since the OOV words consist of many special characters , for example , \u201c /IAtrevido / Provocativo \u201d , \u201c Twets / wek \u201d , and possibly create noisy vectors and most of them are not entity words .", "entities": [[10, 11, "MethodName", "FastText"]]}, {"text": "5 Conclusion This paper presents a bidirectional LSTM - based model with hierarchical architecture using bilingual character RNN to address the OOV words issue .", "entities": [[6, 8, "MethodName", "bidirectional LSTM"]]}, {"text": "Moreover , token replacement , token normalization , and transfer learning reduce OOV words rate even further and signi\ufb01cantly improves the performance .", "entities": [[9, 11, "TaskName", "transfer learning"]]}, {"text": "The model achieved 62.76 % F1score for English - Spanish language pair without using any gazetteer and knowledge - based information .", "entities": []}, {"text": "Acknowledgments This work is partially funded by ITS/319/16FP of the Innovation Technology Commission , HKUST 16214415 & 16248016 of Hong Kong Research Grants Council , and RDC 1718050 - 0 of EMOS.AI .", "entities": [[29, 30, "DatasetName", "0"]]}, {"text": "References Gustavo Aguilar , Fahad AlGhamdi , Victor Soto , Mona Diab , Julia Hirschberg , and Thamar Solorio .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Overview of the CALCS 2018 Shared Task : Named", "entities": []}, {"text": "114Entity Recognition on Code - switched Data .", "entities": []}, {"text": "In Proceedings of the Third Workshop on Computational Approaches to Linguistic Code - Switching , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Gustavo Aguilar , Suraj Maharjan , Adrian Pastor L \u00b4 opez Monroy , and Thamar Solorio .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A multi - task approach for named entity recognition in social media data .", "entities": [[6, 9, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the 3rd Workshop on Noisy User - generated Text , pages 148\u2013153 .", "entities": []}, {"text": "Jason PC Chiu and Eric Nichols .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Named entity recognition with bidirectional lstm - cnns .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 6, "MethodName", "bidirectional lstm"]]}, {"text": "Transactions of the Association for Computational Linguistics , 4:357\u2013370 .", "entities": []}, {"text": "Zhilin Yang Ruslan Salakhutdinov William Cohen .", "entities": [[2, 3, "DatasetName", "Ruslan"]]}, {"text": "Multi - task cross - lingual sequence tagging from scratch .", "entities": []}, {"text": "Ronan Collobert , Jason Weston , L \u00b4 eon Bottou , Michael Karlen , Koray Kavukcuoglu , and Pavel Kuksa . 2011 .", "entities": []}, {"text": "Natural language processing ( almost ) from scratch .", "entities": []}, {"text": "Journal of Machine Learning Research , 12(Aug):2493\u20132537 .", "entities": []}, {"text": "Chris Dyer , Miguel Ballesteros , Wang Ling , Austin Matthews , and Noah A Smith .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Transitionbased dependency parsing with stack long shortterm memory .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , volume 1 , pages 334\u2013343 .", "entities": []}, {"text": "Edouard Grave , Piotr Bojanowski , Prakhar Gupta , Armand Joulin , and Tomas Mikolov .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning word vectors for 157 languages .", "entities": []}, {"text": "In Proceedings of the International Conference on Language Resources and Evaluation ( LREC 2018 ) .", "entities": []}, {"text": "Zhiheng Huang , Wei Xu , and Kai Yu . 2015 .", "entities": []}, {"text": "Bidirectional lstm - crf models for sequence tagging .", "entities": [[0, 2, "MethodName", "Bidirectional lstm"], [3, 4, "MethodName", "crf"]]}, {"text": "arXiv preprint arXiv:1508.01991 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "John Lafferty , Andrew McCallum , and Fernando CN Pereira . 2001 .", "entities": []}, {"text": "Conditional random \ufb01elds : Probabilistic models for segmenting and labeling sequence data .", "entities": []}, {"text": "Guillaume Lample , Miguel Ballesteros , Sandeep Subramanian , Kazuya Kawakami , and Chris Dyer . 2016 .", "entities": []}, {"text": "Neural architectures for named entity recognition .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}, {"text": "InProceedings of NAACL - HLT , pages 260\u2013270 .", "entities": []}, {"text": "Nut Limsopatham and Nigel Henry Collier .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Bidirectional lstm for named entity recognition in twitter messages .", "entities": [[0, 2, "MethodName", "Bidirectional lstm"], [3, 6, "TaskName", "named entity recognition"]]}, {"text": "Wang Ling , Chris Dyer , Alan W Black , Isabel Trancoso , Ramon Fermandez , Silvio Amir , Luis Marujo , and Tiago Luis . 2015 .", "entities": []}, {"text": "Finding function in form : Compositional character models for open vocabulary word representation .", "entities": []}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1520\u20131530.Tomas Mikolov , Edouard Grave , Piotr Bojanowski , Christian Puhrsch , and Armand Joulin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Advances in pre - training distributed word representations .", "entities": []}, {"text": "In Proceedings of the International Conference on Language Resources and Evaluation ( LREC 2018 ) .", "entities": []}, {"text": "Yanyao Shen , Hyokun Yun , Zachary Lipton , Yakov Kronrod , and Animashree Anandkumar . 2017 .", "entities": []}, {"text": "Deep active learning for named entity recognition .", "entities": [[1, 3, "TaskName", "active learning"], [4, 7, "TaskName", "named entity recognition"]]}, {"text": "InProceedings of the 2nd Workshop on Representation Learning for NLP , pages 252\u2013256 .", "entities": [[6, 8, "TaskName", "Representation Learning"]]}, {"text": "Nitish Srivastava , Geoffrey Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov .", "entities": [[13, 14, "DatasetName", "Ruslan"]]}, {"text": "2014 .", "entities": []}, {"text": "Dropout : A simple way to prevent neural networks from over\ufb01tting .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "The Journal of Machine Learning Research , 15(1):1929\u20131958 .", "entities": []}, {"text": "Martin Sundermeyer , Ralf Schl \u00a8uter , and Hermann Ney . 2012 .", "entities": []}, {"text": "Lstm neural networks for language modeling .", "entities": [[0, 1, "MethodName", "Lstm"]]}, {"text": "In Thirteenth Annual Conference of the International Speech Communication Association .", "entities": []}]