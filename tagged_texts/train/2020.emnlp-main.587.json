[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 7241\u20137251 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics7241Cross - lingual Spoken Language Understanding with Regularized Representation Alignment Zihan Liu , Genta Indra Winata , Peng Xu , Zhaojiang Lin , Pascale Fung Center for Arti\ufb01cial Intelligence Research ( CAiRE ) Department of Electronic and Computer Engineering The Hong Kong University of Science and Technology , Clear Water Bay , Hong Kong zihan.liu@connect.ust.hk , pascale@ece.ust.hk Abstract Despite the promising results of current crosslingual models for spoken language understanding systems , they still suffer from imperfect cross - lingual representation alignments between the source and target languages , which makes the performance sub - optimal .", "entities": [[7, 10, "TaskName", "Spoken Language Understanding"], [71, 74, "TaskName", "spoken language understanding"]]}, {"text": "To cope with this issue , we propose a regularization approach to further align word - level and sentence - level representations across languages without any external resource .", "entities": []}, {"text": "First , we regularize the representation of user utterances based on their corresponding labels .", "entities": []}, {"text": "Second , we regularize the latent variable model ( Liu et al . , 2019a ) by leveraging adversarial training to disentangle the latent variables .", "entities": []}, {"text": "Experiments on the cross - lingual spoken language understanding task show that our model outperforms current state - of - the - art methods in both few - shot and zero - shot scenarios , and our model , trained on a few - shot setting with only 3 % of the target language training data , achieves comparable performance to the supervised training with all the training data.1 1 Introduction Data - driven neural - based supervised training approaches have shown effectiveness in spoken language understanding ( SLU ) systems ( Goo et al . , 2018 ; Chen et al . , 2019 ; Haihong et al . , 2019 ) .", "entities": [[6, 9, "TaskName", "spoken language understanding"], [84, 87, "TaskName", "spoken language understanding"]]}, {"text": "However , collecting large amounts of high - quality training data is not only expensive but also timeconsuming , which makes these approaches not scalable to low - resource languages due to the scarcity of training data .", "entities": []}, {"text": "Cross - lingual adaptation has naturally arisen to cope with this issue , which leverages the training data in rich - resource source languages and minimizes the requirement of training data in low - resource target languages .", "entities": []}, {"text": "1The code is available in https://github.com/ zliucr / crosslingual - slu .", "entities": []}, {"text": "2020/5/31 , 9:36 PMMathcha Page 1 of 2https://www.mathcha.io/editor Llr wawbUtterance Enc.sasbLabel Enc.u , ucos(ab)l , lcos(ab)minimizeutterance distlabel dist Utterance Enc .", "entities": []}, {"text": "Label Enc.waUtterance Enc./u1D707Si / u1D70ESiAttentionLatent Variable / u1D707I / u1D70EILatent VariableIntent", "entities": []}, {"text": "DetectionFCSlot Fillinguniformone - hotadversarialtrainingAttentionAttentionAttentionAttentionLSLILlvmLfcuaublalbua Do    you   have   thursday 's   weather        report", "entities": []}, {"text": "Cu\u00e1l   es   el      informe meteorol\u00f3gico ma\u00f1anaOOOB - timeB - weatherI - weatherOOOB - timeB - weatherI - weatherUtter .", "entities": []}, {"text": "Utter .", "entities": []}, {"text": "Intent SlotsFind WeatherIntent SlotsFind WeatherEnglish Training sampleSpanish Test sampleFigure 1 : Illustration of cross - lingual spoken language understanding systems , where English is the source language and Spanish is the target language .", "entities": [[16, 19, "TaskName", "spoken language understanding"]]}, {"text": "In general , there are two challenges in crosslingual adaptation .", "entities": []}, {"text": "First , the imperfect alignment of word - level representations between the source and target language limits the adaptation performance .", "entities": []}, {"text": "Second , even though we assume that the word - level alignment is perfect , the sentence - level alignment is still imperfect owing to grammatical and syntactical variances across languages .", "entities": []}, {"text": "Therefore , we emphasize that cross - lingual methods should focus on the alignments of word - level and sentence - level representations , and increase the robustness for inherent imperfect alignments .", "entities": []}, {"text": "In this paper , we concentrate on the cross - lingual SLU task ( as illustrated in Figure 1 ) , and we consider both few - shot and zero - shot scenarios .", "entities": []}, {"text": "To improve the quality of cross - lingual alignment , we \ufb01rst propose a LabelRegularization ( LR ) method , which utilizes the slot label sequences to regularize the utterance representations .", "entities": []}, {"text": "We hypothesize that if the slot label sequences of user utterances are close to each other , these user utterances should have similar meanings .", "entities": []}, {"text": "Hence , we regularize the distance of utterance representations based on the corresponding representations of label sequences to further improve the cross - lingual alignments .", "entities": []}, {"text": "Then , we extend the latent variable model ( LVM )", "entities": []}, {"text": "7242proposed by Liu et al . ( 2019a ) .", "entities": []}, {"text": "The LVM generates a Gaussian distribution instead of a feature vector for each token , which improves the adaptation robustness .", "entities": []}, {"text": "However , there are no additional constraints on generating distributions , making the latent variables easily entangled for different slot labels .", "entities": []}, {"text": "To handle this issue , we leverage Adversarial training to regularize the LVM ( ALVM ) .", "entities": []}, {"text": "We train a linear layer to \ufb01t latent variables to a uniform distribution over slot types .", "entities": [[3, 5, "MethodName", "linear layer"]]}, {"text": "Then , we optimize the latent variables to fool the trained linear layer to output the correct slot type ( one hot vector ) .", "entities": [[11, 13, "MethodName", "linear layer"]]}, {"text": "In this way , latent variables of different slot types are encouraged to disentangle from each other , leading to a better alignment of cross - lingual representations .", "entities": []}, {"text": "The contributions of our work are summarized as follows : \u000fWe propose LR and ALVM to further improve the alignment of cross - lingual representations , which do not require any external resources .", "entities": []}, {"text": "\u000fOur model outperforms the previous state - ofthe - art model in both zero - shot and few - shot scenarios on the cross - lingual SLU task .", "entities": []}, {"text": "\u000fExtensive analysis and visualizations are made to illustrate the effectiveness of our approaches .", "entities": []}, {"text": "2 Related Work Cross - lingual Transfer Learning Cross - lingual transfer learning is able to circumvent the requirement of enormous training data by leveraging the learned knowledge in the source language and learning inter - connections between the source and the target language .", "entities": [[3, 7, "TaskName", "Cross - lingual Transfer"], [8, 12, "TaskName", "Cross - lingual transfer"]]}, {"text": "Artetxe et", "entities": []}, {"text": "al . ( 2017 ) and Conneau et al .", "entities": []}, {"text": "( 2018 ) conducted cross - lingual word embedding mapping with zero or very few supervision signals .", "entities": []}, {"text": "Recently , pre - training cross - lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks ( e.g. , natural language inference ) ( Conneau and Lample , 2019 ; Devlin et", "entities": [[31, 34, "TaskName", "natural language inference"]]}, {"text": "al . , 2019 ; Pires et al . , 2019 ; Huang et al . , 2019 ) .", "entities": []}, {"text": "Additionally , many cross - lingual transfer algorithms have been proposed to solve speci\ufb01c cross - lingual tasks , for example , named entity recognition ( Xie et al . , 2018 ; Mayhew et al . , 2017 ; Liu et al . , 2020a ) , part of speech tagging ( Kim et al . , 2017 ; Zhang et al . , 2016 ) , entity linking ( Zhang et al . , 2013 ; Sil et al . , 2018 ; Upadhyay et al . , 2018b ) , personalizedconversations ( Lin et al . , 2020 ) , and dialog systems ( Upadhyay et al . , 2018a ; Chen et al . , 2018 ) .", "entities": [[3, 7, "TaskName", "cross - lingual transfer"], [22, 25, "TaskName", "named entity recognition"], [68, 70, "TaskName", "entity linking"]]}, {"text": "Cross - lingual Task - oriented Dialog Systems Deploying task - oriented dialogue systems in lowresource domains ( Bapna et al . , 2017 ; Wu et al . , 2019 ; Liu et al . , 2020b ) or languages ( Chen et al . , 2018 ; Liu et al . , 2019a , b ) , where the number of training of samples is limited , is a challenging task .", "entities": [[9, 14, "TaskName", "task - oriented dialogue systems"]]}, {"text": "Mrk\u02c7si\u00b4c et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2017 ) expanded Wizard of Oz ( WOZ ) into multilingual WOZ by annotating two additional languages .", "entities": []}, {"text": "Schuster et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe ( Yu et al . , 2018 ) to align the representations across languages .", "entities": [[16, 17, "MethodName", "CoVe"]]}, {"text": "Chen et al .", "entities": []}, {"text": "( 2018 ) proposed a teacherstudent framework based on a bilingual dictionary or bilingual corpus for building cross - lingual dialog state tracking .", "entities": []}, {"text": "Instead of highly relying on extensive bilingual resources , Qin et al .", "entities": []}, {"text": "( 2020 ) introduced a data augmentation framework to generate multilingual code - switching data for cross - lingual tasks including the SLU task .", "entities": [[5, 7, "TaskName", "data augmentation"]]}, {"text": "Liu et al .", "entities": []}, {"text": "( 2019b ) leveraged a mixed language training framework for cross - lingual task - oriented dialogue systems .", "entities": [[13, 18, "TaskName", "task - oriented dialogue systems"]]}, {"text": "And Liu et al .", "entities": []}, {"text": "( 2019a ) proposed to re\ufb01ne the crosslingual word embeddings by using very few word pairs , and introduced a latent variable model to improve the robustness of zero - shot cross - lingual SLU .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "Nevertheless , there still exists improvement space for the cross - lingual alignment .", "entities": []}, {"text": "In this paper , we propose to further align the cross - lingual representations so as to boost the performance of cross - lingual SLU systems .", "entities": []}, {"text": "3 Methodology Our model architecture and proposed methods are depicted in Figure 2 , and combine label regularization ( LR ) and the adversarial latent variable model ( ALVM ) to conduct the intent detection and slot \ufb01lling .", "entities": [[33, 35, "TaskName", "intent detection"]]}, {"text": "In the few - shot setting , the input user utterances are in both the source and target languages , while in the zero - shot setting , the user utterances are only in the source language .", "entities": []}, {"text": "Note that both the source and target languages contain only one language .", "entities": []}, {"text": "3.1 Label Regularization 3.1.1 Motivation Intuitively , when the slot label sequences are similar , we expect the corresponding representations of user utterances across languages to be similar .", "entities": []}, {"text": "7243 2020/5/30 , 4:24 PMMathcha Page 1 of 2https://www.mathcha.io/editor Llr wawbUtterance Enc.sasbLabel Enc.u , ucos(ab)l , lcos(ab)minimizeutterance distlabel dist Utterance Enc .", "entities": []}, {"text": "Label Enc.waUtterance Enc./u1D707Si / u1D70ESiAttentionLatent Variable / u1D707I / u1D70EILatent VariableIntent DetectionFCSlot Fillinguniformone - hotadversarialtrainingAttentionAttentionAttentionAttentionLSLILlvmLfcuaublalbuaFigure 2 : Left : Illustration of label regularization ( LR ) .", "entities": []}, {"text": "Right :", "entities": []}, {"text": "The model architecture with adversarial latent variable model ( ALVM ) , where FCconsists of a linear layer and a softmax function .", "entities": [[16, 18, "MethodName", "linear layer"], [20, 21, "MethodName", "softmax"]]}, {"text": "For example , when the slot label sequences contain the weather slot and the location slot , the user utterances should be asking for the weather forecast somewhere .", "entities": []}, {"text": "However , the representations of utterances across languages can not always meet these requirements because of the inherent imperfect alignments in word - level and sentence - level representations .", "entities": []}, {"text": "Therefore , we propose to leverage existing slot label sequences in the training data to regularize the distance of utterance representations .", "entities": []}, {"text": "When a few training samples are available in the target language ( i.e. , few - shot setting ) , we regularize the distance of utterance representations between the source and target languages based on their slot labels .", "entities": []}, {"text": "Given this regularization , the model explicitly learns to further align the sentencelevel utterance representations across languages so as to satisfy the constraints .", "entities": []}, {"text": "Additionally , it can also implicitly align the word - level BiLSTM hidden states across languages because sentence - level representations are produced based on them .", "entities": [[11, 12, "MethodName", "BiLSTM"]]}, {"text": "When zero training samples are available in the target language ( i.e. , zero - shot setting ) , we regularize the utterance representations in the source language .", "entities": []}, {"text": "It can help better distinguish the utterance representations and cluster similar utterance representations based on the slot labels , which increases the generalization ability in the target language .", "entities": []}, {"text": "3.1.2 Implementation Details Figure 2 ( Left ) illustrates an utterance encoder and a label encoder that generate the representations for utterances and labels , respectively .", "entities": []}, {"text": "We denote the user utterance as w= [ w1;w2;:::;w n ] , wherenis the length of the utterance .", "entities": []}, {"text": "Similarly , we represent the slot label sequences as s=", "entities": []}, {"text": "[ s1;s2;:::;s", "entities": []}, {"text": "n ] .", "entities": []}, {"text": "We combine abidirectional LSTM ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) and an attention layer ( Felbo et al . , 2017 ) to encode and produce the representations for user utterances and slot label sequences .", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "BiLSTM"]]}, {"text": "The representation generation process is de\ufb01ned as follows :", "entities": []}, {"text": "[ hw 1;hw 2;:::;hw w ] = BiLSTM utter(E(w));(1 ) [ hs 1;hs 2;:::;hs n ] = BiLSTMlabel(E(s ) ) ; ( 2 ) mw i", "entities": [[7, 8, "MethodName", "BiLSTM"]]}, {"text": "= hw ivw ;", "entities": []}, {"text": "w i = exp(mw i)Pn t=1exp(mw t ) ; ( 3 ) ms i = hs ivs ; \u000b s i = exp(ms i)Pn", "entities": []}, {"text": "t=1exp(ms t ) ; ( 4 ) u = nX i=1 \u000b w ihw i ; l = nX i=1 \u000b s ihs", "entities": []}, {"text": "i ; ( 5 ) where the superscript wandsrepresents utterance and label , respectively , vis a trainable weight vector in the attention layer , \u000b iis the attention score for each token i , Edenotes the embedding layers for utterances and label sequences , and uandl denotes the representation of utterance wand slot label s , respectively .", "entities": []}, {"text": "In each iteration of the training phase , we randomly select two samples for the label regularization .", "entities": []}, {"text": "As illustrated in Figure 2 ( Left ) , we \ufb01rst calculate the cosine similarity of two utterance representations uaandub , and the cosine similarity of two label representations laandlb .", "entities": []}, {"text": "Then , we minimize the distance of these two cosine similarities .", "entities": []}, {"text": "The objective functions can be described as follows : cos(ua;ub )", "entities": []}, {"text": "= ua\u0001ub jjuajjjjubjj ; ( 6 )", "entities": []}, {"text": "7244cos(la;lb )", "entities": []}, {"text": "= la\u0001lb jjlajjjjlbjj ; ( 7 ) Llr = X a;bMSE(cos(ua;ub);cos(la;lb));(8 ) where the superscript lrdenotes label regularization , and MSE represents mean square error .", "entities": [[20, 21, "MetricName", "MSE"]]}, {"text": "In the zero - shot setting , both samples uaandubcome from the source language .", "entities": []}, {"text": "While in the few - shot setting , one sample comes from the source language and the other one comes from the target language .", "entities": []}, {"text": "Since the features of labels and utterances are in different vector spaces , we choose not to share the parameters of their encoders .", "entities": []}, {"text": "During training , it is easy to produce expressive representations for user utterances due to the large training samples , but it is dif\ufb01cult for label sequences since the objective function Llris the only supervision .", "entities": []}, {"text": "This supervision is weak at the beginning of the training since utterance representations are not suf\ufb01ciently expressive , which leads to the label regularization approach not being stable and effective .", "entities": []}, {"text": "To ensure the representations for slot label sequences are meaningful , we conduct pre - training for the label sequence encoder .", "entities": []}, {"text": "3.1.3 Label Sequence Encoder Pre - training We leverage the large amount of source language training data to pre - train the label sequence encoder .", "entities": []}, {"text": "Concretely , we use the model architecture illustrated in Figure 2 to train the SLU system in the source language , and at the same time , we optimize the label sequence encoder based on the objective functionLlrin Eq ( 8) .", "entities": []}, {"text": "The label sequence encoder learns to generate meaningful label sequence representations that differ based on their similarities since the extensive source language training samples ensure the high quality of the utterance encoder .", "entities": []}, {"text": "3.2 Adversarial Latent Variable Model", "entities": []}, {"text": "In this section , we \ufb01rst give an introduction to the latent variable model ( LVM ) ( Liu et al . , 2019a ) , and then we describe how we incorporate the adversarial training into the LVM .", "entities": []}, {"text": "3.2.1 Latent Variable Model Point estimation in the cross - lingual adaptation is vulnerable due to the imperfect alignments across languages .", "entities": []}, {"text": "Hence , as illustrated in Figure 2 ( Right ) , the LVM generates a Gaussian distribution with mean\u0016and variance \u001bfor both word - level andsentence - level representations instead of a feature vector , which eventually improves the robustness of the model \u2019s cross - lingual adaptation ability .", "entities": []}, {"text": "The LVM can be formulated as \" \u0016S i log(\u001bS i)2 # = WS lhi ; \" \u0016I log(\u001bI)2 # = WI lu ; ( 9 ) zS i\u0018qS i(zjhi ) ; zI\u0018qI(zju ) ; ( 10 ) pS i(sijzS i )", "entities": []}, {"text": "= Softmax ( WS pzS i ) ; ( 11 ) pI(IjzI )", "entities": [[1, 2, "MethodName", "Softmax"]]}, {"text": "= Softmax ( WI pzI ) ; ( 12 ) whereWS landWI lare trainable parameters to generate the mean and variance for word - level hidden states hiand sentence - level representationsr , respectively , from user utterances .", "entities": [[1, 2, "MethodName", "Softmax"]]}, {"text": "qS i\u0018 N(\u0016S i;(\u001bS i)2I)andqI\u0018N ( \u0016I;(\u001bI)2I)are the generated Gaussian distributions , which latent variableszS tandzIare sampled from , and pS iandpI is the predictions for the slot of the ithtoken and the intent of the utterance , respectively .", "entities": []}, {"text": "During training , all the sampled points from the same generated distribution will be trained to predict the same slot label , which makes the adaptation more robust .", "entities": []}, {"text": "In the inference time , the true mean \u0016S iand\u0016Iis used to replace zS iandzI , respectively , to make the prediction deterministic .", "entities": []}, {"text": "3.2.2 Adversarial Training Since there are no constraints enforced on the latent Gaussian distribution during training , the latent distributions of different slot types are likely to be close to each other .", "entities": []}, {"text": "Hence , the distributions for the same slot type in different user utterances or languages might not be clustered well , which could hurt the cross - lingual alignment and prevent the model from distinguishing slot types when adapting to the target language .", "entities": []}, {"text": "To improve the cross - lingual alignment of latent variables , we propose to make the latent variables of different slot types more distinguishable by adding adversarial training to the LVM .", "entities": []}, {"text": "As illustrated in Figure 2 ( Right ) , we train a fully connected layer to \ufb01t latent variables into a uniform distribution over slot types .", "entities": []}, {"text": "At the same time , the latent variables are regularized to fool the trained fully connected layer by predicting the correct slot type .", "entities": []}, {"text": "In this way , the latent variables are trained to be more recognizable .", "entities": []}, {"text": "In other words , the generated distributions for different slot types are more likely to repel each other , and for the same slot type are more likely to be close to each other ,", "entities": []}, {"text": "which", "entities": []}, {"text": "7245leads to a more robust cross - lingual adaptation .", "entities": []}, {"text": "We denote the size of the whole training data as Jand the length for data sample jasjYjj .", "entities": []}, {"text": "Note that in the few - shot setting , Jincludes the number of data samples in the target language .", "entities": []}, {"text": "The process of adversarial training can be described as follows : pjk = FC(zS jk ) ; ( 13 ) Lfc = JX j=1jYjjX k=1MSE(pjk;U ) ; ( 14 )", "entities": []}, {"text": "Llvm = JX j=1jYjjX k=1MSE(pjk;yS jk ) ; ( 15 ) whereFCconsists of a linear layer and a Softmax function , and zS jkandpjkis the latent variable and generated distribution , respectively , for the kthtoken in thejthutterance , MSE represents the mean square error , Urepresents the uniform distribution , andyS jkrepresents the slot label .", "entities": [[14, 16, "MethodName", "linear layer"], [18, 19, "MethodName", "Softmax"], [39, 40, "MetricName", "MSE"]]}, {"text": "The slot label is a one - hot vector where the value for the correct slot type is one and zero otherwise .", "entities": []}, {"text": "We optimize Lfcto train onlyFCto \ufb01t a uniform distribution , andLlvmis optimized to constrain the LVM to generate more distinguishable distributions for slot predictions .", "entities": []}, {"text": "Different from the well - known adversarial training ( Goodfellow et", "entities": []}, {"text": "al . , 2014 ) where the discriminator is to distinguish the classes , and the generator is to make the features not distinguishable , in our approach , the FClayer , acting as the discriminator , is trained to generate uniform distribution , and the generator is regularized to make latent variables distinguishable by slot types .", "entities": []}, {"text": "3.3 Optimization The objective functions for the slot \ufb01lling and intent detection tasks are illustrated as follows : LS = JX j=1jYjjX k=1\u0000log(pS jk\u0001(yS jk ) > ) ; ( 16 ) LI = JX j=1\u0000log(pI j\u0001(yI j ) > ) ; ( 17 )", "entities": [[10, 12, "TaskName", "intent detection"]]}, {"text": "wherepS jkandyS jkis the prediction and label , respectively , for the slot of the kthtoken in the jth utterance , and pI jandyI jis the intent prediction and label , respectively , for the jthutterance .", "entities": []}, {"text": "The optimization for our model is to minimize the following objective function :", "entities": []}, {"text": "L = LS+LI+Llr+ \u000b Lfc+ \f Llvm;(18 ) # Utterance English Spanish Thai Train 30,521 3,617 2,156 Validation 4,181 1,983 1,235 Test 8,621 3,043 1,692 Table 1 : Number of utterances for the multilingual SLU dataset .", "entities": []}, {"text": "English is the source language , and Spanish and Thai are the target languages .", "entities": []}, {"text": "where \u000b and \f are hyper - parameters , Lfconly optimizes the parameters in FC , andLlvmoptimizes all the model parameters excluding FC .", "entities": []}, {"text": "4 Experiments 4.1 Dataset We conduct our experiments on the multilingual spoken language understanding ( SLU ) dataset proposed by Schuster et al .", "entities": [[11, 14, "TaskName", "spoken language understanding"]]}, {"text": "( 2019 ) , which contains English , Spanish , and Thai across the weather , reminder , and alarm domains .", "entities": []}, {"text": "The corpus includes 12 intent types and 11 slot types , and the data statistics are shown in Table 1", "entities": []}, {"text": ". 4.2 Training Details", "entities": []}, {"text": "The utterance encoder is a 2 - layer BiLSTM with a hidden size of 250 and dropout rate of 0.1 , and the size of the mean and variance in the latent variable model is 150 .", "entities": [[8, 9, "MethodName", "BiLSTM"]]}, {"text": "The label encoder is a 1layer BiLSTM with a hidden size of 150 , and 100dimensional embeddings for label types .", "entities": [[6, 7, "MethodName", "BiLSTM"]]}, {"text": "We use the Adam optimizer with a learning rate of 0.001 .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [7, 9, "HyperparameterName", "learning rate"]]}, {"text": "We use accuracy to evaluate the performance of intent detection and BIO - based f1 - score to evaluate the performance of slot \ufb01lling .", "entities": [[2, 3, "MetricName", "accuracy"], [8, 10, "TaskName", "intent detection"], [14, 17, "MetricName", "f1 - score"]]}, {"text": "For the adversarial training , we realize that the latent variable model is not able to make slot types recognizable if the FCis too strong .", "entities": []}, {"text": "Hence , we decide to \ufb01rst learn a good initialization for FCby setting both \u000b and \f  parameters in Eq ( 18 ) as 1 in the \ufb01rst two training epochs , and then we gradually decrease the value of \u000b .", "entities": []}, {"text": "We use the re\ufb01ned cross - lingual word embeddings in Liu et al .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "( 2019a)2to initialize the crosslingual word embeddings in our models and let them not be trainable .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "We use the delexicalization ( delex . )", "entities": []}, {"text": "in Liu et al . ( 2019a ) , which replaces the tokens that represent numbers , time , and duration with special tokens .", "entities": []}, {"text": "We use 36training samples in Spanish and 21training samples in Thai on the 1 % few - shot setting , and 108training samples in Spanish and 64training samples in Thai on the 3 % 2Available at https://github.com/zliucr/Crosslingual-NLU", "entities": []}, {"text": "7246ModelSpanish Thai Intent Acc .", "entities": [[3, 4, "MetricName", "Acc"]]}, {"text": "Slot F1 Intent Acc .", "entities": [[1, 2, "MetricName", "F1"], [3, 4, "MetricName", "Acc"]]}, {"text": "Slot F1 Few - shot settings 1%-shot 3 % -shot 1%-shot 3 % -shot 1%-shot 3 % -shot", "entities": [[1, 2, "MetricName", "F1"]]}, {"text": "1%-shot 3 % -shot BiLSTM - CRF 93.03 93.63 75.70 82.60 81.30 87.23 52.57 66.04 + LR 93.08 95.04 77.04 84.09 84.04 89.20 57.40 67.45 BiLSTM - LVM 92.86 94.46 75.19 82.64 83.51 89.08 55.08 67.26 + LR 93.79 95.16 76.96 83.54 86.33 90.80 59.02 70.26 + ALVM 93.78 95.27 78.35 83.69 85.40 90.70 59.75 69.38 + LR & ALVM 93.82 95.20 78.46 84.19 87.43 90.96 61.44 70.88 + LR & ALVM & delex .", "entities": [[4, 5, "MethodName", "BiLSTM"], [6, 7, "MethodName", "CRF"], [25, 26, "MethodName", "BiLSTM"]]}, {"text": "94.71 95.62 80.82 85.18 87.67 91.61 62.01 72.39 XL - SLU 92.70 94.96 77.67 82.22 84.04 89.59 55.57 67.56 M - BERT 92.77 95.56 80.15 84.50 83.87 89.19 58.18 67.88 Zero - shot settings XL - SLU 90.20 65.79 73.43 32.24 + LR 91.51 71.55 74.86 32.86 + ALVM 91.48 71.21 74.35 32.97 + LR & ALVM 92.31 72.49 75.77 33.28 MLT 86.54 74.43 70.57 28.47 CoSDA - ML 94.80 80.40 76.80 37.3 M - BERT 74.91 67.55 42.97 10.68 Multi .", "entities": [[21, 22, "MethodName", "BERT"], [75, 76, "MethodName", "BERT"]]}, {"text": "CoVe 53.34 22.50 66.35 32.52 + Auto - encoder 53.89 19.25 70.70 35.62 Translate Train 85.39 72.89 95.89 55.43 All - shot settings Targety96.08 86.03 92.73 85.52 Source & Targetz98.06 87.65 95.58 88.11 Table 2 : Cross - lingual SLU results ( averaged over three runs).ydenotes supervised training on all the target language training samples.zdenotes supervised training on both the source and target language datasets .", "entities": [[0, 1, "MethodName", "CoVe"]]}, {"text": "The bold numbers denote the best results in the few - shot or zero - shot settings .", "entities": []}, {"text": "The underlined numbers represent that the results are comparable ( distances are within 1 % ) to the all - shot experiment with all the target language training samples .", "entities": []}, {"text": "The results of Multi .", "entities": []}, {"text": "CoVe and Multi .", "entities": [[0, 1, "MethodName", "CoVe"]]}, {"text": "CoVe + Auto - encoder are taken from Schuster et al .", "entities": [[0, 1, "MethodName", "CoVe"]]}, {"text": "( 2019 ) , and the results of XL - SLU in the zero - shot settings are taken from Liu et al .", "entities": []}, {"text": "( 2019a )", "entities": []}, {"text": ".", "entities": []}, {"text": "few - shot setting .", "entities": []}, {"text": "Our models are trained on GTX 1080 Ti .", "entities": []}, {"text": "The number of parameters for our models is around 5 million .", "entities": [[1, 4, "HyperparameterName", "number of parameters"]]}, {"text": "4.3 Baselines We compare our model to the following baselines .", "entities": []}, {"text": "BiLSTM - CRF This is the same cross - lingual SLU model structure as Schuster et al .", "entities": [[0, 1, "MethodName", "BiLSTM"], [2, 3, "MethodName", "CRF"]]}, {"text": "( 2019 ) .", "entities": []}, {"text": "BiLSTM - LVM We replace the conditional random \ufb01eld ( CRF ) in BiLSTM - CRF with the LVM proposed in Liu et al .", "entities": [[0, 1, "MethodName", "BiLSTM"], [10, 11, "MethodName", "CRF"], [13, 14, "MethodName", "BiLSTM"], [15, 16, "MethodName", "CRF"]]}, {"text": "( 2019a )", "entities": []}, {"text": ".", "entities": []}, {"text": "Multi .", "entities": []}, {"text": "CoVe Multilingual CoVe ( Yu et al . , 2018 ) is a bidirectional machine translation system that tends to encode phrases with similar meanings into similar vector spaces across languages .", "entities": [[0, 1, "MethodName", "CoVe"], [2, 3, "MethodName", "CoVe"], [14, 16, "TaskName", "machine translation"]]}, {"text": "Schuster et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) used it for the cross - lingual SLU task .", "entities": []}, {"text": "Multi .", "entities": []}, {"text": "CoVe w/ auto - encoder Based on Multilingual CoVe , Schuster et al .", "entities": [[0, 1, "MethodName", "CoVe"], [8, 9, "MethodName", "CoVe"]]}, {"text": "( 2019 ) added an autoencoder objective so as to produce better - alignedrepresentations for semantically similar sentences across languages .", "entities": [[5, 6, "MethodName", "autoencoder"]]}, {"text": "Multilingual BERT ( M - BERT )", "entities": [[1, 2, "MethodName", "BERT"], [5, 6, "MethodName", "BERT"]]}, {"text": "It is a single language model pre - trained from monolingual corpora in 104 languages ( Devlin et al . , 2019 ) , which is surprisingly good at cross - lingual model transfer .", "entities": []}, {"text": "Mixed Language Training ( MLT ) Liu et al .", "entities": []}, {"text": "( 2019b ) utilized keyword pairs to generate mixed language sentences for training cross - lingual taskoriented dialogue systems , which achieves promising zero - shot transfer ability .", "entities": []}, {"text": "CoSDA - ML Qin et al .", "entities": []}, {"text": "( 2020 ) proposed a multilingual code - switching data augmentation framework to enhance the cross - lingual systems based on M - BERT ( Devlin et al . , 2019 ) .", "entities": [[9, 11, "TaskName", "data augmentation"], [23, 24, "MethodName", "BERT"]]}, {"text": "It is a concurrent work of this paper .", "entities": []}, {"text": "XL - SLU It is a previous state - of - the - art model in the zero - shot cross - lingual SLU task , which combines Gaussian noise , cross - lingual embeddings", "entities": []}, {"text": "7247re\ufb01nement , and the LVM ( Liu et al . , 2019a ) .", "entities": []}, {"text": "Translate Train Schuster", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2019 ) trained a supervised machine translation system to translate English data into the target language , and then trained the model on the translated dataset .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "All - shot Settings We train the BiLSTM - CRF model ( Lample et al . , 2016 ) on all the target language training samples , and on both the source and target language training set .", "entities": [[7, 8, "MethodName", "BiLSTM"], [9, 10, "MethodName", "CRF"]]}, {"text": "5 Results & Discussion 5.1 Few - shot Setting Quantitative Analysis The few - shot results are illustrated in Table 2 , from which we can clearly see consistent improvements made by label regularization and adversarial training .", "entities": []}, {"text": "For example , on the 1 % few - shot setting , our model improves on BiLSTM - LVM in terms of accuracy / f1 - score by 1.85%/1.16 % in Spanish , and by 4.16%/6.93 % in Thai .", "entities": [[16, 17, "MethodName", "BiLSTM"], [22, 23, "MetricName", "accuracy"], [24, 27, "MetricName", "f1 - score"]]}, {"text": "Our model also surpasses a strong baseline , M - BERT , while our model based on BiLSTM has many fewer parameters compared to M - BERT .", "entities": [[10, 11, "MethodName", "BERT"], [17, 18, "MethodName", "BiLSTM"], [26, 27, "MethodName", "BERT"]]}, {"text": "For example , on the 1 % few - shot setting , our model improves on M - BERT in terms of accuracy / f1 - score by 3.80%/3.83 % in Thai .", "entities": [[18, 19, "MethodName", "BERT"], [22, 23, "MetricName", "accuracy"], [24, 27, "MetricName", "f1 - score"]]}, {"text": "Instead of generating a feature point like CRF , the LVM creates a more robust cross - lingual adaptation by generating a distribution for the intent or each token in the utterance .", "entities": [[7, 8, "MethodName", "CRF"]]}, {"text": "However , distributions generated by the LVM for the same slot type across languages might not be suf\ufb01ciently close .", "entities": []}, {"text": "Incorporating adversarial training into the LVM alleviates this problem by regularizing the latent variables and making them more distinguishable .", "entities": []}, {"text": "This improves the performance in both intent detection ( a sentence - level task ) and slot \ufb01lling ( a word - level task ) by 0.92%/3.16 % in Spanish and by 1.89%/4.67 % in Thai on the 1 % fewshot setting .", "entities": [[6, 8, "TaskName", "intent detection"]]}, {"text": "This proves that both sentence - level and word - level representations are better aligned across languages .", "entities": []}, {"text": "In addition , LR aims to further align the sentence - level representations of target language utterances into a semantically similar space of source language utterances .", "entities": []}, {"text": "As a result , there are 0.93%/2.82 % improvements in intent detection for Spanish / Thai on the 1 % few - shot setting after we add LR to BiLSTM - LVM .", "entities": [[10, 12, "TaskName", "intent detection"], [29, 30, "MethodName", "BiLSTM"]]}, {"text": "Interestingly , the performance gains are not only on the intent detection but also on the slot \ufb01lling , with an improvement of 1.77%/3.94 % in Spanish / Thai .", "entities": [[10, 12, "TaskName", "intent detection"]]}, {"text": "This is attributed toModelThai Intent Slot few - shot on 5 % target language training set BiLSTM - CRF 90.05 72.11 + LR 91.11 73.71 BiLSTM - LVM 91.02 73.11 + LR 91.45 75.18 + ALVM 91.08 74.67 + LR & ALVM 91.58 75.87 + LR & ALVM & delex .", "entities": [[16, 17, "MethodName", "BiLSTM"], [18, 19, "MethodName", "CRF"], [25, 26, "MethodName", "BiLSTM"]]}, {"text": "92.51 77.03 XL - SLU 91.05 73.43 M - BERT 92.02 75.52 Table 3 : Results of few - shot learning on 5 % Thai training data , which are averaged over three runs .", "entities": [[9, 10, "MethodName", "BERT"], [17, 21, "TaskName", "few - shot learning"]]}, {"text": "We make the training samples in Thai the same as the 3 % Spanish training samples ( 108 ) .", "entities": []}, {"text": "the fact that utterance representations are produced based on word - level representations from BiLSTM .", "entities": [[14, 15, "MethodName", "BiLSTM"]]}, {"text": "Therefore , the alignment of word - level representations will be implicitly improved in this process .", "entities": []}, {"text": "Furthermore , incorporating LR and ALVM further tackles the inherent dif\ufb01culties for the cross - lingual adaptation and achieves the state - of - the - art fewshot performance .", "entities": []}, {"text": "Notably , by only leveraging 3 % of target language training samples , the results of our best model are on par with the supervised training on all the target language training data .", "entities": []}, {"text": "Adaptation ability to unrelated languages From Table 2 , we observe impressive improvements in Thai , an unrelated language to English , by utilizing our proposed approaches , especially when the number of target language training samples is small .", "entities": []}, {"text": "For example , compared to the BiLSTMLVM , our best model signi\ufb01cantly improves the accuracy and f1 - score by \u00184%/\u00187 % in intent detection and slot \ufb01lling in Thai in the few - shot setting on 1 % data .", "entities": [[14, 15, "MetricName", "accuracy"], [16, 19, "MetricName", "f1 - score"], [23, 25, "TaskName", "intent detection"]]}, {"text": "Additionally , in the same setting , our model surpasses the strong baseline , M - BERT , in terms of accuracy and f1 - score by \u00184 % .", "entities": [[16, 17, "MethodName", "BERT"], [21, 22, "MetricName", "accuracy"], [23, 26, "MetricName", "f1 - score"]]}, {"text": "This illustrates that our approaches provide strong adaptation robustness and are able to tackle the inherent adaptation dif\ufb01culties to unrelated languages .", "entities": []}, {"text": "Comparison between Spanish and Thai To make a fair comparison for the few - shot performance in Spanish and Thai , we increase the training size of Thai to the same as 3 % Spanish training samples , as depicted in Table 3 .", "entities": []}, {"text": "We can see that there is still a performance gap between the Spanish and Thai ( 3.11 % in the intent detection task and 8.15 % in the slot \ufb01lling task ) .", "entities": [[20, 22, "TaskName", "intent detection"]]}, {"text": "This is because", "entities": []}, {"text": "7248 temperature\u0e2d\u0e38\u0e13\u0e2b\u0e20\u0e39\u0e21\u0e34tomorrow\u0e1e\u0e23\u0e38\u0e48\u0e07 EnglishThai(a ) LVM temperaturetomorrow\u0e2d\u0e38\u0e13\u0e2b\u0e20\u0e39\u0e21\u0e34\u0e1e\u0e23\u0e38\u0e48\u0e07 ( b ) LVM + LR temperaturetomorrow\u0e2d\u0e38\u0e13\u0e2b\u0e20\u0e39\u0e21\u0e34\u0e1e\u0e23\u0e38\u0e48\u0e07 ( c )", "entities": []}, {"text": "ALVM temperaturetomorrow\u0e2d\u0e38\u0e13\u0e2b\u0e20\u0e39\u0e21\u0e34\u0e1e\u0e23\u0e38\u0e48\u0e07 ( d ) ALVM + LR Figure 3 : Visualization for latent variables of parallel word pairs in English and Thai over different models trained on 1 % target language training set .", "entities": []}, {"text": "We choose the word pairs \u201c temperature- /uni0E2D / uni0E38 / uni0E13 / uni0E2B / uni0E20 / uni0E39 / uni0E21 / uni0E34 \u201d and \u201c tomorrow- /uni0E1E / uni0E23 / uni0E38 / uni0E48.low / uni0E07 \u201d from the parallel sentences \u201c what will be the temperature tomorrow \u201d and \u201c /uni0E2D / uni0E38 / uni0E13 / uni0E2B / uni0E20 / uni0E39 / uni0E21 / uni0E34 /uni0E08 / uni0E30 /uni0E2D / uni0E22 / uni0E39 / uni0E48.low /uni0E17 /uni0E40 / uni0E17 / uni0E48.low / uni0E32 /uni0E44 / uni0E2B / uni0E23 / uni0E48.low /uni0E1E / uni0E23 / uni0E38 / uni0E48.low / uni0E07 \u201d in English and Thai , respectively .", "entities": []}, {"text": "To draw the contour plot , we sample 3000 points from the distribution of latent variables for the selected words , use PCA to project those points into 2D and calculate the mean and variance for each word .", "entities": [[22, 23, "MethodName", "PCA"]]}, {"text": "ModelSpanish Thai Intent Slot Intent Slot few - shot on 1 % target language training set Our Model 93.82 78.46 87.43 62.44 w/o Pre - training 92.75 77.11 86.29 60.20 few - shot on 3 % target language training set Our Model 95.20 84.19 90.97 70.88 w/o Pre - training 94.51 82.83 89.72 69.66 zero - shot setting Our Model 92.31 72.49 75.77 33.28 w/o Pre - training 91.02 71.72 75.18 32.69 Table 4 : Results of the ablation study for the label sequence encoder pre - training ( averaged over three runs ) .", "entities": []}, {"text": "Our model refers to the one that combines LR , ALVM and delex .", "entities": []}, {"text": "with BiLSTM - LVM .", "entities": [[1, 2, "MethodName", "BiLSTM"]]}, {"text": "Spanish is grammatically and syntactically closer to English than Thai , leading to a better quality of cross - lingual alignment .", "entities": []}, {"text": "Visualization of Latent Variables", "entities": []}, {"text": "The effectiveness of the LR and ALVM can be clearly seen from Figure 3 .", "entities": []}, {"text": "The former approach decreases the distance of latent variables for words with similar semantic meanings in different languages .", "entities": []}, {"text": "For the latter approach , to make the distributions for different slot types distinguishable , our model reg - ularizes the latent variables of different slot types far from each other , and eventually it also improves the alignment of words with the same slot type .", "entities": []}, {"text": "Incorporating both approaches further improves the word - level alignment across languages .", "entities": []}, {"text": "It further proves the robustness of our proposed approaches when adapting from the source language ( English ) to the unrelated language ( Thai ) .", "entities": []}, {"text": "5.2 Zero - shot Setting From Table 2 , we observe the remarkable improvements made by LR and ALVM on the state - of - theart model XL - SLU in the zero - shot setting , and the slot \ufb01lling performance of our best model in Spanish is on par with the strong baseline Translate Train , which leverages large amounts of bilingual resources .", "entities": []}, {"text": "LR improves the adaptation robustness by making the word - level and sentence - level representations of similar utterances distinguishable .", "entities": []}, {"text": "In addition , integrating adversarial training with the LVM further increases the robustness by disentangling the latent variables for different slot types .", "entities": []}, {"text": "However , the performance boost for slot \ufb01lling in Thai is limited .", "entities": []}, {"text": "We conjecture that the inherent discrepancies in cross - lingual word embeddings and language structures for topologically different lan-", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "7249guages pairs make the word - level representations between them dif\ufb01cult to align in the zero - shot scenario .", "entities": []}, {"text": "We notice that Multilingual CoVe with auto - encoder achieves slightly better performance than our model on the slot \ufb01lling task in Thai .", "entities": [[4, 5, "MethodName", "CoVe"]]}, {"text": "This is because this baseline leverages large amounts of monolingual and bilingual resources , which largely bene\ufb01ts the cross - lingual alignment between English and Thai .", "entities": []}, {"text": "CoSDA - ML , a concurrent work of our model , utilizes additional augmented multilingual code - switching data , which signi\ufb01cantly improves the zero - shot cross - lingual performance .", "entities": []}, {"text": "5.3 Effectiveness of Label Sequence Encoder Pre - training Label sequence encoder pre - training helps the label encoder to generate more expressive representations for label sequences , which ensures the effectiveness of the label regularization approach .", "entities": []}, {"text": "From Table 4 , we can clearly observe the consistent performance gains made by pre - training in both few - shot and zero - shot scenarios .", "entities": []}, {"text": "6 Conclusion Current cross - lingual SLU models still suffer from imperfect cross - lingual alignments between the source and target languages .", "entities": []}, {"text": "In this paper , we propose label regularization ( LR ) and the adversarial latent variable model ( ALVM ) to regularize and further align the word - level and sentence - level representations across languages without utilizing any additional bilingual resources .", "entities": []}, {"text": "Experiments on the cross - lingual SLU task illustrate that our model achieves a remarkable performance boost compared to the strong baselines in both zero - shot and few - shot scenarios , and our model has a robust adaptation ability to unrelated target languages in the few - shot scenario .", "entities": []}, {"text": "In addition , visualization for latent variables further proves that our approaches are effective at improving the alignment of crosslingual representations .", "entities": []}, {"text": "Acknowledgments This work is partially funded by ITF/319/16FP and MRP/055/18 of the Innovation Technology Commission , the Hong Kong SAR Government .", "entities": []}, {"text": "References Mikel Artetxe , Gorka Labaka , and Eneko Agirre . 2017 .", "entities": []}, {"text": "Learning bilingual word embeddings with ( almost)no bilingual data .", "entities": [[2, 4, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 451\u2013462 .", "entities": []}, {"text": "Ankur Bapna , Gokhan T \u00a8ur , Dilek Hakkani - T \u00a8ur , and Larry Heck . 2017 .", "entities": []}, {"text": "Towards zero - shot frame semantic parsing for domain scaling .", "entities": [[5, 7, "TaskName", "semantic parsing"]]}, {"text": "Proc .", "entities": []}, {"text": "Interspeech 2017 , pages 2476\u20132480 .", "entities": []}, {"text": "Qian Chen , Zhu Zhuo , and Wen Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bert for joint intent classi\ufb01cation and slot \ufb01lling .", "entities": []}, {"text": "arXiv preprint arXiv:1902.10909 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Wenhu Chen , Jianshu Chen , Yu Su , Xin Wang , Dong Yu , Xifeng Yan , and William Yang Wang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Xlnbt : A cross - lingual neural belief tracking framework .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 414\u2013424 .", "entities": []}, {"text": "Alexis Conneau and Guillaume Lample .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crosslingual language model pretraining .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 32 , pages 7057\u20137067 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Alexis Conneau , Guillaume Lample , Marc\u2019Aurelio Ranzato , Ludovic Denoyer , and Herv \u00b4 e J\u00b4egou .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Word translation without parallel data .", "entities": [[0, 2, "TaskName", "Word translation"]]}, {"text": "In International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Bjarke Felbo , Alan Mislove , Anders S\u00f8gaard , Iyad Rahwan , and Sune Lehmann . 2017 .", "entities": []}, {"text": "Using millions of emoji occurrences to learn any - domain representations for detecting sentiment , emotion and sarcasm .", "entities": [[15, 16, "DatasetName", "emotion"]]}, {"text": "InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 1615\u20131625 .", "entities": []}, {"text": "Chih - Wen Goo , Guang Gao , Yun - Kai Hsu , Chih - Li Huo , Tsung - Chieh Chen , Keng - Wei Hsu , and YunNung Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Slot - gated modeling for joint slot \ufb01lling and intent prediction .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 753\u2013757 .", "entities": []}, {"text": "Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Generative adversarial nets .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 2672\u20132680 .", "entities": []}, {"text": "E Haihong , Peiqing Niu , Zhongfu Chen , and Meina Song .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A novel bi - directional interrelated model for joint intent detection and slot \ufb01lling .", "entities": [[9, 11, "TaskName", "intent detection"]]}, {"text": "In", "entities": []}, {"text": "7250Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5467 \u2013 5471 .", "entities": []}, {"text": "Sepp Hochreiter and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural Computation , 9(8):1735\u20131780 .", "entities": []}, {"text": "Haoyang Huang , Yaobo Liang , Nan Duan , Ming Gong , Linjun Shou , Daxin Jiang , and Ming Zhou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unicoder :", "entities": []}, {"text": "A universal language encoder by pretraining with multiple cross - lingual tasks .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2485\u20132494 .", "entities": []}, {"text": "Joo - Kyung Kim , Young - Bum Kim , Ruhi Sarikaya , and Eric Fosler - Lussier .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Cross - lingual transfer learning for pos tagging without cross - lingual resources .", "entities": [[0, 4, "TaskName", "Cross - lingual transfer"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2832\u20132838 .", "entities": []}, {"text": "Guillaume Lample , Miguel Ballesteros , Sandeep Subramanian , Kazuya Kawakami , and Chris Dyer . 2016 .", "entities": []}, {"text": "Neural architectures for named entity recognition .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}, {"text": "InProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 260\u2013270 .", "entities": []}, {"text": "Zhaojiang Lin , Zihan Liu , Genta Indra Winata , Samuel Cahyawijaya , Andrea Madotto , Yejin Bang , Etsuko Ishii , and Pascale Fung .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Xpersona : Evaluating multilingual personalized chatbot .", "entities": [[5, 6, "TaskName", "chatbot"]]}, {"text": "arXiv preprint arXiv:2003.07568 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zihan Liu , Jamin Shin , Yan Xu , Genta Indra Winata , Peng Xu , Andrea Madotto , and Pascale Fung . 2019a .", "entities": []}, {"text": "Zero - shot cross - lingual dialogue systems with transferable latent variables .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 1297\u20131303 .", "entities": []}, {"text": "Zihan Liu , Genta Indra Winata , Zhaojiang Lin , Peng Xu , and Pascale Fung .", "entities": []}, {"text": "2019b .", "entities": []}, {"text": "Attention - informed mixed - language training for zero - shot cross - lingual task - oriented dialogue systems .", "entities": [[14, 19, "TaskName", "task - oriented dialogue systems"]]}, {"text": "arXiv preprint arXiv:1911.09273 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zihan Liu , Genta Indra Winata , Andrea Madotto , and Pascale Fung . 2020a .", "entities": []}, {"text": "Exploring \ufb01ne - tuning techniques for pre - trained cross - lingual models via continual learning .", "entities": [[14, 16, "TaskName", "continual learning"]]}, {"text": "arXiv preprint arXiv:2004.14218 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zihan Liu , Genta Indra Winata , Peng Xu , and Pascale Fung .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Coach : A coarse - to-\ufb01ne approach for cross - domain slot \ufb01lling .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 19\u201325 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Stephen Mayhew , Chen - Tse Tsai , and Dan Roth . 2017 .", "entities": []}, {"text": "Cheap translation for cross - lingual named entity recognition .", "entities": [[6, 9, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the 2017 conference on empirical methods in natural language processing , pages 2536\u20132545 .", "entities": []}, {"text": "Nikola Mrk \u02c7si\u00b4c , Ivan Vuli \u00b4 c , Diarmuid \u00b4 O S\u00b4eaghdha , Ira Leviant , Roi Reichart , Milica Ga \u02c7si\u00b4c , Anna Korhonen , and Steve Young . 2017 .", "entities": []}, {"text": "Semantic specialization of distributional word vector spaces using monolingual and cross - lingual constraints .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 5(1):309\u2013324 .", "entities": []}, {"text": "Telmo Pires , Eva Schlinger , and Dan Garrette .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "How multilingual is multilingual BERT ?", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4996 \u2013 5001 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Libo Qin , Minheng Ni , Yue Zhang , and Wanxiang Che . 2020 .", "entities": []}, {"text": "Cosda - ml : Multi - lingual code - switching data augmentation for zero - shot cross - lingual nlp .", "entities": [[10, 12, "TaskName", "data augmentation"]]}, {"text": "arXiv preprint arXiv:2006.06402 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sebastian Schuster , Sonal Gupta , Rushin Shah , and Mike Lewis .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Cross - lingual transfer learning for multilingual task oriented dialog .", "entities": [[0, 4, "TaskName", "Cross - lingual transfer"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3795\u20133805 .", "entities": []}, {"text": "Avirup Sil , Gourab Kundu , Radu Florian , and Wael Hamza .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural cross - lingual entity linking .", "entities": [[1, 6, "TaskName", "cross - lingual entity linking"]]}, {"text": "InThirty - Second AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Shyam Upadhyay , Manaal Faruqui , Gokhan T \u00a8ur , Hakkani - T \u00a8ur Dilek , and Larry Heck . 2018a .", "entities": []}, {"text": "( almost ) zero - shot cross - lingual spoken language understanding .", "entities": [[9, 12, "TaskName", "spoken language understanding"]]}, {"text": "In 2018 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pages 6034\u20136038 . IEEE .", "entities": []}, {"text": "Shyam Upadhyay , Nitish Gupta , and Dan Roth . 2018b .", "entities": []}, {"text": "Joint multilingual supervision for cross - lingual entity linking .", "entities": [[4, 9, "TaskName", "cross - lingual entity linking"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2486\u20132495 .", "entities": []}, {"text": "Chien - Sheng Wu , Andrea Madotto , Ehsan HosseiniAsl , Caiming Xiong , Richard Socher , and Pascale Fung .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Transferable multi - domain state generator for task - oriented dialogue systems .", "entities": [[7, 12, "TaskName", "task - oriented dialogue systems"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 808\u2013819 .", "entities": []}, {"text": "Jiateng Xie , Zhilin Yang , Graham Neubig , Noah A Smith , and Jaime Carbonell . 2018 .", "entities": []}, {"text": "Neural crosslingual named entity recognition with minimal resources .", "entities": [[2, 5, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 369\u2013379 .", "entities": []}, {"text": "7251Katherine Yu , Haoran Li , and Barlas Oguz .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Multilingual seq2seq training with similarity loss for cross - lingual document classi\ufb01cation .", "entities": [[1, 2, "MethodName", "seq2seq"], [5, 6, "MetricName", "loss"]]}, {"text": "In Proceedings of The Third Workshop on Representation Learning for NLP , pages 175\u2013179 , Melbourne , Australia .", "entities": [[7, 9, "TaskName", "Representation Learning"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tao Zhang , Kang Liu , and Jun Zhao .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Cross lingual entity linking with bilingual topic model .", "entities": [[2, 4, "TaskName", "entity linking"]]}, {"text": "In Twenty - Third International Joint Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Yuan Zhang , David Gaddy , Regina Barzilay , and Tommi Jaakkola .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Ten pairs to tag \u2013 multilingual pos tagging via coarse mapping between embeddings .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1307\u20131317 .", "entities": []}]