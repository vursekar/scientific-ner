[{"text": "Proceedings of the Sixth Conference on Machine Translation ( WMT ) , pages 1009\u20131013 November 10\u201311 , 2021 .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "\u00a9 2021 Association for Computational Linguistics1009NoahNMT at WMT 2021 :", "entities": []}, {"text": "Dual Transfer for Very Low Resource Supervised Machine Translation Meng Zhang1 , Minghao Wu2 , Pengfei Li1 , Liangyou Li1 , Qun Liu1 1Huawei Noah \u2019s Ark Lab 1fzhangmeng92 , lipengfei111 , liliangyou ,", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "qun.liu g@huawei.com 2Monash", "entities": []}, {"text": "University 2minghao.wu@monash.edu", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "This paper describes the NoahNMT system submitted to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .", "entities": [[18, 20, "TaskName", "Machine Translation"]]}, {"text": "The system is a standard Transformer model equipped with our recent technique of dual transfer .", "entities": [[5, 6, "MethodName", "Transformer"]]}, {"text": "It also employs widely used techniques that are known to be helpful for neural machine translation , including iterative backtranslation , selected \ufb01netuning , and ensemble .", "entities": [[14, 16, "TaskName", "machine translation"]]}, {"text": "The \ufb01nal submission achieves the top BLEU for three translation directions .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "1 Introduction In this paper , we describe the NoahNMT system submitted to one of the WMT 2021 shared tasks .", "entities": []}, {"text": "The shared task features both unsupervised machine translation and very low resource supervised machine translation .", "entities": [[5, 8, "TaskName", "unsupervised machine translation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "As our core technique is mainly suitable for low resource supervised machine translation , we participated in four translation directions between Chuvash - Russian ( chv - ru ) and Upper Sorbian - German ( hsb - de ) .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "Our core technique is called dual transfer ( Zhang et al . , 2021 ) , which belongs to the family of transfer learning .", "entities": [[22, 24, "TaskName", "transfer learning"]]}, {"text": "It transfers from both high resource neural machine translation model and pretrained language model to improve the quality of low resource machine translation .", "entities": [[7, 9, "TaskName", "machine translation"], [21, 23, "TaskName", "machine translation"]]}, {"text": "During the preparation for the shared task , we conducted additional experiments that supplement the original paper , including the choice of parent language , the validation of Transformer big model , and the usage of dual transfer along with iterative back - translation .", "entities": [[28, 29, "MethodName", "Transformer"]]}, {"text": "In addition , we also applied proven techniques to strengthen the quality of our system , including selected \ufb01netuning and ensemble .", "entities": []}, {"text": "Our \ufb01nal submission achieves the top BLEU on the blind test sets for three translation directions : chv!ru , ru!chv , andhsb!de.2 Approach", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "In this section , we describe the techniques used in our system .", "entities": []}, {"text": "Interested readers are encouraged to check out the original papers for further details .", "entities": []}, {"text": "2.1 Dual Transfer We reproduced the illustration of dual transfer from the original paper ( Zhang et al . , 2021 ) , as shown in Figure 1 .", "entities": []}, {"text": "This illustration shows the case of general transfer , where the high resource translation direction is A!B , and the low resource translation direction is P!Q. As discussed in the original paper , in many cases , it is possible to use shared target transfer ( B = Q ) or shared source transfer ( A = P ) .", "entities": []}, {"text": "Taking chv!ruas an example , we can choose en!ruas the high resource translation direction , resulting in an instance of shared target transfer .", "entities": []}, {"text": "In this shared task , when training the high resource translation model , we always initialize the shared language side with the pretrained language model BERT ( Devlin et al . , 2019 ) .", "entities": [[25, 26, "MethodName", "BERT"]]}, {"text": "2.2 Iterative Back - Translation Iterative back - translation ( Hoang et al . , 2018 ) is an extension of back - translation ( Sennrich et al . , 2016a ) .", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "It can exploit both sides of monolingual data of a language pair , and produces translation models for both directions , which is suitable for this shared task .", "entities": []}, {"text": "The initial models for generating synthetic parallel data are produced by using dual transfer with low resource authentic parallel data .", "entities": []}, {"text": "In each iteration of iterative back - translation , we use the latest model to greedily decode a disjoint subset of 4 m monolingual sentences1to generate synthetic parallel data .", "entities": []}, {"text": "Then a new model is trained on a mixture of authentic and synthetic parallel data .", "entities": []}, {"text": "With the use of dual transfer , model training can start from 1Forchv andhsb , all monolingual sentences are used in each iteration .", "entities": []}, {"text": "1010", "entities": []}, {"text": "[ A ] PLM emb.[A ] PLM body Aand Bmono .", "entities": []}, {"text": "( 1)[P ] PLM emb.[A ] PLM body Pand Qmono .", "entities": []}, {"text": "( 2)[A ] NMT   encoder emb.[A ] NMT   encoder body[B ] NMT   decoder emb .", "entities": []}, {"text": "A\u2192Bparallel ( 3)[P ] NMT   encoder emb.[P ] NMT   encoder body[Q ] NMT   decoder emb .", "entities": []}, {"text": "P\u2192Qparallel ( 4)[B ] PLM emb.[B ] PLM body [ Q ] PLM emb.[B ]", "entities": []}, {"text": "PLM body[B ] NMT   decoder body[Q ] NMT   decoder body", "entities": []}, {"text": "[ A ] NMT   encoder emb.[B ] NMT   decoder emb .", "entities": []}, {"text": "[ A ] PLM body[B ] PLM bodyFigure 1 : Dual transfer from pretrained language model and high resource A!Bneural machine translation to low resource P!Qneural machine translation .", "entities": [[20, 22, "TaskName", "machine translation"], [26, 28, "TaskName", "machine translation"]]}, {"text": "Dashed lines represent initialization .", "entities": []}, {"text": "Parameters in striped blocks are frozen in the corresponding step , while other parameters are trainable .", "entities": []}, {"text": "Different colors represent different languages .", "entities": []}, {"text": "Data used in each step is also listed .", "entities": []}, {"text": "language code # sentence ( pair ) cs - de 15 m hsb - de 0.1 m kk - ru 3.9 m en - ru 17 m chv - ru 0.7 m cs 90 m de 100 m hsb 0.8 m kk 17 m en 54 m ru 110 m chv 3 m Table 1 : Training data statistics .", "entities": []}, {"text": "the initial parameters as shown in Step ( 4 ) of Figure 1 .", "entities": []}, {"text": "This has the additional bene\ufb01t of reducing training time , because convergence is faster than training from random initialization .", "entities": []}, {"text": "2.3 Selected Finetuning Selected \ufb01netuning aims to deal with the domain difference that may exist between the test set and the training set .", "entities": []}, {"text": "Given the source side of the test set , we try to select similar source sentences from the training set , and then \ufb01netune the translation model on the selected subset of training sentence pairs .", "entities": []}, {"text": "We use BM25 ( Robertson and Zaragoza , 2009 ) to calculate the similarity between two sentences for retrieval .", "entities": []}, {"text": "The BM25 score between a query sentence Qand a sentence Din the corpus forparent language chv!ruBLEU kk 18.47 en 18.61 Table 2 : Test set BLEU for chv!ru , when the parent language is either kkoren(i.e .", "entities": [[25, 26, "MetricName", "BLEU"]]}, {"text": "the parent translation direction is either kk!ruoren!ru ) .", "entities": []}, {"text": "The translation model is Transformer base .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "retrievalCis given by s(D ; Q ) = LQX i=1IDF ( qi)\u0001(k+ 1)\u0001TF ( qi ; D ) k\u0001\u0010 1\u0000b+b\u0001LD Lavg\u0011 + TF ( qi ; D ) ; where the query sentence Qis a sequence of LQ subwordsfqigLQ i=1,IDF ( qi)is the Inverse Document Frequency for qiin the corpusC , TF ( qi ; D ) is the Term Frequency for qiin the sentence D , LD is the length of the sentence D , Lavgis the average length of the corpus C , kandbare hyperparameters , which are set as 1.5 and 0.75 , respectively .", "entities": []}, {"text": "Based on the BM25 score , we calculate the similarity between a source test sentence ( as the query sentence ) and the source sentences in the training set to obtain the top 500 sentences .", "entities": []}, {"text": "After performing the selection for all the source test sentences , we merge them and remove duplicates to obtain the set for \ufb01netuning .", "entities": []}, {"text": "1011model chv!ru ru!chv hsb!de de!hsb Transformer base 18.61 16.18 * 55.60 55.98 Transformer big 19.24 17.12 56.10 57.12 Table 3 : Test set BLEU for the four translation directions , using either Transformer base or Transformer big for dual transfer .", "entities": [[5, 6, "MethodName", "Transformer"], [12, 13, "MethodName", "Transformer"], [23, 24, "MetricName", "BLEU"], [32, 33, "MethodName", "Transformer"], [35, 36, "MethodName", "Transformer"]]}, {"text": "* : The parent translation direction is ru!kk , and we did not train a Transformer base with ru!enas the parent , though the resulting ru!chv BLEU scores should be close based on the experiment in Section 4.1 .", "entities": [[15, 16, "MethodName", "Transformer"], [26, 27, "MetricName", "BLEU"]]}, {"text": "runtime ( hours ) BERT en 143 BERT chv 54 NMT en!ru 52 NMT chv!ru 14 Table 4 : Runtime of each step in dual transfer for NMT chv!ruwith Transformer big .", "entities": [[4, 5, "MethodName", "BERT"], [7, 8, "MethodName", "BERT"], [29, 30, "MethodName", "Transformer"]]}, {"text": "3 Experimental Setup 3.1 Data We collected allowed data for the involved languages and followed the same preprocessing pipeline of punctuation normalization and tokenization , using scripts from Moses2 .", "entities": []}, {"text": "The English monolingual data came from the English original side of ru - en back - translated news3 , but its automatic translation to Russian was discarded .", "entities": []}, {"text": "The provided Chuvash - Russian dictionary was not used .", "entities": []}, {"text": "Each language was encoded with byte pair encoding ( BPE ) ( Sennrich et al . , 2016b ) .", "entities": [[5, 8, "MethodName", "byte pair encoding"], [9, 10, "MethodName", "BPE"]]}, {"text": "The BPE codes and vocabularies were learned on each language \u2019s monolingual data , and then used to segment parallel data .", "entities": [[1, 2, "MethodName", "BPE"]]}, {"text": "We used 32k merge operations for all languages .", "entities": []}, {"text": "After BPE segmentation , we discarded sentences with more than 128 subwords , and cleaned parallel data with length ratio 1.5 .", "entities": [[1, 2, "MethodName", "BPE"]]}, {"text": "Training data statistics is provided in Table 1 .", "entities": []}, {"text": "Note that we experimented with Kazakh ( kk ) data ( Section 4.1 ) , but did not use it for our \ufb01nal submission .", "entities": []}, {"text": "Evaluation on test sets is given by SacreBLEU4(Post , 2018 ) , after BPE removal and detokenization .", "entities": [[13, 14, "MethodName", "BPE"]]}, {"text": "3.2 Hyperparameters We use Transformer ( Vaswani et al . , 2017 ) as our translation model , but with slight modi\ufb01cations 2https://github.com/moses-smt/ mosesdecoder 3http://data.statmt.org/wmt20/ translation - task / back - translation 4SacreBLEU signature : BLEU+case.mixed+numrefs.1 + smooth.exp+tok.13a+version.1.4.12.that follow the implementation of BERT5 .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "The absolute position embeddings are also learned as in BERT .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "The encoder and decoder embeddings are independent because each language manages its own vocabulary , but we tie the decoder input and output embeddings ( Press and Wolf , 2017 ) .", "entities": []}, {"text": "We apply dropout with probability 0.1 .", "entities": []}, {"text": "We use LazyAdam as the optimizer .", "entities": [[5, 6, "HyperparameterName", "optimizer"]]}, {"text": "Learning rate warms up for 16,000 steps and then follows inverse square root decay .", "entities": [[0, 2, "HyperparameterName", "Learning rate"]]}, {"text": "The peak learning rate is 5\u000210\u00004for parent translation models , and 1\u000210\u00004for child translation models .", "entities": [[2, 4, "HyperparameterName", "learning rate"]]}, {"text": "Early stopping occurs when the validation BLEU does not improve for 10 checkpoints .", "entities": [[0, 2, "MethodName", "Early stopping"], [6, 7, "MetricName", "BLEU"]]}, {"text": "We set checkpoint frequency to 2,000 updates for parent translation models and 1,000 updates for child translation models .", "entities": []}, {"text": "The batch size is 6,144 tokens per GPU and 8 NVIDIA V100 GPUs are used .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "Hyperparameters for BERT are the same as in the original paper ( Zhang et al . , 2021 ) .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "For selected \ufb01netuning , we use stochastic gradient descent as the optimizer , and the learning rate is 1\u000210\u00005 .", "entities": [[6, 9, "MethodName", "stochastic gradient descent"], [11, 12, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"]]}, {"text": "We \ufb01netune for 10,000 updates , and save a checkpoint every 100 updates .", "entities": []}, {"text": "The checkpoint with the highest validation BLEU is kept .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "4 Results 4.1 The Choice of Parent Language", "entities": []}, {"text": "In our preliminary experiments , we found it bene\ufb01cial to use a closely related language as the parent language .", "entities": []}, {"text": "It is clear that there are several factors that should be taken into account , such as the degree of closeness , and the amount of resource for training the parent model .", "entities": []}, {"text": "For Upper Sorbian , Czech ( cs ) is closely related to it , and CzechGerman has a good amount of parallel data , so we directly choose Czech as the parent language .", "entities": []}, {"text": "Chuvash , however , is a rather isolated language in the Turkic family .", "entities": []}, {"text": "The closest language with usable data is Kazakh ( kk ) , but the amount of parallel data for Kazakh - Russian is relatively small , and we found it to be quite noisy .", "entities": []}, {"text": "Therefore , we considered 5https://github.com/google-research/ bert", "entities": []}, {"text": "1012iteration chv!ru ru!chv hsb!de de!hsb 0 19.24 17.12 56.10 57.12 1 19.73 17.45 57.23 56.81 2 20.42 17.69 57.12 56.79 3 19.85 17.81 57.72 57.47 4 19.57 17.78 57.40 57.33 5 19.60 17.48 57.66 57.07", "entities": [[5, 6, "DatasetName", "0"]]}, {"text": "Table", "entities": []}, {"text": "5 : Test set BLEU for the four translation directions with iterative back - translation .", "entities": [[4, 5, "MetricName", "BLEU"]]}, {"text": "Iteration 0 is the Transformer big model in Table 3 .", "entities": [[1, 2, "DatasetName", "0"], [4, 5, "MethodName", "Transformer"]]}, {"text": "Best BLEU scores are in bold .", "entities": [[1, 2, "MetricName", "BLEU"]]}, {"text": "method chv!ru ru!chv before selected \ufb01netuning 20.42 17.69 after selected \ufb01netuning 20.55 18.03 Table 6 :", "entities": []}, {"text": "Test set BLEU to show the effect of selected \ufb01netuning .", "entities": [[2, 3, "MetricName", "BLEU"]]}, {"text": "model hsb!de de!hsb best single 57.72 57.47 ensemble 58.54 58.28 Table 7 : Test set BLEU to show the effect of ensemble .", "entities": [[15, 16, "MetricName", "BLEU"]]}, {"text": "using English ( en ) as the parent language of Chuvash .", "entities": []}, {"text": "Even though English is unrelated to Chuvash and they use different scripts , English - Russian has more parallel data that can guarantee the quality of the parent model .", "entities": []}, {"text": "We conducted an experiment with Transformer base .", "entities": [[5, 6, "MethodName", "Transformer"]]}, {"text": "Results in Table 2 indicate that English can serve as an eligible parent for Chuvash .", "entities": []}, {"text": "Considering that we plan to use Transformer big for which data amount is likely to play a more important role , we decided to use English as the parent language for Chuvash .", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "4.2", "entities": []}, {"text": "The Effect of Transformer Big", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "The original paper ( Zhang et al . , 2021 ) evaluated dual transfer only with Transformer base .", "entities": [[16, 17, "MethodName", "Transformer"]]}, {"text": "In this shared task , we scale up to Transformer big .", "entities": [[9, 10, "MethodName", "Transformer"]]}, {"text": "We also face a more realistic setting where the monolingual data for the low resource languages ( chv andhsb ) are quite scarce .", "entities": []}, {"text": "Therefore it is worth testing the effect of scaling up .", "entities": []}, {"text": "Results in Table 3 show that Transformer big brings consistent improvements .", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "We also report the runtime of each step in dual transfer for NMT chv!ruwith Transformer big in Table 4 for reference , but the numbers can vary depending on implementation and data size .", "entities": [[14, 15, "MethodName", "Transformer"]]}, {"text": "In the following experiments and our \ufb01nal submission , we use Transformer big models.4.3 Iterative Back - Translation We ran \ufb01ve iterations of iterative back - translation .", "entities": [[11, 12, "MethodName", "Transformer"], [17, 18, "TaskName", "Translation"]]}, {"text": "Results are shown in Table 5 .", "entities": []}, {"text": "The best BLEU scores are attained with two or three iterations .", "entities": [[2, 3, "MetricName", "BLEU"]]}, {"text": "Another observation is that iterative back - translation brings larger improvements for chv!ruand hsb!dethanru!chv andde!hsb .", "entities": []}, {"text": "This is probably because the monolingual data for chv andhsb are small in quantity .", "entities": []}, {"text": "4.4 Selected Finetuning We only use selected \ufb01netuning for the chv - ru pair because parallel data for hsb - de is scarce .", "entities": []}, {"text": "In order to test the effect of selected \ufb01netuning , we start from the models of Iteration 2 in Table 5 .", "entities": []}, {"text": "Results in Table 6 indicate that selected \ufb01netuning gives modest improvements .", "entities": []}, {"text": "4.5", "entities": []}, {"text": "Ensemble We validate the effectiveness of ensemble on hsb!deandde!hsb , by performing ensemble decoding from the \ufb01ve models from iterative back - translation .", "entities": []}, {"text": "Results in Table 7 demonstrate that ensemble gives BLEU improvements of about 0.8 . 4.6 Final Submission Forchv!ruandru!chv , we perform selected \ufb01netuning starting from the best models from iterative back - translation ( Iteration 2 for chv!ru , Iteration 3 for ru!chv ) .", "entities": [[8, 9, "MetricName", "BLEU"]]}, {"text": "Note that the selected training subsets are different from those in Section 4.4 because the selection is based on the source side of the blind test sets .", "entities": []}, {"text": "We \ufb01netune \ufb01ve times", "entities": []}, {"text": "1013with different random seeds for model ensemble .", "entities": [[3, 4, "DatasetName", "seeds"]]}, {"text": "Forhsb!deandde!hsb , we ensemble the \ufb01ve models from iterative back - translation .", "entities": []}, {"text": "5 Conclusion In this paper , we describe a series of experiments that contribute to our submission to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .", "entities": [[28, 30, "TaskName", "Machine Translation"]]}, {"text": "These experiments , as well as the good results of the \ufb01nal submission , show that dual transfer can work in synergy with several widely used techniques in realistic scenarios .", "entities": []}, {"text": "References Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Vu Cong Duy Hoang , Philipp Koehn , Gholamreza Haffari , and Trevor Cohn .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Iterative BackTranslation for Neural Machine Translation .", "entities": [[4, 6, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation , pages 18\u201324 .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Matt Post . 2018 .", "entities": []}, {"text": "A Call for Clarity in Reporting BLEU Scores .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 186 \u2013 191 , Brussels , Belgium .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "O\ufb01r Press and Lior Wolf . 2017 .", "entities": []}, {"text": "Using the Output Embedding to Improve Language Models .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers , pages 157\u2013163 , Valencia , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Stephen Robertson and Hugo Zaragoza .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "The Probabilistic Relevance Framework : BM25 and Beyond .", "entities": []}, {"text": "Foundations and Trends in Information Retrieval , 3(4):333\u2013389 .", "entities": [[4, 6, "TaskName", "Information Retrieval"]]}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016a .", "entities": []}, {"text": "Improving Neural Machine Translation Models with Monolingual Data .", "entities": [[2, 4, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 86\u201396 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016b .", "entities": []}, {"text": "Neural Machine Translation of Rare Words with Subword Units .", "entities": [[1, 3, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is All you Need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 , pages 5998\u20136008 .", "entities": []}, {"text": "Meng Zhang , Liangyou Li , and Qun Liu .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Two Parents , One Child : Dual Transfer for Low - Resource Neural Machine Translation .", "entities": [[9, 15, "TaskName", "Low - Resource Neural Machine Translation"]]}, {"text": "In Findings of the Association for Computational Linguistics : ACL - IJCNLP 2021 , pages 2726\u20132738 .", "entities": []}]