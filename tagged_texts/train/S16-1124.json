[{"text": "Proceedings of SemEval-2016 , pages 803\u2013808 , San Diego , California , June 16 - 17 , 2016 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2016 Association for Computational Linguistics UWB at SemEval-2016 Task 2 : Interpretable Semantic Textual Similarity with Distributional Semantics for Chunks", "entities": [[12, 15, "TaskName", "Semantic Textual Similarity"]]}, {"text": "Miloslav Konop", "entities": []}, {"text": "\u00b4 \u0131kand", "entities": []}, {"text": "Ond \u02c7rej Pra \u02c7z\u00b4akand David Steinberger and Tom \u00b4 a\u02c7s Brychc \u00b4 \u0131n NTIS \u2013 New Technologies for the Information Society , Department of Computer Science and Engineering , Faculty of Applied Sciences , University of West Bohemia , Technick \u00b4 a 8 , 306 14 Plze \u02c7n Czech Republic { konopik , brychcin } @ntis.zcu.cz , { ondfa,fenic}@kiv.zcu.cz", "entities": []}, {"text": "Abstract We introduce a system focused on solving SemEval 2016 Task 2 \u2013 Interpretable Semantic Textual Similarity .", "entities": [[14, 17, "TaskName", "Semantic Textual Similarity"]]}, {"text": "The system explores machine learning and rule - based approaches to the task .", "entities": []}, {"text": "We focus on machine learning and experiment with a wide variety of machine learning algorithms as well as with several types of features .", "entities": []}, {"text": "The core of our system consists in exploiting distributional semantics to compare similarity of sentence chunks .", "entities": []}, {"text": "The system won the competition in 2016 in the \u201c Gold standard chunk scenario \u201d .", "entities": []}, {"text": "We have not participated in the \u201c System chunk scenario \u201d .", "entities": []}, {"text": "1 Introduction The goal of the Interpretable Semantic Textual Similarity task is to go deeper with the assessment of semantic textual similarity of sentence pairs .", "entities": [[7, 10, "TaskName", "Semantic Textual Similarity"], [19, 22, "TaskName", "semantic textual similarity"]]}, {"text": "It is requested to add an explanatory layer that offers a deeper insight into the sentence similarities .", "entities": []}, {"text": "The sentences are split into chunks and the \ufb01rst goal is to \ufb01nd corresponding chunks ( with respect to their meanings ) among the compared sentences .", "entities": []}, {"text": "When the corresponding chunks are known , the chunks are annotated with their similarity scores and their relation types ( e.g. equivalent , more speci\ufb01c , etc ) .", "entities": []}, {"text": "The task follows a pilot task from the preceding SemEval 2015 competition ( Agirre et al . , 2015 ) .", "entities": []}, {"text": "The best performing systems adopted various approaches , ( Banjade et al . , 2015 ) relied on handcrafter rules , ( Karumuri et al . , 2015 ) employed a classi\ufb01er for relation types and they associated each relation with a precomputed similarity score and(H\u00a8anig et", "entities": []}, {"text": "al . , 2015 ) extended their word alignment algorithm for the task .", "entities": [[7, 9, "TaskName", "word alignment"]]}, {"text": "1.1 Math notation The data consist of sentence pairs Sa iandSb", "entities": []}, {"text": "i , where adenotes the \ufb01rst item of the pair , bdenotes the second item of the pair and iindexes the sentences ( for simiplicity we further omit ifor sentences ) .", "entities": []}, {"text": "We perceive a sentence Sato be an ordered set of chunks CHa j\u2208Saand the chunks to be ordered sets of wordswk\u2208CHa j(and analogically for sentence Sb ) .", "entities": []}, {"text": "Next we de\ufb01ne two functions : sim(CHa i , CHb j)\u2208 { 0,1,2,3,4,5}for chunk similarity and rel(CHa i , CHb j)\u2208TYPE for chunk relation type .", "entities": []}, {"text": "The possible types are : TYPE = { EQUI , OPPO , SPE1 , SPE2 , SIMI , REL } .", "entities": []}, {"text": "These are the main types .", "entities": []}, {"text": "All these types can have two modi\ufb01ers ( FACT , POL ) .", "entities": []}, {"text": "The modi\ufb01ers are optionally attached to the main types .", "entities": []}, {"text": "For example , you can generate SPE1 FACT .", "entities": []}, {"text": "For more information , please see the annotation guidelines1 .", "entities": []}, {"text": "2 System Overview 2.1 Preprocessing As a \ufb01rst step of our approach we perform the following text preprocessing : \u2022Stopwords removal \u2013 we mark the words found in a prede\ufb01ned list of 32 stopwords .", "entities": []}, {"text": "1http://alt.qcri.org/ semeval2016 / task2 / data / uploads/ annotationguidelinesinterpretablests2016v2 .", "entities": []}, {"text": "2.pdf803", "entities": []}, {"text": "\u2022Special character removal we remove special characters that violate the tokenization .", "entities": []}, {"text": "E.g. in one of the datasets , dots , commas , quotation marks and other punctuation characters were present in tokens .", "entities": []}, {"text": "\u2022Lowercasing \u2013 we remove casing from the words .", "entities": []}, {"text": "\u2022Lemmatization \u2013 we \ufb01nd lemmas with the Stanford CoreNLP tool ( Manning et al . , 2014 ) .", "entities": []}, {"text": "Our preprocessing rather adds new information and does not modify the original information .", "entities": []}, {"text": "Thus , the original word and all the generated variants are always available .", "entities": []}, {"text": "In this way , we can generate the output \ufb01le with identical words ( including the special characters ) from the input .", "entities": []}, {"text": "The dataset are already tokenized .", "entities": []}, {"text": "2.2 Chunk Semantic Similarity The core of our system is based upon computing semantic similarity of sentence chunks .", "entities": [[2, 4, "TaskName", "Semantic Similarity"], [13, 15, "TaskName", "semantic similarity"]]}, {"text": "More precisely , we are looking for the best estimation ofsim(CHa", "entities": []}, {"text": "i , CHb j ) .", "entities": []}, {"text": "The sim score should describe semantic similarity of a given chunk pair \u2013 the higher score the more easily both chunks can be replaced with each other without chaining the meaning of both sentences .", "entities": [[5, 7, "TaskName", "semantic similarity"]]}, {"text": "The similarity score ranges from 0 to 5 , where 0 is the lowest similarity and 5 is the highest similarity .", "entities": [[5, 6, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}, {"text": "Eg .", "entities": []}, {"text": "the sim(\u201ca new laptop \u201d , \u201c a new notebook \u201d )", "entities": []}, {"text": "= 5 and sim(\u201ca new laptop \u201d , \u201c an old rock \u201d )", "entities": []}, {"text": "= 0 .", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "We use the chunk similarity as a feature in our machine learning approach ( Section 3 ) and as a metric in our unsupervised approach ( Section 4 ) .", "entities": []}, {"text": "Our attempts to estimate the sim function are based upon estimating semantic similarity of individual words and compiling them into one number for a given chunk pair .", "entities": [[11, 13, "TaskName", "semantic similarity"]]}, {"text": "We experiment with Word2Vec ( Mikolov et al . , 2013 ) and GloVe ( Pennington et al . , 2014 ) for estimating similarity of words .", "entities": [[13, 14, "MethodName", "GloVe"]]}, {"text": "We compile all the word similarities in one number that re\ufb02ects semantic similarity of whole chunks via the following methods : 1 ) the vector composition method and 2 ) an adapted method for constructing vectors called lexical semantic vectors .Vector", "entities": [[11, 13, "TaskName", "semantic similarity"]]}, {"text": "composition requires that the semantics of words is described by vectors .", "entities": []}, {"text": "E.g. we have vectors for all words /vector", "entities": []}, {"text": "mi:\u2200wi\u2208CHa jand / vector ni:\u2200wi\u2208CHb k in two given chunks CHa jandCHb", "entities": []}, {"text": "k.", "entities": []}, {"text": "The vectors for words in each chunk are summed ( or averaged ) to obtain one vector for each chunk : /vector", "entities": []}, {"text": "m=/summationtext i(/vector mi)and /vector", "entities": []}, {"text": "n=/summationtext j(/vector nj ) .", "entities": []}, {"text": "The vectors are then compared with cosine distance : sim(CHa j , CHb k ) = cos(\u03b8 ) = /vector m\u00b7/vector n /bardbl / vector m|/bardbl / vector n| .", "entities": []}, {"text": "Lexical semantic vectors were originally introduced in ( Li et al . , 2006 ) .", "entities": []}, {"text": "We have made two modi\ufb01cations .", "entities": []}, {"text": "We do not weight words with their information content and we use methods for distributional semantics ( Word2Vec and GloVe ) rather than semantic networks .", "entities": [[19, 20, "MethodName", "GloVe"]]}, {"text": "The modi\ufb01ed method is explained here .", "entities": []}, {"text": "First of all , we create a combined vocabulary of all unique words from chunks CHa kand CHb l : L = unique ( CHa k\u222aCHb l ) .", "entities": []}, {"text": "Then we take all words from vocabulary L : wi\u2208Land look for maximal similarities with words from chunks aand b , respectively .", "entities": []}, {"text": "This way we get vectors /vector mand / vector n containing maximal similarities of chunk words and words from the combined vocabulary : mi= max j:1\u2264j\u2264|CHa k|sim(wi , wj ) : \u2200wi\u2208L ni= max j:1\u2264j\u2264|CHb l|sim(wi , wj ) : \u2200wi\u2208L ( 1 ) wheremiandniare elements of vectors /vector mand / vector n.", "entities": []}, {"text": "In order to obtain similarity of a chunk pair we compare their respective vectors with the cosine similarity similarly to the previous approach .", "entities": []}, {"text": "The principle of the method is illustrated by the example in \ufb01gure 1 . iDF weighting .", "entities": []}, {"text": "We assume that some words are more important than others .", "entities": []}, {"text": "In order to re\ufb02ect this assumption , we try to weight the vectors with iDF weighting .", "entities": []}, {"text": "We compute the iDF weights on the articles from English wikipedia text data ( Wikipedia dump from March 7 , 2015 ) .", "entities": []}, {"text": "3 Machine Learning Approach The main effort of our team was focused on the machine learning approach to the task .", "entities": []}, {"text": "We divided the task into to three classi\ufb01cation / regression tasks:804", "entities": []}, {"text": "CHa :    new   laptop CHb :    new   notebookL =    new   laptop   notebook 1.01.0 1.0 1.0 m =   1.0   1.0 0.68 0.680.68 n = 1.0 0.68 1.0cos \u03b8 = 0.958 CHa :    new   laptop CHb :     old   rock     new   laptop   old   rock    0.221.0 1.0        0.22      0.12 m = 1.0 1.0 0.22 0.12 0.10         1.0      1.0", "entities": [[35, 36, "HyperparameterName", "\u03b8"]]}, {"text": "n = 0.22 0.1 1.0 1.0cos \u03b8 = 0.320sim(\"a new laptop \" , \" a new notebook \" ): sim(\"a new laptop \" , \" an old rock\"):Figure 1 : An example of the modi\ufb01ed lexical semantic vectors method .", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}, {"text": "\u2022Alignment binary classi\ufb01cation \u2013 we decide whether two given chunks should be aligned with each other .", "entities": []}, {"text": "\u2022Score classi\ufb01cation / regression \u2013 we experiment with both classi\ufb01cation and regression of the chunks similarity score .", "entities": []}, {"text": "\u2022Type classi\ufb01cation \u2013 we classify all aligned pairs of chunks into a prede\ufb01ned set of types \u2013 see Section 1.1 .", "entities": []}, {"text": "3.1 Classi\ufb01ers We experiment with the following classi\ufb01ers : Maximum Entropy Classi\ufb01er ( Berger et al . , 1996 ) , Support Vector Machines Classi\ufb01er ( Cortes and Vapnik , 1995 ) , Multilayer perceptron andVoted perceptron neural networks ( Freund and Schapire , 1999 ) and with Decision / regression tree learning ( Breiman et al . , 1984 ) .", "entities": []}, {"text": "We employ the following two frameworks : Brainy ( Konkol , 2014 ) and Weka ( Hall et al . , 2009 ) .", "entities": []}, {"text": "3.2 Features We divide the employed features into four categories : lexical , syntactic , semantic , external .", "entities": []}, {"text": "Lexical features consist of the following features : word base form overlap , word lemma overlap , chunk length difference , word sentence positions difference .", "entities": [[14, 15, "DatasetName", "lemma"]]}, {"text": "Syntactic features contains closest common parent comparison ( we compute the closest common parent of all words for each chunk in the parse tree and retrieve the name of the parent node ) , parse tree path comparison ( we compute the path from the root of the sentence to the chunk ) .", "entities": []}, {"text": "POS ( Part Of Speech ) count difference ( e.g. differences in counts of nouns , adjectives , verbs , etc ) .", "entities": []}, {"text": "POS tagging and syntactic parsing are performed with Stanford CoreNLP ( Manning et al . , 2014 ) .", "entities": []}, {"text": "Semantic features are described in Section 2.2 .", "entities": []}, {"text": "Additionally , some members of our team participated in the STS task ( task 1 ) of the SemEval 2016 ( Brychc \u00b4 \u0131n and Svoboda , 2016 ) and they annotated the semantic similarity of the whole sentences with their system for us .", "entities": [[10, 11, "TaskName", "STS"], [33, 35, "TaskName", "semantic similarity"]]}, {"text": "This score is used as one feature .", "entities": []}, {"text": "External features consist of the WordNet \u2013 Lin similarity metric ( Lin , 1998 ) and the paraphrase database ( Ganitkevitch et al . , 2013 ) feature .", "entities": []}, {"text": "3.3 Post - processing The alignment of chunks is generated by the binary classi\ufb01cation of all possible chunk pairs .", "entities": []}, {"text": "If one chunk is aligned with multiple chunks in the other sentence , these chunks should be merged into one chunk .", "entities": []}, {"text": "Also , impossible multiple chunks to multiple chunks alignments are generated in some cases ( e.g. two chunks from the \ufb01rst sentence belong a chunk in the second sentence but one of the two chunks from the \ufb01rst sentence belong also to a different chunk in the second sentence ) .", "entities": []}, {"text": "These cases are resolved with few hand crafted rules .", "entities": []}, {"text": "4 Rule - based Approach We attempt to solve the task with a rule - based approach as well .", "entities": []}, {"text": "First , we de\ufb01ne the similarity of chunks as described in Section 2.2 .", "entities": []}, {"text": "The similarity is then used for the chunk alignment .", "entities": []}, {"text": "We employ an algorithm inspired by the IBM word model II for machine translation ( Brown et al . , 1993 ) .", "entities": [[12, 14, "TaskName", "machine translation"]]}, {"text": "We iterate over all chunks from sentence Saand \ufb01nd the chunk with maximal similarity from sentence Sb .", "entities": []}, {"text": "More chunks from sentence Sacan be aligned to one chunk in the sentence Sb .", "entities": []}, {"text": "In this way , we obtain N:1 mapping .", "entities": []}, {"text": "Then , we do the same with the reversed order of sentences and get the 1 : M mapping .", "entities": []}, {"text": "Then,805", "entities": []}, {"text": "we compare the mappings and take the one with the highest overall similarity .", "entities": []}, {"text": "In this way , it is ensured that we generate only valid mappings ( unlike in the previous case of machine learning \u2013 see Section 3.3 ) .", "entities": []}, {"text": "The relation types are then determined by an extremely simple algorithm : \u2022If the similarity is 5 , then the relation type is EQUI .", "entities": []}, {"text": "\u2022If the similarity is 4 or 3 and chunks contain the same amount of words , then the relation is SIMI .", "entities": []}, {"text": "\u2022If the similarity is 4 or 3 , then chunk with more words is more speci\ufb01c .", "entities": []}, {"text": "\u2022If the similarity is 2 or 1 , then the relation is SIMI .", "entities": []}, {"text": "\u2022If the similarity is 0 , then the relation type is NOALI .", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "5 Results 5.1 Experimental setup Machine learning approach We employ the following classi\ufb01ers and classi\ufb01cation frameworks : \u2022Alignment binary classi\ufb01cation \u2013 V oted perceptron ( Weka ) .", "entities": []}, {"text": "\u2022Score classi\ufb01cation \u2013 Maximum entropy ( Brainy ) .", "entities": []}, {"text": "\u2022Type classi\ufb01cation \u2013 Support vector machines ( Brainy ) .", "entities": []}, {"text": "These classi\ufb01ers perform best on the evaluation datasets .", "entities": []}, {"text": "We achieved the best results for estimating chunk similarity with Word2Vec and the modi\ufb01ed lexical semantic vectors \u2013 see Section 2.2 .", "entities": []}, {"text": "We experimented with reduced feature set ( word overlap , word positions difference , POS tags difference , semantic similarity , global semantic similarity , paraphrase database ) \u2013 run 1 and with all features \u2013 run 3 .", "entities": [[18, 20, "TaskName", "semantic similarity"], [22, 24, "TaskName", "semantic similarity"]]}, {"text": "The run 1 contains the optimal combination of features .", "entities": []}, {"text": "Since this combination is established on evaluation datasets it does not need to be optimal for the test datasets .", "entities": []}, {"text": "To increase our chances in thecompletion , we also run the system with all features \u2013 run 3 .", "entities": []}, {"text": "We use the provided annotated evaluation dataset ( Images , Headlines , Answer students ) for training the models .", "entities": []}, {"text": "We train three models , each for one dataset .", "entities": []}, {"text": "For development , we use the 10 - fold crossvalidation .", "entities": []}, {"text": "For \ufb01nal test runs , we train the three models on evaluation datasets and run the system on the corresponding test datasets ( e.g. Images evaluation dataset based model is used to annotate Images test data ) .", "entities": []}, {"text": "We do not neither modify the original datasets nor annotate any additional data .", "entities": []}, {"text": "Rule - based approach There are little options in this approach .", "entities": []}, {"text": "Again , we have achieved the best results for estimating chunk similarity with Word2Vec and the modi\ufb01ed lexical semantic vectors \u2013 see Section 2.2 .", "entities": []}, {"text": "We set the threshold for the similarity score to 2.5 .", "entities": []}, {"text": "All lower values are set to 0 .", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "This is the run 2 .", "entities": []}, {"text": "Individual setting for different dataset We restrained from setting individual con\ufb01gurations for different datasets .", "entities": []}, {"text": "The setup is completely identical for all datasets .", "entities": []}, {"text": "5.2 Results In this section , we summarize the of\ufb01cial results for the SemEval 2016 competition \u2013 see table 1 .", "entities": []}, {"text": "The results are calculated for the following dataset : Headlines , Images andAnswer students .", "entities": []}, {"text": "The results show F1 scores for chunk alignment ( Ali ) , determination of the relation type ( Type ) , chunk similarity score ( Score ) and combination of relation type and score similarity ( T+S ) .", "entities": [[3, 4, "MetricName", "F1"], [25, 26, "MetricName", "Score"]]}, {"text": "The bold numbers are the overall best scores .", "entities": []}, {"text": "We participated only in the gold standard chunk scenario .", "entities": []}, {"text": "The results clearly show that the unsupervised run 2 perform much worse than the supervised runs 1 and 3 .", "entities": []}, {"text": "We expected that .", "entities": []}, {"text": "However , it is worth of noticing that the unsupervised alignment algorithm inspired by machine translation alignment placed quite well .", "entities": [[14, 16, "TaskName", "machine translation"]]}, {"text": "In fact , it is newer looses more than 3 % from the best alignment score in all datasets .", "entities": []}, {"text": "The overall rank of the run 2 places in the top half among all system with exception of the answer student dataset .", "entities": []}, {"text": "The poor performance of the run 2 on this dataset is most likely caused by the fact that the hand - crafted rules were prepared for the images and806", "entities": []}, {"text": "Run Ali Type Score T+S Rank Images dataset ( 756 sentence pairs ) 3 0.8922 0.6867 0.8408 0.6708 1 1 0.8937 0.6829 0.8397 0.6672 2 2 0.8713 0.6346 0.8083 0.6206 6 Headlines dataset ( 750 sentence pairs ) 3 0.8987 0.6412 0.8382 0.6296 6 1 0.8979 0.6319 0.8346 0.6212 7 2 0.8897 0.6146 0.815 0.6013 8 Answer students dataset ( 330 sentence pairs ) 1 0.8644 0.6299 0.8089 0.6248 3 3 0.8588 0.6167 0.8038 0.6114 5 2 0.8752 0.4806 0.7826 0.4748 17 Overall results ( mean ) 1 0.6672 0.6212 0.6248 0.6377 1 3 0.6708 0.6296 0.6114 0.6373 2 2 0.6206 0.6013 0.4748 0.5656 12 Table 1 : Of\ufb01cial system evaluation .", "entities": [[3, 4, "MetricName", "Score"], [31, 33, "DatasetName", "Headlines dataset"]]}, {"text": "headlines datasets and they are clearly not applicable on the answer student which is substantially different .", "entities": []}, {"text": "The runs 1 and 3 perform very similarly .", "entities": []}, {"text": "The optimized feature set of the run 1 helps especially in theanswer student dataset .", "entities": []}, {"text": "However , the differences between these runs are too small and they can be caused by chance .", "entities": []}, {"text": "It is worth of noticing that the run 1 is not the best one in any of the datasets and it still wins in the overall results table .", "entities": []}, {"text": "The reason is that it provides the most consistent results among all other runs of all systems in the competition .", "entities": []}, {"text": "In order to provide additional information about the features effectiveness , we have evaluated them on the \ufb01nal test datasets .", "entities": []}, {"text": "In many cases , the obtained results are not conclusive .", "entities": []}, {"text": "On some datasets , the features help slightly on others they even decrease the performance .", "entities": []}, {"text": "However , the following three features have signi\ufb01cant in\ufb02uence on the \ufb01nal results : modi\ufb01ed lexical semantic vectors ( +3 % of the mean of T+S F1 scores ) , shared words ( +2 % ) , POS tags difference ( +2 % ) .", "entities": [[26, 27, "MetricName", "F1"]]}, {"text": "The modi\ufb01ed lexical semantic vectors method performed better than vector composition by 1 % for the machine learning approach and by 2 % for the rule - based approach in average .", "entities": []}, {"text": "By optimizing the feature set , we were able to increase the mean score to 0.6484 of T+S F1 measure.6 Conclusion", "entities": [[18, 19, "MetricName", "F1"]]}, {"text": "The machine learning approach with combination of methods for the distributional semantics ( Word2Vec and GloVe ) proved to be very capable of solving the advanced task of Interpretable Semantic Textual Similarity .", "entities": [[15, 16, "MethodName", "GloVe"], [29, 32, "TaskName", "Semantic Textual Similarity"]]}, {"text": "We have chosen not to tune the system for individual datasets but to tune it for the task as a whole .", "entities": []}, {"text": "The modi\ufb01ed lexical semantic vectors approach seems to be an attractive alternative to the more traditional vector composition .", "entities": []}, {"text": "Acknowledgments This publication was supported by the project LO1506 of the Czech Ministry of Education , Youth and Sports and by Grant No .", "entities": []}, {"text": "SGS-2016 - 018 Data and Software Engineering for Advanced Applications .", "entities": []}, {"text": "Computational resources were provided by the CESNET LM2015042 and the CERIT Scienti\ufb01c Cloud LM2015085 , provided under the programme \u201d Projects of Large Research , Development , and Innovations Infrastructures \u201d .", "entities": []}, {"text": "References Eneko Agirre , Carmen Banea , Claire Cardie , Daniel Cer , Mona Diab , Aitor Gonzalez - Agirre , Weiwei Guo , Inigo Lopez - Gazpio , Montse Maritxalar , Rada Mihalcea , German Rigau , Larraitz Uria , and Janyce Wiebe .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Semeval-2015 task 2 : Semantic textual similarity , english , spanish and pilot on interpretability .", "entities": [[4, 7, "TaskName", "Semantic textual similarity"]]}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 252\u2013263 , Denver , Colorado , June .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rajendra Banjade , Nobal Bikram Niraula , Nabin Maharjan , Vasile Rus , Dan Stefanescu , Mihai Lintean , and Dipesh Gautam .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Nerosim : A system for measuring and interpreting semantic textual similarity .", "entities": [[8, 11, "TaskName", "semantic textual similarity"]]}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 164 \u2013 171 , Denver , Colorado , June .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Adam L. Berger , Vincent J. Della Pietra , and Stephen A. Della Pietra .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "1996 .", "entities": []}, {"text": "A maximum entropy approach to natural language processing .", "entities": []}, {"text": "Computational Linguistics , 22(1):39\u201371 , March .", "entities": []}, {"text": "Leo Breiman , Jerome Friedman , Richard A. Olshen , and Charles Stone .", "entities": []}, {"text": "1984 .", "entities": []}, {"text": "Classi\ufb01cation and Regression Trees .", "entities": []}, {"text": "Wadsworth and Brooks , Monterey , CA.807", "entities": []}, {"text": "Peter F. Brown , Vincent J. Della Pietra , Stephen A. Della Pietra , and Robert L. Mercer .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "The mathematics of statistical machine translation : Parameter estimation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "Computational Linguistics , 19(2):263\u2013311 , June . Tom\u00b4a\u02c7s", "entities": []}, {"text": "Brychc \u00b4 \u0131n and Luk \u00b4 a\u02c7s Svoboda .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Uwb at semeval-2016 task 1 : Semantic textual similarity using lexical , syntactic , and semantic information .", "entities": [[6, 9, "TaskName", "Semantic textual similarity"]]}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation ( SemEval 2016 ) , San Diego , California , June .", "entities": []}, {"text": "Corinna Cortes and Vladimir Vapnik .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Supportvector networks .", "entities": []}, {"text": "Mach .", "entities": []}, {"text": "Learn . , 20(3):273\u2013297 , September .", "entities": []}, {"text": "Yoav Freund and Robert E. Schapire .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Large margin classi\ufb01cation using the perceptron algorithm .", "entities": []}, {"text": "Machine Learning , 37(3):277\u2013296 , December .", "entities": []}, {"text": "Juri Ganitkevitch , Benjamin Van Durme , and Chris Callison - Burch .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "PPDB : The paraphrase database .", "entities": []}, {"text": "In Proceedings of NAACL - HLT , pages 758 \u2013 764 , Atlanta , Georgia , June .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mark Hall , Eibe Frank , Geoffrey Holmes , Bernhard Pfahringer , Peter Reutemann , and Ian H. Witten .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "The weka data mining software : An update .", "entities": []}, {"text": "SIGKDD Explorations Newsletter , 11(1):10\u201318 , November .", "entities": []}, {"text": "Christian H \u00a8anig , Robert Remus , and Xose de la Puente . 2015 .", "entities": []}, {"text": "Exb themis : Extensive feature extraction from word alignments for semantic textual similarity .", "entities": [[10, 13, "TaskName", "semantic textual similarity"]]}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 264\u2013268 , Denver , Colorado , June .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sakethram Karumuri , Viswanadh Kumar Reddy Vuggumudi , and Sai Charan Raj Chitirala . 2015 .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "Umduluthblueteam : Svcsts - a multilingual and chunk level semantic similarity system .", "entities": [[9, 11, "TaskName", "semantic similarity"]]}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 107\u2013110 , Denver , Colorado , June .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Michal Konkol .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Brainy : A machine learning library .", "entities": []}, {"text": "In Leszek Rutkowski , Marcin Korytkowski , Rafa Scherer , Ryszard Tadeusiewicz , Lot\ufb01 A. Zadeh , and Jacek M. Zurada , editors , Arti\ufb01cial Intelligence and Soft Computing , volume 8468 of Lecture Notes in Computer Science , pages 490\u2013499 .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Yuhua Li , David McLean , Zuhair A. Bandar , James D. O\u2019Shea , and Keeley Crockett .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Sentence similarity based on semantic nets and corpus statistics .", "entities": []}, {"text": "IEEE Trans .", "entities": []}, {"text": "on Knowl .", "entities": []}, {"text": "and Data Eng . , 18(8):1138 \u2013 1150 , August .", "entities": []}, {"text": "Dekang Lin .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "Extracting collocations from text corpora .", "entities": []}, {"text": "In First Workshop on Computational Terminology .", "entities": []}, {"text": "Christopher D. Manning , Mihai Surdeanu , John Bauer , Jenny Finkel , Steven J. Bethard , and David McClosky .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The Stanford CoreNLP natural language processing toolkit .", "entities": []}, {"text": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 55\u201360 .", "entities": []}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "CoRR , abs/1301.3781 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Association for Computational Linguistics.808", "entities": []}]