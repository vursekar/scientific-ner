[{"text": "Proceedings of NAACL - HLT 2018 , pages 720\u2013730 New Orleans , Louisiana , June 1 - 6 , 2018 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics Polyglot Semantic Parsing in APIs Kyle Richardson\u2020 , Jonathan Berant\u2021 , Jonas Kuhn\u2020 \u2020Institute for Natural Language Processing , University of Stuttgart , Germany { kyle,jonas}@ims.uni-stuttgart.de \u2021Tel - Aviv University ,", "entities": [[6, 8, "TaskName", "Semantic Parsing"]]}, {"text": "Israel joberant@cs.tau.ac.il", "entities": []}, {"text": "Abstract Traditional approaches to semantic parsing ( SP ) work by training individual models for each available parallel dataset of text - meaning pairs .", "entities": [[4, 6, "TaskName", "semantic parsing"]]}, {"text": "In this paper , we explore the idea of polyglot semantic translation , or learning semantic parsing models that are trained on multiple datasets and natural languages .", "entities": [[15, 17, "TaskName", "semantic parsing"]]}, {"text": "In particular , we focus on translating text to code signature representations using the software component datasets of Richardson and Kuhn ( 2017a , b ) .", "entities": []}, {"text": "The advantage of such models is that they can be used for parsing a wide variety of input natural languages and output programming languages , or mixed input languages , using a single uni\ufb01ed model .", "entities": []}, {"text": "To facilitate modeling of this type , we develop a novel graph - based decoding framework that achieves state - of - the - art performance on the above datasets , and apply this method to two other benchmark SP tasks .", "entities": []}, {"text": "1 Introduction Recent work by Richardson and Kuhn ( 2017a , b ) ; Miceli Barone and Sennrich ( 2017 ) considers the problem of translating source code documentation to lower - level code template representations as part of an effort to model the meaning of such documentation .", "entities": []}, {"text": "Example documentation for a number of programming languages is shown in Figure 1 , where each docstring description in red describes a given function ( blue ) in the library .", "entities": []}, {"text": "While capturing the semantics of docstrings is in general a dif\ufb01cult task , learning the translation from descriptions to formal code representations ( e.g. , formal representations of functions ) is proposed as a reasonable \ufb01rst step towards learning more general natural language understanding models in the software domain .", "entities": [[41, 44, "TaskName", "natural language understanding"]]}, {"text": "Under this approach , one can view a software library , or API , as a kind of parallel translation corpus for studying text\u2192code orcode\u2192texttranslation.1 .", "entities": []}, {"text": "( en , Java )", "entities": []}, {"text": "Documentation * Returns the greater of two long values public static long max(long a , long b ) 2 .", "entities": []}, {"text": "( en , Python ) Documentation max(self , a , b ): \" \" \" Compares two values numerically and returns the maximum \" \" \" 3 . ( en , Haskell ) Documentation --| \" The largest element of a non - empty structure \" maximum : : forall", "entities": []}, {"text": "z. Ord a a = > t a - > a 4 . ( de , PHP ) Documentation * gibt den gr \u00a8o\u00dferen dieser Werte zur \u00a8uck .", "entities": []}, {"text": "max ( mixed $ value1 , mixed $ value2 )", "entities": []}, {"text": "Figure 1 : Example source code documentation .", "entities": []}, {"text": "Richardson and Kuhn ( 2017b ) extracted the standard library documentation for 10 popular programming languages across a number of natural languages to study the problem of text to function signature translation .", "entities": []}, {"text": "Initially , these datasets were proposed as a resource for studying semantic parser induction ( Mooney , 2007 ) , or for building models that learn to translate text to formal meaning representations from parallel data .", "entities": []}, {"text": "In followup work ( Richardson and Kuhn , 2017a ) , they proposed using the resulting models to do automated question - answering ( QA ) and code retrieval on target APIs , and experimented with an additional set of software datasets built from 27 open - source Python projects .", "entities": [[1, 2, "DatasetName", "followup"]]}, {"text": "As traditionally done in SP ( Zettlemoyer and Collins , 2012 ) , their approach involves learning individual models for each parallel dataset or language pair , e.g. , ( en , Java ) , ( de , PHP ) , and ( en , Haskell ) .", "entities": []}, {"text": "Looking again at Figure 1 , we notice that while programming languages differ in terms of representation conventions , there is often overlap between the functionality implemented and naming in these different languages ( e.g. , the max720", "entities": []}, {"text": "function ) , and redundancy in the associated linguistic descriptions .", "entities": []}, {"text": "In addition , each English description ( Figure 1.1 - 1.3 ) describes max differently using the synonyms greater , maximum , largest .", "entities": []}, {"text": "In this case , it would seem that training models on multiple datasets , as opposed to single language pairs , might make learning more robust , and help to capture various linguistic alternatives .", "entities": []}, {"text": "With the software QA application in mind , an additional limitation is that their approach does not allow one to freely translate a given description to multiple output languages , which would be useful for comparing how different programming languages represent the same functionality .", "entities": []}, {"text": "The model also can not translate between natural languages and programming languages that are not observed during training .", "entities": []}, {"text": "While software documentation is easy to \ufb01nd in bulk , if a particular API is not already documented in a language other than English ( e.g. , Haskell inde ) , it is unlikely that such a translation will appear without considerable effort by experienced translators .", "entities": []}, {"text": "Similarly , many individual APIs may be too small or poorly documented to build individual models or QA applications , and will in some way need to bootstrap off of more general models or resources .", "entities": []}, {"text": "To deal with these issues , we aim to learn more general text - to - code translation models that are trained on multiple datasets simultaneously .", "entities": [[16, 18, "TaskName", "code translation"]]}, {"text": "Our ultimate goal is to build polyglot translation models ( cf .", "entities": []}, {"text": "Johnson et", "entities": []}, {"text": "al . ( 2016 ) ) , or models with shared representations that can translate any input text to any output programming language , regardless of whether such language pairs were encountered explicitly during training .", "entities": []}, {"text": "Inherent in this task is the challenge of building an ef\ufb01cient polyglot decoder , or a translation mechanism that allows such crossing between input and output languages .", "entities": []}, {"text": "A key challenge is ensuring that such a decoder generates well - formed code representations , which is not guaranteed when one simply applies standard decoding strategies from SMT and neural MT ( cf .", "entities": []}, {"text": "Cheng et", "entities": []}, {"text": "al . ( 2017 ) ) .", "entities": []}, {"text": "Given our ultimate interest in API QA , such a decoder must also facilitate monolingual translation , or being able to translate to speci\ufb01c output languages as needed .", "entities": []}, {"text": "To solve the decoding problem , we introduce a new graph - based decoding and representation framework that reduces to solving shortest path problems in directed graphs .", "entities": []}, {"text": "We investigate several translation models that work within thisframework , including traditional SMT models and models based on neural networks , and report stateof - the - art results on the technical documentation task of Richardson and Kuhn ( 2017b , a ) .", "entities": []}, {"text": "To show the applicability of our approach to more conventional SP tasks , we apply our methods to the GeoQuery domain ( Zelle and Mooney , 1996 ) and the Sportscaster corpus ( Chen et al . , 2010 ) .", "entities": []}, {"text": "These experiments also provide insight into the main technical documentation task and highlight the strengths and weaknesses of the various translation models being investigated .", "entities": []}, {"text": "2 Related Work Our approach builds on the baseline models introduced in Richardson and Kuhn ( 2017b ) ( see also Deng and Chrupa\u0142a ( 2014 ) ) .", "entities": []}, {"text": "Their work is positioned within the broader SP literature , where traditionally SMT ( Wong and Mooney , 2006a ) and parsing ( Zettlemoyer and Collins , 2009 ) methods are used to study the problem of translating text to formal meaning representations , usually centering around QA applications ( Berant et al . , 2013 ) .", "entities": []}, {"text": "More recently , there has been interest in using neural network approaches either in place of ( Dong and Lapata , 2016 ;", "entities": []}, {"text": "Ko \u02c7cisk\u00b4y et al . , 2016 ) or in combination with ( Misra and Artzi , 2016 ; Jia and Liang , 2016 ; Cheng et al . , 2017 ) these traditional models , the latter idea we look at in this paper .", "entities": []}, {"text": "Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOver\ufb02ow and Github ( cf .", "entities": []}, {"text": "Allamanis et", "entities": []}, {"text": "al . ( 2017 ) ) .", "entities": []}, {"text": "Most of this recent work focuses on processing large amounts of API data in bulk ( Gu et al . , 2016 ; Miceli Barone and Sennrich , 2017 ) , either for learning longer executable programs from text ( Yin and Neubig , 2017 ; Rabinovich et al . , 2017 ) , or solving the inverse problem of code to text generation ( Iyer et al . , 2016 ; Richardson et al . , 2017 ) .", "entities": [[62, 64, "TaskName", "text generation"]]}, {"text": "In contrast to our work , these studies do not look explicitly at translating to target APIs , or at non - English documentation .", "entities": []}, {"text": "The idea of polyglot modeling has gained some traction in recent years for a variety of problems ( Tsvetkov et al . , 2016 ) and has appeared within work in SP under the heading of multilingual SP ( Jie and Lu , 2014 ; Duong et al . , 2017 ) .", "entities": []}, {"text": "A related topic is learning from multiple knowledge sources or domains ( Herzig and Berant , 2017 ) , which is related to our idea of learning from multiple APIs.721", "entities": []}, {"text": "When building models that can translate between unobserved language pairs , we use the term zeroshot translation from Johnson et", "entities": []}, {"text": "al . ( 2016 ) .", "entities": []}, {"text": "3 Baseline Semantic Translator Problem Formulation Throughout the paper , we refer to target code representations as API components .", "entities": []}, {"text": "In all cases , components will consist of formal representations of functions , or function signatures ( e.g. , long max(int a , int b ) ) , which include a function name ( max ) , a sequence of arguments ( int a , int b ) , and other information such as a return value ( long ) and namespace ( for more details , see Richardson ( 2018 ) ) .", "entities": []}, {"text": "For a given API dataset D= { ( xi , zi)}n i=1of sizen , the goal is to learn a model that can generate exactly a correct component sequence z= ( z1, .. ,z|z| ) , within a \ufb01nite space Cof signatures ( i.e. , the space of all de\ufb01ned functions ) , for each input text sequence x= ( x1, ... ,x|x| ) .", "entities": []}, {"text": "This involves learning a probability distribution p(z|x ) .", "entities": []}, {"text": "As such , one can think of this underlying problem as a constrained MT task .", "entities": []}, {"text": "In this section , we describe the baseline approach of Richardson and Kuhn ( 2017b ) .", "entities": []}, {"text": "Technically , their approach has two components : a simple word - based translation model and task speci\ufb01c decoder , which is used to generate a k - best list of candidate component representations for a given input x.", "entities": []}, {"text": "They then use a discriminative model to rerank the translation output using additional nonworld level features .", "entities": []}, {"text": "The goal in this section is to provide the technical details of their translation approach , which we improve in Section 4 . 3.1 Word - based Translation Model", "entities": [[27, 28, "TaskName", "Translation"]]}, {"text": "The translation models investigated in Richardson and Kuhn ( 2017b ) use a noisy - channel formulation wherep(z|x)\u221dp(x|z)p(z)via Bayes rule .", "entities": []}, {"text": "By assuming a uniform prior on output components , p(z ) , the model therefore involves estimatingp(x|z ) , which under a word - translation model is computed using the following formula : p(x|z )", "entities": []}, {"text": "= /summationtext a\u2208Ap(x , a|z ) , where the summation ranges over the set of all many - to - one word alignmentsAfrom x\u2192z , with|A|equal to ( |z|+ 1)|x| .", "entities": []}, {"text": "They investigate various types of sequence - based alignment models ( Och and Ney , 2003 ) , and \ufb01nd that the classic IBM Model 1 outperforms more complex word models .", "entities": []}, {"text": "This model factors in the following way and assumes an inde - pendent word generation process : p(x|z ) = 1 |A||x|/productdisplay j=1|z|/summationdisplay i=0pt(xj|zi ) ( 1 ) where each ptde\ufb01nes a multinomial distribution over a given component term zfor all words x.", "entities": []}, {"text": "The decoding problem for the above translation model involves \ufb01nding the most likely output\u02c6z , which requires solving an arg maxzover Equation 1 .", "entities": []}, {"text": "In the general case , this problem is known to beNP - complete for the models under consideration ( Knight , 1999 ) largely due to the large space of possible predictions z. Richardson and Kuhn ( 2017b ) avoid these issues by exploiting the \ufb01niteness of the target component search space ( an idea we also pursue here and discuss more below ) , and describe a constrained decoding algorithm that runs in time O(|C|log|C| ) .", "entities": []}, {"text": "While this works well for small APIs , it becomes less feasible when dealing with large sets of APIs , as in the polyglot case , or with more complex semantic languages typically used in SP ( Liang , 2013 ) .", "entities": []}, {"text": "4 Shortest Path Framework To improve the baseline translation approach used previously ( Section 3.1 ) , we pursue a graph based approach .", "entities": []}, {"text": "Given the formulation above and the \ufb01niteness of our prediction space C , our approach exploits the fact that we can represent the complete component search space for any set of APIs as a directed acyclic \ufb01nite - state automaton ( DAFSA ) , such as the one shown graphically in Figure 2 .", "entities": []}, {"text": "The underlying graph is constructed by concatenating all of the component representations for each API of interest and applying standard \ufb01nite - state construction and minimization techniques ( Mohri , 1996 ) .", "entities": []}, {"text": "Each path in the resulting compact automaton is therefore a well - formed component representation .", "entities": []}, {"text": "Using an idea from Johnson et al .", "entities": []}, {"text": "( 2016 ) , we add to each component representation an arti\ufb01cial token that identi\ufb01es the output programming language or library .", "entities": []}, {"text": "For example , the two edges from the initial state 0 in Figure 2 are labeled as 2C and2Clojure , which identify the C and Clojure programming languages respectively .", "entities": [[10, 11, "DatasetName", "0"]]}, {"text": "All paths starting from the right of these edges are therefore valid paths in each respective programming language .", "entities": []}, {"text": "The paths starting from the initial state 0 , in contrast , correspond to all valid component representations in all languages.722", "entities": [[7, 8, "DatasetName", "0"]]}, {"text": "0.00s0 \u221e5s5 \u221e1 s1\u221e6s6 \u221e2 s2\u221e3 s3\u221e7s7 \u221e4 s4\u221e9s9 \u221e10 s10\u221e11s11\u221e8s82C 2Clojurenumeric algomath math", "entities": []}, {"text": "ceilatan2atan2ceil", "entities": []}, {"text": "xxarg y Figure 2 : A DAFSA representation for a portion of the component sequence search space Cthat includes math functions in CandClojure , and an example path / translation ( in bold ): 2Cnumeric math ceil arg .", "entities": []}, {"text": "Decoding reduces to the problem of \ufb01nding a path for a given text input x.", "entities": []}, {"text": "For example , given the input the ceiling of a number , we would want to \ufb01nd the paths corresponding to the component translations numeric math ceil arg ( in C ) andalgo math ceil", "entities": []}, {"text": "x ( in Clojure ) in the graph shown in Figure 2 .", "entities": []}, {"text": "Using the trick above , our setup facilitates both monolingual decoding , i.e. , generating components speci\ufb01c to a particular output language ( e.g. , the C language via the path shown in bold ) , and polyglot decoding , i.e. , generating any output language by starting at the initial state 0 ( e.g. , C and Clojure ) .", "entities": [[52, 53, "DatasetName", "0"]]}, {"text": "We formulate the decoding problem using a variant of the well - known single source shortest path ( SSSP ) algorithm for directed acyclic graphs ( DAGs ) ( Johnson ( 1977 ) ) .", "entities": []}, {"text": "This involves a graph G= ( V , E)(nodesVand labeled edges E , see graph in Figure 2 ) , and taking an off - line topological sort of the graph \u2019s vertices .", "entities": []}, {"text": "Using a data structured\u2208R|V|(initialized as\u221e|V| , as shown in Figure 2 ) , the standard SSSP algorithm ( which is the forward update variant of the Viterbi algorithm ( Huang , 2008 ) ) works by searching forward through the graph in sorted order and \ufb01nding for each nodevan incoming labeled edge u , with labelz , that solves the following recurrence : d(v ) = min ( u , z):(u , v , z ) \u2208E / braceleftBig d(u ) + w(u , v , z ) /bracerightBig", "entities": []}, {"text": "( 2 ) whered(u)is shortest path score from a unique source node bto the incoming node u(computed recursively ) and w(u , v , z ) is the weight of the particular labeled edge .", "entities": []}, {"text": "The weight of the resulting shortest path is commonly taken to be the sum of the path edge weights as given by w , and the output translation is the sequence of labels associated with each edge .", "entities": []}, {"text": "This algorithm runs in linear time over the size of the graph \u2019s adjacency matrix ( Adj ) and can be extended to \ufb01nd kSSSPs .", "entities": []}, {"text": "In the standard case , a weighting function wis pro - Algorithm 1 Lexical Shortest Path Search Input : Input xof sizen , DAGG= ( V , E ) , lexical translation function pt , source node bwith initial score o. Output : Shortest component path 1 : d[V[G]]\u2190\u221e,\u03c0[V[G]]\u2190Nil , d", "entities": []}, {"text": "[ b]\u2190o 2 : s[V[G],n]\u21900.0\u22bfShortest path sums at each node 3 : foreach vertexu\u2265b\u2208V[G]in sorted order do 4 : foreach vertex and label ( v , z)\u2208Adj[u]do 5 : score\u2190\u2212log / bracketleftbig / producttextn ipt(xi|z )", "entities": []}, {"text": "+ s[u , i]/bracketrightbig 6 : ifd[v]>score then 7 : d[v]\u2190score , \u03c0[v]\u2190u 8 : foriin1, .. ,n do \u22bfUpdate scores 9 : s[v , i]\u2190pt(xi|z ) + s[u , i ] 10 : return FINDPATH(\u03c0,|V|,b ) vided by assuming a static weighted graph .", "entities": []}, {"text": "In our translation context , we replace wwith a translation model , which is used to dynamically generate edge weights during the SSSP search for each inputxby scoring the translation between xand each edge labelzencountered .", "entities": []}, {"text": "Given this general framework , many different translation models can be used for scoring .", "entities": []}, {"text": "In what follows , we describe two types of decoders based on lexical translation ( or unigram ) and neural sequence models .", "entities": []}, {"text": "Technically , each decoding algorithm involves modifying the standard SSSP search procedure by adding an additional data structure sto each node ( see Figure 2 ) , which is used to store information about translations ( e.g. , running lexical translation scores , RNN state information ) associated with particular shortest paths .", "entities": []}, {"text": "By using these two very different models , we can get insight into the challenges associated with the technical documentation translation task .", "entities": []}, {"text": "As we show in Section 6 , each model achieves varying levels of success when subjected to a wider range of SP tasks , which reveals differences between our task and other SP tasks .", "entities": []}, {"text": "4.1 Lexical Translation Shortest Path In our \ufb01rst model , we use the lexical translation model and probability function ptin Equation 1 as723", "entities": [[2, 3, "TaskName", "Translation"]]}, {"text": "the weighting function , which can be learned ef\ufb01ciently off - line using the EM algorithm .", "entities": [[14, 15, "MetricName", "EM"]]}, {"text": "When attempting to use the SSSP procedure to compute this equation for a given source input x , we immediately have the problem that such a computation requires a complete component representationz(Knight and Al - Onaizan , 1998 ) .", "entities": []}, {"text": "We use an approximation1that involves ignoring the normalizer|A|and exploiting the word independence assumption of the model , which allows us to incrementally compute translation scores for individual source words given output translations corresponding to shortest paths during the SSSP search .", "entities": []}, {"text": "The full decoding algorithm in shown in Algorithm 1 , where the red highlights the adjustments made to the standard SSSP search as presented in Cormen et al .", "entities": []}, {"text": "( 2009 ) .", "entities": []}, {"text": "The main modi\ufb01cation involves adding a data structure s\u2208R|V|\u00d7|x|(initialized as 0.0|V|\u00d7|x|at line 2 ) that stores a running sum of source word scores given the best translations at each node , which can be used for computing the inner sum in Equation 1 .", "entities": []}, {"text": "For example , given an input utterance ceiling function , s6 in Figure 2 contains the independent translation scores for words ceiling andfunction given the edge labelnumeric andpt .", "entities": []}, {"text": "Later on in the search , these scores are used to compute s7 , which will provide translation scores for each word given the edge sequence numeric math .", "entities": []}, {"text": "Taking the product over any givensj(as done in line 7 to get score ) will give the probability of the shortest path translation at the particular point j. Here , the transformation into\u2212logspace is used to \ufb01nd the minimum incoming path .", "entities": []}, {"text": "Standardly , the data structure \u03c0can be used to retrieve the shortest path back to the source node b(done via the F INDPATH method ) .", "entities": []}, {"text": "4.2 Neural Shortest Path Our second set of models use neural networks to compute the weighting function in Equation 2 .", "entities": []}, {"text": "We use an encoder - decoder model with global attention ( Bahdanau et al . , 2014 ;", "entities": []}, {"text": "Luong et al . , 2015 ) , which has the following two components : Encoder Model", "entities": []}, {"text": "The \ufb01rst is an encoder network , which uses a bi - directional recurrent neural network architecture with LSTM units ( Hochreiter and Schmidhuber , 1997 ) to compute a sequence of forward annotations or hidden states ( \u2212 \u2192h1, ... ,\u2212 \u2192h|x|)and a sequence of backward hid1Details about the approx .", "entities": [[18, 19, "MethodName", "LSTM"]]}, {"text": "are provided as supp .", "entities": []}, {"text": "material.den states ( \u2190 \u2212h, ... ,\u2190", "entities": []}, {"text": "\u2212h|x|)for the input sequence ( x1, ... ,x|x| ) .", "entities": []}, {"text": "Standardly , each word is then represented as the concatenation of its forward and backward states :", "entities": []}, {"text": "hj= [ \u2212 \u2192hj,\u2190 \u2212hj ] .", "entities": []}, {"text": "Decoder Model", "entities": []}, {"text": "The second component is a decoder network , which directly computes the conditional distribution p(z|x)as follows : p(z|x )", "entities": []}, {"text": "= |z|/summationdisplay i=1logp\u0398(zi|z < i , x)(3 ) p\u0398(zi|z < i , x)\u223csoftmax ( f(\u0398 , z < i , x))(4 ) wherefis a non - linear function that encodes information about the sequence", "entities": []}, {"text": "z < iand the input xgiven the model parameters \u0398. We can think of this model as an ordinary recurrent language model that is additionally conditioned on the input xusing information from our encoder .", "entities": []}, {"text": "We implement the function fin the following way : f(\u0398 , z < i , x ) = Wo\u03b7i+bo ( 5 ) \u03b7i = MLP(ci , gi ) ( 6 ) gi = LSTM dec(gi\u22121,Eout zi\u22121,ci ) ( 7 ) where MLP is a multi - layer perceptron model with a single hidden layer , Eout\u2208R|\u03a3dec|\u00d7eis a randomly initialized embedding matrix , giis the decoder \u2019s hidden state at step i , andciis a contextvector that encodes information about the input x and the encoder annotations .", "entities": [[33, 34, "MethodName", "LSTM"], [41, 42, "DatasetName", "MLP"]]}, {"text": "Each context vectorciin turn is a weighted sum of each annotationhjagainst an attention vector \u03b1i , j , orci=/summationtext|x| j=1\u03b1i , jhj , which is jointly learned using an additional single layered multi - layer perceptron de\ufb01ned in the following way : \u03b1i , j\u221dexp(ei , j);ei , j = MLP(gi\u22121,hj)(8 ) Lexical Bias and Copying In contrast to standard MT tasks , we are dealing with a relatively low - resource setting where the sparseness of the target vocabulary is an issue .", "entities": []}, {"text": "For this reason , we experimented with integrating lexical translation scores using a biasing technique from Arthur et al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Their method is based on the following computation for each token zi : bias i=\uf8ee \uf8f0pt / prime(z1|x1 ) ...", "entities": []}, {"text": "p t / prime(z1|x|x| ) .........", "entities": []}, {"text": "pt / prime(z|\u03a3dec||x1 ) ...", "entities": []}, {"text": "p t / prime(z|\u03a3dec||x|x|)\uf8f9 \uf8fb\uf8ee \uf8f0\u03b1i,1 ...", "entities": []}, {"text": "\u03b1i,|x|\uf8f9 \uf8fb724", "entities": []}, {"text": "Algorithm 2 Neural Shortest Path Search Input : Input x , DAGG , neural parameters \u0398and non - linear functionf , beam sizel , source node bwith init .", "entities": []}, {"text": "score o. Output : Shortest component path 1 : d[V[G]]\u2190\u221e,d[b]\u2190o , \u03c0[V[G]]\u2190Nil 2 : s[V[G]]\u2190Nil \u22bf Path state information 3 : s[b]\u2190InitState ( ) \u22bfInitialize source state 4 : foreach vertexu\u2265b\u2208V[G]in sorted order do 5 : ifisinf ( d[u])then continue 6 : p\u2190s[u]\u22bfCurrent state at node u , orz < i 7 : L1 [ l]\u2190 arg max ( v1, ... ,v k)\u2208Adj[u]softmax ( f(\u0398 , p , x ) ) 8 : foreach vertex and label ( v , z)\u2208Ldo 9 : score\u2190\u2212 logp\u0398(z|p , x ) + d[u ] 10 : ifd[v]>score then 11 : d[v]\u2190score , \u03c0[v]\u2190u 12 : s[v]\u2190UpdateState ( p , z ) 13 : return FINDPATH(\u03c0,|V|,b )", "entities": []}, {"text": "The \ufb01rst matrix uses the inverse ( pt / prime ) of the lexical translation function ptalready introduced to compute the probability of each word in the target vocabulary \u03a3dec(the columns ) with each word in the input x(the rows ) , which is then weighted by the attention vector from Equation 8 . bias iis then used to modify Equation 5 in the following way : fbias(\u0398 , z < i , x )", "entities": []}, {"text": "= Wo\u03b7i+bo+ log(bias i+/epsilon1 ) where / epsilon1is a hyper - parameter that helps to preserve numerical stability and biases more heavily on the lexical model when set lower .", "entities": []}, {"text": "We also experiment with the copying mechanism from Jia and Liang ( 2016 ) , which works by allowing the decoder to choose from a set of latent actions , aj , that includes writing target words according to Equation 5 , as done standardly , or copying source words from x , orcopy", "entities": []}, {"text": "[ xi]according to the attention scores in Equation 8 .", "entities": []}, {"text": "A distribution is then computed over these actions using a softmax function and particular actions are chosen accordingly during training and decoding .", "entities": [[10, 11, "MethodName", "softmax"]]}, {"text": "Decoding and Learning The full decoding procedure is shown in Algorithm 2 , where the differences with the standard SSSP are again shown in red .", "entities": []}, {"text": "We change the data structure sto contain the decoder \u2019s RNN state at each node .", "entities": []}, {"text": "We also modify the scoring ( line 7 , which uses Equation 4 ) to consider only the top ledges or translations at that point , as opposed to imposing a full search .", "entities": []}, {"text": "When lis set to 1 , for example , the procedure does a greedy search through the graph , whereas when l is large the procedure is closer to a full search .", "entities": []}, {"text": "In general terms , the decoder described aboveworks like an ordinary neural decoder with the difference that each decision ( i.e. , new target - side word translation ) is constrained ( in line 7 ) by the transitions allowed in the underlying graph in order to ensure wellformedness of each component output .", "entities": [[26, 28, "TaskName", "word translation"]]}, {"text": "Standardly , we optimize these models using stochastic gradient descent with the objective of \ufb01nding parameters \u02c6\u0398that minimize the negative conditional log - likelihood of the training dataset .", "entities": [[7, 10, "MethodName", "stochastic gradient descent"], [21, 24, "MetricName", "log - likelihood"]]}, {"text": "4.3 Monolingual vs. Polyglot Decoding Our framework facilitates both monolingual and polyglot decoding .", "entities": []}, {"text": "In the \ufb01rst case , the decoder requires a graph associated with the output semantic language ( more details in next section ) and a trained translation model .", "entities": []}, {"text": "The latter case requires taking the union of all datasets and graphs ( with arti\ufb01cial identi\ufb01er tokens ) for a collection of target datasets and training a single model over this global dataset .", "entities": []}, {"text": "In this setting , we can then decode to a particular language using the language identi\ufb01ers or decode without specifying the output language .", "entities": []}, {"text": "The main focus in this paper is investigating polyglot decoding , and in particular the effect of training models on multiple datasets when translating to individuals APIs or SP datasets .", "entities": []}, {"text": "When evaluating our models and building QA applications , it is important to be able to generate thekbest translations .", "entities": []}, {"text": "This can easily be done in our framework by applying standard kSSSP algorithms ( Brander and Sinclair , 1995 ) .", "entities": []}, {"text": "We use an implementation of the algorithm of Yen ( 1971 ) , which works on top of the SSSP algorithms introduced above by iteratively \ufb01nding deviating or branching paths from an initial SSSP ( more details provided in supplementary materials ) .", "entities": []}, {"text": "5 Experiments We experimented with two main types of resources : 45 API documentation datasets and two multilingual benchmark SP datasets .", "entities": []}, {"text": "In the former case , our main objective is to test whether training polyglot models ( shown as polyglot in Tables 1 - 2 ) on multiple datasets leads to an improvement when compared to training individual monolingual models ( shown as monolingual in Tables 1 - 2 ) .", "entities": []}, {"text": "Experiments involving the latter datasets are meant to test the applicability of our general graph and polyglot method to related SP tasks , and are also used for comparison against our main technical documentation task.725", "entities": []}, {"text": "Figure 3 : Test Acc@1 for the best monolingual models ( in yellow / left ) compared with the best lexical polyglot model ( green / right ) across all 45 technical documentation datasets .", "entities": []}, {"text": "5.1 Datasets Technical API Docs", "entities": []}, {"text": "The \ufb01rst dataset includes the Stdlib and Py27 datasets of Richardson and Kuhn ( 2017b , a ) , which are publicly available via Richardson ( 2017 ) .", "entities": []}, {"text": "Stdlib consists of short description and function signature pairs for 10 programming languages in 7 languages , and Py27 contains the same type of data for 27 popular Python projects in English mined from Github .", "entities": []}, {"text": "We also built new datasets from the Japanese translation of the Python 2.7 standard library , as well as the Lua stdlib documentation in a mixture of Russian , Portuguese , German , Spanish and English .", "entities": []}, {"text": "Taken together , these resources consist of 79,885 training pairs , and we experiment with training models on Stdlib and Py27 separately as well as together ( shown as + more in Table 1 ) .", "entities": []}, {"text": "We use a BPE subword encoding ( Sennrich et al . , 2015 ) of both input and output words to make the representations more similar and transliterated all datasets ( excluding Japanese datasets ) to an 8 - bit latin encoding .", "entities": [[3, 4, "MethodName", "BPE"]]}, {"text": "Graphs were built by concatenating all function representations into a single word list and compiling this list into a minimized DAFSA .", "entities": []}, {"text": "For our global polyglot dataset , this resulted in a graph with 218,505 nodes , 313,288 edges , and 112,107 paths or component representations over an output vocabulary of 9,324 words .", "entities": []}, {"text": "Mixed GeoQuery and Sportscaster We run experiments on the GeoQuery 880 corpus using the splits from Andreas", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2013 ) , which includes geography queries for English , Greek , Thai , and German paired with formal database queries , as well as a seed lexicon or NP list for each language .", "entities": []}, {"text": "In addition to training models on each individual dataset , we also learn polyglot models trained on all datasets concatenated together .", "entities": []}, {"text": "We also created a new mixed language test set that was built by re - placing NPs in 803 test examples with one or more NPs from a different language using the NP lists mentioned above ( see examples in Figure 4 ) .", "entities": []}, {"text": "The goal in the last case is to test our model \u2019s ability to handle mixed language input .", "entities": []}, {"text": "We also ran monolingual experiments on the English Sportscaster corpus , which contains human generated soccer commentary paired with symbolic meaning representation produced by a simulation of four games .", "entities": []}, {"text": "For GeoQuery graph construction , we built a single graph for all languages by extracting general rule templates from all representations in the dataset , and exploited additional information and patterns using the Geobase database and the semantic grammars used in ( Wong and Mooney , 2006b ) .", "entities": [[2, 4, "TaskName", "graph construction"]]}, {"text": "This resulted in a graph with 2,419 nodes , 4,936 edges and 39,482 paths over an output vocabulary of 164 .", "entities": []}, {"text": "For Sportscaster , we directly translated the semantic grammar provided in Chen and Mooney ( 2008 ) to a DAFSA , which resulted in a graph with 98 nodes , 86 edges and 830 paths .", "entities": []}, {"text": "5.2 Experimental Setup For the technical datasets , the goal is to see if our model generates correct signature representations from unobserved descriptions using exact match .", "entities": [[24, 26, "MetricName", "exact match"]]}, {"text": "We follow exactly the experimental setup and data splits from Richardson and Kuhn ( 2017b ) , and measure the accuracy at 1 ( Acc@1 ) , accuracy in top 10 ( Acc@10 ) , and MRR .", "entities": [[20, 21, "MetricName", "accuracy"], [27, 28, "MetricName", "accuracy"], [36, 37, "MetricName", "MRR"]]}, {"text": "For the GeoQuery and Sportscaster experiments , the goal is to see if our models can generate correct meaning representations for unseen input .", "entities": []}, {"text": "For GeoQuery , we follow Andreas", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2013 ) in evaluating extrinsically by checking that each representation evaluates to the same answer as the gold representation when executed against the Geobase database .", "entities": []}, {"text": "For Sportscaster , we evaluate by exact match to a gold representation.726", "entities": [[6, 8, "MetricName", "exact match"]]}, {"text": "Method Acc@1 Acc@10 MRRstdlibmono .", "entities": []}, {"text": "RK Trans", "entities": []}, {"text": "+ rerank 29.9 69.2 43.1 Lexical SP 33.2 70.7 45.9 poly .", "entities": []}, {"text": "Lexical SP + more 33.1 69.7 45.5 Neural SP + bias 12.1 34.3 19.5 Neural SP + copy bias 13.9 36.5 21.5py27mono .", "entities": []}, {"text": "RK Trans", "entities": []}, {"text": "+ rerank 32.4 73.5 46.5 Lexical SP 41.3 77.7 54.1 poly .", "entities": []}, {"text": "Lexical SP + more 40.5 76.7 53.1 Neural SP + bias 8.7 25.5 14.2 Neural SP + copy bias 9.0 26.9 15.1 Table 1 : Test results on the Stdlib and Py27 tasks averaged over all datasets and compared against the best monolingual results from Richardson and Kuhn ( 2017b , a ) , or RK 5.3", "entities": []}, {"text": "Implementation and Model Details We use the Foma \ufb01nite - state toolkit of Hulden ( 2009 ) to construct all graphs used in our experiments .", "entities": []}, {"text": "We also use the Cython version of Dynet ( Neubig et al . , 2017 ) to implement all the neural models ( see supp .", "entities": []}, {"text": "materials for more details ) .", "entities": []}, {"text": "In the results tables , we refer to the lexical and neural models introduced in Section 4 as Lexical Shortest Path andNeural Shortest Path , where models that use copying ( + copy ) and lexical biasing ( + bias ) are marked accordingly .", "entities": []}, {"text": "We also experimented with adding a discriminative reranker to our lexical models ( + rerank ) , using the approach from Richardson and Kuhn ( 2017b ) , which uses additional lexical ( e.g. , word match and alignment ) features and other phrase - level and syntax features .", "entities": []}, {"text": "The goal here is to see if these additional ( mostly non - word level ) features help improve on the baseline lexical models .", "entities": []}, {"text": "6 Results and Discussion Technical Documentation Results Table 1 shows the results for Stdlib and Py27 .", "entities": []}, {"text": "In the monolingual case , we compare against the best performing models in Richardson and Kuhn ( 2017b , a ) .", "entities": []}, {"text": "As summarized in Figure 3 , our experiments show that training polyglot models on multiple datasets can lead to large improvements over training individual models , especially on the Py27 datasets where using a polyglot model resulted in a nearly 9 % average increase in accuracy @1 .", "entities": [[45, 46, "MetricName", "accuracy"]]}, {"text": "In both cases , however , the best performing lexical models are those trained only on the datasets they are evaluated on , as opposed to training on all datasets ( i.e. , + more ) .", "entities": []}, {"text": "This is surprising given that training on all datasets doubles the size of the training data , and shows that adding more data does not necessarily boost performance when the additional data is from another distribution .", "entities": []}, {"text": "Method Acc@1 Acc@10 UBL ( Kwiatkowski et al . , 2010 ) 74.2 \u2013 TreeTrans ( Jones et al . , 2012 ) 76.8 \u2013 nHT ( Susanto and Lu , 2017 ) 83.3 \u2013 Standard Geoquery monolingualLexical Shortest Path 68.6 92.4 Lexical Shortest Path", "entities": []}, {"text": "+ rerank 74.2 94.1 Neural Shortest Path 73.5 91.1 Neural Shortest Path + bias 78.0 92.8 Neural Shortest Path + copy bias 77.8 92.1polyglotLexical Shortest Path 67.3 92.9 Lexical Shortest Path + rerank 75.2 94.7 Neural Shortest Path 78.0", "entities": []}, {"text": "91.4 Neural Shortest Path + bias 78.9 91.7 Neural Shortest Path + copy bias 79.6 91.9Mixed poly .", "entities": []}, {"text": "Best Monolingual Model 4.2 18.2 Lexical Shortest Path", "entities": []}, {"text": "+ rerank 71.1 94.3 Neural Shortest Path + copy bias 75.2 90.0mono .", "entities": []}, {"text": "PCFG ( B \u00a8orschinger et al . , 2011 ) 74.2 \u2013 wo - PCFG ( B \u00a8orschinger et al . , 2011 ) 86.0 \u2013 SportscasterLexical Shortest Path 40.3 86.8 Lexical Shortest Path + rerank 70.3 90.2 Neural Shortest Path 81.9 94.8 Neural Shortest Path + bias 83.4 93.9 Neural Shortest Path + copy bias 83.3 90.5 Table 2 : Test results for the standard ( above ) and mixed ( middle ) GeoQuery tasks averaged over all languages , and results for the English Sportscaster task ( below ) .", "entities": []}, {"text": "The neural models are strongly outperformed by all other models both in the monolingual and polyglot case ( only the latter results shown ) , even when lexical biasing is applied .", "entities": []}, {"text": "While surprising , this is consistent with other studies on lowresource neural MT ( Zoph et al . , 2016 ; \u00a8Ostling and Tiedemann , 2017 ) , where datasets of comparable size to ours ( e.g. , 1 million tokens or less ) typically fail against classical SMT models .", "entities": []}, {"text": "This result has also been found in relation to neural AMR semantic parsing , where similar issues of sparsity are encountered ( Peng et al . , 2017 ) .", "entities": [[11, 13, "TaskName", "semantic parsing"]]}, {"text": "Even by doubling the amount of training data by training on all datasets ( results not shown ) , this did not improve the accuracy , suggesting that much more data is needed ( more discussion below ) .", "entities": [[24, 25, "MetricName", "accuracy"]]}, {"text": "Beyond increases in accuracy , our polyglot models support zero - shot translation as shown in Figure 4 , which can be used for translating between unobserved language pairs ( e.g. , ( es , Clojure ) , ( ru , Haskell ) as shown in 1 - 2 ) , or for \ufb01nding related functionality across different software projects ( as shown in 3 ) .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "These results were obtained by running our decoder model without specifying the output language .", "entities": []}, {"text": "We note , however , that the decoder can be constrained to selectively translate to any speci\ufb01c programming language or project ( e.g. , in a QA setting ) .", "entities": []}, {"text": "Future work will further investigate the decoder \u2019s polyglot capabilities , which is currently hard to evaluate since we do not have an annotated set of function equivalences between different APIs.727", "entities": []}, {"text": "1 .", "entities": []}, {"text": "Source API ( stdlib ): ( es , PHP ) Input :", "entities": []}, {"text": "Devuelve el mensaje asociado al objeto lanzado .", "entities": []}, {"text": "OutputLanguage : PHP Function Translation : public string Throwable::getMessage ( void )", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "Language : Java Function Translation : public String lang.getMessage ( void ) Language :", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "Clojure Function Translation : ( tools.logging.fatal throwable message & more )", "entities": [[2, 3, "TaskName", "Translation"]]}, {"text": "2 . Source API ( stdlib ): ( ru , PHP ) Input : konvertiruet stroku iz formata UTF-32 v format UTF-16.OutputLanguage : PHP Function Translation : string PDF utf32 toutf16 ( ... ) Language : Ruby Function Translation :", "entities": [[25, 26, "TaskName", "Translation"], [38, 39, "TaskName", "Translation"]]}, {"text": "String#toutf16 = > string Language : Haskell Function Translation : Encoding.encodeUtf16LE : : Text - > ByteString 3 .", "entities": [[8, 9, "TaskName", "Translation"]]}, {"text": "Source API ( py ): ( en , stats ) Input : Compute the Moore - Penrose pseudo - inverse of a matrix .", "entities": []}, {"text": "OutputProject : sympy Function Translation : matrices.matrix.base.pinv solve ( B , ... )", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "Project : sklearn Function Translation : utils.pinvh ( a , cond = None , rcond = None , ... )", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "Project : stats Function Translation : tools.pinv2 ( a , cond = None , rcond = None ) 4 . Mixed GeoQuery ( de / gr ) Input : Wie hoch liegt der h \u00a8ochstgelegene punkt in \u0391\u03bb\u03b1 / uni03BC\u03c0\u03ac / uni03BC\u03b1 ?", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "Logical Form Translation : answer(elevation 1(highest(place(loc 2(stateid(\u2019alabama \u2019 ) ) ) ) ) )", "entities": [[2, 3, "TaskName", "Translation"]]}, {"text": "Figure 4 : Examples of zero - shot translation when running in polyglot mode ( 1 - 3 , function representations shown in a conventionalized format ) , and mixed language parsing ( 4 ) .", "entities": []}, {"text": "Semantic Parsing Results SP results are summarized in Table 2 .", "entities": [[0, 2, "TaskName", "Semantic Parsing"]]}, {"text": "In contrast , the neural models , especially those with biasing and copying , strongly outperform all other models and are competitive with related work .", "entities": []}, {"text": "In the GeoQuery case , we compare against two classic grammar - based models , UBL and TreeTrans , as well as a feature rich , neural hybrid tree model ( nHT ) .", "entities": []}, {"text": "We also see that the polyglot Geo achieves the best performance , demonstrating that training on multiple datasets helps in this domain as well .", "entities": []}, {"text": "In the Sportscaster case we compare against two PCFG learning approaches , where the second model ( woPCFG ) involves a grammar with complex wordorder constraints .", "entities": []}, {"text": "The advantage of training a polyglot model is shown on the results related to mixed language parsing ( i.e. , the middle set of results ) .", "entities": []}, {"text": "Here we compared against the best performing monolingual English model ( Best Mono . Model ) , which does not have a way to deal with multilingual NPs .", "entities": []}, {"text": "We also \ufb01nd the neural model to be more robust than the lexical models with reranking .", "entities": []}, {"text": "While the lexical models overall perform poorly on both tasks , the weakness of this model is particularly acute in the Sportscaster case .", "entities": []}, {"text": "We found that mistakes are largely related to the ordering of arguments , which these lexical ( unigram ) models are blind to .", "entities": []}, {"text": "That these models still perform reasonably well on the Geo task shows that such ordering issues are less of a factor in this domain .", "entities": []}, {"text": "Discussion Having results across related SP tasks allows us to re\ufb02ect on the nature of the main technical documentation task .", "entities": []}, {"text": "Consistent with recent \ufb01ndings ( Dong and Lapata , 2016 ) , we show that relatively simple neural sequence models are competitive with , and in some cases outperform , traditional grammar - based SP methods on bench - mark SP tasks .", "entities": []}, {"text": "However , this result is not observed in our technical documentation task , in part because this problem is much harder for neural learners given the sparseness of the target data and lack of redundancy .", "entities": []}, {"text": "For this reason , we believe our datasets provide new challenges for neural - based SP , and serve as a cautionary tale about the scalability and applicability of commonly used neural models to lower - resource SP problems .", "entities": []}, {"text": "In general , we believe that focusing on polyglot and mixed language decoding is not only of interest to applications ( e.g , mixed language API QA ) but also allows for new forms of SP evaluation that are more revealing than only translation accuracy .", "entities": [[44, 45, "MetricName", "accuracy"]]}, {"text": "When comparing the accuracy of the best monolingual Geo model and the worst performing neural polyglot model , one could mistakingly think that these models have equal abilities , though the polyglot model is much more robust and general .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "Moving forward , we hope that our work helps to motivate more diverse evaluations of this type .", "entities": []}, {"text": "7 Conclusion We look at learning from multiple API libraries and datasets in the context of learning to translate text to code representations and other SP tasks .", "entities": []}, {"text": "To support polyglot modeling of this type , we developed a novel graph based decoding method and experimented with various SMT and neural MT models that work in this framework .", "entities": []}, {"text": "We report a mixture of positive results speci\ufb01c to each task and set of models , some of which reveal interesting limitations of different approaches to SP .", "entities": []}, {"text": "We also introduced new API and mixed language datasets to facilitate further work on polyglot SP .", "entities": []}, {"text": "Acknowledgements This work was supported by the German Research Foundation ( DFG ) in project D2 of SFB 732.728", "entities": []}, {"text": "References Miltiadis Allamanis , Earl T Barr , Premkumar Devanbu , and Charles Sutton . 2017 .", "entities": []}, {"text": "A Survey of Machine Learning for Big Code and Naturalness .", "entities": []}, {"text": "arXiv preprint arXiv:1709.06182 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jacob Andreas , Andreas Vlachos , and Stephen Clark . 2013 .", "entities": []}, {"text": "Semantic parsing as machine translation .", "entities": [[0, 2, "TaskName", "Semantic parsing"], [3, 5, "TaskName", "machine translation"]]}, {"text": "In in Proceedings of ACL-2013 .", "entities": []}, {"text": "pages 47\u201352 .", "entities": []}, {"text": "Philip Arthur , Graham Neubig , and Satoshi Nakamura . 2016 .", "entities": []}, {"text": "Incorporating Discrete Translation Lexicons into Neural Machine Translation .", "entities": [[2, 3, "TaskName", "Translation"], [6, 8, "TaskName", "Machine Translation"]]}, {"text": "arXiv preprint arXiv:1606.02006 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Neural Machine Translation by Jointly Learning to Align and Translate .", "entities": [[1, 3, "TaskName", "Machine Translation"]]}, {"text": "arXiv preprint arXiv:1409.0473 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jonathan Berant , Andrew Chou , Roy Frostig , and Percy Liang .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Semantic Parsing on Freebase from Question - Answer Pairs .", "entities": [[0, 2, "TaskName", "Semantic Parsing"]]}, {"text": "In in Proceedings of EMNLP-2013 .", "entities": []}, {"text": "pages 1533\u20131544 .", "entities": []}, {"text": "Benjamin B \u00a8orschinger , Bevan K. Jones , and Mark Johnson .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Reducing grounded learning tasks to grammatical inference .", "entities": []}, {"text": "In Proceedings of EMNLP2011 .", "entities": []}, {"text": "pages 1416\u20131425 .", "entities": []}, {"text": "AW Brander and MC Sinclair .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "A Comparative Study of k - Shortest Path Algorithms .", "entities": []}, {"text": "In In Proc .", "entities": []}, {"text": "of 11th UK Performance Engineering Workshop", "entities": []}, {"text": ".", "entities": []}, {"text": "David L. Chen , Joohyun Kim , and Raymond J. Mooney .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Training a Multilingual Sportscaster : Using Perceptual Context to Learn Language .", "entities": []}, {"text": "Journal of Arti\ufb01cial Intelligence Research 37:397\u2013435 .", "entities": []}, {"text": "David L. Chen and Raymond J. Mooney . 2008 .", "entities": []}, {"text": "Learning to Sportscast : A Test of Grounded Language Acquisition .", "entities": [[8, 10, "TaskName", "Language Acquisition"]]}, {"text": "In Proceedings of ICML-2008 .", "entities": []}, {"text": "pages 128\u2013135 .", "entities": []}, {"text": "Jianpeng Cheng , Siva Reddy , Vijay Saraswat , and Mirella Lapata . 2017 .", "entities": []}, {"text": "Learning Structured Natural Language Representations for Semantic Parsing .", "entities": [[6, 8, "TaskName", "Semantic Parsing"]]}, {"text": "arXiv preprint arXiv:1704.08387 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "T Cormen , C Leiserson , R Rivest , and C Stein .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Introduction to Algorithms .", "entities": []}, {"text": "MIT Press .", "entities": []}, {"text": "Huijing Deng and Grzegorz Chrupa\u0142a .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Semantic Approaches to Software Component Retrieval with English Queries .", "entities": []}, {"text": "In Proceedings of LREC-14 .", "entities": []}, {"text": "pages 441\u2013450 .", "entities": []}, {"text": "Li Dong and Mirella Lapata .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Language to Logical Form with Neural Attention .", "entities": []}, {"text": "arXiv preprint arXiv:1601.01280 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Long Duong , Hadi Afshar , Dominique Estival , Glen Pink , Philip Cohen , and Mark Johnson . 2017 .", "entities": []}, {"text": "Multilingual Semantic Parsing and Code - Switching .", "entities": [[1, 3, "TaskName", "Semantic Parsing"]]}, {"text": "CoNLL 2017 page 379.Xiaodong Gu , Hongyu Zhang , Dongmei Zhang , and Sunghun Kim . 2016 .", "entities": []}, {"text": "Deep API Learning .", "entities": []}, {"text": "arXiv preprint arXiv:1605.08535 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jonathan Herzig and Jonathan Berant . 2017 .", "entities": []}, {"text": "Neural Semantic Parsing over Multiple Knowledge - Bases .", "entities": [[1, 3, "TaskName", "Semantic Parsing"]]}, {"text": "arXiv preprint arXiv:1702.01569 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sepp Hochreiter and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long Short - Term Memory .", "entities": [[0, 5, "MethodName", "Long Short - Term Memory"]]}, {"text": "Neural computation 9(8 ) .", "entities": []}, {"text": "Liang Huang . 2008 .", "entities": []}, {"text": "Advanced Dynamic Programming in Semiring and Hypergraph Frameworks .", "entities": []}, {"text": "In Proceedings of COLING-2008 ( tutorial notes ) .", "entities": []}, {"text": "Mans Hulden .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Foma : a Finite - State Compiler and Library .", "entities": []}, {"text": "In Proceedings of EACL .", "entities": []}, {"text": "Srinivasan Iyer , Ioannis Konstas , Alvin Cheung , and Luke Zettlemoyer .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Summarizing Source Code using a Neural Attention Model .", "entities": [[0, 8, "DatasetName", "Summarizing Source Code using a Neural Attention Model"]]}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "Robin Jia and Percy Liang . 2016 .", "entities": []}, {"text": "Data Recombination for Neural Semantic Parsing .", "entities": [[4, 6, "TaskName", "Semantic Parsing"]]}, {"text": "arXiv preprint arXiv:1606.03622 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhanming Jie and Wei Lu . 2014 .", "entities": []}, {"text": "Multilingual Semantic Parsing :", "entities": [[1, 3, "TaskName", "Semantic Parsing"]]}, {"text": "Parsing Multiple Languages into Semantic Representations .", "entities": []}, {"text": "In COLING .", "entities": []}, {"text": "pages 1291 \u2013 1301 .", "entities": []}, {"text": "Donald B Johnson .", "entities": []}, {"text": "1977 .", "entities": []}, {"text": "Ef\ufb01cient Algorithms for Shortest Paths in Sparse Networks .", "entities": []}, {"text": "Journal of the ACM ( JACM ) 24(1):1\u201313 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Melvin Johnson , Mike Schuster , Quoc V Le , Maxim Krikun , Yonghui Wu , Zhifeng Chen , Nikhil Thorat , Fernanda Vi \u00b4 egas , Martin Wattenberg , Greg Corrado , et al . 2016 .", "entities": []}, {"text": "Google \u2019s Multilingual Neural Machine Translation System : Enabling Zero - Shot Translation .", "entities": [[0, 1, "DatasetName", "Google"], [4, 6, "TaskName", "Machine Translation"], [12, 13, "TaskName", "Translation"]]}, {"text": "arXiv preprint arXiv:1611.04558 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Bevan Keeley Jones , Mark Johnson , and Sharon Goldwater .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Semantic Parsing with Bayesian Tree Transducers .", "entities": [[0, 2, "TaskName", "Semantic Parsing"]]}, {"text": "In Proceedings of ACL-2012 .", "entities": []}, {"text": "pages 488\u2013496 .", "entities": []}, {"text": "Kevin Knight .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Decoding Complexity in WordReplacement Translation Models .", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "Computational linguistics 25(4):607\u2013615 .", "entities": []}, {"text": "Kevin Knight and Yaser Al - Onaizan .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "Translation with Finite - state Devices .", "entities": [[0, 1, "TaskName", "Translation"]]}, {"text": "In Proceedings of AMTA . Tom\u00b4a\u02c7s", "entities": []}, {"text": "Ko \u02c7cisk\u00b4y , G \u00b4 abor Melis , Edward Grefenstette , Chris Dyer , Wang Ling , Phil Blunsom , and Karl Moritz Hermann .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semantic Parsing with Semi - Supervised Sequential Autoencoders .", "entities": [[0, 2, "TaskName", "Semantic Parsing"], [7, 8, "MethodName", "Autoencoders"]]}, {"text": "In Proceedings of EMNLP-16 .", "entities": []}, {"text": "pages 1078\u20131087 .", "entities": []}, {"text": "Tom Kwiatkowski , Luke Zettlemoyer , Sharon Goldwater , and Mark Steedman .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Inducing Probabilistic CCG Grammars from Logical Form with Higherorder Uni\ufb01cation .", "entities": []}, {"text": "In Proceedings of EMNLP-2010 .", "entities": []}, {"text": "pages 1223\u20131233.729", "entities": []}, {"text": "Percy Liang .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Lambda Dependency - Based Compositional Semantics .", "entities": []}, {"text": "arXiv preprint arXiv:1309.4408 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Minh - Thang Luong , Hieu Pham , and Christopher D Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Effective Approaches to AttentionBased Neural Machine Translation .", "entities": [[5, 7, "TaskName", "Machine Translation"]]}, {"text": "arXiv preprint arXiv:1508.04025 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Antonio Valerio Miceli Barone and Rico Sennrich .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation .", "entities": [[14, 16, "TaskName", "Code Generation"]]}, {"text": "arXiv preprint arXiv:1707.02275 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Dipendra Kumar Misra and Yoav Artzi .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "2016 .", "entities": []}, {"text": "Neural Shift - Reduce CCG Semantic Parsing .", "entities": [[5, 7, "TaskName", "Semantic Parsing"]]}, {"text": "In EMNLP .", "entities": []}, {"text": "pages 1775\u20131786 .", "entities": []}, {"text": "Mehryar Mohri .", "entities": []}, {"text": "1996 .", "entities": []}, {"text": "On Some Applications of Finite - State Automata Theory to Natural Language Processing .", "entities": []}, {"text": "Natural Language Engineering 2(1):61\u201380 .", "entities": []}, {"text": "Raymond Mooney .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Learning for Semantic Parsing .", "entities": [[2, 4, "TaskName", "Semantic Parsing"]]}, {"text": "In Proceedings of CICLING .", "entities": []}, {"text": "Graham Neubig , Chris Dyer , Yoav Goldberg , Austin Matthews , Waleed Ammar , Antonios Anastasopoulos , Miguel Ballesteros , David Chiang , Daniel Clothiaux , Trevor Cohn , Kevin Duh , Manaal Faruqui , Cynthia Gan , Dan Garrette , Yangfeng Ji , Lingpeng Kong , Adhiguna Kuncoro , Gaurav Kumar , Chaitanya Malaviya , Paul Michel , Yusuke Oda , Matthew Richardson , Naomi Saphra , Swabha Swayamdipta , and Pengcheng Yin . 2017 .", "entities": [[52, 53, "DatasetName", "Kumar"]]}, {"text": "Dynet : The Dynamic Neural Network Toolkit .", "entities": []}, {"text": "arXiv preprint arXiv:1701.03980 https://github . com / clab / dynet .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Franz Josef Och and Hermann Ney . 2003 .", "entities": []}, {"text": "A Systematic Comparison of Various Statistical Alignment Models .", "entities": []}, {"text": "Computational linguistics 29(1):19\u201351 .", "entities": []}, {"text": "Robert \u00a8Ostling and J \u00a8org Tiedemann . 2017 .", "entities": []}, {"text": "Neural Machine Translation for Low - resource Languages .", "entities": [[1, 3, "TaskName", "Machine Translation"]]}, {"text": "arXiv preprint arXiv:1708.05729 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xiaochang Peng , Chuan Wang , Daniel Gildea , and Nianwen Xue . 2017 .", "entities": []}, {"text": "Addressing the Data Sparsity Issue in Neural AMR Parsing .", "entities": [[7, 9, "TaskName", "AMR Parsing"]]}, {"text": "Proceedings of ACL .", "entities": []}, {"text": "Maxim Rabinovich , Mitchell Stern , and Dan Klein . 2017 .", "entities": []}, {"text": "Abstract Syntax Networks for Code Generation and Semantic Parsing .", "entities": [[4, 6, "TaskName", "Code Generation"], [7, 9, "TaskName", "Semantic Parsing"]]}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "Kyle Richardson . 2017 .", "entities": []}, {"text": "Code - Datasets .", "entities": []}, {"text": "https:// github.com/yakazimir/Code-Datasets .", "entities": []}, {"text": "Kyle Richardson .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A Language for Function Signature Representations .", "entities": []}, {"text": "arXiv preprint arXiv:1804.00987 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kyle Richardson and Jonas Kuhn . 2017a .", "entities": []}, {"text": "Function Assistant : A Tool for NL Querying of APIs .", "entities": []}, {"text": "In Proceedings of EMNLP .Kyle", "entities": []}, {"text": "Richardson and Jonas Kuhn . 2017b .", "entities": []}, {"text": "Learning Semantic Correspondences in Technical Documentation .", "entities": []}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "Kyle Richardson , Sina Zarrie\u00df , and Jonas Kuhn . 2017 .", "entities": []}, {"text": "The Code2Text Challenge : Text Generation in Source Code Libraries .", "entities": [[4, 6, "TaskName", "Text Generation"]]}, {"text": "In Proceedings of INLG .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2015 .", "entities": []}, {"text": "Neural Machine Translation of Rare Words with Subword Units .", "entities": [[1, 3, "TaskName", "Machine Translation"]]}, {"text": "arXiv preprint arXiv:1508.07909 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Raymond Hendy Susanto and Wei Lu . 2017 .", "entities": []}, {"text": "Semantic Parsing with Neural Hybrid Trees .", "entities": [[0, 2, "TaskName", "Semantic Parsing"]]}, {"text": "In AAAI .", "entities": []}, {"text": "pages 3309\u20133315 .", "entities": []}, {"text": "Yulia Tsvetkov , Sunayana Sitaram , Manaal Faruqui , Guillaume Lample , Patrick Littell , David Mortensen , Alan W Black , Lori Levin , and Chris Dyer . 2016 .", "entities": []}, {"text": "Polyglot Neural Language Models : A Case Study in Cross - Lingual Phonetic Representation Learning .", "entities": [[13, 15, "TaskName", "Representation Learning"]]}, {"text": "arXiv preprint arXiv:1605.03832 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yuk Wah Wong and Raymond J. Mooney .", "entities": []}, {"text": "2006a .", "entities": []}, {"text": "Learning for semantic parsing with statistical machine translation .", "entities": [[2, 4, "TaskName", "semantic parsing"], [6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of HLT - NAACL2006 .", "entities": []}, {"text": "pages 439\u2013446 .", "entities": []}, {"text": "Yuk Wah Wong and Raymond J Mooney .", "entities": []}, {"text": "2006b .", "entities": []}, {"text": "Learning for Semantic Parsing with Statistical Machine Translation .", "entities": [[2, 4, "TaskName", "Semantic Parsing"], [6, 8, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of NACL .", "entities": []}, {"text": "Jin Y Yen .", "entities": []}, {"text": "1971 .", "entities": []}, {"text": "Finding the k Shortest Loopless Paths in a Network .", "entities": []}, {"text": "Management Science 17(11):712 \u2013 716 .", "entities": [[0, 1, "TaskName", "Management"]]}, {"text": "Pengcheng Yin and Graham Neubig .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A Syntactic Neural Model for General - Purpose Code Generation .", "entities": [[5, 6, "DatasetName", "General"], [8, 10, "TaskName", "Code Generation"]]}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "John M Zelle and Raymond J Mooney .", "entities": []}, {"text": "1996 .", "entities": []}, {"text": "Learning to Parse Database Queries using Inductive Logic Programming .", "entities": [[6, 9, "TaskName", "Inductive Logic Programming"]]}, {"text": "In Proceedings of AAAI-1996 .", "entities": []}, {"text": "pages 1050\u20131055 .", "entities": []}, {"text": "Luke S. Zettlemoyer and Michael Collins .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Learning context - dependent mappings from sentences to logical form .", "entities": []}, {"text": "In Proceedings of ACL-2009 .", "entities": []}, {"text": "pages 976\u2013984 .", "entities": []}, {"text": "Luke S Zettlemoyer and Michael Collins .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Learning to Map Sentences to Logical Form : Structured Classi\ufb01cation with Probabilistic Categorial Grammars .", "entities": []}, {"text": "arXiv preprint arXiv:1207.1420 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Barret Zoph , Deniz Yuret , Jonathan May , and Kevin Knight .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Transfer Learning for Low - resource Neural Machine Translation .", "entities": [[0, 2, "TaskName", "Transfer Learning"], [3, 9, "TaskName", "Low - resource Neural Machine Translation"]]}, {"text": "Proceedings of ACL .730", "entities": []}]