[{"text": "Proceedings of the Fourth Arabic Natural Language Processing Workshop , pages 208\u2013213 Florence , Italy , August 1 , 2019 .", "entities": [[12, 13, "MethodName", "Florence"]]}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics208ZCU - NLP at MADAR 2019 :", "entities": []}, {"text": "Recognizing Arabic Dialects Pavel P \u02c7rib\u00b4a\u02c7n Stephen Taylor pribanp@kiv.zcu.cz stepheneugenetaylor@gmail.com", "entities": []}, {"text": "Department of Computer Science and Engineering , Faculty of Applied Sciences , University of West Bohemia , Pilsen , Czech Republic http://nlp.kiv.zcu.cz", "entities": []}, {"text": "Abstract In this paper , we present our systems for the MADAR Shared Task : Arabic Fine - Grained Dialect Identi\ufb01cation .", "entities": []}, {"text": "The shared task consists of two subtasks .", "entities": []}, {"text": "The goal of Subtask \u2013 1 ( S-1 ) is to detect an Arabic city dialect in a given text and the goal of Subtask\u20132 ( S-2 ) is to predict the country of origin of a Twitter user by using tweets posted by the user .", "entities": []}, {"text": "InS-1 , our proposed systems are based on language modelling .", "entities": [[8, 10, "TaskName", "language modelling"]]}, {"text": "We use language models to extract features that are later used as an input for other machine learning algorithms .", "entities": []}, {"text": "We also experiment with recurrent neural networks ( RNN ) , but these experiments showed that simpler machine learning algorithms are more successful .", "entities": []}, {"text": "Our system achieves 0:658 macro F1 - score and our rank is 6thout of 19 teams in S-1 and 7thinS-2 with 0:475 macro F1 - score .", "entities": [[5, 8, "MetricName", "F1 - score"], [23, 26, "MetricName", "F1 - score"]]}, {"text": "1 Introduction The Madar shared tasks ( Bouamor et al . , 2019 ) are a follow - up to Salameh \u2019s", "entities": []}, {"text": "( Salameh et al . , 2018 ) work with the synthetic corpus of Bouamor ( Bouamor et al . , 2014 ) and Salameh \u2019s work with tweets based on the corpus .", "entities": []}, {"text": "Two corpora are provided , a six - city corpus of travel sentences rendered into the dialects of \ufb01ve cities and MSA1 , and a 25 - city + MSA corpus using a smaller number of sentences .", "entities": []}, {"text": "In the \ufb01rst task , test data is classi\ufb01ed as one of the 25 cities or MSA .", "entities": []}, {"text": "For the second task , the organizers chose training , development and test tweet - sets for download from Twitter .", "entities": []}, {"text": "The tweets are from 21 Arabic countries , and the goal is to determine , for each tweet author , the country of origin .", "entities": []}, {"text": "ForS-1 we did not use any external data , only data provided by the shared task organizers .", "entities": []}, {"text": "1Modern Standard ArabicThe organizers provided training and development data2consisting of sentences in different dialects with a label denoting the corresponding dialect .", "entities": []}, {"text": "The training data contain 41 K sentences and development data contain 5.2 K sentences .", "entities": []}, {"text": "Organizers also provided additional data with Arabic sentences in seven dialects .", "entities": []}, {"text": "S-2 uses a corpus of tweets .", "entities": []}, {"text": "Twitter does not permit the organizers to distribute tweets , only the user ids and tweet ids .", "entities": []}, {"text": "Every participant must arrange with Twitter to download the tweets themselves , and because tweets are subject to deletion over time , it is possible that each participant \u2019s version of the corpus and test is unique .", "entities": []}, {"text": "2 Related Work The Arabic dialects have a common written form and uni\ufb01ed literary tradition , so it seems most logical to distinguish dialects on the basis of acoustics , and there is a fair amount of work there , including Hanani et al .", "entities": []}, {"text": "( 2013 , 2015 ) ; Ali et al .", "entities": []}, {"text": "( 2016 ) .", "entities": []}, {"text": "\u2014 Biadsy et al .", "entities": []}, {"text": "( 2009 ) distinguish four Arabic dialects and MSA based on ( audio ) phone sequences ; the phones were obtained by phone recognizers for English , German , Japanese , Hindi , Mandarin , Spanish , and three different MSA phone - recognizer implementations .", "entities": []}, {"text": "The dialects were distinguished by phoneme sequences , and the results of classi\ufb01cations based on each phonerecognizer were combined using a logistic regression classi\ufb01er .", "entities": [[21, 23, "MethodName", "logistic regression"]]}, {"text": "They train on 150 hours per dialect of telephone recordings .", "entities": []}, {"text": "They report 61 % accuracy on 5 - second segments , and 84 % accuracy on 120 second segments .", "entities": [[4, 5, "MetricName", "accuracy"], [14, 15, "MetricName", "accuracy"]]}, {"text": "Zaidan and Callison - Burch ( 2011 ) describe building a text corpus , based on reader commen2The participants were not allowed to use these data for any training purposes .", "entities": []}, {"text": "209tary on newspaper websites , with signi\ufb01cant dialect content ; the goal is to provide a corpus to improve machine translation for Arabic dialects .", "entities": [[19, 21, "TaskName", "machine translation"]]}, {"text": "They used Amazon Mechanical Turk to provide annotation for a portion of the corpus .", "entities": []}, {"text": "Zaidan and Callison - Burch ( 2014 ) describe the same work in greater detail , including dialect classi\ufb01ers they built using the Mechanical Turk data for classes and origin metadata as additional features .", "entities": []}, {"text": "They say these classi\ufb01ers are \u2018 approaching human quality . \u2019", "entities": []}, {"text": "ElFardy and Diab ( 2013 ) classify EGY3and MSA sentences from the Zaidan and CallisonBurch ( 2011 ) corpus , that is , from text .", "entities": []}, {"text": "Not only is this a binary task , but orthographic hints , including repeated long vowels , emojis and multiple punctuations , give strong clues of the register , and hence whether MSA is being employed .", "entities": []}, {"text": "They do a number of experiments comparing various preprocessing schemes and different training sizes , ranging from 2 - 28 million tokens .", "entities": []}, {"text": "They achieve 80 % \u2013 86 % accuracy for all of their attempts .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "Malmasi et al . ( 2015 ) do Arabic dialect identi\ufb01cation from text corpora , including the MultiDialect Parallel Corpus of Arabic ( Bouamor et al . , 2014 ) and the Arabic Online Commentary database ( Zaidan and Callison - Burch , 2011 ) .", "entities": []}, {"text": "Hanani et al .", "entities": []}, {"text": "( 2015 ) perform recognition of several Palestinian regional accents , evaluating four different acoustic models , achieving 81.5 % accuracy for their best system , an I - vector framework with 64 Gaussian components .", "entities": [[20, 21, "MetricName", "accuracy"]]}, {"text": "Ali et al .", "entities": []}, {"text": "( 2016 ) developed the corpus on which the DSL Arabic shared task is based .", "entities": []}, {"text": "Their own dialect detection efforts depended largely on acoustical cues .", "entities": []}, {"text": "Arabic dialect recognition appeared in the 2016 edition of the VarDial workshop \u2019s shared task ( Malmasi et al . , 2016 ) .", "entities": []}, {"text": "The shared task data was text - only .", "entities": []}, {"text": "The best classi\ufb01ers ( Malmasi et al . , 2016 ; Ionescu and Popescu , 2016 ) for the shared task performed far below the best results reported by some of the preceding researchers , in particular Ali et al . ( 2016 ) which used some of the same data .", "entities": []}, {"text": "Part of the reason must be that the amount of training data for the workshop is much smaller than that used by some of the other researchers ; the workshop data also did not include the audio recordings on which the transcripts are based .", "entities": []}, {"text": "3Egyptian dialectThe absence of audio was remedied for the 2017 and 2018 VarDial workshops , ( Zampieri et al . , 2017 , 2018 )", "entities": []}, {"text": "However , the \ufb01ve dialects plus MSA targeted by the VarDial shared task comprise a small fraction of Arabic \u2019s dialectical variation .", "entities": []}, {"text": "Salameh et al .", "entities": []}, {"text": "( Salameh et al . , 2018 ) use a corpus ( Bouamor et al . , 2018 ) which differentiates between twenty\ufb01ve different cities and MSA .", "entities": []}, {"text": "This still does n\u2019t address urban rural divides , but it begins to re\ufb02ect more realistic diversity .", "entities": []}, {"text": "3 Overview 3.1 Language Modelling InS-1 , both of our systems used for the of\ufb01cial submission take as an input language model features .", "entities": [[3, 5, "TaskName", "Language Modelling"]]}, {"text": "In our case the objective of a language model in its simplest form is to predict probabilityp(S)of sentence Swhich is composed from strings ( words or character n - grams ) s1 ; s2 : : : s N , where Nis a number of strings in the sentence .", "entities": []}, {"text": "The probability estimation of p(S)can be computed as a product of conditional probabilities p(sijhi)of its strings s1 ; s2 : : : s N , where hiis a history of a string si .", "entities": []}, {"text": "The probability of string siis conditioned by history hii.e.n\u00001preceding strings si\u0000n+1 ; si\u0000n+2 ; : : : s i\u00001which can be rewritten as si\u00001 i\u0000n+1 .", "entities": []}, {"text": "The resulting formula for the p(S)estimation looks as follows : p(S )", "entities": []}, {"text": "= NY i=1p(sijhi )", "entities": []}, {"text": "= NY i=1p(sijsi\u00001 i\u0000n+1)(1 )", "entities": []}, {"text": "The conditioned probability p(sijhi)can be estimated with Maximum Likelihood Estimate ( MLE ) which is de\ufb01ned as : pMLE(sijhi ) = c(si\u0000n+1 ; si\u0000n+2 : : : s i ) c(si\u0000n+1 ; si\u0000n+2 : : : s i\u00001)(2 ) where c(si\u0000n+1 ; si\u0000n+2 : : : s i)is a number of occurrences of string siwith history hiand c(si\u0000n+1 ; si\u0000n+2 : : : s i\u00001)is a number of occurrences of history hi .", "entities": []}, {"text": "These counts are taken from a training corpus .", "entities": []}, {"text": "We followed Salameh ( Salameh et al . , 2018 ) in using the kenlm language modelling tool ( Hea\ufb01eld et al . , 2013 ) .", "entities": [[15, 17, "TaskName", "language modelling"]]}, {"text": "kenlm does n\u2019t have an option to use character n - grams instead of words ,", "entities": []}, {"text": "210so in order to get character - based language models , we prepared input \ufb01les with characters separated by spaces .", "entities": []}, {"text": "Instead of encoding space as a special word , we surrounded words with a < w></w > pair .", "entities": []}, {"text": "This enables noticing strings which occur at the beginning or end of a word ( as would a special sequence for space ) but reduces the possible amount of inter - word information which the language model can keep for a given order , the parameter which indicates to kenlm the largest n - gram to index .", "entities": []}, {"text": "We used order 5 for all our kenlm language models .", "entities": []}, {"text": "We prebuilt models for each dialect .", "entities": []}, {"text": "We prepared six directories , each containing word or character models for each dialect in one of the three corpora .", "entities": []}, {"text": "We wrote a LangModel class which quacks like a sklearn classi\ufb01er , that is , it supports fit ( ) , predict ( ) , andpredict proba ( ) , but its choices are based on a directory of language models .", "entities": []}, {"text": "predict ( ) returns the dialect name whose model gives the highest score .", "entities": []}, {"text": "predict proba ( ) provides a list of languagemodel - score features , adjusted to probabilities .", "entities": []}, {"text": "4 Subtask\u20131 System Description In this section we describe our models4 .", "entities": []}, {"text": "We submitted results for the S-1 from two systems \u2013 Tortuous Classi\ufb01er andNeural Network Classi\ufb01er .", "entities": []}, {"text": "4.1 Tortuous Classi\ufb01er", "entities": []}, {"text": "This submission uses a jumble of features and classi\ufb01ers , most from the sklearn module ( Buitinck et al . , 2013 ) .", "entities": []}, {"text": "The \ufb01nal classi\ufb01er is a hard voting classi\ufb01er with three input streams :", "entities": []}, {"text": "1 . Soft voting classi\ufb01er on : ( a ) Multinomial naive Bayes classi\ufb01er on word 1 - 2grams , ( b ) Multinomial naive Bayes classi\ufb01er on char 3 - 5grams , ( c ) Language model scores adjusted to probabilities , for word - based language models of the corpus 26 dialects ( d ) Language model scores adjusted to probabilities , for char - based language models of the corpus 26 dialects ( e )", "entities": []}, {"text": "Multinomial naive Bayes classi\ufb01er on language - model - scores for character and language models on the 4The source code is available at https://github . com / StephenETaylor / Madar-2019corpus-6 language models and character language models for the corpus-26 language models .", "entities": []}, {"text": "2 . Support vector machine , svm .", "entities": [[2, 5, "MethodName", "Support vector machine"], [6, 7, "MethodName", "svm"]]}, {"text": "SVC ( gamma=\u2019scale \u2019 , kernel = \u2019 poly \u2019 , degree = 2 ) with the same features as item 1e .", "entities": []}, {"text": "3 .", "entities": []}, {"text": "Multinomial naive Bayes classi\ufb01er using word and char language model features for corpus-6 and corpus-26 features , t\ufb01d vectorized word 1 - 2grams , and t\ufb01d vectorized char 3 - 5grams .", "entities": []}, {"text": "The classi\ufb01er did better on the development data , suggesting that it is over-\ufb01tted , but the language model features , which are the most predictive , also did better on the development data .", "entities": []}, {"text": "4.2 Neural Network Classi\ufb01er We experimented with several neural networks .", "entities": []}, {"text": "Our model for the S-1 submission uses as input 26 features which correspond to one of our 26 pretrained dialect language models .", "entities": []}, {"text": "Each feature represents the probability of a given sentence for one language model .", "entities": []}, {"text": "The probability scores measure how close each sentence is to the dialect .", "entities": []}, {"text": "We train Multilayer Perceptron ( MLP ) with one hidden ( dense ) layer with 400 units .", "entities": [[5, 6, "DatasetName", "MLP"]]}, {"text": "The output of the hidden layer is passed to a \ufb01nal fullyconnected softmax layer .", "entities": [[12, 13, "MethodName", "softmax"]]}, {"text": "The output of the softmax layer is a probability distribution over all 26 classes .", "entities": [[4, 5, "MethodName", "softmax"]]}, {"text": "The class with the highest probability is predicted as a \ufb01nal output of our model .", "entities": []}, {"text": "As an activation function in the hidden layer of the MLP a Recti\ufb01ed Linear Unit ( ReLu ) is employed .", "entities": [[2, 4, "HyperparameterName", "activation function"], [10, 11, "DatasetName", "MLP"], [16, 17, "MethodName", "ReLu"]]}, {"text": "We also tried to combine character n - gram features with the language model features .", "entities": []}, {"text": "The input is a sequence of \ufb01rst 200 character n - grams of a given text .", "entities": []}, {"text": "Each sequence of character n - grams is used as a separate input followed by a randomly initialized embedding layer and then two layers of Bidirectional LSTM ( BiLSTM)(Graves and Schmidhuber , 2005 ) with 64 units are employed ( see Figure 1 ) .", "entities": [[25, 27, "MethodName", "Bidirectional LSTM"]]}, {"text": "The output vector of the BiLSTM layers is concatenated with the language model features and this concatenated vector is passed to the MLP layer with 400 units ( the same as described above ) .", "entities": [[5, 6, "MethodName", "BiLSTM"], [22, 23, "DatasetName", "MLP"]]}, {"text": "All models were implemented by using Keras ( Chollet et al . , 2015 ) with TensorFlow backend ( Abadi et al . , 2015 )", "entities": []}, {"text": "211 text n - gramsBiLSTM BiLSTMCharacter \u00a0 Embedings BiLSTM BiLSTM 200 x 1501 x ( n*128 + 26)Language model featuresDense LayerConcatenated Vector SoftmaxFigure 1 : Neural network model architecture 4.3", "entities": [[8, 9, "MethodName", "BiLSTM"], [9, 10, "MethodName", "BiLSTM"]]}, {"text": "Neural Netwok Model Training", "entities": []}, {"text": "We tune all hyperparameters on the development data .", "entities": []}, {"text": "We train our model with Adam ( Kingma and Ba , 2014 ) optimizer with learning rate 0:01and without any dropout .", "entities": [[5, 6, "MethodName", "Adam"], [13, 14, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"]]}, {"text": "The number of epochs is 800 and we do not use mini - batches or dropout regularization technique .", "entities": [[1, 4, "HyperparameterName", "number of epochs"]]}, {"text": "The model with these hyperparameters achieves the best result ( 0:661macro F1 - score ) on the development data and was used for the \ufb01nal submission .", "entities": [[11, 14, "MetricName", "F1 - score"]]}, {"text": "We also experimented with the n - gram inputs .", "entities": []}, {"text": "We tried a different number of character n - grams and we achieve the best result ( 0:555macro F1score ) on the development data using three inputs - character unigrams , bigrams and trigrams , with learning rate 0:005 , mini - batches of size 256 for 11 epochs and with the Adam optimizer .", "entities": [[36, 38, "HyperparameterName", "learning rate"], [52, 53, "MethodName", "Adam"], [53, 54, "HyperparameterName", "optimizer"]]}, {"text": "5 Subtask\u20132 System Description Our tortuous classi\ufb01er did less well on the tweet data , so we used a simpler classi\ufb01er .", "entities": []}, {"text": "The features are the kenlm language model scores for the 21 countries , computed for each of the training tweets , then exponentiated and normalized to sum to 1 .", "entities": []}, {"text": "The tweets are classi\ufb01ed using y_test = KNeighborsClassifier ( n_neighbors=31 ) .fit(X_train , y_train ) .predict(X_test )", "entities": []}, {"text": "The users are predicted based on the plurality prediction for all of their tweets , that is , the country to which the largest number of their tweets were assigned .", "entities": []}, {"text": "There were a signi\ufb01cant number of tweets unavailable , about 10 % in the training and development sets , and 12 % in the test set .", "entities": []}, {"text": "After the submissions had closed we experimented with eliminating the unavailable and non - Arabic tweets from ALEALGALXAMMASWBAGBASBEIBENCAIDAMDOHFESJEDJERKHAMOSMSAMUSRABRIYSALSANSFXTRITUN Predicted labelsALE", "entities": []}, {"text": "ALG ALX AMM ASW BAG BAS BEI BEN CAI DAM DOH FES JED JER KHA MOS", "entities": []}, {"text": "MSA MUS RAB RIY SAL SAN SFX TRI TUNTrue labels127 001234170030100801000031011 0160 110100800171141422210111 03149 4161001101112180000002000 902118 21062897021830020271010 00214134 1100191413420000310010 20052127251311301043310925200 1112129128 1201601008230523210 8107211129 2318001930121350021 25069311131 218042000206141110 64193372002100 2115190000301022 1601164421723108 4121000000081010 11042231110150 18450131421121 071120000214136 30201231111301 11164002533141123 1601301055131 721262127511602121 20010291010 0238101110113042147 11200001020 1000038010000100167 110645011 17220100211000191158 40711010 2313131151315181100328311644010 17030000201136121000139 010311 2003022365013118112970115 27010 331123225327100512213207109 4110 1112162131170913431064140 020 26011230312020200021310146 022 130230029204022400114114151 3 19010321222210100012121380128 0306090120150Figure 2 : Tortuous Classi\ufb01er confusion matrix training and testing and choosing Saudi Arabia ( which is the origin for the plurality of tweets at 36 % ) for users with no remaining tweets .", "entities": []}, {"text": "This improved tweet classi\ufb01cation accuracy by about 5 % , but actually decreased user classi\ufb01cation accuracy on the development set .", "entities": [[4, 5, "MetricName", "accuracy"], [15, 16, "MetricName", "accuracy"]]}, {"text": "6 Results For the Subtask\u20131 we achieved 0.658 macro F1score on the test data , sixth among nineteen submissions with the Tortuous Classi\ufb01er .", "entities": []}, {"text": "The Neural Network Classi\ufb01er achieved a macro F1 - score of 0.648 on the test data .", "entities": [[7, 10, "MetricName", "F1 - score"]]}, {"text": "For the Subtask\u20132 we submitted a single entry .", "entities": []}, {"text": "It ranked 7thamong 9 submissions with 0.475 macro F1 - score .", "entities": [[8, 11, "MetricName", "F1 - score"]]}, {"text": "Figure 2 shows that many of the errors are geographically plausible .", "entities": []}, {"text": "For example , ASWan ALXandria and CAIro are all in Egypt , and each has a sizeable chunk of mistaken identity for the others .", "entities": []}, {"text": "Similarly , DAMascus , ALEppo , AMMan , BEIrut , JERusalem which are all \u2019 Levantine \u2019 and only a few hundred miles apart .", "entities": []}, {"text": "7 Conclusion This paper presents an automatic approach for Arabic dialect detection in the MADAR Shared Task .", "entities": []}, {"text": "Our proposed systems for the Subtask-1 use language model features .", "entities": []}, {"text": "Our experiments showed that simpler machine learning algorithms outperform RNN using language model features .", "entities": []}, {"text": "Subtask\u20132 turned out to be more challenging because Tweets , which are real - world wild data , are more dif\ufb01cult to process than systematically prepared texts .", "entities": []}, {"text": "212Acknowledgments This work has been partly supported by Grant No . SGS-2019 - 018 Processing of heterogeneous data and its specialized applications , and was partly supported from ERDF \u201d Research and Development of Intelligent Components of Advanced Technologies for the Pilsen Metropolitan Area ( InteCom ) \u201d .", "entities": []}, {"text": "Access to computing and storage facilities owned by parties and projects contributing to the National Grid Infrastructure MetaCentrum provided under the programme \u201d Projects of Large Research , Development , and Innovations Infrastructures \u201d ( CESNET LM2015042 ) , is greatly appreciated .", "entities": []}, {"text": "References Mart \u00b4 \u0131n Abadi , Ashish Agarwal , Paul Barham , Eugene Brevdo , Zhifeng Chen , Craig Citro , Greg S. Corrado , Andy Davis , Jeffrey Dean , Matthieu Devin , Sanjay Ghemawat , Ian Goodfellow , Andrew Harp , Geoffrey Irving , Michael Isard , Yangqing Jia , Rafal Jozefowicz , Lukasz Kaiser , Manjunath Kudlur , Josh Levenberg , Dandelion Man \u00b4 e , Rajat Monga , Sherry Moore , Derek Murray , Chris Olah , Mike Schuster , Jonathon Shlens , Benoit Steiner , Ilya Sutskever , Kunal Talwar , Paul Tucker , Vincent Vanhoucke , Vijay Vasudevan , Fernanda Vi \u00b4 egas , Oriol Vinyals , Pete Warden , Martin Wattenberg , Martin Wicke , Yuan Yu , and Xiaoqiang Zheng .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "TensorFlow :", "entities": []}, {"text": "Large - scale machine learning on heterogeneous systems .", "entities": []}, {"text": "Software available from tensor\ufb02ow.org .", "entities": []}, {"text": "Ahmed Ali , Najim Dehak , Patrick Cardinal , Sameer Khurana , Sree Harsha Yella , James Glass , Peter Bell , and Steve Renals .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Automatic dialect detection in Arabic broadcast speech .", "entities": []}, {"text": "In Proceedings of Interspeech 2016 , pages 2934\u20132938 .", "entities": []}, {"text": "Fadi Biadsy , Julia Hirschberg , and Nizar Habash . 2009 .", "entities": []}, {"text": "Spoken arabic dialect identi\ufb01cation using phonotactic modeling .", "entities": []}, {"text": "In Proceedings of the EACL Workshop on Computational Approaches to Semitic Languages , pages 53\u201361 .", "entities": []}, {"text": "Houda Bouamor , Nizar Habash , and Kemal O\ufb02azer .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A multidialectal parallel corpus of Arabic .", "entities": []}, {"text": "In Proceedings of the Ninth International Conference on Language Resources and Evaluation ( LREC14 ) .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Houda Bouamor , Nizar Habash , Mohammad Salameh , Wajdi Zaghouani , Owen Rambow , Dana Abdulrahim , Ossama Obeid , Salam Khalifa , Fadhl Eryani , Alexander Erdmann , and Kemal O\ufb02azer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The madar arabic dialect corpus and lexicon .", "entities": []}, {"text": "In Proceedings of the 11th International Conference on Language Resources and Evaluation .Houda Bouamor , Sabit Hassan , and Nizar Habash .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The MADAR Shared Task on Arabic FineGrained Dialect Identi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the Fourth Arabic Natural Language Processing Workshop ( WANLP19 ) , Florence , Italy .", "entities": [[14, 15, "MethodName", "Florence"]]}, {"text": "Lars Buitinck , Gilles Louppe , Mathieu Blondel , Fabian Pedregosa , Andreas Mueller , Olivier Grisel , Vlad Niculae , Peter Prettenhofer , Alexandre Gramfort , Jaques Grobler , Robert Layton , Jake VanderPlas , Arnaud Joly , Brian Holt , and Ga \u00a8el Varoquaux .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "API design for machine learning software : experiences from the scikit - learn project .", "entities": []}, {"text": "In ECML PKDD Workshop : Languages for Data Mining and Machine Learning , pages 108\u2013122 .", "entities": []}, {"text": "Franc \u00b8ois Chollet et al . 2015 .", "entities": []}, {"text": "Keras .", "entities": []}, {"text": "https:// keras.io .", "entities": []}, {"text": "Heba ElFardy and Mona Diab .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Sentence level dialect identi\ufb01cation in Arabic .", "entities": []}, {"text": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL ) , pages 456\u2013461 .", "entities": []}, {"text": "Alex Graves and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Framewise phoneme classi\ufb01cation with bidirectional lstm and other neural network architectures .", "entities": [[4, 6, "MethodName", "bidirectional lstm"]]}, {"text": "Neural Networks , 18(5 - 6):602\u2013610 .", "entities": []}, {"text": "Abualsoud Hanani , Hanna Basha , Yasmeen Sharaf , and Stephen Taylor . 2015 .", "entities": []}, {"text": "Palestinian Arabic regional accent recognition .", "entities": []}, {"text": "In The 8th International Conference on Speech Technology and Human - Computer Dialogue .", "entities": []}, {"text": "Abualsoud Hanani , Martin J. Russell , and Michael J. Carey .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Human and computer recognition of regional accents and ethnic groups from British english speech .", "entities": []}, {"text": "Computer Speech and Language , 27(1):5974 .", "entities": []}, {"text": "Kenneth Hea\ufb01eld , Ivan Pouzyrevsky , Jonathan H. Clark , , and Philipp Koehn .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Scalable modi\ufb01ed kneser - ney language model estimation .", "entities": []}, {"text": "In ACL .", "entities": []}, {"text": "Radu Tudor Ionescu and Marius Popescu .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "UnibucKernel : An Approach for Arabic Dialect Identi\ufb01cation Based on Multiple String Kernels .", "entities": []}, {"text": "In Proceedings of the Third Workshop on NLP for Similar Languages , Varieties and Dialects ( VarDial3 ) , pages 135\u2013144 , Osaka , Japan .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Lei Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv:1412.6980v9 .", "entities": []}, {"text": "Shervin Malmasi , Eshrag Refaee , and Mark Dras . 2015 .", "entities": []}, {"text": "Arabic dialect identi\ufb01cation using a parallel multidialectal corpus .", "entities": []}, {"text": "In Proceedings of the 14th Conference of the Paci\ufb01c Association for Computational Linguistics ( PACLING 2015 ) , pages 209\u2013217 , Bali , Indonesia .", "entities": []}, {"text": "Shervin Malmasi , Marcos Zampieri , Nikola Ljubei , Preslav Nakov , Ahmed Ali , and Jrg Tiedemann . 2016 .", "entities": []}, {"text": "Discriminating between similar languages", "entities": []}, {"text": "213and arabic dialect identi\ufb01cation : A report on the third dsl shared task .", "entities": []}, {"text": "In Proceedings of the Third Workshop on NLP for Similar Languages , Varieties and Dialects ( VarDial 2016 ) .", "entities": []}, {"text": "Mohammad Salameh , Houda Bouamor , and Nizar Habash . 2018 .", "entities": []}, {"text": "Fine - grained arabic dialect identi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the International Conference on Computational Linguistics ( COLING ) , pages 1332\u20131344 , Santa Fe , New Mexico , USA .", "entities": []}, {"text": "Omar F. Zaidan and Chris Callison - Burch .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "The Arabic Online Commentary dataset : An annotated dataset of informal Arabic with high dialectal content .", "entities": []}, {"text": "In Proceedings of ACL , pages 37\u201341 .", "entities": []}, {"text": "Omar F. Zaidan and Chris Callison - Burch .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Arabic dialect identi\ufb01cation .", "entities": []}, {"text": "Computational Linguistics , 40(1):171\u2013202 .", "entities": []}, {"text": "Marcos Zampieri , Shervin Malmasi , Nikola Ljubei , Preslav Nakov , Ahmed Ali , Jrg Tiedemann , Yves Scherrer , and Nomi Aepli . 2017 .", "entities": []}, {"text": "Findings of the vardial evaluation campaign 2017 .", "entities": []}, {"text": "In Proceedings of the Fourth Workshop on NLP for Similar Languages , Varieties and Dialects ( VarDial 2017 .", "entities": []}, {"text": "Marcos Zampieri , Shervin Malmasi , Preslav Nakov , Ahmed Ali , Suwon Shon , James Glass , Yves Scherrer , Tanja Samard \u02c7zi\u00b4c , Nikola Ljube \u02c7si\u00b4c , J\u00a8org Tiedemann , Chris van der Lee , Stefan Grondelaers , Nelleke Oostdijk , Dirk Speelman , Antal van den Bosch , Ritesh Kumar , Bornini Lahiri , and Mayank Jain .", "entities": [[52, 53, "DatasetName", "Kumar"]]}, {"text": "2018 .", "entities": []}, {"text": "Language identi\ufb01cation and morphosyntactic tagging : The second VarDial evaluation campaign .", "entities": []}, {"text": "In Proceedings of the Fifth Workshop on NLP for Similar Languages , Varieties and Dialects ( VarDial 2018 ) , pages 1\u201317 , Santa Fe , New Mexico , USA . Association for Computational Linguistics .", "entities": []}]