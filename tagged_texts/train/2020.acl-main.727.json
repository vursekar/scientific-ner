[{"text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8172\u20138181 July 5 - 10 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics8172A Negative Case Analysis of Visual Grounding Methods for VQA Robik Shrestha1Kushal Ka\ufb02e1,2Christopher Kanan1,3,4 Rochester Institute of Technology1Adobe Research2Paige3Cornell Tech4 { rss9369 , kk6055 , kanan } @rit.edu Abstract", "entities": [[9, 11, "TaskName", "Visual Grounding"], [13, 14, "TaskName", "VQA"]]}, {"text": "Existing Visual Question Answering ( VQA ) methods tend to exploit dataset biases and spurious statistical correlations , instead of producing right answers for the right reasons .", "entities": [[1, 4, "DatasetName", "Visual Question Answering"], [5, 6, "TaskName", "VQA"]]}, {"text": "To address this issue , recent bias mitigation methods for VQA propose to incorporate visual cues ( e.g. , human attention maps ) to better ground the VQA models , showcasing impressive gains .", "entities": [[10, 11, "TaskName", "VQA"], [27, 28, "TaskName", "VQA"]]}, {"text": "However , we show that the performance improvements are not a result of improved visual grounding , but a regularization effect which prevents over-\ufb01tting to linguistic priors .", "entities": [[14, 16, "TaskName", "visual grounding"]]}, {"text": "For instance , we \ufb01nd that it is not actually necessary to provide proper , humanbased cues ; random , insensible cues also result in similar improvements .", "entities": []}, {"text": "Based on this observation , we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state - of - theart performance on VQA - CPv21 .", "entities": [[29, 30, "TaskName", "VQA"]]}, {"text": "1 Introduction Visual Question Answering ( VQA ) ( Antol et al . , 2015 ) , the task of answering questions about visual content , was proposed to facilitate the development of models with human - like visual and linguistic understanding .", "entities": [[2, 5, "DatasetName", "Visual Question Answering"], [6, 7, "TaskName", "VQA"]]}, {"text": "However , existing VQA models often exploit super\ufb01cial statistical biases to produce responses , instead of producing the right answers for the right reasons ( Ka\ufb02e et al . , 2019 ) .", "entities": [[3, 4, "TaskName", "VQA"]]}, {"text": "The VQA - CP dataset ( Agrawal et al . , 2018 ) showcases this phenomenon by incorporating different question type / answer distributions in the train and test sets .", "entities": [[1, 4, "DatasetName", "VQA - CP"]]}, {"text": "Since the linguistic priors in the train and test sets differ , models that exploit these priors fail on the test set .", "entities": []}, {"text": "To tackle this issue , recent works have endeavored to enforce proper visual grounding , where the goal is to make models produce answers by looking at relevant visual regions ( Gan et al . , 2017 ; Selvaraju et al . ,", "entities": [[12, 14, "TaskName", "visual grounding"]]}, {"text": "Answer distributionVQA - CP Dataset Prediction :   BrownBaseline Methods Affected by language   priors GreenBrownQ : What color is the   couch ?", "entities": []}, {"text": "A : Green Training TestGreenBrownFail to generalize Prediction :   GreenRecent Methods Improve by grounding   on relevant regions +9 % over baselines Prediction :   GreenOur Findings Irrelevant / random regions   result in similar gains", "entities": []}, {"text": "+9 % over baselines Figure 1 : We \ufb01nd that existing visual sensitivity enhancement methods improve performance on VQACPv2 through regularization as opposed to proper visual grounding .", "entities": [[25, 27, "TaskName", "visual grounding"]]}, {"text": "2019 ; Wu and Mooney , 2019 ) , instead of exploiting linguistic priors .", "entities": []}, {"text": "These approaches rely on additional annotations / cues such as human - based attention maps ( Das et al . , 2017 ) , textual explanations ( Huk Park et al . , 2018 ) and object label predictions ( Ren et al . , 2015 ) to identify relevant regions , and train the model to base its predictions on those regions , showing large improvements ( 810 % accuracy ) on the VQA - CPv2 dataset .", "entities": [[70, 71, "MetricName", "accuracy"], [74, 75, "TaskName", "VQA"]]}, {"text": "Here , we study these methods .", "entities": []}, {"text": "We \ufb01nd that their improved accuracy does not actually emerge from proper visual grounding , but from regularization effects , where the model forgets the linguistic priors in the train set , thereby performing better on the test set .", "entities": [[5, 6, "MetricName", "accuracy"], [12, 14, "TaskName", "visual grounding"]]}, {"text": "To support these claims , we \ufb01rst show that it is possible to achieve such gains even when the model is trained to look at : a ) irrelevant visual regions , and b ) random visual regions .", "entities": []}, {"text": "Second , we show that differences in the predictions from the 1https://github.com/erobic/negative _ analysis_of_grounding", "entities": []}, {"text": "8173variants trained with relevant , irrelevant and random visual regions are not statistically signi\ufb01cant .", "entities": []}, {"text": "Third , we show that these methods degrade performance when the priors remain intact and instead work on VQA - CPv2 by hurting its train accuracy .", "entities": [[18, 19, "TaskName", "VQA"], [25, 26, "MetricName", "accuracy"]]}, {"text": "Based on these observations , we hypothesize that controlled degradation on the train set allows models to forget the training priors to improve test accuracy .", "entities": [[24, 25, "MetricName", "accuracy"]]}, {"text": "To test this hypothesis , we introduce a simple regularization scheme that zeros out the ground truth answers , thereby always penalizing the model , whether the predictions are correct or incorrect .", "entities": []}, {"text": "We \ufb01nd that this approach also achieves near state - of - the - art performance ( 48.9%on VQACPv2 ) , providing further support for our claims .", "entities": []}, {"text": "While we agree that visual grounding is a useful direction to pursue , our experiments show that the community requires better ways to test if systems are actually visually grounded .", "entities": [[4, 6, "TaskName", "visual grounding"]]}, {"text": "We make some recommendations in the discussion section .", "entities": []}, {"text": "2 Related Work 2.1 Biases in VQA As expected of any real world dataset , VQA datasets also contain dataset biases ( Goyal et al . , 2017 ) .", "entities": [[6, 7, "TaskName", "VQA"], [15, 16, "TaskName", "VQA"]]}, {"text": "The VQA - CP dataset ( Agrawal et al . , 2018 ) was introduced to study the robustness of VQA methods against linguistic biases .", "entities": [[1, 4, "DatasetName", "VQA - CP"], [20, 21, "TaskName", "VQA"]]}, {"text": "Since it contains different answer distributions in the train and test sets , VQA - CP makes it nearly impossible for the models that rely upon linguistic correlations to perform well on the test set ( Agrawal et al . , 2018 ; Shrestha et al . , 2019 ) .", "entities": [[13, 16, "DatasetName", "VQA - CP"]]}, {"text": "2.2 Bias Mitigation for VQA VQA algorithms without explicit bias mitigation mechanisms fail on VQA - CP , so recent works have focused on the following solutions :", "entities": [[4, 5, "TaskName", "VQA"], [5, 6, "TaskName", "VQA"], [14, 17, "DatasetName", "VQA - CP"]]}, {"text": "2.2.1 Reducing Reliance on Questions Some recent approaches employ a question - only branch as a control model to discover the questions most affected by linguistic correlations .", "entities": []}, {"text": "The question - only model is either used to perform adversarial regularization ( Grand and Belinkov , 2019 ;", "entities": []}, {"text": "Ramakrishnan et al . , 2018 ) or to re - scale the loss based on the dif\ufb01culty of the question ( Cadene et al . , 2019 ) .", "entities": [[13, 14, "MetricName", "loss"]]}, {"text": "However , when these ideas are applied to the UpDn model ( Anderson et al . , 2018 ) , which attempts to learn correct visual grounding , these approaches achieve 4 - 7 % lower accuracy compared to the state - of - the - art methods .", "entities": [[25, 27, "TaskName", "visual grounding"], [36, 37, "MetricName", "accuracy"]]}, {"text": "2.2.2", "entities": []}, {"text": "Enhancing Visual Sensitivities Both Human Importance Aware Network Tuning ( HINT ) ( Selvaraju et al . , 2019 ) and Self Critical Reasoning ( SCR ) ( Wu and Mooney , 2019 ) , train the network to be more sensitive towards salient image regions by improving the alignment between visual cues and gradient - based sensitivity scores .", "entities": []}, {"text": "HINT proposes a ranking loss between humanbased importance scores ( Das et al . , 2016 ) and the gradient - based sensitivities .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "In contrast , SCR does not require exact saliency ranks .", "entities": []}, {"text": "Instead , it penalizes the model if correct answers are more sensitive towards non - important regions as compared to important regions , and if incorrect answers are more sensitive to important regions than correct answers .", "entities": []}, {"text": "3 Existing VQA Methods Given a question Qand an image I , e.g. , represented by bottom - up region proposals : v(Anderson et al . , 2018 ) , a VQA model is tasked with predicting the answer a : P(a|Q , I ) = fVQA(v , Q ) .", "entities": [[2, 3, "TaskName", "VQA"], [31, 32, "TaskName", "VQA"]]}, {"text": "( 1 ) 3.1 Baseline VQA Methods Without additional regularization , existing VQA models such as the baseline model used in this work : UpDn ( Anderson et al . , 2018 ) , tend to rely on the linguistic priors : P(a|Q)to answer questions .", "entities": [[5, 6, "TaskName", "VQA"], [12, 13, "TaskName", "VQA"]]}, {"text": "Such models fail on VQA - CP , because the priors in the test set differ from the train set .", "entities": [[4, 7, "DatasetName", "VQA - CP"]]}, {"text": "3.2 Visual Sensitivity Enhancement Methods To reduce the reliance on linguistic priors , visual sensitivity enhancement methods attempt to train the model to be more sensitive to relevant visual regions when answering questions .", "entities": []}, {"text": "Following ( Wu and Mooney , 2019 ) , we de\ufb01ne the sensitivity of an answerawith respect to a visual region vias : S(a , vi ) :", "entities": []}, {"text": "= ( \u2207viP(a|I , Q))T1 .", "entities": []}, {"text": "( 2 ) Existing methods propose the following training objectives to improve grounding using S : \u2022HINT uses a ranking loss , which penalizes the model if the pair - wise rankings of the sensitivities of visual regions towards ground truth answersagtare different from the ranks computed from the human - based attention maps .", "entities": [[20, 21, "MetricName", "loss"]]}, {"text": "8174\u2022SCR divides the region proposals into in\ufb02uential and non - in\ufb02uential regions and penalizes the model if : 1 ) S(agt)of a non - in\ufb02uential region is higher than an in\ufb02uential region , and 2 ) the region most in\ufb02uential for the correct answer has even higher sensitivity for incorrect answers .", "entities": []}, {"text": "Both methods improve baseline accuracy by 8 - 10 % .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "Is this actually due to better visual grounding ?", "entities": [[6, 8, "TaskName", "visual grounding"]]}, {"text": "4 Why Did the Performance Improve ?", "entities": []}, {"text": "We probe the reasons behind the performance improvements of HINT and SCR .", "entities": []}, {"text": "We \ufb01rst analyze if the results improve even when the visual cues are irrelevant ( Sec . 4.2 ) or random ( Sec . 4.3 ) and examine if their differences are statistically signi\ufb01cant ( Sec . 4.4 ) .", "entities": []}, {"text": "Then , we analyze the regularization effects by evaluating the performance on VQACPv2 \u2019s train split ( Sec . 4.5 ) and the behavior on a dataset without changing priors ( Sec . 4.6 ) .", "entities": []}, {"text": "We present a new metric to assess visual grounding in Sec .", "entities": [[7, 9, "TaskName", "visual grounding"]]}, {"text": "4.7and describe our regularization method in Sec .", "entities": []}, {"text": "5 . 4.1 Experimental Setup We compare the baseline UpDn model with HINT and SCR - variants trained on VQAv2 or VQA - CPv2 to study the causes behind the improvements .", "entities": [[21, 22, "TaskName", "VQA"]]}, {"text": "We report mean accuracies across 5runs , where a pretrained UpDn model is \ufb01ne - tuned on subsets with human attention maps and textual explanations for HINT and SCR respectively .", "entities": []}, {"text": "Further training details are provided in the Appendix .", "entities": []}, {"text": "4.2 Training on Irrelevant Visual Cues", "entities": []}, {"text": "In our \ufb01rst experiment we studied how irrelevant visual cues performed compared to relevant ones .", "entities": []}, {"text": "We \ufb01ne - tune the model with irrelevant cues de\ufb01ned as : Sirrelevant : = ( 1\u2212Sh ) , where , Shrepresents the human - based importance scores .", "entities": []}, {"text": "As shown in the \u2018 Grounding using irrelevant cues \u2019 section of Table 1 , both HINT and SCR are within 0.3 % of the results obtained from looking at relevant regions , which indicates the gains for HINT and SCR are not necessarily from looking at relevant regions .", "entities": []}, {"text": "4.3 Training on Random Visual Cues In our next experiment we studied how random visual cues performed with HINT and SCR .", "entities": []}, {"text": "We assign random importance scores to the visual regions : Srand\u223cuniform(0,1 ) .", "entities": []}, {"text": "We test two variants of randomness : Fixed random regions , whereTable 1 : Results on VQA - CPv2 and VQAv2 datasets for the baseline UpDn , visual sensitivity enhancement methods ( HINT and SCR ) and our own regularization method , including the published ( pub . ) numbers .", "entities": [[16, 17, "TaskName", "VQA"]]}, {"text": "VQA - CPv2 VQAv2 Train Test Train Val Baseline - Without visual grounding UpDn 84.0 40.1 83.4 64.4", "entities": [[0, 1, "TaskName", "VQA"], [11, 13, "TaskName", "visual grounding"]]}, {"text": "Grounding using human - based cues HINTpub .", "entities": []}, {"text": "N / A 46.7 N / A 63.41 SCRpub .", "entities": []}, {"text": "N / A 49.5 N / A 62.2 HINT 73.9 48.2 75.7 61.3 SCR 75.9 49.1 77.9 61.3 Grounding using irrelevant cues HINT 71.2 48.0 73.5 60.3 SCR 75.7 49.2 74.1 59.1 Grounding using \ufb01xed random cues HINT 72.0 48.1 73.0 59.5 SCR 70.0 49.1 78.0 61.4 Grounding using variable random cues HINT 71.9 48.1 72.9 59.4 SCR 69.6 49.2 78.1 61.5 Regularization by zeroing out answers Ours1%fixed 78.0 48.9 80.1 62.6 Ours1%var .", "entities": []}, {"text": "77.6 48.5 80.0 62.6 Ours100 % 75.7 48.2 79.9 62.4 1The published number is a result of \ufb01ne - tuning HINT on the entire training set , but as described in Sec . 4.6 , other published numbers and our experiments \ufb01ne - tune only on the instances with cues .", "entities": []}, {"text": "Srand are \ufb01xed once chosen , and Variable random regions , whereSrand are regenerated every epoch .", "entities": []}, {"text": "As shown in Table 1 , both of these variants obtain similar results as the model trained with human - based importance scores .", "entities": []}, {"text": "The performance improves even when the importance scores are changed every epoch , indicating that it is not even necessary to look at the same visual regions .", "entities": []}, {"text": "4.4 Signi\ufb01cance of Statistical Differences To test if the changes in results were statistically signi\ufb01cant , we performed Welch \u2019s t - tests ( Welch , 1938 ) on the predictions of the variants trained on relevant , irrelevant and random cues .", "entities": []}, {"text": "We pick Welch \u2019s t - test over the Student \u2019s t - test , because the latter assumes equal variances for predictions from different variants .", "entities": []}, {"text": "To perform the tests , we \ufb01rst randomly sample 5000 subsets of non - overlapping test instances .", "entities": []}, {"text": "We then average the accuracy of each subset across 5runs , obtaining 5000 values .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "Next , we run the t - tests for HINT and SCR separately on the subset accuracies .", "entities": []}, {"text": "As shown in Table 2 , the p - values across the variants of HINT and SCR are", "entities": []}, {"text": "8175Table 2 :p - values from the Welch \u2019s t - tests and the percentage of overlap between the predictions ( Ovp . ) of different variants of HINT and SCR .", "entities": []}, {"text": "Methods pOvp.(% ) HINT variants against Baseline Default vs. Baseline 0.0 83.6 Irrelevant vs. Baseline 0.0 82.4 Fixed Random vs. Baseline 0.0 82.0 Variable Random vs. Baseline 0.0 81.5 Among HINT variants Default vs Irrelevant 0.3 89.7 Default vs Fixed random 0.7 90.9 Default vs Variable random 0.6 91.9 Irrelevant vs Fixed random 0.5 95.6 Irrelevant vs Variable random 0.7 93.9 Fixed random vs Variable random 0.9 96.9 SCR variants against Baseline Default vs. Baseline 0.0 85.6 Irrelevant vs. Baseline 0.0 84.2 Fixed Random vs. Baseline 0.0 80.7 Variable Random vs. Baseline 0.0 80.6 Among SCR variants Default vs Irrelevant 0.6 92.0 Default vs Fixed random 0.8 89.3 Default vs Variable random 0.6 89.5 Irrelevant vs Fixed random 0.4 91.7 Irrelevant vs Variable random 1.0 91.6 Fixed random vs Variable random 0.4 96.7 greater than or equal to 0.3 .", "entities": []}, {"text": "Using a con\ufb01dence level of95 % ( \u03b1= 0.05 ) , we fail to reject the null hypothesis that the mean difference between the paired values is 0 , showing that the variants are not statistically signi\ufb01cantly different from each other .", "entities": [[27, 28, "DatasetName", "0"]]}, {"text": "We also compare the predictions of HINT / SCR against baseline , and \ufb01nd that p - values are all zeros , showing that the differences have statistical signi\ufb01cance .", "entities": []}, {"text": "Percentage of Overlaps : To further check if the variants trained on irrelevant or random regions gain performance in a manner similar to the models trained on relevant regions , we compute the overlap between their predictions on VQA - CPv2 \u2019s test set .", "entities": [[38, 39, "TaskName", "VQA"]]}, {"text": "The percentage of overlap is de\ufb01ned as : % Overlap = nsame ntotal\u00d7100 % , where , nsame denotes the number of instances where either both variants were correct or both were incorrect and ntotal denotes the total number of test instances .", "entities": []}, {"text": "As shown in Table 2 , we compare%Overlap between different variants of HINT / SCR with baseline and against each other .", "entities": []}, {"text": "EpochsAccuracy on VQAv2606162636465 0 1 2 3 4 5 6 7 8HINT ( full ) SCR ( full ) HINT ( subset with cues ) SCR ( subset with cues)Figure 2 : Accuracies for HINT and SCR on VQAv2 \u2019s val set , when \ufb01ne - tuned either on the full train set or on the subset containing visual cues .", "entities": [[3, 4, "DatasetName", "0"]]}, {"text": "We \ufb01nd89.7\u221291.9%and89.5\u221292.0%overlaps for different variants of HINT and SCR respectively .", "entities": []}, {"text": "These high overlaps suggest that the variants are not working in fundamentally different manners .", "entities": []}, {"text": "4.5 Drops in Training Accuracy We compare the training accuracies to analyze the regularization effects .", "entities": [[4, 5, "MetricName", "Accuracy"]]}, {"text": "As shown in Table 1 , the baseline method has the highest training results , while the other methods cause 6.0\u221214.0%and 3.3\u221210.5%drops in the training accuracy on VQACPv2 and VQAv2 , respectively .", "entities": [[25, 26, "MetricName", "accuracy"]]}, {"text": "We hypothesize that degrading performance on the train set helps forget linguistic biases , which in turn helps accuracy on VQA - CPv2 \u2019s test set but hurts accuracy on VQAv2 \u2019s val set .", "entities": [[18, 19, "MetricName", "accuracy"], [20, 21, "TaskName", "VQA"], [28, 29, "MetricName", "accuracy"]]}, {"text": "4.6 Drops in VQAv2 Accuracy As observed by Selvaraju et al .", "entities": [[4, 5, "MetricName", "Accuracy"]]}, {"text": "( 2019 ) and as shown in Fig .", "entities": []}, {"text": "2 , we observe small improvements on VQAv2 when the models are \ufb01ne - tuned on the entire train set .", "entities": []}, {"text": "However , if we were to compare against the improvements in VQA - CPv2 in a fair manner , i.e. , only use the instances with visual cues while \ufb01ne - tuning , then , the performance on VQAv2 drops continuously during the course of the training .", "entities": [[11, 12, "TaskName", "VQA"]]}, {"text": "This indicates that HINT and SCR help forget linguistic priors , which is bene\ufb01cial for VQA - CPv2 but not for VQAv2 .", "entities": [[15, 16, "TaskName", "VQA"]]}, {"text": "4.7 Assessment of Proper Grounding In order to quantitatively assess visual grounding , we propose a new metric called : Correctly Predicted but Improperly Grounded ( CPIG ): % CPIG = Ncorrect ans , improper grounding Ncorrect ans\u00d7100 % , which is the number instances for which the most sensitive visual region used to correctly predict the", "entities": [[10, 12, "TaskName", "visual grounding"]]}, {"text": "8176answer is not within top-3 most relevant ground truth regions , normalized by the total number of correct predictions .", "entities": []}, {"text": "HINT and SCR trained on relevant regions obtained lower CPIG values that other variants ( 70.24 % and 80.22 % respectively ) , indicating they are better than other variants at \ufb01nding relevant regions .", "entities": []}, {"text": "However , these numbers are still high , and show that only 29.76 % and 19.78 % of the correct predictions for HINT and SCR were properly grounded .", "entities": []}, {"text": "Further analysis is presented in the Appendix .", "entities": []}, {"text": "5 Embarrassingly Simple Regularizer", "entities": []}, {"text": "The usage of visual cues and sensitivities in existing methods is super\ufb02uous because the results indicate that performance improves through degradation of training accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}, {"text": "We hypothesize that simple regularization that does not rely on cues or sensitivities can also achieve large performance gains for VQA - CP .", "entities": [[20, 23, "DatasetName", "VQA - CP"]]}, {"text": "To test this hypothesis , we devise a simple loss function which continuously degrades the training accuracy by training the network to always predict a score of zero for all possible answers i.e. produce a zero vector ( 0 ) .", "entities": [[9, 10, "MetricName", "loss"], [16, 17, "MetricName", "accuracy"], [38, 39, "DatasetName", "0"]]}, {"text": "The overall loss function can be written as : L:=BCE(P(A),Agt)+\u03bbBCE(P(A),0 ) , where , BCE refers to the binary cross entropy loss andP(A)is a vector consisting of predicted scores for all possible answers .", "entities": [[2, 3, "MetricName", "loss"], [21, 22, "MetricName", "loss"]]}, {"text": "The \ufb01rst term is the binary cross entropy loss between model predictions and ground truth answer vector ( Agt ) , and the second term is our regularizer with a coef\ufb01cient of \u03bb= 1 .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "Note that this regularizer continually penalizes the model during the course of the training , whether its predictions are correct or incorrect .", "entities": []}, {"text": "As shown in Table 1 , we present results when this loss is used on : a ) Fixed subset covering 1%of the dataset , b ) Varying subset covering 1%of the dataset , where a new random subset is sampled every epoch and c ) 100 % of the dataset .", "entities": [[11, 12, "MetricName", "loss"]]}, {"text": "Con\ufb01rming our hypothesis , all variants of our model achieve near state - of - the - art results , solidifying our claim that the performance gains for recent methods come from regularization effects .", "entities": []}, {"text": "It is also interesting to note that the drop in training accuracy is lower with this regularization scheme as compared to the state - of - the - art methods .", "entities": [[11, 12, "MetricName", "accuracy"]]}, {"text": "Of course , if any model was actually visually grounded , then we would expect it to improve performances on both train and test sets .", "entities": []}, {"text": "We do notobserve such behavior in any of the methods , indicating that they are not producing right answers for the right reasons .", "entities": []}, {"text": "6 Discussion on Proper Grounding While our results indicate that current visual grounding based bias mitigation approaches do not suf\ufb01ce , we believe this is still a good research direction .", "entities": [[11, 13, "TaskName", "visual grounding"]]}, {"text": "However , future methods must seek to verify that performance gains are not stemming from spurious sources by using an experimental setup similar to that presented in this paper .", "entities": []}, {"text": "We recommend that both train and test accuracy be reported , because a model truly capable of visual grounding would not cause drastic drops in training accuracy to do well on the test sets .", "entities": [[7, 8, "MetricName", "accuracy"], [17, 19, "TaskName", "visual grounding"], [26, 27, "MetricName", "accuracy"]]}, {"text": "Finally , we advocate for creating a dataset with ground truth grounding available for 100 % of the instances using synthetically generated datasets ( Ka\ufb02e et al . , 2017 ;", "entities": []}, {"text": "Ka\ufb02e and Kanan , 2017 ; Ka\ufb02e et al . , 2018 ; Acharya et al . , 2019b ; Hudson and Manning , 2019 ; Johnson et al . , 2017 ) , enabling the community to evaluate if their methods are able to focus on relevant information .", "entities": []}, {"text": "Another alternative is to use tasks that explicitly test grounding , e.g. , in visual query detection an agent must output boxes around any regions of a scene that match the natural language query ( Acharya et al . , 2019a ) .", "entities": [[18, 19, "DatasetName", "agent"]]}, {"text": "7 Conclusion Here , we showed that existing visual grounding based bias mitigation methods for VQA are not working as intended .", "entities": [[8, 10, "TaskName", "visual grounding"], [15, 16, "TaskName", "VQA"]]}, {"text": "We found that the accuracy improvements stem from a regularization effect rather than proper visual grounding .", "entities": [[4, 5, "MetricName", "accuracy"], [14, 16, "TaskName", "visual grounding"]]}, {"text": "We proposed a simple regularization scheme which , despite not requiring additional annotations , rivals state - of - theart accuracy .", "entities": [[20, 21, "MetricName", "accuracy"]]}, {"text": "Future visual grounding methods should be tested with a more comprehensive experimental setup and datasets for proper evaluation .", "entities": [[1, 3, "TaskName", "visual grounding"]]}, {"text": "Acknowledgement .", "entities": []}, {"text": "This work was supported in part by AFOSR grant", "entities": []}, {"text": "[ FA9550 - 18 - 1 - 0121 ] , NSF award # 1909696 , and a gift from Adobe Research .", "entities": []}, {"text": "We thank NVIDIA for the GPU donation .", "entities": []}, {"text": "The views and conclusions contained herein are those of the authors and should not be interpreted as representing the of\ufb01cial policies or endorsements of any sponsor .", "entities": []}, {"text": "We are grateful to Tyler Hayes for agreeing to review the paper at short notice and suggesting valuable edits and corrections for the paper .", "entities": []}, {"text": "8177References Manoj Acharya , Karan Jariwala , and Christopher Kanan . 2019a .", "entities": []}, {"text": "VQD : Visual query detection in natural scenes .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1955\u20131961 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Manoj Acharya , Kushal Ka\ufb02e , and Christopher Kanan .", "entities": []}, {"text": "2019b .", "entities": []}, {"text": "Tallyqa :", "entities": [[0, 1, "DatasetName", "Tallyqa"]]}, {"text": "Answering complex counting questions .", "entities": []}, {"text": "In Association for the Advancement of Arti\ufb01cial Intelligence ( AAAI ) .", "entities": []}, {"text": "Aishwarya Agrawal , Dhruv Batra , Devi Parikh , and Aniruddha Kembhavi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Do nt just assume ; look and answer : Overcoming priors for visual question answering .", "entities": [[12, 15, "DatasetName", "visual question answering"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 4971\u20134980 .", "entities": []}, {"text": "Peter Anderson , Xiaodong He , Chris Buehler , Damien Teney , Mark Johnson , Stephen Gould , and Lei Zhang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Bottom - up and top - down attention for image captioning and visual question answering .", "entities": [[9, 11, "TaskName", "image captioning"], [12, 15, "DatasetName", "visual question answering"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) .", "entities": []}, {"text": "Stanislaw Antol , Aishwarya Agrawal , Jiasen Lu , Margaret Mitchell , Dhruv Batra , C. Lawrence Zitnick , and Devi Parikh . 2015 .", "entities": []}, {"text": "VQA : Visual question answering .", "entities": [[0, 1, "TaskName", "VQA"], [2, 5, "DatasetName", "Visual question answering"]]}, {"text": "In The IEEE International Conference on Computer Vision ( ICCV ) .", "entities": []}, {"text": "Remi Cadene , Corentin Dancette , Matthieu Cord , Devi Parikh , et al . 2019 .", "entities": []}, {"text": "Rubi :", "entities": []}, {"text": "Reducing unimodal biases for visual question answering .", "entities": [[4, 7, "DatasetName", "visual question answering"]]}, {"text": "In Advances in Neural Information Processing Systems ( NeurIPS ) , pages 839\u2013850 .", "entities": []}, {"text": "Abhishek Das , Harsh Agrawal , C Lawrence Zitnick , Devi Parikh , and Dhruv Batra . 2016 .", "entities": []}, {"text": "Human attention in visual question answering : Do humans and deep networks look at the same regions ?", "entities": [[3, 6, "DatasetName", "visual question answering"]]}, {"text": "In Conference on Empirical Methods on Natural Language Processing ( EMNLP ) .", "entities": []}, {"text": "Abhishek Das , Harsh Agrawal , Larry Zitnick , Devi Parikh , and Dhruv Batra . 2017 .", "entities": []}, {"text": "Human attention in visual question answering : Do humans and deep networks look at the same regions ?", "entities": [[3, 6, "DatasetName", "visual question answering"]]}, {"text": "Computer Vision and Image Understanding ( CVIU ) , 163:90\u2013100 .", "entities": []}, {"text": "Chuang Gan , Yandong Li , Haoxiang Li , Chen Sun , and Boqing Gong . 2017 .", "entities": []}, {"text": "Vqs : Linking segmentations to questions and answers for supervised attention in vqa and question - focused semantic segmentation .", "entities": [[12, 13, "TaskName", "vqa"], [17, 19, "TaskName", "semantic segmentation"]]}, {"text": "In Proceedings of the IEEE International Conference on Computer Vision , pages 1811\u20131820 .", "entities": []}, {"text": "Yash Goyal , Tejas Khot , Douglas Summers - Stay , Dhruv Batra , and Devi Parikh . 2017 .", "entities": []}, {"text": "Making theV in VQA matter : Elevating the role of image understanding in visual question answering .", "entities": [[3, 4, "TaskName", "VQA"], [13, 16, "DatasetName", "visual question answering"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , volume 1 , page 3 .", "entities": []}, {"text": "Gabriel Grand and Yonatan Belinkov . 2019 .", "entities": []}, {"text": "Adversarial regularization for visual question answering : Strengths , shortcomings , and side effects .", "entities": [[3, 6, "DatasetName", "visual question answering"]]}, {"text": "In Proceedings of the Second Workshop on Shortcomings in Vision and Language , pages 1\u201313 , Minneapolis , Minnesota . Association for Computational Linguistics ( ACL ) .", "entities": []}, {"text": "Drew A Hudson and Christopher D Manning .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "GQA : A new dataset for real - world visual reasoning and compositional question answering .", "entities": [[0, 1, "DatasetName", "GQA"], [9, 11, "TaskName", "visual reasoning"], [13, 15, "TaskName", "question answering"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 6700\u20136709 .", "entities": []}, {"text": "Dong Huk Park , Lisa Anne Hendricks , Zeynep Akata , Anna Rohrbach , Bernt Schiele , Trevor Darrell , and Marcus Rohrbach .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Multimodal explanations : Justifying decisions and pointing to the evidence .", "entities": []}, {"text": "InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 8779\u20138788 .", "entities": []}, {"text": "Justin Johnson , Bharath Hariharan , Laurens van der Maaten , Li Fei - Fei , C Lawrence Zitnick , and Ross Girshick . 2017 .", "entities": []}, {"text": "Clevr : A diagnostic dataset for compositional language and elementary visual reasoning .", "entities": [[0, 1, "DatasetName", "Clevr"], [10, 12, "TaskName", "visual reasoning"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 1988\u20131997 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Kushal Ka\ufb02e and Christopher Kanan . 2017 .", "entities": []}, {"text": "An analysis of visual question answering algorithms .", "entities": [[3, 6, "DatasetName", "visual question answering"]]}, {"text": "In Proceedings of the IEEE International Conference on Computer Vision ( ICCV ) , pages 1983\u20131991 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Kushal Ka\ufb02e , Brian Price , Scott Cohen , and Christopher Kanan .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "DVQA :", "entities": [[0, 1, "DatasetName", "DVQA"]]}, {"text": "Understanding data visualizations via question answering .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "In Proc .", "entities": []}, {"text": "IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 5648\u20135656 .", "entities": []}, {"text": "Kushal Ka\ufb02e , Robik Shrestha , and Christopher Kanan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Challenges and prospects in vision and language research .", "entities": []}, {"text": "Frontiers in Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Kushal Ka\ufb02e , Mohammed Yousefhussien , and Christopher Kanan . 2017 .", "entities": []}, {"text": "Data augmentation for visual question answering .", "entities": [[0, 2, "TaskName", "Data augmentation"], [3, 6, "DatasetName", "visual question answering"]]}, {"text": "In Proceedings of the 10th International Conference on Natural Language Generation ( INLG ) , pages 198\u2013202 .", "entities": []}, {"text": "Sainandan Ramakrishnan , Aishwarya Agrawal , and Stefan Lee . 2018 .", "entities": []}, {"text": "Overcoming language priors in visual question answering with adversarial regularization .", "entities": [[4, 7, "DatasetName", "visual question answering"]]}, {"text": "In Advances in Neural Information Processing Systems ( NeurIPS ) , pages 1541\u20131551 .", "entities": []}, {"text": "Shaoqing Ren , Kaiming He , Ross Girshick , and Jian Sun . 2015 .", "entities": []}, {"text": "Faster R - CNN : Towards real - time object detection with region proposal networks .", "entities": [[0, 4, "MethodName", "Faster R - CNN"], [6, 11, "TaskName", "real - time object detection"], [12, 14, "TaskName", "region proposal"]]}, {"text": "In Advances in Neural Information Processing Systems ( NeurIPS ) .", "entities": []}, {"text": "8178Ramprasaath R Selvaraju , Stefan Lee , Yilin Shen , Hongxia Jin , Shalini Ghosh , Larry Heck , Dhruv Batra , and Devi Parikh . 2019 .", "entities": []}, {"text": "Taking a hint : Leveraging explanations to make vision and language models more grounded .", "entities": []}, {"text": "In Proceedings of the IEEE International Conference on Computer Vision ( ICCV ) , pages 2591\u20132600 .", "entities": []}, {"text": "Robik Shrestha , Kushal Ka\ufb02e , and Christopher Kanan . 2019 .", "entities": []}, {"text": "Answer them all !", "entities": []}, {"text": "toward universal visual question answering models .", "entities": [[2, 5, "DatasetName", "visual question answering"]]}, {"text": "In The IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) .", "entities": []}, {"text": "Bernard L Welch .", "entities": []}, {"text": "1938 .", "entities": []}, {"text": "The signi\ufb01cance of the difference between two means when the population variances are unequal .", "entities": []}, {"text": "Biometrika , 29(3/4):350\u2013362 .", "entities": []}, {"text": "Jialin Wu and Raymond Mooney .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Self - critical reasoning for robust visual question answering .", "entities": [[6, 9, "DatasetName", "visual question answering"]]}, {"text": "In Advances in Neural Information Processing Systems ( NeurIPS ) , pages 8601\u20138611 .", "entities": []}, {"text": "A Appendix A.1 Training Details", "entities": []}, {"text": "We compare four different variants of HINT and SCR to study the causes behind the improvements including the models that are \ufb01ne - tuned on : 1 ) relevant regions ( state - of - the - art methods ) 2 ) irrelevant regions 3 ) \ufb01xed random regions and 4 ) variable random regions .", "entities": []}, {"text": "For all variants , we \ufb01ne - tune a pretrained UpDn , which was trained on either VQACPv2 or VQAv2 for 40 epochs with a learning rate of10\u22123 .", "entities": [[25, 27, "HyperparameterName", "learning rate"]]}, {"text": "When \ufb01ne - tuning with HINT , SCR or our method , we also use the main binary cross entropy VQA loss , whose weight is set to 1 .", "entities": [[20, 21, "TaskName", "VQA"], [21, 22, "MetricName", "loss"]]}, {"text": "The batch size is set to384for all of the experiments .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "HINT Following ( Selvaraju et al . , 2019 ) , we train HINT on the subset with human - based attention maps ( Das et al . , 2017 ) , which are available for 9 % of the VQA - CPv2 train and test sets .", "entities": [[40, 41, "TaskName", "VQA"]]}, {"text": "The same subset is used for VQAv2 too .", "entities": []}, {"text": "The learning rate is set to2\u00d710\u22125and the weight for the HINT loss is set to2 .", "entities": [[1, 3, "HyperparameterName", "learning rate"], [11, 12, "MetricName", "loss"]]}, {"text": "SCR Since ( Wu and Mooney , 2019 ) reported that humanbased textual explanations ( Huk Park et al . , 2018 ) gave better results than human - based attention maps for SCR , we train all of the SCR variants on the subset containing textual explanation - based cues .", "entities": []}, {"text": "SCR is trained in two phases .", "entities": []}, {"text": "For the \ufb01rst phase , which strengthens the in\ufb02uential objects , we use a learning rate of 5\u00d710\u22125 , loss weight of 3Table A3 : Results on VQA - CPv2 and VQAv2 datasets for the baseline UpDn , visual sensitivity enhancement methods ( HINT and SCR ) and our own regularization method , including the published ( pub . ) numbers .", "entities": [[14, 16, "HyperparameterName", "learning rate"], [19, 20, "MetricName", "loss"], [27, 28, "TaskName", "VQA"]]}, {"text": "VQA - CPv2 VQAv2 Baseline - Without visual grounding UpDn 0.0110 0.0155 Grounding using human - based cues", "entities": [[0, 1, "TaskName", "VQA"], [7, 9, "TaskName", "visual grounding"]]}, {"text": "HINT 0.1020 0.1350 SCR 0.0340 -0.0670", "entities": []}, {"text": "Grounding using irrelevant cues HINT -0.0048 -0.0200", "entities": []}, {"text": "SCR 0.0580 -0.0100", "entities": []}, {"text": "Grounding using \ufb01xed random cues HINT 0.0510 0.0620 SCR -0.0250 -0.0350", "entities": []}, {"text": "Grounding using variable random cues HINT 0.0570 0.0623 SCR -0.0380", "entities": []}, {"text": "0.0246", "entities": []}, {"text": "Regularization by zeroing out answers Ours1%fixed -0.1050 -0.1200", "entities": []}, {"text": "Ours100 %", "entities": []}, {"text": "-0.0750 -0.0100 and train the model to a maximum of 12 epochs .", "entities": []}, {"text": "Then , following ( Wu and Mooney , 2019 ) , for the second phase , we use the best performing model from the \ufb01rst phase to train the second phase , which criticizes incorrect dominant answers .", "entities": []}, {"text": "For the second phase , we use a learning rate of 10\u22124and weight of 1000 , which is applied alongside the loss term used in the \ufb01rst phase .", "entities": [[8, 10, "HyperparameterName", "learning rate"], [21, 22, "MetricName", "loss"]]}, {"text": "The speci\ufb01ed hyperparameters worked better for us than the values provided in the original paper .", "entities": []}, {"text": "Our Zero - Out Regularizer Our regularization method , which is a binary cross entropy loss between the model predictions and a zero vector , does not use additional cues or sensitivities and yet achieves near state - of - the - art performance on VQA - CPv2 .", "entities": [[15, 16, "MetricName", "loss"], [45, 46, "TaskName", "VQA"]]}, {"text": "We set the learning rate to:2\u00d710\u22126 r , whereris the ratio of the training instances used for \ufb01ne - tuning .", "entities": [[3, 5, "HyperparameterName", "learning rate"]]}, {"text": "The weight for the loss is set to 2 .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "We report the performance obtained at the8thepoch .", "entities": []}, {"text": "A.2 Results Correlation with Ground Truth Visual Cues", "entities": []}, {"text": "Following ( Selvaraju et al . , 2019 ) , we report Spearman \u2019s rank correlation between network \u2019s sensitivity scores and human - based scores in Table A3 .", "entities": []}, {"text": "For HINT and our zero - out regularizer , we use human - based attention maps .", "entities": []}, {"text": "For SCR , we use textual explanation - based scores .", "entities": []}, {"text": "We \ufb01nd that HINT", "entities": []}, {"text": "8179trained on human attention maps has the highest correlation coef\ufb01cients for both datasets .", "entities": []}, {"text": "However , compared to baseline , HINT variants trained on random visual cues also show improved correlations .", "entities": []}, {"text": "For SCR , we obtain surprising results , with the model trained on irrelevant cues obtaining higher correlation than that trained on relevant visual cues .", "entities": []}, {"text": "As expected , applying our regularizer does not improve rank correlation .", "entities": []}, {"text": "Since HINT trained on relevant cues obtains the highest correlation values , it does indicate improvement in visual grounding .", "entities": [[17, 19, "TaskName", "visual grounding"]]}, {"text": "However , as we have seen , the improvements in performance can not necessarily be attributed to better overlap with ground truth localizations .", "entities": []}, {"text": "A Note on Qualitative Examples Presentation of qualitative examples in visual grounding models for VQA suffers from con\ufb01rmation bias i.e. , while it is possible to \ufb01nd qualitative samples that look at relevant regions to answer questions properly , it is also possible to \ufb01nd samples that produce correct answers without looking at relevant regions .", "entities": [[10, 12, "TaskName", "visual grounding"], [14, 15, "TaskName", "VQA"]]}, {"text": "We present examples for such cases in Fig .", "entities": []}, {"text": "A3 .", "entities": []}, {"text": "We next present a quantitative assessment of visual grounding , which does not suffer from the con\ufb01rmation bias .", "entities": [[7, 9, "TaskName", "visual grounding"]]}, {"text": "Quantitative Assessment of Grounding In order to truly assess if existing methods are using relevant regions to produce correct answers , we use our proposed metric : Correctly Predicted but Improperly Grounded ( CPIG ) .", "entities": []}, {"text": "If the CPIG values are large , then it implies that large portion of correctly predicted samples were not properly grounded .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "A4shows%CPIG for different variants of HINT trained on human attention - based cues , whereas Fig .", "entities": []}, {"text": "A5shows the metric for different variants of SCR trained on textual explanationbased cues .", "entities": []}, {"text": "We observe that HINT and SCR trained on relevant regions have the lowest % CPIG values ( 70.24 % and 80.22 % respectively ) , indicating that they are better than other variants in \ufb01nding relevant regions .", "entities": []}, {"text": "However , only a small percentage of correctly predicted samples were properly grounded ( 29.76 % and 19.78 % for HINT and SCR respectively ) , even when trained on relevant cues .", "entities": []}, {"text": "Breakdown by Answer Types Table A4shows VQA accuracy for each answer type on VQACPv2 \u2019s test set .", "entities": [[6, 7, "TaskName", "VQA"], [7, 8, "MetricName", "accuracy"]]}, {"text": "HINT / SCR and our regularizer show large gains in \u2018 Yes / No \u2019 questions .", "entities": []}, {"text": "Table A4 :", "entities": []}, {"text": "VQA accuracy per answer - type on VQACPv2 test set .", "entities": [[0, 1, "TaskName", "VQA"], [1, 2, "MetricName", "accuracy"]]}, {"text": "Overall Yes / NoNum Other Baseline - Without visual grounding UpDn 40.1 41.1 12.0 47.2 Grounding using human - based cues HINT 48.2 65.2 13.8 47.5 SCR 49.1 70.3 11.5 48.0 Grounding using irrelevant cues HINT 48.0 67.2 13.5 47.1 SCR 49.2 73.4 11.5 46.4 Grounding using \ufb01xed random cues HINT 48.1 66.9 13.8 46.9 SCR 49.1 74.7 12.2 45.1 Grounding using variable random cues HINT 48.1 67.1 13.9 46.9 SCR 49.2 74.7 12.2 45.1 Regularization by zeroing out answers Ours1%fixed 48.9 69.8 11.3 47.8", "entities": [[8, 10, "TaskName", "visual grounding"]]}, {"text": "Ours100 % 48.2 66.7 11.7 47.9 We hypothesize that the methods help forget linguistic priors , which improves test accuracy of such questions .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "In the train set of VQACPv2 , the answer \u2018 no \u2019 is more frequent than the answer \u2018 yes \u2019 , tempting the baseline model to answer \u2018 yes / no \u2019 questions with \u2018 no \u2019 .", "entities": []}, {"text": "However , in the test set , answer \u2018 yes \u2019 is more frequent .", "entities": []}, {"text": "Regularization effects caused by HINT / SCR and our method cause the models to weaken this prior i.e. , reduce the tendency to just predict \u2018 no \u2019 , which would increase accuracy at test because \u2018 yes \u2019 is more frequent in the test set .", "entities": [[32, 33, "MetricName", "accuracy"]]}, {"text": "Next , all of the methods perform poorly on \u2018 Number ( Num ) \u2019 answer type , showing that methods \ufb01nd it dif\ufb01cult to answer questions that are most reliant on correct visual grounding such as : localizing and counting objects .", "entities": [[33, 35, "TaskName", "visual grounding"]]}, {"text": "Finally , we do not observe large improvements in \u2018 Other \u2019 question type , most likely due to the large number of answers present under this answer type .", "entities": []}, {"text": "Accuracy versus Size of Train Set We test our regularization method on random subsets of varying sizes .", "entities": [[0, 1, "MetricName", "Accuracy"]]}, {"text": "Fig . A6shows the results when we apply our loss to 1\u2212100 % of the training instances .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "Clearly , the ability to regularize the model does not vary much with respect to the size of the train subset , with the best performance occurring when our loss is applied to 1%of the training instances .", "entities": [[29, 30, "MetricName", "loss"]]}, {"text": "These results support our claims that it is possible to improve performance without actually performing visual grounding .", "entities": [[15, 17, "TaskName", "visual grounding"]]}, {"text": "8180 Q : Is this food sweet ?", "entities": []}, {"text": "A : yes Remarks : The most sensitive regions for irrelevant/ random variants do not contain food , yet their   answers are correct .", "entities": []}, {"text": "Ground Truth LocalizationHINT trained on   relevant cuesHINT trained on   irrelevant cuesHINT trained on    random cues Q : Has the boy worn out his jeans ?", "entities": []}, {"text": "A : yes Remarks : All of the variants look at both relevant and irrelevant regions to produce correct   answer .", "entities": []}, {"text": "Q : Is the sport being played tennis or volleyball ?", "entities": []}, {"text": "A : tennis Remarks : None of the variants look at relevant regi ons , and yet produce correct answer .", "entities": []}, {"text": "Q : What is the swimmer doing ?", "entities": []}, {"text": "A : surfing Remarks : Models trained on irrelevant / random cues", "entities": []}, {"text": "d o not look at the swimmer at all , yet   produce correct answer .", "entities": []}, {"text": "Figure A3 : Visualizations of most sensitive visual regions used by different variants of HINT to make predictions .", "entities": []}, {"text": "We pick samples where all variants produce correct response to the question .", "entities": []}, {"text": "The \ufb01rst column shows ground truth regions and columns 2 - 4 show visualizations from HINT trained on relevant , irrelevant and \ufb01xed random regions respectively .", "entities": []}, {"text": "8181 71.56 % 70.24%79.51%75.22 % 75.90%82.48 % 80.75 % Methods% CPIG 0%25%50%75%100 % Baseline   ( HAT)HINT   ( relevant)HINT   ( irrelevant)HINT   ( random)HINT ( var   random)Ours ( 1 %   fixed)Ours ( 100%)Figure A4 : % CPIG for baseline and different variants of HINT and our method , computed using ground truth relevant regions taken from human attention maps ( lower is better ) .", "entities": []}, {"text": "81.28 % 80.22%84.30 % 86.67 % 86.81 % 87.82 % 87.19 % Methods% CPIG", "entities": []}, {"text": "0%25%50%75%100 % Baseline ( txt ) SCR   ( relevant)SCR   ( irrelevant)SCR   ( random)SCR ( var   random)Ours ( 1 %   fixed)Ours ( 100 % )", "entities": []}, {"text": "Figure A5 : % CPIG for baseline and different variants of SCR and our method , computed using ground truth relevant regions taken from textual explanations ( txt ) .", "entities": []}, {"text": "7876.5 77.4 77.1 76.8 76.5 76.5 76.7 76.8 76.9 48.9 48.7 48.5 48.5 48.4 48.3 48.3 48.2 48.2 48.1", "entities": []}, {"text": "Percentage of training subset used for fine - tuningAccuracy 020406080 1 % 2 % 3 % 4 % 5 % 20 % 40 % 60 % 80 % 100 % Train Test Figure A6 :", "entities": []}, {"text": "The regularization effect of our loss is invariant with respect to the dataset size .", "entities": [[5, 6, "MetricName", "loss"]]}]