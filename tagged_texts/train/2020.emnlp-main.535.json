[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 6605\u20136619 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics6605MovieChats :", "entities": []}, {"text": "Chat like Humans in a Closed Domain Hui Su1\u0003 , Xiaoyu Shen2\u0003,Zhou Xiao1 , Zheng Zhang3y Ernie Chang4 , Cheng Zhang1 , Cheng Niu1and Jie Zhou1 1Pattern Recognition Center , Wechat AI , Tencent Inc , China 2Amazon Alexa AI3Tsinghua University 4Department of Language Science and Technology , Saarland University", "entities": [[30, 31, "DatasetName", "Wechat"]]}, {"text": "aaronsu@tencent.com,gyouu@amazon.com Abstract Being able to perform in - depth chat with humans in a closed domain is a precondition before an open - domain chatbot can ever be claimed .", "entities": [[24, 25, "TaskName", "chatbot"]]}, {"text": "In this work , we take a close look at the movie domain and present a large - scale high - quality corpus with \ufb01ne - grained annotations in hope of pushing the limit of moviedomain chatbots .", "entities": []}, {"text": "We propose a uni\ufb01ed , readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval .", "entities": []}, {"text": "The model is \ufb01rst pretrained on the huge general - domain data , then \ufb01netuned on our corpus .", "entities": []}, {"text": "We show this simple neural approach trained on high - quality data is able to outperform commercial systems replying on complex rules .", "entities": []}, {"text": "On both the static and interactive tests , we \ufb01nd responses generated by our system exhibits remarkably good engagement and sensibleness close to human - written ones .", "entities": []}, {"text": "We further analyze the limits of our work and point out potential directions for future work1 .", "entities": []}, {"text": "1 Introduction Being able to converse like humans in a closed domain is a precondition before an intelligent opendomain chatbot , which further requires transiting among various domains , can be designed ( Gao et al . , 2019 ;", "entities": [[5, 6, "DatasetName", "converse"], [19, 20, "TaskName", "chatbot"]]}, {"text": "Su et al . , 2020 ) .", "entities": []}, {"text": "Nonetheless , even if constrained in a speci\ufb01c domain , current chatbots are still far from satisfactory .", "entities": []}, {"text": "Unlike task - oriented systems that can be relatively well - resolved with handcrafted templates , human conversations feature a complex mixture of QA , chitchat , recommendation , etc . without pre - speci\ufb01ed goals or conversational patterns ( Dodge et al . , 2016 ; Akasaki and Kaji , 2017 ; Shen et al . , 2018 ) .", "entities": []}, {"text": "Selecting proper \u0003Corresponding Authors .", "entities": []}, {"text": "Work done before Xiaoyu Shen joins Amazon .", "entities": []}, {"text": "yWork done while interning at Wechat .", "entities": [[5, 6, "DatasetName", "Wechat"]]}, {"text": "1Dataset and model are available at https://github . com / chin - gyou / MovieChats .domain knowledge to support response generation at all the different situations is challenging ( Milward and Beveridge , 2003 ; Lian et", "entities": [[19, 21, "TaskName", "response generation"]]}, {"text": "al . , 2019 ; Shen et al . , 2019 ) .", "entities": []}, {"text": "In this work , we direct our focus to the movie domain and present a large - scale , crowdsourced Chinese dataset with \ufb01ne - grained annotations in hope of boosting the study towards a human - like closed - domain chatbot .", "entities": [[41, 42, "TaskName", "chatbot"]]}, {"text": "A variety of dialogue datasets with grounded domain knowledge have already been proposed .", "entities": []}, {"text": "However , they are collected either through ( 1 ) online forum crawling ( Dodge et al . , 2016 ; Ghazvininejad et al . , 2018 ; Liu et al . , 2018 ; Zhou", "entities": []}, {"text": "et", "entities": []}, {"text": "al . , 2018a ; Qin et al . , 2019 ) , which are noisy , multi - party , mostly contain only single - exchange QA , or ( 2 ) crowdsourced ( Zhu et al . , 2017 ; Zhou et", "entities": []}, {"text": "al . , 2018b ; Moon et al . , 2019 ; Wu et al . , 2019 ) , which are small - scale and often created in an overconstrained setting like teacher - student ( Moghe et al . , 2018 ) .", "entities": []}, {"text": "Even for datasets crowd - sourced in unconstrained scenarios , suggestive domain knowledge is provided for humans before an utterance is provided .", "entities": []}, {"text": "This would inevitably prompt humans to utilize these knowledge deliberately , yielding unnatural conversations simply connecting the knowledge ( Dinan et al . , 2019 ; Zhou et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "We show examples from other datasets in Appendix Table 10 .", "entities": []}, {"text": "In comparison , our dataset has the following advantages : 1.Natural : Crowdworkers chat in a free environment without further constraint or prompt in order to mimic the human daily conversations to the largest extent .", "entities": []}, {"text": "2.Large - scale : It covers 270k human dialogues with over 3 M utterances , which is at least one order of magnitude larger than all the other crowd - sourced datasets .", "entities": []}, {"text": "3.Annotated : Utterances are labeled with entity information and dialogue acts classi\ufb01ed into", "entities": []}, {"text": "660615 \ufb01ne - grained aspects , based on which linked into different types of knowledge .", "entities": []}, {"text": "Different from previous crowd - sourced works , our annotation process is conducted posteriori so that it will not interfere with human conversations , e.g. , prompt them to overuse suggested knowledge .", "entities": []}, {"text": "Built upon our dataset , we propose a simple uni\ufb01ed language model approach to push the limits of movie - domain chatbots .", "entities": []}, {"text": "The model is \ufb01rst pretrained on 2.2B words collected from various general - domain conversational resources , then \ufb01netuned on the movie dataset with additional knowledge and dialogue acts incorporated .", "entities": []}, {"text": "We pool all components like intent prediction and knowledge retrieval into a sequence prediction task and solve them with a uni\ufb01ed language model architecture .", "entities": []}, {"text": "It avoids designing complex systems for individual components separately and all subtasks can be easily trained simultaneously ( Hosseini - Asl et al . , 2020 ;", "entities": []}, {"text": "Peng et al . , 2020 ) .", "entities": []}, {"text": "We show our simple uni\ufb01ed approach outperforms strong baselines for each separate subtask .", "entities": []}, {"text": "Knowledge retrieval , dialogue acts prediction and general - domain pretrain bene\ufb01t from each other and altogether bring improvement to the generation quality .", "entities": []}, {"text": "In the online interactive test , our best model succeeds at chatting with humans for 11.4 turns without being detected to be a machine , outperforming even commercial chatbots Mitsuku2and Microsoft XiaoIce3which further rely on complex rules .", "entities": []}, {"text": "By analyzing the limitations of our model , we \ufb01nd it especially has dif\ufb01culty at dealing with in - depth discussions over long turns .", "entities": []}, {"text": "Future research can consider employing larger knowledge base or explicit state tracking .", "entities": []}, {"text": "In summary , our main contributions are ( 1 ) presenting a high - quality , large - scale Chinese conversational corpus with \ufb01ne - grained annotations in the movie domain to bene\ufb01t future study , ( 2 ) showing that a simple uni\ufb01ed neural model trained on the high - quality dataset can approach human performance and even outperform commercial systems replying on complex rules , and ( 3 ) studying the shortcomings of current techniques , providing suggestive directions for future research .", "entities": []}, {"text": "2 Dataset Construction The dataset construction consist of ( 1 ) crowdsourcing the dialogues , ( 2 ) annotating dialog acts and entities and ( 3 ) linking utterances into 2https://www.pandorabots.com/mitsuku/ 3https://www.msxiaobing.com/grounded knowledge .", "entities": []}, {"text": "We explain these three steps in order and present the dataset statistics in the end .", "entities": []}, {"text": "Dialogue Crowd - sourcing We obtain the dialogue dataset through a two - phase Wizard - of - Ozstyle collection ( Kelley , 1984 ; Dahlb \u00a8ack et", "entities": []}, {"text": "al . , 1993 ) .", "entities": []}, {"text": "In the \ufb01rst phase , we run small - scale pilot studies and examine the quality of collected conversations .", "entities": []}, {"text": "Based on the examination , we created tutorials and quali\ufb01cation tests .", "entities": []}, {"text": "They are used to train and qualify crowd - workers for the second phase .", "entities": []}, {"text": "During this second phase , we consistently monitor the collected dialogue datasets and perform periodic quality check on samples from every individual work pairs .", "entities": []}, {"text": "If more than 5%from one pair are considered invalid , their collections will be removed .", "entities": []}, {"text": "Before a conversation started , two workers are paired and a movie is chosen agreed by both4 .", "entities": []}, {"text": "We constrain at least one of them to have watched the movie to make sure the conversation is contentful5 .", "entities": []}, {"text": "The annotators are especially instructed to ( 1 ) behave naturally as in daily life , ( 2 ) avoid dirty words and ( 3 ) talk differently in each conversation .", "entities": []}, {"text": "Duplicate conversations will be removed if more than 70 % of their contents are overlapped .", "entities": []}, {"text": "To encourage diverse movies , we further set an upper limit to forbid one movie from being talked about for more than 100 times .", "entities": []}, {"text": "The whole collecting process lasts two months .", "entities": []}, {"text": "In the end , 245 participants are involved with 66,424 movies being talked about in total .", "entities": []}, {"text": "Dialogue Act and Entity Annotation Following prior work , we base our annotation schema on the ISO 24617 - 2 standard ( Bunt et al . , 2010 , 2012 ) .", "entities": []}, {"text": "Table 1 shows our annotation schema , counts , descriptions , and brief examples .", "entities": []}, {"text": "The dialogue acts ( DAs ) are organized in a hierarchical structure .", "entities": []}, {"text": "The \ufb01rst layer makes distinctions on three concepts : objective facts , recommendations and subjective feelings .", "entities": []}, {"text": "Each concept can either be either requested or informed during the conversation .", "entities": []}, {"text": "We further de\ufb01ne an \u201c Other \u201d class to include actions that do not belong to any of the three concepts , like some general non - contentful greetings or echos .", "entities": []}, {"text": "The second layer includes 15 \ufb01ner - grained aspects covering most popular topics being discussed .", "entities": []}, {"text": "Every \ufb01rst - layer DA ( except Other ) will be further group it into one of these 15 aspects , e.g. , the de4We \ufb01x one movie beforehand as the main topic , but they are allowed switch to other movies as the conversation goes .", "entities": []}, {"text": "5The knowledge of the movie is presented to the worker who has watched it to refresh his memory about movie details .", "entities": []}, {"text": "6607 \u6211\u521a\u770b\u5b8c\u300a\u6d77\u4e0a\u94a2\u7434\u5e08\u300b\uff0c\u611f\u89c9 \u2f3c \u7075\u5f88\u9707\u64bcI just finished watching \" The Legend of 1900 \" and feel shockedDA : Inform_fact , Inform_feelingAspects : NameMov_Tracker : The Legend of 1900\u771f\u7684\u5417\uff0c\u8fd9\u662f\u90e8\u4ec0\u4e48\u7535\u5f71Really , what kind of movie is thisDA : Request_factAspects : TypeMov_Tracker : The Legend of 1900 \u2f00 \u90e8\u610f \u2f24 \u5229\u62cd\u7684\u82f1\u8bed\u7535\u5f71\uff0c\u8bb2 \u2f00 \u4f4d\u5929\u624d\u94a2\u7434\u5e08 \u2f00 \u8f88 \u2f26 \u90fd\u4f4f\u8f6e\u8239\u4e0aAn Englishfilm made in Italy , where a talented pianist lived on the ship all his lifeDA : Inform_factAspects :", "entities": []}, {"text": "Region , Language , PlotMov_Tracker : The Legend of 1900\u8fd9\u4e48\u79bb\u5947\u7684\u6545\u4e8b\uff0c\u4e3a\u4ec0\u4e48\u4ed6\u4e0d\u79bb\u5f00\u90a3\u8258\u8239\u5462Such a bizarre story , why did n\u2019t he leave that boat?DA : Inform_feeling , Request_factAspect : PlotMov_Tracker : The Legend of 1900\u4ed6\u628a\u8fd9 \u2fa5 \u5f53\u6210\u4e86\u4ed6\u7684\u7cbe\u795e\u5bb6\u56ed\uff0c\u6240\u4ee5\u8239\u8981\u88ab\u70b8\u6389\u4ed6\u90fd\u4e0d\u80af\u79bb\u5f00He regarded this as his spiritual home , so when the last ship was about to be blown up , he refused to leaveDA : Inform_feeling , Inform_factAspects : PlotMov_Tracker : The Legend of 1900\u5c31\u50cf\u963f \u2f62 \u6b63\u4f20 \u2f00 \u6837\u3002\u73b0\u4ee3\u793e\u4f1a\u5f88\u5c11\u6709 \u2f08 \u80fd\u548c\u4e16\u4fd7\u5272\u88c2\u53bb\u8ffd\u6c42 \u2f83 \u2f30 \u7684\u7cbe\u795e\u4e16\u754cJust like in Forrest Gump .", "entities": []}, {"text": "Few people in modern society can separate themselves from the world to pursue their own spiritual worldDA : Inform_feelingAspects : NameMov_Tracker : Forrest GumpName : The Legend of 1900 |Comment : One of the favorite movies that shocked my soul .", "entities": []}, {"text": "Region : Italy |Language : English", "entities": []}, {"text": "|", "entities": []}, {"text": "Plot : \u201c a talented pianist lived on the ship all his life\u201dPlot : \u201c why did n\u2019t he leave that boat\u201dPlot : \u201c when the last ship was about to be blown up , he refused to leave\u201dComment : What he can not leave is not the boat , but the spiritual home that nurtured him .", "entities": []}, {"text": "Name : Forrest Gump | Comment : The negative perspective of 1900 may represent the director\u2018s reflection on modern civilization .", "entities": []}, {"text": "Figure 1 : An example of our dataset .", "entities": []}, {"text": "The annotations include dialogue act , aspects and movie tracker and grounded knowledge .", "entities": []}, {"text": "tailed DA of the \ufb01rst example in Table 1 will be request factdirector .", "entities": []}, {"text": "If one utterance contains multiple dialogue acts , we order the dialogue acts based on their turn of appearance in the utterance .", "entities": []}, {"text": "As for the named entity recognition , we labeled 5 kinds of entities : movie names , director , actor , type and role ( \ufb01rst 5 aspects ) .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}, {"text": "To speed up the annotation process , we \ufb01rst de\ufb01ne a set of handcrafted regular expressions , which covers most frequent patterns at each class , to train a DA and NER classi\ufb01er .", "entities": [[31, 32, "TaskName", "NER"]]}, {"text": "The annotators are instructed to post - correct the auto - labeled dialogues instead of doing everything from scratch .", "entities": []}, {"text": "The classi\ufb01ers are trained with online learning ( Sahoo et al . , 2018 ) to keep improving the accuracy and lower down the frequency of post - correction in consequence .", "entities": [[5, 7, "TaskName", "online learning"], [19, 20, "MetricName", "accuracy"]]}, {"text": "As we observe , this semi - automated way signi\ufb01cantly speeds up the labeling process .", "entities": []}, {"text": "All the dataset is \ufb01nished labeling within three weeks with 188 annotators involved .", "entities": []}, {"text": "Knowledge Linkage We extract fact knowledge from the structured table in Douban Movie6 , a popular Chinese platform for movies .", "entities": [[11, 12, "DatasetName", "Douban"]]}, {"text": "The knowledge is organized in the form of key - value pairs , 6https://movie.douban.comwhere the key corresponds to the 15 aspects de\ufb01ned by us .", "entities": []}, {"text": "Some aspects , like lines or music , are not directly available from the structured table .", "entities": []}, {"text": "We extract these missing information from other sources and combine it into our knowledge base .", "entities": []}, {"text": "For utterances labeld as inform / request fact , we link them to the key - value pairs from the same aspect .", "entities": []}, {"text": "Apart from the objective knowledge , we also crawl movie comments from Douban Movie to support the generation of responses expressing subjective feelings .", "entities": [[12, 13, "DatasetName", "Douban"]]}, {"text": "These comments can be a good supplementary to provide knowledge that can be hardly organized in the structured form ( Moghe et al . , 2018 ) .", "entities": []}, {"text": "For utterances labeled as inform / request feeling , we compare them with Douban comments from the same movie and compute the similarity score based on weighted average of edit distance , Jaccard distance , tf - idf , sentence vector cosine similarity , common words and entities .", "entities": [[13, 14, "DatasetName", "Douban"]]}, {"text": "Each utterance is linked to the most similar comment with a threshold cutoff .", "entities": []}, {"text": "In the end , 51:7%of the utterances about feelings have grounded comments .", "entities": []}, {"text": "For utterances about recommendations , we simply ground them to the men-", "entities": []}, {"text": "6608Dialogue Act Count(% ) Linked Description Example Request fact 8.62 Fact Request facts .", "entities": []}, {"text": "Who directed this movie ?", "entities": []}, {"text": "Request recommend 4.91 None Ask recommendations .", "entities": []}, {"text": "Which other movies do you recommend ?", "entities": []}, {"text": "Request feeling 4.98 Comment Request feelings .", "entities": []}, {"text": "How do you like its theme music ?", "entities": []}, {"text": "Inform fact 24.85 Fact Inform facts .", "entities": []}, {"text": "Wong Kar - Wai directed this movie .", "entities": []}, {"text": "Inform recommend 4.56 Movie Give recommendations .", "entities": []}, {"text": "I can also recommend Titanic !", "entities": []}, {"text": "Inform feeling 28.95 Comment Convey feelings Its music reminds me of my childhood !", "entities": []}, {"text": "Other 23.10 None Greetings , echos , etc . hahaha .", "entities": []}, {"text": "Aspects : name , director , actor , type , role , region , time , plot , line , award , gross , rating , website , music , others Table 1 : Counts , type of linked knowledge , descriptions and examples of the dialogue acts .", "entities": []}, {"text": "Examples are shown only in translated English for space limit .", "entities": []}, {"text": "Each dialogue act ( except other ) is paired with one of the 15 aspects .", "entities": []}, {"text": "Details descriptions and statistics of the aspects are in Appendix Table 11 .", "entities": []}, {"text": "tioned movie entities7 , and no grounded knowledge is linked for utterances labeled as Other .", "entities": []}, {"text": "An example of our annotation is presented in Table 1 .", "entities": []}, {"text": "Train Valid Test / s Test / u Dialogues 221,526 12,307 6154 6154 Utterances 2,702,618 153,591 79,978 74,463 Turns / D 12.2 12.5 13.0 12.1 Tokens / U 14.9 12.6 13.5 15.9 Movies 59,449 3,146 1454 1454 Table 2 : Number of dialogues , utterances , turns per dialogues , tokens per utterance and movies in the train , valid , test ( seen movies ) and test ( unseen movies ) dataset .", "entities": []}, {"text": "Dataset Statistics", "entities": []}, {"text": "The \ufb01nal dialogue dataset we collect consists of 246,141 dialogues with 3,010,650 turns , which we divide into 221,526 for train , 12,307 for validation , and 12,308 for test .", "entities": []}, {"text": "The test set is split into test / seen for movies seen in the training set , and Test / unseen for unseen movies , each covers conversations about 1454 movies .", "entities": []}, {"text": "The training set covers 59,449 movies with 12.2 turn per dialogue in average .", "entities": []}, {"text": "Each turn contains 14.9 tokens .", "entities": []}, {"text": "Data statistics can be found in Table 2 . 3 Model Architecture Language models have demonstrated impressive performance as a universal learner across NLP tasks ( Shen et al . , 2017 ; Peters et al . , 2018 ; Radford et", "entities": []}, {"text": "al . , 2019 ; Brown et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Inspired by this , our dialogue generation model is implemented as a Transformer - based language model like GPT2 ( Radford et al . , 2019 ; Zhang et al . , 2019 ) .", "entities": [[0, 1, "DatasetName", "Inspired"], [5, 7, "TaskName", "dialogue generation"], [12, 13, "MethodName", "Transformer"]]}, {"text": "It contains a pipeline process of movie tracker , intent prediction , knowledge retrieval and text gener7We only consider recommending movies as for the DA about recommendation .", "entities": []}, {"text": "Recommending other aspects require assembling recommendation systems of different domains , which is beyond the scope of this paper .", "entities": [[5, 7, "TaskName", "recommendation systems"]]}, {"text": "Context [ context ] dialogue context Fact [ fact ] key - value pair(s ) Comment [ comment ] movie comment(s )", "entities": []}, {"text": "Recommend [ recommend ] movie name(s ) Track [ tracker ]", "entities": []}, {"text": "[ inherit ] or a new movie name Intent [ intent ] DA sequence Retrieve", "entities": []}, {"text": "[ retrieve ] knowledge Response [ response ] response Table 3 : A schematic representation of the different components .", "entities": []}, {"text": "Upper parts are the conditions and lower parts are the targets .", "entities": []}, {"text": "The condition and target are concatenated into a single sequence then fed into the language model .", "entities": []}, {"text": "ation .", "entities": []}, {"text": "Unlike in traditional task - oriented systems where subtasks are decomposed separately , we opt fora simple and uni\ufb01ed approach by casting all subtasks into sequence prediction .", "entities": []}, {"text": "A special token is injected in the beginning to indicate which subtask to perform ( Hosseini - Asl et al . , 2020 ;", "entities": []}, {"text": "Peng et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Table 3 shows the schema representation for different components .", "entities": []}, {"text": "The condition and the target are concatenated into a single sequence and then fed into the language model to train .", "entities": []}, {"text": "For example , the task of predicting the intent given the dialogue context will be transformed into \u201c [ context ] dialogue context [ intent ] DA sequence \u201d , where theDA sequence will be predicted conditioned on \u201c [ context ] dialogue context [ intent ] \u201d .", "entities": []}, {"text": "General - domain Pretrain", "entities": [[0, 1, "DatasetName", "General"]]}, {"text": "The model is \ufb01rst pretrained on a mixed general - domain conversational corpus crawled from various sources like", "entities": []}, {"text": "6609douban8 , tieba9 , zhihu10and weibo11 .", "entities": []}, {"text": "The pretrained corpus covers 468 M conversations with 2.2B words .", "entities": []}, {"text": "Each training instance is processed in the form of \u201c [ context ] dialogue context [ response ] response \u201d where response will be predicted given \u201c [ context ] dialogue context [ response ] \u201d .", "entities": []}, {"text": "The objective is a mixture of maximum likelihood and unlikelihood training ( He and Glass , 2019 ; Li et al . , 2019 ) , which we \ufb01nd help reduce repeated and incoherent generations as observed in Adiwardana et", "entities": []}, {"text": "al . ( 2020 ) .", "entities": []}, {"text": "The unlikelihood training minimizes the likelihood of 1 ) randomly sampled responses from the corpus and 2 ) repeated bigrams from the previous generated tokens .", "entities": []}, {"text": "Movie Tracker The movie tracker is like the belief state tracker in task - oriented systems ( Henderson et al . , 2013 ) .", "entities": []}, {"text": "It is used to track which movie will be talked about in the next utterance .", "entities": []}, {"text": "Conditioned on the dialogue context , we feed a special token [ tracker ] to the decoder .", "entities": []}, {"text": "The decoder should decide whether to inherit the movie talked about in the last utterance or switch to a new movie .", "entities": []}, {"text": "The target would be \u201c [ tracker ] [ inherit ] \u201d or \u201c [ tracker ] new movie name \u201d if transiting to a new movie .", "entities": []}, {"text": "The initial state is an empty indicator [ None ] .", "entities": []}, {"text": "Intent Prediction", "entities": []}, {"text": "The intent prediction is also cast as a sequence prediction task .", "entities": []}, {"text": "Compared with the traditional way of multi - label classi\ufb01cation , casting it as sequence prediction is better at addressing the coexistence of multiple DAs and capturing the sequential dependencies among the hierarchy ( Raffel et al . , 2019 ; Vedula et al . , 2020 ) .", "entities": []}, {"text": "For example , to predict the DAs of the 4th utterance in Figure 1 , the sequence fed to the language model will be \u201c [ context ] dialogue context [ intent ] inform , feeling , plot , request , fact , plot \u201d .", "entities": []}, {"text": "By this means , before predicting a DA , the model can condition on both the dialogue context and its previous DAs to improve the accuracy .", "entities": [[25, 26, "MetricName", "accuracy"]]}, {"text": "Knowledge Retrieval", "entities": []}, {"text": "The knowledge retrieval component is similar to the classical DSSM model ( Huang et al . , 2013 ) .", "entities": []}, {"text": "We replace the MLP with our language model encoder to get the embedding for knowledge .", "entities": [[3, 4, "DatasetName", "MLP"]]}, {"text": "Note that we only select knowledge from the current movie , which can be obtained from the movie tracker , so it is possible to 8https://www.douban.com/ 9https://tieba.baidu.com/ 10https://www.zhihu.com/ 11https://www.weibo.com/Parameters Layers Dimension Heads 117 M 12 768 12 345 M 24 1024 16 762 M 36 1280 20 Table 4 : Architecture hyperparameters for the 3 model sizes .", "entities": []}, {"text": "run exact softmax over all knowledge candidates and maximize the likelihood of the ground truth .", "entities": [[2, 3, "MethodName", "softmax"]]}, {"text": "We condition on both the dialogue context and the intent .", "entities": []}, {"text": "The sequence fed to the language model would be \u201c [ context ] dialogue context [ intent ] DA sequence [ retrieve ] knowledge \u201d where knowledge will be predicted .", "entities": []}, {"text": "When an utterance is not linked with any knowledge , it will predict a None token .", "entities": []}, {"text": "Text Generation", "entities": [[0, 2, "TaskName", "Text Generation"]]}, {"text": "The text generation is conditioned on the dialogue context , intent and the retrieved knowledge .", "entities": [[1, 3, "TaskName", "text generation"]]}, {"text": "All the conditions are concatenated into a long sequence .", "entities": []}, {"text": "The knowledge can be fact , comment , movie names of mixture of them according to the DA of the utterance .", "entities": []}, {"text": "For example , if grounded on comment knowledge , \u201c [ context ] dialogue context [ intent ] DA sequence [ comment ] comment [ response ] \u201d will be fed to the language model to generate the response .", "entities": []}, {"text": "To make it consistent with the pretrained general - domain dialogue , the position embedding of the decoded response will skip the concatenated intent and knowledge and directly follow the dialogue context .", "entities": []}, {"text": "We \ufb01nd this bene\ufb01cial when combined with pretrained models .", "entities": []}, {"text": "The objective also follows the pretrained model mixing maximum lilkelihood and unlikelihood training .", "entities": []}, {"text": "4 Experiment Setting We tokenize the Text in the unit of Chinese characters and keep all unique non - Chinese unique tokens appearing for more than 5 times .", "entities": []}, {"text": "The whole vocabulary contains 13,317 words .", "entities": []}, {"text": "We train our model on 24 Nvidia V100 GPUs ( 32 GB ) with three different model sizes as shown in Table 4 .", "entities": []}, {"text": "The batch size is \ufb01xed as 64 per GPU .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "The context length is truncated to be 300 words .", "entities": [[1, 3, "HyperparameterName", "context length"]]}, {"text": "We optimize models with the Adam gradient descent and a drop out rate of 0.1 .", "entities": [[5, 6, "MethodName", "Adam"]]}, {"text": "For the 117/345 M model , the learning rate is set as 1:5e\u00004with a linear warm up of 3600 steps .", "entities": [[7, 9, "HyperparameterName", "learning rate"]]}, {"text": "For 762 M model , the learning rate is 1:25e\u00004with a 2400 - step linear warm up .", "entities": [[6, 8, "HyperparameterName", "learning rate"]]}, {"text": "Models are trained for 150k steps on the pretraining corpus and we observe no over\ufb01tting for all .", "entities": []}, {"text": "For decoding , we employ Nucleus Sampling with p= 0:9(Holtzman et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "All experiments are done with", "entities": []}, {"text": "6610Modelseen unseen Perplexity BLEU-1/2/3 Distinct-1/2 Perplexity BLEU-1/2/3 Distinct-1/2 117 M 26.42 24.71 2.07 0.44 10.6 25.4 31.79 19.12 1.95 0.34 6.7 26.8 + DA 27.33 25.12 1.99 0.62 9.1 24.7 32.22 16.63 2.11 0.41 7.5 28.1 + Aspect 24.29 25.06 1.85 0.86 9.5 28.2 30.36 17.57 2.36 0.39 9.6 22.4 + Knowledge 20.25 29.21 2.36 1.34 8.7 26.3 28.61 24.48 2.71 0.57 10.7 27.5", "entities": [[2, 3, "MetricName", "Perplexity"], [5, 6, "MetricName", "Perplexity"]]}, {"text": "+ Pretrain 18.06 33.46 2.68 1.63 10.6 28.5 26.59 28.12 2.62 0.95 11.3 32.0 345 M 24.74 28.06 2.17 1.40 10.0 30.7 29.61 18.48 2.36 0.49 12.6 26.6 + DA 24.52 22.28 6.61 3.50 9.7 22.6 28.26 16.67 2.95 0.89 12.8 24.3 + Aspect 23.61 25.51 6.70 2.08 12.5 27.9 28.55 16.97 3.55 0.93 13.2 20.7 + Knowledge 18.64 27.01 4.27 2.89 13.4 32.2 24.61 21.13 2.58 1.08 13.8 32.8 + Pretrain 15.76 33.23 6.73 3.12 11.6 29.9 20.27 27.03 4.35 1.56 13.5 33.1 762 M 19.53 28.69 5.45 2.44 11.4 37.3 25.83 22.25 3.90 1.04 12.7 31.3 + DA 19.33 31.25 4.27 2.89 11.1 35.5 25.67 24.95 3.59 2.75 14.4 26.9 + Aspect 18.94 31.05 6.01 2.67 7.8 23.8 24.75 23.84 3.61 2.09 16.8 31.7 + Knowledge 16.45 33.55 5.53 2.71 12.6 31.9 20.30 25.18 4.39 2.17 13.8 30.9 + Pretrain 13.17 38.33 5.88 4.02 15.1 37.2 17.49 29.02 5.79 2.65 15.6 36.8 Table 5 : The perplexity , BLEU score and distinct uni / bi - grams of our model with different sizes .", "entities": [[158, 159, "MetricName", "perplexity"], [160, 162, "MetricName", "BLEU score"]]}, {"text": "Results are reported for seen / unseen movies respectively .", "entities": []}, {"text": "Each line adds one more condition on top of the above one .", "entities": []}, {"text": "PyTorch ( Paszke et al . , 2019 ) .", "entities": []}, {"text": "5 Results and Analysis Automatic Evaluation In Table 5 , we report the perplexity , BLEU scores and distinct uni / bigrams for three model sizes .", "entities": [[13, 14, "MetricName", "perplexity"], [15, 16, "MetricName", "BLEU"]]}, {"text": "To investigate the effects of incorporating annotations and pretraining , we start from a basic model which trains from scratch on our movie corpus .", "entities": []}, {"text": "At each time , we add one more condition to see its in\ufb02uence .", "entities": []}, {"text": "The results show a clear tendency of gradual improvement as more conditions are added to the training .", "entities": []}, {"text": "Adding knowledge especially boosts the performance , which is understandable considering movie - domain chats usually contain many movie - speci\ufb01c rare names .", "entities": []}, {"text": "Without knowledge grounding , it can hardly predict the correct tokens .", "entities": []}, {"text": "Pretraining on general - domain conversations can improve both the overlap with ground truth .", "entities": []}, {"text": "The distinct uni / bigrams also consistently increase , implying the model can learn useful patterns in the pretrained corpus to enrich its generations in the movie domain .", "entities": []}, {"text": "In unseen testset , the performance generally drops for all , especially for models without knowledge grounding as they have to make up facts and comments for totally unseen movies in the training set .", "entities": []}, {"text": "Table 6 measures the accuracy of predicting dialogue act ( DA ) , aspect and movie tracker of our model .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "Our models are all pretrained with general - domain corpus beforehand .", "entities": []}, {"text": "Apart from being trained only to predict the individual tasks , we include the results where all subtasks are cotrained end - to - end in the last line .", "entities": []}, {"text": "We compare our models with the Chinese RoBERTa ( Liu et al . ,Model DA1 DA2 Aspect Movie RoBERTa 75.8 64.0 44.3 90.7 117 M 76.7 66.3 43.8 88.9 345 M 82.5 71.5 46.1 90.3 762 M 82.9 73.4 51.4 92.1 762 M ( co - train ) 84.5 76.5 54.3 91.7 Table 6 : Accuracy of dialogue act , aspect and movie tracker . 2019 ) , which has achieved SOTA performance on Chinese NLU tasks .", "entities": [[7, 8, "MethodName", "RoBERTa"], [19, 20, "MethodName", "RoBERTa"], [56, 57, "MetricName", "Accuracy"]]}, {"text": "We use its optimized version for Chinese which is pretrained by masking full Chinese words ( Cui et al . , 2019)12 .", "entities": []}, {"text": "RoBERTa does the movie tracking by performing softmax over the recognized named entities .", "entities": [[0, 1, "MethodName", "RoBERTa"], [7, 8, "MethodName", "softmax"]]}, {"text": "For dialogue acts , we deal with DA1 ( inform / request / other ) and DA2(fact / feeling / recommend ) separately .", "entities": []}, {"text": "As can be observed , our simple uni\ufb01ed approach for all subtasks does not come at the expense of the accuracy .", "entities": [[20, 21, "MetricName", "accuracy"]]}, {"text": "When all subtasks are co - trained , the performance is further boosted , indicating the multitask training paradigm can bene\ufb01t individual tasks .", "entities": []}, {"text": "ModelFact Comment Recommend Hit@1 Hit@1 Hit@1/5 Random 0.054 0.011 0.000 0.000 BOW 0.135 0.048 0.011 0.014 Bert 0.406 0.067 0.048 0.075 117 M 0.402 0.053 0.045 0.071 345 M 0.455 0.071 0.047 0.072", "entities": []}, {"text": "762 M 0.518 0.120 0.053 0.086 + DA 0.543 0.158 0.066 0.104 Table 7 : Hit rates of knowledge retrieval .", "entities": []}, {"text": "Table 7 measures the performance of retrieving fact knowledge , movie comments and recommen12https://huggingface.co/h\ufb02/chinese-RoBERTa-wwm-ext", "entities": []}, {"text": "6611dation respectively .", "entities": []}, {"text": "We report the hit@1 and hit@5 scores for them ( Zhang et al . , 2018 ) .", "entities": []}, {"text": "We compare our model with a random baseline , bag - ofword ( BOW ) and the Bert ( Devlin et al . , 2019 ) model ( we pass sentences through Bert and derive a \ufb01xed - sized vector by averaging the outputs from the second - to - last layer ( May et al . , 2019 ) ) .", "entities": []}, {"text": "The BOW and Bert model are \ufb01netuned with our knowledge linkage annotations .", "entities": []}, {"text": "We \ufb01nd that our uni\ufb01ed model again outperforms all baseline approaches .", "entities": []}, {"text": "Adding the DA as a condition further helps .", "entities": []}, {"text": "Fact retrieval has the highest hit rate as it is well structured and easy to match .", "entities": []}, {"text": "Recommendation , on the other hand , is very hard to predict .", "entities": []}, {"text": "As an accurate recommendation system is clearly beyond the scope of this paper , it is understandable that our simple way fails to provide satisfying recommendations .", "entities": []}, {"text": "Metric Agreement ( % ) Fleiss\u2019k score Sensibleness 71:5\u00063:4 0.473 Engagement 64:2\u00062:3 0.438 Informativeness 79:2\u00063:1 0.576 Table 8 : Crowd worker agreement scores .", "entities": []}, {"text": "Human Evaluation Automatically evaluating dialogue systems are known to be extremely hard ( Liu et al . , 2016 ;", "entities": []}, {"text": "Su et al . , 2018 ) .", "entities": []}, {"text": "We further conduct a set of static and interactive human evaluations .", "entities": []}, {"text": "We focus on evaluate the machine - generated response from four perspectives .", "entities": []}, {"text": "Apart from the oft - used metrics ( 1 ) Sensibleness ( Sens ) and ( 2 ) Engagement ( Enga ) for open - domain chatbots , we further evaluate on ( 3 ) Factuality ( Fact ) and ( 4)Informativeness ( Info ) to see if models can actively provide informative responses based on movie facts .", "entities": []}, {"text": "Details are in Appendix B.", "entities": []}, {"text": "As evaluating factuality requires speci\ufb01c movie knowledge , this metric is only evaluated by the same person who produced the dialogue .", "entities": []}, {"text": "The other metrics are evaluated by 3 workers each .", "entities": []}, {"text": "Table 8 shows the agreement scores .", "entities": []}, {"text": "The agreement is reasonable considering the evaluations are subjective .", "entities": []}, {"text": "The results are the majority votes of the binary scores .", "entities": []}, {"text": "In the static evaluation , we sample 300 responses for each model from the test set ( mixing seen and unseen ) .", "entities": []}, {"text": "The responses can come from any turn in a conversation .", "entities": []}, {"text": "We show the results in Figure 2 .", "entities": []}, {"text": "Our largest model with 762 M is clearly preferred by human evaluators on almost all metrics and approaches human performance .", "entities": []}, {"text": "By training a larger model and increasing the training size , the gapmight be further closed .", "entities": []}, {"text": "In the interactive evaluation , humans can chat with any topic but restricted in the movie domain .", "entities": []}, {"text": "We conduct an online Turing test where one side is always a human participant not aware whom he is talking with .", "entities": []}, {"text": "The other side could be either Mitsuku , XiaoIce13 , our model ( 762 M with pretraining ) or a real human .", "entities": []}, {"text": "Mitsuku interacts in English , so we hire only English native speakers for the experiment .", "entities": []}, {"text": "We collect 100 conversations for all models .", "entities": []}, {"text": "Humans can stop interacting once they ( 1 ) \ufb01nd the other side is a machine or ( 2 ) reaches the maximum turn of 20 .", "entities": []}, {"text": "Responses from all models are later passed to the third party to judge the scores .", "entities": []}, {"text": "The results are shown on the right of Figure 2 .", "entities": []}, {"text": "Our model outperforms Mitsuku and XiaoIce by a large margin .", "entities": []}, {"text": "As Mitsuku and XiaoIce are designed to be open - domain chatbots , restricting to be on the movie domain will give our model some natural advantage .", "entities": []}, {"text": "We can also notice that Mitsuku and XiaoIce almost never produce fake facts .", "entities": []}, {"text": "The cost is the extremely low ratio of informative responses since they tend to behave over - safely and will only answer it when they are 100 % sure .", "entities": []}, {"text": "Our model is closer to humans in that sense .", "entities": []}, {"text": "It will converse actively at some risk of containing fact errors .", "entities": [[2, 3, "DatasetName", "converse"]]}, {"text": "117 M 345 M 762 M Human0:40:60:81 StaticMitsuku XiaoIce", "entities": []}, {"text": "Ours Human0:20:40:60:81", "entities": []}, {"text": "InteractiveSens Enga Fact Info Figure 2 : Results of static / interactive human evaluation .", "entities": []}, {"text": "246810121416182000:20:40:60:8 Turn NumberSEA 24681012141618200:50:550:60:65", "entities": []}, {"text": "Turn NumberFIAMitsuku", "entities": []}, {"text": "XiaoIce", "entities": []}, {"text": "Our Human Figure 3 : Change of SEA and FIA as the turn proceeds .", "entities": []}, {"text": "13We use its chat service through Weibo .", "entities": [[6, 7, "DatasetName", "Weibo"]]}, {"text": "It will sometimes generate responses containing keywords like \u201c XiaoIce \u201d .", "entities": []}, {"text": "We manually replace it to prevent disclosing its identity .", "entities": []}, {"text": "66121 User Input \u53d8\u5f62\u91d1\u521a\u770b\u4e86\u6ca1\uff0c\u8d85\u9177\u7684\u5462(Have you watched Transformers ?", "entities": []}, {"text": "So cool ) MovieChats [ Others ]", "entities": []}, {"text": "[ None]\u5f53\u7136\u770b\u8fc7\u5566(Sure )", "entities": []}, {"text": "User Input \u6211\u6700\u559c\u6b22\u5927\u9ec4\u8702\u4e86(I like Bumblebee so much )", "entities": []}, {"text": "MovieChats [ inform feeling role ]", "entities": []}, {"text": "[ \u53d8\u5f62\u91d1\u521a\u91cc\u5927\u9ec4\u8702\u6700\u53ef\u7231]\u662f\u554a\u5927\u9ec4\u8702\u771f\u7684\u592a\u592a\u592a\u53ef\u7231\u4e86\u53ed(Yes , Bumblebee is sooo cute )", "entities": []}, {"text": "User Input \u53ef\u4e0d\u561b\u5077\u5077\u628a\u8f66\u505c\u5728\u597d\u98ce\u666f\u7684\u5730\u65b9\u5e2e\u5c71\u59c6\u64a9\u59b9(Exactly , stopped the car in a nice view and helped Sam hit on girls )", "entities": []}, {"text": "MovieChats [ inform feeling plot ]", "entities": []}, {"text": "[ None]\u54c8\u54c8\uff0c\u9738\u5929\u864e\u5f53\u65f6\u4e5f\u5728(Haha , Decepticons were there , too . )", "entities": []}, {"text": "[ factuality=0 ] 2 User Input \u4e09\u50bb\u5927\u95f9\u5b9d\u83b1\u575e\u628a\u6211\u770b\u54ed\u4e86(I was moved into tears by 3 idiots )", "entities": []}, {"text": "MovieChats", "entities": []}, {"text": "[ inform feeling others ]", "entities": []}, {"text": "[ \u611f\u4eba\u7684\u5f71\u7247\uff0c\u770b\u4e86\u5f88\u591a\u904d]\u6211\u4e5f\u662f\uff0c\u770b\u4e86\u4e00\u767e\u904d(Me too , have watched it one hundred times )", "entities": []}, {"text": "User Input \u7537\u4e3b\u662f\u771f\u7684\u5f88\u806a\u660e(The leading actor is really smart . )", "entities": []}, {"text": "MovieChats [ inform feeling role ]", "entities": []}, {"text": "[ . .", "entities": []}, {"text": ".\u4e3b\u89d2\u6562\u4e8e\u5bf9\u4f20\u7edf\u6559\u80b2\u8bf4\u4e0d . . .", "entities": []}, {"text": "] \u5f88\u559c\u6b22\u7537\u4e3b\uff0c\u4f20\u7edf\u6559\u80b2\u7cca\u5f04\u4e0d\u4e86\u4ed6 ( I like him very much .", "entities": []}, {"text": "He is not fooled by the traditional educational system . )", "entities": []}, {"text": "User Input \u6240\u4ee5\u5b66\u6821\u6559\u80b2\u6a21\u5f0f\u771f\u7684\u5f88\u91cd\u8981(So the educational pattern at school is very important )", "entities": []}, {"text": "MovieChats [ inform feeling others ]", "entities": []}, {"text": "[ None ] \u793e\u4f1a\u8981\u8fdb\u6b65\uff0c\u6559\u80b2\u5c31\u8981\u4e0d\u65ad\u9769\u65b0(Education must keep going for social progress )", "entities": []}, {"text": "User Input \u7537\u4e3b\u662f\u8c01\u6f14\u7684(Who plays the actor in the movie )", "entities": []}, {"text": "MovieChats [ inform factactor ]", "entities": []}, {"text": "[ actor : \u963f\u7c73\u5c14\u0001\u6c57 . . .", "entities": []}, {"text": "] \u963f\u7c73\u5c14\u0001\u6c57\uff0c\u975e\u5e38\u51fa\u540d\u7684(Aamir Khan , very famous )", "entities": []}, {"text": "User Input \u4ed6\u8fd8\u6f14\u8fc7\u4ec0\u4e48\u7535\u5f71(Which other movie did he play )", "entities": []}, {"text": "MovieChats [ inform factname ]", "entities": []}, {"text": "[ name : \u4e09\u50bb\u5927\u95f9\u5b9d\u83b1\u575e]\u4e09\u50bb\u5927\u95f9\u5b9d\u83b1\u575e\uff0c\u4e5f\u5f88\u597d\u770b(3 idiots , also a nice movie ) [ sensibleness=0 ] 3 User Input \u6211\u6700\u8fd1\u5728\u91cd\u6e29\u54c8\u5229\u6ce2\u7279\u7cfb\u5217(I\u2019m reviewing Happy Potter series recently . )", "entities": []}, {"text": "MovieChats [ others ]", "entities": []}, {"text": "[ None]\u4f60\u559c\u6b22\u770b\u5417(Do you like it ) MovieChats [ inform feeling others ]", "entities": []}, {"text": "[ \u7b2c\u4e00\u90e8\u597d\u770b\u554a]\u7b2c\u4e00\u90e8\u662f\u6211\u6700\u559c\u6b22\u7684(The \ufb01rst part is my favorite ) MovieChats [ inform feeling role ]", "entities": []}, {"text": "[ \u5c0f\u54c8\u5229\u989c\u503c\u592a\u9ad8\u4e86]\u559c\u6b22\u5c0f\u54c8\u5229\uff0c\u90a3\u65f6\u5019\u989c\u503c\u592a\u9ad8\u4e86(I like little Harry , he was so cute that time )", "entities": []}, {"text": "MovieChats [ request factrole ]", "entities": []}, {"text": "[ role : \u54c8\u5229\u6ce2\u7279 . . .", "entities": []}, {"text": "] \u54c8\u5229\u6ce2\u7279\u662f\u8c01\u6f14\u7684\u5440(Who played the role of Harry Potter ) Table 9 : Examples of interactions with our model .", "entities": []}, {"text": "First two are failing cases from the interactive evaluation .", "entities": []}, {"text": "Last one shows how the response can be controlled by feeding different DA and aspects .", "entities": []}, {"text": "Distance from Human Performance In the interactive evaluation , compared with human performance , our model loses a bit on sensibleness and factuality but wins on the other two .", "entities": []}, {"text": "To investigate where our model fails , \ufb01gure 3 visualizes the change of SSA ( Sensibleness - Engagement average ) and FIA ( Factuality - Informativeness average ) when the conversational turn proceeds .", "entities": []}, {"text": "A good chatbot should balance well these skills ( Adiwardana et al . , 2020 ) .", "entities": [[2, 3, "TaskName", "chatbot"]]}, {"text": "SEA can re\ufb02ect how it behaves as a general chatbot while FIA can better test its capability at incorporating domain knowledge .", "entities": [[9, 10, "TaskName", "chatbot"]]}, {"text": "We can see a clear trend of decrease for all models .", "entities": []}, {"text": "As for human performance , however , the score is quite consistent across turn rounds , implying a large improvement space for current models to deal with multi - turn context .", "entities": []}, {"text": "In \ufb01gure 4 , we further show the \u201c dying distribution \u201d of our model , namely , in which DA our model fails to pass the Turing test and thereby \u201c dies \u201d .", "entities": []}, {"text": "Unsurprisingly , we can see the system fails mostly when informing facts or feelings .", "entities": []}, {"text": "Only a small portion are from non - grounded chitchats ( other ) .", "entities": []}, {"text": "This suggests the most crucial bottleneck lies in the interaction with movie - speci\ufb01c knowledge and seamlessly incorporating it into the response generation .", "entities": [[21, 23, "TaskName", "response generation"]]}, {"text": "We show some snippets of interactions with our model in Table 9 .", "entities": []}, {"text": "The \ufb01rst two Figure 4 : Dying distribution of DAs in interactive evaluation .", "entities": []}, {"text": "are failing cases labeled by humans as not factual and sensible .", "entities": []}, {"text": "We can see the model struggles at replying to too speci\ufb01c facts .", "entities": []}, {"text": "This is understandable since our knowledge base only provide short introductions and can not cover all what happened in the movie .", "entities": []}, {"text": "The second case shows its shortcoming at handing long - range consistency .", "entities": []}, {"text": "It still recommends the current movie when the user asks about \u201c which other movie \u201d .", "entities": []}, {"text": "Employing larger knowledge bases and explicitly tracking the states by a checklist ( Kiddon et al . , 2016 ) might potentially alleviate both issue .", "entities": []}, {"text": "We also provide examples for controllable generations where the DA and aspect are manually assigned .", "entities": []}, {"text": "As observed , the model shows decent performance at \ufb01tting both the dialogue con-", "entities": []}, {"text": "6613text and speci\ufb01ed conditions .", "entities": []}, {"text": "This can be helpful when \ufb01ner - grained control is needed .", "entities": []}, {"text": "6 Conclusion We present MovieChats : a movie - domain chatbot built upon a large - scale , high - quality conversational corpus with \ufb01ne - grained annotations .", "entities": [[10, 11, "TaskName", "chatbot"]]}, {"text": "The model can be trained end - to - end with a simple uni\ufb01ed language model architecture .", "entities": []}, {"text": "We show that our model , powered by well - de\ufb01ned knowledge grounding , is able to approach human performance in some perspective , though still lagged behind when it comes to dealing with detailed knowledge or long - turn consistency .", "entities": []}, {"text": "Acknowledge We thank anonymous reviewers and the dialogue system team at Wechat AI for their valuable comments .", "entities": [[11, 12, "DatasetName", "Wechat"]]}, {"text": "Xiaoyu Shen was funded by IMPRS - CS fellowship .", "entities": [[7, 8, "DatasetName", "CS"]]}, {"text": "Ernie Chang is supported by SFB 248 \u201c Foundations of Perspicuous Software Systems \u201d ( E2 ) .", "entities": []}, {"text": "References Daniel Adiwardana , Minh - Thang Luong , David R", "entities": []}, {"text": "So , Jamie Hall , Noah Fiedel , Romal Thoppilan , Zi Yang , Apoorv Kulshreshtha , Gaurav Nemade , Yifeng Lu , et al . 2020 .", "entities": []}, {"text": "Towards a human - like open - domain chatbot .", "entities": [[8, 9, "TaskName", "chatbot"]]}, {"text": "arXiv preprint arXiv:2001.09977 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Satoshi Akasaki and Nobuhiro Kaji . 2017 .", "entities": []}, {"text": "Chat detection in an intelligent assistant :", "entities": []}, {"text": "Combining taskoriented and non - task - oriented spoken dialogue systems .", "entities": [[8, 11, "TaskName", "spoken dialogue systems"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1308\u20131319 .", "entities": []}, {"text": "Tom B Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . 2020 .", "entities": []}, {"text": "Language models are few - shot learners .", "entities": []}, {"text": "arXiv preprint arXiv:2005.14165 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Harry Bunt , Jan Alexandersson , Jean Carletta , JaeWoong Choe , Alex Chengyu Fang , Koiti Hasida , Kiyong Lee , V olha Petukhova , Andrei PopescuBelis , Laurent Romary , et al . 2010 .", "entities": []}, {"text": "Towards an iso standard for dialogue act annotation .", "entities": []}, {"text": "In Proceedings of the Seventh conference on International Language Resources and Evaluation ( LREC\u201910 ) .", "entities": []}, {"text": "Harry Bunt , Jan Alexandersson , Jae - Woong Choe , Alex Chengyu Fang , Koiti Hasida , V olha Petukhova , Andrei Popescu - Belis , and David R Traum .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Iso 24617 - 2 : A semantically - based standard for dialogue annotation .", "entities": []}, {"text": "In LREC , pages 430\u2013437.Yiming Cui , Wanxiang Che , Ting Liu , Bing Qin , Ziqing Yang , Shijin Wang , and Guoping Hu . 2019 .", "entities": []}, {"text": "Pre - training with whole word masking for chinese bert .", "entities": []}, {"text": "arXiv preprint arXiv:1906.08101 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Nils Dahlb \u00a8ack , Arne J \u00a8onsson , and Lars Ahrenberg .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "Wizard of oz studies : why and how .", "entities": []}, {"text": "In Proceedings of the 1st international conference on Intelligent user interfaces , pages 193\u2013200 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Emily Dinan , Stephen Roller , Kurt Shuster , Angela Fan , Michael Auli , and Jason Weston .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Wizard of wikipedia : Knowledge - powered conversational agents .", "entities": [[0, 3, "DatasetName", "Wizard of wikipedia"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Jesse Dodge , Andreea Gane , Xiang Zhang , Antoine Bordes , Sumit Chopra , Alexander Miller , Arthur Szlam , and Jason Weston .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Evaluating prerequisite qualities for learning end - to - end dialog systems .", "entities": []}, {"text": "ICLR .", "entities": []}, {"text": "Jianfeng Gao , Michel Galley , Lihong Li , et al . 2019 .", "entities": []}, {"text": "Neural approaches to conversational ai .", "entities": []}, {"text": "Foundations and Trends R", "entities": []}, {"text": "in Information Retrieval , 13(23):127\u2013298 .", "entities": [[1, 3, "TaskName", "Information Retrieval"]]}, {"text": "Marjan Ghazvininejad , Chris Brockett , Ming - Wei Chang , Bill Dolan , Jianfeng Gao , Wen - tau Yih , and Michel Galley .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A knowledge - grounded neural conversation model .", "entities": []}, {"text": "In Thirty - Second AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Tianxing He and James Glass .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Negative training for neural dialogue response generation .", "entities": [[5, 7, "TaskName", "response generation"]]}, {"text": "arXiv preprint arXiv:1903.02134 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matthew Henderson , Blaise Thomson , and Steve Young .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Deep neural network approach for the dialog state tracking challenge .", "entities": []}, {"text": "In Proceedings of the SIGDIAL 2013 Conference , pages 467\u2013471 , Metz , France . Association for Computational Linguistics .", "entities": []}, {"text": "Ari Holtzman , Jan Buys , Li Du , Maxwell Forbes , and Yejin Choi .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "The curious case of neural text degeneration .", "entities": []}, {"text": "ICLR .", "entities": []}, {"text": "Ehsan Hosseini - Asl , Bryan McCann , Chien - Sheng Wu , Semih Yavuz , and Richard Socher .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A simple language model for task - oriented dialogue .", "entities": []}, {"text": "arXiv preprint arXiv:2005.00796 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Po - Sen Huang , Xiaodong He , Jianfeng Gao , Li Deng , Alex Acero , and Larry Heck .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Learning deep structured semantic models for web search using clickthrough data .", "entities": []}, {"text": "In Proceedings of the 22nd ACM", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "6614international conference on Information & Knowledge Management , pages 2333\u20132338 .", "entities": [[6, 7, "TaskName", "Management"]]}, {"text": "John F Kelley .", "entities": []}, {"text": "1984 .", "entities": []}, {"text": "An iterative design methodology for user - friendly natural language of\ufb01ce information applications .", "entities": []}, {"text": "ACM Transactions on Information Systems ( TOIS ) , 2(1):26\u201341 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Chlo \u00b4 e Kiddon , Luke Zettlemoyer , and Yejin Choi . 2016 .", "entities": []}, {"text": "Globally coherent text generation with neural checklist models .", "entities": [[2, 4, "TaskName", "text generation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 329\u2013339 .", "entities": []}, {"text": "Margaret Li , Stephen Roller , Ilia Kulikov , Sean Welleck , Y - Lan Boureau , Kyunghyun Cho , and Jason Weston .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Do n\u2019t say that !", "entities": []}, {"text": "making inconsistent dialogue unlikely with unlikelihood training .", "entities": []}, {"text": "arXiv preprint arXiv:1911.03860 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rongzhong Lian , Min Xie , Fan Wang , Jinhua Peng , and Hua Wu . 2019 .", "entities": []}, {"text": "Learning to select knowledge for response generation in dialog systems .", "entities": [[5, 7, "TaskName", "response generation"]]}, {"text": "In Proceedings of the 28th International Joint Conference on Arti\ufb01cial Intelligence , pages 5081\u20135087 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Chia - Wei Liu , Ryan Lowe , Iulian Serban , Mike Noseworthy , Laurent Charlin , and Joelle Pineau .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "How not to evaluate your dialogue system : An empirical study of unsupervised evaluation metrics for dialogue response generation .", "entities": [[17, 19, "TaskName", "response generation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2122\u20132132 .", "entities": []}, {"text": "Shuman Liu , Hongshen Chen , Zhaochun Ren , Yang Feng , Qun Liu , and Dawei Yin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Knowledge diffusion for neural dialogue generation .", "entities": [[4, 6, "TaskName", "dialogue generation"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1489\u20131498 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Chandler May , Alex Wang , Shikha Bordia , Samuel Bowman , and Rachel Rudinger .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "On measuring social biases in sentence encoders .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 622\u2013628 .", "entities": []}, {"text": "David Milward and Martin Beveridge .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Ontologybased dialogue systems .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "3rd Workshop on Knowledge and reasoning in practical dialogue systems ( IJCAI03 ) , pages 9\u201318 .", "entities": []}, {"text": "Nikita Moghe , Siddhartha Arora , Suman Banerjee , and Mitesh M Khapra .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Towards exploiting background knowledge for building conversation systems .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2322\u20132332.Seungwhan", "entities": []}, {"text": "Moon , Pararth Shah , Anuj Kumar , and Rajen Subba .", "entities": [[6, 7, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Opendialkg : Explainable conversational reasoning with attention - based walks over knowledge graphs .", "entities": [[0, 1, "DatasetName", "Opendialkg"], [11, 13, "TaskName", "knowledge graphs"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 845\u2013854 .", "entities": []}, {"text": "Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , et al . 2019 .", "entities": [[0, 1, "MethodName", "Adam"], [9, 10, "MethodName", "Adam"]]}, {"text": "Pytorch :", "entities": []}, {"text": "An imperative style , high - performance deep learning library .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 8024\u20138035 .", "entities": []}, {"text": "Baolin Peng , Chunyuan Li , Jinchao Li , Shahin Shayandeh , Lars Liden , and Jianfeng Gao . 2020 .", "entities": []}, {"text": "Soloist : Few - shot task - oriented dialog with a single pretrained auto - regressive model .", "entities": []}, {"text": "arXiv preprint arXiv:2005.05298 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matthew E Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In Proceedings of NAACL - HLT , pages 2227\u20132237 .", "entities": []}, {"text": "Lianhui Qin , Michel Galley , Chris Brockett , Xiaodong Liu , Xiang Gao , Bill Dolan , Yejin Choi , and Jianfeng Gao . 2019 .", "entities": []}, {"text": "Conversing by reading : Contentful neural conversation with on - demand machine reading .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5427\u20135436 .", "entities": []}, {"text": "Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "OpenAI Blog , 1(8 ) .", "entities": []}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J Liu . 2019 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the limits of transfer learning with a uni\ufb01ed text - to - text transformer .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "arXiv preprint arXiv:1910.10683 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Doyen Sahoo , Quang Pham , Jing Lu , and Steven CH Hoi . 2018 .", "entities": []}, {"text": "Online deep learning : learning deep neural networks on the \ufb02y .", "entities": [[0, 3, "MethodName", "Online deep learning"]]}, {"text": "In Proceedings of the 27th International Joint Conference on Arti\ufb01cial Intelligence , pages 2660\u20132666 .", "entities": []}, {"text": "Xiaoyu Shen , Youssef Oualil , Clayton Greenberg , Mittul Singh , and Dietrich Klakow . 2017 .", "entities": []}, {"text": "Estimation of gap between current language models and human performance .", "entities": []}, {"text": "Proc .", "entities": []}, {"text": "Interspeech 2017 , pages 553 \u2013 557 .", "entities": []}, {"text": "Xiaoyu Shen , Hui Su , Wenjie Li , and Dietrich Klakow .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Nexus network : Connecting the preceding and the following in dialogue generation .", "entities": [[10, 12, "TaskName", "dialogue generation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 4316 \u2013 4327 .", "entities": []}, {"text": "6615Xiaoyu Shen , Jun Suzuki , Kentaro Inui , Hui Su , Dietrich Klakow , and Satoshi Sekine . 2019 .", "entities": []}, {"text": "Select and attend : Towards controllable content selection in text generation .", "entities": [[9, 11, "TaskName", "text generation"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 579\u2013590 .", "entities": []}, {"text": "Hui Su , Xiaoyu Shen , Pengwei Hu , Wenjie Li , and Yun Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Dialogue generation with gan .", "entities": [[0, 2, "TaskName", "Dialogue generation"]]}, {"text": "In AAAI .", "entities": []}, {"text": "Hui Su , Xiaoyu Shen , Sanqiang Zhao , Zhou Xiao , Pengwei Hu , Randy Zhong , Cheng Niu , and Jie Zhou .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Diversifying dialogue generation with non - conversational text .", "entities": [[1, 3, "TaskName", "dialogue generation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7087\u20137097 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nikhita Vedula , Nedim Lipka , Pranav Maneriker , and Srinivasan Parthasarathy .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Open intent extraction from natural language interactions .", "entities": []}, {"text": "In Proceedings of The Web Conference 2020 , pages 2009 \u2013 2020 .", "entities": []}, {"text": "Wenquan Wu , Zhen Guo , Xiangyang Zhou , Hua Wu , Xiyuan Zhang , Rongzhong Lian , and Haifeng Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Proactive human - machine conversation with explicit conversation goal .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3794\u20133804 .", "entities": []}, {"text": "Saizheng Zhang , Emily Dinan , Jack Urbanek , Arthur Szlam , Douwe Kiela , and Jason Weston .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Personalizing dialogue agents : I have a dog , do you have pets too ?", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2204 \u2013 2213 .", "entities": []}, {"text": "Yizhe Zhang , Siqi Sun , Michel Galley , Yen - Chun Chen , Chris Brockett , Xiang Gao , Jianfeng Gao , Jingjing Liu , and Bill Dolan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Dialogpt : Large - scale generative pre - training for conversational response generation .", "entities": [[10, 13, "TaskName", "conversational response generation"]]}, {"text": "arXiv preprint arXiv:1911.00536 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hao Zhou , Tom Young , Minlie Huang , Haizhou Zhao , Jingfang Xu , and Xiaoyan Zhu . 2018a .", "entities": []}, {"text": "Commonsense knowledge aware conversation generation with graph attention .", "entities": []}, {"text": "In Proceedings of the 27th International Joint Conference on Arti\ufb01cial Intelligence , pages 4623\u20134629 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Hao Zhou , Chujie Zheng , Kaili Huang , Minlie Huang , and Xiaoyan Zhu . 2020 .", "entities": []}, {"text": "Kdconv : A chinese multi - domain dialogue dataset towards multi - turn knowledge - driven conversation .", "entities": [[0, 1, "DatasetName", "Kdconv"]]}, {"text": "ACL .", "entities": []}, {"text": "Kangyan Zhou , Shrimai Prabhumoye , and Alan W Black .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "A dataset for document grounded conversations .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 708\u2013713.Wenya Zhu , Kaixiang Mo , Yu Zhang , Zhangbin Zhu , Xuezheng Peng , and Qiang Yang .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Flexible end - to - end dialogue system for knowledge grounded conversation .", "entities": []}, {"text": "arXiv preprint arXiv:1709.04264 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "6616Online Forum Liu et al .", "entities": []}, {"text": "( 2018)A : \u6c42\u63a8\u8350\u52b1\u5fd7\u7535\u5f71\uff0c\u8c22\u8c22\u5927\u5bb6(Any inspirational movies ?", "entities": []}, {"text": "Thanks everyone ) .", "entities": []}, {"text": "B : \u300a\u5f53\u5e78\u798f\u6765\u6572\u95e8\u300b(The Pursuit of Happyness ) .", "entities": []}, {"text": "Crowd - sourced ( Constrained ) Zhou et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018b)A : Hey have you seen the inception ?", "entities": []}, {"text": "B : No , I have not but have heard of it .", "entities": []}, {"text": "What is it about ?", "entities": []}, {"text": "A : It \u2019s about extractors that perform experiments using . . .", "entities": []}, {"text": "Crowd - sourced ( unconstrained ) Zhou et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020)A : \u77e5\u9053\u91cd\u5e86\u68ee\u6797\u8fd9\u4e2a\u7535\u5f71\u5417(Do you know the movie Chungking Express ) ?", "entities": []}, {"text": "B : \u77e5\u9053\u554a\uff0c\u662f\u738b\u5bb6\u536b\u5bfc\u6f14\u7684(Yes , it \u2019s directed by Wong Kar - Wai ) .", "entities": []}, {"text": "A : \u5177\u4f53\u662f\u54ea\u5e74\u4e0a\u6620\u7684\u4f60\u8fd8\u8bb0\u5f97\u5417\uff1f(Remember which year it was on ) ?", "entities": []}, {"text": "B : \u8bb0\u5f97\uff0c\u662f\u57281994\u5e7407\u670814\u65e5(Yes , July 14th , 1994 ) . . .", "entities": []}, {"text": "OursA : \u5a01\u5c14\u53f2\u5bc6\u65af\u6f14\u6280\u771f\u7684\u5f88\u68d2(Will Smith \u2019s acting skill is really good ) .", "entities": []}, {"text": "B : \u4ed6\u7684\u5f53\u5e78\u798f\u6765\u6572\u95e8\u592a\u7ecf\u5178\u4e86(His The Pursuit of Happyness is a classic ) .", "entities": []}, {"text": "A : \u4e00\u76f4\u90fd\u6302\u5728\u7535\u5f71\u6392\u884c\u699c\u9760\u524d\u7684\u4f4d\u7f6e(That \u2019 always among top ranked movies ) .", "entities": []}, {"text": "B : \u55ef\u55ef\uff0c\u8fd9\u90e8\u7535\u5f71\u771f\u7684\u5f88\u52b1\u5fd7\u554a(Yes , it \u2019s really motivational ) .", "entities": []}, {"text": "A : \u5a01\u5c14\u53f2\u5bc6\u65af\u4e5f\u6f14\u51fa\u4e86\u5f88\u60e8\u7684\u611f\u89c9\u4e86(Will Smith plays like he is a real tragedy ) .", "entities": []}, {"text": "B : \u6f14\u6280\u7279\u522b\u597d(Yes , he acts pretty well ) .", "entities": []}, {"text": "Table 10 : Comparison between our dataset and others .", "entities": []}, {"text": "A Dataset Collection Table 10 shows examples comparing our dataset and the others .", "entities": []}, {"text": "As observed , forum conversations are mostly single - turn QA or comments .", "entities": []}, {"text": "Current crowd - sourced datasets are either collected on constrained scenarios ( the scenario in ( Zhou et", "entities": []}, {"text": "al . , 2018b ) \ufb01xed the roles in a conversation as one introducer and one listener ) , or unconstrained but prompting people to deliberately connect knowledge .", "entities": []}, {"text": "Our dataset simulates real - life conversations to the largest extent .", "entities": []}, {"text": "We classify the utterances into one of 15 aspects .", "entities": []}, {"text": "The de\ufb01nitions , counts , and examples of them are shown in Table 11 .", "entities": []}, {"text": "When annotating the corpus , tutorials and examples are provided to the annotators , we show some examples of in the following tables .", "entities": []}, {"text": "All the examples are provided only in Chinese as that is the native language among annotators .", "entities": []}, {"text": "Aspects Count(% ) Description Example \u7535\u5f71\u540d(name ) 17.41 Name of a movie .", "entities": []}, {"text": "Its name is so funny \u5bfc\u6f14(director ) 2.44 Director of a movie .", "entities": []}, {"text": "Who played Jack in the movie ?", "entities": []}, {"text": "\u6f14\u5458(actor ) 9.24 Actor of a movie .", "entities": []}, {"text": "I wonder who picked him as the leading actor .", "entities": []}, {"text": "\u7c7b\u578b(type ) 7.12 Type of a movie .", "entities": []}, {"text": "I do n\u2019t really like horror movies .", "entities": []}, {"text": "\u89d2\u8272(role ) 10.01 Role of a movie .", "entities": []}, {"text": "I like Bumblebee , he is cute .", "entities": []}, {"text": "\u533a\u57df(region ) 2.95 Region of a movie .", "entities": []}, {"text": "Is it a Japanese movie ?", "entities": []}, {"text": "\u4e0a\u6620\u65f6\u95f4(time ) 2.02 Release time of a movie .", "entities": []}, {"text": "I remember it was in 1997 , still at high school .", "entities": []}, {"text": "\u5267\u60c5(plot ) 24.30 Plot of a movie .", "entities": []}, {"text": "What happened to him ?", "entities": []}, {"text": "\u53f0\u8bcd(line ) 1.51 Line of a movie .", "entities": []}, {"text": "That line becomes a popular word that year .", "entities": []}, {"text": "\u5956\u9879(awards )", "entities": []}, {"text": "1.17 Award of a movie .", "entities": []}, {"text": "It won a lot of awards that year .", "entities": []}, {"text": "\u7968\u623f(gross ) 1.00 Gross of a movie .", "entities": []}, {"text": "I doubt if its gross will be satisfying .", "entities": []}, {"text": "\u8bc4\u5206(rating ) 4.04 Rating of a movie .", "entities": []}, {"text": "It \u2019s clearly the best mivie I \u2019ve watched .", "entities": []}, {"text": "\u8d44\u6e90(website ) 1.25 Website of a movie .", "entities": []}, {"text": "You can watch it on iqiyi.com !", "entities": []}, {"text": "\u97f3\u4e50(music ) 2.19 Music in the movie .", "entities": []}, {"text": "Its theme song is \u201c my heart will go on \u201d .", "entities": []}, {"text": "\u5176\u4ed6(other ) 12.31 Other aspects .", "entities": []}, {"text": "I like her costume so much !", "entities": []}, {"text": "Table 11 : Counts , descriptions and examples of the aspects .", "entities": []}, {"text": "Examples are shown only in translated English for space limit .", "entities": []}, {"text": "B Human Evaluation As for the four human evaluation metrics .", "entities": []}, {"text": "The \ufb01rst two will focus only on the conversational backbones without considering domain knowledge .", "entities": []}, {"text": "The second two will check if the responses can provide informative and correct responses powered by domain knowledge .", "entities": []}, {"text": "The detailed de\ufb01nitions of them are :", "entities": []}, {"text": "6617A : \u4e00\u76f4\u6709\u542c\u5230\u300a\u5343\u4e0e\u5343\u5bfb\u300b\uff0c\u5374\u672a\u66fe\u771f\u7684\u770b\u8fc7\u8fd9\u90e8\u7535\u5f71\u544a\u77e5\u4e8b\u5b9e\u7c7b\u578b;\u544a\u77e5\u611f\u53d7\u5176\u4ed6 B :", "entities": []}, {"text": "\u600e\u4e48\u4e0d\u53bb\u770b \u5176\u4ed6 A : \u8bdd\u8bf4\u4f60\u770b\u4e86\u5417 \u5176\u4ed6 B : \u770b\u4e86\uff0c\u633a\u611f\u52a8\u7684 \u544a\u77e5\u611f\u53d7\u5176\u4ed6 A : \u4e3b\u8981\u8bb2\u4ec0\u4e48\u7684 \u8bf7\u6c42\u4e8b\u5b9e\u5267\u60c5 B : \u4e00\u4e2a\u5c0f\u5973\u5b69\uff0c\u5343\u5bfb\uff0c\u4e0d\u754f\u8270\u96be\u6551\u4e86\u81ea\u5df1\u7684\u7236\u6bcd\u548c\u670b\u53cb\u544a\u77e5\u4e8b\u5b9e\u89d2\u8272;\u544a\u77e5\u4e8b\u5b9e\u5267\u60c5", "entities": []}, {"text": "A : \u6d77\u62a5\u91cc\u7684\u5c0f\u5973\u5b69\u770b\u8d77\u6765\u8fd8\u5f88\u5c0f \u544a\u77e5\u4e8b\u5b9e\u89d2\u8272 B : \u6240\u4ee5\u554a\uff0c\u89c9\u5f88\u5979\u5f88\u52c7\u6562 \u544a\u77e5\u611f\u53d7\u5176\u4ed6 A : \u542c\u4e0a\u53bb\u597d\u50cf\u8fd8\u633a\u597d\u770b\u7684 \u544a\u77e5\u611f\u53d7\u8bc4\u5206 B : \u662f\u554a\uff0c\u4f60\u5feb\u53bb\u770b\u770b\u5427 \u5176\u4ed6 A : \u53d1\u73b0\u4e00\u90e8\u6709\u70b9\u5e74\u4ee3\u611f\u7684\u7535\u5f71 \u544a\u77e5\u611f\u53d7\u5176\u4ed6 B : \u4f60\u8bf4\u4ec0\u4e48\u7c7b\u578b\u7684 \u8bf7\u6c42\u4e8b\u5b9e\u7c7b\u578b", "entities": []}, {"text": "A : \u7279\u522b\u53cd\u6620\u73b0\u5728\u7684\u95ee\u9898\u5973\u4eba\u7684\u5730\u4f4d\u544a\u77e5\u4e8b\u5b9e\u5267\u60c5 B : \u5565\u7535\u5f71 \u8bf7\u6c42\u4e8b\u5b9e\u7535\u5f71\u540d A : \u83ca\u8c46\uff0c\u5de9\u4fd0\u6f14\u7684 \u544a\u77e5\u4e8b\u5b9e\u7535\u5f71\u540d\u544a\u77e5\u4e8b\u5b9e\u6f14\u5458\u540d B : \u5367\u69fd\uff0c\u5979\u554a\u5b9e\u529b\u6d3e\u554a\u81ea\u5e26\u6c14\u8d28\u771f\u7684\u544a\u77e5\u611f\u53d7\u6f14\u5458\u540d", "entities": []}, {"text": "A : \u5bf9\u554a\u6211\u624d\u770b\u5b8c \u5176\u4ed6 B : \u600e\u4e48\u6837 \u8bf7\u6c42\u611f\u53d7\u5267\u60c5 A : \u6709\u70b9\u70b9\u538b\u6291\uff0c\u8fd8\u662f\u89c9\u5f97\u6211\u4eec\u73b0\u5728\u7684\u751f\u6d3b\u5e78\u798f\u544a\u77e5\u611f\u53d7\u5267\u60c5 B : \u54c8\u54c8\u54c8\u6211\u4e5f\u53bb\u770b\u770b \u5176\u4ed6 A : \u300a\u8681\u4eba\u300b\u4f60\u770b\u4e86\u6ca1\u6709 \u544a\u77e5\u4e8b\u5b9e\u7535\u5f71\u540d B : \u8681\u4eba\u7edd\u5bf9\u662f\u6700\u63a5\u5730\u6c14\u7684\u8d85\u7ea7\u82f1\u96c4 \uff01 \u544a\u77e5\u4e8b\u5b9e\u89d2\u8272", "entities": []}, {"text": "A : \u4ece\u5c0f\u5077\u53d8\u82f1\u96c4\uff08\u56e0\u4e3a\u662f\u6ca1\u6709\u8d85\u80fd\u529b\u5417 \uff1f \uff09 \u544a\u77e5\u4e8b\u5b9e\u5176\u4ed6 B : \u597d\u770b\u6b7b\u4e86\uff01\uff01\u6bcf\u5929\u70b8\u7ebd\u7ea6\u70b8\u9999\u6e2f\u70b8\u6d1b\u6749\u77f6\u90fd\u770b\u9189\u4e86\uff01\u544a\u77e5\u611f\u53d7\u8bc4\u5206 A : \u5728\u73a9\u5177\u623f\u91cc\u7684\u6253\u6597\u620f\u592a\u6709\u610f\u601d\u4e86 \uff01 \u544a\u77e5\u4e8b\u5b9e\u5267\u60c5;\u544a\u77e5\u611f\u53d7\u8bc4\u5206 B : \u7b11\u70b9\u4e5f\u633a\u591a\u7684\uff0c\u770b\u5f97\u6211\u5f88\u5f00\u5fc3 \u3002 \u544a\u77e5\u611f\u53d7\u8bc4\u5206 A : Paul\u4e0d\u662f\u6700\u5e05\u7684\u4f46\u7edd\u5bf9\u662f\u6700\u6709\u5473\u9053\u7684 \uff01 \u544a\u77e5\u4e8b\u5b9e\u89d2\u8272 B : \u5c31\u662f\u6700\u540e\u5341\u79d2\u949f\u53d8\u661f\u9645\u7a7f\u8d8a!\u771f\u5fc3\u5410\u69fd \u544a\u77e5\u4e8b\u5b9e\u5267\u60c5 A : \u5404\u79cd\u5b98\u65b9\u81ea\u5e26\u5410\u69fd\u548c\u81ea\u9ed1\u6076\u641e\u771f\u662f\u5165\u620f\u4e0d\u8fc7\u4e09\u79d2\u544a\u77e5\u611f\u53d7\u5176\u4ed6 B : \u54c8\u54c8\u54c8\uff0c\u786e\u5b9e\u662f", "entities": []}, {"text": "\u5176\u4ed6", "entities": []}, {"text": "A : \u4f60\u6709\u6ca1\u6709\u770b\u8fc7\u591a\u5566A\u68a6\u7684\u7535\u5f71\u5417 \u544a \u77e5\u4e8b\u5b9e\u7535\u5f71\u540d B : \u4f60\u8bf4\u7684\u662f\u54ea\u4e00\u90e8 \uff1f \uff01 \u8bf7\u6c42\u4e8b\u5b9e\u7535\u5f71\u540d A : \u5927\u96c4\u7684\u6708\u7403\u63a2\u9669\u8bb0 \u544a\u77e5\u4e8b\u5b9e\u7535\u5f71\u540d B : \u5bf9\u554a\uff0c\u591a\u5566A\u68a6\u5c0f\u65f6\u5019\u6700\u559c\u6b22\u4e86\uff0c\u53e3\u888b\u91cc\u9762\u4ec0\u4e48\u90fd\u6709\u544a\u77e5\u4e8b\u5b9e\u5267\u60c5 A : \u54c8\u54c8\u5bf9\uff0c\u8fd9\u91cc\u9762\u4ed6\u4eec\u53bb\u6708\u7403\u627e\u7389\u5154\uff0c\u4f5c\u8005\u60f3\u8c61\u529b\u5f88\u4e30\u5bcc\uff01\u544a\u77e5\u4e8b\u5b9e\u5267\u60c5 B : \u540e\u534a\u90e8\u5206\u5176\u5b9e\u6709\u70b9\u878d\u6897\u300a\u661f\u7403\u5927\u6218\u300b+\u300a\u963f\u4e3d\u5854\u300b\u544a\u77e5\u4e8b\u5b9e\u7535\u5f71\u540d A : \u4f46\u4e5f\u6ca1\u6709\u770b\u51fa\u6765\u6709\u4ec0\u4e48\u8fdd\u548c\u70b9\u554a \u544a\u77e5\u611f\u53d7\u8bc4\u5206 B : \u6069\u5462\uff0c\u90a3\u5012\u662f\u4e00\u76f4\u662f\u840c\u7269\u4e0e\u79d1\u666e\uff0c\u4ee5\u53ca\u4e0d\u53d8\u7684\u53cb\u60c5\u3002\u544a\u77e5\u611f\u53d7\u5267\u60c5 A : \u6700\u540e\u80d6\u864e\u628a\u90a3\u4e2a\u4e1c\u897f\u9001\u7ed9\u90a3\u4e2a\u5c0f\u5b69\u7684\u65f6\u5019\u5c31\u54ed\u4e86\u544a\u77e5\u4e8b\u5b9e\u5267\u60c5 B : \u4f60\u4e5f\u4e00\u6837\u561b \uff1f", "entities": []}, {"text": "\u5176\u4ed6 A : \u5bf9\u554a\u5bf9\u554a\uff0c\u5c31\u662f\u84dd\u80d6\u5b50\u7684\u53e3\u888b\u4e1c\u897f\u592a\u591a\u4e86\u5427 \u544a \u77e5\u4e8b\u5b9e\u5267\u60c5 B : \u54c8\u54c8\u5f53\u7136\u5168\u90fd\u9760\u9053\u5177 \u544a\u77e5\u4e8b\u5b9e\u5267\u60c5", "entities": []}, {"text": "66181.Sensibleness : If the response makes sense given the dialogue context .", "entities": []}, {"text": "Speci\ufb01c domain knowledge is ignored .", "entities": []}, {"text": "For example , if the user asks \u201c Who directed the movie Inception ? \u201d , the bot replies \u201c Steven Spielberg !", "entities": []}, {"text": "I like him so much ! ! \u201d", "entities": []}, {"text": "The answer is wrong but still considered sensible given the context .", "entities": []}, {"text": "2.Engagement : If the response is engaging for the user to follow .", "entities": []}, {"text": "The de\ufb01nition is generally similar to diversity , or interestingness metric used for open - domain chitchat .", "entities": [[9, 10, "DatasetName", "interestingness"]]}, {"text": "If the bot simply passively replies messages like \u201c That \u2019s good \u201d or \u201c No idea \u201d , it will be marked as non - engaging .", "entities": []}, {"text": "3.Factuality : Factuality checks the information correctness in case the model fabricate wrong facts .", "entities": []}, {"text": "This is orthogonal to the above metrics .", "entities": []}, {"text": "The response can be factual but not sensible or the other way around .", "entities": []}, {"text": "The same for Engagement .", "entities": []}, {"text": "4.Informativeness :", "entities": []}, {"text": "Informativeness checks if the response contains new information about the movie .", "entities": []}, {"text": "If the bot replies sth like \u201c That \u2019s so interesting , I \u2019ve never heard of that ! \u201d .", "entities": []}, {"text": "It is considered speci\ufb01c yet non - informative , because the bot provides no new information .", "entities": []}, {"text": "Intuitively a good closed - domain chatbot should balance well all these skills .", "entities": [[6, 7, "TaskName", "chatbot"]]}, {"text": "Sensibleness plus engagement can evaluate its general performance as a chatbot .", "entities": [[10, 11, "TaskName", "chatbot"]]}, {"text": "Factuality and informativeness can evaluate its capability at actively injecting appropriate domain knowledge into the dialogue .", "entities": []}, {"text": "All the three metrics are evaluated by three crow - workers each except for factuality .", "entities": []}, {"text": "As evaluating factuality requires in - depth knowledge about one movie , it is quite dif\ufb01cult for random human evaluators to judge them .", "entities": []}, {"text": "Even if we \ufb01lter to only keep people who have watched a movie , it is hard to guarantee they can recall all the scenes in the movie .", "entities": []}, {"text": "Therefore , the factuality check is only done by the person who performed this dialogue .", "entities": []}, {"text": "In the static evaluation , it is evaluated by the annotator who produced the reference response .", "entities": []}, {"text": "In the interactive evaluation , it is evaluated by the person who chat with the bot .", "entities": []}, {"text": "It is nevertheless not accurate though .", "entities": []}, {"text": "However , if the bot can cheat the human into believing its false information , it can also somehow be considered a \u201c success \u201d .", "entities": []}, {"text": "We provide examples for guiding the human evaluators in Table 12 .", "entities": []}, {"text": "Table 13 shows some interactive examples with humans .", "entities": []}, {"text": "We observe Mitsuku can XiaoIce perform decently in single - turn exchanges but strongly struggle at understanding multi - turn user intents .", "entities": []}, {"text": "Most conversations stop at turn 4 and will not move on .", "entities": []}, {"text": "Conversation Score Input : Do you know who directed the movie Inception ?", "entities": [[1, 2, "MetricName", "Score"]]}, {"text": "Sensibleness = 1 ; Engagement = 1 Bot : I know !", "entities": []}, {"text": "It was Christopher Nolan !", "entities": []}, {"text": "I like him so much !", "entities": []}, {"text": "Factuality = 1 , Informativeness = 1 Input : Do you know who directed the movie Inception ?", "entities": []}, {"text": "Sensibleness = 0 ; Engagement = 1 Bot : The movie is amazing to imagine about a dream within a dream .", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "Factuality = 1 , Informativeness = 1 Input : Do you know who directed the movie Inception ?", "entities": []}, {"text": "Sensibleness = 1 ; Engagement = 0 Bot : Christopher Nolan .", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "Factuality = 1 , Informativeness = 1 Input : Do you know who directed the movie Inception ?", "entities": []}, {"text": "Sensibleness = 1 ; Engagement = 1 Bot : I know !", "entities": []}, {"text": "It was Steven Spielberg !", "entities": []}, {"text": "I like him so much !", "entities": []}, {"text": "Factuality = 0 , Informativeness = 1 Input : Do you know who directed the movie Inception ?", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "Sensibleness = 1 ; Engagement = 1 Bot : I do n\u2019t remember .", "entities": []}, {"text": "Do you like it ?", "entities": []}, {"text": "Factuality = 1 , Informativeness = 0", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "Input : Do you know who directed the movie Inception ?", "entities": []}, {"text": "Sensibleness = 1 ; Engagement = 0 Bot : Steven Spielberg .", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "Factuality = 0 , Informativeness = 1 Table 12 : Examples for human evaluation scores .", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "6619Mistsuku XiaoIce Ours User :", "entities": []}, {"text": "Do you know who directed the movie Inception?User : \u4f60\u559c\u6b22\u770b\u611f\u4eba\u7684\u7535\u5f71\u5417 User : \u4f60\u559c\u6b22\u770b\u611f\u4eba\u7684\u7535\u5f71\u5417 Bot : No idea .", "entities": []}, {"text": "I will guess at Steven Spielberg .", "entities": []}, {"text": "Bot : \u6211\u559c\u6b22\u66f4\u52a0\u53e6\u7c7b\u4e00\u4e9b\u7684 Bot : \u559c\u6b22\u554a\uff0c\u4f60\u6709\u4ec0\u4e48\u63a8\u8350\u5417 User : Maybe , it does look like his style .", "entities": []}, {"text": "User : \u6bd4\u5982\u5462 User : \u6211\u4e0d\u662f\u836f\u795e\u5f88\u611f\u4eba Bot :", "entities": []}, {"text": "Interesting comparison .", "entities": []}, {"text": "Bot : \u6bd4\u5982 \uff1f Bot : \u786e\u5b9e\u662f\u4e00\u90e8\u5f88\u826f\u5fc3\u7684\u7535\u5f71 User : have you watched any movies recently?User : \u6cf0\u5766\u5c3c\u514b\u53f7\u4e3b\u89d2\u662f\u8c01 User : \u786e\u5b9e\uff0c\u5f90\u5ce5\u6f14\u7684\u4e5f\u597d Bot : My favourite movie is Terminator .", "entities": []}, {"text": "Have you seen it?Bot : \u8fd9\u6211\u8fd8\u771f\u77e5\u9053\uff0c\u83b1\u6602\u7eb3\u591a\u0001\u8fea \u5361\u666e\u91cc\u5965\u4e3b\u6f14\u7684Bot : \u5f90\u5ce5\u7684\u6f14\u7ece\u4e00\u76f4\u5f88\u8d70\u5fc3 User : not really , what is it about ?", "entities": []}, {"text": "User : \u90a3\u662f\u8c01\u5bfc\u6f14\u4e86\u5462 User : \u4e0d\u8fc7\u6211\u6700\u559c\u6b22\u7684\u8fd8\u662f\u91cc\u9762 \u7684\u5c0f\u9ec4\u6bdb Bot : If not that , what ?", "entities": []}, {"text": "Bot : \u4f60\u60f3\u8ba9\u6211\u8bf4\u4ec0\u4e48 \uff1f", "entities": []}, {"text": "Bot : \u5c0f\u9ec4\u6bdb\u597d\u96be\u554a\uff0c\u592a\u4e0d\u5bb9\u6613\u4e86 User : I like the Avengers series so muchUser : \u4f60\u770b\u8fc7\u5927\u8bdd\u897f\u6e38\u5417 User : \u6211\u770b\u7535\u5f71\u7684\u65f6\u5019\u4ed6\u6b7b\u90a3\u6bb5 \u7ed9\u6211\u54ed\u5b8c\u4e86 Bot :", "entities": []}, {"text": "Yes .", "entities": []}, {"text": "My favourite is Iron Man , as he \u2019s a kind of robot .", "entities": []}, {"text": "You like Avengers .", "entities": []}, {"text": "Bot : \u4f60\u53ef\u4ee5\u518d\u8bf4\u4e00\u904d\u5417 Bot : \u770b\u5b8c\u4f60\u6709\u4ec0\u4e48\u611f\u60f3\u5462 User : Exactly !", "entities": []}, {"text": "Ca n\u2019t accept he died in the end , sighUser : \u6700\u5927\u7684\u75c5\u5c31\u662f\u7a77\u75c5\u554a Bot : You and I are on the same wavelength .", "entities": []}, {"text": "I often wonder if it can .", "entities": []}, {"text": "Bot : \u786e\u5b9e\u662f\u8fd9\u6837\uff0c\u5e0c\u671b\u6211\u4eec\u80fd\u5e73 \u5b89", "entities": []}, {"text": "User : \u5e73\u5b89\u662f\u6700\u5927\u7684\u5e78\u798f\u4e86 Bot : \u6211\u89c9\u5f97\u8fd8\u884c Table 13 : Sampled conversations from the interactive evaluation .", "entities": []}]