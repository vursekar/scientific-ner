[{"text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics , pages 2753\u20132759 April 19 - 23 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics2753Extremely Small BERT Models from Mixed - Vocabulary Training Sanqiang Zhao * University of Pittsburgh , PA sanqiang.zhao@pitt.eduRaghav Gupta * Google Research , Mountain View , CA raghavgupta@google.com", "entities": [[7, 8, "MethodName", "BERT"], [25, 26, "DatasetName", "Google"]]}, {"text": "Yang Song Kuaishou Technology , Beijing , China yangsong@kuaishou.comDenny Zhou Google Brain , Mountain View , CA dennyzhou@google.com", "entities": [[10, 11, "DatasetName", "Google"]]}, {"text": "Abstract Pretrained language models like BERT have achieved good results on NLP tasks , but are impractical on resource - limited devices due to memory footprint .", "entities": [[1, 4, "TaskName", "Pretrained language models"], [5, 6, "MethodName", "BERT"]]}, {"text": "A large fraction of this footprint comes from the input embeddings with large input vocabulary and embedding dimensions .", "entities": []}, {"text": "Existing knowledge distillation methods used for model compression can not be directly applied to train student models with reduced vocabulary sizes .", "entities": [[1, 3, "MethodName", "knowledge distillation"], [6, 8, "TaskName", "model compression"]]}, {"text": "To this end , we propose a distillation method to align the teacher and student embeddings via mixed - vocabulary training .", "entities": []}, {"text": "Our method compresses BERT LARGE to a task - agnostic model with smaller vocabulary and hidden dimensions , which is an order of magnitude smaller than other distilled BERT models and offers a better size - accuracy trade - off on language understanding benchmarks as well as a practical dialogue task .", "entities": [[3, 4, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"], [36, 37, "MetricName", "accuracy"]]}, {"text": "1 Introduction Recently , pre - trained context - aware language models like ELMo ( Peters et al . , 2018 ) , GPT ( Radford et al . , 2019 ) , BERT ( Devlin et al . , 2018 ) and XLNet ( Yang et al . , 2019 ) have outperformed traditional word embedding models like Word2Vec ( Mikolov et al . , 2013 ) and GloVe ( Pennington et al . , 2014 ) , and achieved strong results on a number of language understanding tasks .", "entities": [[13, 14, "MethodName", "ELMo"], [23, 24, "MethodName", "GPT"], [33, 34, "MethodName", "BERT"], [43, 44, "MethodName", "XLNet"], [69, 70, "MethodName", "GloVe"]]}, {"text": "However , these models are typically too huge to host on mobile / edge devices , especially for real - time inference .", "entities": []}, {"text": "Recent work has explored , inter alia , knowledge distillation ( Ba and Caruana , 2014 ; Hinton et al . , 2015 ) to train small - footprint student models by implicit transfer of knowledge from a teacher model .", "entities": [[8, 10, "MethodName", "knowledge distillation"]]}, {"text": "Most distillation methods , however , need the student and teacher output spaces to be aligned .", "entities": []}, {"text": "This complicates task - agnostic distillation of BERT to Asterisk ( * ) denotes equal contribution .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "Research conducted when all authors were at Google.smaller - vocabulary student BERT models since the input vocabulary is also the output space for the masked language modeling ( MLM ) task used in BERT .", "entities": [[11, 12, "MethodName", "BERT"], [24, 27, "TaskName", "masked language modeling"], [28, 29, "DatasetName", "MLM"], [33, 34, "MethodName", "BERT"]]}, {"text": "This in turn limits these distillation methods \u2019 ability to compress the input embedding matrix , that makes up a major proportion of model parameters e.g. the \u001830 K input WordPiece embeddings of the BERT BASE model make up over 21 % of the model size .", "entities": [[30, 31, "MethodName", "WordPiece"], [34, 35, "MethodName", "BERT"], [35, 36, "MethodName", "BASE"]]}, {"text": "This proportion is even higher for most distilled BERT models , owing to these distilled models typically having fewer layers than their teacher BERT counterparts .", "entities": [[8, 9, "MethodName", "BERT"], [23, 24, "MethodName", "BERT"]]}, {"text": "We present a task and model - agnostic distillation approach for training small , reduced - vocabulary BERT models running into a few megabytes .", "entities": [[17, 18, "MethodName", "BERT"]]}, {"text": "In our setup , the teacher and student models have incompatible vocabularies and tokenizations for the same sequence .", "entities": []}, {"text": "We therefore align the student and teacher WordPiece embeddings by training the teacher on the MLM task with a mix of teacher - tokenized and student - tokenized words in a sequence , and then using these student embeddings to train smaller student models .", "entities": [[7, 8, "MethodName", "WordPiece"], [15, 16, "DatasetName", "MLM"]]}, {"text": "Using our method , we train compact 6 and 12 - layer reducedvocabulary student models which achieve competitive performance in addition to high compression for benchmark datasets as well as a real - world application in language understanding for dialogue .", "entities": []}, {"text": "2 Related Work Work in NLP model compression falls broadly into four classes : matrix approximation , weight quantization , pruning / sharing , and knowledge distillation .", "entities": [[6, 8, "TaskName", "model compression"], [18, 19, "TaskName", "quantization"], [25, 27, "MethodName", "knowledge distillation"]]}, {"text": "The former two seek to map model parameters to low - rank approximations ( Tulloch and Jia , 2017 ) and lower - precision integers/\ufb02oats ( Chen et al . , 2015 ; Zhou et", "entities": []}, {"text": "al . , 2018 ; Shen et al . , 2019 ) respectively", "entities": []}, {"text": ".", "entities": []}, {"text": "In contrast , pruning aims to remove / share redundant model weights ( Li et al . , 2016 ; Lan et al . , 2019 ) .", "entities": []}, {"text": "More recently , dropout ( Srivastava et al . , 2014 ) has been used to cut inference latency by", "entities": []}, {"text": "2754 Figure 1 : Depiction of our mixed - vocabulary training approach .", "entities": []}, {"text": "( Left ) Stage I involving retrained teacher BERT with default con\ufb01g ( e.g. , 30 K vocabulary , 768 hidden dim ) and mixed - vocabulary input .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "( Right ) Stage II involving student model with smaller vocabulary ( 5 K ) and hidden dims ( e.g. , 256 ) and embeddings initialized from stage I. early exit ( Fan et al . , 2019 ; Xin et al . , 2020 ) .", "entities": []}, {"text": "Knowledge distillation focuses on implicit transfer of knowledge as soft teacher predictions ( Tang et al . , 2019 ) , attention distributions ( Zagoruyko and Komodakis , 2016 ) and intermediate outputs ( Romero et al . , 2014 ) .", "entities": [[0, 2, "MethodName", "Knowledge distillation"]]}, {"text": "Approaches close to our work rely on similar methods ( Sanh et al . , 2019 ; Sun et al . , 2019 ) , while others involve combinations of layer - wise transfer ( Sun et al . , 2020 ) , taskspeci\ufb01c distillation ( Jiao et al . , 2019 ) , architecture search ( Chen et al . , 2020 ) and layer dropout ( Xu et", "entities": []}, {"text": "al . , 2020 ) ; many of these are speci\ufb01c to the transformer layer ( Vaswani et al . , 2017 ) .", "entities": []}, {"text": "Another highly relevant line of work focuses on reducing the size of the embedding matrix , either via factorization ( Shu and Nakayama , 2018 ; Lan et al . , 2019 ) or vocabulary selection / pruning ( Provilkov et al . , 2019 ; Chen et al . , 2019b ) .", "entities": []}, {"text": "3 Proposed Approach Here , we discuss our rationale behind reducing the student vocabulary size and its challenges , followed by our mixed - vocabulary distillation approach .", "entities": []}, {"text": "3.1 Student Vocabulary WordPiece ( WP ) tokens ( Wu et al . , 2016 ) are subword units obtained by applying greedy segmentation to a training corpus .", "entities": [[3, 4, "MethodName", "WordPiece"]]}, {"text": "Given such a corpus and a number of desired tokens D , a WordPiece vocabulary is generated by selecting Dsubword tokens such that the resulting corpus is minimal in the number of WordPiece when segmented according to the chosen WordPiece model .", "entities": [[13, 14, "MethodName", "WordPiece"], [32, 33, "MethodName", "WordPiece"], [39, 40, "MethodName", "WordPiece"]]}, {"text": "The greedy algorithm for this optimization problem is described in more detail in Sennrich et al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Most publishedBERT models use a vocabulary of 30522 WordPieces , obtained by running the above algorithm on the Wikipedia and BooksCorpus ( Zhu et al . , 2015 ) corpora with a desired vocabulary size Dof 30000 .", "entities": []}, {"text": "For our student model , we chose a target vocabulary sizeDof 5000 WordPiece tokens .", "entities": [[12, 13, "MethodName", "WordPiece"]]}, {"text": "Using the same WordPiece vocabulary generation algorithm and corpus as above , we obtain a 4928 - WordPiece vocabulary for the student model .", "entities": [[3, 4, "MethodName", "WordPiece"], [17, 18, "MethodName", "WordPiece"]]}, {"text": "This student vocabulary includes all ASCII characters as separate tokens , ensuring no out - of - vocabulary words upon tokenization with this vocabulary .", "entities": []}, {"text": "Additionally , the 30 K teacher BERT vocabulary includes 93:9%of the WP tokens in this 5 K student vocabulary but does not subsume it .", "entities": [[6, 7, "MethodName", "BERT"], [22, 23, "DatasetName", "subsume"]]}, {"text": "We explore other strategies to obtain a small student vocabulary in Section 6 .", "entities": []}, {"text": "For task - agnostic student models , we reuse BERT \u2019s masked language modeling ( MLM ) task : words in context are randomly masked and predicted given the context via softmax over the model \u2019s WP vocabulary .", "entities": [[9, 10, "MethodName", "BERT"], [11, 14, "TaskName", "masked language modeling"], [15, 16, "DatasetName", "MLM"], [19, 22, "DatasetName", "words in context"], [31, 32, "MethodName", "softmax"]]}, {"text": "Thus , the output spaces for our teacher ( 30 K ) and student ( 5 K ) models are unaligned .", "entities": []}, {"text": "This , coupled with both vocabularies tokenizing the same words differently , means existing distillation methods do not apply to our setting .", "entities": []}, {"text": "3.2 Mixed - vocabulary training We propose a two - stage approach for implicit transfer of knowledge to the student via the student embeddings , as described below .", "entities": []}, {"text": "Stage I ( Student Embedding Initialization ): We \ufb01rst train the student embeddings with the teacher model initialized from BERT LARGE .", "entities": [[19, 20, "MethodName", "BERT"]]}, {"text": "For a given input sequence , we mix the vocabularies by randomly", "entities": []}, {"text": "2755selecting ( with probability pSV , a hyperparameter ) words from the sequence to segment using the student vocabulary , with the other words segmented using the teacher vocabulary .", "entities": []}, {"text": "As in Figure 1 on the left , for input [ \u2018 I \u2019 , \u2018 like \u2019 , \u2018 machine \u2019 , \u2018 learning \u2019 ] , the words \u2018 like \u2019 and\u2018learning \u2019 are segmented using the student vocabulary ( in blue ) , with the others using the teacher vocabulary ( in green ) .", "entities": []}, {"text": "Similar to Lample and Conneau ( 2019 ) , this step seeks to align the student and teacher embeddings for the same tokens : the model learns to predict student tokens using context which is segmented using the teacher vocabulary , and vice versa .", "entities": []}, {"text": "Note that since the student embeddings are set to a lower dimension than the teacher embeddings , as they are meant to be used in the smaller student model , we project the student embeddings up to the teacher embedding dimension using a trainable af\ufb01ne layer before these are input to the teacher BERT .", "entities": [[39, 41, "HyperparameterName", "embedding dimension"], [53, 54, "MethodName", "BERT"]]}, {"text": "We choose to keep the two embedding matrices separate despite the high token overlap : this is partly to keep our approach robust to lower vocabulary overlap settings , and partly due to empirical considerations described in Section 6 .", "entities": []}, {"text": "Let\u0012s = ebsand\u0012t = ebtdenote the transformer layer and embedding weights for the student and teacher models respectively .", "entities": []}, {"text": "The loss de\ufb01ned in Equation 1 is the MLM cross entropy summed over masked positions Mtin the teacher input .", "entities": [[1, 2, "MetricName", "loss"], [8, 9, "DatasetName", "MLM"]]}, {"text": "yiand cidenote the predicted and true tokens at position irespectively and can belong to either vocabulary .", "entities": []}, {"text": "vi2fs;tgdenotes the vocabulary used to segment this token .", "entities": []}, {"text": "Separate softmax layers Pviare used for token prediction , one for each vocabulary , depending on the segmenting vocabulary vifor token i.", "entities": [[1, 2, "MethodName", "softmax"]]}, {"text": "All teacher parameters ( \u0012t , ebt ) and student embeddings ( ebs ) are updated in this step .", "entities": []}, {"text": "Ls1=\u0000P i2Mt(logPvi(yi = cij\u0012t;ebs;ebt))(1 ) Stage II ( Student Model Layers ):", "entities": []}, {"text": "With student embeddings initialized in stage I , we now train the student model normally i.e. , using only the student vocabulary and discarding the teacher model .", "entities": []}, {"text": "Equation 2 shows the student MLM loss where Msis the set of positions masked in the student input .", "entities": [[5, 6, "DatasetName", "MLM"], [6, 7, "MetricName", "loss"]]}, {"text": "All student model parameters ( \u0012s , ebs ) are updated .", "entities": []}, {"text": "Ls2=\u0000P i2MslogPs(yi = cij\u0012s;ebs))(2 ) 4 Experiments For evaluation , we \ufb01netune the student model just as one would \ufb01netune the original BERT modeli.e . , without using the teacher model or any taskspeci\ufb01c distillation .", "entities": [[22, 23, "MethodName", "BERT"]]}, {"text": "We describe our experiments below , with dataset details left to the appendix .", "entities": []}, {"text": "4.1 Evaluation Tasks and Datasets We \ufb01ne - tune and evaluate the distilled student models on two classes of language understanding tasks : GLUE benchmark ( Wang et al . , 2019 ) : We pick three classi\ufb01cation tasks from GLUE :", "entities": [[23, 24, "DatasetName", "GLUE"], [40, 41, "DatasetName", "GLUE"]]}, {"text": "\u000fMRPC : Microsoft Research Paraphrase Corpus ( Dolan and Brockett , 2005 ) , a 2 - way sentence pair classi\ufb01cation task with 3.7 K train instances .", "entities": []}, {"text": "\u000fMNLI : Multi - Genre Natural Language Inference ( Williams et al . , 2018 ) , a 3 - way sentence pair classi\ufb01cation task with 393 K training instances .", "entities": [[5, 8, "TaskName", "Natural Language Inference"]]}, {"text": "\u000fSST-2 : Stanford Sentiment Treebank ( Socher et al . , 2013 ) , a 2 - way sentence classi\ufb01cation task with 67 K training instances .", "entities": []}, {"text": "Spoken Language Understanding : Since we are also keen on edge device applications , we also evaluate on spoken language understanding , a practical task in dialogue systems .", "entities": [[0, 3, "TaskName", "Spoken Language Understanding"], [18, 21, "TaskName", "spoken language understanding"]]}, {"text": "We use the SNIPS dataset ( Coucke et al . , 2018 ) of \u001814 K virtual assistant queries , each comprising one of 7 intents and values for one or more of the 39 pre - de\ufb01ned slots .", "entities": [[3, 4, "DatasetName", "SNIPS"]]}, {"text": "The intent detection and slot \ufb01lling subtasks are modeled respectively as 7 - way sentence classi\ufb01cation and sequence tagging with IOB slot labels .", "entities": [[1, 3, "TaskName", "intent detection"]]}, {"text": "4.2 Models and Baselines For GLUE , we train student models with 6 and 12 layers , 4 attention heads , and embedding / hidden dimensions \ufb01xed to 256 , each using a compact 5KWP vocabulary .", "entities": [[5, 6, "DatasetName", "GLUE"]]}, {"text": "We also evaluate baselines without knowledge distillation ( NoKD ) , parameterized identically to the distilled student models ( incl .", "entities": [[5, 7, "MethodName", "knowledge distillation"]]}, {"text": "the 5 K vocabulary ) , trained on the MLM teacher objective from scratch .", "entities": [[9, 10, "DatasetName", "MLM"]]}, {"text": "We also compare our models on GLUE with the following approaches : \u000fDistilBERT ( Sanh et al . , 2019 ) distill BERT BASE to 4/6 layers by aligning teacher predictions , \u000fPatient KD - PKD ( Sun et al . , 2019 ) align hidden states to distill BERT BASE to 3/6 layers , \u000fBERT - of - Theseus ( Xu et al . , 2020 ) use a layer dropout method to distill BERT BASE to 6 layers , \u000fTinyBERT ( Jiao et al . , 2019 ) apply task speci\ufb01c distillation to BERT BASE and align teacher outputs , hidden states as well as embeddings , and \u000fMobileBERT ( Sun et al . , 2020 ) combine layerwise transfer , architecture search and bottleneck", "entities": [[6, 7, "DatasetName", "GLUE"], [22, 23, "MethodName", "BERT"], [23, 24, "MethodName", "BASE"], [49, 50, "MethodName", "BERT"], [50, 51, "MethodName", "BASE"], [75, 76, "MethodName", "BERT"], [76, 77, "MethodName", "BASE"], [95, 96, "MethodName", "BERT"], [96, 97, "MethodName", "BASE"]]}, {"text": "2756Model # Params MRPC MNLI - m / mm SST-2 Average ( F1 / Acc ) ( Acc ) ( Acc ) ( F1 / Acc ) BERT BASE ( Devlin et al . , 2018 ) 109 M 88.9/- 84.6/83.4 93.5 89.0 BERT LARGE ( Devlin et al . , 2018 ) 340 M 89.3/- 86.7/85.9 94.9 90.3 PKD 6(Sun et al . , 2019 ) 67.0 M 85.0/79.9 81.5/81.0 92.0 86.2 PKD 3(Sun et", "entities": [[2, 3, "MetricName", "Params"], [3, 4, "DatasetName", "MRPC"], [4, 7, "DatasetName", "MNLI - m"], [9, 10, "DatasetName", "SST-2"], [12, 13, "MetricName", "F1"], [14, 15, "MetricName", "Acc"], [17, 18, "MetricName", "Acc"], [20, 21, "MetricName", "Acc"], [23, 24, "MetricName", "F1"], [25, 26, "MetricName", "Acc"], [27, 28, "MethodName", "BERT"], [28, 29, "MethodName", "BASE"], [43, 44, "MethodName", "BERT"]]}, {"text": "al . , 2019 ) 45.7 M 80.7/72.5 76.7/76.3 87.5 81.6 DistilBERT 4(Sanh et al . , 2019 ) 52.2 M 82.4/- 78.9/78.0 91.4 84.2 MobileBERT ( Sun et al . , 2020 ) 25.3 M 88.8/84.5 83.3/82.6 92.8 88.3 TinyBERT 4(Jiao et", "entities": [[11, 12, "MethodName", "DistilBERT"], [25, 26, "MethodName", "MobileBERT"]]}, {"text": "al . , 2019 ) 14.5 M 82.0*/ - 76.6/77.2 * - TinyBERT 4y(Jiao et", "entities": []}, {"text": "al . , 2019 ) 14.5 M 86.4/- 82.5/81.8 92.6 87.2 BERT - of - Theseus 6y(Xu et al . , 2020 ) 66 M 87.6/83.2 82.4/82.1 92.2 87.4 NoKD Baseline , L-6 , H-2566.2M81.2/74.1 76.9/76.1 87.0 81.7 Mixed - vocab distilled ( ours ) , L-6 , H-256 84.9/79.3 79.0/78.6 89.1 84.3 NoKD Baseline , L-12 , H-25610.9M85.1/79.8 79.1/79.0 89.4 84.5 Mixed - vocab distilled ( ours ) , L-12 , H-256 87.2/82.6 80.7/80.5 90.6 86.2 * denotes metrics on the development setydenotes results with task - speci\ufb01c distillation Table 1 : Test set accuracy of distilled models , teacher model and baselines on the GLUE test sets .", "entities": [[11, 12, "MethodName", "BERT"], [95, 96, "MetricName", "accuracy"], [106, 107, "DatasetName", "GLUE"]]}, {"text": "MNLI - m and MNLI - mm refer to the genre - matched and mismatched test sets .", "entities": [[0, 3, "DatasetName", "MNLI - m"], [4, 7, "DatasetName", "MNLI - mm"]]}, {"text": "All models other than NoKD and our distilled models use a 30K - WordPiece vocabulary .", "entities": [[14, 15, "MethodName", "WordPiece"]]}, {"text": "The average uses F1 score for MRPC , accuracy for MNLI - m / SST-2 .", "entities": [[3, 5, "MetricName", "F1 score"], [6, 7, "DatasetName", "MRPC"], [8, 9, "MetricName", "accuracy"], [10, 13, "DatasetName", "MNLI - m"], [14, 15, "DatasetName", "SST-2"]]}, {"text": "structures for an optimized student model .", "entities": []}, {"text": "For SNIPS , we shift our focus to smaller , lowlatency models for on - device use cases .", "entities": [[1, 2, "DatasetName", "SNIPS"]]}, {"text": "Here , we train student models with 6 layers and embedding / hidden dimensions 2f96;192;256 g.", "entities": []}, {"text": "The smaller models here may not be competitive on GLUE but are adequate for practical tasks such as spoken LU .", "entities": [[9, 10, "DatasetName", "GLUE"]]}, {"text": "We compare with two strong baselines : \u000fBERT BASE ( Chen et al . , 2019a ) with intent and IOB slot tags predicted using the [ CLS ] and the \ufb01rst WP tokens of each word respectively , and \u000fStackProp ( Qin et al . , 2019 ) , which uses a series of smaller recurrent and self - attentive encoders .", "entities": [[8, 9, "MethodName", "BASE"]]}, {"text": "4.3 Training Details Distillation : For all our models , we train the teacher model with mixed - vocabulary inputs ( stage I ) for 500 K steps , followed by 300 K steps of training just the student model ( stage II ) .", "entities": []}, {"text": "We utilize the same corpora as the teacher model i.e. BooksCorpus ( Zhu et al . , 2015 ) and English Wikipedia .", "entities": []}, {"text": "For both stages , up to 20 input tokens were masked for MLM .", "entities": [[12, 13, "DatasetName", "MLM"]]}, {"text": "In stage I , up to 10 of these masked tokens were tokenized using the teacher vocabulary , the rest using the student vocabulary .", "entities": []}, {"text": "We optimize the loss using LAMB ( You et al . , 2019 ) with a max learning rate of .00125 , linear warmup for the \ufb01rst 10 % of steps , batch size of 2048 and sequence length of 128 .", "entities": [[3, 4, "MetricName", "loss"], [5, 6, "MethodName", "LAMB"], [17, 19, "HyperparameterName", "learning rate"], [22, 24, "MethodName", "linear warmup"], [32, 34, "HyperparameterName", "batch size"], [35, 36, "DatasetName", "2048"]]}, {"text": "Distillation was done on Cloud TPUs in a 8x8 pod con\ufb01guration .", "entities": []}, {"text": "pSV , the probability of segmenting a Stage I inputword using the student vocabulary , is set to 0.5 .", "entities": []}, {"text": "Finetuning :", "entities": []}, {"text": "For all downstream task evaluations on GLUE , we \ufb01netune for 10 epochs using LAMB with a learning rate of 0.0001 and batch size of 64 .", "entities": [[6, 7, "DatasetName", "GLUE"], [14, 15, "MethodName", "LAMB"], [17, 19, "HyperparameterName", "learning rate"], [22, 24, "HyperparameterName", "batch size"]]}, {"text": "For all experiments on SNIPS , we use ADAM with a learning rate of 0.0001 and a batch size of 64 . 5 Results GLUE :", "entities": [[4, 5, "DatasetName", "SNIPS"], [8, 9, "DatasetName", "ADAM"], [11, 13, "HyperparameterName", "learning rate"], [17, 19, "HyperparameterName", "batch size"], [24, 25, "DatasetName", "GLUE"]]}, {"text": "Table 1 shows results on downstream GLUE tasks and model sizes for our proposed models , BERT BASE / LARGE , and baselines .", "entities": [[6, 7, "DatasetName", "GLUE"], [16, 17, "MethodName", "BERT"], [17, 18, "MethodName", "BASE"]]}, {"text": "Our models consistently improve upon the identically parameterized NoKD baselines , indicating mixedvocabulary training is better than training from scratch and avoids a large teacher - student performance gap .", "entities": []}, {"text": "Compared with PKD / DistilBERT , our 6 - layer model outperforms PKD 3while being>7x smaller and our 12 - layer model is comparable to PKD 6and DistilBERT 4while being\u00185 - 6x smaller .", "entities": [[4, 5, "MethodName", "DistilBERT"], [27, 28, "MethodName", "DistilBERT"]]}, {"text": "Interestingly , our models do particularly well on the MRPC task : the 6 - layer distilled model performs almost as well as PKD 6while being over 10x smaller .", "entities": [[9, 10, "DatasetName", "MRPC"]]}, {"text": "This may be due to our smaller models being data - ef\ufb01cient on the smaller MRPC dataset .", "entities": [[15, 16, "DatasetName", "MRPC"]]}, {"text": "TinyBERT and Bert - of - Theseus are trained in task - speci\ufb01c fashion i.e. , a teacher model already \ufb01netuned on the downstream task is used for distillation .", "entities": []}, {"text": "TinyBERT \u2019s non - task - speci\ufb01c model results are reported on GLUE dev sets : these results are , therefore , not directly comparable with ours .", "entities": [[12, 13, "DatasetName", "GLUE"]]}, {"text": "Even so , our 12 - layer model performs credibly", "entities": []}, {"text": "2757Model # Params Latency Intent Acc Slot F1 BERT BASE ( Chen et al . , 2019a ) 109 M 340ms 98.6 97.0 StackProp ( Qin et al . , 2019 ) 2.6M>70ms 98.0 94.2 Mixed - vocab distilled , L-6 , H-96 1.2 M 6ms 98.9 92.8 Mixed - vocab distilled , L-6 , H-192 3.6 M 14ms 98.8 94.6 Mixed - vocab distilled , L-6 , H-256 6.2 M 20ms 98.7 95.0 Table 2 : Results on the SNIPS dataset .", "entities": [[2, 3, "MetricName", "Params"], [5, 6, "MetricName", "Acc"], [7, 8, "MetricName", "F1"], [8, 9, "MethodName", "BERT"], [9, 10, "MethodName", "BASE"], [80, 81, "DatasetName", "SNIPS"]]}, {"text": "Latency is measured with 4 CPU threads on a Pixel 4 mobile device .", "entities": []}, {"text": "compared with the two , presenting a competitive size - accuracy tradeoff , particularly when compared to the 6x larger BERT - of - Theseus .", "entities": [[10, 11, "MetricName", "accuracy"], [20, 21, "MethodName", "BERT"]]}, {"text": "MobileBERT performs strongly for the size while being task - agnostic .", "entities": [[0, 1, "MethodName", "MobileBERT"]]}, {"text": "Our 12 - layer model , in comparison , retains \u001898 % of its performance with 57 % fewer parameters and may thus be bettersuited for use on highly resource - limited devices .", "entities": []}, {"text": "TinyBERT sees major gains from task - speci\ufb01c data augmentation and distillation , and MobileBERT from student architecture search and bottleneck layers .", "entities": [[8, 10, "TaskName", "data augmentation"], [14, 15, "MethodName", "MobileBERT"]]}, {"text": "Notably , our technique targets the student vocabulary without con\ufb02icting with any of the above methods and can , in fact , be combined with these methods for even smaller models .", "entities": []}, {"text": "SNIPS :", "entities": [[0, 1, "DatasetName", "SNIPS"]]}, {"text": "Table 2 shows results on the SNIPS intent and slot tasks for our models and two state - of - theart baselines .", "entities": [[6, 7, "DatasetName", "SNIPS"]]}, {"text": "Our smallest 6 - layer model retains over 95 % of the BERT BASE model \u2019s slot \ufb01lling F1 score ( Sang and Buchholz , 2000 ) while being 30x smaller ( < 10 MB w/o quantization ) and 57x faster on a mobile device , yet task - agnostic .", "entities": [[12, 13, "MethodName", "BERT"], [13, 14, "MethodName", "BASE"], [18, 20, "MetricName", "F1 score"], [36, 37, "TaskName", "quantization"]]}, {"text": "Our other larger distilled models also demonstrate strong performance ( 0:2 - 0:5%slot F1 higher than the respectiveNoKD baselines ) with small model sizes and latencies low enough for real - time inference .", "entities": [[13, 14, "MetricName", "F1"]]}, {"text": "This indicates that small multi - task BERT models ( Tsai et al . , 2019 ) present better trade - offs for on - device usage for size , accuracy and latency versus recurrent encoder - based models such as StackProp .", "entities": [[7, 8, "MethodName", "BERT"], [30, 31, "MetricName", "accuracy"]]}, {"text": "6 Discussion Impact of vocabulary size : We trained a model from scratch identical to BERT BASE except with our 5K - WP student vocabulary .", "entities": [[15, 16, "MethodName", "BERT"], [16, 17, "MethodName", "BASE"]]}, {"text": "On the SST-2 and MNLI - m dev sets , this model obtained 90.9 % and 83.7 % accuracy respectively - only 1.8 % and 0.7 % lower respectively compared to BERT BASE .", "entities": [[2, 3, "DatasetName", "SST-2"], [4, 7, "DatasetName", "MNLI - m"], [18, 19, "MetricName", "accuracy"], [31, 32, "MethodName", "BERT"], [32, 33, "MethodName", "BASE"]]}, {"text": "Since embeddings account for a larger fraction of model parameters with fewer layers , we trained another model identical to our 6 \u0002256 model , but with a 30K - WP vocabulary and teacher label dis - tillation .", "entities": []}, {"text": "This model showed small gains ( 0.1 % / 0.5 % accuracy on SST-2 / MNLI - m dev ) over our analogous distilled model , but with 30 % more parameters solely due to the larger vocabulary .", "entities": [[11, 12, "MetricName", "accuracy"], [13, 14, "DatasetName", "SST-2"], [15, 18, "DatasetName", "MNLI - m"]]}, {"text": "This suggests that a small WordPiece vocabulary may be almost as effective for sequence classi\ufb01cation / tagging tasks , especially for smaller BERT models and up to moderately long inputs .", "entities": [[5, 6, "MethodName", "WordPiece"], [22, 23, "MethodName", "BERT"]]}, {"text": "Curiously , increasing the student vocabulary size to 7 K or 10 K did not lead to an increase in performance on GLUE .", "entities": [[22, 23, "DatasetName", "GLUE"]]}, {"text": "We surmise that this may be due to under\ufb01tting owing to the embeddings accounting for a larger proportion of the model parameters .", "entities": []}, {"text": "Alternative vocabulary pruning : Probing other strategies for a small - vocabulary model , we used the above 6\u0002256 30K - WP vanilla distilled model to obtain a smaller model by pruning the vocabulary to contain the intersection of the 30 K and 5 K vocabularies ( total 4629 WPs ) .", "entities": []}, {"text": "This model is 1.2 % smaller than our 4928 - WP distilled model , but drops 0.8 % / 0.7 % on SST-2 / MNLI - m dev sets .", "entities": [[22, 23, "DatasetName", "SST-2"], [24, 27, "DatasetName", "MNLI - m"]]}, {"text": "Furthermore , to exploit the high overlap in vocabularies , we tried running our distillation pipeline but with the embeddings for student tokens ( after projecting up to the teacher dimension ) also present in the teacher vocabulary tied to the teacher embeddings for those tokens .", "entities": []}, {"text": "This model , however , dropped 0.7 % / 0.5 % on SST-2 / MNLI - m compared to our analogous 6\u0002256 distilled model .", "entities": [[12, 13, "DatasetName", "SST-2"], [14, 17, "DatasetName", "MNLI - m"]]}, {"text": "We also tried pretraining BERT LARGE from scratch with the 5 K vocabulary and doing vanilla distillation for a 6 \u0002256 student : this model dropped 1.2 % / 0.7 % for SST-2 / MNLI - m over our similar distilled model , indicating the ef\ufb01cacy of mixed - vocabulary training over vanilla distillation .", "entities": [[4, 5, "MethodName", "BERT"], [32, 33, "DatasetName", "SST-2"], [34, 37, "DatasetName", "MNLI - m"]]}, {"text": "7 Conclusion We propose a novel approach to knowledge distillation for BERT , focusing on using a signi\ufb01cantly smaller vocabulary for the student BERT models .", "entities": [[8, 10, "MethodName", "knowledge distillation"], [11, 12, "MethodName", "BERT"], [23, 24, "MethodName", "BERT"]]}, {"text": "Ourmixed - vocabulary training method encourages implicit alignment of the teacher and student WordPiece embeddings .", "entities": [[13, 14, "MethodName", "WordPiece"]]}, {"text": "Our highly - compressed 6 and 12 - layer distilled student models are optimized for on - device use cases and demonstrate competitive performance on both benchmark datasets and practical tasks .", "entities": []}, {"text": "Our technique is unique in targeting the student vocabulary size , enabling easy combination with most BERT distillation methods .", "entities": [[16, 17, "MethodName", "BERT"]]}, {"text": "2758References Jimmy Ba and Rich Caruana .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Do deep nets really need to be deep ?", "entities": []}, {"text": "In Advances in neural information processing systems , pages 2654\u20132662 .", "entities": []}, {"text": "Daoyuan Chen , Yaliang Li , Minghui Qiu , Zhen Wang , Bofang Li , Bolin Ding , Hongbo Deng , Jun Huang , Wei Lin , and Jingren Zhou .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Adabert : Task - adaptive bert compression with differentiable neural architecture search .", "entities": [[8, 12, "MethodName", "differentiable neural architecture search"]]}, {"text": "arXiv preprint arXiv:2001.04246 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Qian Chen , Zhu Zhuo , and Wen Wang . 2019a .", "entities": []}, {"text": "Bert for joint intent classi\ufb01cation and slot \ufb01lling .", "entities": []}, {"text": "arXiv preprint arXiv:1902.10909 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Wenhu Chen , Yu Su , Yilin Shen , Zhiyu Chen , Xifeng Yan , and William Yang Wang .", "entities": []}, {"text": "2019b .", "entities": []}, {"text": "How large a vocabulary does text classi\ufb01cation need ?", "entities": []}, {"text": "a variational approach to vocabulary selection .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3487\u20133497 .", "entities": []}, {"text": "Wenlin Chen , James Wilson , Stephen Tyree , Kilian Weinberger , and Yixin Chen . 2015 .", "entities": []}, {"text": "Compressing neural networks with the hashing trick .", "entities": []}, {"text": "In International Conference on Machine Learning , pages 2285\u20132294 .", "entities": []}, {"text": "Alice Coucke , Alaa Saade , Adrien Ball , Th \u00b4 eodore Bluche , Alexandre Caulier , David Leroy , Cl \u00b4 ement Doumouro , Thibault Gisselbrecht , Francesco Caltagirone , Thibaut Lavril , et al .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Snips voice platform : an embedded spoken language understanding system for private - by - design voice interfaces .", "entities": [[0, 1, "DatasetName", "Snips"], [6, 9, "TaskName", "spoken language understanding"]]}, {"text": "arXiv preprint arXiv:1805.10190 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "William B Dolan and Chris Brockett .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Automatically constructing a corpus of sentential paraphrases .", "entities": []}, {"text": "InProceedings of the International Workshop on Paraphrasing .", "entities": []}, {"text": "Angela Fan , Edouard Grave , and Armand Joulin .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Reducing transformer depth on demand with structured dropout .", "entities": []}, {"text": "arXiv preprint arXiv:1909.11556 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Geoffrey Hinton , Oriol Vinyals , and Jeff Dean . 2015 .", "entities": []}, {"text": "Distilling the knowledge in a neural network .", "entities": []}, {"text": "arXiv preprint arXiv:1503.02531 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xiaoqi Jiao , Yichun Yin , Lifeng Shang , Xin Jiang , Xiao Chen , Linlin Li , Fang Wang , and Qun Liu . 2019 .", "entities": []}, {"text": "Tinybert : Distilling bert for natural language understanding .", "entities": [[5, 8, "TaskName", "natural language understanding"]]}, {"text": "arXiv preprint arXiv:1909.10351 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Guillaume Lample and Alexis Conneau .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crosslingual language model pretraining .", "entities": []}, {"text": "arXiv preprint arXiv:1901.07291 .Zhenzhong", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , and Radu Soricut .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Albert :", "entities": []}, {"text": "A lite bert for self - supervised learning of language representations .", "entities": [[4, 8, "TaskName", "self - supervised learning"]]}, {"text": "arXiv preprint arXiv:1909.11942 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hao Li , Asim Kadav , Igor Durdanovic , Hanan Samet , and Hans Peter Graf . 2016 .", "entities": []}, {"text": "Pruning \ufb01lters for ef\ufb01cient convnets .", "entities": []}, {"text": "arXiv preprint arXiv:1608.08710 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , and Jeff Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 3111\u20133119 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 conference on empirical methods in natural language processing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 2227 \u2013 2237 .", "entities": []}, {"text": "Ivan Provilkov , Dmitrii Emelianenko , and Elena V oita .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bpe - dropout : Simple and effective subword regularization .", "entities": [[0, 1, "MethodName", "Bpe"]]}, {"text": "arXiv preprint arXiv:1910.13267 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Libo Qin , Wanxiang Che , Yangming Li , Haoyang Wen , and Ting Liu . 2019 .", "entities": []}, {"text": "A stack - propagation framework with token - level intent detection for spoken language understanding .", "entities": [[9, 11, "TaskName", "intent detection"], [12, 15, "TaskName", "spoken language understanding"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2078\u20132087 .", "entities": []}, {"text": "Alec Radford , Jeff Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Adriana Romero , Nicolas Ballas , Samira Ebrahimi Kahou , Antoine Chassang , Carlo Gatta , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Fitnets : Hints for thin deep nets .", "entities": []}, {"text": "arXiv preprint arXiv:1412.6550 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Erik F Tjong Kim Sang and Sabine Buchholz .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "Introduction to the conll-2000 shared task chunking .", "entities": [[3, 4, "DatasetName", "conll-2000"], [6, 7, "TaskName", "chunking"]]}, {"text": "In Fourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic Workshop .", "entities": []}, {"text": "Victor Sanh , Lysandre Debut , Julien Chaumond , and Thomas Wolf .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Distilbert , a distilled version of bert : smaller , faster , cheaper and lighter .", "entities": [[0, 1, "MethodName", "Distilbert"]]}, {"text": "arXiv preprint arXiv:1910.01108 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "2759Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 .", "entities": []}, {"text": "Sheng Shen , Zhen Dong , Jiayu Ye , Linjian Ma , Zhewei Yao , Amir Gholami , Michael W Mahoney , and Kurt Keutzer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Q - bert : Hessian based ultra low precision quantization of bert .", "entities": [[9, 10, "TaskName", "quantization"]]}, {"text": "arXiv preprint arXiv:1909.05840 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Raphael Shu and Hideki Nakayama . 2018 .", "entities": []}, {"text": "Compressing word embeddings via deep compositional code learning .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Richard Socher , Alex Perelygin , Jean Wu , Jason Chuang , Christopher D Manning , Andrew Ng , and Christopher Potts .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Recursive deep models for semantic compositionality over a sentiment treebank .", "entities": []}, {"text": "In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631\u20131642 .", "entities": []}, {"text": "Nitish Srivastava , Geoffrey Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov .", "entities": [[13, 14, "DatasetName", "Ruslan"]]}, {"text": "2014 .", "entities": []}, {"text": "Dropout : a simple way to prevent neural networks from over\ufb01tting .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "The journal of machine learning research , 15(1):1929\u20131958 .", "entities": []}, {"text": "Siqi Sun , Yu Cheng , Zhe Gan , and Jingjing Liu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Patient knowledge distillation for bert model compression .", "entities": [[1, 3, "MethodName", "knowledge distillation"], [5, 7, "TaskName", "model compression"]]}, {"text": "arXiv preprint arXiv:1908.09355 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhiqing Sun , Hongkun Yu , Xiaodan Song , Renjie Liu , Yiming Yang , and Denny Zhou .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Mobilebert : a compact task - agnostic bert for resource - limited devices .", "entities": [[0, 1, "MethodName", "Mobilebert"]]}, {"text": "arXiv preprint arXiv:2004.02984 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Raphael Tang , Yao Lu , Linqing Liu , Lili Mou , Olga Vechtomova , and Jimmy Lin .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Distilling taskspeci\ufb01c knowledge from bert into simple neural networks .", "entities": []}, {"text": "arXiv preprint arXiv:1903.12136 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Henry Tsai , Jason Riesa , Melvin Johnson , Naveen Arivazhagan , Xin Li , and Amelia Archer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Small and practical bert models for sequence labeling .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3623 \u2013 3627 .", "entities": []}, {"text": "Andrew Tulloch and Yangqing Jia . 2017 .", "entities": []}, {"text": "High performance ultra - low - precision convolutions on mobile devices .", "entities": []}, {"text": "arXiv preprint arXiv:1712.02427 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008.Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel R. Bowman .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "GLUE :", "entities": [[0, 1, "DatasetName", "GLUE"]]}, {"text": "A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[9, 12, "TaskName", "natural language understanding"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Adina Williams , Nikita Nangia , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A broad - coverage challenge corpus for sentence understanding through inference .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1112\u20131122 .", "entities": []}, {"text": "Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , et al . 2016 .", "entities": []}, {"text": "Google \u2019s neural machine translation system : Bridging the gap between human and machine translation .", "entities": [[0, 1, "DatasetName", "Google"], [3, 5, "TaskName", "machine translation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1609.08144 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ji Xin , Raphael Tang , Jaejun Lee , Yaoliang Yu , and Jimmy Lin .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Deebert :", "entities": [[0, 1, "MethodName", "Deebert"]]}, {"text": "Dynamic early exiting for accelerating bert inference .", "entities": [[1, 3, "MethodName", "early exiting"]]}, {"text": "arXiv preprint arXiv:2004.12993 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Canwen Xu , Wangchunshu Zhou , Tao Ge , Furu Wei , and Ming Zhou .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Bert - of - theseus : Compressing bert by progressive module replacing .", "entities": []}, {"text": "arXiv preprint arXiv:2002.02925 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Carbonell , Ruslan Salakhutdinov , and Quoc V Le . 2019 .", "entities": [[12, 13, "DatasetName", "Ruslan"]]}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1906.08237 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yang You , Jing Li , Sashank Reddi , Jonathan Hseu , Sanjiv Kumar , Srinadh Bhojanapalli , Xiaodan Song , James Demmel , and Cho - Jui Hsieh .", "entities": [[13, 14, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Large batch optimization for deep learning : Training bert in 76 minutes .", "entities": []}, {"text": "arXiv preprint arXiv:1904.00962 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sergey Zagoruyko and Nikos Komodakis .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Paying more attention to attention : Improving the performance of convolutional neural networks via attention transfer .", "entities": []}, {"text": "arXiv preprint arXiv:1612.03928 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yiren Zhou , Seyed - Mohsen Moosavi - Dezfooli , NgaiMan Cheung , and Pascal Frossard .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Adaptive quantization for deep neural network .", "entities": [[1, 2, "TaskName", "quantization"]]}, {"text": "In ThirtySecond AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Yukun Zhu , Ryan Kiros , Rich Zemel , Ruslan Salakhutdinov , Raquel Urtasun , Antonio Torralba , and Sanja Fidler .", "entities": [[9, 10, "DatasetName", "Ruslan"]]}, {"text": "2015 .", "entities": []}, {"text": "Aligning books and movies : Towards story - like visual explanations by watching movies and reading books .", "entities": []}, {"text": "In Proceedings of the IEEE international conference on computer vision , pages 19 \u2013 27 .", "entities": []}]