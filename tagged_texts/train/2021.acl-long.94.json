[{"text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing , pages 1172\u20131182 August 1\u20136 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics1172More Identi\ufb01able yet Equally Performant Transformers for Text Classi\ufb01cation Rishabh Bhardwaj1 , Navonil Majumder1 , Soujanya Poria1 , Eduard Hovy2 1Singapore University of Technology and Design , Singapore 2Carnegie Mellon University , Pittsburgh , PA , USA rishabh bhardwaj@mymail.sutd.edu.sg fnavonil majumder , sporia g@sutd.edu.sg", "entities": []}, {"text": "hovy@cs.cmu.edu", "entities": []}, {"text": "Abstract Interpretability is an important aspect of the trustworthiness of a model \u2019s predictions .", "entities": []}, {"text": "Transformer \u2019s predictions are widely explained by the attention weights , i.e. , a probability distribution generated at its self - attention unit ( head ) .", "entities": [[0, 1, "MethodName", "Transformer"]]}, {"text": "Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique .", "entities": []}, {"text": "A recent study showed theoretical justi\ufb01cations to this observation by proving the non - identi\ufb01ability of attention weights .", "entities": []}, {"text": "For a given input to a head and its output , if the attention weights generated in it are unique , we call the weights identi\ufb01able .", "entities": []}, {"text": "In this work , we provide deeper theoretical analysis and empirical observations on the identi\ufb01ability of attention weights .", "entities": []}, {"text": "Ignored in the previous works , we \ufb01nd the attention weights are more identi\ufb01able than we currently perceive by uncovering the hidden role of the key vector .", "entities": []}, {"text": "However , the weights are still prone to be non - unique attentions that make them un\ufb01t for interpretation .", "entities": []}, {"text": "To tackle this issue , we provide a variant of the encoder layer that decouples the relationship between key and value vector and provides identi\ufb01able weights up to the desired length of the input .", "entities": []}, {"text": "We prove the applicability of such variations by providing empirical justi\ufb01cations on varied text classi\ufb01cation tasks .", "entities": []}, {"text": "The implementations are available athttps://github.com/declare-lab/ identifiable - transformers .", "entities": []}, {"text": "1 Introduction Widely adopted Transformer architecture ( Vaswani et al . , 2017 ) has obviated the need for sequential processing of the input that is enforced in traditional Recurrent Neural Networks ( RNN ) .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "As a result , compared to a single - layered LSTM or RNN model , a single - layered Transformer model is computationally more ef\ufb01cient , re\ufb02ecting in a relatively shorter training time ( Vaswani et al . ,2017 ) .", "entities": [[10, 11, "MethodName", "LSTM"], [19, 20, "MethodName", "Transformer"]]}, {"text": "This advantage encourages the training of deep Transformer - based language models on largescale datasets .", "entities": [[7, 8, "MethodName", "Transformer"]]}, {"text": "Their learning on large corpora has already attained state - of - the - art ( SOTA ) performances in many downstream Natural Language Processing ( NLP ) tasks .", "entities": []}, {"text": "A large number of SOTA machine learning systems even beyond NLP ( Lu et al . , 2019 ) are inspired by the building blocks of Transformer that is multi - head self - attention ( Radford et al . , 2018 ; Devlin et", "entities": [[26, 27, "MethodName", "Transformer"]]}, {"text": "al . , 2018 ) .", "entities": []}, {"text": "A model employing an attention - based mechanism generates a probability distributiona = fa1;:::;a ngover theninput units z = fz1;:::;z ng .", "entities": []}, {"text": "The idea is to perform a weighted sum of inputs , denoted byPn i=1aizi , to produce a more context - involved output .", "entities": []}, {"text": "The attention vector , a , are commonly interpreted as scores signifying the relative importance of input units .", "entities": []}, {"text": "However , counter - intuitively , it is recently observed that the weights generated in the model do not provide meaningful explanations ( Jain and Wallace , 2019 ; Wiegreffe and Pinter , 2019 ) .", "entities": []}, {"text": "Attention weights are ( structurally ) identi\ufb01able if we can uniquely determine them from the output of the attention unit ( Brunner et al . , 2019 ) .", "entities": []}, {"text": "Identi\ufb01ability of the attention weights is critical to the model \u2019s prediction to be interpretable and replicable .", "entities": []}, {"text": "If the weights are not unique , explanatory insights from them might be misleading .", "entities": []}, {"text": "Theself - attention transforms an input sequence of vectors z = fz1;:::;z ngto a contextualized output sequence y = fy1;:::;y ng , where yk = Pn i=1a(k;i)zi .", "entities": []}, {"text": "The scalara(k;i)captures how much of the ithtoken contributes to the contextualization ofkthtoken .", "entities": []}, {"text": "A Transformer layer consists of multiple heads , where each head performs selfattention computations , we break the head computations in two phases : \u2022Phase 1 : Calculation of attention weights a(k;i ) .", "entities": [[1, 2, "MethodName", "Transformer"]]}, {"text": "It involves mapping input tokens to", "entities": []}, {"text": "1173key and query vectors .", "entities": []}, {"text": "The dot product of kth query vector and ithkey vector gives a(k;i ) .", "entities": [[4, 5, "DatasetName", "kth"]]}, {"text": "\u2022Phase 2 : Calculation of a contextualized representation for each token .", "entities": []}, {"text": "It involves mapping input tokens to the value vectors .", "entities": []}, {"text": "The contextualized representation for kthtoken can be computed by the weighted average of the value vectors , where the weight of ithtoken is a(k;i)computed in \ufb01rst phase .", "entities": []}, {"text": "The identi\ufb01ability in Transformer has been recently studied by Brunner et al .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "( 2019 ) which provides theoretical claims that under mild conditions of input length , attention weights are not unique to the head \u2019s output .", "entities": []}, {"text": "Essentially their proof was dedicated to the analysis of the computations in the second phase , i.e. , token contextualization .", "entities": []}, {"text": "However , the theoretical analysis ignored the crucial \ufb01rst phase where the attention weights are generated .", "entities": []}, {"text": "Intrinsic to their analysis , the attention identi\ufb01ability can be studied by studying only the second phase of head computations .", "entities": []}, {"text": "However , even if we \ufb01nd another set of weights from the second phase , it depends on the \ufb01rst phase if those weights can be generated as the part of key - query multiplication .", "entities": []}, {"text": "In this work , we probe the identi\ufb01ability of attention weights in Transformer from a perspective that was ignored in Brunner et al .", "entities": [[12, 13, "MethodName", "Transformer"]]}, {"text": "( 2019 ) .", "entities": []}, {"text": "We explore the previously overlooked \ufb01rst phase of selfattention for its contribution to the identi\ufb01ability in Transformer .", "entities": [[16, 17, "MethodName", "Transformer"]]}, {"text": "During our analysis of the \ufb01rst phase , we uncover the critical constraint imposed by the size of the key vector1dk .", "entities": []}, {"text": "The \ufb02ow of analysis can be described as \u2022We \ufb01rst show that the attention weights are identi\ufb01able for the input sequence length dsno longer than the size of value vector dv(\u00a73.1 ) ( Brunner et al . , 2019)2 .", "entities": []}, {"text": "\u2022For the case when ds > dv , we analyse the attention weights as raw dot - product ( logits ) and thesoftmax ed dot - product ( probability simplex ) , independently .", "entities": []}, {"text": "An important theoretical \ufb01nding is that both versions are prone to be unidenti\ufb01able .", "entities": []}, {"text": "\u2022In the case of attention weights as logits ( \u00a7 3.2.1 ) , we analytically construct another set of attention weights to claim the unidenti\ufb01ability .", "entities": []}, {"text": "In the case of attention weights as 1The size of key and query vector is expected to be the same due to the subsequent dot product operation 2The sequence length denotes number of tokens at input.softmax ed logits ( \u00a7 3.2.2 ) , we \ufb01nd the attention identi\ufb01ability to be highly dependent on dk .", "entities": []}, {"text": "Thus , the size of key vector plays an important role in the identi\ufb01ability of the self - attention head .", "entities": []}, {"text": "The pieces of evidence suggest that the current analysis in Brunner et al .", "entities": []}, {"text": "( 2019 ) ignored the crucial constraints from the \ufb01rst phase in their analysis .", "entities": []}, {"text": "To resolve the unidenti\ufb01ability problem , we propose two simple solutions ( \u00a7 4 ) .", "entities": []}, {"text": "For the regular setting of the Transformer encoder where dvdepends on the number of attention heads and token embedding dimension , we propose to reduce dk .", "entities": [[6, 7, "MethodName", "Transformer"], [18, 20, "HyperparameterName", "embedding dimension"]]}, {"text": "This may lead to more identi\ufb01able attention weights .", "entities": []}, {"text": "Alternatively , as a more concrete solution , we propose to setdvequal to token embedding dimension while adding head outputs as opposed to the regular approach of concatenation ( Vaswani et al . , 2017 ) .", "entities": [[14, 16, "HyperparameterName", "embedding dimension"]]}, {"text": "Embedding dimension can be tuned according to the sequence length up to which identi\ufb01ability is desired .", "entities": [[0, 2, "HyperparameterName", "Embedding dimension"]]}, {"text": "We evaluate the performance of the proposed variants on varied text classi\ufb01cation tasks comprising of ten datasets ( \u00a7 5 ) .", "entities": []}, {"text": "In this paper , our goal is to provide concrete theoretical analysis , experimental observations , and possible simple solutions to identi\ufb01ability of attention weights in Transformer .", "entities": [[26, 27, "MethodName", "Transformer"]]}, {"text": "The idea behind identi\ufb01able variants of the Transformer is \u2014 the harder it is to obtain alternative attention weights , the likelier is they are identi\ufb01able , which is a desirable property of the architecture .", "entities": [[7, 8, "MethodName", "Transformer"]]}, {"text": "Thus , our contribution are as follows : \u2022We provide a concrete theoretical analysis of identi\ufb01ability of attention weights which was missing in the previous work by Brunner", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "\u2022We provide Transformer variants that are identi\ufb01able and validate them empirically by analysing the numerical rank of the attention matrix generated in the self - attention head of the Transformer encoder .", "entities": [[2, 3, "MethodName", "Transformer"], [29, 30, "MethodName", "Transformer"]]}, {"text": "The variants have strong mathematical support and simple to adopt in the standard Transformer settings .", "entities": [[13, 14, "MethodName", "Transformer"]]}, {"text": "\u2022We provide empirical evaluations on varied text classi\ufb01cation tasks that show higher identi\ufb01ability does not compromise with the task \u2019s performance .", "entities": []}, {"text": "11742 Background 2.1 Identi\ufb01ability A general trend in machine learning research is to mathematically model the input - output relationship from a dataset .", "entities": []}, {"text": "This is carried out by quantitatively estimating the set of model parameters that best \ufb01t the data .", "entities": []}, {"text": "The approach warrants prior ( to \ufb01tting ) examination of the following aspects : \u2022The suf\ufb01ciency of the informative data to the estimate model parameters , i.e. , practical identi\ufb01ability .", "entities": []}, {"text": "Thus , the limitation comes from the dataset quality or quantity and may lead to ambiguous data interpretations ( Raue et al . , 2009 ) .", "entities": []}, {"text": "\u2022The possibility that the structure of the model allows its parameters to be uniquely estimated , irrespective of the quality or quantity of the available data .", "entities": []}, {"text": "This aspect is called structural identi\ufb01ability .", "entities": []}, {"text": "A model is said to be structurally unidenti\ufb01able if a different set of parameters yield the same outcome .", "entities": []}, {"text": "In this work , we focus on the structural identi\ufb01ability ( Bellman and \u02daAstr\u00a8om , 1970 ) .", "entities": []}, {"text": "It is noteworthy that the goodness of the \ufb01t of a model on the data does not dictate its structural identi\ufb01ability .", "entities": []}, {"text": "Similar to Brunner et al .", "entities": []}, {"text": "( 2019 ) , we focus our analysis on the identi\ufb01ability of attention weights , which are not model parameters , yet demands meaningful interpretations and are crucial to the stability of representations learned by the model .", "entities": []}, {"text": "2.2 Transformer Encoder Layer We base our analysis on the building block of Transformer , i.e. , the encoder layer ( Vaswani et al . , 2017 ) .", "entities": [[1, 2, "MethodName", "Transformer"], [13, 14, "MethodName", "Transformer"]]}, {"text": "The layer has two sub - layers .", "entities": []}, {"text": "First sublayer performs the multi - head self - attention , and second is feed - forward network .", "entities": []}, {"text": "Given a sequence of tokensfx1;:::;x dsg , an embedding layer transforms it to a set of vector fz1;:::;z dsg2Rde , wherededenotes token embedding dimension .", "entities": [[22, 24, "HyperparameterName", "embedding dimension"]]}, {"text": "To this set , we add vectors encoding positional information of tokensfp1;:::;p dsg2Rde .", "entities": []}, {"text": "Multi - head Attention .", "entities": [[0, 4, "MethodName", "Multi - head Attention"]]}, {"text": "Input to a head of multihead self - attention module is W2Rds\u0002de , i.e. , a sequence of dstokens lying in a de - dimensional embedding space .", "entities": []}, {"text": "Tokens are projected to dq - size query , dk - size key , and dv - size value vectors using linear layers , resulting in the respective matrices Query Q2Rds\u0002dq , Key K2Rds\u0002dk , and Value Figure 1 : An illustration for a Transformer with two - head attention units .", "entities": [[44, 45, "MethodName", "Transformer"]]}, {"text": "Triangles depict matrix weights .", "entities": []}, {"text": "The left side shows concatenation of head outputs fed to a linear layer .", "entities": [[11, 13, "MethodName", "linear layer"]]}, {"text": "The right side shows another interpretation of the same set of operations where we consider a linear transform applied to each head \ufb01rst .", "entities": []}, {"text": "The transformed head outputs are then added .", "entities": []}, {"text": "V2Rds\u0002dv .", "entities": []}, {"text": "The attention weights A2Rds\u0002ds can be computed by A= softmax   Q KT p dq ! : ( 1 ) The(i;j)thelement of Ashows how much of ith token is in\ufb02uenced by jthtoken .", "entities": [[9, 10, "MethodName", "softmax"]]}, {"text": "The output of a head H2Rds\u0002deis given by H = A V D = A T ; ( 2 ) where D2Rdv\u0002deis a linear layer and the matrixT2Rds\u0002dedenotes the operation V D.", "entities": [[23, 25, "MethodName", "linear layer"]]}, {"text": "The Rds\u0002deoutput of multi - head attention can be expressed as a summation over Hobtained for each head3 .", "entities": [[3, 7, "MethodName", "multi - head attention"]]}, {"text": "Theithrow of multi - head output matrix corresponds to the dedimensional contextualized representation of ithinput token .", "entities": []}, {"text": "In the original work , Vaswani et al . ( 2017 ) , the multi - head operation is described as the concatenation of A V obtained from each head followed by a linear transformation D2Rde\u0002de .", "entities": []}, {"text": "Both the explanations are associated with the same sequence of matrix operations as shown in \ufb01g . 1 .", "entities": []}, {"text": "In regular Transformer setting , a token vector isti2f(zj+pj)gds i=1isde=512dimensional , number of heads h=8 , size ofdk = dq = dv = de = h=64 .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "Feed - Forward Network .", "entities": []}, {"text": "This sub - layer performs the following transformations on each token representation at the output of a head : y1= Linear 1(Norm(ti+head output for ti ) )", "entities": []}, {"text": "y2= Norm(ti+", "entities": []}, {"text": "ReLU(Linear 2(y1 ) ) )", "entities": []}, {"text": "Linear 1andLinear 2are linear layers with 2048 and 512 nodes , respectively .", "entities": [[6, 7, "DatasetName", "2048"]]}, {"text": "Norm denotes minibatch layer normalization .", "entities": [[3, 5, "MethodName", "layer normalization"]]}, {"text": "3For simplicity , we have omitted head indices .", "entities": []}, {"text": "11753 Identi\ufb01ability of Attention The output of an attention head His the product of AandT(eq .", "entities": []}, {"text": "( 2 ) ) .", "entities": []}, {"text": "Formally , we de\ufb01ne identi\ufb01ability of attention in a head : De\ufb01nition 3.1 .", "entities": []}, {"text": "For an attention head \u2019s output H , attention weights Aare identi\ufb01able if there exists a unique solution of A T = H. The above de\ufb01nition can be reformulated as De\ufb01nition 3.2 .", "entities": []}, {"text": "Ais unidenti\ufb01able if there exist an ~A,(~A6=0 ) , such that ( A+~A)is obtainable from phase-1 of head computations and satisfy ( A+~A)T = A T=)~A T=0 : ( constraint - R1 ) Under this constraint , we get ~aiT= 0where ~ai is theithrow of ~A.", "entities": []}, {"text": "The set of vectors which when multiplied to Tgets mapped to zero describes the left null space of Tdenoted by LN(T ) .", "entities": []}, {"text": "The dimension of the left null space of Tcan be obtained by taking the difference of the total number of rows ( ds ) and the number of linearly independent rows , i.e , rank of the matrix Tdenoted by rank ( T ) .", "entities": []}, {"text": "Let dim(\u0001)denotes the dimension of a vector space , then LN(T )", "entities": []}, {"text": "= fvjvTT=0 g ( 3 ) dim\u0000 LN(T)\u0001 = ds\u0000rank ( T ): ( 4 ) 3.1 \u201c A \u201d is Identi\ufb01able for ds\u0014dv Ifdim(LN ( T ) )", "entities": []}, {"text": "= 0 thenLN(T )", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "= f0 g , it leads to the only solution of constraint - R1 that is ~A=0 .", "entities": []}, {"text": "Therefore , the unidenti\ufb01abilty condition does not hold .", "entities": []}, {"text": "Now we will prove such a situation exists when the number of tokens is not more than the size of value vector .", "entities": []}, {"text": "The matrix Tin eq .", "entities": []}, {"text": "( 2 ) is product of ds\u0002dv value matrix Vanddv\u0002detransformation", "entities": []}, {"text": "D. We utilize the fact that the rank of product of two matrices PandQis upper bounded by the minimum of rank ( P)andrank ( Q ) , i.e. , rank ( P Q)\u0014 min\u0000 rank ( P);rank ( Q)\u0001 .", "entities": []}, {"text": "Thus , the upper bound onrank ( T)in eq .", "entities": []}, {"text": "( 4 ) can be determined by rank ( T)\u0014min\u0010 rank ( V);rank ( D)\u0011 \u0014min\u0010 min(ds ; dv);min(dv ; de)\u0011 \u0014min\u0010 ds ; dv ; dv ; de\u0011 \u0014min\u0010 ds ; dv\u0011 ( asde > dv ) = min\u0010 ds;64\u0011(5 ) where the last inequality is obtained for a head in the regular Transformer for which dv=64 .", "entities": [[54, 55, "MethodName", "Transformer"]]}, {"text": "Figure 2 : Numerical rank of T(IMDB ) and dimension of its left null space are scattered in blue and red , respectively .", "entities": []}, {"text": "Numerical rank .", "entities": []}, {"text": "To substantiate the bounds on rank ( T)as derived above , we set up a model with a single encoder layer ( \u00a7 6 ) .", "entities": []}, {"text": "The model is trained to predict the sentiment of IMDB reviews ( \u00a7 5 ) .", "entities": [[9, 10, "DatasetName", "IMDB"]]}, {"text": "We feed the review tokens to the model and store the values generated in Tof the \ufb01rst head .", "entities": []}, {"text": "A standard technique for calculating the rank of a matrix with \ufb02oating - point values and computations is to use singular value decomposition .", "entities": []}, {"text": "The rank of the matrix will be computed as the number of singular values larger than the prede\ufb01ned threshold4 .", "entities": []}, {"text": "The \ufb01g . 2 illustrates how the rank changes with the sequence length ds .", "entities": []}, {"text": "The numerical rank provides experimental support to the theoretical analysis .", "entities": []}, {"text": "rank ( T ) = \u001adsifds\u0014dv ; dvifds > dv:(6 )", "entities": []}, {"text": "Thus , dim\u0000 LN(T)\u0001 = ds\u0000rank ( T ) = \u001a0 ifds\u0014dv ; ( ds\u0000dv)ifds > dv : = max ( ds\u0000dv;0 ) ( 7 ) With this , we infer Ais identi\ufb01able if ds\u0014dv=64 .", "entities": []}, {"text": "For the identi\ufb01ability study , since we focus on a model \u2019s capability of learning unique attention weights , we will assume Thas the maximum obtainable rank set by its upper bound .", "entities": []}, {"text": "3.2 Ideniti\ufb01ability when ds > dv ( the hidden role of dk )", "entities": []}, {"text": "In this case , from eq .", "entities": []}, {"text": "( 7 ) , we obtain a non zero value of dim\u0000 LN(T)\u0001 .", "entities": []}, {"text": "It allows us to \ufb01nd in\ufb01nite ~ A \u2019s satisfying ( A+~A)T = A T. However , 4The threshold value is max(ds ; de)\u0003eps\u0003jjTjj2 .", "entities": []}, {"text": "The epsis \ufb02oating - point machine epsilon value , i.e. , 1.19209e-07 in our experiments", "entities": [[6, 7, "HyperparameterName", "epsilon"]]}, {"text": "1176constraint - R1 demands ~Ato be obtainable from the \ufb01rst phase of self - attention .", "entities": []}, {"text": "As a \ufb01rst step , we focus our analysis on the attention matrix without applyingsoftmax non - linearity , i.e. , A=\u0012 Q KTp dq\u0013 .", "entities": []}, {"text": "The analysis is crucial to identify constraints coming from the \ufb01rst phase of self - attention in Transformer that impact identi\ufb01ability .", "entities": [[17, 18, "MethodName", "Transformer"]]}, {"text": "Insights from this will help us analyse softmax version of A. 3.2.1 Attention Weights as Logits Since the logits matrix Ais obtained from the product of QandKT , we can assert that rank ( A)\u0014min\u0000 rank ( Q);rank ( KT)\u0001 \u0014min\u0000 de;dk;dq;de\u0001 = dk:(8 ) Therefore , the rank of attention matrix producible by the head in the \ufb01rst phase of self - attention can at most be equal to the size of key vectors dk .", "entities": [[7, 8, "MethodName", "softmax"]]}, {"text": "On this basis , the head can produce only those A+~A satisfying rank ( A+~A)\u0014dk ( constraint - R2 )", "entities": []}, {"text": "Proposition 3.3 .", "entities": []}, {"text": "There exists a non - trivial ~Athat satisfy ( A+~A)T = A Tand constraint - R2 .", "entities": []}, {"text": "Hence , Ais unidenti\ufb01able .", "entities": []}, {"text": "Proof .", "entities": []}, {"text": "Leta1;:::;a dsand ~a1 ; : : : ; ~adsdenote rows of Aand ~ A , respectively .", "entities": []}, {"text": "Without the loss of generality , let a1;:::;a dkbe linearly independent rows .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "For all j > d k , ajcan be represented as a linear combinationPdk i=1\u0015j iai , where \u0015j iis a scalar .", "entities": []}, {"text": "Next , we independently choose \ufb01rstkrows of ~Athat aref ~ a1 ; : : : ; ~adkgfrom LN(T ) .", "entities": []}, {"text": "From the same set of coef\ufb01cients of linear combination \u0015j ifori2 f1;:::;d kgand j2fdk+1;:::;d sg , we can construct jthrow of ~A as ~ aj = Pdk i=1\u0015j i ~ ai .", "entities": []}, {"text": "Now , since we can construct thejthrow of ( A+~A)from the linear combination of its \ufb01rst dkrows asPdk i=1\u0015j i(ai+ ~ai ) , the rank of ( A+~A)is not more than dk .", "entities": []}, {"text": "For a set of vectors lying in a linear space , a vector formed by their linear combination should also lie in the same space .", "entities": []}, {"text": "Thus , the arti\ufb01cially constructed rows of ~Abelongs to LN(T ) .", "entities": []}, {"text": "Therefore , there exist an ~A that establishes the proposition which claims the unidenti\ufb01ability of A.3.2.2 Attention Weights as Softmaxed Logits Thesoftmax over attention logits generates attention weights with each row of A(i.e .", "entities": []}, {"text": ",ai \u2019s ) is constrained to be a probability distribution .", "entities": []}, {"text": "Hence , we can de\ufb01ne constraint over ~Aas ( A+~A)\u00150 ( P1 ) ~A T=0 ( P2 ) ~A 1=0 : ( P3 ) P1 is non - negativity constraint on ( A+~A)as it is supposed to be the output of softmax ; P2 denotes ~A2LN(T ) ; P3 can be derived from the fact ( A+~A)1=1=)(A 1+~A 1 ) = 1=)~A 1=0 as ( A 1=1 ) .", "entities": [[22, 23, "DatasetName", "P3"], [41, 42, "MethodName", "softmax"], [48, 49, "DatasetName", "P3"]]}, {"text": "Where 12Rdsis the vector of ones .", "entities": []}, {"text": "The constraint in P2 and P3 can be combined and reformulated as ~A[T;1 ] = 0 .", "entities": [[5, 6, "DatasetName", "P3"], [15, 16, "DatasetName", "0"]]}, {"text": "Following the similar analysis as in eq .", "entities": []}, {"text": "( 7 ) , we can obtain dim\u0000 LN ( [ T;1])\u0001 = max\u0000 ds\u0000(dv+ 1);0\u0001 .", "entities": []}, {"text": "Disregarding the extreme cases when aiis a one - hot distribution , Brunner et al .", "entities": []}, {"text": "( 2019 ) proved the existence and construction of non - trivial ~A \u2019s satisfying all the constraints P1 , P2 , and", "entities": []}, {"text": "P3.5 However , the proof by Brunner et al .", "entities": []}, {"text": "( 2019 ) missed the constraint - R2 , hence the existence of a non - trivial ~Asatisfying only the set of constraints P1 , P2 and P3 may not be a valid proposition to claim attention weights unidenti\ufb01ability .", "entities": [[27, 28, "DatasetName", "P3"]]}, {"text": "Essentially , the work largely ignored the constraints coming from the rank of the matrix that produces A after softmax6 .", "entities": []}, {"text": "Let Aldenote logits\u0012 Q KTp dq\u0013 andsoftmax ( Al ) = ( A+~A ) , where softmax is operated over each row of Al .", "entities": [[16, 17, "MethodName", "softmax"]]}, {"text": "We add an extra constraint on Al rank ( Al)\u0014dk : ( P4 ) The constraint P4 con\ufb01rms if there exists a logit matrix Althat can generate ( A+~A ) , given constraints P1 , P2 , and P3 are satis\ufb01ed .", "entities": [[38, 39, "DatasetName", "P3"]]}, {"text": "The possibility of such an Alwill provide suf\ufb01cient evidence that Ais unidenti\ufb01able .", "entities": []}, {"text": "Next , we investigate how the existence of ~Ais impacted by the size of key vector dk(query and key vector sizes are the same , i.e. , dq = dk ) .", "entities": []}, {"text": "Let(A+~A)(i;k)denotes ( i;k)thelement of the matrix .", "entities": []}, {"text": "We can retrieve the set of matrices Alsuch thatsoftmax ( Al ) = A+~A , where Al(i;k ) = ci+ log ( A+~A)(i;k ) ( 9 ) 5For the sake of brevity , we skip the construction method .", "entities": []}, {"text": "6(input to the softmax is equivalent to Ain \u00a7 3.2.1 )", "entities": [[3, 4, "MethodName", "softmax"]]}, {"text": "1177 Figure 3 : Column vectors ( c+^ ak)ofAl , where a(i;k)represents log(A+~A)(i ; k ) .", "entities": []}, {"text": "for some arbitrary ci2R;logdenotes natural logarithm .", "entities": []}, {"text": "As shown in \ufb01g . 3 , the column vectors ofAlcan be written as c+^ a1 ; : : : ; c+^ ads .", "entities": []}, {"text": "For an arbitrarily picked ~Asatisfying constraint P1 , P2 , and P3 , the dimensions of af\ufb01ne span Sof f^ a1;:::;^ adsgcould be as high as ds\u00001(\ufb01g . 4 ) .", "entities": [[11, 12, "DatasetName", "P3"], [18, 19, "DatasetName", "Sof"]]}, {"text": "In such cases , the best one could do is to choose aca2S such that the dimension of the linear span off^ a1\u0000ca;:::;^ ads\u0000cag , i.e. , rank ( Al)is ds\u00001 .", "entities": []}, {"text": "Hence , to satisfy P4 , ds\u00001\u0014dk= ) ds\u0014dk+ 1 .", "entities": []}, {"text": "Thus , the set of ( A+~A)satisfying constraint P1 , P2 and P3 are not always obtainable from attention head for ds > dk .", "entities": [[12, 13, "DatasetName", "P3"]]}, {"text": "We postulate Although it is easier to construct ~Asatisfying constraints P1 , P2 and P3 , it is hard to construct ~Asatisfying constraint P4 over the rank of logit matrix Al .", "entities": [[14, 15, "DatasetName", "P3"]]}, {"text": "Therefore , Abecomes more identi\ufb01able as the size of key vector decreases .", "entities": []}, {"text": "Figure 4 : This is a simpli\ufb01ed illustration for the case ds= 3 .", "entities": []}, {"text": "Af\ufb01ne space ( translated linear subspace ) spanned by vectors ^ a1,^ a2and^", "entities": []}, {"text": "a3.cacan be any arbitrary vector in af\ufb01ne space .", "entities": []}, {"text": "By putting c=\u0000ca , we can obtain a linear subspace whose rank is equal to rank of the af\ufb01ne subspace .", "entities": []}, {"text": "Experimental evidence .", "entities": []}, {"text": "We conduct an experiment to validate the minimum possible numerical rank of Alby constructing ~A.", "entities": []}, {"text": "For ~Ato be obtainable from the phase 1 , the minimum possible rank ofAlshould not be higher than dk .", "entities": []}, {"text": "From IMDB dataset ( \u00a7 5 ) , we randomly sample a set of reviewswith token sequence length dsranging from 66 to 1287 .", "entities": [[1, 2, "DatasetName", "IMDB"]]}, {"text": "For each review , we construct 1000 ~A \u2019s satisfying constraints P1 , P2 , and P3 \u2014 First , we train a Transformer encoder - based IMDB review sentiment classi\ufb01er ( \u00a7 6 ) .", "entities": [[16, 17, "DatasetName", "P3"], [23, 24, "MethodName", "Transformer"], [27, 28, "DatasetName", "IMDB"]]}, {"text": "We obtain an orthonormal basis for the left null space of [ T;1]using singular value decomposition .", "entities": []}, {"text": "To form an ~ A , we generate dsrandom linear combinations of the basis vectors ( one for each of its row ) .", "entities": []}, {"text": "Each set of linear combination coef\ufb01cients is sampled uniformly from [ \u000010;10 ] .", "entities": []}, {"text": "All the rows are then scaled to satisfy the constraint P1 as mentioned in Brunner et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Using eq .", "entities": []}, {"text": "( 9 ) , we obtain a minimum rank matrix Al \u2019s by putting c=\u0000^ a1 .", "entities": []}, {"text": "Figure 5 depicts the obtained numerical rank of Al .", "entities": []}, {"text": "We observed all the obtained Alfrom ( A+~A ) ( using eq .", "entities": []}, {"text": "( 9 ) ) are full - row rank matrices .", "entities": []}, {"text": "However , from the \ufb01rst phase of self - attention , the maximum obtainable rank of Alisdk= 64 .", "entities": []}, {"text": "Thus , the experimentally constructed Al \u2019s do not claim unidenti\ufb01ability of Aas it fails to satisfy the constraint P4 , while for Brunner et al .", "entities": []}, {"text": "( 2019 ) , it falls under the solution set to prove unidenti\ufb01ability as it meets constraints P1 , P2 and P3 .", "entities": [[21, 22, "DatasetName", "P3"]]}, {"text": "Figure 5 : The blue curve denotes the expected rank of Al \u2019s obtained from ( A+~A ) , where ~Asatis\ufb01es the constraints P1 , P2 , and P3 .", "entities": [[28, 29, "DatasetName", "P3"]]}, {"text": "The red curve denotes the maximum permissible rank of Althat is obtainable from phase 1 of the head .", "entities": []}, {"text": "4 Solutions to Identi\ufb01ability Based on the Identi\ufb01ability analysis in \u00a7 3 , we propose basic solutions to make Transformer \u2019s attention weights identi\ufb01able .", "entities": [[19, 20, "MethodName", "Transformer"]]}, {"text": "Decoupling dk .", "entities": []}, {"text": "Contrary to the regular Transformer setting where dk = dv , a simple approach is to decrease the value of dkthat is the size of the key and query vector .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "It will reduce the possible 7dim\u0000 LN(T;1)\u0001 > 0fords > dv+ 1 = 65", "entities": []}, {"text": "1178solutions of ~Aby putting harder constraints on the rank of attention logits , i.e. , Alin eq .", "entities": []}, {"text": "( 9 ) .", "entities": []}, {"text": "However , theoretically , dkdecides the upper bound on dimensions of the space to which token embeddings are projected before the dot product .", "entities": []}, {"text": "Higher the upper bound , more degree of freedom to choose the subspace dimensions as compared to the lower dkvariants .", "entities": []}, {"text": "Thus , there is a plausible trade - off when choosing between dkinduced identi\ufb01ability and the upper bound on the dimension of projected space .", "entities": []}, {"text": "Head Addition .", "entities": []}, {"text": "To resolve the unidenti\ufb01ability issue when sequence length exceeds the size of value vector , we propose to keep the value vector size and token embedding dimension to be more than ( or equal to ) the maximum allowed input tokens , i.e. ,dv\u0015ds - max .", "entities": [[25, 27, "HyperparameterName", "embedding dimension"]]}, {"text": "In Vaswani et", "entities": []}, {"text": "al . ( 2017 ) , dv was bound to be equal to de = h , wheredeis token embedding dimension and his number of heads .", "entities": [[19, 21, "HyperparameterName", "embedding dimension"]]}, {"text": "This constraint on dvis because of the concatenation ofhself - attention heads to produce de - sized output at the \ufb01rst sub - layer of the encoder .", "entities": []}, {"text": "Thus , to decoupledvfrom this constraint , we keep dv = de and add each head \u2019s output.8 5 Classi\ufb01cation Tasks For the empirical analysis of our proposed solutions as mentioned in \u00a7 4 , we conduct our experiments on the following varied text classi\ufb01cation tasks : 5.1 Small Scale Datasets IMDB ( Maas et al . , 2011 ) .", "entities": [[51, 52, "DatasetName", "IMDB"]]}, {"text": "The dataset for the task of sentiment classi\ufb01cation consist of IMDB movie reviews with their sentiment as positive or negative .", "entities": [[10, 13, "DatasetName", "IMDB movie reviews"]]}, {"text": "Each of the train and test sets contain 25,000 data samples equally distributed in both the sentiment polarities .", "entities": []}, {"text": "TREC ( Voorhees and Tice , 2000 ) .", "entities": [[0, 1, "DatasetName", "TREC"]]}, {"text": "We use the 6 - class version of the dataset for the task of question classi\ufb01cation consisting of open - domain , facet - based questions .", "entities": []}, {"text": "There are 5,452 and 500 samples for training and testing , respectively .", "entities": []}, {"text": "SST ( Socher et al . , 2013 ) .", "entities": [[0, 1, "DatasetName", "SST"]]}, {"text": "Stanford sentiment analysis dataset consist of 11,855 sentences obtained from movie reviews .", "entities": [[1, 3, "TaskName", "sentiment analysis"]]}, {"text": "We use the 3 - class version of the dataset for the task of sentiment classi\ufb01cation .", "entities": []}, {"text": "Each review is labeled as positive , neutral , or negative .", "entities": []}, {"text": "The provided train / test / valid split is 8,544/2,210/1,101 .", "entities": []}, {"text": "8ds - max < deas in the regular Transformer", "entities": [[8, 9, "MethodName", "Transformer"]]}, {"text": "setting.5.2", "entities": []}, {"text": "Large Scale Datasets SNLI ( Bowman et al . , 2015 ) .", "entities": [[3, 4, "DatasetName", "SNLI"]]}, {"text": "The dataset contain 549,367 samples in the training set , 9,842 samples in the validation set , and 9,824 samples in the test set .", "entities": []}, {"text": "For the task of recognizing textual entailment , each sample consists of a premisehypothesis sentence pair and a label indicating whether the hypothesis entails the premise , contradicts it , or neutral .", "entities": []}, {"text": "Please refer to Zhang et al .", "entities": []}, {"text": "( 2015 ) for more details about the following datasets : Yelp .", "entities": []}, {"text": "We use the large - scale Yelp review dataset for the task of binary sentiment classi\ufb01cation .", "entities": []}, {"text": "There are 560,000 samples for training and 38,000 samples for testing , equally split into positive and negative polarities .", "entities": []}, {"text": "DBPedia .", "entities": [[0, 1, "DatasetName", "DBPedia"]]}, {"text": "The Ontology dataset for topic classi\ufb01cation consist of 14 non - overlapping classes each with 40,000 samples for training and 5,000 samples for testing .", "entities": [[1, 2, "MethodName", "Ontology"]]}, {"text": "Sogou News .", "entities": []}, {"text": "The dataset for news article classi\ufb01cation consist of 450,000 samples for training and 60,000 for testing .", "entities": []}, {"text": "Each article is labeled in one of the 5 news categories .", "entities": []}, {"text": "The dataset is perfectly balanced .", "entities": []}, {"text": "AG News .", "entities": [[0, 2, "DatasetName", "AG News"]]}, {"text": "The dataset for the news articles classi\ufb01cation partitioned into four categories .", "entities": []}, {"text": "The balanced train and test set consist of 120,000 and 7,600 samples , respectively .", "entities": []}, {"text": "Yahoo !", "entities": []}, {"text": "Answers .", "entities": []}, {"text": "The balanced dataset for 10class topic classi\ufb01cation contain 1,400,000 samples for training and 50,000 samples for testing .", "entities": []}, {"text": "Amazon Reviews .", "entities": []}, {"text": "For the task of sentiment classi\ufb01cation , the dataset contain 3,600,000 samples for training and 400,000 samples for testing .", "entities": []}, {"text": "The samples are equally divided into positive and negative sentiment labels .", "entities": []}, {"text": "Except for the SST and SNLI , where the validation split is already provided , we \ufb02ag 30 % of the train set as part of the validation set and the rest 70 % were used for model parameter learning .", "entities": [[3, 4, "DatasetName", "SST"], [5, 6, "DatasetName", "SNLI"]]}, {"text": "6 Experimental Setup Setting up the encoder .", "entities": []}, {"text": "We normalize the text by lower casing , removing special characters , etc.9 9https://pytorch.org/text/_modules/ torchtext / data / utils.html", "entities": []}, {"text": "1179For each task , we construct separate 1 - Gram vocabulary ( U ) and initialize a trainable randomly sampled token embedding ( U\u0002de ) fromN(0;1 ) .", "entities": []}, {"text": "Similarly , we randomly initialize a ( ds - max\u0002de ) positional embedding .", "entities": []}, {"text": "The encoder ( \u00a7 2.2 ) takes input a sequence of token vectors ( ds\u0002de ) with added positional vectors .", "entities": []}, {"text": "The input is then projected to key and query vector of sizedk2f1;2;4;8;16;32;64;128;256 g.", "entities": []}, {"text": "For theregula r Transformer setting , we \ufb01x the number of heads hto 8 and the size of value vector dv = de = hthat is 64 .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "For each token at the input , the outputs of attention heads are concatenated to generate a de - sized vector .", "entities": []}, {"text": "For the identi\ufb01able variant of the Transformer encoder , dv = de= 512 , this is equal to ds - maxto keep it identi\ufb01able up to the maximum permissible number of tokens .", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "The outputs of all the heads are then added .", "entities": []}, {"text": "Each token \u2019s contextualized representations ( added head outputs ) are then passed through the feed - forward network ( \u00a7 2.2 ) .", "entities": []}, {"text": "For classi\ufb01cation , we use the encoder layer \u2019s output for the \ufb01rst token and pass it through a linear classi\ufb01cation layer .", "entities": []}, {"text": "In datasets with more than two classes , the classi\ufb01er output issoftmax ed .", "entities": []}, {"text": "In the case of SNLI , we use the shared encoder for both premise and hypothesis ; the output of their \ufb01rst tokens is then concatenated just before the \ufb01nal classi\ufb01cation layer .", "entities": [[4, 5, "DatasetName", "SNLI"]]}, {"text": "We use Adam optimizer , with learning rate = 0.001 , to minimize the cross - entropy loss between the target and predicted label .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [6, 8, "HyperparameterName", "learning rate"], [17, 18, "MetricName", "loss"]]}, {"text": "For all the experiments , we keep the batch size as 256 and train for 20 epochs .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}, {"text": "We report the test accuracy obtained at the epoch with the best validation accuracy .", "entities": [[4, 5, "MetricName", "accuracy"], [13, 14, "MetricName", "accuracy"]]}, {"text": "Numerical rank .", "entities": []}, {"text": "To generate the numerical rank plot on IMDB dataset as shown in \ufb01g . 2 , we train a separate Transformer encoder - based classi\ufb01er .", "entities": [[7, 8, "DatasetName", "IMDB"], [20, 21, "MethodName", "Transformer"]]}, {"text": "For a particulardsvalue , we sample 100 reviews from the dataset with token length \u0015dsand clip each review to the maximum length ds .", "entities": []}, {"text": "The clipping will ensure the number of tokens is dsbefore feeding it to the encoder .", "entities": []}, {"text": "The numerical rank is calculated forT \u2019s obtained from the \ufb01rst head of the encoder .", "entities": []}, {"text": "7 Results and Discussion For the identi\ufb01able variant , similar to \u00a7 3.1 , we plot the numerical rank of Twith input sequence length as shown in \ufb01g . 6 .", "entities": []}, {"text": "Unlike \ufb01g . 2 , where dim\u0000 LN(T)\u0001 linearly increases after ds= 64 , we \ufb01nd the dimension is zero for a larger ds(\u0018380 ) .", "entities": []}, {"text": "The zero dimensional ( left ) null space of Tcon-\ufb01rms there exist no nontrivial solution to the constraint constraint - R2 , i.e. , ~A = f0 g.", "entities": []}, {"text": "Thus , the attention weights Aare identi\ufb01able for a larger range of length of the input sequence .", "entities": []}, {"text": "Figure 6 : Scatter plots in red and blue show rank ( T ) anddim\u0000 LN(T)\u0001 , respectively , for matrices Tobtained from the second phase of attention by feeding IMDB samples to the encoder .", "entities": [[30, 31, "DatasetName", "IMDB"]]}, {"text": "The green line shows the desired rank ( T)for which dim\u0000 LN(T)\u0001 = 0 and thus attention weights are identi\ufb01able .", "entities": [[13, 14, "DatasetName", "0"]]}, {"text": "It is important that the identi\ufb01ability of attention weights should not come at the cost of reduced performance of the model .", "entities": []}, {"text": "To investigate this issue , we compare the performance of the identi\ufb01able Transformer encoder against its regular settings ( \u00a7 6 ) on varied text classi\ufb01cation tasks .", "entities": [[12, 13, "MethodName", "Transformer"]]}, {"text": "For the regular setting , as discussed in \u00a7 4 as one of the solutions , the Transformer can be made identi\ufb01able by decreasing the size of the key vector dk .", "entities": [[17, 18, "MethodName", "Transformer"]]}, {"text": "The rows of the Table 1 corresponding to Con denotes regular Transformer setting with varying size of key vector .", "entities": [[11, 12, "MethodName", "Transformer"]]}, {"text": "We observe the classi\ufb01cation accuracy at the lower dkis comparable or higher than largedkvalues , thus , the enhanced identi\ufb01ability does not compromise with the model \u2019s classi\ufb01cation accuracy .", "entities": [[4, 5, "MetricName", "accuracy"], [28, 29, "MetricName", "accuracy"]]}, {"text": "However , we notice a general performance decline with an increase in the size of the key vector .", "entities": []}, {"text": "We speculate that for simple classi\ufb01cation tasks , the lower - dimensional projection for key and query vector works well .", "entities": []}, {"text": "However , as the task becomes more involved , a higher dimension for the projected subspace could be essential .", "entities": []}, {"text": "Nonetheless , as we do not have strong theoretical \ufb01ndings , we leave this observation for future work .", "entities": []}, {"text": "Another solution to identi\ufb01ability is to increase dvtodeand add the heads \u2019 outputs .", "entities": []}, {"text": "This setting corresponds to the Add rows in the Table 1 .", "entities": []}, {"text": "For key vector size dk= 1 , 2 , and 4 , We \ufb01nd the identi\ufb01able Transformer \u2019s performance is comparable", "entities": [[16, 17, "MethodName", "Transformer"]]}, {"text": "1180Dataset VersionSize of key vector ( dk ) 1 2 4 8 16 32 64 128 256 IMDBCon 0.884 0.888 0.886 0.888 0.846 0.824 0.803 0.788 0.755", "entities": []}, {"text": "Add 0.888 0.885 0.887 0.884 0.886 0.882 0.877 0.832 0.825 TRECCon 0.836 0.836 0.840 0.822 0.823 0.764 0.786 0.706 0.737 Add 0.841 0.842 0.835 0.842 0.841 0.836 0.809 0.809 0.771 SSTCon 0.643 0.625 0.627 0.609 0.603 0.582 0.574 0.573 0.554 Add 0.599 0.618 0.628 0.633 0.628 0.629 0.592 0.581 0.586 SNLICon 0.675 0.674 0.673 0.672 0.662 0.659 0.659 0.655 0.648 Add 0.683 0.677 0.674 0.676 0.673 0.669 0.663 0.664 0.655 YelpCon 0.913 0.911 0.907 0.898 0.879 0.862 0.857 0.849 0.837 Add 0.914 0.915 0.916 0.914 0.915 0.916 0.910 0.909 0.891 DBPediaCon 0.979 0.977 0.977 0.971 0.966 0.961 0.957 0.951 0.949 Add 0.979 0.978 0.979 0.977 0.978 0.973 0.970 0.969 0.964 SogouCon 0.915 0.907 0.898 0.900 0.893 0.888 0.868 0.858 0.838", "entities": []}, {"text": "Add 0.915 0.908 0.906 0.904 0.913 0.914 0.910 0.906 0.899 AG NewsCon 0.906 0.903 0.904 0.904 0.886 0.877 0.870 0.870 0.869 Add 0.902 0.908 0.907 0.906 0.897 0.899 0.901 0.897 0.893 YahooCon 0.695 0.690 0.684 0.664 0.644 0.627 0.616 0.597 0.574 Add 0.697 0.695", "entities": []}, {"text": "0.696 0.693 0.693 0.694 0.688 0.649 0.683 AmazonCon 0.924 0.925 0.923 0.922 0.900 0.892 0.887 0.882 0.873 Add 0.925 0.923 0.925 0.924 0.924 0.920 0.907 0.896 0.889 Table 1 : The test accuracy on varied text classi\ufb01cation tasks spread over ten datasets .", "entities": [[32, 33, "MetricName", "accuracy"]]}, {"text": "Con means the regular concatenation of heads with dv = de = h ,", "entities": []}, {"text": "Add denotes encoder variant where dv = deand outputs of heads are added .", "entities": []}, {"text": "In the regular Transformer encoder Con , the concatenation of dv - sized output of hheads followed by de\u0002delinear transformation can be understood as \ufb01rst doing linear dv\u0002delinear transform of each head and then addition of the transformed output ( \ufb01g . 1 ) .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "In the Add variant , we \ufb01rst add hdv - sized head outputs followed byde\u0002delinear transformation .", "entities": []}, {"text": "to the regular settings .", "entities": []}, {"text": "For dk\u00158 , as a general observation , we \ufb01nd the performance of Add does not drop as drastically as Con with an increase in dk .", "entities": []}, {"text": "This could be due to the larger size of value vector leading to the more number of parameters in Add that compensate for the signi\ufb01cant reduction in the model \u2019s accuracy .", "entities": [[15, 18, "HyperparameterName", "number of parameters"], [30, 31, "MetricName", "accuracy"]]}, {"text": "On the large - scale datasets , we observe that Add performs slightly better than Con .", "entities": []}, {"text": "Intuitively , as shown in \ufb01g . 1 , we can increase the size of value vector to increase the dimension of the space on which each token is projected .", "entities": []}, {"text": "A higher dimensional subspace can contain more semantic information to perform the speci\ufb01c task .", "entities": []}, {"text": "Even though the theoretical analysis shows the possibility of a full row rank of Tand identi\ufb01able attention weights , the Tobtained from a trained model might not contain all the rows linearly independent as dsincreases .", "entities": []}, {"text": "We can explain this from the semantic similarities between words cooccurring together ( Harris , 1954 ) .", "entities": []}, {"text": "The similarity is captured as the semantic relationship , such as dot product , between vectors in a linear space .", "entities": []}, {"text": "As the number of tokens in a sentence , i.e. , dsincreases , it becomes more likely to obtain a token vector from the linear combination of other tokens.8 Conclusion This work probed Transformer for identi\ufb01ability of self - attention , i.e. , the attention weights can be uniquely identi\ufb01ed from the head \u2019s output .", "entities": [[33, 34, "MethodName", "Transformer"]]}, {"text": "With theoretical analysis and supporting empirical evidence , we were able to identify the limitations of the existing study by Brunner et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "We found the study largely ignored the constraint coming from the \ufb01rst phase of self - attention in the encoder , i.e. , the size of the key vector .", "entities": []}, {"text": "Later , we proved how we can utilize dkto make the attention weights more identi\ufb01able .", "entities": []}, {"text": "To give a more concrete solution , we propose encoder variants that are more identi\ufb01able , theoretically as well as experimentally , for a large range of input sequence lengths .", "entities": []}, {"text": "The identi\ufb01able variants do not show any performance drop when experiments are done on varied text classi\ufb01cation tasks .", "entities": []}, {"text": "Future works may analyse the critical impact of identi\ufb01ability on the explainability and interpretability of the Transformer .", "entities": [[16, 17, "MethodName", "Transformer"]]}, {"text": "Acknowledgments This research is supported by A*STAR under its RIE 2020 Advanced Manufacturing and Engineering programmatic grant , Award No . \u2013 A19E2b0098 .", "entities": []}, {"text": "1181References Ror Bellman and Karl Johan \u02daAstr\u00a8om .", "entities": []}, {"text": "1970 .", "entities": []}, {"text": "On structural identi\ufb01ability .", "entities": []}, {"text": "Mathematical biosciences , 7(34):329\u2013339 .", "entities": []}, {"text": "Samuel R Bowman , Gabor Angeli , Christopher Potts , and Christopher D Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A large annotated corpus for learning natural language inference .", "entities": [[6, 9, "TaskName", "natural language inference"]]}, {"text": "arXiv preprint arXiv:1508.05326 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Gino Brunner , Yang Liu , Damian Pascual , Oliver Richter , Massimiliano Ciaramita , and Roger Wattenhofer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "On identi\ufb01ability in transformers .", "entities": []}, {"text": "arXiv preprint arXiv:1908.04211 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zellig S Harris .", "entities": []}, {"text": "1954 .", "entities": []}, {"text": "Distributional structure .", "entities": []}, {"text": "Word , 10(2 - 3):146\u2013162 .", "entities": []}, {"text": "Sarthak Jain and Byron C Wallace .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Attention is not explanation .", "entities": []}, {"text": "arXiv preprint arXiv:1902.10186 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jiasen Lu , Dhruv Batra , Devi Parikh , and Stefan Lee . 2019 .", "entities": []}, {"text": "Vilbert : Pretraining task - agnostic visiolinguistic representations for vision - and - language tasks .", "entities": [[0, 1, "MethodName", "Vilbert"]]}, {"text": "In Advances in Neural Information Processing Systems , pages 13\u201323 .", "entities": []}, {"text": "Andrew L. Maas , Raymond E. Daly , Peter T. Pham , Dan Huang , Andrew Y .", "entities": []}, {"text": "Ng , and Christopher Potts . 2011 .", "entities": []}, {"text": "Learning word vectors for sentiment analysis .", "entities": [[4, 6, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , pages 142\u2013150 , Portland , Oregon , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training .", "entities": []}, {"text": "Andreas Raue , Clemens Kreutz , Thomas Maiwald , Julie Bachmann , Marcel Schilling , Ursula Klingm \u00a8uller , and Jens Timmer .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Structural and practical identi\ufb01ability analysis of partially observed dynamical models by exploiting the pro\ufb01le likelihood .", "entities": []}, {"text": "Bioinformatics , 25(15):1923\u20131929 .", "entities": []}, {"text": "Richard Socher , Alex Perelygin , Jean Wu , Jason Chuang , Christopher D Manning , Andrew Y Ng , and Christopher Potts . 2013 .", "entities": []}, {"text": "Recursive deep models for semantic compositionality over a sentiment treebank .", "entities": []}, {"text": "In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631\u20131642 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008.Ellen M. V oorhees and Dawn M. Tice .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "The TREC-8 question answering track .", "entities": [[2, 4, "TaskName", "question answering"]]}, {"text": "In Proceedings of the Second International Conference on Language Resources and Evaluation ( LREC\u201900 ) , Athens , Greece .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Sarah Wiegreffe and Yuval Pinter .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Attention is not not explanation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 11\u201320 .", "entities": []}, {"text": "Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 .", "entities": []}, {"text": "Character - level convolutional networks for text classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1509.01626 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "1182A Background on Matrices A.1 Span", "entities": []}, {"text": ", Column space and Row space Given a set of vectors V:=fv1;v2 ; : : : ; vng , the span of V , span ( V ) , is de\ufb01ned as the set obtained from all the possible linear combination of vectors inV , i.e. , span ( V):=fnX i=1\u0015ivij\u0015i2R ; i2f1;2;:::;ngg : Thespan ( V ) can also be seen as the smallest vector space that contains the set V. Given a matrix A2Rm\u0002n , the column space ofA , Cs(A ) , is de\ufb01ned as space spanned by its column vectors .", "entities": []}, {"text": "Similarly , the row space of A , Rs(A ) , is the space spanned by the row vectors of A.Cs(A)andRs(A)are the subspaces of the real spaces RmandRn , respectively .", "entities": []}, {"text": "If the row vectors ofAare linearly independent , the Rs(A)will span Rm .", "entities": []}, {"text": "A similar argument holds between Cs(A)and Rn .", "entities": []}, {"text": "A.2 Matrix Rank", "entities": []}, {"text": "The rank of a matrix P(denoted as rank ( P))tells about the dimensions of the space spanned by the row vectors or column vectors .", "entities": []}, {"text": "It can also be seen as the number of linearly independent rows or columns .", "entities": []}, {"text": "The following properties hold rank\u0000 P\u0001 \u0014min\u0010 mp;np\u0011 rank\u0000 P Q\u0001 \u0014min\u0010 rank ( P);rank ( Q)\u0011 : Where , PandQaremp\u0002npandmq\u0002nqdimensional matrices , respectively .", "entities": []}, {"text": "A.3 Null Space The left null space of a mp\u0002npmatrix Pcan be de\ufb01ned as the set of vectors", "entities": []}, {"text": "vLN\u0000 P\u0001", "entities": []}, {"text": "= fvT2R1\u0002mpjvTP= 0g(10 )", "entities": []}, {"text": "If the rows of Pare linearly independent ( Pis full - row rank ) the left null space of Pis zero dimensional .", "entities": []}, {"text": "The only solution to the system of equations v P= 0is trivial , i.e. , v=0 .", "entities": []}, {"text": "The dimensions of the null space , known as nullity , of Pcan be calculated as dim\u0000 LN(P)\u0001 = mp\u0000rank ( P ): ( 11 ) The nullity of Psets the dimensions of the space vlies in .", "entities": []}, {"text": "In \u00a7 3 , we utilize our knowledge of appendix A.2 and appendix A.3 to analyse identi\ufb01ability in a Transformer .", "entities": [[19, 20, "MethodName", "Transformer"]]}]