[{"text": "Proceedings of SemEval-2016 , pages 680\u2013685 , San Diego , California , June 16 - 17 , 2016 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2016 Association for Computational Linguistics UoB - UK at SemEval-2016 Task 1 : A Flexible and Extendable System for Semantic Text Similarity using Types , Surprise and Phrase", "entities": [[20, 22, "TaskName", "Text Similarity"]]}, {"text": "Linking Harish Tayyar Madabushi , Mark Buhagiar , Mark Lee School of Computer Science , University of Birmingham , Birmingham , United Kingdom .", "entities": []}, {"text": "H.T.Madabushi@cs.bham.ac.uk , Buhagiar.Mark.A@gmail.com , M.G.Lee@cs.bham.ac.uk Abstract We present in this paper a system for measuring Semantic Text Similarity ( STS ) in English .", "entities": [[16, 18, "TaskName", "Text Similarity"], [19, 20, "TaskName", "STS"]]}, {"text": "We introduce three novel techniques : the use ofTypes , methods of linking phrases , and the use of a Surprise Factor to generate 8,370 similarity measures , which we then combine using Support Vector and Kernel Ridge Regression .", "entities": []}, {"text": "Our system out performs the State of the Art in SemEval 2015 , and our best performing run achieved a score of .7094", "entities": []}, {"text": "on the 2016 test set as a whole , and over 0.8 on the majority of the datasets .", "entities": []}, {"text": "Additionally , the use of Surprise , Types and phrase linking is not limited to STS and can be used across various Natural Language Processing tasks , while our method of combining scores provides a \ufb02exible way of combining variously generated Similarity Scores .", "entities": [[15, 16, "TaskName", "STS"]]}, {"text": "1 Introduction and Motivation The goal of Semantic Text Similarity ( STS ) is to \ufb01nd the degree of overlap in the meaning of two pieces of text .", "entities": [[8, 10, "TaskName", "Text Similarity"], [11, 12, "TaskName", "STS"]]}, {"text": "This ranges from text fragments that are exact semantic equivalents , to others that have no semantic relation .", "entities": []}, {"text": "STS has a wide variety of applications , including text summarisation ( Aliguliyev , 2009 ) , machine translation ( Kauchak and Barzilay , 2006 ) , and search optimisation ( Sriram et al . , 2010 ) .", "entities": [[0, 1, "TaskName", "STS"], [17, 19, "TaskName", "machine translation"]]}, {"text": "The STS task , which has been set by the SemEval conference for the past number of years ( Agirre et al . , 2014 ; Agirre et al . , 2015 ) , requires that submitted systems assign a score between 0 ( the sentences are on different topics ) and 5 ( the sentences meanexactly the same thing ) that re\ufb02ects how similar two sentences are .", "entities": [[1, 2, "TaskName", "STS"], [42, 43, "DatasetName", "0"]]}, {"text": "Most systems that tackled SemEval \u2019s STS task in previous years have involved three main approaches : The \ufb01rst is text alignment , based on the content words \u2019 meaning ( Sultan et al . , 2015 ; Sultan et al . , 2014b ) .", "entities": [[6, 7, "TaskName", "STS"]]}, {"text": "The second represents text as vectors , which are used to \ufb01nd the similarity score using a vector similarity metric ( such as cosine ) .", "entities": []}, {"text": "Third , machine learning approaches are used to compute multiple lexical , semantic , and syntactic features to classify each sentence pair \u2019s similarity .", "entities": []}, {"text": "We make use of both text alignments and vector representations , while limiting comparisons to words of the same Type ( Section 4.1 ) , a novel concept we introduce in addition to methods of phrase linking ( Section 4.2 ) and establishing common noun importance ( Section 4.3 ) .", "entities": []}, {"text": "These , combined with several different weight combinations we pick for each word Type , provide us with 8,370 semantic relation measures ( Section 5 ) .", "entities": []}, {"text": "The overall algorithm for generating the several similarity measures is presented in Algorithm 1 .", "entities": []}, {"text": "We choose a subset of these measures using methods detailed in Section 6.1 , combine them with a limited set of features and use Support Vector Regression and Kernel Ridge Regression to generate a Similarity Score ( Section 6.2 ) .", "entities": [[35, 36, "MetricName", "Score"]]}, {"text": "Our approach also handles de\ufb01nitions separately from arbitrary sentences , as we observed that their structure is signi\ufb01cantly different .", "entities": []}, {"text": "Since the test data provided this year did not contain a de\ufb01nition data set , this paper focuses on our generic approach , with de\ufb01nition similarity discussed brie\ufb02y in Section 7.680", "entities": []}, {"text": "2", "entities": []}, {"text": "Preprocessing Due to the varied nature of the input presented we perform various data cleaning operations .", "entities": []}, {"text": "We start by expansion of common contractions ( e.g. \u201c is n\u2019t \u201d ) and informal contractions ( e.g. \u201c howz \u201d , \u201c could ve \u201d ) .", "entities": []}, {"text": "We then perform a spell check and hyphen removal , which are conditional , in the sense that a word is not modi\ufb01ed unless the modi\ufb01ed form appears in the other sentence .", "entities": []}, {"text": "All remaining hyphens are replaced by spaces , a method different from those that previously handled hyphens ( Han et al . , 2013 ) .", "entities": []}, {"text": "We also perform case correction , as has been done previously ( H \u00a8anig et al . , 2015 ) , since we observe several instances wherein sentence capitalisation is not suitable for parsing ( e.g. headlines and forums ) .", "entities": []}, {"text": "3 Similarity Measures We use two measures , which are boosted based on different parameters described in Section 4 . 3.1 Alignments The \ufb01rst measure makes use of the aligner developed by Sultan et al .", "entities": []}, {"text": "( 2014a ) , which was used to achieve State of the Art results in 2014 and 2015 ( Sultan et al . , 2014b ; Sultan et al . , 2015 ) .", "entities": []}, {"text": "Our use of the aligner disregards sequences thus making use of the aligner more as a synonym \ufb01nder , with the additional power of the Paraphrase Database ( PPDB ) ( Ganitkevitch et al . , 2013 ) .", "entities": []}, {"text": "3.2 Word Embeddings Word embeddings provide a method of mapping words or phrases to vectors , whose cosine distance represents semantic similarity .", "entities": [[1, 3, "TaskName", "Word Embeddings"], [3, 5, "TaskName", "Word embeddings"], [20, 22, "TaskName", "semantic similarity"]]}, {"text": "They have proved to be powerful in many NLP tasks , and have been used by top ranking systems at SemEval STS ( Sultan et al . , 2015 ; H \u00a8anig et al . , 2015 ) .", "entities": [[21, 22, "TaskName", "STS"]]}, {"text": "We use word2vec1 , with the model trained by Google on the Google News dataset , through its Python interface Gensim2 .", "entities": [[9, 10, "DatasetName", "Google"], [12, 13, "DatasetName", "Google"]]}, {"text": "We make use of word2vec in two distinct ways .", "entities": []}, {"text": "The \ufb01rst is by extracting the mean of the vector representation of each word in a Type and \ufb01nding its cosine similarity between the two sentences .", "entities": []}, {"text": "The second is by adding the word2vec similarity scores of words not aligned within the same Type .", "entities": []}, {"text": "We also 1https://code.google.com/p/word2vec/ 2https://radimrehurek.com/gensim/models/word2vec.htmlprovide the option of disregarding word pairs that have a score of less than 0.3 , a method similar to that by H \u00a8anig et al . ( 2015 ) .", "entities": []}, {"text": "4 Boosting Similarity In this section , we detail the variations used to generate different similarity measures .", "entities": []}, {"text": "These variations are not used simultaneously , but are instead combined as described in Algorithm 1 ( Section 5 ) , which iterates through all possible variations to generate a different similarity score associated with each combination .", "entities": []}, {"text": "4.1 Type Speci\ufb01c Comparison Given a sentence pair , we calculate their similarity based only on how similar corresponding Parts - ofSpeech ( POS ) are , a method previous systems have made use of , either implicitly ( Kashyap et al . , 2014 ; Sultan et al . , 2015 ) or explicitly ( H \u00a8anig et al . , 2015 ) .", "entities": []}, {"text": "We extend this idea by de\ufb01ning what we call word Types , which further subdivide each POS .", "entities": []}, {"text": "A Type represents an abstract concept that several words can share .", "entities": []}, {"text": "Consider the sentence pair \u201c A man is sitting on a stool \u201d , \u201c A boy is sitting on a chair \u201d .", "entities": []}, {"text": "Although the words \u201c man \u201d , \u201c boy \u201d , \u201c stool \u201d and \u201c chair \u201d are all nouns , an effective strategy for comparing these sentences would be to compare the \ufb01rst two and the last two words independently , before then adding up their similarity .", "entities": []}, {"text": "To achieve this we categorise words into different Types , which are then compared across sentences .", "entities": []}, {"text": "In this case , such a categorisation might place the \ufb01rst two into the Type \u201c Person \u201d and the others into the category \u201c Artifact \u201d .", "entities": []}, {"text": "This problem could very easily extend to the problem of Word Sense Disambiguation , which we avoid by use of a heuristic .", "entities": [[10, 13, "TaskName", "Word Sense Disambiguation"]]}, {"text": "We calculate the Type of a noun by the use of WordNet ( Miller , 1995 ) hypernyms : W1is considered a hypernym of W2if\u2200e\u2208W2 , e is an instance of W1 .", "entities": []}, {"text": "We recursively \ufb01nd hypernyms until we reach a manually selected set of concepts ( such as food.n.02 ) .", "entities": []}, {"text": "We manually combine sets of such concepts to de\ufb01ne a Type .", "entities": []}, {"text": "As a concrete example , we combine the WordNet concepts \u201c communication.n.02 \u201d , \u201c food.n.02 \u201d and other similar concepts into the Type \u201c thing r1 \u201d .", "entities": []}, {"text": "As a single word can be part of several Types , based on the particular sense681", "entities": []}, {"text": "of the word , we pick the most frequently occurring Type for each word.3 4.2 Phrase", "entities": []}, {"text": "Linking Consider sentences with the the phrases \u201c Prime Minister \u201d and \u201c Prime Number \u201d .", "entities": []}, {"text": "Although the word \u201c Prime \u201d is present in both sentences , the context in which it is being used makes this irrelevant .", "entities": []}, {"text": "In this particular case , the semantic similarity of the sentences is dependent on the head of the phrase that the word \u201c Prime \u201d is contained in ( i.e. \u201c Minister \u201d and \u201c Number \u201d ) .", "entities": [[6, 8, "TaskName", "semantic similarity"]]}, {"text": "This is also the case with phrases that contain adjectives and adverbs .", "entities": []}, {"text": "We address this by \ufb01nding phrases that consist of adjectives , adverbs and nouns , and varying the importance of the semantic similarity between words that are not the head of that phrase .", "entities": [[21, 23, "TaskName", "semantic similarity"]]}, {"text": "The similarity of each word , that is part of such a phrase , but not the head of the phrase , is additionally weighted in three different ways : The \ufb01rst assigns a zero or one weight based on whether or not the head of the phrase is aligned , the second provides a weight based on the number of words , following this word , that are aligned in the phrase and the third simply ignores the phrase structure .", "entities": []}, {"text": "4.3 Noun Importance Consider the following sentence pairs with relations assigned by human annotators : \u201c A boyis playing a guitar . \u201d , \u201c A man is playing a guitar . \u201d , rel : 3.2 ; and \u201c A man is cutting up a potato . \u201d , \u201c A man is cutting up carrots . \u201d , rel : 2.4 .", "entities": []}, {"text": "Although both pairs of sentences differ by exactly one noun , the \ufb01rst pair was considered to be more closely associated than the second .", "entities": []}, {"text": "We associate this to what we call the \u201c Surprise \u201d and assign a value to this , which we call the \u201c Surprise Factor \u201d .", "entities": []}, {"text": "Surprise is based on the work by Dunning ( 1993 ) , who observed that the assumption of normality of data is invalid as \u201c simple word counts made on a moderate - sized corpus show that words that have a frequency of less than one in 50,000 words make up about 20 - 30 % of typical English language newswire reports .", "entities": []}, {"text": "This \u2018 rare \u2019 quarter of English includes many of the content - bearing words . . . \u201d", "entities": []}, {"text": "3The termination concepts , corresponding Types , de\ufb01nitions for non - noun types , and Weights are provided online at : www.harishmadabushi.com/SemEval2016/Appendix.pdfWe de\ufb01ne the Surprise Factor of a noun or phrase to be proportional to the number of Web Search Hits for that phrase or term , while inversely proportional to the Search Hits in the case of proper nouns .", "entities": []}, {"text": "Intuitively this makes sense , as words that are more common will generate less Surprise , carry less information , and will also be more widely used on the Internet .", "entities": []}, {"text": "We incorporate this idea of Surprise by adding the option of additionally weighting nouns by the total number of Web Search Hits or Results4 .", "entities": []}, {"text": "We de\ufb01ne , Hito be the the number of Web Search Hits for the noun i , HT the total number of hits for all nouns HT = /summationtextN i=0Hi , Nithe fraction of the Search Hits that noun icaptures Ni = Hn HT , and NT the normalised total of all nouns ( C ) in a given sentence NT = /summationtextC i=0Ni .", "entities": []}, {"text": "We de\ufb01ne the Surprise of word i in terms of the above in Equation 1 .", "entities": []}, {"text": "Si = Ni NT(1 ) 5 System Overview Algorithm 1 provides an overview of the system we use to generate the various Similarity Scores , We call each combination that generates a score a \u201c Method \u201d .", "entities": []}, {"text": "We use thirty weights for Types3 , while providing the option of dividing the scores by the number of WordNet Synsets ( UseSSToWeight ) , which captures any dilution due to a word \u2019s different senses .", "entities": []}, {"text": "We also scale word2vec scores by different values .", "entities": []}, {"text": "This gives us a total of 8,370 \u201c Methods \u201d .", "entities": []}, {"text": "In calculating the similarity score , we capture the fraction of each Type that is aligned and scale it by the weight of that Type .", "entities": []}, {"text": "This is captured in Equation 2 where score trepresents the Similarity Score assigned to Type tby either of the measures detailed in Section 3 , count trepresents the number of words of Type tinboth sentences , wtthe weight of Type t in the current iteration , and Tis the total number of Types .", "entities": [[11, 12, "MetricName", "Score"]]}, {"text": "5\u00d7(/summationtextT t=0score t\u00d7wt\u00d72 /summationtextT t=0count t\u00d7wt ) ( 2 ) 4We use the Bing Web Search API : http://www.bing.com/toolbox/bingsearchapi682", "entities": []}, {"text": "Data : Sentence Pairs Result : List of Similarity Scores Initialise list of similarity scores \u201c SimScores \u201d to empty list ; forw in Type - Weights do forn in NounHandleMethod do forav in Adjective - AdverbHandleMethod do forUseSearchHits in [ True , False ] do ifUseSearchHits = =", "entities": []}, {"text": "True then Calculate Similarity Score ( SS ) using Alignments ; Append SS to SimScores ; Continue ; forUseSSToWeight in [ True , False ]", "entities": [[4, 5, "MetricName", "Score"]]}, {"text": "do Calculate Similarity Score ( SS ) using Alignments ; Append SS to SimScores ; forUseWeightCutOff in [ True , False ]", "entities": [[3, 4, "MetricName", "Score"]]}, {"text": "do forVectorCombineMethod in [ UseAlignments , UseMeanVector ] do forScaleVortorSimBy in [ 6 , 5 , 4 , 3 , 2 , 1 , \u201c log \u201d ] do Calculate Similarity Score ( SS ) using Word Embeddings ; Append SS to SimScores ; Return SimScores ( the list of similarity scores ) ; Algorithm 1 : Calculating Semantic Similarity Scores 6 Combining Similarity Scores As described above , we use variations to generate thousands of Similarity Scores , each of which we call a \u201c Method \u201d .", "entities": [[31, 32, "MetricName", "Score"], [36, 38, "TaskName", "Word Embeddings"], [44, 45, "MetricName", "Return"], [58, 60, "TaskName", "Semantic Similarity"]]}, {"text": "Each Method \u2019s performance varies depending on the input .", "entities": []}, {"text": "In this section , we detail the process for combining these Methods , which is performed using either Support Vector Regression ( SVR ) or Kernel Ridge Regression ( KRR ) .", "entities": []}, {"text": "6.1 Picking a Subset of Methods We \ufb01rst select a subset of the Methods , which are then passed on to either the SVR or KRR model .", "entities": []}, {"text": "To do this , each of our Methods is ranked using three metrics with respect to the training set : The \ufb01rst is by use of the Pearson Correlation ( a criterion we call \u201c Method \u201d ) , the second is by the sum of the absolute error between Similarity Scores ( a criterion we call \u201c Error \u201d ) .", "entities": [[27, 29, "MetricName", "Pearson Correlation"], [58, 59, "MetricName", "Error"]]}, {"text": "The third metric aggregates the the rankings from the two criterion described above , and we call this criterion \u201c Combine \u201d .", "entities": []}, {"text": "We select the top 50 methods using one of the three selection criterion .", "entities": []}, {"text": "6.2 Generating Similarity Scores In addition to using scores from the chosen Methods , we add the following features to some of our submitted runs : a ) a binary value to represent whether each of the sentences were case corrected , b ) the length of each of the sentences , c ) the number of contin - uous aligned or unaligned sequences , d ) the maximum and minimum lengths of continuous aligned or unaligned sequences , and e ) a binary value to represent alignments that are non - sequential .", "entities": []}, {"text": "It should be noted that the speci\ufb01c Methods we choose for use in the SVR or KRR will depend on the training data picked .", "entities": []}, {"text": "We found , by testing our system using several different combinations of training data , that the best results were achieved when our system was trained on the headlines data from the years 2015 , 2014 and 2013 .", "entities": []}, {"text": "The method selection criterion , regression model and parameters used for each of the runs submitted are detailed in Table 1 .", "entities": []}, {"text": "Although some of the settings are very similar ( e.g. run2 ) , we noticed that these minor changes translated to signi\ufb01cant differences in performance .", "entities": []}, {"text": "Run Headlines Other Datasets Run1Model : KRR Model : SVR Features :", "entities": []}, {"text": "False Features : False Train : Headlines Train : Headlines Picked : Combine Picked : Combine Kernel : Poly C : 100 Alpha : 50 Epsilon : 0.05 Gamma : 9e-05 Run2Model : SVR Model : SVR Features True Features :", "entities": [[25, 26, "HyperparameterName", "Epsilon"]]}, {"text": "True Train : Headlines Train : Headlines Picked : Method Picked : Method C : 100 C : 100 Epsilon : 0.01 Epsilon : 0.05 Gamma : 9e-05 Gamma : 9e-06 Run3Model : SVR Model : SVR Features True Features :", "entities": [[19, 20, "HyperparameterName", "Epsilon"], [22, 23, "HyperparameterName", "Epsilon"]]}, {"text": "True Train : Headlines Train : Headlines Picked : Method Picked : Combine C : 100 C : 100 Epsilon : 0.01 Epsilon : 0.01 Gamma : 9e-05 Gamma : 9e-06 Table 1 : Parameters and models used for each run .", "entities": [[19, 20, "HyperparameterName", "Epsilon"], [22, 23, "HyperparameterName", "Epsilon"]]}, {"text": "The row Features represents if features were used , Train represents the training data used , and Picked represents the selection criterion ( Method , Error or Combine ) .", "entities": [[25, 26, "MetricName", "Error"]]}, {"text": "7 Finding Similarities between De\ufb01nitions In order to \ufb01nd similarities between de\ufb01nitions , we \ufb01rst identify the word that a de\ufb01nition is de\ufb01ning .", "entities": []}, {"text": "We achieve this by use of OneLook \u2019s reverse dictionary search5 , which returns a number of candidate 5http://www.onelook.com/reverse-dictionary.shtml683", "entities": []}, {"text": "words for a given de\ufb01nition .", "entities": []}, {"text": "For each de\ufb01nition , the similarity of the top 10 candidates is then computed using Word2Vec and \ufb01ve similarity metrics provided by WordNet :", "entities": []}, {"text": "Path distance , Leacock - Chodorow , Wu and Palmer , Jiang - Conrath and Lin .", "entities": []}, {"text": "The \ufb01nal score is scaled between 0 and 5 and averaged across the 10 candidates returned by OneLook .", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "We found this method of calculating similarities between de\ufb01nitions to be very good at telling if two de\ufb01nitions refer to the same word , but not ideally suited for measuring how similar they are .", "entities": []}, {"text": "As a consequence , we found that results were clustered around 0 and 5 .", "entities": [[11, 12, "DatasetName", "0"]]}, {"text": "The system produced a Pearson correlation of 0.69 on the SemEval 2014 de\ufb01nitions data set .", "entities": [[4, 6, "MetricName", "Pearson correlation"]]}, {"text": "8 Results and Analysis Dataset Best Run1 Run2 Run3 Mean .77807 .70940 .70168 .70911 postediting .86690 .81272 .80835 .81333", "entities": []}, {"text": "ques - ques .74705 .56040 .47904 .56451", "entities": []}, {"text": "headlines .82749 .81894 .82352 .81894", "entities": []}, {"text": "plagiarism .84138 .82066 .82406 .81958", "entities": []}, {"text": "ans - ans .69235 .52460 .55217", "entities": []}, {"text": ".52028", "entities": []}, {"text": "Table 2 : Performance on the 2016 STS Test Set We list the performance of our system in Table 2 .", "entities": [[7, 8, "TaskName", "STS"]]}, {"text": "Our system \u2019s poor performance on the ans - ans and ques - ques datasets can be attributed to our choice of training data , which , although well suited for previous years , was not well suited for these datasets .", "entities": []}, {"text": "However , our system produces State of the Art results on the 2015 Test Sets .", "entities": []}, {"text": "A breakdown of each of the run \u2019s performance against the 2015 STS data set is provided in Table 3 .", "entities": [[12, 13, "TaskName", "STS"]]}, {"text": "We note that the results we have reported for previous State of Art for individual data sources are not the results from just the winning system but the State of Art across all Systems for that data source .", "entities": []}, {"text": "Our system also achieves comparable results ( 0.7793 ) to that presented by Sultan et", "entities": []}, {"text": "al . ( 2015 ) ( 0.779 ) on the 2014 STS dataset .", "entities": [[11, 12, "TaskName", "STS"]]}, {"text": "The weighted mean reported by us does not include de\ufb01nitions as we decided to consider them independently .", "entities": []}, {"text": "Table 4 provides a comparison of our system against the previous State of the Art for the STS 2014 data set .", "entities": [[17, 19, "DatasetName", "STS 2014"]]}, {"text": "The overall State of Art across all data sets was reported by Sultan et al .", "entities": []}, {"text": "( 2015 ) basedSourceSt . of ArtRun1", "entities": []}, {"text": "Run2 Run3 Mean 0.8015 0.8086 0.8147 0.8130 ans - std 0.7879 0.7919 0.7965 0.7953 ans - for 0.739 0.7184 0.7137 0.7090 belief 0.7717 0.7703 0.7811 0.7752 headlines 0.8417 0.8508 0.8532 0.8532 images 0.8713 0.8448 0.8617 0.8615 Table 3 : Performance on the 2015 STS Test Set .", "entities": [[43, 44, "TaskName", "STS"]]}, {"text": "SourceSt . of ArtRun1 Run2 Run3 Mean 0.779 0.7714 0.7793 0.7790 de - forum 0.504 0.5435 0.5630 0.5636 de - news 0.785 0.7718 0.7774 0.7756 headlines 0.765 0.8082 0.8055 0.8055 images 0.834 0.8340 0.8492 0.8496 OnWN 0.875 \u2013 \u2013 \u2013 tweet - n 0.792 0.7551 0.7569 0.7573 Table 4 : Performance on the 2014 STS Test Set .", "entities": [[54, 55, "TaskName", "STS"]]}, {"text": "on their 2015 System .", "entities": []}, {"text": "9 Conclusion and Future Work", "entities": []}, {"text": "In this paper we have described the system we used for participation in the SemEval STS Monolingual Task which made use of Types , Phrase Linking , and a method of establishing common noun importance .", "entities": [[15, 16, "TaskName", "STS"]]}, {"text": "In the future , we intend to experiment with including features for each of the Methods during the training phase , other kinds of phrases , and different Type de\ufb01nitions .", "entities": []}, {"text": "We also intend to use the STS data for learning the weights of different Types for use in other NLP applications .", "entities": [[6, 7, "TaskName", "STS"]]}, {"text": "We believe that Types have signi\ufb01cant potential and intend to explore them in greater detail .", "entities": []}, {"text": "Our immediate objectives will be in better de\ufb01ning types , re - categorising common noun Types based on clearer instructions to manual annotators , including \ufb01ner de\ufb01nitions of Types for proper nouns using named entity recognition , and exploring methods of de\ufb01ning Types for verbs , adverbs and adjectives .", "entities": [[33, 36, "TaskName", "named entity recognition"]]}, {"text": "We also intend to explore the use of Types in Question Classi\ufb01cation and Question Answering .", "entities": [[13, 15, "TaskName", "Question Answering"]]}, {"text": "Acknowledgments This work was supported , in part , by the EPSRC , U.K. , under grant number 1576491 , and is also partially funded by the ENDEA VOUR Scholarships Scheme , which may be part-\ufb01nanced by the European Union - European Social Fund.684", "entities": []}, {"text": "References Eneko Agirre , Carmen Banea , Claire Cardie , Daniel Cer , Mona Diab , Aitor Gonzalez - Agirre , Weiwei Guo , Rada Mihalcea , German Rigau , and Janyce Wiebe .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Semeval-2014 task 10 : Multilingual semantic textual similarity .", "entities": [[5, 8, "TaskName", "semantic textual similarity"]]}, {"text": "In Proceedings of the 8th International Workshop on Semantic Evaluation ( SemEval 2014 ) , pages 81\u201391 .", "entities": []}, {"text": "Eneko Agirre , Carmen Banea , et al . 2015 .", "entities": []}, {"text": "Semeval-2015 task 2 : Semantic textual similarity , english , s - panish and pilot on interpretability .", "entities": [[4, 7, "TaskName", "Semantic textual similarity"]]}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , June .", "entities": []}, {"text": "Ramiz M Aliguliyev .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "A new sentence similarity measure and sentence based extractive technique for automatic text summarization .", "entities": [[12, 14, "TaskName", "text summarization"]]}, {"text": "Expert Systems with Applications , 36(4):7764\u20137772 .", "entities": []}, {"text": "Ted Dunning .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "Accurate methods for the statistics of surprise and coincidence .", "entities": []}, {"text": "Comput .", "entities": []}, {"text": "Linguist . , 19(1):61\u201374 , March .", "entities": []}, {"text": "Juri Ganitkevitch , Benjamin Van Durme , and Chris Callison - Burch .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ppdb : The paraphrase database .", "entities": []}, {"text": "In HLT - NAACL , pages 758\u2013764 .", "entities": []}, {"text": "Lushan Han , Abhay L. Kashyap , Tim Finin , James May\ufb01eld , and Jonathan Weese .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Umbc ebiquitycore : .", "entities": []}, {"text": "In Second Joint Conference on Lexical and Computational Semantics ( * SEM ) , Volume 1 : Proceedings of the Main Conference and the Shared Task :", "entities": []}, {"text": "Semantic Textual Similarity , pages 44\u201352 , Atlanta , Georgia , USA , June .", "entities": [[0, 3, "TaskName", "Semantic Textual Similarity"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Christian H \u00a8anig , Robert Remus , and Xose de la Puente . 2015 .", "entities": []}, {"text": "Exb themis : Extensive feature extraction from word alignments for semantic textual similarity .", "entities": [[10, 13, "TaskName", "semantic textual similarity"]]}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 264\u2013268 , Denver , Colorado , June .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Abhay Kashyap , Lushan Han , Roberto Yus , Jennifer Sleeman , Taneeya Satyapanich , Sunil Gandhi , and Tim Finin .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Meerkat ma\ufb01a : Multilingual and cross - level semantic textual similarity systems .", "entities": [[8, 11, "TaskName", "semantic textual similarity"]]}, {"text": "In Proceedings of the 8th International Workshop on Semantic Evaluation ( SemEval 2014 ) , pages 416\u2013423 , Dublin , Ireland , August .", "entities": []}, {"text": "Association for Computational Linguistics and Dublin City University .", "entities": []}, {"text": "David Kauchak and Regina Barzilay .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Paraphrasing for automatic evaluation .", "entities": []}, {"text": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics , pages 455 \u2013 462 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "George A Miller .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Wordnet : a lexical database for english .", "entities": []}, {"text": "Communications of the ACM , 38(11):39\u201341 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Bharath Sriram , Dave Fuhry , Engin Demir , Hakan Ferhatosmanoglu , and Murat Demirbas .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Short text classi\ufb01cation in twitter to improve information \ufb01ltering .", "entities": []}, {"text": "In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval , pages 841\u2013842 . ACM .", "entities": [[6, 7, "DatasetName", "ACM"], [14, 16, "TaskName", "information retrieval"], [20, 21, "DatasetName", "ACM"]]}, {"text": "Md Arafat Sultan , Steven Bethard , and Tamara Sumner .", "entities": []}, {"text": "2014a .", "entities": []}, {"text": "Back to basics for monolingual alignment : Exploiting word similarity and contextual evidence .", "entities": [[8, 10, "TaskName", "word similarity"]]}, {"text": "Transactions of the Association for Computational Linguistics , 2:219\u2013230 .", "entities": []}, {"text": "Md Arafat Sultan , Steven Bethard , and Tamara Sumner .", "entities": []}, {"text": "2014b .", "entities": []}, {"text": "DLS @CU : Sentence similarity from word alignment .", "entities": [[6, 8, "TaskName", "word alignment"]]}, {"text": "In Proceedings of the 8th International Workshop on Semantic Evaluation ( SemEval 2014 ) , pages 241\u2013246 , Dublin , Ireland , August .", "entities": []}, {"text": "Association for Computational Linguistics and Dublin City University .", "entities": []}, {"text": "Md Arafat Sultan , Steven Bethard , and Tamara Sumner .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "DLS @CU : Sentence similarity from word alignment and semantic vector composition .", "entities": [[6, 8, "TaskName", "word alignment"]]}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 148\u2013153 , Denver , Colorado , June .", "entities": []}, {"text": "Association for Computational Linguistics.685", "entities": []}]