[{"text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 5075\u20135086 November 7\u201311 , 2021 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2021 Association for Computational Linguistics5075Paraphrase Generation : A Survey of the State of the Art Jianing Zhou andSuma Bhat University of Illinois at Urbana - Champaign { zjn1746 , spbhat2}@illinois.edu", "entities": []}, {"text": "Abstract This paper focuses on paraphrase generation , which is a widely studied natural language generation task in NLP .", "entities": [[5, 7, "TaskName", "paraphrase generation"]]}, {"text": "With the development of neural models , paraphrase generation research has exhibited a gradual shift to neural methods in the recent years .", "entities": [[7, 9, "TaskName", "paraphrase generation"]]}, {"text": "This has provided architectures for contextualized representation of an input text and generating \ufb02uent , diverse and human - like paraphrases .", "entities": []}, {"text": "This paper surveys various approaches to paraphrase generation with a main focus on neural methods .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}, {"text": "1 Introduction Paraphrases are texts that convey the same meaning while using different words or sentence structures .", "entities": []}, {"text": "The generation of paraphrases is a longstanding problem for natural language learning .", "entities": []}, {"text": "For example , the question How do I improve my English could be equivalently phrased as What is the best way to learn English .", "entities": []}, {"text": "Paraphrasing can be play an important role in language understanding tasks , such as question answering ( Dong et al . , 2017 ; Zhu et al . , 2017 ) , machine translation ( Seraj et al . , 2015 ;", "entities": [[14, 16, "TaskName", "question answering"], [32, 34, "TaskName", "machine translation"]]}, {"text": "Thompson and Post , 2020a ) , and semantic parsing ( Berant and Liang , 2014 ; Cao et al . , 2020 ) .", "entities": [[8, 10, "TaskName", "semantic parsing"]]}, {"text": "And it is also a good way for data augmentation ( Kumar et al . , 2019 ; Gao et", "entities": [[8, 10, "TaskName", "data augmentation"], [11, 12, "DatasetName", "Kumar"]]}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Given a sentence , paraphrase generation aims to create its paraphrases that can have a different wording or different structure from the original sentence , while preserving the original meaning .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}, {"text": "The focus of paraphrase generation has exhibited a gradual shift from classical approaches to more advanced neural approaches in the recent years with the rapid development of various neural models .", "entities": [[3, 5, "TaskName", "paraphrase generation"]]}, {"text": "Neural models have changed the traditional way paraphrase generation is performed and also provided new directions and architectures for the NLP community .", "entities": [[7, 9, "TaskName", "paraphrase generation"]]}, {"text": "While several surveys on the traditional methods and limited neural methods for paraphrase gener - Sentences Paraphrases How do I improve my EnglishWhat is the best way to learn English How far is Earth from SunWhat is the distance between Sun and Earth if at any time in the preparation of this product the integrity of this container is compromised it should not be used .this container should not be used if the product is compromised at any time in preparation .", "entities": []}, {"text": "Table 1 : Examples of paraphrases from available datasets for paraphrase generation .", "entities": [[10, 12, "TaskName", "paraphrase generation"]]}, {"text": "ation have been published ( Metzler et al . , 2011 ; Gupta and Krzy \u02d9zak , 2020 ) , there is no thorough and comprehensive survey on neural methods for paraphrase generation .", "entities": [[31, 33, "TaskName", "paraphrase generation"]]}, {"text": "To our best knowledge , this is the \ufb01rst survey on neural methods for paraphrase generation .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}, {"text": "Therefore , our goal in this paper is to provide a timely survey on paraphrase generation , with a main focus on neural methods .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}, {"text": "In the following section , we will \ufb01rst introduce the most frequently used datasets for paraphrase generation ( Section 2 ) .", "entities": [[15, 17, "TaskName", "paraphrase generation"]]}, {"text": "Then we list the traditional evaluation metrics in Section 3 .", "entities": []}, {"text": "In Section 4 , we present some of the traditional approaches that were used before the neural methods .", "entities": []}, {"text": "Neural models , the main focus of this paper , will be discussed in Section 5 .", "entities": []}, {"text": "After introducing all the methods , we compare the performance of the different models for paraphrase generation in Section 6 .", "entities": [[15, 17, "TaskName", "paraphrase generation"]]}, {"text": "Finally , we identify some research gaps in paraphrase generation .", "entities": [[8, 10, "TaskName", "paraphrase generation"]]}, {"text": "2 Datasets In this section , we describe several datasets that have been extensively used for paraphrase generation .", "entities": [[16, 18, "TaskName", "paraphrase generation"]]}, {"text": "PPDB The paraphrase database ( Ganitkevitch et al . , 2013 ) contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million para-", "entities": []}, {"text": "5076Dataset Parallel Genre Size Gold Size Length Char Len PPDB \" Phrase , words 220,000,000 220,000,000 2.85 16.25 WikiAnswer \" Question 18,000,000 18,000,000 11.43 54.33 MSCOCO \" Description 493,186 493,186 10.48 51.56 Quora \" Question 404,289", "entities": [[25, 26, "DatasetName", "MSCOCO"]]}, {"text": "149,263 11.14 52.89", "entities": []}, {"text": "Twitter URL \" Twitter 2,869,657 116,000 14.80 ParaNMT \" Novels , laws 51,409,585 51,409,585 12.94 59.18 Table 2 : Highlights of primarily used paraphrase generation datasets .", "entities": [[23, 25, "TaskName", "paraphrase generation"]]}, {"text": "Gold Size represents the size of the subset used for paraphrase generation when the original dataset was not used for generation .", "entities": [[10, 12, "TaskName", "paraphrase generation"]]}, {"text": "Length is the the average number of words per sentence and Char Len is the average number of characters per sentence .", "entities": []}, {"text": "phrase patterns that capture meaning - preserving syntactic transformations .", "entities": []}, {"text": "Each paraphrase pair in PPDB contains a set of associated scores including paraphrase probabilities and monolingual distributional similarity scores .", "entities": []}, {"text": "Despite its size and variety , because this dataset only contains phrasal and lexical paraphrases without any sentence paraphrases , it has recently fallen out of use .", "entities": []}, {"text": "WikiAnswer This dataset ( Fader et al . , 2013 ) contains approximately 18 million word - aligned question pairs that are paraphrases .", "entities": []}, {"text": "The word alignments provided by this dataset also relate the synonyms in the paraphrase sentences .", "entities": []}, {"text": "However , all the sentences provided in this dataset are questions , which restricts the paraphrases to only questions .", "entities": []}, {"text": "MSCOCO MSCOCO ( Lin et al . , 2014 ) was originally described as a large - scale object detection dataset .", "entities": [[0, 1, "DatasetName", "MSCOCO"], [1, 2, "DatasetName", "MSCOCO"], [18, 20, "TaskName", "object detection"]]}, {"text": "It contains human - annotated captions of over 120 K images , and each image is associated with \ufb01ve captions from \ufb01ve different annotators .", "entities": []}, {"text": "There are about 500 K pairs of paraphrases in this dataset .", "entities": []}, {"text": "In most cases , annotators describe the most prominent object / action in an image , which makes this dataset suitable for paraphrase - related tasks .", "entities": []}, {"text": "Quora Quora1released a dataset in 2017 , which consists of over 400 K lines of potential question duplicate pairs .", "entities": []}, {"text": "Among these potential question duplicate pairs , there are 150 K question pairs annotated as paraphrases .", "entities": []}, {"text": "For the paraphrase generation task only use these valid paraphrase question pairs are used for training and testing .", "entities": [[2, 4, "TaskName", "paraphrase generation"]]}, {"text": "Like WikiAnswer , this dataset is restricted to questions .", "entities": []}, {"text": "Twitter URL Twitter URL ( Lan et al . , 2017 ) is constructed by collecting large - scale sentential paraphrases from Twitter by linking tweets through 1https://www.kaggle.com/c/ quora - question - pairsshared URLs .", "entities": []}, {"text": "This dataset consists of two subsets , each of which contains both paraphrases and non - paraphrases .", "entities": []}, {"text": "One subset is labeled by human annotators , and the other is labeled automatically .", "entities": []}, {"text": "Only the paraphrase sentence pairs are used for paraphrase generation .", "entities": [[8, 10, "TaskName", "paraphrase generation"]]}, {"text": "Because this dataset includes sentence pairs that are labeled automatically ( as paraphrase or not ) , the annotation is noisy .", "entities": []}, {"text": "ParaNMT ParaNMT ( Wieting and Gimpel , 2018 ) is a dataset of more than 50 million EnglishEnglish sentential paraphrase pairs .", "entities": []}, {"text": "The pairs were generated automatically by using back - translation to translate the non - English side of a large CzechEnglish parallel corpus .", "entities": []}, {"text": "Owing to its recency , it has not been used widely .", "entities": []}, {"text": "3 Evaluation Methods Two general types of evaluation metrics are commonly used to evaluate paraphrase generation : automatic evaluation and human evaluation .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}, {"text": "Automatic Evaluation Several automatic evaluation metrics are used for the evaluation of paraphrase generation .", "entities": [[12, 14, "TaskName", "paraphrase generation"]]}, {"text": "The widely - used metrics include ( 1 ) BLEU ( Papineni et", "entities": [[9, 10, "MetricName", "BLEU"]]}, {"text": "al . , 2002 ) , which was originally developed to evaluate machine translation systems ; ( 2 ) METEOR ( Denkowski and Lavie , 2014 ) , which aims to address BLEU \u2019s weakness of being unable to measure semantic equivalents when applied to low - resource languages and has a better correlation with human judgment at the sentence / segment level than BLEU ; ( 3 ) ROUGE ( Lin , 2004 ) , a recall - based evaluation metric originally developed for text summarization , has also been used to evaluate paraphrase generation .", "entities": [[12, 14, "TaskName", "machine translation"], [19, 20, "DatasetName", "METEOR"], [32, 33, "MetricName", "BLEU"], [64, 65, "MetricName", "BLEU"], [85, 87, "TaskName", "text summarization"], [94, 96, "TaskName", "paraphrase generation"]]}, {"text": "Its versions , ROUGE - N ( computing the n - gram recall ) and ROUGE - L ( focusing on the longest common subsequence ) are mostly used .", "entities": [[15, 18, "MetricName", "ROUGE - L"]]}, {"text": "( 4 ) TER ( Snover", "entities": []}, {"text": "5077et al . , 2006 ) , which was also developed to evaluate machine translation .", "entities": [[13, 15, "TaskName", "machine translation"]]}, {"text": "It measures the number of edits that a human translator would have to perform to change a translation so it exactly matches a reference translation .", "entities": []}, {"text": "A TER score is a value in the range of 0 - 1 , but is frequently presented as a percentage , where lower is better .", "entities": [[10, 11, "DatasetName", "0"]]}, {"text": "Human Evaluation Due to the fact that automatic evaluation metrics mainly focus on the ngram overlaps instead of meaning , human evaluation is used to provide a more accurate and qualitative evaluation of the generated output .", "entities": []}, {"text": "In human evaluation , human annotators are asked to score generated paraphrases along multiple dimensions of quality such as similarity , clarity , and \ufb02uency .", "entities": []}, {"text": "Owing to the manual annotation efforts , human evaluation is naturally more costly compared to automatic evaluation , but more representative of the quality of the generated output .", "entities": []}, {"text": "4 Traditional Approaches In this section , some traditional approaches without neural models will be introduced .", "entities": []}, {"text": "Rule - Based Approaches Rule - based paraphrase generation approaches build on hand - crafted or automatically collected paraphrase rules .", "entities": [[7, 9, "TaskName", "paraphrase generation"]]}, {"text": "In the early works , these rules were mainly hand - crafted ( McKeown , 1983 ) .", "entities": []}, {"text": "Due to the signi\ufb01cant manual efforts , some researchers have sought to collect paraphrase rules automatically ( Lin and Pantel , 2001 ; Barzilay and Lee , 2003 ) .", "entities": []}, {"text": "However , the limitation of the extracting methods has led to the generation of long and complex paraphrase patterns , in turn impacting performance .", "entities": []}, {"text": "Thesaurus - Based Approaches This approach usually generates paraphrases by substituting some words in the source sentences with their synonyms extracted from a thesaurus ( Bolshakov and Gelbukh , 2004 ; Kauchak and Barzilay , 2006 ) .", "entities": []}, {"text": "Thesaurus - based approaches proceed by \ufb01rst extracting all synonyms from a thesaurus for the words to be replaced .", "entities": []}, {"text": "Then the optimal candidate is selected according to the context in the source sentence .", "entities": []}, {"text": "Although simple and effective , this approach is severely limited by the diversity of the generated paraphrases .", "entities": []}, {"text": "SMT - Based Approach This approach is based on statistical machine translation ( SMT ) and is motivated by the fact that paraphrase generation can be seen as a special case of machine translation ( i.e. , monolingual machine translation ) .", "entities": [[10, 12, "TaskName", "machine translation"], [22, 24, "TaskName", "paraphrase generation"], [32, 34, "TaskName", "machine translation"], [38, 40, "TaskName", "machine translation"]]}, {"text": "A machine translation model normally \ufb01nds a best translation ^eof a text in language fto a text in language eby utilizing a statistical translation modelp(fje)and a language model p(e ): ^e = argmax e2e\u0003p(fje)p(e )", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "Applying this idea to paraphrase generation , such a model will \ufb01nd a best paraphrase ^tof a text in the source side sto a text in the target side t obtained as , ^t = argmax t2t\u0003p(sjt)p(t )", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}, {"text": "For instance , ( Wubben et al . , 2010 ) constructed a large - scale parallel corpus containing paraphrases collected from the headlines that appeared in Google News .", "entities": [[27, 28, "DatasetName", "Google"]]}, {"text": "Then they trained a Phrase - Based Machine Translation model ( PBMT ) ( Koehn et al . , 2007 ) on their parallel corpus using the MOSES package .", "entities": [[7, 9, "TaskName", "Machine Translation"], [27, 28, "DatasetName", "MOSES"]]}, {"text": "The trained PBMT is \ufb01nally used to generate paraphrases .", "entities": []}, {"text": "5 Neural Approaches Early works on paraphrasing mainly focused on template - based or statistical machine translation approaches .", "entities": [[15, 17, "TaskName", "machine translation"]]}, {"text": "However , the matching of templates and modeling of a statistical translation model are both challenging tasks .", "entities": []}, {"text": "With the recent advances of neural networks , especially the sequence - to - sequence framework , Seq2Seq models were \ufb01rst use for paraphrase generation by ( Prakash et", "entities": [[17, 18, "MethodName", "Seq2Seq"], [23, 25, "TaskName", "paraphrase generation"]]}, {"text": "al . , 2016 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Their work inspired the wide use of neural models for paraphrase generation .", "entities": [[10, 12, "TaskName", "paraphrase generation"]]}, {"text": "Below we introduce the main approaches based on neural models that are used for paraphrase generation .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}, {"text": "5.1 Encoder - Decoder Architecture Currently , most of the existing paraphrase generation models are based on sequence - to - sequence models consisting of an encoder and a decoder .", "entities": [[11, 13, "TaskName", "paraphrase generation"]]}, {"text": "The encoder will encode the source texts into a contextualized vector representation along with a list of vector representations capturing the semantics of each word and context .", "entities": []}, {"text": "Then , the decoder will generate paraphrases based on the vectors given by the encoder .", "entities": []}, {"text": "5078Encoding Side The main purpose of encoding is to extract the semantic information for the decoder to generate paraphrases .", "entities": []}, {"text": "With the development of various neural models , researchers also have multiple choices for the encoder .", "entities": []}, {"text": "Encoder With a consistent goal of learning better abstract contextualized representation of the input text , several architectures have been explored by researchers .", "entities": []}, {"text": "( Prakash et al . , 2016 ) \ufb01rst utilized a seq2seq model implemented as recurrent neural networks \u2014 long short term memory networks ( LSTMs ) ( Hochreiter and Schmidhuber , 1997)\u2014to process long sequences .", "entities": [[11, 12, "MethodName", "seq2seq"]]}, {"text": "A nonvolutional neural network ( CNN ) has also been used to construct seq2seq models as a CNN has fewer parameters and thus is faster to train ( Vizcarra and Ochoa - Luna , 2020 ) .", "entities": [[13, 14, "MethodName", "seq2seq"]]}, {"text": "The Transformer model ( Vaswani et al . , 2017 ) has shown state - of - the - art performance on multiple text generation tasks .", "entities": [[1, 2, "MethodName", "Transformer"], [23, 25, "TaskName", "text generation"]]}, {"text": "Due to the Transformer \u2019s improved ability to capture long - range dependencies in sentences , ( Wang et al . , 2019 ) utilized a Transformer to construct their seq2seq model .", "entities": [[3, 4, "MethodName", "Transformer"], [26, 27, "MethodName", "Transformer"], [30, 31, "MethodName", "seq2seq"]]}, {"text": "More recently , large language models using transformer architectures have achieved state - ofthe - art results for many NLP tasks while using less supervised data than before .", "entities": []}, {"text": "Therefore , some researchers also utilized large pretrained language models such as GPT-2 ( Radford et al . , 2019 ) and BART ( Lewis et al . , 2020 ) as their encoder - decoder framework ( Witteveen and Andrews , 2019 ; Hegde and Patil , 2020 ; Garg et al . , 2021 ) .", "entities": [[7, 10, "TaskName", "pretrained language models"], [12, 13, "MethodName", "GPT-2"], [22, 23, "MethodName", "BART"]]}, {"text": "Decoding Side At the decoding side , the contextualized representation is used at each decoding step with the vector representation of previously generated words .", "entities": []}, {"text": "Finally , a distribution over the vocabulary is obtained and the word with highest probability will be generated .", "entities": []}, {"text": "This method is greedy decoding .", "entities": []}, {"text": "Besides , a more commonly used method called beam search ( Wiseman and Rush , 2016 ) is used , which identi\ufb01es the k - best paths up to current timestep during decoding .", "entities": []}, {"text": "However , greedy decoding and beam search methods are both generic approaches for all text generation tasks without a speci\ufb01c focus on paraphrase generation .", "entities": [[14, 16, "TaskName", "text generation"], [22, 24, "TaskName", "paraphrase generation"]]}, {"text": "Therefore , with the goal of generating paraphrases and avoiding the words existing in the source sentences , a few blocking mechanisms have been proposed to prevent the decoderfrom generating the same words in the source sentences .", "entities": []}, {"text": "This is also a way to guarantee the diversity of the generated paraphrases and prevent the models from directly copying the input into the output paraphrases ( Niu et al . , 2020 ; Thompson and Post , 2020b ) .", "entities": []}, {"text": "5.2 Improvements Based on Encoder - Decoder Architecture", "entities": []}, {"text": "The numerous attempts that have been made to improve the Encoder - Decoder architecture for paraphrase generation can be broadly categorized into two types based on their focus : A. Model - focused ; andB. Attribute - focused .", "entities": [[15, 17, "TaskName", "paraphrase generation"]]}, {"text": "Next we introduce them respectively with more \ufb01ne - grained divisions .", "entities": []}, {"text": "A. Model - focused Model - focused improvements only aim to utilize various mechanisms to enhance the encoder or the decoder without paying special attention to the attributes of the generated paraphrases ( e.g. , granularity level such as word - level , phrase - level and sentence - level ) .", "entities": []}, {"text": "Attention TheAttention mechanism ( Bahdanau et al . , 2015 ) enables the decoder to focus on some words / phrases that are of high relevance when generating a word .", "entities": []}, {"text": "First , a weight for each token in the source sequence in each timestep is computed to indicate the importance , emphasizing the important information from the input and de - emphasizing the unimportant information .", "entities": []}, {"text": "Given the weight distribution over all the tokens in the source sequence , this extra input vector , the context vector , is provided to the decoder ..", "entities": []}, {"text": "Copy To counter the effect of rare and outof - vocabulary words in neural sequence models , ( Vinyals et al . , 2015 ) proposed a pointer network .", "entities": [[27, 29, "MethodName", "pointer network"]]}, {"text": "A pointer network copies an element from the input sequence directly into the output .", "entities": [[1, 3, "MethodName", "pointer network"]]}, {"text": "Similarly , copy mechanism copies a span of elements from the input sequence decided by attention mechanism directly into the output .", "entities": []}, {"text": "With copy mechanism , the decoder is able to determine whether a generate mode or a copy mode should be used at each timestep .", "entities": []}, {"text": "First introduced by Gu et al .", "entities": []}, {"text": "( 2016 ) for abstractive summarization ,", "entities": [[5, 6, "TaskName", "summarization"]]}, {"text": "Cao et al .", "entities": []}, {"text": "( 2017 ) haev also applied the copy mechanism to paraphrase generation .", "entities": [[10, 12, "TaskName", "paraphrase generation"]]}, {"text": "Despite the advantage of generating well - formed paraphrases by using the copy mechanism , it leads to the undesirable consequence of making a paraphrase contain many of the phrases", "entities": []}, {"text": "5079 in the original sentence and limits diversity .", "entities": []}, {"text": "This calls for a controlled use of the copy mechanism during paraphrase generation .", "entities": [[11, 13, "TaskName", "paraphrase generation"]]}, {"text": "Variational autoencoder ( VAE ) The V AE ( Kingma et al . , 2014 ) is able to learn rich , nonlinear representations for high - dimensional inputs .", "entities": [[0, 2, "MethodName", "Variational autoencoder"], [3, 4, "MethodName", "VAE"], [7, 8, "MethodName", "AE"]]}, {"text": "Provided a latent representation z\u0018N(\u0016;\u001b)with the distribution learned from inputs by the encoder , the V AE decoder is equipped with the ability of producing realistic outputs conditioned on the latent representation and the learned distribution .", "entities": [[16, 17, "MethodName", "AE"]]}, {"text": "The learning is achieved by reconstructing the original input from the latent code z.", "entities": []}, {"text": "Therefore , with the help of V AE , the paraphrase patterns are encoded into the latent representation z\u0018N(\u0016;\u001b ) , which provides the model with control over the capacity of the learned distribution .", "entities": [[7, 8, "MethodName", "AE"]]}, {"text": "Multiple paraphrase patterns and related words / phrases are grouped under the same latent assignment .", "entities": []}, {"text": "Every time we sample a latent code zfrom the distributionN(\u0016;\u001b ) , we get a new paraphrase pattern .", "entities": []}, {"text": "Researchers have explored V AEs with different encoders and decoders .", "entities": []}, {"text": "For examples , Gupta et al .", "entities": []}, {"text": "( 2017 ) implemented the encoder and decoder with LSTMs , whereas the transformer is utilized by Roy and Grangier ( 2019 ) .", "entities": []}, {"text": "Reinforcement Learning As pointed out by ( Ranzato et al . , 2015 ) , a well - known problem of the encoder - decoder architecture is exposure bias : the decoding of current word is conditioned on the gold references during training but on the generated output from the last timestep during testing .", "entities": []}, {"text": "Therefore , the error might be accumulated and propagated when testing .", "entities": []}, {"text": "Another problem lies in the mismatch between the training goal and the evaluation metrics .", "entities": []}, {"text": "While the generated paraphrases are \ufb01nally evaluated automatically using the previously mentioned metrics , the network is trained to maximize the probability of generating the reference paraphrases .", "entities": []}, {"text": "Therefore , minimizing the training loss might not correspond to optimizing the evaluation metric .", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "To address this limitation , reinforcement learning ( RL ) is leveraged .", "entities": []}, {"text": "RL aims to train an agent to interact with the environment with the goal of maximizing its reward .", "entities": [[5, 6, "DatasetName", "agent"]]}, {"text": "Toward \ufb01nding an optimal policy , RL can be used to maximize the reward indicated as a desired evaluation metric or a combination of multiple desired metrics .", "entities": []}, {"text": "Rather than minimizing loss ( the conventional approach ) , Li et al .", "entities": [[3, 4, "MetricName", "loss"]]}, {"text": "( 2018 ) \ufb01rst utilized RL to maximize thereward given by an evaluator which outputs a real value to represent the matching degree between two sentences as paraphrases of each other .", "entities": []}, {"text": "Other reward functions have been explored by researchers , including ROUGE score , perplexity score and language \ufb02uency ( Siddique et al . , 2020 ; Liu et al . , 2020 ) .", "entities": [[13, 14, "MetricName", "perplexity"]]}, {"text": "Generative adversarial networks ( GAN ) Proposed by Goodfellow et", "entities": [[4, 5, "MethodName", "GAN"]]}, {"text": "al . ( 2014 ) , GANs consist of generators and discriminators , where generators try to generate realistic outputs that match the real distribution and discriminators try to distinguish between the samples generated by generators and the samples that are real .", "entities": []}, {"text": "GAN is originally trained by minimax optimization proposed in ( Goodfellow et", "entities": [[0, 1, "MethodName", "GAN"]]}, {"text": "al . , 2014 ) .", "entities": []}, {"text": "However , when GAN is applied in text generation , the traditional training method can not be used because generating discrete words is non - differentiable .", "entities": [[3, 4, "MethodName", "GAN"], [7, 9, "TaskName", "text generation"]]}, {"text": "Therefore , the idea of policy gradient ( Sutton et al . , 1999 ) is leveraged to solve this problem ( Yu et al . , 2017 ) .", "entities": []}, {"text": "With policy gradient applied , discriminators act like the reward function in RL .", "entities": []}, {"text": "Moreover , different discriminators can provide different desired rewards and thus equip the model with the capacity to generating text with different conditions .", "entities": []}, {"text": "Here , a model is usually trained in an adversarial way : generators and discriminators are \ufb01rst pretrained , then generators are trained to maximize the loss of the \ufb01xed discriminators , then generators are \ufb01xed and discriminators are again trained to minimize the loss by provided the real samples and the samples generated by the \ufb01xed generators .", "entities": [[26, 27, "MetricName", "loss"], [44, 45, "MetricName", "loss"]]}, {"text": "For the task of paraphrase generation , different discriminators are designed to distinguish between generated samples and real samples , paraphrases and non - paraphrases ( Yang et al . , 2019 ; Vizcarra and Ochoa - Luna , 2020 ) .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}, {"text": "B. Attribute - focused For attribute - focused improvements , their purpose is to improve the quality of generated paraphrases in some speci\ufb01c aspects such as diversity and also provide control over some attributes of generated paraphrases such as syntax and granularity level .", "entities": []}, {"text": "These attribute - focused works usually use the previously mentioned models as their backbone models .", "entities": []}, {"text": "Based on the backbone models , different mechanisms are applied for different focuses .", "entities": []}, {"text": "Diversity Attempts focusing on diversity aim to generate multiple diverse paraphrases for a given sentence .", "entities": []}, {"text": "Some works control diversity by provid-", "entities": []}, {"text": "5080Quora MSCOCO Models ROUGE-1 ROUGE-2 BLEU-2 BLEU-4 ROUGE-1 ROUGE-2 BLEU-2 BLEU-4 Seq2Seq ( Prakash et", "entities": [[1, 2, "DatasetName", "MSCOCO"], [3, 4, "MetricName", "ROUGE-1"], [4, 5, "MetricName", "ROUGE-2"], [6, 7, "MetricName", "BLEU-4"], [7, 8, "MetricName", "ROUGE-1"], [8, 9, "MetricName", "ROUGE-2"], [10, 11, "MetricName", "BLEU-4"], [11, 12, "MethodName", "Seq2Seq"]]}, {"text": "al . , 2016 ) 57.27 33.04 40.41 24.97 40.11 14.31 47.14 21.65 Seq2Seq - attn ( Prakash et al . , 2016 ) 57.10 32.86 40.49 24.89 41.07 15.26 49.65 23.66 Seq2Seq - attn - copy ( Gu et al . , 2016 ) 61.96 36.07 - - - - - Seq2Seq - VAE ( Gupta et al . , 2017 ) 56.44 30.12 36.89 23.06 40.10 15.18 52.42 25.99 Transformer ( Vaswani et", "entities": [[13, 14, "MethodName", "Seq2Seq"], [32, 33, "MethodName", "Seq2Seq"], [52, 53, "MethodName", "Seq2Seq"], [54, 55, "MethodName", "VAE"], [71, 72, "MethodName", "Transformer"]]}, {"text": "al . , 2017 ) 61.25 34.23 42.91 30.38 - - - Seq2Seq - LBOW ( Fu et al . , 2019 ) 58.79 34.57 42.03 26.17 42.08 16.13 51.14 25.27 RbM ( Li et al . , 2018 ) 64.39 38.11 43.54 - - - - DB(Niu et al . , 2020 ) 67.49 42.33 - - - - - DNPG ( Li et al . , 2019 ) 63.73 37.75 - 25.03 - - - FSET ( Kazemnejad et al . , 2020 ) 66.17 39.55 51.03 33.46 - - - SCSVED ( Chen et al . , 2020 ) 60.28 35.26 41.56 27.37 40.90 15.70 54.35 28.24 SGCP ( Kumar et al . , 2020 ) 66.9 45.0 - - - - - Table 3 : State - of - the - art performance on Quora dataset and MSCOCO dataset .", "entities": [[12, 13, "MethodName", "Seq2Seq"], [112, 113, "DatasetName", "Kumar"], [141, 142, "DatasetName", "MSCOCO"]]}, {"text": "ing control signals to the decoder .", "entities": []}, {"text": "Random pattern embeddings are used by ( Xu et al . , 2018 ) .", "entities": []}, {"text": "( Kumar et al . , 2019 ) utilized a submodular mechanism to maximize submodular functions measuring \ufb01delity and diversity .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "( An and Liu , 2019 ) , ( Chen et al . , 2020 ) and ( Cao and Wan , 2020 ) all generate diverse paraphrases by providing the decoder with different latent patterns as control signal .", "entities": []}, {"text": "Furthermore , ( Cao and Wan , 2020 ) also incorporated their model with a diversity loss to control diversity .", "entities": [[16, 17, "MetricName", "loss"]]}, {"text": "( Liu et al . , 2020 ) use RL with multiple reward functions to generate diverse paraphrases .", "entities": []}, {"text": "One of the reward functions computes ROUGE score between a generated sentence and original sentence , which can focus on the word variations and diversity .", "entities": []}, {"text": "Acting like a reward function in RL , discriminators naturally can be used to provide control over some desired attributes .", "entities": []}, {"text": "( Qian et", "entities": []}, {"text": "al . , 2019 ) utilized multiple generators in GAN to generate multiple diverse paraphrases .", "entities": [[9, 10, "MethodName", "GAN"]]}, {"text": "A generator discriminator is used to distinguish sentences generated by different generators and guarantee the generated paraphrases are diverse enough .", "entities": []}, {"text": "Word - Level Works on word - level paraphrasing mainly focus on generating paraphrases by replacing original words in the source texts with synonyms .", "entities": []}, {"text": "Some works leveraged external linguistic knowledge ( Cao et al . , 2017 ; Lin et al . , 2020 ) .", "entities": []}, {"text": "( Cao et al . , 2017 ) utilized an alignment table capturing many synonym mappings based on the IBM Model ( Chahuneau et al . , 2013 ) .", "entities": []}, {"text": "( Lin et al . , 2020 ) utilized WordNet ( Miller , 1995 ) to retrieve synonyms .", "entities": []}, {"text": "Other works instead proposed special mechanisms to learn a mapping of synonyms ( Ma et al . , 2018 ; Fu et al . , 2019 ) .", "entities": []}, {"text": "For example , ( Ma et al . , 2018 ) utilized retrieved - based method to learn such a mapping .", "entities": []}, {"text": "( Fu et al . , 2019 ) incorporates a novel latentbag - of - word mechanism into seq2seq model for content planning , which mainly provides candidate synonyms for words in the source texts .", "entities": [[18, 19, "MethodName", "seq2seq"]]}, {"text": "However , generating paraphrases only on a word - level makes the quality and diversity of generated paraphrases limited .", "entities": []}, {"text": "Therefore , paraphrasing has also been studied on other granularity level , e.g. syntax level .", "entities": []}, {"text": "Syntax Works in this category explore methods to provide control over the syntax of generated paraphrases .", "entities": []}, {"text": "Basically , all the methods used by previous works can be split into two classes : 1 . Explicit Control and 2 . Implicit Control .", "entities": []}, {"text": "Methods in the \ufb01rst class \ufb01rst encode the syntax tree of an exemplar sentence into a list of vector representations and then feed them into decoder at each timestep when decoding ( Iyyer et al . , 2018 ; Chen et al . , 2019 ; Goyal and Durrett , 2020 ; Kumar et al . , 2020 ) .", "entities": [[52, 53, "DatasetName", "Kumar"]]}, {"text": "These methods can provide explicit control over the syntax of generated paraphrases and thus has better interpretability .", "entities": []}, {"text": "The second class of methods will \ufb01rst learn a distribution over syntax information by V AE .", "entities": [[15, 16, "MethodName", "AE"]]}, {"text": "Then a latent syntax variable sampled from the learned distribution will be fed into decoder at each decoding step ( Chen et al . , 2020 ) .", "entities": []}, {"text": "Although the control provided by this method is implicit , it does not require exemplar sentences and also can group multiple related syntax under the same latent assignment .", "entities": []}, {"text": "Multi - Level Focusing on a single granularity level of paraphrasing still makes generated paraphrases limited .", "entities": []}, {"text": "Therefore , researchers also explore methods to combine multiple granularity levels together .", "entities": []}, {"text": "Such attempts equip their model with the capacity of generating synonyms , substituting phrases and also rearrange sentential structures ( Li et al . , 2019 ; Huang et al . , 2019 ; Kazemnejad et al . ,", "entities": []}, {"text": "5081Dataset Evaluation Models WikiAnswer MSCOCO Quora Twitter ParaNMT BLEU METEOR ROUGE TER Human ( Prakash et al . , 2016 ) \"", "entities": [[4, 5, "DatasetName", "MSCOCO"], [8, 9, "MetricName", "BLEU"], [9, 10, "DatasetName", "METEOR"]]}, {"text": "\" % % % \" \" % \" % ( Gupta et al . , 2017 ) % \" \" % % \" \" % \" \" ( Fu et al . , 2019 ) % \" \" % % \" % \" % % ( Li et al . , 2018 ) % % \" \" % \" \" \" % \" ( Niu et al . , 2020 )", "entities": []}, {"text": "% % \" % \" \" % \" % \" ( Li et al . , 2019 ) \" % \" % % \" % \" % \" ( Kazemnejad et al . , 2020 )", "entities": []}, {"text": "% % \" \" % \" \" \" % \" ( Chen et al . , 2020 ) % \" \" % % \" % \" % % ( Kumar et al . , 2020 )", "entities": [[29, 30, "DatasetName", "Kumar"]]}, {"text": "% % \" % \" \" \" \" % \" Table 4 : Datasets and evaluation metrics used by different works on paraphrase generation .", "entities": [[22, 24, "TaskName", "paraphrase generation"]]}, {"text": "Twitter represents Twitter URL corpus .", "entities": []}, {"text": "Only ( Prakash et al . , 2016 ) used PPDB .", "entities": []}, {"text": "Therefore , we did not include PPDB into this table .", "entities": []}, {"text": "2020 )", "entities": []}, {"text": ".", "entities": []}, {"text": "By using multiple encoders , ( Li et al . , 2019 ) and ( Kazemnejad et al . , 2020 ) both enable their models to capture paraphrasing patterns on different granularity levels .", "entities": []}, {"text": "( Huang et al . , 2019 ) instead utilized the help of external linguistic knowledge from the paraphrase database ( Ganitkevitch et al . , 2013 ) to retrieve and learn word - level and phraselevel paraphrases .", "entities": []}, {"text": "With different methods , both of them successfully combine multiple granularity levels together when generating paraphrases .", "entities": []}, {"text": "6 State - of - the - Art Performance Table 3 shows the ROUGE and BLEU scores of state - of - the - art performance on some most frequently used evaluation corpus in recent years : MSCOCO and Quora .", "entities": [[15, 16, "MetricName", "BLEU"], [37, 38, "DatasetName", "MSCOCO"]]}, {"text": "Due to the facts that different metrics are used in different works , different datasets are used in different works and many of them did not release their codes , Table 3 is not fully \ufb01lled .", "entities": []}, {"text": "However , with most of the table \ufb01lled , we can still have some observations worth mentioning .", "entities": []}, {"text": "First , the use of attention mechanism achieves a close performance on Quora but has a better performance on MSCOCO ( row 1 and 2 ) .", "entities": [[19, 20, "DatasetName", "MSCOCO"]]}, {"text": "Similarly , the simple application of V AE also achieves a close performance on Quora but further improves the performance on MSCOCO ( row4 ) .", "entities": [[7, 8, "MethodName", "AE"], [21, 22, "DatasetName", "MSCOCO"]]}, {"text": "With the copy mechanism , the Seq2Seq model is able to retain some words and thus yields a much better results ( row 3 ) .", "entities": [[6, 7, "MethodName", "Seq2Seq"]]}, {"text": "Transformer ( row 5 ) outperforms all the Seq2Seq - based models without copy mechanism ( row 1,2,4,6 ) , which shows the advantages of Transformer and meanwhile also proves the effectiveness ofcopy mechanism .", "entities": [[0, 1, "MethodName", "Transformer"], [8, 9, "MethodName", "Seq2Seq"], [25, 26, "MethodName", "Transformer"]]}, {"text": "Second , a model that employs RL ( row 7 ) has a great advantage for generating better paraphrases because of the reward provided .", "entities": []}, {"text": "Therefore , a welldesigned optimization goal plays an important role in the task of paraphrase generation .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}, {"text": "Third , a novel decoding algorithm based on large pretrained language models helps to generate better paraphrases at the word level ( row 8) because of the strength of large pretrained language models and the synonyms learned by decoding algorithm .", "entities": [[9, 12, "TaskName", "pretrained language models"], [30, 33, "TaskName", "pretrained language models"]]}, {"text": "Fourth , the attempts to improve paraphrase generation with a special focus on combining multiple granularity levels also yield good performance ( row 9,10 ) .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}, {"text": "When learning to generate paraphrase in word level , phrase level and sentence level at the same time , their models improve the performance on multiple metrics compared with their backbone Transformer model ( row 5 ) .", "entities": [[31, 32, "MethodName", "Transformer"]]}, {"text": "Finally , incorporating syntax control into paraphrase generation will also yield better results at word level and sentence level ( row 11,12 ) .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}, {"text": "Compared with implicit control ( row 11 ) , explicit control has a much better performance ( row 12 ) based on Quora .", "entities": []}, {"text": "It should be noted that most of the works utilize two datasets for experiments ( as shown in Table 4 ) with one of them focusing on question paraphrases and the other focusing on general sentence paraphrases .", "entities": []}, {"text": "Quora is the most popular dataset for question paraphrases .", "entities": []}, {"text": "However , for corpus focusing on general sentnece paraphrases , different works have different choices among MSCOCO , Twitter URL and ParaNMT .", "entities": [[16, 17, "DatasetName", "MSCOCO"]]}, {"text": "MSCOCO is more preferred for less noise compared with Twitter URL and ParaNMT .", "entities": [[0, 1, "DatasetName", "MSCOCO"]]}, {"text": "Therefore , a combination of MSCOCO and Quora is more reasonable .", "entities": [[5, 6, "DatasetName", "MSCOCO"]]}, {"text": "For evaluation metrics , BLEU is the most frequently used one .", "entities": [[4, 5, "MetricName", "BLEU"]]}, {"text": "However , as proposed by ( Niu et al . , 2020 ) , current automatic evaluation metrics are limited for evaluating paraphrase generation", "entities": [[22, 24, "TaskName", "paraphrase generation"]]}, {"text": "5082BLEU Quality Target Generated Paraphrase High High a picture of someone taking a picture of herself a woman taking a picture with a cell phone .", "entities": []}, {"text": "High Low a batter swinging a bat at a baseball a batter swinging a baseball at a bat Low High a man in sunglasses laying on a green kayak .", "entities": []}, {"text": "the man lying on a boat in the water .", "entities": []}, {"text": "Low Low people on a gold course enjoy a few games a group of peaple walking Table 5 : Samples of generated paraphrases and their quality .", "entities": []}, {"text": "Selected from ( Niu et al . , 2020 ) .", "entities": []}, {"text": "because of \u201c curse of BLEU on paraphrase evaluation \u201d .", "entities": [[5, 6, "MetricName", "BLEU"]]}, {"text": "As shown in Table 5 , examples with low BLEU scores might include both relatively good and bad paraphrasing because BLEU scores only measure the overlap between outputs and references .", "entities": [[9, 10, "MetricName", "BLEU"], [20, 21, "MetricName", "BLEU"]]}, {"text": "However , a generated paraphrase might still be a good paraphrase even it is not same with the reference .", "entities": []}, {"text": "Therefore , for evaluation , it is better to combine automatic evaluation metrics and human evaluation together for a more comprehensive evaluation .", "entities": []}, {"text": "7 Conclusion Although recent neural models have shown great advances , state - of - the - art results are still not satisfactory enough .", "entities": []}, {"text": "Therefore , more advanced paraphrasing models still need to be explored .", "entities": []}, {"text": "Below we discuss several potential directions of research that we believe are worth studying .", "entities": []}, {"text": "Pretrained language models Virtually all recent work related to the application of pretrained language models on paraphrase generation is quite naive .", "entities": [[0, 3, "TaskName", "Pretrained language models"], [12, 15, "TaskName", "pretrained language models"], [16, 18, "TaskName", "paraphrase generation"]]}, {"text": "Therefore , we could combine the large pretrained language models with other mechanisms , for example reinforcement learning , V AE and GAN .", "entities": [[7, 10, "TaskName", "pretrained language models"], [20, 21, "MethodName", "AE"], [22, 23, "MethodName", "GAN"]]}, {"text": "Multi - level controllable paraphrase generation Most recent works on multi - level paraphrase generation only focus on word - level paraphrasing and phrase - level paraphrasing .", "entities": [[4, 6, "TaskName", "paraphrase generation"], [13, 15, "TaskName", "paraphrase generation"]]}, {"text": "However , more granularity levels can be incorporated .", "entities": []}, {"text": "We believe it is worthwhile to study the combination of various levels , including word - level , phrase - level , syntaxlevel and sentence - level .", "entities": []}, {"text": "Transfer learning With the goal of generating different surfaces of given sentences while preserving the meaning , text summarization , text simpli\ufb01cation and paraphrase generation are essentially similar .", "entities": [[0, 2, "TaskName", "Transfer learning"], [17, 19, "TaskName", "text summarization"], [23, 25, "TaskName", "paraphrase generation"]]}, {"text": "Therefore , one could utilize transfer learning of these three tasks to improve the performance .", "entities": [[5, 7, "TaskName", "transfer learning"]]}, {"text": "Stylistic paraphrase generation Currently , word- and phrase - substitution in paraphrase gener - ation can not be carefully controlled .", "entities": [[1, 3, "TaskName", "paraphrase generation"]]}, {"text": "Therefore , it is hard to control the style of generated paraphrases .", "entities": []}, {"text": "We believe it is worthwhile to explore methods of incorporating speci\ufb01c styles into generated paraphrases .", "entities": []}, {"text": "For instance , by controlling the types of words and phrases , we can incorporate metaphor and idiomatic expressions into paraphrases ( Zhou et al . , 2021b , a ) , which could also help to enhance creativity and diversity of generated paraphrases .", "entities": []}, {"text": "Evaluation metrics As stated in Section 6 , BLEU scores and other automatic evaluation metrics based on similar principle are not good enough to evaluate paraphrase generation .", "entities": [[8, 9, "MetricName", "BLEU"], [25, 27, "TaskName", "paraphrase generation"]]}, {"text": "Thus there is a need for better automatic evaluation methods .", "entities": []}, {"text": "One possible method is to utilize paraphrase identi\ufb01cation in the automatic evaluation metrics to explicitly provide an evaluation of if the generated sentence and input sentence are paraphrases .", "entities": []}, {"text": "References Zhecheng An and Sicong Liu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Towards diverse paraphrase generation using multi - class wasserstein gan .", "entities": [[2, 4, "TaskName", "paraphrase generation"], [8, 10, "MethodName", "wasserstein gan"]]}, {"text": "arXiv preprint arXiv:1909.13827 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Dzmitry Bahdanau , Kyung Hyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In 3rd International Conference on Learning Representations , ICLR 2015 .", "entities": []}, {"text": "Regina Barzilay and Lillian Lee . 2003 .", "entities": []}, {"text": "Learning to paraphrase : An unsupervised approach using multiple - sequence alignment .", "entities": []}, {"text": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics , pages 16\u201323 .", "entities": []}, {"text": "Jonathan Berant and Percy Liang .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Semantic parsing via paraphrasing .", "entities": [[0, 2, "TaskName", "Semantic parsing"]]}, {"text": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1415 \u2013 1425 .", "entities": []}, {"text": "Igor A Bolshakov and Alexander Gelbukh .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Synonymous paraphrasing using wordnet and internet .", "entities": []}, {"text": "InInternational Conference on Application of Natural Language to Information Systems , pages 312 \u2013 323 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "5083Ruisheng Cao , Su Zhu , Chenyu Yang , Chen Liu , Rao Ma , Yanbin Zhao , Lu Chen , and Kai Yu . 2020 .", "entities": []}, {"text": "Unsupervised dual paraphrasing for two - stage semantic parsing .", "entities": [[7, 9, "TaskName", "semantic parsing"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 6806\u20136817 .", "entities": []}, {"text": "Yue Cao and Xiaojun Wan . 2020 .", "entities": []}, {"text": "Divgan : Towards diverse paraphrase generation via diversi\ufb01ed generative adversarial network .", "entities": [[4, 6, "TaskName", "paraphrase generation"], [8, 11, "MethodName", "generative adversarial network"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : Findings , pages 2411\u20132421 .", "entities": []}, {"text": "Ziqiang Cao , Chuwei Luo , Wenjie Li , and Sujian Li . 2017 .", "entities": []}, {"text": "Joint copying and restricted generation for paraphrase .", "entities": []}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 31 .", "entities": []}, {"text": "Victor Chahuneau , Eva Schlinger , Noah A Smith , and Chris Dyer . 2013 .", "entities": []}, {"text": "Translating into morphologically rich languages with synthetic phrases .", "entities": []}, {"text": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1677\u20131687 .", "entities": []}, {"text": "Mingda Chen , Qingming Tang , Sam Wiseman , and Kevin Gimpel .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Controllable paraphrase generation with a syntactic exemplar .", "entities": [[1, 3, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5972\u20135984 .", "entities": []}, {"text": "Wenqing Chen , Jidong Tian , Liqiang Xiao , Hao He , and Yaohui Jin . 2020 .", "entities": []}, {"text": "A semantically consistent and syntactically variational encoder - decoder framework for paraphrase generation .", "entities": [[11, 13, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics , pages 1186\u20131198 .", "entities": []}, {"text": "Michael Denkowski and Alon Lavie .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Meteor universal : Language speci\ufb01c translation evaluation for any target language .", "entities": [[0, 1, "DatasetName", "Meteor"]]}, {"text": "In Proceedings of the ninth workshop on statistical machine translation , pages 376\u2013380 .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "Li Dong , Jonathan Mallinson , Siva Reddy , and Mirella Lapata . 2017 .", "entities": []}, {"text": "Learning to paraphrase for question answering .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 875\u2013886 .", "entities": []}, {"text": "Anthony Fader , Luke Zettlemoyer , and Oren Etzioni .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Paraphrase - driven learning for open question answering .", "entities": [[6, 8, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1608\u20131618 .", "entities": []}, {"text": "Yao Fu , Yansong Feng , and John P Cunningham .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Paraphrase generation with latent bag of words .", "entities": [[0, 2, "TaskName", "Paraphrase generation"]]}, {"text": "In Advances in Neural Information Processing Systems , pages 13645\u201313656 .", "entities": []}, {"text": "Juri Ganitkevitch , Benjamin Van Durme , and Chris Callison - Burch .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ppdb : The paraphrase database .", "entities": []}, {"text": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 758\u2013764.Silin Gao , Yichi Zhang , Zhijian Ou , and Zhou Yu . 2020 .", "entities": []}, {"text": "Paraphrase augmented task - oriented dialog generation .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 639\u2013649 .", "entities": []}, {"text": "Sonal Garg , Sumanth Prabhu , Hemant Misra , and G Srinivasaraghavan .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Unsupervised contextual paraphrase generation using lexical control and reinforcement learning .", "entities": [[2, 4, "TaskName", "paraphrase generation"]]}, {"text": "arXiv preprint arXiv:2103.12777 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Generative adversarial nets .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 27 .", "entities": []}, {"text": "Curran Associates , Inc. Tanya Goyal and Greg Durrett .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Neural syntactic preordering for controlled paraphrase generation .", "entities": [[5, 7, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 238 \u2013 252 .", "entities": []}, {"text": "Jiatao Gu , Zhengdong Lu , Hang Li , and Victor OK Li . 2016 .", "entities": []}, {"text": "Incorporating copying mechanism in sequence - to - sequence learning .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1631\u20131640 .", "entities": []}, {"text": "Ankush Gupta , Arvind Agarwal , Prawaan Singh , and Piyush Rai . 2017 .", "entities": []}, {"text": "A deep generative framework for paraphrase generation .", "entities": [[5, 7, "TaskName", "paraphrase generation"]]}, {"text": "arXiv preprint arXiv:1709.05074 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Varun Gupta and Adam Krzy \u02d9zak . 2020 .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "An empirical evaluation of attention and pointer networks for paraphrase generation .", "entities": [[9, 11, "TaskName", "paraphrase generation"]]}, {"text": "In International Conference on Computational Science , pages 399\u2013413 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Chaitra Hegde and Shrikumar Patil .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unsupervised paraphrase generation using pre - trained language models .", "entities": [[1, 3, "TaskName", "paraphrase generation"]]}, {"text": "arXiv preprint arXiv:2006.05477 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural computation , 9(8):1735\u20131780 .", "entities": []}, {"text": "Shaohan Huang , Yu Wu , Furu Wei , and Zhongzhi Luan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Dictionary - guided editing networks for paraphrase generation .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 33 , pages 6546\u20136553 .", "entities": []}, {"text": "Mohit Iyyer , John Wieting , Kevin Gimpel , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Adversarial example generation with syntactically controlled paraphrase networks .", "entities": []}, {"text": "InProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1875\u20131885 .", "entities": []}, {"text": "5084David Kauchak and Regina Barzilay .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Paraphrasing for automatic evaluation .", "entities": []}, {"text": "In Proceedings of the Human Language Technology Conference of the NAACL , Main Conference , pages 455\u2013462 .", "entities": []}, {"text": "Amirhossein Kazemnejad , Mohammadreza Salehi , and Mahdieh Soleymani Baghshah . 2020 .", "entities": []}, {"text": "Paraphrase generation by learning how to edit from samples .", "entities": [[0, 2, "TaskName", "Paraphrase generation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 6010 \u2013 6021 .", "entities": []}, {"text": "Diederik P Kingma , Danilo J Rezende , Shakir Mohamed , and Max Welling .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Semi - supervised learning with deep generative models .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2 , pages 3581\u20133589 .", "entities": []}, {"text": "Philipp Koehn , Hieu Hoang , Alexandra Birch , Chris Callison - Burch , Marcello Federico , Nicola Bertoldi , Brooke Cowan , Wade Shen , Christine Moran , Richard Zens , et al . 2007 .", "entities": []}, {"text": "Moses : Open source toolkit for statistical machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions , pages 177\u2013180 .", "entities": []}, {"text": "Ashutosh Kumar , Kabir Ahuja , Raghuram Vadapalli , and Partha Talukdar . 2020 .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "Syntax - guided controlled generation of paraphrases .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 8:329\u2013345 .", "entities": []}, {"text": "Ashutosh Kumar , Satwik Bhattamishra , Manik Bhandari , and Partha Talukdar .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Submodular optimization - based diverse paraphrasing and its effectiveness in data augmentation .", "entities": [[10, 12, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3609\u20133619 .", "entities": []}, {"text": "Wuwei Lan , Siyu Qiu , Hua He , and Wei Xu .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A continuously growing dataset of sentential paraphrases .", "entities": []}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 1224\u20131234 .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Bart : Denoising sequence - to - sequence pretraining for natural language generation , translation , and comprehension .", "entities": [[2, 3, "TaskName", "Denoising"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871\u20137880 .", "entities": []}, {"text": "Zichao Li , Xin Jiang , Lifeng Shang , and Hang Li .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Paraphrase generation with deep reinforcement learning .", "entities": [[0, 2, "TaskName", "Paraphrase generation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3865\u20133878.Zichao Li , Xin Jiang , Lifeng Shang , and Qun Liu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Decomposable neural paraphrase generation .", "entities": [[2, 4, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3403 \u2013 3414 .", "entities": []}, {"text": "Chin - Yew Lin .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Rouge :", "entities": []}, {"text": "A package for automatic evaluation of summaries .", "entities": []}, {"text": "In Text summarization branches out , pages 74\u201381 .", "entities": [[1, 3, "TaskName", "Text summarization"]]}, {"text": "Dekang Lin and Patrick Pantel .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Discovery of inference rules for question - answering .", "entities": []}, {"text": "Natural Language Engineering , 7(4):343\u2013360 .", "entities": []}, {"text": "Tsung - Yi Lin , Michael Maire , Serge Belongie , James Hays , Pietro Perona , Deva Ramanan , Piotr Doll\u00e1r , and C Lawrence Zitnick .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Microsoft coco :", "entities": [[1, 2, "DatasetName", "coco"]]}, {"text": "Common objects in context .", "entities": []}, {"text": "In European conference on computer vision , pages 740\u2013755 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Zibo Lin , Ziran Li , Ning Ding , Hai - Tao Zheng , Ying Shen , Wei Wang , and Cong - Zhi Zhao . 2020 .", "entities": []}, {"text": "Integrating linguistic knowledge to sentence paraphrase generation .", "entities": [[5, 7, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 34 , pages 8368 \u2013 8375 .", "entities": []}, {"text": "Mingtong Liu , Erguang Yang , Deyi Xiong , Yujie Zhang , Yao Meng , Changjian Hu , Jinan Xu , and Yufeng Chen .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A learning - exploring method to generate diverse paraphrases with multi - objective deep reinforcement learning .", "entities": []}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics , pages 2310\u20132321 .", "entities": []}, {"text": "Shuming Ma , Xu Sun , Wei Li , Sujian Li , Wenjie Li , and Xuancheng Ren . 2018 .", "entities": []}, {"text": "Query and output : Generating words by querying distributed word representations for paraphrase generation .", "entities": [[12, 14, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 196\u2013206 .", "entities": []}, {"text": "Kathleen McKeown .", "entities": []}, {"text": "1983 .", "entities": []}, {"text": "Paraphrasing questions using given and new information .", "entities": []}, {"text": "American Journal of Computational Linguistics , 9(1):1\u201310 .", "entities": []}, {"text": "Donald Metzler , Eduard Hovy , and Chunliang Zhang .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "An empirical evaluation of data - driven paraphrase generation techniques .", "entities": [[7, 9, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , pages 546\u2013551 .", "entities": []}, {"text": "George A Miller .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Wordnet : a lexical database for english .", "entities": []}, {"text": "Communications of the ACM , 38(11):39 \u2013 41 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Tong Niu , Semih Yavuz , Yingbo Zhou , Huan Wang , Nitish Shirish Keskar , and Caiming Xiong .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unsupervised paraphrase generation via dynamic blocking .", "entities": [[1, 3, "TaskName", "paraphrase generation"]]}, {"text": "arXiv preprint arXiv:2010.12885 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "5085Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics , pages 311\u2013318 .", "entities": []}, {"text": "Aaditya Prakash , Sadid A Hasan , Kathy Lee , Vivek Datla , Ashequl Qadir , Joey Liu , and Oladimeji Farri .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Neural paraphrase generation with stacked residual lstm networks .", "entities": [[1, 3, "TaskName", "paraphrase generation"], [6, 7, "MethodName", "lstm"]]}, {"text": "arXiv preprint arXiv:1610.03098 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Lihua Qian , Lin Qiu , Weinan Zhang , Xin Jiang , and Yong Yu . 2019 .", "entities": []}, {"text": "Exploring diverse expressions for paraphrase generation .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3164\u20133173 .", "entities": []}, {"text": "Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "OpenAI blog , 1(8):9 .", "entities": []}, {"text": "Marc\u2019Aurelio Ranzato , Sumit Chopra , Michael Auli , and Wojciech Zaremba .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Sequence level training with recurrent neural networks .", "entities": []}, {"text": "arXiv preprint arXiv:1511.06732 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Aurko Roy and David Grangier . 2019 .", "entities": []}, {"text": "Unsupervised paraphrasing without translation .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6033\u20136039 .", "entities": []}, {"text": "Ramtin Mehdizadeh Seraj , Maryam Siahbani , and Anoop Sarkar . 2015 .", "entities": []}, {"text": "Improving statistical machine translation with a multilingual paraphrase database .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1379\u20131390 .", "entities": []}, {"text": "AB Siddique , Samet Oymak , and Vagelis Hristidis .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unsupervised paraphrasing via deep reinforcement learning .", "entities": []}, {"text": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 1800\u20131809 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Matthew Snover , Bonnie Dorr , Richard Schwartz , Linnea Micciulla , and John Makhoul .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "A study of translation edit rate with targeted human annotation .", "entities": []}, {"text": "InProceedings of association for machine translation in the Americas , volume 200 .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "Citeseer .", "entities": [[0, 1, "DatasetName", "Citeseer"]]}, {"text": "Richard S Sutton , David A McAllester , Satinder P Singh , Yishay Mansour , et al . 1999 .", "entities": []}, {"text": "Policy gradient methods for reinforcement learning with function approximation .", "entities": [[0, 3, "TaskName", "Policy gradient methods"]]}, {"text": "In NIPs , volume 99 , pages 1057\u20131063 .", "entities": []}, {"text": "Citeseer .", "entities": [[0, 1, "DatasetName", "Citeseer"]]}, {"text": "Brian Thompson and Matt Post . 2020a .", "entities": []}, {"text": "Automatic machine translation evaluation in many languages via zero - shot paraphrasing .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 90\u2013121.Brian Thompson and Matt Post . 2020b .", "entities": []}, {"text": "Paraphrase generation as zero - shot multilingual translation : Disentangling semantic similarity from lexical and syntactic diversity .", "entities": [[0, 2, "TaskName", "Paraphrase generation"], [10, 12, "TaskName", "semantic similarity"]]}, {"text": "In Proceedings of the Fifth Conference on Machine Translation , pages 561\u2013570 .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "Advances in Neural Information Processing Systems , 30:5998\u20136008 .", "entities": []}, {"text": "Oriol Vinyals , Meire Fortunato , and Navdeep Jaitly . 2015 .", "entities": []}, {"text": "Pointer networks .", "entities": []}, {"text": "In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2 , pages 2692 \u2013 2700 .", "entities": []}, {"text": "Gerson Vizcarra and Jose Ochoa - Luna .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Paraphrase generation via adversarial penalizations .", "entities": [[0, 2, "TaskName", "Paraphrase generation"]]}, {"text": "In Proceedings of the Sixth Workshop on Noisy Usergenerated Text ( W - NUT 2020 ) , pages 249\u2013259 .", "entities": []}, {"text": "Su Wang , Rahul Gupta , Nancy Chang , and Jason Baldridge .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A task in a suit and a tie : paraphrase generation with semantic augmentation .", "entities": [[9, 11, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 33 , pages 7176\u20137183 .", "entities": []}, {"text": "John Wieting and Kevin Gimpel .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Paranmt-50 m : Pushing the limits of paraphrastic sentence embeddings with millions of machine translations .", "entities": [[0, 2, "DatasetName", "Paranmt-50 m"], [8, 10, "TaskName", "sentence embeddings"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 451\u2013462 .", "entities": []}, {"text": "Sam Wiseman and Alexander M Rush . 2016 .", "entities": []}, {"text": "Sequence - to - sequence learning as beam - search optimization .", "entities": []}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1296\u20131306 .", "entities": []}, {"text": "Sam Witteveen and Martin Andrews .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Paraphrasing with large language models .", "entities": []}, {"text": "arXiv preprint arXiv:1911.09661 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sander Wubben , Antal Van Den Bosch , and Emiel Krahmer .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Paraphrase generation as monolingual translation : Data and evaluation .", "entities": [[0, 2, "TaskName", "Paraphrase generation"]]}, {"text": "In Proceedings of the 6th International Natural Language Generation Conference .", "entities": []}, {"text": "Qiongkai Xu , Juyan Zhang , Lizhen Qu , Lexing Xie , and Richard Nock . 2018 .", "entities": []}, {"text": "D - page : Diverse paraphrase generation .", "entities": [[5, 7, "TaskName", "paraphrase generation"]]}, {"text": "arXiv preprint arXiv:1808.04364 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Qian Yang , Dinghan Shen , Yong Cheng , Wenlin Wang , Guoyin Wang , Lawrence Carin , et al . 2019 .", "entities": []}, {"text": "An endto - end generative architecture for paraphrase generation .", "entities": [[7, 9, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3123\u20133133 .", "entities": []}, {"text": "5086Lantao Yu , Weinan Zhang , Jun Wang , and Yong Yu . 2017 .", "entities": []}, {"text": "Seqgan : Sequence generative adversarial nets with policy gradient .", "entities": []}, {"text": "In Proceedings of the AAAI conference on arti\ufb01cial intelligence , volume 31 .", "entities": []}, {"text": "Jianing Zhou , Hongyu Gong , and Suma Bhat .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "PIE : A parallel idiomatic expression corpus for idiomatic sentence generation and paraphrasing .", "entities": [[0, 1, "DatasetName", "PIE"]]}, {"text": "In Proceedings of the 17th Workshop on Multiword Expressions ( MWE 2021 ) , pages 33\u201348 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jianing Zhou , Hongyu Gong , Srihari Nanniyur , and Suma Bhat . 2021b .", "entities": []}, {"text": "From solving a problem boldly to cutting the gordian knot : Idiomatic text generation .", "entities": [[12, 14, "TaskName", "text generation"]]}, {"text": "arXiv preprint arXiv:2104.06541 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shuguang Zhu , Xiang Cheng , Sen Su , and Shuang Lang . 2017 .", "entities": []}, {"text": "Knowledge - based question answering by jointly generating , copying and paraphrasing .", "entities": [[3, 5, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management , pages 2439\u20132442 .", "entities": [[5, 6, "DatasetName", "ACM"], [12, 13, "TaskName", "Management"]]}]