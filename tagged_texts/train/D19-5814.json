[{"text": "Proceedings of the Second Workshop on Machine Reading for Question Answering , pages 98\u2013104 Hong Kong , China , November 4 , 2019 .", "entities": [[9, 11, "TaskName", "Question Answering"]]}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics98Machine Comprehension Improves Domain - Speci\ufb01c Japanese Predicate - Argument Structure Analysis Norio Takahashi Tomohide Shibata\u0003Daisuke Kawahara Sadao Kurohashi Graduate School of Informatics , Kyoto University Yoshida - honmachi , Sakyo - ku , Kyoto , 606 - 8501 , Japan fntakahashi , shibata , dk , kuro g@nlp.ist.i.kyoto-u.ac.jp Abstract To improve the accuracy of predicateargument structure ( PAS ) analysis , large - scale training data and knowledge for PAS analysis are indispensable .", "entities": [[57, 58, "MetricName", "accuracy"]]}, {"text": "We focus on a speci\ufb01c domain , speci\ufb01cally Japanese blogs on driving , and construct two wide - coverage datasets as a form of QA using crowdsourcing : a PAS - QA dataset and a reading comprehension QA ( RC - QA ) dataset .", "entities": [[35, 37, "TaskName", "reading comprehension"]]}, {"text": "We train a machine comprehension ( MC ) model based on these datasets to perform PAS analysis .", "entities": []}, {"text": "Our experiments show that a stepwise training method is the most effective , which pre - trains an MC model based on the RC - QA dataset to acquire domain knowledge and then \ufb01ne - tunes based on the PAS - QA dataset .", "entities": []}, {"text": "1 Introduction To understand the meaning of a sentence or a text , it is essential to analyze relations between a predicate and its arguments .", "entities": []}, {"text": "Such analysis is called semantic role labeling ( SRL ) or predicate - argument structure ( PAS ) analysis .", "entities": [[4, 7, "TaskName", "semantic role labeling"]]}, {"text": "For English , the accuracy of SRL has reached approximately 80%90 % ( Ouchi et al . , 2018 ; He et al . , 2018 ; Strubell et al . , 2018 ; Tan et al . , 2018 ) .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "However , there are many omissions of arguments in Japanese , and the accuracy of Japanese PAS analysis on omitted arguments is still around 50%-60 % ( Shibata et al . , 2016 ; Shibata and Kurohashi , 2018 ; Kurita et al . , 2018 ; Ouchi et al . , 2017 ) .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "A reason for such low accuracy is the shortage of gold datasets and knowledge about PAS analysis , which require a prohibitive cost of creation ( Iida et al . , 2007 ; Kawahara et al . , 2002 ) .", "entities": [[5, 6, "MetricName", "accuracy"]]}, {"text": "From the viewpoint of text understanding , machine comprehension ( MC ) has been actively studied in recent years .", "entities": []}, {"text": "In MC studies , QA datasets consisting of triplets of a document , a question and \u0003The current af\ufb01liation is Yahoo Japan Corporation.its answer are constructed , and an MC model is trained using these datasets ( e.g. , Rajpurkar et al .", "entities": []}, {"text": "( 2016 ) and Trischler et al .", "entities": []}, {"text": "( 2017 ) )", "entities": []}, {"text": ".", "entities": []}, {"text": "MC has made remarkable progress in the last couple of years , and MC models have even exceeded human accuracy in some datasets ( Devlin et al . , 2019 ) .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "However , MC accuracy is not necessarily high for documents that contain anaphoric phenomena and those that need external knowledge or inference ( Mihaylov et al . , 2018 ; Yang et al . , 2018 ) .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "In this paper , we propose a Japanese PAS analysis method based on the MC framework for a speci\ufb01c domain .", "entities": []}, {"text": "In particular , we focus on a challenging task of \ufb01nding an antecedent of a zero pronoun within PAS analysis .", "entities": []}, {"text": "We construct a widecoverage QA dataset for PAS analysis ( PAS - QA ) in the domain and feed it to an MC model to perform PAS analysis .", "entities": []}, {"text": "We also construct a QA dataset for reading comprehension ( RC - QA ) in the same domain and jointly use the two datasets in the MC model to improve PAS analysis .", "entities": [[7, 9, "TaskName", "reading comprehension"]]}, {"text": "We consider the domain of blogs on driving because of the following two reasons .", "entities": []}, {"text": "Firstly , we can construct high - quality QA datasets in a short time using crowdsourcing .", "entities": []}, {"text": "Crowdworkers can interpret driving blog articles based on the traf\ufb01c commonsense shared by the society .", "entities": []}, {"text": "Secondly , if computers can understand driving situations correctly by extracting driving behavior from blogs , it is possible to predict danger and warn drivers to achieve safer transportation .", "entities": []}, {"text": "Our contributions are summarized as follows .", "entities": []}, {"text": "\u000fWe propose an MC - based PAS analysis model and show its superiority to a state - ofthe - art neural model .", "entities": []}, {"text": "\u000fWe construct PAS - QA and RC - QA datasets in the driving domain using crowdsourcing .", "entities": []}, {"text": "\u000fWe improve Japanese PAS analysis by combining the PAS - QA and RC - QA datasets .", "entities": []}, {"text": "992 Related Work 2.1 QA Dataset Construction FitzGerald et al .", "entities": []}, {"text": "( 2018 ) and Michael et al .", "entities": []}, {"text": "( 2018 )", "entities": []}, {"text": "constructed QA - SRL Bank 2.0 and QAMRs using crowdsourcing , respectively .", "entities": [[1, 6, "DatasetName", "QA - SRL Bank 2.0"]]}, {"text": "They asked crowdworkers to generate question - answer pairs that represent a PAS .", "entities": []}, {"text": "These datasets are similar to our PAS - QA dataset , but different in that we focus on omitted arguments and automatically generate questions ( see Section 3.1 ) .", "entities": []}, {"text": "Many RC - QA datasets have been constructed in recent years .", "entities": []}, {"text": "For example , Rajpurkar et al .", "entities": []}, {"text": "( 2016 ) constructed SQuAD 1.1 , which contains 100 K crowdsourced questions and answer spans in a Wikipedia article .", "entities": [[4, 5, "DatasetName", "SQuAD"]]}, {"text": "Rajpurkar et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) updated SQuAD 1.1 to 2.0 by adding unanswerable questions .", "entities": [[4, 5, "DatasetName", "SQuAD"]]}, {"text": "Some RC - QA datasets have been built in a speci\ufb01c domain ( Welbl et al . , 2017 ; Suster and Daelemans , 2018 ; Pampari et al . , 2018 ) .", "entities": []}, {"text": "2.2 Machine Comprehension Models Many MC models based on neural networks have been proposed to solve RC - QA datasets .", "entities": []}, {"text": "For example , Devlin et al .", "entities": []}, {"text": "( 2019 ) proposed an MC model using a language representation model , BERT , which achieved a high - ranked accuracy on the SQuAD 1.1 leaderboard as of September 30 , 2019 .", "entities": [[13, 14, "MethodName", "BERT"], [21, 22, "MetricName", "accuracy"], [24, 25, "DatasetName", "SQuAD"]]}, {"text": "As a previous study of transfer learning of MC models to other tasks , Pan et al .", "entities": [[5, 7, "TaskName", "transfer learning"]]}, {"text": "( 2018 )", "entities": []}, {"text": "pre - trained an MC model using an RC - QA dataset and transfered the pre - trained knowledge to sequence - tosequence models .", "entities": []}, {"text": "They used SQuAD 1.1 as the RC - QA dataset and experimented on translation and summarization .", "entities": [[2, 3, "DatasetName", "SQuAD"], [15, 16, "TaskName", "summarization"]]}, {"text": "While they used different models for pre - training and \ufb01ne - tuning , we use the same MC model by constructing PAS - QA and RC - QA datasets in the same QA form .", "entities": []}, {"text": "3 QA Dataset Construction We construct PAS - QA and RC - QA datasets in the driving domain .", "entities": []}, {"text": "Both the QA datasets consist of triplets of a document , a question and its answer as in existing RC - QA datasets .", "entities": []}, {"text": "We employ crowdsourcing to create large - scale datasets in a short time .", "entities": []}, {"text": "Figure 1and Figure 2show examples of our PAS - QA and RC - QA datasets .", "entities": []}, {"text": "3.1 PAS - QA Dataset We construct a PAS - QA dataset in which a question asks an omitted argument for a predicate .", "entities": []}, {"text": "We /g1566 / g39 / g82 / g70 / g88 / g80 / g72 / g81 / g87 / g29 /g2958\u306f\u53f3 \u2f9e \u7dda\u306b\u79fb\u52d5\u3057\u305f \u3002 /g11 / g44 / g80 / g82 / g89 / g72 / g71 / g3 / g87 / g82 / g3 / g87 / g75 / g72 / g3 / g85 / g76 / g74 / g75 / g87 / g3 / g79 / g68 / g81 / g72 / g17 / g12 / g3 / g3 /g11 / g791 / g2315 / g12 / g3 / g2434 / g2421 / g2401 / g2449 / g2459 / g1588 / g2385 \u2f92 \u305f / g1563 /g11 / g11 / g791 / g16 / g49 / g50 / g48 / g12 / g3 /g86 / g68 / g90 / g87 / g75 / g72 / g3 / g85 / g72 / g68 / g85 / g89 / g76 / g72 / g90 / g3 / g80 / g76 / g85 / g85 / g82 / g85 / g17 / g12 /g1566 / g52 / g88 / g72 / g86 / g87 / g76 / g82 / g81 / g3 / g29 / g3 /g110 \u2f92", "entities": []}, {"text": "\u305f / g124\u306e\u4e3b\u8a9e\u306f\u4f55\u304b\ufe16 /g11 / g58 / g75 / g68 / g87 / g3 / g76 / g86 / g3 / g87 / g75 / g72 / g3 / g86 / g88 / g69 / g77 / g72 / g70 / g87 / g3 / g82 / g73 / g3 / g110 /g86 / g68 / g90 / g124 / g34 / g12 /g1566 / g36 / g81 / g86 / g90 / g72 / g85 / g3 / g29 /g2958 /g11 / g44 / g12Figure 1 : An example of PAS - QA dataset .", "entities": []}, {"text": "/g1566 /g39 / g82 / g70 / g88 / g80 / g72 / g81 / g87 / g3 / g29 /g2958\u306e \u2f9e \u306e\u524d\u3092 /g2434 / g2390 / g2401 / g2346 / g2365 / g2334 / g2315 / g2338 / g2334 / g2788 / g3012 /g2747 / g2315 / g3026 / g2338 / g2341 / g2307 / g2334 / g1563 /g11 / g36 / g3 / g83 / g82 / g79 / g76 / g70 / g72 / g3 / g82 / g73 / g73 / g76 / g70 / g72 / g85 / g3 / g86 / g87 / g85 / g68 / g71 / g71 / g79 / g76 / g81 / g74 / g3 /g75 / g76 / g86 / g3 / g69 / g76 / g78 / g72 /g90 / g68 / g86 / g3 / g85 / g88 / g81 / g81 / g76 / g81 / g74 / g3 / g76 / g81 / g3 / g73 / g85 / g82 / g81 / g87 / g3 / g82 / g73 / g3 / g80 / g92 / g3 / g70 / g68 / g85 / g17 / g12 /g1566 / g52 / g88 / g72 / g86 / g87 / g76 / g82 / g81 / g3 / g29 / g3 /g2788 / g3012 / g2747\u306f\u4f55\u306b\u4e57\u3063\u3066\u3044\u305f\ufe16 /g11 / g58 / g75 / g68 / g87 / g3 / g90 / g68 / g86 / g3 / g87 / g75 / g72 / g3 / g83 / g82 / g79 / g76 / g70 / g72 / g3 / g82 / g73 / g73 / g76 / g70 / g72 / g85 / g3 / g85 / g76 / g71 / g76 / g81 / g74 / g34 / g12 /g1566 / g36 / g81 / g86 / g90 / g72 / g85 / g3 / g29 /g2434 / g2390 / g2401 /g11 / g75 / g76 / g86 / g3 / g69 / g76 / g78 / g72 / g12 Figure 2 : An example of RC - QA dataset .", "entities": []}, {"text": "focus on the gacase ( nominative ) , the wocase ( accusative ) , and the nicase ( dative ) , which are targeted in the Japanese PAS analysis literature ( Shibata et al . , 2016 ; Shibata and Kurohashi , 2018 ; Kurita et al . , 2018 ; Ouchi et al . , 2017 ) .", "entities": []}, {"text": "As a source corpus , we use blog articles included in the Driving Experience Corpus ( Iwai et al . ,2019 ) .", "entities": []}, {"text": "We \ufb01rst detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP1to the corpus .", "entities": []}, {"text": "KNP tends to overgenerate such predicates , but most erroneous ones are \ufb01ltered out by the following crowdsourcing step .", "entities": []}, {"text": "We extract the sentence that contains the predicate and preceding three sentences as a document .", "entities": []}, {"text": "Then , we automatically generate a question using the following template for nominative .", "entities": []}, {"text": "\u000f\u02a6\u095c\u07a0\u02a7\u0377\u0913\u07a0\u0378\u053f\u0354\u0281 ( What is the subject of [ predicate ] ? )", "entities": []}, {"text": "All the question templates of PAS - QA datasets are shown in Table 1 .", "entities": []}, {"text": "We ask crowdworkers to choose one from answer choices , which consist of nouns extracted from the document and special symbols , 1http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP", "entities": []}, {"text": "100Case Question Nominative\u02a6\u095c\u07a0\u02a7\u0377\u0913\u07a0\u0378\u053f\u0354\u0281 ( What is the subject of [ predicate ] ? )", "entities": []}, {"text": "Accusative\u0293\u0293\u039b\u02a6\u095c\u07a0\u02a7 \u027a\u0377\u0293\u0293\u0374\u0cd6\u0394\u038b\u0377\u0378\u053f\u0354\u0281 ( What is the accusative of [ predicate ] ? )", "entities": []}, {"text": "Dative\u0293\u0293\u0374\u02a6\u095c\u07a0\u02a7 \u027a\u0377\u0293\u0293\u0374\u0cd6\u0394\u038b\u0377\u0378\u053f\u0354\u0281 ( What is the dative of [ predicate ] ? )", "entities": []}, {"text": "Table 1 : Question templates of PAS - QA datasets .", "entities": []}, {"text": "\u201c author , \u201d \u201c other , \u201d and \u201c not sure . \u201d", "entities": []}, {"text": "The details of this procedure are described in the appendix .", "entities": []}, {"text": "We generated questions from 2,146 blog articles .", "entities": []}, {"text": "We asked \ufb01ve crowdworkers per question using Yahoo ! crowdsourcing2 .", "entities": []}, {"text": "We adopted triplets with three or more votes if they are not \u201c not sure . \u201d", "entities": []}, {"text": "For accusative and dative PAS - QA questions , we adopted triplets if they are \u201c other . \u201d", "entities": []}, {"text": "In this case , there is not any antecedent of a zero pronoun in a document , and the answer is \u201c NULL . \u201d", "entities": []}, {"text": "For nominative PAS - QA questions , we did not adopt triplets if they are \u201c other \u201d because a nominative always exists as a noun in a document or \u201c author . \u201d", "entities": []}, {"text": "In addition , because \u201c author \u201d is not explicitly expressed in the document , we add a sentence \u201c \u0bb6\u0900 \u0378\u048e\u053c\u0377\u0e08\u09b7\u039b\u097b\u0356\u0387\u0360\u0368\u027b \u201d ( The author wrote the following document . ) to the beginning of the document to deal with \u201c author \u201d in MC models .", "entities": []}, {"text": "We record the answers as spans in a document or NULL .", "entities": []}, {"text": "We randomly extracted 100 questions for each case from the PAS - QA dataset and judged whether we can answer them .", "entities": []}, {"text": "As a result , 97 % nominative , 87 % accusative and 68 % dative questions were answerable .", "entities": []}, {"text": "For accusative and dative , we checked all the questions and chose answerable ones .", "entities": []}, {"text": "Finally , we created 12,468 nominative , 3,151 accusative and 1,069 dative triplets including 476 accusative and 126 dative questions whose answers are NULL .", "entities": []}, {"text": "It took approximately 32 hours and approximately 210,000 JPY to create this dataset .", "entities": []}, {"text": "3.2 RC - QA Dataset We construct a driving - domain RC - QA dataset in the same way as SQuAD 1.1 .", "entities": [[20, 21, "DatasetName", "SQuAD"]]}, {"text": "We extract a document from the Driving Experience Corpus and ask three crowdworkers to write questions and their answers about the document .", "entities": []}, {"text": "After that , we ask another \ufb01ve crowdworkers to answer a question to validate its answerability and adopt questions with three or more same answers .", "entities": []}, {"text": "2https://crowdsourcing.yahoo.co.jp/Nominative Accusative Dative Other # Questions 41 28 8 123 Ratio 20.5 % 14.0 % 4.0 % 61.5 % Ratio ( Omission ) ( 5.0 % ) ( 2.5 % ) ( 0.5 % ) \u02b5", "entities": []}, {"text": "Table 2 : Classi\ufb01cation of questions in the RC - QA dataset .", "entities": []}, {"text": "Training method Dataset MC - single PAS - QA Joint trainingMC - merged PAS - QA + RC - QA MC - stepwise RC - QA ! PAS - QA Table 3 : Three training methods for PAS analysis .", "entities": []}, {"text": "As a result , we obtained 20,007 RC - QA triplets from 5,146 blog articles .", "entities": []}, {"text": "It took approximately 60 hours and approximately 180,000 JPY to create this dataset .", "entities": []}, {"text": "We randomly extracted 200 questions from the RC - QA dataset and judged the question types .", "entities": []}, {"text": "The result is shown in Table 2 .", "entities": []}, {"text": "A question was classi\ufb01ed according to whether it is a question asking for any argument of nominative , accusative or dative , and if applicable , whether it is an omission or not .", "entities": []}, {"text": "As shown in Table 2 , the RC - QA dataset contains nearly 40 % of questions asking arguments of nominative , accusative and dative , and a few questions asking for omitted arguments , which are similar to the PAS - QA dataset .", "entities": []}, {"text": "There are various other questions asking for arguments other than nominative , accusative and dative , and questions using why and how .", "entities": []}, {"text": "4 PAS Analysis Based on a Machine Comprehension Model We analyze PAS based on the MC model on our constructed PAS - QA dataset .", "entities": []}, {"text": "Each question in the PAS - QA dataset asks an omitted argument and has an answer that is expressed as a span in the given document or NULL .", "entities": []}, {"text": "Because the PAS - QA dataset has the same structure as existing MC datasets including NULL , such as SQuAD 2.0 , we can employ an existing state - of - the - art MC model that answers a span in the document or NULL .", "entities": [[19, 20, "DatasetName", "SQuAD"]]}, {"text": "We refer to the method of MC training based only on the PAS - QA dataset as MC - single .", "entities": []}, {"text": "We also propose two joint training methods that use both the PAS - QA and RC - QA datasets : MCmerged andMC - stepwise , as described in Table3 .", "entities": []}, {"text": "The purpose of these joint training methods is to verify whether domain knowledge can be learned from the RC - QA dataset and whether it is", "entities": []}, {"text": "101\u0279\u0279\u0279 Train Development Test Nominative 11,359 544 565 Accusative 2,756 199 196 Dative 967 50 52 Table 4 : Split of the PAS - QA dataset .", "entities": []}, {"text": "effective in improving the accuracy of PAS analysis .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "In MC - merged , the PAS - QA and RC - QA datasets are just merged and used for training .", "entities": []}, {"text": "In MC - stepwise , the RC - QA dataset is used for pretraining , and this pre - trained model is \ufb01ne - tuned using the PAS - QA dataset .", "entities": []}, {"text": "5 Experiments We conduct PAS analysis experiments of our MCsingle / merged / stepwise methods using the PASQA and RC - QA datasets .", "entities": []}, {"text": "We also compare our methods with the neural network - based PAS analysis model ( Shibata and Kurohashi , 2018 ) ( hereafter , NN - PAS ) , which achieved the state - of - theart accuracy on Japanese PAS analysis .", "entities": [[37, 38, "MetricName", "accuracy"]]}, {"text": "5.1 Experimental Settings We adopt BERT ( Devlin et al . , 2019 ) as an MC model .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "We split the triplets in the PAS - QA dataset as shown in Table 4 .", "entities": []}, {"text": "All sentences in these datasets are preprocessed using the Japanese morphological analyzer , JUMAN++3 .", "entities": []}, {"text": "We trained a Japanese pre - trained BERT model using Japanese Wikipedia , which consists of approximately 18 million sentences .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "The input sentences were segmented into words by JUMAN++ , and words were broken into subwords by applying BPE ( Sennrich et al . , 2016 ) .", "entities": [[18, 19, "MethodName", "BPE"]]}, {"text": "The parameters of BERT are the same as English BERT BASE .", "entities": [[3, 4, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"], [10, 11, "MethodName", "BASE"]]}, {"text": "The number of epochs for the pre - training was 30 .", "entities": [[1, 4, "HyperparameterName", "number of epochs"]]}, {"text": "The state - of - the - art baseline PAS analyzer , NN - PAS , was trained using the existing PAS dataset , KWDLC4(Kyoto University Web Document Leads Corpus ) , as described in Shibata and Kurohashi ( 2018 ) .", "entities": []}, {"text": "We also trained an NN - PAS model using the PAS - QA dataset in addition to KWDLC ( hereafter , NN - PAS\u2032 ) .", "entities": []}, {"text": "For this training , the PAS - QA dataset was converted to the same format as KWDLC , where questions are deleted , and only answers are used .", "entities": []}, {"text": "The PAS - QA test data is used to compare the baseline methods with the proposed methods .", "entities": []}, {"text": "As 3http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN++ 4http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KWDLCTraining method PAS RC NOM ACC DAT NN - PAS - - 0.39 0.38 0.29 NN - PAS\u2032 \u2713 - 0.74 0.45 0.32 MC - single \u2713 - 0.76 0.52 0.37 MC - merged \u2713 \u2713 0.76 0.52 0.43 MC - stepwise \u2713 \u2713 0.76 0.53 0.51 Table 5 : PAS - QA test results of MC models and NNPAS models .", "entities": [[7, 8, "MetricName", "ACC"]]}, {"text": "\u201c PAS \u201d and \u201c RC \u201d denote the use of the PAS - QA and RC - QA datasets , respectively .", "entities": []}, {"text": "\u201c NOM \u201d , \u201c ACC \u201d and \u201c DAT \u201d denote the EM scores of nominative , accusative and dative , respectively .", "entities": [[5, 6, "MetricName", "ACC"], [13, 14, "MetricName", "EM"]]}, {"text": "an evaluation measure , EM ( Exact Match ) is used for all the MC models .", "entities": [[4, 5, "MetricName", "EM"], [6, 8, "MetricName", "Exact Match"]]}, {"text": "EM is de\ufb01ned as ( the number of questions in which the system answer matches the gold answer in the dataset ) / ( the number of questions in the entire dataset ) .", "entities": [[0, 1, "MetricName", "EM"]]}, {"text": "For each experimental condition , training and testing were conducted \ufb01ve times , and the average scores were calculated .", "entities": []}, {"text": "5.2 Results and Discussion Table 5lists evalution results of the NN - PAS models and the MC - single / merged / stepwise models .", "entities": [[6, 7, "DatasetName", "evalution"]]}, {"text": "First , NN - PAS\u2032signi\ufb01cantly outperformed NNPAS , and thus the construction of the domainspeci\ufb01c PAS - QA dataset was effective in domain adaptation of the NN - PAS model .", "entities": [[22, 24, "TaskName", "domain adaptation"]]}, {"text": "Furthermore , our proposed MC- * models outperfomed NN - PAS\u2032. For the joint training models , MCstepwise was better than MC - single for the accusative and dative cases .", "entities": []}, {"text": "MC - merged was inferior to MC - stepwise .", "entities": []}, {"text": "We compared the results of MC - single and MCstepwise .", "entities": []}, {"text": "In examples shown in Figures 3 and 4 , only the outputs of MC - stepwise were correct .", "entities": []}, {"text": "We found some cases that MC - stepwise successfully captured knowledge in the driving domain .", "entities": []}, {"text": "In the example shown in Figure 4 , the correspondence between \u201c \u0854\u039b \u09cd\u0355\u0394 \u201d ( climb up the slope ) and \u201c \u0854\u039b \u04fd\u0351\u0394 \u201d ( going up the slope ) can be recognized .", "entities": []}, {"text": "MC - merged \u2019s answer \u201c \u0854\u0c93 \u201d ( the hill road ) , which has a coreference relation with \u201c \u0854 ( \u201d the slope ) , looked correct although \u201c \u0854 ( \u201d the slope ) was the only answer from crowdsourcing .", "entities": []}, {"text": "Supplying multiple answers considering coreference relations is our future work .", "entities": []}, {"text": "From these results , we think that it is important to use an RC - QA dataset to acquire domain knowledge , and suggest that it is better to construct both PAS - QA and RCQA datasets to develop a PAS analyzer for a new", "entities": []}, {"text": "102 /g1566 / g39 / g82 / g70 / g88 / g80 / g72 / g81 / g87\ufe13 \u5f7c\u3001\u300c \u2f9e \u3092\u8fd4\u3059\u6642\u30ac\u30bd\u30ea\u30f3\u3092\u6e80\u30bf\u30f3\u306b\u3059\u308b\u306e\u304b\u3001\u3060\u3044\u3076 /g3026 / g2338 / g2334 / g2314 / g2376 / g2776 / g2345 / g2318 / g2345 / g2338 / g2334 / g2345 / g2304 / g1615 /g11 / g43 / g72 / g3 / g86 / g68 / g76 / g71 / g15 / g3 / g110 / g58 / g75 / g72 / g81 / g3 / g90 / g72 / g3 / g85 / g72 / g87 / g88 / g85 / g81 / g3 / g87 / g75 / g76 / g86 / g3 / g70 / g68 / g85 / g15 / g3 / g90 / g72 / g3 / g75 / g68 / g89 / g72 / g3 / g87 / g82 / g3 / g73 / g76 / g79 / g79 / g3 / g88 / g83 /g3 /g90 / g76 / g87 / g75 / g3 / g74 / g68 / g86 / g82 / g79 / g76 / g81 / g72 / g17 / g3 / g58 / g72 / g3 / g85 / g68 / g81 / g3 / g68 / g3 / g79 / g82 / g87 /g86 / g82 / g3 / g90 / g72 / g3 / g85 / g68 / g81 / g3 / g82 / g88 / g87 / g3 / g82 / g73 / g3 / g76 / g87 / g17 / g124 / g12 /g2506 / g2861 / g2491 / g1562 / g1614 / g4459 / g2540 / g2798 / g2307 / g2314 / g2376 / g3059 / g2634 / g2350 / g2429 / g2455 / g1588 / g2426 / g2459 / g2461 / g2342 / g3026 / g2380 / g2309 / g2331 / g1615 /g11 / g55 / g75 / g72 / g3 / g83 / g68 / g86 / g86 / g72 / g81 / g74 / g72 / g85 / g3 / g86 / g68 / g76 / g71 / g15 / g3 / g110 / g37 / g72 / g70 / g68 / g88 / g86 / g72 / g3 / g44 / g87 / g3 / g76 / g86 / g3 / g68 / g3 / g90 / g68 / g86 / g87 / g72 / g3 / g87 / g82 / g3 /g70 / g82 / g81 / g86 / g88 / g80 / g72 / g3 / g74 / g68 / g86 / g82 / g79 / g76 / g81 / g72 / g15 / g3 / g79 / g72 / g87 / g10 / g86 / g3 / g85 / g88 / g81 / g3 / g71 / g82 / g90 / g81 / g75 / g76 / g79 / g79 / g3 / g90 / g76 / g87 / g75 / g3 / g87 / g75 / g72 / g3 / g74 / g72 / g68 / g85 / g3 /g76 / g81 / g3 /g81 / g72 / g88 / g87 / g85 / g68 / g79 / g17 / g124 / g12 \u7686\u305d\u308c\u306f\u826f\u3044\u3068\u3001\u8cdb\u540c\u3057\u3066 \u2ed1 \u3044\u5742\u3092\u30cb\u30e5\u30fc\u30c8\u30e9\u30eb\u3067\u4e0b\u308a\u59cb \u3081\u307e\u3057\u305f\u3001\u76f4\u7dda\u306e \u2ed1 \u3044\u5742\u9053\u3067\u3057\u305f \u3002 /g11 / g40 / g89 / g72 / g85 / g92 / g82 / g81 / g72 / g3 / g68 / g74 / g85 / g72 / g72 / g71 / g3 / g87 / g75 / g68 / g87 / g3 / g76 / g87 / g3 / g90 / g68 / g86 / g3 / g74 / g82 / g82 / g71 / g3 / g76 / g71 / g72 / g68 / g15 / g3 / g68 / g81 / g71 / g3 / g86 / g87 / g68 / g85 / g87 / g72 / g71 /g3 / g87 / g82 / g3 /g74 / g82 / g3 / g71 / g82 / g90 / g81 / g3 / g68 / g3 / g79 / g82 / g81 / g74 / g3 / g71 / g82 / g90 / g81 / g75 / g76 / g79 / g79 / g15 / g3 / g90 / g75 / g76 / g70 / g75 / g3 / g90 / g68 / g86 / g3 / g86 / g87 / g85 / g68 / g76 / g74 / g75 / g87 / g17 / g12 \u30fb\u30fb\u30fb\u3068\u5f7c\u306f\u4e0b\u308a\u306b\u30ac\u30bd\u30ea\u30f3\u306f\u8981\u3089\u306a\u3044\u3068\u3001\u30a8\u30f3\u30b8\u30f3\u3092\u5207 /g2377 / g4342 / g2385 / g3063 / g2307 / g2341 / g3750 / g2346 / g1616 \u2f92 \u305b\u307e\u3057\u305f /g1617 / g1563 /g11 / g43 / g72 / g3 / g86 / g68 / g76 / g71 / g3 / g87 / g75 / g68 / g87 / g3 / g90 / g72 / g3 / g71 / g76 / g71 / g3 / g81 / g82 / g87 / g3 / g81 / g72 / g72 / g71 / g3 / g74 / g68 / g86 / g82 / g79 / g76 / g81 / g72 / g3 / g87 / g82 / g3 / g74 / g82 / g3 / g71 / g82 / g90 / g81 / g15 / g3 /g87 / g88 / g85 / g81 / g72 / g71 / g3 / g82 / g73 / g73 / g3 / g87 / g75 / g72 / g3 / g72 / g81 / g74 / g76 / g81 / g72 / g15 / g3 / g88 / g81 / g79 / g82 / g70 / g78 / g72 / g71 / g3 / g87 / g75 / g72 / g3 / g78 / g72 / g92 / g3 / g68 / g81 / g71 / g3 / g110 /g86 / g75 / g82 / g90 / g72 / g71 /g124 / g3 / g76 / g87 / g3 /g87 / g82 / g3 / g72 / g89 / g72 / g85 / g92 / g82 / g81 / g72 / g17 / g12 /g1566 / g52 / g88 / g72 / g86 / g87 / g76 / g82 / g81\ufe13 /g1587 / g1587 / g2385 / g1616 \u2f92 \u305b\u307e\u3057\u305f /g1617 / g1562\u306e\u3007\u3007\u306b \u2f0a \u308b\u3082\u306e\u306f\u4f55\u304b\ufe16 /g11 / g58 / g75 / g68 / g87 / g3 / g76 / g86 / g3 / g87 / g75 / g72 / g3 / g68 / g70 / g70 / g88 / g86 / g68 / g87 / g76 / g89 / g72 / g3 / g82 / g73 / g3 / g110 /g86 / g75 / g82 / g90 / g72 / g71 /g124 / g34 / g12 /g1566 / g36 / g81 / g86 / g90 / g72 / g85 / g3 / g29 /g38 / g82 / g85 / g85 / g72 / g70 / g87 / g3 / g68 / g81 / g86 / g90 / g72 / g85 /g29 / g3 /g4342 /g11 / g87 / g75 / g72 / g3 / g78 / g72 / g92 / g12 /g48 / g38 / g16 / g86 / g76 / g81 / g74 / g79 / g72 / g3 / g3 / g3 / g3 / g3 / g3 / g3 \ufe13/g3059 / g2634 /g11 / g71 / g82 / g90 / g81 / g75 / g76 / g79 / g79 / g12 /g48 / g38 / g16 / g80 / g72 / g85 / g74 / g72 / g71 / g3 / g3 / g3 / g3 / g3 / g3 /g29 \u2f9e /g11 / g87 / g75 / g76 / g86 /g70 / g68 / g85 / g12 /g48 / g38 / g16 / g86 / g87 / g72 / g83 / g90 / g76 / g86 / g72 / g3 / g3 / g3 / g3 / g3 / g3 / g3 / g3 / g3 / g29 /g4342 /g11 / g87 / g75 / g72 / g3 / g78 / g72 / g92 / g12Figure 3 : An example that is correctly answered by MC - stepwise .", "entities": []}, {"text": "domain .", "entities": []}, {"text": "6 Conclusion We constructed driving - domain PAS - QA and RCQA datasets using crowdsourcing5 .", "entities": []}, {"text": "We also proposed an MC - based PAS analysis method .", "entities": []}, {"text": "In particular , the stepwise training method based on BERT was the most effective , which outperformed the previous state - of - the - art NN - PAS model .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "In the future , we will pre - train an MC model based on datasets other than the RC - QA dataset to acquire domain knowledge .", "entities": []}, {"text": "References Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In NAACL - HLT2019 , pages 4171\u20134186 .", "entities": []}, {"text": "Nicholas FitzGerald , Julian Michael , Luheng He , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Large - scale QA - SRL parsing .", "entities": [[3, 6, "DatasetName", "QA - SRL"]]}, {"text": "In ACL2018 , pages 2051\u20132060 .", "entities": []}, {"text": "Shexia He , Zuchao Li , Hai Zhao , and Hongxiao Bai . 2018 .", "entities": []}, {"text": "Syntax for semantic role labeling , to be , or not to be .", "entities": [[2, 5, "TaskName", "semantic role labeling"]]}, {"text": "In ACL2018 , pages 2061\u20132071 .", "entities": []}, {"text": "5These datasets are available at http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?Driving%20domain%20QA%20datasets /g1566 /g39 / g82 / g70 / g88 / g80 / g72 / g81 / g87 / g3 / g29 /g3901 / g3207 / g2385 / g2326 / g2345 / g2315 / g2376 / g2590 / g2492 / g2307 / g2385 / g2507 / g2379 / g2697 / g2326 / g3059 / g2634 / g2346 / g3515 / g2367 / g1563 /g11 / g44 /g80 / g82 / g87 / g76 / g89 / g68 / g87 / g72 / g3 / g80 / g92 / g86 / g72 / g79 / g73 / g3 / g68 / g74 / g68 / g76 / g81 / g3 / g90 / g75 / g76 / g79 / g72 / g3 / g69 / g72 / g81 / g71 / g76 / g81 / g74 / g3 / g68 / g81 / g71 / g3 / g86 / g87 / g85 / g72 / g87 / g70 / g75 / g76 / g81 / g74 /g15 / g3 /g68 / g81 / g71 /g70 / g75 / g68 / g79 / g79 / g72 / g81 / g74 / g72 / g3 / g87 / g75 / g72 / g3 / g75 / g76 / g79 / g79 / g3 / g85 / g82 / g68 / g71 / g17 / g12 /g3059 / g2385 / g3053 / g2311 / g2334 / g2376 / g2434 / g2390 / g2401 / g2763 / g2315 / g2305 / g2378 / g1563 /g11 / g55 / g75 / g72 / g85 / g72 / g3 / g76 / g86 / g3 / g68 / g3 / g80 / g82 / g87 / g82 / g85 / g69 / g76 / g78 / g72 / g3 / g86 / g75 / g82 / g83 / g3 / g90 / g75 / g72 / g81 / g3 / g74 / g82 / g76 / g81 / g74 / g3 / g88 / g83 / g3 / g87 / g75 / g72 / g3 / g86 / g79 / g82 / g83 / g72 / g17 /g12 /g2776 / g2326 / g1616 /g2481 / g2338 / g2334 / g2335 / g2320 / g2342 /g1617 / g2324 / g2338 / g2316 / g2365 / g2342 / g2604 / g2307 / g2341 / g2307 / g2334 / g4072 / g2315 / g2534 / g2365 / g2342 / g2608 /g2481 / g2346 / g4017 / g2316 / g2482 / g2328 / g1563 /g11 / g45 / g88 / g86 / g87 / g3 / g110 /g70 / g79 / g76 / g80 / g69 / g76 / g81 / g74 / g3 / g88 / g83 /g124 / g3 / g68 / g3 / g69 / g76 / g87 / g15 / g3 / g86 / g90 / g72 / g68 / g87 / g3 / g87 / g75 / g68 / g87 / g3 / g86 / g87 / g82 / g83 / g83 / g72 / g71 / g3 / g88 / g81 / g87 / g76 / g79 / g3 / g68 / g3 /g90 / g75 / g76 / g79 / g72 / g3 / g68 / g74 / g82 / g3 / g74 / g88 / g86 / g75 / g72 / g86 / g3 / g82 / g88 / g87 / g3 / g80 / g82 / g85 / g72 / g3 / g87 / g75 / g68 / g81 / g3 / g69 / g72 / g73 / g82 / g85 / g72 / g17 / g12 /g1566 / g52 / g88 / g72 / g86 / g87 / g76 / g82 / g81 / g3 / g29 /g1587 / g1587 / g2385 / g1616 /g2481 / g2338 / g2334 / g2335 / g2320 / g2342 /g1617 / g1562\u306e\u3007\u3007\u306b \u2f0a \u308b\u3082\u306e\u306f\u4f55\u304b\ufe16 /g11 / g58 / g75 / g68 / g87 /g76 / g86 / g3 / g87 / g75 / g72 / g3 / g68 / g70 / g70 / g88 / g86 / g68 / g87 / g76 / g89 / g72 / g3 / g82 / g73 / g3 / g110 /g70 / g79 / g76 / g80 / g69 / g3 / g88 / g83 /g124 / g34 / g12 /g1566 / g36 / g81 / g86 / g90 / g72 / g85 / g3 / g29 /g38 / g82 / g85 / g85 / g72 / g70 / g87 / g3 / g68 / g81 / g86 / g90 / g72 / g85 /g29 / g3 /g3059 /g11 / g87 / g75 / g72 / g3 / g86 / g79 / g82 / g83 / g72 / g12 /g48 / g38 / g16 / g86 / g76 / g81 / g74 / g79 / g72 / g3 / g3 / g3 / g3 / g3 / g3 / g3 \ufe13/g4072 /g11 / g86 / g90 / g72 / g68 / g87 / g12 /g48 / g38 / g16 / g80 / g72 / g85 / g74 / g72 / g71 / g3 / g3 / g3 / g3 / g3 / g3 /g29 /g3059 / g2634 /g11 / g87 / g75 / g72 / g3 / g75 / g76 / g79 / g79 / g3 / g85 / g82 / g68 / g71 / g12 /g48 / g38 / g16 / g86 / g87 / g72 / g83 / g90 / g76 / g86 / g72 / g3 / g3 / g3 / g3 / g3 / g3 / g3 / g3 / g3 / g29 /g3059 /g11 / g87 / g75 / g72 / g3 / g86 / g79 / g82 / g83 / g72 / g12Figure 4 : An example that is correctly answered by MC - stepwise .", "entities": []}, {"text": "Ryu Iida , Mamoru Komachi , Kentaro Inui , and Yuji Matsumoto .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Annotating a Japanese text corpus with predicate - argument and coreference relations .", "entities": []}, {"text": "In ACL2007 , pages 132\u2013139 .", "entities": []}, {"text": "Ritsuko Iwai , Daisuke Kawahara , Takatsune Kumada , and Sadao Kurohashi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Annotating a driving experience corpus with behavior and subjectivity .", "entities": []}, {"text": "In PACLIC 32 , pages 222\u2013231 .", "entities": []}, {"text": "Ritsuko Iwai , Takatsune Kumada , Norio Takahashi , Daisuke Kawahara , and Sadao Kurohashi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Development of driving - related dictionary that includes psychological expressions .", "entities": []}, {"text": "In Proceedings of the 25th Annual Meeting of Natural Language Processing ( in Japanese ) , pages 1201\u20131204 .", "entities": []}, {"text": "Daisuke Kawahara , Sadao Kurohashi , and K \u02c6oiti Hasida .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Construction of a Japanese relevancetagged corpus .", "entities": []}, {"text": "In LREC2002 , pages 2008\u20132013 .", "entities": []}, {"text": "Shuhei Kurita , Daisuke Kawahara , and Sadao Kurohashi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural adversarial training for semisupervised Japanese predicate - argument structure analysis .", "entities": []}, {"text": "In ACL2018 , pages 474\u2013484 .", "entities": []}, {"text": "Julian Michael , Gabriel Stanovsky , Luheng He , Ido Dagan , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Crowdsourcing question - answer meaning representations .", "entities": []}, {"text": "In NAACL2018 , pages 560\u2013568 .", "entities": []}, {"text": "Todor Mihaylov , Peter Clark , Tushar Khot , and Ashish Sabharwal .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Can a suit of armor conduct electricity ?", "entities": []}, {"text": "A new dataset for open book question answering .", "entities": [[6, 8, "TaskName", "question answering"]]}, {"text": "In EMNLP2018 , pages 2381\u20132391 .", "entities": []}, {"text": "Hiroki Ouchi , Hiroyuki Shindo , and Yuji Matsumoto . 2017 .", "entities": []}, {"text": "Neural modeling of multi - predicate interactions for Japanese predicate argument structure analysis .", "entities": []}, {"text": "In ACL2017 , pages 1591\u20131600 .", "entities": []}, {"text": "Hiroki Ouchi , Hiroyuki Shindo , and Yuji Matsumoto .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A span selection model for semantic role labeling .", "entities": [[5, 8, "TaskName", "semantic role labeling"]]}, {"text": "In EMNLP2018 , pages 1630\u20131642 .", "entities": []}, {"text": "103Anusri Pampari , Preethi Raghavan , Jennifer J. Liang , and Jian Peng .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "emrQA :", "entities": [[0, 1, "DatasetName", "emrQA"]]}, {"text": "A large corpus for question answering on electronic medical records .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "InEMNLP2018 , pages 2357\u20132368 .", "entities": []}, {"text": "Boyuan Pan , Yazheng Yang , Hao Li , Zhou Zhao , Yueting Zhuang , Deng Cai , and Xiaofei He .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "MacNet : Transferring knowledge from machine comprehension to Sequence - to - Sequence models .", "entities": []}, {"text": "In NeurIPS2018 , pages 6095\u20136105 .", "entities": []}, {"text": "Pranav Rajpurkar , Robin Jia , and Percy Liang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Know what you do n\u2019t know : Unanswerable questions for SQuAD .", "entities": [[10, 11, "DatasetName", "SQuAD"]]}, {"text": "In ACL2018 , pages 784\u2013789 .", "entities": []}, {"text": "Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "SQuAD : 100 , 000 + questions for machine comprehension of text .", "entities": [[0, 1, "DatasetName", "SQuAD"]]}, {"text": "In EMNLP2016 , pages 2383\u20132392 .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In ACL2016 , pages 1715\u20131725 .", "entities": []}, {"text": "Tomohide Shibata , Daisuke Kawahara , and Sadao Kurohashi . 2016 .", "entities": []}, {"text": "Neural network - based model for Japanese predicate argument structure analysis .", "entities": []}, {"text": "In ACL2016 , pages 1235\u20131244 .", "entities": []}, {"text": "Tomohide Shibata and Sadao Kurohashi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Entitycentric joint modeling of Japanese coreference resolution and predicate argument structure analysis .", "entities": [[5, 7, "TaskName", "coreference resolution"]]}, {"text": "In ACL2018 , pages 579\u2013589 .", "entities": []}, {"text": "Emma Strubell , Patrick Verga , Daniel Andor , David Weiss , and Andrew McCallum .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Linguistically - informed self - attention for semantic role labeling .", "entities": [[7, 10, "TaskName", "semantic role labeling"]]}, {"text": "In EMNLP2018 , pages 5027\u20135038 .", "entities": []}, {"text": "Simon Suster and Walter Daelemans .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "CliCR : a dataset of clinical case reports for machine reading comprehension .", "entities": [[0, 1, "DatasetName", "CliCR"], [9, 12, "TaskName", "machine reading comprehension"]]}, {"text": "In NAACL2018 , pages 1551\u20131563 .", "entities": []}, {"text": "Zhixing Tan , Mingxuan Wang , Jun Xie , Yidong Chen , and Xiaodong Shi . 2018 .", "entities": []}, {"text": "Deep semantic role labeling with self - attention .", "entities": [[1, 4, "TaskName", "semantic role labeling"]]}, {"text": "In AAAI2018 , pages 4929 \u2013 4936 .", "entities": []}, {"text": "Adam Trischler , Tong Wang , Xingdi Yuan , Justin Harris , Alessandro Sordoni , Philip Bachman , and Kaheer Suleman . 2017 .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "NewsQA : A machine comprehension dataset .", "entities": [[0, 1, "DatasetName", "NewsQA"]]}, {"text": "In ACL2017 , pages 191\u2013200 .", "entities": []}, {"text": "Johannes Welbl , Nelson F. Liu , and Matt Gardner . 2017 .", "entities": []}, {"text": "Crowdsourcing multiple choice science questions .", "entities": []}, {"text": "In EMNLP2017 , pages 94\u2013106 .", "entities": []}, {"text": "Zhilin Yang , Peng Qi , Saizheng Zhang , Yoshua Bengio , William W. Cohen , Ruslan Salakhutdinov , and Christopher D. Manning .", "entities": [[16, 17, "DatasetName", "Ruslan"]]}, {"text": "2018 .", "entities": []}, {"text": "HotpotQA : A dataset for diverse , explainable multi - hop question answering .", "entities": [[0, 1, "DatasetName", "HotpotQA"], [8, 13, "TaskName", "multi - hop question answering"]]}, {"text": "In EMNLP2018 , pages 2369\u20132380.A Details of PAS - QA Dataset Construction We construct the PAS - QA dataset asking for omitted nominative arguments using the following procedure : 1.We extract four consecutive sentences that satisfy the following conditions from the Driving Experience Corpus constructed by Iwai et al .", "entities": []}, {"text": "( 2019 )", "entities": []}, {"text": ".", "entities": []}, {"text": "\u000fThe Driving Experience extracting CRF tool ( Iwai et al . , 2018 ) judges that three or more sentences out of four sentences are driving experience .", "entities": [[4, 5, "MethodName", "CRF"]]}, {"text": "\u000fEach sentence contains at least one PAS .", "entities": []}, {"text": "\u000fThe PAS analyzer , KNP , judges that there is a PAS whose nominative argument is omitted in the fourth sentence .", "entities": []}, {"text": "\u000fSentences include at least one \u201c Driving Characteristic Word \u201d ( Iwai et al . , 2019 ) .", "entities": []}, {"text": "2.We automatically make crowdsourcing tasks using an extracted document and a PAS whose nominative argument is omitted ( See Figure 5and Figure 6 ) .", "entities": []}, {"text": "Each task consists of a document , a question and answer choices .", "entities": []}, {"text": "Answer choices consist of nouns extracted from the document and special symbols , \u201c author , \u201d \u201c other , \u201d and \u201c not sure . \u201d", "entities": []}, {"text": "For nominative PAS - QA questions , the special symbol \u201c author \u201d can often be an answer , but it is not explicitly expressed in the document .", "entities": []}, {"text": "So we add it to the choices .", "entities": []}, {"text": "We add \u201c other \u201d so that it can be selected when there is an appropriate answer besides the choices .", "entities": []}, {"text": "We add \u201c not sure \u201d so that workers can select it if they can not \ufb01nd an answer .", "entities": []}, {"text": "We add more explanations to crowdsourcing answer screen ( See Figure 5and Figure 6 ) .", "entities": []}, {"text": "3.Using crowdsourcing , we ask \ufb01ve crowdworkers per question to select one or more appropriate answers from the choices .", "entities": []}, {"text": "We asked \ufb01ve crowdworkers per question using Yahoo !", "entities": []}, {"text": "crowdsourcing .", "entities": []}, {"text": "We adopted triplets with three or more votes if they are not \u201c not sure . \u201d", "entities": []}, {"text": "If they are \u201c other , \u201d we handled them as described in the main paper .", "entities": []}, {"text": "We \ufb01nally record the answers as spans in a document or NULL .", "entities": []}, {"text": "104 Figure 5 : PAS - QA dataset answer screen .", "entities": []}, {"text": "Figure 6 : PAS - QA dataset answer screen ( English translation version ) .", "entities": []}]