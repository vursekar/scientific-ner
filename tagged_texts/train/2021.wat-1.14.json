[{"text": "Proceedings of the 8th Workshop on Asian Translation , pages 138\u2013140 Bangkok , Thailand ( online ) , August 5 - 6 , 2021 .", "entities": [[7, 8, "TaskName", "Translation"]]}, {"text": "\u00a9 2021 Association for Computational Linguistics138", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "In this paper , we describe our participation   in the 2021 Workshop on Asian Translation   ( team ID : tpt_wat ) .", "entities": [[15, 16, "TaskName", "Translation"]]}, {"text": "We submitted results   for all six directions of the JPC2 patent   task .", "entities": []}, {"text": "As a first -time participant in the task ,   we attempted to identify a single   configuration that provided the best   overall   results across all language pairs .", "entities": []}, {"text": "All our   submissions were created using single base   transformer models , trained on only the   task - specific data , using a consistent   configuration of hyperparameters .", "entities": []}, {"text": "In   contrast to the uniformity of our methods ,   our results vary widely across the six   language pairs .", "entities": []}, {"text": "1 Introduction    The field of machine translation has seen rapid   innovation in the last few years , with new model   architectures , pre -training regimens , and   computational algorithms emerging at a dizzying   pace .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "However , translation of these techniques into   industry practice   occurs more slowly .", "entities": []}, {"text": "Companies   utilizing these techniques must take into account   considerations such as deployment costs ( model   speed and size ) , scalability , explainability , the   complexity of training regimens ( resource   constraints limiting independent hyp erparameter   optimization for all language pairs ) , and risk   management , against which advances yielding   performance gains must be weighed .", "entities": []}, {"text": "For our participation in the 2021   Workshop on Asian Translation shared task on   patent translation , we have applied a single ,   standardized data preparation and model training   pipeline as a way of benchmarking the   performance of this process .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "We conducted limited   experiments to test different parameters , before     1   http://lotus.kuee.kyoto -u.ac.jp / WAT / patent/   settling on the approach which provided the best   overall resul ts across all language pairs .", "entities": [[32, 33, "MethodName", "ts"]]}, {"text": "Our NMT   systems are standard base Transformer ( Vaswani et   al . , 2017 ) models , which were trained using only   the data resources provided by the task organizers .", "entities": [[7, 8, "MethodName", "Transformer"]]}, {"text": "These models used shared subword vocabularies   created with Sentence Piece ( Kudo and Richardson ,   2018 ) .", "entities": []}, {"text": "In contrast to the uniformity of our methods ,   our results varied widely across the six language   pairs .", "entities": []}, {"text": "Different scoring metrics prevent the direct   comparison of scores from different language   pairs , but relative to the top performing model in   each languag e pair , our scores ranged from 98.84 %   of the top score for the English \u2192 Japanese   language pair , to 83.89 % of the top score for   Korean \u2192 Japanese .", "entities": []}, {"text": "Below , we describe in detail   our system architecture , hyperparameter   configuration , hardware resources , and r esults .", "entities": []}, {"text": "2 System Overview    2.1 Task Description    The JPC2 patent task consisted of translation in the   patent domain between English and Japanese ,   Korean and Japanese , and Chinese and Japanese .", "entities": []}, {"text": "The training data consisted of para llel corpora   provided by the Japan Patent Office ( JPO ) , with   training sets containing one million sentence pairs   for each language pair .", "entities": []}, {"text": "The data are drawn from   four domains , chemistry , electricity , mechanical   engineering , and physics.1    2.2 Data Processing", "entities": []}, {"text": "The data were encoded using subword encodings   learned from the corpora using the unigram model   trainer provided by SentencePiece ( Kudo and   Richardson , 2018 ) .", "entities": [[20, 21, "MethodName", "SentencePiece"]]}, {"text": "To avoid the added complexity   of using different pre -tokenization strategies for System Description for Transperfect      Wiktor Stribi\u017cew   and Fred Bane   and Jos\u00e9 Concei\u00e7\u00e3o   and Anna Zaretskaya", "entities": []}, {"text": "Transperfect Translations    { wstribizew , fbane , jconceicao , azaretskaya}@translations.com", "entities": []}, {"text": "139      different languages , we did not pre -tokenize the   data prior to learning the subword model .", "entities": []}, {"text": "We tested   vocabulary sizes of 8000 and 32000 , as well as   using shared or split vocabularies for the source   and target languages .", "entities": []}, {"text": "Character coverage was set   to 0.9995 , the recommended value for languages   with extensive character sets such as Chinese and   Japanese .", "entities": []}, {"text": "For the English \u2192 Japanese , Korean \u2192   Japanese , and Chinese \u2192 Japanese language pairs ,   we supplemented the corpora with back translation   ( from Japanese into each language ) , which is a   common data augmentation technique in NMT   ( Sennrich et al . , 2016 ) .", "entities": [[40, 42, "TaskName", "data augmentation"]]}, {"text": "The back translations were   produced by the NMT systems trained for the other   three directions ( Japanese \u2192 English , Korean , and   Chinese ) .", "entities": []}, {"text": "2.3 Models    Our NMT systems were standard base Transformer   models trained using the Marian NMT framework   ( Junczys -Dowmunt et al . , 2018 ) .", "entities": [[9, 10, "MethodName", "Transformer"]]}, {"text": "We trained   separate , unidirectional models for each language   pair .", "entities": []}, {"text": "Hyperparameters such as label smoothing ,   dropout , learning rate ,   batch size , number of   encoder / decoder layers , number of attention heads ,   embedding dimensionality , etc . , were held fixed   across all language pairs .", "entities": [[3, 5, "MethodName", "label smoothing"], [9, 11, "HyperparameterName", "learning rate"], [13, 15, "HyperparameterName", "batch size"]]}, {"text": "The validation frequency   was every 500 updates , and training was continued   for 50 epochs or until the prima ry validation metric   ( ce - mean -words , or mean word cross -entropy   score ) failed to improve for five consecutive   checkpoints .", "entities": [[20, 21, "DatasetName", "prima"]]}, {"text": "Our models were trained on AWS P3   instances using 4 NVIDIA Tesl a V100 GPUs .", "entities": [[6, 7, "DatasetName", "P3"]]}, {"text": "3 Results    Our results show that for most language pairs , a   shared vocabulary of size 8,000 achieved the best   performance .", "entities": []}, {"text": "For the Korean \u2192 Japanese and Japanese \u2192 Korean language pairs , using a   vocabulary size of 32,000 produced better results .", "entities": []}, {"text": "", "entities": []}, {"text": "Using a split vocabul ary for these language pairs   also resulted in better performance , whereas a   shared vocabulary was advantageous for all other   language pairs .", "entities": []}, {"text": "In all cases , the i nclusion of back   translated training data resulted in higher   validation scores .", "entities": []}, {"text": "Table 1 shows ou r results in   terms of BLEU scores ( Papineni et al . , 2002 ) as   calculated on our local machines .", "entities": [[10, 11, "MetricName", "BLEU"]]}, {"text": "Due to   differences in processing , these scores do not   match the scores reported by the Organizers .", "entities": []}, {"text": "4 Discussion    In this shared task , we set out to identify a singl e   configuration of hyperparameters that provided the   best overall performance across all six language   pairs .", "entities": []}, {"text": "While this approach precluded the possibility   of obtaining optimal performance for all language   pairs , it afforded the opportunity to investigate   which hyperparameters have similar effects on   different language pairs , and which have varied   effects on different language pairs .", "entities": []}, {"text": "As   different language pairs require different   hyperparameters , any parameter that can be held   fixed during the experimentation   stage can create   significant savings for companies training their   own machine translation models .", "entities": [[34, 36, "TaskName", "machine translation"]]}, {"text": "", "entities": []}, {"text": "For instance , variation in parameters such   as learning rate , dropout , embedding dimensions ,   and tying the weights of the source and target   embedding layers seemed to have similar effects   on performance across all language pairs that we   tested .", "entities": [[9, 11, "HyperparameterName", "learning rate"]]}, {"text": "Using back translated data to augment the   training sets also appeared to be universally   beneficial .", "entities": []}, {"text": "However , the size of the vocabulary   seemed to have quite different e ffects in different   language pairs .", "entities": []}, {"text": "We are not aware of any theoretical   framework for explaining how the various   Language Pair   Split 32 K   Split 32 K + BT   Shared 32 K   Shared 8 K    Shared 8 K + BT    EN \u2192 JA   23.2 26.6 23.8 23.8 27.1   JA \u2192 EN   38.9 - 39.4 40.2 KO \u2192 JA   46.6", "entities": []}, {"text": "46.8 46.7 45.6 45.6   JA \u2192 KO   52.0 - 50.8 - ZH \u2192 JA   30.6 31.6 31.8 31.9 32.9   JA \u2192 ZH   46.2 - 37.6 47.5 Table 1 :   BLEU scores for different language pairs and different vocabulary configurations", "entities": [[35, 36, "MetricName", "BLEU"]]}, {"text": "140      hyperparameters interact to produce such different   results , nor do we know of any way of predicting   the optimal hyperparameters for a given lan guage   pair other than iterative experimentation .", "entities": []}, {"text": "", "entities": []}, {"text": "If additional resources are used , several   additional steps have also been shown to be   effective at boosting performance , but were not   employed in these experiments in order to maintain   maximum simplicity .", "entities": []}, {"text": "These additional steps   include using an ensemble of models for decoding ,   using larger model sizes , performing word   segmentation prior to creating the vocabularies ,   ordering the training data using the output of a   language model ( a technique referred to as   curriculum learning ) , and employing an additional   model for right -to - left re -ranking .", "entities": []}, {"text": "", "entities": []}, {"text": "With minimal manual intervention , our   models achieved results ranging from fair to   excellent .", "entities": []}, {"text": "The large variance in the relative   performance of these systems shows that no \u201c one size - fits - all \u201d yet exists for the problem of machine   translation .", "entities": []}, {"text": "Despite monumental advances in the   field over the past several years , achieving optimal   performance requires careful selection of   hyperparameters , and different configurations are   required for different languages .", "entities": []}, {"text": "", "entities": []}, {"text": "References    Kudo , Taku , and John Richardson .   2018    \" SentencePiece : A simple and language   independent subword tokenizer and detokenizer for   Neural Text Processing . \"", "entities": [[14, 15, "MethodName", "SentencePiece"]]}, {"text": "In Proceedings of the 2018   Conference on Empirical Methods in Natural   Language Processing : System Demonstrations .", "entities": []}, {"text": "Marcin Junczys -Dowmunt , Ro man Grundkiewicz ,   Tomasz Dwojak , Hieu Hoang , Kenneth Heafield ,   Tom Neckermann , Frank Seide , Ulrich Germann ,   Alham Fikri Aji , Nikolay Bogoychev , Andr\u00e9 F. T.   Martins , Alexandra Birch .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Marian :", "entities": []}, {"text": "Fast   Neural Machine Translation in C++    http://www.aclweb.org /anthology / P18 -4020 .", "entities": [[3, 5, "TaskName", "Machine Translation"]]}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and   WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic   evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [9, 11, "TaskName", "machine translation"]]}, {"text": "In Proceedings of   the 40th Annual Meeting of the Association for   Computational Linguistics , pages 311 \u2013 318 ,   Philadelphia , Pennsylvania , USA . Association for   Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch .   2016 .", "entities": []}, {"text": "Improving neural machine translation models   with monolingual data .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th   Annual Meeting of the Associ ation for   Computational Linguistics ( Volume 1 : Long Papers ) , pages 86 \u2013 96 , Berlin , Germany .   Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob   Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz   Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all   you need .", "entities": []}, {"text": "In Advances in neural information   processing systems , pages 5998 \u2013 6008 .", "entities": []}]