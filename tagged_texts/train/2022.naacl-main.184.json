[{"text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 2551 - 2568 July 10 - 15 , 2022 \u00a9 2022 Association for Computational Linguistics Interactive Query - Assisted Summarization via Deep Reinforcement Learning Ori Shapira1,3\u2217 , Ramakanth Pasunuru2 , Mohit Bansal2 , Ido Dagan1 , and Yael Amsterdamer1 1Bar - Ilan University2UNC", "entities": [[41, 42, "TaskName", "Summarization"]]}, {"text": "Chapel Hill3Amazon obspp18@gmail.com { ram,mbansal}@cs.unc.edu { dagan,amstery}@cs.biu.ac.il", "entities": []}, {"text": "Abstract Interactive summarization is a task that facilitates user - guided exploration of information within a document set .", "entities": [[2, 3, "TaskName", "summarization"]]}, {"text": "While one would like to employ state of the art neural models to improve the quality of interactive summarization , many such technologies can not ingest the full document set or can not operate at sufficient speed for interactivity .", "entities": [[18, 19, "TaskName", "summarization"]]}, {"text": "To that end , we propose two novel deep reinforcement learning models for the task that address , respectively , the subtask of summarizing salient information that adheres to user queries , and the subtask of listing suggested queries to assist users throughout their exploration.1In particular , our models allow encoding the interactive session state and history to refrain from redundancy .", "entities": []}, {"text": "Together , these models compose a state of the art solution that addresses all of the task requirements .", "entities": []}, {"text": "We compare our solution to a recent interactive summarization system , and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience .", "entities": [[8, 9, "TaskName", "summarization"]]}, {"text": "1 Introduction Integrating human interaction into NLP tasks has been gaining the interest of the NLP community .", "entities": []}, {"text": "Human - machine cooperation can improve the general quality of results , as well as provide a higher sense of control for the targeted consumer .", "entities": []}, {"text": "We focus on the task of interactive summarization ( INTSUMM : Shapira et al . , 2021b ) which enables information exploration within a document set on a topic , by means of user - guided summarization .", "entities": [[7, 8, "TaskName", "summarization"], [36, 37, "TaskName", "summarization"]]}, {"text": "As illustrated in Figure 1 , a user can incrementally expand on a summary by submitting requests to the system , in order to expose the information of interest within the topic .", "entities": []}, {"text": "A proper exploration session demands access to allinformation within the document set , and fast reaction time for smooth human \u2217This work was conducted prior to joining Amazon .", "entities": []}, {"text": "1Code and trained models at : https://github.com/ OriShapira / InterExp_DeepRL", "entities": []}, {"text": "System initial   summary User query 1 System response 1 User query 2 System response 2Suggested Queriesquery -Process fulldocset -Low latencyFigure 1 : An INTSUMM system , ingesting a large document set .", "entities": []}, {"text": "A user interactively submits queries in order to expand on the information .", "entities": []}, {"text": "The system is required to process the full document set for comprehensive exploration , respond quickly , and expose nonredundant salient information that also complies to the input queries .", "entities": []}, {"text": "See real example in Figure 5 . engagement ( Anderson , 2020 ; Attig et al . , 2017 ) .", "entities": []}, {"text": "In addition , presented information must consider the session history to refrain from repetitiveness .", "entities": []}, {"text": "While it is worthwhile to apply recent NLP advances that excel at extracting salient and querybiased information , those advances usually come at a cost of rather small input size limits or heavy computation time .", "entities": []}, {"text": "Indeed , all previous interactive summarization systems we know of either apply traditional methods or are inadequate for real - time processing due to high latency ( \u00a7 2 ) .", "entities": [[5, 6, "TaskName", "summarization"]]}, {"text": "Our goal is to overcome these obstacles , and leverage advanced methods to improve information exposure while keeping latency acceptable for interaction .", "entities": []}, {"text": "As depicted in Figure 1 , an INTSUMM system provides an initial generic summary as an overview of the topic , after which a user can iteratively issue queries to the system for summary expansions on subtopics of interest .", "entities": []}, {"text": "To support querying , the system offers a list of suggested queries , hinting at information concealed within the document set .", "entities": []}, {"text": "We address the INTSUMM task components through two subtasks : ( 1 ) generating the initial summary and query responses , and ( 2 ) generating lists of suggested queries .", "entities": []}, {"text": "For each of the subtasks we propose a deep reinforcement learning ( RL ) algorithm that addresses the respective sub-2551", "entities": []}, {"text": "task requirements .", "entities": []}, {"text": "To enable comprehensive topic exploration , our models speedily process the full document set , as inspired by Mao et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "Additionally , they are able to peek at session history to comply to the current state of the interaction .", "entities": []}, {"text": "The model for the query - assisted summarization subtask , MSumm , incorporates the query sequence by ( 1 ) encoding a query into the contextual sentence representations , ( 2 ) attending the representations using a new query - biased variant of the maximal marginal relevance ( MMR : Carbonell and Goldstein , 1998 ) function , and ( 3 ) a dual reward mechanism for policy optimization ( Pasunuru and Bansal , 2018 ) which we adapt to consider both reference summaries and the query ( \u00a7 3 ) .", "entities": [[7, 8, "TaskName", "summarization"]]}, {"text": "The model for the suggested queries list generation subtask , MSugg , works at the phrase level , as opposed to the sentence level , to enable extraction of important phrases that serve as suggested queries .", "entities": []}, {"text": "Similarly toMSumm , the model learns importance with consideration to session history , but without an input query \u2013 as its role is to suggest such a query ( \u00a7 4 ) .", "entities": []}, {"text": "The models are trained on the DUC22007 multidocument summarization ( MDS ) news - domain dataset , with adaptions for our task setting .", "entities": [[8, 9, "TaskName", "summarization"]]}, {"text": "For testing , we follow the INTSUMM evaluation framework of Shapira et al .", "entities": []}, {"text": "( 2021b ) to run simulations , collect real user sessions , and assess the results , using DUC 2006 .", "entities": [[18, 20, "DatasetName", "DUC 2006"]]}, {"text": "In principle , summary informativeness , i.e. general salience , could potentially come at the expense of query responsiveness , but importantly , our results show that our RL - based solution is able to significantly improve information exposure over the baseline of Shapira et al .", "entities": []}, {"text": "( 2021b ) , without compromising user experience ( \u00a7 5 ) .", "entities": []}, {"text": "2 Background and Related Work Interactive summarization facilitates user - guided information navigation within document sets .", "entities": [[6, 7, "TaskName", "summarization"]]}, {"text": "The task suffered from a lack of a methodological evaluation , until Shapira et al .", "entities": []}, {"text": "( 2021b ) formalized the INTSUMM task with a framework consisting of a benchmark , evaluation metrics , a session collection process and baseline systems .", "entities": []}, {"text": "This framework , that we leverage , enables comparison and analysis of systems , allowing principled research on the task and accelerated development of algorithms .", "entities": []}, {"text": "To the best of our knowledge , all previous works onINTSUMM have either applied more traditional text - processing methods or require costly prepro2https://duc.nist.gov/cessing of inputs to facilitate seamless interaction .", "entities": []}, {"text": "Leuski et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2003 ) used surface - form features for processing content , and Baumel et al .", "entities": []}, {"text": "( 2014 ) adapted classic MDS algorithms like LexRank ( Erkan and Radev , 2004 ) and KLSum ( Haghighi and Vanderwende , 2009 ) .", "entities": []}, {"text": "Christensen et al .", "entities": []}, {"text": "( 2014 ) optimized discourse graphs and Shapira et al .", "entities": []}, {"text": "( 2017 ) relied on a knowledge representation , both expensively pre - generating hierarchical summaries that limit expansions to pre - prepared information selections .", "entities": []}, {"text": "Hirsch et al .", "entities": []}, {"text": "( 2021 ) applied advanced coreference resolution algorithms that take several hours for preprocessing a document set .", "entities": [[5, 7, "TaskName", "coreference resolution"]]}, {"text": "The two INTSUMM baseline systems of Shapira et al .", "entities": []}, {"text": "( 2021b ) use sentence clustering or TextRank ( Mihalcea and Tarau , 2004 ) for summarization , sentence similarity heuristics for query - responses , and n - gram frequency or TextRank for suggested query extraction .", "entities": [[16, 17, "TaskName", "summarization"]]}, {"text": "Moreover , their query - response generators strictly consider a given query , ignoring history or global informativeness .", "entities": []}, {"text": "Our proposed algorithms significantly improve information exposure over the latter baselines , using advanced deep RL methods , working in real time .", "entities": []}, {"text": "We next review some recent techniques in MDS , query - focused summarization and multi - document keyphrase extraction , all of which relate to the INTSUMM task and our choice of algorithms .", "entities": [[12, 13, "TaskName", "summarization"], [17, 19, "TaskName", "keyphrase extraction"]]}, {"text": "The subtask of query - assisted summarization .", "entities": [[6, 7, "TaskName", "summarization"]]}, {"text": "Non - interactive MDS has been researched extensively , with few recent neural - based methods that can handle relatively large inputs .", "entities": []}, {"text": "For example , Wang et al .", "entities": []}, {"text": "( 2020 ) use graph neural networks to globally score sentence salience , Xiao et al .", "entities": []}, {"text": "( 2021 ) summarize using Longformers ( Beltagy et al . , 2020 ) , and Pasunuru et al .", "entities": []}, {"text": "( 2021b ) combine a Longformer with BART ( Lewis et al . , 2020 ) and incorporate graphical representation of information .", "entities": [[5, 6, "MethodName", "Longformer"], [7, 8, "MethodName", "BART"]]}, {"text": "Mao et al .", "entities": []}, {"text": "( 2020 ) apply deep RL for autoregressive sentence selection , and , in contrast to most other neural methods , can ingest the fulldocument set .", "entities": []}, {"text": "In the query - focused summarization ( QFS ) task summaries are biased on a query .", "entities": [[5, 6, "TaskName", "summarization"]]}, {"text": "To accommodate a query , Xie et al .", "entities": []}, {"text": "( 2020 ) use conditional selfattention to enforce dependency of the query on source words .", "entities": []}, {"text": "Pasunuru et al . ( 2021a ) and Kulkarni et al . ( 2021 ) hierarchically encode a query with the documents .", "entities": []}, {"text": "These and other QFS methods require large training sets , and limit the allowed input size ( Baumel et al . , 2018 ; Laskar et al . , 2020 ) .", "entities": []}, {"text": "Relatedly , incremental update summarization ( Mc-2552", "entities": [[4, 5, "TaskName", "summarization"]]}, {"text": "Creadie et al . , 2014 ; Lin et al . , 2017 ) marks queryrelevant information as reported texts stream in , avoiding repeating information marked earlier .", "entities": []}, {"text": "Interactivity is not a constraining factor here , yielding solutions with relatively high computation time .", "entities": []}, {"text": "With respect to the above related work , we develop a model inspired by Mao et al .", "entities": []}, {"text": "( 2020 ) , which is closest to our requirements .", "entities": []}, {"text": "To facilitate an interactive setting , our model ( 1 ) enables query+history injection , ( 2 ) supports full input processing , necessary for complete information availability during exploration , ( 3 ) has low latency at inference time , and ( 4 ) requires a relatively small training set .", "entities": []}, {"text": "The subtask of suggested - queries list generation .", "entities": []}, {"text": "Extracting suggested queries on a document set most resembles the multi - document keyphrase extraction ( MDKE ) task since it aims to identify salient keyphrases ( Shapira et al . , 2021a ) .", "entities": [[13, 15, "TaskName", "keyphrase extraction"]]}, {"text": "MDKE was mostly addressed using traditional heuristics or graph - centrality algorithms applied over the documents ( e.g. Mihalcea and Tarau , 2004 ; Florescu and Caragea , 2017 ) .", "entities": []}, {"text": "In contrast to MDKE , the suggested queries extraction subtask is a new paradigm that updates \u201c keyphrases \u201d with respect to session history .", "entities": []}, {"text": "While previous methods for keyphrase extraction could potentially be adapted for our dynamic setting , we choose to focus in this work on a deep RL architecture for suggested queries that resonates our model for query - assisted summarization and allows sharing insights between the models .", "entities": [[4, 6, "TaskName", "keyphrase extraction"], [38, 39, "TaskName", "summarization"]]}, {"text": "3 Query - Assisted Summarization Model", "entities": [[4, 5, "TaskName", "Summarization"]]}, {"text": "The subtask of query - assisted summarization covers two main components of the INTSUMM task : the generators of an initial summary and of queryresponses .", "entities": [[6, 7, "TaskName", "summarization"]]}, {"text": "The initial summary concisely specifies some central issues from the input topic ( not biased on a query ) to initiate the user \u2019s understanding of the topic and to motivate further exploration .", "entities": []}, {"text": "Then , for each user submitted query , the query - response generator non - redundantly expands on the previously presented information with topically salient responses that are also biased around the query .", "entities": []}, {"text": "We next formally define the subtask and then describe our RL model for it .", "entities": []}, {"text": "3.1 Subtask Formulation The input to the query - assisted summarization subtask is tuple ( D , q , Ein , m ) , such that : Dis a document set on a topic where the j - th sentence in the concatenation of D \u2019s documents is denoted sj;qis a query , and can be empty ( denoted _ ) for an unbiased generic summary ; Ein={ein 1 , ... , ein k}is a sequence of sentences from Dtermed the history , containing texts previously output in the session ; andmis the number of sentences to output .", "entities": [[10, 11, "TaskName", "summarization"]]}, {"text": "The output is sentence sequence Eout={eout 1 , ... , eout m } fromD(extractive summarization ) .", "entities": [[14, 15, "TaskName", "summarization"]]}, {"text": "When inputting ( D , _ , { } , m ) , the output is a generic summary of msentences , that can serve as the initial summary ; and when qandEinare not empty , the output is an expansion on Einin response to q , containing new salient information biased on q. Dis paired with a set of generic reference summaries R , which is used for training or as a part of the evaluation effort .", "entities": []}, {"text": "3.2 Model Architecture Our query - assisted summarization model , MSumm , is autoregressive , outputting the requested number of summary sentences one - by - one .", "entities": [[7, 8, "TaskName", "summarization"]]}, {"text": "At time step t , a sentence eout tis output according to the current query and an encoding of the summary - so - far Et={ein 1 , ... , ein k , eout 1 , ... , eout t\u22121}to prevent information repetition .", "entities": []}, {"text": "At inference time , MSumm outputs the summary sentences with the given query and history ( possibly empty ) .", "entities": []}, {"text": "At train time , we emulate a session by invoking MSumm with a sequence of differing queries , Q={q1 , q2 , ... , q m } , for which to generate the corresponding sequence of output sentences .", "entities": []}, {"text": "I.e. , output sentence eout tis biased on query qtand the summary - so - far Etat time step t.", "entities": []}, {"text": "We next describe the architecture3of MSumm , also illustrated in Figure 2 . Sentence encoding .", "entities": []}, {"text": "The first step of the model is hierarchically encoding the sentences of the document set Dto obtain contextualized representation cjfor sentence sj\u2200j .", "entities": []}, {"text": "A CNN ( Kim , 2014 ) encodes sjon the sentence level and then a bi - LSTM ( Huang et al . , 2015 ) forms representation cjon the document level , given the CNN encodings .", "entities": [[17, 18, "MethodName", "LSTM"]]}, {"text": "Query encoding .", "entities": []}, {"text": "Additionally , at each time step twe prepare sentence+query representations ct j= cj\u2295CNN(qt ) , i.e. , obtained by concatenating a sentence representation and the CNN - encoding of the current query .", "entities": []}, {"text": "This sentence+query represen3In general , the implementation choices weighed in the speed at which the full input document set can be processed .", "entities": []}, {"text": "In comparison to other techniques ( some of which are more recent ) , these choices gave as good or better results at lower latency .", "entities": []}, {"text": "Alternative architectural choices and their behavior are discussed in Appendix B.2553", "entities": []}, {"text": "\ud835\udc601 \ud835\udc602 \ud835\udc603 \ud835\udc604\ud835\udc5e\ud835\udc61 \u01b8\ud835\udc501\ud835\udc61\ud835\udc521 in \ud835\udc52\ud835\udc58in \ud835\udc521out \ud835\udc52\ud835\udc61\u22121out", "entities": []}, {"text": "\ud835\udc52\ud835\udc61out=\ud835\udc605\ud835\udc67\ud835\udc61", "entities": []}, {"text": "\ud835\udc605 \ud835\udc606\u01b8\ud835\udc502\ud835\udc61 \u01b8\ud835\udc503\ud835\udc61 \u01b8\ud835\udc504\ud835\udc61 \u01b8\ud835\udc505\ud835\udc61 \u01b8\ud835\udc506\ud835\udc612 - hop attn .", "entities": []}, {"text": "\ud835\udc5d1\ud835\udc61 \ud835\udc5d2\ud835\udc61 \ud835\udc5d3\ud835\udc61 \ud835\udc5d4\ud835\udc61 \ud835\udc5d5\ud835\udc61", "entities": []}, {"text": "\ud835\udc5d6\ud835\udc61qMMR \u03bct Dual Rewards \u2026 backpropconvNN bi - LSTMLSTM Ref Summs ( \u211b)Doc Set   ( \ud835\udc9f ) History +", "entities": []}, {"text": "Output\ud835\udc501\ud835\udc61 \ud835\udc502\ud835\udc61 \ud835\udc503\ud835\udc61 \ud835\udc504\ud835\udc61 \ud835\udc505\ud835\udc61 \ud835\udc506\ud835\udc61 \u2026\u2026 \u2026 Figure 2 : The MSumm query - assisted summarization model architecture .", "entities": [[17, 18, "TaskName", "summarization"]]}, {"text": "Contextual sentence embeddings are concatenated to the current query embedding .", "entities": [[1, 3, "TaskName", "sentence embeddings"]]}, {"text": "The sentence+query representation is softly attended with a transformed query - focused MMR score , and a sentence selection distribution is obtained with a two - hop attention mechanism , considering a summary - so - far representation .", "entities": []}, {"text": "A dual - reward mechanism , using the reference summaries and query , optimizes a policy to train the model for summary content quality and sentence - to - query resemblance .", "entities": []}, {"text": "At inference time , an initial summary is generated with empty Einandqt - s , while for an expansion they are not empty .", "entities": []}, {"text": "tation influences the relevance of a sentence with respect to the current input query .", "entities": []}, {"text": "Query - MMR score weighting .", "entities": []}, {"text": "MMR has been shown to be effective in MDS , where information repeats across documents .", "entities": []}, {"text": "It aims to select a salient sentence for a summary , that is non - redundant to previous summary sentences .", "entities": []}, {"text": "We extend standard MMR so that the importance of the sentence is in regards to both the document set and the query .", "entities": []}, {"text": "Formally , the query - focused MMR function defines a score mt jfor each sjat time step tas follows : mt j = \u03bb\u00b7BISIM(sj , D , qt ) \u2212(1\u2212\u03bb)\u00b7maxe\u2208EtSIM(sj , e)(1 ) BISIM(sj , D , qt ) = \u03b2\u00b7SIM(sj , D\u2295 )", "entities": []}, {"text": "+ ( 1\u2212\u03b2)\u00b7SIM(sj , qt)(2 ) where \u03bb\u2208[0,1]balances salience and redundancy and\u03b2\u2208[0,1]balances a sentence \u2019s salience within its document set and its resemblance to the current query .", "entities": []}, {"text": "SIM(x , y)measures the similarity of texts xandy , andD\u2295is a fully concatenated version of document set D. Following findings of Mao et al . ( 2020 ) , SIMcomputes cosine similarity between the two compared texts \u2019 TF - IDF vectors .", "entities": []}, {"text": "Redundancy to previous sentences is computed as the highest similarity - score against any of the previous sentences .", "entities": []}, {"text": "We set \u03bb= 0.6(following Lebanoff et al . , 2018 ) and \u03b2= 0.5(see", "entities": []}, {"text": "Appendix B.3).The query - focused MMR scores are incorporated into MSumm by softly attending on the sentence representations with their respective translated query - focused MMR scores : \u00b5t = softmax ( MLP(mt ) ) ( 3 ) \u02c6ct j=\u00b5t jct j ( 4 ) State representation .", "entities": [[30, 31, "MethodName", "softmax"]]}, {"text": "At time t , a representationztof the summary - so - far is computed by applying an LSTM encoder on { cidx(ein 1 ) , ... , cidx(ein k ) , cidx(eout 1 ) , ... , cidx(eout t\u22121 ) } , i.e. , on the plain sentence representations of Et , where idx(e)is the index of sentence", "entities": [[17, 18, "MethodName", "LSTM"]]}, {"text": "e. Then , a state representation gtconsiders ztand all sentence representations with the glimpse operation ( Vinyals et al . , 2016 ): at j = v1tanh(W1\u02c6ct j+W2zt ) ( 5 ) \u03b1t = softmax ( at ) ( 6 ) gt=/summationdisplay j\u03b1t jW1\u02c6ct j ( 7 ) where v1,W1andW2are model parameters , and atrepresents the vector composed of at j. Finally , a sentence sjat time tis assigned a selection probability softmax ( pt)jsuch that : pt j=/braceleft\uf8ecigg v2tanh(W3\u02c6ct j+W4gt)ifsj/\u2208Et \u2212\u221e otherwise(8 ) where v2,W3andW4are model parameters .", "entities": [[34, 35, "MethodName", "softmax"], [72, 73, "MethodName", "softmax"]]}, {"text": "Reinforcement learning .", "entities": []}, {"text": "AsMSumm \u2019s goal is to incrementally generate a query - assisted2554", "entities": []}, {"text": "summary , it should strive to optimize ( 1 ) nonredundant salient - sentence extraction and ( 2 ) queryto - sentence similarity , that can be appraised with ROUGE ( Lin , 2004 ) and text - similarity metrics , respectively .", "entities": []}, {"text": "A policy gradient - based RL approach ( Williams , 1992 ) allows optimizing on such nondifferentiable metrics .", "entities": []}, {"text": "Specifically , we adopt the Advantage Actor Critic method ( Mnih et al . , 2016 ) for policy learning , and a dual - reward procedure ( Pasunuru and Bansal , 2018 ) to alternate between the summary and query - similarity rewards .", "entities": []}, {"text": "At time step t , for selected sentence eout t(based onsoftmax ( pt ) ) , reward rtis computed and weighted into MSumm \u2019s loss function .", "entities": [[24, 25, "MetricName", "loss"]]}, {"text": "The reward function alternates , from one train batch to the next , between ROUGE \u2206(eout t , Et , R)and QSIM(eout t , qt ) .", "entities": []}, {"text": "The former computes the ROUGE difference before adding ettoEtand after : ROUGE \u2206(eout t , Et , R ) = ROUGE ( ( Et\u222aeout t)\u2295,R)\u2212ROUGE ( E\u2295 t , R)(9 ) A larger ROUGE \u2206value implies that etconcisely adds more information onto Et , with respect to topic reference summaries R. We use ROUGE-", "entities": []}, {"text": "1 F1as", "entities": []}, {"text": "the ROUGE function here .", "entities": []}, {"text": "The querysimilarity reward function QSIM(eout t , qt ) = avg(SEMSIM ( eout t , qt),LEXSIM ( eout t , qt))(10 ) computes an average of semantic and lexical similarities between the selected sentence and corresponding query .", "entities": []}, {"text": "SEMSIM computes the cosine similarity between the average of word embeddings ( spaCy : Honnibal and Montani , 2021 ) of eout tand that of qt .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "For lexical similarity , LEXSIM ( eout t , qt ) = avg(Rp 1(eout t , qt ) ,", "entities": []}, {"text": "Rp 2(eout t , qt ) , Rp L(eout t , qt))(11 ) is the average of ROUGE -1 , 2 and L precision scores between sentence and query .", "entities": []}, {"text": "By alternating between the two rewards , we train a sentenceselection policy in MSumm to balance summary informativeness and adherence to queries .", "entities": []}, {"text": "Overall system .", "entities": []}, {"text": "OurMSumm model adopts its base architecture from Mao et al .", "entities": []}, {"text": "( 2020 ) ( for generic MDS ) .", "entities": []}, {"text": "Chiefly , we modify their model for handling an input query - sequence and a sentencehistory , and employ a different summarization reward function .", "entities": [[21, 22, "TaskName", "summarization"]]}, {"text": "The query is incorporated in the sentence representation , in the new query - focused MMR function and in the dual - reward mechanism .", "entities": []}, {"text": "3.3 Model Training Pre - training .", "entities": []}, {"text": "To provide a warm start for trainingMSumm , a reduced version of MSumm is first pre - trained for generic extractive single - document summarization using the large - scale CNN / Daily Mail corpus ( Hermann et al . , 2015 ) , as proposed by Chen and Bansal ( 2018 ) .", "entities": [[23, 25, "TaskName", "document summarization"], [30, 34, "DatasetName", "CNN / Daily Mail"]]}, {"text": "The reduced model pre - trains the full model for contextual sentence representation and for salient - sentence selection in the single - document generic setting .", "entities": []}, {"text": "See Appendix B.1 for precise technical details .", "entities": []}, {"text": "Training data .", "entities": []}, {"text": "After pre - training the reduced version of MSumm , we train the full model using the DUC 2007 MDS dataset , with modifications for our query - assisted MDS task .", "entities": [[17, 19, "DatasetName", "DUC 2007"]]}, {"text": "The dataset includes 45 topics ( split into 35/10 train / val ) , each containing 25 documents and 4 reference summaries .", "entities": []}, {"text": "For each topic , we generate an \u201c oracle \u201d extractive summary by greedily aggregating 10 sentences fromD , that maximizes the ROUGE \u2206-1recall against R. Then for each sentence , we extract a bi- or trigram that is most lexically - unique to the sentence , in comparison to all other sentences in D.", "entities": []}, {"text": "This yields a sequence of 10 \u201c queries \u201d that could easily render the corresponding oracle summary .", "entities": []}, {"text": "The intuition for this approach is that it would teach MSumm that it is worthwhile to consider a given query when selecting a sentence that is informative with respect to the reference summaries .", "entities": []}, {"text": "This further assists in fulfilling the dual requirements of selecting a globally informative sentence that also adheres to the query.4Appendix B.3 discusses usage of different query types for training .", "entities": []}, {"text": "Validation metric .", "entities": []}, {"text": "As the interactive session progresses , a recall curve emerges , that maps the ROUGE recall score ( here ROUGE -1 ) versus the expanding summary token - length .", "entities": []}, {"text": "Once the session halts , the area under the curve indicates the efficacy of the session for information exposure .", "entities": []}, {"text": "A higher value implies faster unveiling of salient 4Seemingly , the most natural approach would be to train the model with queries from real sessions ( collected using a different system ) .", "entities": []}, {"text": "However , a session \u2019s queries are dependent on outputs previously produced by the used system .", "entities": []}, {"text": "Hence , these do not benefit the training process more than a synthesized sequence of queries.2555", "entities": []}, {"text": "information .", "entities": []}, {"text": "Normalizing by the final summary length allows approximate comparability between different length sessions .", "entities": []}, {"text": "We hence use the average ( over topics ) length - normalized area under the recall curve for validating the training progress .", "entities": []}, {"text": "4 Suggested Queries Extraction Model 4.1 Subtask Formulation We now consider the second subtask of INTSUMM : generating lists of suggested queries .", "entities": []}, {"text": "The list is regenerated after every interaction , to yield queries that focus on sub - topics that were not yet explored .", "entities": []}, {"text": "Reusing the notations of MSumm in \u00a7 3 , we define a model , MSugg , for suggested queries list generation , that receives an input tuple ( D , Ein , m ) ( notice that a query is not needed here ) .", "entities": []}, {"text": "Here , the jthphrase inDis denoted \u03c1j , when the documents inDare concatenated , and accordingly , history Einis a list of phrases extracted from the session \u2019s current accumulated summary .", "entities": []}, {"text": "mis the number of suggested queries to output .", "entities": []}, {"text": "The model outputs phrase sequence Eout={eout 1 , eout 2 , ... , eout m}from D , accounting for history Ein .", "entities": []}, {"text": "As in MSumm \u2019s setting , Dis paired with a set of generic reference summaries R. 4.2 Model Architecture We adopt and adjust the architecture in \u00a7 3.2 for this subtask .", "entities": []}, {"text": "Similar to MSumm , MSugg selects input units one - by - one considering a history , with the main difference being the absence of query injection .", "entities": []}, {"text": "Additionally , inputs and outputs are processed on the phrase- rather than the sentence level .", "entities": []}, {"text": "Phrase and state representation .", "entities": []}, {"text": "For the given document set , all noun phrases are extracted using a standard part - of - speech regular expression method ( Mihalcea and Tarau , 2004 ; Wan and Xiao , 2008 ) .", "entities": [[14, 17, "DatasetName", "part - of"]]}, {"text": "We obtain document - level contextual phrase embeddings , cjfor phrase \u03c1j , with the CNN and biLSTM networks , and softly attend the embeddings with a standard MMR score : mt j = \u03bb\u00b7SIM(\u03c1j , D\u2295 ) \u2212(1\u2212\u03bb)\u00b7maxe\u2208EtSIM(\u03c1j , e)(12 )", "entities": [[17, 18, "MethodName", "biLSTM"]]}, {"text": "The MMR - based phrase representations then pass through the glimpse attention procedure , which culminates in the phrase probability distribution for selecting the next output phrase .", "entities": []}, {"text": "Reinforcement learning .", "entities": []}, {"text": "The policy in MSugg is trained with a single reward function that measures how prominent the selected phrase is within the reference summaries , and how different it is from previously seen phrases .", "entities": []}, {"text": "Formally , at time step t , the reward rtof selected phrase eout tis : rt = PF(eout t , R)\u2212\u03b31\u00b7PFMAX(eout t , Ein , R ) \u2212\u03b32\u00b7PFMAX(eout t , Et\\Ein , R)(13 ) PF(eout t , R )", "entities": []}, {"text": "= avgr\u2208R(avg w\u2208eout tTF(w , r))(14 ) PFMAX(eout t , L , R )", "entities": []}, {"text": "= maxe\u2208LPF(eout t\u2229e , R ) ( 15 ) where TF(w , r)is the relative frequency of word w in reference summary r. Namely , PFcomputes the average term frequency of a phrase over its words and across the reference summaries , as an estimate of the phrase importance within the topic .", "entities": []}, {"text": "PFMAX computes the highest PFagainst a list of phrases , which is used to lower the reward of a phrase that is redundant to phrases used earlier .", "entities": []}, {"text": "Different weights are given to the PFMAXagainst the input history ( \u03b31 ) and that of the phrases output so far ( \u03b32 ) .", "entities": []}, {"text": "4.3 Model Training Similarly to MSumm , we first pre - train the base model to get a warm start on embedding formation and salience detection .", "entities": []}, {"text": "The reduced architecture of MSumm andMSugg for pre - training are identical .", "entities": []}, {"text": "We use the same DUC 2007 training data , with document sets and reference summaries , and additionally prepare three \u201c histories \u201d per topic : one empty and two non - empty .", "entities": [[4, 6, "DatasetName", "DUC 2007"]]}, {"text": "An empty history mimics generating a session \u2019s initial list of suggested queries , while a non - empty history trains the model to consider previously known information .", "entities": []}, {"text": "Training with two non - empty histories per topic prepares a model for varying informational states .", "entities": []}, {"text": "These are curated from a generic summary ( from a trained MSumm model ) that is truncated at two random sentence - lengths between 1 and 12 .", "entities": []}, {"text": "Overall , the model is trained on three versions of each topic , each time with a different history .", "entities": []}, {"text": "Similarly to MSumm , validation is guided by the average normalized area under the recall curve .", "entities": []}, {"text": "Here , the accumulating rtscores from Equation 13 are used as the recall of the expanding suggested queries list .", "entities": []}, {"text": "I.e. , a higher reward means better suggested queries are output earlier .", "entities": []}, {"text": "The AUC is normalized with the total token - length of all suggested queries to mitigate for lengthy phrase extractions.2556", "entities": [[1, 2, "MetricName", "AUC"]]}, {"text": "5 Experiments We ran several experiments for the assessment of our MSumm andMSugg models , applying the INTSUMM evaluation framework of Shapira et al .", "entities": []}, {"text": "( 2021b ) .", "entities": []}, {"text": "The goals of the experiments are to compare varying configurations of our models and to evaluate against an INTSUMM baseline system .", "entities": []}, {"text": "The experiments include both simulations and interactive sessions with human users .", "entities": []}, {"text": "5.1 Compared Algorithms TheMSumm model architecture ( \u00a7 3.2 ) has several configurable components : encoding the query into sentences , considering the query in the MMR function ( both at train and inference time ) , and the dual reward mechanism .", "entities": []}, {"text": "We compared several variations of these using simulations , presented in \u00a7 5.2 .", "entities": []}, {"text": "In addition , we compare , both via simulations ( \u00a7 5.2 ) and real sessions ( \u00a7 5.3 ) , against the ( betterperforming ) baseline system in ( Shapira et al . , 2021b ) , named S2.S2 \u2019s initial summary algorithm is TextRank , and the query - response generator extracts sentences via lexical+semantic similarity to the query , somewhat resembling QSIMin Equation 10 , fully neglecting the summary - so - far , in contrast toMSumm .S2 \u2019s suggested queries list contains TextRank \u2019s top salient topic phrases .", "entities": []}, {"text": "Since these too do not account for the summary - so - far , they are computed at the session beginning and are not updated along the session , in contrast to MSugg .", "entities": []}, {"text": "5.2 Simulated Experiments TheINTSUMM task involves human users by definition .", "entities": []}, {"text": "Nevertheless , running on simulated query lists and session histories is pertinent for efficient system evaluation and comparison of methods .", "entities": []}, {"text": "To simulate the query - assisted summarization algorithms , we utilize the real sessions recorded by Shapira et al .", "entities": [[6, 7, "TaskName", "summarization"]]}, {"text": "( 2021b ): 3 - 4 user sessions on 20 topics from DUC 2006 collected with S2 .", "entities": [[12, 14, "DatasetName", "DUC 2006"]]}, {"text": "In our simulation , each summary - so - far from a recorded session is fed as input to the system together with the following recorded user query .", "entities": []}, {"text": "We then measure Rrecall 1\u2206(difference of ROUGE- 1recall incurred by the query response compared with the input summary - so - far ) .", "entities": []}, {"text": "Additionally , we use RF1 1 ( ROUGE- 1F1 ) for initial summary informativeness .", "entities": []}, {"text": "Both are measured w.r.t .", "entities": []}, {"text": "the reference summaries , normalized by the output length , and averaged per session recording , and then over all sessions and topics , to get an overall system infor - mativeness score .", "entities": []}, {"text": "We also measure system queryresponsiveness using the QSIMmetric .", "entities": []}, {"text": "Table 1 presents a representative partial ablation of the MSumm model .", "entities": []}, {"text": "All variants were configured to output sentences of up to 30 tokens , initial summaries are 75 tokens , and query responses are 2 sentences .", "entities": []}, {"text": "Configurations i - ivuse the query in training , while vandvido not .", "entities": []}, {"text": "Each configuration is measured for informativeness ( columns marked with\u2020 ) , and for query - responsiveness ( QSIMcolumn ) .", "entities": []}, {"text": "Out of configurations i - iv , config .", "entities": []}, {"text": "i , where we employ all mechanisms for query inclusion , yields the best overall scores in both informativeness and query - responsiveness , despite the inherent tradeoff between the two .", "entities": []}, {"text": "In the second set of configurations ( v - vi ) , we observe that ignoring the query at train time substantially degrades queryresponsiveness , and this is expectedly further exacerbated when also ignoring the query at inference time .", "entities": []}, {"text": "However , disregarding the query gives more informative expansions with respect to reference summaries , since the model was trained only to optimize content informativeness , and is less likely to sidetrack to the query - related information .", "entities": []}, {"text": "Compared to S2(last row ) , our model significantly improves informativeness .", "entities": []}, {"text": "Queryresponsiveness is better in the S2baseline since its query - response generator simply invokes a function similar to QSIM , but for the price of lower informativeness .", "entities": []}, {"text": "Still , this does not lead to inferior overall user experience , see \u00a7 5.3 .", "entities": []}, {"text": "5.3 Real Session Collection and Evaluation We collect real user sessions via controlled crowdsourcing ( which provides high quality work , see Appendix D ) with the use of an INTSUMM web application5running either our MSumm + MSugg models or the S2baseline algorithms , enabling a comparative assessment of the two systems .", "entities": []}, {"text": "Notably , our algorithms have the low latency required for the interactive setting ( Attig et al . , 2017 ) , i.e. , responding almost immediately.6 Using the DUC 2006 INTSUMM test set , we prepared two complementing user sets of 20 topics , each with 10 of the topics to be run on our system and the other 10 on the baseline .", "entities": [[29, 31, "DatasetName", "DUC 2006"]]}, {"text": "We apply the evaluation metrics of Shapira et al .", "entities": []}, {"text": "( 2021b ): ( 1 ) The 5Minimally modified from ( Shapira et al . , 2021b ) to support updating the suggested queries list after each interaction .", "entities": []}, {"text": "6MSumm generates summaries in under a second and MSugg prepares the list of suggested queries in a few seconds .", "entities": []}, {"text": "See Appendix E.2 for more details.2557", "entities": []}, {"text": "MSumm Model Configuration Simulation Results ( \u2020=informativeness metric , R1 = ROUGE-1 ) # Query in EncodingQuery in MMRQuery in Reward ( Dual)Query in MMR at Inference\u2020Initial Summ Norm RF1 1(\u00d710\u22123)Initial Summ Token - Length\u2020Expansion Norm Rrecall 1\u2206(\u00d710\u22123)Expansion Token - Len . QSIMQuery Responsiveness i.", "entities": [[11, 12, "MetricName", "ROUGE-1"]]}, {"text": "yes yes yes yes 3.09(\u00b10.11 ) 86.7(5.6 ) 0.913(\u00b10.055 ) 49.7(2.7)0.488(\u00b10.021 ) ii .", "entities": []}, {"text": "yes yes", "entities": []}, {"text": "no yes 3.04(\u00b10.12 ) 87.4(8.3 ) 0.897(\u00b10.054 ) 49.4(2.7)0.482(\u00b10.022 ) iii .", "entities": []}, {"text": "yes no", "entities": []}, {"text": "no yes 3.00(\u00b10.14 ) 88.0(8.7 ) 0.892(\u00b10.058 ) 50.1(2.9)0.479(\u00b10.020 ) iv .", "entities": []}, {"text": "no", "entities": []}, {"text": "yes yes yes 2.98(\u00b10.17 ) 85.1(6.5 ) 0.892(\u00b10.057 ) 51.3(2.8)0.462(\u00b10.025 )", "entities": []}, {"text": "v. no no no yes 3.05(\u00b10.12 ) 85.4(8.1 ) 0.955(\u00b10.046 ) 51.8(2.9)0.423(\u00b10.027 ) vi .", "entities": []}, {"text": "no no no no 3.05(\u00b10.12 ) 85.4(8.1 ) 0.988(\u00b10.056 ) 52.8(4.0)0.311(\u00b10.023 ) S2Baseline ( Shapira et al . , 2021b ) 2.75(\u00b10.20 ) 85.1(21.8 ) 0.799(\u00b10.040 ) 49.1(2.8)0.601(\u00b10.021 )", "entities": []}, {"text": "Table 1 : Simulation results on previously collected sessions , yielding a partial ablation of our MSumm model , and the results on the baseline system which was originally used to collect those sessions .", "entities": []}, {"text": "Intervals at 95 % confidence .", "entities": []}, {"text": "Metric Ours S2Baseline", "entities": []}, {"text": "Rr 1AUC @ [ 106,250 ] 43.42(\u00b11.54)40.01(\u00b11.52 ) RF1 1@ initial 0.256(\u00b1.011)0.231(\u00b10.014 ) RF1 1@250 0.396(\u00b1.015)0.378(\u00b1.015 ) QSIMquery - resp . 0.471(\u00b1.028)0.623(\u00b1.023", "entities": []}, {"text": ")", "entities": []}, {"text": "Manual query - resp .", "entities": []}, {"text": "3.96(\u00b10.19)4.03(\u00b10.23 )", "entities": []}, {"text": "Manual UMUX - Lite 78.9(\u00b12.5 ) 78.6(\u00b13.4 ) Table 2 : Average scores of our system ( configuration v ) and baseline system S2on actual user sessions .", "entities": []}, {"text": "Our system exposes topical information better , while the user experience is very good despite the slight degradation in query - responsiveness .", "entities": []}, {"text": "Intervals at 95 % confidence .", "entities": []}, {"text": "area under the sessions \u2019 ROUGE recall curves , in a common word - length interval across all sessions and topics , which demonstrates how fast salient information is exposed in sessions .", "entities": []}, {"text": "( 2 ) ROUGE F1at the initial summary and at 250 tokens , that indicate how effectively the interactive system can generate summaries at pre - specified , comparable lengths .", "entities": []}, {"text": "( 3 ) Manually assigned query - responsiveness score ( 1 to 5 scale ) , which expresses how well users think the system responded to their requests .", "entities": []}, {"text": "And ( 4 ) manual UMUX - Lite ( Lewis et al . , 2013 ) score for system usability ( effectiveness and ease of use ) , where 68is considered \u201c acceptable \u201d and 80.3is considered \u201c excellent \u201d .", "entities": []}, {"text": "We also measure automatic query - responsiveness with QSIM.7 We conducted two such comparative collection and assessment experiments , either employing MSumm configuration vori , namely the best of the two configuration sets .", "entities": []}, {"text": "In both cases , the MSugg model used was set with \u03b31= 0.5and\u03b32= 0.9 after some hyperparameter tuning ( Appendix B.4 ) .", "entities": []}, {"text": "The first experiment ( with configuration v ) is described here , and the other in Appendix E.1 .", "entities": []}, {"text": "We hired 6 qualified workers using the controlled crowdsourcing procedure , and collected 2 - 3 sessions per topic per system ( 111 total sessions ) .", "entities": []}, {"text": "In 7While QSIMis a reasonable automatic measure for estimation of query - responsiveness , it is left for future work to assess its true reliability for such use.the sessions , users explore their given topic by submitting queries with a common generic informational goal in mind ( Appendix D ) .", "entities": []}, {"text": "Overall system assessment .", "entities": []}, {"text": "Table 2 , presenting average scores over the collected sessions , shows that our system is significantly more effective for exposing salient information , as depicted in the first three rows .", "entities": []}, {"text": "Users indicate a slight degradation in query - responsiveness of our system , consistent with QSIMscores ( row 4 - 5 ) .", "entities": []}, {"text": "Note that the observed difference in QSIMscores , between simulations and user sessions , partly stems from the fact that they were computed over different sets of queries .", "entities": []}, {"text": "The varying queries issued by the users in user sessions form a less stable query responsiveness comparison than the one in Table 1 , where QSIM scores are computed using consistent queries for all systems .", "entities": []}, {"text": "Despite the gap in QSIMscores between our system and S2 in Table 2 , the overall usability scores are slightly better ( last row ) .", "entities": []}, {"text": "This may suggest that users appreciate the informativeness of the produced summary even when they are aware that the summary is less biased on their queries ; thus our system improves informativeness while still providing a favorable user experience .", "entities": []}, {"text": "Assessment of suggested queries functionality .", "entities": []}, {"text": "We analyzed the types of queries users submitted throughout their sessions , to assess the utility of updating suggested queries , with MSugg , as opposed to a static list of suggestions , with S2 .", "entities": []}, {"text": "To that end , we tallied suggested query clicks and query submissions via other modes , binning the tallies to three sequential temporal segments within their respective sessions ( Appendix E.3 ) .", "entities": []}, {"text": "We found that , on average , the usage of suggested query clicks increased by ~13 % when nearing the end of a session withMSugg , and conversely decreased by ~24 % withS2 .", "entities": []}, {"text": "While the decrease in use of the static list is expected , since appealing queries are likely exhausted earlier in a session , it is encouraging to2558", "entities": []}, {"text": "witness the usefulness of updated queries as the session progresses .", "entities": []}, {"text": "This behavior suggests that the updated list contains suggested queries that are indeed engaging for learning more about the topic .", "entities": []}, {"text": "6 Conclusion Interactive summarization for information exploration is a task that requires compliance to user requests and session history , while comprehensively handling a large input document set .", "entities": [[3, 4, "TaskName", "summarization"]]}, {"text": "These requirements pose a challenge for advanced text processing methods due to the need for fast reaction time .", "entities": []}, {"text": "We present novel deep reinforcement learning based algorithms that answer to the task requirements , improving salient information exposure while satisfying user queries and keeping user experience positive .", "entities": []}, {"text": "We note that while MSumm is designed for the INTSUMM task , it may potentially be serviceable for standard MDS , QFS , update summarization and combinations thereof .", "entities": [[24, 25, "TaskName", "summarization"]]}, {"text": "This can be accommodated by a proper choice of input , e.g. , QFS can be addressed by giving MSumm as input a query , an empty history and target summary length .", "entities": []}, {"text": "In future work , we may study the performance of our solutions for such tasks , as well as strive to further improve their performance on both ends of the INTSUMM task \u2013 selecting topically salient information and responding to user queries .", "entities": []}, {"text": "Acknowledgements We thank the anonymous reviewers for their constructive comments and suggestions .", "entities": []}, {"text": "This work was supported in part by Intel Labs ; by the Israel Science Foundation ( grants no . 2827/21 and 2015/21 ) ; by a grant from the Israel Ministry of Science and Technology ; by the NSF - CAREER Award # 1846185 ; and by a Microsoft PhD Fellowship .", "entities": []}, {"text": "References Shaun Anderson .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "How Fast Should A Website Load ?", "entities": []}, {"text": "https://www.hobo-web.co.uk/your-websitedesign-should-load-in-4-seconds .", "entities": []}, {"text": "Accessed : 202109 - 25 .", "entities": []}, {"text": "Christiane Attig , Nadine Rauh , Thomas Franke , and Josef F. Krems . 2017 .", "entities": []}, {"text": "System Latency Guidelines Then and Now \u2013 Is Zero Latency Really Considered Necessary ?", "entities": []}, {"text": "In Engineering Psychology and Cognitive Ergonomics : Cognition and Design , pages 3\u201314 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Tal Baumel , Raphael Cohen , and Michael Elhadad .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Query - Chain Focused Summarization .", "entities": [[4, 5, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 913\u2013922 , Baltimore , Maryland .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tal Baumel , Raphael Cohen , and Michael Elhadad .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Topic Concentration in Query Focused Summarization Datasets .", "entities": [[5, 6, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence , AAAI\u201916 , page 2573\u20132579 . AAAI Press .", "entities": []}, {"text": "Tal Baumel , Matan Eyal , and Michael Elhadad .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Query Focused Abstractive Summarization :", "entities": [[3, 4, "TaskName", "Summarization"]]}, {"text": "Incorporating Query Relevance , Multi - Document Coverage , and Summary Length Constraints into seq2seq Models.arXiv preprint arXiv:1801.07704 .", "entities": [[14, 15, "MethodName", "seq2seq"]]}, {"text": "Iz Beltagy , Matthew E. Peters , and Arman Cohan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Longformer :", "entities": [[0, 1, "MethodName", "Longformer"]]}, {"text": "The Long - Document Transformer .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "arXiv preprint arXiv:2004.05150 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "John Brooke .", "entities": []}, {"text": "1996 .", "entities": []}, {"text": "SUS - A quick and dirty usability scale .", "entities": []}, {"text": "Usability evaluation in industry , 189(194):4 \u2013 7 . Jaime Carbonell and Jade Goldstein .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "The Use of MMR , Diversity - Based Reranking for Reordering Documents and Producing Summaries .", "entities": []}, {"text": "In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR \u2019 98 , page 335\u2013336 , New York , NY , USA . Association for Computing Machinery .", "entities": [[7, 8, "DatasetName", "ACM"], [15, 17, "TaskName", "Information Retrieval"]]}, {"text": "Yen - Chun Chen and Mohit Bansal .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Fast Abstractive Summarization with Reinforce - Selected Sentence Rewriting .", "entities": [[2, 3, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 675\u2013686 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Janara Christensen , Stephen Soderland , Gagan Bansal , and Mausam .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Hierarchical Summarization : Scaling Up Multi - Document Summarization .", "entities": [[1, 2, "TaskName", "Summarization"], [5, 9, "TaskName", "Multi - Document Summarization"]]}, {"text": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 902\u2013912 , Baltimore , Maryland .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "G\u00fcnes Erkan and Dragomir R. Radev .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "LexRank : Graph - Based Lexical Centrality as Salience in Text Summarization .", "entities": [[10, 12, "TaskName", "Text Summarization"]]}, {"text": "Journal of Artificial Intelligence Research , 22(1):457\u2013479 .", "entities": []}, {"text": "Corina Florescu and Cornelia Caragea . 2017 .", "entities": []}, {"text": "PositionRank :", "entities": []}, {"text": "An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents .", "entities": [[4, 6, "TaskName", "Keyphrase Extraction"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1105\u20131115 , Vancouver , Canada . Association for Computational Linguistics.2559", "entities": []}, {"text": "Dan Gillick and Yang Liu . 2010 .", "entities": []}, {"text": "Non - Expert Evaluation of Summarization Systems is Risky .", "entities": [[5, 6, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the NAACL HLT 2010", "entities": []}, {"text": "Workshop on Creating Speech and Language Data with Amazon \u2019s Mechanical Turk , pages 148\u2013151 , Los Angeles .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Aria Haghighi and Lucy Vanderwende .", "entities": [[0, 1, "MethodName", "Aria"]]}, {"text": "2009 .", "entities": []}, {"text": "Exploring Content Models for Multi - Document Summarization .", "entities": [[4, 8, "TaskName", "Multi - Document Summarization"]]}, {"text": "InProceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics , pages 362\u2013370 , Boulder , Colorado . Association for Computational Linguistics .", "entities": []}, {"text": "Karl Moritz Hermann , Tom\u00e1\u0161 Ko \u02c7cisk\u00fd , Edward Grefenstette , Lasse Espeholt , Will Kay , Mustafa Suleyman , and Phil Blunsom .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Teaching Machines to Read and Comprehend .", "entities": []}, {"text": "In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 , NIPS\u201915 , page 1693\u20131701 , Cambridge , MA , USA . MIT Press .", "entities": [[21, 22, "DatasetName", "Cambridge"]]}, {"text": "Eran Hirsch , Alon Eirew , Ori Shapira , Avi Caciularu , Arie Cattan , Ori Ernst , Ramakanth Pasunuru , Hadar Ronen , Mohit Bansal , and Ido Dagan .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "iFacetSum : Coreference - based Interactive Faceted Summarization for Multi - Document Exploration .", "entities": [[7, 8, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 283\u2013297 , Online and Punta Cana , Dominican Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Matthew Honnibal and Ines Montani .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Linguistic Features - spaCy Usage Documentation .", "entities": []}, {"text": "https://spacy.io/ usage / linguistic - features # vectors - similarity .", "entities": []}, {"text": "Accessed : 202111 - 01 .", "entities": []}, {"text": "Zhiheng Huang , Wei Xu , and Kai Yu . 2015 .", "entities": []}, {"text": "Bidirectional LSTM - CRF Models for Sequence Tagging .", "entities": [[0, 2, "MethodName", "Bidirectional LSTM"], [3, 4, "MethodName", "CRF"]]}, {"text": "CoRR , abs/1508.01991 .", "entities": []}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional Neural Networks for Sentence Classification .", "entities": [[4, 6, "TaskName", "Sentence Classification"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1746\u20131751 , Doha , Qatar .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sayali Kulkarni , Sheide Chammas , Wan Zhu , Fei Sha , and Eugene Ie . 2021 .", "entities": []}, {"text": "CoMSum and SIBERT :", "entities": [[0, 1, "DatasetName", "CoMSum"]]}, {"text": "A Dataset and Neural Model for Query - Based Multidocument Summarization .", "entities": [[10, 11, "TaskName", "Summarization"]]}, {"text": "In Document Analysis and Recognition \u2013 ICDAR 2021 , pages 84\u201398 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Md Tahmid Rahman Laskar , Enamul Hoque , and Jimmy Xiangji Huang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "WSL - DS : Weakly Supervised Learning with Distant Supervision for Query Focused Multi - Document Abstractive Summarization .", "entities": [[17, 18, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics , pages 5647\u20135654 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Logan Lebanoff , Kaiqiang Song , and Fei Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Adapting the Neural Encoder - Decoder Framework from Single to Multi - Document Summarization .", "entities": [[10, 14, "TaskName", "Multi - Document Summarization"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 4131\u20134141 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Anton Leuski , Chin - Yew Lin , and Eduard Hovy .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "iNeATS :", "entities": []}, {"text": "Interactive Multi - Document Summarization .", "entities": [[1, 5, "TaskName", "Multi - Document Summarization"]]}, {"text": "InThe Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics , pages 125\u2013128 , Sapporo , Japan .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "James R. Lewis , Brian S. Utesch , and Deborah E. Maher .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "UMUX - LITE :", "entities": []}, {"text": "When There \u2019s No Time for the SUS .", "entities": []}, {"text": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , CHI \u2019 13 , page 2099\u20132102 , New York , NY , USA .", "entities": []}, {"text": "Association for Computing Machinery .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "BART :", "entities": [[0, 1, "MethodName", "BART"]]}, {"text": "Denoising Sequence - to - Sequence Pretraining for Natural Language Generation , Translation , and Comprehension .", "entities": [[0, 1, "TaskName", "Denoising"], [12, 13, "TaskName", "Translation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871\u20137880 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chin - Yew Lin .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "ROUGE :", "entities": []}, {"text": "A Package for Automatic Evaluation of Summaries .", "entities": []}, {"text": "In Text Summarization Branches Out , pages 74\u201381 , Barcelona , Spain .", "entities": [[1, 3, "TaskName", "Text Summarization"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jimmy Lin , Salman Mohammed , Royal Sequiera , Luchen Tan , Nimesh Ghelani , Mustafa Abualsaud , Richard McCreadie , Dmitrijs Milajevs , and Ellen V oorhees .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Overview of the TREC 2017 RealTime Summarization Track .", "entities": [[3, 4, "DatasetName", "TREC"], [6, 7, "TaskName", "Summarization"]]}, {"text": "Yuning Mao , Yanru Qu , Yiqing Xie , Xiang Ren , and Jiawei Han . 2020 .", "entities": []}, {"text": "Multi - document Summarization with Maximal Marginal Relevance - guided Reinforcement Learning .", "entities": [[0, 4, "TaskName", "Multi - document Summarization"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1737\u20131751 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Richard McCreadie , Craig Macdonald , and Iadh Ounis .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Incremental Update Summarization : Adaptive Sentence Selection Based on Prevalence and Novelty .", "entities": [[2, 3, "TaskName", "Summarization"]]}, {"text": "InProceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management , CIKM \u2019 14 , page 301\u2013310 , New York , NY , USA . Association for Computing Machinery .", "entities": [[4, 5, "DatasetName", "ACM"], [13, 14, "TaskName", "Management"]]}, {"text": "Rada Mihalcea and Paul Tarau .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "TextRank : Bringing Order into Text .", "entities": []}, {"text": "In Proceedings of the 20042560", "entities": []}, {"text": "Conference on Empirical Methods in Natural Language Processing , pages 404\u2013411 , Barcelona , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "V olodymyr Mnih , Adria Puigdomenech Badia , Mehdi Mirza , Alex Graves , Timothy Lillicrap , Tim Harley , David Silver , and Koray Kavukcuoglu .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Asynchronous Methods for Deep Reinforcement Learning .", "entities": []}, {"text": "InProceedings of The 33rd International Conference on Machine Learning , volume 48 of Proceedings of Machine Learning Research , pages 1928\u20131937 , New York , New York , USA . PMLR .", "entities": []}, {"text": "Ramakanth Pasunuru and Mohit Bansal .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "MultiReward Reinforced Summarization with Saliency and Entailment .", "entities": [[2, 3, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 646\u2013653 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ramakanth Pasunuru , Asli Celikyilmaz , Michel Galley , Chenyan Xiong , Yizhe Zhang , Mohit Bansal , and Jianfeng Gao .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "Data Augmentation for Abstractive Query - Focused Multi - Document Summarization .", "entities": [[0, 2, "TaskName", "Data Augmentation"], [7, 11, "TaskName", "Multi - Document Summarization"]]}, {"text": "Proceedings of the AAAI Conference on Artificial Intelligence , 35(15):13666\u201313674 .", "entities": []}, {"text": "Ramakanth Pasunuru , Mengwen Liu , Mohit Bansal , Sujith Ravi , and Markus Dreyer . 2021b .", "entities": []}, {"text": "Efficiently Summarizing Text and Graph Encodings of MultiDocument Clusters .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 4768\u20134779 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Marc\u2019Aurelio Ranzato , Sumit Chopra , Michael Auli , and Wojciech Zaremba .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Sequence Level Training with Recurrent Neural Networks .", "entities": []}, {"text": "In ICLR .", "entities": []}, {"text": "Nils Reimers and Iryna Gurevych .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SentenceBERT : Sentence Embeddings using Siamese BERTNetworks .", "entities": [[2, 4, "TaskName", "Sentence Embeddings"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3982\u20133992 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ori Shapira , Ramakanth Pasunuru , Ido Dagan , and Yael Amsterdamer .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "Multi - Document Keyphrase Extraction : A Literature Review and the First Dataset .", "entities": [[3, 5, "TaskName", "Keyphrase Extraction"]]}, {"text": "arXiv preprint arXiv:2110.01073 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ori Shapira , Ramakanth Pasunuru , Hadar Ronen , Mohit Bansal , Yael Amsterdamer , and Ido Dagan .", "entities": []}, {"text": "2021b .", "entities": []}, {"text": "Extending Multi - Document Summarization Evaluation to the Interactive Setting .", "entities": [[1, 5, "TaskName", "Multi - Document Summarization"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 657\u2013677 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ori Shapira , Hadar Ronen , Meni Adler , Yael Amsterdamer , Judit Bar - Ilan , and Ido Dagan . 2017 .", "entities": []}, {"text": "Interactive Abstractive Summarization for Event News Tweets .", "entities": [[2, 3, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 109\u2013114 , Copenhagen , Denmark .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Oriol Vinyals , Samy Bengio , and Manjunath Kudlur . 2016 .", "entities": []}, {"text": "Order Matters : Sequence to sequence for sets .", "entities": [[3, 6, "MethodName", "Sequence to sequence"]]}, {"text": "arXiv preprint arXiv:1511.06391 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xiaojun Wan and Jianguo Xiao . 2008 .", "entities": []}, {"text": "Single Document Keyphrase Extraction Using Neighborhood Knowledge .", "entities": [[2, 4, "TaskName", "Keyphrase Extraction"]]}, {"text": "In Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2 , AAAI\u201908 , page 855\u2013860 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Danqing Wang , Pengfei Liu , Yining Zheng , Xipeng Qiu , and Xuanjing Huang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Heterogeneous Graph Neural Networks for Extractive Document Summarization .", "entities": [[5, 8, "TaskName", "Extractive Document Summarization"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 6209\u20136219 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ronald J Williams .", "entities": []}, {"text": "1992 .", "entities": []}, {"text": "Simple Statistical GradientFollowing Algorithms for Connectionist Reinforcement Learning .", "entities": []}, {"text": "Machine Learning , 8(3):229\u2013256 .", "entities": []}, {"text": "Wen Xiao , Iz Beltagy , Giuseppe Carenini , and Arman Cohan . 2021 .", "entities": []}, {"text": "PRIMER :", "entities": []}, {"text": "Pyramid - based Masked Sentence Pre - training for Multi - document Summarization .", "entities": [[9, 13, "TaskName", "Multi - document Summarization"]]}, {"text": "arXiv preprint arXiv:2110.08499 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yujia Xie , Tianyi Zhou , Yi Mao , and Weizhu Chen .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Conditional Self - Attention for Query - based Summarization .", "entities": [[8, 9, "TaskName", "Summarization"]]}, {"text": "arXiv preprint arXiv:2002.07338 .2561", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "A Ethical Considerations Datasets .", "entities": []}, {"text": "The DUC 2006 and 2007 datasets were obtained according to the DUC website ( duc . nist.gov ) requirements .", "entities": [[1, 3, "DatasetName", "DUC 2006"]]}, {"text": "It was not possible for others to reconstruct the document sets and reference summaries of the dataset from the crowdsourcing tasks .", "entities": []}, {"text": "The datasets are composed of new articles mainly from the late 1990s from large news outlets , compiled by NIST .", "entities": []}, {"text": "All data exposed by our systems are directly extracted from those articles .", "entities": []}, {"text": "For extraction , we do not intentionally add in any rules for ignoring or boosting certain information due to an opinion .", "entities": []}, {"text": "Crowdsourcing .", "entities": []}, {"text": "Due to the need for English speaking workers , a location filter was set on the Amazon Mechanical Turk ( https://www . mturk.com ) tasks for the US , UK and Australia .", "entities": []}, {"text": "All tasks paid according to a $ 10 per hour wage , according to the estimated required time of each task .", "entities": []}, {"text": "The payment was either paid per assignment , or as a combination with a bonus .", "entities": []}, {"text": "Compute resources .", "entities": []}, {"text": "Our MSumm and MSugg models required between 2 and 20 hours of training ( usually around 4 hours ) , depending on the configuration .", "entities": []}, {"text": "We trained on one NVIDIA GeForce GTX 1080", "entities": []}, {"text": "Ti GPU with 11 GB memory .", "entities": []}, {"text": "The pretrained base model was trained once and reused in all subsequent training .", "entities": []}, {"text": "Outputting at inference time is computationally cheap : MSumm runs upto about 1 second , but mostly in a few hundred milliseconds , and MSugg runs upto about 7 seconds , but mostly in under 4 seconds .", "entities": []}, {"text": "Training with a batch size of 8 used about 3 GB GPU memory for MSumm , and about 9 GB memory for MSugg ( since there are many more input units per document set , i.e. , all noun phrases versus sentences ) .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}, {"text": "B Implementation Details B.1 Pre - training Technicalities To provide a warm start for training MSumm and MSugg , a reduced version of the models , which is the same for both , is first pre - trained for generic extractive single - document summarization using the CNN / Daily Mail corpus ( Hermann et al . , 2015 ) with about 287k samples , as proposed by Chen and Bansal ( 2018 ) .", "entities": [[43, 45, "TaskName", "document summarization"], [47, 51, "DatasetName", "CNN / Daily Mail"]]}, {"text": "In this reduced model , \u02c6ct jis replaced by cjin Equations 5 , 7 and 8 .", "entities": []}, {"text": "Further - more , there is a single reward function for learning the policy , computed per selected sentence eout t asROUGE -L F1w.r.t .", "entities": []}, {"text": "the ( single ) reference summary \u2019s sentence at index t.", "entities": []}, {"text": "The reduced model pre - trains the full model for contextual sentence representation and for salient - sentence selection in the single - document generic setting .", "entities": []}, {"text": "This allows training MSumm andMSugg with a relatively small dataset for their final purposes .", "entities": []}, {"text": "B.2 Training Technicalities Following ( Mao et al . , 2020 ) , the pre - trained base model is the rnn - ext + RL model from Chen and Bansal ( 2018 ) , and is trained like in Lebanoff et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "Both MSumm andMSugg are further trained on our adjusted DUC 2007 data using an Adam optimizer with a learning rate of 5e-4 and no weight decay .", "entities": [[9, 11, "DatasetName", "DUC 2007"], [14, 15, "MethodName", "Adam"], [15, 16, "HyperparameterName", "optimizer"], [18, 20, "HyperparameterName", "learning rate"], [24, 26, "MethodName", "weight decay"]]}, {"text": "A discount factor of 0.99 is used for the reinforcement learning rewards .", "entities": []}, {"text": "The batch size was 8 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "Training was halted once 30 consecutive epochs did not improve the validation score .", "entities": []}, {"text": "The MMR function within our models uses TFIDF vector cosine similarity for all SIMinstances ( in Equations 1 and 12 ) .", "entities": []}, {"text": "The TF - IDF vectorizer is initialized with the document set on which the MMR score is computed .", "entities": []}, {"text": "As is commonly practiced , selection of an output sentence / phrase eout tis done by sampling probability distribution pt(in Equation 8) at train time , and by extracting the maximum scoring sentence / phrase at inference time .", "entities": []}, {"text": "The MLP in Equation 3 transforms the MMR score with a feed - forward network with one - hidden layer of dimension 80 following ( Mao et al . , 2020 ) .", "entities": [[1, 2, "DatasetName", "MLP"]]}, {"text": "B.3 Query - Assisted Summarization Model Model configurations .", "entities": [[4, 5, "TaskName", "Summarization"]]}, {"text": "The architecture of the MSumm model and its training allowed for much creativity in the configuration process .", "entities": []}, {"text": "Other than the combinations mentioned in the paper in Table 1 , we also experimented with other components .", "entities": []}, {"text": "We list here many of the experiments , without formal results .", "entities": []}, {"text": "Anecdotes are taken by looking at validation scores and some eyeballing .", "entities": []}, {"text": "( 1 ) The \u03b2value in the query - focused MMR function in Equation 2 , that impacts the weight of the query on a sentence versus the document set on the sentence .", "entities": []}, {"text": "We tried out a few \u03b2values and mainly noticed that a value of 0.5kept validation results more stable across configurations , or kept training time shorter .", "entities": []}, {"text": "In our experiments , to cancel out this component ( both at training and inference2562", "entities": []}, {"text": "time ) , we simply set \u03b2=", "entities": []}, {"text": "1so that the query is not considered .", "entities": []}, {"text": "( 2 ) Different summary reward functions .", "entities": []}, {"text": "ROUGE \u2206recall ( instead of F1 ) was also a good alternative , but gave somewhat less stable results across configurations .", "entities": [[5, 6, "MetricName", "F1"]]}, {"text": "ROUGE ( not as \u2206 ) was also less stable with recall and F1 , and gave too short and irrelevant sentences with precision .", "entities": [[13, 14, "MetricName", "F1"]]}, {"text": "We also tried sentence level ROUGE -L , like in ( Mao et al . , 2020 ) , eventually outputting sentences that were much less compliant to queries .", "entities": []}, {"text": "( 3 ) Using only the query similarity reward instead of the dual reward mechanism worked surprisingly well .", "entities": []}, {"text": "This may be due to the queries on which the model was trained on .", "entities": []}, {"text": "These queries were very relevant to the gold reference summaries , hence possibly implicitly providing a strong signal to salient sentences within the document set .", "entities": []}, {"text": "Still , this was less productive than our final choice of reward .", "entities": []}, {"text": "( 4)Adding training data ( additional DUC MDS datasets ) did not impact the results .", "entities": []}, {"text": "Importantly , since DUC 2007 is most similar to the test DUC 2006 set , it seems to be more beneficial to include DUC 2007 in the training set .", "entities": [[3, 5, "DatasetName", "DUC 2007"], [11, 13, "DatasetName", "DUC 2006"], [23, 25, "DatasetName", "DUC 2007"]]}, {"text": "( 5 ) We also tried representing the query in the input by concatenating it \u2019s raw text to each input sentence before get the sentence representations .", "entities": []}, {"text": "( 6 ) To represent the sentences , we also tried using average w2v vectors ( Honnibal and Montani , 2021 ) and Sentence - BERT ( Reimers and Gurevych , 2019 ) instead of the CNN network .", "entities": [[25, 26, "MethodName", "BERT"]]}, {"text": "These did not show any apparent improvements , and were notably expensive in terms of execution time .", "entities": []}, {"text": "( 7 ) For the sentence similarity in the queryMMR component , we tried w2v and SentenceBERT representations instead of TF - IDF vectors .", "entities": []}, {"text": "Similarly to ( 6 ) , they did not show improvements over using TF - IDF , and were very time - costly .", "entities": []}, {"text": "( 8) Instead of the dual - reward mechanism that alternates between the two rewards from batch to batch , we also considered using a weighted average of the two rewards , consistently over all batches .", "entities": []}, {"text": "Further experimentation is required on this technique for a more conclusive judgment .", "entities": []}, {"text": "Queries used for training .", "entities": []}, {"text": "The queries used for training the MSumm model can affect the way it learns to respond to a query .", "entities": []}, {"text": "Seemingly , the most natural approach would be to train the model as close as possible to the model \u2019s use at inference time .", "entities": []}, {"text": "This would mean training MSumm withqueries from real sessions .", "entities": []}, {"text": "However , a session \u2019s queries are dependent on outputs previously produced by the used system .", "entities": []}, {"text": "It is therefore not certain that the sequence of queries from a different system \u2019s usage would necessarily benefit the training process when compared to a synthesized sequence of queries .", "entities": []}, {"text": "I.e. , it \u2019s not actually possible to train with \u201c real sessions \u201d in a conventional way .", "entities": []}, {"text": "Also , as stated in \u00a7 3.3 , the synthetic queries we eventually used direct the model to select salient sentences , which can support our dual - objectives : to get a sentence that is both globally salient to the topic , as well as responsive to the query .", "entities": []}, {"text": "We tried training on other query types , synthesized with various keyphrase extraction techniques , and found that our final choice of queries more consistently gave good results overall .", "entities": [[11, 13, "TaskName", "keyphrase extraction"]]}, {"text": "Sentence length .", "entities": []}, {"text": "We segmented the sentences in the document sets with the NLTK8sentence tokenizer , and removed sentences that contain quotes in them or do not end with a period .", "entities": []}, {"text": "During training we did not constrain the input sentences in any way .", "entities": []}, {"text": "Some of the configuration experiments described above were done to check how the configuration might influence the length of the selected sentences .", "entities": []}, {"text": "The best configurations , including the one we eventually used in our tests , tended to output somewhat longer sentences .", "entities": []}, {"text": "Very long sentences are usually tedious for human readers , and we hence limited the sentences to 30 tokens at inference time .", "entities": []}, {"text": "We found that this length constraint caused a slight degradation in simulation score results of our models , however still gave superior informativeness results compared to the baseline system .", "entities": []}, {"text": "Initial summary length .", "entities": []}, {"text": "Sentences are accumulated until surpassing 75 tokens .", "entities": []}, {"text": "Therefore summaries are not shorter than 75 tokens , but mostly not much longer than that .", "entities": []}, {"text": "B.4 Suggested Queries Extraction Model Model configurations .", "entities": []}, {"text": "We experimented with different configurations and hyper - parameter finetuning in the MSugg model as well .", "entities": []}, {"text": "Tuning was performed in accordance to the validation scores and generic keyphrase extraction scores on the MK - DUC-01 multi - document keyphrase extraction dataset of Shapira et al . ( 2021a ) .", "entities": [[11, 13, "TaskName", "keyphrase extraction"], [22, 24, "TaskName", "keyphrase extraction"]]}, {"text": "8https://www.nltk.org2563", "entities": []}, {"text": "( 1 ) In the reward function in Equation 13 , we set\u03b31= 0.5and\u03b32= 0.9 , i.e. , the preceding output phrases are more strongly accounted for than the phrases in the session history .", "entities": []}, {"text": "We tested several values between 0 and 1 for both hyper - parameters .", "entities": [[5, 6, "DatasetName", "0"]]}, {"text": "( 2 ) We implemented altered versions of the reward function in Equation 13 .", "entities": []}, {"text": "Instead of phrase unigram - level frequency , we tried computing the full phrase frequency and computing partial phrase frequency , i.e. , a maximal phrase template match within a reference summary .", "entities": []}, {"text": "All functions tested were adequate overall , though our final choice of reward function was closest to the keyphrase extraction task unigram overlap metric , and gave best results overall .", "entities": [[18, 20, "TaskName", "keyphrase extraction"]]}, {"text": "( 3 ) We also attempted noun phrase extraction with the spaCy9noun chunker and named entity recognizer .", "entities": []}, {"text": "This combined approach misses some noun phrases within the text , but mainly is also more computationally heavy than the simple POS regex search that we use .", "entities": []}, {"text": "Extracting phrases with regular - expression .", "entities": []}, {"text": "We extracted all noun - phrases from the document set by first mapping all tokens to their part - of - speech tags , and then applying a regularexpression chunker with regex : { ( < JJ > * < NN .", "entities": [[17, 20, "DatasetName", "part - of"]]}, {"text": "* >", "entities": []}, {"text": "+ < IN > ) ?", "entities": []}, {"text": "< JJ > * < NN .", "entities": []}, {"text": "* > + } .", "entities": []}, {"text": "These steps were accomplished with NLTK .", "entities": []}, {"text": "Phrase length .", "entities": []}, {"text": "There is no limit set on the phrase length .", "entities": []}, {"text": "We tried training and inferring with a phrase length constraint of 4 words , but found that this gave worse results overall .", "entities": []}, {"text": "History sentences to phrases .", "entities": []}, {"text": "MSugg works on thephrase level .", "entities": []}, {"text": "Meanwhile , in our extractive interactive setting , the history is a set of sentences already presented to the reader .", "entities": []}, {"text": "Therefore , when extracting phrases from D , we also link each phrase to its source sentence , and obtain Einby compiling the phrases linked from the history sentences .", "entities": []}, {"text": "C Dataset Notes While DUC 2006 ( our test set ) and 2007 ( our train / validation set ) were originally designed for the query - focused summarization task , they contain excessive topic concentration due to their long and descriptive topic queries ( Baumel et al . , 2016 ) .", "entities": [[4, 6, "DatasetName", "DUC 2006"], [28, 29, "TaskName", "summarization"]]}, {"text": "Hence , their reference summaries can practically be considered generic .", "entities": []}, {"text": "9https://spacy.io/D Session Collection Controlled crowdsourcing protocol .", "entities": []}, {"text": "We followed the controlled crowdsourcing protocol of Shapira et al .", "entities": []}, {"text": "( 2021b ) , which includes three steps : ( 1 ) a trap task for finding qualified workers ; ( 2 ) practice tasks for explaining the interface and the purpose , as well as reiterating the generic information goal ( see below ) during exploration ; ( 3 ) the session collection tasks .", "entities": []}, {"text": "We used the Amazon Mechanical Turk HITs prepared by Shapira et al .", "entities": []}, {"text": "( 2021b ) .", "entities": []}, {"text": "Process cost .", "entities": []}, {"text": "We paid $ 0.40 for a trap task assignment , with 400 assignments released , and $ 0.90 for a practice task assignment , with 28 assignments completed .", "entities": []}, {"text": "The session collection assignment paid $ 0.70 , and a bonus mainly according to the length of interaction and additional comments provided .", "entities": []}, {"text": "The bonus was between $ 0.15 and $ 0.35 .", "entities": []}, {"text": "A total of 111 sessions were recorded from 6 high quality workers .", "entities": []}, {"text": "The full process cost about $ 385 in total ( including the Mechanical Turk fees ) for the experiment including configuration vin Table 1 .", "entities": []}, {"text": "The second round of experiments done on another variant of our system ( configuration i ) also included 28 practice tasks and compiled 10 final workers for a total of 180 collected sessions .", "entities": []}, {"text": "Bonuses ranged from $ 0.10 and $ 0.40 on the session collection task .", "entities": []}, {"text": "The full process cost of the second experiment was about $ 475 in total ( including the Mechanical Turk fees ) .", "entities": []}, {"text": "Session collection data preparation .", "entities": []}, {"text": "We used the same 20 test topics as Shapira et al .", "entities": []}, {"text": "( 2021b ) , and created 2 batches of tasks .", "entities": []}, {"text": "For the first batch , in alternating order of topics , 10 topics were paired with our system , and the other 10 were paired with theS2baseline .", "entities": []}, {"text": "The other batch consisted of the complementing topic - system pairings .", "entities": []}, {"text": "The workers were assigned a batch to work on such that half of the workers would work on each batch .", "entities": []}, {"text": "User informational goal .", "entities": []}, {"text": "Since all sessions on a topic are evaluated against the same reference summaries , it is important that users aim to explore similar information .", "entities": []}, {"text": "Following Shapira et al .", "entities": []}, {"text": "( 2021b ) , during practice tasks all users received a common informational goal to follow , so that the sessions are comparable .", "entities": []}, {"text": "The emphasized description was : \u201c produce an informative summary draft text which a journalist could use to best produce an overview of the topic\u201d.2564", "entities": []}, {"text": "Sessions filtering .", "entities": []}, {"text": "In the first experiment , we filtered out 7 sessions that accumulated less than 250 tokens ( from 2 different workers ) .", "entities": []}, {"text": "In the second experiment , 9 of the 10 workers completed at least 19 of the 20 topics One worker completed only 3 tasks and we disregarded those sessions .", "entities": []}, {"text": "We also threw away 9 sessions that accumulated less than 250 tokens .", "entities": []}, {"text": "INTSUMM user interface .", "entities": []}, {"text": "We used the same user interface developed by Shapira et al .", "entities": []}, {"text": "( 2021b ) with a small change to enable suggested query list updates after each interaction ( the interface was designed for the baselines , where the suggestedquery list is static ) .", "entities": []}, {"text": "To refrain from any possible user experience bias , we made the UI change as least apparent as possible .", "entities": []}, {"text": "System response time .", "entities": []}, {"text": "MSumm is able to generate summaries mostly in under a second , and MSugg prepares the list in a few seconds .", "entities": []}, {"text": "The summary expansion is hence presented to the user almost immediately after query submission , and the suggested queries list is shown shortly afterwords , before the user finishes reading the expansion .", "entities": []}, {"text": "The small delay in suggested query updating is hence almost unnoticed .", "entities": []}, {"text": "The baseline summarizer responds similarly fast to MSumm , making response - time difference unperceivable between the systems .", "entities": []}, {"text": "User feedback .", "entities": []}, {"text": "Many of the users provided feedback about the session collection tasks after finishing their assignment batch .", "entities": []}, {"text": "The overall impression was that there was no strong preference for either system .", "entities": []}, {"text": "For example , one user wrote : \u201c I did not discern a consistent difference between the two systems that would result in having a clear preference . \u201d", "entities": []}, {"text": "This kind of comment was repeated by several users .", "entities": []}, {"text": "Generally , there were no explicit comments about the difference in quality of the summary outputs , and topics were mostly scored or commented on similarly between the two systems since the complexity of the topic influenced the ability of the systems to comply to the user .", "entities": []}, {"text": "A comment in favor of updating suggested queries during interaction said : \u201c It was nice to have a new list as you progressed through the task , it helped me think of where to go next if I got stuck ... \u201d", "entities": []}, {"text": "This specific comment was written by a user that explored topics quite deeply .", "entities": []}, {"text": "On the other hand , a user that explored more shallow liked that used suggested queries in the static list were marked : \u201c I did notice ... the red font color on the used queries .", "entities": []}, {"text": "That was helpful . \u201d", "entities": []}, {"text": "It therefore seems that updating suggested queries are more useful for lengthy exploration , but for quick navigation , the static list might naturally be enough .", "entities": []}, {"text": "E More Results E.1 Overall System Assessment Metric Ours S2Baseline", "entities": []}, {"text": "Rr 1AUC @ [ 106,250 ] 42.52(\u00b11.65)40.34(\u00b11.40 ) RF1 1@ initial 0.260(\u00b1.011)0.231(\u00b10.014 ) RF1 1@250 0.390(\u00b1.015)0.382(\u00b1.014 ) QSIMquery - resp . 0.527(\u00b1.016)0.603(\u00b1.022 )", "entities": []}, {"text": "Manual query - resp .", "entities": []}, {"text": "3.66(\u00b10.29)3.79(\u00b10.25 )", "entities": []}, {"text": "Manual UMUX - Lite 73.8(\u00b13.6 ) 75.8(\u00b12.9 ) Table 3 : Average scores of our system and a baseline INTSUMM system on real user sessions , in an experiment using a different MSumm configuration ( configuration i ) compared to the experiment of Table 2 ( configuration v ) .", "entities": []}, {"text": "Our system exposes topical information better , while the overall user experience is not significantly harmed .", "entities": []}, {"text": "Intervals at 95 % confidence level .", "entities": []}, {"text": "We conducted two comparative session collection and analysis experiments , one using MSumm model configuration v(from Table 1 ) , as presented in \u00a7 5.3 and Table 2 , and another with MSumm model configuration i.", "entities": []}, {"text": "As explained in \u00a7 5.2 , these two configurations performed best , on simulations , out of their respective configuration sets .", "entities": []}, {"text": "We show here results of the second experiment , where we used MSumm model configuration i , with the same MSugg model as in the first experiment .", "entities": []}, {"text": "TheS2baseline was similarly used for comparison .", "entities": []}, {"text": "We also kept the same AUC length limits ( 106 to 250 tokens ) for easy comparability to Table 2 .", "entities": [[5, 6, "MetricName", "AUC"]]}, {"text": "Table 3 shows the results .", "entities": []}, {"text": "Here too , while less substantially , informativeness is improved with our system without significantly harming the user experience .", "entities": []}, {"text": "Overall , it seems that users were somewhat more satisfied with the INTSUMM system that uses MSumm configuration vthan configuration i. Interestingly , it seems the users may have appreciated the slightly better informativeness of configuration veven if the query - responsiveness was not as good as in configuration i , as shown through the QSIM score .", "entities": []}, {"text": "In addition , we see that absolute manual scores in Table 3 are lower than in Table 2 , but trends are generally similar .", "entities": []}, {"text": "It is common that scaling of manually supplied scores can fluctuate ( e.g. Gillick and Liu , 2010 ) .", "entities": []}, {"text": "Figures 3 and 4 show the averaged ( per topic and then over all topics ) recall curves of the collected2565", "entities": []}, {"text": "sessions in the experiment described in \u00a7 5.3 and above , respectively .", "entities": []}, {"text": "The x - axis is the accumulating token - length of the session , and the y - axis is theROUGE -1 recall .", "entities": []}, {"text": "The points on the curve are the average interpolated values from all the sessions .", "entities": []}, {"text": "The vertical dashed lines are the intersecting bounds of the sessions , from 106 tokens to 250 .", "entities": []}, {"text": "The area under the curve ( AUC ) is computed for each of the curves , and reported in the first row of Tables 2 and 3 .", "entities": [[6, 7, "MetricName", "AUC"]]}, {"text": "The higher AUC scores obtained from the recall curves of our models , compared to those of the S2baseline , highlight the ability to expose more salient information earlier in the session .", "entities": [[2, 3, "MetricName", "AUC"]]}, {"text": "E.2 Execution Time of Systems Systems that are made for interacting with humans must respond quickly in order to keep the user \u2019s engagement .", "entities": []}, {"text": "The exact amount of time does not affect the user experience as long as it does not surpass some limit , after which the user starts losing interest or feeling irritated ( Attig et al . , 2017 ; Anderson , 2020 ) .", "entities": []}, {"text": "As mentioned in Appendix D , MSumm generates summaries in under a second and MSugg prepares the list in a few seconds .", "entities": []}, {"text": "The baseline summarizer also responds in under a second .", "entities": []}, {"text": "The difference between the systems is virtually unperceivable during interaction .", "entities": []}, {"text": "There were no comments from the users in our experiments that stated any issue with execution time .", "entities": []}, {"text": "Figure 3 : Averaged recall curves of our system and the S2baseline system in the experiment described in \u00a7 5.3 and Table 2 ( using MSumm configuration vfrom Table 1 ) .", "entities": []}, {"text": "The intersecting range is bounded by dashed lines ( between 106 and 250 tokens).E.3 Assessment of Suggested Queries Functionality In this analysis , we assessed what modes of query submission users relied on over the course of a session .", "entities": []}, {"text": "To that end , ( 1 ) we divided each session to three segments ( first , second and third part of the session ) , and counted the types of queries .", "entities": []}, {"text": "The types are \u201c suggested query \u201d , \u201c free - text \u201d , \u201c highlight \u201d ( a span from the summary text ) and \u201c repeat \u201d ( repeating the last submitted query ) .", "entities": []}, {"text": "( 2 ) We then computed the percentage of each mode in each segment .", "entities": []}, {"text": "( 3 ) The percentages over all sessions and all topics were computed for each of the three segments .", "entities": []}, {"text": "This process was conducted only for sessions between 4 and 20 interactions , as the few long and short sessions often show different behavior .", "entities": []}, {"text": "For the first experiment , this left 43 sessions with avg . 8.63 ( std . 2.32 ) interactions for our system , and 50 sessions with 8.44 ( 2.48 ) interaction for S2 .", "entities": []}, {"text": "For the second experiment , it left 72 sessions with 10.24 ( 4.82 ) interactions for our system , and 74 sessions with 9.59 ( 4.42 ) interactions for S2 .", "entities": []}, {"text": "We focus here on the use of suggested queries versus all other query types .", "entities": []}, {"text": "In the first experiment we observe a change of +9 % from the first to the third segment in our system , and -20 % in S2 .", "entities": []}, {"text": "In the second experiment we see +18 % and -28 % in S2 .", "entities": []}, {"text": "As discussed in \u00a7 5.3 , this suggests the effectiveness of updated suggested queries , especially by the end of a session .", "entities": []}, {"text": "Figure 4 : Averaged recall curves of our system and the S2baseline system in the experiment described here in Appendix E.1 and Table 3 ( using MSumm configuration ifrom Table 1 ) .", "entities": []}, {"text": "The intersecting range is bounded by dashed lines ( between 106 and 250 tokens).2566", "entities": []}, {"text": "F Further Explanations on Evaluation Metrics The normalized AUC score for the validation metric ( explained in \u00a7 3.3 ) is computed over the recall curve produced from the accumulating summary expansions .", "entities": [[8, 9, "MetricName", "AUC"]]}, {"text": "Each point on the curve marks an accumulating token - length ( x - axis ) and an accumulating recall score ( y - axis ) of an interactive state , as depicted in Figures 3 and 4 ( although these figures show the averaged session recall curves with bounds , whereas during validation the curve is for a single session and there are no bounds set ) .", "entities": []}, {"text": "By computing the area under the full curve , and dividing by the full length , the normalized AUC score is obtained .", "entities": [[18, 19, "MetricName", "AUC"]]}, {"text": "The normalization gives an approximate absolute value that can be compared at different lengths ( although at large length differences this is not comparable due to the decaying slope of the curve ) .", "entities": []}, {"text": "The manual query - responsiveness score , reported in Tables 2 and 3 , is obtained by asking users , at the end of a session , \u201c During the interactive stage , how well did the responses respond to your queries ? \u201d , for which they rate on a 1 - to-5 scale .", "entities": []}, {"text": "The scores are averaged over the topic and then over all topics .", "entities": []}, {"text": "This follows the evaluation defined in Shapira et al .", "entities": []}, {"text": "( 2021b ) .", "entities": []}, {"text": "TheUMUX - Lite score ( Lewis et al . , 2013 ) , reported in Tables 2 and 3 , is obtained by asking users to rate ( 1 - to-5 ) two statements at the end of a session : ( 1 ) \u201c The system \u2019s capabilities meet the need to efficiently collect useful information for a journalistic overview \u201d and ( 2 ) \u201c The system is easy to use \u201d .", "entities": []}, {"text": "The first question refers to the users \u2019 informational goal that they received , in order to follow a consistent objective goal during their exploration .", "entities": []}, {"text": "The final score is a function of these two scores , and is used as a replacement for the popular SUS metric ( Brooke , 1996 ) ( with a much longer questionnaire ) , to which it shows very high correlation , thus offering a cheaper alternative .", "entities": []}, {"text": "This also follows the evaluation defined in Shapira et al .", "entities": []}, {"text": "( 2021b ) .", "entities": []}, {"text": "Allconfidence intervals in Tables 1 , 2 and 3 are computed as margins - of - error , on the topiclevel , over the standard error of the mean with 95 % confidence.10 Thetoken - length values in Table 1 are averages with standard deviations .", "entities": []}, {"text": "10E.g . , see https://www.calculator.net/ standard - deviation - calculator.htmlG A2C Policy Learning A policy gradient - based reinforcement learning approach ( Williams , 1992 ) allows optimizing on nondifferentiable metrics , and eliminates the exposure bias that occurs with traditional training methods , like cross - entropy , on generation tasks ( Ranzato et al . , 2016 ) .", "entities": [[10, 11, "MethodName", "A2C"]]}, {"text": "Specifically , we use the Advantage Actor Critic ( A2C ) policy gradient training method .", "entities": [[9, 10, "MethodName", "A2C"]]}, {"text": "See technical explanations in the appendix of ( Chen and Bansal , 2018 ) .", "entities": []}, {"text": "At a high level , an output reward ( subtracted by a baseline reward \u2013 computed on a version of the model without MMR attention ) is used to weight the output selection in the loss function .", "entities": [[35, 36, "MetricName", "loss"]]}, {"text": "In so , outputs with higher rewards increase the likelihood of those outputs and lower rewards decrease the likelihood .", "entities": []}, {"text": "Since the reward function is not differentiable , it is used as a weight on the probability of the selected output , which is then given to the loss function .", "entities": [[28, 29, "MetricName", "loss"]]}, {"text": "H I NTSUMM", "entities": []}, {"text": "Example We show in Figure 5 an example of an INTSUMM system using the web application of Shapira et al .", "entities": []}, {"text": "( 2021b ) and our our MSumm ( configuration ifrom Table 1 ) and MSugg models in the backend.2567", "entities": []}, {"text": "( a )   ( b ) ( c ) Figure 5 : An INTSUMM system using the web application of Shapira et al .", "entities": []}, {"text": "( 2021b ) , with our MSumm andMSugg models run in the backend , on one of the topics in DUC 2006 with 25 news documents about \u201c Global Warming \u201d .", "entities": [[20, 22, "DatasetName", "DUC 2006"]]}, {"text": "Sub - figure ( a ) shows the initial summary and the initial list of suggested queries .", "entities": []}, {"text": "Sub - figure ( b ) shows the result of clicking the \u201c carbon dioxide gas \u201d suggested query ( with the query response and updated suggested queries list ) .", "entities": []}, {"text": "Sub - figure ( c ) shows the result of subsequently submitting the query \u201c water level \u201d .", "entities": []}, {"text": "Query responses should be informative for the general topic , while also complying to the user queries .", "entities": []}, {"text": "System summaries and expansions must be output fast in order to allow smooth interaction and human engagement.2568", "entities": []}]