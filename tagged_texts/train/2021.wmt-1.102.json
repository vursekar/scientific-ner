[{"text": "Proceedings of the Sixth Conference on Machine Translation ( WMT ) , pages 961\u2013972 November 10\u201311 , 2021 .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "\u00a9 2021 Association for Computational Linguistics961IST - Unbabel 2021 Submission for the Quality Estimation Shared Task Chrysoula Zerva1;2;\u0003Daan van Stigt3;\u0003Ricardo", "entities": []}, {"text": "Rei2;3;4;\u0003Ana C. Farinha3 Pedro G. Ramos3Jos\u00e9 G. C. de Souza3Taisiya Glushkova1;2Miguel Vera3", "entities": []}, {"text": "F\u00e1bio Kepler3Andr\u00e9 F. T. Martins1;2;3", "entities": []}, {"text": "1Instituto Superior T\u00e9cnico2Instituto de Telecomunica\u00e7\u00f5es3Unbabel4INESC - ID 2;3;4Lisbon , Portugal { chrysoula.zerva , ricardo.rei , taisiya.glushkova , andre.t.martins}@tecnico.ulisboa.pt { daan.stigt , catarina.farinha , pedro.ramos , jose.souza , miguel.vera , fabio.kepler}@unbabel.com", "entities": []}, {"text": "Abstract We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation .", "entities": []}, {"text": "Our team participated on two tasks : Direct Assessment and PostEditing Effort , encompassing a total of 35 submissions .", "entities": []}, {"text": "For all submissions , our efforts focused on training multilingual models on top of OpenKiwi predictor - estimator architecture , using pre - trained multilingual encoders combined with adapters .", "entities": []}, {"text": "We further experiment with and uncertainty - related objectives and features as well as training on out - ofdomain direct assessment data .", "entities": []}, {"text": "1 Introduction Quality estimation ( QE ) is the task of evaluating a translation system \u2019s quality without access to reference translations ( Blatz et al . , 2004 ; Specia et", "entities": []}, {"text": "al . , 2018 ) .", "entities": []}, {"text": "This paper describes the joint contribution of Instituto Superior T\u00e9cnico ( IST ) and Unbabel to the WMT21 Quality Estimation shared task ( Specia et al . , 2021 ) , where systems were submitted to two tasks : 1 ) sentence - level direct assessment ; 2 ) word- and sentence - level post - editing effort .", "entities": []}, {"text": "This year \u2019s submission combines several ideas built on top of the OpenKiwi framework .", "entities": []}, {"text": "Motivated by the mixture of blind andseen language pairs in the test sets , we experimented with extensions that would allow us to train multilingual models that maintain good generalization ability and are robust to the presence of epistemic and aleatoric uncertainty .", "entities": []}, {"text": "For both tasks we trained and submitted an ensemble of multilingual models .", "entities": []}, {"text": "All submitted models follow the predictor - estimator architecture ( Kim and Lee , 2016 ;", "entities": []}, {"text": "Kim et al . , 2017 ) and use pretrained models for feature extraction .", "entities": []}, {"text": "Also , we \ufb01ne - tune all models on the provided QE data using stacked adapter layers ( Pfeiffer et al . , 2020 ) .", "entities": []}, {"text": "\u0003The \ufb01rst three authors have equal contribution .", "entities": []}, {"text": "We show that we can thus achieve comparable performance across language pairs while minimising the number of trainable parameters ( see Table 1 ) .", "entities": []}, {"text": "Furthermore , we experimented with different types of uncertainty - related information to leverage it \u2019s bene\ufb01ts , improving performance and robustness of the submitted systems ( see \u00a7 3.1.1 ) .", "entities": []}, {"text": "All related code extensions will be publicly available .", "entities": []}, {"text": "Our main contributions are : \u2022We build on our OpenKiwi architecture by exploring adapter layers ( Houlsby et al . , 2019 ; Pfeiffer et al . , 2020 ) for quality estimation as these demonstrated to be less amenable to over\ufb01tting while presenting the same or superior quality performance than \ufb01ne - tuning the whole base pre - trained model for different NLP tasks ( He et al . , 2021 ) .", "entities": []}, {"text": "\u2022We incorporate different types of uncertainty into our architectures .", "entities": []}, {"text": "We make use of the glass - box features ( Fomicheva et al . , 2020 ) extracted from the NMT models , the aleatoric ( data ) uncertainty derived from the human annotations and the epistemic ( model ) uncertainty ( Hora , 1996 ; Kiureghian and Ditlevsen , 2009 ; Huellermeier and Waegeman , 2021 ) that originates from the QE model .", "entities": []}, {"text": "\u2022We show that training the QE models on additional out - of - domain direct assessment ( DA ) data gives considerable gains in performance for the new language pairs from the blind test sets .", "entities": []}, {"text": "2 Quality Estimation Tasks In this year \u2019s shared task edition we submitted models for the \ufb01rst two tasks : 1 . Task 1 : sentence - level direct assessment 2.Task 2 : word- and sentence level post - editing effort , comprising of two subtasks : a ) predicting the HTER score of the translated sentence", "entities": []}, {"text": "962(hypothesis ) ; and b ) predicting OK / BAD tags for the words and gaps ( both in source and translation )", "entities": []}, {"text": "We note that this year , both tasks 1 and 2 provided additional blind test sets with language pairs that were not included in the data made available for training / development , providing an interesting challenge and motivating multilingual and generalisable approaches .", "entities": []}, {"text": "3 Implemented Systems 3.1 Task 1 For Task 1 our \ufb01nal submission consisted of an ensemble of two different multilingual models , that differ in the way they process the input source ( original sentence ) and hypothesis ( machine translation ) .", "entities": [[39, 41, "TaskName", "machine translation"]]}, {"text": "Both models are based on the predictor - estimator architecture , using different pre - trained models to extract features and different training approaches to optimise for the QE task .", "entities": []}, {"text": "The key idea explored with our \ufb01rst model ( denoted by M1variations in the experiments ) , revolved around pursuing highly generalisable multilingual models , robust to over\ufb01tting .", "entities": []}, {"text": "To this end , we train a cross - lingual transformer ( XLMRoBERTa ( Conneau et al . , 2020 ) ) on large , multilingual data with direct assessments and then use adapters ( Houlsby et al . , 2019 ; Pfeiffer et al . , 2020 ) to adapt to the domain speci\ufb01c data of the QE task with minimal training effort .", "entities": []}, {"text": "In line with our efforts for good generalisation , we use only task - speci\ufb01c adapters and refrain from using speci\ufb01c adapters for each language pair .", "entities": []}, {"text": "For these experiments we build on the OpenKiwi architecture ( Kepler et al . , 2019 ) , using a pre - trained xlm - roberta - large encoder as a feature predictor .", "entities": [[23, 24, "MethodName", "xlm"]]}, {"text": "The source and hypothesis sentences are jointly encoded with hypothesis \ufb01rst .", "entities": []}, {"text": "Then , source and hypothesis features are generated using average pooling over the hypothesis embeddings and forwarded to the estimator module which corresponds to a feed - forward layer .", "entities": [[9, 11, "MethodName", "average pooling"]]}, {"text": "Figure 1 provides the general architecture1", "entities": []}, {"text": "The model was \ufb01rst trained on the direct assessment data provided in the Metrics shared tasks ( Mathur et al . , 2020 ) , as described in \u00a7 3.1.2 .", "entities": []}, {"text": "Upon training , the XML - R encoder is frozen and the the model is \ufb01ne - tuned on sentence regression with 1Note that glass - box features are integrated but not used in this submission as they did not signi\ufb01cantly improve performance .", "entities": []}, {"text": "Figure 1 : General architecture of M1 model variations .", "entities": [[3, 4, "DatasetName", "General"]]}, {"text": "Word tag prediction is used only for Task 2 .", "entities": []}, {"text": "the task - speci\ufb01c data , using stacked adapters .", "entities": []}, {"text": "We hence manage to maintain a low number of trainable parameters during \ufb01ne - tuning and minimize training time while learning to predict task - speci\ufb01c sentence scores .", "entities": []}, {"text": "For the second model ( denoted by M2 - KL - GMCD )", "entities": []}, {"text": "we aimed to explore the potential of a large pre - trained multilingual model ( trained with MT objectives ) .", "entities": []}, {"text": "We use the mBART ( Liu et al . , 2020 ) encoder - decoder architecture to encode the source and force - decode the hypothesis .", "entities": [[3, 4, "MethodName", "mBART"]]}, {"text": "We speci\ufb01cally use the mBART50 model ( Tang et al . , 2020 ) which is trained with multilingual \ufb01netuning on 50 languages , including all languages of interest for the QE 2021 task .", "entities": []}, {"text": "We obtain the features by averaging the decoder embeddings and concatenating with the < eos > token of the sequence .", "entities": []}, {"text": "The estimator part of the model consists of a bottleneck feed - forward layer that reduces the dimensionality of the decoder output , and is concatenated with a vector with additional glass - box features from the NMT models ( see \u00a7 3.1.1 ) .", "entities": []}, {"text": "The combined vector is then forwarded to a feed - forward estimator and the full model is \ufb01ne - tuned on the task speci\ufb01c QE data .", "entities": []}, {"text": "Apart from the glass - box features we experimented further with methods that allow the model to be", "entities": []}, {"text": "963 Figure 2 : General architecture of M2 model variations .", "entities": [[4, 5, "DatasetName", "General"]]}, {"text": "more robust towards the underlying uncertainty of its predictions .", "entities": []}, {"text": "We elaborate that in the next section .", "entities": []}, {"text": "Figure 2 provides a general architecture of the M2 model variations .", "entities": []}, {"text": "3.1.1 Learning from uncertainty Multiple neural models are involved in the process of obtaining and scoring machine translations , which naturally leads to several sources of uncertainty .", "entities": []}, {"text": "These sources can be very informative and useful for MT evaluation .", "entities": []}, {"text": "In this work we try to consider three types of uncertainty : ( 1 ) uncertainty of the NMT models used to obtain the hypotheses , ( 2 ) data ( aleatoric ) uncertainty for which we use the inter - annotator disagreement as a proxy , and ( 3 ) uncertainty of the MT evaluation model itself .", "entities": []}, {"text": "NMT model uncertainty The idea of extracting uncertainty - related features from the MT systems in order to estimate the quality of their predictions , was originally introduced by Fomicheva", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "This glass - box approach to QE is mostly focusing on capturing epistemic uncertainty , and the proposed features are extracted either using Monte Carlo ( MC ) dropout on the NMT or using the output probability distributions obtained from a standard deterministic MT system .", "entities": []}, {"text": "In our last year \u2019s submission ( Moura et al . , 2020 ) the integration of such features proved to be effective , thus we decided to incorporate it into our new model as well .", "entities": []}, {"text": "We list the extracted features below : \u2022TPsentence average of word translation probability\u2022Softmax - Ent sentence average of softmax output distribution entropy \u2022Sent - Std sentence standard deviation of word probabilities \u2022D - TP average TP across N(N = 30 ) stochastic forward - passes \u2022D - Var variance of TP across N stochastic forward - passes \u2022D - Combo combination of D - TP and D - Var de\ufb01ned by 1\u0000D\u0000TP = D\u0000Var \u2022D - Lex - Sim lexical similarity - measured by METEOR score ( Lavie and Denkowski , 2009 ) - of MT output generated in different stochastic passes .", "entities": [[10, 12, "TaskName", "word translation"], [18, 19, "MethodName", "softmax"], [84, 85, "DatasetName", "METEOR"]]}, {"text": "Aleatoric uncertainty The noise and complexity of the training data is a source of predictive uncertainty in itself , referred to as data or aleatoric uncertainty ( Kiureghian and Ditlevsen , 2009 ) .", "entities": []}, {"text": "This uncertainty is often re\ufb02ected in the disagreement between human annotations for the same sourcehypothesis segment ( Cohn and Specia , 2013 ; Fornaciari et al . , 2021 ) .", "entities": []}, {"text": "We hypothesize that the direct assessments can be better modelled as normally distributed scores rather than a single score , and that a model trained to predict this distribution ( mean and standard deviation ) could provide better quality estimates2 .", "entities": []}, {"text": "We formalise this as a KL divergence objective , using the closed form solution to estimate the KL divergence between the target distribution p(x )", "entities": []}, {"text": "= N(\u00161;\u001b1)and the predicted distribution q(x ) = N(\u00162;\u001b2 ) , as shown in Eq .", "entities": []}, {"text": "1 . KL(pjjq ) = log\u001b2 \u001b1+\u001b2 1 + ( \u00161\u0000\u00162)2 2\u001b2 2\u00001 2(1 ) where we take the mean and standard deviation ( std ) of the direct assessment z_scores as the target ( ground truth proxy ) values", "entities": []}, {"text": "p.", "entities": []}, {"text": "This way , we account for the annotator disagreement ( re\ufb02ected in the std value ) during learning .", "entities": []}, {"text": "QE epistemic uncertainty We use MC dropout ( Gal and Ghahramani , 2016 ) to account for the uncertainty of the QE model .", "entities": []}, {"text": "Speci\ufb01cally , we enable dropout during inference and run multiple forward runs over each test instance .", "entities": []}, {"text": "Thus we obtain a distribution of quality predictions for each instance 2Note that for this task \u2019s data we only had access to 3 scores per segment", "entities": []}, {"text": "so the mean and std values are calculated over these numbers .", "entities": []}, {"text": "964instead of a single point estimate .", "entities": []}, {"text": "We use the estimated mean of the distribution as our predicted quality estimate .", "entities": []}, {"text": "MC dropout has been shown to improve predictive accuracy and perform on par or even better compared to deep ensembles for MT evaluation tasks ( Glushkova et al . , 2021 ) .", "entities": [[8, 9, "MetricName", "accuracy"], [18, 20, "MethodName", "deep ensembles"]]}, {"text": "It thus allows us to simulate ensembling in a cheap and effective way , without the need to train multiple checkpoints .", "entities": []}, {"text": "3.1.2 Out - of - domain direct assessment data", "entities": []}, {"text": "The QE data is relatively limited , making it harder to train multilingual models with a large number of parameters without over-\ufb01tting .", "entities": [[17, 20, "HyperparameterName", "number of parameters"]]}, {"text": "Thus , as explained in \u00a7 3.1 we aimed to investigate whether we could obtain models that generalise better and are more robust to noise and out - of - distribution data by training the XLM - RoBERTa model \ufb01rst on a larger \u2013 yet noisier and out - of - domain dataset .", "entities": [[35, 36, "MethodName", "XLM"], [37, 38, "MethodName", "RoBERTa"]]}, {"text": "To that end we leverage the data provided for the past Metrics shared tasks , which covers the language pairs used in this year \u2019s QE task , including the blind tests for which we had no in - domain data available .", "entities": []}, {"text": "Altogether , it encompasses 30 language pairs from the news domain ( versus 7 in the QE dataset ) .", "entities": []}, {"text": "We provide more detailed statistics for each language pair of the Metrics data in Appendix C.", "entities": []}, {"text": "We refer to experiments using the model initially trained on the Metrics data as M1M- .", "entities": []}, {"text": "We also show that using the trained XLM - RoBERTa encoder from the M1 M model can prove bene\ufb01cial for the predictions on post - edited data of Task 2 ( see Table 3 ) .", "entities": [[7, 8, "MethodName", "XLM"], [9, 10, "MethodName", "RoBERTa"]]}, {"text": "3.2 Task 2", "entities": []}, {"text": "For Task 2 we submitted an ensemble of two variations of the \ufb01rst model ( M1- ADAPT andM1MADAPT ) presented for Task 1 ( see \u00a7 3.1 ) .", "entities": []}, {"text": "In both cases , we use multi - task training and a feedforward for each output types : hypothesis word tags , hypothesis gap tags , source word tags , and sentence regression ( on HTER scores ) .", "entities": []}, {"text": "Both variations use a pre - trained XLM - RoBERTa ( large ) encoder to extract features as described for Task 1 , but differ in the training of the encoder .", "entities": [[7, 8, "MethodName", "XLM"], [9, 10, "MethodName", "RoBERTa"]]}, {"text": "In the \ufb01rst case we use the pre - trained model3and \ufb01netune on the QE data using stacked adapters .", "entities": []}, {"text": "In the second variation we swap the original pre - trained model with the XLM - RoBERTa model that has been trained on the Metrics data as described in 3https://huggingface.co/transformers/ model_doc / xlmroberta.html\u00a73.1.2 .", "entities": [[14, 15, "MethodName", "XLM"], [16, 17, "MethodName", "RoBERTa"]]}, {"text": "We note that the two variations favor different language pairs , hence we combine multiple checkpoints from each variation ( ranging training steps ) .", "entities": []}, {"text": "We use the test-20 split of the data to optimise the hyper - parameters and following this approach we use the estimated top-3 checkpoints from each variation using the combined dataset4 and the top checkpoint for the non - augmented model trained exclusively on the train set , resulting in total 7 checkpoints in our \ufb01nal ensemble .", "entities": []}, {"text": "4 Experimental Results We present the performance of the implemented models on the test-20 dataset .", "entities": []}, {"text": "4.1 Task 1", "entities": []}, {"text": "The results can be seen in Tables 1 and 2 .", "entities": []}, {"text": "In line with the shared task guidelines we treat Pearson r as the primary performance metric and select the submitted models accordingly .", "entities": []}, {"text": "We can observe , that while on average the M1model and its variations outperform the M2model , their performance is comparable , and M2 - KL - G - MCD can even outperform M1M - A DAPT for speci\ufb01c language pairs , hence it made sense to combine them in the \ufb01nal ensemble .", "entities": []}, {"text": "We can also see that \ufb01ne - tuning the M1 model on the Metrics data , results in performance gains for the majority of the language pairs .", "entities": []}, {"text": "Specifically , even applying the M1 M directly , without further \ufb01ne - tuning on QE data , achieves competitive performance for most pairs , which further improves upon \ufb01ne - tuning .", "entities": []}, {"text": "It helps in increasing the performance on the blind sets ( denoted as zeroshot in the Appendix B ) .", "entities": []}, {"text": "The performance gains concern mostly the correlation performance indicators ( Pearson and Spearman correlations ) , since especially for M1the error - based indicators ( MAE and RMSE ) seem to favor the versions of the model that have not seen the Metrics data .", "entities": [[25, 26, "MetricName", "MAE"], [27, 28, "MetricName", "RMSE"]]}, {"text": "One possible explanation for this discrepancy could lie in the differences between the range and distribution of DA scores for the two datasets .", "entities": []}, {"text": "Indicatively , the range of scores on the train - dev - test-20 concatenation of the QE data is [ \u00007:542;3:178 ] and for the Metrics data", "entities": []}, {"text": "[ \u00008:624;4:332 ] .", "entities": []}, {"text": "The target DA scores in both datasets are calculated via standardizing ( taking the z score ) the direct assessments for each annotator and then averaging all standardized 4The combined dataset in this case refers to the concatenation of the train / dev / test20 annotated data splits provided for the shared task", "entities": []}, {"text": "965Pears\"Spear\"MAE#RMSE#EN - DEM1 BASE 0.4534 0.4532 0.4482 0.6371 M1- ADAPT 0.5092 0.4825 0.4868 0.6288 M1 M 0.5288 0.4872 0.4485 0.6327 M1M- ADAPT 0.5695 0.5131 0.4127 0.6095EN - ZHM1 BASE 0.4429 0.4362 0.5364 0.6867 M1- ADAPT", "entities": [[3, 4, "MethodName", "BASE"], [28, 29, "MethodName", "BASE"]]}, {"text": "0.4723 0.4755 0.5228 0.6714 M1 M 0.4447 0.4400 0.4772 0.6110 M1M- ADAPT 0.4815 0.4872 0.5502 0.7017ET - ENM1 BASE 0.7939 0.8076 0.5388 0.6928 M1- ADAPT 0.7948 0.8061 0.4518 0.5810 M1 M 0.7580 0.7611 0.5820 0.7134 M1M- ADAPT 0.7956 0.8110 0.5358 0.6921NE - ENM1 BASE 0.7805 0.7592 0.4278 0.5461 M1- ADAPT 0.7609 0.7475 0.4075 0.5393 M1 M 0.7477 0.7324 0.4499 0.6161 M1M- ADAPT 0.7888 0.7556 0.4192 0.5332RO - ENM1 BASE 0.8718 0.8360 0.3598 0.4878 M1- ADAPT 0.8923 0.8533 0.3068 0.4201 M1 M 0.8345 0.8132 0.4585 0.5863 M1M- ADAPT 0.8889 0.8488 0.3142 0.4437RU - ENM1 BASE 0.7587 0.6919 0.4885 0.6949 M1- ADAPT 0.7736 0.7142 0.4138 0.6082 M1 M 0.6703 0.6535 0.5606 0.7583 M1M- ADAPT 0.7425 0.7159 0.4989 0.7250SI - ENM1 BASE 0.6456 0.6112 0.5060 0.6481 M1- ADAPT 0.6613 0.6172 0.4742 0.5939 M1 M 0.6308 0.6535 0.4742 0.5786 M1M- ADAPT 0.6649 0.6225 0.4863 0.6064MLM1 BASE 0.6781 0.6565 0.4722 0.6276 M1-", "entities": [[18, 19, "MethodName", "BASE"], [43, 44, "MethodName", "BASE"], [68, 69, "MethodName", "BASE"], [93, 94, "MethodName", "BASE"], [118, 119, "MethodName", "BASE"], [141, 142, "MethodName", "BASE"]]}, {"text": "ADAPT 0.6949 0.6709 0.4377 0.5775 M1 M 0.6593 0.5131 0.4127 0.6095 M1M- ADAPT 0.7045 0.6791 0.4596 0.6160 Table 1 : Results for Task 1 with the M1 predictorestimator ( XLM - RoBERTa ) and different training/\ufb01netuning approaches .", "entities": [[29, 30, "MethodName", "XLM"], [31, 32, "MethodName", "RoBERTa"]]}, {"text": "M1 M is the M1 model trained on the Metrics dataset and M#- ADAPT signi\ufb01es a model \ufb01ne - tuned on the QE data with adapters .", "entities": []}, {"text": "ML stands for MULTILINGUAL , showing the performance averaged over all language pairs .", "entities": []}, {"text": "Underlined numbers indicate the best result for each language pair and evaluation metric .", "entities": []}, {"text": "Bold systems were selected for the \ufb01nal ensemble .", "entities": []}, {"text": "assessments for each segment .", "entities": []}, {"text": "Thus , the difference in target score range and distribution could affect the magnitude of predicted scores and the distance to the ground truth values , which is re\ufb02ected in the MAE and RMSE metrics .", "entities": [[31, 32, "MetricName", "MAE"], [33, 34, "MetricName", "RMSE"]]}, {"text": "These \ufb01ndings , further supported by the results on Task 2 , is a \ufb01rst step in exploring the underlying connection and bridging the gap between the Metrics and Quality Estimation shared tasks .", "entities": []}, {"text": "4.2 Task 2", "entities": []}, {"text": "The results can be seen in Table 3 .", "entities": []}, {"text": "Similarly to Task 1 , the primary evaluation metric for the sentence level sub - task of Task 2 is the Pearson r coef\ufb01cient , Pears\"Spear\"MAE#RMSE#EN - DEM2 BASE 0.4889 0.4645 0.4608 0.6180 M2 - KL 0.4971 0.4769 0.4549 0.6191 M2 - KL - G 0.5110 0.4738 0.4396 0.6133 M2 - KL - G - MCD 0.5093 0.4754 0.4495 0.6128EN - ZHM2 BASE 0.4484 0.4355 0.4940 0.6374 M2 - KL 0.4574 0.4471 0.5042 0.6485 M2 - KL - G 0.4566 0.4543 0.5278 0.6751 M2 - KL - G - MCD 0.4628 0.4584 0.4973 0.6390ET - ENM2 BASE 0.7792 0.7842 0.4581 0.5624 M2 - KL 0.7833 0.7896 0.4684 0.5824 M2 - KL - G 0.7847 0.7962 0.4643 0.5924 M2 - KL - G - MCD 0.7868 0.7951 0.4539 0.5674NE - ENM2 BASE 0.7333 0.7154 0.4347 0.5531 M2 - KL 0.7638 0.7393 0.4040 0.5247 M2 - KL - G 0.7529 0.7228 0.4194 0.5353 M2 - KL - G - MCD 0.7596 0.7269 0.4125 0.5313RO - ENM2 BASE 0.8780 0.8407 0.3403 0.4514 M2 - KL 0.8826 0.8406 0.3199 0.4305 M2 - KL - G 0.8728 0.8397 0.3314 0.4635 M2 - KL - G - MCD 0.8777 0.8429 0.3209 0.4426RU - ENM2 BASE", "entities": [[28, 29, "MethodName", "BASE"], [62, 63, "MethodName", "BASE"], [96, 97, "MethodName", "BASE"], [130, 131, "MethodName", "BASE"], [164, 165, "MethodName", "BASE"], [198, 199, "MethodName", "BASE"]]}, {"text": "0.7406 0.6874 0.4696 0.6381 M2 - KL 0.7532 0.7123 0.4558 0.6299 M2 - KL - G 0.7485 0.7191 0.4630 0.6612 M2 - KL - G - MCD 0.7509 0.7204 0.4492 0.6358SI - ENM2 BASE 0.6243 0.5899 0.4709 0.5939 M2 - KL 0.6373 0.6000 0.4572 0.5726 M2 - KL - G 0.6506 0.6168 0.4586 0.5796 M2 - KL - G - MCD 0.6545 0.6199 0.4495 0.5697MLM2 BASE 0.6704 0.6454 0.4469 0.5792 M2 - KL 0.6821 0.6580 0.4378 0.5725 M2 - KL - G 0.6825 0.6604 0.4434 0.5886 M2 - KL - G - MCD 0.6859 0.6627 0.4333 0.5712 Table 2 : Results for Task 1 with the M2 predictorestimator ( mBART ) and different uncertainty handling additions .", "entities": [[33, 34, "MethodName", "BASE"], [65, 66, "MethodName", "BASE"], [109, 110, "MethodName", "mBART"]]}, {"text": "\u201c KL \u201d signi\ufb01es the incorporation of KL loss , \u201c G\u201dthe incorporation of glass - box features and MCD the addition of MC dropout .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "ML stands for M ULTILIN GUAL , showing the performance averaged over all language pairs .", "entities": []}, {"text": "Underlined numbers indicate the best result for each language pair and evaluation metric .", "entities": []}, {"text": "Bold systems were selected for the \ufb01nal ensemble .", "entities": []}, {"text": "while the word level sub - task is evaluated using the Matthews correlation coef\ufb01cient ( MCC , ( Matthews , 1975 ) ) as the primary performance indicator .", "entities": []}, {"text": "We can see that while HTER scores do not always correlate highly with DAs ( see Table 4 ) , the use of the M1 M model encoder that was trained on large data with direct assessments can still prove useful .", "entities": []}, {"text": "Indeed , when \ufb01ne - tuning on the Task2 data , the model using the M1 M encoder ( M1MADAPT in the table 3 ) provides a performance boost for the Pearson correlation in most language pairs , and competitive performance for the rest .", "entities": [[31, 33, "MetricName", "Pearson correlation"]]}, {"text": "Based on these results , we deem it worthwhile to include checkpoints trained with this con\ufb01guration in the ensemble estimating that they will contribute in higher performance , especially on the blind test sets .", "entities": []}, {"text": "This can be further con\ufb01rmed when", "entities": []}, {"text": "966Pearson\"SRC - MCC\"TGT - MCC\"EN - DE M1 BASE 0.5256 0.3331", "entities": [[8, 9, "MethodName", "BASE"]]}, {"text": "0.4092 M1- ADAPT 0.5573 0.4211 0.36454 M1M- ADAPT 0.5499 0.3647 0.4239EN - ZH M1 BASE 0.3786 0.3253 0.3589 M1- ADAPT 0.3711 0.4346 0.3288 M1M- ADAPT 0.3721 0.4255 0.3643ET - EN M1 BASE 0.7319 0.4537 0.5110 M1- ADAPT 0.7360 0.5545 0.4978 M1M- ADAPT 0.7498 0.4929 0.5513NE - EN M1 BASE 0.5898 0.5198 0.4386 M1- ADAPT 0.5987 0.6884 0.5426 M1M- ADAPT 0.6252 0.4244 0.4682RO - EN M1 BASE 0.8531 0.5727 0.6190 M1- ADAPT 0.8282 0.5984 0.5653 M1M- ADAPT 0.8280 0.5682 0.5813RU - EN M1", "entities": [[14, 15, "MethodName", "BASE"], [31, 32, "MethodName", "BASE"], [48, 49, "MethodName", "BASE"], [65, 66, "MethodName", "BASE"]]}, {"text": "BASE 0.4899 0.2766 0.3213 M1- ADAPT 0.4811 0.341 0.3071 M1M- ADAPT 0.5060 0.2927 0.3421SI - EN M1 BASE 0.6659 0.4653 0.4776 M1- ADAPT 0.6698 0.6776 0.5057 M1M- ADAPT 0.6935 0.3872 0.4937MLM1", "entities": [[0, 1, "MethodName", "BASE"], [17, 18, "MethodName", "BASE"]]}, {"text": "BASE 0.6050 0.4209 0.4479 M1- ADAPT 0.6061 0.5323 0.4445 M1 M ADAPT 0.6178 0.4222 0.4607 Table 3 : Results for Task 2 with the M1 predictorestimator ( XLM - RoBERTa ) and different training/\ufb01netuning approaches .", "entities": [[0, 1, "MethodName", "BASE"], [27, 28, "MethodName", "XLM"], [29, 30, "MethodName", "RoBERTa"]]}, {"text": "M1 M is the M1 model trained on the Metrics dataset and M#- ADAPT signi\ufb01es a model \ufb01ne - tuned on the QE data with adapters .", "entities": []}, {"text": "ML stands for MULTILINGUAL , showing the performance averaged over all language pairs .", "entities": []}, {"text": "Underlined numbers indicate the best result for each language pair and evaluation metric .", "entities": []}, {"text": "Bold systems were selected for the \ufb01nal ensemble .", "entities": []}, {"text": "inspecting the results for the blind sets ( en - cs , en - ja , km - en andps - en ) in the of\ufb01cial results ontest-21 as shown in Appendix B. lp TRAIN DEV TEST -20", "entities": []}, {"text": "EN - DE-0.1654 -0.4032 -0.3850", "entities": []}, {"text": "EN - ZH-0.2947", "entities": []}, {"text": "-0.1895 -0.1932", "entities": []}, {"text": "ET - EN -0.5464 -0.5850 -0.5995", "entities": []}, {"text": "NE - EN-0.4527 -0.5004 -0.4558 RO - EN-0.5887 -0.7932 -0.7880 RU - EN-0.5358 -0.5055 -0.5152 SI - EN -0.3916 -0.4384 -0.4125", "entities": []}, {"text": "Table 4 : Pearson correlation between the z_mean of the direct assessments for the QE Task 1 data and the HTER score for the post edits in QE Task 2 data .", "entities": [[3, 5, "MetricName", "Pearson correlation"]]}, {"text": "5 Conclusions We presented a joint contribution of IST and Unbabel to the WMT 2021 QE shared task .", "entities": []}, {"text": "Oursubmissions are ensembles of multilingual checkpoints extending the OpenKiwi framework .", "entities": []}, {"text": "We found adapter - tuning to be suitable for \ufb01ne - tuning OpenKiwi on the QE tasks data and less prone to over\ufb01tting .", "entities": []}, {"text": "We showed that pre - training on large , out - of - domain annotated data can prove bene\ufb01cial both for the direct assessment and the postediting QE tasks .", "entities": []}, {"text": "We also demonstrated that handling uncertainty - related sources of information improves the performance when integrated into the QE system .", "entities": []}, {"text": "For Task 2 we do multi - task training based on the models from the previous task and use multiple checkpoints to create the submitted ensemble .", "entities": []}, {"text": "Acknowledgements We are grateful to Alon Lavie and Craig Stewart for their valuable feedback and discussions .", "entities": []}, {"text": "This work was supported by the P2020 programs MAIA ( contract 045909 ) and Unbabel4EU ( contract 042671 ) , by the European Research Council ( ERC StG DeepSPIN 758969 ) , and by the Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia through contract UIDB/50008/2020 .", "entities": []}, {"text": "References John Blatz , Erin Fitzgerald , George Foster , Simona Gandrabur , Cyril Goutte , Alex Kulesza , Alberto Sanchis , and Nicola Uef\ufb01ng .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Con\ufb01dence estimation for machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In COLING 2004 : Proceedings of the 20th International Conference on Computational Linguistics , pages 315\u2013321 , Geneva , Switzerland .", "entities": []}, {"text": "COLING .", "entities": []}, {"text": "Trevor Cohn and Lucia Specia .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Modelling annotator bias with multi - task Gaussian processes : An application to machine translation quality estimation .", "entities": [[7, 9, "TaskName", "Gaussian processes"], [13, 15, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 32\u201342 , So\ufb01a , Bulgaria .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , Francisco Guzm\u00e1n , Edouard Grave , Myle Ott , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unsupervised cross - lingual representation learning at scale .", "entities": [[4, 6, "TaskName", "representation learning"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8440 \u2013 8451 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Marina Fomicheva , Shuo Sun , Lisa Yankovskaya , Fr\u00e9d\u00e9ric Blain , Francisco Guzm\u00e1n , Mark Fishel , Nikolaos Aletras , Vishrav Chaudhary , and Lucia Specia .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unsupervised quality estimation for neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 8:539\u2013555 .", "entities": []}, {"text": "967Tommaso Fornaciari , Alexandra Uma , Silviu Paun , Barbara Plank , Dirk Hovy , and Massimo Poesio .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Beyond black & white : Leveraging annotator disagreement via soft - label multi - task learning .", "entities": [[12, 16, "TaskName", "multi - task learning"]]}, {"text": "InProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 2591\u20132597 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yarin Gal and Zoubin Ghahramani . 2016 .", "entities": []}, {"text": "Dropout as a bayesian approximation : Representing model uncertainty in deep learning .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "In Proceedings of The 33rd International Conference on Machine Learning , volume 48 of Proceedings of Machine Learning Research , pages 1050\u20131059 , New York , New York , USA .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Taisiya Glushkova , Chrysoula Zerva , Ricardo Rei , and Andr\u00e9 F. T. Martins .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Uncertainty - Aware Machine Translation Evaluation .", "entities": [[3, 5, "TaskName", "Machine Translation"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2021 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ruidan He , Linlin Liu , Hai Ye , Qingyu Tan , Bosheng Ding , Liying Cheng , Jiawei Low , Lidong Bing , and Luo Si .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "On the effectiveness of adapterbased tuning for pretrained language model adaptation .", "entities": []}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 2208\u20132222 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Stephen C. Hora .", "entities": []}, {"text": "1996 .", "entities": []}, {"text": "Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management .", "entities": []}, {"text": "Reliability Engineering & System Safety , 54(2):217\u2013223 .", "entities": []}, {"text": "Treatment of Aleatory and Epistemic Uncertainty .", "entities": []}, {"text": "Neil Houlsby , Andrei Giurgiu , Stanislaw Jastrzebski , Bruna Morrone , Quentin De Laroussilhe , Andrea Gesmundo , Mona Attariyan , and Sylvain Gelly .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Parameter - ef\ufb01cient transfer learning for NLP .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "InInternational Conference on Machine Learning , pages 2790\u20132799 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Eyke Huellermeier and Willem Waegeman .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Aleatoric and epistemic uncertainty in machine learning : an introduction to concepts and methods .", "entities": []}, {"text": "Machine Learning , 110(3):457\u2013506 .", "entities": []}, {"text": "Fabio Kepler , Jonay Tr\u00e9nous , Marcos Treviso , Miguel Vera , and Andr\u00e9 F. T. Martins .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "OpenKiwi :", "entities": []}, {"text": "An open source framework for quality estimation .", "entities": []}, {"text": "InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 117\u2013122 , Florence , Italy .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hyun Kim and Jong - Hyeok Lee . 2016 .", "entities": []}, {"text": "A recurrent neural networks approach for estimating the quality of machine translation output .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 494\u2013498 .", "entities": []}, {"text": "Hyun Kim , Jong - Hyeok Lee , and Seung - Hoon Na . 2017 .", "entities": []}, {"text": "Predictor - estimator using multilevel task learning with stack propagation for neural quality estimation .", "entities": []}, {"text": "In Proceedings of the Second Conference on Machine Translation , pages 562\u2013568 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Armen Der Kiureghian and Ove Ditlevsen .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Aleatory or epistemic ?", "entities": []}, {"text": "does it matter ?", "entities": []}, {"text": "Structural Safety , 31(2):105\u2013112 .", "entities": []}, {"text": "Risk Acceptance and Risk Communication .", "entities": []}, {"text": "Alon Lavie and Michael Denkowski .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "The meteor metric for automatic evaluation of machine translation .", "entities": [[1, 2, "DatasetName", "meteor"], [7, 9, "TaskName", "machine translation"]]}, {"text": "Machine Translation , 23:105\u2013115 .", "entities": [[0, 2, "TaskName", "Machine Translation"]]}, {"text": "Yinhan Liu , Jiatao Gu , Naman Goyal , Xian Li , Sergey Edunov , Marjan Ghazvininejad , Mike Lewis , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multilingual denoising pre - training for neural machine translation .", "entities": [[1, 2, "TaskName", "denoising"], [7, 9, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 8:726\u2013742 .", "entities": []}, {"text": "Nitika Mathur , Johnny Wei , Markus Freitag , Qingsong Ma , and Ond \u02c7rej Bojar . 2020 .", "entities": []}, {"text": "Results of the WMT20 metrics shared task .", "entities": []}, {"text": "In Proceedings of the Fifth Conference on Machine Translation , pages 688\u2013725 , Online .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Brian W Matthews .", "entities": []}, {"text": "1975 .", "entities": []}, {"text": "Comparison of the predicted and observed secondary structure of t4 phage lysozyme .", "entities": []}, {"text": "Biochimica et", "entities": []}, {"text": "Biophysica Acta ( BBA)Protein Structure , 405(2):442\u2013451 .", "entities": []}, {"text": "Jo\u00e3o Moura , Miguel Vera , Daan van Stigt , Fabio Kepler , and Andr\u00e9 F. T. Martins .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "ISTunbabel participation in the WMT20 quality estimation shared task .", "entities": []}, {"text": "In Proceedings of the Fifth Conference on Machine Translation , pages 1029\u20131036 , Online .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jonas Pfeiffer , Andreas R\u00fcckl\u00e9 , Clifton Poth , Aishwarya Kamath , Ivan Vuli \u00b4 c , Sebastian Ruder , Kyunghyun Cho , and Iryna Gurevych .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "AdapterHub :", "entities": []}, {"text": "A framework for adapting transformers .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 46\u201354 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Lucia Specia , Fr\u00e9d\u00e9ric Blain , Marina Fomicheva , Chrysoula Zerva , Zhenhao Li , Vishrav Chaudhary , and Andr\u00e9 F. T. Martins .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Findings of the wmt 2021 shared task on quality estimation .", "entities": []}, {"text": "In Proceedings of the Sixth Conference on Machine Translation , Online .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Lucia Specia , Carolina Scarton , and Gustavo Henrique Paetzold .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Quality estimation for machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "Synthesis Lectures on Human Language Technologies , 11(1):1\u2013162 .", "entities": []}, {"text": "968Yuqing Tang , Chau Tran , Xian Li , Peng - Jen Chen , Naman Goyal , Vishrav Chaudhary , Jiatao Gu , and Angela Fan . 2020 .", "entities": []}, {"text": "Multilingual translation with extensible multilingual pretraining and \ufb01netuning .", "entities": []}, {"text": "arXiv preprint arXiv:2008.00401 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "969A Hyperparameters A.1 M1", "entities": []}, {"text": "In Table 5 is an excerpt of the training con\ufb01guration used for training OpenKiwi for our M1 models .", "entities": []}, {"text": "Note that the con\ufb01gurations follow the con\ufb01guration \ufb01le format of OpenKiwi and any additional con\ufb01gurations are identical to the ones proposed in the sample con\ufb01guration \ufb01le of the github repository5 .", "entities": []}, {"text": "System batch_size 2 Encoder hidden_size 1024 Decoder bottleneck_size 1024 dropout 0.05 hidden_size", "entities": []}, {"text": "1024 Optimizer class_name adam encoder_learning_rate 0.0001 learning_rate_decay 1.0 learning_rate_decay_start 0 learning_rate 0.0001 Trainer training_steps 2180 early_stop_patience 10 validation_steps 0.5 gradient_accumulation_steps 4 gradient_max_norm 1.0 Table 5 : Hyperparameters for M1 models A.2 M2", "entities": [[1, 2, "HyperparameterName", "Optimizer"], [9, 10, "DatasetName", "0"]]}, {"text": "In Table 6 is an excerpt of the training con\ufb01guration used for training the M2 models using the mBART encoder - decoder : B Evaluation on test set of WMT21", "entities": [[18, 19, "MethodName", "mBART"]]}, {"text": "We present the performance of the submitted ensembles on the TEST -21dataset as calculated in the of\ufb01cial QE results6for each task and sub - task .", "entities": []}, {"text": "We also provide the comparison with the organisers \u2019 baseline .", "entities": []}, {"text": "5https://github.com/Unbabel/OpenKiwi/ blob / master / config / xlmroberta.yaml 6https://www.statmt.org/wmt21/ quality - estimation - task_results.htmlSystem bottleneck_size 256 dropout 0.1 hidden_size 2048 nr_frozen_epochs 0.333 Optimizer optimizer adam encoder_learning_rate", "entities": [[19, 20, "DatasetName", "2048"], [22, 23, "HyperparameterName", "Optimizer"], [23, 24, "HyperparameterName", "optimizer"]]}, {"text": "6.0e-06 learning_rate 1.0e-05 Trainer training_steps 5512 early_stopping_patience 2 save_top_k 3 batch_size 4 gradient_accumulation_steps 4 Table 6 : Hyperparameters for M2 models B.1 Task 1 : Direct Assessments prediction at sentence - level The results for Task1 on TEST -21 are presented in Table 7 .", "entities": []}, {"text": "B.2 Task 2 : HTER prediction at sentence - level The results for Task2 on TEST -21TEST -21are presented in Table 8 , showing the performance for the sentence level , HTER score predictions .", "entities": []}, {"text": "B.3 Task 2 : Word - level prediction The results for Task2 on TEST -21 are presented in Table 9 , showing the performance for the word tag predictions .", "entities": []}, {"text": "C Statistics on the Metrics data We present below ( Tables 10 and 11 ) the statistics on the Metrics data used to train the M1 M model on direct assessments .", "entities": []}, {"text": "970METHOD PEARSON R\"MAE#RMSE # MULTILINGUAL IST - U NBABEL 0.665 0.627 0.482 BASELINE 0.541 0.729 0.562 EN - DE IST - U NBABEL 0.579 0.567 0.393 BASELINE 0.403 0.629 0.433 EN - ZH IST - U NBABEL 0.586 0.631 0.499 BASELINE 0.525 0.683 0.534 RO - EN IST - U NBABEL 0.899 0.393 0.289 BASELINE 0.818 0.556 0.408 ET - EN IST - U NBABEL 0.796 0.519 0.404 BASELINE 0.660 0.700 0.543 NE - EN IST - U NBABEL 0.856 0.515 0.401 BASELINE 0.738 0.657 0.524 SI - EN IST - U NBABEL 0.605 0.742 0.583 BASELINE 0.513 0.797 0.626 RU - EN IST - U NBABEL 0.792 0.583 0.412 BASELINE 0.677 0.702 0.492 ZERO - SHOT LANGUAGE PAIRS EN - CZ IST - U NBABEL 0.577 0.751 0.583 BASELINE 0.352 0.845 0.686 EN - JA IST - U NBABEL 0.355 0.764 0.566 BASELINE 0.230 0.816 0.617", "entities": []}, {"text": "PS - EN IST - U NBABEL 0.628 0.780 0.658 BASELINE 0.476 0.852 0.711 KM - EN IST - U NBABEL 0.650 0.721 0.568 BASELINE 0.562 0.788 0.614 Table 7 : Results for Task 1 on the held - out evaluation set of WMT", "entities": []}, {"text": "2021.METHOD PEARSON", "entities": []}, {"text": "R\"MAE#RMSE # MULTILINGUAL IST - U NBABEL 0.597 0.219 0.171 BASELINE 0.502 0.235 0.188 EN - DE IST - U NBABEL 0.617 0.172 0.116 BASELINE 0.529 0.183 0.129 EN - ZH IST - U NBABEL 0.290 0.266 0.220 BASELINE", "entities": []}, {"text": "0.282 0.287 0.246", "entities": []}, {"text": "RO - EN IST - U NBABEL 0.879 0.122 0.098 BASELINE", "entities": []}, {"text": "0.831 0.142 0.115 ET - EN IST - U NBABEL 0.811 0.153 0.112 BASELINE 0.714 0.195 0.149 NE - EN IST - U NBABEL 0.718", "entities": []}, {"text": "0.161 0.126 BASELINE 0.626 0.205 0.160 SI - EN IST - U NBABEL 0.710 0.178 0.136 BASELINE 0.607 0.204 0.159 RU - EN IST - U", "entities": []}, {"text": "NBABEL 0.539 0.224 0.165 BASELINE 0.448 0.255 0.188 ZERO - SHOT LANGUAGE PAIRS", "entities": []}, {"text": "EN - CZ IST - U NBABEL 0.529 0.271 0.200 BASELINE 0.306 0.262", "entities": []}, {"text": "0.206 EN - JA IST - U NBABEL 0.275 0.279 0.224 BASELINE 0.098 0.279 0.232 PS - EN IST - U NBABEL 0.555 0.328 0.284 BASELINE 0.503 0.333 0.290 KM - EN IST - U NBABEL", "entities": []}, {"text": "0.655 0.243", "entities": []}, {"text": "0.199 BASELINE 0.576 0.241 0.196 Table 8 : Results for Task 2 sentence - level system on the held - out evaluation set of WMT 2021 .", "entities": []}, {"text": "971METHOD SRC - MCC\"TGT - MCC - W ORDS\"TGT - MCC - G APS \" EN - DE IST - U NBABEL 0.404 0.466 0.183 BASELINE 0.322 0.370 0.116 EN - ZH IST - U NBABEL 0.286 0.310 0.068 BASELINE 0.241 0.247 0.065", "entities": []}, {"text": "RO - EN IST - U NBABEL 0.603 0.649 0.357 BASELINE 0.511 0.536 0.205 ET - EN IST - U NBABEL 0.522 0.570 0.254 BASELINE 0.405 0.461 0.136 NE - EN IST - U NBABEL 0.445 0.508 0.268 BASELINE 0.390 0.440 0.215 SI - EN IST - U NBABEL 0.406 0.528 0.258 BASELINE 0.335 0.425 0.208 RU - EN IST - U NBABEL 0.351 0.332 0.165 BASELINE 0.251 0.256 0.073 ZERO - SHOT LANGUAGE PAIRS EN - CZ IST - U NBABEL 0.294 0.376 0.125 BASELINE 0.224 0.273 0.039 EN - JA IST - U NBABEL 0.175 0.169 0.025 BASELINE 0.175 0.131 0.036 PS - EN IST - U NBABEL 0.294 0.370 0.177 BASELINE 0.249 0.313 0.134 KM - EN IST - U NBABEL 0.345 0.448 0.259 BASELINE 0.279 0.351 0.175 Table 9 : Results for Task 2 word - level system on the held - out evaluation set of WMT 2021 .", "entities": []}, {"text": "972CS - EN DE - EN FI - EN RU - EN RO - EN TR - EN ZH - EN ET - EN LT - EN GU - EN KK - EN JA - EN KM - EN PL - EN PS - EN TA - EN Total tuples 28887 91584 47205 61505 560 30746 71941 20496 10315 9063 6789 8917 4722 11666 4611 7562 Avg . tokens ( reference ) 31.43 24.61 20.48 23.31 24.35 23.32 31.70 23.93 26.84 17.73 20.65 28.64 19.49 21.93 19.87 19.91 Avg . tokens ( source ) 25.65 22.93 14.49 19.77 24.99", "entities": []}, {"text": "19.01 6.05 18.61 20.61 15.13 16.47 3.27 29.91 18.55 21.87 15.31 Avg . tokens ( MT ) 29.99 24.19 19.95 23.51 24.42 22.97 30.60 24.06 25.44 17.15 20.00 27.41 19.59 21.64 19.37 20.14 Table 10 : Statistics for the WMT 15 to 20 Direct Assessments corpus into - English language pairs .", "entities": []}, {"text": "EN - RU EN - CSEN - DE EN - FI EN - LV EN - TR EN - ZH EN - ET EN - LT EN - GU EN - KK EN - JA EN - PL EN - TA Total tuples 63771 60905 55352 30924 5810 5171 66830 13376 8959 6924 8219 9573 10506 7886 Avg . tokens ( reference ) 22.48 23.48 23.96 17.7 20.45 19.74 7.26 18.83 20.61 22.07 19.21 1.4 24.54 19.84 Avg . tokens ( source ) 24.5 25.82 24 23.21 24.99 24.2 28.81 24.23 24.09 24.3 24.13 25.2 25.33 25.15 Avg . tokens ( MT ) 22.14 23 23.84 17.81 21.18 19.24 7.53 18.96 20.62 22.39 19.71 2.29 23.19 19.18 Table 11 : Statistics for the WMT 15 to 20 Direct Assessments corpus from - English language pairs .", "entities": []}]