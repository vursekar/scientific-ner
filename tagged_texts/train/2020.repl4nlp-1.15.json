[{"text": "Proceedings of the 5th Workshop on Representation Learning for NLP ( RepL4NLP-2020 ) , pages 110\u2013119 July 9 , 2020 .", "entities": [[6, 8, "TaskName", "Representation Learning"]]}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics110Contextual and Non - Contextual Word Embeddings : an in - depth Linguistic Investigation Alessio Miaschi?\u0005 , Felice Dell\u2019Orletta\u0005 ?", "entities": [[9, 11, "TaskName", "Word Embeddings"]]}, {"text": "Department of Computer Science , University of Pisa \u0005ItaliaNLP Lab , Istituto di Linguistica Computazionale \u201c Antonio Zampolli \u201d , Pisa alessio.miaschi@phd.unipi.it , felice.dellorletta@ilc.cnr.it", "entities": [[7, 8, "MethodName", "Pisa"], [20, 21, "MethodName", "Pisa"]]}, {"text": "Abstract In this paper we present a comparison between the linguistic knowledge encoded in the internal representations of a contextual Language Model ( BERT ) and a contextual - independent one ( Word2vec ) .", "entities": [[23, 24, "MethodName", "BERT"]]}, {"text": "We use a wide set of probing tasks , each of which corresponds to a distinct sentence - level feature extracted from different levels of linguistic annotation .", "entities": []}, {"text": "We show that , although BERT is capable of understanding the full context of each word in an input sequence , the implicit knowledge encoded in its aggregated sentence representations is still comparable to that of a contextualindependent model .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "We also \ufb01nd that BERT is able to encode sentence - level properties even within single - word embeddings , obtaining comparable or even superior results than those obtained with sentence representations .", "entities": [[4, 5, "MethodName", "BERT"], [17, 19, "TaskName", "word embeddings"]]}, {"text": "1 Introduction Distributional word representations ( Mikolov et al . , 2013 ) trained on large - scale corpora have rapidly become one of the most prominent component in modern NLP systems .", "entities": []}, {"text": "In this context , the recent development of context - dependent embeddings ( Peters et al . , 2018 ; Devlin et al . , 2019 ) has shown that such representations are able to achieve state - ofthe - art performance in many complex NLP tasks .", "entities": []}, {"text": "However , the introduction of such models made the interpretation of the syntactic and semantic properties learned by their inner representations more complex .", "entities": []}, {"text": "Recent studies have begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties ( Marvin and Linzen , 2018 ; Goldberg , 2019 ; Warstadt et al . , 2019 ) .", "entities": []}, {"text": "Much of this work focused on the de\ufb01nition ofprobing models trained to predict simple linguistic properties from unsupervised representations .", "entities": []}, {"text": "In particular , those work provided evidences thatcontextualized Neural Language Models ( NLMs ) are able to capture a wide range of linguistic phenomena ( Adi et al . , 2016 ; Perone et al . , 2018 ; Tenney et al . , 2019b ) and even to organize this information in a hierarchical manner ( Belinkov et al . , 2017 ; Lin et al . , 2019 ; Jawahar et al . , 2019 ) .", "entities": []}, {"text": "Despite this , less study focused on the analysis and the comparison of contextual and non - contextual NLMs according to their ability to encode implicit linguistic properties in their representations .", "entities": []}, {"text": "In this paper we perform a large number of probing experiments to analyze and compare the implicit knowledge stored by a contextual and a non - contextual model within their inner representations .", "entities": []}, {"text": "In particular , we de\ufb01ne two research questions , aimed at understanding : ( i ) which is the best method for combining BERT and Word2vec word representations into sentence embeddings and how they differently encode properties related to the linguistic structure of a sentence ; ( ii ) whether such sentence - level knowledge is preserved within BERT single - word representations .", "entities": [[23, 24, "MethodName", "BERT"], [29, 31, "TaskName", "sentence embeddings"], [58, 59, "MethodName", "BERT"]]}, {"text": "To answer our questions , we rely on a large suite of probing tasks , each of which codi\ufb01es a particular propriety of a sentence , from very shallow features ( such as sentence length and average number of characters per token ) to more complex aspects of morphosyntactic and syntactic structure ( such as the depth of the whole syntactic tree ) , thus making them as suitable to assess the implicit knowledge encoded by a NLM at a deep level of granularity .", "entities": []}, {"text": "The remainder of the paper is organized as follows .", "entities": []}, {"text": "First we present related work ( Sec . 2 ) , then , after brie\ufb02y presenting our approach ( Sec . 3 ) , we describe in more details the data ( Sec . 3.1 ) , our set of probing features ( Sec . 3.2 ) and the models used for the experiments ( Sec . 3.3 ) .", "entities": []}, {"text": "Experiments and results are described in Sec . 4 and 5 .", "entities": []}, {"text": "To conclude , in Sec .", "entities": []}, {"text": "6 we summarize the main \ufb01ndings of the study .", "entities": []}, {"text": "111Contributions In this paper : ( i ) we perform an in - depth study aimed at understanding the linguistic knowledge encoded in a contextual ( BERT ) and a contextual - independent ( Word2vec ) Neural Language Model ; ( ii ) we evaluate the best method for obtaining sentence - level representations from BERT and Word2vec according to a wide spectrum of probing tasks ; ( iii ) we compare the results obtained by BERT and Word2vec according to the different combining methods ; ( iv ) we study whether BERT is able to encode sentence - level properties within its single word representations .", "entities": [[26, 27, "MethodName", "BERT"], [55, 56, "MethodName", "BERT"], [76, 77, "MethodName", "BERT"], [92, 93, "MethodName", "BERT"]]}, {"text": "2 Related Work In the last few years , several methods have been devised to open the black box and understand the linguistic information encoded in NLMs ( Belinkov and Glass , 2019 ) .", "entities": []}, {"text": "They range from techniques to examine the activations of individual neurons ( Karpathy et al . , 2015 ; Li et", "entities": []}, {"text": "al . , 2016 ; K \u00b4 ad\u00b4ar et al . , 2017 ) to more domain speci\ufb01c approaches , such as interpreting attention mechanisms ( Raganato and Tiedemann , 2018 ; Kovaleva et al . , 2019 ; Vig and Belinkov , 2019 ) or designing speci\ufb01c probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word / sentence embeddings of a pre - trained model as training features ( Conneau et al . , 2018 ; Zhang and Bowman , 2018 ; Hewitt and Liang , 2019 ) .", "entities": [[68, 70, "TaskName", "sentence embeddings"]]}, {"text": "These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner ( Belinkov et al . , 2017 ; Blevins et al . , 2018 ; Tenney et al . , 2019b ) and even to support the extraction of dependency parse trees ( Hewitt and Manning , 2019 ) .", "entities": []}, {"text": "Jawahar et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) investigated the representations learned at different layers of BERT , showing that lower layer representations are usually better for capturing surface features , while embeddings from higher layers are better for syntactic and semantic properties .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "Using a suite of probing tasks , Tenney et al .", "entities": []}, {"text": "( 2019a ) found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline : POS tagging , parsing , NER , semantic roles and then coreference .", "entities": [[10, 11, "MethodName", "BERT"], [26, 27, "TaskName", "NER"]]}, {"text": "Liu et al .", "entities": []}, {"text": "( 2019 ) , instead , quanti\ufb01ed differences in the transferability of individual layers between different models , showing that higher layers of RNNs ( ELMo ) are more task - speci\ufb01c ( less general ) , while transformer layers ( BERT ) do not exhibit this increase in task - speci\ufb01city .", "entities": [[25, 26, "MethodName", "ELMo"], [41, 42, "MethodName", "BERT"]]}, {"text": "Closer to our study , Adi et al .", "entities": []}, {"text": "( 2016 ) proposed a method for analyzing and comparing different sentence representations and different dimensions , exploring the effect of the dimensionality on the resulting representations .", "entities": []}, {"text": "In particular , they showed that sentence representations based on averaged Word2vec embeddings are particularly effective and encode a wide amount of information regarding sentence length , while LSTM auto - encoders are very effective at capturing word order and word content .", "entities": [[28, 29, "MethodName", "LSTM"]]}, {"text": "Similarly , but focused on the resolution of speci\ufb01c downstream tasks , Shen et al . ( 2018 ) compared a Single Word Embedding - based model ( SWEM - based ) with existing recurrent and convolutional networks using a suite of 17 NLP datasets , demonstrating that simple pooling operations over SWEM - based representations exhibit comparable or even superior performance in the majority of cases considered .", "entities": []}, {"text": "On the contrary , Joshi et al .", "entities": []}, {"text": "( 2019 ) showed that , in the context of three different classi\ufb01cation problems in health informatics , context - based representations are a better choice than word - based representations to create vectors .", "entities": []}, {"text": "Focusing instead on the geometry of the representation space , Ethayarajh ( 2019 ) \ufb01rst showed that the contextualized word representations of ELMo , BERT and GPT-2 produce more context speci\ufb01c representations in the upper layers and then proposed a method for creating a new type of static embedding that outperforms GloVe and FastText on many benchmarks , by simply taking the \ufb01rst principal component of contextualized representations in lower layers of BERT .", "entities": [[22, 23, "MethodName", "ELMo"], [24, 25, "MethodName", "BERT"], [26, 27, "MethodName", "GPT-2"], [51, 52, "MethodName", "GloVe"], [53, 54, "MethodName", "FastText"], [72, 73, "MethodName", "BERT"]]}, {"text": "Differently from those latter work , our aim is to investigate the implicit linguistic knowledge encoded in pre - trained contextual and contextualindependent models both at sentence and word levels .", "entities": []}, {"text": "3", "entities": []}, {"text": "Our Approach We studied how layer - wise internal representations of BERT encode a wide spectrum of linguistic properties and how such implicit knowledge differs from that learned by a context - independent model such as Word2vec .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "Following the probing task approach as de\ufb01ned in Conneau et al .", "entities": []}, {"text": "( 2018 ) , we proposed a suite of 68 probing tasks , each of which corresponds to a distinct linguistic feature capturing raw - text , lexical , morpho - syntactic and syntactic characteristics of a sentence .", "entities": []}, {"text": "More specifically , we de\ufb01ned two sets of experiments .", "entities": []}, {"text": "The", "entities": []}, {"text": "112Level of Annotation Linguistic Feature Label Raw TextSentence Length sent length Word Length char pertok Type / Token Ratio for words and lemmas ttrform , ttr lemma POS taggingDistibution of UD and language \u2013 speci\ufb01c POSupos dist * , xpos dist *", "entities": [[26, 27, "DatasetName", "lemma"], [30, 31, "DatasetName", "UD"]]}, {"text": "Lexical density lexical density In\ufb02ectional morphology of lexical verbs and auxiliaries ( Mood , Number , Person , Tense and VerbForm)verbs * , aux * Dependency ParsingDepth of the whole syntactic tree parse depth Average length of dependency links and of the longest linkavglinks len , max links len Average length of prepositional chains and distribution by depthavgprepositional chain len , prep dist *", "entities": []}, {"text": "Clause length ( n. tokens / verbal heads ) avg token perclause Order of subject and object subj pre , obj post Verb arity and distribution of verbs by arityavgverb edges , verbal arity *", "entities": [[17, 18, "DatasetName", "subj"]]}, {"text": "Distribution of verbal heads and verbal rootsverbal head dist , verbal root perc Distribution of dependency relations dep dist * Distribution of subordinate and principal clausesprincipal proposition dist , subordinate proposition dist Average length of subordination chains and distribution by depthavgsubordinate chain len , subordinate dist1", "entities": []}, {"text": "Relative order of subordinate clauses subordinate post Table 1 : Linguistic Features used in the probing tasks .", "entities": []}, {"text": "\ufb01rst consists in evaluating which is the best method for generating sentence - level embeddings using BERT and Word2vec single - word representations .", "entities": [[16, 17, "MethodName", "BERT"]]}, {"text": "In particular , we de\ufb01ned a simple probing model that takes as input layer - wise BERT and Word2vec combined representations for each sentence of a gold standard Universal Dependencies ( UD ) ( Nivre et al . , 2016 )", "entities": [[16, 17, "MethodName", "BERT"], [28, 30, "DatasetName", "Universal Dependencies"], [31, 32, "DatasetName", "UD"]]}, {"text": "English dataset and predicts the actual value of a given probing feature .", "entities": []}, {"text": "Moreover , we compared the results to understand which model performs better according to different levels of linguistic sophistication .", "entities": []}, {"text": "In the second set of experiments , we measured how many sentence - level properties are encoded in single - word representations .", "entities": []}, {"text": "To do so , we performed our set of probing tasks using the embeddings extracted from both BERT and Word2vec individual tokens .", "entities": [[17, 18, "MethodName", "BERT"]]}, {"text": "In particular , we considered the word representations corresponding to the \ufb01rst , last and two internal tokens for each sentence of the UD dataset .", "entities": [[23, 24, "DatasetName", "UD"]]}, {"text": "3.1 Data In order to perform the probing experiments on gold annotated sentences , we relied on the Universal Dependencies ( UD ) English dataset .", "entities": [[18, 20, "DatasetName", "Universal Dependencies"], [21, 22, "DatasetName", "UD"]]}, {"text": "The dataset includes three UD English treebanks : UDEnglish - ParTUT , a conversion of a multilin - gual parallel treebank consisting of a variety of text genres , including talks , legal texts and Wikipedia articles ( Sanguinetti and Bosco , 2015 ) ; the Universal Dependencies version annotation from the GUM corpus ( Zeldes , 2017 ) ; the English Web Treebank ( EWT ) , a gold standard universal dependencies corpus for English ( Silveira et al . , 2014 ) .", "entities": [[4, 5, "DatasetName", "UD"], [46, 48, "DatasetName", "Universal Dependencies"], [52, 53, "DatasetName", "GUM"], [61, 64, "DatasetName", "English Web Treebank"], [71, 73, "DatasetName", "universal dependencies"]]}, {"text": "Overall , the \ufb01nal dataset consists of 23,943 sentences .", "entities": []}, {"text": "3.2 Probing Features As previously mentioned , our method is in line with the probing tasks approach de\ufb01ned in Conneau et al .", "entities": []}, {"text": "( 2018 ) , which aims to capture linguistic information from the representations learned by a NLM .", "entities": []}, {"text": "Speci\ufb01cally , in our work , each probing task correspond to predict the value of a speci\ufb01c linguistic feature automatically extracted from the POS tagged and dependency parsed sentences in the English UD dataset .", "entities": [[32, 33, "DatasetName", "UD"]]}, {"text": "The set of features is based on the ones described in Brunato et al .", "entities": []}, {"text": "( 2020 )", "entities": []}, {"text": "and it includes characteristics acquired from raw , morphosyntactic and syntactic levels of annotation .", "entities": []}, {"text": "As described in Brunato et al .", "entities": []}, {"text": "( 2020 ) , this set of features has been shown to have a highly predictive role when leveraged by traditional learning models on a variety of classi\ufb01cation problems , covering different aspects of stylometric and complexity analysis .", "entities": []}, {"text": "As shown in Table 1 , these features capture sev-", "entities": []}, {"text": "113eral linguistic phenomena ranging from the average length of words and sentence , to morpho \u2013 syntactic information both at the level of POS distribution and about the in\ufb02ectional properties of verbs .", "entities": []}, {"text": "More complex aspects of sentence structure are derived from syntactic annotation and model global and local properties of parsed tree structure , with a focus on subtrees of verbal heads , the order of subjects and objects with respect to the verb , the distribution of UD syntactic relations and features referring to the use of subordination .", "entities": [[46, 47, "DatasetName", "UD"]]}, {"text": "3.3 Models We relied on a pre - trained English version of BERT ( BERT - base uncased , 12 layers ) for the extraction of the contextual word embeddings .", "entities": [[12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "BERT"], [28, 30, "TaskName", "word embeddings"]]}, {"text": "To obtain the representations for our sentence - level tasks we experimented the activation of the \ufb01rst input token ( [ CLS ] ) 1and four different combining methods : Max - pooling , Min - pooling , Mean andSum .", "entities": []}, {"text": "Each of this four combining methods returns a single ~ s vector , such that each snis obtained by combining thenthcomponents w1n;w2n;:::;w mnof the embedding of each word in the input sentence .", "entities": []}, {"text": "In order to conduct a comparison of contextbased and word - based representations when solving our set of probing tasks , we performed all the probing experiments using also the embeddings extracted from a pre - trained version of Word2vec .", "entities": []}, {"text": "In particular , we trained the model on the English Wikipedia dataset ( dump of March 2020 ) , resulting in 300 - dimensional vectors .", "entities": []}, {"text": "In the same manner as BERT \u2019s contextual representations , we experimented four combining methods : Max - pooling , Min - pooling , Mean andSum .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "We used a linear Support Vector Regression model ( LinearSVR ) as probing model .", "entities": []}, {"text": "4 Evaluating Sentence Representations The \ufb01rst set of experiments consists in evaluating which is the best method for combining word - level embeddings into sentence representations in order to understand what kind of implicit linguistic properties are encoded within both contextual and noncontextual representations using different combining methods .", "entities": []}, {"text": "To do so , we \ufb01rstly extracted from each sentence in the UD dataset the corresponding word embeddings using the output of the internal representations of Word2vec and BERT layers 1As suggested in Jawahar", "entities": [[12, 13, "DatasetName", "UD"], [16, 18, "TaskName", "word embeddings"], [28, 29, "MethodName", "BERT"]]}, {"text": "et al .", "entities": []}, {"text": "( 2019 ) , the [ CLS ] token somehow summerizes the information encoded in the input sequence .", "entities": []}, {"text": "Categories BERT Word2vec Baseline Raw text 0.65 0.51 0.37 Morphosyntax 0.49 0.57 0.28 Syntax 0.55 0.56 0.44 All features 0.53 0.56 0.38 Table 2 : BERT ( average between layers ) and Word2vec \u001ascores computed by averaging Max- , Min- , Mean and Sum scores according to the three linguistic levels of annotations and considering all the probing features ( All features ) .", "entities": [[1, 2, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"]]}, {"text": "Baseline scores are also reported .", "entities": []}, {"text": "Categories Sum Min Max Mean Raw text 0.56 0.51 0.51 0.46 Morphosyntax 0.59 0.52 0.54 0.61 Syntax 0.61 0.55 0.55 0.54 All features 0.60 0.54 0.55 0.57 Table 3 : Word2vec probing scores obtained with the four sentence combining methods .", "entities": []}, {"text": "( from input layer -12to output layer -1 ) .", "entities": []}, {"text": "Secondly , we computed the sentence - representations according to the different combining strategies de\ufb01ned in 3.3 .", "entities": []}, {"text": "We then performed our set of 68 probing tasks using the LinearSVR model for each sentence representation .", "entities": []}, {"text": "Since the majority of our probing features is correlated to sentence length , we compared probing results with the ones obtained with a baseline computed by measuring the \u001acoef\ufb01cient between the length of the UD sentences and each of the 68 probing features .", "entities": [[34, 35, "DatasetName", "UD"]]}, {"text": "Evaluation was performed with a 5 - cross fold validation and using Spearman correlation score ( \u001a ) between predicted and gold labels as evaluation metric .", "entities": [[12, 14, "MetricName", "Spearman correlation"]]}, {"text": "Table 2 report average \u001ascores aggregating all probing results ( All features ) and according to raw text ( Raw text ) , morphosyntactic ( Morphosyntax ) and syntactic ( Syntax ) levels of annotations .", "entities": []}, {"text": "Scores are computed by averaging Max- , Min - pooling , Mean andSum results .", "entities": []}, {"text": "As a general remark , we notice that the scores obtained by Word2vec and BERT \u2019s internal representations outperforms the ones obtained with the correlation baseline , thus showing that both models are capable of implicitly encoding a wide spectrum of linguistic phenomena .", "entities": [[14, 15, "MethodName", "BERT"]]}, {"text": "Interestingly , we can notice that Word2vec sentence representations outperform BERT ones when considering all the probing features in average .", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "We report in Table 3 and Figure 1 the probing scores obtained by the two models .", "entities": []}, {"text": "For what concerns Word2vec representations , we notice that theSum method prove to be the best one for encoding raw text and syntactic features , while mo-", "entities": []}, {"text": "114 Figure 1 : Layerwise \u001ascores for the three categories of raw - text , morphosyntactic and syntactic features .", "entities": []}, {"text": "Layerwise average results are also reported .", "entities": []}, {"text": "Each line in the four plots corresponds to a different aggregating strategy .", "entities": []}, {"text": "rophosyntactic properties are better represented averaging all the word embeddings ( Mean ) .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "In general , best results are obtained with probing tasks related to morphosyntactic and syntactic features , like the distribution of POS ( e.g. upos distPRON , upos distVERB ) or the maximum depth of the syntactic tree ( parse depth ) .", "entities": [[32, 34, "HyperparameterName", "maximum depth"]]}, {"text": "If we look instead at the average \u001ascores obtained with BERT layerwise representations ( Figure 1 ) , we observe that , differently from Word2vec , best results are the ones related to raw - text features , such as sentence length or Type / Token Ratio .", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "The Mean method prove to be the best one for almost all the probing tasks , achieving highest scores in the \ufb01rst \ufb01ve layers .", "entities": []}, {"text": "The only exceptions mainly concern some of the linguistic features related to syntactic properties , e.g. the average length of dependency links ( avglinks len ) or the maximum depth of the syntactic tree ( parse depth ) , for which best scores across layers are obtained with the Sum strategy .", "entities": [[28, 30, "HyperparameterName", "maximum depth"]]}, {"text": "The MaxandMin - pooling methods , instead , show a similar trend for almost all the probing features .", "entities": []}, {"text": "Interestingly , the representations corresponding to theLayers Mean Max - pooling Min - pooling Sum -12", "entities": []}, {"text": ".052 -.058 -.038 -.091 -11", "entities": []}, {"text": ".065", "entities": []}, {"text": "-.055 -.038 -.084 -10", "entities": []}, {"text": ".063", "entities": []}, {"text": "-.053 -.043", "entities": []}, {"text": "-.088 -9", "entities": []}, {"text": ".058 -.044", "entities": []}, {"text": "-.036 -.089 -8", "entities": []}, {"text": ".066 -.039 -.034", "entities": []}, {"text": "-.088", "entities": []}, {"text": "-7", "entities": []}, {"text": ".058 -.046 -.033", "entities": []}, {"text": "-.088", "entities": []}, {"text": "-6", "entities": []}, {"text": ".051 -.048", "entities": []}, {"text": "-.045 -.094 -5 .046", "entities": []}, {"text": "-.035 -.032", "entities": []}, {"text": "-.096 -4", "entities": []}, {"text": ".042", "entities": []}, {"text": "-.043 -.025", "entities": []}, {"text": "-.102 -3 .026 -.049 -.041 -.113 -2", "entities": []}, {"text": ".006 -.057 -.045 -.119 -1", "entities": []}, {"text": "-.007", "entities": []}, {"text": "-.069 -.063", "entities": []}, {"text": "-.128 Table 4 : Average \u001adifferences between BERT and Word2vec probing results according to the four embedding - aggregation strategies .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "[ CLS ] token , although considered as a summarization of the entire input sequence , achieve results comparable to those obtained with Max- andMinpooling methods .", "entities": [[9, 10, "TaskName", "summarization"]]}, {"text": "Moreover , it can be noticed that , unlike Max- andMin - pooling , the representations computed with Mean andSum methods tend to lose their average precision in encoding our set of linguistic properties across the 12 layers .", "entities": [[25, 27, "MetricName", "average precision"]]}, {"text": "115 Figure 2 : Differences between BERT and Word2vec scores ( multiplied by 100 ) for all the 68 probing features ( ranked by correlation with sentence length ) , obtained with the Mean aggregation strategy .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "BERT scores are reported for all the 12 layers .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "Positive ( red ) and negative ( blue ) cells correspond to scores for which BERT outperforms Word2vec and vice versa .", "entities": [[15, 16, "MethodName", "BERT"]]}, {"text": "In order to investigate more in depth how the linguistic knowledge encoded by BERT across its layers differs from that learned by Word2vec , we report in Table 4 average \u001adifferences between the two models according to the four combining strategies .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "As a general remark , we can notice that , regardless of the aggregation strategy taken into account , BERT and Word2vec sentence representations achieve quite similar results on average .", "entities": [[19, 20, "MethodName", "BERT"]]}, {"text": "Hence , although BERT is capable of understanding the full context of each word in an input sequence , the amount of linguistic knowledge implicitly encoded in its aggregated sentence representations is still comparable to that which can be achieved with a non - contextual language model .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "In Figure 2 we report instead the differences between BERT and Word2vec scores for all the 68 probing features ( ordered by correlation with sentence length ) .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "For the comparison , we used the representations obtained with the Mean combining method .", "entities": []}, {"text": "As a \ufb01rst remark , we notice that there is a clear distinction in terms of \u001ascores between features better predicted by BERT and Word2vec .", "entities": [[22, 23, "MethodName", "BERT"]]}, {"text": "In fact , features most related to syntactic properties ( left heatmap ) are those for which BERT results are generally higher with respect to those obtained with Word2vec .", "entities": [[11, 12, "MethodName", "heatmap"], [17, 18, "MethodName", "BERT"]]}, {"text": "This result demonstrates that BERT , unlike a non - contextual language model as Word2vec , is able to encode information within its representa - tions that involves the entire input sequence , thus making more simple to solve probing tasks that refer to syntatic characteristics .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "Focusing instead on the right heatmap , we observe that Word2vec non - contextual representations are still capable of encoding a wide spectrum of linguistic properties with higher \u001avalues compared to BERT ones , especially if we consider scores closer to BERT \u2019s output layers ( from -4to -1 ) .", "entities": [[5, 6, "MethodName", "heatmap"], [31, 32, "MethodName", "BERT"], [41, 42, "MethodName", "BERT"]]}, {"text": "This is particularly evident for morphosyntactic features related to the distribution of POS categories ( xpos dist*,upos dist * ) , most likely because non - contextual representations tend to encode properties related to single tokens rather than syntactic relations between them .", "entities": []}, {"text": "5 Evaluating Word Representations Once we have probed the linguistic knowledge encoded by BERT and Word2vec using different strategies for computing sentence embeddings , we investigated how much information about the structure of a sentence is encoded within single - word contextual representations .", "entities": [[13, 14, "MethodName", "BERT"], [21, 23, "TaskName", "sentence embeddings"]]}, {"text": "For doing so , we performed our sentence - level probing tasks using a single BERT word embedding for each sentence in the UD dataset .", "entities": [[15, 16, "MethodName", "BERT"], [23, 24, "DatasetName", "UD"]]}, {"text": "We tested four different words , corresponding to the \ufb01rst , the last and two internal tokens for each sentence in the UD dataset .", "entities": [[22, 23, "DatasetName", "UD"]]}, {"text": "In", "entities": []}, {"text": "116 Figure 3 : Probing scores obtained by BERT word ( tok * ) and sentence ( mean ) representations extracted from layers -1and-8 .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "Sentence embeddings are computed using the Mean method .", "entities": [[0, 2, "TaskName", "Sentence embeddings"]]}, {"text": "Embeddings Raw Morphoyntax Syntax All BERT-1 (-8 ) 0.62 0.57 0.55 0.57 BERT-2 (-8 ) 0.59 0.53 0.53 0.53 BERT-3 (-8 ) 0.59 0.52 0.52 0.53 BERT-4 (-8 ) 0.65 0.66 0.62 0.64 BERT-1 ( -1 ) 0.55 0.55 0.51 0.53 BERT-2 ( -1 ) 0.54 0.51 0.49 0.50 BERT-3 ( -1 ) 0.54 0.51 0.49 0.50 BERT-4 ( -1 ) 0.59 0.57 0.53 0.55", "entities": []}, {"text": "[ CLS ] (-8 ) 0.66 0.47 0.52 0.51", "entities": []}, {"text": "[ CLS ] ( -1 ) 0.61 0.45 0.49 0.48 Word2vec-1 0.26 0.26 0.22 0.24 Word2vec-2 0.17 0.21 0.18 0.19 Word2vec-3 0.17 0.19 0.17 0.18", "entities": []}, {"text": "Word2vec-4 0.13 0.15 0.12 0.13 Table 5 : Average \u001ascores obtained by BERT and Word2vec according to word representations corresponding to the \ufb01rst , the last and two internal tokens of each input sentence .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "Results are computed according to the three linguistic levels of annotation and considering all the probing features ( All ) .", "entities": []}, {"text": "Average scores obtained with the [ CLS ] token are also reported .", "entities": []}, {"text": "particular , we extracted the embeddings from the output layer ( -1 ) and from the layer that achieved best results in the previous experiments ( -8 ) .", "entities": []}, {"text": "We used probing scores obtained with Word2vec embeddings for the same tokens as baseline .", "entities": []}, {"text": "In Table 5 we report average \u001ascores obtained by BERT ( BERT- * ) and Word2vec ( Word2vec- * ) according to word - level representations extracted from the four tokens mentioned above .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "Results were computed aggregating all probing results ( All ) and accordingto raw text ( Raw ) , morphosyntactic ( Morphosyntax ) and syntatic ( Syntax ) levels of annotation .", "entities": []}, {"text": "For comparison , we also report average scores obtained with the [ CLS ] token .", "entities": []}, {"text": "As a \ufb01rst remark , we can clearly notice that even with a single - word embedding BERT is able to encode a wide spectrum of sentence - level linguistic properties .", "entities": [[17, 18, "MethodName", "BERT"]]}, {"text": "This result allows us to highlight the main potential of contextual representations , i.e. the capability of capturing linguistic phenomena that refer to the entire input sequence within single - word representations .", "entities": []}, {"text": "An interesting observation is that , except for the raw text features , for which the best scores are achieved using [ CLS ] , higher performance are obtained with the embeddings corresponding to BERT-4 , i.e. the last token of each sentence .", "entities": []}, {"text": "This result seems to indicate that[CLS ] , although being used for classi\ufb01cation predictions , does not necessarily correspond to the most linguistically informative token within each input sequence .", "entities": []}, {"text": "Comparing the results with those achieved using Word2vec word embeddings , we notice that BERT scores greatly outperform Word2vec for all the probing tasks .", "entities": [[8, 10, "TaskName", "word embeddings"], [14, 15, "MethodName", "BERT"]]}, {"text": "This is a straightforward result and can be easily explained by the fact that the lack of contextual knowledge does not allow singleword representations to encode information that are related to the structure of the whole sentence .", "entities": []}, {"text": "117Since the latter results demonstrated that BERT is capable of encoding many sentence - level properties within its single word representations , as a last analysis , we decided to compare these results with the ones obtained using sentence embeddings .", "entities": [[6, 7, "MethodName", "BERT"], [38, 40, "TaskName", "sentence embeddings"]]}, {"text": "In particular , Figure 3 reports probing scores obtained by BERT single word ( tok * ) and Mean sentence representations ( sent ) extracted from the output layer ( -1 ) and from the layer that achieved best results in average ( -8 ) .", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "As already mentioned , for many of these probing tasks , word embeddings performance is comparable to that obtained with the aggregated sentence representations .", "entities": [[11, 13, "TaskName", "word embeddings"]]}, {"text": "Nevertheless , there are several cases in which the difference between performance is particularly signi\ufb01cant .", "entities": []}, {"text": "Interestingly , we can notice that aggregated sentence representations are generally better for predicting properties belonging to the left heatmap , i.e. to the group of features more related to syntactic properties .", "entities": [[19, 20, "MethodName", "heatmap"]]}, {"text": "This is particularly noticeable for the average number of tokens per clause ( avgtoken perclause ) or the distribution of subordinate chains by length ( suborddist ) , for which we observe an improvement from word - level to sentence - level representations of more than .10 \u001apoints .", "entities": []}, {"text": "On the contrary , probing features belonging to the right heatmap , therefore more close to raw text and morphosyntactic properties , are generally better predicted using single word embeddings , especially when considering the inner representations corresponding to the last token in each sentence ( tok4 ) .", "entities": [[10, 11, "MethodName", "heatmap"], [28, 30, "TaskName", "word embeddings"]]}, {"text": "The property most affected by the difference in scores between wordand sentence - level embeddings is the the distribution of periods ( xpos dist . ) .", "entities": []}, {"text": "Focusing instead on differences in performance between the two considered layers , we can notice that regardless of the method used to predict each feature , the representations learned by BERT tend to lose their precision in encoding our set of linguistic properties , most likely because the model is storing task - speci\ufb01c information ( Masked Language Modeling task ) at the expense of its ability to encode general knowledge about the language .", "entities": [[30, 31, "MethodName", "BERT"], [56, 59, "TaskName", "Masked Language Modeling"], [69, 71, "TaskName", "general knowledge"]]}, {"text": "6 Conclusion In this paper we studied the linguistic knowledge implicitly encoded in the internal representations of a contextual Language Model ( BERT ) and a contextual - independent one ( Word2vec ) .", "entities": [[22, 23, "MethodName", "BERT"]]}, {"text": "Using a suite of 68 probing tasks and testing differentmethods for combining word embeddings into sentence representations , we showed that BERT and Word2vec encode a wide set of sentence - level linguistic properties in a similar manner .", "entities": [[12, 14, "TaskName", "word embeddings"], [21, 22, "MethodName", "BERT"]]}, {"text": "Nevertheless , we found that for Word2vec the best method for obtaining sentence representations is the Sum , while BERT is more effective when averaging all the single - word representations ( Mean method ) .", "entities": [[19, 20, "MethodName", "BERT"]]}, {"text": "Moreover , we showed that BERT is able in storing features that are mainly related to raw text and syntactic properties , while Word2vec is good at predicting morphosyntactic characteristics .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "Finally , we showed that BERT is able to encode sentence - level linguistic phenomena even within single - word embeddings , exhibiting comparable or even superior performance than those obtained with aggregated sentence representations .", "entities": [[5, 6, "MethodName", "BERT"], [19, 21, "TaskName", "word embeddings"]]}, {"text": "Moreover , we found that , at least for morphosyntactic and syntactic characteristics , the most informative word representation is the one that correspond to the last token of each input sequence and not , as might be expected , to the [ CLS ] special token .", "entities": []}, {"text": "References Yossi Adi , Einat Kermany , Yonatan Belinkov , Ofer Lavi , and Yoav Goldberg . 2016 .", "entities": []}, {"text": "Fine - grained analysis of sentence embeddings using auxiliary prediction tasks .", "entities": [[5, 7, "TaskName", "sentence embeddings"]]}, {"text": "arXiv preprint arXiv:1608.04207 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yonatan Belinkov and James Glass .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Analysis methods in neural language processing : A survey .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 7:49\u201372 .", "entities": []}, {"text": "Yonatan Belinkov , Llu \u00b4 \u0131s M ` arquez , Hassan Sajjad , Nadir Durrani , Fahim Dalvi , and James Glass . 2017 .", "entities": []}, {"text": "Evaluating layers of representation in neural machine translation on part - of - speech and semantic tagging tasks .", "entities": [[6, 8, "TaskName", "machine translation"], [9, 12, "DatasetName", "part - of"]]}, {"text": "In Proceedings of the Eighth International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 1\u201310 .", "entities": []}, {"text": "Terra Blevins , Omer Levy , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep rnns encode soft hierarchical syntax .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 14\u201319 .", "entities": []}, {"text": "Dominique Brunato , Andrea Cimino , Felice Dell\u2019Orletta , Giulia Venturi , and Simonetta Montemagni .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Pro\ufb01ling - ud : a tool for linguistic pro\ufb01ling of texts .", "entities": [[2, 3, "DatasetName", "ud"]]}, {"text": "In Proceedings of The 12th Language Resources and Evaluation Conference , pages 7147\u20137153 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Alexis Conneau , Germ \u00b4 an Kruszewski , Guillaume Lample , Lo \u00a8\u0131c Barrault , and Marco Baroni .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "What", "entities": []}, {"text": "118you can cram into a single $ & ! # * vector : Probing sentence embeddings for linguistic properties .", "entities": [[14, 16, "TaskName", "sentence embeddings"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2126\u20132136 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Kawin Ethayarajh .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "How contextual are contextualized word representations ?", "entities": []}, {"text": "comparing the geometry of BERT , ELMo , and GPT-2 embeddings .", "entities": [[4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "ELMo"], [9, 10, "MethodName", "GPT-2"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 55\u201365 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yoav Goldberg .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Assessing bert \u2019s syntactic abilities .", "entities": []}, {"text": "arXiv preprint arXiv:1901.05287 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "John Hewitt and Percy Liang . 2019 .", "entities": []}, {"text": "Designing and interpreting probes with control tasks .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2733\u20132743 .", "entities": []}, {"text": "John Hewitt and Christopher D Manning .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A structural probe for \ufb01nding syntax in word representations .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4129\u20134138 .", "entities": []}, {"text": "Ganesh Jawahar , Beno \u02c6\u0131t Sagot , Djam \u00b4 e Seddah , Samuel Unicomb , Gerardo I \u02dcniguez , M \u00b4 arton Karsai , Yannick L\u00b4eo , M \u00b4 arton Karsai , Carlos Sarraute , \u00b4 Eric Fleury , et al . 2019 .", "entities": []}, {"text": "What does bert learn about the structure of language ?", "entities": []}, {"text": "In 57th Annual Meeting of the Association for Computational Linguistics ( ACL ) , Florence , Italy .", "entities": [[14, 15, "MethodName", "Florence"]]}, {"text": "Aditya Joshi , Sarvnaz Karimi , Ross Sparks , Cecile Paris , and C Raina MacIntyre .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A comparison of word - based and context - based representations for classi\ufb01cation problems in health informatics .", "entities": []}, {"text": "In Proceedings of the 18th BioNLP Workshop and Shared Task , pages 135\u2013141 , Florence , Italy . Association for Computational Linguistics .", "entities": [[14, 15, "MethodName", "Florence"]]}, {"text": "Akos K \u00b4 ad\u00b4ar , Grzegorz Chrupa\u0142a , and Afra Alishahi .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Representation of linguistic form and function in recurrent neural networks .", "entities": []}, {"text": "Computational Linguistics , 43(4):761\u2013780.Andrej Karpathy , Justin Johnson , and Li Fei - Fei . 2015 .", "entities": []}, {"text": "Visualizing and understanding recurrent networks .", "entities": []}, {"text": "arXiv preprint arXiv:1506.02078 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Olga Kovaleva , Alexey Romanov , Anna Rogers , and Anna Rumshisky .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Revealing the dark secrets of BERT .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 4365\u20134374 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jiwei Li , Xinlei Chen , Eduard Hovy , and Dan Jurafsky . 2016 .", "entities": []}, {"text": "Visualizing and understanding neural models in nlp .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 681\u2013691 .", "entities": []}, {"text": "Yongjie Lin , Yi Chern Tan , and Robert Frank . 2019 .", "entities": []}, {"text": "Open sesame : Getting inside BERT \u2019s linguistic knowledge .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 ACL Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 241\u2013253 , Florence , Italy .", "entities": [[20, 21, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nelson F Liu , Matt Gardner , Yonatan Belinkov , Matthew E Peters , and Noah A Smith .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Linguistic knowledge and transferability of contextual representations .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1073\u20131094 .", "entities": []}, {"text": "Rebecca Marvin and Tal Linzen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Targeted syntactic evaluation of language models .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1192\u20131202 .", "entities": []}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , and Jeff Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 3111\u20133119 .", "entities": []}, {"text": "Joakim Nivre , Marie - Catherine De Marneffe , Filip Ginter , Yoav Goldberg , Jan Hajic , Christopher D Manning , Ryan McDonald , Slav Petrov , Sampo Pyysalo , Natalia Silveira , et al . 2016 .", "entities": []}, {"text": "Universal dependencies v1 : A multilingual treebank collection .", "entities": [[0, 2, "DatasetName", "Universal dependencies"]]}, {"text": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC\u201916 ) , pages 1659\u20131666 .", "entities": []}, {"text": "Christian S Perone , Roberto Silveira , and Thomas S Paula .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Evaluation of sentence embeddings in downstream and linguistic probing tasks .", "entities": [[2, 4, "TaskName", "sentence embeddings"]]}, {"text": "arXiv preprint arXiv:1806.06259 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In Proceedings of the 2018 Conference", "entities": []}, {"text": "119of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 2227 \u2013 2237 .", "entities": []}, {"text": "Alessandro Raganato and J \u00a8org Tiedemann .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "An analysis of encoder representations in transformerbased machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Manuela Sanguinetti and Cristina Bosco . 2015 .", "entities": []}, {"text": "Parttut : The turin university parallel treebank .", "entities": []}, {"text": "In Harmonization and Development of Resources and Tools for Italian Natural Language Processing within the PARLI Project , pages 51\u201369 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Dinghan Shen , Guoyin Wang , Wenlin Wang , Martin Renqiang Min , Qinliang Su , Yizhe Zhang , Chunyuan Li , Ricardo Henao , and Lawrence Carin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Baseline needs more love :", "entities": []}, {"text": "On simple word - embedding - based models and associated pooling mechanisms .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 440 \u2013 450 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Natalia Silveira , Timothy Dozat , Marie - Catherine De Marneffe , Samuel R Bowman , Miriam Connor , John Bauer , and Christopher D Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A gold standard dependency corpus for english .", "entities": []}, {"text": "In LREC , pages 2897\u20132904 .", "entities": []}, {"text": "Ian Tenney , Dipanjan Das , and Ellie Pavlick . 2019a .", "entities": []}, {"text": "BERT rediscovers the classical NLP pipeline .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4593 \u2013 4601 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Ian Tenney , Patrick Xia , Berlin Chen , Alex Wang , Adam Poliak , R Thomas McCoy , Najoung Kim , Benjamin Van Durme , Samuel R Bowman , Dipanjan Das , et al . 2019b .", "entities": [[12, 13, "MethodName", "Adam"]]}, {"text": "What do you learn from context ?", "entities": []}, {"text": "probing for sentence structure in contextualized word representations .", "entities": []}, {"text": "arXiv preprint arXiv:1905.06316 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jesse Vig and Yonatan Belinkov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Analyzing the structure of attention in a transformer language model .", "entities": []}, {"text": "In Proceedings of the 2019 ACL Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 63\u201376 , Florence , Italy .", "entities": [[20, 21, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alex Warstadt , Yu Cao , Ioana Grosu , Wei Peng , Hagen Blix , Yining Nie , Anna Alsop , Shikha Bordia , Haokun Liu , Alicia Parrish , et al . 2019 .", "entities": []}, {"text": "Investigating bert \u2019s knowledge of language : Five analysis methods with npis .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 2870\u20132880.Amir Zeldes .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "The GUM corpus : Creating multilayer resources in the classroom .", "entities": [[1, 2, "DatasetName", "GUM"]]}, {"text": "Language Resources and Evaluation , 51(3):581\u2013612 .", "entities": []}, {"text": "Kelly Zhang and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Language modeling teaches you more than translation does : Lessons learned through auxiliary syntactic task analysis .", "entities": []}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 359\u2013361 .", "entities": []}]