[{"text": "Proceedings of The 13th International Conference on Natural Language Generation , pages 306\u2013315 , Dublin , Ireland , 15 - 18 December , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics306Neural NLG for Methodius : From RST", "entities": []}, {"text": "Meaning Representations to Texts\u0003 Symon Jory Stevens - GuilleyAleksandre MaskharashviliyAmy Isardz Xintong", "entities": [[4, 5, "DatasetName", "Symon"]]}, {"text": "Liyand Michael Whitey yThe Ohio State UniversityzUniversity of Hamburg Abstract While classic NLG systems typically made use of hierarchically structured content plans that included discourse relations as central components , more recent neural approaches have mostly mapped simple , \ufb02at inputs to texts without representing discourse relations explicitly .", "entities": []}, {"text": "In this paper , we investigate whether it is bene\ufb01cial to include discourse relations in the input to neural data - to - text generators for texts where discourse relations play an important role .", "entities": []}, {"text": "To do so , we reimplement the sentence planning and realization components of a classic NLG system , Methodius , using LSTM sequence - to - sequence ( seq2seq ) models .", "entities": [[21, 22, "MethodName", "LSTM"], [28, 29, "MethodName", "seq2seq"]]}, {"text": "We \ufb01nd that although seq2seq models can learn to generate \ufb02uent and grammatical texts remarkably well with suf\ufb01ciently representative Methodius training data , they can not learn to correctly express Methodius \u2019s SIMILARITY and CONTRAST comparisons unless the corresponding RST relations are included in the inputs .", "entities": [[4, 5, "MethodName", "seq2seq"]]}, {"text": "Additionally , we experiment with using self - training and reverse model reranking to better handle train / test data mismatches , and \ufb01nd that while these methods help reduce content errors , it remains essential to include discourse relations in the input to obtain optimal performance .", "entities": []}, {"text": "1 Introduction Traditional approaches to the task of natural language generation ( NLG ) have employed a pipeline of modules , moving from an initial abstract meaning representation ( MR ) to human - readable natural language ( Reiter and Dale , 2000 ) .", "entities": [[29, 30, "DatasetName", "MR"]]}, {"text": "In the last decade , the success of neural methods in other domains of natural language processing ( NLP ) has led to the development of neural \u2018 end - to - end \u2019 ( e2e ) \u0003The \ufb01rst two authors are listed in random order ( equal contribution ) , then the other authors are listed in alphabetical order by last name .", "entities": [[35, 36, "DatasetName", "e2e"]]}, {"text": "E - mail : stevensguille.1@buckeyemail.osu.eduarchitectures in NLG ( Du \u02c7sek et al . , 2020 ) , where a direct mapping from MRs to text is learned .", "entities": []}, {"text": "Since target texts for training neural models are typically crowd - sourced , the neural approach promises to make it easier to scale up the development of NLG systems in comparison to classic approaches , which generally require domain- or applicationspeci\ufb01c rules to be developed , even if the modules themselves are reusable .", "entities": []}, {"text": "Accompanying the increase in crowd - sourced corpora has been a comparative simpli\ufb01cation of both MRs and tasks .", "entities": []}, {"text": "In particular , classic NLG systems typically made use of hierarchically structured content plans that included discourse relations as central components , where the discourse relations \u2014 often based on Rhetorical Structure Theory ( RST ) ( Mann and Thompson , 1988 ; Taboada and Mann , 2006 ) \u2014 group together and connect elementary propositions or messages ( Hovy , 1993 ; Stede and Umbach , 1998 ; Isard , 2016 ) .", "entities": []}, {"text": "By contrast , more recent neural approaches \u2014 in particular , those developed for the E2E and WebNLG shared task challenges \u2014 have mostly mapped simple , \ufb02at inputs to texts without representing discourse relations explicitly .", "entities": [[15, 16, "DatasetName", "E2E"], [17, 18, "DatasetName", "WebNLG"]]}, {"text": "The absence of discourse relations in work on neural NLG to date is somewhat understandable given that neural systems have primarily tackled texts that merely describe entities , rather than comparing them , situating them in time , discussing causal or other contingency relations among them , or constructing persuasive arguments about them , where discourse relations are crucial for coherence ( Prasad et al . , 2008 ) .", "entities": []}, {"text": "Recently , Balakrishnan et al .", "entities": []}, {"text": "( 2019a ) have argued that discourse relations should be reintroduced into neural generation in order to enable the correct expression of these relations to be more reliably controlled .", "entities": []}, {"text": "However , they do note that only 6 % of the crowd - sourced E2E Challenge texts contain discourse connectives ex-", "entities": [[14, 15, "DatasetName", "E2E"]]}, {"text": "307pressing CONTRAST , and though they introduce a conversational weather dataset that uses both CONTRAST and JUSTIFY relations with greater frequency , it is fair to say that the use of hierarchical MRs that incorporate discourse relations remains far from common practice .", "entities": []}, {"text": "In this paper , we investigate whether it is bene\ufb01cial to include discourse relations in the input to neural data - to - text generators for texts where discourse relations play an important role .", "entities": []}, {"text": "To do so , we reimplement the sentence planning and realization components of a classic NLG system , Methodius ( Isard , 2016 ) , using LSTM sequenceto - sequence ( seq2seq ) models , since Methodius makes similarity or contrast comparisons in most of its outputs .", "entities": [[26, 27, "MethodName", "LSTM"], [31, 32, "MethodName", "seq2seq"]]}, {"text": "Speci\ufb01cally , rather than crowd - source output texts for Methodius \u2019s content plans , we run the existing system to obtain target texts for training seq2seq models , and experiment with input MRs ( derived from the content plans ) that contain discourse relations as well as ones that leave them out.1 In our experiments , we observe that the seq2seq models learn to generate \ufb02uent and grammatical texts remarkably well .", "entities": [[26, 27, "MethodName", "seq2seq"], [61, 62, "MethodName", "seq2seq"]]}, {"text": "As such , we focus our evaluation on the correct and coherent expression of discourse relations .", "entities": []}, {"text": "Since the Methodius texts are somewhat formulaic following delexicalization and entity anonymization , it is possible to write accurate automatic correctness checks for these relations .", "entities": []}, {"text": "Using these automatic checks , we \ufb01nd that even with suf\ufb01ciently representative Methodius training data , LSTM seq2seq models can not learn to correctly express Methodius \u2019s similarity and contrast comparisons unless the corresponding RST relations are included in the inputs .", "entities": [[16, 17, "MethodName", "LSTM"], [17, 18, "MethodName", "seq2seq"]]}, {"text": "This is an at least somewhat surprising result , since these relations are easily inferred from the input facts being compared .", "entities": []}, {"text": "The major conclusion of our experiments is that explicitly encoding discourse information using RST relations boosts coherence by enabling rhetorical structure to be reliably lexicalized .", "entities": []}, {"text": "Several techniques for improving the models are also considered , especially for situations where the training data exhibits mismatches with the test data ( as can happen in practice ) .", "entities": []}, {"text": "One technique involves outputting a beam of possible text outputs and reranking them by checking the correspondence between 1The data and code for this paper can be accessed by the following link :", "entities": []}, {"text": "https://github.com/ Methodius - Project / Neural - Methodius .the", "entities": []}, {"text": "input meaning representation and the meaning representation produced by using a reversed model to map texts to meaning representations .", "entities": []}, {"text": "The other technique is self - training ( Li and White , 2020 ) , i.e. , using an initial model to generate additional training data .", "entities": []}, {"text": "This method drastically increases the amount of training data available for what is otherwise quite a small corpus .", "entities": []}, {"text": "The upshot of these techniques is moderate improvement in the performance of both models with respect to the evaluation metrics just mentioned .", "entities": []}, {"text": "But the conclusion remains that the model trained on explicit RST information continues to outperform the model without explicit RST structure in the input .", "entities": []}, {"text": "2 Methodius The Methodius system ( Isard , 2016 ) was developed for multilingual text generation , based on the M - PIRO project ( Isard et al . , 2003 ; Isard , 2007 ) which focused on museum exhibit descriptions .", "entities": [[14, 16, "TaskName", "text generation"]]}, {"text": "Methodius consists of several components .", "entities": []}, {"text": "The content module selects content from a database and creates a content plan , which is a tree where the nodes are labeled with rhetorical relations or facts , following the structures proposed in RST .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "1 shows a content plan .", "entities": []}, {"text": "The content plan is rewritten into a sequence of logical forms , one per sentence , by the sentence planner .", "entities": []}, {"text": "The logical forms are then realized as a text by means of a Combinatory Categorial Grammar ( CCG ) using OpenCCG ( White , 2006 ) .", "entities": []}, {"text": "The Methodius system is designed to respond to the behaviour of the its intended users .", "entities": []}, {"text": "Sequences of exhibits , dubbed \u2018 chains \u2019 , are constructed while the user moves through the museum .", "entities": []}, {"text": "The chains control dependencies between exhibit descriptions , limit redundancy , and provide discourse continuity .", "entities": []}, {"text": "While RST de\ufb01nes a number of rhetorical relations , Methodius incorporates only four of them : ELABORATION , JOINT , SIMILARITY and CONTRAST .ELABORATION connects the main fact about a focal entity with other , peripheral facts about that entity .", "entities": []}, {"text": "JOINT connects two facts of equal status .", "entities": []}, {"text": "SIMILARITY and CONTRAST each connect two facts of equal status , but they do opposite jobs : SIMILARITY is used to express the similarity of two entities in terms of a commonly shared feature , while CONTRAST is used to show that the values of a shared feature of the given entities differ .", "entities": []}, {"text": "For instance , unlike the previous coins you saw , which are located in the Athens Numismatic Museum , this", "entities": []}, {"text": "308[__content_plan [ _ _ rst_elaboration", "entities": []}, {"text": "[ _ _ fact_type", "entities": []}, {"text": "[ _ _ arg1 entity0 ]", "entities": []}, {"text": "[ _ _ arg2 marriage_cauldron ] ]", "entities": []}, {"text": "[ _ _ rst_joint", "entities": []}, {"text": "[ _ _ fact_creation_period", "entities": []}, {"text": "[ _ _ arg1 entity0 ]", "entities": []}, {"text": "[ _ _ arg2 \" classical period \" ] ]", "entities": []}, {"text": "[ _ _ fact_creation_time", "entities": []}, {"text": "[ _ _ arg1 entity0 ]", "entities": []}, {"text": "[ _ _ arg2 \" between 420 and 410 B.C. \" ] ] ] ]", "entities": []}, {"text": "[ _ _ rst_similarity [ _ _ fact_painting_technique_used compare_additive", "entities": []}, {"text": "[ _ _ arg1", "entities": []}, {"text": "entity1 ]", "entities": []}, {"text": "[ _ _ arg2 \" red figure technique \" ] ]", "entities": []}, {"text": "[ _ _ fact_painting_technique_used [ _ _ arg1 entity0 ]", "entities": []}, {"text": "[ _ _ arg2 \" red figure technique \" ] ] ]", "entities": []}, {"text": "[ _ _ optional_type", "entities": []}, {"text": "[ _ _ arg1", "entities": []}, {"text": "entity1 ]", "entities": []}, {"text": "[ _ _ arg2 vessel ] ] ] Figure 1 : Content plan corresponding to the text ( 1a ) tetradrachm is located in the National Museum of Athens \u2013 here unlike signals CONTRAST .", "entities": []}, {"text": "In the following example , likesignals SIMILARITY : like the previous coins you saw , this tetradrachm is located in the National Museum of Athens .", "entities": []}, {"text": "In the experiments discussed below we focus on SIMILARITY and CONTRAST because the Methodius corpus lexicalizes them .", "entities": []}, {"text": "Due to the dynamic generation of the exhibit descriptions , SIMILAR ITYand CONTRAST link information in the current exhibit to previously mentioned exhibits and their properties \u2014 as such , correctly generating such expressions is vital to maintaining the coherence of the exhibit chain .", "entities": []}, {"text": "3 Data Preprocessing 3.1 Delexicalization The textual output of Methodius is pseudo - English with some expressions replaced by canned text , the morpho - syntactic descriptions of which are not present in either the content plan or in the logical form .", "entities": []}, {"text": "Instead the canned text is retrieved from the Methodius system \u2019s database by looking up the reference given in the content plan .", "entities": []}, {"text": "Such canned texts might occur infrequently in a relatively small corpus .", "entities": []}, {"text": "To avoid data sparsity , we substitute canned texts by their labels , cf .", "entities": []}, {"text": "( 1b ) , ( 1a ) .", "entities": []}, {"text": "Note that the textual output of Methodius does n\u2019t contain nonterminal symbols the sort used in Balakrishnan et", "entities": []}, {"text": "al . \u2019s approach .", "entities": []}, {"text": "We use only special terminal symbols , which appear both in content plans ( decorating terminal nodes in the tree ) and in texts ( representing the corresponding chunks of canned texts ) .", "entities": []}, {"text": "3.2 Anonymization and Augmentation We anonymize exhibits by replacing them with entity0 , entity1 , etc in both the content plans and corresponding text .", "entities": []}, {"text": "In each text , there is a single focal exhibit .", "entities": []}, {"text": "The focal exhibit is compared to one or many exhibits and this is expressed in text using singular and plural forms respectively ( e.g. theother vessel , which originates from region1 VSthe other coins , which were created in city0 ) .", "entities": []}, {"text": "We use two substitution forms : entity1 ( for singular ) and entityplural .", "entities": []}, {"text": "Content plans are augmented with relevant information concerning the types of exhibits that occur in a content plan .", "entities": []}, {"text": "The type predicate relates an exhibit to the NP it corresponds to in the text .", "entities": []}, {"text": "This information is encoded within the Methodius logical form and thus is available for the Methodius system when it comes to generating text .", "entities": []}, {"text": "However , since we anonymize exhibits and we ignore the logical forms , we need to explicitly provide the type information of each exhibit .", "entities": []}, {"text": "Methodius sometimes produces content plans in which the \ufb01rst FACT TYPE is missing arg2 .", "entities": []}, {"text": "This missing position corresponds to the focal exhibit in the text .", "entities": []}, {"text": "The modi\ufb01ed corpus regiments the input by ensuring every FACT TYPE includes arg2 .", "entities": []}, {"text": "For every exhibit in the the Methodius content plan not explicitly typed we add a new OPTIONAL TYPE branch to the tree which includes the type of the exhibit .", "entities": []}, {"text": "( 1 ) a.", "entities": []}, {"text": "This is a marriage cauldron and it was created during the classical period in between 420 and 410 B.C.", "entities": []}, {"text": "Like the other vessel you recently saw , this marriage cauldron was decorated with the red \ufb01gure technique .", "entities": []}, {"text": "b.this is a marriage cauldron and it was created during historical - period0 inexhibit0 - creation - time .", "entities": []}, {"text": "like the other vessel you recently saw , this marriage cauldron was decorated with painting - technique0 .", "entities": []}, {"text": "4 Data Since one of our objectives is to compare the performance of neural networks on data with and without", "entities": []}, {"text": "309Data CONTRAST SIMILARITY Neither train 840 2911 553 valid 79 292 51 standard test 166 495 138 challenge test 77 80 80 Table 1 : Distribution of RST types in content plans in train and test data Data Average Words Average Tokens train 52 95 valid 52 96 standard test 40 73 challenge test 29 52 Table 2 : Average numbers of tokens in content plans and average numbers of words in corresponding texts in train and test data rhetorical relations , we call one of datasets RST and the other FACT .", "entities": []}, {"text": "The model names follow the same convention .", "entities": []}, {"text": "The distributions of RST types in content plans is shown in Table 1 , where the \ufb01rst and second numbers correspond to the number of content plans including CONTRAST and SIMI LARITY , respectively , while the third corresponds to the number of content plans which include neither of these RST types .", "entities": []}, {"text": "The average lengths for input ( number of tokens ) and output ( number of words ) are shown in Table 2 .", "entities": []}, {"text": "The output of Methodius is limited with respect to both the homogeneity and lengths of the texts \u2013 Methodius only infrequently produces very short or long texts , e.g. one or six sentences respectively .", "entities": []}, {"text": "One of the test sets , which is described below , is explicitly constructed to determine whether the model \u2019s knowledge of discourse structure is limited by the length of the texts it sees .", "entities": []}, {"text": "4.1 Training Set In the training set , there are around 4300 examples harvested by using the Methodius system .", "entities": []}, {"text": "The higher number of inputs with SIMILARITY ( 2911 ) is due to the Methodius system .", "entities": []}, {"text": "This proportion ofSIMILARITY persists into every split except the challenge test set , where the number of inputs of distinct RST types is more homogeneous .", "entities": []}, {"text": "4.2 Test sets We have two splits of data for our experiments .", "entities": []}, {"text": "One we dub the \u2018 challenge split \u2019 , the other the \u2018 standard split \u2019 .", "entities": []}, {"text": "The major difference between themis their average lengths .", "entities": []}, {"text": "The average length of the challenge split items are roughly half the length of the training set items , while the average length of the standard split is roughly seventy \ufb01ve percent of the training set items .", "entities": []}, {"text": "Standard In the standard split , the average length of items in the training and validation sets is roughly the same ; the distribution of lengths is similar in the training , valid , and test sets but the training set still includes slightly longer sequences on average .", "entities": []}, {"text": "The proportion of items with distinct RST types is roughly the same between the train , valid , and standard test sets .", "entities": []}, {"text": "This test set does n\u2019t identify possible effects of item length on correct discourse structure production .", "entities": []}, {"text": "Challenge The challenge test set consists of items on average half the length of the the average lengths of items in the train and valid sets .", "entities": []}, {"text": "Due to the lower frequency of short items produced by Methodius , the number of items in the challenge set is reduced .", "entities": []}, {"text": "The distribution of items with CONTRAST and SIMILARITY is homogeneous .", "entities": []}, {"text": "With respect to distinguishing RST types , the challenge test set is no more dif\ufb01cult than the standard test set ; the item length is shorter but no less structured .", "entities": []}, {"text": "Moreover , the set of lexemes \u2013 including delexicalized expressions \u2013 which occur in the test set are present in the training set .", "entities": []}, {"text": "However , there are patterns in the test set which are uncommon or unseen in the training set , e.g. one content plan in the challenge set begins with CONTRAST but no such items are found in training .", "entities": []}, {"text": "This distinguishes possible effects of length , e.g. \u2018 RST type X occurs in the third sentence \u2019 , from effect of RST tree structure in the input for correct discourse structure production , i.e. \u2018 RST type X must correspond to lexeme / structure Y \u2019 .", "entities": []}, {"text": "These challenge test - speci\ufb01c content plans help to determine how well a model learns to associate certain strings with either CONTRAST orSIMILAR ITY .", "entities": []}, {"text": "If the model stumbles on shorter texts then its knowledge of RST structure might be ( erroneously ) conditioned on item length .", "entities": []}, {"text": "5 Evaluation Methods 5.1 Metrics on Special Terminals Since the data we generate after preprocessing contains certain expressions which we dub \u2018 special terminals \u2019 , these expressions can be tracked between the target and the hypothesis .", "entities": []}, {"text": "By obtaining metrics based on the correspondence between these special", "entities": []}, {"text": "310terminals , we get a picture how close the hypothesis is to the target .", "entities": []}, {"text": "This measure enjoys some useful properties .", "entities": []}, {"text": "Firstly , it \u2019s cheap \u2014 it is de\ufb01ned solely in terms of expressions which occur both in the input ( content plan ) and in the output ( text ) .", "entities": []}, {"text": "Second , the special terminals stand for important parts of the text \u2014 those ones that are explicitly provided as values to features in the content plan ( since they are terminals ) .", "entities": []}, {"text": "Hence , having information about their presence gives us a good hint of the quality of a text .", "entities": []}, {"text": "In addition to standard evaluation metrics scores such as BLEU4 , we report the following metrics for each test item:2 \u000fRepetitions : A special terminal is present in the hypothesis ntimes but in the target text it occursmtimes , where m < n .", "entities": []}, {"text": "We calculate n\u0000mfor every such special terminal and sum up .", "entities": []}, {"text": "\u000fOmissions : How many times special terminals occurring in the target text are not generated at all in the hypothesis .", "entities": []}, {"text": "\u000fHallucinations : Number of occurrences of those special terminals in the hypothesis that have no occurrence in the target .", "entities": []}, {"text": "5.2 Discourse adverbials for Contrast and Similarity We also provide a count for the number of items in which ( within tables in Appendix A ): ( a ) Target and Hypothesis both contain unlike ; ( b ) Target and Hypothesis both contain like ; ( c ) Target contains unlike but Hypothesis generates like ; ( d ) Target contains likebut Hypothesis generates unlike ( Like vs. Unlike ) ; ( e ) Target contains neither likenorlike and the same holds of Hypothesis ( No rel in both ) ; ( f ) the rest of the cases .", "entities": []}, {"text": "6 Self - training With most NLG applications , large amounts of parallel data are not readily available .", "entities": []}, {"text": "This is true even in the case of Methodius , because there are a \ufb01nite number of exhibits and facts and thus the number of meaningful combinations which can be constructed from them is limited .", "entities": []}, {"text": "In order to reduce annotated data needs , Kedzie and McKeown ( 2019 ) , Qader et al . ( 2019 ) and He et al . ( 2020 ) propose self - training methods for NLG .", "entities": []}, {"text": "Li and White 2The term \u2018 hypothesis \u2019 is used for the output of the model , following the terminology used in Fairseq.(2020 ) explore self - training for the more challenging case of generating from compositional input representations .", "entities": []}, {"text": "Self - training involves the construction of unlabeled data .", "entities": []}, {"text": "The process of self - training is the following .", "entities": []}, {"text": "First , the model is trained on the initial parallel data , i.e. the data used in the models without self - training .", "entities": []}, {"text": "Subsequently , an additional set of unlabeled inputs is provided : such data might exist but be unlabeled but if no such data exists it can be generated ( e.g. , handcrafted using some heuristics ) .", "entities": []}, {"text": "The unlabelled inputs are , in the present context , content plans without corresponding output text .", "entities": []}, {"text": "Next the existing model is used to generate the labels for the unlabeled data .", "entities": []}, {"text": "This procedure results in a new set of parallel data .", "entities": []}, {"text": "Because its labels do n\u2019t come from the data \u2014 since they \u2019re outputs of the model \u2014 this can not be considered parallel data in the full sense .", "entities": []}, {"text": "We dub the resulting data \u2018 pseudo - labelled . \u2019", "entities": []}, {"text": "We train a new model on this data .", "entities": []}, {"text": "Then we reuse the genuine parallel data for \ufb01ne tuning this model .", "entities": []}, {"text": "This process can be repeated to generate various models .", "entities": []}, {"text": "( 1 ) describes the process in brief : Algorithm 1 : Vanilla Self - Training 1Train a model onL ; 2repeat 3 Pseudo - label the unlabeled data in U ; 4 Train a model on the pseudo - parallel data ; 5 Fine - tune the model on L ; 6until convergence or maximum iteration ; 7 Reranking with reverse models In the syntax - semantics interface , the parsing task is usually to build a correct semantic ( or syntactic ) representation of a sentence .", "entities": []}, {"text": "One can consider this task with respect to neural networks \u2014 which operate on sequences \u2014 straightforwardly by reversing the order of the parallel data : the source sequence ( meaning ) becomes the target , and the target sequence ( text ) becomes the source .", "entities": []}, {"text": "Following the terminology of Li and White ( 2020 ) , we call such models reverse models , while models that generate text from meaning representations are forward models.3 3While this is an arbitrary choice of terminology , in the context of NLG it seems to be appropriate to call the forward model the one that generates text out of meaning representation .", "entities": []}, {"text": "311We can rerank the output of a forward model with the help of its corresponding reverse model .", "entities": []}, {"text": "Given several outputs of a beam search of the forward model , we select the one that makes the best meaning representation if it is given to a reverse model as an input .", "entities": []}, {"text": "Here , best means the one that has lowest perplexity with respect to forced decoding .", "entities": [[9, 10, "MetricName", "perplexity"]]}, {"text": "One can combine self - training and reranking : Train forward and reverse models on the parallel data and then train forward and reverse models on the pseudo - parallel data .", "entities": []}, {"text": "Afterwards \ufb01netune them again on the initial parallel data .", "entities": []}, {"text": "Subsequently , use the reverse models to rerank the output of the forward models .", "entities": []}, {"text": "Algorithm 2 : Reverse Model Reranking 1Train forward and reverse models on L ; 2repeat 3 Pseudo - label the unlabeled data in U with reverse model reranking ; 4 Train forward and reverse models on the pseudo - parallel data ; 5 Fine - tune both models on L ; 6until convergence or maximum iteration ; 8 Experiments and Results We ran self - training experiments with two sets of unlabeled data .", "entities": []}, {"text": "One of them consists of the content plans generated by Methodius .", "entities": []}, {"text": "The other one , dubbed \u2018 heuristic , \u2019 is developed from the existing labeled data .", "entities": []}, {"text": "The heuristic data is produced by the following method : for every content plan produced by Methodius , extract the set of subtrees of the content plan which respect some softconstraints on structure .", "entities": []}, {"text": "We avoid extracting trees that start with an optional type .", "entities": []}, {"text": "The subtrees are randomly selected but their distribution is required to closely follow the distribution of distinct RST types in the training data .", "entities": []}, {"text": "Since the size of the Methodius data set is limited , the heuristic data set provides useful cheap supplementary content for training ( compared to the cost of eliciting text corresponding to content plans through e.g. Turkers ) .", "entities": []}, {"text": "We are thus interested whether having genuine Methodius content plans , which are not straightforward to generate in large amounts , could be completed by a heuristic data set generated from the labeled training data set .", "entities": []}, {"text": "The FACT models were trained on the FACT versions of the data set , which is obtained by simplydeleting the RST structure from the RST data set.4 We refer to the models ( for sake of clarity ) by the names in Table 3 .", "entities": []}, {"text": "Model Name Self - training data size Generated by RST - SM 947 Methodius FACT - SM 934 Methodius RST - LG 81970 heuristic FACT - LG 76531 heuristic Table 3 : Models trained on training set of size 4304 There are only 947 content plans for selftraining , while the training set size is 4304 .", "entities": []}, {"text": "The limited number of content plans for self - trainining is due to the homogeneity of the Methodius output , the intention to sync the length of training and test sets , and the \ufb01nite number of exhibits in the Methodius data base .", "entities": []}, {"text": "These content plans , which are harvested from Methodius , are on average just half the length of the content plans in the training set .", "entities": []}, {"text": "Their shortness ensures the system is exposed to items of multiple lengths .", "entities": []}, {"text": "Because of their reduced length and their production by the Methodius system , variation in the content of the short sequences is limited .", "entities": []}, {"text": "The unique unlabelled data size differs between RST and FACT data sets , because the data for FACT is produced by pruning the RST data , the deletion of structure reduces the heterogeneity of data , resulting in fewer unique sequences for the FACT - LG input .", "entities": []}, {"text": "We trained the following models :", "entities": []}, {"text": "\u000fLBL : A standard LSTM seq2seq model with attention on the labeled data , which is also the base model for the other methods .", "entities": [[4, 5, "MethodName", "LSTM"], [5, 6, "MethodName", "seq2seq"]]}, {"text": "\u000fST - VAN : A model trained with vanilla selftraining .", "entities": []}, {"text": "\u000fST - RMR : A model self - trained with reverse model reranking for pseudo - labeling .", "entities": []}, {"text": "Models were trained over several iterations , though for exposition the results reported below concern just the best model iterations.5 BLEU4 is calculated on both the standard and challenge test sets .", "entities": []}, {"text": "BLEU4 , though limited in the conclusions it supports , seems informative enough to allow one to distinguish between RST and FACT 4Deleting RST structure results in the deletion of the tree structure too .", "entities": []}, {"text": "5In addition to LSTM models , we trained a baseline transformer on the labeled data but the results were unsatisfactory .", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "312models ; we report it in Appendix D. BLEU4 is on average 5 or more points higher for RST models than FACT models across the test sets .", "entities": []}, {"text": "8.1 Repetitions , Hallucinations and Omissions We count the sum of repetitions , hallucinations and omissions per test set and report the average per item , simply dividing the sum by the number of test set samples .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "2 and Fig .", "entities": []}, {"text": "3 show the results , chie\ufb02y the uniform improvement of the self - training and reranking models over the baseline LSTM models .", "entities": [[20, 21, "MethodName", "LSTM"]]}, {"text": "RST - SM with self - training is the best model .", "entities": []}, {"text": "RST - SM with both self - training and reverse model reranking produced some of the best results too .", "entities": []}, {"text": "RST - SM and RST - LG show similar performance when it comes to repetitions , hallucinations , and omissions on the standard test set .", "entities": []}, {"text": "RST - SM outperforms RST - LG on the challenge set .", "entities": []}, {"text": "RST models uniformly outperform FACT models .", "entities": []}, {"text": "We observed the models sometimes produced stuttering , i.e. multiple repetition .", "entities": []}, {"text": "Even one of the best models with respect to the standard test set \u2014 RST - SM - ST - V AN ( see Fig . 2)\u2014produced two examples of stuttering ( out of 799 ) with 57 and 59 repetitions respectively .", "entities": []}, {"text": "Just these two outputs nearly doubled the average error rate of RST - SMST - V AN .", "entities": []}, {"text": "The other models reported here did not produce such extreme stuttering .", "entities": []}, {"text": "But despite stuttering , RST - SM - ST - V AN is still the best model with respect to the metrics considered here .", "entities": []}, {"text": "In Appendix C , model performance is reported by simply counting the total number of test examples in which a model generates neither repetitions , nor omissions , nor hallucinations .", "entities": []}, {"text": "The following error from FACT - LG - ST - RMR shows multiple hallucination of the exhibit item \u2019s creation time .", "entities": []}, {"text": "Tthis is an imperial portrait and it portrays roman - emperor0 .", "entities": []}, {"text": "like the coin you recently saw , this imperial portrait was created during historical - period0 .", "entities": []}, {"text": "Hthis is an imperial portrait and it portrays roman - emperor0 .", "entities": []}, {"text": "like the coin , this imperial portrait was created during historical - period0 .", "entities": []}, {"text": "it was created in entity0 - creation - time and it was created in entity0 - creation - time .", "entities": []}, {"text": "Further errors are shown in Appendix E.8.2 Rhetorical Relation Generation When the FACT - LBL model makes mistakes , such mistakes frequently correspond to the substitution of one lexeme marking a rhetorical relation for another marking a distinct ( sometimes opposite ) relation .", "entities": []}, {"text": "The following hypothesis replaces the CONTRAST in the target with a SIMILARITY , misidentifying the origin of some previous exhibit in the chain .", "entities": []}, {"text": "Tunlike the other exhibits you recently saw , which originate from region0 , this coin was originally from city0 .", "entities": []}, {"text": "Hlike the other exhibits you recently saw , this coin originates from city0 .", "entities": []}, {"text": "In the following hypothesis the erroneous substitution of SIMILARITY byCONTRAST leads to an outright contradiction : Tlike the other exhibits you recently saw , this marriage cauldron is currently in museum0 .", "entities": []}, {"text": "Hunlike the other exhibits you recently saw , which are located in museum0 , this marriage cauldron is located in museum0 .", "entities": []}, {"text": "Less frequently the insertion of SIMILARITY or CONTRAST compares the topic of an exhibit to itself :", "entities": []}, {"text": "Tthis is a statue and it was created during historical - period0 in entity0 - creation - time .", "entities": []}, {"text": "Hthis is a statue and it was created during historical - period0 in entity0 - creation - time .", "entities": []}, {"text": "like the statue , this statue was created during historical - period0 .", "entities": []}, {"text": "The details of the number of errors and successes in generating discourse connectives are reported in Appendix A. Fig . 4 and Fig .", "entities": []}, {"text": "5 show Fisher \u2019s Exact Test statistics for best performing RST ( SM and LG ) and FACT ( SM and LG ) models .", "entities": []}, {"text": "8.2.1 Standard Test The best performances are shown by RST - SM and RST - LG .", "entities": []}, {"text": "Even RST - LBL produces only 12 mistakes out of 799 test items .", "entities": []}, {"text": "Production of rhetorical connectives corresponding to CONTRAST and SIMILARITY is uniformly correct .", "entities": []}, {"text": "After \ufb01ne tuning and reranking , the errors reduced to 0 and 2 respectively .", "entities": [[10, 11, "DatasetName", "0"]]}, {"text": "With respect to the FACT models , LBL", "entities": []}, {"text": "313makes mistakes , but improves upon self - training and reranking .", "entities": []}, {"text": "Nonetheless RST models outperform the FACT models .", "entities": []}, {"text": "While the best FACT model performs well with respect to producing the correct discourse connective / structure , this model produces serious content errors that render some outputs ( discussed in Section 8.1 ) incoherent .", "entities": []}, {"text": "8.2.2 Challenge Test On the challenge test , no model achieved perfect accuracy .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "The best performances are by RST - SM and RST - LG .", "entities": []}, {"text": "Their performance is similar .", "entities": []}, {"text": "It is worth noting that in the case of FACT - SM , reranking with self - training gave results comparable to RSTSM ( there is no signi\ufb01cance difference in terms of Fisher \u2019s test with signi\ufb01cance at 5 % ) .", "entities": []}, {"text": "This is not the case for FACT - LG and RST - LG models .", "entities": []}, {"text": "RST - LG - ST - RMR outperforms the best FACT - LG model ( see Fig . 5 ) .", "entities": []}, {"text": "From these experiments , we see that on the standard test set , RST - Large and RST - Small models performed best in terms of producing the correct discourse connective for SIMILARITY ( respectively CONTRAST ) .", "entities": []}, {"text": "While errors occurred \u2014 sometimes matching the results of the corresponding FACT models \u2014 RST models correctly distinguish between producing the lexeme for SIMILARITY versus CONTRAST , while FACT models sometimes confuse SIMILARITY with CONTRAST .", "entities": []}, {"text": "On the challenge data every model made errors .", "entities": []}, {"text": "The RST models outperformed the corresponding FACT models , signi\ufb01cantly in the case of RST - LG over RST - LG , as seen in Fig .", "entities": []}, {"text": "5 .", "entities": []}, {"text": "Though the RST models yielded less dramatic improvements on comparisons in the challenge set , it is worth emphasizing that the RST models produce signi\ufb01cantly fewer repetitions , omissions and hallucination compared to the FACT models ( Figs . 6 and 7 , Appendix C ) , further supporting the conclusion that the RST input produces better output .", "entities": []}, {"text": "This result is interesting , since the content plans in the FACT models are shorter than those in RST models , yet still prompt the former models to produce more words than RST models do .", "entities": []}, {"text": "9 Related and Future Work While traditional natural language generation systems , e.g. Methodius , often employ knowledge graphs , the use of such structure in neural NLG is underdeveloped .", "entities": [[17, 19, "TaskName", "knowledge graphs"]]}, {"text": "An exception in this respect is WebNLG ( Gardent et", "entities": [[6, 7, "DatasetName", "WebNLG"]]}, {"text": "al . , 2017 ) , which is a multilingual corpus for natural language generation .", "entities": []}, {"text": "AnFACT - LBL FACT - SM - ST - RMR FACT - LG - ST - RMRRST - LBL RST - SM - ST - VAN RST - LG - ST - RMR00:511:5Mean Errors per ItemRepetitions Omissions Hallucinations Figure 2 : Standard Set FACT - LBL FACT - SM - ST - RMR FACT - LG - ST - RMRRST - LBL RST - SM - ST - VAN RST - LG - ST - RMR0123Mean Errors per ItemRepetitions Omissions Hallucinations Figure 3 : Challenge Set entry of WebNLG is a set of RDF triples ( representingsubj , predicate , object ) paired with the corresponding text , which is the sequences of sentences which serve as verbalization of those triples .", "entities": [[88, 89, "DatasetName", "WebNLG"]]}, {"text": "But it is noteworthy that the main focus in WebNLG is micro - planning ( sentence - level generation ) .", "entities": [[9, 10, "DatasetName", "WebNLG"]]}, {"text": "Consequently , WebNLG only makes use of a single , implicit rhetorical relation , namely ELABORATION .", "entities": [[2, 3, "DatasetName", "WebNLG"]]}, {"text": "ELABORATION is frequent in the Methodius corpus .", "entities": []}, {"text": "But Methodius uses more interesting rhetorical relations , too , including CONTRAST and SIMILARITY , thus the content ( both in terms of meaning representations and texts ) is signi\ufb01cantly different from WebNLG .", "entities": [[32, 33, "DatasetName", "WebNLG"]]}, {"text": "For future work , there are number of direction we intend to explore , including the following : \u000fStudy whether large - scale pretrained models likewise fail to generalize well without dis-", "entities": []}, {"text": "314FACT - LBL FACT - SM - ST - RMR FACT - LG - ST - RMRRST - LBL RST - SM - ST - VAN RST - LG - ST - VAN02040Total Number of Errors Figure 4 : Standard Set : Errors in generating discourse cue words for SIMILARITY and/or CONTRAST ( unlike and/or like ) , where towards errors counts if either there is an incorrectly generated discourse cue word , or there has been a cue word generated while the target has none , or no cue word is generated but the reference contains one .", "entities": []}, {"text": "The dotted line links two models if there is a signi\ufb01cant difference between their performance in terms of Fisher \u2019s Exact Test statistics ( we take the signi\ufb01cance threshold 5 % ) .", "entities": []}, {"text": "course relations in the input .", "entities": []}, {"text": "\u000fExperiment", "entities": []}, {"text": "with more diverse outputs for Methodius , e.g. crowd - sourcing further outputs to express the content plans .", "entities": []}, {"text": "\u000fStudy whether constrained decoding could be used to reduce discourse structure errors .", "entities": []}, {"text": "10 Conclusions The overall conclusion is that including RST relations in the input content plans is necessary to achieve optimum performance in correctly and coherently expressing discourse relations in the neural reimplementation of Methodius .", "entities": []}, {"text": "This is somewhat surprising since the FACT - only inputs actually have all the information necessary to infer", "entities": []}, {"text": "that aSIMILARITY", "entities": []}, {"text": "orCONTRAST relation should be expressed , but the models nevertheless struggle to learn the desired same / different generalization .", "entities": []}, {"text": "Moreover , the errors are often jarring \u2013 they produce genuine incoherence in the text .", "entities": []}, {"text": "We see the best performance from the RST model with small but clean self - training data ( RSTSM ) , as it comes from Methodius and thus follows the same general patterns as the ones in the test set .", "entities": []}, {"text": "The large RST model ( RST - LG ) had similarFACT - LBL FACT - SM - ST - RMR FACT - LG - ST - RMRRST - LBL RST - SM - ST - VAN RST - LG - ST - RMR051015Total Number of Errors Figure 5 : Challenge Set : Errors in generating discourse cue words for SIMILARITY and/or CONTRAST ( unlike and/or like ) , where an item produces an error if either there is an incorrectly generated discourse cue word , or there has been a cue word generated while the target has none , or no cue word is generated but the reference contains one .", "entities": []}, {"text": "The dotted line links two models if there is a signi\ufb01cant difference between their performance in terms of Fisher \u2019s Exact Test statistics ( with signi\ufb01cance threshold of 5 % ) .", "entities": []}, {"text": "performance to the small one .", "entities": []}, {"text": "FACT models , both small and large , show signi\ufb01cant self - training improvements when reranking with reverse models .", "entities": []}, {"text": "Because the RST baseline already performs relatively well , such an improvement is not observable with them .", "entities": []}, {"text": "RST - SM with vanilla self - training already showed high performance .", "entities": []}, {"text": "In the case of the FACT models , we saw that reranking with reverse models lowers repetitions , omissions and hallucinations in total .", "entities": []}, {"text": "It was also bene\ufb01cial for the RST - LG model .", "entities": []}, {"text": "Despite the highly regular nature of the rulebased texts , even our best models do not get close to zero content errors , highlighting the importance of continued work on eliminating these errors , e.g. using pretrained models ( Kale , 2020 ; Kale and Rastogi , Forthcoming ) or constrained decoding ( Balakrishnan et al . , 2019b ) .", "entities": []}, {"text": "Acknowledgments This research was supported by a collaborative open science research agreement between Facebook and The Ohio State University .", "entities": []}, {"text": "315References Anusha Balakrishnan , Vera Demberg , Chandra Khatri , Abhinav Rastogi , Donia Scott , Marilyn Walker , and Michael White . 2019a .", "entities": []}, {"text": "Proceedings of the 1st workshop on discourse structure in neural nlg .", "entities": []}, {"text": "In Proceedings of the 1st Workshop on Discourse Structure in Neural NLG .", "entities": []}, {"text": "Anusha Balakrishnan , Jinfeng Rao , Kartikeya Upasani , Michael White , and Rajen Subba . 2019b .", "entities": []}, {"text": "Constrained decoding for neural NLG from compositional representations in task - oriented dialogue .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 831 \u2013 844 , Florence , Italy .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ond\u02c7rej Du \u02c7sek , Jekaterina Novikova , and Verena Rieser .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Evaluating the state - of - the - art of end - to - end natural language generation : The e2e nlg challenge .", "entities": [[20, 23, "DatasetName", "e2e nlg challenge"]]}, {"text": "Computer Speech & Language , 59:123\u2013156 .", "entities": []}, {"text": "Claire Gardent , Anastasia Shimorina , Shashi Narayan , and Laura Perez - Beltrachini . 2017 .", "entities": []}, {"text": "Creating training corpora for NLG micro - planners .", "entities": []}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 179\u2013188 , Vancouver , Canada . Association for Computational Linguistics .", "entities": []}, {"text": "Junxian He , Jiatao Gu , Jiajun Shen , and Marc\u2019Aurelio Ranzato . 2020 .", "entities": []}, {"text": "Revisiting self - training for neural sequence generation .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Eduard H. Hovy .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "Automated discourse generation using discourse structure relations .", "entities": []}, {"text": "Arti\ufb01cial Intelligence , 63(1):341 \u2013 385 . Amy Isard .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Choosing the best comparison under the circumstances .", "entities": []}, {"text": "In Proceedings of the International Workshop on Personalization Enhanced Access to Cultural Heritage ( PATCH07 ) , Corfu , Greece . Amy Isard .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "The methodius corpus of rhetorical discourse structures and generated texts .", "entities": []}, {"text": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC 2016 ) , pages 1732\u20131736 , Portoro \u02c7z , Slovenia .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Amy Isard , Jon Oberlander , Colin Matheson , and Ion Androutsopoulos .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Speaking the users \u2019 languages .", "entities": []}, {"text": "Intelligent Systems , IEEE , 18(1):40\u201345 .", "entities": []}, {"text": "Mihir Kale .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Text - to - text pre - training for data - totext tasks .", "entities": []}, {"text": "Mihir Kale and Abhinav Rastogi .", "entities": []}, {"text": "Forthcoming .", "entities": []}, {"text": "Textto - text pre - training for data - to - text tasks .", "entities": []}, {"text": "Chris Kedzie and Kathleen McKeown . 2019 .", "entities": []}, {"text": "A good sample is hard to \ufb01nd : Noise injection sampling and self - training for neural language generation models .", "entities": []}, {"text": "InProceedings of the 12th International Conference on Natural Language Generation , pages 584\u2013593 , Tokyo , Japan .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Diederik P Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xintong Li and Michael White .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Self - training for compositional neural NLG .", "entities": []}, {"text": "The third annual West Coast NLP Summit ( WeCNLP ) , poster session .", "entities": []}, {"text": "William C. Mann and Sandra A. Thompson .", "entities": []}, {"text": "1988 .", "entities": []}, {"text": "Rhetorical structure theory : Toward a functional theory of text organization .", "entities": []}, {"text": "Text & Talk , 8(3):243 \u2013 281 .", "entities": []}, {"text": "Myle Ott , Sergey Edunov , Alexei Baevski , Angela Fan , Sam Gross , Nathan Ng , David Grangier , and Michael Auli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "fairseq :", "entities": []}, {"text": "A fast , extensible toolkit for sequence modeling .", "entities": []}, {"text": "In Proceedings of NAACL - HLT 2019 : Demonstrations .", "entities": []}, {"text": "Rashmi Prasad , Nikhil Dinesh , Alan Lee , Eleni Miltsakaki , Livio Robaldo , Aravind K Joshi , and Bonnie L Webber . 2008 .", "entities": []}, {"text": "The penn discourse treebank 2.0 .", "entities": []}, {"text": "In LREC .", "entities": []}, {"text": "Citeseer .", "entities": [[0, 1, "DatasetName", "Citeseer"]]}, {"text": "Raheel Qader , Franc \u00b8ois Portet , and Cyril Labb \u00b4 e. 2019 .", "entities": []}, {"text": "Semi - supervised neural text generation by joint learning of natural language generation and natural language understanding models .", "entities": [[4, 6, "TaskName", "text generation"], [14, 17, "TaskName", "natural language understanding"]]}, {"text": "In Proceedings of the 12th International Conference on Natural Language Generation , pages 552\u2013562 , Tokyo , Japan .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ehud Reiter and Robert Dale .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "Building natural language generation systems .", "entities": []}, {"text": "Cambridge university press .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Manfred Stede and Carla Umbach .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "Dimlex : A lexicon of discorse markers for text generation and understanding .", "entities": [[8, 10, "TaskName", "text generation"]]}, {"text": "In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics , Volume 2 , pages 1238\u20131242 .", "entities": []}, {"text": "Maite Taboada and William C. Mann . 2006 .", "entities": []}, {"text": "Rhetorical structure theory : looking back and moving ahead .", "entities": []}, {"text": "Discourse Studies , 8(3):423\u2013459 .", "entities": []}, {"text": "Michael White .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Ef\ufb01cient realization of coordinate structures in combinatory categorial grammar .", "entities": []}, {"text": "Research on Language and Computation , 4(1):39 \u2013 75 .", "entities": []}]