[{"text": "Proceedings of the 2020 EMNLP Workshop W - NUT : The Sixth Workshop on Noisy User - generated Text , pages 485\u2013490 Online , Nov 19 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics485SunBear at WNUT-2020 Task 2 : Improving RoBERTa - Based Noisy Text Classi\ufb01cation with Knowledge of the Data domain", "entities": [[6, 9, "DatasetName", "WNUT-2020 Task 2"], [11, 12, "MethodName", "RoBERTa"]]}, {"text": "Linh Bao Doan Sun Asterisk Inc. doan.bao.linh @sun - asterisk.comViet - Anh Nguyen Sun Asterisk Inc. nguyen.viet.anh @sun - asterisk.comQuang", "entities": []}, {"text": "Pham Huu Sun Asterisk Inc. pham.huu.quang @sun - asterisk.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "This paper proposes an improved custom model for WNUT task 2 : Identi\ufb01cation of Informative COVID-19 English Tweet .", "entities": []}, {"text": "We improve experiment with the effectiveness of \ufb01ne - tuning methodologies for state - of - the - art language model RoBERTa ( Liu et al . , 2019 ) .", "entities": [[21, 22, "MethodName", "RoBERTa"]]}, {"text": "We make a preliminary instantiation of this formal model for the text classi\ufb01cation approaches .", "entities": []}, {"text": "With appropriate training techniques , our model is able to achieve 0.9218 F1score on public validation set and the ensemble version settles at top 9 F1 - score ( 0.9005 ) and top 2 Recall ( 0.9301 ) on private test set .", "entities": [[25, 28, "MetricName", "F1 - score"], [34, 35, "MetricName", "Recall"]]}, {"text": "1 Introduction Since the outbreak of COVID-19 pandemic , frequently updated information becomes a huge problem of concern .", "entities": []}, {"text": "Social media platforms consequently become real - time sources for news about \ufb02are - up data .", "entities": []}, {"text": "In any case , the \ufb02are - up has been spreading quickly , we observe a monstrous amount of information on social networks , for example around 4 million COVID-19 English Tweets every day on Twitter , in which most of these Tweets are uninformative .", "entities": []}, {"text": "Therefore , it is crucial to collect the informative ones ( for example Corona Virus Tweets identi\ufb01ed with new cases or dubious cases ) for downstream applications .", "entities": []}, {"text": "In any case , manual ways to deal with recognizing useful Tweets require critical human endeavors , and hence are expensive .", "entities": []}, {"text": "Based on the dataset provided in WNUT-2020 Task 2 : Identi\ufb01cation of informative COVID-19 English Tweets ( Nguyen et al . , 2020 ) , we propose a \ufb01ne - tuning strategy to adopt the universal language model RoBERTa as an backbone model for text classi\ufb01cation purposes .", "entities": [[6, 9, "DatasetName", "WNUT-2020 Task 2"], [38, 39, "MethodName", "RoBERTa"]]}, {"text": "We also conduct several experiments in varied \ufb01ne - tuning architectures on the pre - trained RoBERTa .", "entities": [[16, 17, "MethodName", "RoBERTa"]]}, {"text": "Our best model results in a high F1 - score of 0.9005 on the task \u2019s private testdataset and that of 0.9218 on the public validation set with Multilayer Perceptron Head .", "entities": [[7, 10, "MetricName", "F1 - score"]]}, {"text": "2 Related work One of the most important parts in text classi\ufb01cation problems is input representation .", "entities": []}, {"text": "Traditional methods construct context - independent embeddings for words .", "entities": []}, {"text": "Mikolov et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( Mikolov et al . , 2013 ) introduce an open - source Word2Vec , which consists of two models : Continuous Bag of Words ( CBOW ) and Skip - gram model .", "entities": []}, {"text": "The models were trained on 1.6 billion words to learn linguistic contexts of words .", "entities": []}, {"text": "While Word2Vec is a selfsupervised algorithm , GloVe ( Pennington et al . , 2014 ) is trained unsupervised to form word embeddings .", "entities": [[7, 8, "MethodName", "GloVe"], [21, 23, "TaskName", "word embeddings"]]}, {"text": "GloVe factorizes co - occurrence matrix of words , resulting in dense word vectors .", "entities": [[0, 1, "MethodName", "GloVe"]]}, {"text": "However , both GloVe and Word2Vec fail representing rare or out - of - vocabulary words .", "entities": [[3, 4, "MethodName", "GloVe"]]}, {"text": "FastText ( Mikolov et al . , 2018 ) mitigates this problem by decomposing words as a sum of character n - grams .", "entities": [[0, 1, "MethodName", "FastText"]]}, {"text": "This handles unseen words very well because these character n - grams may still occur in other words .", "entities": []}, {"text": "In contrast to context - independent embeddings , modern language models encode word semantics within contexts .", "entities": []}, {"text": "Word vectors obtained from these methods achieve better results on downstream tasks because a word in different contexts expresses different meanings .", "entities": []}, {"text": "Bidirectional Encoder Representations from Transformers ( Devlin et al . , 2018 ) , or BERT for short , outperforms the previous best result with GLUE score of 80.4 % , which is 7.6 % improvement .", "entities": [[15, 16, "MethodName", "BERT"], [25, 26, "DatasetName", "GLUE"]]}, {"text": "There are two variants of BERT : base and large ; the large model is a stack of 24 Transformers \u2019 encoders for a total of 340 M parameters while the base one has only 12 encoders .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "GPT-2 ( Radford et al . , 2019 ) by OpenAI is a gigantic model with 1.5 billion parameters and 48 layers , setting new state - of - the - art results on 7 out of 8 datasets .", "entities": [[0, 1, "MethodName", "GPT-2"]]}, {"text": "Face-", "entities": []}, {"text": "486 Figure 1 : Our overall pipeline for hierarchical MLM tuning and main task training .", "entities": [[9, 10, "DatasetName", "MLM"]]}, {"text": "book Research team improves training procedures for BERT , introducing RoBERTa ( Liu et al . , 2019 ) .", "entities": [[7, 8, "MethodName", "BERT"], [10, 11, "MethodName", "RoBERTa"]]}, {"text": "The improvements include extended training time on a ten - times bigger dataset , increased batch size , using byte - level encoding with larger vocabulary , excluding next sentence predicting task , and dynamic masking pattern modifying .", "entities": [[15, 17, "HyperparameterName", "batch size"]]}, {"text": "3 Proposed method Figure 1 illustrates our process .", "entities": []}, {"text": "For MLM tuning we propose hierarchical tuning process that consists of two steps : Domain adaptation using extra COVID data and Task adaptation using the given training data .", "entities": [[1, 2, "DatasetName", "MLM"], [14, 16, "TaskName", "Domain adaptation"]]}, {"text": "After MLM Tuning , we utilize different training techniques for text classi\ufb01cation such as back translation , warm - up learning rate , layer freezing and layer - wise learning rates .", "entities": [[1, 2, "DatasetName", "MLM"], [20, 22, "HyperparameterName", "learning rate"]]}, {"text": "This section provides details of this pipeline .", "entities": []}, {"text": "3.1 RoBERTa network for Text Classi\ufb01cation Task Taking advantage of RoBERTa as a backbone , we propose a customized network with appreciably modi\ufb01cations .", "entities": [[1, 2, "MethodName", "RoBERTa"], [10, 11, "MethodName", "RoBERTa"]]}, {"text": "Figure 2 illustrates our proposed architecture .", "entities": []}, {"text": "The \u201c base \u201d version of RoBERTa is used .", "entities": [[6, 7, "MethodName", "RoBERTa"]]}, {"text": "It has 12 Transformer blocks , each block outputs a 768 - D vector for each token .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "Since the output of different Transformer blocks represent different semantic levels for the inputs , in our experiments we combine outputs of those Transformer blocks by concatenation .", "entities": [[5, 6, "MethodName", "Transformer"], [23, 24, "MethodName", "Transformer"]]}, {"text": "This combination is fed to a classi\ufb01cation head .", "entities": []}, {"text": "We propose two types of the head : \u000fMLP Head : A simple feed forward network with one hidden layer .", "entities": []}, {"text": "This head takes the last token embedding as its input .", "entities": []}, {"text": "\u000fBiLSTM Head : A recurrent neural network with one Bidirectional LSTM layer .", "entities": [[9, 11, "MethodName", "Bidirectional LSTM"]]}, {"text": "This network takes embeddings of all tokens .", "entities": []}, {"text": "The hyperparameters are shown in Section 4.3.2 Fine - tuning Masked Language Model ( MLM ) 3.2.1 Direct tuning on task data RoBERTa apparently is an excellent language model since it was trained on a huge dataset in a broad domain .", "entities": [[14, 15, "DatasetName", "MLM"], [22, 23, "MethodName", "RoBERTa"]]}, {"text": "However , the general domain is also a drawback when it comes to downstream tasks with completely different domains such as classifying users \u2019 tweets on Twitter .", "entities": []}, {"text": "Therefore , in order to produce high - quality outputs from the model , there is a need of \ufb01ne - tuning MLM task on the task dataset for RoBERTa .", "entities": [[22, 23, "DatasetName", "MLM"], [29, 30, "MethodName", "RoBERTa"]]}, {"text": "This adapts the universal language model into our narrow domain , giving it prior knowledge for later classi\ufb01cation training .", "entities": []}, {"text": "Choosing learning rate is the key factor for the convergence .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "If learning rate is too small , the model may converge too slow causing harder to \ufb01t to new data distribution .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "On the other hand , large learning rate can lead to the problem of useful feature forgetting .", "entities": [[6, 8, "HyperparameterName", "learning rate"]]}, {"text": "Hence , we employ warm - up learning rate scheduler ( Howard and Ruder , 2018 ) to help the model converge faster while preserving its good initialization .", "entities": [[7, 9, "HyperparameterName", "learning rate"]]}, {"text": "3.2.2 Hierarchical tuning with extra data We assume \ufb01ne - tuning only on the dataset might cause over\ufb01tting on the chosen dataset only .", "entities": []}, {"text": "Hence , we propose a hierarchical \ufb01ne - tuning strategy for RoBERTa : the \ufb01rst phase we train with custom domain COVID Tweets dataset for domain adaptation , then the second phase is a \ufb01ne - tuning process with WNUT Task 2 dataset for task adaptation .", "entities": [[11, 12, "MethodName", "RoBERTa"], [25, 27, "TaskName", "domain adaptation"]]}, {"text": "Our custom COVID Tweets dataset is gathered from Twitter platform , including unlabeled 1 million posts in general COVID domain , which has the hashtag of # Covid , # Covid19 , and # Coronavirus .", "entities": []}, {"text": "We expect this model to generalize better on different distributed dataset in the same \ufb01eld of COVID Tweets .", "entities": []}, {"text": "487 Figure 2 : The architecture of the proposed model .", "entities": []}, {"text": "The input is tokenized into a sequence of BPE tokens .", "entities": [[8, 9, "MethodName", "BPE"]]}, {"text": "RoBERTa , the \u201c base \u201d version , takes this sequence and propagates it through 12 Transformer layers .", "entities": [[0, 1, "MethodName", "RoBERTa"], [16, 17, "MethodName", "Transformer"]]}, {"text": "By concatenating outputs from these 12 layers , we form a long sentence representation for the follow - up classi\ufb01cation head , which is a simple Multi - layer Perceptron / Long Short - Term Memory network .", "entities": [[31, 36, "MethodName", "Long Short - Term Memory"]]}, {"text": "3.3 Text classi\ufb01cation training 3.3.1 Back Translation Recently research ( Xie et al . , 2019 ; Edunov et al . , 2018 ) have shown that back - translating monolingual data can be used as a potential form of data augmentation in Text Classi\ufb01cation .", "entities": [[6, 7, "TaskName", "Translation"], [40, 42, "TaskName", "data augmentation"]]}, {"text": "The idea behind back translation is to translate a sentence from the original language ( English ) to another selected language and then translate back to the original language .", "entities": []}, {"text": "This utilizes the power of current welldeveloped translation engines .", "entities": []}, {"text": "In our experiment , 25 % of the data samples is back - translated into Vietnamese , the same amount goes for Italian and French , and the rest 25 % is kept unchanged .", "entities": []}, {"text": "This assures the languages contribute equally to the overall dataset .", "entities": []}, {"text": "Totally , the dataset size is increased by 75 % .", "entities": []}, {"text": "3.3.2 Model freezing with layer - wise learning rates Layer freezing helps preserving useful knowledge that a pre - trained neural network has learned .", "entities": []}, {"text": "Since RoBERTa has been trained on a huge dataset , we would not want the model to derive too far from its pre - train weights .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}, {"text": "The training procedure is divided into 2 steps : \u000fStep 1 : We freeze RoBERTa to train the classi\ufb01cation head for the \ufb01rst epoch .", "entities": [[14, 15, "MethodName", "RoBERTa"]]}, {"text": "Warm - up learning rate ( Section 3.2.1 ) is also applied .", "entities": [[3, 5, "HyperparameterName", "learning rate"]]}, {"text": "Because RoBERTa \u2019s weights are already well trained , this step helps escape from narrow local optimum .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}, {"text": "\u000fStep 2 : RoBERTa is unfrozen , a whole network is trained .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}, {"text": "In RoBERTa , upper layers produce embeddings with more contextspeci\ufb01c than lower layers .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}, {"text": "This motivates us to further apply layer - wise learning rate : set a small learning rate for the shallowest layer , increase the learning rate as the layer goes deeper .", "entities": [[9, 11, "HyperparameterName", "learning rate"], [15, 17, "HyperparameterName", "learning rate"], [24, 26, "HyperparameterName", "learning rate"]]}, {"text": "3.3.3 Label Smoothing When training a huge neural network on a relatively small dataset , overcon\ufb01dence is a problem leading to bad behaviours of the model .", "entities": [[1, 3, "MethodName", "Label Smoothing"]]}, {"text": "This phenomenon occurs when the model gives predictions with con\ufb01dence higher than its accuracy .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "While there have been a lot of studies for over\ufb01tting reduction , overcon\ufb01dence problem attracts less attention from researchers .", "entities": []}, {"text": "In this study , we employ label smoothing ( Szegedy et al . , 2015 ) to prevent model from being too certain about its predictions .", "entities": [[6, 8, "MethodName", "label smoothing"]]}, {"text": "Instead of assigning \u201c hard \u201d one - hot encoded ground truth , label smoothing adds a small perturbation into the label by a smoothing parameter \u000b .", "entities": [[13, 15, "MethodName", "label smoothing"]]}, {"text": "488Model Precision Recall F1 Accuracy RoBERTa + MLP Head 0.9407 0.8740 0.9061 0.9080 RoBERTa + BiLSTM Head 0.9322 0.8853 0.9082 0.9110 Direct tuning +", "entities": [[1, 2, "MetricName", "Precision"], [2, 3, "MetricName", "Recall"], [3, 4, "MetricName", "F1"], [4, 5, "MetricName", "Accuracy"], [5, 6, "MethodName", "RoBERTa"], [7, 8, "DatasetName", "MLP"], [13, 14, "MethodName", "RoBERTa"], [15, 16, "MethodName", "BiLSTM"]]}, {"text": "MLP Head + Label smoothing 0.9492 0.8960 0.9218 0.9240 Direct tuning + BiLSTM Head + Label smoothing 0.9364 0.8983 0.9170 0.9200 Direct tuning + MLP Head + Back translation + Label smoothing0.9343 0.8909 0.9121 0.9150 Hierarchical tuning +", "entities": [[0, 1, "DatasetName", "MLP"], [3, 5, "MethodName", "Label smoothing"], [12, 13, "MethodName", "BiLSTM"], [15, 17, "MethodName", "Label smoothing"], [24, 25, "DatasetName", "MLP"]]}, {"text": "MLP Head + Back translation + Label smoothing0.9449 0.8745 0.9084 0.9100 Table 1 : Comparison of different tuning and training techniques on the public validation set .", "entities": [[0, 1, "DatasetName", "MLP"]]}, {"text": "y0 k = yk(1\u0000 \u000b )", "entities": [[1, 3, "HyperparameterName", "k ="]]}, {"text": "+ \u000b = K , whereykis output probabilities of Kclasses .", "entities": []}, {"text": "Moreover , label smoothing also helps stabilize the training process .", "entities": [[2, 4, "MethodName", "label smoothing"]]}, {"text": "When using cross - entropy loss , one - hot encoded labels cause numerical instabilities if the prediction is close to one - hot form .", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "In that case , the loss will become 1 log 0", "entities": [[5, 6, "MetricName", "loss"], [10, 11, "DatasetName", "0"]]}, {"text": "= \u00001 .", "entities": []}, {"text": "By setting \u000b 6= 0 , this problem can be solved .", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "4 Experiments and Results 4.1 Experiment setup Our set - up is proceeded as following instruction .", "entities": []}, {"text": "We trained our networks with PyTorch framework on GPU GeForce GTX 2080Ti with batch size 32 for 20 epochs .", "entities": [[13, 15, "HyperparameterName", "batch size"]]}, {"text": "We used AdamW ( Loshchilov and Hutter , 2017 ) for the optimization and a learning rate of 3e\u00005 , decayed 0.01 except for LayerNorm layers .", "entities": [[2, 3, "MethodName", "AdamW"], [15, 17, "HyperparameterName", "learning rate"]]}, {"text": "Label smoothing hyperparameter \u000b was empirically experimented with multiple values of 0 , 0.1 , 0.15 , 0.2 and the last value possessed promising results .", "entities": [[0, 2, "MethodName", "Label smoothing"], [11, 12, "DatasetName", "0"]]}, {"text": "The numbers of hidden units of MLP Head and BiLSTM Head to 768 and 256 respectively .", "entities": [[6, 7, "DatasetName", "MLP"], [9, 10, "MethodName", "BiLSTM"]]}, {"text": "4.2 Evaluation metrics Evaluation metrics for assessing are Accuracy , F1score , Recall and Precision metrics on public validation set .", "entities": [[8, 9, "MetricName", "Accuracy"], [12, 13, "MetricName", "Recall"], [14, 15, "MetricName", "Precision"]]}, {"text": "Accuracy can be used when the class distribution is similar while F1 - score is a better choice of metric when there are imbalanced classes .", "entities": [[0, 1, "MetricName", "Accuracy"], [11, 14, "MetricName", "F1 - score"]]}, {"text": "precision = TP TP+FP recall = TP TP+FNF1=2 precision\u00001+recall\u00001 , whereTP : True Positive , FP : False Positive , FN : False Negative 4.3 Results Table 1 compares the performance of multiple trial architectures training with pre - trained method using RoBERTa in our base settings .", "entities": [[42, 43, "MethodName", "RoBERTa"]]}, {"text": "The original RoBERTa with MLP Head shows the better result than LSTM head , but the difference is not really noticeable ( 0.9082 vs. 0.9061 ) .", "entities": [[2, 3, "MethodName", "RoBERTa"], [4, 5, "DatasetName", "MLP"], [11, 12, "MethodName", "LSTM"]]}, {"text": "When applying direct tuning MLM and label smoothing , the gap has been widened , speci\ufb01cally , 0.9218 for MLP Head and 0.9170 for LSTM Head .", "entities": [[4, 5, "DatasetName", "MLM"], [6, 8, "MethodName", "label smoothing"], [19, 20, "DatasetName", "MLP"], [24, 25, "MethodName", "LSTM"]]}, {"text": "Figure 3 : Confusion matrix on the public validation set In the table , hierarchical tuning and back translation method did not yield better results than direct tuning without back translation one .", "entities": []}, {"text": "Nevertheless , we expect this method can generalize well on many different distributed datasets and thus , we ensembled the two versions with voting and submitted to the private test benchmark .", "entities": []}, {"text": "We ended up at top 9 on the leaderboard with 0.9005 F1 - score and", "entities": [[11, 14, "MetricName", "F1 - score"]]}, {"text": "489Table 2 : Some failures of our system .", "entities": []}, {"text": "Text Model prediction Truth label Some people metaphorically shake their walking sticks at the TV like Grandpa Simpson & amp ; rage \u201d \ufb02u has already killed thousands in USA \u201d , \u201d but guns have already killed over 6,000 in USA this year \u201d - all true .", "entities": []}, {"text": "But Coronavirus is In Addition to those deaths .", "entities": []}, {"text": "HTTPURLINFORMATIVE UNINFORMATIVE 2/26 PCR test 2/27 Negative result 2/28 X - ray shows n.p . Discharge .", "entities": []}, {"text": "Stay near Haneda airport 2/29 Akita airport Return 3/6 Follow up : Visit B in Akita .", "entities": [[7, 8, "MetricName", "Return"]]}, {"text": "Fever & amp ; cough - B consults with designated outpatient service ( C ) And PCR + covid19", "entities": []}, {"text": "HTTPURLINFORMATIVE UNINFORMATIVE Amazon and Facebook ask Seattle employees to work from home after coronavirus cases HTTPURLUNINFORMATIVE INFORMATIVE Third of Sacramento coronavirus cases linked to church events - Los Angeles Times .", "entities": []}, {"text": "Pathetic !", "entities": []}, {"text": "HTTPURLUNINFORMATIVE INFORMATIVE 0.9301 Recall , in which our Recall score reached the second place .", "entities": [[3, 4, "MetricName", "Recall"], [8, 9, "MetricName", "Recall"]]}, {"text": "4.4 Error Analysis A question that as important as designing a subtle method is \u201c what make the model fail \u201d .", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "By answering this question we can gain an insight into our model performance and further improve it .", "entities": []}, {"text": "The method used for analyzing is the bottom row in Table 1 .", "entities": []}, {"text": "Firstly , we plot confusion matrix ( Figure 3 ) , observe that both True Positive and True Negative are evenly distributed with a small proportion of False Negative and False Positive , indicating our model did not bias towards any classes .", "entities": []}, {"text": "Secondly , we randomly sample some failures the model made ( Table 2 ) .", "entities": []}, {"text": "It seems like sentences containing more numbers are usually ( mis)classi\ufb01ed as INFORMATIVE while the ones containing less numbers are classi\ufb01ed as UNINFORMATIVE .", "entities": []}, {"text": "This can be explained that INFORMATIVE tweets provide information about recovered , suspected , con\ufb01rmed and death cases .", "entities": []}, {"text": "Therefore , numbers appearance is inevitable .", "entities": []}, {"text": "5 Conclusion In this paper , we have explored and proposed our pipeline to solve the Identi\ufb01cation of Informative COVID-19 English Tweet task by using a pretrained universal language model .", "entities": []}, {"text": "By conductinga lot of experiments , we have demonstrated that the use of RoBERTa and our \ufb01ne - tuning strategy is highly effective in text classi\ufb01cation tasks .", "entities": [[13, 14, "MethodName", "RoBERTa"]]}, {"text": "With our proposed methods , we have achieved prominent results on the WNUT Task 2 .", "entities": []}, {"text": "For future work , we will design more complex classi\ufb01cation head architectures to improve model \u2019s performance as well as solving problems indicated in Section 4.4 .", "entities": []}, {"text": "Furthermore , we would like to employ our model and pipeline in different languages such as Vietnamese to see how they adapt to new languages .", "entities": []}, {"text": "References Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "Sergey Edunov , Myle Ott , Michael Auli , and David Grangier . 2018 .", "entities": []}, {"text": "Understanding back - translation at scale .", "entities": []}, {"text": "Jeremy Howard and Sebastian Ruder .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Universal language model \ufb01ne - tuning for text classi\ufb01cation .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "490Ilya Loshchilov and Frank Hutter . 2017 .", "entities": []}, {"text": "Decoupled weight decay regularization .", "entities": [[1, 3, "MethodName", "weight decay"]]}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "Tomas Mikolov , Edouard Grave , Piotr Bojanowski , Christian Puhrsch , and Armand Joulin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Advances in pre - training distributed word representations .", "entities": []}, {"text": "In Proceedings of the International Conference on Language Resources and Evaluation ( LREC 2018 ) .", "entities": []}, {"text": "Dat Quoc Nguyen , Thanh Vu , Afshin Rahimi , Mai Hoang Dao , Linh The Nguyen , and Long Doan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "WNUT-2020 Task 2 : Identi\ufb01cation of Informative COVID-19 English Tweets .", "entities": [[0, 3, "DatasetName", "WNUT-2020 Task 2"]]}, {"text": "In Proceedings of the 6th Workshop on Noisy User - generated Text .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "GloVe : Global vectors for word representation .", "entities": [[0, 1, "MethodName", "GloVe"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 , Doha , Qatar .", "entities": []}, {"text": "Association for Computational Linguistics . A. Radford , Jeffrey Wu , R. Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Christian Szegedy , Vincent Vanhoucke , Sergey Ioffe , Jonathon Shlens , and Zbigniew Wojna . 2015 .", "entities": []}, {"text": "Rethinking the inception architecture for computer vision .", "entities": []}, {"text": "Qizhe Xie , Zihang Dai , Eduard Hovy , Minh - Thang Luong , and Quoc V .", "entities": []}, {"text": "Le . 2019 .", "entities": []}, {"text": "Unsupervised data augmentation for consistency training .", "entities": [[1, 3, "TaskName", "data augmentation"]]}]