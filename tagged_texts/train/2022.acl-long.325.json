[{"text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 4738 - 4752 May 22 - 27 , 2022 c", "entities": []}, {"text": "2022 Association for Computational Linguistics Large Scale Substitution - based Word Sense Induction Matan Eyal1Shoval Sadde1Hillel Taub - Tabib1Yoav Goldberg1,2 1Allen Institute for AI , Israel 2Bar Ilan University , Ramat - Gan , Israel matane,shovals,hillelt,yoavg@allenai.org Abstract We present a word - sense induction method based on pre - trained masked language models ( MLMs ) , which can cheaply scale to large vocabularies and large corpora .", "entities": [[10, 13, "TaskName", "Word Sense Induction"]]}, {"text": "The result is a corpus which is sense - tagged according to a corpus - derived sense inventory and where each sense is associated with indicative words .", "entities": []}, {"text": "Evaluation on English Wikipedia that was sense - tagged using our method shows that both the induced senses , and the per - instance sense assignment , are of high quality even compared to WSD methods , such as Babelfy .", "entities": []}, {"text": "Furthermore , by training a static word embeddings algorithm on the sense - tagged corpus , we obtain high - quality static senseful embeddings .", "entities": [[6, 8, "TaskName", "word embeddings"]]}, {"text": "These outperform existing senseful embeddings methods on the WiC dataset and on a new outlier detection dataset we developed .", "entities": [[8, 9, "DatasetName", "WiC"], [14, 16, "TaskName", "outlier detection"]]}, {"text": "The data driven nature of the algorithm allows to induce corpora - speci\ufb01c senses , which may not appear in standard sense inventories , as we demonstrate using a case study on the scienti\ufb01c domain .", "entities": []}, {"text": "1 Introduction Word forms are ambiguous , and derive meaning from the context in which they appear .", "entities": []}, {"text": "For example , the form \u201c bass \u201d can refer to a musical instrument , a low - frequency sound , a type of voice , or a kind of \ufb01sh .", "entities": []}, {"text": "The correct reference is determined by the surrounding linguistic context .", "entities": []}, {"text": "Traditionally , this kind of ambiguity was dealt via word sense disambiguation ( WSD ) , a task that disambiguates word forms in context between symbolic sense - ids from a sense inventory such as WordNet ( Miller , 1992 ) or , more recently , BabelNet ( Navigli and Ponzetto , 2010 ) .", "entities": [[9, 12, "TaskName", "word sense disambiguation"]]}, {"text": "Such sense inventories rely heavily on manual curation , are labor intensive to produce , are not available in specialized domains and inherently unsuitable for words with emerging senses.1This 1For example , in current WordNet version , Corona has 6 synsets , none of them relates to the novel Coronavirus .can be remedied by word sense induction ( WSI ) , a task where the input is a given word - type and a corpus , and the output is a derived sense inventory for that word .", "entities": [[54, 57, "TaskName", "word sense induction"]]}, {"text": "Then , sense disambiguation can be performed over the WSI - derived senses .", "entities": []}, {"text": "The introduction of large - scale pre - trained LMs and Masked LMs ( MLM ) seemingly made WSI / WSD tasks obsolete : instead of representing tokens with symbols that encode sense information , each token is associated with a contextualized vector embeddings that captures various aspects of its in - context semantics , including the word - sense .", "entities": [[14, 15, "DatasetName", "MLM"]]}, {"text": "These contextualized vectors proved to be very effective as features for downstream NLP tasks .", "entities": []}, {"text": "However , contextualized embeddings also have some major shortcomings : most notably for our case , they are expensive to store ( e.g. BERT embeddings are 768 or 1024 \ufb02oating point numbers for each token ) , and are hard to index and query at scale .", "entities": [[23, 24, "MethodName", "BERT"]]}, {"text": "Even if we do manage to store and query them , they are not interpretable , making it impossible for a user to query for a particular sense of a word without providing a full disambiguating context for that word .", "entities": []}, {"text": "For example , consider a user wishing to query a dataset for sentences discussing Oracle in the mythology - prophet sense , rather than the tech company sense .", "entities": []}, {"text": "It is not clear how to formulate such a query to an index of contextualized word vectors .", "entities": []}, {"text": "However , it is trivial to do for an index that annotates each token with its derived sense - id ( in terms of UI , after a user issues a query such as \u201c Oracle \u201d , the system may show a prompt such as \u201c did you mean Oracle related to IBM ; Sun ; Microsoft , or to Prophet ; Temple ; Queen \u201d , allowing to narrow the search in the right direction ) .", "entities": []}, {"text": "Amrami and Goldberg ( 2018 , 2019 ) show how contextualized embeddings can be used for achieving state - of - the - art WSI results .", "entities": []}, {"text": "The core idea of their WSI algorithm is based on the intuition , \ufb01rst proposed by Ba\u00b8 skaya et al .", "entities": []}, {"text": "( 2013 ) , that occurrences of a word that share a sense , also share in - context4738", "entities": []}, {"text": "bug Representatives Neighbours bug0 bug1 bug2 bug3 bug4 bug0 bug1 bug2 bug3 bug4 insect problem feature bomb virus bugs 0 vulnerability 2bugs 1 bugs 3 \ufb02u2 \ufb02y \ufb02aws \ufb01x device infection beetle 0glitch patches 2dumpster staph beetle hole code bite crisis spider 0rootkit bug 1 laptop 1 hangover Bugs patch dog screen disease snake 1 bugs 1 updates 1footage 1 nosebleed worm mistake software tag surprise worm 0 virus 2 patch 2 cruiser 3 pain 4 Java chair Representatives Neighbours Representatives Neighbours Java 0", "entities": [[19, 20, "DatasetName", "0"], [41, 42, "DatasetName", "spider"], [67, 68, "DatasetName", "0"], [83, 84, "DatasetName", "0"]]}, {"text": "Java 1 Java 0", "entities": [[3, 4, "DatasetName", "0"]]}, {"text": "Java 1 chair 0 chair 1 chair 0 chair 1 Jakarta Eclipse Timor 0", "entities": [[3, 4, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [13, 14, "DatasetName", "0"]]}, {"text": "Python 0 head seat Chair 0 stool 0", "entities": [[1, 2, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [7, 8, "DatasetName", "0"]]}, {"text": "Indonesia Jo Sumatra 1 JavaScript chairman position chairperson podium 2 Bali Apache Sulawesi Pascal 2 president wheelchair chairman 0 desk 0 Indies software Sumatra 0 SQL presided professor president 0 professorship Holland Ruby Kalimantan library 3 lead table Chairman 0 throne 1 pound train Representatives Neighbours Representatives Neighbours pound 0 pound 1pound 2pound 0 pound 1pound 2 train 0 train 1 train 0 train 1 lb dollar beat lb0 rupee smash 2 training railway recruit 0", "entities": [[18, 19, "DatasetName", "0"], [20, 21, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [29, 30, "DatasetName", "0"], [39, 40, "DatasetName", "0"], [49, 50, "DatasetName", "0"], [53, 54, "DatasetName", "0"], [58, 59, "DatasetName", "0"], [62, 63, "DatasetName", "0"], [75, 76, "DatasetName", "0"]]}, {"text": "bus0 foot marks punch pounds 0shilling kick 1 prepare track equip tram 1 weight coin pump lbs0 dollar 1 stomp educate rail recruit 1 trains 1 ton Mark crush ton2 franc slash 0 practice line volunteer 2carriage 0 kilograms mile attack lbs1 penny 0 throw 4 quali\ufb01ed railroad retrain coach 3 Figure 1 : Examples of induced word - senses for various words .", "entities": [[32, 33, "DatasetName", "0"], [37, 38, "DatasetName", "0"], [43, 44, "DatasetName", "0"]]}, {"text": "For each sense we list the top-5 representatives , as well as the 5 closest neighbours in the static embeddings space .", "entities": []}, {"text": "substitutes .", "entities": []}, {"text": "An MLM is then used to derive top- k word substitutes for each word , and these substitutevectors are clustered to derive word senses .", "entities": [[1, 2, "DatasetName", "MLM"]]}, {"text": "Our main contribution in this work is proposing a method that scales up Amrami and Goldberg ( 2018 ) \u2019s work to ef\ufb01ciently annotate all tokens in a large corpus ( e.g. Wikipedia ) with automatically derived word - senses .", "entities": []}, {"text": "This combines the high - accuracy of the MLM - based approach , with the symbolic representation provided by discrete sense annotations .", "entities": [[5, 6, "MetricName", "accuracy"], [8, 9, "DatasetName", "MLM"]]}, {"text": "The discrete annotations are interpretable ( each sense is represented as a set of words ) , editable , indexable and searchable using standard IR techniques .", "entities": []}, {"text": "We show two applications of the discrete annotations , the \ufb01rst one is senseaware information retrieval ( \u00a7 7 ) , and the second is high - quality senseful static word embeddings we can derive by training a static embeddings model on the large sense annotated corpus ( \u00a7 8) .", "entities": [[14, 16, "TaskName", "information retrieval"], [30, 32, "TaskName", "word embeddings"]]}, {"text": "We \ufb01rst show how the method proposed by Amrami and Goldberg ( 2018 ) can be adapted from deriving senses of individual lemmas to ef\ufb01ciently and cheaply annotating all the corpus occurrences ofall the words in a large vocabulary ( \u00a7 3 ) .", "entities": []}, {"text": "Deriving word - sense clusters for all of English Wikipedia words that appear as single - token words in BERTLARGE \u2019s", "entities": []}, {"text": "(", "entities": []}, {"text": "Devlin et al . , 2019 ) vocabulary , and assigning a sense to each occurrence in the corpus , required 100 hours of cheap P100 GPUs ( 5 hoursof wall - clock time on 20 single GPU machines ) followed by roughly 4 hours on a single 96 - cores CPU machines .", "entities": []}, {"text": "The whole process requires less than 50 GB of disk space , and costs less than 150 $ on Google Cloud platform .", "entities": [[19, 20, "DatasetName", "Google"]]}, {"text": "After describing the clustering algorithm ( \u00a7 4 ) , we evaluate the quality of our system and of the automatic sense tagging using SemEval datasets and a new manually annotated dataset we created ( \u00a7 5 ) .", "entities": []}, {"text": "We show that with the produced annotated corpora it is easy to serve sense - aware information retrieval applications ( \u00a7 7 ) .", "entities": [[16, 18, "TaskName", "information retrieval"]]}, {"text": "Another immediate application is feeding the sense - annotated corpora to a static embedding algorithm such as word2vec ( Mikolov et al . , 2013 ) , for deriving sense - aware static embeddings ( \u00a7 8) .", "entities": []}, {"text": "This results in state - of - theart sense - aware embeddings , which we evaluate both on an existing WiC benchmark ( Pilehvar and Camacho - Collados , 2019 ) and on a new challenging benchmark which we create ( \u00a7 9 ) .", "entities": [[20, 21, "DatasetName", "WiC"]]}, {"text": "In contrast to WSD which relies on curated sense inventories , our method is data - driven , therefore resulting senses are corpus dependent .", "entities": []}, {"text": "The method can be applied to any domain for which a BERTlike model is available , as we demonstrate by applying it to the PubMed Abstracts of scienti\ufb01c papers , using SCIBERT ( Beltagy et al . , 2019 ) .", "entities": []}, {"text": "The resulting senses cover scienti\ufb01c terms which are not typically found in standard sense inventories ( \u00a7 6).4739", "entities": []}, {"text": "Figure 1 shows examples of induced senses for selected words from the English Wikipedia corpus .", "entities": []}, {"text": "For each sense we list 5 communitybased representatives ( \u00a7 3 ) , as well as the 5 closest neighbours in the sense - aware embedding space ( \u00a7 8) .", "entities": []}, {"text": "Additional examples are available in Appendix A. Code and resources are available in github.com/allenai/WSIatScale .", "entities": []}, {"text": "2 Related Work Word Sense Induction and Disambiguation Previous challenges like Jurgens and Klapaftis ( 2013 ) focused on word sense induction for small sized datasets .", "entities": [[3, 6, "TaskName", "Word Sense Induction"], [19, 22, "TaskName", "word sense induction"]]}, {"text": "To the best of our knowledge we are the \ufb01rst to perform large - scale all - words WSI .", "entities": []}, {"text": "The closest work to our method is the substitution - based method proposed in Amrami and Goldberg ( 2018 , 2019 ) which is the starting point to our paper .", "entities": []}, {"text": "In that paper , the authors suggested a WSI algorithm designed for a small dataset ( SemEval 2010 , 2013 ) with a prede\ufb01ned set of ambiguous target words ( See ( \u00a7 3 ) for more details on the algorithm ) .", "entities": []}, {"text": "In our work , we change Amrami and Goldberg ( 2019 ) such that we can ef\ufb01ciently run sense induction on all the words in very large corpora .", "entities": []}, {"text": "An alternative approach for sense tagging is based on Word Sense Disambiguation ( WSD ) .", "entities": [[9, 12, "TaskName", "Word Sense Disambiguation"]]}, {"text": "The two main WSD methods are Supervised WSD and Knowledge - based WSD .", "entities": []}, {"text": "Supervised WSD suffers from the dif\ufb01culty of obtaining an adequate amount of annotated data .", "entities": []}, {"text": "Indeed , even SemCor , the largest manually annotated tagged corpus , consists of only 226,036 annotated tokens .", "entities": []}, {"text": "Among different supervisied WSD methods , Zhong and Ng ( 2010 ) suggested a SVM based approach and Melamud et", "entities": [[14, 15, "MethodName", "SVM"]]}, {"text": "al . ( 2016 ) ; Yuan et al .", "entities": []}, {"text": "( 2016 ) suggested LSTMs paired with nearest neighbours classi\ufb01cation .", "entities": []}, {"text": "Knowledgebase WSD ( Moro et al . , 2014 ; Pasini and Navigli , 2017 ) , on the other hand , avoids the reliance on large annotated word - to - sense corpus and instead maps words to senses from a closed sense inventory ( e.g. WordNet ( Miller , 1992 ) , BabelNet ( Navigli and Ponzetto , 2010 ) ) .", "entities": []}, {"text": "As such , the quality of knowledge - based WSD heavily depends on the availability , quality and coverage of the associated annotated resources .", "entities": []}, {"text": "Sense Embeddings", "entities": []}, {"text": "In \u00a7 8 we exploit the sense - induced corpus to train sense embeddings .", "entities": []}, {"text": "Reisinger and Mooney ( 2010 ) were the \ufb01rst to suggest creating multiple representations for ambiguous words .", "entities": []}, {"text": "Numerous recent papers ( Chen et al . ,2014 ; Rothe and Sch\u00fctze , 2015 ; Iacobacci et al . , 2015 ; Pilehvar and Collier , 2016 ; Mancini et al . , 2017 ; Iacobacci and Navigli , 2019 ) aim to produce similar embeddings , all of which use either WordNet or BabelNet as semantic network .", "entities": []}, {"text": "Our method is similar to Iacobacci et", "entities": []}, {"text": "al . ( 2015 ) , with the difference being that they rely on semantic networks ( via Babelfy ( Moro et al . , 2014 ) ) .", "entities": []}, {"text": "In contrast and similarly to us , Pelevina et al .", "entities": []}, {"text": "( 2016 ) does not rely on lexical resources such as WordNet .", "entities": []}, {"text": "The authors proposed splitting pretrained embeddings ( such as word2vec ) to a number of prototype senseembeddings .", "entities": []}, {"text": "Yet in our work , we directly learn the multi - prototype sense - embeddings which is only possible due to the large - scale corpus annotation .", "entities": []}, {"text": "When comparing both methods in \u00a7 9.1 we infer it is better to directly learn multi - prototype senseembeddings .", "entities": []}, {"text": "3 Large Scale Sense Induction 3.1 De\ufb01nition We de\ufb01ne large - scale sense induction as deriving sense clusters for all words in a large vocabulary and assigning a sense cluster to each corpus occurrence of these words.2 3.2 Algorithm Contextualized BERT vectors contain sense information , and clustering the contextualized vectors results in sense clusters .", "entities": [[40, 41, "MethodName", "BERT"]]}, {"text": "However , storing a 1024 dimensional vector of 32bit \ufb02oats for each relevant token in the English Wikipedia corpus requires over 8 TB of disk - space , making the approach cumbersome and not - scalable .", "entities": []}, {"text": "However , as shown by Amrami and Goldberg ( 2019 ) , MLM based wordsubstitutes also contain the relevant semantic information , and are much cheaper to store : each word - id in BERT LARGE \u2019s vocabulary can be represented by 2 bytes , and storing the top-5 substitutes for each corpus position requires less than 20 GB of storage space.3 2InBERT - large - cased - whole - word - masking this corresponds to 16k vocabulary items , that match to 1.59B full words in English Wikipedia , or 92 % of all word occurrences .", "entities": [[12, 13, "DatasetName", "MLM"], [35, 36, "MethodName", "BERT"]]}, {"text": "Analyzing the remaining words , only 0.01 % appear in Wikipedia more than 100 times .", "entities": []}, {"text": "We derive word senses to a substantial chunk of the vocabulary , which also corresponds to the most ambiguous words as less frequent words are substantially less polysemous ( Hern\u00e1ndez - Fern\u00e1ndez et al . , 2016 ; Fenk - Oczlon et al . , 2010 ; Zipf , 1945 ) .", "entities": []}, {"text": "3The size can be reduced further using adaptive encoding techniques that assign fewer bits to frequent words .", "entities": []}, {"text": "We did not implement this in this work.4740", "entities": []}, {"text": "Figure 2 : Scalable WSI \ufb02ow .", "entities": []}, {"text": "Given raw text , we annotate each word with its top - k substitutes , create inverted word index , \ufb01nd best clusters for each distinct lemma and associate all corpus words with a matching cluster .", "entities": [[26, 27, "DatasetName", "lemma"]]}, {"text": "In order to perform WSI at scale , we keep the main intuition from Amrami and Goldberg ( 2019 ) , namely to cluster sparse vectors of lemmas of the top - k MLM - derived word substitutions .", "entities": [[33, 34, "DatasetName", "MLM"]]}, {"text": "This results in vast storage saving , and also in a more interpretable representations .", "entities": []}, {"text": "However , for scalability , we iterate over the corpus sentences and collect the top - k substitutes for all words in the sentence at once based on a single BERT call for that sentence .", "entities": [[30, 31, "MethodName", "BERT"]]}, {"text": "This precludes us from using the dynamic - patterns component of their method , which requires separately running BERT for each word in each sentence .", "entities": [[18, 19, "MethodName", "BERT"]]}, {"text": "However , as we show in Section \u00a7 5.1 we still obtain suf\ufb01ciently high WSI results .", "entities": []}, {"text": "The steps for performing Scalable WSI are summarized in Fig .", "entities": []}, {"text": "2 . We elaborate on each step below , using English Wikipedia as a running example.4 Annotation : We run BERT - large - cased - wholeword - masking on English Wikipedia , inferring substitutes for all corpus positions .", "entities": [[20, 21, "MethodName", "BERT"]]}, {"text": "For positions that correspond to single - token words,5we consider the predicted words , \ufb01lter stop - words , lemmatize the remaining words ( Honnibal et al . , 2020 ) , and store the top-5 most probable lemmas to disk .", "entities": []}, {"text": "This step takes 5 hours on 20 cloud - based GPU machines ( total of 100 GPU hours ) , resulting in 1.63B tokens with their corresponding top-5 lemmas .", "entities": []}, {"text": "Inverted Word Index : We create an inverted index mapping from each single - token word to its corpus occurrences ( and their corresponding top-5 lemmas ) .", "entities": []}, {"text": "This takes 5 minutes on a 96 cores CPU machine , and 10 GB of disk .", "entities": []}, {"text": "Sense Induction : For each of 16,081 lemmas corresponding to single - token words , we retrieve random 1000 instances,6and induce senses using 4The Wikipedia corpus is based on a dump from August 2020 , with text extracted using WikiExtractor ( Attardi , 2015 ) .", "entities": []}, {"text": "5We exclude single - character tokens , stopwords and punctuation .", "entities": []}, {"text": "6The clustering algorithm scales super - linearly with the number of instances .", "entities": []}, {"text": "To reduce computation cost for tokens that appear more than 1000 times in the dataset , we sample min ( numOccur , 1000 ) instances for each token word , and cluster given the subset of instances .", "entities": []}, {"text": "We then associate each of the remaining instances to one of the clusters as explainedbass 0 bass 1 bass 2 bass 3 bass 4 bassist double \ufb01sh tenor trap guitar second bottom baritone swing lead tail perch voice heavy drum steel shark soprano dub rhythm electric add singer dance Table 1 : Top 5 representatives of the sense - speci\ufb01c communities of word bass .", "entities": [[15, 16, "DatasetName", "0"]]}, {"text": "The communities roughly match to bass as a musical instrument , register , \ufb01sh species , voice and in the context of Drum&Bass the community - based algorithm described in \u00a7 4 .", "entities": []}, {"text": "This process requires 30 minutes on the 96 - core CPU machine , and uses 100 MB of disk space .", "entities": []}, {"text": "The average number of senses per lemma is 3.13 .", "entities": [[6, 7, "DatasetName", "lemma"]]}, {"text": "Each sense is associated with up to 100 representative words , which represent the highest - degree words in the sense \u2019s community .", "entities": []}, {"text": "Table 1 shows the 5 senses found for the word bass with their top-5 representative words .", "entities": []}, {"text": "See additional examples in Fig .", "entities": []}, {"text": "1 and Appendix A. Tagging : Each of the remaining wordoccurrences is associated with a sense cluster by computing the Jaccard similarity between the occurrences \u2019 top-5 lemmas and the cluster representatives , and choosing the cluster that maximizes this score .", "entities": []}, {"text": "For example , an occurrence of the word bass with lemmas tenor , baritone , lead , opera , soprano will be associated with bass 3 .", "entities": []}, {"text": "This takes 100 minutes on 96 - core machine , and 25 GB of storage .", "entities": []}, {"text": "4 Sense Clustering Algorithm We replace the hierarchical clustering algorithm used by Amrami and Goldberg ( 2018 , 2019 ) with a community - detection , graph - based clustering algorithm .", "entities": []}, {"text": "One major bene\ufb01t of the community detection algorithms is that they naturally produces a dynamic number of clusters , and provide a list of interpretable discrete representative lemmas for each cluster .", "entities": [[5, 7, "TaskName", "community detection"]]}, {"text": "We additionally found this method to be more stable .", "entities": []}, {"text": "Graph - based clustering for word - sense induction typically constructs a graph from word occurrences in the \ufb01nal step of the algorithm.4741", "entities": []}, {"text": "or collocations , where the goal is to identify sensespeci\ufb01c sub - graphs within the graph that best induce different senses ( Klapaftis and Manandhar , 2008 , 2010 ) .", "entities": []}, {"text": "We instead construct the graph based on word substitutes .", "entities": []}, {"text": "Following Jurgens ( 2011 ) , we pose identifying sense - speci\ufb01c clusters as a community detection problem , where a community is de\ufb01ned as a group of connected nodes that are more connected to each other than to the rest of the graph .", "entities": [[15, 17, "TaskName", "community detection"]]}, {"text": "Graph construction For each word win the vocabulary , we construct a graph Gw= ( Vw;Ew ) where each vertex v2Vwis a substitute - word predicted by the MLM for w , and an edge ( u;v)2Ew connects substitutes that are predicted for the same instance .", "entities": [[0, 2, "TaskName", "Graph construction"], [28, 29, "DatasetName", "MLM"]]}, {"text": "The edge is weighted by the number of instances in which both uandvwere predicted .", "entities": []}, {"text": "More formally , let X = fxi wgn i=1bet the set of all top - ksubstitutes for ninstances of word w , and xi w = fw0j xiwgk j=1represents the ktop substitutes for theith instance of word", "entities": []}, {"text": "w. The graph Gwis de\ufb01ned as follows :", "entities": []}, {"text": "Vw = fu:9iu2xi wg Ew = f(u;v ) : 9iu2xi w^v2xi wg W(u;v )", "entities": []}, {"text": "= jfi : ( u;v)2xi wgj Community detection", "entities": [[6, 8, "TaskName", "Community detection"]]}, {"text": "A community in a subgraph corresponds to a set of tokens that tend to co - occur in top- ksubstitutes of many instances , and not co - occur with top- ksubstitutes of other instances .", "entities": []}, {"text": "This corresponds well to senses and we take community \u2019s nodes as sense \u2019s representatives .", "entities": []}, {"text": "We identify communities using the fast \u201c Louvain \u201d method ( Blondel et al . , 2008 ) .", "entities": []}, {"text": "Brie\ufb02y , Louvain searches for an assignment of nodes to clusters such that the modularity score Q \u2014 which measures the density of edges inside communities compared to edges between communities \u2014 is maximized : Q=1", "entities": []}, {"text": "2mX u v\u0014 W(u;v)\u0000kukv 2m\u0015 \u000e(cu;cv ) mis the sum of all edge weights in the graph , ku = P vW(u;v)is the sum of the weights of the edges attached to node u , cuis the community to which u is assigned , and \u000eis Kronecker delta function .", "entities": []}, {"text": "This objective is optimized using an iterative heuristic process .", "entities": []}, {"text": "For details , see Blondel et al .", "entities": []}, {"text": "( 2008).5 Intrinsic Evaluation of Clustering Algorithm We start by intrinsically evaluating the WSI clustering method on : ( a ) SemEval 2010 and SemEval 2013 ; and ( b ) a new test set we develop for largescale WSI .", "entities": [[24, 26, "DatasetName", "SemEval 2013"]]}, {"text": "In section 9 , we additionally extrinsically evaluate the accuracy of static embeddings derived from a sense - induced Wikipedia dataset .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "When collecting word - substitutes , we lemmatize the top - k list , join equivalent lemmas , remove stopwords and the target word from the list , and keep the top-5 remaining lemmas .", "entities": []}, {"text": "5.1 SemEval Evaluation We evaluate the community - based WSI algorithm on two WSI datasets :", "entities": []}, {"text": "SemEval 2010 Task 14 ( Manandhar et al . , 2010 ) and SemEval 2013 Task 13 ( Jurgens and Klapaftis , 2013 ) .", "entities": [[13, 15, "DatasetName", "SemEval 2013"]]}, {"text": "Table 2 compares our method to Amrami and Goldberg ( 2018 , 2019 ) and AutoSense ( Amplayo et al . , 2019 ) , which is the second - best available WSI method .", "entities": []}, {"text": "Bert - noDP / DP are taken from Amrami and Goldberg ( 2019 ) .", "entities": []}, {"text": "BertDP uses \u201c dynamic patterns \u201d which precludes widescale application .", "entities": []}, {"text": "We follow previous work ( Manandhar et al . , 2010 ; Komninos and Manandhar , 2016 ; Amrami and Goldberg , 2019 ) and evaluate SemEval 2010 using F - Score and V - Measure and SemEval 2013 using Fuzzy Normalized Mutual Information ( FNMI ) and Fuzzy B - Cubed ( FBC ) as well as their geometric mean ( A VG ) .", "entities": [[31, 32, "MetricName", "Score"], [37, 39, "DatasetName", "SemEval 2013"]]}, {"text": "Our method performs best on SemEval 2010 and comparable to state - of - the - art results on SemEval 2013 .", "entities": [[19, 21, "DatasetName", "SemEval 2013"]]}, {"text": "The algorithm performs on - par with the Bert - noDP method , and does not fall far behind the Bert - DP method .", "entities": []}, {"text": "We now turn to assess the end - to - end induction and tagging over Wikipedia .", "entities": []}, {"text": "5.2 Large Scale Manual Evaluation We evaluate our method on large corpora by randomly sampling 2000 instances from the senseinduced Wikipedia , focusing on frequent words with many senses .", "entities": []}, {"text": "We manually annotate the samples \u2019 senses without access to the automatically induced senses , and then compare our annotations to the system \u2019s sense assignments .", "entities": []}, {"text": "We publicly release our manual sense annotations .", "entities": []}, {"text": "Sampling and Manual Annotation We used a list of 20 ambiguous words from CoarseWSD-20 ( Loureiro et al . , 2021 ) .", "entities": [[13, 14, "DatasetName", "CoarseWSD-20"]]}, {"text": "The full list and per - word results can be found in Appendix C.", "entities": []}, {"text": "For each word we sampled 100 passages from English Wikipedia4742", "entities": []}, {"text": "Model F - S V - M A VG AutoSense 61.7 9.8 24.59 Bert - noDP 70.9 ( 0.4 ) 37.8 ( 1.5 ) 51.7 ( 1.2 )", "entities": []}, {"text": "Ours 70.95 ( 0.63 ) 40.79 ( 0.19 ) 53.79 ( 0.31 ) Bert - DP 71.3 ( 0.1 ) 40.4 ( 1.8 ) 53.6 ( 1.2", "entities": []}, {"text": ") Model FNMI", "entities": []}, {"text": "FBC A VG AutoSense 7.96 61.7 22.16 Bert - noDP 19.3 ( 0.7 ) 63.6 ( 0.2 ) 35.1 ( 0.6 )", "entities": []}, {"text": "Ours 19.42 ( 0.39 ) 61.98 ( 0.12 ) 34.69 ( 0.33 ) Bert - DP 21.4 ( 0.5 ) 64.0 ( 0.5 ) 37.0 ( 0.5 ) Table 2 : Evaluation on the SemEval 2010 ( top ) and SemEval 2013 ( bottom ) datasets .", "entities": [[40, 42, "DatasetName", "SemEval 2013"]]}, {"text": "We report mean ( STD ) scores over 10 runs .", "entities": []}, {"text": "with the target word , including in\ufb02ected forms ( case insensitive ) .", "entities": []}, {"text": "Unlike CoarseWSD-20 , we sampled examples without any respect to a prede\ufb01ned set of senses .", "entities": [[1, 2, "DatasetName", "CoarseWSD-20"]]}, {"text": "For example , the only two senses that appear in CoarseWSD-20 for the target word arm arearm ( anatomy ) , and arm ( computing ) , leaving out instances matching senses re\ufb02ecting weapons , subdivisions , mechanical arms etc .", "entities": [[10, 11, "DatasetName", "CoarseWSD-20"]]}, {"text": "With the notion that word sense induction systems should be robust to different annotations schemes , we gave two \ufb02uent English speakers 100 sentences for each of the 20 ambiguous words from CoarseWSD-20 .", "entities": [[4, 7, "TaskName", "word sense induction"], [32, 33, "DatasetName", "CoarseWSD-20"]]}, {"text": "Annotators were not given a sense inventory .", "entities": []}, {"text": "Each annotator was asked to label each instance with the matching sense according to their judgment .", "entities": []}, {"text": "For example , for the target word apple in the sentence \u201c The iPhone was announced by Apple CEO . \" , annotators can label the target sense with Apple Inc. , Apple The Company etc .", "entities": []}, {"text": "Annotation Guidelines are available in Appendix B.", "entities": []}, {"text": "On average annotators labeled 6:65senses per word ( 5:85and7:45average clusters per word for the two annotators ) .", "entities": []}, {"text": "This is more than the 2:65 average senses according to CoarseWSD-20 and less than WordNet \u2019s 9:85 .", "entities": [[10, 11, "DatasetName", "CoarseWSD-20"]]}, {"text": "Results We report our system \u2019s performance alongside two additional methods : A strong baseline of the most frequent sense ( MFS ) , and Babelfy ( Moro et al . , 2014)\u2014the sense disambiguation system used in BabelNet ( Tested using Babelfy live version April 2021 ) .", "entities": []}, {"text": "Differently from the latter , our system does not disambiguates but induces senses , therefore , clusters are not labeled with a sense tag from a sense inventory .", "entities": []}, {"text": "Instead , we represent senses to annotators using a list of common substitute words and a few examples .", "entities": []}, {"text": "Thus , after annotating the Wikipedia passages , we additionally asked annotators to name the system \u2019s clusters with the same naming convention as in their annotations .", "entities": []}, {"text": "MFS Babelfy Ours Ann # 1 49.55 41.5 89.05 Ann # 2 49.9 41.95 85.95 average 49.72 41.72 87.50 Table 3 : Classi\ufb01cation F1 scores for MFS , Babelfy and our proposed system by annotator on our manually annotated dataset .", "entities": [[23, 24, "MetricName", "F1"]]}, {"text": "Given a similar naming convention between systems and annotators , we report F1 scores of systems \u2019 tagging accuracy with respect to the manual annotations .", "entities": [[12, 13, "MetricName", "F1"], [18, 19, "MetricName", "accuracy"]]}, {"text": "We report F1 averaged over words in Table 3 .", "entities": [[2, 3, "MetricName", "F1"]]}, {"text": "Our system outperforms both baselines , despite Babelfy having access to a list of prede\ufb01ned word senses .", "entities": []}, {"text": "A full by - word table and comprehensive results analysis are in Appendix C.", "entities": []}, {"text": "While a 1 - to-1 mapping between system clusters and manual senses is optimal , our system sometimes splits senses into smaller clusters , thus annotators will name two system clusters with the same label .", "entities": []}, {"text": "Therefore it is also important to report the number of clusters produced by the system comparing to the number of senses after the annotators merged similar clusters .", "entities": []}, {"text": "Our system produced 7:25 clusters with 2:25clusters on average merged by the annotators.7Additionally , in rare cases our system encapsulates a few senses in a single cluster : this happened 3 and 5 times for both annotators across all the dataset .", "entities": []}, {"text": "6 Application to Scienti\ufb01c Corpora", "entities": []}, {"text": "A bene\ufb01t of a WSI approach compared to WSD methods is that it does not rely on a pre - speci\ufb01ed sense inventory , and can be applied to any corpus for which a BERT - like model is available .", "entities": [[34, 35, "MethodName", "BERT"]]}, {"text": "Thus , in addition to the Wikipedia dataset that has been presented throughout the paper , we also automatically induce senses over a corpus of 31 million PubMed Abstracts,8using SciBERT ( Beltagy et al . , 2019 ) .", "entities": []}, {"text": "As this dataset is larger than the Wikipedia dump , the process required roughly 145 GPU hours and resulting in 14;225sense - annotated lemmas , with an average number of 2:89senses per lemma .", "entities": [[32, 33, "DatasetName", "lemma"]]}, {"text": "This dataset highlights the data - driven advantages of sense - induction : the algorithm recovers many senses that are science speci\ufb01c and are not represented in the Wikipedia corpora .", "entities": []}, {"text": "While performing a wide - scale evaluation of the scienti\ufb01c WSI is beyond our scope in this work , we do show 7This is partially due to using clusters from two casing ( e.g. bank andBank ) , some of the merges share sense meaning but of different casing .", "entities": []}, {"text": "8www.nlm.nih.gov/databases/download/pubmed_medline4743", "entities": []}, {"text": "a few examples to qualitatively demonstrate the kinds of induced senses we get for scienti\ufb01c texts .", "entities": []}, {"text": "For each of the words mosaic , race andswine we show the induced clusters and the top-5 cluster representatives for each cluster .", "entities": []}, {"text": "mosaic 0mosaic 1 mosaic 2 mosaic 3 virus partial mixture mixed dwarf chimeric landscape genetic mild congenital combination spatial cmv heterozygous pattern functional stripe mutant matrix cellular While senses mosaic 0(the common mosaic virus of plants ) and mosaic 2(\u201csomething resembling a mosaic \" , \u201c mosaic of .. \" ) are represented in Wikipedia , senses mosaic 1(the mosaic genetic disorder ) and mosaic 3(mosaic is a quality , e.g. , \u201c mosaic border \u201d , \u201c mosaic pattern \u201d ) are speci\ufb01c to the scienti\ufb01c corpora ( The Wikipedia corpora , on the other hand , includes a sense of mosaic as a decorative art - form , which is not represented in Pubmed ) .", "entities": [[115, 116, "DatasetName", "Pubmed"]]}, {"text": "race 0 race 1 race 2 race 3 racial exercise class pcr ethnicity run group clone black training state sequence rac competition population rt gender sport genotype ra Senses race 0(ethnic group ) , race 1(competition ) and race 2(population / civilization ) are shared with wikipedia , while the sense race 3(\u201cRapid ampli\ufb01cation of cDNA ends \u201d , a technique for obtaining the sequence length of an RNA transcript using reverse transcription ( RT ) and PCR ) is Pubmed - speci\ufb01c .", "entities": [[1, 2, "DatasetName", "0"], [80, 81, "DatasetName", "Pubmed"]]}, {"text": "swine 0 swine 1 swine 2 pig seasonal patient porcine avian infant animal in\ufb02uenza group livestock pandemic case goat bird myocardium Here swine 1captures", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "the Swine In\ufb02uenza pandemic , while swine 2refers to swine as experimental Pigs .", "entities": []}, {"text": "7 Sense - aware Information Retrieval An immediate application of a high quality sensetagged corpus is sense - aware retrieval .", "entities": [[4, 6, "TaskName", "Information Retrieval"]]}, {"text": "We incorporate the sense information in the SPIKE extractive search system ( Shlain et al . , 2020)9for Wikipedia and Pubmed datasets .", "entities": [[20, 21, "DatasetName", "Pubmed"]]}, {"text": "When entering a search term , suf\ufb01xing it with @ triggers sense selection allowing 9spike.apps.allenai.orgto narrow the search for the speci\ufb01c sense .", "entities": []}, {"text": "Consider a scientist looking for PubMed occurrences of the word \u201c swine \" in its in\ufb02uenza meaning .", "entities": []}, {"text": "As shown in Figure 3 , this can be easily done by writing \u201c swine@ \u201d and choosing the second item in the resulting popup window .", "entities": []}, {"text": "The outputs are sentences with the word \u201c swine \" in the matching sense .", "entities": []}, {"text": "As far as we know , SPIKE is the \ufb01rst system with such WSI capabilities for IR .", "entities": []}, {"text": "Similarly , Blloshmi et al .", "entities": []}, {"text": "( 2021 ) suggested to enhance IR with sense information , but differently from us , this is done by automatically tagging words with senses from a prede\ufb01ned inventory .", "entities": []}, {"text": "8 Sense - aware Static Embeddings Learning static word embeddings of senseambiguous words is a long standing research goal ( Reisinger and Mooney , 2010 ; Huang et al . , 2012 ) .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "There are numerous real - world tasks where context is not available , precluding the use of contextualized - embeddings .", "entities": []}, {"text": "These include Outlier Detection ( Camacho - Collados and Navigli , 2016 ; Blair et al . , 2016 ) , Term Set Expansion ( Roark and Charniak , 2000 ) the Hypernymy task ( Breit et al . , 2021 ) , etc .", "entities": [[2, 4, "TaskName", "Outlier Detection"]]}, {"text": "Additionally , static embeddings are substantially more ef\ufb01cient to use , can accommodate larger vocabulary sizes , and can accommodate ef\ufb01cient indexing and retrieval .", "entities": []}, {"text": "Yet , despite their \ufb02exibility and success , common word embedding methods still represent ambiguous words as a single vector , and suffer from the inability to distinguish between different meanings of a word ( CamachoCollados and Pilehvar , 2018 ) .", "entities": []}, {"text": "Using our sense - tagged corpus we suggest a simple and effective method for deriving sense - aware static embeddings : We run an off - the - shelf embedding algorithm,10on the corpus where single - token words are replaced with a concatenation of the word and its induced sense ( e.g. \u201c I caught a bass . \" becomes \u201c I caught@0 a bass@2 . \" ) .", "entities": []}, {"text": "This makes the embedding algorithm learn embeddings for all senses of each word out - of - the - box.11An integral property of the embedding algorithm is that it represents both the sense - annotated tokens and the other vocabulary items in the same embedding space \u2014 10We use the CBOW variant of the word2vec algorithm ( Mikolov et al . , 2013 ) as implemented in Gensim ( \u02c7Reh\u02da u \u02c7rek and Sojka , 2010 ) .", "entities": []}, {"text": "We derive 100 - dimensional embeddings using the negative - sampling algorithm and a window size of 5 . 11A similar approach was used by Iacobacci", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2015 ) over a corpus which was labeled with BabelNet and WordNet", "entities": []}, {"text": "senses.4744", "entities": []}, {"text": "Figure 3 : User interaction in SPIKE when looking for the word \u201c swine \" in its \u201c swine \ufb02u \" sense .", "entities": []}, {"text": "( Unlike the animal / experimental pig senses ) this helps sense inferring about words that are represented in the MLM as multi - tokens words ( Even though these correspond to less - frequent and often less ambiguous words ( Hern\u00e1ndez - Fern\u00e1ndez et al . , 2016 ; Fenk - Oczlon et al . , 2010 ; Zipf , 1945 ) ) .", "entities": [[20, 21, "DatasetName", "MLM"]]}, {"text": "For example , in the top-5 nearest neighbours for the different bass senses as shown below , smallmouth and pumpkinseed , multi - token words in BERT LARGE \u2019s vocabulary , are close neighbours the bass instances that correspond to the \ufb01shsense .", "entities": [[26, 27, "MethodName", "BERT"]]}, {"text": "bass 0 bass 1 bass 2 bass 3 bass 4 guitar 0 tuba crappie baritone 0synth drums 0 trombone 0smallmouth tenor 0 drum 1 guitar 3 horn 0 pumpkinseed alto 0 synths keyboards 0\ufb02ute 0", "entities": [[1, 2, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [21, 22, "DatasetName", "0"], [27, 28, "DatasetName", "0"], [30, 31, "DatasetName", "0"], [34, 35, "DatasetName", "0"]]}, {"text": "sun\ufb01sh bassoon breakbeats keyboard 0 trumpet 0 perch 0 \ufb02ute 0 trap 4 Note that some neighbours are sense annotated ( single - token words that were tagged by our system ) , while others are not ( multi - token words ) .", "entities": [[4, 5, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}, {"text": "For English Wikipedia , we obtain a total vocabulary of 1.4 M forms , 90;023of which are senseannotated .", "entities": []}, {"text": "Compared to the community - based representative words , the top neighbours in the embedding space tend to capture members of the same semantic class rather than direct potential replacements .", "entities": []}, {"text": "9 Sense - aware Embeddings Evaluation 9.1 WiC Evaluation Pilehvar and Camacho - Collados ( 2019 ) introduced the WiC dataset for the task of classifying word meaning in context .", "entities": [[4, 6, "TaskName", "Embeddings Evaluation"], [7, 8, "DatasetName", "WiC"], [19, 20, "DatasetName", "WiC"]]}, {"text": "Each instance in WiC has a target word and two contexts in which it appears .", "entities": [[3, 4, "DatasetName", "WiC"]]}, {"text": "The goal is to classify whether the word in the different contexts share the same meaning .", "entities": []}, {"text": "e.g. given two contexts : There \u2019s a lot of trash on the bedof the river andI keep a glass of water next to my bedwhen I sleep , our method should return False as the sense of the target word bedis different .", "entities": []}, {"text": "Method Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "JBT ( Pelevina et al . , 2016 ) 53.6 Sense - aware Embeddings ( this work ) 58.3 SW2V * ( Mancini et al . , 2017 ) 58.1 DeConf * ( Pilehvar and Collier , 2016 ) 58.7 LessLex * ( Colla et al . , 2020 ) 59.2 Table 4 : Accuracy scores on the WiC dataset .", "entities": [[54, 55, "MetricName", "Accuracy"], [58, 59, "DatasetName", "WiC"]]}, {"text": "Systems marked with * make use of external lexical resources .", "entities": []}, {"text": "Word Embeddings OPP Acc . GloVe 93.31 65 word2vec 93.31 68 DeConf 93.37 73 Ours ( Skip - gram ) 96.31 83.5 Ours ( CBOW ) 96.68 86 Table 5 : OPP and Accuracy on the 25 - 7 - 1 - 8 dataset .", "entities": [[0, 2, "TaskName", "Word Embeddings"], [3, 4, "MetricName", "Acc"], [5, 6, "MethodName", "GloVe"], [33, 34, "MetricName", "Accuracy"]]}, {"text": "Our method is the following : Given the senseaware embeddings , a target word wand two contexts , we calculate the context vector as the average of the context words .", "entities": []}, {"text": "The matching sense vector is the closest out of all wembeddings .", "entities": []}, {"text": "We then classify the contexts as corresponding to the same meaning if the cosine distance of the found sense embedding is more than threshold apart .", "entities": []}, {"text": "We do not use the train set .", "entities": []}, {"text": "The threshold is optimized over the development set and \ufb01xed to 0:68 .", "entities": []}, {"text": "This task has a few tracks , we compare our embeddings systems to the best performing methods from the Sense Representations track .", "entities": []}, {"text": "Of these , JBT ( Pelevina et al . , 2016 ) , a lexical embedding method , is the only one that does not use an external lexical resource ( induction ) .", "entities": []}, {"text": "The results in Table 4 show accuracy on this task .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "We outperform the induction method , and are on - par with the lexicon - based methods , despite not using any external lexical resource .", "entities": []}, {"text": "9.2 Evaluation via Outlier Detection Another setup for evaluating word embeddings is that of outlier detection : given a set of words , identify which one does not belong to the set ( Blair4745", "entities": [[3, 5, "TaskName", "Outlier Detection"], [9, 11, "TaskName", "word embeddings"], [14, 16, "TaskName", "outlier detection"]]}, {"text": "et al . , 2016 ) .", "entities": []}, {"text": "Outlier detection instances are composed of in - group elements and a set of outliers from a related semantic space .", "entities": [[0, 2, "TaskName", "Outlier detection"]]}, {"text": "In each evaluation round , one outlier is added to the in - group items , and the algorithm is tasked with \ufb01nding the outlier .", "entities": []}, {"text": "Existing outlier detection datasets either did not explicitly target sense - ambiguous words ( 8 - 8 - 8 ( Camacho - Collados and Navigli , 2016 ) , WikiSem500 ( Blair et al . , 2016 ) ) or explicitly removed ambiguous words altogether ( 25 - 8 - 8 - sem ( Brink Andersen et", "entities": [[1, 3, "TaskName", "outlier detection"], [29, 30, "DatasetName", "WikiSem500"]]}, {"text": "al . , 2020 ) ) .", "entities": []}, {"text": "Ambiguity - driven Outlier Detection .", "entities": [[3, 5, "TaskName", "Outlier Detection"]]}, {"text": "We construct a challenge set for outlier detection that speci\ufb01cally targets ambiguous cases .", "entities": [[6, 8, "TaskName", "outlier detection"]]}, {"text": "In order to account for sense ambiguity , we add a distractor to each of the in - group sets : the distractor is an item which has multiple senses , where the most salient sense does not belong to the group , while another sense does belong to the group .", "entities": []}, {"text": "For example : In - group : zeus , hades , poseidon , aphrodite , ares , athena , artemis Outliers : mercury , odysseus , jesus , sparta , delphi , rome , wrath , atlanta Distractor : nike Here , a model which does not explicitly represent the greek - god sense of nike is likely to place it far away from the in - group instances , causing it to be mistakenly marked as the outlier .", "entities": [[19, 20, "DatasetName", "artemis"], [24, 25, "DatasetName", "odysseus"]]}, {"text": "The starting point for our dataset is 25 - 8 - 8 - Sem ( Brink Andersen et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "This dataset contains 25 test groups , each with 8 in - group elements and 8 outliers , resulting in 200 unique test cases .", "entities": []}, {"text": "The outliers are sorted in a decreasing degree of relatedness to the in - group elements .", "entities": []}, {"text": "In our dataset we replace one of the in - group elements with an ambiguous distractor .", "entities": []}, {"text": "For example , in the Greek - gods case above , we replaced the original 8thitem ( \u201c hera \" ) with the ambiguous distractor nike.12The dataset consists of 25 groups of 7 non ambiguous group elements , 1 distractor and 8 outliers ( 25 - 7 - 1 - 8 ) , similarly resulting 200 unique test cases .", "entities": []}, {"text": "Method Following Camacho - Collados and Navigli ( 2016 ) , we rank each word likelihood of being the outlier by the average of all pair - wise semantic similarities of the words in Wnfwg .", "entities": []}, {"text": "Therefore ifwis an outlier , this score should be low .", "entities": []}, {"text": "See Appendix D for additional details .", "entities": []}, {"text": "Metrics Camacho - Collados and Navigli ( 2016 ) 12We additionally changed terms that are debatably ambiguous and changed the \u201c African animals \" group to the more general \u201c animals \" as no distractors were found.proposed evaluating outlier detection using the accuracy ( The fraction of correctly classi\ufb01ed outliers among the total cases ) and Outlier Position Percentage ( OPP ) metric .", "entities": [[38, 40, "TaskName", "outlier detection"], [42, 43, "MetricName", "accuracy"]]}, {"text": "OPP indicates how close outliers are to being classi\ufb01ed correctly : OPP = P W2DOP(W ) jWj\u00001 jDj\u0002100 whereOP(W)is the position of the outlier according to the algorithm .", "entities": []}, {"text": "Results In Table 5 we report performance of on the25 - 7 - 1 - 8 set .", "entities": []}, {"text": "Word2vec and GloVe accuracy scores are low while having high OPP scores .", "entities": [[2, 3, "MethodName", "GloVe"], [3, 4, "MetricName", "accuracy"]]}, {"text": "This is the expected behaviour for embeddings without sense awareness .", "entities": []}, {"text": "These will position the distractor and the outlier furthest away from the group items while not designed to make the hard decision required for high Accuracy .", "entities": [[25, 26, "MetricName", "Accuracy"]]}, {"text": "Our sense - aware embeddings strongly outperform GloVe and word2vec which do not include senses .", "entities": [[7, 8, "MethodName", "GloVe"]]}, {"text": "Our embeddings also outperform the word embeddings proposed in DeConf ( Pilehvar and Collier , 2016 ) , which are the best performing sense embeddings on WiC which are also publicly available .", "entities": [[5, 7, "TaskName", "word embeddings"], [26, 27, "DatasetName", "WiC"]]}, {"text": "10 Conclusion We show that substitution - based word - sense induction algorithms based on word - substitutions derived from MLMs are easily scalable to large corpora and vocabulary sizes , allowing to ef\ufb01ciently obtain high - quality sense annotated corpora .", "entities": []}, {"text": "We demonstrate the utility of such large - scale sense annotation , both in the context of a scienti\ufb01c search application , and for deriving high - quality senseaware static word embeddings .", "entities": [[30, 32, "TaskName", "word embeddings"]]}, {"text": "As a secondary contribution , we also develop a new variant of the Outlier Detection evaluation task , which explicitly targets ambiguous words .", "entities": [[13, 15, "TaskName", "Outlier Detection"]]}, {"text": "11 Acknowledgments This project has received funding from the European Research Council ( ERC ) under the European Union \u2019s Horizon 2020 research and innovation programme , grant agreement No . 802774 ( iEXTRACT ) .", "entities": []}, {"text": "References Reinald Kim Amplayo , Seung - won Hwang , and Min Song .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Autosense model for word sense induction .", "entities": [[3, 6, "TaskName", "word sense induction"]]}, {"text": "In The Thirty - Third AAAI Conference on Arti\ufb01-4746", "entities": []}, {"text": "cial Intelligence , AAAI 2019 , The Thirty - First Innovative Applications of Arti\ufb01cial Intelligence Conference , IAAI 2019 , The Ninth AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence , EAAI 2019 , Honolulu , Hawaii , USA , January 27 - February 1 , 2019 , pages 6212\u20136219 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Asaf Amrami and Yoav Goldberg .", "entities": [[0, 1, "MethodName", "Asaf"]]}, {"text": "2018 .", "entities": []}, {"text": "Word sense induction with neural biLM and symmetric patterns .", "entities": [[0, 3, "TaskName", "Word sense induction"]]}, {"text": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 4860\u20134867 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Asaf Amrami and Yoav Goldberg .", "entities": [[0, 1, "MethodName", "Asaf"]]}, {"text": "2019 .", "entities": []}, {"text": "Towards better substitution - based word sense induction .", "entities": [[5, 8, "TaskName", "word sense induction"]]}, {"text": "arXiv preprint arXiv:1905.12598 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Giusepppe Attardi .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Wikiextractor .", "entities": []}, {"text": "https:// github.com/attardi/wikiextractor .", "entities": []}, {"text": "Osman Ba\u00b8 skaya , Enis Sert , V olkan Cirik , and Deniz Yuret .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "AI - KU : Using substitute vectors and co - occurrence modeling for word sense induction and disambiguation .", "entities": [[13, 16, "TaskName", "word sense induction"]]}, {"text": "In Second Joint Conference on Lexical and Computational Semantics ( * SEM ) , Volume 2 : Proceedings of the Seventh International Workshop on Semantic Evaluation ( SemEval 2013 ) , pages 300\u2013306 , Atlanta , Georgia , USA .", "entities": [[27, 29, "DatasetName", "SemEval 2013"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Iz Beltagy , Kyle Lo , and Arman Cohan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Scibert : Pretrained language model for scienti\ufb01c text .", "entities": []}, {"text": "In EMNLP .", "entities": []}, {"text": "Philip Blair , Yuval Merhav , and Joel Barry .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Automated generation of multilingual clusters for the evaluation of distributed representations .", "entities": []}, {"text": "arXiv preprint arXiv:1611.01547 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rexhina Blloshmi , Tommaso Pasini , Niccol\u00f2 Campolungo , Somnath Banerjee , Roberto Navigli , and Gabriella Pasi . 2021 .", "entities": []}, {"text": "Ir like a sir : Sense - enhanced information retrieval for multiple languages .", "entities": [[8, 10, "TaskName", "information retrieval"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 1030 \u2013 1041 .", "entities": []}, {"text": "Vincent D Blondel , Jean - Loup Guillaume , Renaud Lambiotte , and Etienne Lefebvre . 2008 .", "entities": []}, {"text": "Fast unfolding of communities in large networks .", "entities": []}, {"text": "Journal of statistical mechanics : theory and experiment , 2008(10):P10008 .", "entities": []}, {"text": "Anna Breit , Artem Revenko , Kiamehr Rezaee , Mohammad Taher Pilehvar , and Jose Camacho - Collados . 2021 .", "entities": []}, {"text": "WiC - TSV : An evaluation benchmark for target sense veri\ufb01cation of words in context .", "entities": [[0, 3, "DatasetName", "WiC - TSV"], [12, 15, "DatasetName", "words in context"]]}, {"text": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics : Main Volume , pages 1635\u20131645 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jesper Brink Andersen , Mikkel Bak Bertelsen , Mikkel H\u00f8rby Schou , Manuel R. Ciosici , and Ira Assent .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "One of these words is not like the other : a reproduction of outlier identi\ufb01cation using noncontextual word representations .", "entities": []}, {"text": "In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems , pages 120\u2013130 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jos\u00e9 Camacho - Collados and Roberto Navigli .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Find the word that does not belong : A framework for an intrinsic evaluation of word vector representations .", "entities": []}, {"text": "In Proceedings of the 1st Workshop on Evaluating Vector - Space Representations for NLP , pages 43\u201350 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Jose Camacho - Collados and Mohammad Taher Pilehvar .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "From word to sense embeddings : A survey on vector representations of meaning .", "entities": []}, {"text": "Journal of Arti\ufb01cial Intelligence Research , 63:743\u2013788 .", "entities": []}, {"text": "Xinxiong Chen , Zhiyuan Liu , and Maosong Sun .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A uni\ufb01ed model for word sense representation and disambiguation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1025\u20131035 , Doha , Qatar .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Davide Colla , Enrico Mensa , and Daniele P. Radicioni .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "LessLex :", "entities": []}, {"text": "Linking multilingual embeddings to SenSe representations of LEXical items .", "entities": []}, {"text": "Computational Linguistics , 46(2):289\u2013333 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Gertraud Fenk - Oczlon , August Fenk , and Pamela Faber .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Frequency effects on the emergence of polysemy and homophony .", "entities": []}, {"text": "International Journal of Information Technologies and Knowledge , 4(2):103 \u2013 109 .", "entities": []}, {"text": "Antoni Hern\u00e1ndez - Fern\u00e1ndez , Bernardino Casas , Ramon Ferrer - i Cancho , and Jaume Baixeries .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Testing the robustness of laws of polysemy and brevity versus frequency .", "entities": []}, {"text": "In International Conference on Statistical Language and Speech Processing , pages 19\u201329 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Matthew Honnibal , Ines Montani , So\ufb01e Van Landeghem , and Adriane Boyd .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "spaCy : Industrial - strength Natural Language Processing in Python .", "entities": []}, {"text": "Eric Huang , Richard Socher , Christopher Manning , and Andrew Ng . 2012 .", "entities": []}, {"text": "Improving word representations via global context and multiple word prototypes .", "entities": []}, {"text": "In Proceedings of the 50th Annual Meeting of4747", "entities": []}, {"text": "the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 873\u2013882 , Jeju Island , Korea .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ignacio Iacobacci and Roberto Navigli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Lstmembed : Learning word and sense representations from a large semantically annotated corpus with long short - term memories .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1685\u20131695 .", "entities": []}, {"text": "Ignacio Iacobacci , Mohammad Taher Pilehvar , and Roberto Navigli . 2015 .", "entities": []}, {"text": "SensEmbed :", "entities": []}, {"text": "Learning sense embeddings for word and relational similarity .", "entities": []}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 95\u2013105 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "David Jurgens .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Word sense induction by community detection .", "entities": [[0, 3, "TaskName", "Word sense induction"], [4, 6, "TaskName", "community detection"]]}, {"text": "In Proceedings of TextGraphs6 : Graph - based Methods for Natural Language Processing , pages 24\u201328 , Portland , Oregon .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "David Jurgens and Ioannis Klapaftis .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "SemEval2013 task 13 : Word sense induction for graded and non - graded senses .", "entities": [[4, 7, "TaskName", "Word sense induction"]]}, {"text": "In Second Joint Conference on Lexical and Computational Semantics ( * SEM ) , Volume 2 : Proceedings of the Seventh International Workshop on Semantic Evaluation ( SemEval 2013 ) , pages 290\u2013299 , Atlanta , Georgia , USA .", "entities": [[27, 29, "DatasetName", "SemEval 2013"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ioannis Klapaftis and Suresh Manandhar .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Word sense induction & disambiguation using hierarchical random graphs .", "entities": [[0, 3, "TaskName", "Word sense induction"]]}, {"text": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , pages 745\u2013755 , Cambridge , MA . Association for Computational Linguistics .", "entities": [[17, 18, "DatasetName", "Cambridge"]]}, {"text": "Ioannis P Klapaftis and Suresh Manandhar .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Word sense induction using graphs of collocations .", "entities": [[0, 3, "TaskName", "Word sense induction"]]}, {"text": "InECAI , pages 298\u2013302 .", "entities": []}, {"text": "Alexandros Komninos and Suresh Manandhar .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Structured generative models of continuous features for word sense induction .", "entities": [[7, 10, "TaskName", "word sense induction"]]}, {"text": "In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 3577\u20133587 .", "entities": []}, {"text": "Daniel Loureiro , Kiamehr Rezaee , Mohammad Taher Pilehvar , and Jose Camacho - Collados . 2021 .", "entities": []}, {"text": "Analysis and evaluation of language models for word sense disambiguation .", "entities": [[7, 10, "TaskName", "word sense disambiguation"]]}, {"text": "Computational Linguistics , pages 1\u201355 .", "entities": []}, {"text": "Suresh Manandhar , Ioannis Klapaftis , Dmitriy Dligach , and Sameer Pradhan .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "SemEval-2010 task 14 : Word sense induction & disambiguation .", "entities": [[4, 7, "TaskName", "Word sense induction"]]}, {"text": "In Proceedings of the 5th International Workshop on Semantic Evaluation , pages 63\u201368 , Uppsala , Sweden .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Massimiliano Mancini , Jose Camacho - Collados , Ignacio Iacobacci , and Roberto Navigli .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Embedding words and senses together via joint knowledgeenhanced training .", "entities": []}, {"text": "In Proceedings of the 21st Conference on Computational Natural Language Learning ( CoNLL 2017 ) , pages 100\u2013111 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Oren Melamud , Jacob Goldberger , and Ido Dagan .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "context2vec : Learning generic context embedding with bidirectional LSTM .", "entities": [[0, 1, "MethodName", "context2vec"], [7, 9, "MethodName", "bidirectional LSTM"]]}, {"text": "In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning , pages 51\u201361 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tom\u00e1s Mikolov , Ilya Sutskever , Kai Chen , Gregory S. Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 26 : 27th Annual Conference on Neural Information Processing Systems 2013 .", "entities": []}, {"text": "Proceedings of a meeting held December 5 - 8 , 2013 , Lake Tahoe , Nevada , United States , pages 3111 \u2013 3119 .", "entities": []}, {"text": "George A. Miller .", "entities": []}, {"text": "1992 .", "entities": []}, {"text": "WordNet :", "entities": []}, {"text": "A lexical database for English .", "entities": []}, {"text": "In Speech and Natural Language : Proceedings of a Workshop Held at Harriman , New York , February 23 - 26 , 1992 .", "entities": []}, {"text": "Andrea Moro , Alessandro Raganato , and Roberto Navigli .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Entity linking meets word sense disambiguation : a uni\ufb01ed approach .", "entities": [[0, 2, "TaskName", "Entity linking"], [3, 6, "TaskName", "word sense disambiguation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 2:231 \u2013 244 .", "entities": []}, {"text": "Roberto Navigli and Simone Paolo Ponzetto .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "BabelNet : Building a very large multilingual semantic network .", "entities": []}, {"text": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , pages 216\u2013225 , Uppsala , Sweden .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tommaso Pasini and Roberto Navigli .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "TrainO - Matic : Large - scale supervised word sense disambiguation in multiple languages without manual training data .", "entities": [[8, 11, "TaskName", "word sense disambiguation"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 78\u201388 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Maria Pelevina , Nikolay Are\ufb01ev , Chris Biemann , and Alexander Panchenko .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Making sense of word embeddings .", "entities": [[3, 5, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 1st Workshop on Representation Learning for NLP , pages 174 \u2013 183 , Berlin , Germany . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Representation Learning"]]}, {"text": "Mohammad Taher Pilehvar and Jose CamachoCollados .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "WiC : the word - in - context dataset for evaluating context - sensitive meaning representations .", "entities": [[0, 1, "DatasetName", "WiC"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association4748", "entities": []}, {"text": "for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1267\u20131273 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mohammad Taher Pilehvar and Nigel Collier .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "De - con\ufb02ated semantic representations .", "entities": []}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1680\u20131690 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Radim \u02c7Reh\u02da u \u02c7rek and Petr Sojka .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Software Framework for Topic Modelling with Large Corpora .", "entities": []}, {"text": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks , pages 45\u201350 , Valletta , Malta .", "entities": []}, {"text": "ELRA .", "entities": []}, {"text": "Joseph Reisinger and Raymond J. Mooney .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Multi - prototype vector - space models of word meaning .", "entities": []}, {"text": "In Human Language Technologies : The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics , pages 109\u2013117 , Los Angeles , California . Association for Computational Linguistics .", "entities": []}, {"text": "Brian Roark and Eugene Charniak . 2000 .", "entities": []}, {"text": "Noun - phrase co - occurrence statistics for semi - automatic semantic lexicon construction .", "entities": []}, {"text": "arXiv preprint cs/0008026 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sascha Rothe and Hinrich Sch\u00fctze . 2015 .", "entities": []}, {"text": "AutoExtend :", "entities": []}, {"text": "Extending word embeddings to embeddings for synsets and lexemes .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 1793\u20131803 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Micah Shlain , Hillel Taub - Tabib , Shoval Sadde , and Yoav Goldberg .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Syntactic search by example .", "entities": []}, {"text": "InACL .", "entities": []}, {"text": "Dayu Yuan , Julian Richardson , Ryan Doherty , Colin Evans , and Eric Altendorf . 2016 .", "entities": []}, {"text": "Semi - supervised word sense disambiguation with neural models .", "entities": [[3, 6, "TaskName", "word sense disambiguation"]]}, {"text": "In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 1374\u20131385 , Osaka , Japan .", "entities": []}, {"text": "The COLING 2016 Organizing Committee .", "entities": []}, {"text": "Zhi Zhong and Hwee Tou Ng . 2010 .", "entities": []}, {"text": "It makes sense : A wide - coverage word sense disambiguation system for free text .", "entities": [[8, 11, "TaskName", "word sense disambiguation"]]}, {"text": "In Proceedings of the ACL 2010 System Demonstrations , pages 78\u201383 , Uppsala , Sweden .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "George Kingsley Zipf .", "entities": []}, {"text": "1945 .", "entities": []}, {"text": "The meaning - frequency relationship of words .", "entities": []}, {"text": "The Journal of general psychology , 33(2):251\u2013256.4749", "entities": []}, {"text": "A Additional Examples Due to limit of space we provide additional examples in the appendix .", "entities": []}, {"text": "We start with the senses found for the word face :", "entities": []}, {"text": "Representatives face 0 face 1 face 2 face 3 confront head look side meet front address line encounter name point wall suffer cheek serve surface experience body toward slope Neighbours face 0 face 1 face 2 face 3 meet 3 hand 0 faced 2 slope 0 challenge 3forehead 0sit1 rim0 suffer 0 hands 0 hang 1 \ufb02ank 2 confront 0 nose 0 facing 2ridge 4 lose1 eyes 3 rotate 0 slope 1", "entities": [[2, 3, "DatasetName", "0"], [31, 32, "DatasetName", "0"], [41, 42, "DatasetName", "0"], [45, 46, "DatasetName", "0"], [51, 52, "DatasetName", "0"], [53, 54, "DatasetName", "0"], [59, 60, "DatasetName", "0"], [61, 62, "DatasetName", "0"], [69, 70, "DatasetName", "0"]]}, {"text": "The face senses refer to meeting / confronting , the body part , turn / look and side , respectively .", "entities": []}, {"text": "Here we present two senses of the word orange , corresponding to the color and fruit : Representatives Neighbours orange 0orange 1orange 0orange 1 yellow apple yellow 0apple 0 red lemon purple 0 avocado amber lime amber 0 almond pink fruit blue 0 apple 1 olive banana orangish apricot", "entities": [[28, 29, "DatasetName", "0"], [32, 33, "DatasetName", "0"], [37, 38, "DatasetName", "0"], [42, 43, "DatasetName", "0"], [48, 49, "DatasetName", "apricot"]]}, {"text": "Finally we present the senses for Jordan : Representatives Jordan 0Jordan 1 Jordan 2Jordan 3 Johnson Jerusalem David River Jones Palestine Jason Zion Jackson Israel Joel Water Murray Yemen Justin City Mason Turkey Jonathan water", "entities": []}, {"text": "Neighbours Jordan 0", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "Jordan 1 Jordan 2 Jordan 3 Jones 1 Kuwait 1 Jeremy 1 Huleh Kramer 1 Lebanon 0Aaron 0", "entities": [[17, 18, "DatasetName", "0"]]}, {"text": "Yarkon Allen 0 Syria 0", "entities": [[2, 3, "DatasetName", "0"], [4, 5, "DatasetName", "0"]]}, {"text": "Justin 0 Arabah Mack 0 Iraq0 Brandon 0Khabur Robinson 0Sudan 1 Josh 0", "entities": [[1, 2, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}, {"text": "Tyropoeon Here the clusters correspond to Jordan the surname , the country , \ufb01rst name and the Jordan River , respectively .", "entities": []}, {"text": "B Annotation Guidelines for Manual Evaluation The objective of this task is to annotate wordmeanings of 20 ambiguous words in a total of 2000 different contexts .", "entities": []}, {"text": "What is word - meaning ?", "entities": []}, {"text": "Words have different meanings in different contexts , for example , in the sentence : \u201c there is a light that never goes out \" , the word \u201c light \" refers to any device serving as asource of illumination .", "entities": []}, {"text": "While \u201c light \" in the sentence \u201c light as a feather \" refers to the comparatively little physical weight or density of an object .", "entities": []}, {"text": "Step 1 : In this dataset we examine 20 ambiguous words as targets .", "entities": []}, {"text": "For each of these words we collected 100 sentences in which the target word appears .", "entities": []}, {"text": "For every sentence in the 100 set per target word , you will be asked to write a short label expressing the meaning of the target word in that particular context .", "entities": []}, {"text": "For example , here are three sentences with the target word \u201c light \" , each with its possible annotation .", "entities": []}, {"text": "1.\u201cthere is a light that never goes out \" !", "entities": []}, {"text": "visible light .", "entities": []}, {"text": "2.\u201clight as a feather\"!light as in weight .", "entities": []}, {"text": "3.\u201cmagnesium is a light metal\"!light as in weight .", "entities": []}, {"text": "Note that in this example the annotator found the second and third meanings of the word \u201c light \" to be the same and therefore labeled them with the same label.13", "entities": []}, {"text": "While some annotations are indeed intuitive , labeling word - meanings when the target word is part of a name can be challenging .", "entities": []}, {"text": "Here are a few guidelines for such use case : Whenever a target word appeared as part of a name ( Person , Organization etc . ) , one of three heuristics should be used14 : 1 .", "entities": []}, {"text": "If the target word is the surname of a person , the example should be tagged surname .15 2 .", "entities": []}, {"text": "If the entity ( as a whole ) refers to one of the word - meanings , it should be labeled as such .", "entities": []}, {"text": "For example , Quitobaquito Springs label should refer to a natural source of water .", "entities": []}, {"text": "3 .", "entities": []}, {"text": "If the target word is part of a name different from the original word - meaning , it should be tagged as Part of Name .", "entities": []}, {"text": "This includes song names , companies ( Cold Spring Ice ) , restaurants etc .", "entities": []}, {"text": "Possible exceptions for this case are when a speci\ufb01c named entity is signi\ufb01cantly frequent .", "entities": []}, {"text": "Step 2:16 13For ease of use for future evaluators , at the end of this step , both annotators picked a single naming convention when two labels referred to the same sense .", "entities": []}, {"text": "Names of labels that were used only by one annotator were not changed .", "entities": []}, {"text": "14Some of the dissimilarities between the annotations are with respect the tension between the second and third guidelines .", "entities": []}, {"text": "15As opposed to Babelfy , there was no attempt for entity linking , so all persons were tagged the same .", "entities": [[10, 12, "TaskName", "entity linking"]]}, {"text": "16This step is presented to annotators once step 1 is done4750", "entities": []}, {"text": "For each of the target words you labeled , you will now receive a short list of indirect wordmeaning de\ufb01nitions .", "entities": []}, {"text": "Indirect word - meanings are composed of : ( a ) A list of 10 words that may appear instead of the target word in speci\ufb01c contexts ( b ) A list of 5 sentences in which the target word has this speci\ufb01c word - meaning .", "entities": []}, {"text": "For example , this is a possible indirect wordmeaning for the target word \u201c Apple \" , representing the fruit , as opposed to the tech company : Alternatives : orange , olive , cherry , lime , banana , emerald , lemon , tomato , oak , arrow , Sentences in which Apple appears in this word - meaning : \u201c He and his new bride planted apple trees to celebrate their marriage . \"", "entities": []}, {"text": "\u201c While visiting , Luther offers Alice an apple . \"", "entities": []}, {"text": "\u201c When she picks the apple up , it is revealed that Luther has stolen a swipe card and given it to Alice to help her escape . \"", "entities": []}, {"text": "You will be asked to label the indirect wordmeanings with one of the labels you used in step 1 .", "entities": []}, {"text": "If no label matches the indirect word - meaning you are allowed to propose a new label or de\ufb01ne it to be \u201c Unknown \" .", "entities": []}, {"text": "Additionally , if you \ufb01nd several indirect word - meanings too close in meaning , label them the same .", "entities": []}, {"text": "C Analysis of Manual Evaluation In table", "entities": []}, {"text": "6 we report a by - word analysis of our manual evaluation results .", "entities": []}, {"text": "For each word we detail F1 scores of the most frequent sense ( MFS ) , Babelfy , and our proposed system .", "entities": [[5, 6, "MetricName", "F1"]]}, {"text": "Similarly to Loureiro et", "entities": []}, {"text": "al . ( 2021 ) , we report the ratio of the \ufb01rst sense with respect to the rest ( F2R ) and normalized entropy17to re\ufb02ect sense balance .", "entities": []}, {"text": "All of which are reported per annotator .", "entities": []}, {"text": "Analysis Analysis of our system \u2019s error shows that for some words the system could not create a matching cluster for speci\ufb01c senses ( to name a few examples , \" yard \" as a ship identi\ufb01er and \" impound / enclosure \" sense for the word \" pound \" ) .", "entities": []}, {"text": "It appears that a matching cluster was not created due to the low tally of these senses in the English Wikipedia , and indeed the two senses appeared only two and three times respectively in the 100 for all words 17Computed as\u0000Pk i=1ci nlogci n log(k ) , where kis the number of annotated senses , each of size ciandnis the size of annotated examples per word , in our case n= 100 .passages sample .", "entities": []}, {"text": "Additionally , annotator 2 annotated in a more \ufb01ne - grained manner that does not correspond to our system tendency to merge capitalized instances of the target word into a sense that corresponds to \" part of named entity \" .", "entities": []}, {"text": "As described above , in rare cases our system merged two senses into a single cluster .", "entities": []}, {"text": "For example , the same cluster of the word \" trunk \" contained occurrences which annotator 1 tagged either \" human torso \" or \" tube - like organs \" ( like the pulmonary trunk ) .", "entities": []}, {"text": "While such annotation was uncommon ( 3 out of 117 senses for annotator 1 and 5 out of 149 senses for annotator 2 ) , it does affect our system \u2019s micro F1 score for the better .", "entities": [[31, 33, "MetricName", "micro F1"]]}, {"text": "In case we do not allow such annotation our overall score drops from 87:52to86:65 .", "entities": []}, {"text": "A comparison between Babelfy and our gold annotation shows a common mistake in its labeling where Babelfy attributes the vast majority of sentences to the same non - salient sense .", "entities": []}, {"text": "For example , Babelfy attributes 77 out of 100 instances of hood to \" An aggressive and violent young criminal \" - a sense that was not found even once in the manual annotation .", "entities": []}, {"text": "While in a number of cases Babelfy used \ufb01ner - grained sysnset groups than in our annotations we took into account any senses that are a subset of our annotated senses .", "entities": []}, {"text": "For examples , Babelfy \u2019s \" United States writer who lived in Europe ; strongly in\ufb02uenced the development of modern literature ( 1885 - 1972 ) \" synset was attribute any instances from the senses surname that refer to the writer Ezra Pound .", "entities": []}, {"text": "D Outlier Detection Method When using a single - prototype vector - space models , Camacho - Collados and Navigli ( 2016 ) proposed a procedure for detecting outliers based on semantic similarity using compactness score : c(w ) = 1 n2\u0000nX wi2WnfwgX wj2Wnfwg wi6 = wjsim(wi;wj )", "entities": [[1, 3, "TaskName", "Outlier Detection"], [31, 33, "TaskName", "semantic similarity"]]}, {"text": "WhereDis the entire dataset and Wis de\ufb01ned asfw1;w2;\u0001\u0001\u0001;wn;wn+1gwhere w.l.o.g .", "entities": []}, {"text": "fw1;w2;\u0001\u0001\u0001;wngare the group elements ( including the distractor ) and wn+1is the outlier .", "entities": []}, {"text": "We use the same procedure with an additional nuance , we expanded the procedure to receive more than a single vector representation per word such that it will \ufb01t multi - prototype embeddings ( e.g. our embeddings and DeConf ) and case sensitive embeddings4751", "entities": []}, {"text": "Annotator # 1 Annotator # 2 Word MFS Babelfy Ours F2R Ent .", "entities": []}, {"text": "MFS Babelfy Ours F2R Ent .", "entities": []}, {"text": "Apple 48 69 94 0.92 0.71 47 66 86 0.89 0.05 Arm 34 31 89 0.52 0.87 34 33 85 0.52 0.83 Bank 48 61 94 0.92 0.78 46 61 85 0.85 0.69 Bass 61 6 82 1.56 0.64 65 17 83 1.86 0.62 Bow 31 14 80 0.45 0.80 32 16 80 0.47 0.83 Chair 66 29 90 1.94 0.66 67 31 86 2.03 0.63 Club 49 45 80 0.96 0.78 53 50 77 1.13 0.72 Crane 39 36 86 0.64 0.90 39 35 83 0.64 0.69 Deck 45 49 72 0.82 0.80 48 52 71 0.92 0.68 Digit 87 96 99 6.69 0.56 87 96 98 6.69 0.38", "entities": []}, {"text": "Hood 27 6 82 0.37 0.88 28 5 82 0.39 0.83", "entities": []}, {"text": "Java 63 32 98 1.70 0.67 63 31 97 1.70 0.69 Mole 37 32 90 0.59 0.81 39 32 88 0.64 0.73 Pitcher 95 97 97 19.00 0.20 95 97 97 19.00 0.20 Pound 46 58 91 0.85 0.75 46 58 91 0.85 0.72 Seal 30 48 88 0.43 0.91 27 40 74 0.37 0.80 Spring 57 0 90 1.33 0.63 56 0 88 1.27 0.64 Square 37 15 88 0.59 0.86 36 15 85 0.56 0.82 Trunk 33 46 98 0.49 0.90 33 46 92 0.49 0.86 Yard 58 60 93 1.38 0.63 57 58 91 1.33 0.59 Average 49.55 41.5 89.05 2.11 0.74 49.9 41.95 85.95 2.13 0.65 Table 6 : Manually annotated set scores by annotator .", "entities": [[57, 58, "DatasetName", "0"], [62, 63, "DatasetName", "0"]]}, {"text": "The \ufb01rst three columns for each annotator re\ufb02ect disambiguation and induction scores with respect to the most frequent sense , Babelfy and our proposed system .", "entities": []}, {"text": "We also report F2R and normalized entropy ( Ent ) .", "entities": []}, {"text": "( e.g.word2vec ) .", "entities": []}, {"text": "When given as set of words ( like Wnfwgwhen calculating c(w ) )", "entities": []}, {"text": "we \ufb01rst \ufb01nd the relevant sense for each element before inferring the outlier .", "entities": []}, {"text": "Camacho - Collados and Navigli ( 2016 ) suggested calculating c(w)using the pseudo inverted compactness score .4752", "entities": []}]