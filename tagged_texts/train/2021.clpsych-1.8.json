[{"text": "Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology , pages 81\u201386 June 11 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics81Determining a Person \u2019s Suicide Risk by Voting on the Short - Term History of Tweets for the CLPsych 2021 Shared Task Ulya Bayram Department of Electronics Engineering \u00c7anakkale Onsekiz Mart University \u00c7anakkale , Turkey ulya.bayram@comu.edu.trLamia Benhiba IAD Department , ENSIAS , Mohammed V University in Rabat Rabat , Morocco lamia.benhiba@um5.ac.ma", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "In this shared task , we accept the challenge of constructing models to identify Twitter users who attempted suicide based on their tweets 30 and 182 days before the adverse event \u2019s occurrence .", "entities": []}, {"text": "We explore multiple machine learning and deep learning methods to identify a person \u2019s suicide risk based on the short - term history of their tweets .", "entities": []}, {"text": "Taking the real - life applicability of the model into account , we make the design choice of classifying on the tweet level .", "entities": []}, {"text": "By voting the tweet - level suicide risk scores through an ensemble of classi\ufb01ers , we predict the suicidal users 30 - days before the event with an 81.8 % true - positives rate .", "entities": []}, {"text": "Meanwhile , the tweet - level voting falls short on the six - month - long data as the number of tweets with weak suicidal ideation levels weakens the overall suicidal signals in the long term .", "entities": []}, {"text": "1 Introduction Suicide is amongst the most pressing public health issues facing today \u2019s society , stressing the need for rapid and effective detection tools .", "entities": []}, {"text": "As people are increasingly self - expressing their distress on social media , an unprecedented volume of data is currently available to detect a person \u2019s suicide risk ( Roy et al . , 2020 ; Tadesse et al . , 2020 ; Luo et al . , 2020 ) .", "entities": []}, {"text": "In this shared task , we aim to construct tools to identify suicidal Twitter users ( who attempted suicide ) based on their tweets collected from spans of 30 - days ( subtask 1 ) and six months ( subtask 2 ) before the adverse event \u2019s occurrence date ( Macavaney et al . , 2021 ) .", "entities": []}, {"text": "The small number of users in the labeled collections of subtask 1 ( 57 suicidal/57 control ) and subtask 2 ( 82 suicidal/82 control ) and the scarcity of tweets for some users pose these tasks as small - dataset classi\ufb01cation challenges .", "entities": []}, {"text": "On that note , Coppersmith et al .", "entities": []}, {"text": "( 2018 ) reported high performance with deep learning ( DL ) methods on these collections after enriching them with additional data ( 418 suicidal/418 control).When formulating the strategy to attack the challenge , we were motivated by the real - life applicability of the methods .", "entities": []}, {"text": "Some social media domains already started implementing auto - detection tools to prevent suicide ( Ji et al . , 2020 ) .", "entities": []}, {"text": "These tools continuously monitor the presence of suicide risk in new posts .", "entities": []}, {"text": "Therefore , we chose to train the models at the tweet level .", "entities": []}, {"text": "Next , we develop a majority voting scheme over the classi\ufb01ed tweets to report an overall suicide risk score for a user .", "entities": []}, {"text": "We employ simple machine learning ( ML ) methods and create an ensemble .", "entities": []}, {"text": "We also experiment with DL methods to assess whether complexity would improve the results .", "entities": []}, {"text": "Since successful ML applications thrive on feature engineering ( Domingos , 2012 ) , we conduct feature selection to evaluate and determine the best feature sets for the models .", "entities": [[6, 8, "TaskName", "feature engineering"], [16, 18, "MethodName", "feature selection"]]}, {"text": "Our experiments suggest that majority voting ( MV ) over tweet - level classi\ufb01cation scores is a viable approach for the short - term prediction of suicide risk .", "entities": []}, {"text": "We observe that DL methods require plentiful resources despite the small size of the datasets .", "entities": []}, {"text": "Simple ML methods with feature selection return satisfactory results , and the performance further improves by the ensemble classi\ufb01er .", "entities": [[4, 6, "MethodName", "feature selection"]]}, {"text": "We also observe that the MV approach falls short on the six - month - long data regardless of the applied model .", "entities": []}, {"text": "Yet this limitation provides the invaluable insight that suicidal ideation signals are more signi\ufb01cant when the date of the suicidal event is closer , which stresses the need for more complex , noise immune models for longer time - spanning data .", "entities": []}, {"text": "In this context , we consider a noise - immune model as a suicidal ideation detection model that is not affected by tweets lacking suicidal ideation .", "entities": []}, {"text": "2 Methods Pre - processing : We clean the tweets by removing user mentions , URLs , punctuation , and non - ASCII characters , then normalize hashtags into words using a probabilistic splitting tool based on English", "entities": []}, {"text": "82Wikipedia unigram frequencies ( Anderson , 2019 ) .", "entities": []}, {"text": "We maintain stopwords and emojis , as they might provide clues regarding the suicidal ideation of the users .", "entities": []}, {"text": "Experimentation Framework : Before designing the experiments , we face a critical choice : Should we merge all tweets per user , or should we perform the assessment per tweet and then aggregate the scores ?", "entities": []}, {"text": "To answer this , we consider a real - life risk assessment system .", "entities": []}, {"text": "The system should provide a score every time someone posts a tweet .", "entities": []}, {"text": "Some social media domains already implement these systems ( Ji et al . , 2020 ) .", "entities": []}, {"text": "Hence , we select to train the models to classify tweets , then apply majority voting ( MV ) per user to compute a risk score based on the tweet scores .", "entities": []}, {"text": "Our framework is described in Figure 1 .", "entities": []}, {"text": "Figure 1 : Classi\ufb01cation framework used to compute person - level risk scores from the tweet - level scores .", "entities": []}, {"text": "Experiments with Standard ML methods : Before ML experiments , we initially explore a simple approach that constructs graphs from training sets and computes how well the given texts match the graphs ( Bayram et al . , 2018 ) .", "entities": []}, {"text": "However , tweets proved to be un\ufb01t for the method due to low word counts .", "entities": []}, {"text": "As most ML methods depend on learning from features , we select n - gram features where n\u22642 for their popularity in suicide studies ( O\u2019Dea et al . , 2015 ; De Choudhury et", "entities": []}, {"text": "al . , 2016 ; Pestian et al . , 2020 ) .", "entities": []}, {"text": "For bigrams ( n= 2 ) , we apply a sliding window over concurrent words using the NLTK library ( Bird et al . , 2009 ) .", "entities": []}, {"text": "Next , we eliminate infrequent n - grams from the training set to reduce uninformative features ( occurring in \u22643 tweets in 30days,\u226410 tweets in 182 - days training sets ) .", "entities": []}, {"text": "Subsequently , we scale the features by row - normalizing them with the root of the sum of the square ( i.e.variation ) of the feature values .", "entities": []}, {"text": "Among the popular ML methods in suicide literature is logistic regression ( LR ) ( Walsh et al . , 2017 ; De Choudhury et", "entities": [[9, 11, "MethodName", "logistic regression"]]}, {"text": "al . , 2016 ; O\u2019Dea et al . , 2015 ) .", "entities": []}, {"text": "We select the \u201c liblinear \u201d solver with default settings for being recommended for small datasets ( Buitinck et al . , 2013 ) .", "entities": []}, {"text": "To cover diverse mathematical frameworks and assumptions , we also include two naive Bayes methods ( Gaussian ( GNB ) and Multinomial ( MNB ) with default settings ) ( Buitinck et al . , 2013 ) .", "entities": []}, {"text": "We also experiment with K - Nearest Neighbors with different distance ( uniform , weighted ) and neighborhood ( k \u2208{3 , 5 , 8 } ) settings , but we eliminate it for low within - dataset results .", "entities": [[4, 8, "MethodName", "K - Nearest Neighbors"]]}, {"text": "Similarly , ensemblelearning methods ( Adaboost , XGBoost , Random Forest ) also return underwhelming performance despite the parameter tuning , and thus , were eliminated .", "entities": []}, {"text": "Additionally , we evaluate support vector machines ( SVM ) for their popularity in suicide research ( Zhu et al . , 2020 ; Pestian et al . , 2020 ; O\u2019Dea et al . , 2015 ) .", "entities": [[8, 9, "MethodName", "SVM"]]}, {"text": "SVM with rbf kernel proves to be successful but requires costly parameter tuning , while linear SVM ( lSVM ) shows success on withindataset evaluations with less cost .", "entities": [[0, 1, "MethodName", "SVM"], [16, 17, "MethodName", "SVM"]]}, {"text": "Consequently , we select lSVM of sklearn ( default settings ) for the shared task ( Buitinck et al . , 2013 ) , which returns only binary classi\ufb01cation results .", "entities": []}, {"text": "To convert them to probabilities , we apply probability calibration with logistic regression ( CalibratedClassi\ufb01erCV ) .", "entities": [[11, 13, "MethodName", "logistic regression"]]}, {"text": "Feature selection : Following the ML method selections , we evaluate the effect of feature selection on ML performance .", "entities": [[0, 2, "MethodName", "Feature selection"], [14, 16, "MethodName", "feature selection"]]}, {"text": "To compute feature importance scores , we also use the LR .", "entities": [[2, 4, "TaskName", "feature importance"]]}, {"text": "For each selected number of features , we gather top suicidal and control features .", "entities": []}, {"text": "Next , we train and evaluate the ML methods in a leave - one - out ( LOO ) framework using those features .", "entities": []}, {"text": "The feature selection results of the selected ML methods for two subtasks are in Figure 2 .", "entities": [[1, 3, "MethodName", "feature selection"]]}, {"text": "We select the best ML models from these plots .", "entities": []}, {"text": "Experiments with Ensemble : Ensemble classi\ufb01ers previously showed success in ML challenges ( Niculescu - Mizil et al . , 2009 ) .", "entities": []}, {"text": "Since every classi\ufb01er renders predicted probabilities for every data point , we build an ensemble classi\ufb01er to optimize the results of four selected ML methods ( LR , GNB , MNB , lSVM ) .", "entities": []}, {"text": "We adopt a weighting ensemble method where the weight of each classi\ufb01er is set proportional to its performance ( Rokach , 2010 ) .", "entities": []}, {"text": "We call this method weighted Ensemble ( wEns ) .", "entities": []}, {"text": "Experiments with DL : To measure whether re-", "entities": []}, {"text": "83 ( a ) Subtask 1 ( b ) Subtask 2 Figure 2 : Feature selection evaluations on the labeled datasets of two subtasks .", "entities": [[14, 16, "MethodName", "Feature selection"]]}, {"text": "sults would improve with complexity , we also evaluate shallow DL methods .", "entities": []}, {"text": "We use the pre - trained transformer model Bert - base - uncased ( Devlin et al . , 2018 ) to catch the linguistics features of the tweets .", "entities": []}, {"text": "The embeddings are then fed to a DL Recurrent Units - based architecture to learn text sequence orders .", "entities": []}, {"text": "We experiment with two types of recurrent neural networks ( RNNs ): Long Short Term Memory ( LSTM ) ( Gers et al . , 1999 ) , and Gated Recurrent Unit ( GRU ) known for overcoming vanishing and exploding gradient problems faced by vanilla RNNs during training ( Cho et al . , 2014 ) .", "entities": [[17, 18, "MethodName", "LSTM"], [29, 32, "MethodName", "Gated Recurrent Unit"], [33, 34, "MethodName", "GRU"]]}, {"text": "After assessing various con\ufb01gurations of both architectures , we settle on a multi - layer bi - directional GRU with the following characteristics : embedding dimension=256 , number of layers=2 , batch size=32 .", "entities": [[18, 19, "MethodName", "GRU"]]}, {"text": "We call this model GRU - Bert .", "entities": [[4, 5, "MethodName", "GRU"]]}, {"text": "We include a drop - out to regularise learning and a fully connected layer with a Sigmoid activation to produce the classi\ufb01cation for each tweet .", "entities": [[16, 18, "MethodName", "Sigmoid activation"]]}, {"text": "Finally , we include the same majority voting framework to infer the classi\ufb01cation on the user level .", "entities": []}, {"text": "We use Pytorch ( Paszke et al . , 2019 ) and scikit - learn ( Buitinck et al . , 2013 ) libraries for implementation .", "entities": []}, {"text": "3 Results Before training each classi\ufb01er , we employ the best performing top features from the Figure 2 , where every classi\ufb01er has its most \ufb01tting top features for each subtask .", "entities": []}, {"text": "Next , we construct a LOO crossvalidation framework for within - dataset evaluations.1It is important to note that , in each step of the LOO , we choose new user ids for evaluation and completely exclude all of their tweets from the training sets to evade ML methods potentially learning the way a person drafts tweets .", "entities": []}, {"text": "That means the within - dataset LOO results of a subtask are reported for all users of the labeled set .", "entities": []}, {"text": "Moreover , the labeled datasets have more users than the unlabeled test sets per subtask ( e.g. 57 vs. 11 suicidal users in subtask1 ) .", "entities": []}, {"text": "Ergo , we expect a high magnitudinal difference between the within - dataset and the test results .", "entities": []}, {"text": "Table 1 : Within - dataset evaluation results .", "entities": []}, {"text": "F1 F2 TPR FPR AUC Subtask 1 : ( 30 days ) LR 78.0 81.6 84.2 31.6 80.8 GNB 81.2 88.8 94.7 38.6 89.3 MNB 83.1 84.8 86.0 21.0 86.8 lSVM 81.9 87.2 91.2 31.6 88.6 wEns 85.0 90.6 94.7 28.1 93.2", "entities": [[0, 1, "MetricName", "F1"], [4, 5, "MetricName", "AUC"]]}, {"text": "GRU - Bert 81.2 82.2 83.1 21.7 84.0 Subtask 2 : ( 6 months ) LR 81.9 83.9 85.4 23.2 85.5 GNB 69.6 83.0 95.1 78.0 81.5 MNB 75.7 77.1 78.0 28.0 82.8 lSVM 78.6 87.1 93.9 45.1 84.6 wEns 81.7 88.0 92.7 34.1 88.5 GRU - Bert 74.5 75.4 76.0 28.6 77.5 The within - dataset evaluation results of the selected methods are in Table 1 .", "entities": [[0, 1, "MethodName", "GRU"], [45, 46, "MethodName", "GRU"]]}, {"text": "For subtask 1 , we obtain the best LOO cross - validation score from the wEns method that combines the results of four ML methods ( LR , MNB , GNB , lSVM ) in a way that improves the results obtained from each of them .", "entities": []}, {"text": "Meanwhile , GRU - Bert and MNB return the lowest false positive rates ( FPR ) for this subtask , 1Within - dataset evaluation results of the selected ML and weighted ensemble methods are obtained from LOO crossvalidation .", "entities": [[2, 3, "MethodName", "GRU"]]}, {"text": "While for GRU - Bert , collections were split into training - validation - test sets in 70:10:20 ratios .", "entities": [[2, 3, "MethodName", "GRU"]]}, {"text": "84which might be a critical rate to consider in real - life applications in social media domains .", "entities": []}, {"text": "LOO results of subtask 2 in Table 1show that wEns returns the best scores for the longer - spanning dataset as well , where LR returns the best FPR , and GBN returns the highest true positives rate ( TPR ) .", "entities": []}, {"text": "Table 2 : Test results over unlabeled data and the results from the baseline method of CLPsych2021 .", "entities": []}, {"text": "F1 F2 TPR FPR AUC Subtask 1 : ( 30 days ) Baseline 63.6 63.6 63.6 36.4 66.1 LR 63.6 63.6 63.6 36.4 74.0 wEns 69.2 76.3 81.8 54.5 70.2", "entities": [[0, 1, "MetricName", "F1"], [4, 5, "MetricName", "AUC"]]}, {"text": "Subtask", "entities": []}, {"text": "2 : ( 6 months )", "entities": []}, {"text": "Baseline 71.0 72.4 73.3 33.3 76.4 LR 64.5 65.8 66.7 40.0 56.9 wEns 59.5 67.1 73.3 73.3 58.2 Based on the LOO results , we select three different methods we were allowed to submit for the evaluation of the test set : LR , wEns , and GRU - Bert .", "entities": [[47, 48, "MethodName", "GRU"]]}, {"text": "We choose LR and wEns for their high performance on LOO experiments , while we select GRU - Bert for measuring how a DL method would generalize over the test sets .", "entities": [[16, 17, "MethodName", "GRU"]]}, {"text": "The baseline classi\ufb01er provided by the organizers is also a logistic regression .", "entities": [[10, 12, "MethodName", "logistic regression"]]}, {"text": "However , it performs the classi\ufb01cation over merged tweets of users - therefore is different from our implementation of LR .", "entities": []}, {"text": "In Table 2 , wEns appears to provide the best F1 , F2 , and TPR scores over the test set of subtask 1 , while our LR outperforms the AUC of the baseline method .", "entities": [[10, 11, "MetricName", "F1"], [30, 31, "MetricName", "AUC"]]}, {"text": "While these methods show the success of generalizability on the 30 - days test set , the results are not that successful for subtask 2 .", "entities": []}, {"text": "The wEns method performs the same as the baseline in terms of TPR , but the rest of the scores are lower than the baseline results .", "entities": []}, {"text": "4 Discussion In subtask 1 , the test set results show that feature selection can considerably enhance the performance of ML models compared to the baseline .", "entities": [[12, 14, "MethodName", "feature selection"]]}, {"text": "We also \ufb01nd that the ensemble classi\ufb01er is comparably better than the baseline in this subtask .", "entities": []}, {"text": "Meanwhile , though the baseline of CLPsych2021 is the same as our LR , our additional MV and feature selection together enable LR to substantially outperform the baseline .", "entities": [[18, 20, "MethodName", "feature selection"]]}, {"text": "These successes of simple ML methods indicate that a collection of tweets from withinthe 30 - days of a suicidal event is good enough to capture the existence of suicidal ideation , which is an important \ufb01nding for future real - life suicide prevention applications .", "entities": []}, {"text": "In contrast to the observations from subtask 1 , our test results on subtask 2 are unsatisfactory .", "entities": []}, {"text": "Yet , they provide the valuable insight that suicidal signals are more signi\ufb01cant in the short - term , and older tweets lacking suicidal ideation generate noise .", "entities": []}, {"text": "This insight suggests the need to account for a time - domain aspect .", "entities": []}, {"text": "To investigate the viability of this claim , we experiment with a simple time - decay coef\ufb01cient in the MV framework and evaluate it through LR on the test set .", "entities": []}, {"text": "We multiply each vote by the coef\ufb01cient 2\u2212timeDiff halfLife where timeDiff is the number of days between the current and last tweets , and halfLife (= 7 days ) is a hyperparameter that re\ufb02ects the weight of a vote in the \ufb01nal suicide risk score of a user .", "entities": []}, {"text": "Initial experiments show that even this simple time - decay coef\ufb01cient improves the test results signi\ufb01cantly .", "entities": []}, {"text": "This observation suggests that tweet dates are critical features for this subtask and should be included in future work .", "entities": []}, {"text": "Notwithstanding , on both subtasks , the shallow DL methods we experimented with perform poorly .", "entities": []}, {"text": "These results could be attributed to over\ufb01tting on the small dataset and noise sensitivity for the larger time - spanning dataset .", "entities": []}, {"text": "Additionally , regardless of the dataset size , these methods proved to be computationally expensive .", "entities": []}, {"text": "As within - dataset experiments using simple ML methods outperformed these expensive shallow DL methods , we excluded the latter from the test set evaluation .", "entities": []}, {"text": "Future work on DL will include deeper , more complex , and noise immune methods that could integrate Convolutional neural networks ( CNN ) , deeper LSTM or GRU layers , and experiments with various word embedding models .", "entities": [[26, 27, "MethodName", "LSTM"], [28, 29, "MethodName", "GRU"]]}, {"text": "If we compare our \ufb01ndings with those in Coppersmith et al .", "entities": []}, {"text": "( 2018 ) , we observe different results in terms of short - term versus long - term dataset classi\ufb01cations .", "entities": []}, {"text": "We attribute these different outcomes to the fact that the original study optimizes the design for detecting trait - level ( relevant to risk for any point in time ) suicide risk when we endeavor to identify suicidal ideation at the state level ( immediate risk presence ) .", "entities": []}, {"text": "This design choice , along with tweet - level classi\ufb01cation , enabled our model to recognize suicidal nuances in short - term tweets .", "entities": []}, {"text": "Meanwhile , we were unable to detect any suicidal", "entities": []}, {"text": "85ideation through manual inspection ( reading and interpreting the tweets ) over most of these tweets due to their noisy and ambiguous nature .", "entities": []}, {"text": "5 Conclusion In this shared task , we investigate various models for identifying suicide risk based on user \u2019s tweets .", "entities": []}, {"text": "Inspired by real - life applications , we focus on assessing suicide risk on the tweet level .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "Experimental results reveal that the ensemble classi\ufb01er can identify suicidal users from 30 - days tweets with a high performance rate , demonstrating the power of majority voting over tweet - level classi\ufb01cations for short - term suicide risk detection .", "entities": []}, {"text": "Meanwhile , we construe from the underwhelming results on the six - month dataset that these models were more sensitive to the signals relevant to short term risk than those relevant to long term risk .", "entities": []}, {"text": "In future work , we will incorporate a temporal aspect to improve the noise immunity of our models , and we will continue experimenting with more complex models .", "entities": []}, {"text": "Ethics Statement Secure access to the shared task dataset was provided with IRB approval under University of Maryland , College Park protocol 1642625 .", "entities": [[0, 1, "DatasetName", "Ethics"]]}, {"text": "Acknowledgements", "entities": []}, {"text": "The organizers are particularly grateful to the users who donated data to the OurDataHelps project without whom this work would not be possible , to Qntfy for supporting the OurDataHelps project and making the data available , to NORC for creating and administering the secure infrastructure , and to Amazon for supporting this research with computational resources on AWS .", "entities": []}, {"text": "The authors are thankful to the anonymous reviewers for their constructive comments and valuable suggestions .", "entities": []}, {"text": "References Derek Anderson .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "wordninja Python library.https://github.com/keredson/ wordninja .", "entities": []}, {"text": "[ Online ; accessed 11 - March-2021 ] .", "entities": []}, {"text": "Ulya Bayram , Ali A Minai , and John Pestian .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A lexical network approach for identifying suicidal ideation in clinical interview transcripts .", "entities": []}, {"text": "In International Conference on Complex Systems , pages 165 \u2013 172 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Steven Bird , Ewan Klein , and Edward Loper .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Natural language processing with Python : analyzing text with the natural language toolkit . \"", "entities": []}, {"text": "O\u2019Reilly Media , Inc. \" .", "entities": []}, {"text": "Lars Buitinck , Gilles Louppe , Mathieu Blondel , Fabian Pedregosa , Andreas Mueller , Olivier Grisel , Vlad Niculae , Peter Prettenhofer , Alexandre Gramfort , Jaques Grobler , Robert Layton , Jake VanderPlas , Arnaud Joly , Brian Holt , and Ga\u00ebl Varoquaux .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "API design for machine learning software : experiences from the scikit - learn project .", "entities": []}, {"text": "In ECML PKDD Workshop : Languages for Data Mining and Machine Learning , pages 108\u2013122 .", "entities": []}, {"text": "Kyunghyun Cho , Bart Van Merri\u00ebnboer , Dzmitry Bahdanau , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "On the properties of neural machine translation : Encoder - decoder approaches .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1409.1259 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Glen Coppersmith , Ryan Leary , Patrick Crutchley , and Alex Fine .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Natural language processing of social media as screening for suicide risk .", "entities": []}, {"text": "Biomedical informatics insights , 10:1178222618792860 .", "entities": []}, {"text": "Munmun De Choudhury , Emre Kiciman , Mark Dredze , Glen Coppersmith , and Mrinal Kumar . 2016 .", "entities": [[15, 16, "DatasetName", "Kumar"]]}, {"text": "Discovering shifts to suicidal ideation from mental health content in social media .", "entities": []}, {"text": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems , pages 2098\u20132110 . ACM .", "entities": [[17, 18, "DatasetName", "ACM"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Pedro Domingos .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "A few useful things to know about machine learning .", "entities": []}, {"text": "Communications of the ACM , 55(10):78\u201387 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Felix A Gers , J\u00fcrgen Schmidhuber , and Fred Cummins .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Learning to forget : Continual prediction with lstm .", "entities": [[7, 8, "MethodName", "lstm"]]}, {"text": "In 9th International Conference on Arti\ufb01cial Neural Networks : ICANN \u2019 99 , pages 850\u2013855 .", "entities": []}, {"text": "IET .", "entities": []}, {"text": "Shaoxiong Ji , Shirui Pan , Xue Li , Erik Cambria , Guodong Long , and Zi Huang . 2020 .", "entities": []}, {"text": "Suicidal ideation detection : A review of machine learning methods and applications .", "entities": []}, {"text": "IEEE Transactions on Computational Social Systems .", "entities": []}, {"text": "Jianhong Luo , Jingcheng Du , Cui Tao , Hua Xu , and Yaoyun Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Exploring temporal suicidal behavior patterns on social media : Insight from twitter analytics .", "entities": []}, {"text": "Health informatics journal , 26(2):738 \u2013 752 .", "entities": []}, {"text": "Sean Macavaney , Anjali Mittu , Glen Coppersmith , Jeff Leintz , and Philip Resnik . 2021 .", "entities": []}, {"text": "Community - level research on suicidality prediction in a secure environment : Overview of the CLPsych 2021 shared task .", "entities": []}, {"text": "In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology ( CLPsych 2021 ) .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "86Alexandru Niculescu - Mizil , Claudia Perlich , Grzegorz Swirszcz , Vikas Sindhwani , Yan Liu , Prem Melville , Dong Wang , Jing Xiao , Jianying Hu , Moninder Singh , et al . 2009 .", "entities": []}, {"text": "Winning the kdd cup orange challenge with ensemble selection .", "entities": []}, {"text": "In KDD - Cup 2009 Competition , pages 23\u201334 . PMLR .", "entities": []}, {"text": "Bridianne O\u2019Dea , Stephen Wan , Philip J Batterham , Alison L Calear , Cecile Paris , and Helen Christensen . 2015 .", "entities": [[18, 19, "DatasetName", "Helen"]]}, {"text": "Detecting suicidality on twitter .", "entities": []}, {"text": "Internet Interventions , 2(2):183\u2013188 .", "entities": []}, {"text": "Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , Alban Desmaison , Andreas Kopf , Edward Yang , Zachary DeVito , Martin Raison , Alykhan Tejani , Sasank Chilamkurthy , Benoit Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . 2019 .", "entities": [[0, 1, "MethodName", "Adam"], [9, 10, "MethodName", "Adam"]]}, {"text": "Pytorch :", "entities": []}, {"text": "An imperative style , high - performance deep learning library .", "entities": []}, {"text": "In H. Wallach , H. Larochelle , A. Beygelzimer , F. d'Alch\u00e9 - Buc , E. Fox , and R. Garnett , editors , Advances in Neural Information Processing Systems 32 , pages 8024\u20138035 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "John Pestian , Daniel Santel , Michael Sorter , Ulya Bayram , Brian Connolly , Tracy Glauser , Melissa DelBello , Suzanne Tamang , and Kevin Cohen . 2020 .", "entities": []}, {"text": "A machine learning approach to identifying changes in suicidal language .Suicide and Life - Threatening Behavior , 50(5):939\u2013947 .", "entities": []}, {"text": "Lior Rokach .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Ensemble - based classi\ufb01ers .", "entities": []}, {"text": "Arti\ufb01cial intelligence review , 33(1):1\u201339 .", "entities": []}, {"text": "Arunima Roy , Katerina Nikolitch , Rachel McGinn , Sa\ufb01ya Jinah , William Klement , and Zachary A Kaminsky .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A machine learning approach predicts future risk to suicidal ideation from social media data .", "entities": []}, {"text": "NPJ digital medicine , 3(1):1\u201312 .", "entities": []}, {"text": "Michael Mes\ufb01n Tadesse , Hongfei Lin , Bo Xu , and Liang Yang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Detection of suicide ideation in social media forums using deep learning .", "entities": []}, {"text": "Algorithms , 13(1):7 .", "entities": []}, {"text": "Colin G Walsh , Jessica D Ribeiro , and Joseph C Franklin .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Predicting risk of suicide attempts over time through machine learning .", "entities": []}, {"text": "Clinical Psychological Science , 5(3):457\u2013469 .", "entities": []}, {"text": "H Zhu , X Xia , J Yao , H Fan , Q Wang , and Q Gao .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Comparisons of different classi\ufb01cation algorithms while using text mining to screen psychiatric inpatients with suicidal behaviors .", "entities": []}, {"text": "Journal of psychiatric research , 124:123\u2013130 .", "entities": []}]