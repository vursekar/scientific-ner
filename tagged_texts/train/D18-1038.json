[{"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 414\u2013424 Brussels , Belgium , October 31 - November 4 , 2018 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics414XL - NBT : A Cross - lingual Neural Belief Tracking Framework Wenhu Chen1 , Jianshu Chen2 , Yu Su3 , Xin Wang1 , Dong Yu2", "entities": []}, {"text": "Xifeng Yan1and William Wang1 University of California , Santa Barbara , CA , USA1 Tencent AI Lab , Bellevue , WA , USA2 The Ohio State University , Columbus , Ohio3 fwenhuchen , xwang , xyan , william g@cs.ucsb.edu , su.809@osu.edu jianshuchen@tencent.com , dongyu@ieee.org Abstract Task - oriented dialog systems are becoming pervasive , and many companies heavily rely on them to complement human agents for customer service in call centers .", "entities": []}, {"text": "With globalization , the need for providing cross - lingual customer support becomes more urgent than ever .", "entities": []}, {"text": "However , cross - lingual support poses great challenges \u2014 it requires a large amount of additional annotated data from native speakers .", "entities": []}, {"text": "In order to bypass the expensive human annotation and achieve the \ufb01rst step towards the ultimate goal of building a universal dialog system , we set out to build a cross - lingual state tracking framework .", "entities": []}, {"text": "Speci\ufb01cally , we assume that there exists a source language with dialog belief tracking annotations while the target languages have no annotated dialog data of any form .", "entities": []}, {"text": "Then , we pre - train a state tracker for the source language as a teacher , which is able to exploit easy - to - access parallel data .", "entities": []}, {"text": "We then distill and transfer its own knowledge to the student state tracker in target languages .", "entities": []}, {"text": "We speci\ufb01cally discuss two types of common parallel resources : bilingual corpus and bilingual dictionary , and design different transfer learning strategies accordingly .", "entities": [[19, 21, "TaskName", "transfer learning"]]}, {"text": "Experimentally , we successfully use English state tracker as the teacher to transfer its knowledge to both Italian and German trackers and achieve promising results .", "entities": []}, {"text": "1 Introduction Over the past few years , we have witnessed the burgeoning of real - world applications of dialog systems , with many academic , industrial , and startup efforts racing to lead the widely - believed next - generation human - machine interfaces .", "entities": []}, {"text": "As a result , numerous task - oriented dialog systems such as virtual assistants and customer conversation services were developed ( Wen et al . , 2015 ; Rojas - Barahona et", "entities": []}, {"text": "al . , 2017 ; Bordes and Weston,2017 ; Williams et al . , 2017 ; Li et al . , 2017 ) , with Google Duplexbeing the most recent example .", "entities": [[25, 26, "DatasetName", "Google"]]}, {"text": "With the rapid process of globalization , more countries have observed growing populations of immigrants , and more companies have moved forward to develop their overseas business sectors .", "entities": []}, {"text": "To provide better customer service and bring down the cost of labor at call centers , the development of universal dialog systems has become a practical issue .", "entities": []}, {"text": "A straightforward strategy is to separately collect training data and train dialog systems for each language .", "entities": []}, {"text": "However , it is not only tedious but also expensive .", "entities": []}, {"text": "Two settings naturally arise for more ef\ufb01cient usage of the training data : ( 1 ) Multi - lingual setting : we annotate data for multiple languages and train a single model , with possible innovations on joint training .", "entities": []}, {"text": "( 2 ) Crosslingual setting : we annotate data and train a model for only one ( popular ) language , and transfer the learned knowledge to other languages .", "entities": []}, {"text": "Here we are interested in the second case , and the important research question we ask is : How can we build cross - lingual dialog systems that can support less popular , low- or even zero - resource languages ?", "entities": []}, {"text": "As an initial step towards cross - lingual dialog systems , we focus on the cornerstone of dialog systems \u2013 dialog state tracking ( DST ) , or belief tracking , a key component for understanding user inputs and updating belief state , i.e. , a system \u2019s internal representation of the state of conversation ( Young et al . , 2010 ) .", "entities": []}, {"text": "Based on the perceived belief state , the dialog manager can decide which action to take , and what verbal response to generate ( Precup and Teh , 2017 ; Bordes and Weston , 2017 ) .", "entities": []}, {"text": "DST models require a considerable amount of annotated data for training ( Henderson et al . , 2014b ; Mrksic et al . , 2015 , 2017 ) .", "entities": []}, {"text": "For a common dialog shown in Figure 1 , a typical data acquisition process ( Rojas - Barahona et al . , 2017 ) not only requires two human users to converse for multiple", "entities": [[31, 32, "DatasetName", "converse"]]}, {"text": "415turns but also requires annotators to identify user \u2019s intention in each turn .", "entities": []}, {"text": "Such two - step annotation is very expensive , especially for rare languages .", "entities": []}, {"text": "We study the novel problem of cross - lingual DST , where one leverages the annotated data of a source language to train DST for a target language with zero annotated data ( Figure 1 ) ; no conversation dialog or dialog state annotation is available for the target language .", "entities": []}, {"text": "In order to deal with this zero - resource challenging scenario , we \ufb01rst decouple the state - of - the - art neural belief tracker framework ( Mrksic et al . , 2017 ) into sub - modules , namely utterance encoder , context gate , and slotvalue decoder .", "entities": []}, {"text": "By introducing a teacher - student framework , we are able to transfer knowledge across languages module by module , following the divide - and - conquer philosophy .", "entities": []}, {"text": "Requiring no target - side dialog data , our method relies on other easy - to - access parallel resources to understand the connection between languages .", "entities": []}, {"text": "Depending on the popularity and availability of target language resources , we study two kinds of parallel data : bilingual corpus and bilingual dictionary , and we respectively design two transfer learning strategies .", "entities": [[30, 32, "TaskName", "transfer learning"]]}, {"text": "We use the popular Wizard - of - Oz ( RojasBarahona et al . , 2017 ) dataset as our DST benchmark to evaluate the effectiveness of our crosslingual transfer learning .", "entities": [[4, 9, "DatasetName", "Wizard - of - Oz"], [29, 31, "TaskName", "transfer learning"]]}, {"text": "We specify English as the source ( primary ) language and two different European languages ( German and Italian ) as our zero - annotation target languages .", "entities": []}, {"text": "Compared with an array of alternative transfer learning strategies , our cross - lingual DST models consistently achieve promising results in both scenarios for both zero - annotation languages .", "entities": [[6, 8, "TaskName", "transfer learning"]]}, {"text": "To ensure reproducibility , we release our code , training data and parallel resources in the github1 .", "entities": []}, {"text": "Our main contributions are three - fold : \u000fTowards building cross - lingual dialog systems , we are the \ufb01rst to study the crosslingual dialog state tracking problem .", "entities": []}, {"text": "\u000fWe systematically study different scenarios for this problem based on the availability of parallel data and propose novel transfer learning methods to tackle the problem .", "entities": [[18, 20, "TaskName", "transfer learning"]]}, {"text": "\u000fWe empirically demonstrate the ef\ufb01cacy of the proposed methods , showing that our methods can accurately track dialog states for 1https://github.com/wenhuchen/ Cross - Lingual - NBTlanguages with zero annotated data .", "entities": []}, {"text": "2 Related Work 2.1 Dialog State Tracking Broadly speaking , the dialog belief tracking algorithms can be divided into three families : 1 ) hand - crafted rules 2 ) generative models , and 3 ) maximum - entropy model ( Metallinou et al . , 2013 ) .", "entities": []}, {"text": "Later on , many deep learning based discriminative models have surged to replace the traditional strategies ( Henderson et al . , 2014a ; Mrksic et al . , 2017 ; Williams et al . , 2016 ) and achieved state - of - the - art results on various datasets .", "entities": []}, {"text": "Though the discriminative models are reported to achieve fairly high accuracy , their applications are heavily restricted by the domain , ontology , and language .", "entities": [[10, 11, "MetricName", "accuracy"], [21, 22, "MethodName", "ontology"]]}, {"text": "Recently , a pointer network based algorithm ( Xu and Hu , 2018 ) and another multi - domain algorithm ( Rastogi et al . , 2017 ) have been proposed to break the ontology and domain boundary .", "entities": [[3, 5, "MethodName", "pointer network"], [34, 35, "MethodName", "ontology"]]}, {"text": "Besides , ( Mrk \u02c7si\u00b4c et al . , 2017 ) has proposed an algorithm to train a uni\ufb01ed framework to deal with multiple languages with annotated datasets .", "entities": []}, {"text": "In contrast , our paper focuses on breaking the language boundary and transfer DST knowledge from one language into other zeroannotation languages .", "entities": []}, {"text": "2.2 Cross - Lingual Transfer Learning Cross - lingual transfer learning has been a very popular topic during the years , which can be seen as a transductive process .", "entities": [[1, 5, "TaskName", "Cross - Lingual Transfer"], [6, 10, "TaskName", "Cross - lingual transfer"]]}, {"text": "In such process , the input domains of the source and target are different ( Pan and Yang , 2010 ) since each language has its own distinct lexicon .", "entities": []}, {"text": "By discovering the underlying connections between the source and target domain , we could design transfer algorithms for different tasks .", "entities": []}, {"text": "Recently , algorithms have been successfully designed for POS tagging ( Zhang et al . , 2016 ;", "entities": []}, {"text": "Kim et al . , 2017 ) , NER ( Pan et al . , 2017 ; Ni et al . , 2017 ) as well as image captioning ( Miyazaki and Shimizu , 2016 ) .", "entities": [[8, 9, "TaskName", "NER"], [27, 29, "TaskName", "image captioning"]]}, {"text": "These methods \ufb01rst aim at discovering the relatedness between two languages and separate languagecommon modules from language - speci\ufb01c modules , then resort to external resources to transfer the knowledge across the language boundary .", "entities": []}, {"text": "Our method addresses the transfer learning using a teacher - student framework and proposes to use the teacher to gradually guide the student to make more proper decisions .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "416 RestaurantPrice(Preis)Food(Essen)Area \t ( Bereich)Location(Ort)Telephone(Telefon)The \t HouseCheap(Billig)Thai(Thail\u00e4ndisch)Center(Center)106 \t Regent \t Street000 - 00000AladdinExpensive(Teuer)Greek(Griechisch)North(Norden)Mesa \t Road \t 100000 - 00000User : \t I \u2019m \t looking \t for \t a \t cheaperRestaurantInform(price = cheap)System : \t Sure , \t What \t kind \t \u2013 and \t where?User : \t Thaifood , \t somewhere \t downtownInform(price = cheap , food = Thai , area = center)System : \t The \t House \t serves \t cheap \t Thai \t foodUser : \t\t Whereis \t it?Inform(price = cheap , food = Thai , area = center)Request(location)System : \t The \t House \t is \t at \t 106 \t Regent \t StreetUser : \t IchsucheeinteuresRestaurantInform(preis= ? )", "entities": []}, {"text": "System : \t Sicher , \t welche \t Art \t und \t wann?User : \t griechischesEssen , \t irgendwo \t i m \t NordenInform(preis=?,essen=?,ort=?)System : \t The \t House \t serviertsehrbilliges \t thail\u00e4ndisches \t Essen \t User : \t\t Woistdas?Inform(preis=?,essen=?,ort=?);Request(?)System : \t Aladdin \t befindetsichin \t der \t Mesa \t RoadTransferEnglish training datasetGerman test datasetFigure 1 : Cross - lingual transfer learning for dialog state tracking , where the underlying database ( the table above ) is shared across languages .", "entities": [[57, 61, "TaskName", "Cross - lingual transfer"]]}, {"text": "The source language has annotated dialogs and the ground truth states , but the target language has neither dialogs nor ground truth states ( only a testing dataset for evaluation ) .", "entities": []}, {"text": "3 Problem De\ufb01nition Figure 2 : Cross - lingual DST structure , the ontology and database between multiple languages are shared .", "entities": [[13, 14, "MethodName", "ontology"]]}, {"text": "The dialog states are de\ufb01ned as a set of search constraints ( i.e. informable slots or goals ) that the user speci\ufb01ed through the dialog and a set of attribute questions regarding the search results ( i.e. requestable slots or requests ) .", "entities": []}, {"text": "The objective of dialog state tracking ( DST ) is to predict and track the user intention ( i.e. , the values of the aforementioned slots ) at each time step based on the current user utterance and the entire dialog history .", "entities": []}, {"text": "As shown in Figure 2 , for each slot , the DST computes an output distribution of the candidate values using three inputs : ( i ) system response at , which is the sentence generated by the system , ( ii ) utteranceut , which is the sentence from the user , and ( iii ) previous state , which denotes the selected slot - value pairs .", "entities": []}, {"text": "We de\ufb01ne the ontology of the dialog system to be the set of all the possible words the dialog slot and value can take .", "entities": [[3, 4, "MethodName", "ontology"]]}, {"text": "In this paper , we are interested in learning a cross - lingual DST .", "entities": []}, {"text": "Speci\ufb01cally , we assume that the DST for the source language has access to a human - annotated training dataset Dwhile the DSTs for the targetlanguages do not have access to annotated data in other languages except for testing data .", "entities": []}, {"text": "We here mainly consider two different types of parallel resources to assist the transfer learning : ( 1 ) Bilingual Corpus , where abundant bilingual corpora exist between the source and the target languages .", "entities": [[13, 15, "TaskName", "transfer learning"]]}, {"text": "This is often the case for common language pairs like German , Italian , and French , etc . ( 2 ) Bilingual Dictionary , where public bilingual dictionaries exist between the source and the target languages , but large - scaled parallel corpus are harder to obtain .", "entities": []}, {"text": "This can be the case for rarer languages like Finnish , Bulgarian , etc .", "entities": []}, {"text": "Furthermore , we assume that all the languages share a common multi - lingual database , whose column / row names and entry values are stored via multiple languages ( see the database in Figure 1 ) .", "entities": []}, {"text": "That is , the ontology of dialog among different languages is known with a one - to - one mapping between them ( e.g. , greek = griechisch = greco , food = essen = cibo ) .", "entities": [[4, 5, "MethodName", "ontology"]]}, {"text": "Based on that , we could construct a mapping function Mto associate the ontology terms from different languages with predesigned language - agnostic concepts : for example , M(foods )", "entities": [[13, 14, "MethodName", "ontology"]]}, {"text": "= M(Essen ) = M(Cibo)= FOOD .", "entities": []}, {"text": "We illustrate our problem de\ufb01nition in Figure 2 . 4 Decoupled Neural Belief Tracker We design our cross - lingual DST on top of the state - of - the - art Neural Belief Tracker ( NBT ) ( Mrksic et al . , 2017 ) , which demonstrates many advantages ( no hand - crafted lexicons , no linguistic knowledge required , etc ) .", "entities": []}, {"text": "These nice properties are essential for our cross - lingual DST design because we are pursuing a general and simple framework regardless of the language properties .", "entities": []}, {"text": "In short , NBT consists of a neural", "entities": []}, {"text": "417network that computes the matching score for every candidate slot - value pair ( cs;cv)based on the following three inputs : ( i ) the system dialog actsat= ( tq;ts;tv),2(ii ) the user utterance ut , and ( iii ) the candidate slot - value pair .", "entities": []}, {"text": "And it identi\ufb01es the user intents by evaluating the scores for all the slot - value pairs ( see Figure 3 ) .", "entities": []}, {"text": "With a slight abuse of notation , we still use cs;cv;ts;tv;tq2RHto denote the vector representations of themselves , where His the embedding dimension .", "entities": [[21, 23, "HyperparameterName", "embedding dimension"]]}, {"text": "We will use pre - trained embedding vectors in our cross - lingual NBT , just like the original NBT and they will be \ufb01xed during training .", "entities": []}, {"text": "To enable cross - lingual transfer learning , we \ufb01rst re - interpret the architecture of the original NBT by decomposing it into three components :", "entities": [[2, 6, "TaskName", "cross - lingual transfer"]]}, {"text": "Utterance Encoding The \ufb01rst component is an utterance encoder , which maps the utterance ut= fw1;w2;\u0001\u0001\u0001;wNgof a particular language into a semantic representation vector r(ut)2RH , wherewi2RHis the word vector for the i - th token andNis the length of the utterance .", "entities": []}, {"text": "Note that the dimension of the semantic vector r(ut)is the same as that of the word vector .", "entities": []}, {"text": "We implement \ud835\udc5f\ud835\udc54\ud835\udc66\ud835\udc64%\ud835\udc64&\ud835\udc64'\ud835\udc64(Utterance\ud835\udc50*\ud835\udc50+Candidate \ud835\udc61*\ud835\udc61+\ud835\udc61-System acts Utterance EncodingSlot - value DecodingAggregationGateScoreRequestgate \ud835\udc54&Candidategate \ud835\udc54%Confirmgate \ud835\udc54'CNN / RNN Figure 3 : Our implementation of baseline NBT , slightly modi\ufb01ed from ( Mrksic et al . , 2017 ) .", "entities": []}, {"text": "the encoder using the same convolutional neural network ( CNN ) as the original NBT , with a slight modi\ufb01cation of adding a top batch normalization layer .", "entities": [[24, 26, "MethodName", "batch normalization"]]}, {"text": "We will explain this change in section 5 .", "entities": []}, {"text": "Context Gate", "entities": []}, {"text": "The second part is the context gate , which takes the system acts at= ( tq;ts;tv ) 2tqrepresents the system request , ts;tvrepresents the system con\ufb01rmation .", "entities": []}, {"text": "If the system wants to request some information from the user by asking \u201c what \u2019s your favorite area ? \u201d , then NBT sets tq=\u201cAREA \u201d .", "entities": []}, {"text": "If the system wants to con\ufb01rm some information from a user by asking \u201c should I try Persian restaurants in the north ? \u201d", "entities": []}, {"text": "then NBT sets ts;tv=\u201carea , north\u201d.and the candidate slot - value pair ( cs;cv)as its inputs and \ufb01lter out the desired information from the encoded utterance .", "entities": []}, {"text": "The context gate gis a sum of three separate gates : g(cs;cv;at )", "entities": []}, {"text": "= g1+g2+g3 ( 1 ) where the individual gates are de\ufb01ned as : g1=\u001b(Ws c(cs+cv ) + bs c ) g2= ( cs\u0001Wq ttq )", "entities": []}, {"text": "[ 1;\u0001\u0001\u0001;1]H g3= ( cs\u0001Ws tts)(cv\u0001Wv ttv )", "entities": []}, {"text": "[ 1;\u0001\u0001\u0001;1]H(2 ) whereWs c;Wq t;Ws t;Wv t2RH\u0002Hare the weight matrices , and \f and\u0001denote the Hadamard product and the inner product , respectively .", "entities": []}, {"text": "The three gatesg12RH;g22RH;g32RHmodel the relevance between the candidate slot and value , the system request and the system con\ufb01rms , respectively .", "entities": []}, {"text": "The transformation matrices Wq t;Ws t;Wv t are added to the original NBT to increase the model \ufb02exibility of the gates .", "entities": []}, {"text": "Slot - Value Decoding The \ufb01nal component is a slot - value decoder , which predicts the score yof a given slot - value pair using the \ufb01ltered information from the utterance representation ras : y(cs;cv;ut;at ) = WT y[r(ut ) \f g(cs;cv;at)](3 ) whereWy2RH\u00021is the weight vector .", "entities": []}, {"text": "The above expression computes the score for the slotvalue pair based on the information from the current turn .", "entities": []}, {"text": "We combine it with the information from previous turns to get the \ufb01nal score : ^y(cvjut;at;cs ) = \u0015y(cs;cv;ut;at)+ ( 1\u0000\u0015)^y(cs;cv;ut\u00001;at\u00001)(4 )", "entities": []}, {"text": "here\u0015is a combination weight .", "entities": []}, {"text": "For each given slotcs , NBT selects the single highest value for informable slots and selects all values above a certain threshold for request slots .", "entities": []}, {"text": "Here we replace the multi - layer perceptron in the orginal NBT by a linear output layer ( to be explained in section 5 ) .", "entities": []}, {"text": "5 Cross - lingual Neural Belief Tracker In this section , we develop a cross - lingual Neural Belief Tracker ( XL - NBT ) that distills knowledge from one NBT to another using a teacherstudent framework .", "entities": []}, {"text": "We assume the ontology mappingMis known a priori ( see Figure 3 ) .", "entities": [[3, 4, "MethodName", "ontology"]]}, {"text": "XLNBT uses language - speci\ufb01c utterance encoder and context gate for each input language while sharing a common ( language - agnostic ) slot - value decoder across different languages ( see Figure 3 ) .", "entities": []}, {"text": "418The key idea is to optimize the language - speci\ufb01c components of the student network ( NBT of the target language ) so that their outputs are languageagnostic .", "entities": []}, {"text": "This is achieved by making these outputs close to that of the teacher network ( NBT of the source language ) , as we detail below .", "entities": []}, {"text": "5.1 Teacher - Student Framework We are given a well - trained NBT for a source languagee , and we want to learn an NBT for a target language fwithout any annotated training data .", "entities": []}, {"text": "Therefore , we can not learn the target - side NBT from standard supervised learning .", "entities": []}, {"text": "Instead , we use a teacher - student framework to distill the knowledge from the source - side NBT ( teacher network ) into the target - side NBT ( student network ) ( see Figure 4 ) .", "entities": []}, {"text": "Let xe,(ce s;ce v;ue t;ae", "entities": []}, {"text": "t)be the input to the teacher network and let xf , ( cf s;cf v;uf t;af t)be the associated input to the student network .", "entities": []}, {"text": "The standard teacher - student framework trains the student network by minimizing ^J1 = X xe;xfjjy(ce s;ce", "entities": []}, {"text": "v;ue t;ae t)\u0000y(cf s;cf v;uf t;af t)jj2 ( 5 ) wherey(ce s;ce v;ue t;ae t)andy(cf s;cf v;uf t;af t)denote the scores by the teacher and the student networks , respectively , and the slot - value pairs satisfy M(cf v )", "entities": []}, {"text": "= M(ce v)andM(cf s ) = M(ce s ) .", "entities": []}, {"text": "However , the target - side inputs ( cf s;cf v;uf t;af t)parallel to(ce s;ce v;ue t;ae t)are usually not available in crosslingual DST , and , even worse , the target - side utteranceue tis not available .", "entities": []}, {"text": "We may have to generate synthetic input data for the student network or leverage external data sources .", "entities": []}, {"text": "It is relatively easy to use the mapping M(\u0001)to generate ( cf s;cf v;af t ) ) ( i.e. , the inputs of the target - side context gate ) from the ( ce s;ce v;ae t ) .", "entities": []}, {"text": "But it is more challenging to obtain the parallel utterance data uf tfromue t ) .", "entities": []}, {"text": "Therefore , we have to leverage external bilingual data sources to alleviate the problem .", "entities": []}, {"text": "However , the external bilingual data are usually not in the same domain as the utterance , and hence they are not aligned with the slot - value pair and system acts ( i.e. ,(ce s;ce v;ae t)or(cf s;cf v;af t ) ) .", "entities": []}, {"text": "For this reason , we can not perform the knowledge transfer by minimizing the cost ( 5 ) .", "entities": []}, {"text": "Instead , we need to develop a new cost function where the utterance is not required to be aligned with the slot - value pair and the system acts .", "entities": []}, {"text": "To this end , let ge = ge(ce s;ce v;ae t ) andgf = gf(cf s;cf v;af t ) .", "entities": []}, {"text": "And we substitute ( 3)into ( 5 ) and get : ^J1\u0014jjWyjj2X cf v;cevjjre \f ge\u0000rf \f gfjj2", "entities": []}, {"text": "= jjWyjj2X cf v;cevjjge \f ( re\u0000rf ) + rf \f ( ge\u0000gf)jj2 \u0014jjWyjj2X cf v;cevjjgejj2jjre\u0000rfjj2+jjrfjj2jjge\u0000gfjj2 wherere = re(ue t)andrf = rf(uf t ) .", "entities": []}, {"text": "As we mentioned earlier , the weight Wyin the slotvalue decoder is shared between the student and the teacher networks and will not be updated .", "entities": []}, {"text": "The teacher - student optimization only adjusts the weights related to the language - speci\ufb01c parts in Figure 3 ( i.e. , utterance encoding and context gating ) .", "entities": []}, {"text": "Therefore , the shared weight jjWyjjis seen as a constant .", "entities": []}, {"text": "Furthermore , P cf v;cevjjgejj2can be seen as a constant since the teacher gate is \ufb01xed .", "entities": []}, {"text": "Since we use batch normalization layer to normalize the encoder output ( described in Figure 3 ) , jjrf(uf t)jj2can also be treated as a constant C2 .", "entities": [[3, 5, "MethodName", "batch normalization"]]}, {"text": "Therefore , we formally write the upper bound of ^J1as our surrogate cost function J : J = C1jjre(ue t)\u0000rf(uf t)jj2+C2X cf v;cevjjge\u0000gfjj2 ( 6 ) The surrogate cost has successfully decoupled utterance encoder with context gate , and we use Jr andJgto measure the encoder matching cost and the gate matching cost , respectively .", "entities": []}, {"text": "Jr = jjre(ue t)\u0000rf(uf t)jj2 Jg = X cf v;cevjjge\u0000gfjj2 ( 7 ) The encoder cost Jris optimized to distill the knowledge from the teacher encoder to student encoder while gate cost Jgis optimized to distill the knowledge from teacher gate to student gate .", "entities": []}, {"text": "This objective function successfully decouplesthe optimization of encoder and gate , thus we are able to optimize JrandJgseparately from different data sources .", "entities": []}, {"text": "Recall that we can easily simulate the target - side system acts , slot - value pairs ( cf s;cf v;af)by using the ontology mapping M. Therefore , optimizing Jgis relatively easy .", "entities": [[0, 1, "MetricName", "Recall"], [23, 24, "MethodName", "ontology"]]}, {"text": "Formally , we write the gate matching cost as follows : Jg = X ae t;ce s;ce v af t;cf s;cf vjjge(ce s;ce v;ae t)\u0000gf(cf s;cf v;af t)jj2 ( 8)", "entities": [[14, 15, "MethodName", "ae"]]}, {"text": "419 I \t want \t to \t order \t Greek \t foodWhat \t kind \t of \t food \t do \t you \t want?greekchinesethaijapanese\ud835\udc72\ud835\udc73(\ud835\udc91\ud835\udc86(\ud835\udc84\ud835\udc97|\ud835\udc2e\ud835\udc2d,\ud835\udc1a\ud835\udc2d,\ud835\udc84\ud835\udc94)||\ud835\udc91\ud835\udc87(\ud835\udc84\ud835\udc97|\ud835\udc2e\ud835\udc2d,\ud835\udc1a\ud835\udc2d,\ud835\udc84\ud835\udc94 ) ) \ud835\udc95\ud835\udc92:\ud835\udc6d\ud835\udc76\ud835\udc76\ud835\udc6b \t\t\t\t\t \ud835\udc95\ud835\udc94,\ud835\udc95\ud835\udc94 : \t None , \t None \t \ud835\udc84\ud835\udc94\ud835\udc86,\ud835\udc84\ud835\udc97\ud835\udc86 : \t ( FOOD , \t greek)||\ud835\udc93\ud835\udc96\ud835\udc95\ud835\udc86\u2212\ud835\udc93(\ud835\udc96\ud835\udc95\ud835\udc87)||Ichm\u00f6chtegriechischesEssenWelcheArt \t von \t Essen \t willstdu?\ud835\udc95\ud835\udc92:\ud835\udc6c\ud835\udc7a\ud835\udc7a\ud835\udc6c\ud835\udc75\ud835\udc95\ud835\udc94,\ud835\udc95\ud835\udc94 : \t None , \t None \t \ud835\udc84\ud835\udc94\ud835\udc87,\ud835\udc84\ud835\udc97\ud835\udc87 : \t ( ESSEN , \t grieschisch)\ud835\udc82\ud835\udc95\ud835\udc87\ud835\udc96\ud835\udc95\ud835\udc87grieschischChinesischthail\u00e4ndischjapanisch\ud835\udc82\ud835\udc95\ud835\udc86\ud835\udc96\ud835\udc95\ud835\udc86||\ud835\udc88(\ud835\udc82\ud835\udc95\ud835\udc87,\ud835\udc84\ud835\udc94\ud835\udc87,\ud835\udc84\ud835\udc97\ud835\udc87)\u2212\ud835\udc88(\ud835\udc82\ud835\udc95\ud835\udc86,\ud835\udc84\ud835\udc94\ud835\udc86,\ud835\udc84\ud835\udc97\ud835\udc86)||+Figure 4 : Teacher - Student Framework for cross - lingual transfer learning .", "entities": [[72, 76, "TaskName", "cross - lingual transfer"]]}, {"text": "The dotted line denotes the imaginary utterances , which expresses the same intention as the source side .", "entities": []}, {"text": "In \t birds , \t life \t gains \t new \t mobility .", "entities": []}, {"text": "Bei \t V\u00f6geln \t erh\u00e4lt \t das \t Leben \t neue \t Mobilit\u00e4t .", "entities": []}, {"text": "I \t can \t take \t a \t look \t at \t your \t records .", "entities": []}, {"text": "Ich \t kann \t mir \t deine \t Unterlagen \t ansehen .", "entities": []}, {"text": "Parallel   Corpora \ud835\udc6b\ud835\udc91StudentEncoder||\ud835\udc93\ud835\udc96\ud835\udc95\ud835\udc86\u2212\ud835\udc93(\ud835\udc96\ud835\udc95\ud835\udc87)||Teacher EncoderCNN / RNNCNN / RNNI \t want \t to \t order \t Greek \t foodDialogue Corpora DIchwant", "entities": []}, {"text": "to \t bestellenGreek \t food#sub=2 ; pos=0 , 3Mixed Language Corpora D\u2019Bilingual \t EmbeddingReplaceIchmirorderbestellenordnenBilingual Dictionary \ud835\udc6b\ud835\udc69I CNN / RNNCNN / RNNStudentEncoderTeacherEncoder Figure 5 : XL - NBT - C and XL - NBT - D for two scenarios However , exact optimization of Jris dif\ufb01cult and we have to approximate it using external parallel data .", "entities": []}, {"text": "We consider two kinds of external resources ( bilingual corpus and bilingual dictionary ) in the sections 5.2 - 5.3 ( see Figure 5 for the main idea ) .", "entities": []}, {"text": "5.2 Bilingual Corpus ( XL - NBT - C )", "entities": []}, {"text": "In our \ufb01rst scenario , we assume there exists a parallel corpus Dpconsisting of sentence pairs from the source language and the target language .", "entities": []}, {"text": "In this case , the cost function ( 6 ) is approximated by J= E ( me;mf)2Dpjjre(me)\u0000rf(mf)jj2 + \u000b Jg ( 9 ) where \u000b is the balancing factor and Jgis de\ufb01ned in ( 6 ) .", "entities": []}, {"text": "The cost function ( 9 ) is minimized by stochastic gradient descent .", "entities": [[9, 12, "MethodName", "stochastic gradient descent"]]}, {"text": "At test time , we switch the encoder to receive target language inputs.5.3 Bilingual Dictionary ( XL - NBT - D )", "entities": []}, {"text": "In the second scenario , we assume there exists no parallel corpus but a bilingual dictionary DB that de\ufb01nes the correspondence between source words and target words ( a one - to - many mapping fw : MD(w)g ) .", "entities": []}, {"text": "Likewise , it is infeasible to optimize the exact encoder cost Jrdue to the lack of target - side utterances .", "entities": []}, {"text": "We propose a word replacement strategy ( to be described later ) to generate synthetic parallel sentence ^uf tof \u201c mixed \u201d language .", "entities": []}, {"text": "Then , we use the generated target parallel sentences to approximate the cost ( 6 ) by Jr = E ut2Djjre(ue t)\u0000rf(^uf t)jj2 + \u000b Jg ( 10 ) where \u000b is the balancing factor .", "entities": []}, {"text": "For word replacement , we \ufb01rst decide the number of words Nwto be replaced , then we draw Nwpositions randomly from the source utterance and substitute the corresponding word wiwith their target word synonym fromMD(w)based on the context as follows :", "entities": []}, {"text": "jp(Nw = i ) = exp(\u0000i= \u001c ) P i0 < Nexp(\u0000i0= \u001c ) p ( ^w ) = ^w\u0001h^wP w02M(wi)w0\u0001h^w(11 ) whereh^w = P2 k=\u00002 : k6=0wi+krepresents the context vector and Ndenotes the utterance length .", "entities": []}, {"text": "The context similarity of context and the targetside synonym can better help us in choosing the most appropriate candidate from the list .", "entities": []}, {"text": "In our following experiments , we adjust the temperature of \u001c to control the aggressiveness of replacement .", "entities": []}, {"text": "4206 Experiments 6.1 Dataset The Wizard of Oz ( WOZ ) ( Rojas - Barahona et al . , 2017 ) dataset is used for training and evaluation , which consists of user conversations with taskoriented dialog systems designed to help users \ufb01nd suitable restaurants around Cambridge , UK .", "entities": [[46, 47, "DatasetName", "Cambridge"]]}, {"text": "The corpus contains three informable ( i.e. goaltracking ) slots : FOOD , AREA , and PRICE .", "entities": []}, {"text": "The users can specify values for these slots in order to \ufb01nd which best meet their criteria .", "entities": []}, {"text": "Once the system suggests a restaurant , the users can ask about the values of up to eight requestable slots ( PHONE NUMBER , ADDRESS , etc . ) .", "entities": []}, {"text": "Multilingual WOZ 2.0 ( Mrksic et al . , 2017 ) has expanded this dataset to include more dialogs and more languages .", "entities": []}, {"text": "The train , valid and test datasets for three different languages ( English , German , Italian ) are available online3 .", "entities": []}, {"text": "We use the English as source language where 600 dialogs are used for training , 200 for validation and 400 for testing .", "entities": []}, {"text": "We use the German and Italian as the target language to transfer our knowledge from English DST system .", "entities": []}, {"text": "In the experiments , we do not have access to any training or validation dataset for German and Italian , and we only have access to their testing dataset which is composed of 400 dialogs .", "entities": [[13, 15, "DatasetName", "validation dataset"]]}, {"text": "For external resource , we use the IWSLT2014 Ted Talk parallel corpus ( Mauro et al . , 2012 ) from the of\ufb01cial website4for bilingual corpus scenario .", "entities": []}, {"text": "In the IWSLT2014 parallel corpus , we only keep the sentences between 4 and 40 words and decrease the sentence pairs to around 150K.", "entities": []}, {"text": "We use Panlex ( Kamholz et al . , 2014 ) as our data source and crawl translations for all the words appearing in the dialog datasets to build our bilingual dictionary .", "entities": [[2, 3, "DatasetName", "Panlex"]]}, {"text": "We speci\ufb01cally investigate two kinds of pretrained embedding , and we use Glove ( Pennington et al . , 2014 ) as the monolingual embedding and MUSE ( Conneau et al . , 2017 ) as the bilingual embedding to see their impacts on the DST performance .", "entities": [[26, 27, "DatasetName", "MUSE"]]}, {"text": "We split the raw DST corpus into turn - level examples .", "entities": []}, {"text": "During training , we use the ground truth previous state Vt\u00001as inputs .", "entities": []}, {"text": "At test time , we use the model searched states as the previous state to continue tracking intention until the end of the 3https://github.com/nmrksic/ neural - belief - tracker / tree / master / data 4https://wit3.fbk.eu/mt.php?release= 2014 - 01dialog .", "entities": []}, {"text": "When the dialog terminates , we use two evaluation metrics introduced in Henderson et", "entities": []}, {"text": "al . ( 2014a ) to evaluate the DST performance : ( 1 ) Goals : the proportion of dialog turns where all the users search goal constraints were correctly identi\ufb01ed .", "entities": []}, {"text": "( 2 ) Requests : similarly , the proportion of dialog turns where users requests for information were identi\ufb01ed correctly .", "entities": []}, {"text": "Our implementation is based on the NBT5 , the details of our system setting are described in the appendix .", "entities": []}, {"text": "6.2 Results Here we highlight the baselines we use to compare with our cross - lingual algorithm as follows : ( 1 ) Supervised : this baseline algorithm assumes the existence of annotated dialog belief tracking datasets , and it determines the upper bound of the DST model .", "entities": [[23, 25, "DatasetName", "Supervised :"]]}, {"text": "( 2 ) w/o Transfer : this algorithm trains an English NBT , and then directly feeds target language into the embedding level as inputs during test time to evaluate the performance .", "entities": []}, {"text": "( 3 ) Ontology - match : this algorithm directly uses exact string matching against the utterance to discover the perceived slot - value pairs , it directly assigns a high score to the appearing candidates .", "entities": [[3, 4, "MethodName", "Ontology"]]}, {"text": "( 4 ) Translation - based : this system pre - trains a translator on the external bilingual corpus and then translates the English dialog and ontology into target language as \u201c annotated \u201d data , which is used to train the NBT in the target language domain ( more details about the implementation , performance and examples are listed in the appendix ) .", "entities": [[3, 4, "TaskName", "Translation"], [26, 27, "MethodName", "ontology"]]}, {"text": "( 5 ) Word - By - Word ( WBW ) : this system transforms the English dialog corpus into target language word by word using the bilingual dictionary , which is used to train the NBT in target side .", "entities": []}, {"text": "We demonstrate the results for our proposed algorithms and other competing algorithms in Table 2 , from which we can easily conclude that that ( i ) our Decoupled NBT does not affect the performance , and ( ii ) our cross - lingual NBT framework is able to achieve signi\ufb01cantly better accuracy for both languages in both parallel - resource scenarios .", "entities": [[52, 53, "MetricName", "accuracy"]]}, {"text": "Compare with Translator / WBW .", "entities": []}, {"text": "With bilingual corpus , XL - NBT - C with pre - trained bilingual embedding can signi\ufb01cantly outperform our Translator baseline ( Klein et al . , 2017 ) .", "entities": []}, {"text": "This is intuitive because the translation model requires 5https://github.com/nmrksic/ neural - belief - tracker", "entities": []}, {"text": "421Error Type Examples Modify FailureMachine : I have two options that \ufb01t that description , golden wok Chinese restaurant and the Nirala which serves Indian food , do you have a preference ?", "entities": []}, {"text": "User : How about Nirala , what s the address and phone of that ?", "entities": []}, {"text": "Previous State : food = Chinese ; Prediction : food = none ; Groundtruth : food = Indian Maintain FailureMachine : there are $ num places with a moderate price range .", "entities": []}, {"text": "can you please tell me what kind of food you would like ?", "entities": []}, {"text": "User : well I want to eat in the north , what s up that way ?", "entities": []}, {"text": "Previous State : food = expensive ; Prediction : food = none ; Groundtruth : food = expensive History FailureMachine : Anatolia is located at $ num bridge street city center .", "entities": []}, {"text": "User : thank you goodbye !", "entities": []}, {"text": "Previous State : food = Chinese ; Prediction : food = Chinese;,Groundtruth : food = Turkish Table 1 : Here we show the frequent error types , the examples are translated to English for better understanding .", "entities": []}, {"text": "Language German ( student ) Italian ( student ) English ( teacher ) Models Goal Request Goal Request Goal Request w/ Supervised DialogNBT ( Mrksic et al . , 2017 ) - - - - 0.84 0.91 Decoupled NBT ( mono ) 0.79 0.83 0.86 0.91 0.82 0.89 Decoupled NBT ( bilingual ) 0.80 0.84 0.88 0.91 0.84 0.90 w/o Bilingual Dataw / o Transfer ( mono ) 0.15 0.10 0.15 0.11 - w/o Transfer ( bilingual ) 0.13 0.13 0.11 0.12 - Ontology Matching 0.24 0.21 0.23 0.21 - w/ Bilingual CorpusTranslate ( Klein et al . , 2017 ) 0.41 0.42 0.48 0.51 - XL - NBT - C ( mono ) 0.48 0.54 0.65 0.60 - XL - NBT - C ( bilingual ) 0.55 0.59 0.72 0.69 - w/ Bilingual DictionaryWord - by - Word 0.22 0.25 0.25 0.27 - XL - NBT - D ( mono ) 0.14 0.15 0.23 0.22 - XL - NBT - D ( bilingual ) 0.51 0.56 0.73 0.63 - Table 2 : Experimental results for cross - lingual NBT and other baseline algorithms .", "entities": [[83, 84, "MethodName", "Ontology"]]}, {"text": "All results are averaged over 5 runs .", "entities": []}, {"text": "Here we use \u201c mono \u201d to refer to the experiments with pre - trained monolingual embedding , \u201c bilingual \u201d to refer to the experiments with pre - trained bilingual embedding .", "entities": []}, {"text": "both source - side encoding and target - side wordby - word decoding , while our XL - NBT only needs a bilingual source - encoding to align two vector space , which averts the compounded decoding errors .", "entities": []}, {"text": "With the bilingual dictionary , the word - byword translator is very weak and leading to many broken target sentences , which poses challenges for DST training .", "entities": []}, {"text": "In comparison , our XL - NBTD can control the replacement by adjusting its temperature to maintain the stability of utterance representation .", "entities": []}, {"text": "Furthermore , for both cases , our teacher - student framework can make use of the knowledge learned in source - side NBT to assist its decision making , while translator - based methods learn from scratch .", "entities": [[26, 28, "TaskName", "decision making"]]}, {"text": "Bilingual Corpus vs. Bilingual Dictionary .", "entities": []}, {"text": "From the table , we can easily observe that bilingual corpus is obviously a more informative parallel resource to perform cross - lingual transfer learning .", "entities": [[20, 24, "TaskName", "cross - lingual transfer"]]}, {"text": "The accuracy of XL - NBT - D is lower than XL - NBT - C. We conjecture that our replace - ment strategy to generate \u201c mixed \u201d language utterance can sometimes break the semantic coherence and cause additional noises during the transfer process , which remarkably degrades the DST performance .", "entities": [[1, 2, "MetricName", "accuracy"]]}, {"text": "Monolingual vs. Bilingual embedding .", "entities": []}, {"text": "From the table , we can observe that the bilingual embedding and monolingual embedding does not make much difference in supervised training .", "entities": []}, {"text": "However , the gap in the bilingual corpus case is quite obvious .", "entities": []}, {"text": "Monolingual embedding even causes the transfer to fail in a bilingual dictionary case .", "entities": []}, {"text": "We conjecture that the bilingual word embedding already contain many alignment information between two languages , which largely eases the training of encoder matching objective .", "entities": []}, {"text": "German vs. Italian As can be seen , the transfer learning results for Italian are remarkably higher than German , especially for the \u201c Goal \u201d accuracy .", "entities": [[9, 11, "TaskName", "transfer learning"], [26, 27, "MetricName", "accuracy"]]}, {"text": "We conjecture that it is due to German declension , which can produce many word forms .", "entities": []}, {"text": "The", "entities": []}, {"text": "422very diverse word forms present great challenges for DST to understand its intention behind .", "entities": []}, {"text": "Especially for the bilingual dictionary , German tends to have much longer replacement candidate lists than Italian , which introduces more noises to the replacement procedure .", "entities": []}, {"text": "Error Analysis Here we showcase the most frequent error types in subsection 6.1 .", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "From our observation , these three types of errors distribute evenly in the test dialogs .", "entities": []}, {"text": "The error mainly comes from the unaligned utterance space , which leads to failure in understanding the intention of human utterance in the target language .", "entities": []}, {"text": "This can lead the system to fail in modifying the dialog state or maintaining the previous dialog states .", "entities": []}, {"text": "6.3 Discussion Here we want to further highlight the comparison between our transfer learning algorithm with the MT - based approach .", "entities": [[12, 14, "TaskName", "transfer learning"]]}, {"text": "Though our approach outperforms the standard Translator trained on IWSLT2014 , it does not necessarily claim that our transfer algorithm outperforms any translation methods on any parallel corpus .", "entities": []}, {"text": "In our further ablation studies , we found that using Google Translator6can actually achieve a better score than our transfer algorithm , which is understandable considering the complexity of Google Translator and the much larger parallel corpus it leverages .", "entities": [[10, 11, "DatasetName", "Google"], [29, 30, "DatasetName", "Google"]]}, {"text": "By leveraging more close - to - domain corpus and comprehensive entity recognition / replacement strategy , the translator model is able to achieve a higher score .", "entities": []}, {"text": "Apparently , we need to trade off the ef\ufb01ciency for the accuracy .", "entities": [[11, 12, "MetricName", "accuracy"]]}, {"text": "For DST problem , it is an overkill to introduce a more complex translation algorithm , what we pursue is a simple yet ef\ufb01cient algorithm to achieve promising scores .", "entities": []}, {"text": "It is also worth mentioning that our XL - NBT algorithm only takes several hours to achieve the reported score , while the translator model takes much more time and memory to train depending on the complexity .", "entities": []}, {"text": "Thus , the simplicity and ef\ufb01ciency makes our model a better \ufb01t for rarelanguage and limited - budget scenarios .", "entities": []}, {"text": "6.4 Ablation Test Here we investigate the effect \u2018 of hyper - parameter \u000b ; \u001c  on the evaluation results .", "entities": []}, {"text": "The \u000b is used to balance the optimization of encoder constraint and 6https://translate.google.com/gate constraint , where larger \u000b means more optimization on gate constraint .", "entities": []}, {"text": "The temperature \u001c is used to control the aggressiveness of the replacement XL - NBT - D , where smaller \u001c means more source words are replaced by target synonyms .", "entities": []}, {"text": "From the table Table 3 , we can observe that the \u000b ablation ( \u001c \ufb01xed to 0.1 ) \u001c ablation ( \u000b \ufb01xed to 1 ) value Goal Request value Goal Request \u000b = 0 0.13 0.00 \u001c = 0 0.14 0.08 \u000b = 0.1 0.46 0.54 \u001c = 0.03 0.43 0.50", "entities": [[35, 36, "DatasetName", "0"], [40, 41, "DatasetName", "0"]]}, {"text": "= 1 0.51 0.56 \u001c = 0.1 0.51 0.56", "entities": []}, {"text": "= 5 0.46 0.54 \u001c = 0.3 0.47 0.51 \u000b = 10 0.46 0.52 \u001c = 1 0.44 0.52 \u000b = 100 0.44 0.50 \u001c = 10 0.33 0.32 Table 3 : Ablation test for hyper - parameter \u000b and \u001c on English - to - German XL - NBT - D. experimental results are not very sensitive to \u000b , a dramatic change of \u000b will not harm the \ufb01nal results too much , we simply choose \u000b = 1 as the hyper - parameter .", "entities": []}, {"text": "In contrast , the system is more sensitive to temperature .", "entities": []}, {"text": "Too conservative replacement will lead to weak transfer , while too aggressive replacement will destroy the utterance representation .", "entities": []}, {"text": "Therefore , we choose the a moderate temperature of \u001c =", "entities": []}, {"text": "0:1throughout our experiments .", "entities": []}, {"text": "We also draw the learning curve ( Precision vs. Iteration ) in the Appendix for both XL - NBTC and XL - NBT - D. The learning curves show that our algorithm is stable and converges quickly , and the reported results are highly reproducible .", "entities": [[7, 8, "MetricName", "Precision"]]}, {"text": "7 Conclusion In our paper , we propose a novel teacher - student framework to perform cross - lingual transfer learning for DST .", "entities": [[16, 20, "TaskName", "cross - lingual transfer"]]}, {"text": "The key idea of our model is to decouple the current DST neural network into two separate modules and transfer them separately .", "entities": []}, {"text": "We believe our method can be further extended into a general purpose multi - lingual transfer framework to resolve other NLP matching or classi\ufb01cation problems .", "entities": []}, {"text": "8 Acknowledgement We are gratefully supported by a Tencent AI Lab Rhino - Bird Gift Fund .", "entities": []}, {"text": "We are also very thankful for the public belief tracking code and multilingual state - tracking datasets released by Nikola Mrksic from the University of Cambridge .", "entities": [[25, 26, "DatasetName", "Cambridge"]]}, {"text": "423References Antoine Bordes and Jason Weston .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Learning end - to - end goal - oriented dialog .", "entities": [[6, 10, "TaskName", "goal - oriented dialog"]]}, {"text": "In Proceedings of the International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Alexis Conneau , Guillaume Lample , Marc\u2019Aurelio Ranzato , Ludovic Denoyer , and Herv \u00b4 e J\u00b4egou . 2017 .", "entities": []}, {"text": "Word translation without parallel data .", "entities": [[0, 2, "TaskName", "Word translation"]]}, {"text": "In Proceedings of the International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Matthew Henderson , Blaise Thomson , and Steve Young .", "entities": []}, {"text": "2014a .", "entities": []}, {"text": "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation .", "entities": []}, {"text": "In Spoken Language Technology Workshop ( SLT ) , 2014 IEEE , pages 360\u2013365 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Matthew Henderson , Blaise Thomson , and Steve J. Young .", "entities": []}, {"text": "2014b .", "entities": []}, {"text": "Word - based dialog state tracking with recurrent neural networks .", "entities": []}, {"text": "In Proceedings of the SIGDIAL 2014 Conference , The 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue , 18 - 20 June 2014 , Philadelphia , PA , USA , pages 292\u2013299 .", "entities": []}, {"text": "David Kamholz , Jonathan Pool , and Susan M Colowick . 2014 .", "entities": []}, {"text": "Panlex : Building a resource for panlingual lexical translation .", "entities": [[0, 1, "DatasetName", "Panlex"]]}, {"text": "In LREC , pages 3145\u20133150 .", "entities": []}, {"text": "Joo - Kyung Kim , Young - Bum Kim , Ruhi Sarikaya , and Eric Fosler - Lussier .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Cross - lingual transfer learning for pos tagging without cross - lingual resources .", "entities": [[0, 4, "TaskName", "Cross - lingual transfer"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2832\u20132838 .", "entities": []}, {"text": "G. Klein , Y .", "entities": []}, {"text": "Kim , Y .", "entities": []}, {"text": "Deng , J. Senellart , and A. M. Rush .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "OpenNMT : Open - Source Toolkit for Neural Machine Translation .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "ArXiv e - prints .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Xiujun Li , Yun - Nung Chen , Lihong Li , Jianfeng Gao , and Asli C \u00b8 elikyilmaz . 2017 .", "entities": []}, {"text": "End - to - end taskcompletion neural dialogue systems .", "entities": []}, {"text": "In Proceedings of the Eighth International Joint Conference on Natural Language Processing , IJCNLP 2017 , Taipei , Taiwan , November 27 - December 1 , 2017 - Volume 1 : Long Papers , pages 733\u2013743 .", "entities": []}, {"text": "Cettolo Mauro , Girardi Christian , and Federico Marcello . 2012 .", "entities": []}, {"text": "Wit3 : Web inventory of transcribed and translated talks .", "entities": []}, {"text": "In Conference of European Association for Machine Translation , pages 261\u2013268 .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "Angeliki Metallinou , Dan Bohus , and Jason Williams .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Discriminative state tracking for spoken dialog systems .", "entities": []}, {"text": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , volume 1 , pages 466\u2013475.Takashi Miyazaki and Nobuyuki Shimizu .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Cross - lingual image caption generation .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , volume 1 , pages 1780\u20131790 .", "entities": []}, {"text": "Nikola Mrksic , Diarmuid \u00b4 O S\u00b4eaghdha , Blaise Thomson , Milica Gasic , Pei - hao Su , David Vandyke , Tsung - Hsien Wen , and Steve J. Young .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Multidomain dialog state tracking using recurrent neural networks .", "entities": []}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing , ACL 2015 , July 2631 , 2015 , Beijing , China , Volume 2 : Short Papers , pages 794\u2013799 .", "entities": []}, {"text": "Nikola Mrksic , Diarmuid \u00b4 O S\u00b4eaghdha , Tsung - Hsien Wen , Blaise Thomson , and Steve J. Young .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Neural belief tracker : Data - driven dialogue state tracking .", "entities": [[7, 10, "TaskName", "dialogue state tracking"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , ACL 2017 , Vancouver , Canada , July 30 - August 4 , Volume 1 : Long Papers , pages 1777\u20131788 .", "entities": []}, {"text": "Nikola Mrk \u02c7si\u00b4c , Ivan Vuli \u00b4 c , Diarmuid \u00b4 O S\u00b4eaghdha , Ira Leviant , Roi Reichart , Milica Ga \u02c7si\u00b4c , Anna Korhonen , and Steve Young . 2017 .", "entities": []}, {"text": "Semantic specialisation of distributional word vector spaces using monolingual and cross - lingual constraints .", "entities": []}, {"text": "arXiv preprint arXiv:1706.00374 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jian Ni , Georgiana Dinu , and Radu Florian . 2017 .", "entities": []}, {"text": "Weakly supervised cross - lingual named entity recognition via effective annotation and representation projection .", "entities": [[5, 8, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , ACL 2017 , Vancouver , Canada , July 30 August 4 , Volume 1 : Long Papers , pages 1470\u20131480 .", "entities": []}, {"text": "Sinno Jialin Pan and Qiang Yang .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "A survey on transfer learning .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "IEEE Transactions on knowledge and data engineering , 22(10):1345\u20131359 .", "entities": []}, {"text": "Xiaoman Pan , Boliang Zhang , Jonathan May , Joel Nothman , Kevin Knight , and Heng Ji . 2017 .", "entities": []}, {"text": "Crosslingual name tagging and linking for 282 languages .", "entities": []}, {"text": "InProceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , volume 1 , pages 1946\u20131958 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 conference on empirical methods in natural language processing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Doina Precup and Yee Whye Teh , editors .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Proceedings of the 34th International Conference on Machine Learning , ICML 2017 , Sydney , NSW , Australia , 6 - 11 August 2017 , volume 70 of Proceedings of Machine Learning Research .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "424Abhinav Rastogi , Dilek Hakkani - T \u00a8ur , and Larry P. Heck . 2017 .", "entities": []}, {"text": "Scalable multi - domain dialogue state tracking .", "entities": [[1, 7, "TaskName", "multi - domain dialogue state tracking"]]}, {"text": "In 2017 IEEE Automatic Speech Recognition and Understanding Workshop , ASRU 2017 , Okinawa , Japan , December 16 - 20 , 2017 , pages 561 \u2013 568 .", "entities": [[3, 6, "TaskName", "Automatic Speech Recognition"]]}, {"text": "Lina Maria Rojas - Barahona , Milica Gasic , Nikola Mrksic , Pei - Hao Su , Stefan Ultes , Tsung - Hsien Wen , Steve J. Young , and David Vandyke . 2017 .", "entities": []}, {"text": "A network - based end - to - end trainable task - oriented dialogue system .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics , EACL 2017 , Valencia , Spain , April 3 - 7 , 2017 , Volume 1 : Long Papers , pages 438\u2013449 .", "entities": []}, {"text": "Tsung - Hsien Wen , Milica Gasic , Nikola Mrksic , Peihao Su , David Vandyke , and Steve J. Young .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Semantically conditioned lstm - based natural language generation for spoken dialogue systems .", "entities": [[2, 3, "MethodName", "lstm"], [9, 12, "TaskName", "spoken dialogue systems"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , EMNLP 2015 , Lisbon , Portugal , September 17 - 21 , 2015 , pages 1711\u20131721 .", "entities": []}, {"text": "Jason Williams , Antoine Raux , and Matthew Henderson .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "The dialog state tracking challenge series : A review .", "entities": []}, {"text": "Dialogue & Discourse , 7(3):4\u201333 .", "entities": []}, {"text": "Jason D. Williams , Kavosh Asadi , and Geoffrey Zweig .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Hybrid code networks : practical and ef\ufb01cient end - to - end dialog control with supervised and reinforcement learning .", "entities": []}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , ACL 2017 , Vancouver , Canada , July 30 - August 4 , Volume 1 : Long Papers , pages 665\u2013677 .", "entities": []}, {"text": "Puyang Xu and Qi Hu . 2018 .", "entities": []}, {"text": "An end - to - end approach for handling unknown slot values in dialogue state tracking .", "entities": [[13, 16, "TaskName", "dialogue state tracking"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , ACL 2018 , Melbourne , Australia , July 15 - 20 , 2018 , Volume 1 : Long Papers , pages 1448\u20131457 .", "entities": []}, {"text": "Steve Young , Milica Ga \u02c7si\u00b4c , Simon Keizer , Franc \u00b8ois Mairesse , Jost Schatzmann , Blaise Thomson , and Kai Yu . 2010 .", "entities": []}, {"text": "The hidden information state model : A practical framework for pomdp - based spoken dialogue management .", "entities": [[14, 16, "TaskName", "dialogue management"]]}, {"text": "Computer Speech & Language , 24(2):150\u2013174 .", "entities": []}, {"text": "Yuan Zhang , David Gaddy , Regina Barzilay , and Tommi S. Jaakkola .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Ten pairs to tag - multilingual POS tagging via coarse mapping between embeddings .", "entities": []}, {"text": "In NAACL HLT 2016 , The 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , San Diego California , USA , June 12 - 17 , 2016 , pages 1307\u20131317 .", "entities": []}]