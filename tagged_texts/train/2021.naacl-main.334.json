[{"text": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 4228\u20134238 June 6\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics4228Self - Alignment Pretraining for Biomedical Entity Representations Fangyu Liu| , Ehsan Shareghi};| , Zaiqiao Meng| , Marco Basaldella~\u0003 , Nigel Collier| |Language Technology Lab , TAL , University of Cambridge } Department of Data Science & AI , Monash University ~ Amazon Alexa |{fl399 , zm324 , nhc30}@cam.ac.uk } ehsan.shareghi@monash.edu~mbbasald@amazon.co.uk Abstract Despite the widespread success of selfsupervised learning via masked language models ( MLM ) , accurately capturing \ufb01ne - grained semantic relationships in the biomedical domain remains a challenge .", "entities": [[35, 36, "DatasetName", "Cambridge"], [44, 45, "DatasetName", "Monash"], [69, 70, "DatasetName", "MLM"]]}, {"text": "This is of paramount importance for entity - level tasks such as entity linking where the ability to model entity relations ( especially synonymy ) is pivotal .", "entities": [[12, 14, "TaskName", "entity linking"]]}, {"text": "To address this challenge , we propose S APBERT , a pretraining scheme that selfaligns the representation space of biomedical entities .", "entities": []}, {"text": "We design a scalable metric learning framework that can leverage UMLS , a massive collection of biomedical ontologies with 4M+ concepts .", "entities": [[4, 6, "TaskName", "metric learning"], [10, 11, "DatasetName", "UMLS"]]}, {"text": "In contrast with previous pipelinebased hybrid systems , S APBERT offers an elegant one - model - for - all solution to the problem of medical entity linking ( MEL ) , achieving a new state - of - the - art ( SOTA ) on six MEL benchmarking datasets .", "entities": [[26, 28, "TaskName", "entity linking"]]}, {"text": "In the scienti\ufb01c domain , we achieve SOTA even without taskspeci\ufb01c supervision .", "entities": []}, {"text": "With substantial improvement over various domain - speci\ufb01c pretrained MLMs such as B IOBERT , SCIBERTand P UBMEDBERT , our pretraining scheme proves to be both effective and robust.1 1", "entities": []}, {"text": "Introduction Biomedical entity2representation is the foundation for a plethora of text mining systems in the medical domain , facilitating applications such as literature search ( Lee et al . , 2016 ) , clinical decision making ( Roberts et al . , 2015 ) and relational knowledge discovery ( e.g. chemical - disease , drug - drug and protein - protein relations , Wang et al . 2018 ) .", "entities": [[15, 17, "DatasetName", "medical domain"], [34, 36, "TaskName", "decision making"]]}, {"text": "The heterogeneous naming of biomedical concepts \u0003Work conducted prior to joining Amazon .", "entities": []}, {"text": "1For code and pretrained models , please visit : https : //github.com / cambridgeltl / sapbert .", "entities": []}, {"text": "2In this work , biomedical entity refers to the surface forms of biomedical concepts , which can be a single word ( e.g. fever ) , a compound ( e.g. sars - cov-2 ) or a short phrase ( e.g. abnormal retinal vascular development ) .", "entities": []}, {"text": "PUBMEDBERT+ SAPBERTPUBMEDBERT Figure 1 : The t - SNE ( Maaten and Hinton , 2008 ) visualisation of UMLS entities under P UBMEDBERT ( BERT pretrained on PubMed papers ) & P UBMEDBERT+SAPBERT ( PUBMEDBERT further pretrained on UMLS synonyms ) .", "entities": [[18, 19, "DatasetName", "UMLS"], [24, 25, "MethodName", "BERT"], [38, 39, "DatasetName", "UMLS"]]}, {"text": "The biomedical names of different concepts are hard to separate in the heterogeneous embedding space ( left ) .", "entities": []}, {"text": "After the self - alignment pretraining , the same concept \u2019s entity names are drawn closer to form compact clusters ( right ) .", "entities": []}, {"text": "poses a major challenge to representation learning .", "entities": [[5, 7, "TaskName", "representation learning"]]}, {"text": "For instance , the medication Hydroxychloroquine is often referred to as Oxichlorochine ( alternative name ) , HCQ ( in social media ) and Plaquenil ( brand name ) .", "entities": []}, {"text": "MEL addresses this problem by framing it as a task of mapping entity mentions to uni\ufb01ed concepts in a medical knowledge graph.3The main bottleneck of MEL is the quality of the entity representations ( Basaldella et al . , 2020 ) .", "entities": []}, {"text": "Prior works in this domain have adopted very sophisticated text pre - processing heuristics ( D\u2019Souza and Ng , 2015 ;", "entities": []}, {"text": "Kim et al . , 2019 ; Ji et al . , 2020 ; Sung et al . , 2020 ) which can hardly cover all the variations of biomedical names .", "entities": []}, {"text": "In parallel , self - supervised learning has shown tremendous success in NLP via leveraging the masked language modelling ( MLM ) 3Note that we consider only the biomedical entities themselves and not their contexts , also known as medical concept normalisation / disambiguation in the BioNLP community .", "entities": [[3, 7, "TaskName", "self - supervised learning"], [17, 19, "TaskName", "language modelling"], [20, 21, "DatasetName", "MLM"]]}, {"text": "4229objective to learn semantics from distributional representations ( Devlin et al . , 2019 ; Liu et al . , 2019 ) .", "entities": []}, {"text": "Domain - speci\ufb01c pretraining on biomedical corpora ( e.g. BIOBERT , Lee et al . 2020 and BIOMEGATRON , Shin et al . 2020 ) have made much progress in biomedical text mining tasks .", "entities": []}, {"text": "Nonetheless , representing medical entities with the existing SOTA pretrained MLMs ( e.g. PUBMEDBERT , Gu et al . 2020 ) as suggested in Fig .", "entities": []}, {"text": "1 ( left ) does not lead to a well - separated representation space .", "entities": []}, {"text": "To address the aforementioned issue , we propose to pretrain a Transformer - based language model on the biomedical knowledge graph of UMLS ( Bodenreider , 2004 ) , the largest interlingua of biomedical ontologies .", "entities": [[11, 12, "MethodName", "Transformer"], [22, 23, "DatasetName", "UMLS"]]}, {"text": "UMLS contains a comprehensive collection of biomedical synonyms in various forms ( UMLS 2020AA has 4M+ concepts and 10M+ synonyms which stem from over 150 controlled vocabularies including MeSH , SNOMED CT , RxNorm , Gene Ontology and OMIM).4We design a selfalignment objective that clusters synonyms of the same concept .", "entities": [[0, 1, "DatasetName", "UMLS"], [12, 13, "DatasetName", "UMLS"], [36, 37, "MethodName", "Ontology"]]}, {"text": "To cope with the immense size of UMLS , we sample hard training pairs from the knowledge base and use a scalable metric learning loss .", "entities": [[7, 8, "DatasetName", "UMLS"], [22, 24, "TaskName", "metric learning"], [24, 25, "MetricName", "loss"]]}, {"text": "We name our model as Self - aligning pretrained BERT ( SAPBERT ) .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "Being both simple and powerful , SAPBERTobtains new SOTA performances across all six MEL benchmark datasets .", "entities": []}, {"text": "In contrast with the current systems which adopt complex pipelines and hybrid components ( Xu et al . , 2020 ; Ji et al . , 2020 ; Sung et al . , 2020 ) , SAPBERT applies a much simpler training procedure without requiring any pre- or post - processing steps .", "entities": []}, {"text": "At test time , a simple nearest neighbour \u2019s search is suf\ufb01cient for making a prediction .", "entities": []}, {"text": "When compared with other domain - speci\ufb01c pretrained language models ( e.g. BIOBERT and SCIBERT),SAPBERT also brings substantial improvement by up to 20 % on accuracy across all tasks .", "entities": [[7, 10, "TaskName", "pretrained language models"], [25, 26, "MetricName", "accuracy"]]}, {"text": "The effectiveness of the pretraining in SAPBERTis especially highlighted in the scienti\ufb01c language domain where SAPBERToutperforms previous SOTA even without \ufb01ne - tuning on any MEL datasets .", "entities": []}, {"text": "We also provide insights on pretraining \u2019s impact across domains and explore pretraining with fewer model parameters by using a recently introduced A DAPTER module in our training scheme .", "entities": []}, {"text": "4https://www.nlm.nih.gov/research/umls/knowledge _ sources / metathesaurus / release / statistics.html Figure 2 : The distribution of similarity scores for all sampled P UBMEDBERT representations in a minibatch .", "entities": []}, {"text": "The left graph shows the distribution of + andpairs which are easy and already well - separated .", "entities": []}, {"text": "The right graph illustrates larger overlap between the two groups generated by the online mining step , making them harder and more informative for learning .", "entities": []}, {"text": "2 Method : Self - Alignment Pretraining We design a metric learning framework that learns to self - align synonymous biomedical entities .", "entities": [[10, 12, "TaskName", "metric learning"]]}, {"text": "The framework can be used as both pretraining on UMLS , and \ufb01ne - tuning on task - speci\ufb01c datasets .", "entities": [[9, 10, "DatasetName", "UMLS"]]}, {"text": "We use an existing BERT model as our starting point .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "In the following , we introduce the key components of our framework .", "entities": []}, {"text": "Formal De\ufb01nition .", "entities": []}, {"text": "Let(x;y)2 X \u0002Y denote a tuple of a name and its categorical label .", "entities": []}, {"text": "For the self - alignment pretraining step , X\u0002Y is the set of all ( name , CUI5 ) pairs in UMLS , e.g. ( Remdesivir , C4726677 ) ; while for the \ufb01netuning step , it is formed as an entity mention and its corresponding mapping from the ontology , e.g. ( scratchy throat , 102618009 ) .", "entities": [[21, 22, "DatasetName", "UMLS"], [49, 50, "MethodName", "ontology"]]}, {"text": "Given any pair of tuples ( xi;yi);(xj;yj)2X\u0002Y , the goal of the self - alignment is to learn a function f(\u0001;\u0012 ) :X !", "entities": []}, {"text": "Rdparameterised by \u0012.", "entities": []}, {"text": "Then , the similarityhf(xi);f(xj)i(in this work we use cosine similarity ) can be used to estimate the resemblance ofxiandxj(i.e . , high if xi;xjare synonyms and low otherwise ) .", "entities": []}, {"text": "We model fby a BERT model with its output", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "[ CLS ] token regarded as the representation of the input.6During the learning , a sampling procedure selects the informative pairs of training samples and uses them in the pairwise metric learning loss function ( introduced shortly ) .", "entities": [[30, 32, "TaskName", "metric learning"], [32, 33, "MetricName", "loss"]]}, {"text": "Online Hard Pairs Mining .", "entities": []}, {"text": "We use an online hard triplet mining condition to \ufb01nd the most 5In UMLS , CUI is the Concept Unique Identi\ufb01er .", "entities": [[13, 14, "DatasetName", "UMLS"]]}, {"text": "6We tried multiple strategies including \ufb01rst - token , meanpooling , [ CLS ] and also NOSPEC ( recommended by Vuli \u00b4 c et al . 2020 ) but found no consistent best strategy ( optimal strategy varies on different * B ERTs ) .", "entities": []}, {"text": "4230informative training examples ( i.e. hard positive / negative pairs ) within a mini - batch for ef\ufb01cient training , Fig .", "entities": []}, {"text": "2 . For biomedical entities , this step can be particularly useful as most examples can be easily classi\ufb01ed while a small set of very hard ones cause the most challenge to representation learning.7We start from constructing all possible triplets for all names within the mini - batch where each triplet is in the form of ( xa;xp;xn ) .", "entities": []}, {"text": "Here xais called anchor , an arbitrary name in the minibatch;xpa positive match of xa(i.e.ya = yp ) and xna negative match of xa(i.e.ya6 = yn ) .", "entities": []}, {"text": "Among the constructed triplets , we select out all triplets that violate the following condition : kf(xa)\u0000f(xp)k2 < kf(xa)\u0000f(xn)k2+\u0015;(1 )", "entities": []}, {"text": "where\u0015is a pre - set margin .", "entities": []}, {"text": "In other words , we only consider triplets with the negative sample closer to the positive sample by a margin of \u0015. These are the hard triplets as their original representations were very far from correct .", "entities": []}, {"text": "Every hard triplet contributes one hard positive pair ( xa;xp ) and one hard negative pair ( xa;xn ) .", "entities": []}, {"text": "We collect all such positive & negative pairs and denote them asP;N.", "entities": []}, {"text": "A similar but not identical triplet mining condition was used by Schroff et al .", "entities": []}, {"text": "( 2015 ) for face recognition to select hard negative samples .", "entities": [[4, 6, "TaskName", "face recognition"]]}, {"text": "Switching - off this mining process , causes a drastic performance drop ( see Tab . 2 ) .", "entities": []}, {"text": "Loss Function .", "entities": []}, {"text": "We compute the pairwise cosine similarity of all the BERT - produced name representations and obtain a similarity matrix S2 RjXbj\u0002jX bjwhere each entry Sijcorresponds to the cosine similarity between the i - th andj - th names in the mini - batch b.", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "We adapted the Multi - Similarity loss ( MS loss , Wang et al . 2019 ) , a SOTA metric learning objective on visual recognition , for learning from the positive and negative pairs : L=1 jXbjjXbjX i=1   1 \u000b log\u0010 1 + X n2Nie \u000b ( Sin\u0000\u000f)\u0011 +1 \f log\u0010 1 + X p2Pie\u0000 \f ( Sip\u0000\u000f)\u0011 !", "entities": [[6, 7, "MetricName", "loss"], [9, 10, "MetricName", "loss"], [20, 22, "TaskName", "metric learning"]]}, {"text": "; ( 2 ) where \u000b ; \f  are temperature scales ; \u000fis an offset applied on the similarity matrix ; Pi;Niare indices 7Most of Hydroxychloroquine \u2019s variants are easy : Hydroxychlorochin , Hydroxychloroquine ( substance ) , Hidroxicloroquina , but a few can be very hard : Plaquenil andHCQ .of", "entities": []}, {"text": "positive and negative samples of the anchori.8 While the \ufb01rst term in Eq . 2 pushes negative pairs away from each other , the second term pulls positive pairs together .", "entities": []}, {"text": "This dynamic allows for a re - calibration of the alignment space using the semantic biases of synonymy relations .", "entities": []}, {"text": "The MS loss leverages similarities among and between positive and negative pairs to re - weight the importance of the samples .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "The most informative pairs will receive more gradient signals during training and thus can better use the information stored in data .", "entities": []}, {"text": "3 Experiments and Discussions 3.1 Experimental Setups Data Preparation Details for UMLS Pretraining .", "entities": [[11, 12, "DatasetName", "UMLS"]]}, {"text": "We download the full release of UMLS 2020AA version.9We then extract all English entries from the MRCONSO.RFF raw \ufb01le and convert all entity names into lowercase ( duplicates are removed ) .", "entities": [[6, 7, "DatasetName", "UMLS"]]}, {"text": "Besides synonyms de\ufb01ned inMRCONSO.RFF , we also include tradenames of drugs as synonyms ( extracted from MRREL.RRF ) .", "entities": []}, {"text": "After pre - processing , a list of 9,712,959 ( name , CUI ) entries is obtained .", "entities": []}, {"text": "However , random batching on this list can lead to very few ( if not none ) positive pairs within a mini - batch .", "entities": []}, {"text": "To ensure suf\ufb01cient positives present in each mini - batch , we generate of\ufb02ine positive pairs in the format of ( name 1 , name 2 , CUI ) where name 1and name 2have the same CUI label .", "entities": []}, {"text": "This can be achieved by enumerating all possible combinations of synonym pairs with common CUIs .", "entities": []}, {"text": "For balanced training , any concepts with more than 50 positive pairs are randomly trimmed to 50 pairs .", "entities": []}, {"text": "In the end we obtain a training list with 11,792,953 pairwise entries .", "entities": []}, {"text": "UMLS Pretraining Details .", "entities": [[0, 1, "DatasetName", "UMLS"]]}, {"text": "During training , we use AdamW ( Loshchilov and Hutter , 2018 ) with a learning rate of 2e-5 and weight decay rate of 1e-2 .", "entities": [[5, 6, "MethodName", "AdamW"], [15, 17, "HyperparameterName", "learning rate"], [20, 23, "HyperparameterName", "weight decay rate"]]}, {"text": "Models are trained on the prepared pairwise UMLS data for 1 epoch ( approximately 50k iterations ) with a batch size of 512 ( i.e. , 256 pairs per mini - batch ) .", "entities": [[7, 8, "DatasetName", "UMLS"], [19, 21, "HyperparameterName", "batch size"]]}, {"text": "We train with Automatic Mixed Precision ( AMP)10provided in PyTorch 1.7.0 .", "entities": [[5, 6, "MetricName", "Precision"]]}, {"text": "This takes approximately 5 hours on our machine ( con8We explored several loss functions such as InfoNCE ( Oord et al . , 2018 ) , NCA loss ( Goldberger et al . , 2005 ) , simple cosine loss ( Phan et al . , 2019 ) , max - margin triplet loss ( Basaldella et al . , 2020 ) but found our choice is empirically better .", "entities": [[12, 13, "MetricName", "loss"], [16, 17, "MethodName", "InfoNCE"], [27, 28, "MetricName", "loss"], [39, 40, "MetricName", "loss"], [52, 54, "MethodName", "triplet loss"]]}, {"text": "See App .", "entities": []}, {"text": "\u00a7 B.2 for comparison .", "entities": []}, {"text": "9https://download.nlm.nih.gov/umls/kss/2020AA/ umls-2020AA-full.zip 10https://pytorch.org/docs/stable/amp.html", "entities": []}, {"text": "4231scienti\ufb01c language social media language modelNCBI BC5CDR - d BC5CDR - c MedMentions AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 vanilla B ERT(Devlin et", "entities": [[6, 7, "DatasetName", "BC5CDR"], [9, 10, "DatasetName", "BC5CDR"], [12, 13, "DatasetName", "MedMentions"], [14, 15, "DatasetName", "COMETA"]]}, {"text": "al . , 2019 ) 67.6 77.0 81.4 89.1 79.8 91.2 39.6 60.2 38.2 43.3 40.4 47.7 + SAPBERT 91.6 95.2 92.7 95.4 96.1 98.0 52.5 72.6 68.4 87.6 59.5 76.8 BIOBERT(Lee et al . , 2020 ) 71.3 84.1 79.8 92.3 74.0 90.0 24.2 38.5 41.4 51.5 35.9 46.1 + SAPBERT 91.0 94.7 93.3 95.5 96.6 97.6 53.0 73.7 72.4 89.1 63.3 77.0 BLUEBERT(Peng et al . , 2019 )", "entities": []}, {"text": "75.7 87.2 83.2 91.0 87.7 94.1 41.6 61.9 41.5 48.5 42.9 52.9 + SAPBERT 90.9 94.0 93.4 96.0 96.7 98.2 49.6 73.1 72.4 89.4 66.0 78.8 CLINICAL BERT(Alsentzer et al . , 2019 ) 72.1 84.5 82.7 91.6 75.9 88.5 43.9 54.3 43.1 51.8 40.6 61.8 + SAPBERT 91.1 95.1 93.0 95.7 96.6 97.7 51.5 73.0 71.1 88.5 64.3 77.3 SCIBERT(Beltagy et al . , 2019 ) 85.1 88.4 89.3 92.8 94.2 95.5 42.3 51.9 48.0 54.8 45.8 66.8 + SAPBERT 91.7 95.2 93.3 95.7 96.6 98.0 50.1 73.9 72.1 88.7 64.5 77.5 UMLSBERT(Michalopoulos et al . , 2020 ) 77.0 85.4 85.5 92.5 88.9 94.1 36.1 55.8 44.4 54.5 44.6 53.0 + SAPBERT 91.2 95.2 92.8 95.5 96.6 97.7 52.1 73.2 72.6 89.3 63.4 76.9 PUBMEDBERT(Gu et al . , 2020 ) 77.8 86.9 89.0 93.8 93.0 94.6 43.9 64.7 42.5 49.6 46.8 53.2 + SAPBERT 92.0 95.6 93.5 96.0 96.5 98.2 50.8 74.4 70.5 88.9 65.9 77.9 supervised SOTA 91.1 93.9 93.2 96.0 96.6 97.2 OOM OOM 87.5 - 79.0 PUBMEDBERT 77.8 86.9 89.0 93.8 93.0 94.6 43.9 64.7 42.5 49.6 46.8 53.2 + SAPBERT 92.0 95.6 93.5 96.0 96.5 98.2 50.8 74.4 70.5 88.9 65.9 77.9 + SAPBERT(ADAPTER 13 % ) 91.5 95.8 93.6 96.3 96.5 98.0 50.7 75.0y67.5 87.1 64.5 74.9 + SAPBERT(ADAPTER 1 % ) 90.9 95.4 93.8y96.5y96.5 97.9 52.2y74.8 65.7 84.0 63.5 74.2 + SAPBERT(FINE - TUNED )", "entities": []}, {"text": "92.3 95.5 93.2 95.4 96.5 97.9 50.4 73.9 89.0y96.2y75.1 ( 81.1y)85.5 ( 86.1y ) BIOSYN 91.1 93.9 93.2 96.0 96.6 97.2 OOM OOM 82.6 87.0 71.3 77.8 + ( init .", "entities": []}, {"text": "w/ )", "entities": []}, {"text": "S APBERT 92.5y96.2y93.6", "entities": []}, {"text": "96.2 96.8 98.4yOOM", "entities": []}, {"text": "OOM 87.6 95.6 77.0 84.2 Table 1 : Top : Comparison of 7 B ERT - based models before and after S APBERT pretraining ( + S APBERT ) .", "entities": []}, {"text": "All results in this section are from unsupervised learning ( not \ufb01ne - tuned on task data ) .", "entities": []}, {"text": "The gradient of green indicates the improvement comparing to the base model ( the deeper the more ) .", "entities": []}, {"text": "Bottom : SAPBERTvs .", "entities": []}, {"text": "SOTA results .", "entities": []}, {"text": "Blue and red denote unsupervised and supervised models .", "entities": []}, {"text": "Bold and underline denote the best and second best results in the column .", "entities": []}, {"text": "\u201c y \u201d denotes statistically signi\ufb01cant better than supervised SOTA ( T - test , \u001a<0:05 ) .", "entities": []}, {"text": "On COMETA , the results inside the parentheses added the supervised SOTA \u2019s dictionary back - off technique ( Basaldella et al . , 2020 ) .", "entities": [[1, 2, "DatasetName", "COMETA"]]}, {"text": "\u201c - \u201d : not reported in the SOTA paper .", "entities": []}, {"text": "\u201c OOM \u201d : out - of - memory ( 192GB+ ) .", "entities": []}, {"text": "\ufb01gurations speci\ufb01ed in App .", "entities": []}, {"text": "\u00a7 B.4 ) .", "entities": []}, {"text": "For other hyperparameters used , please view App .", "entities": []}, {"text": "\u00a7 C.2 .", "entities": []}, {"text": "Evaluation Data and Protocol .", "entities": []}, {"text": "We experiment on 6 different English MEL datasets : 4 in the scienti\ufb01c domain ( NCBI , Do \u02d8gan et", "entities": []}, {"text": "al . 2014 ; BC5CDR - c and BC5CDR - d , Li et al . 2016 ; MedMentions , Mohan and Li 2018 ) and 2 in the social media domain ( COMETA , Basaldella", "entities": [[4, 5, "DatasetName", "BC5CDR"], [8, 9, "DatasetName", "BC5CDR"], [18, 19, "DatasetName", "MedMentions"], [33, 34, "DatasetName", "COMETA"]]}, {"text": "et al . 2020 and AskAPatient , Limsopatham and Collier 2016 ) .", "entities": []}, {"text": "Descriptions of the datasets and their statistics are provided in App .", "entities": []}, {"text": "\u00a7 A. We report Acc @1and Acc @5(denoted as@1and@5 ) for evaluating performance .", "entities": [[4, 5, "MetricName", "Acc"], [6, 7, "MetricName", "Acc"]]}, {"text": "In all experiments , SAPBERTdenotes further pretraining with our self - alignment method on UMLS .", "entities": [[14, 15, "DatasetName", "UMLS"]]}, {"text": "At the test phase , for all SAPBERT models we use nearest neighbour search without further \ufb01ne - tuning on task data ( unless stated otherwise ) .", "entities": []}, {"text": "Except for numbers reported in previous papers , all results are the average of \ufb01ve runs with different random seeds .", "entities": [[19, 20, "DatasetName", "seeds"]]}, {"text": "Fine - Tuning on Task Data .", "entities": []}, {"text": "The red rows in Tab . 1 are results of models ( further ) \ufb01ne - tuned on the training sets of the six MEL datasets .", "entities": []}, {"text": "Similar to pretraining , a positive pair list is generated through traversing the combinations of mention and all ground truth synonyms where mentions are fromthe training set and ground truth synonyms are from the reference ontology .", "entities": [[35, 36, "MethodName", "ontology"]]}, {"text": "We use the same optimiser and learning rates but train with a batch size of 256 ( to accommodate the memory of 1 GPU ) .", "entities": [[12, 14, "HyperparameterName", "batch size"]]}, {"text": "On scienti\ufb01c language datasets , we train for 3 epochs while on AskAPatient and COMETA we train for 15 and 10 epochs respectively .", "entities": [[14, 15, "DatasetName", "COMETA"]]}, {"text": "For BIOSYNon social media language datasets , we empirically found that 10 epochs work the best .", "entities": []}, {"text": "Other con\ufb01gurations are the same as the original B IOSYNpaper .", "entities": []}, {"text": "3.2 Main Results and Analysis * BERT + SAPBERT ( Tab . 1 , top ) .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "We illustrate the impact of SAPBERT pretraining over 7 existingBERT - based models ( * BERT = { BIOBERT , PUBMEDBERT , ... } ) .", "entities": [[15, 16, "MethodName", "BERT"]]}, {"text": "SAPBERT obtains consistent improvement over all * BERTmodels across all datasets , with larger gains ( by up to 31.0 % absolute Acc @1increase ) observed in the social media domain .", "entities": [[22, 23, "MetricName", "Acc"]]}, {"text": "While SCIBERTis the leading model before applying SAPBERT , PUBMEDBERT+SAPBERT performs the best afterwards .", "entities": []}, {"text": "SAPBERT vs. SOTA ( Tab . 1 , bottom ) .", "entities": []}, {"text": "We take PUBMEDBERT+SAPBERT(w / wo \ufb01ne - tuning ) and compare against various published SOTA results ( see App .", "entities": []}, {"text": "\u00a7 C.1 for a full listing of 10 baselines )", "entities": []}, {"text": "4232which", "entities": []}, {"text": "all require task supervision .", "entities": []}, {"text": "For the scienti\ufb01c language domain , the SOTA is BIOSYN(Sung et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "For the social media domain , the SOTA are Basaldella et al .", "entities": []}, {"text": "( 2020 ) and GENRANK ( Xu et", "entities": []}, {"text": "al . , 2020 ) on COMETA and AskAPatient respectively .", "entities": [[6, 7, "DatasetName", "COMETA"]]}, {"text": "All these SOTA methods combine BERT with heuristic modules such as tf - idf , string matching and information retrieval system ( i.e. Apache Lucene ) in a multi - stage manner .", "entities": [[5, 6, "MethodName", "BERT"], [18, 20, "TaskName", "information retrieval"]]}, {"text": "Measured by Acc @1,SAPBERT achieves new SOTA with statistical signi\ufb01cance on 5 of the 6 datasets and for the dataset ( BC5CDR - c ) where SAPBERTis not signi\ufb01cantly better , it performs on par with SOTA ( 96.5 vs. 96.6 ) .", "entities": [[2, 3, "MetricName", "Acc"], [21, 22, "DatasetName", "BC5CDR"]]}, {"text": "Interestingly , on scienti\ufb01c language datasets , SAPBERT outperforms SOTA without any task supervision ( \ufb01ne - tuning mostly leads to over\ufb01tting and performance drops ) .", "entities": []}, {"text": "On social media language datasets , unsupervised SAPBERTlags behind supervised SOTA by large margins , highlighting the well - documented complex nature of social media language ( Baldwin et al . , 2013 ; Limsopatham and Collier , 2015 , 2016 ; Basaldella et al . , 2020 ; Tutubalina et al . , 2020 ) .", "entities": []}, {"text": "However , after \ufb01ne - tuning on the social media datasets ( using the MS loss introduced earlier ) , SAPBERT outperforms SOTA signi\ufb01cantly , indicating that knowledge acquired during the selfaligning pretraining can be adapted to a shifted domain without much effort .", "entities": [[15, 16, "MetricName", "loss"]]}, {"text": "The A DAPTER Variant .", "entities": []}, {"text": "As an option for parameter ef\ufb01cient pretraining , we explore a variant of SAPBERTusing a recently introduced training module named ADAPTER ( Houlsby et al . , 2019 ) .", "entities": []}, {"text": "While maintaining the same pretraining scheme with the same SAPBERTonline mining + MS loss , instead of training from the full model of PUBMEDBERT , we insert new ADAPTER layers between Transformer layers of the \ufb01xed PUBMEDBERT , and only train the weights of these ADAPTER layers .", "entities": [[13, 14, "MetricName", "loss"], [31, 32, "MethodName", "Transformer"]]}, {"text": "In our experiments , we use the enhanced ADAPTER con\ufb01guration by Pfeiffer et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "We include two variants where trained parameters are 13.22 % and 1.09 % of the full SAPBERTvariant .", "entities": []}, {"text": "The ADAPTER variant of SAPBERTachieves comparable performance to full - model - tuning in scienti\ufb01c datasets but lags behind in social media datasets , Tab .", "entities": []}, {"text": "1 .", "entities": []}, {"text": "The results indicate that more parameters are needed in pretraining for knowledge transfer to a shifted domain , in our case , the social media datasets .", "entities": []}, {"text": "The Impact of Online Mining ( Eq . ( 1 ) ) .", "entities": []}, {"text": "As suggested in Tab . 2 , switching off the online hard pairs mining procedure causes a large performance drop in @1and a smaller but still signi\ufb01cant drop in@5 .", "entities": []}, {"text": "This is due to the presence of many easy and already well - separated samples in the mini - batches .", "entities": []}, {"text": "These uninformative training examples dominated the gradients and harmed the learning process .", "entities": []}, {"text": "con\ufb01guration @1 @5 Mining switched - on 67.2 80.3 Mining switched - off 52.3 # 14:976.1#4:2 Table 2 : This table compares P UBMEDBERT+SAPBERT \u2019s performance with and without online hard mining on COMETA ( zeroshot general ) .", "entities": [[33, 34, "DatasetName", "COMETA"]]}, {"text": "Integrating S APBERT in Existing Systems .", "entities": []}, {"text": "SAPBERT can be easily inserted into existing BERT - based MEL systems by initialising the systems with SAPBERT pretrained weights .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "We use the SOTA scienti\ufb01c language system , BIOSYN ( originally initialised with BIOBERTweights ) , as an example and show the performance is boosted across all datasets ( last two rows , Tab . 1 ) .", "entities": []}, {"text": "4 Conclusion We present SAPBERT , a self - alignment pretraining scheme for learning biomedical entity representations .", "entities": []}, {"text": "We highlight the consistent performance boost achieved by SAPBERT , obtaining new SOTA in all six widely used MEL benchmarking datasets .", "entities": []}, {"text": "Strikingly , without any \ufb01ne - tuning on task - speci\ufb01c labelled data , SAPBERT already outperforms the previous supervised SOTA ( sophisticated hybrid entity linking systems ) on multiple datasets in the scienti\ufb01c language domain .", "entities": [[24, 26, "TaskName", "entity linking"]]}, {"text": "Our work opens new avenues to explore for general domain self - alignment ( e.g. by leveraging knowledge graphs such as DBpedia ) .", "entities": [[17, 19, "TaskName", "knowledge graphs"], [21, 22, "DatasetName", "DBpedia"]]}, {"text": "We plan to incorporate other types of relations ( i.e. , hypernymy and hyponymy ) and extend our model to sentence - level representation learning .", "entities": [[23, 25, "TaskName", "representation learning"]]}, {"text": "In particular , our ongoing work using a combination of SAPBERT andADAPTER is a promising direction for tackling sentence - level tasks .", "entities": []}, {"text": "Acknowledgements We thank the three reviewers and the Area Chair for their insightful comments and suggestions .", "entities": []}, {"text": "FL is supported by Grace & Thomas C.H. Chan Cambridge Scholarship .", "entities": [[9, 10, "DatasetName", "Cambridge"]]}, {"text": "NC and MB would like to", "entities": []}, {"text": "4233acknowledge funding from Health Data Research UK as part of the National Text Analytics project .", "entities": []}, {"text": "References Emily Alsentzer , John Murphy , William Boag , WeiHung Weng , Di Jindi , Tristan Naumann , and Matthew McDermott .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Publicly available clinical BERT embeddings .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2nd Clinical Natural Language Processing Workshop , pages 72\u201378 , Minneapolis , Minnesota , USA .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Timothy Baldwin , Paul Cook , Marco Lui , Andrew MacKinlay , and Li Wang .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "How noisy social media text , how diffrnt social media sources ?", "entities": []}, {"text": "InProceedings of the Sixth International Joint Conference on Natural Language Processing ( IJCNLP ) , pages 356\u2013364 , Nagoya , Japan .", "entities": []}, {"text": "Asian Federation of Natural Language Processing .", "entities": []}, {"text": "Marco Basaldella , Fangyu Liu , Ehsan Shareghi , and Nigel Collier .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "COMETA :", "entities": [[0, 1, "DatasetName", "COMETA"]]}, {"text": "A corpus for medical entity linking in the social media .", "entities": [[4, 6, "TaskName", "entity linking"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 3122\u20133137 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Iz Beltagy , Kyle Lo , and Arman Cohan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SciBERT : A pretrained language model for scienti\ufb01c text .", "entities": []}, {"text": "InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3615 \u2013 3620 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Olivier Bodenreider .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "The uni\ufb01ed medical language system ( UMLS ): integrating biomedical terminology .", "entities": [[6, 7, "DatasetName", "UMLS"]]}, {"text": "Nucleic Acids Research , 32 : D267 \u2013 D270 .", "entities": []}, {"text": "Allan Peter Davis , Cynthia J Grondin , Robin J Johnson , Daniela Sciaky , Roy McMorran , Jolene Wiegers , Thomas C Wiegers , and Carolyn J Mattingly .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The comparative toxicogenomics database : update 2019 .", "entities": []}, {"text": "Nucleic Acids Research , 47 : D948 \u2013 D954 .", "entities": []}, {"text": "Allan Peter Davis , Thomas C Wiegers , Michael C Rosenstein , and Carolyn J Mattingly .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "MEDIC : a practical disease vocabulary used at the comparative toxicogenomics database .", "entities": [[0, 1, "DatasetName", "MEDIC"]]}, {"text": "Database .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies ( NAACL ) , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rezarta Islamaj Do \u02d8gan , Robert Leaman , and Zhiyong Lu .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "NCBI disease corpus : a resource for disease name recognition and concept normalization .", "entities": [[0, 3, "DatasetName", "NCBI disease corpus"]]}, {"text": "Journal of Biomedical Informatics , 47:1\u201310 .", "entities": []}, {"text": "Kevin Donnelly .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "SNOMED - CT : The advanced terminology and coding system for eHealth .", "entities": []}, {"text": "Studies in health technology and informatics , 121:279 .", "entities": []}, {"text": "Jennifer D\u2019Souza and Vincent Ng . 2015 .", "entities": []}, {"text": "Sieve - based entity linking for the biomedical domain .", "entities": [[3, 5, "TaskName", "entity linking"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( ACL - IJCNLP ) ( Volume 2 : Short Papers ) , pages 297\u2013302 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Goldberger , Geoffrey E Hinton , Sam T Roweis , and Russ R Salakhutdinov .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Neighbourhood components analysis .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 513\u2013520 .", "entities": []}, {"text": "Yu Gu , Robert Tinn , Hao Cheng , Michael Lucas , Naoto Usuyama , Xiaodong Liu , Tristan Naumann , Jianfeng Gao , and Hoifung Poon .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Domainspeci\ufb01c language model pretraining for biomedical natural language processing .", "entities": []}, {"text": "arXiv:2007.15779 .", "entities": []}, {"text": "Kaiming He , Haoqi Fan , Yuxin Wu , Saining Xie , and Ross Girshick .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Momentum contrast for unsupervised visual representation learning .", "entities": [[0, 2, "MethodName", "Momentum contrast"], [5, 7, "TaskName", "representation learning"]]}, {"text": "In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 9729\u20139738 .", "entities": []}, {"text": "Neil Houlsby , Andrei Giurgiu , Stanislaw Jastrzebski , Bruna Morrone , Quentin de Laroussilhe , Andrea Gesmundo , Mona Attariyan , and Sylvain Gelly .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Parameter - ef\ufb01cient transfer learning for NLP .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "InProceedings of the 36th International Conference on Machine Learning , ICML 2019 , 9 - 15 June 2019 , Long Beach , California , USA , volume 97 of Proceedings of Machine Learning Research , pages 2790\u20132799 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Zongcheng Ji , Qiang Wei , and Hua Xu . 2020 .", "entities": []}, {"text": "BERTbased ranking for biomedical entity normalization .", "entities": []}, {"text": "AMIA Summits on Translational Science Proceedings , 2020:269 .", "entities": []}, {"text": "Donghyeon Kim , Jinhyuk Lee , Chan Ho", "entities": []}, {"text": "So , Hwisang Jeon , Minbyul Jeong , Yonghwa Choi , Wonjin Yoon , Mujeen Sung , , and Jaewoo Kang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A neural named entity recognition and multi - type normalization tool for biomedical text mining .", "entities": [[2, 5, "TaskName", "named entity recognition"]]}, {"text": "IEEE Access , 7:73729\u201373740 .", "entities": []}, {"text": "Robert Leaman and Zhiyong Lu . 2016 .", "entities": []}, {"text": "TaggerOne : joint named entity recognition and normalization with semi - markov models .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}, {"text": "Bioinformatics , 32:2839\u20132846 .", "entities": []}, {"text": "Jinhyuk Lee , Wonjin Yoon , Sungdong Kim , Donghyeon Kim , Sunkyu Kim , Chan Ho", "entities": []}, {"text": "So , and Jaewoo Kang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "BioBERT : a pretrained biomedical language representation model", "entities": []}, {"text": "4234for biomedical text mining .", "entities": []}, {"text": "Bioinformatics , 36(4):1234\u20131240 .", "entities": []}, {"text": "Sunwon Lee , Donghyeon Kim , Kyubum Lee , Jaehoon Choi , Seongsoon Kim , Minji Jeon , Sangrak Lim , Donghee Choi , Sunkyu Kim , Aik - Choon Tan , et al . 2016 .", "entities": []}, {"text": "BEST : next - generation biomedical entity search tool for knowledge discovery from biomedical literature .", "entities": []}, {"text": "PloS one , 11 : e0164680 .", "entities": [[0, 1, "DatasetName", "PloS"]]}, {"text": "Jiao Li , Yueping Sun , Robin J Johnson , Daniela Sciaky , Chih - Hsuan Wei , Robert Leaman , Allan Peter Davis , Carolyn J Mattingly , Thomas C Wiegers , and Zhiyong Lu . 2016 .", "entities": []}, {"text": "BioCreative V CDR task corpus : a resource for chemical disease relation extraction .", "entities": [[2, 3, "DatasetName", "CDR"], [11, 13, "TaskName", "relation extraction"]]}, {"text": "Database , 2016 .", "entities": []}, {"text": "Nut Limsopatham and Nigel Collier .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Adapting phrase - based machine translation to normalise medical terms in social media messages .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1675 \u2013 1680 , Lisbon , Portugal .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nut Limsopatham and Nigel Collier .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Normalising medical concepts in social media texts by learning semantic representation .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , pages 1014\u20131023 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ilya Loshchilov and Frank Hutter .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Decoupled weight decay regularization .", "entities": [[1, 3, "MethodName", "weight decay"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Laurens van der Maaten and Geoffrey Hinton .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Visualizing data using t - SNE .", "entities": []}, {"text": "Journal of machine learning research , 9(Nov):2579\u20132605 .", "entities": []}, {"text": "George Michalopoulos , Yuanxin Wang , Hussam Kaka , Helen Chen , and Alex Wong . 2020 .", "entities": [[9, 10, "DatasetName", "Helen"]]}, {"text": "Umlsbert : Clinical domain knowledge augmentation of contextual embeddings using the uni\ufb01ed medical language system metathesaurus .", "entities": []}, {"text": "arXiv preprint arXiv:2010.10391 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sunil Mohan and Donghui Li .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "MedMentions :", "entities": [[0, 1, "DatasetName", "MedMentions"]]}, {"text": "A large biomedical corpus annotated with UMLS concepts .", "entities": [[6, 7, "DatasetName", "UMLS"]]}, {"text": "In Automated Knowledge Base Construction .", "entities": []}, {"text": "Hyun Oh Song , Yu Xiang , Stefanie Jegelka , and Silvio Savarese .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Deep metric learning via lifted structured feature embedding .", "entities": [[1, 3, "TaskName", "metric learning"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4004\u20134012 .", "entities": []}, {"text": "Aaron van den Oord , Yazhe Li , and Oriol Vinyals .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Representation learning with contrastive predictive coding .", "entities": [[0, 2, "TaskName", "Representation learning"], [3, 6, "MethodName", "contrastive predictive coding"]]}, {"text": "arXiv preprint arXiv:1807.03748 .Yifan", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Peng , Shankai Yan , and Zhiyong Lu . 2019 .", "entities": []}, {"text": "Transfer learning in biomedical natural language processing : An evaluation of bert and elmo on ten benchmarking datasets .", "entities": [[0, 2, "TaskName", "Transfer learning"], [13, 14, "MethodName", "elmo"]]}, {"text": "In Proceedings of the 2019 Workshop on Biomedical Natural Language Processing , pages 58\u201365 .", "entities": []}, {"text": "Jonas Pfeiffer , Ivan Vuli \u00b4 c , Iryna Gurevych , and Sebastian Ruder .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "MAD - X : An Adapter - Based Framework for Multi - Task Cross - Lingual Transfer .", "entities": [[0, 1, "DatasetName", "MAD"], [5, 6, "MethodName", "Adapter"], [13, 17, "TaskName", "Cross - Lingual Transfer"]]}, {"text": "InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 7654\u20137673 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Minh C Phan , Aixin Sun , and Yi Tay .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Robust representation learning of biomedical names .", "entities": [[1, 3, "TaskName", "representation learning"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3275 \u2013 3285 .", "entities": []}, {"text": "Kirk Roberts , Matthew S Simpson , Ellen M V oorhees , and William R Hersh . 2015 .", "entities": []}, {"text": "Overview of the trec 2015 clinical decision support track .", "entities": [[3, 4, "DatasetName", "trec"]]}, {"text": "In TREC .", "entities": [[1, 2, "DatasetName", "TREC"]]}, {"text": "Florian Schroff , Dmitry Kalenichenko , and James Philbin . 2015 .", "entities": []}, {"text": "Facenet :", "entities": []}, {"text": "A uni\ufb01ed embedding for face recognition and clustering .", "entities": [[4, 6, "TaskName", "face recognition"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 815\u2013823 .", "entities": []}, {"text": "Elliot Schumacher , Andriy Mulyar , and Mark Dredze . 2020 .", "entities": []}, {"text": "Clinical concept linking with contextualized neural representations .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8585\u20138592 .", "entities": []}, {"text": "Hoo - Chang Shin , Yang Zhang , Evelina Bakhturina , Raul Puri , Mostofa Patwary , Mohammad Shoeybi , and Raghav Mani . 2020 .", "entities": []}, {"text": "BioMegatron :", "entities": []}, {"text": "Larger biomedical domain language model .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 4700\u20134706 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yifan Sun , Changmao Cheng , Yuhan Zhang , Chi Zhang , Liang Zheng , Zhongdao Wang , and Yichen Wei . 2020 .", "entities": []}, {"text": "Circle loss : A uni\ufb01ed perspective of pair similarity optimization .", "entities": [[1, 2, "MetricName", "loss"]]}, {"text": "In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 6398\u20136407 .", "entities": []}, {"text": "Mujeen Sung , Hwisang Jeon , Jinhyuk Lee , and Jaewoo Kang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Biomedical entity representations with synonym marginalization .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics ( ACL ) , pages 3641\u20133650 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Elena Tutubalina , Artur Kadurin , and Zulfat Miftahutdinov . 2020 .", "entities": []}, {"text": "Fair evaluation in concept normalization : a large - scale comparative analysis for bertbased models .", "entities": []}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics ( COLING ) .", "entities": []}, {"text": "4235Elena Tutubalina , Zulfat Miftahutdinov , Sergey Nikolenko , and Valentin Malykh .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Medical concept normalization in social media posts with recurrent neural networks .", "entities": []}, {"text": "Journal of Biomedical Informatics , 84:93\u2013102 .", "entities": []}, {"text": "Ivan Vuli \u00b4 c , Edoardo Maria Ponti , Robert Litschko , Goran Glava\u0161 , and Anna Korhonen .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Probing pretrained language models for lexical semantics .", "entities": [[1, 4, "TaskName", "pretrained language models"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 7222\u20137240 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xun Wang , Xintong Han , Weilin Huang , Dengke Dong , and Matthew R Scott .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Multi - similarity loss with general pair weighting for deep metric learning .", "entities": [[3, 4, "MetricName", "loss"], [10, 12, "TaskName", "metric learning"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 5022 \u2013 5030 .", "entities": []}, {"text": "Yanshan Wang , Sijia Liu , Naveed Afzal , Majid Rastegar - Mojarad , Liwei Wang , Feichen Shen , Paul Kingsbury , and Hongfang Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A comparison of word embeddings for the biomedical natural language processing .", "entities": [[3, 5, "TaskName", "word embeddings"]]}, {"text": "Journal of Biomedical Informatics , 87:12\u201320 .", "entities": []}, {"text": "Dustin Wright , Yannis Katsis , Raghav Mehta , and Chun - Nan Hsu . 2019 .", "entities": []}, {"text": "Normco :", "entities": []}, {"text": "Deep disease normalization for biomedical knowledge base construction .", "entities": []}, {"text": "In Automated Knowledge Base Construction .", "entities": []}, {"text": "Dongfang Xu , Zeyu Zhang , and Steven Bethard .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A generate - and - rank framework with semantic type regularization for biomedical concept normalization .", "entities": []}, {"text": "InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8452\u20138464 .", "entities": []}, {"text": "A Evaluation Datasets Details We divide our experimental datasets into two categories ( 1 ) scienti\ufb01c language datasests where the data is extracted from scienti\ufb01c papers and ( 2 ) social media language datasets where the data is coming from social media forums like Reddit.com .", "entities": []}, {"text": "For an overview of the key statistics , see Tab .", "entities": []}, {"text": "3 . A.1 Scienti\ufb01c Language Datasets NCBI disease ( Do \u02d8gan et", "entities": [[6, 8, "DatasetName", "NCBI disease"]]}, {"text": "al . , 2014 ) is a corpus containing 793 fully annotated PubMed abstracts and 6,881 mentions .", "entities": []}, {"text": "The mentions are mapped into the MEDIC dictionary ( Davis et al . , 2012 ) .", "entities": [[6, 7, "DatasetName", "MEDIC"]]}, {"text": "We denote this dataset as \u201c NCBI \u201d in our experiments .", "entities": []}, {"text": "BC5CDR ( Li et", "entities": [[0, 1, "DatasetName", "BC5CDR"]]}, {"text": "al . , 2016 ) consists of 1,500 PubMed articles with 4,409 annotated chemicals , 5,818 diseases and 3,116 chemical - disease interactions .", "entities": []}, {"text": "The disease mentions are mapped into the MEDIC dictionary like the NCBI disease corpus .", "entities": [[7, 8, "DatasetName", "MEDIC"], [11, 14, "DatasetName", "NCBI disease corpus"]]}, {"text": "The chemical mentions are mapped into the Comparative Toxicogenomics Database ( CTD ) ( Davis et al . , 2019 ) chemical dictionary .", "entities": []}, {"text": "We denote the disease and chemical mention sets as \u201c BC5CDRd \u201d and \u201c BC5CDR - c \u201d respectively .", "entities": [[14, 15, "DatasetName", "BC5CDR"]]}, {"text": "For NCBI and BC5CDR we use the same data and evaluation protocol by Sung et al .", "entities": [[3, 4, "DatasetName", "BC5CDR"]]}, {"text": "( 2020).11 MedMentions ( Mohan and Li , 2018 ) is a verylarge - scale entity linking dataset containing over 4,000 abstracts and over 350,000 mentions linked to UMLS 2017AA .", "entities": [[2, 3, "DatasetName", "MedMentions"], [15, 17, "TaskName", "entity linking"], [28, 29, "DatasetName", "UMLS"]]}, {"text": "According to Mohan and Li ( 2018 ) , training TAGGER ONE(Leaman and Lu , 2016 ) , a very popular MEL system , on a subset of MedMentions require > 900 GB of RAM .", "entities": [[28, 29, "DatasetName", "MedMentions"], [34, 35, "MethodName", "RAM"]]}, {"text": "Its massive number of mentions and more importantly the used reference ontology ( UMLS 2017AA has 3M+ concepts ) make the application of most MEL systems infeasible .", "entities": [[11, 12, "MethodName", "ontology"], [13, 14, "DatasetName", "UMLS"]]}, {"text": "However , through our metric learning formulation , SAPBERTcan be applied on MedMentions with minimal effort .", "entities": [[4, 6, "TaskName", "metric learning"], [12, 13, "DatasetName", "MedMentions"]]}, {"text": "A.2 Social - Media Language Datasets AskAPatient ( Limsopatham and Collier , 2016 ) includes 17,324 adverse drug reaction ( ADR ) annotations collected from askapatient.com blog posts .", "entities": []}, {"text": "The mentions are mapped to 1,036 medical concepts grounded onto SNOMED - CT ( Donnelly , 2006 ) and AMT ( the Australian Medicines Terminology ) .", "entities": []}, {"text": "For this dataset , we follow the 10 - fold evaluation protocol stated in the original paper.12 COMETA ( Basaldella et al . , 2020 ) is a recently released large - scale MEL dataset that speci\ufb01cally focuses on MEL in the social media domain , containing around 20k medical mentions extracted from health - related discussions on reddit.com .", "entities": [[17, 18, "DatasetName", "COMETA"]]}, {"text": "Mentions are mapped to SNOMED - CT .", "entities": []}, {"text": "We use the \u201c strati\ufb01ed ( general ) \u201d split and follow the evaluation protocol of the original paper.13 B Model & Training Details B.1 The Choice of Base Models We list all the versions of BERT models used in this study , linking to the speci\ufb01c versions in Tab . 5 .", "entities": [[36, 37, "MethodName", "BERT"]]}, {"text": "Note that we exhaustively tried all of\ufb01cial variants of the selected models and the best performing ones are chosen .", "entities": []}, {"text": "All BERTmodels refer to the BERT Base architecture in this paper .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "11https://github.com/dmis-lab/BioSyn 12https://zenodo.org/record/55013 13https://www.siphs.org/corpus", "entities": []}, {"text": "4236dataset NCBI BC5CDR - d BC5CDR - c MedMentions AskAPAtient COMETA ( s.g . )", "entities": [[2, 3, "DatasetName", "BC5CDR"], [5, 6, "DatasetName", "BC5CDR"], [8, 9, "DatasetName", "MedMentions"], [10, 11, "DatasetName", "COMETA"]]}, {"text": "COMETA ( z.g . )", "entities": [[0, 1, "DatasetName", "COMETA"]]}, {"text": "Ontology MEDIC MEDIC CTD UMLS 2017AA SNOMED & AMT SNOMED SNOMED Csearched(Contology ?", "entities": [[0, 1, "MethodName", "Ontology"], [1, 2, "DatasetName", "MEDIC"], [2, 3, "DatasetName", "MEDIC"], [4, 5, "DatasetName", "UMLS"]]}, {"text": "7 7 7 7 3 7 7 jCsearchedj 11,915 11,915 171,203 3,415,665 1,036 350,830 350,830 jSsearchedj 71,923 71,923 407,247 14,815,318 1,036 910,823 910,823 jM trainj 5,134 4,182 5,203 282,091 15,665.2 13,489 14,062 jM validationj 787 4,244 5,347 71,062 792.6 2,176 1,958 jM testj 960 4,424 5,385 70,405 866.2 4,350 3,995 Table 3 : This table contains basic statistics of the MEL datasets used in the study .", "entities": []}, {"text": "Cdenotes the set of concepts ; Sdenotes the set of all surface forms / synonyms of all concepts in C;Mdenotes the set of mentions / queries .", "entities": []}, {"text": "COMETA ( s.g . )", "entities": [[0, 1, "DatasetName", "COMETA"]]}, {"text": "and ( z.g . ) are the strati\ufb01ed ( general ) and zeroshot ( general ) split respectively .", "entities": []}, {"text": "modelNCBI BC5CDR - d BC5CDR - c MedMentions AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 SIEVE -BASED ( D\u2019Souza and Ng , 2015 ) 84.7 - 84.1 - 90.7 - - WORDCNN ( Limsopatham and Collier , 2016 ) - - - - - - - - 81.4 - - WORDGRU+TF - IDF ( Tutubalina et al . , 2018 ) - - - - - - - - 85.7 - - TAGGER ONE(Leaman and Lu , 2016 )", "entities": [[1, 2, "DatasetName", "BC5CDR"], [4, 5, "DatasetName", "BC5CDR"], [7, 8, "DatasetName", "MedMentions"], [9, 10, "DatasetName", "COMETA"]]}, {"text": "87.7 - 88.9 - 94.1 - OOM OOM - - - NORM CO(Wright et", "entities": []}, {"text": "al . , 2019 ) 87.8 - 88.0 - - - - - - - - BNE ( Phan et al . , 2019 ) 87.7 - 90.6 - 95.8 - - - - - - BERTRANK ( Ji et al . , 2020 ) 89.1 - - - - - - - - - - GEN - RANK ( Xu et al . , 2020 ) - - - - - - - - 87.5 - - BIOSYN ( Sung et al . , 2020 ) 91.1 93.9 93.2 96.0 96.6 97.2 OOM OOM 82.6\u000387.0\u000371.3\u000377.8\u0003", "entities": []}, {"text": "DICT+SOILOS + NEURAL ( Basaldella et al . , 2020 ) - - - - - - - - - - 79.0 supervised SOTA 91.1 93.9 93.2 96.0 96.6 97.2 OOM OOM 87.5 - 79.0 Table 4 : A list of baselines on the 6 different MEL datasets , including both scienti\ufb01c and social media language ones .", "entities": []}, {"text": "The last row collects reported numbers from the best performing models .", "entities": []}, {"text": "\u201c \u0003 \u201d denotes results produced using of\ufb01cial released code .", "entities": []}, {"text": "\u201c - \u201d denotes results not reported in the cited paper .", "entities": []}, {"text": "\u201c OOM \u201d means out - of - memoery .", "entities": []}, {"text": "B.2 Comparing Loss Functions We use COMETA ( zeroshot general ) as a benchmark for selecting learning objectives .", "entities": [[6, 7, "DatasetName", "COMETA"]]}, {"text": "Note that this split of COMETA is different from the strati\ufb01ed - general split used in Tab .", "entities": [[5, 6, "DatasetName", "COMETA"]]}, {"text": "4 .", "entities": []}, {"text": "It is very challenging ( so easy to see the difference of the performance ) and also does not directly affect the model \u2019s performance on other datasets .", "entities": []}, {"text": "The results are listed in Tab .", "entities": []}, {"text": "6 . Note that online mining is switched on for all models here .", "entities": []}, {"text": "loss @1 @5 cosine loss ( Phan et al . , 2019 ) 55.1 64.6 max - margin triplet loss ( Basaldella et al . , 2020 ) 64.6 74.6 NCA loss ( Goldberger et al . , 2005 ) 65.2 77.0 Lifted - Structure loss ( Oh Song et al . , 2016 ) 62.0 72.1 InfoNCE ( Oord et al . , 2018 ; He et al . , 2020 ) 63.3 74.2 Circle loss ( Sun et al . , 2020 ) 66.7 78.7 Multi - Similarity loss ( Wang et al . , 2019 ) 67.2 80.3 Table 6 : This table compares loss functions used for S APBERT pretraining .", "entities": [[0, 1, "MetricName", "loss"], [4, 5, "MetricName", "loss"], [18, 20, "MethodName", "triplet loss"], [31, 32, "MetricName", "loss"], [45, 46, "MetricName", "loss"], [57, 58, "MethodName", "InfoNCE"], [76, 77, "MetricName", "loss"], [90, 91, "MetricName", "loss"], [107, 108, "MetricName", "loss"]]}, {"text": "Numbers reported are on COMETA ( zeroshot general ) .", "entities": [[4, 5, "DatasetName", "COMETA"]]}, {"text": "The cosine loss was used by Phan et al .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "( 2019 ) for learning UMLS synonyms for LSTM models .", "entities": [[5, 6, "DatasetName", "UMLS"], [8, 9, "MethodName", "LSTM"]]}, {"text": "The max - margin triplet loss was used by Basaldellaet al .", "entities": [[4, 6, "MethodName", "triplet loss"]]}, {"text": "( 2020 ) for training MEL models .", "entities": []}, {"text": "A very similar ( though not identical ) hinge - loss was used by Schumacher et al .", "entities": [[10, 11, "MetricName", "loss"]]}, {"text": "( 2020 ) for clinical concept linking .", "entities": []}, {"text": "InfoNCE has been very popular in selfsupervised learning and contrastive learning ( Oord et al . , 2018 ; He et al . , 2020 ) .", "entities": [[0, 1, "MethodName", "InfoNCE"], [9, 11, "MethodName", "contrastive learning"]]}, {"text": "Lifted - Structure loss ( Oh Song et al . , 2016 ) and NCA loss ( Goldberger et al . , 2005 ) are two very classic metric learning objectives .", "entities": [[3, 4, "MetricName", "loss"], [15, 16, "MetricName", "loss"], [28, 30, "TaskName", "metric learning"]]}, {"text": "Multi - Similarity loss ( Wang et al . , 2019 ) and Circle loss ( Sun et al . , 2020 ) are two recently proposed metric learning objectives and have been considered as SOTA on large - scale visual recognition benchmarks .", "entities": [[3, 4, "MetricName", "loss"], [14, 15, "MetricName", "loss"], [27, 29, "TaskName", "metric learning"]]}, {"text": "B.3 Details of A DAPTER s In Tab .", "entities": []}, {"text": "7 we list number of parameters trained in the three ADAPTER variants along with full - modeltuning for easy comparison .", "entities": [[3, 6, "HyperparameterName", "number of parameters"]]}, {"text": "4237model URL vanilla B ERT(Devlin et al . , 2019 ) https://huggingface.co/bert-base-uncased BIOBERT(Lee et al . , 2020 ) https://huggingface.co/dmis-lab/biobert-v1.1", "entities": []}, {"text": "BLUEBERT(Peng et", "entities": []}, {"text": "al . , 2019 ) https://huggingface.co/bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12 CLINICAL BERT(Alsentzer et al . , 2019 ) https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT SCIBERT(Beltagy et al . , 2019 ) https://huggingface.co/allenai/scibert_scivocab_uncased UMLSBERT(Michalopoulos et al . , 2020 ) https://www.dropbox.com/s/qaoq5gfen69xdcc/umlsbert.tar.xz?dl=0 PUBMEDBERT(Gu et al . , 2020 )", "entities": []}, {"text": "https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext Table 5 : This table lists the URL of models used in this study .", "entities": []}, {"text": "method reduction rate # params#params # params in B ERT ADAPTER 13 % 1 14.47 M 13.22 % ADAPTER 1 % 16 0.60 M 1.09 % full - model - tuning - 109.48 M 100 % Table 7 : This table compares number of parameters trained in A DAPTER variants and also full - modeltuning .", "entities": [[6, 7, "MetricName", "params"], [42, 45, "HyperparameterName", "number of parameters"]]}, {"text": "B.4 Hardware Con\ufb01gurations", "entities": []}, {"text": "All our experiments are conducted on a server with speci\ufb01cations listed in Tab . 8 .", "entities": []}, {"text": "hardware speci\ufb01cation RAM 192 GB CPU Intel Xeon W-2255 @3.70GHz , 10 - core 20 - threads GPU NVIDIA GeForce RTX 2080", "entities": [[2, 3, "MethodName", "RAM"]]}, {"text": "Ti ( 11 GB ) \u00024 Table 8 : Hardware speci\ufb01cations of the used machine .", "entities": []}, {"text": "C Other Details C.1", "entities": []}, {"text": "The Full Table of Supervised Baseline Models", "entities": []}, {"text": "The full table of supervised baseline models is provided in Tab .", "entities": []}, {"text": "4 . C.2 Hyper - Parameters Search Scope Tab . 9 lists hyper - parameter search space for obtaining the set of used numbers .", "entities": []}, {"text": "Note that the chosen hyper - parameters yield the overall best performance but might be sub - optimal on any single dataset .", "entities": []}, {"text": "Also , we balanced the memory limit and model performance .", "entities": []}, {"text": "C.3", "entities": []}, {"text": "A High - Resolution Version of Fig .", "entities": []}, {"text": "1 We show a clearer version of t - SNE embedding visualisation in Fig .", "entities": []}, {"text": "3 .", "entities": []}, {"text": "4238hyper - parameters search space learning rate for pretraining & \ufb01ne - tuning S APBERT { 1e-4 , 2e-5\u0003,5e-5 , 1e-5 , 1e-6 } pretraining batch size { 128 , 256 , 512\u0003 , 1024 } pretraining training iterations { 10k , 20k , 30k , 40k , 50k ( 1 epoch)\u0003 , 100k ( 2 epochs ) } \ufb01ne - tuning epochs on scienti\ufb01c language datasets { 1 , 2 , 3\u0003 , 5 } \ufb01ne - training epochs on AskAPatient { 5 , 10 , 15\u0003 , 20 } \ufb01ne - training epochs on COMETA { 5 , 10\u0003 , 15 , 20 } max_seq_length of B ERTtokenizer { 15 , 20 , 25\u0003 , 30 } \u0015in Online Mining { -0.05 , -0.1 , -0.2\u0003 , -0.3 } \u000b in MS loss { 1 , 2 ( Wang et al . , 2019)\u0003 , 3 } \f in MS loss { 40 , 50 ( Wang et al . , 2019)\u0003 , 60 } \u000fin MS loss { 0.5\u0003 , 1 ( Wang et", "entities": [[5, 7, "HyperparameterName", "learning rate"], [25, 27, "HyperparameterName", "batch size"], [96, 97, "DatasetName", "COMETA"], [134, 135, "MetricName", "loss"], [152, 153, "MetricName", "loss"], [169, 170, "MetricName", "loss"]]}, {"text": "al . , 2019 ) } \u000b in max - margin triplet loss { 0.05 , 0.1 , 0.2 ( Basaldella et", "entities": [[11, 13, "MethodName", "triplet loss"]]}, {"text": "al . , 2020)\u0003 , 0.3 } softmax scale in NCA loss { 1 ( Goldberger et al . , 2005 ) , 5 , 10 , 20\u0003 , 30 } \u000b in Lifted - Structured loss { 0.5\u0003 , 1 ( Oh Song et al . , 2016 ) } \u001c ( temperature ) in InfoNCE { 0.07 ( He et al . , 2020)\u0003 , 0.5 ( Oord et al . , 2018 ) } min Circle loss { 0.25 ( Sun et al . , 2020)\u0003 , 0.4 ( Sun et al . , 2020 ) }", "entities": [[7, 8, "MethodName", "softmax"], [11, 12, "MetricName", "loss"], [36, 37, "MetricName", "loss"], [56, 57, "MethodName", "InfoNCE"], [79, 80, "MetricName", "loss"]]}, {"text": "in Circle loss { 80 ( Sun et al . , 2020 ) , 256 ( Sun et al . , 2020)\u0003 } Table 9 : This table lists the search space for hyper - parameters used .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "\u0003means the used ones for reporting results .", "entities": []}, {"text": "PUDMEDBERT + SAPBERT PUDMEDBERT Figure 3 : Same as Fig .", "entities": []}, {"text": "1 in the main text , but generated with a higher resolution .", "entities": []}]