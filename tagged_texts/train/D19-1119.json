[{"text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 1236\u20131242 , Hong Kong , China , November 3\u20137 , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics1236An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction Shun Kiyono1;2Jun Suzuki2;1Masato Mita1;2Tomoya Mizumoto1;2\u0003Kentaro Inui2;1 1RIKEN Center for Advanced Intelligence Project2Tohoku University fshun.kiyono , masato.mita , tomoya.mizumoto g@riken.jp ; fjun.suzuki , inui g@ecei.tohoku.ac.jp Abstract", "entities": [[12, 15, "TaskName", "Grammatical Error Correction"]]}, {"text": "The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models .", "entities": [[9, 12, "TaskName", "grammatical error correction"]]}, {"text": "However , consensus is lacking on experimental con\ufb01gurations , namely , choosing how the pseudo data should be generated or used .", "entities": []}, {"text": "In this study , these choices are investigated through extensive experiments , and state - of - the - art performance is achieved on the CoNLL-2014 test set ( F0:5= 65:0 ) and the of\ufb01cial test set of the BEA-2019 shared task ( F0:5= 70:2 ) without making any modi\ufb01cations to the model architecture .", "entities": []}, {"text": "1 Introduction To date , many studies have tackled grammatical error correction ( GEC ) as a machine translation ( MT ) task , in which ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language .", "entities": [[9, 12, "TaskName", "grammatical error correction"], [17, 19, "TaskName", "machine translation"]]}, {"text": "This approach allows cutting - edge neural MT models to be adopted .", "entities": []}, {"text": "For example , the encoder - decoder ( EncDec ) model ( Sutskever et al . , 2014 ; Bahdanau et al . , 2015 ) , which was originally proposed for MT , has been applied widely to GEC and has achieved remarkable results in the GEC research \ufb01eld ( Ji et al . , 2017 ; Chollampatt and Ng , 2018 ; JunczysDowmunt et al . , 2018 ) .", "entities": []}, {"text": "However , a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data ( Koehn and Knowles , 2017 ) , but the largest set of publicly available parallel data ( Lang-8 ) in GEC has only two million sentence pairs ( Mizumoto et al . , 2011 ) .", "entities": []}, {"text": "Consequently , the method of augmenting the data by incorporating pseudo training data has been studied intensively ( Xie et al . , 2018 ; Ge et al . , 2018 ; Lichtarge et al . , 2019 ;", "entities": []}, {"text": "Zhao et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "\u0003Current af\ufb01liation : Future CorporationWhen incorporating pseudo data , several decisions must be made about the experimental con\ufb01gurations , namely , ( i ) the method of generating the pseudo data , ( ii ) the seed corpus for the pseudo data , and ( iii ) the optimization setting ( Section 2 ) .", "entities": []}, {"text": "However , consensus on these decisions in the GEC research \ufb01eld is yet to be formulated .", "entities": []}, {"text": "For example , Xie et al .", "entities": []}, {"text": "( 2018 ) found that a variant of the backtranslation ( Sennrich et al . , 2016b ) method ( BACKTRANS ( NOISY ) )", "entities": []}, {"text": "outperforms the generation of pseudo data from raw grammatical sentences ( DIRECT NOISE ) .", "entities": []}, {"text": "By contrast , the current state of the art model ( Zhao et al . , 2019 ) uses the DIRECT NOISE method .", "entities": []}, {"text": "In this study , we investigate these decisions regarding pseudo data , our goal being to provide the research community with an improved understanding of the incorporation of pseudo data .", "entities": []}, {"text": "Through extensive experiments , we determine suitable settings for GEC .", "entities": []}, {"text": "We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets .", "entities": []}, {"text": "Speci\ufb01cally , without any task - speci\ufb01c techniques or architecture , our model outperforms not only all previous single - model results but also all ensemble results except for the ensemble result by Grundkiewicz et al .", "entities": []}, {"text": "( 2019)1 .", "entities": []}, {"text": "By applying task - speci\ufb01c techniques , we further improve the performance and achieve state - of - the - art performance on the CoNLL-2014 test set and the of\ufb01cial test set of the BEA-2019 shared task .", "entities": []}, {"text": "2 Problem Formulation and Notation In this section , we formally de\ufb01ne the GEC task discussed in this paper .", "entities": []}, {"text": "Let Dbe the GEC training data that comprise pairs of an ungrammatical source sentence Xand grammatical target sentence 1The paper ( Grundkiewicz et al . 2019 ) has not been published yet at the time of submission .", "entities": []}, {"text": "1237Y , i.e. ,D = f(Xn;Yn)gn .", "entities": []}, {"text": "Here , jDjdenotes the number of sentence pairs in the dataset D. Let\u0002represent all trainable parameters of the model .", "entities": []}, {"text": "Our objective is to \ufb01nd the optimal parameter setb\u0002that minimizes the following objective functionL(D;\u0002)for the given training data D : L(D;\u0002 )", "entities": []}, {"text": "= \u00001 jDjX ( X;Y)2Dlog(p(YjX;\u0002));(1 ) wherep(YjX;\u0002)denotes the conditional probability of YgivenX.", "entities": []}, {"text": "In the standard supervised learning setting , the parallel dataDcomprise only \u201c genuine \u201d parallel dataDg(i.e .", "entities": []}, {"text": ",D = Dg ) .", "entities": []}, {"text": "However , in GEC , incorporating pseudo data Dpthat are generated from grammatical sentences Y2 T , whereTrepresents seed corpus ( i.e. , a set of grammatical sentences ) , is common ( Xie et al . , 2018 ; Zhao et", "entities": []}, {"text": "al . , 2019 ; Grundkiewicz et al . , 2019 ) .", "entities": []}, {"text": "Our interest lies in the following three nontrivial aspects of Equation 1 .", "entities": []}, {"text": "Aspect ( i ) : multiple methods for generating pseudo data Dpare available ( Section 3 ) .", "entities": []}, {"text": "Aspect ( ii ) : options for the seed corpusTare numerous .", "entities": []}, {"text": "To the best of our knowledge , how the seed corpus domain affects the model performance is yet to be shown .", "entities": []}, {"text": "We compare three corpora , namely , Wikipedia , Simple Wikipedia ( SimpleWiki ) and English Gigaword , as a \ufb01rst trial .", "entities": []}, {"text": "Wikipedia and SimpleWiki have similar domains , but different grammatical complexities .", "entities": []}, {"text": "Therefore , we can investigate how grammatical complexity affects model performance by comparing these two corpora .", "entities": []}, {"text": "We assume that Gigaword contains the smallest amount of noise among the three corpora .", "entities": []}, {"text": "We can therefore use Gigaword to investigate whether clean text improves model performance .", "entities": []}, {"text": "Aspect ( iii ) : at least two major settings for incorporatingDpinto the optimization of Equation 1 are available .", "entities": []}, {"text": "One is to use the two datasets jointly by concatenating them as D = Dg[Dp , which hereinafter we refer to as JOINT .", "entities": []}, {"text": "The other is to use Dpfor pretraining , namely , minimizing L(Dp;\u0002 ) to acquire \u00020 , and then \ufb01ne - tuning the model by minimizingL(Dg;\u00020 ) ; hereinafter , we refer to this setting as PRETRAIN .", "entities": []}, {"text": "We investigate these aspects through our extensive experiments ( Section 4 ) .", "entities": []}, {"text": "3 Methods for Generating Pseudo Data In this section , we describe three methods for generating pseudo data .", "entities": []}, {"text": "In Section 4 , we experimentally compare these methods .", "entities": []}, {"text": "BACKTRANS ( NOISY ) and B ACKTRANS ( SAM PLE)Backtranslation for the EncDec model was proposed originally by Sennrich et al . ( 2016b ) .", "entities": []}, {"text": "In backtranslation , a reverse model , which generates an ungrammatical sentence from a given grammatical sentence , is trained .", "entities": []}, {"text": "The output of the reverse model is paired with the input and then used as pseudo data .", "entities": []}, {"text": "BACKTRANS ( NOISY ) is a variant of backtranslation that was proposed by Xie et al .", "entities": []}, {"text": "( 2018)2 .", "entities": []}, {"text": "This method adds r \f random to the score of each hypothesis in the beam for every time step .", "entities": []}, {"text": "Here , noise r is sampled uniformly from the interval [ 0;1 ] , and \f random2R\u00150is a hyper - parameter that controls the noise scale .", "entities": []}, {"text": "If we set \f random = 0 , then BACKTRANS ( NOISY ) is identical to standard backtranslation .", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "BACKTRANS ( SAMPLE ) is another variant of backtranslation , which was proposed by Edunov et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) for MT .", "entities": []}, {"text": "In BACKTRANS ( SAMPLE ) , sentences are decoded by sampling from the distribution of the reverse model .", "entities": []}, {"text": "DIRECT NOISE", "entities": []}, {"text": "Whereas BACKTRANS ( NOISY ) and BACKTRANS ( SAMPLE ) generate ungrammatical sentences with a reverse model , DIRECT NOISE injects noise \u201c directly \u201d into grammatical sentences ( Edunov et al . , 2018 ; Zhao et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "Speci\ufb01cally , for each token in the given sentence , this method probabilistically chooses one of the following operations : ( i ) masking with a placeholder tokenhmaski , ( ii ) deletion , ( iii ) insertion of a random token , and ( iv ) keeping the original3 .", "entities": []}, {"text": "For each token , the choice is made based on the categorical distribution ( \u0016mask;\u0016deletion;\u0016insertion;\u0016keep ) .", "entities": []}, {"text": "4 Experiments The goal of our experiments is to investigate aspect ( i)\u2013(iii ) introduced in Section 2 .", "entities": []}, {"text": "To ensure that the experimental \ufb01ndings are applicable to GEC in general , we design our experiments by using the following two strategies : ( i ) we use an off - the - shelf EncDec model without any task - speci\ufb01c architecture or techniques ; ( ii ) we conduct hyper - parameter tuning , evaluation and comparison of each method or setting on the validation set .", "entities": []}, {"text": "At the end of experiments ( Section 4.5 ) , we summarize our \ufb01ndings and propose suitable settings .", "entities": []}, {"text": "We then perform a single - shot evaluation of their performance on the test set .", "entities": []}, {"text": "2referred as \u201c random noising \u201d in Xie et al .", "entities": []}, {"text": "( 2018 ) 3The detailed algorithm is described in Appendix A.", "entities": []}, {"text": "1238Dataset # sent ( pairs ) # refs .", "entities": []}, {"text": "Split Scorer BEA - train 561,410 1 train BEA - valid 2,377 1 valid ERRANT CoNLL-2014 1,312 2 test ERRANT & M2scorer JFLEG 1,951 4 test GLEU BEA - test 4,477 5 test ERRANT SimpleWiki\u00031,369,460 - - Wikipedia\u0003145,883,941 - - Gigaword\u0003131,864,979 - - Table 1 : Summary of datasets used in our experiments .", "entities": [[22, 23, "DatasetName", "JFLEG"]]}, {"text": "Dataset marked with \u201c * \u201d is a seed corpus", "entities": []}, {"text": "T. 4.1 Experimental Con\ufb01gurations Dataset", "entities": []}, {"text": "The BEA-2019 workshop of\ufb01cial dataset4is the origin of the training and validation data of our experiments .", "entities": []}, {"text": "Hereinafter , we refer to the training data as BEA - train .", "entities": []}, {"text": "We create validation data ( BEA - valid ) by randomly sampling sentence pairs from the of\ufb01cial validation split5 .", "entities": []}, {"text": "As a seed corpus T , we use SimpleWiki6 , Wikipedia7or Gigaword8 .", "entities": []}, {"text": "We apply the noizing methods described in Section 3 to each corpus and generate pseudo data Dp .", "entities": []}, {"text": "The characteristics of each dataset are summarized in Table 1 .", "entities": []}, {"text": "Evaluation We report results on BEA - valid , the of\ufb01cial test set of the BEA-2019 shared task ( BEA - test ) , the CoNLL-2014 test set ( CoNLL2014 ) ( Ng et al . , 2014 ) , and the JFLEG test set ( JFLEG ) ( Napoles et al . , 2017 ) .", "entities": [[42, 43, "DatasetName", "JFLEG"], [46, 47, "DatasetName", "JFLEG"]]}, {"text": "All reported results ( except ensemble ) are the average of \ufb01ve distinct trials using \ufb01ve different random seeds .", "entities": [[18, 19, "DatasetName", "seeds"]]}, {"text": "We report the scores measured by ERRANT ( Bryant et al . , 2017 ; Felice et al . , 2016 ) for BEA - valid , BEA - test , and CoNLL-2014 .", "entities": []}, {"text": "As the reference sentences of BEAtest are publicly unavailable , we evaluate the model outputs on CodaLab9for BEA - test .", "entities": []}, {"text": "We also report results measured by the M2scorer ( Dahlmeier and Ng , 2012 ) on CoNLL-2014 to compare them with those of previous studies .", "entities": []}, {"text": "We use the GLEU metric ( Napoles et al . , 2015 , 2016 ) for JFLEG .", "entities": [[16, 17, "DatasetName", "JFLEG"]]}, {"text": "Model We adopt the Transformer EncDec model ( Vaswani et al . , 2017 ) using the fairseq toolkit ( Ott et al . , 2019 ) and use the \u201c Transformer ( big ) \u201d settings of Vaswani", "entities": [[4, 5, "MethodName", "Transformer"], [31, 32, "MethodName", "Transformer"]]}, {"text": "et", "entities": []}, {"text": "al . ( 2017 ) .", "entities": []}, {"text": "Optimization For the JOINT setting , we opti4Details of the dataset is in Appendix B. 5The detailed data preparation process is in Appendix C. 6https://simple.wikipedia.org 7We used 2019 - 02 - 25 dump \ufb01le at https://dumps .", "entities": []}, {"text": "wikimedia.org/other/cirrussearch/ .", "entities": []}, {"text": "8We used the English Gigaword Fifth Edition ( LDC Catalog No . : LDC2011T07 ) .", "entities": []}, {"text": "9https://competitions.codalab.org/ competitions/20228Method Prec .", "entities": []}, {"text": "Rec .", "entities": []}, {"text": "F0:5 Baseline 46.6 23.1 38.8 BACKTRANS ( SAMPLE )", "entities": []}, {"text": "44.6 27.4 39.6 BACKTRANS ( NOISY ) 42.5 31.3 39.7 DIRECT NOISE 48.9 25.7 41.4 Table 2 : Performance of models on BEA - valid : a value inbold indicates the best result within the column .", "entities": []}, {"text": "The seed corpusTis SimpleWiki .", "entities": []}, {"text": "mize the model with Adam ( Kingma and Ba , 2015 ) .", "entities": [[4, 5, "MethodName", "Adam"]]}, {"text": "For the PRETRAIN setting , we pretrain the model with Adam and then \ufb01ne - tune it on BEA - train using Adafactor ( Shazeer and Stern , 2018)10 .", "entities": [[10, 11, "MethodName", "Adam"], [22, 23, "MethodName", "Adafactor"]]}, {"text": "4.2 Aspect ( i ): Pseudo Data Generation", "entities": []}, {"text": "We compare the effectiveness of the BACKTRANS ( NOISY ) , BACKTRANS ( SAMPLE ) , and DIRECT NOISE methods for generating pseudo data .", "entities": []}, {"text": "In DIRECT NOISE , we set ( \u0016mask;\u0016deletion;\u0016insertion;\u0016keep ) = ( 0:5;0:15;0:15;0:2)11 .", "entities": []}, {"text": "We use \f random = 6 forBACKTRANS ( NOISY ) 12 .", "entities": []}, {"text": "In addition , we use ( i ) the JOINT setting and ( ii ) all of SimpleWiki as the seed corpusTthroughout this section .", "entities": []}, {"text": "The results are summarized in Table 2 . BACKTRANS ( NOISY ) andBACKTRANS ( SAMPLE ) show competitive values of F0:5 .", "entities": []}, {"text": "Given this result , we exclusively use BACKTRANS ( NOISY ) and discard BACKTRANS ( SAMPLE ) for the rest of the experiments .", "entities": []}, {"text": "The advantage of BACKTRANS ( NOISY ) is that its effectiveness in GEC has already been demonstrated by Xie et al .", "entities": []}, {"text": "( 2018 )", "entities": []}, {"text": ".", "entities": []}, {"text": "In addition , in our preliminary experiment , BACKTRANS ( NOISY ) decoded ungrammatical sentence 1.2 times faster than BACKTRANS ( SAMPLE ) did .", "entities": []}, {"text": "We also use DIRECT NOISE because it achieved the best value of F0:5among all the methods .", "entities": []}, {"text": "4.3 Aspect ( ii ): Seed Corpus T", "entities": []}, {"text": "We investigate the effectiveness of the seed corpus Tfor generating pseudo data Dp .", "entities": []}, {"text": "The three corpora ( Wikipedia , SimpleWiki and Gigaword ) are compared in Table 3 .", "entities": []}, {"text": "We set jDpj= 1:4M.", "entities": []}, {"text": "The difference in F0:5is small , which implies that the seed corpusThas only a minor effect on the model performance .", "entities": []}, {"text": "Nevertheless , Gigaword consistently outperforms the other two corpora .", "entities": []}, {"text": "In particular , 10The detailed hyper - parameters are listed in Appendix D. 11These values are derived from preliminary experiments ( Appendix E ) .", "entities": []}, {"text": "12 \f random = 6 achieved the best F0:5 in our preliminary experiments ( Appendix F ) .", "entities": []}, {"text": "1239Method Seed Corpus TPrec .", "entities": []}, {"text": "Rec .", "entities": []}, {"text": "F0:5 Baseline", "entities": []}, {"text": "N / A 46.6 23.1 38.8 BACKTRANS ( NOISY ) Wikipedia 43.8 30.8 40.4 BACKTRANS ( NOISY ) SimpleWiki 42.5 31.3 39.7 BACKTRANS ( NOISY ) Gigaword 43.1 33.1 40.6 DIRECT NOISE Wikipedia 48.3 25.5 41.0 DIRECT NOISE SimpleWiki 48.9 25.7 41.4 DIRECT NOISE Gigaword 48.3 26.9 41.7 Table 3 : Performance on BEA - valid when changing the seed corpusTused for generating pseudo data ( jDpj= 1:4 M ) .", "entities": []}, {"text": "DIRECT NOISE with Gigaword achieves the best value of F0:5among all the con\ufb01gurations .", "entities": []}, {"text": "4.4 Aspect ( iii ): Optimization Setting We compare the JOINT andPRETRAIN optimization settings .", "entities": []}, {"text": "We are interested in how each setting performs when the scale of the pseudo data Dp compared with that of the genuine parallel data Dgis ( i ) approximately the same ( jDpj= 1:4 M ) and ( ii ) substantially bigger ( jDpj= 14 M ) .", "entities": []}, {"text": "Here , we use Wikipedia as the seed corpus Tinstead of SimpleWiki or Gigaword for two reasons .", "entities": []}, {"text": "First , SimpleWiki is too small for the experiment ( b ) ( see Table 1 ) .", "entities": []}, {"text": "Second , the fact that Gigaword is not freely available makes it dif\ufb01cult for other researchers to replicate our results .", "entities": []}, {"text": "( a ) Joint Training or Pretraining Table 4 presents the results .", "entities": []}, {"text": "The most notable result here is that PRETRAIN demonstrates the properties of more pseudo data and better performance , whereas JOINT does not .", "entities": []}, {"text": "For example , in BACKTRANS ( NOISY ) , increasingjDpj(1.4M!14 M ) improves F0:5onPRETRAIN ( 41:1!44:5 ) .", "entities": []}, {"text": "By contrast , F0:5does not improve on JOINT ( 40:4!40:3 ) .", "entities": []}, {"text": "An intuitive explanation for this case is that when pseudo dataDpare substantially more than genuine dataDg , the teaching signal from Dpbecomes dominant in JOINT .PRETRAIN", "entities": []}, {"text": "alleviates this problem because the model is trained with only Dgduring \ufb01ne - tuning .", "entities": []}, {"text": "We therefore suppose that P RETRAIN is crucial for utilizing extensive pseudo data .", "entities": []}, {"text": "( b ) Amount of Pseudo Data We investigate how increasing the amount of pseudo data affects the PRETRAIN setting .", "entities": []}, {"text": "We pretrain the model with different amounts of pseudo data f1.4 M , 7 M , 14 M , 30 M , 70Mg .", "entities": []}, {"text": "The results in Figure 1 show that BACKTRANS ( NOISY ) has superior sample ef\ufb01ciency to DIRECT NOISE .", "entities": []}, {"text": "The best model ( pretrained with 70 M BACKTRANS ( NOISY ) ) achievesOptimization Method jDpjPrec .", "entities": []}, {"text": "Rec .", "entities": []}, {"text": "F0:5 N / A Baseline 0 46.6 23.1 38.8 PRETRAIN BACKTRANS ( NOISY ) 1.4 M 49.6 24.3 41.1 PRETRAIN DIRECT NOISE 1.4 M 48.4 21.2 38.5 JOINT BACKTRANS ( NOISY ) 1.4 M 43.8 30.8 40.4 JOINT DIRECT NOISE 1.4 M 48.3 25.5 41.0 PRETRAIN BACKTRANS ( NOISY ) 14 M 50.6 30.1 44.5 PRETRAIN DIRECT NOISE 14 M", "entities": [[5, 6, "DatasetName", "0"]]}, {"text": "49.8 25.8 42.0 JOINT BACKTRANS ( NOISY ) 14 M 43.0 32.3 40.3 JOINT DIRECT NOISE 14 M 48.7 23.5 40.1 Table 4 : Performance of the model with different optimization settings on BEA - valid .", "entities": []}, {"text": "The seed corpus Tis Wikipedia .", "entities": []}, {"text": "0 10 20 30 40 50 60 70 Amount of Pseudo Data jDpj(M)40424446F0:5score BaselineBacktrans ( noisy ) DirectNoise Figure 1 : Performance on BEA - valid for different amounts of pseudo data ( jDpj ) .", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "The seed corpus Tis Wikipedia .", "entities": []}, {"text": "F0:5=45:9 .", "entities": []}, {"text": "4.5 Comparison with Current Top Models The present experimental results show that the following con\ufb01gurations are effective for improving the model performance : ( i ) the combination of JOINT and Gigaword ( Section 4.3 ) , ( ii ) the amount of pseudo dataDpnot being too large in JOINT ( Section 4.4(a ) ) , and ( iii ) PRETRAIN with BACKTRANS ( NOISY ) using large pseudo data Dp(Section 4.4(b ) ) .", "entities": []}, {"text": "We summarize these \ufb01ndings and attempt to combine PRETRAIN andJOINT .", "entities": []}, {"text": "Specifically , we pretrain the model using 70 M pseudo data of BACKTRANS ( NOISY ) .", "entities": []}, {"text": "We then \ufb01ne - tune the model by combining BEA - train and relatively small DIRECT NOISE pseudo data generated from Gigaword ( we setjDpj= 250 K ) .", "entities": []}, {"text": "However , the performance does not improve on BEA - valid .", "entities": []}, {"text": "Therefore , the best approach available is simply to pretrain the model with large ( 70 M ) BACKTRANS ( NOISY ) pseudo data and then \ufb01ne - tune using BEAtrain , which hereinafter we refer to as PRETLARGE .", "entities": []}, {"text": "We use Gigaword for the seed corpus Tbecause it has the best performance in Table 3 .", "entities": []}, {"text": "We evaluate the performance of PRETLARGE on test sets and compare the scores with the current top models .", "entities": []}, {"text": "Table 5 shows a remarkable result , that is ,", "entities": []}, {"text": "1240CoNLL-2014 ( M2scorer)CoNLL-2014 ( ERRANT)JFLEGBEA - test ( ERRANT ) Model Ensemble Prec .", "entities": []}, {"text": "Rec .", "entities": []}, {"text": "F0:5 Prec .", "entities": []}, {"text": "Rec .", "entities": []}, {"text": "F0:5 GLEU Prec .", "entities": []}, {"text": "Rec .", "entities": []}, {"text": "F0:5 Chollampatt and Ng ( 2018 ) 60.9 23.7 46.4 - - - 51.3 - - Junczys - Dowmunt et al .", "entities": []}, {"text": "( 2018 ) - - 53.0 - - - 57.9 - - Grundkiewicz and Junczys - Dowmunt ( 2018 ) 66.8 34.5 56.3 - - - 61.5 - - Lichtarge et al .", "entities": []}, {"text": "( 2019 ) 65.5 37.1 56.8 - - - 61.6 - - Chollampatt and Ng ( 2018 ) X 65.5 33.1 54.8 - - - 57.5 - - Junczys - Dowmunt et al .", "entities": []}, {"text": "( 2018 ) X 61.9 40.2 55.8 - - - 59.9 - - Lichtarge et al .", "entities": []}, {"text": "( 2019 ) X 66.7 43.9 60.4 - - - 63.3 - - Zhao et al .", "entities": []}, {"text": "( 2019 ) X 71.6 38.7 61.2 - - - 61.0 - - Grundkiewicz et al .", "entities": []}, {"text": "( 2019 ) X - - 64.2 - - - 61.2 72.3 60.1 69.5 PRETLARGE 67.9 44.1 61.3 61.2 42.0 56.0 59.7 65.5 59.4 64.2 PRETLARGE + SSE+R2L X 72.4 46.1 65.0 67.3 44.0 60.9 61.4 72.1 61.8 69.8 PRETLARGE", "entities": []}, {"text": "+ SSE+R2L+SED X", "entities": []}, {"text": "73.3 44.2 64.7 68.1 42.1 60.6 61.2 74.7 56.7 70.2 Table 5 : Comparison of our best model and current top models : a bold value indicates the best result within the column .", "entities": []}, {"text": "ourPRETLARGE achieves F0:5= 61:3on CoNLL2014 .", "entities": []}, {"text": "This result outperforms not only all previous single - model results but also all ensemble results except for that by Grundkiewicz et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "To further improve the performance , we incorporate the following techniques that are widely used in shared tasks such as BEA-2019 and WMT13 : Synthetic Spelling Error ( SSE ) Lichtarge et", "entities": [[26, 27, "MetricName", "Error"], [28, 29, "MethodName", "SSE"]]}, {"text": "al .", "entities": []}, {"text": "( 2019 ) proposed the method of probabilistically injecting character - level noise into the source sentence of pseudo data Dp .", "entities": []}, {"text": "Speci\ufb01cally , one of the following operations is applied randomly at a rate of 0.003 per character : deletion , insertion , replacement , or transposition of adjacent characters .", "entities": []}, {"text": "Right - to - left Re - ranking ( R2L ) Following Sennrich", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2016a , 2017 ) ;", "entities": []}, {"text": "Grundkiewicz et al .", "entities": []}, {"text": "( 2019 ) , we train four right - to - left models .", "entities": []}, {"text": "The ensemble of four left - to - right models generate n - best candidates and their corresponding scores ( i.e. , conditional probabilities ) .", "entities": []}, {"text": "We then pass each candidate to the ensemble of the four right - to - left models and compute the score .", "entities": []}, {"text": "Finally , we re - rank the n - best candidates based on the sum of the two scores .", "entities": []}, {"text": "Sentence - level Error Detection ( SED ) SED classi\ufb01es whether a given sentence contains a grammatical error .", "entities": [[3, 4, "MetricName", "Error"]]}, {"text": "Asano et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) proposed incorporating SED into the evaluation pipeline and reported improved precision .", "entities": []}, {"text": "Here , the GEC model is applied only if SED detects a grammatical error in the given source sentence .", "entities": []}, {"text": "The motivation is that SED could potentially reduce the number of false - positive errors of the GEC model .", "entities": []}, {"text": "We use the re - implementation of the BERT - based SED model ( Asano et al . , 2019 ) .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "Table 5 presents the results of applying SSE , 13http://www.statmt.org/wmt19/R2L , and SED .", "entities": [[7, 8, "MethodName", "SSE"]]}, {"text": "It is noteworthy that PRETLARGE + SSE+R2L achieves state - of - the - art performance on both CoNLL-2014 ( F0:5= 65:0 ) and BEA - test ( F0:5= 69:8 ) , which are better than those of the best system of the BEA-2019 shared task ( Grundkiewicz et al . , 2019 ) .", "entities": []}, {"text": "In addition , PRETLARGE + SSE+R2L + SED can further improve the performance on BEA - test ( F0:5= 70:2 ) .", "entities": []}, {"text": "However , unfortunately , incorporating SED decreased the performance on CoNLL-2014 and JFLEG .", "entities": [[12, 13, "DatasetName", "JFLEG"]]}, {"text": "This fact implies that SED is sensitive to the domain of the test set since the SED model is \ufb01ne - tuned with the of\ufb01cial validation split of BEA dataset .", "entities": []}, {"text": "We leave this sensitivity issue as our future work .", "entities": []}, {"text": "5 Conclusion In this study , we investigated several aspects of incorporating pseudo data for GEC .", "entities": []}, {"text": "Through extensive experiments , we found the following to be effective : ( i ) utilizing Gigaword as the seed corpus , and ( ii ) pretraining the model with BACKTRANS ( NOISY ) data .", "entities": []}, {"text": "Based on these \ufb01ndings , we proposed suitable settings for GEC .", "entities": []}, {"text": "We demonstrated the effectiveness of our proposal by achieving stateof - the - art performance on the CoNLL-2014 test set and the BEA-2019 test set .", "entities": []}, {"text": "Acknowledgements We thank the three anonymous reviewers for their insightful comments .", "entities": []}, {"text": "We are deeply grateful to Takumi Ito and Tatsuki Kuribayashi for kindly sharing the re - implementation of BACKTRANS ( NOISY ) .", "entities": []}, {"text": "The work of Jun Suzuki was supported in part by JSPS KAKENHI Grant Number JP19104418 and AIRPF Grant Number 30AI036 - 8 .", "entities": []}, {"text": "1241References Hiroki Asano , Masato Mita , Tomoya Mizumoto , and Jun Suzuki . 2019 .", "entities": []}, {"text": "The AIP - Tohoku System at the BEA-2019 Shared Task .", "entities": []}, {"text": "In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications ( BEA 2019 ) , pages 176\u2013182 .", "entities": []}, {"text": "Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Neural Machine Translation by Jointly Learning to Align and Translate .", "entities": [[1, 3, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 3rd International Conference on Learning Representations ( ICLR 2015 ) .", "entities": []}, {"text": "Christopher Bryant , Mariano Felice , and Ted Briscoe . 2017 .", "entities": []}, {"text": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction .", "entities": [[5, 6, "MetricName", "Error"], [8, 11, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( ACL 2017 ) , pages 793\u2013805 .", "entities": []}, {"text": "Shamil Chollampatt and Hwee Tou Ng . 2018 .", "entities": []}, {"text": "A Multilayer Convolutional Encoder - Decoder Neural Network for Grammatical Error Correction .", "entities": [[9, 12, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the Thirty - Second AAAI Conference on Arti\ufb01cial Intelligence ( AAAI 2018 ) , pages 5755\u20135762 .", "entities": []}, {"text": "Daniel Dahlmeier and Hwee Tou Ng . 2012 .", "entities": []}, {"text": "Better Evaluation for Grammatical Error Correction .", "entities": [[3, 6, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL 2012 ) , pages 568\u2013572 .", "entities": []}, {"text": "Daniel Dahlmeier , Hwee Tou Ng , and Siew Mei Wu .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Building a Large Annotated Corpus of Learner English : The NUS Corpus of Learner English .", "entities": []}, {"text": "In Proceedings of the 8th Workshop on Building Educational Applications Using NLP ( BEA 2013 ) , pages 22\u201331 .", "entities": []}, {"text": "Sergey Edunov , Myle Ott , Michael Auli , and David Grangier .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Understanding Back - Translation at Scale .", "entities": [[3, 4, "TaskName", "Translation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing ( EMNLP 2018 ) , pages 489\u2013500 .", "entities": []}, {"text": "Mariano Felice , Christopher Bryant , and Ted Briscoe .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Automatic Extraction of Learner Errors in ESL Sentences Using Linguistically Enhanced Alignments .", "entities": []}, {"text": "In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 825\u2013835 .", "entities": []}, {"text": "Tao Ge , Furu Wei , and Ming Zhou .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Fluency Boost Learning and Inference for Neural Grammatical Error Correction .", "entities": [[7, 10, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( ACL 2018 ) , pages 1055\u20131065 .", "entities": []}, {"text": "Sylviane Granger .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "The computer learner corpus : A versatile new source of data for SLA research .", "entities": []}, {"text": "In Sylviane Granger , editor , Learner English on Computer , pages 3\u201318 .", "entities": []}, {"text": "Addison Wesley Longman , London and New York .", "entities": []}, {"text": "Roman Grundkiewicz and Marcin Junczys - Dowmunt .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Near Human - Level Performance in Grammatical Error Correction with Hybrid Machine Translation .", "entities": [[6, 9, "TaskName", "Grammatical Error Correction"], [11, 13, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL 2018 ) , pages 284 \u2013 290 .", "entities": []}, {"text": "Roman Grundkiewicz , Marcin Junczys - Dowmunt , and Kenneth Hea\ufb01eld .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Neural grammatical error correction systems with unsupervised pre - training on synthetic data .", "entities": [[1, 4, "TaskName", "grammatical error correction"], [6, 10, "TaskName", "unsupervised pre - training"]]}, {"text": "In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications ( BEA 2019 ) , pages 252\u2013263 .", "entities": []}, {"text": "Jianshu Ji , Qinlong Wang , Kristina Toutanova , Yongen Gong , Steven Truong , and Jianfeng Gao . 2017 .", "entities": []}, {"text": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction .", "entities": [[7, 10, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( ACL 2017 ) , pages 753\u2013762 .", "entities": []}, {"text": "Marcin Junczys - Dowmunt , Roman Grundkiewicz , Shubha Guha , and Kenneth Hea\ufb01eld .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Approaching Neural Grammatical Error Correction as a Low - Resource Machine Translation Task .", "entities": [[2, 5, "TaskName", "Grammatical Error Correction"], [10, 12, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL 2018 ) , pages 595\u2013606 .", "entities": []}, {"text": "Diederik Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A Method for Stochastic Optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "Stochastic Optimization"]]}, {"text": "In Proceedings of the 3rd International Conference on Learning Representations ( ICLR 2015 ) .", "entities": []}, {"text": "Philipp Koehn and Rebecca Knowles .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Six Challenges for Neural Machine Translation .", "entities": [[4, 6, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the First Workshop on Neural Machine Translation , pages 28\u201339 .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Jared Lichtarge , Chris Alberti , Shankar Kumar , Noam Shazeer , Niki Parmar , and Simon Tong .", "entities": [[7, 8, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Corpora Generation for Grammatical Error Correction .", "entities": [[3, 6, "TaskName", "Grammatical Error Correction"]]}, {"text": "InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL 2019 ) .", "entities": []}, {"text": "Tomoya Mizumoto , Mamoru Komachi , Masaaki Nagata , and Yuji Matsumoto . 2011 .", "entities": []}, {"text": "Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners .", "entities": [[10, 11, "MetricName", "Error"]]}, {"text": "In Proceedings of the 5th International Joint Conference on Natural Language Processing ( IJCNLP 2011 ) , pages 147\u2013155 .", "entities": []}, {"text": "Courtney Napoles , Keisuke Sakaguchi , Matt Post , and Joel Tetreault . 2015 .", "entities": []}, {"text": "Ground Truth for Grammatical Error Correction Metrics .", "entities": [[3, 6, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( ACL & IJCNLP 2015 ) , pages 588\u2013593 .", "entities": []}, {"text": "Courtney Napoles , Keisuke Sakaguchi , Matt Post , and Joel Tetreault . 2016 .", "entities": []}, {"text": "GLEU Without Tuning .", "entities": []}, {"text": "arXiv preprint arXiv:1605.02592 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "1242Courtney Napoles , Keisuke Sakaguchi , and Joel Tetreault . 2017 .", "entities": []}, {"text": "JFLEG : A Fluency Corpus and Benchmark for Grammatical Error Correction .", "entities": [[0, 1, "DatasetName", "JFLEG"], [8, 11, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics ( EACL 2017 ) , pages 229\u2013234 .", "entities": []}, {"text": "Hwee Tou Ng , Siew Mei Wu , Ted Briscoe , Christian Hadiwinoto , Raymond Hendy Susanto , and Christopher Bryant .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The CoNLL-2014 Shared Task on Grammatical Error Correction .", "entities": [[1, 4, "DatasetName", "CoNLL-2014 Shared Task"], [5, 8, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning : Shared Task , pages 1\u201314 .", "entities": []}, {"text": "Myle Ott , Sergey Edunov , Alexei Baevski , Angela Fan , Sam Gross , Nathan Ng , David Grangier , and Michael Auli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "fairseq :", "entities": []}, {"text": "A Fast , Extensible Toolkit for Sequence Modeling .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL 2019 ) .", "entities": []}, {"text": "Rico Sennrich , Alexandra Birch , Anna Currey , Ulrich Germann , Barry Haddow , Kenneth Hea\ufb01eld , Antonio Valerio Miceli Barone , and Philip Williams . 2017 .", "entities": []}, {"text": "The university of Edinburgh \u2019s neural MT systems for WMT17 .", "entities": []}, {"text": "In Proceedings of the Second Conference on Machine Translation ( WMT 2017 ) , pages 389\u2013399 .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016a .", "entities": []}, {"text": "Edinburgh neural machine translation systems for WMT 16 .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the First Conference on Machine Translation : Volume 2 , Shared Task Papers ( WMT 2016 ) , pages 371\u2013376 .", "entities": [[7, 9, "TaskName", "Machine Translation"], [17, 19, "DatasetName", "WMT 2016"]]}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016b .", "entities": []}, {"text": "Improving Neural Machine Translation Models with Monolingual Data .", "entities": [[2, 4, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( ACL 2016 ) , pages 86\u201396 .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016c .", "entities": []}, {"text": "Neural Machine Translation of Rare Words with Subword Units .", "entities": [[1, 3, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( ACL 2016 ) , pages 1715\u20131725 .", "entities": []}, {"text": "Noam Shazeer and Mitchell Stern .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Adafactor :", "entities": [[0, 1, "MethodName", "Adafactor"]]}, {"text": "Adaptive Learning Rates with Sublinear Memory Cost .", "entities": []}, {"text": "In Proceedings of the 35th International Conference on Machine Learning ( ICML 2018 ) , pages 4603\u20134611 .", "entities": []}, {"text": "Ilya Sutskever , Oriol Vinyals , and Quoc V .", "entities": []}, {"text": "Le . 2014 .", "entities": []}, {"text": "Sequence to Sequence Learning with Neural Networks .", "entities": [[0, 3, "MethodName", "Sequence to Sequence"]]}, {"text": "In Advances in Neural Information Processing Systems 28 ( NIPS 2014 ) , pages 3104\u20133112 .", "entities": []}, {"text": "Christian Szegedy , Vincent Vanhoucke , Sergey Ioffe , Jon Shlens , and Zbigniew Wojna .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Rethinking the inception architecture for computer vision .", "entities": []}, {"text": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR 2016 ) , pages 2818\u20132826.Toshikazu Tajiri , Mamoru Komachi , and Yuji Matsumoto . 2012 .", "entities": []}, {"text": "Tense and Aspect Error Correction for ESL Learners Using Global Context .", "entities": [[3, 4, "MetricName", "Error"]]}, {"text": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics ( ACL 2012 ) , pages 198\u2013202 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention Is All You Need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 31 ( NIPS 2017 ) , pages 5998\u20136008 .", "entities": []}, {"text": "Ziang Xie , Guillaume Genthial , Stanley Xie , Andrew Ng , and Dan Jurafsky .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Noising and Denoising Natural Language : Diverse Backtranslation for Grammar Correction .", "entities": [[2, 3, "TaskName", "Denoising"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL 2018 ) , pages 619\u2013628 .", "entities": []}, {"text": "Helen Yannakoudakis , \u00d8istein E. Andersen , Ardeshir Geranpayeh , Ted Briscoe , and Diane Nicholls .", "entities": [[0, 1, "DatasetName", "Helen"]]}, {"text": "2018 .", "entities": []}, {"text": "Developing an Automated Writing Placement system for ESL Learners .", "entities": []}, {"text": "Applied Measurement in Education , 31(3):251\u2013267 .", "entities": []}, {"text": "Helen Yannakoudakis , Ted Briscoe , and Ben Medlock . 2011 .", "entities": [[0, 1, "DatasetName", "Helen"]]}, {"text": "A New Dataset and Method for Automatically Grading ESOL Texts .", "entities": []}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics ( ACL 2011 ) , pages 180\u2013189 .", "entities": []}, {"text": "Wei Zhao , Liang Wang , Kewei Shen , Ruoyu Jia , and Jingming Liu . 2019 .", "entities": []}, {"text": "Improving Grammatical Error Correction via Pre - Training a Copy - Augmented Architecture with Unlabeled Data .", "entities": [[1, 4, "TaskName", "Grammatical Error Correction"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL 2019 ) .", "entities": []}]