[{"text": "Joint Workshop on Multiword Expressions and Electronic Lexicons , pages 124\u2013129 Barcelona , Spain ( Online ) , December 13 , 2020.124Seen2Unseen at PARSEME Shared Task 2020 : All Roads do not Lead to Unseen Verb - Noun VMWEs Caroline Pasquer University of Tours , LIFAT France first.last@etu.univ - tours.frAgata Savary University of Tours , LIFAT France first.last@univ-tours.fr Carlos Ramisch Aix Marseille Univ , Universit\u00e9 de Toulon , CNRS , LIS , Marseille , France first.last@lis - lab.frJean - Yves Antoine University of Tours , LIFAT France first.last@univ-tours.fr", "entities": []}, {"text": "Abstract We describe the Seen2Unseen system that participated in edition 1.2 of the PARSEME shared task on automatic identi\ufb01cation of verbal multiword expressions ( VMWEs ) .", "entities": []}, {"text": "The identi\ufb01cation of VMWEs that do not appear in the provided training corpora ( called unseen VMWEs ) \u2013 with a focus here on verb - noun VMWEs \u2013 is based on mutual information and lexical substitution or translation of seen VMWEs .", "entities": []}, {"text": "We present the architecture of the system , report results for 14 languages , and propose an error analysis .", "entities": []}, {"text": "1 Introduction The identi\ufb01cation of multiword expressions ( MWEs ) such as spill the beans is a challenging problem ( Baldwin and Kim , 2010 ;", "entities": []}, {"text": "Constant et al . , 2017 ) , all the more so for verbal MWEs ( VMWEs ) subject to morphological ( spill the bean ) and syntactic variability ( the beans were spilled ) .", "entities": []}, {"text": "The PARSEME shared task ( PST ) provided training , development and test corpora ( hereafter Train , Dev , and Test ) manually annotated for VMWEs.1Our system aimed at identifying every VMWE in Test which also appears in Train or Dev , including possible morphological or syntactic variants ( henceforth seen VMWEs ) or not present in Train / Dev ( unseen VMWEs ) .", "entities": []}, {"text": "Unseen VMWE identi\ufb01cation , the main focus of this PST edition , is harder than seen VMWE identi\ufb01cation , as shown by previous results ( Ramisch et al . , 2018 ) .", "entities": []}, {"text": "We submitted two systems : Seen2Seen ( closed track ) and Seen2Unseen ( open track ) .", "entities": []}, {"text": "Seen2Unseen relies on Seen2Seen for the identi\ufb01cation of seen VMWEs and has an additional module for unseen ones .", "entities": []}, {"text": "Its best global unseen F - score ( i.e. not only for verb - noun constructions ) was obtained for Hindi ( 42.66 ) and it reached 25.36 in French , which was our main focus .", "entities": []}, {"text": "Despite the lower global MWE - based F1score of Seen2Unseen ( 63.02 ) compared to Seen2Seen ( 66.23 ) , we describe the former ( Sec . 2 ) , analyse its interesting negative results ( Sec . 3 ) , and conclude with ideas for future work ( Sec . 4 ) .", "entities": []}, {"text": "2 System Description While describing the architecture of our system , we use the notions of a VMWE token ( its occurrence in running text ) and a VMWE type ( abstraction over all occurrences of a given VMWE ) , as introduced by Savary et al . ( 2019b ) .", "entities": []}, {"text": "We represent VMWE types as multisets of lemmas and POS.2Our system uses a mixture of discovery and identi\ufb01cation methods , as de\ufb01ned by Constant et al .", "entities": []}, {"text": "( 2017 ) .", "entities": []}, {"text": "Namely , VMWE discovery consists in generating lists of MWE types out of context , while VMWE identi\ufb01cation marks VMWE tokens in running text .", "entities": []}, {"text": "The system is freely available online ( https://gitlab.com/ cpasquer / st_2020 ) .", "entities": []}, {"text": "This work is licensed under a Creative Commons Attribution 4.0 International License .", "entities": []}, {"text": "License details : http:// creativecommons.org/licenses/by/4.0/ .", "entities": []}, {"text": "1http://hdl.handle.net/11234/1-3367 2VMWEs are represented as multisets ( i.e. bags of elements with repetition allowed ) , since the same lemma and/or POS can occur twice , as in appeler un chat unchat \u2018 to call a cat a cat \u2019 ) \u2018 to call a spade a spade \u2019 .", "entities": [[19, 20, "DatasetName", "lemma"], [45, 46, "MethodName", "spade"], [47, 48, "MethodName", "spade"]]}, {"text": "125Seen2Seen in a nutshell Seen2Seen is a VMWE identi\ufb01cation system dedicated to only those VMWEs which have been previously seen in the training data .", "entities": []}, {"text": "Its detailed description is provided in Pasquer et al .", "entities": []}, {"text": "( 2020 ) , but a brief overview is included here to make the current paper self - contained .", "entities": []}, {"text": "Seen2Seen extracts lemma combinations of VMWEs seen in Train , looking for the same combinations ( within one sentence ) in Test , with an expected high recall .", "entities": [[2, 3, "DatasetName", "lemma"]]}, {"text": "To improve precision , up to eight independent criteria can be used : ( 1 ) component lemmas should be disambiguated by their POS , ( 2 ) components should appear in speci\ufb01c orders ( e.g. the determiner before the noun ) , ( 3 ) the order of \u201c gap \u201d words possibly occurring between components is also considered , ( 4 ) components should not be too far from each other in a sentence , ( 5 ) closer components are preferred over distant ones , ( 6 ) components should be syntactically connected , ( 7 ) nominal components should appear with a previously seen in\ufb02ection , and ( 8) nested VMWEs should be annotated as in Train .", "entities": []}, {"text": "We select the combination of criteria with maximal performance on Dev among all 28= 256 possibilities .", "entities": []}, {"text": "The candidates remaining after applying the criteria are annotated as VMWEs .", "entities": []}, {"text": "This relatively simple system relying on morphosyntactic \ufb01lters and tuned for 8 parameters was evaluated on 11 languages of the PARSEME shared task 1.1 ( Ramisch et al . , 2018 ) .", "entities": []}, {"text": "Seen2Seen outperformed the best systems not only on seen ( F=0.8276 ) , but even on all seen and unseen VMWEs ( F=0.6653).3In edition 1.2 of the PARSEME shared task , Seen2Seen scored best ( out of 2 ) in the global ranking of the closed track and second ( out of 9 ) across both tracks .", "entities": []}, {"text": "It outperformed 6 other open track systems , notably those using complex neural architectures and contextual word embeddings .", "entities": [[16, 18, "TaskName", "word embeddings"]]}, {"text": "We believe that these competitive results are due to carefully taking the nature of VMWEs into account ( Savary et al . , 2019a ) .", "entities": []}, {"text": "Since Seen2Seen , by design , does not account for unseen VMWEs , its score in this category is very low ( F=1.12).4Therefore , it was later extended with a VMWE discovery module .", "entities": []}, {"text": "Seen2Unseen is precisely this extended system .", "entities": []}, {"text": "It relies on Seen2Seen for seen VMWEs and on discovery methods described below for unseen VMWEs .", "entities": []}, {"text": "From Seen2Seen to Seen2Unseen We assume that seen VMWEs could help identify unseen ones by using ( i ) lexical variation , tolerated by some VMWEs ( e.g. take abath / shower ) , and ( ii ) translation , e.g. ( FR)prendre d\u00e9cision \u2018 take decision \u2019 = ( PL)podejmowa \u00b4 c decyzj\u02db e = ( PT)tomar decis\u00e3o = ( SV ) fatta beslut .5We", "entities": []}, {"text": "also expect seen and unseen VMWEs to share characteristics , such as the distance between components or their syntactic dependency relations , e.g. nouns often being objects of verbs .", "entities": []}, {"text": "The categories that should bene\ufb01t from our strategy are , mainly , light - verb constructions ( LVCs ) containing nouns and , in some cases , verbal idioms ( VIDs ) .", "entities": []}, {"text": "These categories are universal , so our method can be applied to the 14 languages of the PST .", "entities": []}, {"text": "Since LVCs are often verb - noun pairs , Seen2Unseen quasiexclusively focuses on them.6Consequently , we do not aim at exhaustively identifying unseen VMWEs , but at determining to what extent seen verb - noun VMWEs can help us discover new unseen ones .", "entities": []}, {"text": "Resources", "entities": []}, {"text": "In addition to the PST Train , Dev and Test corpora , we used the CoNLL 2017 shared task parsed corpora , hereafter CoNLL - ST ( Ginter et al . , 2017).7TheCoNLL - ST corpora were preferred over the PST - provided parsed corpora because they are conveniently released with pre - trained 100 - dimensional word2vec embeddings for the 14 languages of the PST , which we used to generate lexical variants .", "entities": []}, {"text": "Additionally , we used a free library to implement translation towards French and Italian.8 We automatically translated all VMWEs in the other 13 languages into French ( resp .", "entities": []}, {"text": "Italian ) , privileged due to the availability of two Wikitionary - based lexicons in the same format for both languages.9These lexicons were used to lemmatize and POS - tag automatic translations , e.g. ( PT)\ufb01rmar contrato \u2018 sign contract\u2019translation\u0000\u0000\u0000\u0000\u0000\u0000\u0000 !", "entities": []}, {"text": "( FR ) asign\u00e9 uncontratlemma;POS\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 !", "entities": []}, {"text": "signer VERB contrat NOUN.10 3In this paragraph we refer to macro - averaged MWE - based F - scores .", "entities": []}, {"text": "4The score is not null due to different implementations of unseen VMWEs in the evaluation script and in Seen2Seen .", "entities": []}, {"text": "5Languages are referred to with their PST identi\ufb01er : e.g. FRfor French .", "entities": []}, {"text": "6We also model inherently re\ufb02exive verbs with cranberry words , i.e. verbs which never occur without a re\ufb02exive pronoun , e.g. ( FR)s\u2019\u00e9vanouir vs. * \u00e9vanouir .", "entities": []}, {"text": "With 1 VMWE discovered in Portuguese and 3 in French , this module is omitted here .", "entities": []}, {"text": "7http://hdl.handle.net/11234/1-1989 8Googletrans : https://pypi.org/project/googletrans , implementing the Google Translate API .", "entities": [[7, 8, "DatasetName", "Google"]]}, {"text": "9For French : http://redac.univ-tlse2.fr/lexicons/glaff_en.html , for Italian : http://redac .", "entities": []}, {"text": "univ-tlse2.fr/lexiques/glaffit.html 10In case of multiple POS or lemmas , the most frequent verb - noun combination in CoNLL - ST was selected .", "entities": []}, {"text": "126Unseen VMWE identi\ufb01cation To support identi\ufb01cation of unseen VMWEs we use a combination of semi - supervised discovery and identi\ufb01cation methods : lexical replacement , translation and statistical ranking .", "entities": []}, {"text": "For a language L , letSeenV NLbe the set of all seen LVC and VID types having exactly one verb and one noun ( and any number of components with other POS tags ) .", "entities": []}, {"text": "Let each type in SeenV NL be linked with its manually annotated occurrences in Train .", "entities": []}, {"text": "This set is used in the following steps : 1", "entities": []}, {"text": "Lexical replacement :", "entities": []}, {"text": "The idea is to observe lexical variability of seen VMWEs and to generate on this basis new potential VMWEs .", "entities": []}, {"text": "Let LV CL V var contain LVC types in SeenV", "entities": []}, {"text": "NLthat tolerate variation in verbs , e.g. accomplir /effectuer VERB mission NOUN \u2018 ful\ufb01l / perform mission \u2019 .", "entities": []}, {"text": "Similarly , let LV CL Nvar contain LVCs types with variation in nouns , e.g. accomplir VERB mission /t\u00e2che", "entities": []}, {"text": "NOUN \u2018 ful\ufb01l mission / task \u2019 .", "entities": []}, {"text": "Then we de\ufb01ne two sets of candidates : \u000fMIXLcombines each verb in LV CL V var with each noun in LV CL Nvar to predict new combinations .", "entities": []}, {"text": "e.g.effectuer t\u00e2che \u2018 perform task \u2019 .", "entities": []}, {"text": "\u000fSIMLcontains VMWEs from LV CL V var ( resp .", "entities": []}, {"text": "LV CL Nvar ) where we replace the verb ( resp . noun ) by its closest verb ( resp . noun )", "entities": []}, {"text": "according to cosine similarity in CoNLL - ST word embeddings.11 2", "entities": []}, {"text": "Translation : By translating seen VMWE types in one language we obtain a list of VMWE type candidates in another language : \u000fTRANSLis built only for French and Italian , and is empty for other languages .", "entities": [[0, 1, "TaskName", "Translation"]]}, {"text": "TRANSFR(resp .", "entities": []}, {"text": "TRANSIT ) contains automatic translations of each VMWE in SeenV NL0 , with L06 = FR(resp .", "entities": []}, {"text": "L06 = IT ) , into French ( resp .", "entities": []}, {"text": "Italian ) .", "entities": []}, {"text": "We eliminate translations which do not contain exactly one verb and one noun ( and possible components of other POS ) , e.g. due to a wrong translation .", "entities": []}, {"text": "For the remaining translations , we keep only the verb and the noun lemmas .", "entities": []}, {"text": "3", "entities": []}, {"text": "Statistical ranking : This approach is based on statistical characteristics of both seen VMWEs and unseen VMWE candidates .", "entities": []}, {"text": "We \ufb01rst calculate 3 sets of features for the whole SeenV NLlist : \u000fDistLis the maximal verb - noun distance for all VMWE tokens occurring at least twice in SeenV NL .", "entities": []}, {"text": "This should help eliminate candidates whose components are too distant in a sentence .", "entities": []}, {"text": "\u000fPL Dep(Dep V ; Dep N)is the ratio of VMWE tokens in SeenV NLin which the incoming dependencies of the verb and of the noun are Dep VandDep N.", "entities": []}, {"text": "For instance , PFR Dep(root ; obj ) is higher than PFR Dep(root ; nsubj ) because , in French , active voice ( e.g. rendre unevisite \u2018 pay a visit \u2019 ) is more frequent than passive voice ( e.g. malediction futlanc\u00e9e \u2018 curse was cast \u2019 ) .", "entities": []}, {"text": "We thus favour the most commonly observed VMWE dependencies .", "entities": []}, {"text": "\u000fPL Dist(i)is the ratio of VMWE tokens in SeenV NLin which the number of words inserted between the verb and the noun is i. For instance , PFR Dist(0 )", "entities": []}, {"text": "= 0 : 46 , i.e. occurrences in which the verb and the noun are contiguous represent 46 % of SeenV NFR .", "entities": [[1, 2, "DatasetName", "0"], [21, 22, "MethodName", "NFR"]]}, {"text": "This ratio tends to decrease as iincreases : PFR Dist(2 ) = 0 : 11,PFR Dist(5 )", "entities": [[12, 13, "DatasetName", "0"]]}, {"text": "= 0 : 006 , etc .", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "Candidates whose number of intervening words ihas higher PL Dist(i)likely are true VMWEs .", "entities": []}, {"text": "Given these characteristics of seen VMWEs , we proceed to extracting and ranking unseen VMWE candidates .", "entities": []}, {"text": "Namely , CandLis the list of all occurrences of verb - noun pairs in Test such that : ( i ) the verb and the noun are directly connected by a syntactic dependency , ( ii ) the distance between the verb and the noun does not exceed DistL , and ( iii ) the verb and the noun never co - occur with a direct dependency link in Train or in Dev .", "entities": []}, {"text": "The latter condition excludes both seen VMWEs ( already covered by Seen2Seen ) and verb - noun constructions not annotated as VMWEs in Train or Dev , i.e. being no VMWEs , e.g. ( FR ) avoir an \u2018 have year \u2019 inelle a quinze ans \u2018 she is 15 years old \u2019 .CandLis", "entities": []}, {"text": "then ranked by considering statistical properties .", "entities": []}, {"text": "For each candidate cinCandL , we calculate three measures : \u000fP(c)is the estimated joint dependency- and distance - based probability .", "entities": []}, {"text": "Suppose that iis the number of words inserted between c \u2019s verb and noun , and their incoming dependencies are Dep VandDep N , respectively .", "entities": []}, {"text": "Then , P(c ) = PL Dep(Dep V ; Dep N)\u0002PL Dist(i ) .", "entities": []}, {"text": "11In this way , we limit the lexical replacement to only these components whose variability within VMWEs is attested in Train .", "entities": []}, {"text": "We previously applied this method to all seen VMWEs but the results were too noisy .", "entities": []}, {"text": "127List DE EL EU FR GA HE HI MIXL0 ( 0 ) 0.31 ( 42 ) 0.34 ( 41 ) 0.57 ( 21 ) 0 ( 0 ) 0 ( 0 ) 0 ( 0 ) SIML0 ( 0 ) 0 ( 0 ) 0.45 ( 11 ) 0.17 ( 6 ) 0 ( 0 ) 0 ( 0 ) 0 ( 0 ) RANKL0.19 ( 101 ) 0.05 ( 228 ) 0.09 ( 329 ) 0.19 ( 159 ) 0.21 ( 137 ) 0.04 ( 129 ) 0.46 ( 273 ) List IT PL PT RO SV TR ZH MIXL0 ( 0 ) 0.40 ( 20 ) 0.29 ( 35 ) 0 ( 0 ) 0(2 ) 0.48 ( 21 ) 0.29 ( 7 ) SIML0 ( 0 ) 0.33 ( 3 ) 0.20 ( 20 ) 0 ( 0 ) 0 ( 0 ) 0.33 ( 6 ) 0", "entities": [[5, 6, "MethodName", "GA"], [10, 11, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [30, 31, "DatasetName", "0"], [32, 33, "DatasetName", "0"], [34, 35, "DatasetName", "0"], [38, 39, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [42, 43, "DatasetName", "0"], [52, 53, "DatasetName", "0"], [54, 55, "DatasetName", "0"], [56, 57, "DatasetName", "0"], [58, 59, "DatasetName", "0"], [60, 61, "DatasetName", "0"], [62, 63, "DatasetName", "0"], [102, 103, "DatasetName", "0"], [112, 113, "DatasetName", "0"], [114, 115, "DatasetName", "0"], [128, 129, "DatasetName", "0"], [138, 139, "DatasetName", "0"], [140, 141, "DatasetName", "0"], [142, 143, "DatasetName", "0"], [144, 145, "DatasetName", "0"], [150, 151, "DatasetName", "0"]]}, {"text": "( 0 ) RANKL0.11 ( 163 ) 0.14 ( 164 ) 0.08 ( 225 ) 0.03 ( 422 ) 0.21 ( 100 ) 0.15 ( 214 ) 0(32 ) Table 1 : Unseen MWE - based precision ( and number of predicted VMWEs ) in Test for the 14 languages L , when using only MIXL , SIMLorRANKLlists .", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "\u000fAMI ( c)is the augmented mutual information of c \u2019s type in the CoNLL - ST corpus .", "entities": []}, {"text": "MWEs are known to have a Zip\ufb01an distribution and to often mix very frequent words with very rare ones .", "entities": []}, {"text": "AMI is designed speci\ufb01cally to address this phenomenon , so as to leverage the rarely occurring expressions or components ( Zhang et al . , 2009 ): AMI ( x ; y ) = log2P(x;y ) P(x)P(y)(1\u0000P(x;y ) P(x))(1\u0000P(x;y ) P(y ) ) \u000fRR(c)is the reciprocal rank combining the two indicators above .", "entities": []}, {"text": "Let rank P(c)andrank AMI ( c ) be the ranks of cinCandLaccording to the values of P(c)andAMI ( c)withP(c)>0and AMI ( c)>0 .", "entities": []}, {"text": "Then RR(c )", "entities": []}, {"text": "= 1 rank P(c)+1 rank AMI ( c ) .", "entities": []}, {"text": "CandLis then ranked by RR(c ) .", "entities": []}, {"text": "We keep ntop - ranked candidates , where nis estimated by scaling the number ( provided the organizers ) of VIDs and LVCs in Test \u2013 when all the expressions annotated as seen during the Seen2Seen phase have been eliminated \u2013 by the recall of our method on Dev on the target constructions ( unseen verb - noun LVCs and VIDs).12Thisn - best list is called RANKL n. 4", "entities": []}, {"text": "Identi\ufb01cation proper : In step 3", "entities": []}, {"text": "we obtain a list of unseen VMWE candidate tokens Cand Lextracted from Test .", "entities": []}, {"text": "The aim of identi\ufb01cation is to discriminate among true and false VMWEs on this list .", "entities": []}, {"text": "Statistical ranking and retaining top- ncandidates is one possible statistically - based criterion .", "entities": []}, {"text": "But we hypothesise that some candidates whose rank is worse than n , notably due to data sparseness , can still be correct if they result from lexical replacement or translation of seen VMWEs .", "entities": []}, {"text": "Therefore , every cinCandLis annotated as an LVC if cbelongs to RANKL nor ifc \u2019s type belongs to MIXL[SIML[TRANSL .", "entities": []}, {"text": "3 Results Although Seen2Unseen uses 4 lists of candidates , here we analyse their contribution separately , that is , we use one list at a time in step 4", "entities": []}, {"text": "above .", "entities": []}, {"text": "We report unseen MWE / token - based precision.13Sec .", "entities": []}, {"text": "3.1 analyses the impact of MIXL , SIMLandRANKL n , while Sec . 3.2 discusses TRANSLfor French .", "entities": []}, {"text": "3.1 Impact of MIXL , SIMLandRANKL n As shown in Table 1 , using MIXLalone leads to precision values above 0.29 for 7 languages out of 14 .", "entities": []}, {"text": "Conversely , RANKLalone mostly leads to values below 0.22 ( except for Hindi with P= 0.46 ) .", "entities": []}, {"text": "The precision using SIMLalone reaches a maximum of 0.45 for Basque .", "entities": []}, {"text": "The error analysis below suggests ways to improve precision .", "entities": []}, {"text": "In French , using MIXFRalone yields 21 candidates in Test .", "entities": []}, {"text": "Among the 5 false positives , there is one literal reading ( faire dessin \u2018 make drawing \u2019 ) , one omitted VMWE ( recevoir aide \u2018 receive help \u2019 ) and three other verb - noun pairs that could have been disregarded ( being coincidental occurrences ) if we had taken into account not only the existence of the syntactic dependency but also its nature ( e.g. nous avons VERB cinq points \u00e0", "entities": []}, {"text": "l\u2019ordre NOUN .xcomp", "entities": []}, {"text": "du jour \u2018 we have \ufb01ve items on the agenda \u2019 ) .", "entities": []}, {"text": "This major problem for MIXLis shared by SIML , but a speci\ufb01c drawback with SIMLis that not all words that occur in similar contexts are actually similar .", "entities": []}, {"text": "Indeed , we obtain relevant generated unseen 12When the proportion of VIDs and LVCs in Test is unknown , it can be approximated by the analogous proportion in Dev .", "entities": []}, {"text": "13Shortly before submitting the \ufb01nal version of this paper the de\ufb01nition of a seen VMWE was updated by the PST organizers .", "entities": []}, {"text": "Initially , a VMWE from Test was considered seen if a VMWE with the same ( multi-)set of lemmas was annotated at least once in Train .", "entities": []}, {"text": "Now , it is considered seen if it is annotated in Train or in Dev .", "entities": []}, {"text": "In this paper we report on the evaluation results conforming to the previous de\ufb01nition .", "entities": []}, {"text": "The change in de\ufb01nition probably ( slightly ) impacts the results on seen VMWEs but does not impact the general scores ( cf . Sec . 1 ) .", "entities": []}, {"text": "128verb - noun pairs , including synonyms , antonyms and hyponyms , but also irrelevant ones .", "entities": []}, {"text": "We should therefore either use more reliable resources , such as synonym / antonym dictionaries , and/or disregard frequent verbs ( to have , to do , etc . ) .", "entities": []}, {"text": "For these frequent verbs , the more reliable equivalences obtained by MIXLcompared to SIMLshould be preferred ( faire \u2018 do\u2019MIXFR = subir \u2018 suffer \u2019 vs. faire \u2018 do\u2019SIMFR = passer \u2018 pass \u2019 ) .", "entities": []}, {"text": "Indeed , as shown in Table 1 , over 5 languages with MIXLandSIMLcandidates , 4 exhibit a better precision and higher number of candidates for MIXL .", "entities": []}, {"text": "In French , by dividing nby 4 in RANKFR n , the precision would have increased from 0.19 to 0.45 ( 18 VMWEs over 40 candidates ) .", "entities": []}, {"text": "In other words , using RANKL nin step 4", "entities": []}, {"text": "can slightly increase recall but causes a drop in precision , unless nis low .", "entities": []}, {"text": "Hindi appears as an exception : no negative impact is observed with RANKHI ndue to a bias in the corpora ( compound mentioned in the dependency label ) .", "entities": []}, {"text": "3.2 Impact of TRANSL : ( IT ) Traduttore , traditore \u2018 translator , traitor \u2019 ?", "entities": []}, {"text": "With translational equivalences , we hypothesized that TRANSLwould lead to situations such as : \u000fexact matches : ( PT)cometer crime \u2018 commit a crime \u2019 ! ( FR)commettre crime , \u000fpartial matches leading to VMWEs nonetheless : ( PT)causar problema \u2018 cause problem \u2019 ! ( FR ) causer ennui , instead of causer probl\u00e8me , \u000fno match , but another VMWE : ( PT)ter destaque \u2018 highlight \u2019 ! ( FR)mettre en \u00e9vidence .", "entities": []}, {"text": "\u000fliteral , non-\ufb02uent or ambiguous translations ( Constant et al . , 2017 ): ( PT)jogar o toalha \u2018 throw the towel \u2019 ) \u2018 give up\u2019!(FR)jeter la serviette instead of jeter l\u2019\u00e9ponge \u2018 throw the sponge \u2019 , \u000fnon - existing VMWEs in the target language : ( TR)el atma!(FR)lancer main \u2018 throw hand \u2019 We focus on French due to the high number of candidates in TRANSFR .", "entities": []}, {"text": "In Test- FR , among the 44 annotated verb - noun candidates using TRANSFRalone , 18 are actually VMWEs and 3 partially correspond to VMWEs due to omitted determiners , yielding an unseen MWE - based precision of 0.41 and an unseen token - based precision value of 0.48 .", "entities": []}, {"text": "These 21 candidates are mainly provided by Greek ( 10 vs. 6 from PTand 0 from ITorRO ) .", "entities": [[14, 15, "DatasetName", "0"]]}, {"text": "Thus , the size of the training corpora may have more in\ufb02uence on the probability to obtain good translations than the source language family .", "entities": []}, {"text": "The 23 false positives include ( i ) 13 candidates that can be VMWEs or not depending on the context , including coincidental co - occurrences , literal readings and errors in the manually annotated reference Test corpus , and ( ii ) 10 candidates that are not VMWEs , whatever the context , e.g. the inchoative commencer recherche \u2018 start research \u2019 ( from Hebrew ) or payer taxe \u2018 pay tax \u2019 ( from ( PL)uiszcza \u00b4 c op\u0142at\u02db e ) .", "entities": []}, {"text": "Consequently , translation may be a clue to discover unseen VMWEs , since 78 % of CandFR\\ TRANSFRare VMWEs out of context , but barely half of them were manually annotated in context .", "entities": []}, {"text": "As highlighted above , a restriction to the most frequent VMWE syntactic relations could help \ufb01lter out coincidental occurrences corresponding to 39 % of false positives ( e.g. lancer la balle \u00e0 la main OBL : MOD\u2018throw the ball with the hand \u2019 ) .", "entities": []}, {"text": "4 Conclusions and Future Work", "entities": []}, {"text": "We proposed an error analysis for our system Seen2Unseen dedicated to unseen verb - noun VMWE identi\ufb01cation .", "entities": []}, {"text": "It reveals that lexical variation and translation can produce valid unseen VMWEs but their ambiguity in context must be solved : we should take into account both the dependency labels ( to avoid coincidental occurrences ) and the probability of the verb to be light in Train ( to avoid frequent co - ocurrences like fumer cigarette \u2018 smoke cigarette \u2019 ) .", "entities": []}, {"text": "Using contextual rather than non - contextual word embeddings might also be helpful , even if computationally more intensive .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "We could also combine TRANSLandMIXL[SIMLby applying lexical substitution to the translated VMWEs .", "entities": []}, {"text": "Acknowledgements This work was funded by the French PARSEME - FR grant ( ANR-14 - CERA-0001 ) .", "entities": []}, {"text": "We are grateful to Guillaume Vidal for his prototype , and to the anonymous reviewers for their useful comments .", "entities": []}, {"text": "129References Timothy Baldwin and Su Nam Kim . 2010 .", "entities": []}, {"text": "Multiword expressions .", "entities": []}, {"text": "In Nitin Indurkhya and Fred J. Damerau , editors , Handbook of Natural Language Processing , pages 267\u2013292 .", "entities": []}, {"text": "CRC Press , Taylor and Francis Group , Boca Raton , FL , USA , 2 edition .", "entities": [[0, 1, "DatasetName", "CRC"]]}, {"text": "Mathieu Constant , G\u00fcl\u00b8 sen Eryi \u02d8git , Johanna Monti , Lonneke van der Plas , Carlos Ramisch , Michael Rosner , and Amalia Todirascu .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Multiword expression processing : A survey .", "entities": []}, {"text": "Computational Linguistics , 43(4):837 \u2013 892 .", "entities": []}, {"text": "Filip Ginter , Jan Haji \u02c7c , Juhani Luotolahti , Milan Straka , and Daniel Zeman . 2017 .", "entities": []}, {"text": "CoNLL 2017 shared task automatically annotated raw texts and word embeddings .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "LINDAT / CLARIAH - CZ digital library at the Institute of Formal and Applied Linguistics ( \u00daFAL ) , Faculty of Mathematics and Physics , Charles University .", "entities": []}, {"text": "Caroline Pasquer , Agata Savary , Carlos Ramisch , and Jean - Yves Antoine .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Verbal multiword expression identi\ufb01cation : Do we need a sledgehammer to crack a nut ?", "entities": []}, {"text": "In Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons ( MWE - LEX 2020 ) , online , December .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Carlos Ramisch , Silvio Ricardo Cordeiro , Agata Savary , Veronika Vincze , Verginica Barbu Mititelu , Archna Bhatia , Maja Buljan , Marie Candito , Polona Gantar , V oula Giouli , Tunga G\u00fcng\u00f6r , Abdelati Hawwari , Uxoa I\u00f1urrieta , Jolanta Kovalevskait \u02d9e , Simon Krek , Timm Lichte , Chaya Liebeskind , Johanna Monti , Carla Parra Escart\u00edn , Behrang QasemiZadeh , Renata Ramisch , Nathan Schneider , Ivelina Stoyanova , Ashwini Vaidya , and Abigail Walsh .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Edition 1.1 of the PARSEME Shared Task on Automatic Identi\ufb01cation of Verbal Multiword Expressions .", "entities": []}, {"text": "In Proceedings of the Joint Workshop on Linguistic Annotation , Multiword Expressions and Constructions ( LAW - MWE - CxG-2018 ) , pages 222\u2013240 .", "entities": [[15, 16, "DatasetName", "LAW"]]}, {"text": "ACL .", "entities": []}, {"text": "https://aclweb.org/anthology/ W18 - 4925 .", "entities": []}, {"text": "Agata Savary , Silvio Cordeiro , and Carlos Ramisch .", "entities": []}, {"text": "2019a .", "entities": []}, {"text": "Without lexicons , multiword expression identi\ufb01cation will never \ufb02y : A position statement .", "entities": []}, {"text": "In Proceedings of the Joint Workshop on Multiword Expressions and WordNet ( MWE - WN 2019 ) , pages 79\u201391 , Florence , Italy , August .", "entities": [[21, 22, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Agata Savary , Silvio Ricardo Cordeiro , Timm Lichte , Carlos Ramisch , Uxoa", "entities": []}, {"text": "I nurrieta , and V oula Giouli .", "entities": []}, {"text": "2019b .", "entities": []}, {"text": "Literal Occurrences of Multiword Expressions : Rare Birds That Cause a Stir .", "entities": []}, {"text": "The Prague Bulletin of Mathematical Linguistics , 112:5\u201354 , April .", "entities": []}, {"text": "Wen Zhang , Taketoshi Yoshida , Tu Bao Ho , and Xijin Tang .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Augmented mutual information for multi - word extraction .", "entities": []}, {"text": "International Journal of Innovative Computing , Information and Control , 5(2):543\u2013554 .", "entities": []}]