[{"text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 2407\u20132417 November 7\u201311 , 2021 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2021 Association for Computational Linguistics2407Improving Graph - based Sentence Ordering with Iteratively Predicted Pairwise Orderings Shaopeng Lai1 , Ante Wang1 , Fandong Meng2 , Jie Zhou2 , Yubin Ge3 , Jiali Zeng4 , Junfeng Yao1 , Degen Huang5and Jinsong Su1;6\u0003 1Xiamen University , China2Pattern Recognition Center , WeChat AI , Tencent Inc , China 3University of Illinois at Urbana - Champaign , USA4Tencent Cloud Xiaowei , China 5Dalian University of Technology , China6Pengcheng Laboratory , China splai@stu.xmu.edu.cn , fandongmeng@tencent.com , jssu@xmu.edu.cn Abstract Dominant sentence ordering models can be classi\ufb01ed into pairwise ordering models and set - to - sequence models .", "entities": [[8, 10, "TaskName", "Sentence Ordering"], [47, 48, "DatasetName", "WeChat"], [83, 85, "TaskName", "sentence ordering"]]}, {"text": "However , there is little attempt to combine these two types of models , which inituitively possess complementary advantages .", "entities": []}, {"text": "In this paper , we propose a novel sentence ordering framework which introduces two classi\ufb01ers to make better use of pairwise orderings for graph - based sentence ordering ( Yin et al . , 2019 , 2021 ) .", "entities": [[8, 10, "TaskName", "sentence ordering"], [26, 28, "TaskName", "sentence ordering"]]}, {"text": "Specially , given an initial sentence - entity graph , we \ufb01rst introduce a graph - based classi\ufb01er to predict pairwise orderings between linked sentences .", "entities": []}, {"text": "Then , in an iterative manner , based on the graph updated by previously predicted highcon\ufb01dent pairwise orderings , another classi\ufb01er is used to predict the remaining uncertain pairwise orderings .", "entities": []}, {"text": "At last , we adapt a GRN - based sentence ordering model ( Yin et al . , 2019 , 2021 ) on the basis of \ufb01nal graph .", "entities": [[9, 11, "TaskName", "sentence ordering"]]}, {"text": "Experiments on \ufb01ve commonly - used datasets demonstrate the effectiveness and generality of our model .", "entities": []}, {"text": "Particularly , when equipped with BERT ( Devlin et al . , 2019 ) and FHDecoder ( Yin et al . , 2020 ) , our model achieves state - of - the - art performance .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "Our code is available at https:// github.com/DeepLearnXMU/IRSEG .", "entities": []}, {"text": "1 Introduction With the rapid development and increasing applications of natural language processing ( NLP ) , modeling text coherence has become a signi\ufb01cant task , since it can provide bene\ufb01cial information for understanding , evaluating and generating multi - sentence texts .", "entities": []}, {"text": "As an important subtask , sentence ordering aims at recovering unordered sentences back to naturally coherent paragraphs .", "entities": [[5, 7, "TaskName", "sentence ordering"]]}, {"text": "It is required to deal with logic and syntactic consistency , and has increasingly attracted attention due to its wide applications on several tasks such as text generation ( Konstas and Lapata , 2012 ;", "entities": [[26, 28, "TaskName", "text generation"]]}, {"text": "Holtzman et al . , 2018 ) \u0003Corresponding authorand extractive summarization ( Barzilay et al . , 2002 ; Nallapati et al . , 2012 ) .", "entities": [[9, 11, "TaskName", "extractive summarization"]]}, {"text": "Recently , inspired by the great success of deep learning in other NLP tasks , researchers have resorted to neural sentence ordering models , which can be classi\ufb01ed into : pairwise ordering models ( Chen et al . , 2016 ; Agrawal et al . , 2016 ; Li and Jurafsky , 2017 ; Moon et al . , 2019 ; Kumar et al . , 2020 ; Prabhumoye et al . , 2020 ;", "entities": [[20, 22, "TaskName", "sentence ordering"], [61, 62, "DatasetName", "Kumar"]]}, {"text": "Zhu et al . , 2021 ) andset - to - sequence models ( Gong et al . , 2016 ; Nguyen and Joty , 2017 ; Logeswaran et al . , 2018 ; Mohiuddin et al . , 2018 ; Cui et al . , 2018 ; Yin et al . , 2019 ; Oh et al . , 2019 ; Yin et al . , 2020 ; Cui et al . , 2020 ; Yin et al . , 2021 ) .", "entities": []}, {"text": "Generally , the former predicts the relative orderings between pairwise sentences , which are then leveraged to produce the \ufb01nal ordered sentence sequence .", "entities": []}, {"text": "Its advantage lies in the lightweight pairwise ordering predictions , since the predictions only depend on the semantic representations of involved sentences .", "entities": []}, {"text": "By contrast , the latter is mainly based on an encoder - decoder framework , where an encoder is \ufb01rst used to learn contexualized sentence representations by considering other sentences , and then a decoder , such as pointer network ( Vinyals et al . , 2015a ) , outputs ordered sentences .", "entities": [[38, 40, "MethodName", "pointer network"]]}, {"text": "Overall , these two kinds of models have their own strengths , which are complementary to each other .", "entities": []}, {"text": "To combine their advantages , Yin et al .", "entities": []}, {"text": "( 2020 ) propose FHDecoder that is equipped with three pairwise ordering prediction modules to enhance the pointer network decoder .", "entities": [[17, 19, "MethodName", "pointer network"]]}, {"text": "Along this line , Cui et al . ( 2020 ) introduce BERT to exploit the deep semantic connection and relative orderings between sentences and achieve SOTA performance when equipped with FHDecoder .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "However , there still exist two drawbacks : 1 ) their pairwise ordering predictions only depend on involved sentence pairs , without considering other sentences in the same set ; 2 ) their one - pass pairwise ordering predictions are relatively rough , ignoring distinct dif\ufb01culties in", "entities": []}, {"text": "2408predicting different sentence pairs .", "entities": []}, {"text": "Therefore , we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited .", "entities": [[12, 14, "TaskName", "sentence ordering"]]}, {"text": "In this paper , we propose a novel iterative pairwise ordering prediction framework which introduces two classi\ufb01ers to make better use of pairwise orderings for graph - based sentence ordering ( Yin et al . , 2019 , 2021 ) .", "entities": [[28, 30, "TaskName", "sentence ordering"]]}, {"text": "As an extension of SentenceEnity Graph Recurrent Network ( SE - GRN ) ( Yin et al . , 2019 , 2021 ) , our framework enriches the graph representation with iteratively predicted orderings between pairwise sentences , which further bene\ufb01ts the subsequent generation of ordered sentences .", "entities": []}, {"text": "The basic intuitions behind our work are two - fold .", "entities": []}, {"text": "First , learning contextual sentence representations is helpful to predict pairwise orderings .", "entities": []}, {"text": "Second , dif\ufb01culties of predicting ordering vary with respect to different sentence pairs .", "entities": []}, {"text": "Thus , it is more reasonable to \ufb01rst predict the orderings of pairwise sentences easily to be predicted , and then leverage these predicted orderings to re\ufb01ne the predictions for other pairwise sentences .", "entities": []}, {"text": "Concretely , we propose two graph - based classi\ufb01ers to iteratively conduct ordering predictions for pairwise sentences .", "entities": []}, {"text": "The \ufb01rst classi\ufb01er takes the sentence - entity graph ( SE - Graph ) ( Yin et al . , 2019 , 2021 ) as input and yields relative orderings of linked sentences via corresponding probabilities .", "entities": []}, {"text": "Next , in an iterative manner , the second classi\ufb01er enriches the previous graph representation by converting high - value probabilities into the weights of the corresponding edges , and then reconduct graph encoding to predict orderings for the other pairwise sentences .", "entities": []}, {"text": "Based on the \ufb01nal weighted graph representation , we adapt SE - GRN to construct a graph - based sentence ordering model , of which the decoder is also a pointer network .", "entities": [[19, 21, "TaskName", "sentence ordering"], [30, 32, "MethodName", "pointer network"]]}, {"text": "To the best of our knowledge , our work is the \ufb01rst to exploit pairwise orderings to enhance the graph encoding for graph - based set - to - squence sentence ordering .", "entities": [[30, 32, "TaskName", "sentence ordering"]]}, {"text": "To investigate the effectiveness of our framework , we conduct extensive experiments on several commonly - used datasets .", "entities": []}, {"text": "Experimental results and in - depth analyses show that our model enhanced with some proposed technologies ( Devlin et al . , 2019 ; Yin et", "entities": []}, {"text": "al . , 2020 ) achieves the state - of - the - art performance.2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering ( Lapata , 2003 ; Barzilay and Lee , 2004 ; Barzilay and Lapata , 2005 , 2008 ; Elsner and Charniak , 2011 ; Guinaudeau and Strube , 2013 ) .", "entities": [[26, 28, "TaskName", "sentence ordering"]]}, {"text": "Recently , neural network based sentence ordering models have become dominant , consisting of the following two kinds of models : 1)Pairwise models .", "entities": [[5, 7, "TaskName", "sentence ordering"]]}, {"text": "Generally , they \ufb01rst predict the pairwise orderings between sentences and then use them to produce the \ufb01nal sentence order via ranking algorithms ( Chen et al . , 2016 ;", "entities": []}, {"text": "Agrawal et al . , 2016 ; Li and Jurafsky , 2017 ; Kumar et al . , 2020 ; Prabhumoye et al . , 2020 ; Zhu et al . , 2021 ) .", "entities": [[13, 14, "DatasetName", "Kumar"]]}, {"text": "For example , Chen et al .", "entities": []}, {"text": "( 2016 ) \ufb01rst framed sentence ordering as a ranking task conditioned on pairwise scores .", "entities": [[5, 7, "TaskName", "sentence ordering"]]}, {"text": "Agrawal et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2016 ) conducted the same experiments as ( Chen et al . , 2016 ) in the task of image caption storytelling .", "entities": []}, {"text": "Similarly , Li and Jurafsky ( 2017 ) investigated the effectiveness of discriminative and generative models on ordering pairs of sentences in small domains .", "entities": []}, {"text": "Moon et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) proposed a uni\ufb01ed model that incorporates sentence grammar , pairwise coherence and global coherence into a common neural framework .", "entities": []}, {"text": "Recently , Prabhumoye et al . ( 2020 ) and Zhu et al .", "entities": []}, {"text": "( 2021 ) employed ranking techniques to \ufb01nd the right order of the sentences under the constraint of the predicted pairwise sentence ordering ; 2)Set - to - sequence Models .", "entities": [[21, 23, "TaskName", "sentence ordering"]]}, {"text": "Basically , these models are based on an encoder - decoder framework , where the encoder is used to obtain sentence representations and then the decoder produces ordered sentences progressively .", "entities": []}, {"text": "Among them , both Gong et al . ( 2016 ) and Logeswaran et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) explored RNN based encoder , while both Nguyen and Joty ( 2017 ) and Mohiuddin et al .", "entities": []}, {"text": "( 2018 ) employed neural entity grid models as encoders .", "entities": []}, {"text": "Typically , Cui et al . ( 2018 ) proposed ATTOrderNet that uses self - attention mechanism to learn sentence representations .", "entities": []}, {"text": "Inspired by the successful applications of graph neural network ( GNN ) in many NLP tasks ( Song et al . , 2018 ;", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "Xue et al . , 2019 ; Song et al . , 2019 , 2020 ) , Yin", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2019 , 2021 ) represented input sentences with a uni\ufb01ed SE - Graph and then applied GRN to learn sentence representations .", "entities": []}, {"text": "Very recently , we notice that Chowdhury et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2021 ) proposes a BART - based sentence ordering model .", "entities": [[5, 6, "MethodName", "BART"], [8, 10, "TaskName", "sentence ordering"]]}, {"text": "Please note that our porposed framework is compatible with BART ( Lewis et al . , 2020 ) .", "entities": [[9, 10, "MethodName", "BART"]]}, {"text": "For", "entities": []}, {"text": "2409 Figure 1 : The architecture of SE - GRN model ( Yin et al . , 2019 , 2021 ) .", "entities": []}, {"text": "example , we can easily adapt the BART encoder as our sentence encoder .", "entities": [[7, 8, "MethodName", "BART"]]}, {"text": "With similar motivation with ours , that is , to combine advantages of above - mentioned two kinds of models , Yin et al .", "entities": []}, {"text": "( 2020 ) introduced three pairwise ordering predicting modules ( FHDecoder ) to enhance the pointer network decoder of ATTOrderNet .", "entities": [[15, 17, "MethodName", "pointer network"]]}, {"text": "Recently , Cui et al . ( 2020 ) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences .", "entities": [[19, 20, "MethodName", "BERT"]]}, {"text": "However , signi\ufb01cantly different from them , we borrow the idea from the mask - predict framework ( Gu et al . , 2018 ; Ghazvininejad et al . , 2019 ; Deng et al . , 2020 ) to progressively incorporate pairwise ordering information into SE - Graph , which is the basis of our graph - based sentence ordering model .", "entities": [[32, 35, "DatasetName", "Deng et al"], [59, 61, "TaskName", "sentence ordering"]]}, {"text": "To the best of our knowledge , our work is the \ufb01rst attempt to explore iteratively re\ufb01ned GNN for sentence ordering .", "entities": [[19, 21, "TaskName", "sentence ordering"]]}, {"text": "3 Background In this section , we give a brief introduction to the SE - GRN ( Yin et al . , 2019 , 2021 ) , which is selected as our baseline due to its competitive performance .", "entities": []}, {"text": "As shown in Figure 1 , SE - GRN is composed of a Bi - LSTM sentence encoder , GRN ( Zhang et al . , 2018 ) paragraph encoder , and a pointer network ( Vinyals et al . , 2015b ) decoder .", "entities": [[15, 16, "MethodName", "LSTM"], [33, 35, "MethodName", "pointer network"]]}, {"text": "3.1 Sentence - Entity Graph The SE - GRN takes Isentencess=", "entities": []}, {"text": "[ so1;:::;s oI ] as input and tries to predict their correct order o\u0003= [ o\u0003 1;:::;o\u0003 I ] .", "entities": []}, {"text": "At \ufb01rst , each sentence soi is fed into a Bi - LSTM sentence encoder , where the concatenation of the last hidden states in two directions is used as the context - aware sentence representation \u0014(0 ) oi .", "entities": [[12, 13, "MethodName", "LSTM"]]}, {"text": "As illustrated in the middle of Figure 1 , each input sentence set is rep - resented as an undirected sentence - entity graph G= ( V;E ) ,", "entities": []}, {"text": "whereV = fvigI i=1[f^vjgJ j=1and E", "entities": []}, {"text": "= fei;i0gI;I i=1;i0=1[f\u0016ei;jgI;J i=1;j=1[f^ej;j0gJ;J j=1;j0=1 represent the nodes and edges respectively .", "entities": []}, {"text": "Here , nodes include sentence nodes ( such asvi ) and entity nodes ( such as ^vj ) , and each edge is 1 ) sentencesentence edge ( ss - edge , such asei;i0 ) linking two sentences having the same entity ; or 2 ) sentenceentity edge ( se - edge , such as \u0016ei;j ) connecting an entity to a sentence that contains it .", "entities": [[8, 9, "MethodName", "asvi"]]}, {"text": "Each se - edge is assigned with a label including subject , object or other , based on the syntactic role of its involved entity ; or 3 ) entity - entity edge ( ee - edge , such as ^ej;j0 ) connecting two semantic related entities .", "entities": []}, {"text": "Besides , a virtual global node connecting to all nodes is introduced to capture global information effectively .", "entities": []}, {"text": "3.2 Paragraph Encoding with GRN Node representations of each sentence and each entity are \ufb01rst initialized with the concatenation of bidirectional last states of the Bi - LSTM sentence encoder and the corresponding GloVe word embedding , respectively .", "entities": [[27, 28, "MethodName", "LSTM"], [33, 34, "MethodName", "GloVe"]]}, {"text": "Then , a GRN is adapted to encode the above sentence - entity graph , where node states are updated iteratively .", "entities": []}, {"text": "During the process of updating hidden states , the messages for each node are aggregated from its adjacent nodes .", "entities": []}, {"text": "Speci\ufb01cally , the sentence - level message m(l ) iand entity - level message ~m(l ) ifor a sentence siare de\ufb01ned as follows : m(l ) i = X vi02Niw(\u0014(l-1 ) i;\u0014(l-1 ) i0)\u0014(l-1 ) i0 ; ^m(l )", "entities": []}, {"text": "i = X vj2^Ni\u0016w(\u0014(l-1 ) i;\u000f(l-1 ) j;rij)\u000f(l-1 ) j;(1 ) where\u0014(l-1 ) i0and\u000f(l-1 ) jstand for the neighboring sentence and entity representations of the i - th sentence nodeviat the ( l\u00001)-th layer , Niand^Nidenote the sets of neighboring sentences and entities of vi , and bothw(\u0003)and\u0016w(\u0003)are gating functions with single - layer networks , involving associated node states and edge label rij(if any ) .", "entities": []}, {"text": "Afterwards,\u0014(l-1 ) iis updated by concatenating its original representation \u0014(0 )", "entities": []}, {"text": "i , the messages from neighbours ( m(l ) iand^m(l ) i ) and the global state g(l-1)via GRU : \u0018(l ) i=", "entities": [[18, 19, "MethodName", "GRU"]]}, {"text": "[ \u0014(0 ) i;m(l ) i;^m(l ) i;g(l-1 ) ] ; \u0014(l ) i", "entities": []}, {"text": "= GRU(\u0018(l ) i;\u0014(l-1 ) i0):(2 ) Similar to updating sentence nodes , each entity state\u000f(l-1 ) jis updated based on its word embedding", "entities": []}, {"text": "2410 Figure 2 : The architecture of our model during inference .", "entities": []}, {"text": "IRSE - Graph is a weighted graph representation , of which weights of ss - edges are iteratively re\ufb01ned by iterative classi\ufb01er .", "entities": []}, {"text": "Note that we construct the sentence ordering model based on the \ufb01nal IRSE - Graph .", "entities": [[5, 7, "TaskName", "sentence ordering"]]}, {"text": "emb j , hidden states of its connected sentence nodes ( such as\u0014(l-1 ) i ) , andg(l-1 ): m(l ) j = X vi2Nj\u0016w(\u000f(l-1 ) j;\u0014(l-1 ) i;rij)\u0014(l-1 ) i ; ^m(l )", "entities": []}, {"text": "j = X vj02^Nj ~ w(\u000f(l-1 ) j;\u000f(l-1 ) j0)\u000f(l-1 ) j ; \u0018(l ) j= [ emb j;m(l ) j;^m(l ) j;g(l-1 ) ] ; \u000f(l ) j = GRU(\u0018(l ) j;\u000f(l-1 ) j):(3 ) Finally , the messages from both sentence and entity states are used to update global state", "entities": []}, {"text": "g(l-1)via g(l)=GRU(1", "entities": []}, {"text": "jVjX vi2V\u0014(l-1 ) i;1 j^VjX", "entities": []}, {"text": "^vj2^V\u000f(l-1 ) j;g(l-1)):(4 )", "entities": []}, {"text": "The above updating process is iterated for L times .", "entities": []}, {"text": "Usually , the top hidden states are considered as \ufb01ne - grained graph representations , which will provide dynamical context for the decoder via attention mechanism .", "entities": []}, {"text": "3.3 Decoding with Pointer Network Given the learned hidden states f\u0014(L ) igandg(L ) , the prediction procedure for order o0can be formalized as follows : P(o0jK(L ) )", "entities": [[3, 5, "MethodName", "Pointer Network"]]}, {"text": "= IY t=1P(o0 tjo0 < t;K(L ) o0 t\u00001 ) ; P(o0 tjo0 < t;K(L ) o0 t\u00001 ) = softmax ( qTtanh(Whd t+UK(L ) o0 t\u00001 ) ) ; hd t = LSTM ( hd t\u00001;\u0014(0 ) o0 t\u00001 ): ( 5 ) Here , q , WandUare learnable parameters , K(L )", "entities": [[20, 21, "MethodName", "softmax"], [33, 34, "MethodName", "LSTM"]]}, {"text": "o0 t\u00001andhd tdenote the sentence representations with predicted orderh \u0014(L ) o0 1;:::;\u0014(L ) o0 t\u00001i and the decoder hidden state at the t - th time step , which is initialized byg(L)ast=0 , respectively.4 Our Framework In this section , we give a detailed description to our framework .", "entities": []}, {"text": "As shown in Figure 2 , we \ufb01rst introduce two graph - based classi\ufb01ers to construct an iteratively re\ufb01ned sentence - entity graph ( IRSEGraph ) .", "entities": []}, {"text": "It is a weighted version of SE - Graph , where pairwise ordering inforamtion is iteratively incorporated to update ss - edge weights .", "entities": []}, {"text": "Then , we adapt the conventional GRN to establish a neural sentence ordering model based on the \ufb01nal IRSEGraph .", "entities": [[11, 13, "TaskName", "sentence ordering"]]}, {"text": "4.1 The De\ufb01nition of IRSE - Graph As an extension of SE - Graph , IRSE - Graph can be denoted asG=(V;E;W ) , whereVandEshare the same de\ufb01nitions with those of SE - Graph .", "entities": []}, {"text": "Particularly , in IRSE - Graph , each ss - edge ei;i0is a directed one with a weight wi;i02Windicating the probability of sentence sioccurring before sentence si0 .", "entities": []}, {"text": "Meanwhile , there must exist a corresponding ssedgeei0;iwith the weight wi0;i=1\u0000wi;i0denoting the probability of siappearing after si0 .", "entities": []}, {"text": "For example , in Figure 2 , for two linked sentence nodes v1andv2 , there exist two ss - edges e1;2ande2;1 with weights w1;2andw2;1respectively , both of which are iteratively updated during constructing IRSE - Graph .", "entities": []}, {"text": "4.2 Constructing IRSE - Graph Inspired by Gui et al .", "entities": [[5, 6, "DatasetName", "Inspired"]]}, {"text": "( 2020 ) , we successively introduce two classi\ufb01ers \u2014 initial classi\ufb01er anditerative classi\ufb01er to construct IRSE - Graph .", "entities": []}, {"text": "Both classi\ufb01ers are constructed using slightly adapted GRN and utilized to deal with different scenarios , respectively .", "entities": []}, {"text": "In this way , we can fully exploit the", "entities": []}, {"text": "2411potential of iterative classi\ufb01er to predict better pairwise orderings .", "entities": []}, {"text": "We will give a detail introduction to the slightly adapted GRN in Section \u00a7 4.3 .", "entities": []}, {"text": "To better understand the procedure of constructing IRSE - Graph , we provide the details in Algorithm 1 .", "entities": []}, {"text": "During this procedure , pairwise orderings are iteratively predicted and gradually incorporated to re\ufb01ne IRSE - Graph .", "entities": []}, {"text": "Here we introduce a set VP(k)to collect sentence node pairs with uncertain pairwise orderings at the k - th iteration .", "entities": []}, {"text": "First , we bulid an initial classi\ufb01er based on the initial IRSE - Graph , where the learned sentence representations are used to predict pairwise orderings between any two linked sentences only once ( Lines 2 - 6 ) .", "entities": []}, {"text": "Note that in the initial IRSEGraph , all weights of ss - edges are set to 0.5 .", "entities": []}, {"text": "In this case , IRSE - Graph degrades to the conventional SE - Graph .", "entities": []}, {"text": "Concretely , for any two linked sentence nodesviandvi0 , we concatenate their vector representations\u0014iand\u0014i0as[\u0014i;\u0014i0]and[\u0014i0;\u0014i ] , which are fed into an MLP classi\ufb01er to obtain two probabilities .", "entities": [[21, 22, "DatasetName", "MLP"]]}, {"text": "Then , we normalize and convert these two probabilities into ss - edge weights wi;i0 andwi0;i .", "entities": []}, {"text": "If bothwi;i0andwi0;iare within a pre\ufb01xed interval [ \u000emin;\u000emax ] , we consider ( vi;vi0)as a sentence node pair with uncertain pairwise ordering and add it into VP(0 ) .", "entities": []}, {"text": "Moreover , we replace bothwi;i0andwi0;iwith 0.5 , indicating that they will be repredicted in the next iteration .", "entities": []}, {"text": "In the following , we also construct an iterative classi\ufb01er based on IRSE - Graph .", "entities": []}, {"text": "However , in an easy - to - hard manner , we use iterative classi\ufb01er to perform pairwise ordering predictions , where the ss - edge weights of IRSE - Graph are continously updated with previously - predicted pairwise orderings with high con\ufb01dence ( Lines 13 - 26 ) .", "entities": []}, {"text": "By doing so , graph representations can be continously re\ufb01ned for better subsequent predictions .", "entities": []}, {"text": "More speci\ufb01cally , thek - th iteration of this classi\ufb01er mainly involve three steps : InStep 1 , based on the current IRSE - Graph , we employ the adapted GRN to conduct graph encoding to learn sentence representations ( Line 15 ) .", "entities": []}, {"text": "InStep 2 , on the top of learned sentence representations , we stack an MLP classi\ufb01er to predict pairwise orderings for sentence node pairs in VP(k ) ( Lines 16 - 19 ) .", "entities": [[14, 15, "DatasetName", "MLP"]]}, {"text": "Likewise , we collect sentence node pairs with uncertain pairwise orderings to form VP(k+1 ) , and reassign their corresponding ss - edge weights as 0.5 , so as to avoid the negative effect of these uncertain ss - edge weights during the nextAlgorithm 1", "entities": []}, {"text": "The procedure of constructing IRSE - Graph Input : the initial IRSE - Graph : G=(V;E;W)with all wi;i0=0 ; two thresholds : \u000emin;\u000emax Output : the \ufb01nal IRSE - Graph : G= ( V;E;W ) 1 : VP(0 ) ; 2 : f\u0014igI i=1 GRN(G ) 3 : forany linked sentence node pair ( vi;vi0)&&i < i0do 4 : wi;i0 InitialClassifer ( [ \u0014i;\u0014i0 ] ) 5 : wi0;i InitialClassifer ( [ \u0014i0;\u0014i ] ) 6 : wi;i0;wi0;i Normalize ( wi;i0;wi0;i ) 7 : if\u000emin\u0014wi;i0\u0014\u000emax then 8 : VP(0 ) VP(0)[f(vi;vi0)g 9 : wi;i0 0:5,wi0;i 0:5 10 : end if 11 : end for 12 : k 0 13 : repeat 14 : VP(k+1 ) ; 15 : f\u0014igI i=1 GRN(G ) 16 : for(vi;vi0)2VP(k)do 17 :", "entities": [[107, 108, "DatasetName", "0"]]}, {"text": "wi;i0 IterativeClassifer ( [ \u0014i;\u0014i0 ] ) 18 : wi0;i IterativeClassifer ( [ \u0014i0;\u0014i ] ) 19 : wi;i0;wi0;i Normalize ( wi;i0;wi0;i ) 20 : if\u000emin\u0014wi;i0\u0014\u000emax then 21 : VP(k+1 ) VP(k+1)[f(vi;vi0)g 22 : wi;i0 0:5,wi0;i 0:5 23 : end if 24 : end for 25 : k k+ 1 26 : until VP(k+1)==VP(k)||VP(k)== ; 27 : returnG Figure 3 :", "entities": []}, {"text": "Introducing noisy ss - edge weights into IRSEGraph .", "entities": []}, {"text": "iteration ( Lines 20 - 23 ) .", "entities": []}, {"text": "InStep 3 , ifVP(k+1)is equal to VP(k)or empty , we believe the learning of IRSE - Graph Ghas converged and thus return it ( Lines 26 - 27 ) .", "entities": []}, {"text": "Although both of our classi\ufb01ers are constructed using IRSE - Graph , their training procedures are slightly different .", "entities": []}, {"text": "As for initial classi\ufb01er , we directly train it on the initial IRSE - Graph without any pairwise ordering information ( all ss - edge weights are set to 0.5 ) .", "entities": []}, {"text": "By contrast , we train iterative classi\ufb01er on IRSE - Graph with partial pairwise orderings .", "entities": []}, {"text": "To enable iterative classi\ufb01er generalizable to any IRSE - Graph with partial predicted pairwise orderings , we \ufb01rst set all ss - edge weights to 1 or 0 according to their ground - truth pairwise orderings , and then train the classi\ufb01er to correctly predict pari-", "entities": [[27, 28, "DatasetName", "0"]]}, {"text": "2412wise orderings for other pairs .", "entities": []}, {"text": "Concretely , if siappears beforesi0 , we setwi;i0=1andwi0;i=0 , vice versa .", "entities": []}, {"text": "For example , in the left part of Figure 3 , the ground - truth sentence sequence is s1,s2,s3,s4 , and thus we assign the ss - edge weights of linked sentence node pairs ( v1;v2),(v3;v2),(v3;v4),(v2;v4 ) as follows : w1;2=1,w2;3=1,w2;4=1,w3;4=1 , and w2;1=0,w3;2=0,w4;2=0,w4;3=0 .", "entities": []}, {"text": "Moreover , to enhance the robustness of the iterative classi\ufb01er , we randomly select a certain ratio \u0011of sentence pairs and assign their ss - edges with incorrect weights .", "entities": []}, {"text": "Let us revisit Figure 3 , for the randomly selected sentence node pair ( v1;v2 ) , we assign ss - edges weights w1;2andw2;1with randomly generated noisy values 0.3 and 0.7 respectively .", "entities": []}, {"text": "In this way , we expect that iterative classi\ufb01er can conduct correct predictions even given incorrect previously - predicted pairwise orderings .", "entities": []}, {"text": "4.3 IRSE - Graph Sentence Ordering Model Finally , following the conventional SE - GRN ( Yin et al . , 2019 , 2021 ) , we construct a graph - based sentence ordering model .", "entities": [[4, 6, "TaskName", "Sentence Ordering"], [32, 34, "TaskName", "sentence ordering"]]}, {"text": "Note that the above two classi\ufb01ers and our sentence ordering model are all based on IRSE - Graph rather than the conventional SE - Graph , which makes the standard GRN unable to be applied directly .", "entities": [[8, 10, "TaskName", "sentence ordering"]]}, {"text": "To deal with this issue , we slightly adapt GRN to utilize pairwise ordering information for graph encoding .", "entities": []}, {"text": "Speci\ufb01cally , we adapt Equation 1 to incorporate ss - edge weights into the message aggregation of sentence - level nodes : m(l ) i = X vi02Niwi;i0\u0001w(\u0014(l-1 ) i;\u0014(l-1 ) i0)\u0014(l-1 ) i0 ; w(\u0014(l-1 ) i;\u0014(l-1 ) i0 )", "entities": []}, {"text": "= \u001b(Wg[\u0014(l-1 ) i;\u0014(l-1 ) i0]):(6 ) Here\u001bdenotes sigmoid function and Wgis learnable parameter matrix .", "entities": []}, {"text": "Equation 6 expresses that the sentence - level aggregation should consider not only the semantic representations of the two involved sentences , but also the relative ordering between them .", "entities": []}, {"text": "In addition , other Equations are the same as those of conventional GRN , which have been described in Section \u00a7 3.2 . 5 Experiment 5.1 Setup Datasets .", "entities": []}, {"text": "Following previous work ( Yin et al . , 2020 ;", "entities": []}, {"text": "Cui et al . , 2018 ; Yin et al . , 2021 ) , we carry out experiments on \ufb01ve benchmark datasets : \u000fSIND , ROCStory .", "entities": []}, {"text": "SIND ( Huang et al . , 2016 ) is a visual storytelling dataset and ROCStory(Mostafazadeh et", "entities": [[0, 1, "DatasetName", "SIND"], [11, 13, "TaskName", "visual storytelling"]]}, {"text": "al . , 2016 ) is about commonsense stories .", "entities": []}, {"text": "Both two datasets are composed of 5 - sentence stories and randomly split by 8:1:1 for the training / validation / test sets .", "entities": []}, {"text": "\u000fNIPS Abstract , AAN Abstract , arXiv Abstract .", "entities": [[6, 7, "DatasetName", "arXiv"]]}, {"text": "These three datasets consist of abstracts from research papers , which are collected from NIPS , ACL anthology and arXiv , respectively ( Radev et al . , 2016 ; Chen et al . , 2016 ) .", "entities": [[19, 20, "DatasetName", "arXiv"]]}, {"text": "The partitions for training / validation / test of each dataset are as follows : NIPS Abstract : 2,427/408/377 , AAN Abstract : 8,569/962/2,626 , arXiv Abstract : 884,912/110,614/110,615 for the training / validation / test sets .", "entities": [[25, 26, "DatasetName", "arXiv"]]}, {"text": "Settings .", "entities": []}, {"text": "For fair comparison , we use the same settings as our most related baseline SE - GRN ( Yin et al . , 2021 ) for our model and its variants .", "entities": []}, {"text": "Speci\ufb01cally , we apply 100 - dimensional GloVe word embeddings , and set the sizes of Bi - LSTM hidden states , sentence node states , and entity node states as 512 , 512 and 150 , respectively .", "entities": [[7, 8, "MethodName", "GloVe"], [8, 10, "TaskName", "word embeddings"], [18, 19, "MethodName", "LSTM"]]}, {"text": "The recurrent step of GRN is 3 .", "entities": []}, {"text": "We empirically set thresholds \u000eminand\u000emax as 0.2 and 0.8 , and set \u0011as 20 % , 15 % , 25 % , 15 % , 15 % according to accuracies of initial classi\ufb01er on validation sets .", "entities": []}, {"text": "Besides , we individually set the coef\ufb01cient \u0015(See Equation 18 in ( Yin et al . , 2020 ) ) as 0.5 , 0.5 , 0.2 , 0.4 , 0.5 on the \ufb01ve datasets .", "entities": []}, {"text": "We adopt Adadelta ( Zeiler , 2012 ) with\u000f=10\u00006,\u001a=0:95and initial learning rate 1.0 as the optimizer .", "entities": [[2, 3, "MethodName", "Adadelta"], [10, 12, "HyperparameterName", "learning rate"], [15, 16, "HyperparameterName", "optimizer"]]}, {"text": "We employ L2 weight decay with coef\ufb01cient 10\u00005 , batch size of 16 and dropout rate of 0.5 .", "entities": [[3, 5, "MethodName", "weight decay"], [9, 11, "HyperparameterName", "batch size"]]}, {"text": "When constructing our model based on BERT , we use the same settings as ( Cui et al . , 2020 ) .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "Concretely , we set sizes of hidden states and node states to 768 , the learning rate of BERT as 3e-3 , the batch size as 16 , 32 , 128 , 128 , 64 for the \ufb01ve datasets .", "entities": [[15, 17, "HyperparameterName", "learning rate"], [18, 19, "MethodName", "BERT"], [23, 25, "HyperparameterName", "batch size"]]}, {"text": "Baselines .", "entities": []}, {"text": "To demonstrate the effectiveness of our model ( IRSE - GRN ) , we compare it with SEGRN ( Yin et al . , 2021 ) .", "entities": []}, {"text": "Besides , we report the performance of following sentence ordering models : 1)Pairwise models : Pairwise Model ( Chen et al . , 2016 ) , RankTxNet ( Kumar et al . , 2020 ) , and BTSort ( Prabhumoye et al . , 2020 ) , ConsGraph ( Zhu et al . , 2021 ) ; 2 ) Set - to - sequence models : HAN ( Wang and Wan , 2019 ) , LSTM+PtrNet ( Gong et al . , 2016 ) , V - LSTM+PtrNet ( Logeswaran et al . , 2018 ) , ATTOrderNet ( Cui et", "entities": [[8, 10, "TaskName", "sentence ordering"], [28, 29, "DatasetName", "Kumar"]]}, {"text": "al . , 2018 ) , TGCM", "entities": []}, {"text": "( Oh et al . , 2019 ) , SE - GRN ( Yin et al . , 2019 ) , SE - GRN ( Yin et al . , 2021 ) , ATTOrderNet+FHDecoder ( Yin et al . ,", "entities": []}, {"text": "2413Model NIPS Abstract AAN Abstract SIND ROCStory arXiv Abstract Acc \u001c  PMR Acc \u001c  PMR Acc \u001c  PMR Acc \u001c  PMR Acc \u001c  PMR Pairwise Model ( Chen et al . , 2016)y- - - - - - - - - - - - - 66.00 33.43 LSTM+PtrNet ( Gong et al . , 2016)y50.87 67.00 - 58.20 69.00 - - 48.42 12.34 - - - - 71.58 40.44 V - LSTM+PtrNet ( Logeswaran et al . , 2018)y51.55 72.00 - 58.07 73.00 - - - - - - - - - ATTOrderNet ( Cui et al . , 2018)y56.09 72.00 - 63.24 73.00 - - 49.00 14.01 - - - - 73.00 42.19 HAN ( Wang and Wan , 2019)y- - - - - - - 50.00 15.01 - 73.00 39.62 - 75.00 44.55 SE - GRN ( Yin et al . , 2019)y57.27 75.00 - 64.64 78.00 - - 52.00 16.22 - - - - 75.00 44.33 SE - GRN ( Yin et al . , 2021 ) 58.25 76.49", "entities": [[5, 6, "DatasetName", "SIND"], [7, 8, "DatasetName", "arXiv"], [9, 10, "MetricName", "Acc"], [12, 13, "MetricName", "Acc"], [15, 16, "MetricName", "Acc"], [18, 19, "MetricName", "Acc"], [21, 22, "MetricName", "Acc"]]}, {"text": "25.73 65.06 78.60 44.87 49.58 53.16 17.17 68.96 75.46 42.67 59.07 75.74 44.72 ATTOrderNet+FHDecoder ( Yin et al . , 2020)y- - - - - - - 53.19 17.37 - 76.81 46.00 - 76.54 46.58 TGCM ( Oh et al . , 2019)y59.43 75.00 31.44 65.16 75.00 36.69 38.71 15.18 53.00 - - - 58.31 75.00 44.28 RankTxNet ( Kumar et al . , 2020)y- 75.00 24.13 - 77.00 39.18 - 57.00 15.48 - 76.00 38.02 - 77.00 43.44 B - TSort ( Prabhumoye et al . , 2020)y61.48 81.00 32.59 69.22 83.00 50.76 52.23 60.00 20.32 - - - - - ConsGraph ( Zhu et al . , 2021)y- 80.29 32.84 - 82.36 49.81 - 58.56 19.07 - 81.22 49.52 - - BERSON ( Cui et al . , 2020)y73.87 85.00 48.01 78.03 85.00 59.79 58.91 65.00 31.69 82.86 88.00 68.23 75.08 83.00 56.06 IRSE - GRN 63.14 80.45 32.63 68.51 82.09 49.56 51.01 54.97 18.77 71.28 77.43 46.38 70.15 84.22 56.85 IRSE - GRN+FHDecoder 73.62 87.45 50.19 77.34 87.87 62.24 54.98 61.87 22.77 77.70 84.20 57.11 74.45 88.57 60.30 IRSE - GRN+BERT+FHDecoder 78.00 90.35 58.81 82.07 91.11 68.93 59.08 66.14 28.79 83.77 89.09 69.06 78.64 90.30 66.59 Table 1 : Main results on the sentence ordering task , where yindicates previously reported scores .", "entities": [[59, 60, "DatasetName", "Kumar"], [206, 208, "TaskName", "sentence ordering"]]}, {"text": "Please note that RankTxNet , B - TSort and ConsGraph are pairwise models based on BERT , and the previous SOTA BERSON is also based on BERT and equipped with FHDecoder . 2020 ) and BERSON ( Cui et al . , 2020 ) .", "entities": [[15, 16, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"]]}, {"text": "Furthermore , to examine the compatibility of other technologies with our model , we report the performance of IRSE - GRN equipped with some effective components : 1 ) IRSE - GRN+FHDecoder .", "entities": []}, {"text": "In this variant , we equip our model with FHDecoder ( Yin et al . , 2020 ) , where pairwise ordering information is incorporated ; 2 ) IRSEGRN+BERT+FHDecoder .", "entities": []}, {"text": "In addition to FHDecoder , we construct the sentence encoder based on BERT , where the mean - pooling outputs of all learned word representations are used to initialize sentence nodes .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "Evaluation Metrics .", "entities": []}, {"text": "Following previous work ( Oh et al . , 2019 ; Cui et al . , 2020 ; Prabhumoye et al . , 2020 ; Zhu et al . , 2021 ; Yin et al . , 2021 ) , we use the following three metrics : 1 ) Kendall \u2019s Tau ( \u001c ): Formally , this metric is calculated as 12\u0002(number of inversions ) /\u0000I", "entities": []}, {"text": "2\u0001 , whereIdenotes the sequence length and number of inversions is the number of pairs in the predicted sequence with incorrect relative order ( Lapata , 2003 ) ; 2 ) Perfect Match Ratio ( PMR ) :", "entities": []}, {"text": "This metric calculates the ratio of samples where the entire sequence is correctly predicted ( Chen et al . , 2016 ) ; 3 ) Accuracy ( Acc ) :", "entities": [[25, 26, "MetricName", "Accuracy"], [27, 28, "MetricName", "Acc"]]}, {"text": "This metric measures the percentage of sentences , whose absolute positions are correctly predicted ( Logeswaran et al . , 2018 ) .", "entities": []}, {"text": "5.2 Pairwise Ordering Since pairwise ordering plays a crucial role in our proposed framework , we \ufb01rst compare the performance of different classi\ufb01ers on various datasets .", "entities": []}, {"text": "Table 2 shows the experimental results .", "entities": []}, {"text": "Obviously , the utilization of iterative classi\ufb01er further bene\ufb01tsDataset Initial Classi\ufb01er Initial + Iterative Classi\ufb01ers NIPS Abstract 80.46 % 86.32 % AAN Abstract 84.53 % 86.74 % SIND 77.72 % 83.55 % ROCstory 87.59 % 92.23 % arXiv Abstract 84.09 % 86.82 % Table 2 : The accuracies of our two classi\ufb01ers on \ufb01ve test datasets .", "entities": [[27, 28, "DatasetName", "SIND"], [37, 38, "DatasetName", "arXiv"]]}, {"text": "the predictions of pairwise orderings .", "entities": []}, {"text": "5.3 Main Results Table 1 reports the overall experimental results of sentence ordering .", "entities": [[11, 13, "TaskName", "sentence ordering"]]}, {"text": "When incorporating BERT and FHDecoder into IRSE - GRN , our model achieves SOTA performance on most of datasets .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "Besides , we arrive at the following conclusions : First , IRSE - GRN signi\ufb01cantly surpasses SEGRN on all datasets ( bootstrapping test , p<0:01 ) , indicating that iteratively re\ufb01ning graph representations indeed bene\ufb01t the ordering of input sentences .", "entities": []}, {"text": "Second , IRSE - GRN+FHDecoder exhibits better performance than IRSE - GRN and all non - BERT baselines , which are shown above the upper dotted line of Table 1 , across datasets in different domains .", "entities": [[16, 17, "MethodName", "BERT"]]}, {"text": "Therefore , we con\ufb01rm that our framework is orthogonal to the current approach exploiting pairwise ordering information for decoder .", "entities": []}, {"text": "Third , when constructing our model based on BERT , IRSE - GRN+BERT+FHDecoder also outperforms all BERT - based baselines , such as ConsGraph , BERSON , achieving SOTA performance .", "entities": [[8, 9, "MethodName", "BERT"], [16, 17, "MethodName", "BERT"]]}, {"text": "It can be known that our proposed framework is also effective when combining with pretrained language model .", "entities": []}, {"text": "2414 Figure 4 : The Kendall \u2019s \u001c of different models with respect to different sentence numbers on the arXiv abstract test set .", "entities": [[19, 20, "DatasetName", "arXiv"]]}, {"text": "Finally , we note that IRSE - GRN+BERT+FHDecoder gains relatively marginal improvement on SIND and ROCStory , and performs worse than BERSON in PMR on SIND .", "entities": [[13, 14, "DatasetName", "SIND"], [25, 26, "DatasetName", "SIND"]]}, {"text": "We speculate that there exist less ss - edges on these two datasets , resulting in that our proposed framework can not achieve its full potential .", "entities": []}, {"text": "Speci\ufb01cally , average edge numbers of SIND and ROCStory are 2.85 and 5.66 respectively , far fewer than 16.60 , 10.86 and 16.73 on NIPS Abstract , ANN Abstract and arXiv Abstract .", "entities": [[6, 7, "DatasetName", "SIND"], [30, 31, "DatasetName", "arXiv"]]}, {"text": "Besides , since it is a challenge to order longer paragraphs , we investigate the Kendall \u2019s \u001c of our models and SE - GRN with respect to different sentence numbers , as shown in Figure 4 .", "entities": []}, {"text": "Overall , all models degrade with the increase of sentence number .", "entities": []}, {"text": "However , our model and its two enhanced versions always exhibit better performance than SE - GRN .", "entities": []}, {"text": "5.4 Predictions of the First and Last Sentences As mentioned in previous studies ( Gong et al . , 2016 ; Chen et al . , 2016 ; Cui et al . , 2018 ;", "entities": []}, {"text": "Oh et al . , 2019 ) , the \ufb01rst and last sentences are very important in a paragraph .", "entities": []}, {"text": "Following these studies , we compare models by conducting experiments to predict the \ufb01rst and last sentences .", "entities": []}, {"text": "As displayed in Table 3 , IRSE - GRN surpasses all non - BERT baselines , and IRSE - GRN+BERT+ FHDecoder wins against BERTSON .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "These results are consistent with those reported in Table 1 , further demonstrating the effectiveness of our model .", "entities": []}, {"text": "5.5 Ablation Study We conduct several experiments to investigate the impacts of our proposed components on ROCstory dataset and arXiv dataset which are the two largestModel SIND arXiv Abstract head tail head tail Pairwise Model ( Chen et al . , 2016)y- - 84.85 62.37 LSTM+PtrNet ( Gong et al . , 2016)y74.66 53.30 90.47 66.49 ATTOrderNet ( Cui et al . , 2018)y76.00 54.42 91.00 68.08 SE - GRN ( Yin et al . , 2019)y78.12 56.68 92.28 70.45 SE - GRN ( Yin et al . , 2021 ) 79.01 57.27 92.23 70.46 ATTOrderNet+FHDecoder ( Yin et al . , 2020)y78.08 57.32 92.76 71.49 TGCM ( Oh et al . , 2019)y78.98 56.24 92.46 69.45 RankTxNet ( Kumar et al . , 2020)y80.32 59.68 92.97 69.13 B - Tsort ( Prabhumoye et al . , 2020)y78.06 58.36 - ConsGraph ( Zhu et al . , 2021)y79.80 60.44 - BERSON ( Cui et al . , 2020)y84.95 64.87 94.75 76.69 IRSE - GRN 78.62 59.11 94.46 80.97 IRSE - GRN+FHDecoder 82.87 64.15 96.09 85.04 IRSE - GRN+BERT+FHDecoder 86.21 67.14 98.23 88.33 Table 3 : The ratios of correctly predicting \ufb01rst and last sentences on arXiv Abstract and SIND .", "entities": [[19, 20, "DatasetName", "arXiv"], [26, 27, "DatasetName", "SIND"], [27, 28, "DatasetName", "arXiv"], [119, 120, "DatasetName", "Kumar"], [195, 196, "DatasetName", "arXiv"], [198, 199, "DatasetName", "SIND"]]}, {"text": "yindicates previously reported scores .", "entities": []}, {"text": "Model ROCStory arXiv Abstract Pairwise \u001c  PMR Pairwise \u001c  PMR IRSE - GRN 92.23 77.43 46.38 86.82 84.22 56.85 w/o initial classi\ufb01er 88.96 77.31 45.06 77.90 81.03 51.65 iterative number k=1 91.73 77.21 46.13 86.22 83.89 56.03 w/o iterative classi\ufb01er 87.59 75.98 44.14 84.09 83.24 55.05 w/o noise 90.42 77.06 46.02 80.45 82.23 53.10 Table 4 : Ablation study on the impacts of our proposed components on ROCStory dataset and arXiv abstract dataset .", "entities": [[2, 3, "DatasetName", "arXiv"], [70, 71, "DatasetName", "arXiv"]]}, {"text": "datasets .", "entities": []}, {"text": "All results are provided in Table 4 , where we draw the following conclusions : First , using only iterative classi\ufb01er , IRSEGRN(w / o initial classi\ufb01er ) performs worse than IRSE - GRN .", "entities": []}, {"text": "This result proves that iterative classi\ufb01er fails to predict well from scratch and the pairwise ordering predicted by initial classi\ufb01er is bene\ufb01cial to construct a well - formed graph representation for iterative classi\ufb01er .", "entities": []}, {"text": "Second , when the iteration number kis set as 1 , the performance of IRSE - GRN decreases .", "entities": []}, {"text": "Moreover , if we remove iterative classi\ufb01er , the performance of IRSE - GRN becomes even worse .", "entities": []}, {"text": "Therefore , we con\ufb01rm that the iterative predictions of pairwise ordering indeed bene\ufb01t the learning of graph representations .", "entities": []}, {"text": "Finally , the result in the last line indicates that removing noisy weights leads to a signi\ufb01cant performance drop .", "entities": []}, {"text": "It suggests that the utilization of noisy weights is useful for the training of iterative classi\ufb01er , which makes our model more robust .", "entities": []}, {"text": "5.6 Summary Coherence Evaluation Following previous studies ( Barzilay and Lapata , 2005 ; Nayeem and Chali , 2017 ) , we further in-", "entities": [[2, 4, "TaskName", "Coherence Evaluation"]]}, {"text": "2415Dataset SE - GRN IRSE - GRN IRSE - GRN+FHDecoder IRSE - GRN+BERT+FHDecoder Runtime # Params Runtime # Params Runtime # Params Runtime # Params NIPS abstract 6s", "entities": [[15, 16, "MetricName", "Params"], [18, 19, "MetricName", "Params"], [21, 22, "MetricName", "Params"], [24, 25, "MetricName", "Params"]]}, {"text": "23.9 M 6.2s 24.0 M 18s 25.0 M 29s 128.0 M AAN abstract 31s", "entities": []}, {"text": "23.9 M 32.5s 24.0 M 1min8s 25.0 M 1min20s", "entities": []}, {"text": "128.0 M SIND 1min6s 23.9 M 1min9s 24.0 M 2min3s 25.0 M 2min16s 128.0 M ROCStory 2min 23.9 M 2min5s 24.0 M 4min2s 25.0 M 4min42s", "entities": [[2, 3, "DatasetName", "SIND"]]}, {"text": "128.0 M arXiv abstract 25min 23.9 M 27min57s 24.0 M 46min 25.0 M 56min 128.0 M Table 6 : The runtime on the validation sets and the numbers of parameters for our enhanced models and baseline .", "entities": [[2, 3, "DatasetName", "arXiv"]]}, {"text": "Model Coherence SE - GRN ( Yin et al . , 2021 ) 46.71 59.47 IRSE - GRN 47.48 60.01 IRSE - GRN+FHDecoder 49.84 61.81 IRSE - GRN+BERT+FHDecoder 51.01 62.87 Table 5 : Coherence probabilities of summaries reordered by different models using weights of 0.8 ( left ) and 0.5 ( right ) .", "entities": []}, {"text": "spect the validity of our proposed framework via multi - document summarization .", "entities": [[8, 12, "TaskName", "multi - document summarization"]]}, {"text": "Concretely , we train different neural sentence ordering models on a large - scale summarization corpus ( Fabbri et al . , 2019 ) , and then individually use them to reorder the small - scale summarization data of DUC2004 ( Task2 ) .", "entities": [[6, 8, "TaskName", "sentence ordering"], [14, 15, "TaskName", "summarization"], [36, 37, "TaskName", "summarization"]]}, {"text": "Finally , we use coherence probability proposed by ( Nayeem and Chali , 2017 ) to evaluate the coherence of summaries .", "entities": []}, {"text": "In this group of experiments , we conduct experiments using different weights : 0.5 and 0.8 , as implemented in ( Nayeem and Chali , 2017 ) and ( Yin et al . , 2020 ) respectively .", "entities": []}, {"text": "The results are reported in Table 5 .", "entities": []}, {"text": "We can observe that the summaries reordered by IRSE - GRN and its variants achieve higher coherence probabilities than baseline , verifying the effectiveness of our proposed framework in the downstream task .", "entities": []}, {"text": "5.7 Further Experiment Results To provide more experimental results , we summarize the runtime on the validation sets and the numbers of parameters for our enhanced models and baseline SE - GRN in Table 6 . 6 Conclusion In this work , we propose a novel sentence ordering framework that makes better use of pairwise orderings for graph - based sentence ordering .", "entities": [[46, 48, "TaskName", "sentence ordering"], [60, 62, "TaskName", "sentence ordering"]]}, {"text": "Specifically , we introduce two classi\ufb01ers to iteratively predict pairwise orderings , which are gradually incorporated into the graph as edge weights .", "entities": []}, {"text": "Then , based on this re\ufb01ned graph , we construct a graph - based sentence ordering model .", "entities": [[14, 16, "TaskName", "sentence ordering"]]}, {"text": "Experiments on \ufb01ve datasets demonstrate not only the superiority of our model over baselines , but also the compatibility to other modules utilizing pairwise ordering information .", "entities": []}, {"text": "Moreover , when equipped with BERT and FHDecoder , our enhanced model achieves SOTA performance across datasets .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "In the future , we plan to explore more effective GNN for sentence ordering .", "entities": [[12, 14, "TaskName", "sentence ordering"]]}, {"text": "In particular , we will improve our model by iteratively merging nodes to re\ufb01ne the graph representation .", "entities": []}, {"text": "Acknowledgment The project was supported by National Key Research and Development Program of China ( No . 2020AAA0108004 ) , National Natural Science Foundation of China ( No . 61672440 ) , Natural Science Foundation of Fujian Province of China ( No . 2020J06001 ) , and Youth Innovation Fund of Xiamen ( No . 3502Z20206059 ) .", "entities": []}, {"text": "We also thank the reviewers for their insightful comments .", "entities": []}, {"text": "References Harsh Agrawal , Arjun Chandrasekaran , Dhruv Batra , Devi Parikh , and Mohit Bansal .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Sort story : Sorting jumbled images and captions into stories .", "entities": []}, {"text": "In EMNLP , pages 925\u2013931 .", "entities": []}, {"text": "Regina Barzilay , Noemie Elhadad , and Kathleen R. McKeown . 2002 .", "entities": []}, {"text": "Inferring strategies for sentence ordering in multidocument news summarization .", "entities": [[3, 5, "TaskName", "sentence ordering"], [7, 9, "TaskName", "news summarization"]]}, {"text": "Journal of Arti\ufb01cial Intelligence Research , 17:35\u201355 .", "entities": []}, {"text": "Regina Barzilay and Mirella Lapata .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Modeling local coherence : An entity - based approach .", "entities": []}, {"text": "In ACL , pages 141\u2013148 .", "entities": []}, {"text": "Regina Barzilay and Mirella Lapata . 2008 .", "entities": []}, {"text": "Modeling local coherence : An entity - based approach .", "entities": []}, {"text": "Computational Linguistics , 34(1):1\u201334 .", "entities": []}, {"text": "Regina Barzilay and Lillian Lee . 2004 .", "entities": []}, {"text": "Catching the drift : Probabilistic content models , with applications to generation and summarization .", "entities": [[13, 14, "TaskName", "summarization"]]}, {"text": "In NAACL , pages 113\u2013120 .", "entities": []}, {"text": "2416Xinchi Chen , Xipeng Qiu , and Xuanjing Huang . 2016 .", "entities": []}, {"text": "Neural sentence ordering .", "entities": [[1, 3, "TaskName", "sentence ordering"]]}, {"text": "arXiv:1607.06952 .", "entities": []}, {"text": "Somnath Basu Roy Chowdhury , Faeze Brahman , and Snigdha Chaturvedi .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Reformulating sentence ordering as conditional text generation .", "entities": [[1, 3, "TaskName", "sentence ordering"], [4, 7, "TaskName", "conditional text generation"]]}, {"text": "arXiv:2104.07064 .", "entities": []}, {"text": "Baiyun Cui , Yingming Li , Ming Chen , and Zhongfei Zhang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep attentive sentence ordering network .", "entities": [[2, 4, "TaskName", "sentence ordering"]]}, {"text": "In EMNLP , pages 4340\u20134349 .", "entities": []}, {"text": "Baiyun Cui , Yingming Li , and Zhongfei Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Bert - enhanced relational sentence ordering network .", "entities": [[4, 6, "TaskName", "sentence ordering"]]}, {"text": "InEMNLP , pages 6310\u20136320 .", "entities": []}, {"text": "Chaorui Deng , Ning Ding , Mingkui Tan , and Qi Wu . 2020 .", "entities": []}, {"text": "Length - controllable image captioning .", "entities": [[3, 5, "TaskName", "image captioning"]]}, {"text": "In ECCV , pages 712\u2013729 .", "entities": []}, {"text": "J. Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In NAACL - HLT , pages 4171\u20134186 .", "entities": []}, {"text": "Micha Elsner and Eugene Charniak .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Extending the entity grid with entity - speci\ufb01c features .", "entities": []}, {"text": "In ACL , pages 125\u2013129 .", "entities": []}, {"text": "Alexander R. Fabbri , Irene Li , Tianwei She , Suyi Li , and Dragomir R. Radev .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Multi - news : A largescale multi - document summarization dataset and abstractive hierarchical model .", "entities": [[0, 3, "DatasetName", "Multi - news"], [6, 10, "TaskName", "multi - document summarization"]]}, {"text": "In ACL , pages 1074 \u2013 1084 .", "entities": []}, {"text": "Marjan Ghazvininejad , Omer Levy , Yinhan Liu , and Luke Zettlemoyer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Mask - predict : Parallel decoding of conditional masked language models .", "entities": []}, {"text": "In EMNLP - IJCNLP , pages 6111\u20136120 .", "entities": []}, {"text": "Jingjing Gong , Xinchi Chen , Xipeng Qiu , and Xuanjing Huang . 2016 .", "entities": []}, {"text": "End - to - end neural sentence ordering using pointer network .", "entities": [[6, 8, "TaskName", "sentence ordering"], [9, 11, "MethodName", "pointer network"]]}, {"text": "arXiv:1611.04953 .", "entities": []}, {"text": "Jiatao Gu , James Bradbury , Caiming Xiong , Victor O. K. Li , and Richard Socher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Nonautoregressive neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In ICLR .", "entities": []}, {"text": "Tao Gui , Jiacheng Ye , Qi Zhang , Zhengyan Li , Zichu Fei , Yeyun Gong , and Xuanjing Huang . 2020 .", "entities": []}, {"text": "Uncertainty - aware label re\ufb01nement for sequence labeling .", "entities": []}, {"text": "In EMNLP , pages 2316\u20132326 .", "entities": []}, {"text": "Camille Guinaudeau and Michael Strube .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Graphbased local coherence modeling .", "entities": []}, {"text": "In ACL , pages 93 \u2013 103 .", "entities": []}, {"text": "Ari Holtzman , Jan Buys , Maxwell Forbes , Antoine Bosselut , David Golub , and Yejin Choi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning to write with cooperative discriminators .", "entities": []}, {"text": "InACL , pages 1638\u20131649 .", "entities": []}, {"text": "Ting - Hao Huang , Francis Ferraro , Nasrin Mostafazadeh , Ishan Misra , Aishwarya Agrawal , Jacob Devlin , Ross Girshick , Xiaodong He , Pushmeet Kohli , Dhruv Batra , C. Lawrence Zitnick , Devi Parikh , LucyVanderwende , Michel Galley , and Margaret Mitchell .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Visual storytelling .", "entities": [[0, 2, "TaskName", "Visual storytelling"]]}, {"text": "In NAACL , pages 1233 \u2013 1239 .", "entities": []}, {"text": "Ioannis Konstas and Mirella Lapata .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Unsupervised concept - to - text generation with hypergraphs .", "entities": [[1, 7, "TaskName", "concept - to - text generation"]]}, {"text": "InNAACL , pages 752\u2013761 .", "entities": []}, {"text": "Pawan Kumar , Dhanajit Brahma , Harish Karnick , and Piyush Rai . 2020 .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "Deep attentive ranking networks for learning to order sentences .", "entities": []}, {"text": "In AAAI , pages 8115\u20138122 .", "entities": []}, {"text": "Mirella Lapata .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Probabilistic text structuring : Experiments with sentence ordering .", "entities": [[6, 8, "TaskName", "sentence ordering"]]}, {"text": "In ACL , pages 545\u2013552 .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "BART : denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension .", "entities": [[0, 1, "MethodName", "BART"], [2, 3, "TaskName", "denoising"]]}, {"text": "In ACL , pages 7871\u20137880 .", "entities": []}, {"text": "Jiwei Li and Dan Jurafsky .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Neural net models of open - domain discourse coherence .", "entities": []}, {"text": "In EMNLP , pages 198\u2013209 .", "entities": []}, {"text": "Lajanugen Logeswaran , Honglak Lee , and Dragomir R. Radev .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Sentence ordering and coherence modeling using recurrent neural networks .", "entities": [[0, 2, "TaskName", "Sentence ordering"]]}, {"text": "In AAAI , pages 5285\u20135292 .", "entities": []}, {"text": "Muhammad Tasnim Mohiuddin , Sha\ufb01q R. Joty , and Dat Tien Nguyen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Coherence modeling of asynchronous conversations : A neural entity grid approach .", "entities": []}, {"text": "In ACL , pages 558\u2013568 .", "entities": []}, {"text": "Han Cheol Moon , Tasnim Mohiuddin , Sha\ufb01q R. Joty , and Xu Chi . 2019 .", "entities": []}, {"text": "A uni\ufb01ed neural coherence model .", "entities": []}, {"text": "In EMNLP - IJCNLP , pages 2262\u20132272 .", "entities": []}, {"text": "Nasrin Mostafazadeh , Nathanael Chambers , Xiaodong He , Devi Parikh , Dhruv Batra , Lucy Vanderwende , Pushmeet Kohli , and James Allen . 2016 .", "entities": []}, {"text": "A corpus and cloze evaluation for deeper understanding of commonsense stories .", "entities": []}, {"text": "In NAACL , pages 839\u2013849 .", "entities": []}, {"text": "Ramesh Nallapati , Feifei Zhai , and Bowen Zhou .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Summarunner : A recurrent neural network based sequence model for extractive summarization of documents .", "entities": [[10, 12, "TaskName", "extractive summarization"]]}, {"text": "In AAAI , pages 3075\u20133081 .", "entities": []}, {"text": "Mir Tafseer Nayeem and Yllias Chali . 2017 .", "entities": []}, {"text": "Extract with order for coherent multi - document summarization .", "entities": [[5, 9, "TaskName", "multi - document summarization"]]}, {"text": "In ACL TextGraphs Workshop , pages 51\u201356 .", "entities": []}, {"text": "Dat Tien Nguyen and Sha\ufb01q Rayhan Joty .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A neural local coherence model .", "entities": []}, {"text": "In ACL , pages 1320 \u2013 1330 .", "entities": []}, {"text": "Byungkook Oh , Seungmin Seo , Cheolheon Shin , Eunju Jo , and Kyong - Ho Lee . 2019 .", "entities": []}, {"text": "Topic - guided coherence modeling for sentence ordering by preserving global and local information .", "entities": [[6, 8, "TaskName", "sentence ordering"]]}, {"text": "In EMNLP - IJCNLP , pages 2273\u20132283 .", "entities": []}, {"text": "2417Shrimai Prabhumoye , Ruslan Salakhutdinov , and Alan W. Black . 2020 .", "entities": [[3, 4, "DatasetName", "Ruslan"]]}, {"text": "Topological sort for sentence ordering .", "entities": [[3, 5, "TaskName", "sentence ordering"]]}, {"text": "In ACL , pages 2783\u20132792 .", "entities": []}, {"text": "Dragomir R. Radev , Mark Thomas Joseph , Bryan R. Gibson , and Pradeep Muthukrishnan .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A bibliometric and network analysis of the \ufb01eld of computational linguistics .", "entities": []}, {"text": "Journal of the Association for Information Science and Technology , 67(3):683\u2013706 .", "entities": []}, {"text": "Linfeng Song , Daniel Gildea , Yue Zhang , Zhiguo Wang , and Jinsong Su . 2019 .", "entities": []}, {"text": "Semantic neural machine translation using AMR .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 7:19\u201331 .", "entities": []}, {"text": "Linfeng Song , Ante Wang , Jinsong Su , Yue Zhang , Kun Xu , Yubin Ge , and Dong Yu . 2020 .", "entities": []}, {"text": "Structural information preserving for graph - to - text generation .", "entities": [[8, 10, "TaskName", "text generation"]]}, {"text": "InACL , pages 7987\u20137998 .", "entities": []}, {"text": "Linfeng Song , Zhiguo Wang , Mo Yu , Yue Zhang , Radu Florian , and Daniel Gildea .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Exploring graph - structured passage representation for multihop reading comprehension with graph neural networks .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}, {"text": "arXiv:1809.02040 .", "entities": []}, {"text": "Oriol Vinyals , Meire Fortunato , and Navdeep Jaitly . 2015a .", "entities": []}, {"text": "Pointer networks .", "entities": []}, {"text": "In NIPS , page 3104\u20133112 .", "entities": []}, {"text": "Oriol Vinyals , Meire Fortunato , and Navdeep Jaitly . 2015b .", "entities": []}, {"text": "Pointer networks .", "entities": []}, {"text": "In NIPS , pages 2692 \u2013 2700 .", "entities": []}, {"text": "Tianming Wang and Xiaojun Wan . 2019 .", "entities": []}, {"text": "Hierarchical attention networks for sentence ordering .", "entities": [[4, 6, "TaskName", "sentence ordering"]]}, {"text": "In AAAI , pages 7184\u20137191 .", "entities": []}, {"text": "Mengge Xue , Weiming Cai , Jinsong Su , Linfeng Song , Yubin Ge , Yubao Liu , and Bin Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Neural collective entity linking based on recurrent random walk network learning .", "entities": [[2, 4, "TaskName", "entity linking"]]}, {"text": "In IJCAI , pages 5327\u20135333 .", "entities": []}, {"text": "Yongjing Yin , Shaopeng Lai , Linfeng Song , Chulun Zhou , Xianpei Han , Junfeng Yao , and Jinsong Su . 2021 .", "entities": []}, {"text": "An external knowledge enhanced graphbased neural network for sentence ordering .", "entities": [[8, 10, "TaskName", "sentence ordering"]]}, {"text": "Journal of Arti\ufb01cial Intelligence Research , 70:545\u2013566 .", "entities": []}, {"text": "Yongjing Yin , Fandong Meng , Jinsong Su , Yubin Ge , Linfeng Song , Jie Zhou , and Jiebo Luo . 2020 .", "entities": []}, {"text": "Enhancing pointer network for sentence ordering with pairwise ordering predictions .", "entities": [[1, 3, "MethodName", "pointer network"], [4, 6, "TaskName", "sentence ordering"]]}, {"text": "In AAAI , pages 9482 \u2013 9489 .", "entities": []}, {"text": "Yongjing Yin , Linfeng Song , Jinsong Su , Jiali Zeng , Chulun Zhou , and Jiebo Luo . 2019 .", "entities": []}, {"text": "Graph - based neural sentence ordering .", "entities": [[4, 6, "TaskName", "sentence ordering"]]}, {"text": "In IJCAI , pages 5387 \u2013 5393 .", "entities": []}, {"text": "Matthew D. Zeiler .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "ADADELTA : an adaptive learning rate method .", "entities": [[0, 1, "MethodName", "ADADELTA"], [4, 6, "HyperparameterName", "learning rate"]]}, {"text": "arXiv:1212.5701 .", "entities": []}, {"text": "Yue Zhang , Qi Liu , and Linfeng Song .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Sentencestate lstm for text representation .", "entities": [[1, 2, "MethodName", "lstm"]]}, {"text": "In ACL , pages 317\u2013327.Yutao Zhu , Kun Zhou , Jian - Yun Nie , Shengchao Liu , and Zhicheng Dou . 2021 .", "entities": []}, {"text": "Neural sentence ordering based on constraint graphs .", "entities": [[1, 3, "TaskName", "sentence ordering"]]}, {"text": "In AAAI , pages 14656 \u2013 14664 .", "entities": []}]