[{"text": "Proceedings of the 5th Workshop on Representation Learning for NLP ( RepL4NLP-2020 ) , pages 143\u2013155 July 9 , 2020 .", "entities": [[6, 8, "TaskName", "Representation Learning"]]}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics143Compressing BERT : Studying the Effects of Weight Pruning on Transfer Learning Mitchell A. Gordon & Kevin Duh & Nicholas Andrews Johns Hopkins University mitchg@jhu.edu , kevinduh@cs.jhu.edu , noa@jhu.edu Abstract Pre - trained feature extractors , such as BERT for natural language processing and VGG for computer vision , have become effective methods for improving deep learning models without requiring more labeled data .", "entities": [[5, 6, "MethodName", "BERT"], [14, 16, "TaskName", "Transfer Learning"], [42, 43, "MethodName", "BERT"], [48, 49, "MethodName", "VGG"]]}, {"text": "While effective , these feature extractors may be prohibitively large for some deployment scenarios .", "entities": []}, {"text": "We explore weight pruning for BERT and ask : how does compression during pretraining affect transfer learning ?", "entities": [[5, 6, "MethodName", "BERT"], [15, 17, "TaskName", "transfer learning"]]}, {"text": "We \ufb01nd that pruning affects transfer learning in three broad regimes .", "entities": [[5, 7, "TaskName", "transfer learning"]]}, {"text": "Low levels of pruning ( 30 - 40 % ) do not affect pre - training loss or transfer to downstream tasks at all .", "entities": [[16, 17, "MetricName", "loss"]]}, {"text": "Medium levels of pruning increase the pre - training loss and prevent useful pre - training information from being transferred to downstream tasks .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "High levels of pruning additionally prevent models from \ufb01tting downstream datasets , leading to further degradation .", "entities": []}, {"text": "Finally , we observe that \ufb01netuning BERT on a speci\ufb01c task does not improve its prunability .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "We conclude that BERT can be pruned once during pre - training rather than separately for each task without affecting performance .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "1 Introduction Pre - trained feature extractors , such as BERT ( Devlin et al . , 2018 ) for natural language processing and VGG ( Simonyan and Zisserman , 2014 ) for computer vision , have become effective methods for improving the performance of deep learning models .", "entities": [[10, 11, "MethodName", "BERT"], [24, 25, "MethodName", "VGG"]]}, {"text": "In the last year , models similar to BERT have become state - of - the - art in many NLP tasks , including natural language inference ( NLI ) , named entity recognition ( NER ) , sentiment analysis , etc .", "entities": [[8, 9, "MethodName", "BERT"], [24, 27, "TaskName", "natural language inference"], [31, 34, "TaskName", "named entity recognition"], [35, 36, "TaskName", "NER"], [38, 40, "TaskName", "sentiment analysis"]]}, {"text": "These models follow a pre - training paradigm : they are trained on a large amount of unlabeled text via a task that resembles language modeling ( Yang et al . , 2019 ; Chan et al . , 2019 ) and are then \ufb01ne - tuned on a smaller amount of \u201c downstream \u201d data , whichis labeled for a speci\ufb01c task .", "entities": []}, {"text": "Pre - trained models usually achieve higher accuracy than any model trained on downstream data alone .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "The pre - training paradigm , while effective , still has some problems .", "entities": []}, {"text": "While some claim that language model pre - training is a \u201c universal language learning task \u201d ( Radford et al . , 2019 ) , there is no theoretical justi\ufb01cation for this , only empirical evidence .", "entities": []}, {"text": "Second , due to the size of the pre - training dataset , BERT models tend to be slow and require impractically large amounts of GPU memory .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "BERT - Large can only be used with access to a Google TPU , and BERT - Base requires some optimization tricks such as gradient checkpointing or gradient accumulation to be trained effectively on consumer hardware ( Sohoni et al . , 2019 ) .", "entities": [[0, 1, "MethodName", "BERT"], [11, 12, "DatasetName", "Google"], [15, 16, "MethodName", "BERT"], [24, 26, "MethodName", "gradient checkpointing"]]}, {"text": "Training BERT - Base from scratch costs \u0018$7k and emits \u00181438 pounds of CO 2(Strubell et al . , 2019 ) .", "entities": [[1, 2, "MethodName", "BERT"]]}, {"text": "Model compression ( Bucila et al . , 2006 ) , which attempts to shrink a model without losing accuracy , is a viable approach to decreasing GPU usage .", "entities": [[0, 2, "TaskName", "Model compression"], [19, 20, "MetricName", "accuracy"]]}, {"text": "It might also be used to trade accuracy for memory in some low - resource cases , such as deploying to smartphones for real - time prediction .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "The main questions this paper attempts to answer are : Does compressing BERT impede it \u2019s ability to transfer to new tasks ?", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "And does \ufb01ne - tuning make BERT more or less compressible ?", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "To explore these questions , we compressed English BERT using magnitude weight pruning ( Han et al . , 2015 ) and observed the results on transfer learning to the General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al . , 2019 ) , a diverse set of natural language understanding tasks including sentiment analysis , NLI , and textual similarity evaluation .", "entities": [[8, 9, "MethodName", "BERT"], [26, 28, "TaskName", "transfer learning"], [30, 31, "DatasetName", "General"], [35, 36, "DatasetName", "GLUE"], [51, 54, "TaskName", "natural language understanding"], [56, 58, "TaskName", "sentiment analysis"]]}, {"text": "We chose magnitude weight pruning , which compresses models by removing weights close to 0 , because it is one of the most \ufb01ne - grained and effective compression methods and because there are many interesting ways to", "entities": [[14, 15, "DatasetName", "0"]]}, {"text": "144view pruning , which we explore in the next section .", "entities": []}, {"text": "Our \ufb01ndings are as follows : Low levels of pruning ( 30 - 40 % ) do not increase pre - training loss or affect transfer to downstream tasks at all .", "entities": [[22, 23, "MetricName", "loss"]]}, {"text": "Medium levels of pruning increase the pre - training loss and prevent useful pre - training information from being transferred to downstream tasks .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "This information is not equally useful to each task ; tasks degrade linearly with pre - train loss , but at different rates .", "entities": [[17, 18, "MetricName", "loss"]]}, {"text": "High levels of pruning , depending on the size of the downstream dataset , may additionally degrade performance by preventing models from \ufb01tting downstream datasets .", "entities": []}, {"text": "Finally , we observe that \ufb01ne - tuning BERT on a speci\ufb01c task does not improve its prunability or change the order of pruning by a meaningful amount .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "To our knowledge , prior work had not shown whether BERT could be compressed in a taskgeneric way , keeping the bene\ufb01ts of pre - training while avoiding costly experimentation associated with compressing and re - training BERT multiple times .", "entities": [[10, 11, "MethodName", "BERT"], [37, 38, "MethodName", "BERT"]]}, {"text": "Nor had it shown whether BERT could be over - pruned for a memory / accuracy trade - off for deployment to low - resource devices .", "entities": [[5, 6, "MethodName", "BERT"], [15, 16, "MetricName", "accuracy"]]}, {"text": "In this work , we conclude that BERT can be pruned prior to distribution without affecting it \u2019s universality , and that BERT may be over - pruned during pre - training for a reasonable accuracy trade - off for certain tasks .", "entities": [[7, 8, "MethodName", "BERT"], [22, 23, "MethodName", "BERT"], [35, 36, "MetricName", "accuracy"]]}, {"text": "2 Pruning : Compression , Regularization , Architecture Search Neural network pruning involves examining a trained network and removing parts deemed to be unnecessary by some heuristic saliency criterion .", "entities": [[10, 12, "TaskName", "network pruning"]]}, {"text": "One might remove weights , neurons , layers , channels , attention heads , etc . depending on which heuristic is used .", "entities": []}, {"text": "Below , we describe three different lenses through which we might interpret pruning .", "entities": []}, {"text": "Compression Pruning a neural network decreases the number of parameters required to specify the model , which decreases the disk space required to store it .", "entities": [[7, 10, "HyperparameterName", "number of parameters"]]}, {"text": "This allows large models to be deployed on edge computing devices like smartphones .", "entities": []}, {"text": "Pruning can also increase inference speed if whole neurons or convolutional channels are pruned , which reduces GPU usage.1", "entities": []}, {"text": "Regularization Pruning a neural network also regularizes it .", "entities": []}, {"text": "We might consider pruning to be 1If weights are pruned , however , the weight matrices become sparse .", "entities": []}, {"text": "Sparse matrix multiplication is dif\ufb01cult to optimize on current GPU architectures ( Han et al . , 2016 ) , although progress is being made.a form of permanent dropout ( Molchanov et al . , 2017 ) or a heuristic - based L0 regularizer ( Louizos et al . , 2018 ) .", "entities": []}, {"text": "Through this lens , pruning decreases the complexity of the network and therefore narrows the range of possible functions it can express.2 The main difference between L0 or L1 regularization and weight pruning is that the former induce sparsity via a penalty on the loss function , which is learned during gradient descent via stochastic relaxation .", "entities": [[28, 30, "MethodName", "L1 regularization"], [44, 45, "MetricName", "loss"]]}, {"text": "It \u2019s not clear which approach is more principled or preferred .", "entities": []}, {"text": "( Gale et al . , 2019 )", "entities": []}, {"text": "Sparse Architecture Search Finally , we can view neural network pruning as a type of sparse architecture search .", "entities": [[9, 11, "TaskName", "network pruning"]]}, {"text": "Liu et al . ( 2019b ) and Frankle and Carbin ( 2019 ) show that they can train carefully re - initialized pruned architectures to similar performance levels as dense networks .", "entities": []}, {"text": "Under this lens , stochastic gradient descent ( SGD ) induces network sparsity , and pruning simply makes that sparsity explicit .", "entities": [[4, 7, "MethodName", "stochastic gradient descent"], [8, 9, "MethodName", "SGD"]]}, {"text": "These sparse architectures , along with the appropriate initializations , are sometimes referred to as \u201c lottery tickets . \u201d3", "entities": []}, {"text": "2.1 Magnitude Weight Pruning In this work , we focus on weight magnitude pruning because it is one of the most \ufb01ne - grained and effective pruning methods .", "entities": []}, {"text": "It also has a compelling saliency criterion ( Han et al . , 2015 ): if a weight is close to zero , then its input is effectively ignored , which means the weight can be pruned .", "entities": []}, {"text": "Magnitude weight pruning itself is a simple procedure : 1 . Pick a target percentage of weights to be pruned , say 50 % .", "entities": []}, {"text": "2 . Calculate a threshold such that 50 % of weight magnitudes are under that threshold .", "entities": []}, {"text": "3 . Remove those weights .", "entities": []}, {"text": "4 . Continue training the network to recover any lost accuracy .", "entities": [[10, 11, "MetricName", "accuracy"]]}, {"text": "5 .", "entities": []}, {"text": "Optionally , return to step 1 and increase the percentage of weights pruned .", "entities": []}, {"text": "This procedure is conveniently implemented in a Tensor\ufb02ow ( Abadi et al . , 2016 ) package4 , which we use ( Zhu and Gupta , 2017 ) .", "entities": []}, {"text": "Calculating a threshold and pruning can be done for all network parameters holistically ( global pruning ) or for each weight matrix individually ( matrix2Interestingly , recent work used compression not to induce simplicity but to measure it ( Arora et al . , 2018 ) .", "entities": []}, {"text": "3Sparse networks are dif\ufb01cult to train from scratch ( Evci et al . , 2019 ) .", "entities": []}, {"text": "However , Dettmers and Zettlemoyer ( 2019 ) and Mostafa and Wang ( 2019 ) present methods to do this by allowing SGD to search over the space of possible subnetworks .", "entities": [[22, 23, "MethodName", "SGD"]]}, {"text": "Our \ufb01ndings suggest that these methods might be used to train sparse BERT from scratch .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "4https://www.tensorflow.org/versions/ r1.15 / api_docs / python / tf / contrib / model _ pruning", "entities": []}, {"text": "145local pruning ) .", "entities": []}, {"text": "Both methods will prune to the same sparsity , but in global pruning the sparsity might be unevenly distributed across weight matrices .", "entities": []}, {"text": "We use matrix - local pruning because it is more popular in the community.5For information on other pruning techniques , we recommend Gale et al .", "entities": []}, {"text": "( 2019 ) and Liu et al .", "entities": []}, {"text": "( 2019b )", "entities": []}, {"text": ".", "entities": []}, {"text": "3 Experimental Setup BERT is a large Transformer encoder ; for background , we refer readers to Vaswani et", "entities": [[3, 4, "MethodName", "BERT"], [7, 8, "MethodName", "Transformer"]]}, {"text": "al . ( 2017 ) or one of these excellent tutorials ( Alammar , 2018 ; Klein et al . , 2017 ) .", "entities": []}, {"text": "3.1 Implementing BERT Pruning BERT - Base consists of 12 encoder layers , each of which contains 6 prunable matrices : 4 for the multiheaded self - attention and 2 for the layer \u2019s output feed - forward network .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "BERT"]]}, {"text": "Recall that self - attention \ufb01rst projects layer inputs into key , query , and value embeddings via linear projections .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "While there is a separate key , query , and value projection matrix for each attention head , implementations typically \u201c stack \u201d matrices from each attention head , resulting in only 3 parameter matrices : one for key projections , one for value projections , and one for query projections .", "entities": []}, {"text": "We prune each of these matrices separately , calculating a threshold for each .", "entities": []}, {"text": "We also prune the linear output projection , which combines outputs from each attention head into a single embedding.6 We prune word embeddings in the same way we prune feed - foward networks and self - attention parameters.7The justi\ufb01cation is similar : if a word embedding value is close to zero , we can assume it \u2019s zero and store the rest in a sparse matrix .", "entities": [[21, 23, "TaskName", "word embeddings"]]}, {"text": "This is useful because token / subword embeddings tend to account for a large portion of a natural language model \u2019s memory .", "entities": []}, {"text": "In BERT - Base speci\ufb01cally , 5The weights in almost every matrix in BERT - Base are approximately normally distributed with mean 0 and variance between 0.03 and 0.05 ( Table A ) .", "entities": [[1, 2, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"], [22, 23, "DatasetName", "0"]]}, {"text": "This similarity may imply that global pruning would perform similarly to matrix - local pruning .", "entities": []}, {"text": "6We could have calculated a single threshold for the entire self - attention layer or for each attention head separately .", "entities": []}, {"text": "Similar to global pruning vs. matrix - local pruning , it \u2019s not clear which one should be preferred .", "entities": []}, {"text": "7Interestingly , pruning word embeddings is slightly more interpretable that pruning other matrices .", "entities": [[3, 5, "TaskName", "word embeddings"]]}, {"text": "See Figure ? ?", "entities": []}, {"text": "for a heatmap of embedding magnitudes , which shows that shorter subwords tend to be pruned more than longer subwords and that certain dimensions are almost never pruned in any subword.the embeddings account for \u001821 % of the model \u2019s memory .", "entities": [[2, 3, "MethodName", "heatmap"]]}, {"text": "Our experimental code for pruning BERT , based on the public BERT repository , is available here.8 3.2 Pruning During Pre - Training We perform weight magnitude pruning on a pretrained BERT - Base model.9We select sparsities from 0 % to 90 % in increments of 10 % and gradually prune BERT to this sparsity over the \ufb01rst 10k steps of training .", "entities": [[5, 6, "MethodName", "BERT"], [11, 12, "MethodName", "BERT"], [31, 32, "MethodName", "BERT"], [38, 39, "DatasetName", "0"], [51, 52, "MethodName", "BERT"]]}, {"text": "We continue pre - training on English Wikipedia and BookCorpus for another 90k steps to regain any lost accuracy.10The resulting pre - training losses are shown in Table 1 .", "entities": [[9, 10, "DatasetName", "BookCorpus"]]}, {"text": "We then \ufb01ne - tune these pruned models on tasks from the General Language Understanding Evaluation ( GLUE ) benchmark , which is a standard set of 9 tasks that include sentiment analysis , natural language inference , etc .", "entities": [[12, 13, "DatasetName", "General"], [17, 18, "DatasetName", "GLUE"], [31, 33, "TaskName", "sentiment analysis"], [34, 37, "TaskName", "natural language inference"]]}, {"text": "We avoid WNLI , which is known to be problematic.11We also avoid tasks with less than 5k training examples because the results tend to be noisy ( RTE , MRPC , STS - B ) .", "entities": [[2, 3, "DatasetName", "WNLI"], [27, 28, "DatasetName", "RTE"], [29, 30, "DatasetName", "MRPC"], [31, 34, "DatasetName", "STS - B"]]}, {"text": "We \ufb01ne - tune a separate model on each of the remaining 5 GLUE tasks for 3 epochs and try 4 learning rates : [ 2;3;4;5]\u000210\u00005 .", "entities": [[13, 14, "DatasetName", "GLUE"]]}, {"text": "The best evaluation accuracies are averaged and plotted in Figure 1 .", "entities": []}, {"text": "Individual task results are in Table 1 .", "entities": []}, {"text": "BERT can be used as a static feature - extractor or as a pre - trained model which is \ufb01ne - tuned endto - end .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In all experiments , we \ufb01ne - tune weights in all layers of BERT on downstream tasks .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "3.3", "entities": []}, {"text": "Disentangling Complexity Restriction and Information Deletion Pruning involves two steps : it deletes the information stored in a weight by setting it to 0 and then regularizes the model by preventing that weight from changing during further training .", "entities": [[23, 24, "DatasetName", "0"]]}, {"text": "To disentangle these two effects ( model complexity restriction and information deletion ) , we repeat the experiments from Section 3.2 with an identical pre - training setup , but instead of pruning we simply set the weights to 0 and allow them to vary during downstream training .", "entities": [[39, 40, "DatasetName", "0"]]}, {"text": "This deletes the pre - training information associated with the weight but does not prevent the model from \ufb01tting downstream datasets by keeping the weight at zero during downstream training .", "entities": []}, {"text": "We also \ufb01ne - tune on downstream tasks 8https://github.com/mitchellgordon95/bert-prune 9https://github.com/google-research/bert 10Evaluation curves leveled out at 20k steps .", "entities": []}, {"text": "11https://gluebenchmark.com/faq", "entities": []}, {"text": "146until training loss becomes comparable to models with no pruning .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "We trained most models for 13 epochs rather than 3 .", "entities": []}, {"text": "Models with 70 - 90 % information deletion required 15 epochs to \ufb01t the training data .", "entities": []}, {"text": "The results are also included in Figure 1 and Table 1 .", "entities": []}, {"text": "3.4 Pruning After Downstream Fine - tuning We might expect that BERT would be more compressible after downstream \ufb01ne - tuning .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "Intuitively , the information needed for downstream tasks is a subset of the information learned during pretraining ; some tasks require more semantic information than syntactic , and vice - versa .", "entities": []}, {"text": "We should be able to discard the \u201c extra \u201d information and only keep what we need for , say , parsing ( Li and Eisner , 2019 ) .", "entities": []}, {"text": "For magnitude weight pruning speci\ufb01cally , we might expect downstream training to change the distribution of weights in the parameter matrices .", "entities": []}, {"text": "This , in turn , changes the sort - order of the absolute values of those weights , which changes the order that we prune them in .", "entities": []}, {"text": "This new pruning order , hypothetically , would be less degrading to our speci\ufb01c downstream task .", "entities": []}, {"text": "To test this , we \ufb01ne - tuned pre - trained BERTBase on downstream data for 3 epochs .", "entities": []}, {"text": "We then pruned at various sparsity levels and continued training for 5 more epochs ( 7 for 80/90 % sparsity ) , at which point the training losses became comparable to those of models pruned during pretraining .", "entities": []}, {"text": "We repeat this for learning rates in [ 2;3;4;5]\u000210\u00005and show the results with the best development accuracy in Figure 1 / Table 1 .", "entities": [[16, 17, "MetricName", "accuracy"]]}, {"text": "We also measure the difference in which weights are selected for pruning during pre - training vs. downstream \ufb01ne - tuning and plot the results in Figure 3 . 4 Pruning Regimes 4.1 30 - 40 % of Weights Are Discardable Figure 1 shows that the \ufb01rst 30 - 40 % of weights pruned by magnitude weight pruning do not impact pre - training loss or inference on any downstream task .", "entities": [[64, 65, "MetricName", "loss"]]}, {"text": "These weights can be pruned either before or after \ufb01ne - tuning .", "entities": []}, {"text": "This makes sense from the perspective of pruning as sparse architecture search : when we initialize BERT - Base , we initialize many possible subnetworks .", "entities": [[16, 17, "MethodName", "BERT"]]}, {"text": "SGD selects the best one for pre - training and pushes the rest of the weights to 0 .", "entities": [[0, 1, "MethodName", "SGD"], [17, 18, "DatasetName", "0"]]}, {"text": "We can then prune those weights without affectingthe output of the network.12 4.2 Medium Pruning Levels Prevent Information Transfer Past 40 % pruning , performance starts to degrade .", "entities": []}, {"text": "Pre - training loss increases as we prune weights necessary for \ufb01tting the pre - training data ( Table 1 ) .", "entities": [[3, 4, "MetricName", "loss"]]}, {"text": "Feature activations of the hidden layers start to diverge from models with low levels of pruning ( Figure 2).13Downstream accuracy also begins to degrade at this point .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "Why does pruning at these levels hurt downstream performance ?", "entities": []}, {"text": "On one hand , pruning deletes pre - training information by setting weights to 0 , preventing the transfer of the useful inductive biases learned during pre - training .", "entities": [[14, 15, "DatasetName", "0"]]}, {"text": "On the other hand , pruning regularizes the model by keeping certain weights at zero , which might prevent \ufb01tting downstream datasets .", "entities": []}, {"text": "Figure 1 and Table 1 show information deletion is the main cause of performance degradation between 40 - 60 % sparsity , since pruning and information deletion degrade models by the same amount .", "entities": []}, {"text": "Information deletion would not be a problem if pretraining and downstream datasets contained similar information .", "entities": []}, {"text": "However , pre - training is effective precisely because the pre - training dataset is much larger than the labeled downstream dataset , which allows learning of more robust representations .", "entities": []}, {"text": "We see that the main obstacle to compressing pre - trained models is maintaining the inductive bias of the model learned during pre - training .", "entities": []}, {"text": "Encoding this bias requires many more weights than \ufb01tting downstream datasets , and it can not be recovered due to a fundamental information gap between pretraining and downstream datasets.14This leads us to believe that the amount a model can be pruned 12We know , however , that increasing the size of BERT to BERT - Large improves performance .", "entities": [[51, 52, "MethodName", "BERT"], [53, 54, "MethodName", "BERT"]]}, {"text": "This view does not fully explain why even an obviously under - parameterized model should become sparse .", "entities": []}, {"text": "This may be caused by dropout , or it may be a general property of our training regime ( SGD ) .", "entities": [[19, 20, "MethodName", "SGD"]]}, {"text": "Perhaps an extension of Tian et al .", "entities": []}, {"text": "( 2019 ) to under - parameterized models would provide some insight .", "entities": []}, {"text": "13We believe this observation may point towards a more principled stopping criterion for pruning .", "entities": []}, {"text": "Currently , the only way to know how much to prune is by trial and ( dev - set ) error .", "entities": []}, {"text": "Predictors of performance degradation while pruning might help us decide which level of sparsity is appropriate for a given trained network without trying many at once .", "entities": []}, {"text": "14We might consider \ufb01nding a lottery ticket for BERT , which we would expect to \ufb01t the GLUE training data just as well as pre - trained BERT ( Morcos et al . , 2019 ; Yu et al . , 2019 ) .", "entities": [[8, 9, "MethodName", "BERT"], [17, 18, "DatasetName", "GLUE"], [27, 28, "MethodName", "BERT"]]}, {"text": "However , we predict that the lottery - ticket will not reach similar generalization levels unless the lottery ticket encodes enough information to close the information gap .", "entities": []}, {"text": "147 Figure 1 : ( Blue ) The best GLUE dev accuracy and training losses for models pruned during pre - training , averaged over 5 tasks .", "entities": [[9, 10, "DatasetName", "GLUE"], [11, 12, "MetricName", "accuracy"]]}, {"text": "Also shown are models with information deletion during pre - training ( orange ) , models pruned after downstream \ufb01ne - tuning ( green ) , and models pruned randomly during pre - training instead of by lowest magnitude ( red ) .", "entities": []}, {"text": "30 - 40 % of weights can be pruned using magnitude weight pruning without decreasing dowsntream accuracy .", "entities": [[16, 17, "MetricName", "accuracy"]]}, {"text": "Notice that information deletion \ufb01ts the training data better than un - pruned models at all sparsity levels but does not fully recover evaluation accuracy .", "entities": [[24, 25, "MetricName", "accuracy"]]}, {"text": "Also , models pruned after downstream \ufb01ne - tuning have the same or worse development accuracy , despite achieving lower training losses .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "Note : none of the pruned models are over\ufb01tting because un - pruned models have the lowest training loss and the highest development accuracy .", "entities": [[18, 19, "MetricName", "loss"], [23, 24, "MetricName", "accuracy"]]}, {"text": "While the results for individual tasks are in Table 1 , each task does not vary much from the average trend , with an exception discussed in Section 4.3 .", "entities": []}, {"text": "Figure 2 : ( Left ) Pre - training loss predicts information deletion GLUE accuracy linearly as sparsity increases .", "entities": [[9, 10, "MetricName", "loss"], [13, 14, "DatasetName", "GLUE"], [14, 15, "MetricName", "accuracy"]]}, {"text": "We believe the slope of each line tells us how much a bit of BERT is worth to each task .", "entities": [[14, 15, "MethodName", "BERT"]]}, {"text": "( CoLA at 90 % is excluded from the line of best \ufb01t . )", "entities": [[1, 2, "DatasetName", "CoLA"]]}, {"text": "( Right ) The cosine similarities of features extracted for a subset of the pre - training development data before and after pruning .", "entities": []}, {"text": "Features are extracted from activations of all 12 layers of BERT and compared layer - wise to a model that has not been pruned .", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "As performance degrades , cosine similarities of features decreases .", "entities": []}, {"text": "148is limited by the largest dataset the model has been trained on : in this case , the pre - training dataset.15 4.3 High Pruning Levels Also Prevent Fitting Downstream Datasets At 70 % sparsity and above , models with information deletion recover some accuracy w.r.t . pruned models , so complexity restriction is a secondary cause of performance degradation .", "entities": [[44, 45, "MetricName", "accuracy"]]}, {"text": "However , these models do not recover all evaluation accuracy , despite matching un - pruned model \u2019s training loss .", "entities": [[9, 10, "MetricName", "accuracy"], [19, 20, "MetricName", "loss"]]}, {"text": "Table 1 shows that on the MNLI and QQP tasks , which have the largest amount of training data , information deletion performs much better than pruning .", "entities": [[6, 7, "DatasetName", "MNLI"], [8, 9, "DatasetName", "QQP"]]}, {"text": "In contrast , models do not recover as well on SST-2 and CoLA , which have less data .", "entities": [[10, 11, "DatasetName", "SST-2"], [12, 13, "DatasetName", "CoLA"]]}, {"text": "We believe this is because the larger datasets require larger models to \ufb01t , so complexity restriction becomes an issue earlier .", "entities": []}, {"text": "We might be concerned that poorly performing models are over-\ufb01tting , since they have lower training losses than unpruned models .", "entities": []}, {"text": "But the best performing information - deleted models have the lowest training error of all , so over\ufb01tting seems unlikely.16 4.4 How Much Is A Bit Of BERT Worth ?", "entities": [[27, 28, "MethodName", "BERT"]]}, {"text": "We \u2019ve seen that over - pruning BERT deletes information useful for downstream tasks .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "Is this information equally useful to all tasks ?", "entities": []}, {"text": "We might consider the pre - training loss as a proxy for how much pre - training information we \u2019ve deleted in total .", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "Similarly , the performance of informationdeletion models is a proxy for how much of that information was useful for each task .", "entities": []}, {"text": "Figure 2 shows that the pre - training loss linearly predicts the effects of information deletion on downstream accuracy .", "entities": [[8, 9, "MetricName", "loss"], [18, 19, "MetricName", "accuracy"]]}, {"text": "For every bit of information we delete from BERT , it appears only a fraction is useful for CoLA , and an even smaller fraction useful for QQP.17This relationship should be taken into account when considering the memory / accuracy trade - off of overpruning .", "entities": [[8, 9, "MethodName", "BERT"], [18, 19, "DatasetName", "CoLA"], [39, 40, "MetricName", "accuracy"]]}, {"text": "Pruning an extra 30 % of BERT \u2019s weights 15We would have more con\ufb01dence in this supposition if we had experiments where the pre - training data is much smaller than the downstream data .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "It would also be useful to have a more information - theoretic analysis of how data complexity in\ufb02uences model compressibility .", "entities": []}, {"text": "This is may be an interesting direction for future work .", "entities": []}, {"text": "16We are reminded of the double - descent risk curve proposed by Belkin et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "17We ca n\u2019t quantify this now , but perhaps compression will help quantify the \u201c universality \u201d of the LM task .", "entities": []}, {"text": "Figure 3 : ( Top ) The measured difference in pruning masks between models pruned during pre - training and models pruned during downstream \ufb01ne - tuning .", "entities": []}, {"text": "As predicted , the differences are less than 6 % , since \ufb01netuning only changes the magnitude sorting order of weights locally , not globally .", "entities": []}, {"text": "( Bottom ) The average GLUE development accuracy and pruning mask difference for models trained on downstream datasets before pruning 60 % at learning rate 5e-5 .", "entities": [[5, 6, "DatasetName", "GLUE"], [7, 8, "MetricName", "accuracy"], [23, 25, "HyperparameterName", "learning rate"]]}, {"text": "After pruning , models are trained for an additional 2 epochs to regain accuracy .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "We see that training between 3 and 12 epochs before pruning does not change which weights are pruned or improve performance .", "entities": []}, {"text": "is worth only one accuracy point on QQP but 10 points on CoLA .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 8, "DatasetName", "QQP"], [12, 13, "DatasetName", "CoLA"]]}, {"text": "It \u2019s unclear , however , whether this is because the pre - training task is less relevant to QQP or whether QQP simply has a bigger dataset with more information content.18 5 Downstream Fine - tuning Does Not Improve Prunability Since pre - training information deletion plays a central role in performance degradation while overpruning , we might expect that downstream \ufb01ne18Hendrycks et al .", "entities": [[19, 20, "DatasetName", "QQP"], [22, 23, "DatasetName", "QQP"]]}, {"text": "( 2019 ) suggest that pruning these weights might have a hidden cost : decreasing model robustness .", "entities": []}, {"text": "149 Figure 4 : ( Left ) The average , min , and max percentage of individual attention heads pruned at each sparsity level .", "entities": []}, {"text": "We see at 60 % sparsity , each attention head individually is pruned strictly between 55 % and 65 % .", "entities": []}, {"text": "( Right ) We compute the magnitude sorting order of each weight before and after downstream \ufb01ne - tuning .", "entities": []}, {"text": "If a weight \u2019s original position is 59 / 100 before \ufb01ne - tuning and 63 / 100 after \ufb01ne - tuning , then that weight moved 4 % in the sorting order .", "entities": []}, {"text": "After even an epoch of downstream \ufb01ne - tuning , weights quickly stabilize in a new sorting order which is not far from the original sorting order .", "entities": []}, {"text": "Variances level out similarly .", "entities": []}, {"text": "tuning would improve prunability by making important weights more salient ( increasing their magnitude ) .", "entities": []}, {"text": "However , Figure 1 shows that models pruned after downstream \ufb01ne - tuning do not surpass the development accuracies of models pruned during pre - training , despite achieving similar training losses .", "entities": []}, {"text": "Figure 3 shows \ufb01ne - tuning changes which weights are pruned by less than 6 % .", "entities": []}, {"text": "Why does n\u2019t \ufb01ne - tuning change which weights are pruned much ?", "entities": []}, {"text": "Table 2 shows that the magnitude sorting order of weights is mostly preserved ; weights move on average 0 - 4 % away from their starting positions in the sort order .", "entities": [[18, 19, "DatasetName", "0"]]}, {"text": "We also see that high magnitude weights are more stable than lower ones ( Figure 6 ) .", "entities": []}, {"text": "Our experiments suggest that training on downstream data before pruning is too blunt an instrument to improve prunability .", "entities": []}, {"text": "Even so , we might consider simply training on the downstream tasks for much longer , which would increase the difference in weights pruned .", "entities": []}, {"text": "However , Figure 4 shows that even after an epoch of downstream \ufb01ne - tuning , weights quickly re - stabilize in a new sorting order , meaning longer downstream training will have only a marginal effect on which weights are pruned .", "entities": []}, {"text": "Indeed , Figure 3 shows that the weights selected for 60 % pruning quickly stabilize and evaluation accuracy does not improve with more training before pruning.6", "entities": [[17, 18, "MetricName", "accuracy"]]}, {"text": "Related Work Compressing BERT for Speci\ufb01c Tasks Section 5 showed that downstream \ufb01ne - tuning does not increase prunability .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "However , several alternative compression approaches have been proposed to discard non - task - speci\ufb01c information .", "entities": []}, {"text": "Li and Eisner ( 2019 ) used an information bottleneck to discard non - syntactic information .", "entities": []}, {"text": "Tang et al .", "entities": []}, {"text": "( 2019 ) used BERT as a knowledge distillation teacher to compress relevant information into smaller Bi - LSTMs , while Kuncoro et al .", "entities": [[4, 5, "MethodName", "BERT"], [7, 9, "MethodName", "knowledge distillation"]]}, {"text": "( 2019 ) took a similar distillation approach .", "entities": []}, {"text": "While \ufb01ne - tuning does not increase prunability , task - speci\ufb01c knowledge might be extracted from BERT with other methods .", "entities": [[17, 18, "MethodName", "BERT"]]}, {"text": "Attention Head Pruning previously showed redundancy in transformer models by pruning entire attention heads .", "entities": []}, {"text": "Michel et al .", "entities": []}, {"text": "( 2019 ) showed that after \ufb01ne - tuning on MNLI , up to 40 % of attention heads can be pruned from BERT without affecting test accuracy .", "entities": [[10, 11, "DatasetName", "MNLI"], [23, 24, "MethodName", "BERT"], [27, 28, "MetricName", "accuracy"]]}, {"text": "They show redundancy in BERT after \ufb01ne - tuning on a single downstream task ; in contrast , our work emphasizes the interplay between compression and transfer learning to many tasks , pruning both before and after \ufb01netuning .", "entities": [[4, 5, "MethodName", "BERT"], [26, 28, "TaskName", "transfer learning"]]}, {"text": "Also , magnitude weight pruning allows us to additionally prune the feed - foward networks and sub - word embeddings in BERT ( not just selfattention ) , which account for \u001872 % of BERT \u2019s total memory usage .", "entities": [[18, 20, "TaskName", "word embeddings"], [21, 22, "MethodName", "BERT"], [34, 35, "MethodName", "BERT"]]}, {"text": "We suspect that attention head pruning and weight pruning remove different redundancies from", "entities": []}, {"text": "150BERT .", "entities": []}, {"text": "Figure 4 shows that weight pruning does not prune any speci\ufb01c attention head much more than the pruning rate for the whole model .", "entities": []}, {"text": "It is not clear , however , whether weight pruning and recovery training makes attention heads less prunable by distributing functionality to unused heads .", "entities": []}, {"text": "7 Conclusion And Future Work We \u2019ve shown that encoding BERT \u2019s inductive bias requires many more weights than are required to \ufb01t downstream data .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "Future work on compressing pre - trained models should focus on maintaining that inductive bias and quantifying its relevance to various tasks during accuracy / memory trade - offs .", "entities": [[23, 24, "MetricName", "accuracy"]]}, {"text": "For magnitude weight pruning , we \u2019ve shown that 30 - 40 % of the weights do not encode any useful inductive bias and can be discarded without affecting BERT \u2019s universality .", "entities": [[30, 31, "MethodName", "BERT"]]}, {"text": "The relevance of the rest of the weights vary from task to task , and \ufb01ne - tuning on downstream tasks does not change the nature of this trade - off by changing which weights are pruned .", "entities": []}, {"text": "In future work , we will investigate the factors that in\ufb02uence language modeling \u2019s relevance to downstream tasks and how to improve compression in a task - general way .", "entities": []}, {"text": "It \u2019s reasonable to believe that these conclusions will generalize to other pre - trained language models such as Kermit ( Chan et al . , 2019 ) , XLNet ( Yang et al . , 2019 ) , GPT-2 ( Radford et al . , 2019 ) , RoBERTa ( Liu et al . , 2019a ) or ELMO ( Peters et al . , 2018 ) .", "entities": [[29, 30, "MethodName", "XLNet"], [39, 40, "MethodName", "GPT-2"], [49, 50, "MethodName", "RoBERTa"], [59, 60, "MethodName", "ELMO"]]}, {"text": "All of these learn some variant of language modeling , and most use Transformer architectures .", "entities": [[13, 14, "MethodName", "Transformer"]]}, {"text": "While it remains to be shown in future work , viewing pruning as architecture search implies these models will be prunable due to the training dynamics inherent to neural networks .", "entities": []}, {"text": "References Mart \u00b4 \u0131n Abadi , Ashish Agarwal , Paul Barham , Eugene Brevdo , Zhifeng Chen , Craig Citro , Gregory S. Corrado , Andy Davis , Jeffrey Dean , Matthieu Devin , Sanjay Ghemawat , Ian J. Goodfellow , Andrew Harp , Geoffrey Irving , Michael Isard , Yangqing Jia , Rafal J\u00b4ozefowicz , Lukasz Kaiser , Manjunath Kudlur , Josh Levenberg , Dan Man \u00b4 e , Rajat Monga , Sherry Moore , Derek Gordon Murray , Chris Olah , Mike Schuster , Jonathon Shlens , Benoit Steiner , Ilya Sutskever , Kunal Talwar , Paul A. Tucker , Vincent Vanhoucke , Vijay Vasudevan , Fernanda B. Vi \u00b4 egas , Oriol Vinyals , Pete Warden , Martin Wattenberg , Martin Wicke , Yuan Yu , and Xiaoqiang Zheng .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Tensor\ufb02ow : Large - scale machine learning on heterogeneous distributed systems .", "entities": []}, {"text": "CoRR , abs/1603.04467.Jay Alammar .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The illustrated transformer .", "entities": []}, {"text": "Sanjeev Arora , Rong Ge , Behnam Neyshabur , and Yi Zhang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Stronger generalization bounds for deep nets via a compression approach .", "entities": []}, {"text": "CoRR , abs/1802.05296 .", "entities": []}, {"text": "Mikhail Belkin , Daniel Hsu , Siyuan Ma , and Soumik Mand al . 2018 .", "entities": []}, {"text": "Reconciling modern machine learning practice and the bias - variance trade - off .", "entities": []}, {"text": "arXiv e - prints , page arXiv:1812.11118 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Cristian Bucila , Rich Caruana , and Alexandru Niculescu - Mizil .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Model compression .", "entities": [[0, 2, "TaskName", "Model compression"]]}, {"text": "In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Philadelphia , PA , USA , August 20 - 23 , 2006 , pages 535\u2013541 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "William Chan , Nikita Kitaev , Kelvin Guu , Mitchell Stern , and Jakob Uszkoreit . 2019 .", "entities": []}, {"text": "KERMIT : generative insertion - based modeling for sequences .", "entities": []}, {"text": "CoRR , abs/1906.01604 .", "entities": []}, {"text": "Tim Dettmers and Luke S. Zettlemoyer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Sparse networks from scratch :", "entities": []}, {"text": "Faster training without losing performance .", "entities": []}, {"text": "ArXiv , abs/1907.04840 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "CoRR , abs/1810.04805 .", "entities": []}, {"text": "Utku Evci , Fabian Pedregosa , Aidan N. Gomez , and Erich Elsen .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The dif\ufb01culty of training sparse neural networks .", "entities": []}, {"text": "CoRR , abs/1906.10732 .", "entities": []}, {"text": "Jonathan Frankle and Michael Carbin .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The lottery ticket hypothesis : Finding sparse , trainable neural networks .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Trevor Gale , Erich Elsen , and Sara Hooker .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The state of sparsity in deep neural networks .", "entities": []}, {"text": "CoRR , abs/1902.09574 .", "entities": []}, {"text": "Song Han , Xingyu Liu , Huizi Mao , Jing Pu , Ardavan Pedram , Mark A. Horowitz , and William J. Dally .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Eie : Ef\ufb01cient inference engine on compressed deep neural network .", "entities": []}, {"text": "In Proceedings of the 43rd International Symposium on Computer Architecture , ISCA \u2019 16 , pages 243\u2013254 , Piscataway , NJ , USA .", "entities": []}, {"text": "IEEE Press .", "entities": []}, {"text": "Song Han , Jeff Pool , John Tran , and William Dally . 2015 .", "entities": []}, {"text": "Learning both weights and connections for ef\ufb01cient neural network .", "entities": []}, {"text": "In C. Cortes , N. D. Lawrence , D. D. Lee , M. Sugiyama , and R. Garnett , editors , Advances in Neural Information Processing Systems 28 , pages 1135\u20131143 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Dan Hendrycks , Kimin Lee , and Mantas Mazeika . 2019 .", "entities": []}, {"text": "Using pre - training can improve model robustness and uncertainty .", "entities": []}, {"text": "In ICML , pages 2712\u20132721 .", "entities": []}, {"text": "151Guillaume Klein , Yoon Kim , Yuntian Deng , Jean Senellart , and Alexander M. Rush .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Opennmt : Open - source toolkit for neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "InProc .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Adhiguna Kuncoro , Chris Dyer , Laura Rimell , Stephen Clark , and Phil Blunsom .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Scalable syntaxaware language models using knowledge distillation .", "entities": [[5, 7, "MethodName", "knowledge distillation"]]}, {"text": "CoRR , abs/1906.06438 .", "entities": []}, {"text": "Xiang Lisa Li and Jason Eisner .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Specializing word embeddings ( for parsing ) by information bottleneck .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing , Hong Kong .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov . 2019a .", "entities": []}, {"text": "Roberta : A robustly optimized BERT pretraining approach .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "CoRR , abs/1907.11692 .", "entities": []}, {"text": "Zhuang Liu , Mingjie Sun , Tinghui Zhou , Gao Huang , and Trevor Darrell . 2019b .", "entities": []}, {"text": "Rethinking the value of network pruning .", "entities": [[4, 6, "TaskName", "network pruning"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Christos Louizos , Max Welling , and Diederik P. Kingma .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning sparse neural networks through l-0 regularization .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Paul Michel , Omer Levy , and Graham Neubig .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Are sixteen heads really better than one ?", "entities": []}, {"text": "ArXiv , abs/1905.10650 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Dmitry Molchanov , Arsenii Ashukha , and Dmitry Vetrov . 2017 .", "entities": []}, {"text": "Variational dropout sparsi\ufb01es deep neural networks .", "entities": [[0, 2, "MethodName", "Variational dropout"]]}, {"text": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICML\u201917 , pages 2498\u20132507 .", "entities": []}, {"text": "JMLR.org .", "entities": []}, {"text": "Ari S. Morcos , Haonan Yu , Michela Paganini , and Yuand ong Tian .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "One ticket to win them all : generalizing lottery ticket initializations across datasets and optimizers .", "entities": []}, {"text": "arXiv e - prints , page arXiv:1906.02773 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hesham Mostafa and Xin Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Parameter ef\ufb01cient training of deep convolutional neural networks by dynamic sparse reparameterization .", "entities": []}, {"text": "Matthew E. Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "CoRR , abs/1802.05365 .", "entities": []}, {"text": "Alec Radford , Jeff Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Karen Simonyan and Andrew Zisserman .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Very Deep Convolutional Networks for LargeScale Image Recognition .", "entities": [[6, 8, "TaskName", "Image Recognition"]]}, {"text": "arXiv e", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "- prints ,", "entities": []}, {"text": "page arXiv:1409.1556.Nimit Sharad Sohoni , Christopher Richard Aberger , Megan Leszczynski , Jian Zhang , and Christopher R\u00b4e .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Low - memory neural network training : A technical report .", "entities": []}, {"text": "CoRR , abs/1904.10631 .", "entities": []}, {"text": "Emma Strubell , Ananya Ganesh , and Andrew McCallum . 2019 .", "entities": []}, {"text": "Energy and policy considerations for deep learning in NLP .", "entities": []}, {"text": "CoRR , abs/1906.02243 .", "entities": []}, {"text": "Raphael Tang , Yao Lu , Linqing Liu , Lili Mou , Olga Vechtomova , and Jimmy Lin .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Distilling taskspeci\ufb01c knowledge from BERT into simple neural networks .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "CoRR , abs/1903.12136 .", "entities": []}, {"text": "Yuandong Tian , Tina Jiang , Qucheng Gong , and Ari S. Morcos .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Luck matters : Understanding training dynamics of deep relu networks .", "entities": [[8, 9, "MethodName", "relu"]]}, {"text": "CoRR , abs/1905.13405 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "CoRR , abs/1706.03762 .", "entities": []}, {"text": "Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel R. Bowman . 2019 .", "entities": []}, {"text": "GLUE :", "entities": [[0, 1, "DatasetName", "GLUE"]]}, {"text": "A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[9, 12, "TaskName", "natural language understanding"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime G. Carbonell , Ruslan Salakhutdinov , and Quoc V .", "entities": [[13, 14, "DatasetName", "Ruslan"]]}, {"text": "Le . 2019 .", "entities": []}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "CoRR , abs/1906.08237 .", "entities": []}, {"text": "Haonan Yu , Sergey Edunov , Yuandong Tian , and Ari S. Morcos .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Playing the lottery with rewards and multiple languages : lottery tickets in RL and NLP .", "entities": []}, {"text": "arXiv e", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "- prints , page arXiv:1906.02768 .", "entities": []}, {"text": "Michael Zhu and Suyog Gupta .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "To prune , or not to prune : exploring the ef\ufb01cacy of pruning for model compression .", "entities": [[14, 16, "TaskName", "model compression"]]}, {"text": "arXiv e - prints , page arXiv:1710.01878 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "A Appendix", "entities": []}, {"text": "152PrunedPre - train LossMNLI 392kQQP 363kQNLI 108kSST-2 67kCoLA 8.5kA VG 0 1.82 83.1 j0.25 90.5j0.10 91.1j0.12 92.1j0.06 79.1j0.26 87.2j15.7 10 1.82 83.3 j0.21 90.4j0.10 91.0j0.12 91.6j0.07 79.4j0.30 87.2j16.0 20 1.83 83.3 j0.24 90.5j0.11 91.1j0.11 91.6j0.05 79.1j0.30 87.1j16.0 30 1.86 83.3 j0.23 90.2j0.12 90.7j0.12 91.9j0.06 79.5j0.31", "entities": [[10, 11, "DatasetName", "0"]]}, {"text": "87.1j16.9 40 1.93 83.0 j0.25 90.1j0.12 90.4j0.12 91.5j0.06 78.4j0.23 86.7j15.6 50 2.03 82.6 j0.27 89.8j0.13 90.2j0.13 90.9j0.07", "entities": []}, {"text": "77.4j0.30 86.2j18.0 60 2.25 81.8 j0.32", "entities": []}, {"text": "89.4j0.16 89.3j0.16 91.4j0.07 75.9j0.44 85.6j23.0 70 2.62 79.5 j0.40 88.6j0.18 88.4j0.21 90.1j0.10 72.7j0.47 83.9j27.1 80 3.44 75.9 j0.49 86.9j0.24 85.3j0.29 88.1j0.12", "entities": []}, {"text": "69.1j0.61 81.1j34.8 90 5.83 64.8 j0.76 81.1j0.36 71.7j0.52 80.3j0.25 69.1j0.61 73.4j49.8 Information Deletion 0 1.82 83.0 j0.20 90.6j0.06 90.0j0.10 92.1j0.03 80.6j0.18 87.3j11.6 10 1.82 82.8 j0.01 90.5j0.05 90.5j0.09 92.2j0.05 80.8j0.16 87.4j07.2 20 1.83 82.9 j0.01 90.5j0.05 90.5j0.09 91.5j0.05 80.3j0.16 87.2j07.3 30 1.86 82.3 j0.01 90.6j0.04 90.5j0.10", "entities": [[13, 14, "DatasetName", "0"]]}, {"text": "90.8j0.05 80.0j0.18", "entities": []}, {"text": "86.9j07.7 40 1.93 82.2 j0.19 90.5j0.05 90.1j0.10 92.0j0.05 79.0j0.17 86.7j11.1 50 2.03 82.5 j0.19 90.3j0.05 90.2j0.10 91.2j0.05 77.9j0.19 86.4j11.6 60 2.25 81.9 j0.20 90.1j0.05 89.5j0.10 90.8j0.05 76.4j0.23 85.7j12.6 70 2.62 80.8 j0.01 90.2j0.01 88.7j0.10 90.3j0.06 74.4j0.28 84.9j09.3 80 3.44 78.6 j0.01 89.3j0.02 86.0j0.02 88.8j0.07", "entities": []}, {"text": "70.0j0.45 82.5j11.5 90 5.83 72.9 j0.01 87.5j0.02", "entities": []}, {"text": "76.8j0.06 83.0j0.09", "entities": []}, {"text": "69.1j0.61 77.9j15.7 Pruned after Downstream Fine - tuning 0 - 82.6 j0.15 90.6j0.06 90.1j0.10", "entities": [[8, 9, "DatasetName", "0"]]}, {"text": "92.1j0.04 78.7j0.25 86.8j12.0 10 - 82.9 j0.19 90.6j0.06 90.3j0.10", "entities": []}, {"text": "91.6j0.05 79.0j0.11 86.9j10.3 20 - 82.7 j0.15 90.6j0.07 90.2j0.07", "entities": []}, {"text": "92.0j0.04 79.0j0.22 86.9j10.7 30 - 82.7 j0.23 90.4j0.07 89.7j0.07 91.6j0.04 78.5j0.23 86.6j12.8 40 - 82.7 j0.25", "entities": []}, {"text": "90.5j0.11 89.9j0.12 91.7j0.05 78.8j0.17 86.7j13.9 50 - 82.6 j0.19 90.3j0.08 89.7j0.11 90.8j0.06 78.0j0.22 86.3j13.0 60 - 81.8 j0.22 90.2j0.10 89.3j0.12", "entities": []}, {"text": "90.6j0.06 76.1j0.31 85.6j16.4 70 - 80.5 j0.30 89.4j0.14 86.2j0.19", "entities": []}, {"text": "88.2j0.07 69.5j0.58 82.7j25.8 80 - 73.7 j0.53 87.8j0.12 80.4j0.21 86.4j0.07 69.1j0.59 79.5j30.5 90 - 58.7 j0.86 82.5j0.26 65.2j0.52 81.5j0.16 69.1j0.61 71.4j47.9 Random Pruning 0 1.82 83.3 j0.26 90.5j0.10 90.6j0.15 92.4j0.07 78.7j0.18", "entities": [[23, 24, "DatasetName", "0"]]}, {"text": "87.1j15.3 10 2.09 82.0 j0.27 90.1j0.12 90.3j0.13 92.3j0.05 77.0j0.32 86.3j18.0 20 2.46 80.6 j0.32 89.8j0.12 88.5j0.14", "entities": []}, {"text": "91.1j0.07 73.5j0.39 84.7j20.8 30 2.98 79.1 j0.36 89.2j0.14 86.9j0.23 89.3j0.10 71.8j0.47 83.3j25.9 40 3.76 75.4 j0.45 88.2j0.16 84.5j0.23", "entities": []}, {"text": "88.6j0.09 69.3j0.57 81.2j30.3 50 4.73 71.6 j0.60 86.6j0.20 81.5j0.28 85.0j0.10", "entities": []}, {"text": "69.1j0.61 78.8j35.8 60 5.63 70.4 j0.60 85.2j0.24 71.7j0.45 81.5j0.21 69.1j0.61 75.6j42.3 70 6.22 64.1 j0.76", "entities": []}, {"text": "81.4j0.34 63.0j0.62 80.6j0.20", "entities": []}, {"text": "69.1j0.61 71.6j50.3 80 6.87 58.8 j0.84 76.6j0.46", "entities": []}, {"text": "61.1j0.64 80.6j0.23", "entities": []}, {"text": "69.1j0.61 69.3j55.6 90 7.37 49.8 j0.98 74.3j0.51 60.2j0.65 75.1j0.33 69.1j0.61 65.7j61.4 Table 1 : Pre - training development losses and GLUE task development accuracies for various levels of pruning .", "entities": [[20, 21, "DatasetName", "GLUE"]]}, {"text": "Each development accuracy is accompanied on its right by the achieved training loss , evaluated on the entire training set .", "entities": [[2, 3, "MetricName", "accuracy"], [12, 13, "MetricName", "loss"]]}, {"text": "Averages are summarized in Figure 1 .", "entities": []}, {"text": "Pre - training losses are omitted for models pruned after downstream \ufb01ne - tuning because it is not clear how to measure their performance on the pre - training task in a fair way .", "entities": []}, {"text": "153 Figure 5 : The sum of weights pruned at each sparsity level for one shot pruning of BERT .", "entities": [[18, 19, "MethodName", "BERT"]]}, {"text": "Given the motivation for our saliency criterion , it seems strange that such a large magnitude of weights can be pruned without decreasing accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}, {"text": "LR MNLI QQP QNL SST-2 CoLA", "entities": [[1, 2, "DatasetName", "MNLI"], [2, 3, "DatasetName", "QQP"], [4, 5, "DatasetName", "SST-2"], [5, 6, "DatasetName", "CoLA"]]}, {"text": "2e-5 1.91\u00061.81 1.82\u00061.72 1.27\u00061.22 1.06\u00061.03 0.79\u00060.77", "entities": []}, {"text": "3e-5 2.68\u00062.51 2.56\u00062.40 1.79\u00061.69 1.54\u00061.47 1.06\u00061.03 4e-5 3.41\u00063.18 3.30\u00063.10 2.31\u00062.19 1.99\u00061.89", "entities": []}, {"text": "1.11\u00061.09 5e-5 4.12\u00063.83 4.02\u00063.74 2.77\u00062.62 2.38\u00062.29 1.47\u00061.43 Table 2 : We compute the magnitude sorting order of each weight before and after downstream \ufb01ne - tuning .", "entities": []}, {"text": "If a weight \u2019s original position is 59 / 100 before \ufb01ne - tuning and 63 / 100 after \ufb01ne - tuning , then that weight moved 4 % in the sorting order .", "entities": []}, {"text": "We then list the average movement of weights in each model , along with the standard deviation .", "entities": []}, {"text": "Sorting order changes mostly locally across tasks : a weight moves , on average , 0 - 4 % away from its starting position .", "entities": [[15, 16, "DatasetName", "0"]]}, {"text": "As expected , larger datasets and larger learning rates have more movement ( per epoch ) .", "entities": []}, {"text": "We also see that higher magnitude weights are more stable than lower weights , see Figure 6 .", "entities": []}, {"text": "Figure 6 : We show how weight sort order movements are distributed during \ufb01ne - tuning , given a weight \u2019s starting magnitude .", "entities": []}, {"text": "We see that higher magnitude weights are more stable than lower magnitude weights and do not move as much in the sort order .", "entities": []}, {"text": "This plot is nearly identical for every model and learning rate , so we only show it once .", "entities": [[9, 11, "HyperparameterName", "learning rate"]]}, {"text": "154 Figure 7 : A heatmap of the weight magnitudes of the 12 horizontally stacked self - attention key projection matrices for layer 1 .", "entities": [[5, 6, "MethodName", "heatmap"]]}, {"text": "A banding pattern can be seen : the highest values of the matrix tend to cluster in certain attention heads .", "entities": []}, {"text": "This pattern appears in most of the self - attention parameter matrices , but it does not cause pruning to prune one head more than another .", "entities": []}, {"text": "However , it may prove to be a useful heuristic for attention head pruning , which would not require making many passes over the training data .", "entities": []}, {"text": "( Right ) A heatmap of the weight magnitudes of BERT \u2019s subword embeddings .", "entities": [[4, 5, "MethodName", "heatmap"], [10, 11, "MethodName", "BERT"]]}, {"text": "Interestingly , pruning BERT embeddings are more interpretable ; we can see shorter subwords ( top rows ) have smaller magnitude values and thus will be pruned earlier than other subword embeddings .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "Weight Matrix Weight Mean Weight STD embeddings word embeddings -0.0282 0.042 layer 0 attention output", "entities": [[7, 9, "TaskName", "word embeddings"], [12, 13, "DatasetName", "0"]]}, {"text": "FC -0.0000 0.029 layer 0 self attn key 0.0000 0.043 layer 0 self attn query 0.0000 0.043 layer 0 self attn value -0.0000 0.029 layer 0 intermediate FC -0.0000 0.037 layer 0 output FC -0.0012 0.036 layer 1 attention output FC 0.0001 0.028 layer 1 self attn key 0.0000 0.043 layer 1 self attn query -0.0003 0.043 layer 1 self attn value -0.0000 0.029 layer 1 intermediate FC 0.0001 0.039 layer 1 output FC -0.0014 0.038 layer 10 attention output FC -0.0000 0.033 layer 10 self attn key -0.0000 0.046 layer 10 self attn query 0.0002 0.046 layer 10 self attn value -0.0000 0.036 layer 10 intermediate FC 0.0000 0.039 layer 10 output FC -0.0011 0.038 layer 11 attention output", "entities": [[4, 5, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [31, 32, "DatasetName", "0"]]}, {"text": "FC -0.0000 0.037 layer 11 self attn key 0.0002 0.044 layer 11 self attn query -0.0001 0.045 layer 11 self attn value -0.0000 0.039 layer 11 intermediate FC 0.0004 0.039 layer 11 output FC -0.0008 0.036", "entities": []}, {"text": "155layer 2 attention output FC 0.0000 0.027 layer 2 self attn key 0.0000 0.047 layer 2 self attn query 0.0000 0.048 layer 2 self attn value -0.0000", "entities": []}, {"text": "0.028 layer 2 intermediate FC 0.0001 0.040 layer 2 output FC -0.0015 0.038 layer 3 attention output FC 0.0001 0.029 layer 3 self attn key 0.0000 0.043 layer 3 self attn query 0.0003 0.043 layer 3 self attn value -0.0001 0.031 layer 3 intermediate FC -0.0001 0.040 layer 3 output FC -0.0014 0.039 layer 4 attention output FC 0.0000 0.033 layer 4 self attn key 0.0000 0.042 layer 4 self attn query -0.0001 0.042 layer 4 self attn value 0.0001 0.035 layer 4 intermediate FC 0.0001 0.041 layer 4 output FC -0.0014 0.040 layer 5 attention output", "entities": []}, {"text": "FC -0.0000 0.033 layer 5 self attn key -0.0001 0.043 layer 5 self attn query -0.0000 0.043 layer 5 self attn value -0.0000 0.035 layer 5 intermediate FC 0.0000 0.041 layer 5 output FC -0.0014 0.039 layer 6 attention output FC 0.0001 0.032 layer 6 self attn key -0.0000 0.043 layer 6 self attn query 0.0001 0.043 layer 6 self attn value 0.0000 0.034 layer 6 intermediate FC -0.0000 0.041 layer 6 output FC -0.0014 0.039 layer 7 attention output FC 0.0000 0.032 layer 7 self attn key -0.0000 0.044 layer 7 self attn query -0.0000 0.044 layer 7 self attn value 0.0001 0.033 layer 7 intermediate FC 0.0003 0.039 layer 7 output FC -0.0013", "entities": []}, {"text": "0.038 layer 8 attention output FC 0.0000 0.034 layer 8 self attn key -0.0000 0.044 layer 8 self attn query 0.0001 0.044 layer 8 self attn value 0.0000 0.035 layer 8 intermediate FC 0.0004 0.039 layer 8 output", "entities": []}, {"text": "FC -0.0013 0.037 layer 9 attention output FC 0.0001 0.033 layer 9 self attn key 0.0000 0.046 layer 9 self attn query -0.0001 0.046 layer 9 self attn value 0.0000 0.035 layer 9 intermediate FC 0.0005 0.040 layer 9 output FC -0.0012 0.039 pooler FC 0.0000 0.029 Table 3 : The values of BERT \u2019s weights are normally distributed in each weight matrix .", "entities": [[53, 54, "MethodName", "BERT"]]}, {"text": "The means and variances are listed for each .", "entities": []}]