[{"text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2886\u20132892 Copenhagen , Denmark , September 7\u201311 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics Deep Multi - Task Learning for Aspect Term Extraction with Memory Interaction\u2217 Xin Li andWai Lam Key Laboratory on High Con\ufb01dence Software Technologies ( Sub - Lab , CUHK ) , Ministry of Education , and Department of Systems Engineering and Engineering Management", "entities": [[6, 10, "TaskName", "Multi - Task Learning"], [12, 14, "TaskName", "Term Extraction"], [33, 34, "DatasetName", "CUHK"], [47, 48, "TaskName", "Management"]]}, {"text": "The Chinese University of Hong Kong , Hong Kong { lixin , wlam}@se.cuhk.edu.hk", "entities": []}, {"text": "Abstract We propose a novel LSTM - based deep multi - task learning framework for aspect term extraction from user review sentences .", "entities": [[5, 6, "MethodName", "LSTM"], [9, 13, "TaskName", "multi - task learning"], [16, 18, "TaskName", "term extraction"]]}, {"text": "Two LSTMs equipped with extended memories and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions .", "entities": []}, {"text": "Sentimental sentence constraint is also added for more accurate prediction via another LSTM .", "entities": [[12, 13, "MethodName", "LSTM"]]}, {"text": "Experiment results over two benchmark datasets demonstrate the effectiveness of our framework .", "entities": []}, {"text": "1 Introduction The aspect - based sentiment analysis ( ABSA ) task is to identify opinions expressed towards speci\ufb01c entities such as laptop or attributes of entities such asprice ( Liu , 2012a ) .", "entities": [[3, 8, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "This task involves three subtasks : Aspect Term Extraction ( ATE ) , Aspect Polarity Detection and Aspect Category Detection .", "entities": [[7, 9, "TaskName", "Term Extraction"], [17, 20, "TaskName", "Aspect Category Detection"]]}, {"text": "As a fundamental subtask in ABSA , the goal of the ATE task is to identify opinionated aspect expressions .", "entities": []}, {"text": "One of most important characteristics is that opinion words can provide indicative clues for aspect detection since opinion words should co - occur with aspect words .", "entities": []}, {"text": "Most publicly available datasets contain the gold standard annotations for opinionated aspects , but the ground truth of the corresponding opinion words is not commonly provided .", "entities": []}, {"text": "Some works tackling the ATE task ignore the consideration of opinion words and just focus on aspect term modeling and learning ( Jin \u2217The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project Code : 14203414 ) .", "entities": []}, {"text": "We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback.et al . , 2009 ; Jakob and Gurevych , 2010 ; Toh and Wang , 2014 ; Chernyshevich , 2014 ; Manek et al . , 2017 ; San Vicente et al . , 2015 ; Liu et al . , 2015 ; Poria et al . , 2016 ; Toh and Su , 2016 ; Yin et al . , 2016 ) .", "entities": []}, {"text": "They fail to leverage opinion information which is supposed to be useful clues .", "entities": []}, {"text": "Some works tackling the ATE task consider opinion information ( Hu and Liu , 2004a , b ; Popescu and Etzioni , 2005 ; Zhuang et al . , 2006 ; Qiu et al . , 2011 ; Liu et al . , 2012b , 2013a , b , 2014 ) in an unsupervised or partially supervised manner .", "entities": []}, {"text": "Qiu et al .", "entities": []}, {"text": "( 2011 ) proposed Double Propagation ( DP ) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph .", "entities": []}, {"text": "One drawback is that it heavily relies on the dependency parser , which is prone to generate mistakes when applying on informal online reviews .", "entities": []}, {"text": "Liu et al .", "entities": []}, {"text": "( 2014 ) modeled relation between aspects and opinions by constructing a bipartite heterogenous graph .", "entities": []}, {"text": "It can not perform well without a high - quality phrase chunker and POS tagger reducing its \ufb02exibility .", "entities": []}, {"text": "As unsupervised or partially supervised frameworks can not take the full advantages of aspect annotations commonly found in the training data , the above methods lead to de\ufb01ciency in leveraging the data .", "entities": []}, {"text": "Recently , Wang et al .", "entities": []}, {"text": "( 2016 ) considered relation between opinion words and aspect words in a supervised model named RNCRF .", "entities": []}, {"text": "However , RNCRF tends to suffer from parsing errors since the structure of the recursive network hinges on the dependency parse tree .", "entities": []}, {"text": "CMLA ( Wang et al . , 2017a ) used a multilayer neural model where each layer consists of aspect attention and opinion attention .", "entities": []}, {"text": "However CMLA merely employs standard GRU without extended memories .", "entities": [[5, 6, "MethodName", "GRU"]]}, {"text": "We propose MIN ( Memory Interaction Network ) , a novel LSTM - based deep multi - task learning framework for the ATE task .", "entities": [[11, 12, "MethodName", "LSTM"], [15, 19, "TaskName", "multi - task learning"]]}, {"text": "Two LSTMs with extended memory are designed for handling2886", "entities": []}, {"text": "the extraction tasks of aspects and opinions .", "entities": []}, {"text": "The aspect - opinion relationship is established based on neural memory interactions between aspect extraction and opinion extraction where the global indicator score of opinion terms and local positional relevance between aspects and opinions are considered .", "entities": [[13, 15, "TaskName", "aspect extraction"]]}, {"text": "To ensure that aspects are from sentimental sentences , MIN employs a third LSTM for sentimental sentence classi\ufb01cation facilitating more accurate aspect term extraction .", "entities": [[13, 14, "MethodName", "LSTM"], [22, 24, "TaskName", "term extraction"]]}, {"text": "Experiment results over two benchmark datasets show that our framework achieves superior performance .", "entities": []}, {"text": "2 Model 2.1 Overview Let an input review sentence with Tword tokens and the corresponding distributed representations bew = { w1, ... ,w T}andx", "entities": []}, {"text": "= { x1, ... ,x T}respectively .", "entities": []}, {"text": "The ATE task is treated as a sequence labeling task with BIO tagging scheme and the set of aspect tags for the word wtisyA t\u2208{B , I , O } , whereB , I , O represent beginning of , inside and outside of the aspect span respectively .", "entities": []}, {"text": "Commonly found training data contains gold annotations for aspect terms and opinionated sentences , but the gold standard of opinion words are usually not available .", "entities": []}, {"text": "In our multi - task learning framework , three tasks are involved : ( 1 ) aspect term extraction ( ATE ) , ( 2 ) opinion word extraction and ( 3 ) sentimental sentence classi\ufb01cation .", "entities": [[2, 6, "TaskName", "multi - task learning"], [17, 19, "TaskName", "term extraction"]]}, {"text": "We design a taskspeci\ufb01c LSTM , namely , A - LSTM , O - LSTM and S - LSTM , for tackling each of the above tasks respectively .", "entities": [[4, 5, "MethodName", "LSTM"], [10, 11, "MethodName", "LSTM"], [14, 15, "MethodName", "LSTM"], [18, 19, "MethodName", "LSTM"]]}, {"text": "The \ufb01rst component of our proposed framework consists of A - LSTM and O - LSTM where we equip LSTMs with extended operational memories and some operations are de\ufb01ned over the memories for task - level memory interactions .", "entities": [[11, 12, "MethodName", "LSTM"], [15, 16, "MethodName", "LSTM"]]}, {"text": "The second component is to determine if a review sentence is sentimental .", "entities": []}, {"text": "This is achieved by employing a vanilla LSTM , namely , S - LSTM .", "entities": [[7, 8, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}, {"text": "2.2 Model Description The \ufb01rst component of our framework MIN is composed of A - LSTM and O - LSTM .", "entities": [[15, 16, "MethodName", "LSTM"], [19, 20, "MethodName", "LSTM"]]}, {"text": "Both LSTMs have extended memories for task - level memory interactions .", "entities": []}, {"text": "A - LSTM involves a large aspect memory HA t\u2208Rnm\u00d7dimA hand an opinion summary vector mO t\u2208RdimO", "entities": [[2, 3, "MethodName", "LSTM"]]}, {"text": "hwhereHA tcontainsnmpieces of aspect hidden states of dimensiondimA handmO tis distilled from HO t.", "entities": []}, {"text": "As forO - LSTM , similarly , an opinion memory HO t\u2208 Rnm\u00d7dimO hand an aspect - speci\ufb01c summary vector mA t\u2208RdimA hare included .", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "We use the aspect term annotations in the training data for training A - LSTM .", "entities": [[14, 15, "MethodName", "LSTM"]]}, {"text": "As there is no ground truth available for opinion words in the training data , sentiment lexicon and highprecision dependency rules are introduced to \ufb01nd potential opinion words .", "entities": []}, {"text": "Commonly used opinion words can be found in some general sentiment lexicons .", "entities": []}, {"text": "To \ufb01nd opinion words , not in sentiment lexicons , in a sentence , we build a small rule set R composed of dependency relations with high con\ufb01dence , e.g. , amod , nsubj , and determine if wt directly depends on the gold aspect word through the dependencies in R. If so , wtwill be treated as a potential opinion word .", "entities": []}, {"text": "Then such opinion words are used as training data for O - LSTM .", "entities": [[12, 13, "MethodName", "LSTM"]]}, {"text": "In the memory - enhanced A - LSTM and OLSTM , we manually design three kinds of operations : ( 1 ) READ to selectnmpieces of aspect ( opinion ) hidden states from the past memories and build HA t(HO t ) ; ( 2 ) DIGEST to distill an aspect ( opinion)-speci\ufb01c summary mA t(mO t ) fromHA t(HO t ) where in\ufb02uences of opinion terms and relative positions of inputs are considered ; ( 3 ) INTERACT to perform interaction between ALSTM and O - LSTM using the task speci\ufb01c summaries ( i.e. , mA tandmO t ) .", "entities": [[7, 8, "MethodName", "LSTM"], [87, 88, "MethodName", "LSTM"]]}, {"text": "Consider the work \ufb02ow of A - LSTM for aspect term extraction .", "entities": [[7, 8, "MethodName", "LSTM"], [10, 12, "TaskName", "term extraction"]]}, {"text": "Since opinion words and aspect terms should co - occur , the goal of A - LSTM participating in memory interactions is to acquire opinion summaries from O - LSTM ( i.e. , mO t ) for better aspect prediction .", "entities": [[16, 17, "MethodName", "LSTM"], [29, 30, "MethodName", "LSTM"]]}, {"text": "First of all , MIN will READnmpieces of opinion memories which are most related to wtfrom O - LSTM .", "entities": [[18, 19, "MethodName", "LSTM"]]}, {"text": "Syntax structure could be used but syntactic parsers are not effective for processing short informal review sentences .", "entities": []}, {"text": "Therefore , MIN selects memory segments temporally related to wt .", "entities": []}, {"text": "Precisely , the opinion memory at the time step tisHO t=", "entities": []}, {"text": "[ hO t\u22121; ...", "entities": []}, {"text": ";hO t\u2212nm]wherehO t\u2212iis the ( t\u2212i)-th hidden state from O - LSTM .", "entities": [[12, 13, "MethodName", "LSTM"]]}, {"text": "Since the linear context contains most of the parent nodes and the child nodes ofwton the dependency parse tree , treating the corresponding memory segments as relevant segments to wtis reasonable .", "entities": []}, {"text": "Then MIN will DIGEST the collected opinion memoriesHO tin the A - LSTM .", "entities": [[12, 13, "MethodName", "LSTM"]]}, {"text": "As different memory segments are not of equal importance for the2887", "entities": []}, {"text": "current decision and the same segment in different memories ( i.e. , different HO t ) also makes a difference , MIN leverages two kind of weights to summarize the collected content .", "entities": []}, {"text": "The \ufb01rst weight is the indicator score of being opinion terms denoted asvI\u2208Rnm , which is used to measure how much opinion information the word wt\u2212i ( i= 1, .. ,n m ) holds .", "entities": []}, {"text": "We adopt Euclidean distance between distributed representations of wt\u2212i and opinion words .", "entities": []}, {"text": "It is obvious that computing the distance between xt\u2212iand each opinion word is expensive .", "entities": []}, {"text": "Thus , we run an off - the - shelf clustering algorithm over opinion words in the training set and then use the produced nccentroids to estimate the indicator score vI iofwt\u2212ibeing an opinion word :", "entities": []}, {"text": "vI i = nc / summationdisplay j=11 ||xt\u2212i\u2212cj||2(1 ) wherext\u2212iis the distributed representation of wt\u2212iandcjis the centroid vector representation ofj - th cluster .", "entities": []}, {"text": "This weighting scheme ensures thatwt\u2212iis assigned a high score as long as xt\u2212i is close to a particular centroid .", "entities": []}, {"text": "The aspect decision ofwtis also affected by relative position betweenwt\u2212iandwt .", "entities": []}, {"text": "Thus , MIN employs the second weight vPto explicitly model their positional relevance and the initial weight for the i - th segmentvP iis calculated as below : vP i = nm\u2212i+ 1 / summationtextnm k=1k(2 ) wherenmis the number of hidden state in HO t.", "entities": []}, {"text": "This position - aware weight enables that the closer the wordwt\u2212iis to the current input , the more the corresponding memory segment will contribute to the current decision .", "entities": []}, {"text": "To better capture the local positional relevance , we make the initialized vPas learnable parameters .", "entities": []}, {"text": "Combining the above two weights helps to utilize each active memory segment according to the importance for prediction andmO t , the summary of HO tis generated : mO t= ( HO t)/latticetop(vI\u2299vP ||vI||2 ) ( 3 ) where\u2299denotes element - wise multiplication and", "entities": []}, {"text": "||\u2217|| 2is", "entities": []}, {"text": "Euclidean norm of vectors .", "entities": []}, {"text": "From Equation 3,mO tis dominated by the associated memory segment of wt\u2212ithat obtains the high combined weights .", "entities": []}, {"text": "In the last operation INTERACT , A - LSTM communicates with O - LSTM by acquiring mO t from O - LSTM and incorporating the summary into the memory update .", "entities": [[8, 9, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"], [21, 22, "MethodName", "LSTM"]]}, {"text": "The update process is as follows : iA t = \u03c3(WA ixt+UA i[HA t[1 ] : mO t ] )", "entities": []}, {"text": "+ bA i ) fA t = \u03c3(WA fxt+UA f[HA t[1 ] : mO t ] )", "entities": []}, {"text": "+ bA f ) \u02c6cA t = tanh(WA cxt+UA c[HA t[1 ] : mO t ] )", "entities": []}, {"text": "+ bA c ) oA t = \u03c3(WA oxt+UA o[HA t[1 ] : mO t ] )", "entities": []}, {"text": "+ bA o ) cA t = iA t\u2299\u02c6cA t+cA t\u22121\u2299fA t hA t = tanh(cA t)\u2299oA t(4 )", "entities": []}, {"text": "whereWA \u2217,UA \u2217andbA \u2217are weight parameters of the A - LSTM and \u03c3is the sigmoid activation function .", "entities": [[10, 11, "MethodName", "LSTM"], [14, 16, "MethodName", "sigmoid activation"]]}, {"text": "[: ] denotes vector concatenation operation.mO tcan be seen as the summary of the opinion indicator in the left context of wtand HA t[1]is the most immediate hidden memory of A - LSTM .", "entities": [[32, 33, "MethodName", "LSTM"]]}, {"text": "MIN blends the opinion summary from O - LSTM with the memory from A - LSTM .", "entities": [[8, 9, "MethodName", "LSTM"], [15, 16, "MethodName", "LSTM"]]}, {"text": "The co - occurrence relation between aspects and opinion words is modeled by such \u201c memory fusion \u201d strategy .", "entities": []}, {"text": "Since opinion words can appear on both sides ofwt , memory segments corresponding to the right context ( i.e. , \u201c future \u201d memory ) should be included .", "entities": []}, {"text": "Hence , we conduct bi - directional training for A - LSTM .", "entities": [[11, 12, "MethodName", "LSTM"]]}, {"text": "The work \ufb02ow of memory interaction and the update process of the internal memories in OLSTM are kept same with those in A - LSTM except the DIGEST operation .", "entities": [[24, 25, "MethodName", "LSTM"]]}, {"text": "Speci\ufb01cally , we set mA t , the task - speci\ufb01c summary of A - LSTM , as hA t.", "entities": [[15, 16, "MethodName", "LSTM"]]}, {"text": "The second component of MIN is a generic LSTM called S - LSTM for discriminating sentimental sentences and non - sentimental sentences .", "entities": [[8, 9, "MethodName", "LSTM"], [12, 13, "MethodName", "LSTM"]]}, {"text": "The design and the process of the memory update in this component are similar to that in Jozefowicz et al .", "entities": []}, {"text": "( 2015 )", "entities": []}, {"text": ".", "entities": []}, {"text": "In sentences not conveying any sentimental meanings , some words like food , service tend to be misclassi\ufb01ed as aspect terms since they are commonly used in user reviews .", "entities": []}, {"text": "To avoid this kind of error , we add a constraint that an aspect term should come from sentimental sentence .", "entities": []}, {"text": "Speci\ufb01cally , S - LSTM learns the sentimental representationhS", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "Tof the sentence", "entities": []}, {"text": "and then feeds it in aspect prediction as a soft constraint : P(yA t|xt )", "entities": []}, {"text": "= softmax ( WA fc([hA t : hS T ] ) ) ( 5 ) whereWA fcdenotes the weight matrix of the fullyconnected softmax layer.2888", "entities": [[1, 2, "MethodName", "softmax"], [23, 24, "MethodName", "softmax"]]}, {"text": "On the whole , our proposed MIN framework has three LSTMs and each of them is differentiable .", "entities": []}, {"text": "Thus , our MIN framework can be ef\ufb01ciently trained with gradient descent .", "entities": []}, {"text": "For ALSTM and O - LSTM , we use the token - level cross - entropy error between the predicted distributionP(yT t|xt)and the gold standard distribution P(yT , g t|xt)as the loss function ( T \u2208{A , O } ):", "entities": [[5, 6, "MethodName", "LSTM"], [31, 32, "MetricName", "loss"]]}, {"text": "Loss ( T ) = \u22121 N\u2217TN / summationdisplay i=1T / summationdisplay t=1P(YT , g i , t|Xi , t)\u2299 log[P(YT", "entities": []}, {"text": "i , t|Xi , t)](6 )", "entities": []}, {"text": "For S - LSTM , sentence - level cross entropy error are employed to calculate the corresponding loss : Loss ( S ) = \u22121 NN / summationdisplay i=1P(YS , g i|Xi)\u2299log[P(YS i|Xi ) ] ( 7 ) Then , losses from different LSTMs are combined to form the training objective of the MIN framework : J(\u03b8 )", "entities": [[3, 4, "MethodName", "LSTM"], [17, 18, "MetricName", "loss"]]}, {"text": "= Loss ( A ) + Loss ( O )", "entities": []}, {"text": "+ Loss ( S)(8 ) .", "entities": []}, {"text": "# TRAIN/#TEST Sentences # TRAIN/#TEST Aspects D1 3045/800 2358/654 D2 2000/676 1743/622 Table 1 : Statistics of datasets .", "entities": []}, {"text": "3 Experiment 3.1 Dataset We conduct experiments on two benchmark datasets from SemEval ABSA challenge ( Pontiki et al . , 2014 , 2016 ) as shown in Table 1 . D1(SemEval 2014 ) contains reviews from the laptop domain andD2(SemEval 2016 ) contains reviews from the restaurant domain .", "entities": []}, {"text": "In these datasets , aspect terms have been labeled and sentences containing at least one golden truth aspect are regarded as sentimental sentences .", "entities": []}, {"text": "As gold standard annotations for opinion words are not provided , we select words with strong subjectivity from MPQA1as potential opinion words .", "entities": []}, {"text": "Apart from the common opinion words in the sentiment lexicon , we also treat words , which directly depend on gold standard aspect terms through highprecision dependency rules , as opinion words .", "entities": []}, {"text": "1http://mpqa.cs.pitt.edu/3.2 Experiment Design To evaluate the proposed MIN framework , we perform comparison with the following two groups of methods : ( 1)CRF based methods : \u2022CRF : Conditional Random Fields with basic feature templates2and word embeddings .", "entities": [[35, 37, "TaskName", "word embeddings"]]}, {"text": "\u2022Semi - CRF : First - order semi - Markov conditional random \ufb01elds ( Sarawagi et al . , 2004 ) and the feature template in Cuong et al .", "entities": [[2, 3, "MethodName", "CRF"]]}, {"text": "( 2014 ) is adopted .", "entities": []}, {"text": "\u2022IHS RD ( Chernyshevich , 2014 ) , NLANGP ( Toh and Su , 2016 ): Best systems in ATE subtask in SemEval ABSA challenges ( Pontiki et al . , 2014 , 2016 ) .", "entities": []}, {"text": "\u2022DLIREC ( Toh and Wang , 2014 ) , AUEB ( Xenos et al . , 2016 ): Top - ranked CRF - based systems in ATE subtask in SemEval ABSA challenges ( Pontiki et al . , 2014 , 2016 ) .", "entities": [[21, 22, "MethodName", "CRF"]]}, {"text": "\u2022WDEmb ( Yin et al . , 2016 ): Enhanced CRF with word embeddings , linear context embeddings and dependency path embeddings .", "entities": [[10, 11, "MethodName", "CRF"], [12, 14, "TaskName", "word embeddings"]]}, {"text": "( 2)Neural Network based methods \u2022LSTM : Vanilla bi - directional LSTM with pre - trained word embeddings3 .", "entities": [[11, 12, "MethodName", "LSTM"]]}, {"text": "\u2022RNCRF ( Wang et al . , 2016 ): Dependency Tree based Recursive Neural Network with CRF extractor4 .", "entities": [[16, 17, "MethodName", "CRF"]]}, {"text": "For datasets in the restaurant domain , we train word embeddings of dimension 200 with word2vec ( Mikolov et al . , 2013 ) on Yelp reviews5 .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "For those in laptop domain , we use pre - trained glove .", "entities": []}, {"text": "840B.300d6 .", "entities": []}, {"text": "2http://sklearn-crfsuite.readthedocs.io/en/latest/ 3As we use our own implementation of LSTM , the reported results are different from those in ( Liu et al . , 2015 ) 4Speci\ufb01cally , we list the result of RNCRF over D1without opinion annotations for fair comparison .", "entities": [[8, 9, "MethodName", "LSTM"]]}, {"text": "As no result is provided for RNCRF - no - opinion over D2 , we report the corresponding performance of the full model .", "entities": []}, {"text": "See their following works ( Wang et al . , 2017a , b ) .", "entities": []}, {"text": "Also , CMLA ( Wang et al . , 2017a ) reports better results than RNCRF but we do not compare with it .", "entities": []}, {"text": "The reason is that CMLA introduces the gold standard opinion labels in the training data while such labels are not available for our experiments 5https://www.yelp.com/dataset challenge 6https://nlp.stanford.edu/projects/glove/2889", "entities": []}, {"text": "D1D2 CRF 74.01 % 69.56 % Semi - CRF 68.75 % 66.35 % IHS RD 74.55 % DLIREC 73.78 % NLANGP - 72.34 % AUEB - 70.44 % WDEmb 75.16 % LSTM 75.25 % 71.26 % RNCRF 77.26 % 69.74 % Our Work 77.58 % 73.44 % Table 2 : Experiment results The hyper - parameters are selected via ten - fold cross validation .", "entities": [[1, 2, "MethodName", "CRF"], [8, 9, "MethodName", "CRF"], [31, 32, "MethodName", "LSTM"]]}, {"text": "The dimension of hidden representations are 100 , 20 , 40 for A - LSTM , O - LSTM and S - LSTM respectively .", "entities": [[14, 15, "MethodName", "LSTM"], [18, 19, "MethodName", "LSTM"], [22, 23, "MethodName", "LSTM"]]}, {"text": "The dropout rate for O - LSTM and S - LSTM is 0.4 .", "entities": [[6, 7, "MethodName", "LSTM"], [10, 11, "MethodName", "LSTM"]]}, {"text": "The size of the aspect ( opinion ) memory nmis 4 .", "entities": []}, {"text": "The batch size is set to 32 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "As for initialization of network parameters , we adopt the strategy that the initial weights are sampled from the uniform distribution ( Glorot and Bengio , 2010 ) .", "entities": []}, {"text": "We employ ADAM ( Kingma and Ba , 2014 ) as optimizer and the default settings of ADAM are used .", "entities": [[2, 3, "DatasetName", "ADAM"], [11, 12, "HyperparameterName", "optimizer"], [17, 18, "DatasetName", "ADAM"]]}, {"text": "To better reveal the capability of the proposed MIN , we train 5 models with the same group of hyper - parameters and report the average F1score over the testing set .", "entities": []}, {"text": "3.3 Results and Analysis Table 2 depicts experiment results .", "entities": []}, {"text": "Compared to the best systems in SemEval challenge , MIN achieves 3.0 % and 1.1 % absolute gains on D1and D2respectively .", "entities": []}, {"text": "Besides , our MIN outperforms WDEmb , a strong CRF - based system bene\ufb01ting from several kinds of useful word embeddings , by 2.1 % on D1 .", "entities": [[9, 10, "MethodName", "CRF"], [19, 21, "TaskName", "word embeddings"]]}, {"text": "With memory interactions and consideration of sentimental sentence , our MIN boosts the performance of vanilla bi - directional LSTM ( +2.0 % and +1.7 % respectively ) .", "entities": [[19, 20, "MethodName", "LSTM"]]}, {"text": "It validates the effectiveness of the manually designed memory operations and the proposed memory interaction mechanism .", "entities": []}, {"text": "MIN also outperforms the state - of - the - art RNCRF on each dataset suggesting that memory interactions can be an alternative strategy instead of syntactic parsing .", "entities": []}, {"text": "To further study the impact of each element in MIN , we conduct ablation experiments .", "entities": []}, {"text": "As shown in Table 3 , removing bi - directionality decreases the extraction performances ( -2.0 % and -1.0 % ) .", "entities": []}, {"text": "The softsentimental constraint proves to be useful since MIN is 1.5 % and 1.0 % superior than the framework without S - LSTM on D1andD2respectively .", "entities": [[22, 23, "MethodName", "LSTM"]]}, {"text": "O - LSTM brings in the largest performance gains onD2compared with ablated framework ( i.e. , MIN without O - LSTM ) , verifying our postulation that aspect - opinion \u201c interaction \u201d is more effective than only considering aspect terms .", "entities": [[2, 3, "MethodName", "LSTM"], [20, 21, "MethodName", "LSTM"]]}, {"text": "We also observe that the contribution of O - LSTM is less signi\ufb01cant than that of bi - directionality on D1 ( +1.6 % vs +2.0 % ) .", "entities": [[9, 10, "MethodName", "LSTM"]]}, {"text": "This is reasonable since using opinion words as adjective modi\ufb01ers placed after the aspects is common in English .", "entities": []}, {"text": "D1D2 MIN without bi - directionality 75.59 % 71.87 % MIN without S - LSTM 76.04 % 72.55 % MIN without O - LSTM 75.97 % 71.80 % MIN 77.58 % 73.44 % Table 3 : Ablation experiment results .", "entities": [[14, 15, "MethodName", "LSTM"], [23, 24, "MethodName", "LSTM"]]}, {"text": "4 Conclusions We propose Memory Interaction Network ( MIN ) , a multi - task learning framework , to detect aspect terms from the online user reviews .", "entities": [[12, 16, "TaskName", "multi - task learning"]]}, {"text": "Compared with previous studies , our MIN has following features : \u2022Co - occurrence pattern between aspects and opinions is captured via memory interactions , where the neural memory operations are designed to summarize task - level information and perform interactions .", "entities": []}, {"text": "\u2022A novel LSTM unit with extended memories is developed for memory interactions .", "entities": [[2, 3, "MethodName", "LSTM"]]}, {"text": "References Maryna Chernyshevich .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Ihs r&d belarus : Crossdomain extraction of product features using crf .", "entities": [[10, 11, "MethodName", "crf"]]}, {"text": "In Proceedings of the 8th International Workshop on Semantic Evaluation ( SemEval 2014 ) , pages 309 \u2013 313 .", "entities": []}, {"text": "Nguyen Viet Cuong , Nan Ye , Wee Sun Lee , and Hai Leong Chieu .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Conditional random \ufb01eld with high - order dependencies for sequence labeling and segmentation .", "entities": []}, {"text": "Journal of Machine Learning Research , 15(1):981\u20131009 .", "entities": []}, {"text": "Xavier Glorot and Yoshua Bengio .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Understanding the dif\ufb01culty of training deep feedforward neural networks .", "entities": []}, {"text": "In Proceedings of AISTATS , pages 249 \u2013 256.2890", "entities": []}, {"text": "Minqing Hu and Bing Liu . 2004a .", "entities": []}, {"text": "Mining and summarizing customer reviews .", "entities": []}, {"text": "In Proceedings of KDD , pages 168\u2013177 .", "entities": []}, {"text": "Minqing Hu and Bing Liu . 2004b .", "entities": []}, {"text": "Mining opinion features in customer reviews .", "entities": []}, {"text": "In Proceedings of AAAI , pages 755\u2013760 .", "entities": []}, {"text": "Niklas Jakob and Iryna Gurevych . 2010 .", "entities": []}, {"text": "Extracting opinion targets in a single and cross - domain setting with conditional random \ufb01elds .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 1035\u20131045 .", "entities": []}, {"text": "Wei Jin , Hung Hay Ho , and Rohini K Srihari .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "A novel lexicalized hmm - based learning framework for web opinion mining .", "entities": [[10, 12, "TaskName", "opinion mining"]]}, {"text": "In Proceedings of ICML , pages 465\u2013472 .", "entities": []}, {"text": "Rafal Jozefowicz , Wojciech Zaremba , and Ilya Sutskever . 2015 .", "entities": []}, {"text": "An empirical exploration of recurrent network architectures .", "entities": []}, {"text": "In Proceedings of ICML , pages 2342\u20132350 .", "entities": []}, {"text": "Diederik Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In Proceedings of ICLR .", "entities": []}, {"text": "Bing Liu .", "entities": []}, {"text": "2012a .", "entities": []}, {"text": "Sentiment analysis and opinion mining .", "entities": [[0, 2, "TaskName", "Sentiment analysis"], [3, 5, "TaskName", "opinion mining"]]}, {"text": "Synthesis lectures on human language technologies , 5(1):1\u2013167 .", "entities": []}, {"text": "Kang Liu , Heng Li Xu , Yang Liu , and Jun Zhao .", "entities": []}, {"text": "2013a .", "entities": []}, {"text": "Opinion target extraction using partially - supervised word alignment model .", "entities": [[7, 9, "TaskName", "word alignment"]]}, {"text": "In Proceedings of IJCAI , pages 2134\u20132140 .", "entities": []}, {"text": "Kang Liu , Liheng Xu , and Jun Zhao .", "entities": []}, {"text": "2012b .", "entities": []}, {"text": "Opinion target extraction using word - based translation model .", "entities": []}, {"text": "In Proceedings of EMNLP / CoNLL , pages 1346\u20131356 .", "entities": []}, {"text": "Kang Liu , Liheng Xu , and Jun Zhao .", "entities": []}, {"text": "2013b .", "entities": []}, {"text": "Syntactic patterns versus word alignment : Extracting opinion targets from online reviews .", "entities": [[3, 5, "TaskName", "word alignment"]]}, {"text": "In Proceedings of ACL , pages 1754\u20131763 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kang Liu , Liheng Xu , and Jun Zhao .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Extracting opinion targets and opinion words from online reviews with graph co - ranking .", "entities": []}, {"text": "In Proceedings of ACL , pages 314\u2013324 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Pengfei Liu , Sha\ufb01q Joty , and Helen Meng . 2015 .", "entities": [[7, 8, "DatasetName", "Helen"]]}, {"text": "Finegrained opinion mining with recurrent neural networks and word embeddings .", "entities": [[1, 3, "TaskName", "opinion mining"], [8, 10, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of EMNLP , pages 1433\u20131443 .", "entities": []}, {"text": "Asha S Manek , P Deepa Shenoy , M Chandra Mohan , and KR Venugopal . 2017 .", "entities": []}, {"text": "Aspect term extraction for sentiment analysis in large movie reviews using gini index feature selection method and svm classi\ufb01er .", "entities": [[1, 3, "TaskName", "term extraction"], [4, 6, "TaskName", "sentiment analysis"], [13, 15, "MethodName", "feature selection"], [17, 18, "MethodName", "svm"]]}, {"text": "World Wide Web Journal , 20(2):135\u2013154.Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , and Jeff Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In Proceedings of NIPS , pages 3111\u20133119 .", "entities": []}, {"text": "Maria Pontiki , Dimitris Galanis , Haris Papageorgiou , Ion Androutsopoulos , Suresh Manandhar , Mohammad AL - Smadi , Mahmoud Al - Ayyoub , Yanyan Zhao , Bing Qin , Orphee De Clercq , Veronique Hoste , Marianna Apidianaki , Xavier Tannier , Natalia Loukachevitch , Evgeniy Kotelnikov , N \u00b4 uria", "entities": []}, {"text": "Bel , Salud Mar \u00b4 \u0131a Jim \u00b4 enez - Zafra , and G \u00a8uls \u00b8en Eryi \u02d8git .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semeval-2016 task 5 : Aspect based sentiment analysis .", "entities": [[6, 8, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation ( SemEval-2016 ) , pages 19\u201330 .", "entities": []}, {"text": "Maria Pontiki , Dimitris Galanis , John Pavlopoulos , Harris Papageorgiou , Ion Androutsopoulos , and Suresh Manandhar .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Semeval-2014 task 4 : Aspect based sentiment analysis .", "entities": [[6, 8, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 8th International Workshop on Semantic Evaluation ( SemEval 2014 ) , pages 27\u201335 .", "entities": []}, {"text": "Ana - Maria Popescu and Oren Etzioni .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Extracting product features and opinions from reviews .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 339\u2013346 .", "entities": []}, {"text": "Soujanya Poria , Erik Cambria , and Alexander Gelbukh . 2016 .", "entities": []}, {"text": "Aspect extraction for opinion mining with a deep convolutional neural network .", "entities": [[0, 2, "TaskName", "Aspect extraction"], [3, 5, "TaskName", "opinion mining"]]}, {"text": "Knowledge - Based Systems , 108:42\u201349 .", "entities": []}, {"text": "Guang Qiu , Bing Liu , Jiajun Bu , and Chun Chen .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Opinion word expansion and target extraction through double propagation .", "entities": []}, {"text": "Computational Linguistics , 37(1):9\u201327 .", "entities": []}, {"text": "I\u02dcnaki", "entities": []}, {"text": "San Vicente , Xabier Saralegi , and Rodrigo Agerri .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Elixa : A modular and \ufb02exible absa platform .", "entities": []}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 748\u2013752 .", "entities": []}, {"text": "Sunita Sarawagi , William W Cohen , et al . 2004 .", "entities": []}, {"text": "Semimarkov conditional random \ufb01elds for information extraction .", "entities": []}, {"text": "In Proceedings of NIPS , pages 1185 \u2013 1192 .", "entities": []}, {"text": "Zhiqiang Toh and Jian Su . 2016 .", "entities": []}, {"text": "Nlangp at semeval2016 task 5 : Improving aspect based sentiment analysis using neural network features .", "entities": [[9, 11, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation ( SemEval-2016 ) , pages 282\u2013288 .", "entities": []}, {"text": "Zhiqiang Toh and Wenting Wang .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Dlirec : Aspect term extraction and term polarity classi\ufb01cation system .", "entities": [[3, 5, "TaskName", "term extraction"]]}, {"text": "In Proceedings of the 8th International Workshop on Semantic Evaluation ( SemEval 2014 ) , pages 235\u2013240 .", "entities": []}, {"text": "Wenya Wang , Sinno Jialin Pan , and Daniel Dahlmeier . 2017b .", "entities": []}, {"text": "Multi - task coupled attentions for categoryspeci\ufb01c aspect and opinion terms co", "entities": []}, {"text": "-", "entities": []}, {"text": "extraction .", "entities": []}, {"text": "arXiv preprint arXiv:1702.01776 .2891", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Wenya Wang , Sinno Jialin Pan , Daniel Dahlmeier , and Xiaokui Xiao . 2016 .", "entities": []}, {"text": "Recursive neural conditional random \ufb01elds for aspect - based sentiment analysis .", "entities": [[6, 11, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "InProceedings of EMNLP , pages 616\u2013626 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Wenya Wang , Sinno Jialin Pan , Daniel Dahlmeier , and Xiaokui Xiao . 2017a .", "entities": []}, {"text": "Coupled multi - layer attentions for co - extraction of aspect and opinion terms .", "entities": []}, {"text": "InProceedings of AAAI , pages 3316\u20133322 .", "entities": []}, {"text": "Dionysios Xenos , Panagiotis Theodorakakos , John Pavlopoulos , Prodromos Malakasiotis , and Ion Androutsopoulos . 2016 .", "entities": []}, {"text": "Aueb - absa at semeval-2016 task 5 : Ensembles of classi\ufb01ers and embeddings for aspect based sentiment analysis .", "entities": [[16, 18, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation ( SemEval-2016 ) , pages 312\u2013317 .", "entities": []}, {"text": "Yichun Yin , Furu Wei , Li Dong , Kaimeng Xu , Ming Zhang , and Ming Zhou .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Unsupervised word and dependency path embeddings for aspect term extraction .", "entities": [[8, 10, "TaskName", "term extraction"]]}, {"text": "In Proceedings of IJCAI , pages 2979 \u2013 2985 .", "entities": []}, {"text": "Li Zhuang , Feng Jing , and Xiao - Yan Zhu . 2006 .", "entities": []}, {"text": "Movie review mining and summarization .", "entities": [[4, 5, "TaskName", "summarization"]]}, {"text": "In Proceedings of CIKM , pages 43\u201350.2892", "entities": []}]