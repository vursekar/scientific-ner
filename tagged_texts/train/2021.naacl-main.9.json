[{"text": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 94\u2013105 June 6\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics94Automatic Generation of Contrast Sets from Scene Graphs : Probing the Compositional Consistency of GQA Yonatan Bitton}Gabriel Stanovsky}Roy Schwartz}Michael Elhadad\u007f } School of Computer Science and Engineering , The Hebrew University of Jerusalem , Jerusalem , Israel \u007fDepartment of Computer Science , Ben Gurion University , Beer Sheva , Israel { yonatanbitton,gabis,roys}@cs.huji.ac.il", "entities": [[19, 20, "DatasetName", "GQA"]]}, {"text": "elhadad@cs.bgu.ac.il Abstract Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution .", "entities": []}, {"text": "Contrast sets ( Gardner et al . , 2020 ) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modi\ufb01ed .", "entities": []}, {"text": "While most contrast sets were created manually , requiring intensive annotation effort , we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task .", "entities": [[31, 34, "DatasetName", "visual question answering"]]}, {"text": "Our method computes the answer of perturbed questions , thus vastly reducing annotation cost and enabling thorough evaluation of models \u2019 performance on various semantic aspects ( e.g. , spatial or relational reasoning ) .", "entities": [[31, 33, "TaskName", "relational reasoning"]]}, {"text": "We demonstrate the effectiveness of our approach on the popular GQA dataset ( Hudson and Manning , 2019 ) and its semantic scene graph image representation .", "entities": [[10, 11, "DatasetName", "GQA"]]}, {"text": "We \ufb01nd that , despite GQA \u2019s compositionality and carefully balanced label distribution , two strong models drop 13\u201317 % in accuracy on our automatically - constructed contrast set compared to the original validation set .", "entities": [[5, 6, "DatasetName", "GQA"], [21, 22, "MetricName", "accuracy"]]}, {"text": "Finally , we show that our method can be applied to the training set to mitigate the degradation in performance , opening the door to more robust models.1 1 Introduction NLP benchmarks typically evaluate in - distribution generalization , where test sets are drawn i.i.dfrom a distribution similar to the training set .", "entities": []}, {"text": "Recent works showed that high performance on test sets sampled in this manner is often achieved by exploiting systematic gaps , annotation artifacts , lexical cues and other heuristics , rather than learning meaningful task - related signal .", "entities": []}, {"text": "As a result , 1Our contrast sets and code are available at https://github.com/yonatanbitton/ AutoGenOfContrastSetsFromSceneGraphs .", "entities": []}, {"text": "Figure 1 : Illustration of our approach based on an example from the GQA dataset .", "entities": [[13, 14, "DatasetName", "GQA"]]}, {"text": "Top : QA pairs and an image annotated with bounding boxes from the scene graph .", "entities": []}, {"text": "Bottom : relations among the objects in the scene graph .", "entities": []}, {"text": "First line at the top is the original QA pair , while the following 3 lines show our pertubated questions : replacing a single element in the question ( a fence ) with other options ( a wall , men , an elephant ) , leading to a change in the output label .", "entities": []}, {"text": "For each QA pair , the LXMERT predicted output is shown .", "entities": [[6, 7, "MethodName", "LXMERT"]]}, {"text": "the out - of - domain performance of these models is often severely deteriorated ( Jia and Liang , 2017 ; Ribeiro et", "entities": []}, {"text": "al . , 2018 ; Gururangan et al . , 2018 ; Geva et al . , 2019 ; McCoy et al . , 2019 ; Feng et al . , 2019 ; Stanovsky et al . , 2019 ) .", "entities": []}, {"text": "Recently , Kaushik et al .", "entities": []}, {"text": "( 2019 ) and Gardner et al .", "entities": []}, {"text": "( 2020 ) introduced the contrast sets approach to probe out - of - domain generalization .", "entities": [[14, 16, "TaskName", "domain generalization"]]}, {"text": "Contrast sets are constructed via minimal modi\ufb01cations to test inputs , such that their label is modi\ufb01ed .", "entities": []}, {"text": "For example , in Fig .", "entities": []}, {"text": "1 , replacing \u201c a fence \u201d with \u201c a wall \u201d , changes the answer", "entities": []}, {"text": "95from \u201c Yes \u201d to \u201c No \u201d .", "entities": []}, {"text": "Since such perturbations introduce minimal additional semantic complexity , robust models are expected to perform similarly on the test and contrast sets .", "entities": []}, {"text": "However , a range of NLP models severely degrade in performance on contrast sets , hinting that they do not generalize well ( Gardner et al . , 2020 ) .", "entities": []}, {"text": "Except two recent exceptions for textual datasets ( Li et al . , 2020 ; Rosenman et al . , 2020 ) , contrast sets have so far been built manually , requiring extensive human effort and expertise .", "entities": []}, {"text": "In this work , we propose a method for automatic generation of large contrast sets for visual question answering ( VQA ) .", "entities": [[16, 19, "DatasetName", "visual question answering"], [20, 21, "TaskName", "VQA"]]}, {"text": "We experiment with the GQA dataset ( Hudson and Manning , 2019 ) .", "entities": [[4, 5, "DatasetName", "GQA"]]}, {"text": "GQA includes semantic scene graphs ( Krishna et al . , 2017 ) representing the spatial relations between objects in the image , as exempli\ufb01ed in Fig .", "entities": [[0, 1, "DatasetName", "GQA"]]}, {"text": "1 .", "entities": []}, {"text": "The scene graphs , along with functional programs that represent the questions , are used to balance the dataset , thus aiming to mitigate spurious dataset correlations .", "entities": []}, {"text": "We leverage the GQA scene graphs to create contrast sets , by automatically computing the answers to question perturbations , e.g. , verifying that there is no wall near the puddle in Fig .", "entities": [[3, 4, "DatasetName", "GQA"]]}, {"text": "1 .", "entities": []}, {"text": "We create automatic contrast sets for 29 K samples or\u001922 % of the validation set .", "entities": []}, {"text": "We manually verify the correctness of 1,106 of these samples on Mechanical Turk .", "entities": []}, {"text": "Following , we evaluate two leading models , LXMERT ( Tan and Bansal , 2019 ) and MAC ( Hudson and Manning , 2019 ) on our contrast sets , and \ufb01nd a 13\u201317 % reduction in performance compared to the original validation set .", "entities": [[8, 9, "MethodName", "LXMERT"]]}, {"text": "Finally , we show that our automatic method for contrast set construction can be used to improve performance by employing it during training .", "entities": []}, {"text": "We augment the GQA training set with automatically constructed training contrast sets ( adding 80 K samples to the existing 943 K in GQA ) , and observe that when trained with it , both LXMERT and MAC improve by about 14 % on the contrast sets , while maintaining their original validation performance .", "entities": [[3, 4, "DatasetName", "GQA"], [23, 24, "DatasetName", "GQA"], [35, 36, "MethodName", "LXMERT"]]}, {"text": "Our key contributions are : ( 1 ) We present an automatic method for creating contrast sets for VQA datasets with structured input representations ; ( 2 ) We automatically create contrast sets for GQA , and \ufb01nd that for two strong models , performance on the contrast sets is lower than on the original validation set ; and ( 3 ) We apply our method to augment the training data , improving both models \u2019 performance on the contrast sets.2 Automatic Contrast Set Construction To construct automatic contrast sets for GQA we \ufb01rst identify a large subset of questions requiring speci\ufb01c reasoning skills ( \u00a7 2.1 ) .", "entities": [[18, 19, "TaskName", "VQA"], [34, 35, "DatasetName", "GQA"], [91, 92, "DatasetName", "GQA"]]}, {"text": "Using the scene graph representation , we perturb each question in a manner which changes its gold answer ( \u00a7 2.2 ) .", "entities": []}, {"text": "Finally , we validate the automatic process via crowdsourcing ( \u00a7 2.3 ) .", "entities": []}, {"text": "2.1 Identifying Recurring Patterns in GQA The questions in the GQA dataset present a diverse set of modelling challenges , as exempli\ufb01ed in Table 1 , including object identi\ufb01cation and grounding , spatial reasoning and color identi\ufb01cation .", "entities": [[5, 6, "DatasetName", "GQA"], [10, 11, "DatasetName", "GQA"]]}, {"text": "Following the contrast set approach , we create perturbations testing whether models are capable of solving questions which require this skill set , but that diverge from their training distribution .", "entities": []}, {"text": "To achieve this , we identify commonly recurring question templates which speci\ufb01cally require such skills .", "entities": []}, {"text": "For example , to answer the question \u201c Are there any cats near the boat ? \u201d", "entities": []}, {"text": "a model needs to identify objects in the image ( cats , boat ) , link them to the question , and identify their relative position .", "entities": []}, {"text": "We identify six question templates , testing various skills ( Table 1 ) .", "entities": []}, {"text": "We abstract each question template with a regular expression which identi\ufb01es the question types as well as the physical objects , their attributes ( e.g. , colors ) , and spatial relations .", "entities": []}, {"text": "Overall , these regular expressions match 29 K questions in the validation set ( \u001922 % ) , and 80 K questions in the training set ( \u00198 % ) .", "entities": []}, {"text": "2.2 Perturbing Questions with Scene Graphs We design a perturbation method which guarantees a change in the gold answer for each question template .", "entities": []}, {"text": "For example , looking at Fig .", "entities": []}, {"text": "2 , for the question template are there Xnear the Y ?", "entities": []}, {"text": "( e.g. , \u201c Is there any fence near the players ? \u201d ) , we replace either X or Y with a probable distractor ( e.g. \u201e replace \u201c fence \u201d with \u201c trees \u201d ) .", "entities": []}, {"text": "We use the scene graph to ensure that the answer to the question is indeed changed .", "entities": []}, {"text": "In our example , this would entail grounding \u201c players \u201d in the question to the scene graph ( either via exact match or several other heuristics such as hard - coded lists of synonyms or co - hyponyms ) , locating its neighbors , and verifying that none of them are \u201c trees . \u201d", "entities": [[21, 23, "MetricName", "exact match"]]}, {"text": "We then apply heuristics to \ufb01x syntax ( e.g. , changing from singular to plural determiner , see Appendix A.3 ) , and verify that the perturbed sample", "entities": []}, {"text": "96Question template Tested attributes Example On which side is the X ? Relational ( left vs. right )", "entities": []}, {"text": "On which side is the dishwasher ? !", "entities": []}, {"text": "On which side are the dishes ?", "entities": []}, {"text": "What color is the X ?", "entities": []}, {"text": "Color identi\ufb01cation What color is the cat?!What color is the jacket ?", "entities": []}, {"text": "Do you see XorY ?", "entities": []}, {"text": "Compositions Do you see laptops or cameras?!Do you see headphones or cameras ?", "entities": []}, {"text": "Are there Xnear the Y ?", "entities": []}, {"text": "Spatial , relationalAre there any catsnear the boat?!Isthere any bush near the boat ?", "entities": []}, {"text": "Is the XReltheY ?", "entities": []}, {"text": "Is the boy to the right of the man?!Is the boy to the leftof the man ?", "entities": []}, {"text": "Is the XReltheY ?", "entities": []}, {"text": "Is the boyto the right of the man ? !", "entities": []}, {"text": "Is the zebra to the right of the man ?", "entities": []}, {"text": "Table 1 : Question templates with original question examples , and generated perturbations modifying the answer .", "entities": []}, {"text": "Italic text indicates variables , bold text indicates the perturbed atoms .", "entities": []}, {"text": "does not already exist in GQA .", "entities": [[5, 6, "DatasetName", "GQA"]]}, {"text": "The speci\ufb01c perturbation is performed per question template .", "entities": []}, {"text": "In question templates with two objects ( XandY ) , we replace Xwith X \u2019 , such that X\u2019is correlated with Yin other GQA scene graphs .", "entities": [[23, 24, "DatasetName", "GQA"]]}, {"text": "In question templates with a single object X , we replace Xwith a textually - similar X \u2019 .", "entities": []}, {"text": "For example in the \ufb01rst row in Table 1 we replace dishwasher with dishes .", "entities": []}, {"text": "Our perturbation code is publicly available .", "entities": []}, {"text": "This process may yield an arbitrarily large number of contrasting samples per question , as there are many candidates for replacing objects participating in questions .", "entities": []}, {"text": "We report experiments with up to 1 , 3 and 5 contrasting samples per question .", "entities": []}, {"text": "Illustrating the perturbation process .", "entities": []}, {"text": "Looking at Fig .", "entities": []}, {"text": "1 , we see the scene - graph information : objects have bounding - boxes around them in the image ( e.g. , zebra ) ; Objects have attributes ( wood is an attribute of the fence object ) ; and there are relationships between the objects ( the puddle is to theright of the zebra , and it is near the fence ) .", "entities": []}, {"text": "The original ( question , answer ) pair is ( \u201c is there a fence near the puddle ? \u201d , \u201c Yes \u201d ) .", "entities": []}, {"text": "We \ufb01rst identify the question template by regular expressions : \u201c Is there X near the Y \u201d , and isolate X= fence ,", "entities": []}, {"text": "Y = puddle .", "entities": []}, {"text": "The answer is \u201c Yes \u201d , so we know that X is indeed near Y .", "entities": []}, {"text": "We then use the existing information given in the scene - graph .", "entities": []}, {"text": "We search for X \u2019 that is not near Y .", "entities": []}, {"text": "To achieve this , we sample a random object ( wall ) , and verify that it does n\u2019t exist in the set of scenegraph objects .", "entities": []}, {"text": "This results in a perturbed example \u201c Is there a wall near the puddle ? \u201d , and now the ground truth is computed to be \u201c No \u201d .", "entities": []}, {"text": "Consider a different example : ( \u201c Is the puddle to the left of the zebra ? \u201d , \u201c Yes \u201d ) .", "entities": []}, {"text": "We identify the question template \u201c Is the X Relthe Y \u201d , where X= puddle , Rel= to the left , Y = zebra .", "entities": []}, {"text": "The answer is \u201c Yes \u201d .", "entities": []}, {"text": "Now we can easily change Rel\u2019= to the right , resulting in the ( question , answer ) pair ( \u201c Is the puddle to the rightof the zebra ? \u201d , \u201c No \u201d ) .", "entities": []}, {"text": "We highlight the following : ( 1 ) This process is done entirely automatically ( we validate it in Section 2.3 ) ; ( 2 ) The answer is deterministic given the information in the scene - graph ; ( 3 ) We do not produce unanswerable questions .", "entities": []}, {"text": "If we could n\u2019t \ufb01nd an alternative atom for which the presuppositions hold , we do not create the perturbed ( question , answer ) pair ; ( 4 ) Grounding objects from the question to the scene - graph can be tricky .", "entities": []}, {"text": "It can involve exact match , number match ( dogs in the question , anddogin the scene - graph ) , hyponyms ( animal in the question , and dogin the scene - graph ) , and synonyms ( motorbike in the question , and motorcycle in the scene - graph ) .", "entities": [[3, 5, "MetricName", "exact match"]]}, {"text": "The details are in the published code ; ( 5 ) The only difference between the original and the perturbed instance is a single atom : an object , relationship , or attribute .", "entities": []}, {"text": "2.3 Validating Perturbed Instances To verify the correctness of our automatic process , we sampled 553 images , each one with an original and perturbed QA pair for a total of 1,106 instances ( \u00194 % of the validation contrast pairs ) .", "entities": []}, {"text": "The ( image , question ) pairs were answered independently by human annotators on Amazon Mechanical Turk ( see Fig .", "entities": []}, {"text": "3 in Appendix A.4 )", "entities": []}, {"text": ", oblivious to whether the question originated from GQA or from our automatic contrast set .", "entities": [[8, 9, "DatasetName", "GQA"]]}, {"text": "We found that the workers were able to correctly answer 72.3 % of the perturbed questions , slightly lower than their performance on the original questions ( 76.6%).2We observed high agreement between annotators ( \u0014= 0:679 ) .", "entities": []}, {"text": "Our analysis shows that the human performance difference between the perturbed questions and the original questions can be attributed to the scene 2The GQA paper reports higher human accuracy ( around 90 % ) on their original questions .", "entities": [[23, 24, "DatasetName", "GQA"], [28, 29, "MetricName", "accuracy"]]}, {"text": "We attribute this difference to the selection of a subset of questions that match our templates , which are potentially more ambiguous than average GQA questions ( see Section 3 ) .", "entities": [[24, 25, "DatasetName", "GQA"]]}, {"text": "97 Thebat the batter is holding has what color ?", "entities": []}, {"text": "Brown !", "entities": []}, {"text": "Thehelmet has what color ?", "entities": []}, {"text": "Blue Is there any fence near the players ?", "entities": []}, {"text": "Yes !", "entities": []}, {"text": "Are there any trees near the players ?", "entities": []}, {"text": "No", "entities": []}, {"text": "Do you see either bakers orphotographers ?", "entities": []}, {"text": "No !", "entities": []}, {"text": "Do you see either spectators orphotographers ?", "entities": []}, {"text": "Yes Is the catcher to the right of an umpire ?", "entities": []}, {"text": "No !", "entities": []}, {"text": "Is the catcher to the right of abatter ?", "entities": []}, {"text": "Yes Is the catcher to the right of an umpire ?", "entities": []}, {"text": "No !", "entities": []}, {"text": "Is the catcher to the leftof an umpire ?", "entities": []}, {"text": "Yes Figure 2 : GQA image ( left ) with example perturbations for different question templates ( right ) .", "entities": [[4, 5, "DatasetName", "GQA"]]}, {"text": "Each perturbation aims to change the label in a predetermined manner , e.g. , from \u201c yes \u201d to \u201c no \u201d .", "entities": []}, {"text": "Model Training set Original Augmented MACBaseline 64.9 % 51.5 % Augmented 64.4 % 68.4 % LXMERTBaseline 83.9 % 67.2 % Augmented 82.6 % 77.2 % Table 2 : Model accuracy on the original validation set and on our generated contrast sets with maximum of 5 augmentations .", "entities": [[29, 30, "MetricName", "accuracy"]]}, {"text": "Baseline refers to the original models , augmented refers to the models trained with our augmented training contrast sets .", "entities": []}, {"text": "graph annotation errors in the GQA dataset : 3.5 % of the 4 % difference is caused by a discrepancy between image and scene graph ( objects appearing in the image and not in the graph , and vice versa ) .", "entities": [[5, 6, "DatasetName", "GQA"]]}, {"text": "Examples are available in Fig .", "entities": []}, {"text": "5 in Appendix A.5 .", "entities": []}, {"text": "3 Experiments We experiment with two top - performing GQA models , MAC ( Hudson and Manning , 2018 ) and LXMERT ( Tan and Bansal , 2019),3to test their generalization on our automatic contrast sets , leading to various key observations .", "entities": [[9, 10, "DatasetName", "GQA"], [21, 22, "MethodName", "LXMERT"]]}, {"text": "Models struggle with our contrast set .", "entities": []}, {"text": "Table 2 shows that despite GQA \u2019s emphasis on dataset balance and compositionality , both MAC and LXMERT degraded on the contrast set : MAC 64.9%!51.5 % and LXMERT 83.9 % ! 67.2 % , compared to only 4 % degradation in human performance .", "entities": [[5, 6, "DatasetName", "GQA"], [17, 18, "MethodName", "LXMERT"], [28, 29, "MethodName", "LXMERT"]]}, {"text": "Full breakdown of the results by template is shown in Table 3 .", "entities": []}, {"text": "As expected , question templates that reference two objects ( XandY ) result in larger performance drop compared to those containing a single object ( X ) .", "entities": []}, {"text": "Questions about colors 3MAC and LXMERT are the top two models in the GQA leaderboard with a public implementation as of the time of submission : https://github.com/airsplay/ lxmert andhttps://github.com/stanfordnlp/ mac - network/ .MAC", "entities": [[5, 6, "MethodName", "LXMERT"], [13, 14, "DatasetName", "GQA"], [27, 28, "MethodName", "lxmert"]]}, {"text": "LXMERT Original Aug. Original Aug.", "entities": [[0, 1, "MethodName", "LXMERT"]]}, {"text": "On which side is the X ? 68 % 57 % 94 % 81 % What color is the X ? 49 % 49 % 69 % 62 % Are there Xnear the Y ?", "entities": []}, {"text": "85 % 66 % 98 % 79 % Do you see XorY ?", "entities": []}, {"text": "88 % 53 % 95 % 65 % Is the XReltheY ?", "entities": []}, {"text": "85 % 44 % 96 % 69 % Is the XReltheY ?", "entities": []}, {"text": "71 % 38 % 93 % 55 % Overall 65 % 52 % 84 % 67 % Table 3 : Model accuracy on the original and augmented validation set by question template for a maximum 5 augmentations per instance .", "entities": [[21, 22, "MetricName", "accuracy"]]}, {"text": "had the smallest performance drop , potentially because the models performance on such multi - class , subjective questions is relatively low to begin with .", "entities": []}, {"text": "Training on perturbed set leads to more robust models .", "entities": []}, {"text": "Previous works tried to mitigate spurious datasets biases by explicitly balancing labels during dataset construction ( Goyal et al . , 2017 ; Zhu et al . , 2016 ; Zhang et al . , 2016 ) or using adversarial \ufb01ltering ( Zellers et al . , 2018 , 2019 ) .", "entities": []}, {"text": "In this work we take an inoculation approach ( Liu et al . , 2019 ) and augment the original GQA training set with contrast training data , resulting in a total of 1,023,607 training samples .", "entities": [[20, 21, "DatasetName", "GQA"]]}, {"text": "We retrain both models on the augmented training data , and observe in Table 2 that their performance on the contrast set almost matches that of the original validation set , with no loss ( MAC ) or only minor loss ( LXMERT ) to original validation accuracy.4These results indicate that the perturbed training set is a valuable signal , which helps models recognize more patterns .", "entities": [[33, 34, "MetricName", "loss"], [40, 41, "MetricName", "loss"], [42, 43, "MethodName", "LXMERT"]]}, {"text": "Contrast Consistency .", "entities": []}, {"text": "Our method can be used to generate many augmented questions by simply sampling more items for replacement ( Section 2 ) .", "entities": []}, {"text": "4To verify that this is not the result of training on more data , we repeated this experiment , removing the same amount of original training instances ( so the \ufb01nal dataset size is the same as the original one ) , and observed very similar results .", "entities": []}, {"text": "98Augmentations per instanceContrast sets Acc . Consistency 1 11,263 66 % 63.4 % 3 23,236 67 % 51.1 % 5 28,968 67 % 46.1 % Table 4 : Accuracy and consistency results for the LXMERT model on different contrast set sizes .", "entities": [[4, 5, "MetricName", "Acc"], [28, 29, "MetricName", "Accuracy"], [34, 35, "MethodName", "LXMERT"]]}, {"text": "This allows us to measure the contrast consistency ( Gardner et al . , 2020 ) of our contrast set , de\ufb01ned as the percentage of the contrast sets for which a model \u2019s predictions are correct for all examples in the set ( including the original example ) .", "entities": []}, {"text": "For example , in Fig .", "entities": []}, {"text": "1 the set size is 4 , and only 2/4 predictions are correct .", "entities": []}, {"text": "We experiment with 1 , 3 , and 5 augmentations per question with the LXMERT model trained on the original GQA training set .", "entities": [[14, 15, "MethodName", "LXMERT"], [20, 21, "DatasetName", "GQA"]]}, {"text": "Our results ( Table 4 ) show that sampling more objects leads to similar accuracy levels for the LXMERT model , indicating that quality of our contrast sets does not depend on the speci\ufb01c selection of replacements .", "entities": [[14, 15, "MetricName", "accuracy"], [18, 19, "MethodName", "LXMERT"]]}, {"text": "However , we observe that consistency drops fast as the size of the contrast sets per QA instance grows , indicating that model success on a speci\ufb01c instance does not mean it can generalize robustly to perturbations .", "entities": []}, {"text": "4 Discussion and Conclusion Our results suggest that both MAC and LXMERT under - perform when tested out of distribution .", "entities": [[11, 12, "MethodName", "LXMERT"]]}, {"text": "A remaining question is whether this is due to model architecture or dataset design .", "entities": []}, {"text": "Bogin et al .", "entities": []}, {"text": "( 2020 ) claim that both of these models are prone to fail on compositional generalization because they do not decompose the problem into smaller sub - tasks .", "entities": []}, {"text": "Our results support this claim .", "entities": []}, {"text": "On the other hand , it is possible that a different dataset could prevent these models from \ufb01nding shortcuts .", "entities": []}, {"text": "Is there a dataset that can prevent allshortcuts ?", "entities": []}, {"text": "Our automatic method for creating contrast sets allows us to ask those questions , while we believe that future work in better training mechanisms , as suggested in Bogin et al .", "entities": []}, {"text": "( 2020 ) and Jin et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) , could help in making more robust models .", "entities": []}, {"text": "We proposed an automatic method for creating contrast sets for VQA datasets that use annotated scene graphs .", "entities": [[10, 11, "TaskName", "VQA"]]}, {"text": "We created contrast sets for the GQA dataset , which is designed to be compositional , balanced , and robust against statistical biases .", "entities": [[6, 7, "DatasetName", "GQA"]]}, {"text": "We observed a large performance drop between the original and augmented sets .", "entities": []}, {"text": "As our contrast setscan be generated cheaply , we further augmented the GQA training data with additional perturbed questions , and showed that this improves models \u2019 performance on the contrast set .", "entities": [[12, 13, "DatasetName", "GQA"]]}, {"text": "Our proposed method can be extended to other VQA datasets .", "entities": [[8, 9, "TaskName", "VQA"]]}, {"text": "Acknowledgements We thank the reviewers for the helpful comments and feedback .", "entities": []}, {"text": "We thank the authors of GQA for building the dataset , and the authors of LXMERT and MAC for sharing their code and making it usable .", "entities": [[5, 6, "DatasetName", "GQA"], [15, 16, "MethodName", "LXMERT"]]}, {"text": "This work was supported in part by the Center for Interdisciplinary Data Science Research at the Hebrew University of Jerusalem , and research gifts from the Allen Institute for AI .", "entities": []}, {"text": "References Ben Bogin , Sanjay Subramanian , Matt Gardner , and Jonathan Berant .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Latent compositional representations improve systematic generalization in grounded question answering .", "entities": [[4, 6, "TaskName", "systematic generalization"], [8, 10, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:2007.00266 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shi Feng , Eric Wallace , and Jordan Boyd - Graber .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Misleading failures of partial - input baselines .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5533\u20135538 , Florence , Italy .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Matt Gardner , Yoav Artzi , Victoria Basmov , Jonathan Berant , Ben Bogin , Sihao Chen , Pradeep Dasigi , Dheeru Dua , Yanai Elazar , Ananth Gottumukkala , Nitish Gupta , Hannaneh Hajishirzi , Gabriel Ilharco , Daniel Khashabi , Kevin Lin , Jiangming Liu , Nelson F. Liu , Phoebe Mulcaire , Qiang Ning , Sameer Singh , Noah A. Smith , Sanjay Subramanian , Reut Tsarfaty , Eric Wallace , Ally Zhang , and Ben Zhou .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Evaluating models \u2019 local decision boundaries via contrast sets .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 1307\u20131323 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mor Geva , Yoav Goldberg , and Jonathan Berant .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Are we modeling the task or the annotator ?", "entities": []}, {"text": "an investigation of annotator bias in natural language understanding datasets .", "entities": [[6, 9, "TaskName", "natural language understanding"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 1161\u20131166 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yash Goyal , Tejas Khot , Douglas Summers - Stay , Dhruv Batra , and Devi Parikh . 2017 .", "entities": []}, {"text": "Making the v in vqa matter : Elevating the role of image understanding in visual question answering .", "entities": [[4, 5, "TaskName", "vqa"], [14, 17, "DatasetName", "visual question answering"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6904\u20136913 .", "entities": []}, {"text": "99Suchin Gururangan , Swabha Swayamdipta , Omer Levy , Roy Schwartz , Samuel Bowman , and Noah A. Smith .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Annotation artifacts in natural language inference data .", "entities": [[3, 6, "TaskName", "natural language inference"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 107\u2013112 , New Orleans , Louisiana . Association for Computational Linguistics .", "entities": []}, {"text": "Drew A. Hudson and Christopher D. Manning .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "GQA : A new dataset for real - world visual reasoning and compositional question answering .", "entities": [[0, 1, "DatasetName", "GQA"], [9, 11, "TaskName", "visual reasoning"], [13, 15, "TaskName", "question answering"]]}, {"text": "In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition ( CVPR ) .", "entities": []}, {"text": "Drew Arad Hudson and Christopher D. Manning .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Compositional attention networks for machine reasoning .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Robin Jia and Percy Liang . 2017 .", "entities": []}, {"text": "Adversarial examples for evaluating reading comprehension systems .", "entities": [[4, 6, "TaskName", "reading comprehension"]]}, {"text": "InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2021\u20132031 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Xisen Jin , Junyi Du , Arka Sadhu , Ram Nevatia , and Xiang Ren . 2020 .", "entities": []}, {"text": "Visually grounded continual learning of compositional phrases .", "entities": [[2, 4, "TaskName", "continual learning"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 2018\u20132029 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Divyansh Kaushik , Eduard Hovy , and Zachary Lipton .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Learning the difference that makes a difference with counterfactually - augmented data .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Ranjay Krishna , Yuke Zhu , Oliver Groth , Justin Johnson , Kenji Hata , Joshua Kravitz , Stephanie Chen , Yannis Kalantidis , Li - Jia Li , David A Shamma , et al . 2017 .", "entities": []}, {"text": "Visual genome : Connecting language and vision using crowdsourced dense image annotations .", "entities": [[0, 2, "DatasetName", "Visual genome"]]}, {"text": "International journal of computer vision , 123(1):32 \u2013 73 .", "entities": []}, {"text": "Chuanrong Li , Lin Shengshuo , Zeyu Liu , Xinyi Wu , Xuhui Zhou , and Shane Steinert - Threlkeld .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Linguistically - informed transformations ( LIT ): A method for automatically generating contrast sets .", "entities": []}, {"text": "InProceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP , pages 126\u2013135 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nelson F. Liu , Roy Schwartz , and Noah A. Smith .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Inoculation by \ufb01ne - tuning : A method for analyzing challenge datasets .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 2171\u20132179 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tom McCoy , Ellie Pavlick , and Tal Linzen .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Right for the wrong reasons : Diagnosing syntactic heuristics in natural language inference .", "entities": [[10, 13, "TaskName", "natural language inference"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3428\u20133448 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Semantically equivalent adversarial rules for debugging NLP models .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 856\u2013865 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Shachar Rosenman , Alon Jacovi , and Yoav Goldberg .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data .", "entities": [[4, 6, "TaskName", "Relation Extraction"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 3702\u20133710 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Gabriel Stanovsky , Noah A. Smith , and Luke Zettlemoyer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Evaluating gender bias in machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1679\u20131684 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Hao Tan and Mohit Bansal .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "LXMERT :", "entities": [[0, 1, "MethodName", "LXMERT"]]}, {"text": "Learning cross - modality encoder representations from transformers .", "entities": [[0, 8, "MethodName", "Learning cross - modality encoder representations from transformers"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 5100\u20135111 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rowan Zellers , Yonatan Bisk , Roy Schwartz , and Yejin Choi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SWAG :", "entities": [[0, 1, "DatasetName", "SWAG"]]}, {"text": "A large - scale adversarial dataset for grounded commonsense inference .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 93 \u2013 104 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Rowan Zellers , Ari Holtzman , Yonatan Bisk , Ali Farhadi , and Yejin Choi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "HellaSwag :", "entities": [[0, 1, "DatasetName", "HellaSwag"]]}, {"text": "Can a machine really \ufb01nish your sentence ?", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4791 \u2013 4800 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Peng Zhang , Yash Goyal , Douglas Summers - Stay , Dhruv Batra , and Devi Parikh . 2016 .", "entities": []}, {"text": "Yin and yang : Balancing and answering binary visual questions .", "entities": []}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 5014\u20135022 .", "entities": []}, {"text": "Yuke Zhu , Oliver Groth , Michael Bernstein , and Li FeiFei .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Visual7w : Grounded question answering in images .", "entities": [[0, 1, "DatasetName", "Visual7w"], [3, 5, "TaskName", "question answering"]]}, {"text": "In Proceedings of the IEEE conference", "entities": []}, {"text": "100on computer vision and pattern recognition , pages 4995\u20135004 .", "entities": []}, {"text": "101A Appendix Ethical Considerations We created contrast sets automatically , and veri\ufb01ed their correctness via the crowdsourcing annotation of a sample of roughly 1 K instances .", "entities": []}, {"text": "Section 2.3 describes the annotation process on Amazon Mechanical Turk .", "entities": []}, {"text": "The images and original questions were sampled from the public GQA dataset ( Hudson and Manning , 2019 ) , in the English language .", "entities": [[10, 11, "DatasetName", "GQA"]]}, {"text": "Fig .", "entities": []}, {"text": "3 in Appendix A.4 provides example of the annotation task .", "entities": []}, {"text": "Overall , the crowdsourcing task resulted in\u00196 hours of work , which paid an average of 11USD per hour per annotator .", "entities": []}, {"text": "Reproducibility The augmentations were performed with a MacBook Pro laptop .", "entities": []}, {"text": "Augmentations for the validation data takes < 1 hour per question template , and for the training data < 3 hours per question template .", "entities": []}, {"text": "Overall process , < 24 hours .", "entities": []}, {"text": "The experiments have been performed with the public implementations of MAC ( Hudson and Manning , 2018 ) and LXMERT ( Tan and Bansal , 2019 ) , models : https : //github.com / airsplay / lxmert , https://github.com/stanfordnlp/", "entities": [[19, 20, "MethodName", "LXMERT"], [36, 37, "MethodName", "lxmert"]]}, {"text": "mac - network/ .", "entities": []}, {"text": "The con\ufb01gurations were modi\ufb01ed to not include the validation set in the training process .", "entities": []}, {"text": "The experiments were performed with a Linux virtual machine with a NVIDIA \u2019s Tesla V100 GPU .", "entities": [[6, 7, "DatasetName", "Linux"]]}, {"text": "The training took \u00181 - 2 days in each model .", "entities": []}, {"text": "Validation took \u001830 minutes .", "entities": []}, {"text": "A.1 Generated Contrast Sets Statistics Table 5 reports the basic statistics of automatic contrast sets generation method when applied on the GQA validation dataset .", "entities": [[21, 22, "DatasetName", "GQA"], [22, 24, "DatasetName", "validation dataset"]]}, {"text": "It shows the overall number of images and QA pairs that matched the 6 question types we identi\ufb01ed .", "entities": []}, {"text": "Tables 6 shows the statistics per question type , indicating how productive each augmentation method is .", "entities": []}, {"text": "Tables 7 and 8 shows the same statistics for the GQA Training dataset .", "entities": [[10, 11, "DatasetName", "GQA"]]}, {"text": "# Aug. QA pairs Max 1 Max 3 Max 5 # Images 10,696 10,696 10,696 # QA pairs 132,062 132,062 132,062 #", "entities": []}, {"text": "Aug. QA pairs 12,962 26,189 32,802 # Aug. images 6,166 6,166 6,166 % Aug. images 57.6 % 57.6 % 57.6 % % Aug. QA pairs 9.8 % 19.8 % 24.8 % Table 5 : Validation data augmentation statistics Question template #", "entities": [[35, 37, "TaskName", "data augmentation"]]}, {"text": "Aug.", "entities": []}, {"text": "QA pairs Max 1 Max 3 Max 5 On which side is the X ? 2,516 4,889 5,617 What color is the X ? 4,608 10,424 12,414 Are there Xnear the Y ?", "entities": []}, {"text": "382 867 1,320 Do you see XorY ?", "entities": []}, {"text": "1,506 4,514 7,516 Is the XReltheY ?", "entities": []}, {"text": "766 1,314 1,392 Is the XReltheY ?", "entities": []}, {"text": "1,417 1,416 1,416 Table 6 : Augmentation statistics per question template for the validation data A.2 Models Performance Breakdown by Question Type and Number of Augmentations Table 3 shows the breakdown of the performance of the MAC and LXMERT models per question type , on both the original GQA validation set and on the augmented contrast sets on validation .", "entities": [[38, 39, "MethodName", "LXMERT"], [48, 49, "DatasetName", "GQA"]]}, {"text": "The LXMERT model has two stages of training : pre - training on several datasets ( which includes GQA training and validation data ) and \ufb01ne - tuning .", "entities": [[1, 2, "MethodName", "LXMERT"], [18, 19, "DatasetName", "GQA"]]}, {"text": "To avoid in\ufb02ating results on the validation data , we re - trained the pre - training stage without the GQA data , and \ufb01ne - tuned on the training sets .", "entities": [[20, 21, "DatasetName", "GQA"]]}, {"text": "Table 2 .", "entities": []}, {"text": "We discovered lower performance on the original set ( -\u00185 % ) with both models , but the same improvement on the augmented set ( + \u001810 ) .", "entities": []}, {"text": "102 # Images 72,140 # QA pairs 943,000 # Aug.", "entities": []}, {"text": "QA pairs 89,936 # Aug. images 43,463 % Aug. images 60.2 % % Aug. QA pairs 9.5 % Table 7 : Training data augmentation statistics A.3 Linguistic Heuristics for Questions Generation For each question type , we select an object in the image scene graph , and update the question by substituting the reference to this object by another object .", "entities": [[22, 24, "TaskName", "data augmentation"]]}, {"text": "When substituting one object by another , we need to adjust the question to keep it \ufb02uent .", "entities": []}, {"text": "Table 10 shows the speci\ufb01c linguistic rules we verify when performing this substitution .", "entities": []}, {"text": "A.4 Annotation Task for Verifying Generated Contrast Sets Fig .", "entities": []}, {"text": "3 shows the annotation task that is shown to Turkers to validate the QA pairs generated by our method .", "entities": []}, {"text": "Figure 3 : Example of the annotation task at the Amazon Mechanical Turk website", "entities": []}, {"text": "103Question template # Aug.", "entities": []}, {"text": "QA pairs # Aug. images % Aug. questions On which side is the X ? 17,935 16,224 2.2 % What color is the X ? 32,744 27,704 4.1 % Are there Xnear the Y ?", "entities": []}, {"text": "2,682 2,323 0.3 % Do you see XorY ?", "entities": []}, {"text": "10,666 9,704 1.1 % Is the XReltheY ?", "entities": []}, {"text": "6,302 5,479 0.6 % Is the XReltheY ?", "entities": []}, {"text": "9,938 8,007 1.1 % Table 8 : Augmentation statistics per question template for the training data Original DatasetAug .", "entities": []}, {"text": "dataset Max 1 Max 3 Max 5 Size MAC LXMERT MAC LXMERT Size MAC LXMERT Size MAC LXMERT", "entities": [[9, 10, "MethodName", "LXMERT"], [11, 12, "MethodName", "LXMERT"], [14, 15, "MethodName", "LXMERT"], [17, 18, "MethodName", "LXMERT"]]}, {"text": "On which side is the X ? 2,538 68 % 94 % 56 % 79 % 4,927 57 % 80 % 5,662 57 % 81 % What color is the X ? 4,654 49 % 69 % 48 % 62 % 10,506 49 % 62 % 12,498 49 % 62 % Are there Xnear the Y ?", "entities": []}, {"text": "382 85 % 98 % 72 % 84 % 867 69 % 80 % 1,320 66 % 79 % Do you see XorY ?", "entities": []}, {"text": "1,506 88 % 95 % 53 % 63 % 4,205 53 % 64 % 6,679 53 % 65 % Is the XReltheY ?", "entities": []}, {"text": "766 85 % 96 % 42 % 67 % 1,314 44 % 69 % 1,392 44 % 69 % Is the XReltheY ?", "entities": []}, {"text": "1,417 71 % 93 % 38 % 55 % 1,417 38 % 55 % 1,417 38 % 55 % Overall 11,263 65 % 84 % 50 % 66 % 23,236 51 % 67 % 28,968 52 % 67 % Table 9 : Model accuracy by question template and maximum number of augmentations .", "entities": [[43, 44, "MetricName", "accuracy"]]}, {"text": "Italic text indicates variables , bold text indicates the perturbed atoms .", "entities": []}, {"text": "A.5 Examples", "entities": []}, {"text": "104Linguistic rule Explanation Examples Singular vs. pluralIf the noun is singular and countable : add \u201c a \u201d or \u201c an \u201d If needed , replace \u201c Are \u201d and \u201c Is\u201d\u201ca fence \u201d , \u201c men \u201d \u201c a boy \u201d , \u201c an elephant \u201d De\ufb01nite vs. inde\ufb01niteDo not change de\ufb01nite articles to inde\ufb01nite articles , and vice versa\u201dis there any fence near the boy \u201d suggests that there is a boy in the scene graph , which is not always correct General vs. speci\ufb01cMeaning can be changed When replacing to general or speci\ufb01c terms\u201cCats in the image \u201d = > \u201c Animals in the image \u201d , \u201c Animals not in the image \u201d = > \u201c cats not in the image \u201d , The opposite directions not necessarily holds Countable vs. uncountableIf the noun is uncountable , do not add \u201c a \u201d or \u201c an\u201d\u201cA cat \u201d , \u201c water \u201d Table 10 : Partial linguistic rules to notice using our method .", "entities": [[84, 85, "DatasetName", "General"]]}, {"text": "Figure 4 : Original QA Augmented QA On which side is the blanket ?", "entities": []}, {"text": "Right On which side is the ornament ?", "entities": []}, {"text": "Left What color is the teddy bear to the right of the pillow ?", "entities": []}, {"text": "Brown", "entities": []}, {"text": "What color is the christmas lights ?", "entities": []}, {"text": "Yellow Is there a couch near the blanket ?", "entities": []}, {"text": "Yes Is there a catnear the blanket ?", "entities": []}, {"text": "No", "entities": []}, {"text": "Do you see a pillow orcouch there ?", "entities": []}, {"text": "Yes Do you see a dress or acarpet there ?", "entities": []}, {"text": "No", "entities": []}, {"text": "If the pillow to the leftof acat ?", "entities": []}, {"text": "No Is the pillow to the leftof ateddy bear ?", "entities": []}, {"text": "Yes Is the pillow to the leftof acat ?", "entities": []}, {"text": "No", "entities": []}, {"text": "No aug . -", "entities": []}, {"text": "No relation between ( pillow , cat )", "entities": []}, {"text": "105 ( a ) First case example - multiple objects Augmented question : On which side of the photo are the bananas ?", "entities": []}, {"text": "Expected answer : right \u201c bananas \u201d are annotated in green text color in the right side of the image , but it also appears in additional locations ( b ) Second case example - missing annotation Augmented question : Do you see either a brown chair or couch in this picture ?", "entities": []}, {"text": "Expected answer :", "entities": []}, {"text": "No We can see a couch in the left side of the image which is not annotated in the scene graph ( c ) Third case example - incorrect annotation Augmented question : Do you see either any windows or fences ?", "entities": []}, {"text": "Expected answer :", "entities": []}, {"text": "Yes We can see an incorrect annotation of \u201c windows \u201d on the person shirt in azure text color .", "entities": []}, {"text": "Figure 5 : Scene graph annotation mistakes", "entities": []}]