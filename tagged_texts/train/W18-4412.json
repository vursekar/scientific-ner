[{"text": "Proceedings of the First Workshop on Trolling , Aggression and Cyberbullying , pages 98\u2013105 Santa Fe , USA , August 25 , 2018.98Identifying Aggression and Toxicity in Comments using Capsule Network Saurabh Srivastava Prerna Khurana Vartika", "entities": [[29, 31, "MethodName", "Capsule Network"]]}, {"text": "Tewari TCS Research fsriv.saurabh , prerna.khurana2,vartika.tewari", "entities": []}, {"text": "g@tcs.com Abstract Aggression and related activities like trolling , hate speech etc . involve toxic comments in various forms .", "entities": [[9, 11, "DatasetName", "hate speech"]]}, {"text": "These are common scenarios in today \u2019s time and websites react by shutting down their comment sections .", "entities": []}, {"text": "To tackle this , an algorithmic solution is preferred to human moderation which is slow and expensive .", "entities": []}, {"text": "In this paper , we propose a single model capsule network with focal loss to achieve this task which is suitable for production environment .", "entities": [[9, 11, "MethodName", "capsule network"], [12, 14, "MethodName", "focal loss"]]}, {"text": "Our model achieves competitive results over other strong baseline methods , which show its effectiveness and that focal loss exhibits signi\ufb01cant improvement in such cases where class imbalance is a regular issue .", "entities": [[17, 19, "MethodName", "focal loss"]]}, {"text": "Additionally , we show that the problem of extensive data preprocessing , data augmentation can be tackled by capsule networks implicitly .", "entities": [[12, 14, "TaskName", "data augmentation"]]}, {"text": "We achieve an overall ROC AUC of 98.46 on Kaggletoxic comment dataset and show that it beats other architectures by a good margin .", "entities": [[4, 6, "MetricName", "ROC AUC"]]}, {"text": "As comments tend to be written in more than one language , and transliteration is a common problem , we further show that our model handles this effectively by applying our model on TRAC shared task dataset which contains comments in code - mixed Hindi - English .", "entities": []}, {"text": "1 Introduction In today \u2019s time , with an ever increasing penetration of social media , news portals , blogs , QnA forums , and other websites that allow user interaction , users often end up inviting comments that are nasty , harrasing , insulting , toxic etc .", "entities": []}, {"text": "This can have adverse effects on users , who then become victims of cyberbullying or online harrasment .", "entities": []}, {"text": "An online survey carried out by the Pew Research Centre in 2017 states that 4 in 10 Americans have personally experienced online harrasment .", "entities": []}, {"text": "Strikingly , 1 in 5 Americans have witnessed severe form of online harrasment like physical threats , stalking , sexual harrasment etc .", "entities": []}, {"text": "There are several challenges associated with solving this kind of problem .", "entities": []}, {"text": "First being the problem of class imbalance found in the dataset .", "entities": []}, {"text": "Since such type of comments are sparse in nature , they introduce skewness in the dataset .", "entities": []}, {"text": "There are several ways to handle this problem , however , we choose a more recent technique which modi\ufb01es the standard cross entropy loss function known as Focal Loss ( Lin et al . , 2017 ) .", "entities": [[23, 24, "MetricName", "loss"], [27, 29, "MethodName", "Focal Loss"]]}, {"text": "We will brie\ufb02y describe how it helps in improving classi\ufb01er performance .", "entities": []}, {"text": "The next problem we want to address is that of data preprocessing .", "entities": []}, {"text": "This is the most time consuming task and requires a good understanding of the data .", "entities": []}, {"text": "However , we wish to minimise this process so as to have a good model with minimal preprocessing of the data .", "entities": []}, {"text": "Another frequently observed challenge is transliteration , which is often observed , especially , when we are working with text data from social networking websites .", "entities": []}, {"text": "Users tend to speak in more than one language in the same statement .", "entities": []}, {"text": "This leads to several out of vocabulary or OOV words for which the model would not have any word embedding .", "entities": []}, {"text": "We use randomly initialised word embeddings in such a case and show how they can be trained during model training procedure such that it results in clusters of OOV words which have similar meaning in Hindi .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "We propose to tackle all the above described challenges using a single model as opposed to ensemble of several other models , which is a common practice in such competitive challenges .", "entities": []}, {"text": "We also show that our proposed model can converge really quickly , hence the model can be trained in lesser time .", "entities": []}, {"text": "This is essential when the model has to be deployed in a production environment where it requires retraining periodically .", "entities": []}, {"text": "992 Related Work Early works in automated detection of abusive language made use of basic machine learning like Tf - Idf ( Yin et al . , 2009 ) , SVM ( Warner and Hirschberg , 2012 ) , Naive Bayes , random forests , or logistic regression over a bag - of - ngrams and achieved limited success .", "entities": [[9, 11, "TaskName", "abusive language"], [30, 31, "MethodName", "SVM"], [46, 48, "MethodName", "logistic regression"]]}, {"text": "Newer approaches include solving problems using deep learning architectures like CNNs ( Kim , 2014 ; Zhang et", "entities": []}, {"text": "al . , 2015 ; Conneau et al . , 2017b ; Park and Fung , 2017 ) which just focus on spatial patterns or LSTMs which treat text as sequences ( Tai et al . , 2015 ; Mousa and Schuller , 2017 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Another popular approach completely ignores the order of words but focuses on their compositions as a collection , like probabilistic topic modeling ( Blei et al . , 2003 ;", "entities": []}, {"text": "Mcauliffe and Blei , 2008 ) and Earth Movers Distance based modeling ( Kusner et al . , 2015 ; Ye et al . , 2017 ) .", "entities": []}, {"text": "Recently Capsule Network ( Sabour et al . , 2017 ) has been used in text classi\ufb01cation ( Zhao et al . , 2018).It makes use of the dynamic routing process to alleviate the disturbance of some noise capsules which may contain background information such as stop words and words that are unrelated to speci\ufb01c categories and show that capsule networks achieves signi\ufb01cant improvement over strong baseline methods .", "entities": [[1, 3, "MethodName", "Capsule Network"]]}, {"text": "As we focus to solve the problem of toxic comments and cyberbullying , we are confronted with the issue of large class imbalance .", "entities": []}, {"text": "We use focal loss ( Lin et al . , 2017 ) to tackle it as it prevents the vast number of easy negatives from overwhelming the detector during training .", "entities": [[2, 4, "MethodName", "focal loss"]]}, {"text": "Also , in the online space people tend to talk using different languages in the same comment and often use transliteration .", "entities": []}, {"text": "We show that our model is suitable for such data as well .", "entities": []}, {"text": "3 Capsule Net for Classi\ufb01cation Proposed Model : The model proposed in ( Zhao et al . , 2018 ) has been used for the experimentation with an inclusion of Focal Loss ( Lin et al . , 2017 ) as a loss function to address the class imbalance problem .", "entities": [[30, 32, "MethodName", "Focal Loss"], [42, 43, "MetricName", "loss"]]}, {"text": "In our experiments we have compared performances of CNNs and RNNs as feature extractors and found that sentence representation obtained from RNNs performs better than representations obtained after applying convolution operation , although CNNs tends to perform better on short texts .", "entities": [[29, 30, "MethodName", "convolution"]]}, {"text": "The model consists of four layers : ( i)Word Embedding Layer : We represent every comment xi , as a sequence of one - hot encoding of its words , xi= ( w1;w2;:::wn)of lengthnmax , which is the maximum length of the comment , with zero padding .", "entities": []}, {"text": "Such a sequence becomes the input to the embedding layer .", "entities": []}, {"text": "To represent word tokens several ideas like sparse representation or dense representation ( Collobert and Weston , 2008 ; Bengio et al . , 2003 ) have been proposed .", "entities": []}, {"text": "( ii)Feature Extraction Layer : This layer has been used to extract either n - grams feature at different position of a sentence through different \ufb01lters ( CNNs ) or long term temporal dependencies within the sentence ( RNNs ) .", "entities": []}, {"text": "We use RNNs as feature extractors in our \ufb01nal model .", "entities": []}, {"text": "( iii)Capsule Layer : The Capsule layer is primarily composed of two sub - layers Primary Capsule Layer andConvolutional Capsule Layer .", "entities": []}, {"text": "The primary capsule layer is supposed to capture the instantiated parameters of the inputs , for example , in case of texts local order of words and their semantic representation .", "entities": []}, {"text": "Suppose we have ^enumber of feature extractors , then the input to the Primary capsule layer will be Z2Rn\u0002^e(where n is the number of timesteps in RNNs ) .", "entities": [[22, 25, "HyperparameterName", "number of timesteps"]]}, {"text": "The primary capsules transform a scalar - output feature detector to vector - valued capsules to capture the instantiated features .", "entities": []}, {"text": "Let dbe the dimension of each capsule , then for each capsule pi2Rd , where p denotes instantiated parameters set of a capsule ( Sabour et al . , 2017 ) , we have pi = g(WZi+b ) , whereZi is captured by RNNs in the feature extractor layer .", "entities": []}, {"text": "Here , gis the nonlinear squash function which shrinks the small vectors to around 0 and large vectors around 1 .", "entities": [[14, 15, "DatasetName", "0"]]}, {"text": "( iv)The Convolutional Capsule : The Conv layers capsules output a local grid of vectors to capsules in earlier layers using different transformation matrices for each capsule and grids member ( Sabour et al . , 2017).Capsule networks are trained using a dynamic routing algorithm that overlooks words that are not important or unrelated in the text , like stopwords and name mentions .", "entities": []}, {"text": "100Model NameKaggle - toxic comment classi\ufb01cation ( ROC - AUC)TRAC - 1 ( English - FB ) ( Weighted F1)TRAC - 1 ( English - TW ) ( Weighted F1 ) CNN - multi\ufb01lter 95.16 55.43 53.41 CNN - LSTM 96.85 62.20 47.68 Bi - directional LSTM with maxpool 97.35 59.79 51.146 FeedForward Attention Networks 97.42 57.43 55.49 Hierarchical ConvNets 97.95 51.38 50.43 Bi - LSTM , Logistic Regression 98.17 57.17 52.1 Bi - LSTM , xgboosted 98.19 57.33 52.31 Bi - LSTM with skip connections 98.20 61.78 51.98 Pre - trained LSTMs 98.25 60.18 58.7 CapsuleNet without Focal Loss 98.21 62.032 58.600 CapsuleNet with Focal Loss 98.46 63.43 59.41 Table 1 : Comparison of several deep learning approaches with Capsule Net on the three datasets Focal Loss : To handle the class imbalance problem , we have used Focal Loss which is given by the following formula : FL(pt )", "entities": [[29, 30, "MetricName", "F1"], [39, 40, "MethodName", "LSTM"], [46, 47, "MethodName", "LSTM"], [65, 66, "MethodName", "LSTM"], [67, 69, "MethodName", "Logistic Regression"], [74, 75, "MethodName", "LSTM"], [82, 83, "MethodName", "LSTM"], [98, 100, "MethodName", "Focal Loss"], [105, 107, "MethodName", "Focal Loss"], [126, 128, "MethodName", "Focal Loss"], [139, 141, "MethodName", "Focal Loss"]]}, {"text": "= \u0000 \u000b t(1\u0000pt )", "entities": []}, {"text": "log(pt);wherept = fp", "entities": []}, {"text": "if y = 1 1\u0000p else", "entities": []}, {"text": "is the focusing parameter which smoothens the rate at which easy examples are down weighted", "entities": []}, {"text": "and , \u000b is the weight assigned to the rare class .", "entities": []}, {"text": "Figure 1 : CapsNet with LSTMs as feature extractor 4 Experiments In this section we attempt to describe different models that we have used for the classi\ufb01cation process .", "entities": [[3, 4, "MethodName", "CapsNet"]]}, {"text": "We seek to answer the following questions : ( 1 ) Is combination of Capsules and focal loss the new apotheosis for toxic comment classi\ufb01cation problems ?", "entities": [[16, 18, "MethodName", "focal loss"]]}, {"text": "( 2 ) Can capsules solve the problem of OOV and transliteration implicitly ?", "entities": []}, {"text": "4.1 Datasets 4.1.1 Kaggle Toxic Comment Classi\ufb01cation : Recently , Kaggle hosted a competition named Toxic Comment Classi\ufb01cation .", "entities": []}, {"text": "This dataset has been contributed by Conversation AI , which is a research initiative founded by Jigsaw and Google .", "entities": [[16, 17, "MethodName", "Jigsaw"], [18, 19, "DatasetName", "Google"]]}, {"text": "The task was comprised of calculating the log - likelihood of a sentence for the six classes , i.e. , given a sentence calculate the probability of it belonging to six classes .", "entities": [[7, 10, "MetricName", "log - likelihood"]]}, {"text": "The six different classes were toxic , severe toxic , obscene , threat , insult and identity hate .", "entities": []}, {"text": "1014.1.2 TRAC dataset : \u201c First Shared Task on Aggression Identi\ufb01cation \u201d released a dataset for Aggression Identi\ufb01cation .", "entities": []}, {"text": "The task was to classify the comments into one of the three different classes Overtly Aggressive , Covertly Aggressive , and Non - aggressive .", "entities": []}, {"text": "The train data was given in English and Hindi , where some of the comments in Hindi dataset were transliterated to English .", "entities": []}, {"text": "4.2 Data Preprocessing For all our experiments , to show ef\ufb01cacy of our approach we kept the preprocessing as minimal as possible .", "entities": []}, {"text": "Apart from word lowerization , tokenization , and punctuation removal we did n\u2019t perform any other activity .", "entities": []}, {"text": "4.3 Baseline Algorithms We evaluate and compare our model with several strong baseline methods including : LSTM with Maxpool ( Lai et al . , 2015 ) , Attention networks ( Raffel and Ellis , 2015 ) , Pre - trained LSTMs ( Dai and Le , 2015 ) , Hierarchical ConvNet ( Conneau et al . , 2017a ) , Bi - LSTM with Skip - connections , variation of CNN - LSTM ( Wang et al . , 2016 ) , CNN - multi\ufb01lter ( Kim , 2014 ) , Bi - LSTM with xgboost and logistic regression .", "entities": [[16, 17, "MethodName", "LSTM"], [63, 64, "MethodName", "LSTM"], [73, 74, "MethodName", "LSTM"], [94, 95, "MethodName", "LSTM"], [98, 100, "MethodName", "logistic regression"]]}, {"text": "We experiment with these models on three datasets .", "entities": []}, {"text": "The models were \ufb01rst evaluated on Kaggle competition for Toxic Comment Classi\ufb01cation .", "entities": []}, {"text": "All the model parameters and attributes were decided on the basis of our best performing model , and were kept same for the rest of experimentations and datasets .", "entities": []}, {"text": "4.4 Model Training For all our experiments we have used pre - trained embeddings for each word token obtained from ( Joulin et al . , 2016 ) .", "entities": []}, {"text": "We have also exploited ( Pennington et al . , 2014 ) , ( Mikolov et al . , 2013 ) , random and manually trained embeddings for initialization .", "entities": []}, {"text": "After experimentation , fasttext embeddings with dimension of 300 were found to perform better than rest of the initialization process .", "entities": [[3, 4, "MethodName", "fasttext"]]}, {"text": "In our experiments we observed that RMSProp ( Tieleman and Hinton , 2012 ) and Adam ( Kingma and Ba , 2014 ) as an optimizer works well for training RNNs and CNNs respectively and used this throughout .", "entities": [[6, 7, "MethodName", "RMSProp"], [15, 16, "MethodName", "Adam"], [25, 26, "HyperparameterName", "optimizer"]]}, {"text": "The learning rate was kept between [ .1 and .001 ] .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "For CNNs , number of Kernels was chosen from the range [ 128 , 256 , 512 ] and the LSTM units were selected from the range", "entities": [[20, 21, "MethodName", "LSTM"]]}, {"text": "[ 128 , 256 ] .", "entities": []}, {"text": "In all of our experiments with the proposed model only a single layer for feature extraction was used .", "entities": []}, {"text": "Number of capsules was varied from [ 8 , 10 , 16 ] , the vector length of 8 for each capsule was found to be the best , and the dropout values for RNNs were taken as per suggestions from ( Zaremba et al . , 2014 ) .", "entities": []}, {"text": "The \u000b and", "entities": []}, {"text": "values in focal loss were experimented for [ 1.5 , 2 , 2.5 , 3 , 3.5 ] and [ .2 , .25 , .3 ] and \ufb01nally \u000b = 2 and", "entities": [[2, 4, "MethodName", "focal loss"]]}, {"text": "= 0.25 were taken .", "entities": []}, {"text": "5 Results and Discussions The proposed CapsNet architecture was able to beat other strong baseline algorithms with reasonable difference in accuracies with minimal preprocessing .", "entities": [[6, 7, "MethodName", "CapsNet"]]}, {"text": "We demonstrated that using focal loss along with CapsNet gave us .25 raise in the ROC - AUC for Kaggle \u2019s toxic comment classi\ufb01cation and 1.39 and .80 gain in F1 scores on TRAC shared task dataset in English , from Facebook and Twitter comments respectively .", "entities": [[4, 6, "MethodName", "focal loss"], [8, 9, "MethodName", "CapsNet"], [15, 18, "MetricName", "ROC - AUC"], [30, 31, "MetricName", "F1"]]}, {"text": "All of our experiments were performed on NVIDIA Quadro M1200 4096 MB GPU system with 32 GB RAM and Intel i7 processor .", "entities": [[17, 18, "MethodName", "RAM"]]}, {"text": "The model took almost 33 minutes for an epoch to train which was faster in comparison with other models , with exception to the models using CNNs as feature extractors .", "entities": []}, {"text": "For example , the second best performing model , which uses Pre - trained LSTM embeddings takes more than a day for the autoencoder to train and further 39 + minutes for each epoch .", "entities": [[14, 15, "MethodName", "LSTM"], [23, 24, "MethodName", "autoencoder"]]}, {"text": "Hence , we can say that our model is viable for production environment .", "entities": []}, {"text": "We have tested the capability of the architecture to handle the OOV words or misspelled words .", "entities": []}, {"text": "For this we used TRAC shared dataset , initialised the word embeddings randomly and trained the model for classi\ufb01cation process .", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "Next , we enabled the embeddings to be changed during training process which is mentioned as dynamic channel in ( Kim , 2014 ) to let the model learn new embeddings .", "entities": []}, {"text": "After training , we took the weights of embedding layer and plotted these embedings using Tensorboard ( Abadi et al . , 2015 ) .", "entities": []}, {"text": "From \ufb01gure 2 we can see that the model is able to minimise the distance between the misspelled word", "entities": []}, {"text": "102 ( a ) Training and Validation Loss for Kaggle Toxic Comment Classi\ufb01cation Dataset   ( b ) Clusters for word obtained after training Figure 2 and is able to capture the relationship between transliterated words as in Table:2 .", "entities": []}, {"text": "We found that total of 3 clusters were formed after the experiment as shown in Fig:2b .", "entities": []}, {"text": "We investigated these clusters and found that some of the highly used words in the comments belonged to certain classes .", "entities": []}, {"text": "For example , one of the cluster contained more of neutral words , another cluster contained highly aggressive and abusive words , and the third cluster contained some toxic words along with place and country names related to one \u2019s origin which were used in some foul comments .", "entities": []}, {"text": "We show the capability of our model to tackle the problem of over\ufb01tting , as observed during training we see the model to have comparatively lower difference in training and validation loss than other models .", "entities": [[31, 32, "MetricName", "loss"]]}, {"text": "Same can be seen from Fig:2a the loss margin difference does n\u2019t change .", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "We have shown that , not only our model has performed well on the classi\ufb01cation task , it also has ability to generalise well and can learn good representation for word tokens .", "entities": []}, {"text": "NN to \u201c politics \u201d NN to \u201c bharat \u201d politic bharatiya politican bhar politico mahabharata politicize bharti politician bhaskarNN to \u201c kut*e\u201d(Hindi ) chu**ya sa*le tere g**d ma***rc**d", "entities": []}, {"text": "Table 2 : Example of handling misspelt words and transliteration .", "entities": []}, {"text": "NN : Nearest Neighbour 6 Conclusion and Future Work", "entities": []}, {"text": "In this work , we have proposed to automatically detect toxicity and aggression in comments , we show that with minimal preprocessing techniques we are able to achieve a good model performance and demonstrated how OOV words and semantic sense are learnt implicitly with random initialisation .", "entities": []}, {"text": "We show the effectiveness of our proposed model against strong benchmark algorithms and that it outperforms others .", "entities": []}, {"text": "In this work , we did basic preprocessing of the data , however in future we intend to explore more preprocessing techniques for the dataset , like data augmentation using translation approaches and methods to deal with mispelled words .", "entities": [[27, 29, "TaskName", "data augmentation"]]}, {"text": "We further would examine the results of capsule net by visualising which words or phrases does the model correctly recognises for classi\ufb01cation as opposed to benchmark algorithms .", "entities": []}, {"text": "Also , we would like to examine the usage of focal loss with the rest of the baseline models .", "entities": [[10, 12, "MethodName", "focal loss"]]}, {"text": "103References Mart \u00b4 \u0131n Abadi , Ashish Agarwal , Paul Barham , Eugene Brevdo , Zhifeng Chen , Craig Citro , Greg S. Corrado , Andy Davis , Jeffrey Dean , Matthieu Devin , Sanjay Ghemawat , Ian Goodfellow , Andrew Harp , Geoffrey Irving , Michael Isard , Yangqing Jia , Rafal Jozefowicz , Lukasz Kaiser , Manjunath Kudlur , Josh Levenberg , Dandelion Man \u00b4 e , Rajat Monga , Sherry Moore , Derek Murray , Chris Olah , Mike Schuster , Jonathon Shlens , Benoit Steiner , Ilya Sutskever , Kunal Talwar , Paul Tucker , Vincent Vanhoucke , Vijay Vasudevan , Fernanda Vi \u00b4 egas , Oriol Vinyals , Pete Warden , Martin Wattenberg , Martin Wicke , Yuan Yu , and Xiaoqiang Zheng .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "TensorFlow :", "entities": []}, {"text": "Large - scale machine learning on heterogeneous systems .", "entities": []}, {"text": "Software available from tensor\ufb02ow.org .", "entities": []}, {"text": "Yoshua Bengio , R \u00b4 ejean Ducharme , Pascal Vincent , and Christian Jauvin .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "A neural probabilistic language model .", "entities": [[1, 5, "MethodName", "neural probabilistic language model"]]}, {"text": "Journal of machine learning research , 3(Feb):1137\u20131155 .", "entities": []}, {"text": "David M Blei , Andrew Y Ng , and Michael I Jordan .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Latent dirichlet allocation .", "entities": []}, {"text": "Journal of machine Learning research , 3(Jan):993\u20131022 .", "entities": []}, {"text": "Ronan Collobert and Jason Weston .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "A uni\ufb01ed architecture for natural language processing :", "entities": []}, {"text": "Deep neural networks with multitask learning .", "entities": []}, {"text": "In Proceedings of the 25th international conference on Machine learning , pages 160\u2013167 . ACM .", "entities": [[14, 15, "DatasetName", "ACM"]]}, {"text": "Alexis Conneau , Douwe Kiela , Holger Schwenk , Loic Barrault , and Antoine Bordes . 2017a .", "entities": []}, {"text": "Supervised learning of universal sentence representations from natural language inference data .", "entities": [[7, 10, "TaskName", "natural language inference"]]}, {"text": "arXiv preprint arXiv:1705.02364 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Alexis Conneau , Holger Schwenk , Lo \u00a8\u0131c Barrault , and Yann Lecun .", "entities": []}, {"text": "2017b .", "entities": []}, {"text": "Very deep convolutional networks for text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers , volume 1 , pages 1107\u20131116 .", "entities": []}, {"text": "Andrew M Dai and Quoc V Le . 2015 .", "entities": []}, {"text": "Semi - supervised sequence learning .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 3079\u20133087 .", "entities": []}, {"text": "Armand Joulin , Edouard Grave , Piotr Bojanowski , and Tomas Mikolov .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Bag of tricks for ef\ufb01cient text classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1607.01759 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1408.5882 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Diederik P Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matt Kusner , Yu Sun , Nicholas Kolkin , and Kilian Weinberger . 2015 .", "entities": []}, {"text": "From word embeddings to document distances .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In International Conference on Machine Learning , pages 957\u2013966 .", "entities": []}, {"text": "Siwei Lai , Liheng Xu , Kang Liu , and Jun Zhao . 2015 .", "entities": []}, {"text": "Recurrent convolutional neural networks for text classi\ufb01cation .", "entities": []}, {"text": "In AAAI , volume 333 , pages 2267\u20132273 .", "entities": []}, {"text": "Tsung - Yi Lin , Priya Goyal , Ross B. Girshick , Kaiming He , and Piotr Doll \u00b4 ar . 2017 .", "entities": []}, {"text": "Focal loss for dense object detection .", "entities": [[0, 2, "MethodName", "Focal loss"], [3, 6, "TaskName", "dense object detection"]]}, {"text": "In IEEE International Conference on Computer Vision , ICCV 2017 , Venice , Italy , October 22 - 29 , 2017 , pages 2999\u20133007 .", "entities": []}, {"text": "Jon D Mcauliffe and David M Blei . 2008 .", "entities": []}, {"text": "Supervised topic models .", "entities": [[1, 3, "TaskName", "topic models"]]}, {"text": "In Advances in neural information processing systems , pages 121\u2013128 .", "entities": []}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , and Jeff Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In Advances in neural information processing systems .", "entities": []}, {"text": "Amr Mousa and Bj \u00a8orn Schuller .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Contextual bidirectional long short - term memory recurrent neural network language models : A generative approach to sentiment analysis .", "entities": [[2, 7, "MethodName", "long short - term memory"], [17, 19, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers , volume 1 , pages 1023\u20131032 .", "entities": []}, {"text": "Ji Ho Park and Pascale Fung .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "One - step and two - step classi\ufb01cation for abusive language detection on twitter .", "entities": [[9, 11, "TaskName", "abusive language"]]}, {"text": "arXiv preprint arXiv:1706.01206 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Global vectors for word representation .", "entities": []}, {"text": "In Empirical Methods in Natural Language Processing , EMNLP .", "entities": []}, {"text": "Colin Raffel and Daniel PW Ellis . 2015 .", "entities": []}, {"text": "Feed - forward networks with attention can solve some long - term memory problems .", "entities": []}, {"text": "arXiv preprint arXiv:1512.08756 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "104Sara Sabour , Nicholas Frosst , and Geoffrey E Hinton . 2017 .", "entities": []}, {"text": "Dynamic routing between capsules .", "entities": []}, {"text": "In I. Guyon , U. V .", "entities": []}, {"text": "Luxburg , S. Bengio , H. Wallach , R. Fergus , S. Vishwanathan , and R. Garnett , editors , Advances in Neural Information Processing Systems 30 , pages 3856\u20133866 .", "entities": []}, {"text": "Curran Associates , Inc. Kai Sheng Tai , Richard Socher , and Christopher D Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Improved semantic representations from tree - structured long short - term memory networks .", "entities": [[7, 12, "MethodName", "long short - term memory"]]}, {"text": "arXiv preprint arXiv:1503.00075 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tijmen Tieleman and Geoffrey Hinton .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Lecture 6.5 - rmsprop : Divide the gradient by a running average of its recent magnitude .", "entities": [[3, 4, "MethodName", "rmsprop"]]}, {"text": "COURSERA :", "entities": []}, {"text": "Neural networks for machine learning , 4(2):26\u201331 .", "entities": []}, {"text": "Xingyou Wang , Weijie Jiang , and Zhiyong Luo . 2016 .", "entities": []}, {"text": "Combination of convolutional and recurrent neural network for sentiment analysis of short texts .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 2428\u20132437 .", "entities": []}, {"text": "William Warner and Julia Hirschberg .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Detecting hate speech on the world wide web .", "entities": [[1, 3, "DatasetName", "hate speech"]]}, {"text": "In Proceedings of the Second Workshop on Language in Social Media , LSM \u2019 12 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ellery Wulczyn , Nithum Thain , and Lucas Dixon . 2017 .", "entities": []}, {"text": "Ex machina : Personal attacks seen at scale .", "entities": []}, {"text": "In Proceedings of the 26th International Conference on World Wide Web , pages 1391\u20131399 .", "entities": []}, {"text": "International World Wide Web Conferences Steering Committee .", "entities": []}, {"text": "Jianbo Ye , Yanran Li , Zhaohui Wu , James Z Wang , Wenjie Li , and Jia Li . 2017 .", "entities": []}, {"text": "Determining gains acquired from word embedding quantitatively using discrete distribution clustering .", "entities": []}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , volume 1 , pages 1847\u20131856 .", "entities": []}, {"text": "Dawei Yin , Brian D. Davison , Zhenzhen Xue , Liangjie Hong , April Kontostathis , and Lynne Edwards .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Detection of harassment on web 2.0 .", "entities": []}, {"text": "Wenpeng Yin , Katharina Kann , Mo Yu , and Hinrich Sch \u00a8utze . 2017 .", "entities": []}, {"text": "Comparative study of cnn and rnn for natural language processing .", "entities": []}, {"text": "arXiv preprint arXiv:1702.01923 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Wojciech Zaremba , Ilya Sutskever , and Oriol Vinyals .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Recurrent neural network regularization .", "entities": []}, {"text": "arXiv preprint arXiv:1409.2329 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 .", "entities": []}, {"text": "Character - level convolutional networks for text classi\ufb01cation .", "entities": []}, {"text": "InAdvances in neural information processing systems , pages 649\u2013657 .", "entities": []}, {"text": "Wei Zhao , Jianbo Ye , Min Yang , Zeyang Lei , Suofei Zhang , and Zhou Zhao .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Investigating capsule networks with dynamic routing for text classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1804.00538 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "A Baseline Algorithm Descriptions A.1 CNN Multi\ufb01ter", "entities": []}, {"text": "The idea of applying CNNs for text classi\ufb01cation was proposed in ( Kim , 2014 ) , where authors applied \ufb01lters of different length to extract N - gram features from text .", "entities": []}, {"text": "The authors tried static and dynamic embedding channels and concluded that the model with combination of both outperformed others .", "entities": []}, {"text": "For our setting we found that \ufb01lters of length", "entities": []}, {"text": "[ 2 , 3 , 4 ] have outperformed other \ufb01lter sizes , we tried various combinations from range", "entities": []}, {"text": "[ 2 , 5 ] .", "entities": []}, {"text": "For activations we used Leaky ReLU , and performed Batch Normalization to stablize the data .", "entities": [[4, 6, "MethodName", "Leaky ReLU"], [9, 11, "MethodName", "Batch Normalization"]]}, {"text": "A.2 CNN LSTM", "entities": [[2, 3, "MethodName", "LSTM"]]}, {"text": "A joint architecture of CNNs and RNNs were proposed in ( Wang et al . , 2016 ) , where the authors tried combination of CNNs with different RNNs like GRUs and LSTMs .", "entities": []}, {"text": "In our experiment , we again used Leaky ReLU for CNNs activations , \ufb01lter size of 3 was \ufb01xed for the experiments to decide the dropout values and other hyperparameters tuning .", "entities": [[7, 9, "MethodName", "Leaky ReLU"]]}, {"text": "A.3 Bi - directional LSTM with maxpool", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "In ( Lai et al . , 2015 ) , authors took Max Over Time on the RNN representation of the input .", "entities": []}, {"text": "Their model RNN outperformed other models in 3 out of 4 datasets .", "entities": []}, {"text": "In our experiments , we \ufb01xed LSTM units to be 51 , and rest of the parameters were decided on the basis of validation - data experiments .", "entities": [[6, 7, "MethodName", "LSTM"]]}, {"text": "105A.4 Hierarchical ConvNets Convolutional Neural Networks are known to perform well on short texts ( Yin et al . , 2017 ) , in ( Conneau et al . , 2017a ) authors proposed to concatenate representation at different levels of input sentence .", "entities": []}, {"text": "The model was claimed to capture hierarchical abstractions of the input sentence .", "entities": []}, {"text": "For our experiments , we \ufb01xed 128 kernels of size 2 , 3 , 4 , 4 at 4 different levels .", "entities": []}, {"text": "These values were decided after the experiments with different number of kernels and their sizes .", "entities": []}, {"text": "A.5 Bi - LSTMS with skip connections In one of our experiments , the summary vector obtained from LSTMs was concatenated with the vector obtained after appying Max Over Time on the hidden state representation of the input .", "entities": []}, {"text": "The intuition behind this was that , by passing most relevant features along with summary of the input to the softmax layer may enhance the clasi\ufb01cation process .", "entities": [[20, 21, "MethodName", "softmax"]]}, {"text": "From the experiments we obtained competetive results using this model .", "entities": []}, {"text": "A.6 Pre - trained LSTMs In ( Dai and Le , 2015 ) , authors claimed that by pretraining LSTMs on some related task as Auto - Encoder or as a Language Model , could optimize the stability of the LSTMs training process .", "entities": []}, {"text": "The authors reported improvenemt in error rates by good margin in many tasks like , text classi\ufb01cation on 20 Newsgroup , IMDB etc .", "entities": [[21, 22, "DatasetName", "IMDB"]]}, {"text": "For our experiments we gathered many related datasets like all of Wikimedia datasets ( Wulczyn et al . , 2017 ) , TRAC shared dataset , IMDB movie reviews dataset .", "entities": [[26, 29, "DatasetName", "IMDB movie reviews"]]}, {"text": "An autoencoder was trained on these datasets and the LSTMs from the encoder part were extracted and used in the classifcation task .", "entities": [[1, 2, "MethodName", "autoencoder"]]}]