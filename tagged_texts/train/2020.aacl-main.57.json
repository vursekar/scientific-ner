[{"text": "Proceedings of the 1st Conference of the Asia - Paci\ufb01c Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 576\u2013581 December 4 - 7 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics576STIL - Simultaneous Slot Filling , Translation , Intent Classi\ufb01cation , and Language Identi\ufb01cation : Initial Results using mBART on MultiATIS++ Jack G. M. FitzGerald Amazon Alexa AI Seattle ,", "entities": [[7, 9, "TaskName", "Slot Filling"], [10, 11, "TaskName", "Translation"], [22, 23, "MethodName", "mBART"]]}, {"text": "WA jgmf@amazon.com", "entities": []}, {"text": "Abstract Slot-\ufb01lling , Translation , Intent classi\ufb01cation , and Language identi\ufb01cation , or STIL , is a newly - proposed task for multilingual Natural Language Understanding ( NLU ) .", "entities": [[3, 4, "TaskName", "Translation"], [23, 26, "TaskName", "Natural Language Understanding"]]}, {"text": "By performing simultaneous slot \ufb01lling and translation into a single output language ( English in this case ) , some portion of downstream system components can be monolingual , reducing development and maintenance cost .", "entities": []}, {"text": "Results are given using the multilingual BART model ( Liu et al . , 2020 ) \ufb01ne - tuned on 7 languages using the MultiATIS++ dataset .", "entities": [[6, 7, "MethodName", "BART"]]}, {"text": "When no translation is performed , mBART \u2019s performance is comparable to the current state of the art system ( Cross - Lingual BERT by Xu et", "entities": [[6, 7, "MethodName", "mBART"], [23, 24, "MethodName", "BERT"]]}, {"text": "al . ( 2020 ) ) for the languages tested , with better average intent classi\ufb01cation accuracy ( 96.07 % versus 95.50 % ) but worse average slot F1 ( 89.87 % versus 90.81 % ) .", "entities": [[16, 17, "MetricName", "accuracy"], [28, 29, "MetricName", "F1"]]}, {"text": "When simultaneous translation is performed , average intent classi\ufb01cation accuracy degrades by only 1.7 % relative and average slot F1 degrades by only 1.2 % relative .", "entities": [[9, 10, "MetricName", "accuracy"], [19, 20, "MetricName", "F1"]]}, {"text": "1 Introduction Multilingual Natural Language Understanding ( NLU ) , also called cross - lingual NLU , is a technique by which an NLU - based system can scale to multiple languages .", "entities": [[3, 6, "TaskName", "Natural Language Understanding"]]}, {"text": "A single model is trained on more than one language , and it can accept input from more than one language during inference .", "entities": []}, {"text": "In most recent high - performing systems , a model is \ufb01rst pre - trained using unlabeled data for all supported languages and then \ufb01ne tuned for a speci\ufb01c task using a small set of labeled data ( Conneau and Lample , 2019 ; Pires et al . , 2019 ) .", "entities": []}, {"text": "Two typical tasks for goal - based systems , such as virtual assistants and chatbots , are intent classi\ufb01cation and slot \ufb01lling ( Gupta et al . , 2006 ) .", "entities": []}, {"text": "Though intent classi\ufb01cation creates a language agnostic output ( the intent of the user ) , slot \ufb01lling does not .", "entities": []}, {"text": "Input \u4ece\u76d0\u6e56\u57ce\u5230\u52a0\u5dde\u5965\u514b\u5170\u7684\u822a\u73ed Traditional Outputintent : \ufb02ight slots : ( \u76d0\u6e56\u57ce , fromloc.cityname ) , . . .", "entities": []}, {"text": "( \u5965\u514b\u5170 , toloc.cityname ) , . . .", "entities": []}, {"text": "( \u52a0\u5dde , toloc.statename ) STIL Outputintent : \ufb02ight slots : ( salt lake city , fromloc.cityname ) , . . .", "entities": []}, {"text": "( oakland , toloc.cityname ) , . . .", "entities": []}, {"text": "( california , toloc.statename ) lang : zh Table 1 : Today \u2019s slot \ufb01lling systems do not translate the slot content , as shown in \u201c Traditional Ouput . \u201d", "entities": []}, {"text": "With a STIL model , the slot content is translated and language identi\ufb01cation is performed .", "entities": []}, {"text": "Instead , a slot-\ufb01lling model outputs the labels for each of input tokens from the user .", "entities": []}, {"text": "Suppose the slot-\ufb01lling model can handle Llanguages .", "entities": []}, {"text": "Downstream components must therefore handle all L languages for the full system to be multilingual acrossLlanguages .", "entities": []}, {"text": "Machine translation could be performed before the slot \ufb01lling model at system runtime , though the latency would be fully additive , and some amount of information useful to the slot\ufb01lling model may be lost .", "entities": [[0, 2, "TaskName", "Machine translation"]]}, {"text": "Similarly , translation could occur after the slot-\ufb01lling model at runtime , but slot alignment between the source and target language is a non - trivial task ( Jain et al . , 2019 ; Xu et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Instead , the goal of this work was to build a single model that can simultaneously translate the input , output slotted text in a single language ( English ) , classify the intent , and classify the input language ( See Table 1 ) .", "entities": []}, {"text": "The STIL task is de\ufb01ned such that the input language tag is not given to the model as input .", "entities": []}, {"text": "Thus , language identi\ufb01cation is necessary so that the system can communicate back to the user in the correct language .", "entities": []}, {"text": "Contributions of this work include ( 1 ) the introduction of a new task for multilingual NLU , namely simultaneous Slot \ufb01lling , Translation , Intent clas-", "entities": [[23, 24, "TaskName", "Translation"]]}, {"text": "577Example Input Example Output \ufb02\u00a8uge von salt lake city nach oakland kaliforniensalt < B - fromloc.city name > lake < I - fromloc.city name > city < I - fromloc.city name > oakland < B-toloc.city name > california < B-toloc.state name >", "entities": []}, {"text": "< intent-\ufb02ight > < lang - de > \u4ece\u76d0\u6e56\u57ce\u5230\u52a0\u5dde\u5965\u514b\u5170 \u7684\u822a\u73edsalt < B - fromloc.city name > lake < I - fromloc.city name > city < I - fromloc.city name > oakland < B-toloc.city name > california < B-toloc.state name >", "entities": []}, {"text": "< intent-\ufb02ight > < lang - zh > Table 2 : Two text - to - text STIL examples .", "entities": []}, {"text": "In all STIL cases , the output is in English .", "entities": []}, {"text": "Each token is followed by its BIO - tagged slot label .", "entities": []}, {"text": "The sequence of tokens and slots are followed by the intent and then the language .", "entities": []}, {"text": "si\ufb01cation , and Language identi\ufb01cation ( STIL ) ; ( 2 ) both non - translated and STIL results using the mBART model ( Liu et al . , 2020 ) trained using a fully text - to - text data format ; and ( 3 ) public release of source code used in this study , with a goal toward reproducibility and future work on the STIL task1 .", "entities": [[21, 22, "MethodName", "mBART"]]}, {"text": "2 Dataset The Airline Travel Information System ( ATIS ) dataset is a classic benchmark for goal - oriented NLU ( Price , 1990 ; Tur et al . , 2010 ) .", "entities": [[8, 9, "DatasetName", "ATIS"]]}, {"text": "It contains utterances focused on airline travel , such as how much is the cheapest \ufb02ight from Boston to New York tomorrow morning ?", "entities": []}, {"text": "The dataset is annotated with 17 intents , though the distribution is skewed , with 70 % of intents being the \ufb02ight intent .", "entities": []}, {"text": "Slots are labeled using the Beginning Inside Outside ( BIO ) format .", "entities": []}, {"text": "ATIS was localized to Turkish and Hindi in 2018 , forming MultiATIS ( Upadhyay et al . , 2018 ) , and then to Spanish , Portuguese , German , French , Chinese , and Japanese in 2020 , forming MultiATIS++ ( Xu et al . , 2020 ) .", "entities": [[0, 1, "DatasetName", "ATIS"]]}, {"text": "In this work , Portuguese was excluded due to a lack of Portuguese pretraining in the publicly available mBART model , and Japanese was excluded due to a current lack of alignment between Japanese and English samples in MultiATIS++ .", "entities": [[18, 19, "MethodName", "mBART"]]}, {"text": "Hindi and Turkish data were taken from MultiATIS , and the training data were upsampled by 3x for Hindi and 7x for Turkish .", "entities": []}, {"text": "Prior to any upsampling , there were 4,488 training samples for English , Spanish , German , French , and Chinese .", "entities": []}, {"text": "The test sets contained 893 samples for all languages except Turkish , which had 715 samples .", "entities": []}, {"text": "For English , Spanish , German , French , and Chinese , validation sets of 490 samples were used in all cases .", "entities": []}, {"text": "Given the smaller data quantities for Hindi and Turkish , two training and validation set con\ufb01gurations were considered .", "entities": []}, {"text": "The \ufb01rst con\ufb01guration 1https://github.com/jgm\ufb01tz/stil-mbart-multiatisppaacl2020matched that of Xu et", "entities": []}, {"text": "al . ( 2020 ) , using training sets of 1,495 for Hindi and 626 for Turkish along with validation sets of 160 for Hindi and 60 for Turkish .", "entities": []}, {"text": "In the second con\ufb01guration , no validation sets were made for Hindi and Turkish ( though there were still validation sets for the other languages ) , and the training sets of 1,600 Hindi samples and 638 samples from MultiATIS were used .", "entities": []}, {"text": "Two output formats are considered , being ( 1 ) the non - translated , traditional case , in which translation of slot content is not performed , and ( 2 ) the translated , STIL case , in which translation of slot content is performed .", "entities": []}, {"text": "In both cases , the tokens , the labels , the intent , and the detected language are all output from the model as a single ordered text sequence , as shown in Table 2 . 3", "entities": []}, {"text": "Related Work Previous approaches for intent classi\ufb01cation and slot \ufb01lling have used either ( 1 ) separate models for slot \ufb01lling , including support vector machines ( Moschitti et al . , 2007 ) , conditional random \ufb01elds ( Xu and Sarikaya , 2014 ) , and recurrent neural networks of various types ( Kurata et al . , 2016 ) or ( 2 ) joint models that diverge into separate decoders or layers for intent classi\ufb01cation and slot \ufb01lling ( Xu and Sarikaya , 2013 ; Guo et al . , 2014 ; Liu and Lane , 2016 ;", "entities": []}, {"text": "Hakkani - T \u00a8ur et", "entities": []}, {"text": "al . , 2016 ) or that share hidden states ( Wang et al . , 2018 ) .", "entities": []}, {"text": "In this work , a fully text - to - text approach similar to that of the T5 model was used , such that the model would have maximum information sharing across the four STIL sub - tasks .", "entities": [[17, 18, "MethodName", "T5"]]}, {"text": "Encoder - decoder models , \ufb01rst introduced in 2014 ( Sutskever et al . , 2014 ) , are a mainstay of neural machine translation .", "entities": [[23, 25, "TaskName", "machine translation"]]}, {"text": "The original transformer model included both an encoder and a decoder ( Vaswani et al . , 2017 ) .", "entities": []}, {"text": "Since then , much of the work on transformers focuses on models with only an encoder pretrained with autoencoding techniques ( e.g. BERT by Devlin et", "entities": [[22, 23, "MethodName", "BERT"]]}, {"text": "al . ( 2018 ) ) or auto - regressive models with only a decoder ( e.g.", "entities": []}, {"text": "578GPT by Radford ( 2018 ) ) .", "entities": []}, {"text": "In this work , it was assumed that encoder - decoder models , such as BART ( Lewis et al . , 2019 ) and T5 ( Raffel et", "entities": [[15, 16, "MethodName", "BART"], [25, 26, "MethodName", "T5"]]}, {"text": "al . , 2019 ) , are the best architectural candidates given the translation component of the STIL task , as well as past state of the art advancement by encoder - decoder models on ATIS , cited above .", "entities": [[35, 36, "DatasetName", "ATIS"]]}, {"text": "Rigorous architectural comparisons are left to future work .", "entities": []}, {"text": "4 The Model 4.1 The Pretrained mBART Model", "entities": [[6, 7, "MethodName", "mBART"]]}, {"text": "The multilingual BART ( mBART ) model architecture was used ( Liu et al . , 2020 ) , as well as the pretrained mBART.cc25 model described in the same paper .", "entities": [[2, 3, "MethodName", "BART"], [4, 5, "MethodName", "mBART"]]}, {"text": "The model consists of 12 encoder layers , 12 decoder layers , a hidden layer size of 1,024 , and 16 attention heads , yielding a parameter count of 680M.", "entities": [[13, 16, "HyperparameterName", "hidden layer size"]]}, {"text": "The mBART.cc25 model was trained on 25 languages for 500k steps using a 1.4 TB corpus of scraped website data taken from Common Crawl ( Wenzek et al . , 2019 ) .", "entities": [[22, 24, "DatasetName", "Common Crawl"]]}, {"text": "The model was trained to reconstruct masked tokens and to rearrange scrambled sentences .", "entities": []}, {"text": "SentencePiece tokenization ( Kudo and Richardson , 2018 ) was used for mBART.cc25 with a sub - word vocabulary size of 250k .", "entities": [[0, 1, "MethodName", "SentencePiece"]]}, {"text": "4.2 This Work The same vocabulary as that of the pretrained model was used for this work , and SentencePiece tokenization was performed on the full sequence , including the slot tags , intent tags , and language tags .", "entities": [[19, 20, "MethodName", "SentencePiece"]]}, {"text": "For all mBART experiments and datasets , data from all languages were shuf\ufb02ed together .", "entities": [[2, 3, "MethodName", "mBART"]]}, {"text": "The fairseq library was used for all experimentation ( Ott et al . , 2019 ) .", "entities": []}, {"text": "Training was performed on 8 Nvidia V100 GPUs ( 16 GB ) using a batch size of 32 , layer normalization for both the encoder and the decoder ( Xu et", "entities": [[14, 16, "HyperparameterName", "batch size"], [19, 21, "MethodName", "layer normalization"]]}, {"text": "al . , 2019 ) ; label smoothed cross entropy with \u000f= 0:2 ( Szegedy et al . , 2016 ) ; the ADAM optimizer with \f 1= 0:9and \f 2= 0:999(Kingma and Ba , 2014 ) ; an initial learning rate of 3\u000210\u00005with polynomial decay over 20,000 updates after 1 epoch of warmup ; attention dropout of 0.1 and dropout of 0.2 elsewhere ; and FP16 type for weights .", "entities": [[23, 24, "DatasetName", "ADAM"], [24, 25, "HyperparameterName", "optimizer"], [40, 42, "HyperparameterName", "learning rate"], [55, 57, "MethodName", "attention dropout"]]}, {"text": "Each model was trained for 19 epochs , which took 5 - 6 hours .", "entities": []}, {"text": "5 Results and Discussion Results from the models are given in Table 3 .", "entities": []}, {"text": "Statistical signi\ufb01cance was evaluated using the Wilsonmethod ( Wilson , 1927 ) with 95 % con\ufb01dence .", "entities": []}, {"text": "5.1 Comparing to Xu et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 )", "entities": []}, {"text": "Examining the \ufb01rst training con\ufb01guration ( 1,496 samples for Hindi and 626 for Turkish ) , the nontranslated mBART \u2019s macro - averaged intent classi\ufb01cation ( 96.07 % ) outperforms Cross - Lingual BERT by Xu et", "entities": [[18, 19, "MethodName", "mBART"], [33, 34, "MethodName", "BERT"]]}, {"text": "al . ( 2020 ) ( 95.50 % ) , but slot F1 is worse ( 89.87 % for non - translated mBART and 90.81 % for Cross - Lingual BERT ) .", "entities": [[12, 13, "MetricName", "F1"], [22, 23, "MethodName", "mBART"], [30, 31, "MethodName", "BERT"]]}, {"text": "The differences are statistically signi\ufb01cant in both cases .", "entities": []}, {"text": "5.2 With and Without Translation When translation is performed ( the STIL task ) , intent classi\ufb01cation accuracy degrades by 1.7 % relative from 96.07 % to 94.40 % , and slot F1 degrades by 1.2 % relative from 89.87 % to 88.79 % .", "entities": [[4, 5, "TaskName", "Translation"], [17, 18, "MetricName", "accuracy"], [32, 33, "MetricName", "F1"]]}, {"text": "The greatest degradation occurred for utterances involving \ufb02ight number , airfare , and airport name ( in that order ) .", "entities": []}, {"text": "5.3 Additional Hindi and Turkish Training Data Adding 105 more Hindi and 12 more Turkish training examples results in improved performance for the translated , STIL mBART model .", "entities": [[26, 27, "MethodName", "mBART"]]}, {"text": "Macro - averaged intent classi\ufb01cation improves from 94.40 % to 95.94 % , and slot F1 improves from 88.79 % to 90.10 % , both of which are statistically signi\ufb01cant .", "entities": [[15, 16, "MetricName", "F1"]]}, {"text": "By adding these 117 samples , the STIL mBART model matches the performance ( within con\ufb01dence intervals ) of the non - translated mBART model .", "entities": [[8, 9, "MethodName", "mBART"], [23, 24, "MethodName", "mBART"]]}, {"text": "This \ufb01nding suggests that the STIL models may require more training data than traditional , non - translated slot \ufb01lling models .", "entities": []}, {"text": "Additionally , by adding more Hindi and Turkish data , both the intent accuracy and the slot \ufb01lling F1 improves for every individual language of the translated , STIL models , suggesting that some portion of the internal , learned representation is language agnostic .", "entities": [[13, 14, "MetricName", "accuracy"], [18, 19, "MetricName", "F1"]]}, {"text": "Finally , the results suggest that there is a trainingsize - dependent performance advantage in using a single output language , as contrasted with the nontranslated mBART model , for which the intent classi\ufb01cation accuracy and slot F1 does not improve ( with statistical signi\ufb01cance ) when using the additional Hindi and Turkish training samples .", "entities": [[26, 27, "MethodName", "mBART"], [34, 35, "MetricName", "accuracy"], [37, 38, "MetricName", "F1"]]}, {"text": "5.4 Language Identi\ufb01cation Language identi\ufb01cation F1 is above 99.7 % for all languages , with perfect performance in many cases .", "entities": [[5, 6, "MetricName", "F1"]]}, {"text": "579Intent accuracy en es de zh fr hi tr Mac Avg Cross - Lingual BERT ( Xu et", "entities": [[1, 2, "MetricName", "accuracy"], [14, 15, "MethodName", "BERT"]]}, {"text": "al . , 2020 ) 97.20 96.77 96.86 95.54 97.24 92.70 tr=149592.20 tr=62695.50 Seq2Seq - Ptr ( Rongali et al . , 2020 ) 97.42 Stack Propagation ( Qin et al . , 2019 ) 97.5 Joint BERT + CRF ( Chen et al . , 2019 ) 97.9 Non - translated mBART , with hi - tr val 96.98 96.98 97.09 96.08 97.65 95.07 tr=149592.73 tr=62696.07 Translated / STIL mBART , with hi - tr val 95.86 94.62 95.63 93.84 95.97 93.84 tr=149591.05 tr=62694.40 Non - translated mBART , no hi - tr val 97.09 97.20 97.20 96.30 97.42 94.74 tr=160094.27 tr=63896.32 Translated / STIL mBART , no hi - tr val 96.98 96.53 96.64 96.42 97.31 94.85 tr=160092.87 tr=63895.94 Slot F1 en es de zh fr hi tr Mac Avg Bi - RNN ( Upadhyay et al . , 2018 ) 95.2 80.6 tr=60078.9 tr=60084.90 Cross - Lingual BERT ( Xu et", "entities": [[13, 14, "MethodName", "Seq2Seq"], [15, 16, "DatasetName", "Ptr"], [37, 38, "MethodName", "BERT"], [39, 40, "MethodName", "CRF"], [52, 53, "MethodName", "mBART"], [70, 71, "MethodName", "mBART"], [88, 89, "MethodName", "mBART"], [106, 107, "MethodName", "mBART"], [122, 123, "MetricName", "F1"], [150, 151, "MethodName", "BERT"]]}, {"text": "al . , 2020 ) 95.90 87.95 95.00 93.67 90.39 86.73 tr=149586.04 tr=62690.81 Stack Propagation ( Qin et al . , 2019 ) 96.1 Joint BERT ( Chen et al . , 2019 ) 96.1 Non - translated mBART , with hi - tr val 95.03 86.76 94.42 92.13 89.31 86.91 tr=149584.53 tr=62689.87 Translated / STIL mBART , with hi - tr val 93.81 90.38 91.41 85.93 91.24 83.98 tr=149584.79 tr=62688.79 Non - translated mBART , no hi - tr val 95.00 86.87 94.14 92.22 89.32 87.42 tr=160084.33 tr=63889.90 Translated / STIL mBART , no hi - tr val 94.66 91.55 92.61 87.73 92.15 86.74 tr=160085.23 tr=63890.10 Language Identi\ufb01cation F1 en es de zh fr hi tr Mac Avg Translated / STIL mBART , with hi - tr val 100.00 98.87 100.00 100.00 98.95 100.00 99.93 99.68 Translated / STIL mBART , no hi - tr val 99.78 99.83 100.00 100.00 99.72 100.00 99.86 99.88 Table 3 : Results are shown for intent accuracy , slot F1 score , and language identi\ufb01cation F1 score .", "entities": [[25, 26, "MethodName", "BERT"], [38, 39, "MethodName", "mBART"], [56, 57, "MethodName", "mBART"], [74, 75, "MethodName", "mBART"], [92, 93, "MethodName", "mBART"], [109, 110, "MetricName", "F1"], [122, 123, "MethodName", "mBART"], [140, 141, "MethodName", "mBART"], [163, 164, "MetricName", "accuracy"], [166, 168, "MetricName", "F1 score"], [172, 174, "MetricName", "F1 score"]]}, {"text": "For English , Spanish , German , Chinese , and French in all of the models shown above ( including other work ) , training sets were between 4,478 and 4,488 samples , and validation sets were between 490 and 500 samples .", "entities": []}, {"text": "In this work , two training set sizes were used for Hindi and Turkish , denoted by \u201c tr= \u201d and \u201c with hi - tr val[idation set ] \u201d or \u201c no hi - tr val[idation set ] \u201d .", "entities": []}, {"text": "Across all work shown above , the tests sets contained 893 samples for all languages except Turkish , for which the test set was 715 samples .", "entities": []}, {"text": "Perfect performance on Chinese and Hindi is unsurprising given their unique scripts versus the other languages tested .", "entities": []}, {"text": "6 Conclusion This preliminary work demonstrates that a single NLU model can perform simultaneous slot \ufb01lling , translation , intent classi\ufb01cation , and language identi\ufb01cation across 7 languages using MultiATIS++ .", "entities": []}, {"text": "Such an NLU model would negate the need for multiple - language support in some portion of downstream system components .", "entities": []}, {"text": "Performance is not irreconcilably worse than traditional slot-\ufb01lling models , and performance is statistically equivalent with a small amount of additional training data .", "entities": []}, {"text": "Looking forward , a more challenging dataset is needed to further develop the translation compo - nent of the STIL task .", "entities": []}, {"text": "The English MultiATIS++ test set only contains 455 unique entity - slot pairs .", "entities": []}, {"text": "An ideal future dataset would include freeform and varied content , such as text messages , song titles , or open - domain questions .", "entities": []}, {"text": "Until then , work remains to achieve parity with English - only ATIS models .", "entities": [[12, 13, "DatasetName", "ATIS"]]}, {"text": "Acknowledgments The author would like to thank Saleh Soltan , Gokhan Tur , Saab Mansour , and Batool Haider for reviewing this work and providing valuable feedback .", "entities": []}, {"text": "References Qian Chen , Zhu Zhuo , and Wen Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bert for joint intent classi\ufb01cation and slot \ufb01lling .", "entities": []}, {"text": "ArXiv , abs/1902.10909 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "580Alexis Conneau and Guillaume Lample .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crosslingual language model pretraining .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 32 , pages 7059\u20137069 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "Daniel ( Zhaohan ) Guo , Gokhan Tur , Scott Wen - tau Yih , and Geoffrey Zweig .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Joint semantic utterance classi\ufb01cation and slot \ufb01lling with recursive neural networks .", "entities": []}, {"text": "In 2014 IEEE Spoken Language Technology Workshop ( SLT 2014 ) .", "entities": []}, {"text": "IEEE - Institute of Electrical and Electronics Engineers .", "entities": []}, {"text": "N. Gupta , G. Tur , D. Hakkani - Tur , S. Bangalore , G. Riccardi , and M. Gilbert .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "The at t spoken language understanding system .", "entities": [[3, 6, "TaskName", "spoken language understanding"]]}, {"text": "IEEE Transactions on Audio , Speech , and Language Processing , 14(1):213\u2013222 .", "entities": []}, {"text": "Dilek Hakkani - T \u00a8ur , Gokhan Tur , Asli Celikyilmaz , Yun - Nung Vivian Chen , Jianfeng Gao , Li Deng , and Ye - Yi Wang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Multi - domain joint semantic frame parsing using bi - directional rnn - lstm .", "entities": [[13, 14, "MethodName", "lstm"]]}, {"text": "In Proceedings of The 17th Annual Meeting of the International Speech Communication Association ( INTERSPEECH 2016 ) .", "entities": []}, {"text": "ISCA .", "entities": []}, {"text": "Alankar Jain , Bhargavi Paranjape , and Zachary C. Lipton .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Entity projection via machine translation for cross - lingual NER .", "entities": [[3, 5, "TaskName", "machine translation"], [6, 10, "TaskName", "cross - lingual NER"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 1083\u20131092 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "Taku Kudo and John Richardson .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Sentencepiece :", "entities": [[0, 1, "MethodName", "Sentencepiece"]]}, {"text": "A simple and language independent subword tokenizer and detokenizer for neural text processing .", "entities": []}, {"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing : System Demonstrations .", "entities": []}, {"text": "Gakuto Kurata , Bing Xiang , Bowen Zhou , and Mo Yu . 2016 .", "entities": []}, {"text": "Leveraging sentence - level information with encoder lstm for semantic slot \ufb01lling .", "entities": [[7, 8, "MethodName", "lstm"]]}, {"text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Ves Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bart : Denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension .", "entities": [[2, 3, "TaskName", "Denoising"]]}, {"text": "Bing Liu and Ian Lane . 2016 .", "entities": []}, {"text": "Attention - based recurrent neural network models for joint intent detection and slot \ufb01lling .", "entities": [[9, 11, "TaskName", "intent detection"]]}, {"text": "Interspeech 2016 .Yinhan", "entities": []}, {"text": "Liu , Jiatao Gu , Naman Goyal , Xian Li , Sergey Edunov , Marjan Ghazvininejad , Mike Lewis , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multilingual denoising pre - training for neural machine translation", "entities": [[1, 2, "TaskName", "denoising"], [7, 9, "TaskName", "machine translation"]]}, {"text": ".", "entities": []}, {"text": "Alessandro Moschitti , Giuseppe Riccardi , and Christian Raymond .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Spoken language understanding with kernels for syntactic / semantic structures .", "entities": [[0, 3, "TaskName", "Spoken language understanding"]]}, {"text": "2007 IEEE Workshop on Automatic Speech Recognition and Understanding ( ASRU ) , pages 183\u2013188 .", "entities": [[4, 7, "TaskName", "Automatic Speech Recognition"]]}, {"text": "Myle Ott , Sergey Edunov , Alexei Baevski , Angela Fan , Sam Gross , Nathan Ng , David Grangier , and Michael Auli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "fairseq :", "entities": []}, {"text": "A fast , extensible toolkit for sequence modeling .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics ( Demonstrations ) , pages 48\u201353 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Telmo Pires , Eva Schlinger , and Dan Garrette .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "How multilingual is multilingual BERT ?", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4996 \u2013 5001 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "P. J. Price .", "entities": []}, {"text": "1990 .", "entities": []}, {"text": "Evaluation of spoken language systems : the ATIS domain .", "entities": [[7, 8, "DatasetName", "ATIS"]]}, {"text": "In Speech and Natural Language : Proceedings of a Workshop Held at Hidden Valley , Pennsylvania , June 24 - 27,1990 .", "entities": []}, {"text": "Libo Qin , Wanxiang Che , Yangming Li , Haoyang Wen , and Ting Liu . 2019 .", "entities": []}, {"text": "A stack - propagation framework with token - level intent detection for spoken language understanding .", "entities": [[9, 11, "TaskName", "intent detection"], [12, 15, "TaskName", "spoken language understanding"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2078\u20132087 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training .", "entities": []}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J. Liu . 2019 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the limits of transfer learning with a uni\ufb01ed text - to - text transformer .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "Subendhu Rongali , Luca Soldaini , Emilio Monti , and Wael Hamza . 2020 .", "entities": []}, {"text": "Do n\u2019t parse , generate !", "entities": []}, {"text": "a sequence to sequence architecture for task - oriented semantic parsing .", "entities": [[1, 4, "MethodName", "sequence to sequence"], [9, 11, "TaskName", "semantic parsing"]]}, {"text": "In Proceedings of The Web Conference 2020 , WWW \u2019 20 , page 2962\u20132968 , New York , NY , USA .", "entities": []}, {"text": "Association for Computing Machinery .", "entities": []}, {"text": "Ilya Sutskever , Oriol Vinyals , and Quoc V .", "entities": []}, {"text": "Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "InProceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2 , NIPS\u201914 , page 3104\u20133112 , Cambridge , MA , USA . MIT Press .", "entities": [[20, 21, "DatasetName", "Cambridge"]]}, {"text": "581Christian Szegedy , Vincent Vanhoucke , Sergey Ioffe , Jon Shlens , and Zbigniew Wojna .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Rethinking the inception architecture for computer vision .", "entities": []}, {"text": "2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 2818\u20132826 .", "entities": []}, {"text": "Gokhan Tur , Dilek Z. Hakkani - T \u00a8ur , and Larry Heck . 2010 .", "entities": []}, {"text": "What is left to be understood in atis ?", "entities": [[7, 8, "DatasetName", "atis"]]}, {"text": "2010 IEEE Spoken Language Technology Workshop , pages 19\u201324 .", "entities": []}, {"text": "Shyam Upadhyay , Manaal Faruqui , Gokhan Tur , Dilek Hakkani - T \u00a8ur , and Larry Heck .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "( almost ) zeroshot cross - lingual spoken language understanding .", "entities": [[7, 10, "TaskName", "spoken language understanding"]]}, {"text": "InProceedings of the IEEE ICASSP .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , unde\ufb01nedukasz Kaiser , and Illia Polosukhin .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Proceedings of the 31st International Conference on Neural Information Processing Systems , NIPS\u201917 , page 6000\u20136010 , Red Hook , NY , USA .", "entities": []}, {"text": "Curran Associates Inc.", "entities": []}, {"text": "Yu Wang , Yilin Shen , and Hongxia Jin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A bimodel based RNN semantic frame parsing model for intent detection and slot \ufb01lling .", "entities": [[9, 11, "TaskName", "intent detection"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 309\u2013314 , New Orleans , Louisiana . Association for Computational Linguistics .", "entities": []}, {"text": "Guillaume Wenzek , Marie - Anne Lachaux , Alexis Conneau , Vishrav Chaudhary , Francisco Guzm \u00b4 an , Armand Joulin , and Edouard Grave .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Ccnet :", "entities": [[0, 1, "MethodName", "Ccnet"]]}, {"text": "Extracting high quality monolingual datasets from web crawl data .", "entities": []}, {"text": "Edwin B. Wilson .", "entities": []}, {"text": "1927 .", "entities": []}, {"text": "Probable inference , the law of succession , and statistical inference .", "entities": []}, {"text": "Journal of the American Statistical Association , 22(158):209\u2013212 .", "entities": []}, {"text": "Jingjing Xu , Xu Sun , Zhiyuan Zhang , Guangxiang Zhao , and Junyang Lin .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Understanding and improving layer normalization .", "entities": [[3, 5, "MethodName", "layer normalization"]]}, {"text": "In Advances in Neural Information Processing Systems 32 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "P. Xu and R. Sarikaya .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Convolutional neural network based triangular crf for joint intent detection and slot \ufb01lling .", "entities": [[5, 6, "MethodName", "crf"], [8, 10, "TaskName", "intent detection"]]}, {"text": "In 2013 IEEE Workshop on Automatic Speech Recognition and Understanding , pages 78\u201383 .", "entities": [[5, 8, "TaskName", "Automatic Speech Recognition"]]}, {"text": "Puyang Xu and Ruhi Sarikaya .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Targeted feature dropout for robust slot \ufb01lling in natural language understanding .", "entities": [[8, 11, "TaskName", "natural language understanding"]]}, {"text": "ISCA - International Speech Communication Association .", "entities": []}, {"text": "Weijia Xu , Batool Haider , and Saab Mansour . 2020 .", "entities": []}, {"text": "End - to - end slot alignment and recognition for crosslingual nlu .", "entities": []}]