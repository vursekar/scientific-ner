[{"text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1202\u20131217 July 5 - 10 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics1202Learning and Evaluating Emotion Lexicons for 91 Languages Sven Buechel , Susanna R \u00a8ucker , and Udo Hahn fsven.buechel|susanna.ruecker|udo.hahn g@uni-jena.de Jena University Language and Information Engineering ( JULIE ) Lab Friedrich - Schiller - Universit \u00a8at Jena , Jena , Germany https://julielab.de Abstract Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis .", "entities": [[36, 37, "DatasetName", "Schiller"], [64, 65, "DatasetName", "emotion"]]}, {"text": "Yet , manually curated lexicons are only available for a handful of languages , leaving most languages of the world without such a precious resource for downstream applications .", "entities": []}, {"text": "Even worse , their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature .", "entities": []}, {"text": "In order to break this bottleneck , we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language .", "entities": [[17, 18, "DatasetName", "emotion"]]}, {"text": "Our approach requires nothing but a source language emotion lexicon , a bilingual word translation model , and a target language embedding model .", "entities": [[8, 9, "DatasetName", "emotion"], [13, 15, "TaskName", "word translation"]]}, {"text": "Ful\ufb01lling these requirements for 91 languages , we are able to generate representationally rich high - coverage lexicons comprising eight emotional variables with more than 100k lexical entries each .", "entities": []}, {"text": "We evaluated the automatically generated lexicons against human judgment from 26 datasets , spanning 12 typologically diverse languages , and found that our approach produces results in line with state - of - the - art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables .", "entities": []}, {"text": "Code and data are available at github.com/JULIELab/MEmoLon archived under DOI 10.5281 / zenodo.3779901 .", "entities": []}, {"text": "1 Introduction An emotion lexicon is a lexical repository which encodes the affective meaning of individual words ( lexical entries ) .", "entities": [[3, 4, "DatasetName", "emotion"]]}, {"text": "Most simply , affective meaning can be encoded in terms of polarity , i.e. , the distinction whether an item is considered as positive , negative , or neutral .", "entities": []}, {"text": "This is the case for many well - known resources such as WORDNET - AFFECT ( Strapparava and Valitutti , 2004 ) ,", "entities": []}, {"text": "SENTI WORDNET(Baccianella et al . , 2010 ) , or VADER ( Huttoand Gilbert , 2014 ) .", "entities": []}, {"text": "Yet , an increasing number of researchers focus on more expressive encodings for affective states inspired by distinct lines of work in psychology ( Yu et al . , 2016 ; Buechel and Hahn , 2017 ; Sedoc et al . , 2017 ; Abdul - Mageed and Ungar , 2017 ; Bostan and Klinger , 2018 ; Mohammad , 2018 ; Troiano et al . , 2019 ) .", "entities": []}, {"text": "Psychologists , on the one hand , value such lexicons as a controlled set of stimuli for designing experiments , e.g. , to investigate patterns of lexical access or the structure of memory ( Hofmann et al . , 2009 ; Monnier and Syssau , 2008 ) .", "entities": []}, {"text": "NLP researchers , on the other hand , use them to augment the emotional loading of word embeddings ( Yu et al . , 2017 ; Khosla et al . , 2018 ) , as additional input to sentence - level emotion models so that the performance of even the most sophisticated neural network gets boosted ( Mohammad and Bravo - Marquez , 2017 ; Mohammad et al . , 2018 ; De Bruyne et al . , 2019 ) , or rely on them in a keyword - spotting approach when no training data is available , e.g. , for studies dealing with historical language stages ( Buechel et al . , 2016 ) .", "entities": [[16, 18, "TaskName", "word embeddings"], [41, 42, "DatasetName", "emotion"]]}, {"text": "As with any kind of manually curated resource , the availability of emotion lexicons is heavily restricted to only a few languages whose exact number varies depending on the variables under scrutiny .", "entities": [[12, 13, "DatasetName", "emotion"]]}, {"text": "For example , we are aware of lexicons for 15 languages that encode the emotional variables of Valence , Arousal , and Dominance ( see Section 2 ) .", "entities": []}, {"text": "This number leaves the majority of the world \u2019s ( less - resourced ) languages without such a dataset .", "entities": []}, {"text": "In case such a lexicon exists for a particular language , it is often severely limited in size , sometimes only comprising some hundreds of entries ( Davidson and Innes - Ker , 2014 ) .", "entities": []}, {"text": "Yet , even the largest lexicons typically cover only some ten thousands of words , still leaving out major portions of the emotion - carrying vocabulary .", "entities": [[22, 23, "DatasetName", "emotion"]]}, {"text": "This is especially true for languages with complex morphology or", "entities": []}, {"text": "1203productive compounding , such as Finnish , Turkish , Czech , or German .", "entities": []}, {"text": "Finally , the diversity of emotion representation schemes adds another layer of complexity .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "While psychologists and NLP researchers alike \ufb01nd that different sets of emotional variables are complementary to each other ( Stevenson et al . , 2007 ; Pinheiro et al . , 2017 ; Barnes et al . , 2019 ; De Bruyne et al . , 2019 ) , manually creating emotion lexicons for every language and every emotion representation scheme is virtually impossible .", "entities": [[51, 52, "DatasetName", "emotion"], [58, 59, "DatasetName", "emotion"]]}, {"text": "We here propose an approach based on crosslingual distant supervision to generate almost arbitrarily large emotion lexicons for any target language and emotional variable , provided the following requirements are met : a source language emotion lexicon covering the desired variables , a bilingual word translation model , and a target language embedding model .", "entities": [[15, 16, "DatasetName", "emotion"], [35, 36, "DatasetName", "emotion"], [44, 46, "TaskName", "word translation"]]}, {"text": "By ful\ufb01lling these preconditions , we can automatically generate emotion lexicons for 91 languages covering ratings for eight emotional variables and hundreds of thousands of lexical entries each .", "entities": [[9, 10, "DatasetName", "emotion"]]}, {"text": "Our experiments reveal that our method is on a par with state - of - the - art monolingual approaches and compares favorably with ( sometimes even outperforms ) human reliability .", "entities": []}, {"text": "2 Related Work Representing Emotion .", "entities": []}, {"text": "Whereas research in NLP has focused for a very long time almost exclusively on polarity , more recently , there has been a growing interest in more informative representation structures for affective states by including different groups of emotional variables ( Bostan and Klinger , 2018 ) .", "entities": []}, {"text": "Borrowing from distinct schools of thought in psychology , these variables can typically be subdivided into dimensional vs.discrete approaches to emotion representation ( Calvo and Mac Kim , 2013 ) .", "entities": [[20, 21, "DatasetName", "emotion"]]}, {"text": "The dimensional approach assumes that emotional states can be composed out of several foundational factors , most noticeably Valence ( corresponding to polarity ) , Arousal ( measuring calmness vs. excitement ) , and Dominance ( the perceived degree of control in a social situation ) ; V AD , for short ( Bradley and Lang , 1994 ) .", "entities": []}, {"text": "Conversely , the discrete approach assumes that emotional states can be reduced to a small , evolutionary motivated set of basic emotions ( Ekman , 1992 ) .", "entities": []}, {"text": "Although the exact division of the set has been subject of hot debates , recently constructed datasets ( see Section 4 ) most often cover the categories of Joy , Anger , Sadness , Fear , and Disgust ; BE5 , forshort .", "entities": []}, {"text": "Plutchik \u2019s Wheel of Emotion takes a middle ground between those two positions by postulating emotional categories which are yet grouped into opposite pairs along different levels of intensity ( Plutchik , 1980 ) .", "entities": []}, {"text": "Another dividing line between representational approaches is whether target variables are encoded in terms of ( strict ) class - membership or scores for numerical strength .", "entities": []}, {"text": "In the \ufb01rst case , emotion analysis translates into a ( multi - class ) classi\ufb01cation problem , whereas the latter turns it into a regression problem ( Buechel and Hahn , 2016 ) .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "While our proposed methodology is agnostic towards the chosen emotion format , we will focus on the V AD and BE5 formats here , using numerical ratings ( see the examples in Table 1 ) due to the widespread availability of such data .", "entities": [[9, 10, "DatasetName", "emotion"]]}, {"text": "Accordingly , this paper treats word emotion prediction as a regression problem .", "entities": [[6, 7, "DatasetName", "emotion"]]}, {"text": "Val Aro Dom Joy Ang Sad Fea Dis sunshine 8.1 5.3 5.4 4.2 1.2 1.3 1.3 1.2 terrorism 1.6 7.4 2.7 1.2 2.9 3.3 3.9 2.5 nuclear 4.3 7.3 4.1 1.4 2.2 1.9 3.2 1.6 ownership 5.9 4.4 7.5 2.1 1.4 1.2 1.4 1.3 Table 1 : Sample entries from our English source lexicon described via eight emotional variables : Valence , Arousal , Dom inance", "entities": []}, {"text": "[ V AD ] , and Joy , Anger , Sadness , Fear , and Disgust [ BE5 ] .", "entities": []}, {"text": "V AD uses 1 - to-9 scales ( \u201c 5 \u201d encodes the neutral value ) and BE5 1 - to-5 scales ( \u201c 1 \u201d encodes the neutral value ) .", "entities": []}, {"text": "Building Emotion Lexicons .", "entities": []}, {"text": "Usually , the ground truth for affective word ratings ( i.e. , the assignment of emotional values to a lexical item ) is acquired in a questionnaire study design where subjects ( annotators ) receive lists of words which they rate according to different emotion variables or categories .", "entities": [[44, 45, "DatasetName", "emotion"]]}, {"text": "Aggregating individual ratings of multiple annotators then results in the \ufb01nal emotion lexicon ( Bradley and Lang , 1999 ) .", "entities": [[11, 12, "DatasetName", "emotion"]]}, {"text": "Recently , this work\ufb02ow has often been enhanced by crowdsourcing ( Mohammad and Turney , 2013 ) and best - worst scaling ( Kiritchenko and Mohammad , 2016 ) .", "entities": []}, {"text": "As a viable alternative to manual acquisition , such lexicons can also be created by automatic means ( Bestgen , 2008 ; K \u00a8oper and Schulte i m Walde , 2016 ; Shaikh et al . , 2016 ) , i.e. , by learning to predict emotion labels for unseen words .", "entities": [[46, 47, "DatasetName", "emotion"]]}, {"text": "Researchers have worked on this prediction problem for quite a long time .", "entities": []}, {"text": "Early work tended to focus on word statistics , often in combination with linguistic rules ( Hatzivassiloglou and McKeown ,", "entities": []}, {"text": "12041997 ; Turney and Littman , 2003 )", "entities": []}, {"text": ".", "entities": []}, {"text": "More recent approaches focus heavily on word embeddings , either using semi - supervised graph - based approaches ( Wang et al . , 2016 ; Hamilton et al . , 2016 ; Sedoc et al . , 2017 ) or fully supervised methods ( Rosenthal et al . , 2015 ; Li et al . , 2017 ; Rothe et al . , 2016 ; Du and Zhang , 2016 ) .", "entities": [[6, 8, "TaskName", "word embeddings"]]}, {"text": "Most important for this work , Buechel and Hahn ( 2018b ) report on near - human performance using a combination of FAST TEXT vectors and a multi - task feed - forward network ( see Section 4 ) .", "entities": []}, {"text": "While this line of work can add new words , it does not extend lexicons to other emotional variables or languages .", "entities": []}, {"text": "A relatively new way of generating novel labels isemotion representation mapping ( ERM ) , an annotation projection that translates ratings from one emotion format into another , e.g. , mapping V AD labels into BE5 , or vice versa ( Hoffmann et al . , 2012 ; Buechel and Hahn , 2016 , 2018a ; Alarc \u02dcao and Fonseca , 2017 ; Landowska , 2018 ;", "entities": [[23, 24, "DatasetName", "emotion"]]}, {"text": "Zhou et", "entities": []}, {"text": "al . , 2020 ; Park et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "While our work uses ERM to add additional emotion variables to the source lexicon , ERM alone can neither increase the coverage of a lexicon , nor adapt it to another language .", "entities": [[8, 9, "DatasetName", "emotion"]]}, {"text": "Translating Emotions .", "entities": []}, {"text": "The approach we propose is strongly tied to the observation by Leveau et al .", "entities": []}, {"text": "( 2012 ) and Warriner et al .", "entities": []}, {"text": "( 2013 ) who found \u2014 comparing a large number of existing emotion lexicons of different languages \u2014 that translational equivalents of words show strong stability and adherence to their emotional value .", "entities": [[12, 13, "DatasetName", "emotion"]]}, {"text": "Yet , their work is purely descriptive .", "entities": []}, {"text": "They do not exploit their observation to create new ratings , and only consider manual rather than automatic translation .", "entities": []}, {"text": "Making indirect use of this observation , Mohammad and Turney ( 2013 ) offer machine - translated versions of their NRC Emotion Lexicon .", "entities": []}, {"text": "Also , many approaches in cross - lingual sentiment analysis ( on the sentence - level ) rely on translating polarity lexicons ( Abdalla and Hirst , 2017 ; Barnes et al . , 2018 ) .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}, {"text": "Perhaps most similar to our work , Chen and Skiena ( 2014 ) create ( polarity - only ) lexicons for 136 languages by building a multilingual word graph and propagating sentiment labels through that graph .", "entities": []}, {"text": "Yet , their method is restricted to high frequency words \u2014 their lexicons cover between 12 and 4,653 entries , whereas our approach exceeds this limit by more than two orders of magnitude .", "entities": []}, {"text": "Our methodology also resembles previous work which models word emotion for historical language stages ( Cook and Stevenson , 2010 ; Hamilton et al . ,2016 ;", "entities": [[9, 10, "DatasetName", "emotion"]]}, {"text": "Hellrich et al . , 2018 ; Li et al . , 2019 ) .", "entities": []}, {"text": "Work in this direction typically comes up with a set of seed words with assumingly temporally stable affective meaning ( our work assumes stability against translation ) and then uses distributional methods to derive emotion ratings in the target language stage .", "entities": [[34, 35, "DatasetName", "emotion"]]}, {"text": "However , gold data for the target language ( stage ) is usually inaccessible , often preventing evaluation against human judgment .", "entities": []}, {"text": "In contrast , we here propose several alternative evaluation set - ups as an integral part of our methodology .", "entities": []}, {"text": "3", "entities": []}, {"text": "A Novel Approach to Lexicon Creation Our methodology integrates ( 1 ) cross - lingual generation and expansion of emotion lexicons and ( 2 ) their evaluation against gold and silver standard data .", "entities": [[19, 20, "DatasetName", "emotion"]]}, {"text": "Consequently , a key aspect of our work\ufb02ow design is how data is split into train , dev , and test sets at different points of the generation process .", "entities": []}, {"text": "Figure 1 gives an overview of our framework including a toy example for illustration .", "entities": []}, {"text": "Lexicon Generation .", "entities": []}, {"text": "We start with a lexicon ( Source ) of arbitrary size , emotion format1and source language which is partitioned into train , dev , and test splits denoted by Source - train , Source - dev , andSource - test , respectively .", "entities": [[12, 13, "DatasetName", "emotion"]]}, {"text": "Next , we leverage a bilingual word translation model between source and desired target language to build the \ufb01rst target - side emotion lexicon denoted as TargetMT .", "entities": [[6, 8, "TaskName", "word translation"], [22, 23, "DatasetName", "emotion"]]}, {"text": "Source words are translated according to the model , whereas target - side emotion labels are simply copied from the source to the target ( see Section 2 ) .", "entities": [[13, 14, "DatasetName", "emotion"]]}, {"text": "Entries are assigned to train , dev , or test set according to their source - side assignment ( cf .", "entities": []}, {"text": "Figure 1 ) .", "entities": []}, {"text": "The choice of our translation service ( see below ) ensures that each source word receives exactly one translation .", "entities": []}, {"text": "TargetMT is then used as the distant supervisor to train a model that predicts word emotions based on target - side word embeddings .", "entities": [[21, 23, "TaskName", "word embeddings"]]}, {"text": "TargetMT - train and TargetMT - dev are used to \ufb01t model parameters and optimize hyperparameters , respectively , whereas TargetMT - test is held out for later evaluation .", "entities": []}, {"text": "Once \ufb01nalized , the model is used to predict new labels for the words in TargetMT , resulting in a second target - side emotion lexicon denoted TargetPred .", "entities": [[24, 25, "DatasetName", "emotion"]]}, {"text": "Our rationale for doing so is that a reasonably trained model should generalize well 1This encompasses not only V A(D ) and BE5 , but also any sort of ( real - valued ) polarity encodings .", "entities": []}, {"text": "1205 fit predict   silver evaluation gold evaluationSource   train   ( sunshine , ( 8.1 , 5.3 ) )   dev ( nuclear , ( 4.3 , 7.3 ) )   test   ( terrorism , ( 1.6 , 7.4 ) )", "entities": []}, {"text": "TargetMT   train   ( Sonnenschein , ( 8.1 , 5.3 ) )   dev ( nuklear , ( 4.3 , 7.3 ) )   test   ( T errorismus , ( 1.6 , 7.4 ) )", "entities": []}, {"text": "( T errorismus , ( 1.9 , 7.5 ) )   ( Erdbeben , ( 1.4 , 7.3 ) )", "entities": []}, {"text": "TargetGold TargetPred   train   ( Sonnenschein , ( 6.6 , 4.1 ) )   dev   ( nuklear , ( 2.7 , 5.3 ) )   test   ( T errorismus , ( 2.4 , 5.9 ) )   ( Erdbeben , ( 2.7 , 6.1 ) )   ( V ernunft , ( 5.6 , 4.2 ) )   ( langsam , ( 4.3 , 4.5 ) )", "entities": []}, {"text": "translate   model develop Erdbeben   V ernunft   langsam embeddings Figure 1 : Schematic view on the methodology for generating and evaluating an emotion lexicon for a given target language based on source language supervision .", "entities": [[24, 25, "DatasetName", "emotion"]]}, {"text": "Included is a toy example starting with an English V A lexicon ( sunshine , nuclear , terrorism and the associated numerical scores for Valence and Arousal ) and resulting in an extended German lexicon which incorporates translated entries with altered V A scores and additional entries originating from the embedding model with newly learned scores .", "entities": []}, {"text": "over the entire TargetMT lexicon because it has access to the target - side embedding vectors .", "entities": []}, {"text": "Hence , it may mitigate some of the errors which were introduced in previous steps , either by machine translation or by assuming that sourceand target - side emotion are always identical .", "entities": [[18, 20, "TaskName", "machine translation"], [28, 29, "DatasetName", "emotion"]]}, {"text": "We validate this assumption in Section 6 .", "entities": []}, {"text": "We also predict ratings for allthe words in the embedding model , leading to a large number of new entries .", "entities": []}, {"text": "The splits are de\ufb01ned as follows : let MT train , MT dev , and MT testdenote the set of words in train , dev , and test split of TargetMT , respectively .", "entities": []}, {"text": "Likewise , let Ptrain , Pdev , andPtestdenote the splits of TargetPred and let Edenote the set of words in the embedding model .", "entities": []}, {"text": "Then Ptrain : = MT train Pdev :", "entities": []}, {"text": "= MT devnMT train Ptest : = ( MT test[E)n(MT dev[MT train )", "entities": []}, {"text": "The above de\ufb01nitions help clarify the way we address polysemy.2Ambiguity on the target - side 2In short , our work evades this problem by dealing with lexical entries exclusively on the type- rather than the senselevel .", "entities": []}, {"text": "From a lexicological perspective , this may seem like a strong assumption .", "entities": []}, {"text": "From a modeling perspective , however , it appears almost obvious as it aligns well with the major components of our methodology , i.e. , lexicons , embeddings , and translation .", "entities": []}, {"text": "The lexicons we work with follow the design of behavioral experiments : a stimulus ( word type ) is given tomay result in multiple source entries translating to the same target - side word.3This circumstance leads to \u201c partial duplicates \u201d in TargetMT , i.e. , groups of entries with the same word type but different emotion values ( because they were derived from distinct Source entries ) .", "entities": [[56, 57, "DatasetName", "emotion"]]}, {"text": "Such overlap could do harm to the integrity of our evaluation since knowledge may \u201c leak \u201d from training to validation phase , i.e. , by testing the model on words it has already seen during training , although with distinct emotion labels .", "entities": [[41, 42, "DatasetName", "emotion"]]}, {"text": "The proposed data partitioning eliminates such distortion effects .", "entities": []}, {"text": "Since partial duplicates receive the same embedding vector , the prediction model assigns the same emotion value to both , thus merging them in TargetPred .", "entities": [[15, 16, "DatasetName", "emotion"]]}, {"text": "Evaluation Methodology .", "entities": []}, {"text": "The main advantage of the above generation method is that it allows us to create large - scale emotion lexicons for languages a subject and the response ( rating ) is recorded .", "entities": [[18, 19, "DatasetName", "emotion"]]}, {"text": "The absence of sense - level annotation simpli\ufb01es the mapping between lexicon and embedding entries .", "entities": []}, {"text": "While sense embeddings form an active area of research ( Camacho - Collados and Pilehvar , 2018 ; Chi and Chen , 2018 ) , to the best of our knowledge , type - level embeddings yield state - of - the - art performance in downstream applications .", "entities": []}, {"text": "3Source - side polysemy , in contrast to its target - side counterpart , is less of a problem , because we receive only a single candidate during translation .", "entities": []}, {"text": "This may result in cases where the translation misaligns with the copied emotion value in TargetMT .", "entities": [[12, 13, "DatasetName", "emotion"]]}, {"text": "Yet , the prediction step partly mitigates such inconsistencies ( see Section 6 ) .", "entities": []}, {"text": "1206for which gold data is lacking .", "entities": []}, {"text": "But if that is the case , how can we assess the quality of the generated lexicons ?", "entities": []}, {"text": "Our solution is to propose two different evaluation scenarios \u2014 a gold evaluation which is a strict comparison against human judgment , meaning that it is limited to languages where such data ( denoted TargetGold ) is available , and a silver evaluation which substitutes human judgments by automatically derived ones ( silver standard ) which is feasible for any language in our study .", "entities": []}, {"text": "The rationale is that if both , gold and silver evaluation , strongly agree with each other , we can use one as proxy for the other when no target - side gold data exists ( examined in Section 6 ) .", "entities": []}, {"text": "Note that our lexicon generation approach consists of two major steps , translation andprediction .", "entities": []}, {"text": "However , these two steps are not equally important for each generated entry in TargetPred .", "entities": []}, {"text": "Words , such as German Sonnenschein for which a translational equivalent already exists in the Source ( \u201c sunshine \u201d ; see Figure 1 ) , mainly rely on translation , while the prediction step acts as an optional re\ufb01nement procedure .", "entities": []}, {"text": "In contrast , the prediction step is crucial for words , such as Erdbeben , whose translational equivalents ( \u201c earthquake \u201d ) are missing in the Source .", "entities": []}, {"text": "Yet , these words also depend on the translation step for producing training data .", "entities": []}, {"text": "These considerations are important for deciding which words to evaluate on .", "entities": []}, {"text": "We may choose to base our evaluation on the full TargetPred lexicon , including words from the training set \u2014 after all , the word emotion model does not have access toanytarget - side gold data .", "entities": [[25, 26, "DatasetName", "emotion"]]}, {"text": "The problem with this approach is that it merges words that mainly rely ontranslation , because their equivalents are in the Source , and those which largely depend on prediction , because they are taken from the embedding model .", "entities": []}, {"text": "In this case , generalizability of evaluation results becomes questionable .", "entities": []}, {"text": "Thus , our evaluation methodology needs to ful\ufb01ll the following two requirements : ( 1 ) evaluation must not be performed on translational equivalents of the Source entries to which the model already had access during training ( e.g. , Sonnenschein and nuklear in our example from Figure 1 ) ; but , on the other hand , ( 2 ) a reasonable number of instances must be available for evaluation ( ideally , as many as possible to increase reliability ) .", "entities": []}, {"text": "The intricate cross - lingual train - dev - test set assignment of our generation methodology is in place so that we meet these two requirements .", "entities": []}, {"text": "ID Encoding Size Citation en1 V AD 1032 Warriner et al .", "entities": []}, {"text": "( 2013 ) en2 V AD 1034 Bradley and Lang ( 1999 )", "entities": []}, {"text": "en3 BE5 1034 Stevenson et al .", "entities": []}, {"text": "( 2007 ) es1 V AD 1034 Redondo et al .", "entities": []}, {"text": "( 2007 )", "entities": []}, {"text": "es2 V", "entities": []}, {"text": "A 14031 Stadthagen - Gonz \u00b4 alez et al .", "entities": []}, {"text": "( 2017 )", "entities": []}, {"text": "es3 V", "entities": []}, {"text": "A 875 Hinojosa et al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": "es4 BE5 875 Hinojosa et al .", "entities": []}, {"text": "( 2016 ) es5 BE5 10491 Stadthagen - Gonz \u00b4 alez et al .", "entities": []}, {"text": "( 2018 ) es6 BE5 2266", "entities": []}, {"text": "Ferr\u00b4e et al .", "entities": []}, {"text": "( 2017 ) de1 V AD 1003 Schmidtke et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2014 )", "entities": []}, {"text": "de2 V", "entities": []}, {"text": "A 2902 V\u02dco et al .", "entities": []}, {"text": "( 2009 ) de3 V A 1000 Kanske and Kotz ( 2010 )", "entities": []}, {"text": "de4 BE5 1958 Briesemeister et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2011 ) pl1 V AD 4905 Imbir ( 2016 ) pl2 V", "entities": []}, {"text": "A 2902 Riegel et al .", "entities": []}, {"text": "( 2015 ) pl3 BE5 2902 Wierzba", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2015 )", "entities": []}, {"text": "zh1 V A 2794 Yu et al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": "zh2 V A 1100 Yao et al .", "entities": []}, {"text": "( 2017 ) it V AD 1121 Monte\ufb01nese et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2014 ) pt V AD 1034 Soares et al .", "entities": []}, {"text": "( 2012 ) nl V A 4299 Moors et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2013 ) i d V AD 1487 Sianipar et al .", "entities": []}, {"text": "( 2016 ) el V AD 1034 Palogiannidi et al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": "tr1 V A 2029 Kapucu et al .", "entities": []}, {"text": "( 2018 ) tr2 BE5 2029 Kapucu et al .", "entities": []}, {"text": "( 2018 )", "entities": []}, {"text": "hr V", "entities": []}, {"text": "A 3022 \u00b4 Coso et al .", "entities": []}, {"text": "( 2019 ) Table 2 : Lexicons used for gold evaluation .", "entities": []}, {"text": "IDs consist of the respective ISO 639 - 1 language code plus a cardinal number to distinguish different datasets , if needed ; the format of emotion Encoding is speci\ufb01ed and Size gives the number of lexical entries per lexicon .", "entities": [[26, 27, "DatasetName", "emotion"]]}, {"text": "In particular , for our silver evaluation , we intersect TargetMT - test with TargetPred - test and compute the correlation of these two sets individually for each emotion variable .", "entities": [[28, 29, "DatasetName", "emotion"]]}, {"text": "Pearson \u2019s rwill be used as correlation measure throughout this paper .", "entities": []}, {"text": "Establishing a test set at the very start of our work\ufb02ow , Source - test , assures that there is a relatively large overlap between the two sets and , by extension , that our requirements for the evaluation are met .", "entities": []}, {"text": "The gold evaluation is a somewhat more challenging case , because we can , in general , not guarantee that the overlap of a TargetGold lexicon withTargetPred - test will be of any particular size .", "entities": []}, {"text": "For this reason , the words of the embedding model are added to TargetPred - test ( see above ) , maximizing the expected overlap withTargetGold .", "entities": []}, {"text": "In practical terms , we intersect TargetGold withTargetPred - test and compute the variable - wise correlation between these sets , in parallel to the silver evaluation .", "entities": []}, {"text": "A complementary strategy for maximizing overlap , by exploiting dependencies between published lexicons , is described below .", "entities": []}, {"text": "12074 Experimental Setup Gold Lexicons and Data Splits .", "entities": []}, {"text": "We use the English emotion lexicon from Warriner et al .", "entities": [[4, 5, "DatasetName", "emotion"]]}, {"text": "( 2013 ) as \ufb01rst part of our Source dataset .", "entities": []}, {"text": "This popular resource comprises about 14k entries in V AD format collected via crowdsourcing .", "entities": []}, {"text": "Since manually gathered BE5 ratings are available only for a subset of this lexicon ( Stevenson et al . , 2007 ) , we add BE5 ratings from Buechel and Hahn ( 2018a ) who used emotion representation mapping ( see Section 2 ) to convert the existing V AD ratings , showing that this is about as reliable as human annotation .", "entities": [[36, 37, "DatasetName", "emotion"]]}, {"text": "As apparent from the previous section , a crucial aspect for applying our methodology is the design of the train - dev - test split of the Source because it directly impacts the amount of words we can test our lexicons on during gold evaluation .", "entities": []}, {"text": "In line with these considerations , we choose the lexical items which are already present in ANEW ( Bradley and Lang , 1999 ) as Source - test set .", "entities": []}, {"text": "ANEW is the precursor to the version later distributed by Warriner et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2013 ) ; it is widely used and has been adapted to a wide range of languages .", "entities": []}, {"text": "With this choice , it is likely that a resulting TargetPred - test set has a large overlap with the respective TargetGold lexicon .", "entities": []}, {"text": "As for the TargetGold lexicons , we included every V A(D ) and BE5 lexicon we could get hold of with more than 500 entries .", "entities": []}, {"text": "This resulted in 26 datasets covering 12 quite diverse languages ( see Table 2 ) .", "entities": []}, {"text": "Note that we also include English lexicons in the gold evaluation .", "entities": []}, {"text": "In these cases , no translation will be carried out ( Source is identical to TargetMT )", "entities": []}, {"text": "so that only the expansion step is validated .", "entities": []}, {"text": "Appendix A.1 gives further details on data preparation .", "entities": []}, {"text": "Translation .", "entities": [[0, 1, "TaskName", "Translation"]]}, {"text": "We used the GOOGLE CLOUD TRANSLATION API4to produce word - to - word translation tables .", "entities": [[4, 5, "DatasetName", "CLOUD"], [12, 14, "TaskName", "word translation"]]}, {"text": "This is a commercial service , total translation costs amount to 160 EUR .", "entities": []}, {"text": "API calls were performed in November 2019 .", "entities": []}, {"text": "Embeddings .", "entities": []}, {"text": "We use the fastText embedding models from Grave et al .", "entities": [[3, 4, "MethodName", "fastText"]]}, {"text": "( 2018 ) trained for 157 languages on the respective WIKIPEDIA and the respective part of COMMON CRAWL .", "entities": [[16, 18, "DatasetName", "COMMON CRAWL"]]}, {"text": "These resources not only greatly facilitate our work but also increase comparability across languages .", "entities": []}, {"text": "The restriction to \u201c only \u201d 91 languages comes from intersecting the ones covered by the vectors with the languages covered by the translation service .", "entities": []}, {"text": "4https://cloud.google.com/translate/Models .", "entities": []}, {"text": "Since our proposed methodology is agnostic towards the chosen word emotion model , we will re - use models from the literature .", "entities": [[10, 11, "DatasetName", "emotion"]]}, {"text": "In particular , we will rely on the multi - task learning feed - forward network ( MTLFFN ) worked out by Buechel and Hahn ( 2018b ) .", "entities": [[8, 12, "TaskName", "multi - task learning"]]}, {"text": "This network constitutes the current state of the art for monolingual emotion lexicon creation ( expanding an existing lexicon for a given language ) for many of the datasets in Table 2 .", "entities": [[11, 12, "DatasetName", "emotion"]]}, {"text": "The MTLFFN has two hidden layers of 256 and 128 units , respectively , and takes pre - trained embedding vectors as input .", "entities": []}, {"text": "Its distinguishing feature is that hidden layer parameters are shared between the different emotion target variables , thus constituting a mild form of multi - task learning ( MTL ) .", "entities": [[13, 14, "DatasetName", "emotion"], [23, 27, "TaskName", "multi - task learning"]]}, {"text": "We apply MTL to V AD and BE5 variables individually ( but not between both groups ) , thus training twodistinct emotion models per language , following the outcome of a development experiment .", "entities": [[21, 22, "DatasetName", "emotion"]]}, {"text": "Details are given in Appendix A.2 together with the remainder of the model speci\ufb01cations .", "entities": []}, {"text": "Being aware of the infamous instability of neural approaches ( Reimers and Gurevych , 2017 ) , we also employ a ridge regression model , an L2regularized version of linear regression , as a more robust , yet also powerful baseline ( Li et al . , 2017 ) .", "entities": [[29, 31, "MethodName", "linear regression"]]}, {"text": "5 Results The size of the resulting lexicons ( a complete list is provided in Table 8 in the Appendix ) ranges from roughly 100k to more than 2 M entries mainly depending on the vocabulary of the respective embeddings .", "entities": []}, {"text": "We want to point out that not every single entry should be considered meaningful because of noise in the embedding vocabulary caused by typos and tokenization errors .", "entities": []}, {"text": "However , choosing the \u201c best \u201d size for an emotion lexicon necessarily translates into a quality - coverage trade - off for which there is no general solution .", "entities": [[10, 11, "DatasetName", "emotion"]]}, {"text": "Instead , we release the full - size lexicons and leave it to prospective users to apply any sort of \ufb01ltering they deem appropriate .", "entities": []}, {"text": "Silver Evaluation .", "entities": []}, {"text": "Figure 2 displays the results of our silver evaluation .", "entities": []}, {"text": "Languages ( x - axis ) are sorted by their average performance over all variables ( not shown in the plot ; tabular data given in the Appendix ) .", "entities": []}, {"text": "As can be seen , the evaluation results for English are markedly better than for any other language .", "entities": []}, {"text": "This is not surprising since no ( potentially error - prone ) machine translation was performed .", "entities": [[12, 14, "TaskName", "machine translation"]]}, {"text": "Apart from that , performance remains relatively stable across most of the languages and", "entities": []}, {"text": "1208 Figure 2 : Silver evaluation results in Pearson \u2019s r. Languages ( x - axis ) are sorted according to mean correlation .", "entities": []}, {"text": "starts degrading more quickly only for the last third of them .", "entities": []}, {"text": "In particular , for Valence \u2014 typically the easiest variable to predict \u2014 we achieve a strong performance of r > : 7for 56 languages .", "entities": []}, {"text": "On the other hand , for Arousal \u2014 typically , the most dif\ufb01cult one to predict \u2014 we achieve a solid performance ofr > : 5for 55 languages .", "entities": []}, {"text": "Dominance and the discrete emotion variables show performance trajectories swinging between these two extremes .", "entities": [[4, 5, "DatasetName", "emotion"]]}, {"text": "We assume that the main factors for explaining performance differences between languages are the quality of the translation and embedding models which , in turn , both depend on the amount of available text data ( parallel or monolingual , respectively ) .", "entities": []}, {"text": "Comparing MTLFFN and ridge baseline , we \ufb01nd that the neural network reliably outperforms the linear model .", "entities": []}, {"text": "On average over all languages and variables , the MTL models achieve 6.7%-points higher Pearson correlation .", "entities": [[14, 16, "MetricName", "Pearson correlation"]]}, {"text": "Conversely , ridge regression outperforms MTLFFN in only 15 of the total 728 cases ( 91 languages \u00028 variables ) .", "entities": []}, {"text": "Gold Evaluation .", "entities": []}, {"text": "Results for V AD variables on gold data are given in Table 3 .", "entities": []}, {"text": "As can be seen , our lexicons show a good correlation with human judgment and do so robustly , even for less - resourced languages , such as Indonesian ( i d ) , Turkish ( tr ) , or Croatian ( hr ) , and across affective variables .", "entities": []}, {"text": "Perhaps the strongest negative outliers are the Arousal results for the two Chinese datasets ( zh ) , which are likely to result from the low reliability of the gold ratings ( see below).ID Shared ( % ) Val Aro Dom en1 1032 100 .94(.87 ) .76(.67 ) .88(.76 )", "entities": []}, {"text": "en2 1034 100 .92(.92 ) .71", "entities": []}, {"text": "( .73 ) .78 ( .82 ) es1 612 59 .91(.88 ) .71(.70 ) .82 ( .83 ) es2 7685 54 .79 ( .82 )", "entities": []}, {"text": ".64", "entities": []}, {"text": "( .74 ) \u2014 es3 363 41 .91 .73 \u2014 de1 677 67 .89(.87", "entities": []}, {"text": ") .78 ( .80 ) .68", "entities": []}, {"text": "( .74 ) de2 2329 80 .75 .64 \u2014 de3 916 91 .80 .67 \u2014 pl1 2271 46 .83(.74 ) .74(.70 ) .60", "entities": []}, {"text": "( .69 ) pl2 1381 47 .82 .61 \u2014 zh1 1685 60 .84 ( .85 ) .56 ( .63 ) \u2014 zh2 701 63 .84 .44 \u2014 it 660 58 .89(.86 ) .63", "entities": []}, {"text": "( .65).76(.75 ) pt 645 62 .89(.86 ) .71 ( .71 ) .75", "entities": []}, {"text": "( .73 ) nl 2064 48 .85(.79 ) .58 ( .74 ) \u2014 i d", "entities": []}, {"text": "696 46 .84(.80 ) .64(.60 ) .63(.58 ) el 633 61 .86 .50 .74", "entities": []}, {"text": "tr1 721 35 .75 .57", "entities": []}, {"text": "\u2014 hr 1331 44 .81 .66 \u2014 Mn(all ) .85", "entities": []}, {"text": ".65 .74", "entities": []}, {"text": "Mn(vs .", "entities": []}, {"text": "monolingual ) .87(.84 ) .68 ( .70).74(.74 )", "entities": []}, {"text": "Table 3 : Gold evaluation results for V AD ( Valence , Arousal , Dom inance ) in Pearson \u2019s r. Parentheses give comparative monolingual results from Buechel and Hahn ( 2018b ) .", "entities": []}, {"text": "Shared words between TargetGold andTargetPred - test ; ( % ): percentage relative to TargetGold ; Mn(all ): mean over all datasets ; Mn ( vs. monolingual ): mean over datasets with comparative results .", "entities": []}, {"text": "We compare these results against those from Buechel and Hahn ( 2018b ) which were acquired on the respective TargetGold dataset in a monolingual fashion using 10 - fold cross - validation ( 10-", "entities": []}, {"text": "1209ID Shared ( % ) Joy Ang Sad Fea Dis en3 1033 99 .89 .83 .80 .82", "entities": []}, {"text": ".78 es4 363 41 .86 .84 .84", "entities": []}, {"text": ".84 .76 es5 6096 58", "entities": []}, {"text": ".64 .72 .72", "entities": []}, {"text": ".72 .63", "entities": []}, {"text": "es6 992 43 .80 .74", "entities": []}, {"text": ".71", "entities": []}, {"text": ".72 .68", "entities": []}, {"text": "de4 848 43 .80 .66", "entities": []}, {"text": ".52 .68 .42 pl3 1381 47 .78 .71 .66", "entities": []}, {"text": ".69 .71 tr2 721 35 .77 .69 .71", "entities": []}, {"text": ".70 .65 Mean .79", "entities": []}, {"text": ".74 .71", "entities": []}, {"text": ".74 .66", "entities": []}, {"text": "Table 4 : Gold evaluation results for BE5 ( Joy , Anger , Sadness , Fear , Disgust ) in Pearson \u2019s r. Shared words between TargetGold andTargetPred - test ;", "entities": []}, {"text": "( % ): percentage relative to TargetGold ; Mean over all datasets .", "entities": []}, {"text": "CV ) .", "entities": []}, {"text": "We admit that those results are not fully comparable to those presented here because we use \ufb01xed splits rather than 10 - CV .", "entities": []}, {"text": "Nevertheless , we \ufb01nd that the results of our cross - lingual set - up are more than competitive , outperforming the monolingual results from Buechel and Hahn ( 2018b ) in 17 out of 30 cases ( mainly for Valence and Dominance , less often for Arousal ) .", "entities": []}, {"text": "This is surprising since we use an otherwise identical model and training procedure .", "entities": []}, {"text": "We conjecture that the large size of the English Source lexicon , compared to most TargetGold lexicons , more than compensates for error - prone machine translation .", "entities": [[25, 27, "TaskName", "machine translation"]]}, {"text": "Table 4 shows the results for BE5 datasets which are in line with the V AD results .", "entities": []}, {"text": "Regarding the ordering of the emotional variables , again , we \ufb01nd Valence to be the easiest one to predict , Arousal the hardest , whereas basic emotions and Dominance take a middle ground .", "entities": []}, {"text": "Comparison against Human Reliability .", "entities": []}, {"text": "We base this analysis on inter - study reliability ( ISR ) , a rather strong criterion for human performance .", "entities": []}, {"text": "ISR is computed , per variable , as the correlation between the ratings from two distinct annotation studies ( Warriner et al . , 2013 ) .", "entities": []}, {"text": "Hence , this analysis is restricted to languages where more than one gold lexicon exists per emotion format .", "entities": [[16, 17, "DatasetName", "emotion"]]}, {"text": "We intersect the entries from both gold standards as well as the respective TargetPred - test set and compute the correlation between all three pairs of lexicons .", "entities": []}, {"text": "If our lexicon agrees more with one of the gold standards than the two gold standards agree with each other , we consider this as an indicator for superhuman reliability ( Buechel and Hahn , 2018b ) .", "entities": []}, {"text": "As shown in Table 5 , our lexicons are often competitive with human reliability for Valence ( especially for English and Chinese ) , but outperformGold1 Gold2 Shared Emo G1vsG2 G1vsPr G2vsPr en1 en2 1032V .953 .941", "entities": []}, {"text": ".922", "entities": []}, {"text": "A .760 .761 .711 D .794 .879 .782 es1 es2 610V .976", "entities": []}, {"text": ".905 .912 A .758", "entities": []}, {"text": ".714 .725 es2", "entities": []}, {"text": "es3", "entities": []}, {"text": "222V .976 .906 .907", "entities": []}, {"text": "A .710 .724 .691", "entities": []}, {"text": "de2", "entities": []}, {"text": "de3 498V .963 .806", "entities": []}, {"text": ".812 A .760 .721 .663 pl1 pl2 445V", "entities": []}, {"text": ".943 .838 .852 A .725 .764 .643 zh1 zh2 140V .932 .918 .898", "entities": []}, {"text": "A .482 .556 .455 Table 5 : Comparison against human performance .", "entities": []}, {"text": "Correlation between two gold standards , Gold1 andGold2 , with each other ( G1vsG2 ) , as well as with our lexicons TargetPred - test ( G1vsPr andG2vsPr ) relative toEmo tional variable and Shared number of words .", "entities": []}, {"text": "human reliability in 4 out of 6 cases for Arousal , and in the single test case for Dominance .", "entities": []}, {"text": "There are no cases of overlapping gold standards for BE5 .", "entities": []}, {"text": "6 Methodological Assumptions Revisited This section investigates patterns in prediction qualityacross languages , validating design decisions of our methodology .", "entities": []}, {"text": "Translation vs. Prediction .", "entities": [[0, 1, "TaskName", "Translation"]]}, {"text": "Is it bene\ufb01cial to predict new ratings for the words in TargetMT rather than using them as \ufb01nal lexicon entries straight away ?", "entities": []}, {"text": "For each TargetGold lexicon ( cf .", "entities": []}, {"text": "Table 2 )", "entities": []}, {"text": ", we intersect its word material with that inTargetMT andTargetPred .", "entities": []}, {"text": "Then , we compute the correlation between TargetPred and TargetMT with the gold standard .", "entities": []}, {"text": "This analysis was done on the respective train sets because using TargetMT rather than TargetPred is only an option for entries known at training time .", "entities": []}, {"text": "Table 6 depicts the results of this comparison averaged over all gold lexicons .", "entities": []}, {"text": "As hypothesized , the TargetPred lexicons agree , on average , more with human judgment than the TargetMT lexicons , suggesting that the word emotion model acts as a value - adding post - processor , partly mitigating rating inconsistencies introduced by mere translation of the lexicons .", "entities": [[24, 25, "DatasetName", "emotion"]]}, {"text": "The observation holds for each individual emotion variable with particularly large bene\ufb01ts for Arousal , where the postprocessed TargetPred lexicons are on average", "entities": [[6, 7, "DatasetName", "emotion"]]}, {"text": "1210Val Aro Dom Joy Ang Sad Fea Dis Pred .871 .652 .733 .767 .734 .692 .728 .650 MT .796 .515", "entities": []}, {"text": ".613 .699 .677 .636 .654 .579 Diff .076 .137 .119 .068 .057 .056 .074 .071 Table 6 : Quality of TargetMT vs. TargetPred in terms of average Pearson correlation over all languages and gold standards .", "entities": [[27, 29, "MetricName", "Pearson correlation"]]}, {"text": "Diff : = Pred\u0000MT .", "entities": []}, {"text": "14%-points better compared to the translation - only TargetMT lexicons .", "entities": []}, {"text": "This seems to indicate that lexical Arousal is less consistent between translational equivalents compared to other emotional meaning components like Valence and Sadness , which appear to be more robust against translation .", "entities": []}, {"text": "Gold vs. Silver Evaluation .", "entities": []}, {"text": "How meaningful is silver evaluation without gold data ?", "entities": []}, {"text": "We compute the Pearson correlation between gold and silver evaluation results across languages per emotion variable .", "entities": [[3, 5, "MetricName", "Pearson correlation"], [14, 15, "DatasetName", "emotion"]]}, {"text": "For languages where we consider multiple datasets during gold evaluation , we \ufb01rst average the gold evaluation results for each emotion variable .", "entities": [[20, 21, "DatasetName", "emotion"]]}, {"text": "As can be seen from Table 7 , the correlation values range between r=:91for Joy and r=:27for Disgust .", "entities": []}, {"text": "This relatively large dispersion is not surprising when we take into account that we correlate very small data series ( for Valence and Arousal there are just 12 languages for which both gold and silver evaluation results are available ; for BE5 there are only 5 such languages ) .", "entities": []}, {"text": "However , the mean over all correlation values in Table 7 is:64 , indicating that there is a relatively strong correlation between both types of evaluation .", "entities": []}, {"text": "This suggests that the silver evaluation may be used as a rather reliable proxy of lexicon quality even in the absence of language - speci\ufb01c gold data .", "entities": []}, {"text": "Val Aro Dom Joy Ang Sad Fea Dis # Lg 12 12 8 5 5 5 5 5 r .54 .57", "entities": []}, {"text": ".52 .91", "entities": []}, {"text": ".85 .57", "entities": []}, {"text": ".87 .27 Table 7 : Agreement between gold and silver evaluation across languages in Pearson \u2019s rrelative to the number of applicable languages ( \u201c # Lg \u201d ) .", "entities": []}, {"text": "7 Conclusion Emotion lexicons are at the core of sentiment analysis , a rapidly \ufb02ourishing \ufb01eld of NLP .", "entities": [[9, 11, "TaskName", "sentiment analysis"]]}, {"text": "Yet , despite large community efforts , the coverage of existing lexicons is still limited in terms of languages , size , and types of emotion variables .", "entities": [[25, 26, "DatasetName", "emotion"]]}, {"text": "While there are techniques to tackle these three forms of sparsity in isolation , we introduced a methodology which allows us to cope with them simultaneously by jointly combining emotion representation mapping , machine translation , and embedding - based lexicon expansion .", "entities": [[29, 30, "DatasetName", "emotion"], [33, 35, "TaskName", "machine translation"]]}, {"text": "Our study is \u201c large - scale \u201d in many respects .", "entities": []}, {"text": "We created representationally complex lexicons \u2014 comprising 8 distinct emotion variables \u2014 for 91 languages with up to 2 million entries each .", "entities": [[9, 10, "DatasetName", "emotion"]]}, {"text": "The evaluation of the generated lexicons featured 26 manually annotated datasets spanning 12 diverse languages .", "entities": []}, {"text": "The predicted ratings showed consistently high correlation with human judgment , compared favorably with state - of - the - art monolingual approaches to lexicon expansion and even surpassed human inter - study reliability in some cases .", "entities": []}, {"text": "The sheer number of test sets we used allowed us to validate fundamental methodological assumptions underlying our approach .", "entities": []}, {"text": "Firstly , the evaluation procedure , which is integrated into the generation methodology , allows us to reliably estimate the quality of resulting lexicons , even without target language gold standard .", "entities": []}, {"text": "Secondly , our data suggests that embedding - based word emotion models can be used as a repair mechanism , mitigating poor target - language emotion estimates acquired by simple word - to - word translation .", "entities": [[10, 11, "DatasetName", "emotion"], [25, 26, "DatasetName", "emotion"], [34, 36, "TaskName", "word translation"]]}, {"text": "Future work will have to deepen the way we deal with word sense ambiguity by way of exchanging the simplifying type - level approach our current work is based on with a semantically more informed sense - level approach .", "entities": []}, {"text": "A promising direction would be to combine a multilingual sense inventory such as BABEL NET(Navigli and Ponzetto , 2012 ) with sense embeddings ( Camacho - Collados and Pilehvar , 2018 ) .", "entities": [[13, 14, "DatasetName", "BABEL"]]}, {"text": "Acknowledgments We would like to thank the anonymous reviewers for their helpful suggestions and comments , and Tinghui Duan , JULIE LAB , for assisting us with the Chinese gold data .", "entities": []}, {"text": "This work was partially funded by the German Federal Ministry for Economic Affairs and Energy ( funding line \u201c Big Data in der makro \u00a8okonomischen Analyse \u201d", "entities": []}, {"text": "[ Big data in macroeconomic analysis ] ; Fachlos 2 ; GZ 23305/003#002 ) .", "entities": []}, {"text": "1211References Mohamed Abdalla and Graeme Hirst . 2017 .", "entities": []}, {"text": "Crosslingual sentiment analysis without ( good ) translation .", "entities": [[1, 3, "TaskName", "sentiment analysis"]]}, {"text": "In IJCNLP 2017 \u2014 Proceedings of the 8th International Joint Conference on Natural Language Processing , volume 1 : Long Papers , pages 506\u2013515 , Taipei , Taiwan , November 27 \u2013 December 1 , 2017 .", "entities": []}, {"text": "Muhammad Abdul - Mageed and Lyle H. Ungar .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "EmoNet : Fine - grained emotion detection with gated recurrent neural networks .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "In ACL 2017 \u2014 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , volume 1 : Long Papers , pages 718\u2013728 , Vancouver , British Columbia , Canada , July 30 \u2013 August 4 , 2017 .", "entities": []}, {"text": "Soraia M. Alarc \u02dcao and Manuel J. Fonseca .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Identifying emotions in images from valence and arousal ratings .", "entities": []}, {"text": "Multimedia Tools and Applications , 77(13):17413\u201317435 .", "entities": []}, {"text": "Stefano Baccianella , Andrea Esuli , and Fabrizio Sebastiani .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "S ENTI WORDNET3.0 :", "entities": []}, {"text": "An enhanced lexical resource for sentiment analysis and opinion mining .", "entities": [[5, 7, "TaskName", "sentiment analysis"], [8, 10, "TaskName", "opinion mining"]]}, {"text": "In LREC 2010 \u2014 Proceedings of the 7th International Conference on Language Resources and Evaluation , pages 2200\u20132204 , La Valletta , Malta , May 17\u201323 , 2010 .", "entities": []}, {"text": "Jeremy Barnes , Roman Klinger , and Sabine Schulte i m Walde .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Bilingual sentiment embeddings : Joint projection of sentiment across languages .", "entities": []}, {"text": "In ACL 2018 \u2014 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , volume 1 : Long Papers , pages 2483\u20132493 , Melbourne , Victoria , Australia , July 15\u201320 , 2018 .", "entities": []}, {"text": "Jeremy Barnes , Samia Touileb , Lilja \u00d8vrelid , and Erik Velldal .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Lexicon information in neural sentiment analysis : a multi - task learning approach .", "entities": [[4, 6, "TaskName", "sentiment analysis"], [8, 12, "TaskName", "multi - task learning"]]}, {"text": "In NoDaLiDa 2019 \u2014 Proceedings of the 22nd Nordic Conference on Computational Linguistics , pages 175\u2013186 , Turku , Finland , September 30 \u2013 October 2 , 2019 .", "entities": []}, {"text": "Yves Bestgen .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Building affective lexicons from speci\ufb01c corpora for automatic sentiment analysis .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}, {"text": "In LREC 2008 \u2014 Proceedings of the 6th International Conference on Language Resources and Evaluation , pages 496\u2013500 , Marrakesh , Morocco , May 28\u201330 , 2008 .", "entities": []}, {"text": "Laura - Ana - Maria Bostan and Roman Klinger .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "An analysis of annotated corpora for emotion classi\ufb01cation in text .", "entities": [[6, 7, "DatasetName", "emotion"]]}, {"text": "In COLING 2018 \u2014 Proceedings of the 27th International Conference on Computational Linguistics , pages 2104\u20132119 , Santa Fe , New Mexico , USA , August 20\u201326 , 2018 .", "entities": []}, {"text": "Margaret M. Bradley and Peter J. Lang .", "entities": []}, {"text": "1994 .", "entities": []}, {"text": "Measuring emotion : The Self - Assessment Manikin and the semantic differential .", "entities": [[1, 2, "DatasetName", "emotion"]]}, {"text": "Journal of Behavior Therapy and Experimental Psychiatry , 25(1):49\u201359.Margaret M. Bradley and Peter J. Lang .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Affective norms for English words ( A NEW ): Stimuli , instruction manual and affective ratings .", "entities": []}, {"text": "Technical Report C-1 , The Center for Research in Psychophysiology , University of Florida , Gainesville , Florida , USA .", "entities": []}, {"text": "Margaret M. Bradley and Peter J. Lang .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Affective norms for English words ( A NEW ): Stimuli , instruction manual and affective ratings .", "entities": []}, {"text": "Technical Report C-2 , University of Florida , Gainesville , Forida , USA .", "entities": []}, {"text": "Benny B. Briesemeister , Lars Kuchinke , and Arthur M. Jacobs .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Discrete Emotion Norms for Nouns : Berlin Affective Word List ( D ENN \u2013 BAWL).Behavior Research Methods , 43(2):#441 .", "entities": []}, {"text": "Sven Buechel and Udo Hahn .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Emotion analysis as a regression problem : Dimensional models and their implications on emotion representation and metrical evaluation .", "entities": [[13, 14, "DatasetName", "emotion"]]}, {"text": "In ECAI 2016 \u2014 Proceedings of the 22nd European Conference on Arti\ufb01cial Intelligence , pages 1114\u20131122 , The Hague , The Netherlands , August 29 \u2013 September 2 , 2016 .", "entities": []}, {"text": "Sven Buechel and Udo Hahn .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "E MOBANK : Studying the impact of annotation perspective and representation format on dimensional emotion analysis .", "entities": [[14, 15, "DatasetName", "emotion"]]}, {"text": "In EACL 2017 \u2014 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics , volume 2 : Short Papers , pages 578\u2013585 , Valencia , Spain , April 3\u20137 , 2017 .", "entities": []}, {"text": "Sven Buechel and Udo Hahn .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Emotion representation mapping for automatic lexicon construction ( mostly ) performs on human level .", "entities": []}, {"text": "In COLING 2018 \u2014 Proceedings of the 27th International Conference on Computational Linguistics , pages 2892 \u2013 2904 , Santa Fe , New Mexico , USA , August 20\u201326 , 2018 .", "entities": []}, {"text": "Sven Buechel and Udo Hahn .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "Word emotion induction for multiple languages as a deep multi - task learning problem .", "entities": [[1, 2, "DatasetName", "emotion"], [9, 13, "TaskName", "multi - task learning"]]}, {"text": "In NAACL - HLT 2018 \u2014 Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , volume 1 : Long Papers , pages 1907\u20131918 , New Orleans , Louisiana , USA , June 1\u20136 , 2018 .", "entities": []}, {"text": "Sven Buechel , Johannes Hellrich , and Udo Hahn .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Feelings from the past : Adapting affective lexicons for historical emotion analysis .", "entities": [[10, 11, "DatasetName", "emotion"]]}, {"text": "In LT4DH 2016 \u2014 Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities @ COLING 2016 , pages 54\u201361 , Osaka , Japan , December 11 , 2016 .", "entities": []}, {"text": "Rafael A. Calvo and Sunghwan Mac Kim .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Emotions in text : Dimensional and categorical models .", "entities": []}, {"text": "Computational Intelligence , 29(3):527\u2013543 .", "entities": []}, {"text": "1212Jos\u00b4e Camacho - Collados and Mohammad Taher Pilehvar .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "From word to sense embeddings : A survey on vector representations of meaning .", "entities": []}, {"text": "Journal of Arti\ufb01cial Intelligence Research , 63:743\u2013788 .", "entities": []}, {"text": "Yanqing Chen and Steven Skiena .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Building sentiment lexicons for all major languages .", "entities": []}, {"text": "In ACL 2014 \u2014 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics , volume 2 : Short Papers , pages 383\u2013389 , Baltimore , Maryland , USA , June 23\u201325 , 2014 .", "entities": []}, {"text": "Ta - Chung Chi and Yun - Nung Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "C LUSE : Cross - Lingual Unsupervised Sense Embeddings .", "entities": []}, {"text": "In EMNLP 2018 \u2014 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 271\u2013281 , Brussels , Belgium , October 31 \u2013 November 4 , 2018 .", "entities": []}, {"text": "Paul Cook and Suzanne Stevenson . 2010 .", "entities": []}, {"text": "Automatically identifying changes in the semantic orientation of words .", "entities": []}, {"text": "In LREC 2010 \u2014 Proceedings of the 7th International Conference on Language Resources and Evaluation , pages 28\u201334 , La Valletta , Malta , May 17\u201323 , 2010 .", "entities": []}, {"text": "Bojana \u00b4 Coso , Marc Guasch , Pilar Ferr \u00b4 e , and Jos \u00b4 e Antonio Hinojosa .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Affective and concreteness norms for 3,022 Croatian words .", "entities": []}, {"text": "Quarterly Journal of Experimental Psychology , 72(9):2302\u20132312 .", "entities": []}, {"text": "Per Davidson and \u02daAse Innes - Ker . 2014 .", "entities": []}, {"text": "Valence and arousal norms for Swedish affective words .", "entities": []}, {"text": "Lund Psychological Reports , 14:#2 .", "entities": []}, {"text": "Luna De Bruyne , Pepa Atanasova , and Isabelle Augenstein .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Joint emotion label space modelling for affect lexica .", "entities": [[1, 2, "DatasetName", "emotion"]]}, {"text": "arXiv:1911.08782", "entities": []}, {"text": "[ cs ] .", "entities": []}, {"text": "Steven Du and Xi Zhang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Aicyber \u2019s system for IALP 2016 Shared Task : Character - enhanced word vectors and boosted neural networks .", "entities": []}, {"text": "In IALP 2016 \u2014 Proceedings of the 2016 International Conference on Asian Language Processing , pages 161 \u2013 163 , Tainan , Taiwan , November 21\u201323 , 2016 .", "entities": []}, {"text": "Paul Ekman .", "entities": []}, {"text": "1992 .", "entities": []}, {"text": "An argument for basic emotions .", "entities": []}, {"text": "Cognition and Emotion , 6(3 - 4):169\u2013200 .", "entities": []}, {"text": "Pilar Ferr \u00b4 e , Marc Guasch ,", "entities": []}, {"text": "Natalia Mart \u00b4 \u0131nez - Garc \u00b4 \u0131a , Isabel Fraga , and Jos \u00b4 e Antonio Hinojosa . 2017 .", "entities": []}, {"text": "Moved by words : Affective ratings for a set of 2,266 Spanish words in \ufb01ve discrete emotion categories .", "entities": [[16, 17, "DatasetName", "emotion"]]}, {"text": "Behavior Research Methods , 49(3):1082\u20131094 .", "entities": []}, {"text": "Edouard Grave , Piotr Bojanowski , Prakhar Gupta , Armand Joulin , and Tomas Mikolov .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning word vectors for 157 languages .", "entities": []}, {"text": "In LREC 2018 \u2014 Proceedings of the 11th International Conference on Language Resources and Evaluation , pages 3483 \u2013 3487 , Miyazaki , Japan , May 7\u201312 , 2018 .", "entities": []}, {"text": "William L. Hamilton , Kevin Clark , Jure Leskovec , and Dan Jurafsky .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Inducing domain - speci\ufb01c sentiment lexicons from unlabeled corpora .", "entities": []}, {"text": "In EMNLP2016 \u2014 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 595\u2013605 , Austin , Texas , USA , November 1 \u2013 5 , 2016 .", "entities": [[21, 22, "DatasetName", "Texas"]]}, {"text": "Vasileios Hatzivassiloglou and Kathleen R. McKeown .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Predicting the semantic orientation of adjectives .", "entities": []}, {"text": "In ACL - EACL 1997 \u2014 Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics & 8th Conference of the European Chapter of the Association for Computational Linguistics , pages 174\u2013181 , Madrid , Spain , July 7 \u2013 12 , 1997 .", "entities": []}, {"text": "Johannes Hellrich , Sven Buechel , and Udo Hahn .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "JESEME : Interleaving semantics and emotions in a Web service for the exploration of language change phenomena .", "entities": []}, {"text": "In COLING 2018 \u2014 Proceedings of the 27th International Conference on Computational Linguistics , volume System Demonstrations , pages 10\u201314 , Santa Fe , New Mexico , USA , August 20\u201326 , 2018 .", "entities": []}, {"text": "Jos\u00b4e Antonio Hinojosa , Natalia Mart \u00b4 \u0131nez - Garc \u00b4 \u0131a , Cristina Villalba - Garc \u00b4 \u0131a , Uxia Fern \u00b4 andezFolgueiras , Alberto S \u00b4 anchez - Carmona , Miguel Angel Pozo , and Pedro R. Montoro .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Affective norms of 875 Spanish words for \ufb01ve discrete emotional categories and two emotional dimensions .", "entities": []}, {"text": "Behavior Research Methods , 48(1):272\u2013284 .", "entities": []}, {"text": "Holger Hoffmann , Andreas Scheck , Timo Schuster , Steffen Walter , Kerstin Limbrecht - Ecklundt , Harald C. Traue , and Henrik Kessler .", "entities": [[6, 7, "DatasetName", "Timo"]]}, {"text": "2012 .", "entities": []}, {"text": "Mapping discrete emotions into the dimensional space : An empirical approach .", "entities": []}, {"text": "In SMC 2012 \u2014 Proceedings of the 2012 IEEE International Conference on Systems , Man , and Cybernetics , pages 3316\u20133320 , Seoul , Korea , October 14\u201317 , 2012 .", "entities": []}, {"text": "Markus J. Hofmann , Lars Kuchinke , Sascha Tamm , Melissa L.-H. V \u02dco , and Arthur M. Jacobs .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Affective processing within 1/10th of a second : High arousal is necessary for early facilitative processing of negative but not positive words .", "entities": []}, {"text": "Cognitive , Affective , & Behavioral Neuroscience , 9(4):389\u2013397 .", "entities": []}, {"text": "Clayton J. Hutto and Eric Gilbert .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "V ADER :", "entities": []}, {"text": "A parsimonious rule - based model for sentiment analysis of social media text .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}, {"text": "In ICWSM 2014 \u2014 Proceedings of the 8th International AAAI Conference on Weblogs and Social Media , pages 216\u2013225 , Ann Arbor , Michigan , USA , June 1\u20134 , 2014 .", "entities": []}, {"text": "Kamil K. Imbir .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Affective Norms for 4900 Polish Words Reload ( ANPW R ): Assessments for valence , arousal , dominance , origin , signi\ufb01cance , concreteness , imageability and , age of acquisition .", "entities": []}, {"text": "Frontiers in Psychology , 7:#1081 .", "entities": []}, {"text": "Philipp Kanske and Sonja A. Kotz .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Leipzig Affective Norms for German : A reliability study .", "entities": []}, {"text": "Behavior Research Methods , 42(4):987\u2013991 .", "entities": []}, {"text": "1213Aycan Kapucu , Asl\u0131 K\u0131l\u0131c \u00b8 , Y\u0131ld\u0131z \u00a8Ozk\u0131l\u0131c \u00b8 , and Bengisu Sar\u0131baz .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Turkish emotional word norms for arousal , valence , and discrete emotion categories .", "entities": [[11, 12, "DatasetName", "emotion"]]}, {"text": "Psychological Reports , pages 1\u201322 .", "entities": []}, {"text": "[ Available online Dec 4 , 2018 ] .", "entities": []}, {"text": "Sopan Khosla , Niyati Chhaya , and Kushal Chawla . 2018 .", "entities": []}, {"text": "A FF2VEC : Affect \u2013 enriched distributional word representations .", "entities": []}, {"text": "In COLING 2018 \u2014 Proceedings of the 27th International Conference on Computational Linguistics , pages 2204\u20132218 , Santa Fe , New Mexico , USA , August 20\u201326 , 2018 .", "entities": []}, {"text": "Diederik Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "A DAM : A method for stochastic optimization .", "entities": [[6, 8, "TaskName", "stochastic optimization"]]}, {"text": "In ICLR 2015 \u2014 Proceedings of the 3rd International Conference on Learning Representations , San Diego , California , USA , May 7\u20139 , 2015 .", "entities": []}, {"text": "Svetlana Kiritchenko and Saif M. Mohammad . 2016 .", "entities": []}, {"text": "Capturing reliable \ufb01ne - grained sentiment associations by crowdsourcing and best - worst scaling .", "entities": []}, {"text": "In NAACL - HLT 2016 \u2014 Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 811\u2013817 , San Diego , California , USA , June 12\u201317 , 2016 .", "entities": []}, {"text": "Maximilian K \u00a8oper and Sabine Schulte i m Walde .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Automatically generated affective norms of abstractness , arousal , imageability and valence for 350,000 German lemmas .", "entities": []}, {"text": "In LREC 2016 \u2014 Proceedings of the 10th International Conference on Language Resources and Evaluation , pages 2595\u20132598 , Portoro \u02c7z , Slovenia , May 23\u201328 , 2016 .", "entities": []}, {"text": "Agnieszka Landowska .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Towards new mappings between emotion representation models .", "entities": [[4, 5, "DatasetName", "emotion"]]}, {"text": "Applied Sciences , 8(2):#274 .", "entities": []}, {"text": "Nicolas Leveau , Sandra Jhean - Larose , Guy Denhi ` ere , and Ba - Linh Nguyen .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Validating an interlingual metanorm for emotional analysis of texts .", "entities": []}, {"text": "Behavior Research Methods , 44(4):1007\u20131014 .", "entities": []}, {"text": "Minglei Li , Qin Lu , Yunfei Long , and Lin Gui . 2017 .", "entities": []}, {"text": "Inferring affective meanings of words from word embedding .", "entities": []}, {"text": "IEEE Transactions on Affective Computing , 8(4):443\u2013456 .", "entities": []}, {"text": "Ying Li , Tomas Engelthaler , Cynthia S. Q. Siew , and Thomas T. Hills .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The M ACROSCOPE :", "entities": []}, {"text": "A tool for examining the historical structure of language .", "entities": []}, {"text": "Behavior Research Methods , 51(4):1864\u20131877 .", "entities": []}, {"text": "Saif Mohammad .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Obtaining reliable human ratings of valence , arousal , and dominance for 20,000 English words .", "entities": []}, {"text": "In ACL 2018 \u2014 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , volume 1 : Long Papers , pages 174\u2013184 , Melbourne , Victoria , Australia , July 15 \u2013 20 , 2018 .", "entities": []}, {"text": "Saif Mohammad and Felipe Bravo - Marquez . 2017 .", "entities": []}, {"text": "WASSA -2017 Shared Task on Emotion Intensity .", "entities": []}, {"text": "In WASSA 2017 \u2014 Proceedings of the 8th Workshopon Computational Approaches to Subjectivity , Sentiment and Social Media Analysis @ EMNLP 2017 , pages 34\u201349 , Copenhagen , Denmark , September 8 , 2017 .", "entities": []}, {"text": "Saif Mohammad , Felipe Bravo - Marquez , Mohammad Salameh , and Svetlana Kiritchenko .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SEMEVAL-2018 Task 1 : Affect in Tweets .", "entities": []}, {"text": "In SemEval 2018 \u2014 Proceedings of the 12th International Workshop on Semantic Evaluation @ NAACL - HLT 2018 , pages 1\u201317 , New Orleans , Louisiana , USA , June 5\u20136 , 2018 .", "entities": []}, {"text": "Saif M. Mohammad and Peter D. Turney .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Crowdsourcing a word - emotion association lexicon .", "entities": [[4, 5, "DatasetName", "emotion"]]}, {"text": "Computational Intelligence , 29(3):436\u2013465 .", "entities": []}, {"text": "Catherine Monnier and Arielle Syssau . 2008 .", "entities": []}, {"text": "Semantic contribution to verbal short - term memory : Are pleasant words easier to remember than neutral words in serial recall and serial recognition ?", "entities": []}, {"text": "Memory & Cognition , 36(1):35\u201342 .", "entities": []}, {"text": "Maria Monte\ufb01nese , Ettore Ambrosini , Beth Fair\ufb01eld , and Nicola Mammarella . 2014 .", "entities": []}, {"text": "The adaptation of the Affective Norms for English Words ( A NEW ) for Italian .", "entities": []}, {"text": "Behavior Research Methods , 46(3):887 \u2013 903 .", "entities": []}, {"text": "Agnes Moors , Jan De Houwer , Dirk Hermans , Sabine Wanmaker , Kevin van Schie , Anne - Laura Van Harmelen , Maarten De Schryver , Jeffrey De Winne , and Marc Brysb\u00e6rt .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Norms of valence , arousal , dominance , and age of acquisition for 4,300 Dutch words .", "entities": []}, {"text": "Behavior Research Methods , 45(1):169\u2013177 .", "entities": []}, {"text": "Roberto Navigli and Simone Paolo Ponzetto .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "BABEL NET :", "entities": [[0, 1, "DatasetName", "BABEL"]]}, {"text": "The automatic construction , evaluation and application of a wide - coverage multilingual semantic network .", "entities": []}, {"text": "Arti\ufb01cial Intelligence , 193:217 \u2013 250 .", "entities": []}, {"text": "Elisavet Palogiannidi , Polychronis Koutsakis , Elias Iosif , and Alexandros Potamianos . 2016 .", "entities": []}, {"text": "Affective lexicon creation for the Greek language .", "entities": []}, {"text": "In LREC 2016 \u2014 Proceedings of the 10th International Conference on Language Resources and Evaluation , pages 2867\u20132872 , Portoro \u02c7z , Slovenia , May 23\u201328 , 2016 .", "entities": []}, {"text": "Sungjoon Park , Jiseon Kim , Jaeyeol Jeon , Heeyoung Park , and Alice Oh . 2019 .", "entities": []}, {"text": "Toward dimensional emotion detection from categorical emotion annotations .", "entities": [[2, 3, "DatasetName", "emotion"], [6, 7, "DatasetName", "emotion"]]}, {"text": "arXiv:1911.02499", "entities": []}, {"text": "[ cs , eess ] .", "entities": []}, {"text": "Fabian Pedregosa , Ga \u00a8el Varoquaux , Alexandre Gramfort , Vincent Michel , Bertrand Thirion , Olivier Grisel , Mathieu Blondel , Peter Prettenhofer , Ron Weiss , Vincent Dubourg , Jake Vanderplas , Alexandre Passos , David Cournapeau , Matthieu Brucher , Matthieu Perrot , and \u00b4 Edouard Duchesnay .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "S CIKIT -LEARN", "entities": []}, {"text": ": Machine learning in PYTHON .Journal of Machine Learning Research , 12(85):2825\u20132830 .", "entities": []}, {"text": "1214Ana P. Pinheiro , Marcelo Dias , Jo \u02dcao Pedrosa , and Ana P. Soares . 2017 .", "entities": []}, {"text": "Minho Affective Sentences ( MAS ): Probing the roles of sex , mood , and empathy in affective ratings of verbal stimuli .", "entities": [[4, 5, "MethodName", "MAS"]]}, {"text": "Behavior Research Methods , 49(2):698\u2013716 .", "entities": []}, {"text": "Robert Plutchik .", "entities": []}, {"text": "1980 .", "entities": []}, {"text": "A general psychoevolutionary theory of emotion .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "In Robert Plutchik and Henry Kellerman , editors , Emotion : Theory , Research and Experience , volume 1 : Theories of Emotion , pages 3\u201333 .", "entities": []}, {"text": "Academic Press , New York , NY , USA .", "entities": []}, {"text": "Jaime Redondo , Isabel Fraga , Isabel Padr \u00b4 on , and Montserrat Comesa \u02dcna .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "The Spanish adaptation of A NEW ( Affective Norms for English Words ) .", "entities": []}, {"text": "Behavior Research Methods , 39(3):600\u2013605 .", "entities": []}, {"text": "Nils Reimers and Iryna Gurevych .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Reporting score distributions makes a difference : Performance study of LSTM - networks for sequence tagging .", "entities": [[10, 11, "MethodName", "LSTM"]]}, {"text": "In EMNLP 2017 \u2014 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 338\u2013348 , Copenhagen , Denmark , September 9\u201311 , 2017 .", "entities": []}, {"text": "Monika Riegel , Ma\u0142gorzata Wierzba , Marek Wypych , \u0141ukasz \u02d9Zurawski , Katarzyna Jednor \u00b4 og , Anna Grabowska , and Artur Marchewka . 2015 .", "entities": []}, {"text": "Nencki Affective Word List ( N AWL ): The cultural adaptation of the Berlin Affective Word List \u2013 Reloaded ( BAWL \u2013 R ) for Polish .", "entities": []}, {"text": "Behavior Research Methods , 47(4):1222\u20131236 .", "entities": []}, {"text": "Sara Rosenthal , Preslav I. Nakov , Svetlana Kiritchenko , Saif M. Mohammad , Alan Ritter , and Veselin Stoyanov .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "S EMEVAL 2015 Task 10 : Sentiment Analysis in Twitter .", "entities": [[6, 8, "TaskName", "Sentiment Analysis"]]}, {"text": "In SemEval 2015 \u2014 Proceedings of the 9th International Workshop on Semantic Evaluation @ NAACL - HLT 2015 , pages 451\u2013463 , Denver , Colorado , USA , June 4\u20135 , 2015 .", "entities": []}, {"text": "Sascha Rothe , Sebastian Ebert , and Hinrich Sch \u00a8utze . 2016 .", "entities": []}, {"text": "Ultradense word embeddings by orthogonal transformation .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In NAACL 2016 \u2014 Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 767 \u2013 777 , San Diego , California , USA , June 12\u201317 , 2016 .", "entities": []}, {"text": "David S. Schmidtke , Tobias Schr \u00a8oder , Arthur M. Jacobs , and Markus Conrad .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A NGST : Affective Norms for German Sentiment Terms , derived from the Affective Norms for English Words .", "entities": []}, {"text": "Behavior Research Methods , 46(4):1108\u20131118 . Jo\u02dcao", "entities": []}, {"text": "Sedoc , Daniel Preot \u00b8iuc - Pietro , and Lyle H. Ungar .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Predicting emotional word ratings using distributional representations and signed clustering .", "entities": []}, {"text": "InEACL 2017 \u2014 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics , volume 2 : Short Papers , pages 564\u2013571 , Valencia , Spain , April 3\u20137 , 2017 .", "entities": []}, {"text": "Samira Shaikh , Kit Cho , Tomek Strzalkowski , Laurie Feldman , John Lien , Ting Liu , and George AaronBroadwell . 2016 .", "entities": []}, {"text": "A NEW + : Automatic expansion and validation of Affective Norms of Words lexicons in multiple languages .", "entities": []}, {"text": "In LREC 2016 \u2014 Proceedings of the 10th International Conference on Language Resources and Evaluation , pages 1127\u20131132 , Portoro \u02c7z , Slovenia , May 23\u201328 , 2016 .", "entities": []}, {"text": "Agnes Sianipar , Pieter van Groenestijn , and Ton Dijkstra . 2016 .", "entities": []}, {"text": "Affective meaning , concreteness , and subjective frequency norms for Indonesian words .", "entities": []}, {"text": "Frontiers in Psychology , 7:#1907 .", "entities": []}, {"text": "Ana Paula Soares , Montserrat Comesa \u02dcna , Ana P. Pinheiro , Alberto Sim \u02dcoes , and Carla So\ufb01a Frade .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "The adaptation of the Affective Norms for English Words ( A NEW ) for European Portuguese .", "entities": []}, {"text": "Behavior Research Methods , 44(1):256\u2013269 .", "entities": []}, {"text": "Hans Stadthagen - Gonz \u00b4 alez , Pilar Ferr \u00b4 e , Miguel A. P\u00b4erez - S \u00b4 anchez , Constance Imbault , and Jos \u00b4 e Antonio Hinojosa .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Norms for 10,491 Spanish words for \ufb01ve discrete emotions : Happiness , disgust , anger , fear , and sadness .", "entities": []}, {"text": "Behavior Research Methods , 50(5):1943\u20131952 .", "entities": []}, {"text": "Hans Stadthagen - Gonz \u00b4 alez , Constance Imbault , Miguel A. P \u00b4 erez - S \u00b4 anchez , and Marc Brysb\u00e6rt . 2017 .", "entities": []}, {"text": "Norms of valence and arousal for 14,031 Spanish words .", "entities": []}, {"text": "Behavior Research Methods , 49(1):111\u2013123 .", "entities": []}, {"text": "Ryan A. Stevenson , Joseph A. Mikels , and Thomas W. James .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Characterization of the Affective Norms for English Words by discrete emotional categories .", "entities": []}, {"text": "Behavior Research Methods , 39(4):1020 \u2013 1024 .", "entities": []}, {"text": "Carlo Strapparava and Alessandro Valitutti .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "WORDNET - AFFECT : An affective extension of WORDNET .", "entities": []}, {"text": "In LREC 2004 \u2014 Proceedings of the 4th International Conference on Language Resources and Evaluation , pages 1083\u20131086 , Lisbon , Portugal , May 24\u201330 , 2004 .", "entities": []}, {"text": "Enrica Troiano , Sebastian Pad \u00b4 o , and Roman Klinger .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crowdsourcing and validating event - focused emotion corpora for German and English .", "entities": [[3, 12, "DatasetName", "event - focused emotion corpora for German and English"]]}, {"text": "In ACL 2019 \u2014 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4005\u20134011 , Florence , Italy , July 28 \u2013 August 2 , 2019 .", "entities": [[20, 21, "MethodName", "Florence"]]}, {"text": "Peter D. Turney and Michael L. Littman .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Measuring praise and criticism : Inference of semantic orientation from association .", "entities": []}, {"text": "ACM Transactions on Information Systems , 21(4):315\u2013346 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Melissa L.-H. V \u02dco , Markus Conrad , Lars Kuchinke , Karolina Urton , Markus J. Hofmann , and Arthur M. Jacobs .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "The Berlin Affective Word List Reloaded ( B AWL \u2013 R ) .", "entities": []}, {"text": "Behavior Research Methods , 41(2):534\u2013538 .", "entities": []}, {"text": "Jin Wang , Liang - Chih Yu , K. Robert Lai , and Xuejie Zhang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Community - based weighted graph model for valence - arousal prediction of affective", "entities": []}, {"text": "1215words .", "entities": []}, {"text": "IEEE / ACM Transactions on Audio , Speech , and Language Processing , 24(11):1957\u20131968 .", "entities": [[2, 3, "DatasetName", "ACM"]]}, {"text": "Amy Beth Warriner , Victor Kuperman , and Marc Brysb\u00e6rt .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Norms of valence , arousal , and dominance for 13,915 English lemmas .", "entities": []}, {"text": "Behavior Research Methods , 45(4):1191\u20131207 .", "entities": []}, {"text": "Ma\u0142gorzata Wierzba , Monika Riegel , Marek Wypych , Katarzyna Jednor \u00b4 og , Pawe\u0142 Turnau , Anna Grabowska , and Artur Marchewka . 2015 .", "entities": []}, {"text": "Basic emotions in the Nencki Affective Word List ( NAWL BE ): New method of classifying emotional stimuli .", "entities": []}, {"text": "PLoS ONE , 10(7):#e0132305 .", "entities": [[0, 1, "DatasetName", "PLoS"]]}, {"text": "Zhao Yao , Jia Wu , Yanyan Zhang , and Zhenhong Wang .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Norms of valence , arousal , concreteness , familiarity , imageability , and context availability for 1,100 Chinese words .", "entities": []}, {"text": "Behavior Research Methods , 49(4):1374\u20131385 .", "entities": []}, {"text": "Liang - Chih Yu , Lung - Hao Lee , Shuai Hao , Jin Wang , Yunchao He , Jun Hu , K. Robert Lai , and Xuejie Zhang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Building Chinese affective resources in valence - arousal dimensions .", "entities": []}, {"text": "In NAACL - HLT 2016 \u2014 Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 540\u2013545 , San Diego , California , USA , June 12\u201317 , 2016 .", "entities": []}, {"text": "Liang - Chih Yu , Jin Wang , K. Robert Lai , and Xuejie Zhang .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Re\ufb01ning word embeddings for sentiment analysis .", "entities": [[1, 3, "TaskName", "word embeddings"], [4, 6, "TaskName", "sentiment analysis"]]}, {"text": "In EMNLP 2017 \u2014 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 534\u2013539 , Copenhagen , Denmark , September 9\u201311 , 2017 .", "entities": []}, {"text": "Feng Zhou , Shu Kong , Charless C. Fowlkes , Tao Chen , and Baiying Lei . 2020 .", "entities": []}, {"text": "Fine - grained facial expression analysis using dimensional emotion model .", "entities": [[8, 9, "DatasetName", "emotion"]]}, {"text": "Neurocomputing .", "entities": []}, {"text": "[ Available online Jan 23 , 2020 ] .", "entities": []}, {"text": "1216A Appendices A.1 Data Preparation The exact design of the Source train - dev - test split is as follows : All entries ( words plus ratings ) from all splits are taken from Warriner et al .", "entities": []}, {"text": "( 2013 ) .", "entities": []}, {"text": "The data was then partitioned based on the overlap with the two precursory versions by Bradley and Lang ( 1999 ) ( the original ANEW ) and Bradley and Lang ( 2010 ) ( an early extended version of ANEW roughly twice as large ) .", "entities": []}, {"text": "Source - test was built by intersecting the lexicon from Warriner et al .", "entities": []}, {"text": "( 2013 ) with the original ANEW .", "entities": []}, {"text": "A similar process was applied for Source - dev : we intersected the words from Warriner et al .", "entities": []}, {"text": "( 2013 ) and Bradley and Lang ( 2010 )", "entities": []}, {"text": "and removed the ones present inSource - test .", "entities": []}, {"text": "Lastly , Source - train is made up by all words from Warriner et al .", "entities": []}, {"text": "( 2013 ) which are neither in Source - test nor inSource - dev .", "entities": []}, {"text": "The reason why the ratings in Source are taken exclusively from Warriner et al . ( 2013 ) is that these are distributed under a more permissive license compared to their precursors .", "entities": []}, {"text": "We removed multi - token entries ( e.g. , boa constrictor ) and entries with upper case characters ( e.g. , Budweiser ) from all data splits of Source , thus restricting the lexicon to single - token , nonproper noun entries to make it more suitable for word embedding - based research .", "entities": []}, {"text": "All splits combined have 13,791 entries ( train : 11,463 , dev : 1,296 , test : 1,032 ) , thus removing less than 1 % from the original lexicon.5 Regarding the remaining gold standards , the only cases which needed additional preparation or cleansing steps were zh1 ( Yu et al . , 2016 ) andzh2 ( Yao et al . , 2017 ) .", "entities": []}, {"text": "zh1 was created and is distributed using traditional Chinese characters , whereas the embedding model by Grave et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) employs simpli\ufb01ed ones .", "entities": []}, {"text": "Therefore , we converted zh1 into simpli\ufb01ed characters using GOOGLE TRANSLATE6prior to evaluation .", "entities": []}, {"text": "While manually examining the zh2 lexicon , we noticed several cases where the ratings seemed rather counter - intuitive ( e.g. , seemingly positive words which received very negative ratings ) .", "entities": []}, {"text": "We contacted the authors who con\ufb01rmed the problem and sent us a corrected version .", "entities": []}, {"text": "We did not \ufb01nd any such problems in the second version .", "entities": []}, {"text": "We consulted 5The data split is available at :", "entities": []}, {"text": "https://github.com/ JULIELab / XANEW 6In this case the regular Web application , not the API , was used : https://translate.google.com/with a Chinese native speaker for both of these procedures regarding the zh1 andzh2 lexicons .", "entities": []}, {"text": "A.2 Model Training and Implementation Training of the MTLFFN model closely followed the procedure speci\ufb01ed by Buechel and Hahn ( 2018b ):", "entities": []}, {"text": "For each language , the model was trained for roughly 15k iterations ( exactly 168 epochs ) with a batch size of 128 using the Adam optimizer ( Kingma and Ba , 2015 ) with learning rate 10\u00003 , and:5dropout on the hidden layers and : 2on the input layer .", "entities": [[19, 21, "HyperparameterName", "batch size"], [25, 26, "MethodName", "Adam"], [26, 27, "HyperparameterName", "optimizer"], [35, 37, "HyperparameterName", "learning rate"]]}, {"text": "As nonlinear activation function we used leaky ReLU with \u201c leakage \u201d of 0:01 .", "entities": [[2, 4, "HyperparameterName", "activation function"], [6, 8, "MethodName", "leaky ReLU"]]}, {"text": "Embedding vectors are the only model input .", "entities": []}, {"text": "They have 300 dimensions for every language , independent of their respective training data size ( Grave et al . , 2018 ) .", "entities": []}, {"text": "Since the automatic translation of Source is not guaranteed to result in single - word translations , we use the following workaround to derive embedding vectors for multi - token translations :", "entities": []}, {"text": "If the translation as a whole can not be found in the embedding model , the multi - token term gets split up into its constituent parts , using spaces , apostrophes or hyphens as separators .", "entities": []}, {"text": "Each substring is looked up in the embedding model , the averaged vector is taken as input .", "entities": []}, {"text": "If no substring is recognized , we use the zero vector instead .", "entities": []}, {"text": "We also use the zero vector for single - token entries in TargetMT that are missing in the embeddings .", "entities": []}, {"text": "Since Buechel and Hahn ( 2018b ) considered only V AD but not BE5 datasets , we conducted a development experiment on the TargetMT - dev sets for all 91 languages where we assessed whether MTL is advantageous for BE5 variables as well , or for a combination of V AD and BE5 variables .", "entities": []}, {"text": "We found that MTL improved performance when applied separately among all V AD and BE5 variables .", "entities": []}, {"text": "Yet , when jointly learning all eight emotion variables , the results were somewhat inconclusive .", "entities": [[7, 8, "DatasetName", "emotion"]]}, {"text": "Performance increased for BE5 , but decreased for V AD .", "entities": []}, {"text": "Hence , for lexicon creation , we took a cautious approach and trained two separate models per language , one for V AD , the other for BE5 .", "entities": []}, {"text": "An analysis of MTL across V AD and BE5 is left for future work .", "entities": []}, {"text": "The MTLFFN model is implemented in PYTORCH , adapting part of the TENSORFLOW code from Buechel and Hahn ( 2018b ) .", "entities": []}, {"text": "The ridge regression baseline model is implemented with SCIKIT LEARN ( Pedregosa et al . , 2011 ) using default parameters .", "entities": []}, {"text": "1217No .", "entities": []}, {"text": "ISO Full Name Size Val Aro Dom Joy Ang Sad Fea Dis Mean 1 en English 2,000,004 .94 .76 .88", "entities": []}, {"text": ".90 .91", "entities": []}, {"text": ".90 .89", "entities": []}, {"text": ".89 .88 2 es Spanish 2,001,183", "entities": []}, {"text": ".89 .70 .80 .83 .86 .85 .82 .81", "entities": []}, {"text": ".82 3 it Italian 2,001,137 .88", "entities": []}, {"text": ".69 .81", "entities": []}, {"text": ".82 .85 .84", "entities": []}, {"text": ".82 .81 .81 4 de German 2,000,507 .89 .66 .81 .82 .84 .82 .80", "entities": []}, {"text": ".81 .81", "entities": []}, {"text": "5 sv Swedish 2,000,980 .87 .64 .80 .82", "entities": []}, {"text": ".84 .82 .81 .80", "entities": []}, {"text": ".80 6 pt Portuguese", "entities": []}, {"text": "2,001,078 .86 .70 .78", "entities": []}, {"text": ".78 .83 .81 .78 .82", "entities": []}, {"text": ".79 7 i d Indonesian 2,002,221 .85 .67", "entities": []}, {"text": ".79 .78 .82", "entities": []}, {"text": ".80 .79", "entities": []}, {"text": ".77 .79 8 hu Hungarian 2,000,975 .86 .67 .79", "entities": []}, {"text": ".80 .82", "entities": []}, {"text": ".79 .77 .79 .79 9 fr French 2,001,517 .85", "entities": []}, {"text": ".65 .79", "entities": []}, {"text": ".78 .82 .81", "entities": []}, {"text": ".78 .81", "entities": []}, {"text": ".78 10 \ufb01", "entities": []}, {"text": "Finnish 2,000,841 .86 .64 .79 .81 .82", "entities": []}, {"text": ".78 .77 .80", "entities": []}, {"text": ".78 11 ro Romanian 2,001,501 .85 .65 .78", "entities": []}, {"text": ".78 .82 .81 .79 .78 .78 12 cs Czech 2,001,203 .84", "entities": []}, {"text": ".64 .77 .78 .82 .80", "entities": []}, {"text": ".79", "entities": []}, {"text": ".79", "entities": []}, {"text": ".78 13 pl Polish 2,001,460 .85 .63", "entities": []}, {"text": ".78 .80 .82", "entities": []}, {"text": ".80 .78", "entities": []}, {"text": ".78 .78", "entities": []}, {"text": "14 nl Dutch 2,000,721 .85", "entities": []}, {"text": ".64", "entities": []}, {"text": ".78 .77 .80", "entities": []}, {"text": ".79 .77 .78 .77 15 no Norwegian ( Bokm \u02daal )", "entities": []}, {"text": "2,000,876 .84 .63 .77 .78 .82", "entities": []}, {"text": ".78 .78", "entities": []}, {"text": ".78 .77 16 tr Turkish 2,002,489 .84 .62 .78 .78 .80", "entities": []}, {"text": ".80", "entities": []}, {"text": ".75 .77 .77 17 ru Russian 2,001,317 .82 .64 .75 .80", "entities": []}, {"text": ".81 .77 .77 .77 .77 18 el Greek 2,001,704 .82 .63 .76 .78 .80", "entities": []}, {"text": ".78 .77", "entities": []}, {"text": ".78 .77 19 uk Ukrainian 2,001,261", "entities": []}, {"text": ".83 .63 .77 .78 .80 .77 .76 .77 .76 20 et Estonian 2,001,125 .83 .59 .75", "entities": []}, {"text": ".77 .81 .78 .77 .78 .76 21 ca Catalan 2,001,538 .84", "entities": []}, {"text": ".60 .80 .77 .79", "entities": []}, {"text": ".78 .76 .74", "entities": []}, {"text": ".76 22", "entities": []}, {"text": "da Danish 2,000,654 .84 .61 .77 .78 .79 .77 .73 .79 .76 23 lv Latvian 1,642,923 .82 .63 .75 .76 .79 .78 .76 .77 .76 24 lt Lithuanian 2,001,306 .83 .63 .77 .75 .79 .77 .75 .76 .76 25 bg Bulgarian 2,001,391 .82 .60 .76 .75", "entities": []}, {"text": ".77 .77 .73 .76 .74 26", "entities": []}, {"text": "he Hebrew 2,001,984 .80 .62 .72 .76 .78 .76 .74 .75", "entities": []}, {"text": ".74 27 zh Chinese 2,001,799 .79", "entities": []}, {"text": ".60 .75 .72 .77 .75", "entities": []}, {"text": ".75 .73 .73 28 mk Macedonian 1,356,402 .82 .54 .75", "entities": []}, {"text": ".77 .76 .73 .72", "entities": []}, {"text": ".74 .73", "entities": []}, {"text": "29 af Afrikaans 883,464 .80 .58 .74 .76", "entities": []}, {"text": ".75", "entities": []}, {"text": ".74 .71", "entities": []}, {"text": ".74 .73 30 tl Tagalog 716,272 .80 .56", "entities": []}, {"text": ".76 .70 .77 .76 .74 .72", "entities": []}, {"text": ".73 31", "entities": []}, {"text": "sk Slovak 2,001,221 .80 .60 .75", "entities": []}, {"text": ".74 .74", "entities": []}, {"text": ".73 .71", "entities": []}, {"text": ".73 .72 32 sq Albanian 1,169,697 .80 .57", "entities": []}, {"text": ".73 .75", "entities": []}, {"text": ".75 .75", "entities": []}, {"text": ".72 .72", "entities": []}, {"text": ".72", "entities": []}, {"text": "33 az Azerbaijani 2,002,146 .81 .60 .73 .74", "entities": []}, {"text": ".75", "entities": []}, {"text": ".73 .70", "entities": []}, {"text": ".71 .72", "entities": []}, {"text": "34 mn Mongolian 608,598 .78 .57", "entities": []}, {"text": ".73 .71", "entities": []}, {"text": ".78 .72", "entities": []}, {"text": ".74 .74 .72", "entities": []}, {"text": "35 hy Armenian 2,001,329 .80 .52 .72 .75 .77 .73 .71 .73 .72 36 eo Esperanto 2,001,575 .77 .55 .71 .72 .76 .74 .73", "entities": []}, {"text": ".73 .71 37 sl Slovenian 1,992,272 .81 .54", "entities": []}, {"text": ".75 .74", "entities": []}, {"text": ".74 .70", "entities": []}, {"text": ".70 .72 .71 38 hr Croatian 2,001,570 .78 .56", "entities": []}, {"text": ".71 .72 .74 .71 .71 .73 .71 39 gl Galician 1,336,256 .78 .53", "entities": []}, {"text": ".72 .72", "entities": []}, {"text": ".76 .74 .71 .71 .71 40 sr Serbian 2,002,395 .76 .57", "entities": []}, {"text": ".71 .72", "entities": []}, {"text": ".74 .70 .70 .73 .70 41 ar Arabic 2,003,155", "entities": []}, {"text": ".78 .53 .70 .70 .75 .72", "entities": []}, {"text": ".71 .74 .70", "entities": []}, {"text": "42 fa Persian 2,003,533 .77 .58", "entities": []}, {"text": ".70 .70 .74 .73", "entities": []}, {"text": ".70 .70 .70 43 ms Malay 1,213,397 .75 .58 .69 .69 .72 .70 .65 .73 .69 44 mr Marathi 848,549 .74 .54 .68 .70 .74 .70 .69 .71 .69 45 ka Georgian 1,567,232 .76 .52 .72 .70 .72 .71 .70 .66 .69 46 ja Japanese 2,003,306 .72 .58 .67 .68 .71 .70 .70 .68 .68", "entities": []}, {"text": "47 hi Hindi 1,879,196 .76 .56 .68 .69 .73 .64", "entities": []}, {"text": ".65 .72", "entities": []}, {"text": ".68 48 is Icelandic 945,214 .76 .55", "entities": []}, {"text": ".70 .68 .70 .69 .68 .64 .67 49 kk Kazakh 1,981,562 .72 .53 .65 .67", "entities": []}, {"text": ".73 .69 .67 .70 .67 50 ko Korean 2,002,600 .74 .57", "entities": []}, {"text": ".69 .67 .67 .66 .65 .69 .67 51", "entities": []}, {"text": "be Belarusian 1,715,582 .73 .49 .66", "entities": []}, {"text": ".68 .71 .67 .67 .70 .66 52 bn Bengali 1,471,709 .74", "entities": []}, {"text": ".50 .67 .67", "entities": []}, {"text": ".70 .67 .67 .66", "entities": []}, {"text": ".66 53 kn Kannada 1,747,421 .70 .47", "entities": []}, {"text": ".65 .67", "entities": []}, {"text": ".71 .68 .67 .68 .65", "entities": []}, {"text": "54", "entities": []}, {"text": "cy Welsh 502,006 .72 .51 .67", "entities": []}, {"text": ".64 .69 .65 .64", "entities": []}, {"text": ".66 .65 55 ur Urdu 1,157,969 .69 .52", "entities": [[4, 5, "DatasetName", "Urdu"]]}, {"text": ".61 .63", "entities": []}, {"text": ".70 .65 .64", "entities": []}, {"text": ".68 .64 56 ta Tamil 2,002,514 .70 .51 .66 .64 .66", "entities": []}, {"text": ".66 .63", "entities": []}, {"text": ".64", "entities": []}, {"text": ".64 57 eu Basque 1,828,013 .70 .46 .66", "entities": []}, {"text": ".64 .68 .67 .64 .64", "entities": []}, {"text": ".64 58 ml", "entities": []}, {"text": "Malayalam 2,002,920 .67 .51", "entities": []}, {"text": ".62 .63 .67 .67 .62", "entities": []}, {"text": ".61 .63 59 gu Gujarati 557,270 .69 .46", "entities": []}, {"text": ".62", "entities": []}, {"text": ".61 .67 .65 .63", "entities": []}, {"text": ".64 .62 60 si Sinhalese 812,356 .66", "entities": []}, {"text": ".48 .59", "entities": []}, {"text": ".65 .67", "entities": []}, {"text": ".62 .63", "entities": []}, {"text": ".65 .62", "entities": []}, {"text": "61 te Telugu 1,880,585 .69 .46", "entities": []}, {"text": ".62 .60", "entities": []}, {"text": ".65 .63", "entities": []}, {"text": ".61 .65", "entities": []}, {"text": ".61", "entities": []}, {"text": "62 ne Nepali 580,582 .68 .44 .62 .63", "entities": []}, {"text": ".65 .63", "entities": []}, {"text": ".61 .62", "entities": []}, {"text": ".61 63 tg Tajik 508,617 .67 .38", "entities": []}, {"text": ".64 .57", "entities": []}, {"text": ".65 .65", "entities": []}, {"text": ".60", "entities": []}, {"text": ".60 .60", "entities": []}, {"text": "64 vi Vietnamese 2,008,605 .65 .47 .58 .59 .65 .59 .58 .62 .59 65 pa Eastern Punjabi 403,997 .67 .37", "entities": [[16, 17, "DatasetName", "Punjabi"]]}, {"text": ".61 .59 .64 .61 .58 .62", "entities": []}, {"text": ".59 66 bs Bosnian", "entities": []}, {"text": "1,124,938 .63 .43 .60 .57", "entities": []}, {"text": ".64", "entities": []}, {"text": ".61 .61 .60 .58 67 ky Kirghiz 751,902", "entities": []}, {"text": ".65 .37", "entities": []}, {"text": ".61 .56 .64", "entities": []}, {"text": ".62 .59 .60 .58 68", "entities": []}, {"text": "ga Irish 321,249 .64 .47 .59", "entities": []}, {"text": ".58 .61 .61 .59", "entities": []}, {"text": ".55 .58 69 fy West Frisian", "entities": []}, {"text": "530,054 .61 .43 .54 .53", "entities": []}, {"text": ".60 .59", "entities": []}, {"text": ".55 .58 .56 70 uz Uzbek 833,860 .60", "entities": []}, {"text": ".38", "entities": []}, {"text": ".55 .56 .57", "entities": []}, {"text": ".56 .54", "entities": []}, {"text": ".53 .53", "entities": []}, {"text": "71 sw Swahili 391,312 .59 .34 .57", "entities": []}, {"text": ".52 .59 .58 .57", "entities": []}, {"text": ".51 .53", "entities": []}, {"text": "72 jv Javanese 518,634 .58 .45", "entities": []}, {"text": ".53 .53 .56 .58 .54", "entities": []}, {"text": ".49 .53 73 ps Pashto 300,927 .58 .40 .56 .52 .55", "entities": []}, {"text": ".54 .55", "entities": []}, {"text": ".49 .53 74 am Amharic 308,109 .56 .31 .52 .48 .53 .54", "entities": []}, {"text": ".52 .47 .49 75 lb Luxembourgish 642,504 .53 .37 .47", "entities": []}, {"text": ".45 .55 .52", "entities": []}, {"text": ".50 .51", "entities": []}, {"text": ".49 76", "entities": []}, {"text": "su Sundanese 327,533 .54", "entities": []}, {"text": ".36 .47 .45 .53 .52 .48", "entities": []}, {"text": ".52 .48 77 th Thai 2,006,540 .51 .38", "entities": []}, {"text": ".45", "entities": []}, {"text": ".50 .49 .46 .45", "entities": []}, {"text": ".49 .47 78 km Khmer 247,498 .51", "entities": []}, {"text": ".39 .44 .49 .51 .44 .45 .48 .46 79 sd Sindhi 139,063 .47", "entities": []}, {"text": ".35 .39 .41", "entities": []}, {"text": ".50 .49", "entities": []}, {"text": ".50 .46 .45 80 yi Yiddish 205,727 .49", "entities": []}, {"text": ".34 .40 .43 .50", "entities": []}, {"text": ".47 .45", "entities": []}, {"text": ".44 .44 81 my Burmese 339,628 .49", "entities": []}, {"text": ".36 .42", "entities": []}, {"text": ".43 .49", "entities": []}, {"text": ".45 .46 .43 .44", "entities": []}, {"text": "82 la Latin 1,088,139", "entities": []}, {"text": ".47", "entities": []}, {"text": ".33 .40 .39 .47", "entities": []}, {"text": ".46 .43", "entities": []}, {"text": ".44 .42", "entities": []}, {"text": "83 mt Maltese 204,630 .47 .32", "entities": []}, {"text": ".44 .38", "entities": []}, {"text": ".43 .40 .39 .38", "entities": []}, {"text": ".40 84 gd Scottish Gaelic 150,694 .45", "entities": [[2, 3, "DatasetName", "gd"]]}, {"text": ".36 .39 .40 .36", "entities": []}, {"text": ".36", "entities": []}, {"text": ".35 .33", "entities": []}, {"text": ".38 85", "entities": []}, {"text": "so Somali 177,405 .40 .22 .35 .36", "entities": []}, {"text": ".44 .41", "entities": []}, {"text": ".41 .38", "entities": []}, {"text": ".37 86", "entities": []}, {"text": "mg Malagasy 415,050 .40 .32", "entities": []}, {"text": ".36", "entities": []}, {"text": ".34 .41 .37 .36", "entities": []}, {"text": ".36 .37", "entities": []}, {"text": "87", "entities": []}, {"text": "ht Haitian 118,302 .39", "entities": []}, {"text": ".22", "entities": []}, {"text": ".33 .30", "entities": []}, {"text": ".42 .42", "entities": []}, {"text": ".37 .38 .35 88 ku Kurdish ( Kurmanji ) 395,645 .37 .22 .33", "entities": []}, {"text": ".33 .34 .33 .31", "entities": []}, {"text": ".35 .32 89", "entities": []}, {"text": "ceb Cebuano 2,006,001 .34 .22 .29 .34 .36 .32", "entities": []}, {"text": ".33 .34 .32 90 co Corsican 108,035 .29 .24 .27 .27 .32 .30", "entities": []}, {"text": ".29 .30 .29 91 yo Yoruba 156,764 .24", "entities": []}, {"text": ".08 .19 .18 .24", "entities": []}, {"text": ".21 .21 .26 .20", "entities": []}, {"text": "Table 8 : Overview of generated emotion lexicons with silver evaluation results ; sorted by Mean performance over the eight emotional variables .", "entities": [[6, 7, "DatasetName", "emotion"]]}]