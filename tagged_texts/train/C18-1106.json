[{"text": "Proceedings of the 27th International Conference on Computational Linguistics , pages 1246\u20131256 Santa Fe , New Mexico , USA , August 20 - 26 , 2018.1246Dialogue - act - driven Conversation Model : An Experimental Study Harshit Kumar IBM Research , Delhi , India harshitk@in.ibm.comArvind Agarwal IBM Research , Delhi , India arvagarw@in.ibm.comSachindra Joshi IBM Research , Delhi ,", "entities": [[37, 38, "DatasetName", "Kumar"]]}, {"text": "India jsachind@in.ibm.com Abstract", "entities": []}, {"text": "The utility of additional semantic information for the task of next utterance selection in an automated dialogue system is the focus of study in this paper .", "entities": []}, {"text": "In particular , we show that additional information available in the form of dialogue acts \u2013 when used along with context given in the form of dialogue history \u2013 improves the performance irrespective of the underlying model being generative or discriminative .", "entities": []}, {"text": "In order to show the model agnostic behavior of dialogue acts , we experiment with several well - known models such as sequence - to - sequence encoder - decoder model , hierarchical encoder - decoder model , and Siamese - based models with and without hierarchy ; and show that in all models , incorporating dialogue acts improves the performance by a signi\ufb01cant margin .", "entities": []}, {"text": "We , furthermore , propose a novel way of encoding dialogue act information , and use it along with hierarchical encoder to build a model that can use the sequential dialogue act information in a natural way .", "entities": []}, {"text": "Our proposed model achieves an MRR of about 84:8%for the task of next utterance selection on a newly introduced DailyDialog dataset , and outperform the baseline models .", "entities": [[5, 6, "MetricName", "MRR"], [19, 20, "DatasetName", "DailyDialog"]]}, {"text": "We also provide a detailed analysis of results including key insights that explain the improvement in MRR because of dialogue act information .", "entities": [[16, 17, "MetricName", "MRR"]]}, {"text": "1 Introduction In the last decade , natural language processing and machine learning \u2013 in particular deep learning \u2013 have come a long way towards building an automated dialogue system .", "entities": []}, {"text": "In a fully automated dialogue system , the goal is to predict an appropriate response given the dialogue history .", "entities": []}, {"text": "This problem of response prediction can be formulated in two ways .", "entities": []}, {"text": "One is purely generative , where the task is to generate a text response , i.e. generating a sentence or utterance from scratch , whereas the other is Next Utterance Selection , where the task is to select an appropriate response from a set of given candidates .", "entities": []}, {"text": "Despite signi\ufb01cant research in text generation , a pure generative model capable of generating syntactically and semantically correct text still remains a distant reality .", "entities": [[4, 6, "TaskName", "text generation"]]}, {"text": "There have been several efforts such as ( Vinyals and Le , 2015 ; Serban et al . , 2016a ; Serban et al . , 2016b ; Serban et al . , 2017b ) for the task of dialogue generation , however these models still do not seem to work in practice ( Liu et al . , 2016 ) .", "entities": [[39, 41, "TaskName", "dialogue generation"]]}, {"text": "This is particularly true for open domain dialogue systems .", "entities": []}, {"text": "Dialogue generation in a task - oriented oriented dialogue system , such as \ufb02ight - booking and troubleshooting , is much easier than in a non - task oriented dialogue system .", "entities": [[0, 2, "TaskName", "Dialogue generation"]]}, {"text": "This level of dif\ufb01culty arises because a non - task - oriented dialogue system has no prede\ufb01ned goal ( or domain ) , and the vocabulary and possibilities of the dialogues could be endless .", "entities": []}, {"text": "Given these challenges , researchers have de\ufb01ned a simpler problem for conversation modeling based on retrieval , i.e. next utterance selection .", "entities": []}, {"text": "In this paper we use this second formulation of the problem , and show that using additional information available in the form of dialogue acts help in improving the performance of the underlying model .", "entities": []}, {"text": "Dialogue acts ( DA ) are higher level semantic abstractions assigned to utterances in a conversation .", "entities": []}, {"text": "An example of a dialogue act for an utterance i \u2019ll give you a call tonight isInform since speaker is providing information .", "entities": []}, {"text": "In a traditional dialogue system , where dialogues are formulated by \ufb01rst sentence planning This work is licensed under a Creative Commons Attribution 4.0 International License .", "entities": []}, {"text": "License details : http:// creativecommons.org/licenses/by/4.0/", "entities": []}, {"text": "1247and then by surface realization , the \ufb01rst step is to understand the dialogue act of the utterance that needs to be generated , and then plan and realize the dialogue accordingly .", "entities": []}, {"text": "To better understand the importance of dialogue acts , consider an example of a simple conversation , where if the previous utterance is of type Question then the next utterance is most likely going to be of the type , i.e. Inform , providing information to that question .", "entities": []}, {"text": "Knowing that the next utterance is of type Inform , a conversation system with support of dialogue act information can \ufb01lter a set of candidate responses , and select the most appropriate one .", "entities": []}, {"text": "Driven by this intuition , we hypothesize that understanding dialogue acts and using them in the task of next utterance selection should improve the performance irrespective of the underlying model .", "entities": []}, {"text": "Driven by this intuition , we hypothesize that understanding dialogue acts and using them in the task of next utterance selection should improve the performance irrespective of the underlying model .", "entities": []}, {"text": "Most of the existing literature for the task of next utterance selection can be classi\ufb01ed into two categories .", "entities": []}, {"text": "First is based on Sequence - to - sequence models ( generative models ) ( Serban et al . , 2016a ; Serban et al . , 2017a ; Vinyals and Le , 2015 ) , where a model is trained to generate a response given context ; and the other is Siamese models ( discriminative models ) ( Lowe et al . , 2017 ) , where a model is trained to discriminate between positive and negative responses for a similar context .", "entities": []}, {"text": "In both types of models , at test time , a set of candidate responses is provided consisting of one correct response and several incorrect responses , and the model is evaluated on its ability to assign a higher rank to the true response .", "entities": []}, {"text": "In this paper , through the experimentation with both generative and discriminative types of models , we validate the hypothesis that additional information available in the form of dialogue act signi\ufb01cantly improves the performance irrespective of the underlying model .", "entities": []}, {"text": "In addition to showing the utility of dialogue acts , we propose a novel model that can use the sequential dialogue act information in a natural way .", "entities": []}, {"text": "More speci\ufb01cally , we propose a dialogue - act - driven hierarchical Siamese model .", "entities": []}, {"text": "Hierarchical models have shown to perform better than non - hierarchical models for the task of dialogue generation , whereas Siamese models have been shown to outperform the encoder - decoder based models for the task of next utterance selection .", "entities": [[16, 18, "TaskName", "dialogue generation"]]}, {"text": "In this paper , we combine both of these models , and further enhance them with a dialogue act encoder .", "entities": []}, {"text": "The proposed model has a hierarchical encoder which encodes the past utterances , and combine them with the representation of additional contextual information , obtained from the dialogue acts associated with the past utterances , to discriminate the correct response from the incorrect ones .", "entities": []}, {"text": "Our proposed model provides us the best of both worlds and outperforms the baseline models by a signi\ufb01cant margin .", "entities": []}, {"text": "Among others , a key contribution of this paper is that we do a deeper analysis of the reasons for the performance improvement due to inclusion of dialogue act and draw several important key insights such as , dialogue acts induce uniformity in the data , they aid in learning the right patterns .", "entities": []}, {"text": "We believe that these insights would inspire new research in this \ufb01eld and push the boundary even further .", "entities": []}, {"text": "The main contributions of this paper are as follows :", "entities": []}, {"text": "1 . For the task of next utterance selection , we validate the hypothesis that additional information available in the form of dialogue acts improves the performance irrespective of the underlying models .", "entities": []}, {"text": "2 . We propose a novel model that combines the strength of Siamese network with strengths of hierarchical structure inherent in the conversations and dialogue act information .", "entities": [[12, 14, "MethodName", "Siamese network"]]}, {"text": "The model gives us the best of all , and outperforms the baseline models by a signi\ufb01cant margin on the DailyDialog Dataset .", "entities": [[20, 21, "DatasetName", "DailyDialog"]]}, {"text": "3 . We perform a deeper analysis of the utility of the dialogue act information and draw three key insights : models learn dominant dialogue act patterns ; dialogue acts induce uniformity ; dialogue acts reinforce correct dialogue act patterns .", "entities": []}, {"text": "4 . We modify the DailyDialog ( Li et al . , 2017b ) dataset for the task of next utterance selection , and release it publicly along with the code - base of the proposed model1 .", "entities": [[5, 6, "DatasetName", "DailyDialog"]]}, {"text": "We believe that this dataset will work as a benchmark dataset for further research on this problem .", "entities": []}, {"text": "Similar benchmark datasets have been released earlier(Lowe et al . , 2015 ; Serban et al . , 2015 ) , however they do not come with dialogue act information .", "entities": []}, {"text": "1https://github.com/hk-bmi/ddialog-da-generation", "entities": []}, {"text": "12482 Approach In this section , we provide details of several existing models that we will use to validate our hypothesis .", "entities": []}, {"text": "These models include generative models ( such as encoder - decoder model and its hierarchical version i.e. , hierarchical encoder - decoder ) and discriminative model ( Siamese - based model ) .", "entities": []}, {"text": "Next , we provide details of the proposed model that adds the hierarchical structure to the Siamese model along with the dialogue act information .", "entities": []}, {"text": "To set the notations , we are given a set DofNconversations , i.e. D= ( C1;C2;:::CN ) , with each conversation Cibeing a sequence of Riutterances , Ci= ( u1;u2;:::u Ri ) .", "entities": []}, {"text": "Each utterance ujin turn is itself a sequence of Sjwords , i.e.uj= ( w1;w2;:::w Sj ) .", "entities": []}, {"text": "2.1 Generative Model Generative models are the most widely used models for conversation modeling .", "entities": []}, {"text": "These models include encoder - decoder model and hierarchical encoder - decoder model .", "entities": []}, {"text": "2.1.1 Encoder - decoder Model An encoder - decoder is a generative model that works on the idea of obtaining a representation of an input and use it for generating an output .", "entities": []}, {"text": "It has two main components , encoder and decoder .", "entities": []}, {"text": "The encoder encodes the \ufb01rst Kutterances , and the decoder uses that encoding to generate the next K+ 1th utterance .", "entities": []}, {"text": "In a conversation , all words in \ufb01rst Kutterances can be stringed together to form a single long chain and passed to an RNN encoder as following : ek = f1 embed ( wk)8k21;2 ; : : : he k = f1 rnn(he k\u00001;ek)8k21;2;:::(1 ) where , f1 embedrepresents the embedding layer , whereas f1 rnnis the encoder ( RNN ) .", "entities": [[39, 41, "HyperparameterName", "k ="]]}, {"text": "Let vbe the \ufb01nal output of the encoder which is considered as a representation of the entire context , and used to initialize the decoder ( another RNN ) .", "entities": []}, {"text": "Mathematically , the sequence of operations at the decoder are as follows : hd 0 = v hd k = f2 rnn(hd k\u00001;f2 embed ( wk))8k21;2;:::n\u00001 Pk = Logistic ( hd k):(2 )", "entities": [[14, 15, "DatasetName", "0"], [18, 20, "HyperparameterName", "k ="]]}, {"text": "Here , f2 embedrepresents the embedding layer .", "entities": []}, {"text": "Logistic is the \ufb01nal layer , which outputs the probability distribution over the vocabulary .", "entities": []}, {"text": "Encoder - decoder models are trained to maximize the likelihood of generating the next utterance , however , for the task of next utterance selection , they are tested based on the probability of generating the candidate utterances .", "entities": []}, {"text": "2.1.2 Hierarchical Encoder - decoder Model A simple encoder - decoder treats the \ufb01rst Kutterances as a single long chain of words , and therefore fails to leverage the hierarchical structure , which is an inherent part of a conversation .", "entities": []}, {"text": "Hierarchy is important for conversation modeling since it captures the natural dependency among utterances .", "entities": []}, {"text": "Several researchers ( Sordoni et al . , 2015 ; Serban et al . , 2016b ; Serban et al . , 2017b ; Dehghani et al . , 2017 ; Kumar et al . , 2017 ) have shown that hierarchical models outperform standard non - hierarchical models .", "entities": [[31, 32, "DatasetName", "Kumar"]]}, {"text": "Hierarchical models use two encoders to capture the hierarchical structure .", "entities": []}, {"text": "The \ufb01rst encoder , referred as utterance encoder , operates at the utterance level , encoding each word in each utterance .", "entities": []}, {"text": "The second encoder , referred as conversation encoder , operates at the conversation level , encoding each utterance in the conversation , based on the representations of the previous encoder .", "entities": []}, {"text": "These two encoders make sure that the output of the conversation encoder captures the dependencies among utterances .", "entities": []}, {"text": "For a given conversation , each word wkof each utterance ujis processed by an embedding layer , followed by an RNN which serves as the utterance encoder .", "entities": []}, {"text": "Similar to the encoder in equation ( 1 ) , an utterance encoder gives us a sequence of representations v1;v2;:::v K , corresponding to the \ufb01rst Kutterancesu1;u2;:::u Kin a conversation .", "entities": []}, {"text": "These representations are passed on to the conversation encoder , another RNN , which transformsvjto another representation gj .", "entities": []}, {"text": "The representation obtained from the last time - step of the", "entities": []}, {"text": "1249conversation - level encoder i.e. gKis considered as the representation of the entire conversation and used to initialize the decoder which works in the same way as Equation 2 . 2.2 Discriminative Model A decoder in the encoder - decoder model generates the next word given the context ,", "entities": []}, {"text": "and though it has several valid and reasonable choices , it is burdened with the task of generating exactly a particular choice that matches the ground truth .", "entities": []}, {"text": "For example , for a context I am enjoying the day , it is warm and sunny , if decoder generates yes , it is .", "entities": []}, {"text": "and the ground truth dictates yes , indeed , it is a lovely day , the decoder has failed , though it is a valid response .", "entities": []}, {"text": "Due to these challenges with generative models , discriminative models are trained directly to discriminate between positive and negative utterances .", "entities": []}, {"text": "A typical discriminative model , or in particular Siamese model , consists of two encoders , one encoder encoding the context , while another encoding the candidate utterance , i.e utterance K+ 1 .", "entities": []}, {"text": "These two representations are passed to a \ufb01nal layer that computes the probability of candidate being a valid response given the context .", "entities": []}, {"text": "Let h(1 ) andh(2)be the representations obtained from the \ufb01rst encoder and second encoder , respectively , then the probability of their association can be computed using the following expression .", "entities": []}, {"text": "p(sjh(1);h(2 ) )", "entities": []}, {"text": "= \u001b\u0000 h(1)TAh(2)+b\u0001 ( 3 ) where , the bias b and matrix A are learned model parameters .", "entities": []}, {"text": "2.3 Dialog - act - driven Models Dialogue acts are higher level abstractions assigned to utterances .", "entities": []}, {"text": "In our problem setting , we are given a list of dialogue acts da1;da 2;:::da K , corresponding to \ufb01rst Kutterances in the conversation .", "entities": []}, {"text": "These dialogue acts are treated as an additional sequence of signals that can aid in the learning process , and are passed through an encoder , denoted as Dialog - Act encoder ( DA - encoder ) .", "entities": []}, {"text": "The DA - encoder works on the same principle as the utterance encoder .", "entities": []}, {"text": "It builds a dialogue act vocabulary and uses that to learn dialogue act embeddings .", "entities": []}, {"text": "Similar to the utterance encoder , the input to the DA - encoder are one hot encodings of the dialogue acts , which are then passed through an embedding layer to learn DA embeddings .", "entities": []}, {"text": "These DA embeddings are sent to an RNN to learn dialogue act representations .", "entities": []}, {"text": "The sequence of operations for the DA - encoder are as follows : edak = f3 embed ( dak)8k21;2;:::K hdak = f4 rnn(hdak\u00001;edak)8k21;2;:::K : qK = hdaK(4 )", "entities": []}, {"text": "The output of the DA - encoder at the last time step ( qK ) gives us the representation of the entire DA sequence which is then used in the further modeling process in generative and discriminative models .", "entities": []}, {"text": "In generative models , it is used in the decoder by concatenating gKandqK , whereas in discriminative models , it is used along with encoder \u2019s output by combining gKwithqKthrough a linear combination .", "entities": []}, {"text": "2.4 Dialog - act - driven Hierarchical Siamese - Proposed Model Our proposed model , i.e. Dialog - act - driven Hierarchical Siamese Model ( HSiamese - DA ) , uses the following three components : a hierarchical encoder to obtain a representation that captures the dependencies among Kutterances ; an utterance encoder to obtain a representation of the candidate response , ( K+ 1)thutterance ; a DA - encoder ( Equation 4 ) that captures the dependencies among the dialogue acts of the \ufb01rstKutterances .", "entities": []}, {"text": "Let the representation obtained from the hierarchical encoder , DA - encoder and utterance encoder be gK , qKandvK+1 , respectively .", "entities": []}, {"text": "The two representations , gKandqK , are linearly combined to obtained a compositional representation of the context , which is then used along with candidate representation to compute the probability of associating the candidate response with the context using following expression : dK= \u000b \u0003gK+ ( 1\u0000 \u000b ) \u0003qK p(sjdK;vK+1 ) = \u001b(dT K\u0001A\u0001vK+1+b)(5 )", "entities": []}, {"text": "1250The model is trained by minimizing the cross - entropy of all labeled conversations including positive and negative examples .", "entities": []}, {"text": "At the test time , each conversation has Kutterances followed by a set of 10 candidates responses .", "entities": []}, {"text": "The system is tested in its ability to assign a higher rank to the true response .", "entities": []}, {"text": "3 Experiments In this section , we describe the details of the experiments , i.e. dataset and its preparation , baseline models , experimental setup , and analysis of results .", "entities": []}, {"text": "3.1 Dataset In our problem setting , we require a dataset that is of reasonable size2and has utterances annotated with the corresponding dialogue acts .", "entities": []}, {"text": "Although there are several available datasets(Serban et al . , 2015 ) , such as SwDA ( Switchboard Dialogue Act Corpus ( Jurafsky , 1997 ) ) , MRDA ( Meeting Recorder Dialogue Act corpus ( Janin et al . , 2003 ) ) , Ubuntu(Lowe et al . , 2015 ) , OpenSubtitles(Tiedemann , 2009 ) , etc . , they are not really suitable for our problem setting .", "entities": [[28, 29, "DatasetName", "MRDA"]]}, {"text": "Most of these datasets do not come with dialogue acts , and the ones which do ( i.e. SWDA and MRDA ) are small in size .", "entities": [[20, 21, "DatasetName", "MRDA"]]}, {"text": "Note that the SwDA and MRDA datasets contain 1003 and 51 conversations , respectively .", "entities": [[5, 6, "DatasetName", "MRDA"]]}, {"text": "To the best of our knowledge , a recently released dataset , DailyDialog(Li et", "entities": []}, {"text": "al . , 2017b ) , is the only dataset that has utterances annotated with dialogue acts and is large enough for conversation modeling methods to work .", "entities": []}, {"text": "Furthermore , in this dataset , conversations are non - task oriented , and each conversation focuses on one topic .", "entities": []}, {"text": "Each utterance is annotated with four dialogue acts as described in Table 1 .", "entities": []}, {"text": "The dataset has train , validation , and test splits of 11118 , 1000 , and 1000 conversations , respectively .", "entities": []}, {"text": "We evaluate and report our results on the DailyDialog dataset .", "entities": [[8, 9, "DatasetName", "DailyDialog"]]}, {"text": "In this paper , we hypothesize that dialogue acts improve conversation modeling .", "entities": []}, {"text": "However , it is not always possible that such dialogue acts are available in practice , and it would be ideal to predict dialogue acts \ufb01rst ( Kumar et al . , 2017 ) , and then use them for next utterance generation / retrieval ; having a model where both tasks , i.e. prediction and generation , are performed simultaneously may not be ideal for validating the hypothesis .", "entities": [[27, 28, "DatasetName", "Kumar"]]}, {"text": "Note that the error from the dialogue act prediction may propagate to the next utterance generation / retrieval .", "entities": []}, {"text": "Therefore , we intentionally did not use the predicted dialogue acts ( rather used the available dialogue acts ) to make sure that the insights about the usefulness of the dialogue acts are not corrupted due to the error in the upstream prediction model .", "entities": []}, {"text": "Dialogue Act Description Inform A speaker is providing information by means of a question or statement", "entities": []}, {"text": "Question A speaker intends to obtain information by asking a question", "entities": []}, {"text": "Directive A speaker is requesting , accept / reject offer , or making a suggestion Comissive A speaker accept / reject a request or suggestion Table 1 : Dialogue Acts and their description available in the DailyDialog Dataset .", "entities": [[36, 37, "DatasetName", "DailyDialog"]]}, {"text": "3.1.1 Dataset Preparation for Next Utterance Selection Task", "entities": []}, {"text": "The DailyDialog dataset in its original form is not directly useful for the task of next utterance selection , and hence requires preparation .", "entities": [[1, 2, "DatasetName", "DailyDialog"]]}, {"text": "The dataset has the dialogues from both the speakers .", "entities": []}, {"text": "Owing to the different conversational style of human and conversation agent , our objective is to build a model that is speci\ufb01c to the agent , i.e. bot .", "entities": [[10, 11, "DatasetName", "agent"], [24, 25, "DatasetName", "agent"]]}, {"text": "Therefore , we need to modify the dataset in such a way that we only consider those turns where we need to predict the bot \u2019s utterance .", "entities": []}, {"text": "To clarify further , consider the example conversation given in Table 2 .", "entities": []}, {"text": "The conversation has 8utterances , and each utterances is marked with the speaker , i.e. human ( H ) and bot ( B ) .", "entities": []}, {"text": "Since we are only interested in building bot - speci\ufb01c model , we only pick those subsequences from this conversation where the last utterance is \u201c B \u201d .", "entities": []}, {"text": "This gives us three subsequences : 1,2,3,4;3,4,5,6;5,6,7,8for a context of size 3 .", "entities": []}, {"text": "In each of these sub - conversations , 2Generative models such as sequence - to - sequence or discriminative models such as Siamese only perform better when there is large amount of data for training .", "entities": []}, {"text": "1251the \ufb01rst three utterances constitute the context , while the last utterance is the true response .", "entities": []}, {"text": "Our training data consists of such subsequences made up of 4utterances .", "entities": []}, {"text": "In the test data , each subsequence , in addition to these 4utterances , has 9more utterances selected randomly from the test pool , therefore a total of 13utterances .", "entities": []}, {"text": "These 9utterances along with the 4thresponse ( i.e. , true response ) utterance constitute the candidate pool .", "entities": []}, {"text": "With this data preparation exercise , the total number of conversations in train , test , and valid are 30515 , 2849 , and 2695 , respectively .", "entities": []}, {"text": "This version of dataset is used for training and testing generative models .", "entities": []}, {"text": "For discriminative models , data required is a bit different .", "entities": []}, {"text": "The training of discriminative models require positive examples and an equal number of negative examples .", "entities": []}, {"text": "Note that training data of the generative models did not have any negative examples .", "entities": []}, {"text": "In - order to prepare negative examples , we replicate each conversation by replacing the last utterance with a random utterance from the test data .", "entities": []}, {"text": "The test and valid dataset remain as in the generative models .", "entities": []}, {"text": "Thus , with this data preparation exercise , the total number of conversations in train , test , and valid are 61030 , 2849 , and 2695 , respectively .", "entities": []}, {"text": "I d Utterance DA 1", "entities": []}, {"text": "HHello , this is Mike , Kara .", "entities": []}, {"text": "I 2 BMike !", "entities": []}, {"text": "Good to hear from you .", "entities": []}, {"text": "How are you ?", "entities": []}, {"text": "Q 3 HEverything is \ufb01ne , and how are you ?", "entities": []}, {"text": "Q 4 BThings are going well with me .", "entities": []}, {"text": "I 5 HKara , I had fun the other night at the movies and was wondering if you would like to go out again this Friday .", "entities": []}, {"text": "D 6 BMike , I do n\u2019t think that it \u2019s a good idea to go out again .", "entities": []}, {"text": "C 7 HMaybe we could just meet for coffee or something .", "entities": []}, {"text": "D 8 BI ca n\u2019t really deal with any distractions right now , but I appreciate the nice evening we spent together .", "entities": []}, {"text": "C Table 2 : A snippet of a conversation showing few dialogues between a Human ( H ) and Bot ( B ) .", "entities": []}, {"text": "3.2 Baseline models and Proposed Model Here we list the baseline models , their modi\ufb01ed version enhanced with dialogue act information , and the proposed model .", "entities": []}, {"text": "Generative Models : \u2022ED-", "entities": []}, {"text": "It is a vanilla sequence to sequence model that uses an utterance encoder to obtain a representation of \ufb01rst Kutterances which is then used in a decoder to generate next utterance .", "entities": [[4, 7, "MethodName", "sequence to sequence"]]}, {"text": "\u2022HRED - An extension of sequence to sequence model that uses a hierarchical encoder to obtain a representation of \ufb01rst Kutterances , which is then used in decoder to generate next utterance .", "entities": [[5, 8, "MethodName", "sequence to sequence"]]}, {"text": "\u2022ED - DA - An extension of the ED model which uses dialogue act information .", "entities": []}, {"text": "It has a conditional decoder , that conditions the generation of each word on the dialogue acts representation .", "entities": []}, {"text": "\u2022HRED - DA - An extension of the HRED model which uses dialogue act information .", "entities": []}, {"text": "Similar to EDDA , it also has a conditional decoder that conditions the generation of each word on the dialogue acts representation .", "entities": []}, {"text": "Discriminative Models \u2022Siamese - Also known as Dual - Encoder , it uses two encoders ( both utterance encoders ) with shared weights , to produce the representation for the Kutterances and the ( K+ 1 ) utterance .", "entities": []}, {"text": "\u2022HSiamese - A Hierarchical version of the Siamese model that uses a hierarchical encoder to produce a representation for the Kutterances , and a plain encoder ( utterance encoder ) to produce a representation for the ( K+ 1 ) utterance .", "entities": []}, {"text": "1252\u2022Siamese - DA - An extension of Siamese model that uses the additional dialogue act information obtained through DA - encoder .", "entities": []}, {"text": "The representation obtained from the DA - encoder is linearly combined with the representation of the Kutterances obtained from an utterance encoder .", "entities": []}, {"text": "\u2022HSiamese - DA - The proposed model uses a Hierarchical Encoder and a DA - Encoder .", "entities": []}, {"text": "The representation obtained from the DA - Encoder is linearly combined with the representation obtained from the hierarchical encoder .", "entities": []}, {"text": "3.3 Hyper - parameter Tuning In our experiments , the parameters are tuned on validation set while the results are reported on test set .", "entities": []}, {"text": "Each utterance in a mini - batch was padded to the maximum length for that batch .", "entities": []}, {"text": "The maximum batch size allowed was 32 .", "entities": [[2, 4, "HyperparameterName", "batch size"]]}, {"text": "The word vectors were initialized with the 300 - dimensional Glove embeddings ( Pennington et al . , 2014 ) , and were also updated during training .", "entities": [[10, 12, "MethodName", "Glove embeddings"]]}, {"text": "For the generative models , the utterance encoder , conversation encoder , DA - Encoder and decoder are all GRUs with rnnsize set to 1000 ( optimized over 100to1200 in steps of 100 ) .", "entities": []}, {"text": "For the discriminative model , the utterance encoder , conversation encoder , and DA - Encoder are all GRUs with rnnsize set to 300(optimized over 100to 500 in steps of 100 ) .", "entities": []}, {"text": "Dropout of 0:1(optimized over 0:0to0:7 in steps of 0.1 ) was applied to embeddings obtained from the output of conversation encoder .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "Note that , dropout was not used in the discriminative model and its variations .", "entities": []}, {"text": "Models were trained to minimize cross entropy using Adam optimizer with learning rate of 0:0003 ( optimized over 0:0001 , 0:0003 , 0:0005 , 0:0007 , 0:001 ) .", "entities": [[8, 9, "MethodName", "Adam"], [9, 10, "HyperparameterName", "optimizer"], [11, 13, "HyperparameterName", "learning rate"]]}, {"text": "We found that a higher learning rate up - to 0:0005 helps the model to learn quickly , whereas learning rate greater than 0:0005 leads to oscillations .", "entities": [[5, 7, "HyperparameterName", "learning rate"], [19, 21, "HyperparameterName", "learning rate"]]}, {"text": "3.4 Results and Discussion In this section , we present results of our experimental study , followed by its analysis .", "entities": []}, {"text": "3.4.1 Performance Evaluation Since our problem formulation is retrieval based , we use standard IR metrics such as Mean Reciprocal Rank ( MRR ) and Recall@k as our evaluation metrics .", "entities": [[22, 23, "MetricName", "MRR"]]}, {"text": "MRR is calculated as the mean of the reciprocal rank of the true candidate response among other candidate responses .", "entities": [[0, 1, "MetricName", "MRR"]]}, {"text": "Recall @kmeasures whether the true candidate response appears in a ranked list of kresponses .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "In this work , our hypothesis is that additional information about utterances available in the form of dialogue acts helps irrespective of the underlying model , i.e. generative or discriminative .", "entities": []}, {"text": "Results in Table 3 support our hypothesis .", "entities": []}, {"text": "These results clearly indicate that the MRR of the true candidate response improves when dialogue acts of previous utterances are provided .", "entities": [[6, 7, "MetricName", "MRR"]]}, {"text": "From these tables we see that for all underlying models , the dialogue act version performs better than non dialogue act version .", "entities": []}, {"text": "These results furthermore indicate that hierarchical version performs better than non - hierarchical version for both generative and discriminative models .", "entities": []}, {"text": "In the generative case , the plain ED has an MRR of 0:474 , whereas the same model , when conditioned with DA - Encoder , has an MRR of 0:54 , an improvement of 13.9 % .", "entities": [[10, 11, "MetricName", "MRR"], [28, 29, "MetricName", "MRR"]]}, {"text": "The hierarchical encoder - decoder HRED and HRED - DA has an MRR of 0:523and0:583 , respectively , an improvement of 11.4 % .", "entities": [[12, 13, "MetricName", "MRR"]]}, {"text": "Generative models are sequence - to - sequence models and rather complex in nature , so it is interesting to note that even a much simpler discriminative model , i.e. plain Siamese model , without any dialogue act information , has an MRR of 0:8compared to 0:58of the best performing generative model , i.e. HRED - DA .", "entities": [[42, 43, "MetricName", "MRR"]]}, {"text": "This observation demonstrates the strength of the discriminative models , and therefore is a motivation behind the proposed model .", "entities": []}, {"text": "The proposed model improves these baseline numbers by incorporating hierarchy and dialogue act information , and pushes the MRR to 0:848 .", "entities": [[18, 19, "MetricName", "MRR"]]}, {"text": "3.4.2 Performance Analysis While we have shown that using dialogue act information does help in the next utterance selection task , in this section , we dig deeper and understand reasons for it .", "entities": []}, {"text": "In order to do that , we analyze the dialogue act distribution of the test data and model outputs .", "entities": []}, {"text": "Although all Kdialogue acts corresponding to K utterances in the context might play a role in ranking candidate utterances , the following analysis only", "entities": []}, {"text": "1253MRR R@1 R@2 R@5", "entities": [[1, 2, "MetricName", "R@1"], [3, 4, "MetricName", "R@5"]]}, {"text": "ED 0:474 0 : 327 0 : 405 0 : 639 ED - DA 0:54 0 : 407 0 : 478 0 : 690 HRED 0:523 0 : 384 0 : 471 0 : 676 HRED - DA 0:583 0 : 448 0 : 542 0 : 742 ( a ) Generative ModelsMRR R@1 R@2 R@5", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [21, 22, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [29, 30, "DatasetName", "0"], [32, 33, "DatasetName", "0"], [39, 40, "DatasetName", "0"], [42, 43, "DatasetName", "0"], [45, 46, "DatasetName", "0"], [53, 54, "MetricName", "R@1"], [55, 56, "MetricName", "R@5"]]}, {"text": "Siamese 0:800 0 : 711 0 : 785 0.949 Siamese - DA 0:844 0 : 784 0.824 0:944", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [13, 14, "DatasetName", "0"]]}, {"text": "HSiamese 0:817 0 : 743 0 : 792 0 : 948 HSiamese - DA 0.848 0.795 0:821 0 : 932 ( b ) Discriminative Models Table 3 : Comparison of different models with and without dialogue acts uses the pairs of dialogue acts , i.e. dialogue acts of Kthand(K+ 1)thutterances .", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [17, 18, "DatasetName", "0"]]}, {"text": "Tables 4(a ) , 4(b ) and 4(c ) show the distribution of such dialogue act pairs for test data , HSiamese , and HSiamese - DA models respectively .", "entities": []}, {"text": "Here , rows indicate the dialogue act of Kthutterance , whereas columns indicate the dialogue act of ( K+ 1)thutterance .", "entities": []}, {"text": "A cell value indicates the count of utterance pairs with the respective dialogue act combinations where ( K+ 1)thutterance was ranked 1 .", "entities": []}, {"text": "Note that in the test data , ( K+ 1)thutterance is the true candidate response and always have the rank 1 .", "entities": []}, {"text": "For instance , there are 742utterance pairs in the test data , where Kthand(K+ 1)thutterances have dialogue acts QandI , respectively , however , out of those 742instances , HSiamese ranked only 605as 1 while HSiamese - DA ranked 638as 1 .", "entities": []}, {"text": "From these tables , we draw following observations .", "entities": []}, {"text": "I Q D C I 600 336 176 16 Q 742 73 87 2 D 26 75 56 305 C 69 50 76 6 ( a ) Ground - truth test dataI Q D C R@1", "entities": [[35, 36, "MetricName", "R@1"]]}, {"text": "I", "entities": []}, {"text": "403", "entities": []}, {"text": "208 115 9 0:65 Q 605 34 64 2 0:78 D 16 41 42 192 0:63 C 48 33 56 4 0:70 ( b )", "entities": []}, {"text": "HSiameseI Q D C R@1", "entities": [[4, 5, "MetricName", "R@1"]]}, {"text": "I", "entities": []}, {"text": "446 253 121 12 0:74 Q 638 43 61 2 0:82 D 17 58 42 243 0:78 C 52 39 68 5 0:82 ( c ) HSiamese - DA Table 4 : Number of rank-1 conversations and theie DAs for Kthand(K+ 1)thutterances .", "entities": [[34, 35, "MetricName", "rank-1"]]}, {"text": "Models Learn Dominant Patterns : The \ufb01rst is that there are certain dominant communication patterns that we observe in both , test data and model outputs ( See Table 4 ) , suggesting that models are able to learn these patterns and retain them in their outputs .", "entities": []}, {"text": "We observe that a Question is often followed by an Information , whereas an Information can be followed by another Information or aQuestion .", "entities": []}, {"text": "ADirective tends to be followed by Commissive .", "entities": []}, {"text": "These communication patterns not only make sense intuitively but they are also in agreement with previous studies ( Li et al . , 2017b ; Ribeiro et al . , 2015 ) .", "entities": []}, {"text": "Dialogue Acts Bring Uniformity : The second and a rather more important observation is that dialogue acts help the most for the dialogue act class ( DA - class ) when the utterances belonging to that class are non - uniform in their linguistic construct .", "entities": []}, {"text": "In order to better exlpain this , we \ufb01rst compute the break - up of recall@ 1according to the dialogue act classes .", "entities": []}, {"text": "A DA - class of a conversation in the test data is de\ufb01ned based on the dialogue act of the last utterance ( Kthutterance ) in the context .", "entities": []}, {"text": "These numbers are shown in the last column of Tables 4(b ) and 4(c ) for the respective models .", "entities": []}, {"text": "In Table 4(b ) , \ufb01rst row in recall@1 column is 0:65 , which indicates that out of the total number of test conversations where dialogue act of the last utterance of context was I , 65 % of true candidate responses were ranked 1 by the HSiamese model .", "entities": [[8, 9, "MetricName", "recall@1"]]}, {"text": "Such a DA - class wise breakup of the recall@1 numbers helps us do an analysis with respect to individual DA - classes .", "entities": [[9, 10, "MetricName", "recall@1"]]}, {"text": "From this break - up , it is clear that for the HSiamese model , Question DA - class has the best performance of 78 % whereas Directive has the worst performance of 63 % .", "entities": []}, {"text": "This difference can be attributed to the fact that all utterances with dialogue act as Question have rather uniform construct .", "entities": []}, {"text": "Some examples of Question utterances are , \u2018 Q : Do you have a fever ? \u2019", "entities": []}, {"text": "and \u2018 Q : Why do you want to work for our company ? \u2019", "entities": []}, {"text": ", while the examples of Directive utterances are , \u2018 D : when we have the \ufb01nal results , we will call you . \u2019", "entities": []}, {"text": "and \u2018 D : we will take the trip .", "entities": []}, {"text": "could you give us a pamphlet ? \u2019 .", "entities": []}, {"text": "From these examples , we observe that utterances belonging to DA - class Question have rather uniform construct in terms of linguistic features , whereas utterances belonging to DA - class Directive are ambiguous \u2013 some of the utterances of type Directive can be easily confused for Question .", "entities": []}, {"text": "This uniformity makes the learning task easier for Question class , and thereby giving us better results in the next utterance selection", "entities": []}, {"text": "1254task , even for the model that does not use the dialogue act information .", "entities": []}, {"text": "This performance difference reduces when we provide the dialogue act information along with the textual content ( See Table 4(c ) ) .", "entities": []}, {"text": "Inclusion of dialogue act information within a non - uniform class such as Directive brings in uniformity , and therefore , results in signi\ufb01cant performance improvement .", "entities": []}, {"text": "In the case of Directive , we see as much as15percentage point improvement .", "entities": []}, {"text": "Similar improvement pattern has been observed for DA - class Information andComissive .", "entities": []}, {"text": "Similar to Directive , utterances belonging to Comissive have rather nonuniform construct , and with the availability of dialogue acts , this DA - class is able to gain much more than the Information DA - class .", "entities": []}, {"text": "I Q D C Total", "entities": []}, {"text": "I 43 45 6 3 97 Q 33 9\u00003 0 39 D 1 17 0 51 69 C 4 6 12 1 23 Total 81 77 15 55 228 Table 5 : Difference of number of rank-1 conversations between HSiamese - DA and HSiamese .", "entities": [[9, 10, "DatasetName", "0"], [14, 15, "DatasetName", "0"], [36, 37, "MetricName", "rank-1"]]}, {"text": "Dialogue Acts Help Model Learn the Right Patterns :", "entities": []}, {"text": "In Table 5 , we show the relative improvement of HSiamese - DA model over HSiamese .", "entities": []}, {"text": "From this table , we observe that there are a total of 228conversations where the proposed model was able to improve the ranking of true candidate response to 1 .", "entities": []}, {"text": "We further observe that the biggest improvement is in I!I , I!Q , Q!I , andD!C , which make sense intuitively .", "entities": []}, {"text": "These are dominant patterns observed in the training data which should be preserved in the model output as well , however these patterns will only be preserved when model is able to capture the correct dialogue act information .", "entities": []}, {"text": "Since in many cases DandQhave similar construct , without explicit dialogue act information , a model may get confused and may learn patterns not observed in the training data .", "entities": []}, {"text": "For example , Q!IandD!Care the dominant and right patterns in the training data , however in the absence of explicit dialogue act information , the model may get confused between DandQand may learnD!IandQ!Cinstead of the dominant patterns i.e. Q!IandD!C. With the explicit dialogue act information , this ambiguity is alleviated and model learns the right patterns as demonstrated by Table 5 .", "entities": []}, {"text": "Similar observations are true for other two constructs , i.e. Information andCommisive .", "entities": []}, {"text": "Both are rather similar in construct , \u2018 I : No , thank you \u2019 , \u2018 I : It does n\u2019t matter .", "entities": []}, {"text": "it happens to everyone . \u2019", "entities": []}, {"text": "and \u2018 C : I knew you \u2019d see it my way . \u2019 , \u2018 C : Ok , i am ready to think of other things . \u2019 , and there is no obvious distinguishing factor .", "entities": []}, {"text": "However , providing explicit DA information helps in disambiguation , and learn the patterns that are observed in the training data such as I!I , I!Q. 4 Related Work In conversation modeling , the most basic problem is to generate a response given a context .", "entities": []}, {"text": "Several efforts have been made towards solving the problem of dialogue generation(Vinyals and Le , 2015 ; Liu et al . , 2016 ; Li et", "entities": []}, {"text": "al . , 2015 ) , however , due to the inherent dif\ufb01culty of the problem , these efforts have only had limited success and are known to have issues like generating repetitive and generalized responses such as I do n\u2019t know orOk .", "entities": []}, {"text": "For the task of Next Utterance Selection , which is a relatively simpler problem than generation , though existing generative models can be easily adopted , their counterpart discriminative models have shown to have better performance .", "entities": []}, {"text": "In generative models , the most notable work is from ( Vinyals and Le , 2015 ) , however this work considers the context as a \ufb02at long string of words and ignores the hierarchical structure .", "entities": []}, {"text": "Researchers have proposed hierarchical model ( Serban et al . , 2016b ) and their variations ( Serban et al . , 2017b ; Serban et al . , 2017a ; Li et", "entities": []}, {"text": "al . , 2017a ) but none of these models take into account the dialogue act information .", "entities": []}, {"text": "In Discriminative models , such as Siamese , a very notable work by ( Kannan et al . , 2016 ) , smart reply , retrieves the most likely response from a set of candidate response clusters .", "entities": []}, {"text": "( Lowe et al . , 2017 ) has used a retrieval based Siamese model and shown its results on the Ubuntu corpus .", "entities": []}, {"text": "Our", "entities": []}, {"text": "1255proposed model builds upon the strengths of generative and discriminative models , and uses hierarchy along with the dialogue act information to achieve the best performance .", "entities": []}, {"text": "A recent work by ( Zhao et al . , 2017 ) has used dialogue acts for the task of dialogue generation .", "entities": [[20, 22, "TaskName", "dialogue generation"]]}, {"text": "Our work complements their \ufb01ndings , and further show that dialogue acts improve the model performance across the board irrespective of underlying model ( i.e. generative or discriminative models ) and for the task of next utterance selection .", "entities": []}, {"text": "5 Conclusion For the task of next utterance selection , we show that dialogue acts helps achieve better performance irrespective of the underlying model , be it generative or discriminative .", "entities": []}, {"text": "We also propose a novel discriminative model that leverages the hierarchical structure in a conversation and dialogue act information to produce much improved results , an MRR of 0.848 .", "entities": [[26, 27, "MetricName", "MRR"]]}, {"text": "Our results not only show the improvement in performance , but we also present key reasons for it by doing a detailed analysis and drawing key insights that the inclusion of dialogue act information induces uniformity and removes ambiguity .", "entities": []}, {"text": "References Mostafa Dehghani , Sascha Rothe , Enrique Alfonseca , and Pascal Fleury .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Learning to attend , copy , and generate for session based query suggestion .", "entities": []}, {"text": "In CIKM .", "entities": []}, {"text": "Adam Janin , Don Baron , Jane Edwards , Dan Ellis , David Gelbart , Nelson Morgan , Barbara Peskin , Thilo Pfau , Elizabeth Shriberg , Andreas Stolcke , et al . 2003 .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "The icsi meeting corpus .", "entities": []}, {"text": "In ICASSP .", "entities": []}, {"text": "Dan Jurafsky .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Switchboard swbd - damsl shallow - discourse - function annotation coders manual .", "entities": []}, {"text": "www .", "entities": []}, {"text": "dcs .", "entities": []}, {"text": "shef .", "entities": []}, {"text": "ac .", "entities": []}, {"text": "uk / nlp / amities/\ufb01les / bib / ics - tr-97 - 02 . pdf .", "entities": []}, {"text": "Anjuli Kannan , Karol Kurach , Sujith Ravi , Tobias Kaufmann , Andrew Tomkins , Balint Miklos , Greg Corrado , L \u00b4 aszl\u00b4o Luk \u00b4 acs , Marina Ganea , Peter Young , et al . 2016 .", "entities": []}, {"text": "Smart reply : Automated response suggestion for email .", "entities": []}, {"text": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 955\u2013964 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Harshit Kumar , Arvind Agarwal , Riddhiman Dasgupta , Sachindra Joshi , and Arun Kumar . 2017 .", "entities": [[1, 2, "DatasetName", "Kumar"], [14, 15, "DatasetName", "Kumar"]]}, {"text": "Dialogue act sequence labeling using hierarchical encoder with crf . arXiv preprint arXiv:1709.04250 .", "entities": [[8, 9, "MethodName", "crf"], [10, 11, "DatasetName", "arXiv"]]}, {"text": "Jiwei Li , Michel Galley , Chris Brockett , Jianfeng Gao , and Bill Dolan .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A diversity - promoting objective function for neural conversation models .", "entities": []}, {"text": "In NAACL .", "entities": []}, {"text": "Jiwei Li , Will Monroe , Tianlin Shi , Alan Ritter , and Dan Jurafsky .", "entities": []}, {"text": "2017a .", "entities": []}, {"text": "Adversarial learning for neural dialogue generation .", "entities": [[4, 6, "TaskName", "dialogue generation"]]}, {"text": "arXiv preprint arXiv:1701.06547 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yanran Li , Hui Su , Xiaoyu Shen , Wenjie Li , Ziqiang Cao , and Shuzi Niu . 2017b .", "entities": []}, {"text": "Dailydialog : A manually labelled multi - turn dialogue dataset .", "entities": [[0, 1, "DatasetName", "Dailydialog"]]}, {"text": "arXiv preprint arXiv:1710.03957 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Chia - Wei Liu , Ryan Lowe , Iulian V Serban , Michael Noseworthy , Laurent Charlin , and Joelle Pineau .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "How not to evaluate your dialogue system : An empirical study of unsupervised evaluation metrics for dialogue response generation .", "entities": [[17, 19, "TaskName", "response generation"]]}, {"text": "arXiv preprint arXiv:1603.08023 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ryan Lowe , Nissan Pow , Iulian V Serban , and Joelle Pineau . 2015 .", "entities": []}, {"text": "The ubuntu dialogue corpus : A large dataset for research in unstructured multi - turn dialogue systems .", "entities": []}, {"text": "In 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue , page 285 .", "entities": []}, {"text": "Ryan Thomas Lowe , Nissan Pow , Iulian Vlad Serban , Laurent Charlin , Chia - Wei Liu , and Joelle Pineau . 2017 .", "entities": []}, {"text": "Training end - to - end dialogue systems with the ubuntu dialogue corpus .", "entities": []}, {"text": "Dialogue & Discourse , 8(1):31\u201365 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher D. Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In EMNLP .", "entities": []}, {"text": "Eug\u00b4enio Ribeiro , Ricardo Ribeiro , and David Martins de Matos . 2015 .", "entities": []}, {"text": "The in\ufb02uence of context on dialogue act recognition .", "entities": []}, {"text": "arXiv preprint arXiv:1506.00839 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Iulian Vlad Serban , Ryan Lowe , Peter Henderson , Laurent Charlin , and Joelle Pineau . 2015 .", "entities": []}, {"text": "A survey of available corpora for building data - driven dialogue systems .", "entities": []}, {"text": "arXiv preprint arXiv:1512.05742 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Iulian Vlad Serban , Ryan Lowe , Laurent Charlin , and Joelle Pineau .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "Generative deep neural networks for dialogue : A short review .", "entities": []}, {"text": "arXiv preprint arXiv:1611.06216 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Iulian Vlad Serban , Alessandro Sordoni , Yoshua Bengio , Aaron C Courville , and Joelle Pineau .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "Building end - to - end dialogue systems using generative hierarchical neural network models .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "1256Iulian Vlad Serban , Tim Klinger , Gerald Tesauro , Kartik Talamadupula , Bowen Zhou , Yoshua Bengio , and Aaron C Courville .", "entities": []}, {"text": "2017a .", "entities": []}, {"text": "Multiresolution recurrent neural networks : An application to dialogue response generation .", "entities": [[9, 11, "TaskName", "response generation"]]}, {"text": "In AAAI , pages 3288 \u2013 3294 .", "entities": []}, {"text": "Iulian Vlad Serban , Alessandro Sordoni , Ryan Lowe , Laurent Charlin , Joelle Pineau , Aaron C Courville , and Yoshua Bengio .", "entities": []}, {"text": "2017b .", "entities": []}, {"text": "A hierarchical latent variable encoder - decoder model for generating dialogues .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "Alessandro Sordoni , Yoshua Bengio , Hossein Vahabi , Christina Lioma , Jakob Grue Simonsen , and Jian - Yun Nie . 2015 .", "entities": []}, {"text": "A hierarchical recurrent encoder - decoder for generative context - aware query suggestion .", "entities": []}, {"text": "In CIKM .", "entities": []}, {"text": "J\u00a8org Tiedemann .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "News from opus - a collection of multilingual parallel corpora with tools and interfaces .", "entities": []}, {"text": "In Recent advances in natural language processing , volume 5 , pages 237\u2013248 .", "entities": []}, {"text": "Oriol Vinyals and Quoc Le . 2015 .", "entities": []}, {"text": "A neural conversational model .", "entities": []}, {"text": "arXiv preprint arXiv:1506.05869 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tiancheng Zhao , Ran Zhao , and Maxine Eskenazi . 2017 .", "entities": []}, {"text": "Learning discourse - level diversity for neural dialog models using conditional variational autoencoders .", "entities": [[12, 13, "MethodName", "autoencoders"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , volume 1 , pages 654\u2013664 .", "entities": []}]