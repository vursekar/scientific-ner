[{"text": "Proceedings of the 6th Workshop on Representation Learning for NLP ( RepL4NLP-2021 ) , pages 163\u2013173 Bangkok , Thailand ( Online ) , August 6 , 2021 .", "entities": [[6, 8, "TaskName", "Representation Learning"]]}, {"text": "\u00a9 2021 Association for Computational Linguistics163In - Batch Negatives for Knowledge Distillation with Tightly - Coupled Teachers for Dense Retrieval Sheng - Chieh Lin\u0003 , Jheng - Hong Yang\u0003and Jimmy Lin David R. Cheriton School of Computer Science University of Waterloo", "entities": [[10, 12, "MethodName", "Knowledge Distillation"]]}, {"text": "Abstract We present an ef\ufb01cient training approach to text retrieval with dense representations that applies knowledge distillation using the ColBERT late - interaction ranking model .", "entities": [[15, 17, "MethodName", "knowledge distillation"]]}, {"text": "Specifically , we propose to transfer the knowledge from a bi - encoder teacher to a student by distilling knowledge from ColBERT \u2019s expressive MaxSim operator into a simple dot product .", "entities": []}, {"text": "The advantage of the bi - encoder teacher \u2013 student setup is that we can ef\ufb01ciently add inbatch negatives during knowledge distillation , enabling richer interactions between teacher and student models .", "entities": [[20, 22, "MethodName", "knowledge distillation"]]}, {"text": "In addition , using ColBERT as the teacher reduces training cost compared to a full cross - encoder .", "entities": []}, {"text": "Experiments on the MS MARCO passage and document ranking tasks and data from the TREC 2019", "entities": [[3, 5, "DatasetName", "MS MARCO"], [7, 9, "TaskName", "document ranking"], [14, 15, "DatasetName", "TREC"]]}, {"text": "Deep Learning Track demonstrate that our approach helps models learn robust representations for dense retrieval effectively and ef\ufb01ciently .", "entities": []}, {"text": "1 Introduction For well over half a century , solutions to the ad hocretrieval problem \u2014 where the system \u2019s task is return a list of top ktexts from an arbitrarily large corpusDthat maximizes some metric of quality such as average precision or NDCG \u2014 has been dominated by sparse vector representations , for example , bag - of - words BM25 .", "entities": [[40, 42, "MetricName", "average precision"]]}, {"text": "Even in modern multi - stage ranking architectures , which take advantage of large pretrained transformers such as BERT ( Devlin et al . , 2019 ) , the models are deployed as rerankers over initial candidates retrieved based on sparse vector representations ; this is sometimes called \u201c \ufb01rst - stage retrieval \u201d .", "entities": [[18, 19, "MethodName", "BERT"]]}, {"text": "One well - known example of this design is the BERT - based reranker of Nogueira and Cho ( 2019 ) ; see Lin et al .", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "( 2020 ) for a recent survey .", "entities": []}, {"text": "\u0003Contributed equally .", "entities": []}, {"text": "The standard reranker architecture , while effective , exhibits high query latency , on the order of seconds per query ( Hofst\u00e4tter and Hanbury , 2019 ; Khattab and Zaharia , 2020 ) because expensive neural inference must be applied at query time on query \u2013 passage pairs .", "entities": []}, {"text": "This design is known as a cross - encoder ( Humeau et al . , 2020 ) , which exploits query \u2013 passage attention interactions across all transformer layers .", "entities": []}, {"text": "As an alternative , a biencoder design provides an approach to ranking with dense representations that is far more ef\ufb01cient than cross - encoders ( Lee et al . , 2019 ; Reimers and Gurevych , 2019 ; Khattab and Zaharia , 2020 ; Karpukhin et al . , 2020 ; Luan et al . , 2021 ; Xiong et al . , 2021 ; Qu et al . , 2020 ; Hofst\u00e4tter et al . , 2021 ) .", "entities": []}, {"text": "Prior to retrieval , the vector representations can be precomputed for each of the texts in a corpus .", "entities": []}, {"text": "When retrieving texts in response to a given query , computationally expensive transformer inference is replaced by much faster approximate nearest neighbor ( ANN ) search ( Liu et al . , 2004 ; Malkov and Yashunin , 2020 ) .", "entities": []}, {"text": "Recently , researchers have proposed bi - encoders that produce multiple vectors to represent a query ( or a passage ) ( Humeau et al . , 2020 ; Luan et al . , 2021 ; Khattab and Zaharia , 2020 ) , which have proven to be effective both theoretically and empirically .", "entities": []}, {"text": "However , the main disadvantage of these designs is their high storage requirements .", "entities": []}, {"text": "For example , ColBERT ( Khattab and Zaharia , 2020 ) requires storing all the WordPiece token vectors of each text ( passage ) in the corpus .", "entities": [[15, 16, "MethodName", "WordPiece"]]}, {"text": "On the MS MARCO passage corpus comprising 8.8 M passages , for example , this requires 154 GiB. Of course , a common alternative is to produce single vectors for queries and passages ( Reimers and Gurevych , 2019 ) .", "entities": [[2, 4, "DatasetName", "MS MARCO"]]}, {"text": "Although this design is less storage - demanding , it sacri\ufb01ces ranking effectiveness since its structure breaks rich interactions between queries and passages compared to", "entities": []}, {"text": "164multi - vector bi - encoders or cross - encoders", "entities": []}, {"text": ".", "entities": []}, {"text": "Hence , improving the effectiveness of single - vector biencoders represents an important problem .", "entities": []}, {"text": "One approach to improving the effectiveness of single - vector bi - encoders is hard negative mining , by training with carefully selected negative examples that emphasize discrimination between relevant and non - relevant texts .", "entities": []}, {"text": "There are several approaches to accomplish this .", "entities": []}, {"text": "Karpukhin et", "entities": []}, {"text": "al . ( 2020 ) and Qu et al . ( 2020 ) leverage large in - batch negatives to enrich training signals .", "entities": []}, {"text": "Guu et al .", "entities": []}, {"text": "( 2020 ) and Xiong et al .", "entities": []}, {"text": "( 2021 ) propose to mine hard negatives using the trained bi - encoder itself .", "entities": []}, {"text": "By searching for global negative samples from an asynchronously updated ANN index , the bi - encoder can learn information not present in the training data produced by sparse representations ( Xiong et al . , 2021 ) .", "entities": []}, {"text": "However , both large in - batch negative sampling and asynchronous ANN index updates are computationally demanding .", "entities": []}, {"text": "The later is especially impractical for large corpora since it requires periodic inference over alltexts in the corpus to ensure that the best negative examples are retrieved .", "entities": []}, {"text": "There is also work that explores knowledge distillation ( KD ) ( Hinton et al . , 2015 ) to enhance retrieval effectiveness and ef\ufb01ciency .", "entities": [[6, 8, "MethodName", "knowledge distillation"]]}, {"text": "Most related to our study is Hofst\u00e4tter et al .", "entities": []}, {"text": "( 2020 ) , who demonstrate that KD using a cross - encoder teacher significantly improves the effectiveness of bi - encoders for dense retrieval .", "entities": []}, {"text": "Similarly , Barkan et al .", "entities": []}, {"text": "( 2020 ) investigate the effectiveness of distilling a trained cross - encoder into a bi - encoder for sentence similarity tasks .", "entities": []}, {"text": "Gao et al .", "entities": []}, {"text": "( 2020a ) explore KD combinations of different objectives such as language modeling and ranking .", "entities": []}, {"text": "However , the above papers use computationally expensive cross - encoder teacher models ; thus , combining them for KD with more advanced negative sampling techniques can be impractical .", "entities": []}, {"text": "In light of existing work on hard negative mining and knowledge distillation , we propose to improve the effectiveness of single - vector bi - encoders with a more ef\ufb01cient KD approach : in - batch KD using a bi - encoder teacher .", "entities": [[10, 12, "MethodName", "knowledge distillation"]]}, {"text": "The advantage of our design is that , during distillation , it enables the ef\ufb01cient exploitation of all possible query \u2013 passage pairs within a minibatch , which we call tight coupling ( illustrated in Figure 1 ) .", "entities": []}, {"text": "This is a key difference between our KD approach and previous methods for dense retrieval , where only the scores of given query \u2013 passage triplets ( not all combinations ) are Bi - EncoderCross - Encoderd\u2212q0q0d+q0q0q1q2q1d\u2212q1d+q1q2d\u2212q2d+q2q0q1q2d+q0d+q1d+q2d\u2212q0d\u2212q1d\u2212q2Batch triplets Bi - EncoderTarget : Pairwise KDTarget : In - batch KDTeacherStudentEmbeddingsq0q1q2d\u2212q0d\u2212q1d\u2212q2d+q0d+q1d+q2d\u2212q2d+q2d\u2212q1d+q1d\u2212q0d+q0d\u2212q2d+q2d\u2212q1d+q1d\u2212q0d+q0d\u2212q2d+q2d\u2212q1d+q1d\u2212q0d+q0q0q1q2d\u2212q0d\u2212q1d\u2212q2d+q0d+q1d+q2TeacherFigure 1 : Illustration of the differences between pairwise knowledge distillation and our proposed in - batch knowledge distillation .", "entities": [[57, 59, "MethodName", "knowledge distillation"], [65, 67, "MethodName", "knowledge distillation"]]}, {"text": "computed due to the computational costs of crossencoders ( Hofst\u00e4tter et al . , 2020 ; Gao et al . , 2020a ; Barkan et al . , 2020 ) .", "entities": []}, {"text": "The contribution of this work is a simple technique for ef\ufb01ciently adding in - batch negative samples during knowledge distillation when training a single - vector bi - encoder .", "entities": [[18, 20, "MethodName", "knowledge distillation"]]}, {"text": "For the remainder of this paper , we refer to this technique as \u201c in - batch KD \u201d for convenience .", "entities": []}, {"text": "We empirically show that our model , even trained with BM25 negatives , can be more effective than cross - encoder teachers .", "entities": []}, {"text": "With hard negatives , our method approaches the state of the art in dense retrieval .", "entities": []}, {"text": "Our in - batch KD technique is able to incorporate hard negatives in a computationally ef\ufb01cient manner , without requiring large amounts of GPU memory for large batch sizes or expensive periodic index refreshes .", "entities": []}, {"text": "2 Background We focus on improving the training ef\ufb01ciency and retrieval effectiveness of dense retrieval and begin by formalizing it as a dense representation learning problem .", "entities": [[23, 25, "TaskName", "representation learning"]]}, {"text": "To be more speci\ufb01c , we propose to use knowledge distillation to enrich training signals and stabilize the representation learning procedure of bi - encoder models in the context of the well - known Noise - Contrastive Estimation ( NCE ) framework .", "entities": [[9, 11, "MethodName", "knowledge distillation"], [18, 20, "TaskName", "representation learning"]]}, {"text": "2.1 Dense Retrieval with Bi - encoders", "entities": []}, {"text": "The bi - encoder design has been widely adopted for dense retrieval ( Lee et al . , 2019 ; Chang et al . , 2020 ;", "entities": []}, {"text": "Guu et al . , 2020 ; Karpukhin et al . , 2020 ; Luan et al . , 2021 ; Qu et al . , 2020 ; Xiong et al . , 2021 ) , where queries and passages are encoded in a low - dimensional space .", "entities": []}, {"text": "It aims to learn lowdimensional representations that pull queries and relevant passages together and push queries and non - relevant passages apart .", "entities": []}, {"text": "Following the work of Mnih and Kavukcuoglu", "entities": []}, {"text": "165(2013 ) , we formulate a common objective for dense representation learning for passage retrieval .", "entities": [[10, 12, "TaskName", "representation learning"], [13, 15, "TaskName", "passage retrieval"]]}, {"text": "Given a queryqand a parameterized scoring function \u001e \u0012that computes the relevance between a query and a candidate passage p , we de\ufb01ne a probability distribution over documents in a corpus Dwith respect to relevance , as follows : Pq \u0012(p;D )", "entities": []}, {"text": "= exp ( \u001e \u0012(q;p))P p02Dexp ( \u001e \u0012(q;p0 ) )", "entities": []}, {"text": "= exp(hq\u0001hp)P p02Dexp(hq\u0001hp0 ) ; ( 1 ) where hq(hp)2Rddenotes the query ( passage ) representation produced by the bi - encoder .", "entities": []}, {"text": "A typical bi - encoder uses a simple scoring function for \u001e \u0012 , for example , the inner product of two vectors , as shown above .", "entities": []}, {"text": "The main challenge of evaluating and computing gradients of Eq .", "entities": []}, {"text": "( 1 ) is the prohibitively expensive computation cost given the number of passages in the corpusD , typically millions ( or even more ) .", "entities": []}, {"text": "This is already setting aside the cost of using pretrained transformers such as BERT as the encoder to compute hqandhp .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "Thus , previous work approximates Eq .", "entities": []}, {"text": "( 1 ) by NCE , which samples p2D+from training data andp02D0 = fD+[D\u0000g , whereD\u0000is from a noisy distribution such as candidates retrieved by BM25 ( Nogueira and Cho , 2019 ) , \ufb01ltered by \ufb01netuned transformers ( Qu et al . , 2020 ) , or retrieved by an asynchronously updated bi - encoder model itself ( Xiong et al . , 2021 ) .", "entities": []}, {"text": "Another simple yet effective approach is in - batch negative sampling , as used by Karpukhin et", "entities": []}, {"text": "al . ( 2020 ) , which takes pandp0of other queries within a minibatch as negative examples in NCE .", "entities": []}, {"text": "2.2 Knowledge Distillation Other than designing sophisticated sampling methods forp0 , training bi - encoder models using knowledge distillation ( KD ) with effective teacher models is another promising approach ( Hofst\u00e4tter et al . , 2020 ) .", "entities": [[1, 3, "MethodName", "Knowledge Distillation"], [17, 19, "MethodName", "knowledge distillation"]]}, {"text": "In this case , we aim to make the bi - encoder model mimic the teacher model \u2019s probability distribution as follows : Pq \u0012;student(p;D0 )", "entities": []}, {"text": "= exp(hq\u0001hp)P p02D0exp(hq\u0001hp0 ) \u0019exp ( \u001e ^\u0012(q;p)= \u001c ) P p02D0exp ( \u001e ^\u0012(q;p0)= \u001c )", "entities": []}, {"text": "= Pq ^\u0012;teacher(p;D0 ) ; ( 2)where \u001e ^\u0012denotes the relevance score estimated by a pretrained model parameterized by ^\u0012and \u001c , the temperature hyperparameter used in the KD framework .", "entities": []}, {"text": "To improve retrieval effectiveness , one can leverage pre - computed scores from pretrained models such as cross - encoders , e.g. , BERT , bi - encoders , e.g. , ColBERT , or ensembled scores from multiple models \u001e ^\u0012=P j \u001e ^\u0012;j .", "entities": [[23, 24, "MethodName", "BERT"]]}, {"text": "3", "entities": []}, {"text": "Our Approach 3.1 In - batch Knowledge Distillation Using KD in Eq .", "entities": [[6, 8, "MethodName", "Knowledge Distillation"]]}, {"text": "( 2 ) provides soft labels for biencoder training , and can be integrated with the previously mentioned NCE framework .", "entities": []}, {"text": "In this work , we propose to enhance teacher \u2013 student interactions by adding in - batch negatives to our knowledge distillation .", "entities": [[20, 22, "MethodName", "knowledge distillation"]]}, {"text": "Speci\ufb01cally , we estimate \u001e \u0012on in - batch examples from a minibatch Bguided by an auxiliary teacher model \u001e ^\u0012through the minimization of Kullback \u2013 Leibler ( KL ) divergence of the two distributions : arg min \u0012X q2QBX p2D0", "entities": []}, {"text": "BL \u001e \u0012 ; \u001e ^\u0012 ; ( 3 ) whereL \u001e \u0012 ; \u001e ^\u0012is :", "entities": []}, {"text": "Pq ^\u0012;teacher(p;D0 B ) logPq ^\u0012;teacher(p;D0 B )", "entities": []}, {"text": "Pq", "entities": []}, {"text": "\u0012;student(p;D0 B):(4 )", "entities": []}, {"text": "Note that here we consider all pairwise relationship between queries and passages within a minibatch that contains a query set QBand a passage setD0 B. 3.2 Teacher Model Choice Across - encoder has been shown to be an effective teacher ( Hofst\u00e4tter et al . , 2020 ; Gao et al . , 2020a ) since it allows rich interactions between the intermediate transformer representations of a query q and a passage p.", "entities": []}, {"text": "For example , a \u201c vanilla \u201d crossencoder design using BERT can be denoted as : \u001e ^\u0012;Cat , Wf(hq\bp ) ; ( 5 ) where the ranking score is \ufb01rst computed by the hidden representation of the concatenation q\bp from BERT ( along with the standard special tokens ) and then mapped to a scalar by a pooling operation fand a mapping matrix W.", "entities": [[10, 11, "MethodName", "BERT"], [41, 42, "MethodName", "BERT"]]}, {"text": "Although effective , due to BERT \u2019s quadratic complexity with respect to input sequence length , this design makes exhaustive combinations between a query and possible candidates impractical ,", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "166since this requires evaluating cross - encoders jBj2 times to compute Eq .", "entities": []}, {"text": "( 3 ) using Eq .", "entities": []}, {"text": "( 5 ) .", "entities": []}, {"text": "Thus , an alternative is to conduct pairwise KD by computing the KL divergence of only two probabilities of a positive pair ( q;p)and a negative pair ( q;p0)for each queryq .", "entities": []}, {"text": "However , this might not yield a good approximation of Eq .", "entities": []}, {"text": "( 2 ) .", "entities": []}, {"text": "Abi - encoder can also be leveraged as a teacher model , which has the advantage that it is more feasible to perform exhaustive comparisons between queries and passages since they are passed through the encoder independently .", "entities": []}, {"text": "Among biencoder designs , ColBERT is a representative model that uses late interactions of multiple vectors(fh1 q;:::;hi qg;fh1 p;:::;hj pg)to improve the robustness of dense retrieval , as compared to inner products of pairs of single vectors ( hq;hp ) .", "entities": []}, {"text": "Speci\ufb01cally , Khattab and Zaharia ( 2020 ) propose the following \ufb01ne - grained scoring function : \u001e ^\u0012;MaxSim , X i2jhqjmax j2jhpjhi q\u0001hj p ; ( 6 ) whereiandjare the indices of token representations of a query qand a passage pof ColBERT ( Khattab and Zaharia , 2020 ) .", "entities": []}, {"text": "The contribution of our work is in - batch knowledge distillation with a tightly - coupled teacher .", "entities": [[9, 11, "MethodName", "knowledge distillation"]]}, {"text": "The computation of \u001e ^\u0012;MaxSimenables exhaustive inference over all query \u2013 passage combinations in the minibatchBwith only 2\u0001jBjcomputation cost , enabling enriched interactions between teacher and student .", "entities": []}, {"text": "We call this design Tightly-", "entities": []}, {"text": "Coupled Teacher ColBERT ( TCT - ColBERT ) .", "entities": []}, {"text": "Table 1 provides a training cost comparison between different teachers .", "entities": []}, {"text": "When training with pairwise KD , crossencoders exhibit the highest training cost .", "entities": []}, {"text": "On the other hand , ColBERT enables in - batch KD at a modest training cost compared to pairwise KD .", "entities": []}, {"text": "TCT - ColBERT provides a \ufb02exible design for biencoders , as long as the encoders produce query and passage representations independently .", "entities": []}, {"text": "For simplicity , our student model adopts shared encoder weights for both the query and the passage , just like the teacher model ColBERT .", "entities": []}, {"text": "Following Khattab and Zaharia ( 2020 ) , for each query ( passage ) , we prepend the [ CLS ] token and another special [ Q ] ( [ D ] ) token in the input sequence for both our teacher and student models .", "entities": []}, {"text": "The student encoder outputs single - vector dense representations ( hq;hp)by performing average pooling over the token embeddings from the \ufb01nal layer .", "entities": [[12, 14, "MethodName", "average pooling"]]}, {"text": "Table 1 : Training cost comparison .", "entities": []}, {"text": "We report the training time per batch against the baseline ( without a teacher model ) on a single TPU - v2 .", "entities": []}, {"text": "Our backbone model is BERT - base , with batch size 96 .", "entities": [[4, 5, "MethodName", "BERT"], [9, 11, "HyperparameterName", "batch size"]]}, {"text": "The in - batch cross - encoder training time is not available because it exceeds the memory limit .", "entities": []}, {"text": "Teacher / KD strategy Pairwise In - batch Cross - encoder ( \u001e ^\u0012;Cat ) +48.1 %", "entities": []}, {"text": "OOM ColBERT ( \u001e ^\u0012;MaxSim ) +32.7 % +33.5 % 3.3 Hard Negative Sampling Given that in - batch negative sampling is an ef\ufb01cient way to add more information into knowledge distillation , we wonder whether our tightly - coupled teacher design works well when applied to more sophisticated sampling methods .", "entities": [[30, 32, "MethodName", "knowledge distillation"]]}, {"text": "Following the work of Xiong et al .", "entities": []}, {"text": "( 2021 ) , we use our pretrained bi - encoder model , namely TCT - ColBERT , to encode the corpus and sample \u201c hard \u201d negatives for each query to create new training triplets by using the negativesD\u0000of the bi - encoder instead of BM25 .", "entities": []}, {"text": "Speci\ufb01cally , we explore three different training strategies : 1.HN : we train the bi - encoder using in - batch hard negatives without the guide of ColBERT .", "entities": []}, {"text": "2.TCT", "entities": []}, {"text": "HN : we train the bi - encoder with TCTColBERT ; 3.TCT HN+ : we \ufb01rst \ufb01ne - tune our ColBERT teacher with augmented training data containing hard negatives and then distill its knowledge into the bi - encoder student through TCT - ColBERT .", "entities": []}, {"text": "We empirically explore the effectiveness of these strategies for both passage and document retrieval .", "entities": []}, {"text": "4 Experiments In this section , we conduct experiments on the MS MARCO passage and document corpora .", "entities": [[11, 13, "DatasetName", "MS MARCO"]]}, {"text": "For passage ranking , we \ufb01rst train models on BM25 negatives as warm - up and compare different KD methods .", "entities": [[1, 3, "TaskName", "passage ranking"]]}, {"text": "We then further train models on the hard negatives retrieved by the BM25 warmed - up checkpoint .", "entities": []}, {"text": "For document ranking , following previous work ( Xiong et al . , 2021 ; Zhan et al . , 2020 ; Lu et", "entities": [[1, 3, "TaskName", "document ranking"]]}, {"text": "al . , 2021 ) , we start with our BM25 warmed - up checkpoint for passage ranking and conduct additional hard negative training .", "entities": [[16, 18, "TaskName", "passage ranking"]]}, {"text": "167Table 2 : Passage retrieval results with BM25 negative training .", "entities": [[3, 5, "TaskName", "Passage retrieval"]]}, {"text": "For knowledge distillation ( KD ) methods , the effectiveness of teacher ( T ) models is also reported .", "entities": [[1, 3, "MethodName", "knowledge distillation"]]}, {"text": "All our implemented models are labeled with a number and superscripts represent signi\ufb01cant improvements over the labeled model ( paired t - test , p<0:05 ) .", "entities": []}, {"text": "Strategy Model # params of TeacherMARCO", "entities": [[3, 4, "MetricName", "params"]]}, {"text": "Dev TREC - DL \u2019 19 MRR@10 ( T / S ) R@1", "entities": [[1, 2, "DatasetName", "TREC"], [12, 13, "MetricName", "R@1"]]}, {"text": "K NDCG@10 ( T / S ) R@1 K - ( 1 ) Baseline - - / .310 .945 - / .626 .658", "entities": [[1, 2, "MetricName", "NDCG@10"], [7, 8, "MetricName", "R@1"]]}, {"text": "Pairwise KDKD - T1 ( Hofst\u00e4tter et al . , 2020 ) 110 M .376 / .304 .931 .730 / .631 .702 KD - T2 ( Hofst\u00e4tter et al . , 2020 )", "entities": []}, {"text": "467 M .399 / .315 .947 .743 / .668 .737", "entities": []}, {"text": "( 2 ) KD - T2 ( Ours ) 467 M .399 / .3411.9641.743 / .6591.7081", "entities": []}, {"text": "( 3 ) KD - ColBERT 110 M .350 / .3391.9621.730 / .6701.7101", "entities": []}, {"text": "In - batch KD ( 4 ) TCT - ColBERT 110 M .350 / .3441;3.9671;3.730 /", "entities": []}, {"text": ".6851.7451;2;3 4.1 Passage Retrieval We perform ad hoc passage retrieval on the MS MARCO passage ranking dataset ( Bajaj et al . , 2016 ) , which consists of a collection of 8.8 M passages from web pages and a set of \u00180.5 M relevant ( query , passage ) pairs as training data .", "entities": [[2, 4, "TaskName", "Passage Retrieval"], [8, 10, "TaskName", "passage retrieval"], [12, 14, "DatasetName", "MS MARCO"], [14, 16, "TaskName", "passage ranking"]]}, {"text": "We evaluate model effectiveness on two test sets of queries : 1.MARCO Dev : the development set of MS MARCO comprises 6980 queries , with an average of one relevant passage per query .", "entities": [[18, 20, "DatasetName", "MS MARCO"]]}, {"text": "2.TREC - DL \u2019 19 ( Craswell et al . , 2019 ): the organizers of the Deep Learning Track at the 2019 Text REtrieval Conference ( TREC ) released 43 queries with multi - graded ( 0\u20133 ) relevance labels on 9 K ( query , passage ) pairs .", "entities": [[27, 28, "DatasetName", "TREC"]]}, {"text": "To evaluate output quality , we report MRR@10 ( NDCG@10 ) for MARCO Dev ( TREC - DL \u2019 19 ) and Recall@1 K , denoted as R@1K. To compare with current state - of - the - art models , we evaluate our design , TCT - ColBERT , under two approaches for negative sampling : ( 1 ) BM25 and ( 2 ) hard negatives retrieved by the bi - encoder itself .", "entities": [[9, 10, "MetricName", "NDCG@10"], [15, 16, "DatasetName", "TREC"], [22, 23, "MetricName", "Recall@1"]]}, {"text": "4.1.1 Training with BM25 Negatives In this setting , models are trained using the of\ufb01cial public data triples.train.small , where negative samples are produced by BM25 .", "entities": []}, {"text": "We compare different bi - encoder models using BERT - base as the backbone , which uses single 768 - dim vectors to represent each query and passage : 1.Baseline : a single - vector bi - encoder trained with in - batch negatives , as discussed in Section 2.1 , which is similar to Karpukhin et", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "al . ( 2020 ) but with a smaller batch size .", "entities": [[9, 11, "HyperparameterName", "batch size"]]}, {"text": "2.Pairwise KD : the approach of Hofst\u00e4tter et al .", "entities": []}, {"text": "( 2020 ) , who improve ranking effectiveness using cross - encoders with pairwise KD.We also compare against two models , KD - T1 and KD - T2 , which use BERT - base bi - encoders as student models .", "entities": [[31, 32, "MethodName", "BERT"]]}, {"text": "In the former , the student is distilled from a BERT - base cross - encoder , while the latter is distilled from ensembled cross - encoders comprising BERT - base , BERT - large , and ALBERTlarge .", "entities": [[10, 11, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"], [32, 33, "MethodName", "BERT"]]}, {"text": "These \ufb01gures reported in Table 2 are copied from Hofst\u00e4tter", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "For a fair comparison with our models based on KL - divergence KD , we also implement our KD - T2 using the precomputed pairwise softmax probabilities provided by Hofst\u00e4tter et", "entities": [[25, 26, "MethodName", "softmax"]]}, {"text": "al .", "entities": []}, {"text": "( 2020 ) ( who use MSE margin loss for KD ) .", "entities": [[6, 7, "MetricName", "MSE"], [8, 9, "MetricName", "loss"]]}, {"text": "In addition , we adopt pairwise softmax probabilities from \ufb01ne - tuned ColBERT to train KDColBERT for comparison .", "entities": [[6, 7, "MethodName", "softmax"]]}, {"text": "All our models are \ufb01ne - tuned with batch size 96 and learning rate 7\u000210\u00006for 500 K steps on a single TPU - V2 .", "entities": [[8, 10, "HyperparameterName", "batch size"], [12, 14, "HyperparameterName", "learning rate"]]}, {"text": "For TCT - ColBERT , there are two steps in our training procedure : ( 1 ) \ufb01netune \u001e ^\u0012;MaxSimas our teacher model , ( 2 ) freeze \u001e ^\u0012;MaxSimand distill knowledge into our student model \u001e \u0012.", "entities": []}, {"text": "We keep all the hyperparameter settings the same but adjust temperature \u001c = 0:25for KD at the second step .", "entities": []}, {"text": "For all our models , including the baseline , we initialize the student model using the \ufb01ne - tuned weights of the teacher model in the \ufb01rst step .", "entities": []}, {"text": "We limit the input tokens to 32 ( 150 ) for queries ( passages ) .", "entities": []}, {"text": "To evaluate effectiveness , we encode all passages in the corpus and conduct brute force search over the vector representations .", "entities": []}, {"text": "Our main results , including paired t - test for signi\ufb01cance testing , are shown in Table 2 .", "entities": []}, {"text": "In addition to the effectiveness of the student models , we also show the effectiveness of the teacher models for the KD methods.1", "entities": []}, {"text": "First , we see that pairwise KD methods show signi\ufb01cant improvements over the baseline , indicat1We report our trained ColBERT \u2019s accuracy by reranking the top-1000 candidates provided of\ufb01cially .", "entities": [[21, 22, "MetricName", "accuracy"]]}, {"text": "168 4 5 6 7 8 9 Index Size ( 106)0.3350.3400.3450.3500.3550.360MRR@10 KD - T2 KD - ColBERT TCT - ColBERT(a )", "entities": []}, {"text": "MARCO Dev 0 1 2 3 4 5 6 7 8 9 Index Size ( 106)0.6500.6700.6900.7100.7300.750NDCG@10 KD - T2 KD - ColBERT TCT - ColBERT ( b ) TREC - DL \u2019 19 Figure 2 : Passage retrieval effectiveness on a synthetic corpus comprising relevant passages and BM25 results as additional \u201c distractors \u201d randomly sampled from the corpus are added .", "entities": [[2, 3, "DatasetName", "0"], [28, 29, "DatasetName", "TREC"], [36, 38, "TaskName", "Passage retrieval"]]}, {"text": "ing that information from BM25 negatives can not be fully exploited without teacher models .", "entities": []}, {"text": "Second , although KD - T2 improves the bi - encoder \u2019s effectiveness over KD - T1 , it is not consistently better than KD - ColBERT in terms of students \u2019 effectiveness .", "entities": []}, {"text": "We suspect that they have comparable capabilities to discriminate most paired passages ( BM25 negative vs. positive samples ) , i.e. , ColBERT is good enough to guide bi - encoder student models to discriminate them .", "entities": []}, {"text": "On the other hand , our TCT - ColBERT model , which uses only one teacher model and adds only 33 % more training time over the baseline , yields the best effectiveness , demonstrating the advantages of our proposed inbatch KD \u2014 exhaustive exploitation of all query \u2013 document combinations in a minibatch .", "entities": []}, {"text": "To understand why TCT - ColBERT yields better results , we study the models \u2019 retrieval effectiveness against carefully selected distractors .", "entities": []}, {"text": "We start with a small synthetic corpus composed of the relevant passages and the top-1000", "entities": []}, {"text": "BM25 candidates of the 6980 ( 43 ) queries from MARCO Dev ( TREC - DL \u2019 19 ) .", "entities": [[13, 14, "DatasetName", "TREC"]]}, {"text": "To increase the corpus size , we gradually add passages uniformly sampled from the corpus without replacement .", "entities": []}, {"text": "From Figure 2 , we see that the three KD models exhibit nearly the same effectiveness when the corpus only contains BM25 candidates .", "entities": []}, {"text": "This shows that the bi - encoders learn to discriminate relevant passages from the BM25 negative samples well .", "entities": []}, {"text": "However , as the index size increases , TCT - ColBERT demonstrates better ranking effectiveness than the other pairwise KD methods , indicating that the learned representations are more robust .", "entities": []}, {"text": "We attribute this robustness against \u201c distractors \u201d to the enriched information from inbatch KD , where we are able to exploit all in - batch query \u2013 document combinations.4.1.2 Training with Hard Negatives In this subsection , we evaluate TCT - ColBERT when training with hard negatives ( HNs ) .", "entities": []}, {"text": "We compare our model to four competitive approaches : 1.ANCE ( Xiong et al . , 2021 ) is the most representative work , which proposes asynchronous index refreshes to mine hard negatives .", "entities": []}, {"text": "The model is trained for 600 K steps with index refreshes every 10 K steps .", "entities": []}, {"text": "ANCE uses RoBERTa - base as its backbone .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}, {"text": "2.LTRe ( Zhan et al . , 2020 ) further improves from an ANCE checkpoint by adding more training steps with the same hard negative mining approach ; thus , the computation cost of index refreshes from ANCE can not be neglected .", "entities": []}, {"text": "LTRe also use RoBERTa - base as its backbone .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}, {"text": "3.SEED - Encoder ( Lu et al . , 2021 ) leverages a pretraining strategy to enhance the capability of the bi - encoder , which is further \ufb01ne - tuned with HNs using asynchronous index refreshes .", "entities": []}, {"text": "4.RocketQA ( Qu et al . , 2020 ) trains a bi - encoder model using hard negatives denoised by a crossencoder , ERNIE-2.0 - Large ( Sun et al . , 2019 ) .", "entities": []}, {"text": "It further demonstrates that training bi - encoders with many in - batch negatives ( batch size up to 4096 ) signi\ufb01cantly improves ranking effectiveness ; however , this approach is computationally expensive ( the authors report using 8 \u0002V100 GPUs for training ) .", "entities": [[15, 17, "HyperparameterName", "batch size"]]}, {"text": "To the best of our knowledge , RocketQA represents the state of the art in single - vector bi - encoders for dense retrieval .", "entities": []}, {"text": "For a more fair comparison , we also report the ranking effectiveness of their model trained with a smaller batch size of 128 .", "entities": [[19, 21, "HyperparameterName", "batch size"]]}, {"text": "For all the approaches above , we directly copy the reported effectiveness from the original papers .", "entities": []}, {"text": "169Table 3 : Passage retrieval results with hard negative training .", "entities": [[3, 5, "TaskName", "Passage retrieval"]]}, {"text": "All our implemented models are labeled with a number and superscripts represent signi\ufb01cant improvements over the labeled model ( paired t - test , p<0:05 ) .", "entities": []}, {"text": "Model # Index RefreshBatch SizeMARCO Dev TREC - DL \u2019 19 MRR@10 R@1", "entities": [[6, 7, "DatasetName", "TREC"], [12, 13, "MetricName", "R@1"]]}, {"text": "K NDCG@10 R@1 K ANCE ( Xiong et al . , 2021 ) 60 32 .330 .959 .648 LTRe ( Zhan et", "entities": [[1, 2, "MetricName", "NDCG@10"], [2, 3, "MetricName", "R@1"]]}, {"text": "al . , 2020 ) 60 32 .341 .962 .675 SEED - Encoder ( Lu et", "entities": [[10, 11, "DatasetName", "SEED"]]}, {"text": "al . , 2021 ) \u001510 ( est . )", "entities": []}, {"text": "- .339 .961 - RocketQA ( Qu et al . , 2020 ) 1 128 .310 - - RocketQA ( Qu et al . , 2020 ) 1 4096 .364 - - ( 1 ) TCT - ColBERT 0 96 .344 .967 .685 .745 ( 2 ) w/", "entities": [[38, 39, "DatasetName", "0"]]}, {"text": "HN 1 96 .237 .929 .543 .674 ( 3 ) w/ TCT", "entities": []}, {"text": "HN 1 96 .3541;2.9711;2.7052.7651;2 ( 4 ) w/ TCT HN+ 1 96 .3591;2.9701.7191;2.7601", "entities": []}, {"text": "For our TCT - ColBERT model , following the settings of the above approaches , we \ufb01rst use our TCT - ColBERT model trained on BM25 negatives as a warm - up starting point and index all 8.8 M MARCO passages .", "entities": []}, {"text": "Using the warmed - up index , we retrieve top-200 passages for each training query and randomly sample ( with replacement ) hard negatives from the 200 candidates to form our training data .", "entities": []}, {"text": "Note that due to resource limitations we do not conduct experiments with asynchronous index refreshes since multiple V100 GPUs are required for such a model training scheme.2In this experiment , all the hyperparameter settings are the same as the ones in the BM25 negative training , except for training steps , which is set to 100 K for both student and teacher training .", "entities": []}, {"text": "Table 3 reports the results of our experiments with hard negative training .", "entities": []}, {"text": "First , we observe that our TCT - ColBERT model trained with BM25 negatives marginally outperforms the other models trained with HNs , except for RocketQA .", "entities": []}, {"text": "Comparing the different training strategies discussed in Section 3.3 ( second main block of the table ) , we see that the ranking effectiveness of TCT - ColBERT ( HN ) degrades when training on hard negatives without the guide of a teacher .", "entities": []}, {"text": "This is consistent with the \ufb01ndings of Qu et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) that hard negatives contain noisy information ( i.e. , some hard negatives may actually be relevant ) .", "entities": []}, {"text": "Also , Xiong et al .", "entities": []}, {"text": "( 2021 ) show that training bi - encoders with hard negatives can be unstable : hard negatives bene\ufb01t ranking effectiveness only under certain hyperparameter settings .", "entities": []}, {"text": "In contrast , hard negative training using ColBERT \u2019s in - batch KD further boosts ranking effectiveness , especially when our teacher ( ColBERT ) 2Re - encoding the entire corpus takes \u001810 hours on one GPU.is trained with the same hard negative samples beforehand .", "entities": []}, {"text": "It is also worth noting that our TCTColBERT ( w/ TCT HN+ ) with batch size 96 yields competitive ranking effectiveness compared to RocketQA ( the current state of the art ) , which uses batch size 4096 .", "entities": [[14, 16, "HyperparameterName", "batch size"], [35, 37, "HyperparameterName", "batch size"]]}, {"text": "These results demonstrate the advantages of our TCT design : our approach effectively exploits hard negatives in a computationally ef\ufb01cient manner ( i.e. , without the need for large batch sizes or periodic index refreshes ) .", "entities": []}, {"text": "4.2 Document Retrieval To validate the effectiveness and generality of our training strategy , we conduct further experiments on document retrieval using the MS MARCO document ranking dataset .", "entities": [[23, 25, "DatasetName", "MS MARCO"], [25, 27, "TaskName", "document ranking"]]}, {"text": "This dataset contains 3.2 M web pages gathered from passages in the MS MARCO passage ranking dataset .", "entities": [[12, 14, "DatasetName", "MS MARCO"], [14, 16, "TaskName", "passage ranking"]]}, {"text": "Similar to the passage condition , we evaluate model effectiveness on two test sets of queries : 1.MARCO Dev : the development set contains 5193 queries , each with exactly one relevant document .", "entities": []}, {"text": "2.TREC - DL \u2019 19 : graded relevance judgments are available from the TREC", "entities": [[13, 14, "DatasetName", "TREC"]]}, {"text": "2019", "entities": []}, {"text": "Deep Learning Track ,", "entities": []}, {"text": "but on only 43 queries .", "entities": []}, {"text": "Per of\ufb01cial guidelines , we report different metrics for the two query sets : MRR@100 for MARCO Dev and NDCG@10 for TREC - DL \u2019 19 .", "entities": [[19, 20, "MetricName", "NDCG@10"], [21, 22, "DatasetName", "TREC"]]}, {"text": "Following the FirstP setting for document retrieval described in Xiong et al .", "entities": []}, {"text": "( 2021 ) , we feed the \ufb01rst 512 tokens of each document for encoding , and start with the warmed - up checkpoint for our encoder \u2019s parameters trained for passage retrieval ( using BM25 negatives , as described in Section 4.1.1 ) .", "entities": [[31, 33, "TaskName", "passage retrieval"]]}, {"text": "The settings for \ufb01ne - tuning our warmed - up encoder", "entities": []}, {"text": "170Table 4 : Document retrieval results using the FirstP approach .", "entities": []}, {"text": "All our implemented models are labeled with a number and superscripts represent signi\ufb01cant improvements over the labeled model ( paired t - test , p<0:05 ) .", "entities": []}, {"text": "ModelMARCO Dev TREC - DL \u2019 19 MRR@100 NDCG@10 ANCE ( Xiong et al . , 2021 ) .368 .614 LTRe ( Zhan et", "entities": [[2, 3, "DatasetName", "TREC"], [8, 9, "MetricName", "NDCG@10"]]}, {"text": "al . , 2020 ) - .634 SEED - Encoder ( Lu et al . , 2021 ) .394 ( 1 ) TCT - ColBERT .339 .573 ( 2 ) w/ TCT HN+ .3921.613 ( 3 ) w/ 2\u0002TCT HN+ .4181;2.6501;2", "entities": [[7, 8, "DatasetName", "SEED"]]}, {"text": "( e.g. , learning rate , training steps , top-200 negative sampling ) are the same as passage retrieval except for batch size , which is set to 64 .", "entities": [[3, 5, "HyperparameterName", "learning rate"], [17, 19, "TaskName", "passage retrieval"], [21, 23, "HyperparameterName", "batch size"]]}, {"text": "Ranking effectiveness is reported in Table 4 .", "entities": []}, {"text": "First , we observe that TCT - ColBERT ( our warmedup checkpoint ) performs far worse than other approaches to document retrieval using the FirstP method .", "entities": []}, {"text": "This may be due to the fact that FirstP document retrieval is very different from passage retrieval , making zero - shot transfer ineffective .", "entities": [[15, 17, "TaskName", "passage retrieval"]]}, {"text": "After applying HN training on both teacher and student models ( condition 2 ) , the ranking effectiveness increases signi\ufb01cantly .", "entities": []}, {"text": "In addition , we \ufb01nd that another iteration of training with an index refresh ( condition 3 ) further improves ranking effectiveness .", "entities": []}, {"text": "To sum up , in the document ranking task , TCT - ColBERT yields competitive effectiveness with a one - time index refresh and outperforms other computationally expensive methods with one additional index refresh .", "entities": [[6, 8, "TaskName", "document ranking"]]}, {"text": "4.3 Dense \u2013 Sparse Hybrids In our \ufb01nal set of experiments , we show that dense retrieval with single - vector representations can be integrated with results from sparse retrieval to further increase effectiveness .", "entities": []}, {"text": "We illustrate the endto - end tradeoffs in terms of quality , time , and space of different dense \u2013 sparse hybrid combinations on the passage retrieval tasks .", "entities": [[25, 27, "TaskName", "passage retrieval"]]}, {"text": "Many papers ( Luan et al . , 2021 ; Gao et al . , 2020b ; Ma et al . , 2021 ; Lin et al . , 2021 ) have demonstrated that sparse retrieval can complement dense retrieval via a simple linear combination of their scores .", "entities": []}, {"text": "In our implementation , for each queryq , we use sparse and dense techniques to retrieve the top-1000 passages , DspandDds , with their relevance scores , \u001e sp(q;p2Dsp)and \u001e ds(q;p2Dds ) , respectively .", "entities": []}, {"text": "Then , we compute the \ufb01nal relevance score for each retrieved passage \u001e ( q;p ) , wherep2Dsp[Dds , as follows : 8 > > > < > > > : \u000b \u0001 \u001e sp(q;p ) + min p2Dds \u001e ds(q;p);ifp = 2Dds \u000b \u0001min p2Dsp \u001e sp(q;p ) + \u001e ds(q;p);ifp = 2Dsp \u000b \u0001 \u001e sp(q;p ) + \u001e ds(q;p ) ; otherwise .", "entities": []}, {"text": "This technique is an approximation of a linear combination of sparse and dense retrieval scores .", "entities": []}, {"text": "Speci\ufb01cally , if p = 2Dsp(orDds ) , we instead use the minimum score of \u001e sp(q;p2Dsp),or \u001e ds(q;p2 Dds)as a substitute .", "entities": []}, {"text": "For the sparse and dense retrieval combinations , we tune the hyperparameter \u000b on 6000 randomly sampled queries from the MS MARCO training set .", "entities": [[20, 22, "DatasetName", "MS MARCO"]]}, {"text": "We conduct dense \u2013 sparse hybrid experiments with sparse retrieval ( BM25 ranking ) on the original passages ( denoted BM25 ) and on passages with docTTTTTquery document expansion ( Nogueira and Lin , 2019 ) ( denoted doc2queryT5 ) .", "entities": []}, {"text": "To characterize end - to - end effectiveness and ef\ufb01ciency , we perform sparse retrieval with the Pyserini toolkit ( Lin et al . , 2021 ) and dense retrieval with Faiss ( Johnson et al . , 2017 ) , but implement the score combination in separate custom code .", "entities": []}, {"text": "Table 5 shows passage retrieval results in terms of ranking effectiveness , query latency , and storage requirements ( i.e. , index size ) for each model and Table 6 reports the component latencies of our TCT - ColBERT dense \u2013 sparse hybrid.3The crossencoder reranker of Nogueira and Cho ( 2019 ) provides a point of reference for multi - stage reranking designs , which is effective but slow .", "entities": [[3, 5, "TaskName", "passage retrieval"]]}, {"text": "Generally , dense retrieval methods ( whether single - vector or multi - vector ) are more effective but slower than sparse retrieval methods , which rely on bag - of - words querying using inverted indexes .", "entities": []}, {"text": "Single - vector dense models also require more space than sparse retrieval methods .", "entities": []}, {"text": "Moving 3Here we assume running dense and sparse retrieval in parallel .", "entities": []}, {"text": "171Table 5 : End - to - end comparisons of output quality , query latency , and storage requirements for passage retrieval .", "entities": [[20, 22, "TaskName", "passage retrieval"]]}, {"text": "Ranking effectiveness Latency Storage MARCO Dev TREC - DL \u2019 19 ms / q GiB Sparse retrieval BM25 with Anserini ( Yang et al . , 2018 )", "entities": [[6, 7, "DatasetName", "TREC"]]}, {"text": ".184 .506 55 4 DeepCT ( Dai and Callan , 2020 ) .243 .551 55 4 doc2query - T5 ( Nogueira and Lin , 2019 ) .277 .551 64 14 Dense retrieval : single - vector TAS - B ( Hofst\u00e4tter et al . , 2021 ) .343 .722 64 13 RocketQA ( Qu et al . , 2020 ) .370 - 107b13a", "entities": [[18, 19, "MethodName", "T5"]]}, {"text": "TCT - ColBERT .344 .685 107 13 TCT - ColBERT ( w/ TCT HN+ ) .359 .719 107 13 Dense retrieval : multi - vector ME - BERT ( Luan et", "entities": [[27, 28, "MethodName", "BERT"]]}, {"text": "al . , 2021 ) .334 .687 - 96 ColBERT ( Khattab and Zaharia , 2020 ) .360 - 458 154 Hybrid dense + sparse CLEAR", "entities": [[25, 26, "DatasetName", "CLEAR"]]}, {"text": "( Gao et al . , 2020b ) .338 .699 - 17a ME - HYBRID - E ( Luan et al . , 2021 ) .343 .706 - 100 TAS - B + doc2query - T5 ( Hofst\u00e4tter et al . , 2021 ) .360 .753 67 27a TCT - ColBERT + BM25 .356 .720 110 17 TCT - ColBERT + doc2query - T5 .366 .734 110 27 TCT - ColBERT ( w/ TCT HN+ )", "entities": [[35, 36, "MethodName", "T5"], [63, 64, "MethodName", "T5"]]}, {"text": "+", "entities": []}, {"text": "BM25 .369 .730 110 17 TCT - ColBERT ( w/ TCT HN+ ) + doc2query - T5 .375 .741 110 27 Multi - stage reranking BM25 + BERT - large ( Nogueira and Cho , 2019 ) .365 .736 3500 4 TAS - B + doc2query - T5 + Mono - Duo - T5 ( Hofst\u00e4tter et al . , 2021 ) .421 .759 12800 27a RocketQA with reranking ( Qu et al . , 2020 ) .439 - - 13a aWe estimate dense index size using 16 - bit \ufb02oats ; for hybrid , we add the sizes of sparse and dense indexes .", "entities": [[16, 17, "MethodName", "T5"], [27, 28, "MethodName", "BERT"], [47, 48, "MethodName", "T5"], [53, 54, "MethodName", "T5"]]}, {"text": "bWe assume latency comparable to our settings .", "entities": []}, {"text": "Table 6 : Component latencies per query of our model .", "entities": []}, {"text": "Stage latency ( ms ) device BERT query encoder 7 GPU Dot product search 100 GPU Score combination 3 CPU from single - vector to multi - vector dense models , we see that ColBERT exhibits higher effectiveness but is slower and requires much more storage .", "entities": [[6, 7, "MethodName", "BERT"], [16, 17, "MetricName", "Score"]]}, {"text": "Finally , when integrated with sparse retrieval methods , TCT - ColBERT is able to beat a basic multi - stage reranking design ( BM25 + BERTlarge ) , but with much lower query latency , although at the cost of increased storage .", "entities": []}, {"text": "Hybrid TCT - ColBERT ( w/ TCT HN+ )", "entities": []}, {"text": "+ doc2query - T5 compares favorably with a recent advanced model , TAS - B + doc2query - T5 ( Hofst\u00e4tter et al . , 2021 ) , which introduces topic - aware sampling and dual teachers , incorporating part of our TCT - ColBERT work .", "entities": [[3, 4, "MethodName", "T5"], [18, 19, "MethodName", "T5"]]}, {"text": "Nevertheless , even the best hybrid variant of TCT - ColBERT alone , without further reranking , remains quite some distance from RocketQA , the current state of the art ( with reranking using cross - encoders ) .", "entities": []}, {"text": "This suggests that there remain relevance signals that require full attention interactions to exploit.5 Conclusions Improving the effectiveness of single - vector biencoders is an important research direction in dense retrieval because of lower latency and storage requirements compared to multi - vector approaches .", "entities": []}, {"text": "We propose a teacher \u2013 student knowledge distillation approach using tightly coupled bi - encoders that enables exhaustive use of query \u2013 passage combinations in each minibatch .", "entities": [[6, 8, "MethodName", "knowledge distillation"]]}, {"text": "More importantly , a bi - encoder teacher requires less computation than a cross - encoder teacher .", "entities": []}, {"text": "Finally , our approach leads to robust learned representations .", "entities": []}, {"text": "Overall , our hard negative sampling strategy leads to an effective andef\ufb01cient dense retrieval technique , which can be further combined with sparse retrieval techniques in dense \u2013 sparse hybrids .", "entities": []}, {"text": "Together , these designs provide a promising solution for end - to - end text retrieval that balances quality , query latency , and storage requirements .", "entities": []}, {"text": "Acknowledgements This research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council ( NSERC ) of Canada .", "entities": []}, {"text": "172References Payal Bajaj , Daniel Campos , Nick Craswell , Li Deng , Jianfeng Gao , Xiaodong Liu , Rangan Majumder , Andrew McNamara , Bhaskar Mitra , Tri Nguyen , et al . 2016 .", "entities": []}, {"text": "MS MARCO :", "entities": [[0, 2, "DatasetName", "MS MARCO"]]}, {"text": "A human generated machine reading comprehension dataset .", "entities": [[3, 6, "TaskName", "machine reading comprehension"]]}, {"text": "arXiv:1611.09268 .", "entities": []}, {"text": "Oren Barkan , Noam Razin , Itzik Malkiel , Ori Katz , Avi Caciularu , and Noam Koenigstein .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Scalable attentive sentence - pair modeling via distilled sentence embedding .", "entities": [[8, 10, "TaskName", "sentence embedding"]]}, {"text": "In Proc .", "entities": []}, {"text": "AAAI .", "entities": []}, {"text": "Wei - Cheng Chang , Felix X. Yu , Yin - Wen Chang , Yiming Yang , and Sanjiv Kumar .", "entities": [[19, 20, "DatasetName", "Kumar"]]}, {"text": "2020 .", "entities": []}, {"text": "Pre - training tasks for embedding - based large - scale retrieval .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "ICLR .", "entities": []}, {"text": "Nick Craswell , Bhaskar Mitra , and Daniel Campos .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Overview of the TREC 2019 deep learning track .", "entities": [[3, 4, "DatasetName", "TREC"]]}, {"text": "In Proc .", "entities": []}, {"text": "TREC .", "entities": [[0, 1, "DatasetName", "TREC"]]}, {"text": "Zhuyun Dai and Jamie Callan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Context - aware term weighting for \ufb01rst stage passage retrieval .", "entities": [[8, 10, "TaskName", "passage retrieval"]]}, {"text": "In Proc .", "entities": []}, {"text": "SIGIR , page 1533\u20131536 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proc .", "entities": []}, {"text": "NAACL , pages 4171\u20134186 .", "entities": []}, {"text": "Luyu Gao , Zhuyun Dai , and Jamie Callan . 2020a .", "entities": []}, {"text": "Understanding BERT rankers under distillation .", "entities": [[1, 2, "MethodName", "BERT"]]}, {"text": "In Proc .", "entities": []}, {"text": "ICTIR , pages 149\u2013152 .", "entities": []}, {"text": "Luyu Gao , Zhuyun Dai , Zhen Fan , and Jamie Callan . 2020b .", "entities": []}, {"text": "Complementing lexical retrieval with semantic residual embedding .", "entities": []}, {"text": "arXiv:2004.13969 .", "entities": []}, {"text": "Kelvin Guu , Kenton Lee , Zora Tung , Panupong Pasupat , and Ming - Wei Chang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "REALM : Retrieval - augmented language model pre - training .", "entities": []}, {"text": "arXiv:2002.08909 .", "entities": []}, {"text": "Geoffrey Hinton , Oriol Vinyals , and Jeffrey Dean . 2015 .", "entities": []}, {"text": "Distilling the knowledge in a neural network .", "entities": []}, {"text": "InProc .", "entities": []}, {"text": "NeurIPS :", "entities": []}, {"text": "Deep Learning and Representation Learning Workshop .", "entities": [[3, 5, "TaskName", "Representation Learning"]]}, {"text": "Sebastian Hofst\u00e4tter , Sophia Althammer , Michael Schr\u00f6der , Mete Sertkan , and Allan Hanbury .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Improving ef\ufb01cient neural ranking models with cross - architecture knowledge distillation . arXiv:2010.02666v2 .", "entities": [[9, 11, "MethodName", "knowledge distillation"]]}, {"text": "Sebastian Hofst\u00e4tter and Allan Hanbury .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Let \u2019s measure run time !", "entities": []}, {"text": "Extending the IR replicability infrastructure to include performance aspects .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "OSIRRC :", "entities": []}, {"text": "CEUR Workshop , pages 12\u201316 .", "entities": []}, {"text": "Sebastian Hofst\u00e4tter , Sheng - Chieh Lin , Jheng - Hong Yang , Jimmy Lin , and Allan Hanbury .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Ef\ufb01ciently teaching an effective dense retriever with balanced topic aware sampling .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "SIGIR .Samuel", "entities": []}, {"text": "Humeau , Kurt Shuster , Marie - Anne Lachaux , and Jason Weston .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Poly - encoders : Architectures and pre - training strategies for fast and accurate multi - sentence scoring .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "ICLR .", "entities": []}, {"text": "Jeff Johnson , Matthijs Douze , and Herv\u00e9 J\u00e9gou . 2017 .", "entities": []}, {"text": "Billion - scale similarity search with GPUs .", "entities": []}, {"text": "arXiv:1702.08734 .", "entities": []}, {"text": "Vladimir Karpukhin , Barlas O \u02d8guz , Sewon Min , Ledell Wu , Sergey Edunov , Danqi Chen , and Wen - tau Yih . 2020 .", "entities": []}, {"text": "Dense passage retrieval for open - domain question answering .", "entities": [[1, 3, "TaskName", "passage retrieval"], [4, 9, "TaskName", "open - domain question answering"]]}, {"text": "In Proc .", "entities": []}, {"text": "EMNLP , pages 6769 \u2013 6781 .", "entities": []}, {"text": "Omar Khattab and Matei Zaharia .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "ColBERT : Ef\ufb01cient and effective passage search via contextualized late interaction over BERT .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "In Proc .", "entities": []}, {"text": "SIGIR , page 39\u201348 .", "entities": []}, {"text": "Kenton Lee , Ming - Wei Chang , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Latent retrieval for weakly supervised open domain question answering .", "entities": [[7, 9, "TaskName", "question answering"]]}, {"text": "In Proc .", "entities": []}, {"text": "ACL , pages 6086\u20136096 .", "entities": []}, {"text": "Jimmy Lin , Xueguang Ma , Sheng - Chieh Lin , JhengHong Yang , Ronak Pradeep , and Rodrigo Nogueira . 2021 .", "entities": []}, {"text": "Pyserini : A Python toolkit for reproducible information retrieval research with sparse and dense representations .", "entities": [[7, 9, "TaskName", "information retrieval"]]}, {"text": "In Proc .", "entities": []}, {"text": "SIGIR .", "entities": []}, {"text": "Jimmy Lin , Rodrigo Nogueira , and Andrew Yates .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Pretrained transformers for text ranking : BERT and beyond .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "arXiv:2010.06467 .", "entities": []}, {"text": "Ting Liu , Andrew W. Moore , Alexander Gray , and Ke Yang .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "An investigation of practical approximate nearest neighbor algorithms .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "NeurIPS , page 825\u2013832 .", "entities": []}, {"text": "Shuqi Lu , Chenyan Xiong , Di He , Guolin Ke , Waleed Malik , Zhicheng Dou , Paul Bennett , Tieyan Liu , and Arnold Overwijk . 2021 .", "entities": []}, {"text": "Less is more : Pretraining a strong siamese encoder using a weak decoder .", "entities": []}, {"text": "arXiv:2102.09206 .", "entities": []}, {"text": "Yi Luan , Jacob Eisenstein , Kristina Toutanova , and Michael Collins .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Sparse , dense , and attentional representations for text retrieval .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 9:329\u2013345 .", "entities": []}, {"text": "Xueguang Ma , Kai Sun , Ronak Pradeep , and Jimmy Lin . 2021 .", "entities": []}, {"text": "A replication study of dense passage retriever .", "entities": []}, {"text": "arXiv:2104.05740 .", "entities": []}, {"text": "Yu A. Malkov and D. A. Yashunin . 2020 .", "entities": []}, {"text": "Ef\ufb01cient and robust approximate nearest neighbor search using hierarchical navigable small world graphs .", "entities": []}, {"text": "Transactions on Pattern Analysis and Machine Intelligence , 42(4):824\u2013836 .", "entities": []}, {"text": "Andriy Mnih and Koray Kavukcuoglu .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Learning word embeddings ef\ufb01ciently with noise - contrastive estimation .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Proc .", "entities": []}, {"text": "NIPS , pages 2265\u20132273 .", "entities": []}, {"text": "173Rodrigo Nogueira and Kyunghyun Cho . 2019 .", "entities": []}, {"text": "Passage re - ranking with BERT . arXiv:1901.04085 .", "entities": [[0, 4, "TaskName", "Passage re - ranking"], [5, 6, "MethodName", "BERT"]]}, {"text": "Rodrigo Nogueira and Jimmy Lin .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "From doc2query to docTTTTTquery .", "entities": []}, {"text": "Yingqi Qu , Yuchen Ding , Jing Liu , Kai Liu , Ruiyang Ren , Wayne Xin Zhao , Daxiang Dong , Hua Wu , and Haifeng Wang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "RocketQA : An optimized training approach to dense passage retrieval for open - domain question answering .", "entities": [[8, 10, "TaskName", "passage retrieval"], [11, 16, "TaskName", "open - domain question answering"]]}, {"text": "arxiv:2010.08191v1 .", "entities": []}, {"text": "Nils Reimers and Iryna Gurevych .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SentenceBERT : Sentence embeddings using Siamese BERTnetworks .", "entities": [[2, 4, "TaskName", "Sentence embeddings"]]}, {"text": "In Proc .", "entities": []}, {"text": "EMNLP , pages 3982\u20133992 .", "entities": []}, {"text": "Yu Sun , Shuohuan Wang , Yukun Li , Shikun Feng , Hao Tian , Hua Wu , and Haifeng Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "ERNIE 2.0 : A continual pre - training framework for language understanding .", "entities": []}, {"text": "arXiv:1907.12412 .", "entities": []}, {"text": "Lee Xiong , Chenyan Xiong , Ye Li , Kwok - Fung Tang , Jialin Liu , Paul Bennett , Junaid Ahmed , and Arnold Overwijk . 2021 .", "entities": []}, {"text": "Approximate nearest neighbor negative contrastive learning for dense text retrieval .", "entities": [[4, 6, "MethodName", "contrastive learning"]]}, {"text": "In Proc .", "entities": []}, {"text": "ICLR .", "entities": []}, {"text": "Peilin Yang , Hui Fang , and Jimmy Lin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Anserini :", "entities": []}, {"text": "Reproducible ranking baselines using Lucene .", "entities": []}, {"text": "Journal of Data and Information Quality , 10(4):Article 16 .", "entities": []}, {"text": "Jingtao Zhan , Jiaxin Mao , Yiqun Liu , Min Zhang , and Shaoping Ma . 2020 .", "entities": []}, {"text": "Learning to retrieve : How to train a dense retrieval model effectively and ef\ufb01ciently .", "entities": []}, {"text": "arXiv:2010.10469 .", "entities": []}]