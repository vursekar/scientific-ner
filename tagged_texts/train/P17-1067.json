[{"text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , pages 718\u2013728 Vancouver , Canada , July 30 - August 4 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1067Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , pages 718\u2013728 Vancouver , Canada , July 30 - August 4 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1067 EmoNet : Fine - Grained Emotion Detection with Gated Recurrent Neural Networks Muhammad Abdul - Mageed School of Library , Archival & Information Studies University of British Columbia muhammad.mageed@ubc.caLyle Ungar Computer and Information Science University of Pennsylvania ungar@cis.upenn.edu Abstract Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives .", "entities": [[48, 49, "DatasetName", "emotion"]]}, {"text": "However , progress on emotion detection has been hampered by the absence of large labeled datasets .", "entities": [[4, 5, "DatasetName", "emotion"]]}, {"text": "In this work , we build a very large dataset for \ufb01ne - grained emotions and develop deep learning models on it .", "entities": []}, {"text": "We achieve a new state - of - the - art on 24 \ufb01ne - grained types of emotions ( with an average accuracy of 87.58 % ) .", "entities": [[22, 24, "MetricName", "average accuracy"]]}, {"text": "We also extend the task beyond emotion types to model Robert Plutchik \u2019s 8 primary emotion dimensions , acquiring a superior accuracy of 95.68 % .", "entities": [[6, 7, "DatasetName", "emotion"], [15, 16, "DatasetName", "emotion"], [21, 22, "MetricName", "accuracy"]]}, {"text": "1 Introduction According to the Oxford English Dictionary , emotion is de\ufb01ned as \u201c [ a ] strong feeling deriving from one \u2019s circumstances , mood , or relationships with others .", "entities": [[9, 10, "DatasetName", "emotion"]]}, {"text": "\u201d1This \u201c standard \u201d de\ufb01nition identi\ufb01es emotions as constructs involving something innate that is often invoked in social interactions and that aids in communicating with others(Hwang and Matsumoto , 2016 ) .", "entities": []}, {"text": "It is no exaggeration that humans are emotional beings : Emotions are an integral part of human life , and affect our decision making as well as our mental and physical health .", "entities": [[22, 24, "TaskName", "decision making"]]}, {"text": "As such , developing emotion detection models is important ; they have a wide array of applications , ranging from building nuanced virtual assistants that cater for the emotions of their users to detecting the emotions of social media users in order to understand their mental and/or physical health .", "entities": [[4, 5, "DatasetName", "emotion"]]}, {"text": "1https://en.oxforddictionaries.com/ definition / emotion .However , emotion detection has remained a challenging task , partly due to the limited availability of labeled data and partly due the controversial nature of what emotions themselves are ( Aaron C. Weidman and Tracy , 2017 ) .", "entities": [[3, 4, "DatasetName", "emotion"], [6, 7, "DatasetName", "emotion"]]}, {"text": "Recent advances in machine learning for natural language processing ( NLP ) suggest that , given enough labeled data , there should be an opportunity to build better emotion detection models .", "entities": [[28, 29, "DatasetName", "emotion"]]}, {"text": "Manual labeling of data , however , is costly and so it is desirable to develop labeled emotion data without annotators .", "entities": [[17, 18, "DatasetName", "emotion"]]}, {"text": "While the proliferation of social media has made it possible for us to acquire large datasets with implicit labels in the form of hashtags ( Mohammad and Kiritchenko , 2015 ) , such labels are noisy and reliable .", "entities": []}, {"text": "In this work , we seek to enable deep learning by creating a large dataset of \ufb01ne - grained emotions using Twitter data .", "entities": []}, {"text": "More speci\ufb01cally , we harness cues in Twitter data in the form of emotion hashtags as a way to build a labeled emotion dataset that we then exploit using distant supervision ( Mintz et al . , 2009 ) ( the use of hashtags as a surrogate for annotator - generated emotion labels ) to build emotion models grounded in psychology .", "entities": [[13, 14, "DatasetName", "emotion"], [22, 23, "DatasetName", "emotion"], [51, 52, "DatasetName", "emotion"], [56, 57, "DatasetName", "emotion"]]}, {"text": "We construct such a dataset and exploit it using powerful deep learning methods to build accurate , high coverage models for emotion prediction .", "entities": [[21, 22, "DatasetName", "emotion"]]}, {"text": "Overall , we make the following contributions : 1 ) Grounded in psychological theory of emotions , we build a large - scale , high quality dataset of tweets labeled with emotions .", "entities": []}, {"text": "Key to this are methods to ensure data quality , 2 ) we validate the data collection method using human annotations , 3 ) we develop powerful deep learning models using a gated recurrent network to exploit the data , yielding new state - of - the - art on 24 \ufb01ne - grained types of emotions , and 4 ) we extend the task beyond these emotion types to model Plutick \u2019s 8 primary emotion dimensions.718", "entities": [[67, 68, "DatasetName", "emotion"], [75, 76, "DatasetName", "emotion"]]}, {"text": "Our emotion modeling relies on distant supervision(Read , 2005 ; Mintz et al . , 2009 ) , the approach of using cues in data ( e.g. , hashtags or emoticons ) as a proxy for \u201c ground truth \u201d labels as we explained above .", "entities": [[1, 2, "DatasetName", "emotion"]]}, {"text": "Distant supervision has been investigated by a number of researchers for emotion detection ( Tanaka et al . , 2005 ; Mohammad , 2012 ; Purver and Battersby , 2012 ; Wang et al . , 2012 ; Pak and Paroubek , 2010 ;", "entities": [[11, 12, "DatasetName", "emotion"]]}, {"text": "Yang et al . , 2007 ) and for other semantic tasks such as sentiment analysis ( Read , 2005 ; Go et al . , 2009 ) and sarcasm detection ( Gonz \u00b4 alez - Ib \u00b4 anez et al . , 2011 ) .", "entities": [[14, 16, "TaskName", "sentiment analysis"], [29, 31, "TaskName", "sarcasm detection"]]}, {"text": "In these works , authors successfully use emoticons and/or hashtags as marks to label data after performing varying degrees of data quality assurance .", "entities": []}, {"text": "We take a similar approach , using a larger collection of tweets , richer emotion de\ufb01nitions , and stronger \ufb01ltering for tweet quality .", "entities": [[14, 15, "DatasetName", "emotion"]]}, {"text": "The remainder of the paper is organized as follows : We \ufb01rst overview related literature in Section 2 , describe our data collection in Section 3.1 , and the annotation study we performed to validate our distant supervision method in Section 4 .", "entities": []}, {"text": "We then describe our methods in Section 5 , provide results in Section 6 , and conclude in Section 8 . 2 Related Work 2.1 Computational Treatment of Emotion The SemEval-2007 Affective Text task ( Strapparava and Mihalcea , 2007 )", "entities": [[31, 33, "DatasetName", "Affective Text"]]}, {"text": "[ SEM07 ] focused on classi\ufb01cation of emotion and valence ( i.e. , positive and negative texts ) in news headlines .", "entities": [[7, 8, "DatasetName", "emotion"]]}, {"text": "A total of 1,250 headlines were manually labeled with the 6 basic emotions of Ekman ( Ekman , 1972 ) and made available to participants .", "entities": []}, {"text": "Similarly , ( Aman and Szpakowicz , 2007 ) describe an emotion annotation task of identifying emotion category , emotion intensity and the words / phrases that indicate emotion in blog post data of 4,090 sentences and a system exploiting the data .", "entities": [[11, 12, "DatasetName", "emotion"], [16, 17, "DatasetName", "emotion"], [19, 20, "DatasetName", "emotion"], [28, 29, "DatasetName", "emotion"]]}, {"text": "Our work differs from both that of SEM07 ( Strapparava and Mihalcea , 2007 ) and ( Aman and Szpakowicz , 2007 ) in that we focus on a different genre ( i.e. , Twitter ) and investigate distant supervision as a way to acquire a signi\ufb01cantly larger labeled dataset .", "entities": []}, {"text": "Our work is similar to ( Mohammad , 2012 ; Mohammad and Kiritchenko , 2015 ) , ( Wang et al . , 2012 ) , and ( V olkova and Bachrach , 2016 ) who use distant supervision to acquire Twitter data with emotion hashtags and report analyses and experiments to validate the utility of this approach .", "entities": [[44, 45, "DatasetName", "emotion"]]}, {"text": "Forexample , ( Mohammad , 2012 ) shows that by using a simple domain adaptation method to train a classi\ufb01er on their data they are able to improve both precision and recall on the SemEval-2007 ( Strapparava and Mihalcea , 2007 ) dataset .", "entities": [[13, 15, "TaskName", "domain adaptation"]]}, {"text": "As the author points out , this is another premise that the selflabeled hashtags acquired from Twitter are consistent , to some degree , with the emotion labels given by the trained human judges who labeled the SemEval-2007 data .", "entities": [[26, 27, "DatasetName", "emotion"]]}, {"text": "As pointed out earlier , ( Wang et al . , 2012 ) randomly sample a set of 400 tweets from their data and human - label as relevant / irrelevant , as a way to verify the distant supervision approach with the quality assurance heuristics they employ .", "entities": []}, {"text": "The authors found that the precision on a test set is 93.16 % , thus con\ufb01rming the utility of the heuristics .", "entities": []}, {"text": "( Wang et al . , 2012 ) provide a number of important observations , as conclusions based on their work .", "entities": []}, {"text": "These include that since they are provided by the tweets \u2019 writers , the emotion hashtags are more natural and reliable than the emotion labels traditionally assigned by annotators to data by a few annotators .", "entities": [[14, 15, "DatasetName", "emotion"], [23, 24, "DatasetName", "emotion"]]}, {"text": "This is the case since in the lab - condition method annotators need to infer the writers emotions from text , which may not be accurate .", "entities": []}, {"text": "Additionally , ( V olkova and Bachrach , 2016 ) follow the same distant supervision approach and \ufb01nd correlations of users \u2019 emotional tone and the perceived demographics of these users \u2019 social networks exploiting the emotion hashtag - labeled data .", "entities": [[36, 37, "DatasetName", "emotion"]]}, {"text": "Our dataset is more than an order of magnitude larger than ( Mohammad , 2012 ) and ( V olkova and Bachrach , 2016 ) and the range of emotions we target is much more \ufb01ne grained than ( Mohammad , 2012 ; Wang et al . , 2012 ; V olkova and Bachrach , 2016 ) since we model 24 emotion types , rather than focus on \u22647 basic emotions .", "entities": [[61, 62, "DatasetName", "emotion"]]}, {"text": "( Yan et al . , 2016 ; Yan and Turtle , 2016a , b ) develop a dataset of 15,553 tweets labeled with 28 emotion types and so target a \ufb01ne - grained range as we do .", "entities": [[1, 4, "DatasetName", "Yan et al"], [25, 26, "DatasetName", "emotion"]]}, {"text": "The authors instruct human annotators under lab conditions to assign any emotion they feel is expressed in the data , allowing them to assign more than one emotion to a given tweet .", "entities": [[11, 12, "DatasetName", "emotion"], [27, 28, "DatasetName", "emotion"]]}, {"text": "A set of 28 chosen emotions was then decided upon and further annotations were performed using Amazon Mechanical Turk ( AMT ) .", "entities": []}, {"text": "The authors cite an agreement of 0.50 Krippendorff \u2019s alpha ( \u03b1 ) between the lab / expert annotators , and an ( \u03b1 ) of 0.28 between experts and AMT workers .", "entities": [[9, 10, "HyperparameterName", "alpha"], [11, 12, "HyperparameterName", "\u03b1"], [23, 24, "HyperparameterName", "\u03b1"]]}, {"text": "EmoTweet-719", "entities": []}, {"text": "28 is a useful resource .", "entities": []}, {"text": "However , the agreement between annotators is not high and the set of assigned labels do not adhere to a speci\ufb01c theory of emotion .", "entities": [[23, 24, "DatasetName", "emotion"]]}, {"text": "We use a much larger dataset and report an accuracy of the hashtag approach at 90 % based on human judgement as reported in Section 4 . 2.2 Mood A number of studies have also been performed to analyze and/or model mood in social media data .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "( De Choudhury et", "entities": []}, {"text": "al . , 2012 ) identify more than 200 moods frequent on Twitter as extracted from psychological literature and \ufb01ltered by AMT workers .", "entities": []}, {"text": "They then collect tweets which have one of the moods in their mood lexicon in the form of a hashtag .", "entities": []}, {"text": "To verify the quality of the mood data , the authors run AMT studies where they ask workers whether a tweet displayed the respective mood hashtag or not and \ufb01nd that in 83 % of the cases hashtagged moods at the end of posts did capture users \u2019 moods , whereas for posts with mood hashtags anywhere in the tweet , only 58 % of the cases capture the mood of users .", "entities": []}, {"text": "Although they did not build models for mood detection , the annotation studies ( De Choudhury et al . , 2012 ) perform further support our speci\ufb01c use of hashtags to label emotions .", "entities": []}, {"text": "( Mishne and De Rijke , 2006 ) collect user - labeled mood from blog post text on LiveJournal and exploit them for predicting the intensity of moods over a time span rather than at the post level .", "entities": []}, {"text": "Similarly , ( Nguyen , 2010 ) builds models to infer patterns of moods in a large collection of LiveJournal posts .", "entities": []}, {"text": "Some of the moods in these LiveJournal studies ( e.g. , hungry , cold ) , as ( De Choudhury et al . , 2012 ) explain , would not \ufb01t any psychological theory .", "entities": []}, {"text": "Our work is different in that it is situated in psychological theory of emotion .", "entities": [[13, 14, "DatasetName", "emotion"]]}, {"text": "2.3 Deep Learning for NLP In spite of the effectiveness of feature engineering for NLP , it is a labor intensive task that also needs domain expertise .", "entities": [[11, 13, "TaskName", "feature engineering"]]}, {"text": "More importantly , feature engineering falls short of extracting and organizing all the discriminative information from data ( LeCun et al . , 2015 ; Goodfellow et", "entities": [[3, 5, "TaskName", "feature engineering"]]}, {"text": "al . , 2016 ) .", "entities": []}, {"text": "Neural networks ( Goodfellow et", "entities": []}, {"text": "al . , 2016 ) have emerged as a successful class of methods that has the power of automatically discovering the representations needed for detection or classi\ufb01cation and has been successfully applied to multiple NLP tasks .", "entities": []}, {"text": "A line of studies in the literature ( e.g. , ( Labutov and Lip - son , 2013 ; Maas et", "entities": []}, {"text": "al . , 2011 ; Tang et al . , 2014b , a ) aim to learn sentiment - speci\ufb01c word embeddings ( Bengio et al . , 2003 ; Mikolov et", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "al . , 2013 ) from neighboring text .", "entities": []}, {"text": "Another thread of research focuses on learning semantic composition ( Mitchell and Lapata , 2010 ) , including extensions to phrases and sentences with recursive neural networks ( a class of syntax - tree models )", "entities": [[7, 9, "TaskName", "semantic composition"]]}, {"text": "( Socher et al . , 2013 ; Irsoy and Cardie , 2014 ; Li et al . , 2015 ) and to documents with distributed representations of sentences and paragraphs ( Le and Mikolov , 2014 ; Tang et al . , 2015 ) for modeling sentiment .", "entities": []}, {"text": "Long - short term memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) and Gated Recurrent Neural Nets ( GRNNs ) ( Cho et al . , 2014 ; Chung et", "entities": [[6, 7, "MethodName", "LSTM"]]}, {"text": "al . , 2015 ) , variations of recurrent neural networks ( RNNs ) , a type of networks suitable for handling time - series data like speech ( Graves et al . , 2013 ) or handwriting recognition ( Graves , 2012 ; Graves and Schmidhuber , 2009 ) , have also been used successfully for sentiment analysis ( Ren et al . , 2016 ;", "entities": [[37, 39, "TaskName", "handwriting recognition"], [57, 59, "TaskName", "sentiment analysis"]]}, {"text": "Liu et al . , 2015 ; Tai et al . , 2015 ; Tang et al . , 2015 ; Zhang et al . , 2016 ) .", "entities": []}, {"text": "Convolutional neural networks ( CNNs ) have also been quite successful in NLP , and have been applied to a range of sentence classi\ufb01cation tasks , including sentiment analysis ( Blunsom et al . , 2014 ; Kim , 2014 ; Zhang et al . , 2015 ) .", "entities": [[27, 29, "TaskName", "sentiment analysis"]]}, {"text": "Other architectures have also been recently proposed ( e.g. , ( Bradbury et al . , 2016 ) ) .", "entities": []}, {"text": "A review of neural network methods for NLP can be found in ( Goldberg , 2016 ) .", "entities": []}, {"text": "3 Data 3.1 Collection of a Large - Scale Dataset To be able to use deep learning for modeling emotion , we needed a large dataset of labeled tweets .", "entities": [[19, 20, "DatasetName", "emotion"]]}, {"text": "Since there is no such human - labeled dataset publicly available , we follow ( Mohammad , 2012 ; Mintz et al . , 2009 ; Purver and Battersby , 2012 ; Gonz \u00b4 alez - Ib \u00b4 anez", "entities": []}, {"text": "et", "entities": []}, {"text": "al . , 2011 ; Wang et al . , 2012 ) in adopting distant supervision : We collect tweets with emotion - carrying hashtags as a surrogate for emotion labels .", "entities": [[21, 22, "DatasetName", "emotion"], [29, 30, "DatasetName", "emotion"]]}, {"text": "To be able to collect enough tweets to serve our need , we developed a list of hashtags representing each of the 24 emotions proposed by Robert Plutchick ( Plutchik , 1980 , 1985 , 1994 ) .", "entities": []}, {"text": "Plutchik ( Plutchik , 2001 ) organizes emotions in a three - dimensional circumplex model analogous to the colors on a color wheel .", "entities": []}, {"text": "The cone \u2019s vertical dimension represents intensity , and the 3 circle represent degrees of similarity720", "entities": []}, {"text": "Figure 1 : Plutchik \u2019s wheel of emotion .", "entities": [[7, 8, "DatasetName", "emotion"]]}, {"text": "among the various emotion types .", "entities": [[3, 4, "DatasetName", "emotion"]]}, {"text": "The eight sectors are meant to capture that there are eight primary emotion dimensions arranged as four pairs of opposites .", "entities": [[12, 13, "DatasetName", "emotion"]]}, {"text": "Emotions in the blank spaces are the primary emotion dyads ( i.e. , emotions that are mixtures of two of the primary emotions ) .", "entities": [[8, 9, "DatasetName", "emotion"]]}, {"text": "For this work , we exclude the dyads in the exploded model from our treatment .", "entities": []}, {"text": "For simplicity , we refer to the circles as plutchik-1 : with the emotions { admiration , amazement , ecstasy , grief , loathing , rage , terror , vigilance } , plutchik-2 : with the emotions{joy , trust , fear , surprise , sadness , disgust , anger , anticipation } , and plutchik-3 : with the emotions { acceptance , annoyance , apprehension , boredom , distraction , interest , pensiveness , serenity } .", "entities": []}, {"text": "The wheel is shown in Figure 1 .", "entities": []}, {"text": "For each emotion type , we prepared a seed set of hashtags representing the emotion .", "entities": [[2, 3, "DatasetName", "emotion"], [14, 15, "DatasetName", "emotion"]]}, {"text": "We used Google synonyms and other online dictionaries and thesauri ( e.g. , www.thesaurus .", "entities": [[2, 3, "DatasetName", "Google"]]}, {"text": "com ) to expand the initial seed set of each emotion .", "entities": [[10, 11, "DatasetName", "emotion"]]}, {"text": "We acquire a total of 665 emotion hashtags across the 24 emotion types .", "entities": [[6, 7, "DatasetName", "emotion"], [11, 12, "DatasetName", "emotion"]]}, {"text": "For example , for the joyemotion , a subset of the seeds in our expanded set is { \u201c happy \u201d , \u201c happiness \u201d , \u201c joy \u201d , \u201c joyful \u201d , \u201c joyfully \u201d , \u201c delighted \u201d , \u201c feelingsunny \u201d , \u201c blithe \u201d , \u201c beati\ufb01c \u201d , \u201c exhilarated \u201d , \u201c blissful \u201d , \u201c walkingonair \u201d , \u201c jubilant \u201d } .", "entities": [[11, 12, "DatasetName", "seeds"]]}, {"text": "We then used the expanded set to extract tweets with hashtags from the set from a number of massive - scale in - house Twitter datasets .", "entities": []}, {"text": "We also used Twitter API to crawl Twitter with hashtags from the expanded set .", "entities": []}, {"text": "Using this method , we were able to acquire a dataset of about 1/4billion tweets covering an extended time span from July 2009 till January 2017.3.2", "entities": []}, {"text": "Preprocessing and Quality Assurance Twitter data are very noisy , not only because of use of non - standard typography ( which is less of a problem here ) but due to the many duplicate tweets and the fact that tweets often have multiple emotion hashtags .", "entities": [[44, 45, "DatasetName", "emotion"]]}, {"text": "Since these reduce our ability to build accurate models , we need to clean the data and remove duplicates .", "entities": []}, {"text": "Starting with > 1/4 billion tweets , we employ a rigorous and strict pipeline .", "entities": []}, {"text": "This results in a vastly smaller set of about 1.6 million dependable labeled tweets .", "entities": []}, {"text": "Since our goal is to create non - overlapping categories at the level of a tweet , we \ufb01rst removed all tweets with hashtags belonging to more than one emotion of the 24 emotion categories .", "entities": [[29, 30, "DatasetName", "emotion"], [33, 34, "DatasetName", "emotion"]]}, {"text": "Since it was observed ( e.g. , ( Mohammad , 2012 ; Wang et al . , 2012 ) ) and also con\ufb01rmed by our annotation study as described in Section 4 , that hashtags in tweets with URLs are less likely to correlate with a true emotion label , we remove all tweets with URLs from our data .", "entities": [[47, 48, "DatasetName", "emotion"]]}, {"text": "We \ufb01lter out duplicates using a two - step procedure : 1 ) we remove all retweets ( based on existence of the token \u201c RT \u201d regardless of case ) and 2 ) we use the Python library pandas http://pandas . pydata.org/ \u201c drop duplicates \u201d method to compare the tweet texts of all the tweets after normalizing character repetitions [ all consecutive characters of > 2to 2 ] and user mentions ( as detected by a string starting with an \u201c @ \u201d sign ) .", "entities": []}, {"text": "We then performed a manual inspection of a random sample of 1,000 tweets from the data and found no evidence of any remaining tweet duplicates .", "entities": []}, {"text": "Next , even though the emotion hashtags themselves are exclusively in English , we observe the data do have tweets in languages other than English .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "This is due to code - switching , but also to the fact that our data dates back to 2009 and Twitter did not allow use of hashtags for several non - English languages until 2012 .", "entities": []}, {"text": "To \ufb01lter out non - English , we use the langid ( Lui and Baldwin , 2012 ) ( https://github.com/ saffsd / langid.py ) library to assign language tags to the tweets .", "entities": []}, {"text": "Since the common wisdom in the literature ( e.g. , ( Mohammad , 2012 ; Wang et al . , 2012 ) ) is to restrict data to hashtags occurring in \ufb01nal position of a tweet , we investigate correlations between a tweet \u2019s relevance and emotion hashtag location in Section 4 and test models exclusively on data with hashtags occurring in \ufb01nal position .", "entities": [[46, 47, "DatasetName", "emotion"]]}, {"text": "We also only use tweets con-721", "entities": []}, {"text": "taining at least 5words .", "entities": []}, {"text": "Table 2 shows statistics of the data after applying our cleaning , \ufb01ltering , language identi\ufb01cation , and deduplication pipeline .", "entities": []}, {"text": "Since our focus is on English , we only show statistics for tweets tagged with an \u201c en \u201d ( for \u201c English \u201d ) label by langid .", "entities": []}, {"text": "Table 2 provides three types of relevant statistics : 1 ) counts of all tweets , 2 ) counts of tweets with at least 5 words and the emotion hashtags occurring in the last quarter of the tweet text ( based on character count ) , and 3 ) counts of tweets with at least 5 words and the emotion hashtags occurring as the \ufb01nal word in the tweet text .", "entities": [[28, 29, "DatasetName", "emotion"], [59, 60, "DatasetName", "emotion"]]}, {"text": "As the last column in Table 2 shows , employing our most strict criterion where an emotion hashtag must occur \ufb01nally in a tweet of a minimal length 5words , we acquire a total of 1,608,233 tweets : 205,125 tweets for plutchik-1 , 790,059 for plutchik-2 , and 613,049 for plutchik-3 .2", "entities": [[16, 17, "DatasetName", "emotion"]]}, {"text": "Emotion ct ct@lq ct@end admiration 292,153 150,509 112,694 amazement 568,255 358,472 34,826 ecstasy 54,174 34,307 23,856 grief 102,980 33,141 12,568 loathing 90,465 41,787 456 rage 30,994 11,777 4,749 terror 84,827 25,908 15,268 vigilance 6,171 1,028 708 plutchik-1 1,230,019 656,929 205,125 anger 131,082 82,447 56,472 anticipation 67,175 36,846 26,655 disgust 212,770 145,052 52,067 fear 302,989 153,513 98,657 joy 974,226 522,689 330,738 sadness 1,252,192 762,901 142,300 surprise 143,755 78,570 53,915 trust 198,619 103,332 29,255 plutchik-2 3,282,808 1,885,350 790,059 acceptance 138,899 54,706 16,522 annoyance 954,027 791,869 364,135 apprehension 29,174 11,650 7,828 boredom 872,246 583,994 152,105 distraction 122,009 52,633 617 interest 113,555 67,216 56,659 pensiveness 11,751 5,012 3,513 serenity 97,467 36,817 11,670 plutchik-3 2,339,128 1,603,897 613,049 ALL 6,851,955 4,146,176 1,608,233 Table 2 : Data statistics .", "entities": []}, {"text": "4 Annotation Study In their work , ( Wang et al . , 2012 ) manually label a random sample of 400 tweets extracted with hash2The data can be acquired by emailing the \ufb01rst author .", "entities": []}, {"text": "The distribution is in the form of tweet ids and labels , to adhere to Twitter conditions.tags in a similar way as we acquire our data and \ufb01nd that human annotators agree 93 % of the time with the hashtag emotion type if the hashtag occurs as the last word in the tweet .", "entities": [[40, 41, "DatasetName", "emotion"]]}, {"text": "We wanted to validate our use of hashtags in a similar fashion and on a bigger random sample .", "entities": []}, {"text": "We had human annotators label a random sample of 5,600 tweets that satisfy our preprocessing pipeline .", "entities": []}, {"text": "Manual inspection during annotation resulted in further removing a negligible 16 tweets that were found to have problems .", "entities": []}, {"text": "For each of the remaining 5,584 tweets , the annotators assign a binary tag from the set{relevant , irrelevant } to indicate whether a tweet carries an emotion category as assigned using our distant supervision method or not .", "entities": [[27, 28, "DatasetName", "emotion"]]}, {"text": "Annotators assigned 61.37 % ( n= 3,427 ) \u201c relevant \u201d tags and 38.63 % ( n= 2,157 ) \u201c irrelevant \u201d tags .", "entities": []}, {"text": "Our analysis of this manually labeled dataset also supports the \ufb01ndings of ( Wang et al . , 2012 ): When we limit position of the emotion hashtag to the end of a tweet , we acquire 90.57 % relevant data .", "entities": [[26, 27, "DatasetName", "emotion"]]}, {"text": "We also \ufb01nd that if we relax the constraint on the hashtag position such that we allow the hashtag to occur in the last quarter of a tweet ( based on a total tweet character count ) , we acquire 85.43 % relevant tweets .", "entities": []}, {"text": "We also \ufb01nd that only 23.20 % ( n= 795 out of 3,427 ) of the emotion carrying tweets have the emotion hashtags occurring in \ufb01nal position , whereas 31.75 % ( n= 1,088out of 3,427 ) of the tweets have the emotion hashtags in the last quarter of the tweet string .", "entities": [[16, 17, "DatasetName", "emotion"], [21, 22, "DatasetName", "emotion"], [42, 43, "DatasetName", "emotion"]]}, {"text": "This shows how enforcing a \ufb01nal hashtag location results in loss of a considerable number of emotion tweets .", "entities": [[10, 11, "MetricName", "loss"], [16, 17, "DatasetName", "emotion"]]}, {"text": "As shown in Table 2 , only 1,608,233tweets out of a total of 6,851,955tweets", "entities": []}, {"text": "( % = 23,47 ) in our bigger dataset have emotion hashtags occurring in \ufb01nal position .", "entities": [[10, 11, "DatasetName", "emotion"]]}, {"text": "Overall , we agree with ( Mohammad , 2012 ; Wang et al . , 2012 ) that the accuracy acquired by enforcing a strict pipeline and limiting to emotion hashtags to \ufb01nal position is a reasonable measure for warranting good - quality data for training supervised systems , an assumption we have also validated with our empirical \ufb01ndings here .", "entities": [[19, 20, "MetricName", "accuracy"], [29, 30, "DatasetName", "emotion"]]}, {"text": "One advantage of using distant supervision under these conditions for labeling emotion data , as ( Wang et al . , 2012 ) also notes , is that the label is assigned by the writer of the tweet himself / herself rather than an annotator who could wrongly decide what category a tweet is .", "entities": [[11, 12, "DatasetName", "emotion"]]}, {"text": "After all , emotion is a fuzzy concept and > 90 % agreement as we722", "entities": [[3, 4, "DatasetName", "emotion"]]}, {"text": "report here is higher than the human agreement usually acquired on many NLP tasks .", "entities": []}, {"text": "Another advantage of this method is obviously that it enables us to acquire a suf\ufb01ciently large training set to use deep learning .", "entities": []}, {"text": "We now turn to describing our deep learning methods .", "entities": []}, {"text": "5 Methods For our core modeling , we use Gated Recurrent Neural Networks ( GRNNs ) , a modern variation of recurrent neural networks ( RNNs ) , which we now turn to introduce .", "entities": []}, {"text": "For notation , we denote scalars with italic lowercase ( e.g. , x ) , vectors with bold lowercase ( e.g. , x ) , and matrices with bold uppercase ( e.g. , W ) .", "entities": []}, {"text": "Recurrent Neural Network A recurrent neural network ( RNN ) is one type of neural network architecture that is particularly suited for modeling sequential information .", "entities": []}, {"text": "At each time step t , an RNN takes an input vector xt / epsilon1I", "entities": []}, {"text": "Rnand a hidden state vector ht\u22121 / epsilon1I", "entities": []}, {"text": "Rmand produces the next hidden state htby applying the recursive operation : ht = f(Wx t+Uht\u22121+b ) ( 1 ) Where the input to hidden matrix W / epsilon1I", "entities": []}, {"text": "Rmxn , the hidden to hidden matrix U / epsilon1I Rmxm , and the bias vector b / epsilon1I", "entities": []}, {"text": "Rmare parameters of an af\ufb01ne transformation and fis an element - wise nonlinearity .", "entities": []}, {"text": "While an RNN can in theory summarize all historical information up to time step ht , in practice it runs into the problem of vanishing / exploding gradients ( Bengio et al . , 1994 ; Pascanu et al . , 2013 ) while attempting to learn longrange dependencies .", "entities": []}, {"text": "LSTM Long short - term memory ( LSTM ) networks ( Hochreiter and Schmidhuber , 1997 ) addresses this exact problem of learning long - term dependencies by augmenting an RNN with a memory cell ct / epsilon1I Rnat each time step .", "entities": [[0, 1, "MethodName", "LSTM"], [1, 6, "MethodName", "Long short - term memory"], [7, 8, "MethodName", "LSTM"]]}, {"text": "As such , in addition to the input vector xt , the hiddent vector ht\u22121 , an LSTM takes a cell state vector ct\u22121and produces htandctvia the following calculations : it = \u03c3 / parenleftbig Wixt+Uiht\u22121+bi / parenrightbig ft = \u03c3 / parenleftBig Wfxt+Ufht\u22121+bf / parenrightBig", "entities": [[17, 18, "MethodName", "LSTM"]]}, {"text": "ot = \u03c3(Woxt+Uoht\u22121+bo ) gt= tanh ( Wgxt+Ught\u22121+bg ) ct = ft\u2299ct\u22121+it\u2299gt ht = ot\u2299tanh ( ct)(2)Where\u03c3(\u00b7)andtanh(\u00b7)are the element - wise sigmoid and hyperbolic tangent functions , \u2299the element - wise multiplication operator , and it , ft , ot are the input , forget , and output gates .", "entities": []}, {"text": "The gtis a new memory cell vector with candidates that could be added to the state .", "entities": []}, {"text": "The LSTM parameters Wj , Uj , and bjare forj / epsilon1{i , f , o , g } .", "entities": [[1, 2, "MethodName", "LSTM"]]}, {"text": "GRNNs ( Cho et al . , 2014 ; Chung et al . , 2015 ) propose a variation of LSTM with a reset gate rt , an update state zt , and a new simpler hidden unit ht , as follows : rt = \u03c3(Wrxt+Urht\u22121+br )", "entities": [[20, 21, "MethodName", "LSTM"]]}, {"text": "zt = \u03c3(Wzxt+Uzht\u22121+bz )", "entities": []}, {"text": "\u02dcht= tanh / parenleftBig Wxt+rt\u2217U\u02dchht\u22121+b\u02dch / parenrightBig", "entities": []}, {"text": "ht = zt\u2217ht\u22121 + ( 1\u2212zt)\u2217\u02dcht(3 ) The GRNN parameters Wj , Uj , and bjare forj / epsilon1 { r , z,\u02dch } .", "entities": []}, {"text": "In this set up , the hidden state is forced to ignore a previous hidden state when the reset gate is close to 0 , thus enabling the network to forget or drop irrelevant information .", "entities": [[23, 24, "DatasetName", "0"]]}, {"text": "Additionally , the update gate controls how much information carries over from a previous hidden state to the current hidden state ( similar to an LSTM memory cell ) .", "entities": [[25, 26, "MethodName", "LSTM"]]}, {"text": "We use GRNNs as they are simpler and faster than LSTM .", "entities": [[10, 11, "MethodName", "LSTM"]]}, {"text": "For GRNNs , we use Theano ( Theano Development Team , 2016 ) .", "entities": []}, {"text": "Online Classi\ufb01ers We compare the performance of the GRNNs to four online classi\ufb01ers that are capable of handling the data size : Stochastic Gradient Descent ( SGD ) , Multinomial Naive Bayes ( MNB ) , Perceptron , and the Passive Agressive Classi\ufb01er ( PAC ) .", "entities": [[22, 25, "MethodName", "Stochastic Gradient Descent"], [26, 27, "MethodName", "SGD"]]}, {"text": "These classi\ufb01ers learn online from mini - batches of data .", "entities": []}, {"text": "We use minibatches of 10,000 instances with all the four classi\ufb01ers .", "entities": []}, {"text": "We use the scikit - learn implementation of these classi\ufb01ers ( http://scikit - learn . org ) .", "entities": []}, {"text": "Settings We aim to model Plutchik \u2019s 24 \ufb01negrained emotions as well as his 8 primary emotion dimensions where each 3 related types of emotion ( perceived as varying in intensity ) are combined in one dimension .", "entities": [[16, 17, "DatasetName", "emotion"], [24, 25, "DatasetName", "emotion"]]}, {"text": "We now turn to describing our experiments experiments .", "entities": []}, {"text": "6 Experiments 6.1", "entities": []}, {"text": "Predicting Fine - Grained Emotions As explained earlier , Plutchik organizes the 24 emotion types in the 3 main circles that we will refer to as plutchik-1 , plutchik-2 , and plutchik-3 .723", "entities": [[13, 14, "DatasetName", "emotion"]]}, {"text": "Emotion Qadir ( 2013 ) Roberts ( 2012 ) MD ( 2015 ) Wang ( 2012 ) Volkova ( 2016 )", "entities": []}, {"text": "This work anger 400 0.44 583 0.64 1,555 0.28 457,972 0.72 4,963 0.80 56,472 0.75 anticip - - - - - - - - - - 26,655 0.70 disgust - - 922 0.67 761 0.19 - - 12,948 0.92 52,067 0.82 fear 592 0.54 222 0.74 2,816 0.51 11,156 0.44 9,097 0.77 98,657 0.74 joy", "entities": []}, {"text": "1,005 0.59 716 0.68 8,240 0.62 567,487 0.72 15,559 0.79 330,738 0.91 sadness 560 0.46 493 0.69 3,830 0.39 489,831 0.65 4,232 0.62 142,300 0.73 surprise - - 324 0.61 3849 0.45 1,991 0.14 8,244 0.64 53,915 0.86 trust - - - - - - - - - - 29,255 0.82", "entities": []}, {"text": "ALL 4,500 0.53 3,777 0.67 21,051 0.49 1,991,184 - 52,925 0.78 790,059 0.83 Table 6 : Comparison ( in F - score ) of our results with GRNNs to published literature .", "entities": []}, {"text": "MD = Mohammad ( 2015 ) .", "entities": []}, {"text": "Note : For space restrictions , we take the liberty of using the last name of only the \ufb01rst author of each work .", "entities": []}, {"text": "Emotion SGD MNB PRCPTN PAC baseline 60.00 60.00 60.00 60.00 admiration 78.30 78.01 74.24 79.86 amazement 37.57 35.71 42.51 46.69 ecstasy 51.53 51.89 47.37 53.53 grief 38.64 36.94 37.33 48.10 loathing 0.00 0.00 2.09 2.99 rage 3.47 4.49 14.02 17.04 terror 33.23 44.12 40.48 47.00 vigilance 2.53 2.56 5.52 8.42 plutchik-1 60.26 60.54 59.11 64.86 anger 19.41 13.84 24.54 29.26 anticipation 7.46 12.63 17.29 26.70 disgust 29.51 29.87 31.83 36.60 fear 21.45 25.49 30.41 33.59 joy 72.83 72.96 72.32 75.50 sadness 50.04 51.72 39.58 49.21 surprise 8.46 4.75 17.34 19.54 trust 42.09 38.52 44.48 47.51 plutchik-2 48.05 48.33 48.60 53.30 acceptance 0.12 2.74 13.98 13.04 annoyance 80.28 80.71 78.80 81.47 apprehension 0.80 0.00 9.72 10.66 boredom 49.53 51.27 52.02 57.84 distraction 0.00 2.99 3.42 0.00 interest 21.69 30.45 34.85 44.14 pensiveness 2.61 8.08 11.22 12.27 serenity 8.87 19.57 27.23 38.59 plutchik-3 62.20 64.00 64.04 68.14 ALL 56.84 57.62 57.25 62.10 Table 3 : Results in F - score with traditional online classi\ufb01ers .", "entities": [[1, 2, "MethodName", "SGD"]]}, {"text": "We model the set of emotions belonging to each of the 3 circles independently , thus casting each as an 8 - way classi\ufb01cation task .", "entities": []}, {"text": "Inspired by observations from the literature and our own annotation study , we limit our data to tweets of at least 5 words with an emotional hashtag occurring at the end .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "We then split the data representing each of the 3 circles into 80 % training ( TRAIN ) , 10 % development ( DEV ) , and 10 % testing ( TEST ) .", "entities": []}, {"text": "As mentioned above , we run experiments with a range of online , out - of - core classi\ufb01ers as well as theGRNNs .", "entities": []}, {"text": "To train the GRNNs , we optimize the hyper - parameters of the network on a development set as we describe below , choosing a vocabulary size of 80 K words ( a vocabulary size we also use for the out - of - core classi\ufb01ers ) , a word embedding vector of size 300 dimensions learnt directly from the training data , an input maximum length of 30 words , 7 epochs , and the Adam ( Kingma and Ba , 2014 ) optimizer with a learning rate of 0.001 .", "entities": [[76, 77, "MethodName", "Adam"], [84, 85, "HyperparameterName", "optimizer"], [87, 89, "HyperparameterName", "learning rate"]]}, {"text": "We use 3 dense layers each with 1,000units .", "entities": []}, {"text": "We use dropout ( Hinton et al . , 2012 ) for regularization , with a dropout rate of 0.5 .", "entities": []}, {"text": "For our loss function , we use categorical cross - entropy .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "We use a minibatch ( Cotter et al . , 2011 ) size of 128 .", "entities": []}, {"text": "We found this architecture to work best with almost all the settings and so we \ufb01x it across the board for all experiments with GRNNs .", "entities": []}, {"text": "Results with Traditional Classi\ufb01ers Results with the online classi\ufb01ers are presented in terms ofF - score in Table 3 .", "entities": []}, {"text": "As the table shows , among this group of classi\ufb01ers , the Passive Agressive classi\ufb01er ( PAC ) acquires the best performance .", "entities": []}, {"text": "PAC achieves an overall F - score of 64.86 % on plutchik-1 , 53.30 % on plutchik-2 , and 68.14 % on plutchik-3 , two of which are higher than an arbitrary baseline3of 60 % .", "entities": []}, {"text": "Results with GRNNs Table 4 presents results with GRNNs , compared with the best results using the traditional classi\ufb01ers as acquired with PAC .", "entities": []}, {"text": "As the table shows , the GRNN models are very successful across all the 3 classi\ufb01cation tasks .", "entities": []}, {"text": "With GRNNs , we acquire an overall F - scores of : 91.21 % on plutchik-1 , 82.32 % onplutchik-2 , and 87.47 % on plutchik-3 .", "entities": []}, {"text": "These results are 26.35 % , 29.02 % , and 25.37 % higher than PAC , respectively .", "entities": []}, {"text": "Negative Results", "entities": []}, {"text": "We experiment with aug3The arbitrary baseline is higher than the majority class in the training data in any of the 3 cases.724", "entities": []}, {"text": "PAC GRNNs Emotion f - score prec rec f - score admiration 79.86 94.53 95.28 94.91 amazement 46.69 90.44 89.02 89.73 ecstasy 53.53 83.49 90.01 86.62 grief 48.10 85.07 81.13 83.05 loathing 2.99 83.87 54.17 65.82 rage 17.04 80.00 75.11 77.48 terror 47.00 91.15 84.01 87.44 vigilance 8.42 71.93 70.69 71.30 plutchik-1 64.86 91.26 91.24 91.21 anger 29.26 74.95 69.20 71.96 anticipation 26.70 70.05 69.00 69.52 disgust 36.60 82.18 68.84 74.92 fear 33.59 73.74", "entities": []}, {"text": "72.51 73.12 joy 75.50 90.96 93.88 92.40 sadness 49.21 73.20 82.04 77.37 surprise 19.54 85.60 67.40 75.42 trust 47.51 82.43 76.83 79.53 plutchik-2 53.30 82.53 82.46 82.32 acceptance 13.04 77.10 71.76 74.33 annoyance 81.47 91.46 95.01 93.20 apprehension 10.66 80.40 61.07 69.41 boredom 57.84 85.95 84.40 85.16 distraction 0.00 87.50 25.00 38.89 interest 44.14 86.79 78.38 82.37 pensiveness 12.27 91.87 43.24 58.80 serenity 38.59 82.15 78.16 80.11 plutchik-3 68.14 88.94 89.08 88.89 ALL 62.10 87.58 87.59 87.47 Table 4 : Results with GRNNs across Plutchik \u2019s 24 emotion categories .", "entities": [[87, 88, "DatasetName", "emotion"]]}, {"text": "We compare to bestperforming traditional classi\ufb01er ( i.e. Passive Aggressive ) .", "entities": []}, {"text": "menting training data reported here in two ways : 1 ) For each emotion type , we concatenate the training data with training data of tweets that are more ( or less ) intense from the same sector / dimension in the wheel , and 2 ) for each emotion type , we add tweets where emotion hashtags occur in the last quarter of a tweet ( which were originally \ufb01ltered out from TRAIN ) .", "entities": [[13, 14, "DatasetName", "emotion"], [49, 50, "DatasetName", "emotion"], [56, 57, "DatasetName", "emotion"]]}, {"text": "However , we gain no improvements based on either of these methods , thus re\ufb02ecting the importance of using high - quality training data and the utility of our strict pipeline .", "entities": []}, {"text": "6.2 Predicting 8 Primary Dimensions", "entities": []}, {"text": "We now investigate the task of predicting each of the 8 primary emotion dimensions represented by the sectors of the wheel ( where the three degrees of intensity of a given emotion are reduced to a single emotion dimension [ e.g. , { ecstasy , joy , serenity}are reduced to the joydimension ] ) .", "entities": [[12, 13, "DatasetName", "emotion"], [31, 32, "DatasetName", "emotion"], [37, 38, "DatasetName", "emotion"]]}, {"text": "We concatenate the 80 % training data ( TRAIN ) from each of the 3 circles \u2019 data into a single training setDimension prec rec f - score anger 97.40 97.72 97.56 anticipation 91.18 89.95 90.56 disgust 96.20 93.94 95.06 fear 94.97 94.38 94.68 joy 94.61 96.40 95.50 sadness 95.52 95.25 95.39 surprise 94.99 91.62 93.27 trust 96.36 97.58 96.96 All 95.68 95.68 95.68 Table 5 : GRNNs results across 8 emotion dimensions .", "entities": [[71, 72, "DatasetName", "emotion"]]}, {"text": "Each dimension represents three different emotions .", "entities": []}, {"text": "For example , the joydimension represents serenity , joy andecstasy .", "entities": []}, {"text": "Emotion Volkova ( 2016 ) model This work anger 12.38 74.95 disgust 5.71 82.18 fear 11.18 73.74 joy 44.57 90.96 sadness 18.04 73.20 surprise 5.33 85.60 ALL 26.95 80.12 Table 7 : Comparison ( in acc ) to ( V olkova and Bachrach , 2016 ) \u2019s model .", "entities": [[35, 36, "MetricName", "acc"]]}, {"text": "( TRAIN - ALL ) , the 10 % DEV to form DEV - ALL , and the 10 % TEST to form TEST - ALL .", "entities": []}, {"text": "We test a number of hyper - parameters on DEV and \ufb01nd the ones we have identi\ufb01ed on the \ufb01ne - grained prediction to work best and so we adopt them as is with the exception of limiting to only 2 epochs .", "entities": []}, {"text": "We believe that with a wider exploration of hyperparameters , improvements could be possible .", "entities": []}, {"text": "As Table 5 shows , we are able to model the 8 dimensions with an overall superior accuracy of 95.68 % .", "entities": [[17, 18, "MetricName", "accuracy"]]}, {"text": "As far as we know , this is the \ufb01rst work on modeling these dimensions .", "entities": []}, {"text": "7 Comparisons to Other Systems We compare our results on the 8 basic emotions to the published literature .", "entities": []}, {"text": "As Table 6 shows , on this subset of emotions , our system is 4.53 % ( acc ) higher than the best published results ( V olkova and Bachrach , 2016 ) , facilitated by the fact that we have an order of magnitude more training data .", "entities": [[17, 18, "MetricName", "acc"]]}, {"text": "As shown in Table 7 , we also apply ( V olkova and Bachrach , 2016 ) \u2019s pre - trained model on our test set of the 6 emotions they predict ( which belong toplutchik-2 ) , and acquire an overall accuracy of 26.95 % , which is signi\ufb01cantly lower than our accuracy.725", "entities": [[41, 43, "MetricName", "overall accuracy"]]}, {"text": "8 Conclusion In this paper , we built a large , automatically curated dataset for emotion detection using distant supervision and then used GRNNs to model \ufb01negrained emotion , achieving a new state - of - the - art performance .", "entities": [[15, 16, "DatasetName", "emotion"], [27, 28, "DatasetName", "emotion"]]}, {"text": "We also extended the classi\ufb01cation to 8 primary emotion dimensions situated in psychological theory of emotion .", "entities": [[8, 9, "DatasetName", "emotion"], [15, 16, "DatasetName", "emotion"]]}, {"text": "References Conor M. Steckler Aaron C. Weidman and Jessica L. Tracy . 2017 .", "entities": []}, {"text": "The jingle and jangle of emotion assessment : Imprecise measurement , casual scale usage , and conceptual fuzziness in emotion research .", "entities": [[5, 6, "DatasetName", "emotion"], [19, 20, "DatasetName", "emotion"]]}, {"text": "Emotion .", "entities": []}, {"text": "Saima Aman and Stan Szpakowicz .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Identifying expressions of emotion in text .", "entities": [[3, 4, "DatasetName", "emotion"]]}, {"text": "In Text , Speech and Dialogue .", "entities": []}, {"text": "Springer , pages 196\u2013205 .", "entities": []}, {"text": "Yoshua Bengio , R \u00b4 ejean Ducharme , Pascal Vincent , and Christian Jauvin .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "A neural probabilistic language model .", "entities": [[1, 5, "MethodName", "neural probabilistic language model"]]}, {"text": "Journal of machine learning research 3(Feb):1137\u20131155 .", "entities": []}, {"text": "Yoshua Bengio , Patrice Simard , and Paolo Frasconi . 1994 .", "entities": []}, {"text": "Learning long - term dependencies with gradient descent is dif\ufb01cult .", "entities": []}, {"text": "IEEE transactions on neural networks 5(2):157\u2013166 .", "entities": []}, {"text": "Phil Blunsom , Edward Grefenstette , and Nal Kalchbrenner .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A convolutional neural network for modelling sentences .", "entities": []}, {"text": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics .", "entities": []}, {"text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics .", "entities": []}, {"text": "James Bradbury , Stephen Merity , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Quasi - recurrent neural networks .", "entities": []}, {"text": "arXiv preprint arXiv:1611.01576 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kyunghyun Cho , Bart Van Merri \u00a8enboer , Caglar Gulcehre , Dzmitry Bahdanau , Fethi Bougares , Holger Schwenk , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Learning phrase representations using rnn encoder - decoder for statistical machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1406.1078 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Junyoung Chung , Caglar G \u00a8ulc \u00b8ehre , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Gated feedback recurrent neural networks .", "entities": []}, {"text": "In ICML .", "entities": []}, {"text": "pages 2067\u20132075 .", "entities": []}, {"text": "Andrew Cotter , Ohad Shamir , Nati Srebro , and Karthik Sridharan . 2011 .", "entities": []}, {"text": "Better mini - batch algorithms via accelerated gradient methods .", "entities": []}, {"text": "In Advances in neural information processing systems .", "entities": []}, {"text": "pages 1647\u20131655 .", "entities": []}, {"text": "Munmun De Choudhury , Scott Counts , and Michael Gamon .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Not all moods are created equal !", "entities": []}, {"text": "exploring human emotional states in social media .", "entities": []}, {"text": "P. Ekman .", "entities": []}, {"text": "1972 .", "entities": []}, {"text": "Universal and cultural differences in facial expression of emotion .", "entities": [[8, 9, "DatasetName", "emotion"]]}, {"text": "Nebraska Symposium on Motivation pages 207\u2013283 .", "entities": []}, {"text": "Alec Go , Richa Bhayani , and Lei Huang .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Twitter sentiment classi\ufb01cation using distant supervision .", "entities": []}, {"text": "CS224N Project Report , Stanford 1(12 ) .", "entities": []}, {"text": "Yoav Goldberg .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A primer on neural network models for natural language processing .", "entities": []}, {"text": "Journal of Arti\ufb01cial Intelligence Research 57:345\u2013420 .", "entities": []}, {"text": "Roberto Gonz \u00b4 alez - Ib \u00b4 anez , Smaranda Muresan , and Nina Wacholder .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Identifying sarcasm in twitter : a closer look .", "entities": []}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies : Short Papers - Volume 2 .", "entities": []}, {"text": "Association for Computational Linguistics , pages 581\u2013586 .", "entities": []}, {"text": "Ian Goodfellow , Yoshua Bengio , and Aaron Courville .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Deep learning .", "entities": []}, {"text": "MIT Press .", "entities": []}, {"text": "Alex Graves . 2012 .", "entities": []}, {"text": "Supervised sequence labelling .", "entities": []}, {"text": "In Supervised Sequence Labelling with Recurrent Neural Networks , Springer , pages 5\u201313 .", "entities": []}, {"text": "Alex Graves , Abdel - rahman Mohamed , and Geoffrey Hinton .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Speech recognition with deep recurrent neural networks .", "entities": [[0, 2, "TaskName", "Speech recognition"]]}, {"text": "In Acoustics , speech and signal processing ( icassp ) , 2013 ieee international conference on .", "entities": []}, {"text": "IEEE , pages 6645\u20136649 .", "entities": []}, {"text": "Alex Graves and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Of\ufb02ine handwriting recognition with multidimensional recurrent neural networks .", "entities": [[1, 3, "TaskName", "handwriting recognition"]]}, {"text": "In Advances in neural information processing systems .", "entities": []}, {"text": "pages 545\u2013552 .", "entities": []}, {"text": "Geoffrey E Hinton , Nitish Srivastava , Alex Krizhevsky , Ilya Sutskever , and Ruslan R Salakhutdinov .", "entities": [[14, 15, "DatasetName", "Ruslan"]]}, {"text": "2012 .", "entities": []}, {"text": "Improving neural networks by preventing coadaptation of feature detectors .", "entities": []}, {"text": "arXiv preprint arXiv:1207.0580 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sepp Hochreiter and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural computation 9(8):1735\u20131780 .", "entities": []}, {"text": "Hyisung C Hwang and David Matsumoto .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Emotional expression .", "entities": []}, {"text": "The Expression of Emotion : Philosophical , Psychological and Legal Perspectives page 137 .", "entities": []}, {"text": "Ozan Irsoy and Claire Cardie .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Deep recursive neural networks for compositionality in language .", "entities": []}, {"text": "InAdvances in Neural Information Processing Systems .", "entities": []}, {"text": "pages 2096\u20132104 .", "entities": []}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1408.5882 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Diederik Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .726", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Igor Labutov and Hod Lipson .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Re - embedding words .", "entities": []}, {"text": "In ACL ( 2 ) .", "entities": []}, {"text": "pages 489\u2013493 .", "entities": []}, {"text": "Quoc V Le and Tomas Mikolov .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Distributed representations of sentences and documents .", "entities": []}, {"text": "In ICML .", "entities": []}, {"text": "volume 14 , pages 1188\u20131196 .", "entities": []}, {"text": "Yann LeCun , Yoshua Bengio , and Geoffrey Hinton . 2015 .", "entities": []}, {"text": "Deep learning .", "entities": []}, {"text": "Nature 521(7553):436\u2013444 .", "entities": []}, {"text": "Jiwei Li , Minh - Thang Luong , Dan Jurafsky , and Eudard Hovy . 2015 .", "entities": []}, {"text": "When are tree structures necessary for deep learning of representations ?", "entities": []}, {"text": "arXiv preprint arXiv:1503.00185 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Pengfei Liu , Xipeng Qiu , Xinchi Chen , Shiyu Wu , and Xuanjing Huang . 2015 .", "entities": []}, {"text": "Multi - timescale long shortterm memory neural network for modelling sentences and documents .", "entities": []}, {"text": "In EMNLP .", "entities": []}, {"text": "Citeseer , pages 2326\u20132335 .", "entities": [[0, 1, "DatasetName", "Citeseer"]]}, {"text": "Marco Lui and Timothy Baldwin .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "langid .", "entities": []}, {"text": "py : An off - the - shelf language identi\ufb01cation tool .", "entities": []}, {"text": "In Proceedings of the ACL 2012 system demonstrations .", "entities": []}, {"text": "Association for Computational Linguistics , pages 25\u201330 .", "entities": []}, {"text": "Andrew L Maas , Raymond E Daly , Peter T Pham , Dan Huang , Andrew Y Ng , and Christopher Potts . 2011 .", "entities": []}, {"text": "Learning word vectors for sentiment analysis .", "entities": [[4, 6, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies - Volume 1 .", "entities": []}, {"text": "Association for Computational Linguistics , pages 142\u2013150 .", "entities": []}, {"text": "Tomas Mikolov , Wen - tau Yih , and Geoffrey Zweig .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Linguistic regularities in continuous space word representations .", "entities": []}, {"text": "In Hlt - naacl .", "entities": []}, {"text": "volume 13 , pages 746\u2013751 .", "entities": []}, {"text": "Mike Mintz , Steven Bills , Rion Snow , and Dan Jurafsky . 2009 .", "entities": []}, {"text": "Distant supervision for relation extraction without labeled data .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP : Volume 2 - Volume 2 .", "entities": []}, {"text": "Association for Computational Linguistics , pages 1003\u20131011 .", "entities": []}, {"text": "Gilad Mishne and Maarten De Rijke .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Capturing global mood levels using blog posts .", "entities": []}, {"text": "In AAAI spring symposium : computational approaches to analyzing weblogs .", "entities": []}, {"text": "pages 145\u2013152 .", "entities": []}, {"text": "Jeff Mitchell and Mirella Lapata .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Composition in distributional models of semantics .", "entities": []}, {"text": "Cognitive science 34(8):1388\u20131429 .", "entities": []}, {"text": "Saif M Mohammad .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "# emotional tweets .", "entities": []}, {"text": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1 : Proceedings of the main conference and the shared task , and Volume 2 : Proceedings of the Sixth International Workshop on Semantic Evaluation .", "entities": []}, {"text": "Association for Computational Linguistics , pages 246\u2013255.Saif M Mohammad and Svetlana Kiritchenko .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Using hashtags to capture \ufb01ne emotion categories from tweets .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "Computational Intelligence 31(2):301\u2013326 .", "entities": []}, {"text": "Thin Nguyen .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Mood patterns and affective lexicon access in weblogs .", "entities": []}, {"text": "In Proceedings of the ACL 2010 Student Research Workshop . Association for Computational Linguistics , pages 43\u201348 .", "entities": []}, {"text": "Alexander Pak and Patrick Paroubek .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Twitter as a corpus for sentiment analysis and opinion mining .", "entities": [[5, 7, "TaskName", "sentiment analysis"], [8, 10, "TaskName", "opinion mining"]]}, {"text": "InLREc .", "entities": []}, {"text": "volume 10 .", "entities": []}, {"text": "Razvan Pascanu , Tomas Mikolov , and Yoshua Bengio .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "On the dif\ufb01culty of training recurrent neural networks .", "entities": []}, {"text": "ICML ( 3 ) 28:1310\u20131318 .", "entities": []}, {"text": "Robert Plutchik .", "entities": []}, {"text": "1980 .", "entities": []}, {"text": "Emotion : A psychoevolutionary synthesis .", "entities": []}, {"text": "Harpercollins College Division .", "entities": []}, {"text": "Robert Plutchik .", "entities": []}, {"text": "1985 .", "entities": []}, {"text": "On emotion : The chickenand - egg problem revisited .", "entities": [[1, 2, "DatasetName", "emotion"]]}, {"text": "Motivation and Emotion 9(2):197\u2013200 .", "entities": []}, {"text": "Robert Plutchik . 1994 .", "entities": []}, {"text": "The psychology and biology of emotion . .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "HarperCollins College Publishers .", "entities": []}, {"text": "Robert Plutchik .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "The nature of emotions human emotions have deep evolutionary roots , a fact that may explain their complexity and provide tools for clinical practice .", "entities": []}, {"text": "American scientist 89(4):344\u2013350 .", "entities": []}, {"text": "Matthew Purver and Stuart Battersby .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Experimenting with distant supervision for emotion classi\ufb01cation .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics . Association for Computational Linguistics , pages 482\u2013491 .", "entities": []}, {"text": "Jonathon Read .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Using emoticons to reduce dependency in machine learning techniques for sentiment classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the ACL student research workshop .", "entities": []}, {"text": "Association for Computational Linguistics , pages 43\u201348 .", "entities": []}, {"text": "Yafeng Ren , Yue Zhang , Meishan Zhang , and Donghong Ji . 2016 .", "entities": []}, {"text": "Context - sensitive twitter sentiment classi\ufb01cation using neural network .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "pages 215\u2013221 .", "entities": []}, {"text": "Richard Socher , Alex Perelygin , Jean Y Wu , Jason Chuang , Christopher D Manning , Andrew Y Ng , Christopher Potts , et al . 2013 .", "entities": []}, {"text": "Recursive deep models for semantic compositionality over a sentiment treebank .", "entities": []}, {"text": "In Proceedings of the conference on empirical methods in natural language processing ( EMNLP ) .", "entities": []}, {"text": "Citeseer , volume 1631 , page 1642 .", "entities": [[0, 1, "DatasetName", "Citeseer"]]}, {"text": "Carlo Strapparava and Rada Mihalcea .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Semeval2007 task 14 : Affective text .", "entities": [[4, 6, "DatasetName", "Affective text"]]}, {"text": "In Proceedings of the 4th International Workshop on Semantic Evaluations .", "entities": []}, {"text": "Association for Computational Linguistics , pages 70\u201374.727", "entities": []}, {"text": "Kai Sheng Tai , Richard Socher , and Christopher D Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Improved semantic representations from tree - structured long short - term memory networks .", "entities": [[7, 12, "MethodName", "long short - term memory"]]}, {"text": "arXiv preprint arXiv:1503.00075 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yuki Tanaka , Hiroya Takamura , and Manabu Okumura . 2005 .", "entities": []}, {"text": "Extraction and classi\ufb01cation of facemarks .", "entities": []}, {"text": "In Proceedings of the 10th international conference on Intelligent user interfaces . ACM , pages 28\u201334 .", "entities": [[12, 13, "DatasetName", "ACM"]]}, {"text": "Duyu Tang , Bing Qin , and Ting Liu . 2015 .", "entities": []}, {"text": "Document modeling with gated recurrent neural network for sentiment classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "pages 1422\u20131432 .", "entities": []}, {"text": "Duyu Tang , Furu Wei , Bing Qin , Ming Zhou , and Ting Liu . 2014a .", "entities": []}, {"text": "Building large - scale twitter - speci\ufb01c sentiment lexicon : A representation learning approach .", "entities": [[11, 13, "TaskName", "representation learning"]]}, {"text": "In COLING .", "entities": []}, {"text": "pages 172\u2013182 .", "entities": []}, {"text": "Duyu Tang , Furu Wei , Nan Yang , Ming Zhou , Ting Liu , and Bing Qin . 2014b .", "entities": []}, {"text": "Learning sentimentspeci\ufb01c word embedding for twitter sentiment classi\ufb01cation .", "entities": []}, {"text": "In ACL ( 1 ) .", "entities": []}, {"text": "pages 1555\u20131565 .", "entities": []}, {"text": "Theano Development Team .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Theano :", "entities": []}, {"text": "A Python framework for fast computation of mathematical expressions .", "entities": []}, {"text": "arXiv e - prints abs/1605.02688 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "http://arxiv.org/abs/1605.02688 .", "entities": []}, {"text": "Svitlana V olkova and Yoram Bachrach .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Inferring perceived demographics from user emotional tone and user - environment emotional contrast .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , ACL .", "entities": []}, {"text": "Wenbo Wang , Lu Chen , Krishnaprasad Thirunarayan , and Amit P Sheth .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Harnessing twitter \u201d big data \u201d for automatic emotion identi\ufb01cation .", "entities": [[8, 9, "DatasetName", "emotion"]]}, {"text": "In Privacy , Security , Risk and Trust ( PASSAT ) , 2012 International Conference on and 2012 International Confernece on Social Computing ( SocialCom ) .", "entities": []}, {"text": "IEEE , pages 587\u2013592 .", "entities": []}, {"text": "Jasy Liew Suet Yan and Howard R Turtle .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "Exploring \ufb01ne - grained emotion detection in tweets .", "entities": [[4, 5, "DatasetName", "emotion"]]}, {"text": "In Proceedings of NAACL - HLT .", "entities": []}, {"text": "pages 73\u201380 .", "entities": []}, {"text": "Jasy Liew Suet Yan and Howard R Turtle .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "Exposing a set of \ufb01ne - grained emotion categories from tweets .", "entities": [[7, 8, "DatasetName", "emotion"]]}, {"text": "In 25th International Joint Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "page 8 .", "entities": []}, {"text": "Jasy Liew Suet Yan , Howard R Turtle , and Elizabeth D Liddy .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Emotweet-28 : A \ufb01ne - grained emotion corpus for sentiment analysis .", "entities": [[6, 7, "DatasetName", "emotion"], [9, 11, "TaskName", "sentiment analysis"]]}, {"text": "Changhua Yang , Kevin Hsin - Yih Lin , and Hsin - Hsi Chen .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Emotion classi\ufb01cation using web blog corpora .", "entities": []}, {"text": "In Web Intelligence , IEEE / WIC / ACM International Conference on .", "entities": [[6, 7, "DatasetName", "WIC"], [8, 9, "DatasetName", "ACM"]]}, {"text": "IEEE , pages 275\u2013278 .", "entities": []}, {"text": "Meishan Zhang , Yue Zhang , and Duy - Tin V o. 2016 .", "entities": []}, {"text": "Gated neural networks for targeted sentiment analysis .", "entities": [[5, 7, "TaskName", "sentiment analysis"]]}, {"text": "In AAAI .", "entities": []}, {"text": "pages 3087\u20133093.Xiang", "entities": []}, {"text": "Zhang , Junbo Zhao , and Yann LeCun . 2015 .", "entities": []}, {"text": "Character - level convolutional networks for text classi\ufb01cation .", "entities": []}, {"text": "In Advances in neural information processing systems .", "entities": []}, {"text": "pages 649\u2013657.728", "entities": []}]