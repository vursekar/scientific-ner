[{"text": "Domain Adaptation for Sentiment Analysis using Keywords in the Target Domain as the Learning Weight \u0279Jing Bai \u0279\u0279 Hiroyuki Shinnou \u0279\u0279 Kanako Komiya Ibaraki University , Department of Computer and Information Sciences 4 - 12 - 1 Nakanarusawa , Hitachi , Ibaraki JAPAN 316 - 8511 f17nm719x , hiroyuki.shinnou.0828 , kanako.komiya.nlp g @vc.ibaraki.ac.jp", "entities": [[0, 2, "TaskName", "Domain Adaptation"], [3, 5, "TaskName", "Sentiment Analysis"]]}, {"text": "Abstract This paper proposes a new method of instance - based domain adaptation for sentiment analysis .", "entities": [[11, 13, "TaskName", "domain adaptation"], [14, 16, "TaskName", "sentiment analysis"]]}, {"text": "First , our method de\ufb01nes the likelihood of keywords , through the value of inverse document frequency ( IDF ) , for each word in documents in the target domain .", "entities": []}, {"text": "Next , the keyword content rate of a document is calculated using the likelihood of keywords and the domain adaptation is performed by giving the keyword content rate to each document in the source domain as the weight .", "entities": [[18, 20, "TaskName", "domain adaptation"]]}, {"text": "The experiment used an Amazon dataset to demonstrate the effectiveness of our proposed method .", "entities": []}, {"text": "Although the instance - based method has not shown great ef\ufb01ciency , the advantages combining instance - based method and feature - based method are shown in this paper .", "entities": []}, {"text": "1 Introduction This paper proposes a new method of instance - based domain adaptation for sentiment analysis .", "entities": [[12, 14, "TaskName", "domain adaptation"], [15, 17, "TaskName", "sentiment analysis"]]}, {"text": "Sentiment analysis involves judging a polarity , positive or negative , of a review such as a movie review .", "entities": [[0, 2, "TaskName", "Sentiment analysis"]]}, {"text": "This is one of the document classi\ufb01cation tasks and supervised learning can be used to solve it .", "entities": []}, {"text": "However , if the domain of the test data is different from the domain for the learning data ( for example , book reviews ) , the accuracy of the classi\ufb01er obtained through standard supervised learning reduced .", "entities": [[27, 28, "MetricName", "accuracy"]]}, {"text": "This is the problem with domain shift .", "entities": []}, {"text": "The solution to this problem is domain adaptation .", "entities": [[6, 8, "TaskName", "domain adaptation"]]}, {"text": "Domain adaptation can be roughly divided into two categories : feature - based and instance - based ( Panand Yang , 2010 ) .", "entities": [[0, 2, "TaskName", "Domain adaptation"]]}, {"text": "In summary , both are weightedlearning methods , but feature - based gives weights to features and instance - based gives weight to instance .", "entities": []}, {"text": "Here , we present a new instance - based method .", "entities": []}, {"text": "Generally , the instance - based method assumes a covariate shift , and gives the weight based on the probability density ratio between target domain and source domain .", "entities": []}, {"text": "However , the computational cost for the instance - based method is too high .", "entities": []}, {"text": "The method presented here is simple and its effect is better than methods using a typical probability density ratio .", "entities": []}, {"text": "Our method \ufb01rst de\ufb01nes lw , the likelihood of the keyword of the word wusing the IDF in the target domain .", "entities": []}, {"text": "Using lw , the weight of a review xin the source domain is set as the keyword content rate wx .", "entities": []}, {"text": "After that , weighted - learning is performed by givingwxto each document in the source domain xto overcome the domain shift .", "entities": []}, {"text": "In the experiment , we used Amazon dataset ( Blitzer et al . , 2007 ) , and compared our proposed method with two typical instance - based methods : unconstrained least squares importance \ufb01tting ( uLSIF ) ( Yamada et al . , 2011 ) using the probability density ratio and the method de\ufb01ning weight through Naive Bayes model ( Shinnou and Sasaki , 2014 ) , to demonstrate the effectiveness of the proposed method .", "entities": []}, {"text": "2 Related Work Domain adaptation is roughly divided into two types : the supervised approach using labeled data in the target domain and the unsupervised approach that does not use them .", "entities": [[3, 5, "TaskName", "Domain adaptation"]]}, {"text": "For supervised approach , Daum \u00b4 e \u2019s method ( Daum \u00b4 e III , 2007 ) has become a PACLIC 32 37   32nd Pacific Asia Conference on Language , Information and Computation   Hong Kong , 1 - 3 December 2018", "entities": []}, {"text": "Copyright 2018 by the authors", "entities": []}, {"text": "standard method because of its simplicity and high ability .", "entities": []}, {"text": "The method in the current research is an unsupervised approach .", "entities": []}, {"text": "Unsupervised approaches can further be divided into two types : feature - based and instance - based ( Pan and Yang , 2010 ) .", "entities": []}, {"text": "They are both weighted learning methods ; feature - based methods give weights to features and instance - based methods give weights to instances .", "entities": []}, {"text": "Among featurebased methods , the most representative method is structural correspondence learning ( SCL ) ( Blitzer et al . , 2006 ) .", "entities": []}, {"text": "In addition , CORAL ( Sun et al . , 2016 ) has attracted much attention for its simplicity and high ability in recent years .", "entities": []}, {"text": "Moreover , the feature - based methods with deep learning ( Glorot et al . , 2011 ) , the expanded CORAL ( Sun and Saenko , 2016 ) and adversarial networks ( Ganin and Lempitsky , 2015)(Tzeng et al . , 2017 ) are also considered as the state of the art .", "entities": []}, {"text": "On the other hand , instance - based methods have not been studied as much as feature - based methods .", "entities": []}, {"text": "The instance - based method assumes a covariate shift .", "entities": []}, {"text": "A covariate shift assumes PS(cjx ) = PT(cjx ) andPS(x )", "entities": []}, {"text": "= PT(x ) .", "entities": []}, {"text": "Under a covariate shift , PT(cjx ) can be obtained by the weighted learning that uses the probability density ratio r= PT(x)=P", "entities": []}, {"text": "S(x)as the weight of the document of the source data", "entities": []}, {"text": "x.", "entities": []}, {"text": "There are a variety of methods for calculating the probability density ratio .", "entities": []}, {"text": "The simplest way to calculate the ratio is directly estimate PS(x)andPT(x ) , but in the case of complex models , the problem will be more complicated .", "entities": []}, {"text": "Thus , the method that directly models the probability density ratio was studied .", "entities": []}, {"text": "Among these methods , uLSIF ( Yamada et al . , 2011 ) is widely used because the time complexity of the method is relatively small .", "entities": []}, {"text": "However , P(x)of bag - of - words can be modeled by Naive Bayes model if the problem is limited to natural language processing .", "entities": []}, {"text": "Therefore , ( Shinnou and Sasaki , 2014 ) de\ufb01ned Pr(x ) , the prior of x , as follows : PR(x )", "entities": []}, {"text": "= \u220fn i=1PR(fi ) , where xdenotes a data in the domain Randxhas a set of features , that is , x = ff1 ; f2;\u0001 \u0001 \u0001 ; fng .", "entities": []}, {"text": "They also obtain PR(fi ) using the following equation : PR(f )", "entities": []}, {"text": "= n(R;f ) +1 N(R)+2 .", "entities": []}, {"text": "Here , n(R;f)is the frequency of feature fin the domain R , and n(R)is the number of data in the domain R. Therefore , the probability density ratiois obtained as follows : r = PT(x ) PS(x)=n(T ; f ) + 1 N(T ) + 2\u0001N(S ) + 2 n(S ; f ) + 1(1 ) 3 Proposed Method 3.1 Likelihood of the Keyword in the Target Domain The likelihood of the keyword in the target domain islx , and lxis set as the value of IDF in the target domain of w : lx= log(N di ) + 1 Here Nis the number of articles in the article collection in the target domain , and diis the number of articles containing the word win the article collection in the target domain .", "entities": []}, {"text": "3.2 The Content of Keywords in the Source Case Set the weight wxof the instance xin", "entities": []}, {"text": "the source domain .", "entities": []}, {"text": "The words ( \ufb01le ) xisfwigK i=1 , and the frequency within xfor word wiisfi .", "entities": []}, {"text": "Using these , wx is given by the following equation : wx=1\u2211k i=1fiK\u2211 i=1fi\u0001lwi 4 Experiment The Amazon dataset ( Blitzer et al . , 2007 ) used in the experiment is speci\ufb01cally developed using the processed_acl.tar.gz \ufb01le on the following website .", "entities": []}, {"text": "https://www.cs.jhu.edu/ \u02dcmdredze/ datasets / sentiment/.", "entities": []}, {"text": "The data include books ( B ) , dvd ( D ) , electronics ( E ) , and kitchen ( K ) .", "entities": []}, {"text": "The number of \ufb01les contained in each domain is shown in Table 2 .", "entities": []}, {"text": "There are 1000 positive data and negative data in each domain , and these 2000 data are used as training data in this domain .", "entities": []}, {"text": "PACLIC 32 38   32nd Pacific Asia Conference on Language , Information and Computation   Hong Kong , 1 - 3 December 2018   Copyright 2018 by the authors", "entities": []}, {"text": "Table 1 : Experimental result IDEAL NONE uLSIF NB our method B\u02e0D 0.822 0.806 0.806 0.811 0.809 B\u02e0E 0.852 0.761 0.756 0.755 0.765 B\u02e0K 0.878 0.845 0.778 0.779 0.785 D\u02e0B 0.831 0.762 0.733 0.745 0.741 D\u02e0E 0.852 0.761 0.748 0.753 0.758 D\u02e0K 0.878 0.795 0.773 0.782 0.789 E\u02e0B 0.831 0.712 0.714 0.723 0.719 E\u02e0D 0.822 0.722 0.708 0.723 0.714", "entities": []}, {"text": "E\u02e0K 0.878 0.849 0.854 0.857 0.855 K\u02e0B 0.831 0.713 0.707 0.714 0.715 K\u02e0D 0.822 0.740 0.733 0.723 0.736 K\u02e0E 0.852 0.842 0.847 0.852 0.845 Average 0.846 0.776 0.763 0.768 0.769 Table 2 : The number of \ufb01les in each domain positive negativ etest data books 1,000 1,000 4,465 dvd 1,000 1,000 3,586 electronics 1,000 1,000 5,681 kitchen 1,000 1,000 5,945 The learning algorithm is an SVM with scikitlearn .", "entities": [[65, 66, "MethodName", "SVM"]]}, {"text": "The core is linear , the value of the c parameter is \ufb01xed at 0.1 , and the scikit - learn SVM supports Weighted - Learning1 , so the scikit - learn SVM is used here .", "entities": [[21, 22, "MethodName", "SVM"], [32, 33, "MethodName", "SVM"]]}, {"text": "domain adaptations are : B\u02e0D , B\u02e0E , B\u02e0K , D \u02e0B , D\u02e0E , D\u02e0K , E\u02e0B , E\u02e0D , E\u02e0K , K\u02e0B , K \u02e0D , K\u02e0E.", "entities": []}, {"text": "See Table 1 for the results of the two methods\u02a2uLSIF ( Yamada et al . , 2011 ) and using equation ( 1 ) of Naive Bayes\u02a3for determining the rate density ratio of each domain and the proposed method.\u02a0NONE\u02a1in Table 1 means that the domain adaptation method was not used but simply applies the classi\ufb01er formed from the training data of the source domain to the result of the test data in the target domain was applied .", "entities": [[44, 46, "TaskName", "domain adaptation"]]}, {"text": "In addition , IDEAL is a result using the training data in the target domain to learn through the classi\ufb01er and apply it to the test data in the target domain .", "entities": []}, {"text": "Using the case as a weighted method , a compari1http://scikit-learn.org/stable/auto _ examples / svm / plot_weighted_samples.htmlson of uLSIF , NB , the our method shows that the six highest correct answer rates in the 12 domain adaptations are obtained by our method , and the remaining six highest positive answer rates are obtained by NB .", "entities": [[13, 14, "MethodName", "svm"]]}, {"text": "When we take 12 averages , the solution rate of our method is more than that of NB , and our method is weighted with example and , which is excellent .", "entities": []}, {"text": "5 Discussion NONE in Table1 is compared with the case weighting method ( uLSIF , NB , and our method ) .", "entities": []}, {"text": "It is clear that NONE has a high positive solution rate .", "entities": []}, {"text": "For theae data only , the instance - based method has no effect on domain adaptation .", "entities": [[14, 16, "TaskName", "domain adaptation"]]}, {"text": "However , feature - based and instance - based methods are easy to combine .", "entities": []}, {"text": "Here , the four domains applied in paper ( Sun et al . , 2016 ) are adapted to B \u02e0E , D\u02e0B , E\u02e0KandK\u02e0D , and SCL conversion training is used \ufb01rst .", "entities": []}, {"text": "The prime vector of the data , then experiment with the weighted - learning in this transformed vector is performed using the proposed method .", "entities": []}, {"text": "The results are shown in Table 3 .", "entities": []}, {"text": "The CORAL in Table 3 is taken from ( Sun et al . , 2016 ) .", "entities": []}, {"text": "From Table 3,it can be seen that SCL has no effect .", "entities": []}, {"text": "After SCL is combined with our method , the accuracy is not high enough .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "However , when SCL is combined with the proposed method , the precision for SCL alone is improved .", "entities": []}, {"text": "The positive effect of PACLIC 32 39   32nd Pacific Asia Conference on Language , Information and Computation   Hong Kong , 1 - 3 December 2018   Copyright 2018 by the authors", "entities": []}, {"text": "Table 3 : Combination of feature - based method and instance - based method IDEAL NONE CORAL our methodSCL SCL + our method B\u02e0E 0.852 0.761 0.763 0.760 0.757 0.756 D\u02e0B 0.831 0.762 0.783 0.756 0.732 0.733 E\u02e0K 0.878 0.849 0.836 0.849 0.852 0.853 K\u02e0D 0.822 0.740 0.739 0.743 0.732 0.733 Average 0.846 0.778 0.780 0.777 0.768 0.769 \u061e\u061e\u061e\u061e\u061e\u061eAutoEncoder 1W2W", "entities": []}, {"text": "T Sx x / T", "entities": []}, {"text": "Sx x/ \u061e\u061e\u061e\u061e\u061e\u061e Sx\u061e\u061e\u061eNN'yw y y lossu ) , ' ( Learning Sxencoded yw label weig ht", "entities": []}, {"text": "Figure 1 : AE+NN+Weighted - Learning the combined the instance - based and feature - based methods can be con\ufb01rmed .", "entities": []}, {"text": "There are many ways in useing the feature - based method , in addition to SCL , so it can be improved by combining these techniques with the proposed method .", "entities": []}, {"text": "In addition , although the weighted - learning SVM is used in this paper , the loss value of the loss function in the neural network is multiplied by the weight , and it is easier to realize weighted - learning as the loss value .", "entities": [[8, 9, "MethodName", "SVM"], [16, 17, "MetricName", "loss"], [20, 21, "MetricName", "loss"], [43, 44, "MetricName", "loss"]]}, {"text": "There are many options for using the domain adaptation method on deep learning , and these solutions combined with instance - based are easier to compute .", "entities": [[7, 9, "TaskName", "domain adaptation"]]}, {"text": "In a simple example , we used the AutoEncoder ( AE ) as the feature - based method .", "entities": [[8, 9, "MethodName", "AutoEncoder"], [10, 11, "MethodName", "AE"]]}, {"text": "Using AE , the dimension of the data in the source and target domain was reduced , that is , encoded .", "entities": [[1, 2, "MethodName", "AE"]]}, {"text": "In learning and testing , we used the connected data of the orig - inal data and the encoded data instead of the original data .", "entities": []}, {"text": "In learning , as described above , the value of the loss function was multiplied by the weight obtained by our method and was taken as the loss value\u02a2FIG . 1\u02a3 .", "entities": [[11, 12, "MetricName", "loss"], [27, 28, "MetricName", "loss"]]}, {"text": "Only the experiment of B\u02e0E is performed , and the results in Table 4 \u0371FIG . 2 were obtained .", "entities": []}, {"text": "In addition , in this experiment , neural network learning was ended in 50 epochs , and the correct rate was the result from evaluating the model obtained by learning data after 50 epochs .", "entities": []}, {"text": "Moreover , the dimension was reduced to 200 .", "entities": []}, {"text": "Every method used the same multi - layer perceptron , which has three layers .", "entities": []}, {"text": "The use of the connected data of the original data and the encoded data is a feature - based method .", "entities": []}, {"text": "FIG .", "entities": []}, {"text": "2 shows that this feature - based method NN+AE improves the precision of the standard neural network NN .", "entities": []}, {"text": "Moreover , combining PACLIC 32 40   32nd Pacific Asia Conference on Language , Information and Computation   Hong Kong , 1 - 3 December 2018   Copyright 2018 by the authors", "entities": []}, {"text": "0.70.710.720.730 .", "entities": []}, {"text": "740.750.760.770.78 1 2 3 4 5 6 7 8 9 1011121314151617181920212223242526272829303132333435363738394041424344454647484950 epochprec isionNN+AE+WT    0.7697 NN+AE    0.7667 NN     0.7618Figure 2 : Weighted - Learning by neural network Table 4 : weighted - learning by neural network IDEAL NONE NN NN+AE NN+AE+WT 0.852 0.761 0.7618 0.7667 0.7697 NN+AE", "entities": []}, {"text": "and our method further improves it .", "entities": []}, {"text": "This result shows that the combination of the featurebased method and the instance - based method is easy in network learning and effective .", "entities": []}, {"text": "In the future , we are planning to design a domain adaptation method in this framework .", "entities": [[10, 12, "TaskName", "domain adaptation"]]}, {"text": "6 Conclusion This paper proposed a method for instance - based domain adaptation of sentiment analysis .", "entities": [[11, 13, "TaskName", "domain adaptation"], [14, 16, "TaskName", "sentiment analysis"]]}, {"text": "For outline , from the target domain , using IDF to set the likelihood of keywords , and the data in the source domain , the content rate in the target domain keyword , and the keyword content rate as the weight .", "entities": []}, {"text": "In the experiment , we compared our proposed method with two typical instance - based methods : uLSI using the probability density ratio and the method de\ufb01ning weight through Naive Bayes model .", "entities": []}, {"text": "However , using an instance - based alone to perform domain adaptation has a very small effect , the combining instance - based method and feature - based method is assured as shown in this paper .", "entities": [[10, 12, "TaskName", "domain adaptation"]]}, {"text": "Further , the combination is easy to implement in the neural network model .", "entities": []}, {"text": "Thus , we will investigate this approach in future .", "entities": []}, {"text": "References John Blitzer , Ryan McDonald , and Fernando Pereira . 2006 .", "entities": []}, {"text": "Domain adaptation with structural correspondence learning .", "entities": [[0, 2, "TaskName", "Domain adaptation"]]}, {"text": "In EMNLP-2006 , pages 120\u2013128 .", "entities": []}, {"text": "John Blitzer , Mark Dredze , and Fernando Pereira . 2007 .", "entities": []}, {"text": "Biographies , Bollywood , Boom - boxes and Blenders : Domain adaptation for Sentiment Classi\ufb01cation .", "entities": [[10, 12, "TaskName", "Domain adaptation"]]}, {"text": "In ACL-2007 , pages 440\u2013447 .", "entities": []}, {"text": "Hal Daum \u00b4 e III .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Frustratingly easy domain adaptation .", "entities": [[2, 4, "TaskName", "domain adaptation"]]}, {"text": "In ACL-2007 , pages 256\u2013263 .", "entities": []}, {"text": "Yaroslav Ganin and Victor S. Lempitsky .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Unsupervised domain adaptation by backpropagation .", "entities": [[0, 3, "TaskName", "Unsupervised domain adaptation"]]}, {"text": "In ICML , pages 1180\u20131189 .", "entities": []}, {"text": "Xavier Glorot , Antoine Bordes , and Yoshua Bengio .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Domain Adaptation for Large - Scale Sentiment Classi\ufb01cation : A Deep Learning Approach .", "entities": [[0, 2, "TaskName", "Domain Adaptation"]]}, {"text": "In ICML11 , pages 513\u2013520 .", "entities": []}, {"text": "PACLIC 32 41   32nd Pacific Asia Conference on Language , Information and Computation   Hong Kong , 1 - 3 December 2018   Copyright 2018 by the authors", "entities": []}, {"text": "Sinno Jialin Pan and Qiang Yang .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "A survey on transfer learning .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "Knowledge and Data Engineering , IEEE Transactions on , 22(10):1345\u20131359 .", "entities": []}, {"text": "Hiroyuki Shinnou and Minoru Sasaki .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Domain Adaptations for Word Sense Disambiguation under the Problem of Covariate Shift ( in Japanese ) .", "entities": [[3, 6, "TaskName", "Word Sense Disambiguation"]]}, {"text": "Journal of Natural Language Processing , 21(1):61\u201379 .", "entities": []}, {"text": "Baochen Sun and Kate Saenko .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Deep coral : Correlation alignment for deep domain adaptation .", "entities": [[7, 9, "TaskName", "domain adaptation"]]}, {"text": "In Computer Vision \u2013 ECCV 2016 Workshops , pages 443 \u2013 450 .", "entities": []}, {"text": "Baochen Sun , Jiashi Feng , and Kate Saenko .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Return of Frustratingly Easy Domain Adaptation .", "entities": [[0, 1, "MetricName", "Return"], [4, 6, "TaskName", "Domain Adaptation"]]}, {"text": "AAAI .", "entities": []}, {"text": "Eric Tzeng , Judy Hoffman , Kate Saenko , and Trevor Darrell . 2017 .", "entities": []}, {"text": "Adversarial discriminative domain adaptation .", "entities": [[2, 4, "TaskName", "domain adaptation"]]}, {"text": "arXiv preprint arXiv:1702.05464 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Makoto Yamada , Taiji Suzuki , Takafumi Kanamori , Hirotaka Hachiya , and Masashi Sugiyama .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Relative density - ratio estimation for robust distribution comparison .", "entities": []}, {"text": "Neural Computation , 25(5):1370\u20131370 .", "entities": []}, {"text": "PACLIC 32 42   32nd Pacific Asia Conference on Language , Information and Computation   Hong Kong , 1 - 3 December 2018   Copyright 2018 by the authors", "entities": []}]