[{"text": "Proceedings of the 6th Workshop on Representation Learning for NLP ( RepL4NLP-2021 ) , pages 90\u201399 Bangkok , Thailand ( Online ) , August 6 , 2021 .", "entities": [[6, 8, "TaskName", "Representation Learning"]]}, {"text": "\u00a9 2021 Association for Computational Linguistics90Revisiting Pretraining with Adapters Seungwon Kim , Alex Shum , Nathan Susanj , Jonathan Hilgart Georgia Institute of Technology fskim3222 , ashum7 , nsusanj3 , jhilgart3 g@gatech.edu", "entities": []}, {"text": "Abstract Pretrained language models have served as the backbone for many state - of - the - art NLP results .", "entities": [[1, 4, "TaskName", "Pretrained language models"]]}, {"text": "These models are large and expensive to train .", "entities": []}, {"text": "Recent work suggests that continued pretraining on task - speci\ufb01c data is worth the effort as pretraining leads to improved performance on downstream tasks .", "entities": []}, {"text": "We explore alternatives to full - scale task - speci\ufb01c pretraining of language models through the use of adapter modules , a parameter - ef\ufb01cient approach to transfer learning .", "entities": [[27, 29, "TaskName", "transfer learning"]]}, {"text": "We \ufb01nd that adapter - based pretraining is able to achieve comparable results to task - speci\ufb01c pretraining while using a fraction of the overall trainable parameters .", "entities": []}, {"text": "We further explore direct use of adapters without pretraining and \ufb01nd that the direct \ufb01netuning performs mostly on par with pretrained adapter models , contradicting previously proposed bene\ufb01ts of continual pretraining in full pretraining \ufb01ne - tuning strategies .", "entities": [[29, 31, "TaskName", "continual pretraining"]]}, {"text": "Lastly , we perform an ablation study on task - adaptive pretraining to investigate how different hyperparameter settings can change the effectiveness of the pretraining .", "entities": []}, {"text": "1 Introduction Pretrained Language Models ( PLM ) are predominant in tackling current Natural Language Processing ( NLP ) tasks .", "entities": [[2, 5, "TaskName", "Pretrained Language Models"]]}, {"text": "Most PLMs based on the Transformer architecture ( Vaswani et al . , 2017 ) are \ufb01rst trained on massive text corpora with the selfsupervised objective to learn word representations ( Devlin et al . , 2019 ; Liu et al . , 2019 ) , and then are \ufb01ne - tuned for a speci\ufb01c target task .", "entities": [[5, 6, "MethodName", "Transformer"]]}, {"text": "The pretraining and \ufb01ne - tuning of PLMs achieves state - ofthe - art ( SOTA ) performance in many NLP tasks .", "entities": []}, {"text": "Inspired by the bene\ufb01ts of pretraining , there have been studies demonstrate the effects of continued pretraining on the domain of a target task or the target task dataset ( Mitra et al . , 2020 ; Han and Eisenstein , 2019 ; Gururangan et al . , 2020 ) .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "Gururangan et", "entities": []}, {"text": "al . , 2020 adapt PLMs on the target taskby further pretraining RoBERTa ( Liu et al . , 2019 ) on the target text corpus before it is \ufb01ne - tuned for the corresponding task and showed that this task adaptation consistently improves the performance for text classi\ufb01cation tasks .", "entities": [[12, 13, "MethodName", "RoBERTa"]]}, {"text": "However , this full process of pretraining and then \ufb01ne - tuning can be parameter inef\ufb01cient for recent PLMs that have millions or billions of parameters ( Devlin et al . , 2019 ; Radford et al . , 2018 ) .", "entities": []}, {"text": "This parameter inef\ufb01ciency becomes even worse when one continues pre - training all the parameters of PLMs on the task - speci\ufb01c corpus .", "entities": []}, {"text": "Furthermore , recent PLMs need more than 100s of MB to store all the weights ( Liu et al . , 2019 ; Radford et al . , 2018 ) , making it dif\ufb01cult to download and share the pre - trained models on the \ufb02y .", "entities": []}, {"text": "Recently , adapters have been proposed as an alternative approach to decrease the substantial number of parameters of PLMs in the \ufb01ne - tuning stage ( Houlsby et al . , 2019 ) .", "entities": [[14, 17, "HyperparameterName", "number of parameters"]]}, {"text": "Finetuning with adapters mostly matches the performance of those with the full \ufb01ne - tuning strategy on many NLP tasks including GLUE benchmark ( Wang et al . , 2018 ) and reduces the size of the model from 100s of MB to the order of MB ( Pfeiffer et al . , 2020b ) .", "entities": [[21, 22, "DatasetName", "GLUE"]]}, {"text": "As such , a natural question arises from the successes of the adapter approach : can the adapter alone adapt PLMs to the target task when it is used in the second phase of the pretraining stage and thus lead to the improvement of the performance on the corresponding task ?", "entities": []}, {"text": "In this paper , we explore task - adaptive pretraining , termed TAPT ( Gururangan et al . , 2020 ) , with adapters to address this question and overcome the limitations of the conventional full pretraining and \ufb01ne - tuning .", "entities": []}, {"text": "We only train the adapter modules in the second phase of pretraining as well as the \ufb01ne - tuning stage to achieve both parameter ef\ufb01ciency and the bene\ufb01ts of continual pretraining and compare those with the adapter - based model without pretraining .", "entities": [[29, 31, "TaskName", "continual pretraining"]]}, {"text": "Surprisingly , we \ufb01nd that directly", "entities": []}, {"text": "91\ufb01ne - tuning adapters performs mostly on par with the pre - trained adapter model and outperforms the full TAPT , contradicting the previously proposed bene\ufb01ts of continual pretraining in the full pretraining \ufb01ne - tuning scheme .", "entities": [[27, 29, "TaskName", "continual pretraining"]]}, {"text": "As directly \ufb01ne - tuning adapters skips the second phase of pretraining and the training steps of adapters are faster than those of the full model , it substantially reduces the training time .", "entities": []}, {"text": "We further investigate different hyperparameter settings that affect the effectiveness of pretraining . 2 Pretraining and Adapters Pre - trained language model We use RoBERTa ( Liu et al . , 2019 ) , a Transformer - based language model that is pre - trained on a massive text corpus , following Gururangan et al . , 2020 .", "entities": [[24, 25, "MethodName", "RoBERTa"], [35, 36, "MethodName", "Transformer"]]}, {"text": "RoBERTa is an extension of BERT ( Devlin et", "entities": [[0, 1, "MethodName", "RoBERTa"], [5, 6, "MethodName", "BERT"]]}, {"text": "al . , 2019 ) with optimized hyperparameters and a modi\ufb01cation of the pretraining objective , which excludes next sentence prediction and only uses the randomly masked tokens in the input sentence .", "entities": []}, {"text": "To evaluate the performance of RoBERTa on a certain task , a classi\ufb01cation layer is appended on top of the language model after the pretraining and all the parameters in RoBERTa are trained in a supervised way using the label of the dataset .", "entities": [[5, 6, "MethodName", "RoBERTa"], [30, 31, "MethodName", "RoBERTa"]]}, {"text": "In this paper , training word representations using RoBERTa on a masked language modeling task will be referred to as pretraining .", "entities": [[8, 9, "MethodName", "RoBERTa"], [11, 14, "TaskName", "masked language modeling"]]}, {"text": "Further , taking this pretrained model and adding a classi\ufb01cation layer with additional updates to the language model parameters will be referred to as \ufb01ne - tuning .", "entities": []}, {"text": "Task - adaptive pretraining ( TAPT )", "entities": []}, {"text": "Although RoBERTa achieves strong performance by simply \ufb01ne - tuning the PLMs on a target task , there can be a distributional mismatch between the pretraining and target corpora .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}, {"text": "To address this issue , pretraining on the target task or the domain of the target task can be usefully employed to adapt the language models to the target task and it further improves the performance of the PLMs .", "entities": []}, {"text": "Such methods can be referred to as Domain - Adaptive Pretraining ( DAPT ) or Task Adaptive - Pretraining ( TAPT ) ( Gururangan et al . , 2020 ) .", "entities": []}, {"text": "In this paper , we limit the scope of our works to TAPT as domain text corpus is not always available for each task , whereas TAPT can be easily applied by directly using the dataset of the target task while its performance often matches with DAPT ( Gururangan et al . , 2020 ) .", "entities": []}, {"text": "In TAPT , the second phase of pretraining is perFigure 1 : The adapter achitecture in the Transformer layer ( Pfeiffer et al . , 2020a ) formed with RoBERTa using the unlabeled text corpus of the target task , and then it is \ufb01ne - tuned on the target task .", "entities": [[17, 18, "MethodName", "Transformer"], [29, 30, "MethodName", "RoBERTa"]]}, {"text": "Adapter Adapter modules have been employed as a feature extractor in computer vision ( Rebuf\ufb01 et al . , 2017 ) and have been recently adopted in the NLP literature as an alternative approach to fully \ufb01ne - tuning PLMs .", "entities": [[0, 1, "MethodName", "Adapter"], [1, 2, "MethodName", "Adapter"]]}, {"text": "Adapters are sets of new weights that are typically embedded in each transformer layer of PLMs and consist of feed - forward layers with normalizations , residual connections , and projection layers .", "entities": []}, {"text": "The architectures of adapters vary with respect to the different con\ufb01guration settings .", "entities": []}, {"text": "We use the con\ufb01guration proposed by Pfeiffer et al . , 2020a in Figure 1 , which turned out to be effective on diverse NLP tasks , and add the adapter layer to each transformer layer .", "entities": []}, {"text": "Pfeiffer et al . , 2020c use two types of adapter : language - speci\ufb01c adapters and taskspeci\ufb01c adapters for cross - lingual transfer .", "entities": [[20, 24, "TaskName", "cross - lingual transfer"]]}, {"text": "These two types of adapter modules have similar architecture as in Figure 1 .", "entities": []}, {"text": "However , the language adapters involve invertible adapters after the embedding layer to capture token - level language representation when those are trained via masked language modeling in the pretraining stage , whereas the task adapters are simply embedded in each transformer layer and trained in the \ufb01ne - tuning stage to learn the task representation .", "entities": [[24, 27, "TaskName", "masked language modeling"]]}, {"text": "Following Pfeiffer et al . , 2020c , we employ language adapter modules with invertible adapter layers to perform pretraining adapters on the unlabeled target dataset .", "entities": []}, {"text": "However , we perform \ufb01ne - tuning pre - trained parameters of the language adapter modules for evaluation to align with", "entities": []}, {"text": "92Domain Task Label type Number of inst ( Train / Dev / Test )", "entities": []}, {"text": "Classes", "entities": []}, {"text": "Biomedical CHEMPROT Relationship classi\ufb01cation 4169 / 2427 / 3469 13 Biomedical RCT Abstract sentence roles 18040 / 30212 / 30135 5 Computer Science ACL - ARC Citation intent 1688 / 114 / 139 6 Computer Science SCIERC Relation classi\ufb01cation 3219 / 455 / 974 7 News HYPERPARTISAN Partisanship 515 / 65 / 65 2 News AGNEWS Topic 115000", "entities": [[1, 2, "DatasetName", "CHEMPROT"], [23, 26, "DatasetName", "ACL - ARC"], [36, 37, "DatasetName", "SCIERC"]]}, {"text": "/ 5000 / 7600 4 Reviews HELPFULNESS Review helpfulness 115251 / 5000 / 25000 2 Reviews IMDB Review sentiment 20000 / 5000 / 25000 2 Table 1 : Datasets used for experimentation .", "entities": [[16, 17, "DatasetName", "IMDB"]]}, {"text": "Datasets include both high - resource ( RCT ( Dernoncourt and Lee , 2017 ) , AGNEWS ( Zhang et al . , 2015 ) , HELPFULNESS ( McAuley et al . , 2015 ) , IMDB ( Maas et al . , 2011 ) ) and low - resource ( CHEMPROT ( Kringelum et al . , 2016 ) , ACL - ARC ( Jurgens et al . , 2018 ) , SCIERC ( Luan et al . , 2018 ) , HYPERPARTISAN ( Kiesel et al . , 2019 ) settings .", "entities": [[36, 37, "DatasetName", "IMDB"], [51, 52, "DatasetName", "CHEMPROT"], [61, 64, "DatasetName", "ACL - ARC"], [73, 74, "DatasetName", "SCIERC"]]}, {"text": "TAPT , whereas Pfeiffer et al . , 2020c employ both the language and the task adapters by stacking task adapters on top of the language adapters .", "entities": []}, {"text": "3 Experiments We now propose an adapter - based approach that is a parameter ef\ufb01cient variant of Task - Adaptive Pretraining ( TAPT ) and measure the margin of the performance between the pre - trained adapter model and the adapter model without pretraining .", "entities": []}, {"text": "For pretraining adapters , we added the adapter module in each transformer layer of RoBERTa using adaptertransformer ( Pfeiffer et al . , 2020b)1and continued pretraining all the weights in adapter layers on target text corpus while keeping the original parameters in RoBERTa \ufb01xed .", "entities": [[14, 15, "MethodName", "RoBERTa"], [42, 43, "MethodName", "RoBERTa"]]}, {"text": "After \ufb01nishing the second phase of pretraining , we performed \ufb01ne - tuning of RoBERTa by training the weights in the adapters and the \ufb01nal classi\ufb01cation layers while keeping all of the parameters in RoBERTa frozen .", "entities": [[14, 15, "MethodName", "RoBERTa"], [34, 35, "MethodName", "RoBERTa"]]}, {"text": "3.1 Dataset Following Gururangan", "entities": []}, {"text": "et al . , 20202 , we consider 8 classi\ufb01cation tasks from 4 different domains .", "entities": []}, {"text": "The speci\ufb01cation of each task is shown in Table 1 .", "entities": []}, {"text": "We covered news and review texts that are similar to the pretraining corpus of RoBERTa as well as scienti\ufb01c domains in which text corpora can have largely different distributions from those of RoBERTa .", "entities": [[14, 15, "MethodName", "RoBERTa"], [32, 33, "MethodName", "RoBERTa"]]}, {"text": "Furthermore , the pretraining corpora of the target tasks include both large and small cases to determine whether the adapter - based approach can be applicable in both low and high - resource settings .", "entities": []}, {"text": "1https://github.com/Adapter-Hub/ adapter - transformers 2Downloadble link for task dataset : https://github . com / allenai / dont - stop - pretraining3.2 Implementation Details Our implementation is based on HuggingFace since we found AllenNLP ( Gardner et al . , 2018 ) used in Gururangan et al . , 2020 is incompatible with adapter - transformer ( Pfeiffer et al . , 2020b ) .", "entities": []}, {"text": "We follow the hyperparameters setting in Gururangan et al . , 2020 , and each model in the pretraining and \ufb01ne - tuning stage is trained on a single GPU ( NVIDIA RTX 3090 ) .", "entities": []}, {"text": "Details of hyperparameters are described in", "entities": []}, {"text": "Appendix A. Note that for the pretraining step , we use a batch size of 8 and accumulate the gradient for every 32 steps to be consistent with the hyperparameter setting in Gururangan et al . , 2020 .", "entities": [[12, 14, "HyperparameterName", "batch size"]]}, {"text": "We perform pretraining with the self - supervised objectives , which are randomly masked tokens , with a probability of 15 % for each epoch and we do not apply validation to pretraining and save the model at the end of the training from a single seed .", "entities": []}, {"text": "For TAPT , we train the entire parameters of the RoBERTa via masked language modeling ( MLM ) on the target dataset , whereas for the adapter - based model , we embed the language adapters in each transformer layer and add invertible adapters after the embedding layers to perform MLM while freezing the original parameters of RoBERTa , following Pfeiffer et al . , 2020c .", "entities": [[10, 11, "MethodName", "RoBERTa"], [12, 15, "TaskName", "masked language modeling"], [16, 17, "DatasetName", "MLM"], [50, 51, "DatasetName", "MLM"], [57, 58, "MethodName", "RoBERTa"]]}, {"text": "Fine - tuning step is straightforward .", "entities": []}, {"text": "We perform \ufb01ne - tuning parameters that are pretrained via MLM for both TAPT and the adapter model .", "entities": [[10, 11, "DatasetName", "MLM"]]}, {"text": "Validation is performed after each epoch and the best checkpoint is loaded at the end of the training to evaluate the performance on the test set .", "entities": []}, {"text": "3.3 Experimental setup Experiments cover four different models .", "entities": []}, {"text": "First , we reproduce the performance of RoBERTa and TAPT in Gururangan et al . , 2020 as presented in Appendix C. Then we proceed to the adapter - based approach .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}, {"text": "93Dataset Baseline RoBERTa TAPT Adapter w/o PT Adapter w/ PT CHEMPROT 81.9 1:0 82.6 0:4 82.69 0:4 82.71 0:4 RCT 87.2 0:1 87.7 0:1 87.35 0:04 87.4 0:1", "entities": [[2, 3, "MethodName", "RoBERTa"], [4, 5, "MethodName", "Adapter"], [7, 8, "MethodName", "Adapter"], [10, 11, "DatasetName", "CHEMPROT"]]}, {"text": "ACL - ARC 63.0 5:8 67.4 1:8 69.47 2:4 69.25 2:5 SCIERC 77.3 1:9 79.3 1:5 81.5 0:9 82.37 1:0 HYPERPARTISAN 86.6 0:9 90.4 5:2 93.01 4:7 84.97 6:4 AGNEWS 93.9 0:2 94.5 0:1 94.00 0:1 93.94 0:1 HELPFULNESS 65.1 3:4 68.5 1:9 70.96 0:6 70.83 0:8 IMDB 95.0 0:2 95.5 0:1 95.51 0:1", "entities": [[0, 3, "DatasetName", "ACL - ARC"], [11, 12, "DatasetName", "SCIERC"], [47, 48, "DatasetName", "IMDB"]]}, {"text": "95.57 0:1", "entities": []}, {"text": "Average F1 81.3 83.24 84.31 83.38 Trainable params per task ( PT / FT ) -/124.64", "entities": [[0, 2, "MetricName", "Average F1"], [7, 8, "MetricName", "params"]]}, {"text": "M 163.35M/124.64 M -/1.78 M 2.18M/2.08 M Ratio to total params ( PT / FT ) -/100 % 100 %", "entities": [[10, 11, "MetricName", "params"]]}, {"text": "/100 %", "entities": []}, {"text": "-/1.42 % 1.32%/1.65 % Relative training speed ( PT / FT ) -/1.0 1.0/1.0 -/1.29 1.14/1.24 Relative inference speed ( PT / FT ) -/1.0 1.0/1.0 -/0.98 0.88/0.98 Table 2 : Average F1score with standard deviation on test set .", "entities": []}, {"text": "Each score is averaged over 5 random seeds .", "entities": [[7, 8, "DatasetName", "seeds"]]}, {"text": "Evaluation metric is macro- F1scores on test set for each task except for CHMEPROT and RCT which use microF1 .", "entities": []}, {"text": "We report the results of baseline RoBERTa and TAPT from Gururangan et al . , 2020 .", "entities": [[6, 7, "MethodName", "RoBERTa"]]}, {"text": "Following R \u00a8uckl\u00b4e et al . , 2020 , we measure the average relative speed for the training and the inference time across all tasks except for the the inference speed in \ufb01ne - tuning stage , which excludes low - resource tasks .", "entities": []}, {"text": "PT and FT indicate pretraining and \ufb01ne - tuning respectively .", "entities": []}, {"text": "To investigate the bene\ufb01ts of task - adaptive pretraining with adapters , we compare the performance of the pre - trained adapter model with the model without pretraining , i.e. , directly \ufb01ne - tuning adapters in RoBERTa on the target task .", "entities": [[37, 38, "MethodName", "RoBERTa"]]}, {"text": "For the adapter - based approach , we compare the adapter - based model with the second phase of pretraining and the model without the pretraining .", "entities": []}, {"text": "Since the weights of the adapters are randomly initialized , we empirically found that a larger learning rate worked well compared to the full \ufb01ne - tuning experiments .", "entities": [[16, 18, "HyperparameterName", "learning rate"]]}, {"text": "We sweep the learning rates in f2e-5 , 1e-4 , 3e-4 , 6e-4 gand the number of epochs in f10 , 20gon the validation set and report the test score that performs the best on the validation set .", "entities": [[15, 18, "HyperparameterName", "number of epochs"]]}, {"text": "3.4 Results The results are summarized in Table 2 .", "entities": []}, {"text": "Surprisingly , for the average F1score , the adapter - based model without task - adaptive pretraining performs best , followed by the other adapter with the pretraining model , TAPT , and the baseline RoBERTa .", "entities": [[35, 36, "MethodName", "RoBERTa"]]}, {"text": "Except for Hyperpartisan news , the adapter model without pretraining performs mostly on par with the counterpart adapter model that involves pretraining on target text corpus , suggesting that the bene\ufb01ts of additional task - adaptive pretraining diminish when we use the adapter - based approach .", "entities": []}, {"text": "Furthermore , directly \ufb01ne - tuned adapter model only trains 1.42 % of the entire parameters which leads to the 30 % faster - training step than the full model and skips the pretraining stage that typically expensive to train than the \ufb01ne - tuning , substantially reducing Figure 2 : F1score as a function of learning rate on test set with log scale on x - axis .", "entities": [[56, 58, "HyperparameterName", "learning rate"]]}, {"text": "F1score is averaged over 5 random seeds for low - resource tasks ( CHEMPROT , ACL - ARC , SCIERC , HYPER ) due to the high variance .", "entities": [[6, 7, "DatasetName", "seeds"], [13, 14, "DatasetName", "CHEMPROT"], [15, 18, "DatasetName", "ACL - ARC"], [19, 20, "DatasetName", "SCIERC"]]}, {"text": "For high - resource tasks ( RCT , AGNEWS , HELPFULNESS , IMDB ) , we report the F1score from a single random seed for each task .", "entities": [[12, 13, "DatasetName", "IMDB"]]}, {"text": "For RoBERTa and TAPT , we follow the hyper - parameter settings in Gururangan et al . , 2020 except for the learning rate .", "entities": [[1, 2, "MethodName", "RoBERTa"], [22, 24, "HyperparameterName", "learning rate"]]}, {"text": "the training time while the relative speed for the inference only decreases by 2 % to the full model .", "entities": []}, {"text": "3.5 Analysis We analyze how the adapter alone can surpass or perform on par with both the full model and adapter model with task - adaptive pretraining .", "entities": []}, {"text": "Since we sweep the learning rates and the number of epochs in the range that includes larger \ufb01gures compared to those in the full model when \ufb01ne - tuning adapters and kept the other hyper - parameters the same as in Gururangan et al . , 2020 , we hypothesize that", "entities": [[8, 11, "HyperparameterName", "number of epochs"]]}, {"text": "94Dataset Baseline RoBERTa TAPT CHEMPROT 82.8 0:9 82.62 0:5 RCT 86.89 0:1 87.4 0:2 ACL - ARC 69.24 2:6 70.08 2:3 SCIERC 80.59 0:9 81.28 1:2 HYPER 94.53 2:0 86.17 1:3 AGNEWS 93.9 0:2 94.05 0:1 HELPFUL 69.63 0:6 71.28 0:8 IMDB 94.93 0:1 95.33 0:1 Average F1 84.06 83.52 Table 3 : Best performance of baseline RoBERTa and TAPT ( Gururangan et al . , 2020 ) on our implementation .", "entities": [[2, 3, "MethodName", "RoBERTa"], [4, 5, "DatasetName", "CHEMPROT"], [14, 17, "DatasetName", "ACL - ARC"], [21, 22, "DatasetName", "SCIERC"], [41, 42, "DatasetName", "IMDB"], [46, 48, "MetricName", "Average F1"], [57, 58, "MethodName", "RoBERTa"]]}, {"text": "Each score is averaged over 5 random seeds .", "entities": [[7, 8, "DatasetName", "seeds"]]}, {"text": "Best con\ufb01guration settings for each task is described in Appendix Table 8 .", "entities": []}, {"text": "the larger learning rate zeroes out the bene\ufb01ts of pretraining .", "entities": [[2, 4, "HyperparameterName", "learning rate"]]}, {"text": "Figure 2 . shows the average F1score across all tasks as a function of learning rate .", "entities": [[14, 16, "HyperparameterName", "learning rate"]]}, {"text": "The adapter model without a second phase of pretraining consistently outperforms or performs on par with the adapter model with pretraining from 1e-4 to 6e-4 , demonstrating that the additional pretraining turns out to be ineffective .", "entities": []}, {"text": "In contrast , TAPT outperforms baseline RoBERTa from 2e-5 , where both TAPT and baseline RoBERTa perform best .", "entities": [[6, 7, "MethodName", "RoBERTa"], [15, 16, "MethodName", "RoBERTa"]]}, {"text": "The results show that different learning rates used in the \ufb01ne - tuning stage can affect the effectiveness of pretraining and demonstrate that directly \ufb01ne - tuning a fraction of parameters can provide comparable performance to the full - model as well as the adapter model with pretraining while substantially reducing the training time .", "entities": []}, {"text": "Inspired by the results of the adapter models , we perform the same experiments for the full model ( baseline RoBERTa and TAPT ) on our implementation by sweeping the learning rates and the number of epochs .", "entities": [[0, 1, "DatasetName", "Inspired"], [20, 21, "MethodName", "RoBERTa"], [34, 37, "HyperparameterName", "number of epochs"]]}, {"text": "We hypothesize that proper hyperparameter settings such as a larger learning rate or increasing the number of training steps in the \ufb01ne - tuning stage can improve the performance of baseline RoBERTa , making pretraining on the unlabeled target task less effective .", "entities": [[10, 12, "HyperparameterName", "learning rate"], [31, 32, "MethodName", "RoBERTa"]]}, {"text": "We sweep the learning rates in f1e-5 , 2e-5 , 3e-5 gand the number of epochs in f10 , 20 gon the validation set and report the test score that performs the best on the validation set .", "entities": [[13, 16, "HyperparameterName", "number of epochs"]]}, {"text": "Table 3 shows the best performance of the full models for each task among different hyper - parameter settings .", "entities": []}, {"text": "The average F1score of baseline RoBERTa greatly increases and surprisingly , it surpasses the performance of TAPT in some tasks .", "entities": [[5, 6, "MethodName", "RoBERTa"]]}, {"text": "The results ensure that although pretraining PLMs on the target task results in betterperformance , one can achieve comparable performance by simply using a larger learning rate or increasing training steps in the \ufb01ne - tuning stage while skipping the pretraining step that is computationally demanding compared to the \ufb01ne - tuning .", "entities": [[25, 27, "HyperparameterName", "learning rate"]]}, {"text": "4 Conclusion Our work demonstrates that adapters provide a competitive alternative to large - scale task - adaptive pretraining for NLP classi\ufb01cation tasks .", "entities": []}, {"text": "We show that it is possible to achieve similar performance to TAPT with pretraining training just 1.32 % of the parameters through pretraining with adapters .", "entities": []}, {"text": "However , the most computationally ef\ufb01cient option is to skip pretraining and only perform \ufb01ne - tuning with adapters .", "entities": []}, {"text": "We found that skipping pretraining altogether and just \ufb01ne - tuning with adapters outperforms or performs mostly on par with TAPT and the adapter model with pretraining across our tasks while substantially reducing the training time .", "entities": []}, {"text": "Acknowledgments We would like to thank Zsolt Kira , Mandeep Baines , Shruti Bhosale , and Siddharth Goyal for helpful feedback and suggestions .", "entities": []}, {"text": "We also would like to thank anonymous reviewers for their insightful comments on the earlier version of the paper .", "entities": []}, {"text": "References Franck Dernoncourt and Ji Young Lee . 2017 .", "entities": []}, {"text": "PubMed 200k RCT : a dataset for sequential sentence classi\ufb01cation in medical abstracts .", "entities": []}, {"text": "In Proceedings of the Eighth International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 308\u2013313 , Taipei , Taiwan .", "entities": []}, {"text": "Asian Federation of Natural Language Processing .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Matt Gardner , Joel Grus , Mark Neumann , Oyvind Tafjord , Pradeep Dasigi , Nelson F. Liu , Matthew Peters , Michael Schmitz , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "AllenNLP : A deep semantic natural language processing platform .", "entities": []}, {"text": "In Proceedings of Workshop for NLP Open Source Software ( NLP - OSS ) , pages 1 \u2013 6 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "95Suchin Gururangan , Ana Marasovi \u00b4 c , Swabha Swayamdipta , Kyle Lo , Iz Beltagy , Doug Downey , and Noah A. Smith .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Do n\u2019t stop pretraining : Adapt language models to domains and tasks .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8342\u20138360 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xiaochuang Han and Jacob Eisenstein .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unsupervised domain adaptation of contextualized embeddings for sequence labeling .", "entities": [[0, 3, "TaskName", "Unsupervised domain adaptation"]]}, {"text": "In EMNLP .", "entities": []}, {"text": "Neil Houlsby , Andrei Giurgiu , Stanislaw Jastrzebski , Bruna Morrone , Quentin De Laroussilhe , Andrea Gesmundo , Mona Attariyan , and Sylvain Gelly .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Parameter - ef\ufb01cient transfer learning for NLP .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "InProceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pages 2790\u20132799 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "David Jurgens , Srijan Kumar , Raine Hoover , Dan McFarland , and Dan Jurafsky .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "2018 .", "entities": []}, {"text": "Measuring the evolution of a scienti\ufb01c \ufb01eld through citation frames .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 6:391\u2013406 .", "entities": []}, {"text": "Johannes Kiesel , Maria Mestre , Rishabh Shukla , Emmanuel Vincent , Payam Adineh , David Corney , Benno Stein , and Martin Potthast .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SemEval2019 task 4 : Hyperpartisan news detection .", "entities": []}, {"text": "In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 829\u2013839 , Minneapolis , Minnesota , USA .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jens Kringelum , Sonny Kim Kjaerulff , S\u00f8ren Brunak , Ole Lund , Tudor I Oprea , and Olivier Taboureau .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Chemprot-3.0 : a global chemical biology diseases mapping .", "entities": []}, {"text": "Database , 2016 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yi Luan , Luheng He , Mari Ostendorf , and Hannaneh Hajishirzi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Multi - task identi\ufb01cation of entities , relations , and coreference for scienti\ufb01c knowledge graph construction .", "entities": [[14, 16, "TaskName", "graph construction"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3219\u20133232 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Andrew L. Maas , Raymond E. Daly , Peter T. Pham , Dan Huang , Andrew Y .", "entities": []}, {"text": "Ng , and Christopher Potts . 2011 .", "entities": []}, {"text": "Learning word vectors for sentiment analysis .", "entities": [[4, 6, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , pages 142\u2013150 , Portland , Oregon , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Julian McAuley , Christopher Targett , Qinfeng Shi , and Anton Van Den Hengel . 2015 .", "entities": []}, {"text": "Image - based recommendations on styles and substitutes .", "entities": []}, {"text": "In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval , pages 43\u201352 .", "entities": [[6, 7, "DatasetName", "ACM"], [14, 16, "TaskName", "information retrieval"]]}, {"text": "Arindam Mitra , Pratyay Banerjee , Kuntal Kumar Pal , Swaroop Ranjan Mishra , and Chitta Baral .", "entities": [[7, 8, "DatasetName", "Kumar"]]}, {"text": "2020 .", "entities": []}, {"text": "Exploring ways to incorporate additional knowledge to improve natural language commonsense question answering . arXiv:1909.08855v3 .", "entities": [[11, 13, "TaskName", "question answering"]]}, {"text": "Jonas Pfeiffer , Aishwarya Kamath , Andreas R \u00a8uckl\u00b4e , Kyunghyun Cho , and Iryna Gurevych .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "Adapterfusion : Non - destructive task composition for transfer learning .", "entities": [[8, 10, "TaskName", "transfer learning"]]}, {"text": "arXiv preprint arXiv:2005.00247 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jonas Pfeiffer , Andreas R \u00a8uckl\u00b4e , Clifton Poth , Aishwarya Kamath , Ivan Vuli \u00b4 c , Sebastian Ruder , Kyunghyun Cho , and Iryna Gurevych .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "AdapterHub :", "entities": []}, {"text": "A framework for adapting transformers .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 46\u201354 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jonas Pfeiffer , Ivan Vuli \u00b4 c , Iryna Gurevych , and Sebastian Ruder .", "entities": []}, {"text": "2020c .", "entities": []}, {"text": "MAD - X : An Adapter - Based Framework for Multi - Task Cross - Lingual Transfer .", "entities": [[0, 1, "DatasetName", "MAD"], [5, 6, "MethodName", "Adapter"], [13, 17, "TaskName", "Cross - Lingual Transfer"]]}, {"text": "InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 7654\u20137673 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training .", "entities": []}, {"text": "Sylvestre - Alvise Rebuf\ufb01 , Hakan Bilen , and Andrea Vedaldi .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Learning multiple visual domains with residual adapters .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 , pages 506 \u2013 516 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Andreas R \u00a8uckl\u00b4e , Gregor Geigle , Max Glockner , Tilman Beck , Jonas Pfeiffer , Nils Reimers , and Iryna Gurevych . 2020 .", "entities": []}, {"text": "Adapterdrop :", "entities": []}, {"text": "On the ef\ufb01ciency of adapters in transformers .", "entities": []}, {"text": "arXiv preprint arXiv:2010.11918 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 , pages 5998\u20136008 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "GLUE :", "entities": [[0, 1, "DatasetName", "GLUE"]]}, {"text": "A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[9, 12, "TaskName", "natural language understanding"]]}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 353\u2013355 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "96Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 .", "entities": []}, {"text": "Character - level convolutional networks for text classi\ufb01cation .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 28 , pages 649\u2013657 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "A Hyperparameter Details Details of hyperparameter setting including the learning rates for the best performing results are provided in Table 4 , 5 , and 6 . B Validation Results", "entities": []}, {"text": "We present validation performance in Table 7 and Figure 3 and 8 .", "entities": []}, {"text": "C Replication results We provide replication results of Gururangan et al . , 2020 in Table 9 .", "entities": []}, {"text": "97Hyper - parameter Value Optimizer Adam Adam epsilon 1e-8 , 0.999 Learning rate 1e-4 Batch size 8 Gradient accumulation step 32 Epochs 40 or 100 Adapter reduction factor 12 Maximum sequence length 512 Table 4 : Details of hyperparameters used in pretraining experiments .", "entities": [[4, 5, "HyperparameterName", "Optimizer"], [5, 6, "MethodName", "Adam"], [6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "epsilon"], [11, 13, "HyperparameterName", "Learning rate"], [14, 16, "HyperparameterName", "Batch size"], [25, 26, "MethodName", "Adapter"]]}, {"text": "We used 40 number of epochs for HELPFULNESS and 100 for the other tasks .", "entities": [[3, 6, "HyperparameterName", "number of epochs"]]}, {"text": "Hyper - parameter Value Optimizer Adam Adam epsilon 1e-8 , 0.999 Batch size 16 Gradient accumulation step 1 Epochs 10 or 20 Patience 3 or 5 Adapter reduction factor 12 Dropout 0.1 Feedforward layer 1", "entities": [[4, 5, "HyperparameterName", "Optimizer"], [5, 6, "MethodName", "Adam"], [6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "epsilon"], [11, 13, "HyperparameterName", "Batch size"], [26, 27, "MethodName", "Adapter"], [30, 31, "MethodName", "Dropout"]]}, {"text": "Feedforward nonlinearity tanh Classi\ufb01cation layer 1", "entities": []}, {"text": "Learning rate see Table 6 Learning rate decay linear", "entities": [[0, 2, "HyperparameterName", "Learning rate"], [5, 7, "HyperparameterName", "Learning rate"]]}, {"text": "Warmup proportion 0.06 Maximum sequence length 512 Table 5 : Details of hyperparameters used in \ufb01ne - tuning experiments .", "entities": []}, {"text": "For baseline RoBERTa and TAPT , we used 10 number of epochs with patience of 3 and the learning rate of 2e-5 .", "entities": [[2, 3, "MethodName", "RoBERTa"], [9, 12, "HyperparameterName", "number of epochs"], [18, 20, "HyperparameterName", "learning rate"]]}, {"text": "For adapter experiments , see Table 6 .", "entities": []}, {"text": "Dataset Adapter w/o PT ( LR , Epochs , Patience )", "entities": [[1, 2, "MethodName", "Adapter"]]}, {"text": "Adapter w/ PT ( LR , Epochs , Patience ) CHEMPROT 3e-4 , 20 , 5 6e-4 , 20 , 5 RCT 1e-4 , 10 , 3 1e-4 , 10 , 3 ACL - ARC 6e-4 , 10 , 3 6e-4 , 20 , 5 SCIERC 3e-4 , 20 , 5 6e-4 , 20 , 5 HYPER 3e-4 , 20 , 5 1e-4 , 20 , 5 AGNEWS 1e-4 , 10 , 3 1e-4 , 10 , 3 HELPFUL 3e-4 , 20 , 5 1e-4 , 20 , 5 IMDB 1e-4 , 10 , 3 1e-4 , 10 , 3 Table 6 : Learning rate , the nubmer of epochs and patience for best - performing models .", "entities": [[0, 1, "MethodName", "Adapter"], [10, 11, "DatasetName", "CHEMPROT"], [32, 35, "DatasetName", "ACL - ARC"], [45, 46, "DatasetName", "SCIERC"], [89, 90, "DatasetName", "IMDB"], [103, 105, "HyperparameterName", "Learning rate"]]}, {"text": "For adapter experiments , we sweep the learning rates in f1e-4 , 3e-4 , 6e-4 g , the number of epochs in f10 , 20 g , and patience factor in f3 , 5 g on validation set .", "entities": [[18, 21, "HyperparameterName", "number of epochs"]]}, {"text": "98Dataset Adapter w/o pretraining Adapter w/ pretraining CHEMPROT 83.77 0:5 84.02 0:7 RCT 88.16 0:1 88.13 0:1 ACL - ARC 72.41 2:2 77.31 2:9 SCIERC 86.86 0:5 87.87 0:3 HYPER 86.33 1:4 86.00 3:5 AGNEWS 94.28 0:1 94.57 0:1 HELPFUL 70.83 1:2 70.8 0:7 IMDB 95.52 0:1 95.6 0:1", "entities": [[1, 2, "MethodName", "Adapter"], [4, 5, "MethodName", "Adapter"], [7, 8, "DatasetName", "CHEMPROT"], [17, 20, "DatasetName", "ACL - ARC"], [24, 25, "DatasetName", "SCIERC"], [44, 45, "DatasetName", "IMDB"]]}, {"text": "Average F1 84.77 85.54 Table 7 : Validation performance of adapter experiments .", "entities": [[0, 2, "MetricName", "Average F1"]]}, {"text": "Each score is averaged over 5 random seeds .", "entities": [[7, 8, "DatasetName", "seeds"]]}, {"text": "Evaluation metric is macro- F1scores for each task except for CHMEPROT and RCT which use micro- F1 .", "entities": [[16, 17, "MetricName", "F1"]]}, {"text": "Figure 3 : F1score as a function of learning rate on development setwith log scale on x - axis .", "entities": [[8, 10, "HyperparameterName", "learning rate"]]}, {"text": "F1score is averaged over 5 random seeds for low - resource tasks ( CHEMPROT , ACL - ARC , SCIERC , HYPER ) due to the high variance .", "entities": [[6, 7, "DatasetName", "seeds"], [13, 14, "DatasetName", "CHEMPROT"], [15, 18, "DatasetName", "ACL - ARC"], [19, 20, "DatasetName", "SCIERC"]]}, {"text": "For high - resource tasks ( RCT , AGNEWS , HELPFULNESS , IMDB ) , we report the F1score from a single random seed for each task .", "entities": [[12, 13, "DatasetName", "IMDB"]]}, {"text": "Here we sweep the learning rates in f1e-4 , 3e-4 , 6e-4 g , the number of epochs in f10 , 20 g , and the patience factor in f3 , 5 g. Dataset Baseline RoBERTa TAPT Hyper - parameters ( LR , Epochs , Patience ) CHEMPROT 82.8 0:9 82.62 0:5 3e-5 , 20 , 5 RCT 86.89 0:1 87.4 0:2 2e-5 , 10 , 3 ACL - ARC 69.24 2:6 70.08 2:3 3e-5 , 20 , 5 SCIERC 80.59 0:9 81.28 1:2 2e-5 , 20 , 5 HYPER 94.53 2:0 86.17 1:3 3e-5 , 10 , 3 AGNEWS 93.9 0:2 94.05 0:1 2e-5 , 10 , 3 HELPFUL 69.63 0:6 71.28 0:8 2e-5 , 20 , 5 IMDB 94.93 0:1 95.33 0:1 2e-5 , 20 , 5 Average F1 84.06 83.52 Table 8 : Validation performance of Baseline RoBERTa and TAPT experiments that corresponds to Table 3 .", "entities": [[15, 18, "HyperparameterName", "number of epochs"], [35, 36, "MethodName", "RoBERTa"], [47, 48, "DatasetName", "CHEMPROT"], [67, 70, "DatasetName", "ACL - ARC"], [79, 80, "DatasetName", "SCIERC"], [119, 120, "DatasetName", "IMDB"], [129, 131, "MetricName", "Average F1"], [140, 141, "MethodName", "RoBERTa"]]}, {"text": "Each score is averaged over 5 random seeds .", "entities": [[7, 8, "DatasetName", "seeds"]]}, {"text": "99Original Results Original Results Our Results Our Results Dataset Baseline RoBERTa TAPT Baseline RoBERTa TAPT CHEMPROT 81.9 1:0 82.6 0:4 81.64 0:8 82.58 0:5 RCT 87.2 0:1 87.7 0:1 86.89 0:1 87.4 0:2", "entities": [[10, 11, "MethodName", "RoBERTa"], [13, 14, "MethodName", "RoBERTa"], [15, 16, "DatasetName", "CHEMPROT"]]}, {"text": "ACL - ARC 63.0 5:8 67.4 1:8 64.12 5:5 66.11 4:6 SCIERC 77.3 1:9 79.3 1:5 78.89 2:7 79.94 0:7 HYPER 86.6 0:9 90.4 5:2 85.03 6:0 91.56 2:5 AGNEWS 93.9 0:2 94.5 0:1 93.72 0:2 94.05 0:1 HELPFULNESS 65.1 3:4 68.5 1:9 69.2 1:4 71.24 0:7 IMDB 95.0 0:2 95.5 0:1 95.15 0:1 95.33 0:1 Average F1 81.3 83.24 81.83 83.53 Table 9 : Reproducing Baseline RoBERTa and TAPT Results , average F1Scores with standard deviation .", "entities": [[0, 3, "DatasetName", "ACL - ARC"], [11, 12, "DatasetName", "SCIERC"], [47, 48, "DatasetName", "IMDB"], [56, 58, "MetricName", "Average F1"], [67, 68, "MethodName", "RoBERTa"]]}, {"text": "F1score is averaged over 5 random seeds .", "entities": [[6, 7, "DatasetName", "seeds"]]}, {"text": "We use the same hyper - parameters in Gururangan et al . , 2020 .", "entities": []}]