[{"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 774\u2013778 Brussels , Belgium , October 31 - November 4 , 2018 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics774Evaluating Multiple System Summary Lengths : A Case Study Ori Shapira1 , David Gabay1 , Hadar Ronen2 , Judit Bar - Ilan2 , Yael Amsterdamer1 , Ani Nenkova3 , and Ido Dagan1 1Computer Science Department , Bar - Ilan University , Ramat - Gan , Israel 2Information Science Department , Bar - Ilan University , Ramat - Gan , Israel 3University of Pennsylvania , Philadelphia , PA fobspp18 , dawid.gabay , hadarg , juditb g@gmail.com amstery@cs.biu.ac.il , nenkova@seas.upenn.edu , dagan@cs.biu.ac.il Abstract Practical summarization systems are expected to produce summaries of varying lengths , per user needs .", "entities": [[86, 87, "TaskName", "summarization"]]}, {"text": "While a couple of early summarization benchmarks tested systems across multiple summary lengths , this practice was mostly abandoned due to the assumed cost of producing reference summaries of multiple lengths .", "entities": [[5, 6, "TaskName", "summarization"]]}, {"text": "In this paper , we raise the research question of whether reference summaries of a single length can be used to reliably evaluatesystem summaries of multiple lengths .", "entities": []}, {"text": "For that , we have analyzed a couple of datasets as a case study , using several variants of the ROUGE metric that are standard in summarization evaluation .", "entities": [[26, 27, "TaskName", "summarization"]]}, {"text": "Our \ufb01ndings indicate that the evaluation protocol in question is indeed competitive .", "entities": []}, {"text": "This result paves the way to practically evaluating varying - length summaries with simple , possibly existing , summarization benchmarks .", "entities": [[18, 19, "TaskName", "summarization"]]}, {"text": "1 Introduction Automated summarization systems typically produce a text that mimics a manual summary .", "entities": [[3, 4, "TaskName", "summarization"]]}, {"text": "In these systems , an important aspect is the output summary length , which may vary according to user needs .", "entities": []}, {"text": "Consequently , output length has been a common tunable parameter in pre - neural summarization systems and has been incorporated recently in few neural models as well ( Kikuchi et al . , 2016 ; Fan et al . , 2017 ; Ficler and Goldberg , 2017 ) .", "entities": [[14, 15, "TaskName", "summarization"]]}, {"text": "It was originally assumed that summarization systems should be assessed across multiple summary lengths .", "entities": [[5, 6, "TaskName", "summarization"]]}, {"text": "For that , the earliest Document Understand Conference ( DUC ) ( NIST , 2011 ) benchmarks , in 2001 and 2002 , de\ufb01ned several target summary lengths and evaluated each summary against ( manually written ) reference summaries of the same length .", "entities": []}, {"text": "However , due to the high cost incurred , subsequent DUC and TAC ( NIST , 2018 ) benchmarks(2003 - 2014 ) , as well as the more recently popular datasets CNN / Daily Mail ( Nallapati et al . , 2016 ) and Gigaword ( Graff et al . , 2003 ) , included references and evaluation for just one summary length per input text .", "entities": [[31, 35, "DatasetName", "CNN / Daily Mail"]]}, {"text": "Accordingly , systems were asked to produce a single summary , of corresponding length .", "entities": []}, {"text": "This decision was partly supported by an observation that system rankings tended to correlate across different summary lengths ( Over et al . , 2007 ) , even though , as we show in Section 2 , this correlation is limited .", "entities": []}, {"text": "In this paper , we propose that the summarization community should consider resuming evaluating summarization systems over multiple length outputs , as it would allow better assessment of length - related performance within and across systems ( illustrated in Section 3 ) .", "entities": [[8, 9, "TaskName", "summarization"], [14, 15, "TaskName", "summarization"]]}, {"text": "To avoid the need in multiple - length reference summaries we raise the following research question : can reference summaries of a single length be used to evaluate system summaries of multiple lengths , as reliably as when using references of multiple lengths , with respect to different standard evaluation metrics ?", "entities": []}, {"text": "Recently , Kikuchi et al .", "entities": []}, {"text": "( 2016 ) evaluated system summaries of three different lengths against reference summaries of a single length .", "entities": []}, {"text": "Yet , their evaluation methodology was not assessed through correlation to human judgment , as has been commonly done for other automatic evaluation protocols .", "entities": []}, {"text": "Here , we provide a closer look into this methodology , given its potential value .", "entities": []}, {"text": "As a \ufb01rst accessible case study , we test our research question over the DUC 2001 and 2002 data ( Section 2 ) .", "entities": []}, {"text": "To the best of our knowledge , these are the only two datasets that include multiple length reference and submitted system summaries , as well as manual assessment of the latter .", "entities": []}, {"text": "Our analysis reveals that , for this data and with respect to various highly utilized automatic ROUGE metrics , the answer to our question is af\ufb01rmative , in", "entities": []}, {"text": "775 # refs ref lengths ( # words ) # clusters # systems 2001 3 50 , 100 , 200 , 400 30 14 2002 2 10 , 50 , 100 , 200 59 10 Table 1 : DUC 2001 and 2002 .", "entities": []}, {"text": "Number of reference summaries per length for each text cluster , reference lengths , number of clusters and number of evaluated systems .", "entities": []}, {"text": "terms of correlation with human judgment .", "entities": []}, {"text": "Our promising results suggest repeating the assessment methodology presented here in future work , to test our question over more recent and broader summarization datasets and human evaluation schemes .", "entities": [[23, 24, "TaskName", "summarization"]]}, {"text": "This , in turn , would allow the community to feasibly resume proper evaluation and deliberate development of systems that target effective summaries across a range of lengths .", "entities": []}, {"text": "2 Case Study Analysis Here , we \ufb01rst examine the relevance of our proposal to reinstitute summarization evaluation over multiple summary lengths .", "entities": [[16, 17, "TaskName", "summarization"]]}, {"text": "Then , we investigate our research question of whether using reference summaries of a single length suf\ufb01ces for evaluating system summaries of multiple lengths .", "entities": []}, {"text": "We turn to the DUC 2001 and 2002 multi - document summarization datasets , which , to the best of our knowledge , are the only available datasets that provide the necessary requirements for this analysis ( see Table 1 ) .", "entities": [[8, 12, "TaskName", "multi - document summarization"]]}, {"text": "The importance of evaluating and comparing systems at several lengths is demonstrated with the observation that system rankings can change quite signi\ufb01cantly at different summary lengths .", "entities": []}, {"text": "In 2001 , the Spearman correlation between the available human rankings of systems at the 50word and 400 - word lengths is 0.61 .", "entities": [[4, 6, "MetricName", "Spearman correlation"]]}, {"text": "For example , the system ranked \ufb01rst at length 50 ranks sixth at lengths 200 and 400 .", "entities": []}, {"text": "Even for the human system ranking at the 100 - word length , which deviates the least from human rankings at the other lengths , the correlation with system ranking at the 400 length is only 0.73 .", "entities": []}, {"text": "Generally , the larger the difference between a pair of summary lengths , the greater the \ufb02uctuation in system rankings .", "entities": []}, {"text": "Similar trends were observed for DUC 2002 , and when comparing system rankings by automatic ROUGE scoring ( both rankings are elaborated below ) .", "entities": []}, {"text": "Obviously , such performance differences are overlooked when evaluating systems over summaries of a single length .", "entities": []}, {"text": "Next , we turn to investigate our research question .", "entities": []}, {"text": "In this paper , we examine it with respect to automatic summary evaluation , which has become most common for system development and evaluation , thanks to its speed and low cost .", "entities": []}, {"text": "Speci\ufb01cally , we use several variants of the ROUGE metric ( Lin , 2004 ) , which is almost exclusively utilized as an automatic evaluation metric class for summarization .", "entities": [[28, 29, "TaskName", "summarization"]]}, {"text": "ROUGE variants are based on word sequence overlap between a system summary and a reference summary , where each variant measures a different aspect of text comparison .", "entities": []}, {"text": "Despite its pitfalls , ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods ( Lin , 2004 ; Over and James , 2004 ; Over et al . , 2007 ; Nenkova et al . , 2007 ; Louis and Nenkova , 2013 ; Peyrard et al . , 2017 ) , such as SEE ( Lin , 2001 ) , responsiveness ( NIST , 2006 ) and Pyramid ( Nenkova et al . , 2007 ) .", "entities": []}, {"text": "We follow the same methodology of assessing the reliability of automatic evaluation scores by measuring their correlation to human evaluation scores .", "entities": []}, {"text": "In our case , DUC 2001 and 2002 applied the SEE manual evaluation method .", "entities": []}, {"text": "NIST assessors compared systems \u2019 summaries to reference summaries , which were all decomposed into a list of elementary discourse units ( EDUs ) .", "entities": []}, {"text": "Each reference EDU was marked throughout the system EDUs and was scored for how well it was expressed .", "entities": []}, {"text": "The \ufb01nal manually evaluated scores , called the human mean content coverage scores , are provided in the DUC datasets .", "entities": []}, {"text": "We can then correlate the human - based system ranking , attained from these provided scores , to the system ranking attained from the automatic scores that we calculate using our proposed methodology .", "entities": []}, {"text": "As a baseline , we consider the ROUGE Recall scores obtained by the standard reference summary con\ufb01guration ( Standard , \ufb01rst row in Table 2 ) , that is , when system summaries of each length ( table columns ) are evaluated against reference summaries of the same length .", "entities": [[8, 9, "MetricName", "Recall"]]}, {"text": "This is the same con\ufb01guration used by Lin ( 2004 ) when introducing and assessing ROUGE .", "entities": []}, {"text": "Then , looking into our research question , we consider reference summary con\ufb01gurations in which system summaries of all lengths are evaluated against reference summaries of a single chosen length ( OnlyNNN , subsequent rows of Table 2 ) .", "entities": []}, {"text": "In each con\ufb01guration ( each row ) , we repeat the evaluation twice : once using the complete set of available reference sum-", "entities": []}, {"text": "776System Summary Length 50 100 200 400 Avg . across lengths 3refs 1ref 3refs 1ref 3refs 1ref 3refs 1ref 3refs 1refReference SetStandard 0.72 0.65 0.88 0.85 0.9 0.86 0.95 0.94 0.86 0.83 Only50 0 0 +0.02 0 +0.01 +0.04 +0.01 +0.02 +0.010 +0.015 Only100 -0.01 +0.04 0 0 +0.01 -0.01 +0.02 0", "entities": [[33, 34, "DatasetName", "0"], [34, 35, "DatasetName", "0"], [36, 37, "DatasetName", "0"], [46, 47, "DatasetName", "0"], [47, 48, "DatasetName", "0"], [51, 52, "DatasetName", "0"]]}, {"text": "+0.005 +0.008 Only200", "entities": []}, {"text": "-0.09 -0.09 -0.06 -0.08 0 0 +0.01 -0.01 -0.035 -0.0045 Only400 -0.06 +0.02", "entities": [[4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"]]}, {"text": "-0.09 -0.09 -0.01 +0.03 0 0 -0.040 -0.010 Table 2 : Pearson correlations between ROUGE-1 and human scores over DUC 2001 for different system summary lengths ( column pairs ) and different reference summary con\ufb01gurations ( rows ) , when using one reference or three .", "entities": [[4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [14, 15, "MetricName", "ROUGE-1"]]}, {"text": "The \ufb01rst baseline row presents absolute correlations while successive rows show relative differences to the baseline .", "entities": []}, {"text": "2001 2002 R-1 R-2 R - L R-1 R-2 R - L 3refs 1ref 3refs 1ref 3refs 1ref 2refs 1ref 2refs 1ref 2refs 1refReference SetStandard 0.86 0.83 0.79 0.77 0.87 0.83 0.78 0.75 0.86 0.82 0.82 0.77 Only10 - - - - - - 0 -0.015 -0.100 -0.178 -0.003 -0.045 Only50 +0.010 +0.015 -0.013 -0.038 +0.008 +0.010 +0.035 +0.053 -0.050 -0.038", "entities": [[44, 45, "DatasetName", "0"]]}, {"text": "+0.020 +0.080 Only100 +0.005 +0.008 -0.010 -0.003 +0.005 +0.013 +0.023 +0.048 -0.035 0 -0.008 +0.040 Only200 -0.035 -0.045 -0.055 -0.053 -0.033 -0.04", "entities": [[12, 13, "DatasetName", "0"]]}, {"text": "+0.013 +0.023 -0.068 -0.025", "entities": []}, {"text": "-0.028 +0.005 Only400 -0.040 -0.010", "entities": []}, {"text": "-0.075 -0.075 -0.038 0 - - - - - Table 3 : Averaged correlations ( across system summary lengths , equivalent to the rightmost columns in Table 2 ) for different ROUGE variants ( column pairs ) and reference summary con\ufb01gurations ( rows ) , when using 1 reference or multiple .", "entities": [[3, 4, "DatasetName", "0"]]}, {"text": "The \ufb01rst row presents absolute correlations , with relative differences in subsequent rows .", "entities": []}, {"text": "maries of the utilized reference length , and once with just one randomly chosen reference summary from that set ( the 3refs and 1ref sub - columns ) .", "entities": []}, {"text": "For each reference summary con\ufb01guration , we compute ROUGE Recall system scores1for the three common ROUGE variants R-1 , R-2 and R - L , which compare unigrams , bigrams and the longest common subsequence , respectively .", "entities": [[9, 10, "MetricName", "Recall"]]}, {"text": "System scores , per summary length , are obtained by averaging across all summarized texts .", "entities": []}, {"text": "We then calculate their Pearson correlation2with the available human mean content coverage scores for the systems .", "entities": []}, {"text": "The \ufb01rst row of Table 2 shows these correlations , considering the R-1 scores for the DUC 2001 systems , per summary length .", "entities": []}, {"text": "The subsequent rows show the corresponding \ufb01gures for the single - reference - length con\ufb01gurations .", "entities": []}, {"text": "For readability , we present in these rows the relative differences to the Standard baseline row .", "entities": []}, {"text": "Hence , positive values indicate a con\ufb01guration that is at least as good as the standard con\ufb01guration .", "entities": []}, {"text": "Table 3 presents correlations averaged over all summary lengths , for the three ROUGE variants 1Omitting stop words .", "entities": []}, {"text": "2Following Lin ( 2004 ) .", "entities": []}, {"text": "Spearman ranking correlations provide similar results.over both datasets .", "entities": []}, {"text": "We see in the tables that evaluating system summaries of all lengths against references of a single length often performs on par with the standard con\ufb01guration .", "entities": []}, {"text": "In particular , the single \ufb01xed set of 50 - word reference summaries performs overall as well as the standard approach , and , although not substantially , is the most effective con\ufb01guration within the data analyzed .", "entities": []}, {"text": "In other words , in this dataset , the 50 - word reference summaries provide a \u201c test sample \u201d for evaluating the longer system summaries , which is as effective as the same length references used by the standard method .", "entities": []}, {"text": "We note that even when a single reference summary is available , reasonable correlations with human scores are obtained for the 50 word reference .", "entities": []}, {"text": "This suggests that it may be possible to compare system summaries of multiple lengths even against a single reference summary , of a relatively short length .", "entities": []}, {"text": "This observation seems to deserve further assessment over recent large scale datasets , such as CNN / DailyMail , which provide a single relatively short reference for each summarized text .", "entities": []}, {"text": "In addition to correlation to human assessment , we computed the correlations between system rankings calculated by Standard and those calcu-", "entities": []}, {"text": "777100 200 300 4000:10:20:30:40:5 System Summary LengthROUGE-1ICSI R S T Figure 1 : R-1 scores of a few systems , evaluated against the 50 - word reference set of DUC 01 .", "entities": []}, {"text": "Systems R , S and T are from DUC 01 ; ICSISumm is a later competitive system ( Gillick et al . , 2008 ) .", "entities": []}, {"text": "lated by Only50 , at each system summary length .", "entities": []}, {"text": "We \ufb01nd very high correlations ( above 0.95 for all system summary lengths , in both datasets ) when using multiple references and slightly lower ( 0.85 to 0.9 ) with one reference summary .", "entities": []}, {"text": "These \ufb01gures show that the Only50 con\ufb01guration ranks systems very similarly to Standard .", "entities": []}, {"text": "To further verify our results , we computed correlations in two additional settings .", "entities": []}, {"text": "First , we conducted the same analysis , excluding 2 - 3 of the worst systems , which might arti\ufb01cially boost the correlation ( Rankel et al . , 2013 ) .", "entities": []}, {"text": "Second , we computed score differences between all pairs of systems , for both human and ROUGE scores , and computed the correlation between these two sets of differences ( Rankel et al . , 2011 ) .", "entities": []}, {"text": "In both cases we observed rather consistent results , assessing that a single set of short reference summaries evaluates system summaries of different lengths just as well as the standard con\ufb01guration .", "entities": []}, {"text": "3 Cross - length Summary Evaluation This section illustrates how system performances can be measured and compared when evaluating them on outputs of varying lengths against a single reference point .", "entities": []}, {"text": "Figure 1 presents the ROUGE scores of the Only50 con\ufb01guration for three DUC01 submitted systems , and for ICSISumm ( Gillick et al . , 2008 ) , a later competitive system .", "entities": []}, {"text": "As expected when measuring ROUGE Recall against a \ufb01xed reference length , longer system summaries typically cover more of the reference summaries content than shorter ones , yielding higher scores .", "entities": [[5, 6, "MetricName", "Recall"]]}, {"text": "Yet , it can be noted , for example , that the value of the 400 - word summary of system R in the \ufb01gure is lower than that of the 200 - word summaries of the other systems .", "entities": []}, {"text": "Such a compar - ison is impossible in the standard setup , as each system length is evaluated against different reference summaries .", "entities": []}, {"text": "We note that similar comparisons are embedded in the evaluations of Steinberger and Jezek ( 2004 ) and", "entities": []}, {"text": "Kikuchi et al . ( 2016 ) , who also evaluated multiple summary lengths .", "entities": []}, {"text": "Further , one can de\ufb01ne the marginal value of longer summaries of a given system as the ROUGE score increase per number of additional words , namely the graph slope .", "entities": []}, {"text": "This denotation allows measuring the effectiveness of producing longer summaries .", "entities": []}, {"text": "For example , deploying system R , we might decide to output only summaries no longer than 200 words , since the marginal value of longer summaries becomes too small .", "entities": []}, {"text": "The other systems , on the other hand , seem marginally effective also in 400 word summaries .", "entities": []}, {"text": "4 Discussion We proposed the potential value of evaluating summarization systems at different summary lengths .", "entities": [[9, 10, "TaskName", "summarization"]]}, {"text": "Such evaluations would allow proper evaluation of systems \u2019 \u201c length knob \u201d , tracking how their ranking changes across summary lengths as well as tracking the cross - length behavior of individual systems .", "entities": []}, {"text": "Given that reference summaries of a single length are usually available in practice , we analyzed the potential use of reference summaries of a single length for evaluating system summaries of multiple lengths .", "entities": []}, {"text": "We found , on the only two datasets readily available for such analysis , that this con\ufb01guration is as reliable as the standard con\ufb01guration , which evaluates each system summary against a reference of a matching length .", "entities": []}, {"text": "To broadly substantiate our \ufb01ndings , we propose future work that would follow our assessment methodology over test samples from current datasets ( e.g. CNN / DailyMail ) , judging performance of current systems and utilizing current manual evaluation protocols .", "entities": []}, {"text": "This would require preparing , for limited samples , additional manually crafted summaries of several lengths and manually evaluating system summaries of corresponding lengths .", "entities": []}, {"text": "Using such data , it will be possible to repeat our analysis and test the broader validity of the single - reference - length con\ufb01guration .", "entities": []}, {"text": "If broadly assessed , it will be possible to start evaluating system summaries of multiple lengths over most currently available datasets , leveraging the available single - length reference summaries .", "entities": []}, {"text": "Fu-", "entities": []}, {"text": "778ture benchmarks could require systems to produce different length outputs , while feasibly evaluating them using the existing , single length , reference summaries .", "entities": []}, {"text": "This , in turn , is likely to drive research to better address the need for producing high quality summaries \ufb02exibly across a range of summary lengths , a dimension that has been disregarded for long .", "entities": []}, {"text": "Acknowledgments We would like to thank the anonymous reviewers for their constructive comments .", "entities": []}, {"text": "We thank Yinfei Yang for his assistance in producing the ICSISumm summaries that we utilized in our analysis .", "entities": []}, {"text": "This work was supported in part by grants from the MAGNET program of the Israel Innovation Authority ; by the German Research Foundation through the German - Israeli Project Cooperation ( DIP , grant DA 1600/1 - 1 ) ; by the BIU Center for Research in Applied Cryptography and Cyber Security in conjunction with the Israel National Cyber Bureau in the Prime Ministers Of\ufb01ce ; and by the Israel Science Foundation ( grants 1157/16 and 1951/17 ) .", "entities": []}, {"text": "References Angela Fan , David Grangier , and Michael Auli . 2017 .", "entities": []}, {"text": "Controllable abstractive summarization .", "entities": [[2, 3, "TaskName", "summarization"]]}, {"text": "CoRR , abs/1711.05217 .", "entities": []}, {"text": "Jessica Ficler and Yoav Goldberg . 2017 .", "entities": []}, {"text": "Controlling linguistic style aspects in neural language generation .", "entities": []}, {"text": "In Proceedings of the Workshop on Stylistic Variation , pages 94\u2013104 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Daniel Gillick , Benoit Favre , and Dilek Hakkani - T \u00a8ur . 2008 .", "entities": []}, {"text": "The ICSI Summarization System at TAC 2008 .", "entities": [[2, 3, "TaskName", "Summarization"]]}, {"text": "In TAC .", "entities": []}, {"text": "David Graff , Junbo Kong , Ke Chen , and Kazuaki Maeda . 2003 .", "entities": []}, {"text": "English gigaword .", "entities": []}, {"text": "Linguistic Data Consortium , Philadelphia , 4:1 .", "entities": []}, {"text": "Yuta Kikuchi , Graham Neubig , Ryohei Sasano , Hiroya Takamura , and Manabu Okumura . 2016 .", "entities": []}, {"text": "Controlling output length in neural encoder - decoders .", "entities": []}, {"text": "InProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1328\u20131338 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chin - Yew Lin .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Summary evaluation environment user guide .", "entities": []}, {"text": "http://www1.cs .", "entities": []}, {"text": "columbia.edu/nlp/tides/SEEManual . pdf .", "entities": []}, {"text": "Chin - Yew Lin .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "ROUGE :", "entities": []}, {"text": "A package for automatic evaluation of summaries .", "entities": []}, {"text": "Text Summarization Branches Out .", "entities": [[0, 2, "TaskName", "Text Summarization"]]}, {"text": "Annie Louis and Ani Nenkova .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Automatically assessing machine summary content without a gold standard .", "entities": []}, {"text": "Computational Linguistics , 39(2):267 \u2013 300 .", "entities": []}, {"text": "Ramesh Nallapati , Bowen Zhou , C \u00b4 \u0131cero Nogueira dos Santos , C \u00b8 aglar G \u00a8ulc \u00b8ehre , and Bing Xiang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Abstractive text summarization using sequence - tosequence rnns and beyond .", "entities": [[0, 3, "TaskName", "Abstractive text summarization"]]}, {"text": "In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning , CoNLL 2016 , Berlin , Germany , August 11 - 12 , 2016 , pages 280\u2013290 .", "entities": []}, {"text": "Ani Nenkova , Rebecca Passonneau , and Kathleen McKeown . 2007 .", "entities": []}, {"text": "The pyramid method : Incorporating human content selection variation in summarization evaluation .", "entities": [[10, 11, "TaskName", "summarization"]]}, {"text": "ACM Transactions on Speech and Language Processing ( TSLP ) , 4(2):4 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "NIST .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Responsiveness assessment instructions .", "entities": []}, {"text": "www-nlpir.nist.gov/projects/duc/ duc2006 / responsiveness.assessment .", "entities": []}, {"text": "instructions .", "entities": []}, {"text": "NIST .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Document Understanding Conferences .", "entities": []}, {"text": "https://duc.nist.gov/ .", "entities": []}, {"text": "NIST .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Text Analysis Conferences .", "entities": []}, {"text": "https:// tac.nist.gov/ .", "entities": []}, {"text": "Paul Over , Hoa Dang , and Donna Harman .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "DUC in context .", "entities": []}, {"text": "Information Processing & Management , 43(6):1506\u20131520 .", "entities": [[3, 4, "TaskName", "Management"]]}, {"text": "Paul Over and Yen James .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "An Introduction to DUC 2004 Intrisic Evaluation of Generic News Text Summarization Systems .", "entities": [[3, 5, "DatasetName", "DUC 2004"], [10, 12, "TaskName", "Text Summarization"]]}, {"text": "duc.nist.gov/", "entities": []}, {"text": "pubs/2004slides / duc2004.intro.pdf .", "entities": []}, {"text": "Maxime Peyrard , Teresa Botschen , and Iryna Gurevych . 2017 .", "entities": []}, {"text": "Learning to score system summaries for better content selection evaluation .", "entities": []}, {"text": "InProceedings of the EMNLP 2017 Workshop on New Frontiers in Summarization , pages 74\u201384 .", "entities": [[10, 11, "TaskName", "Summarization"]]}, {"text": "Peter Rankel , John M Conroy , Eric V Slud , and Dianne P O\u2019Leary .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Ranking human and machine summarization systems .", "entities": [[4, 5, "TaskName", "summarization"]]}, {"text": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 467\u2013473 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Peter A Rankel , John M Conroy , Hoa Trang Dang , and Ani Nenkova .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "A decade of automatic content evaluation of news summaries : Reassessing the state of the art .", "entities": []}, {"text": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , volume 2 , pages 131\u2013136 .", "entities": []}, {"text": "Josef Steinberger and Karel Jezek .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Using latent semantic analysis in text summarization and summary evaluation .", "entities": [[5, 7, "TaskName", "text summarization"]]}, {"text": "Proc .", "entities": []}, {"text": "ISIM , 4:93\u2013100 .", "entities": []}]