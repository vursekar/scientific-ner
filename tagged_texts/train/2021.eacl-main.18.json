[{"text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics , pages 234\u2013243 April 19 - 23 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics234Non - Autoregressive Text Generation with Pre - trained Language Models Yixuan Su}Deng Cai ~ Yan Wang\u007fDavid Vandyke|", "entities": [[8, 10, "TaskName", "Text Generation"]]}, {"text": "Simon Baker}Piji", "entities": []}, {"text": "Li\u007fNigel Collier } } Language Technology Lab , University of Cambridge ~The Chinese University of Hong Kong \u007fTencent AI Lab |Apple fys484,sb895,nhc30 g@cam.ac.uk thisisjcykcd@gmail.com , dvandyke@apple.com fbrandenwang , pijili g@tencent.com", "entities": [[10, 11, "DatasetName", "Cambridge"]]}, {"text": "Abstract Non - autoregressive generation ( NAG ) has recently attracted great attention due to its fast inference speed .", "entities": []}, {"text": "However , the generation quality of existing NAG models still lags behind their autoregressive counterparts .", "entities": []}, {"text": "In this work , we show that BERT can be employed as the backbone of a NAG model to greatly improve performance .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "Additionally , we devise mechanisms to alleviate the two common problems of vanilla NAG models : the in\ufb02exibility of pre\ufb01xed output length and the conditional independence of individual token predictions .", "entities": []}, {"text": "Lastly , to further increase the speed advantage of the proposed model , we propose a new decoding strategy , ratio-\ufb01rst , for applications where the output lengths can be approximately estimated beforehand .", "entities": []}, {"text": "For a comprehensive evaluation , we test the proposed model on three text generation tasks , including text summarization , sentence compression and machine translation .", "entities": [[12, 14, "TaskName", "text generation"], [17, 19, "TaskName", "text summarization"], [20, 22, "DatasetName", "sentence compression"], [23, 25, "TaskName", "machine translation"]]}, {"text": "Experimental results show that our model signi\ufb01cantly outperforms existing non - autoregressive baselines and achieves competitive performance with many strong autoregressive models .", "entities": []}, {"text": "In addition , we also conduct extensive analysis experiments to reveal the effect of each proposed component.1 1 Introduction Autoregressive generation ( AG ) models achieve state - of - the - art performance on a wide range of text generation tasks , such as machine translation ( Vaswani et al . , 2017 ) and text summarization ( Rush et al . , 2015 ) .", "entities": [[39, 41, "TaskName", "text generation"], [45, 47, "TaskName", "machine translation"], [56, 58, "TaskName", "text summarization"]]}, {"text": "Such models generate a token sequence in a left - to - right , token - by - token fashion .", "entities": []}, {"text": "The prediction for the next token is conditioned on all previously generated tokens .", "entities": []}, {"text": "This characteristic makes it impossible to parallelize the computational overhead for token predictions in different 1All related code , data , and models can be found in https://github.com/yxuansu/NAG-BERT.positions , which leads to a relatively high latency in inference .", "entities": []}, {"text": "On the other hand , non - autoregressive generation ( NAG ) models ( Gu et al . , 2018 ) have emerged as a promising alternative due to their fast inference speed .", "entities": []}, {"text": "NAG models omit the sequential dependencies within the output - side sequence and predict tokens in all positions simultaneously once the output length has been determined beforehand .", "entities": []}, {"text": "While NAG models enjoy full parallelism and faster inference , the generation quality of NAG models often lags behind their autoregressive counterparts .", "entities": []}, {"text": "In this work , we explore the potential of largescale pre - trained language models for improving the performance of non - autoregressive generation .", "entities": []}, {"text": "Speci\ufb01cally , we utilize BERT ( Devlin et al . , 2019 ) as the backbone for NAG modelling and extend the architecture of BERT with a CRF output layer ( Lafferty et al . , 2001 ; Sun et al . , 2019 ) for better capturing the output - side dependencies .", "entities": [[4, 5, "MethodName", "BERT"], [24, 25, "MethodName", "BERT"], [27, 28, "MethodName", "CRF"]]}, {"text": "In addition , we analyze two signi\ufb01cant limitations that NAG models currently suffer from : ( 1 ) the in\ufb02exibility of pre\ufb01xed output length , and ( 2 ) the conditional independence of individual token predictions .", "entities": []}, {"text": "Accordingly , we devise two solutions to these two problems .", "entities": []}, {"text": "First , prior NAG models require the output length to be determined before token generation , thus an extra module for output length prediction is always required .", "entities": []}, {"text": "Nevertheless , the most likely length from the prediction module is not necessarily the best - suited one for the token generation model .", "entities": []}, {"text": "To this end , previous works ( Gu et al . , 2018 ; Ma et al . , 2019 ) usually rely on length - parallel decoding ( LPD ) ( Wei et al . , 2019 ) for performance enhancement ; that is , generating and re - ranking the results from different output length candidates .", "entities": []}, {"text": "In this work , we propose a simple and elegant decoding mechanism that lets the model determine the output length on - the-\ufb02y .", "entities": []}, {"text": "Speci\ufb01cally , our model dynamically adjusts the output sequence length via", "entities": []}, {"text": "235emitting an [ eos ] token at any output position to indicate the ending of the generated sequence .", "entities": []}, {"text": "Therefore , we can avoid the additional efforts of output length prediction and results re - ranking .", "entities": []}, {"text": "Second , most existing NAG models assume the token predictions in different positions are conditionally independent .", "entities": []}, {"text": "As a consequence , they often tend to generate results that are ungrammatical with repetitions ( Wang et al . , 2019b ) .", "entities": []}, {"text": "To alleviate this problem , we propose a context - aware learning objective which impels the model to output different tokens at adjacent positions , thereby reducing the possibility of repetitive generation .", "entities": []}, {"text": "Furthermore , for tasks like text summarization , the output sequence ( summary ) is known to be shorter than the source sequence ( article ) .", "entities": [[5, 7, "TaskName", "text summarization"]]}, {"text": "In such cases , to further improve the model \u2019s inference ef\ufb01ciency , we introduce a new ratio-\ufb01rst decoding strategy .", "entities": []}, {"text": "Speci\ufb01cally , instead of performing inference on all source - side hidden states , ratio-\ufb01rst generates the result only based on a subset of source hidden states .", "entities": []}, {"text": "The subset size is jointly determined by the source length Tand a prede\ufb01ned ratio \u000b  that is set based on our prior knowledge from the data statistics .", "entities": []}, {"text": "In the experiments , we show that ratio-\ufb01rst can signi\ufb01cantly improve the inference speed while maintaining the generation quality .", "entities": []}, {"text": "We evaluate the proposed model on three typical text generation tasks , including text summarization , sentence compression and machine translation .", "entities": [[8, 10, "TaskName", "text generation"], [13, 15, "TaskName", "text summarization"], [16, 18, "DatasetName", "sentence compression"], [19, 21, "TaskName", "machine translation"]]}, {"text": "Experimental results show that our model signi\ufb01cantly outperforms many strong non - autoregressive baselines , and even performs competitively with several strong autoregressive models .", "entities": []}, {"text": "In addition , we conduct extensive analysis experiments to study the effect of individual proposed components .", "entities": []}, {"text": "In summary , our contributions are : ( 1 ) We propose a novel framework that utilizes BERT for text generation under the non - autoregressive generation paradigm ; ( 2 ) We propose a decoding mechanism that allows the model to dynamically determine the output length , and a new context - aware learning objective that reduces errors stemming from the output - side conditional independence assumption ; ( 3 ) We introduce a ratio-\ufb01rst decoding strategy that further improve the model \u2019s inference ef\ufb01ciency .", "entities": [[17, 18, "MethodName", "BERT"], [19, 21, "TaskName", "text generation"]]}, {"text": "2 Background Autoregressive generation ( AG ) models generate sequences based on a left - to - right factorization .", "entities": []}, {"text": "As shown in Figure 1 , given the source sequence X , Figure 1 : ( a ) Autoregressive ; ( b ) Non - Autoregressive the target sequence Ywith lengthT0is generated via a chain of conditional probabilities based on the left - to - right sequential dependencies as : p(YjX )", "entities": []}, {"text": "= T0Y i=1p(yijy < i;X ) ; ( 1 ) wherey < idenotes the tokens before the i - th step .", "entities": []}, {"text": "This property of autoregressive factorization makes the generation process hard to be parallelized as the result is generated token by token .", "entities": []}, {"text": "Unlike AG models , non - autoregressive ( NAG ) models generate sequences without modelling the output - side dependencies .", "entities": []}, {"text": "As shown in Figure 1 , given the prespeci\ufb01ed output length T0 , the probability of the target sequence Yis then modelled as : p(YjX ) = T0Y i=1p(yijX;i;T0 ): ( 2 ) With this conditional independence assumption , NAG models can fully parallelize their generation process , which signi\ufb01cantly improves the inference speed .", "entities": []}, {"text": "However , it has been shown that , the choice of the prespeci\ufb01ed output length has a notable impact on the model \u2019s generation quality ( Gu et al . , 2018 ) .", "entities": []}, {"text": "In addition , the removal of output - side sequential dependency also causes the generation quality of NAG models to be inferior to their autoregressive counterparts ( Wang et al . , 2019b ) .", "entities": []}, {"text": "3 Proposed Model", "entities": []}, {"text": "In this section , we give a detailed explanation of the proposed model .", "entities": []}, {"text": "First , we describe how to utilize BERT as a non - autoregressive generation model .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "Then we discuss the decoding mechanism which allows the model to determine the output length dynamically .", "entities": []}, {"text": "Finally , we introduce the new ratio\ufb01rst decoding strategy which further improves the model \u2019s decoding ef\ufb01ciency .", "entities": []}, {"text": "236 Figure 2 : The overall illustration of the proposed model : During training , the model parameters are only updated on the positions of the target sequence .", "entities": []}, {"text": "During inference , once the decoded trajectory ( colored in red ) gets into the [ eos ] state , it will only transit to the [ eos ] state in the remaining steps .", "entities": []}, {"text": "The \ufb01nal result is obtained by removing the generated [ eos ] tokens from the entire decoded trajectory .", "entities": []}, {"text": "3.1 Model Architecture The architecture of the proposed model is presented in Figure 2 , in which the embedding layer and the stack of transformer layers are initialized with BERT ( Devlin et al . , 2019 ) .", "entities": [[29, 30, "MethodName", "BERT"]]}, {"text": "Input Representation Following the setup of BERT , we \ufb01rst append a [ cls ] and a [ sep ] token on both sides of the source sequence .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "Then we attach a number of [ pad ] tokens at the end of source sequence to make its length equal to the prede\ufb01ned maximum size ( e.g. , 256 ) .", "entities": []}, {"text": "Thus we can make sure the source length is longer than or equal to the output length .", "entities": []}, {"text": "As a special case , for tasks like text summarization where the source is known to be longer than the target , we do not attach the [ pad ] tokens when constructing the input .", "entities": [[8, 10, "TaskName", "text summarization"]]}, {"text": "Transformer Layers Given the source sequence X , it is processed by a stack of Ntransformer ( Vaswani et al . , 2017 ) layers .", "entities": [[0, 1, "MethodName", "Transformer"]]}, {"text": "Formally , the MultiHead Attention is de\ufb01ned as MultiHead ( Q;K;V ) , whereQ , K , Vdenotes the query , key and value respectively .", "entities": []}, {"text": "The computation of the \ufb01rst transformer layer is then de\ufb01ned as : V(1)=MultiHead ( E(X);E(X);E(X));(3 ) O(1)=FFN(V(1 ) ) ; ( 4 ) FFN(x ) = max(0;xW 1+b1)W2+b2;(5 ) whereE(X )", "entities": []}, {"text": "= TE(X)+PE(X)in whichTE(\u0001 ) denotes the token embedding and PE(\u0001)denotes the position embedding .", "entities": []}, {"text": "For other layers : V(n)=MultiHead ( O(n\u00001);O(n\u00001);O(n\u00001 ) ) ; ( 6 ) O(n)=FFN(V(n ) ) ; ( 7)wheren= 2;:::;N andNis the total number of transformer layers .", "entities": []}, {"text": "The \ufb01nal sequence representationH2RT\u0002dmodel is the output states of BERT from the last layer , where Tis the source sequence length anddmodel is the model size .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "CRF Layer Then , His passed through a linearchain CRF ( Lafferty et al . , 2001 ) .", "entities": [[0, 1, "MethodName", "CRF"], [9, 10, "MethodName", "CRF"]]}, {"text": "Under the CRF framework , the likelihood of the target sequence Y with lengthT0is then modelled as : PCRF(YjX ) = eS(X;Y ) P Y0eS(X;Y0 )", "entities": [[2, 3, "MethodName", "CRF"]]}, {"text": "= 1 Z(X)exp(T0X i=1\byi(hi )", "entities": []}, {"text": "+ T0X i=2t(yi\u00001;yi ) ) ; ( 8) whereZ(X)is the normalizing factor and \byi(hi ) denotes the label score of yiat positioni .", "entities": []}, {"text": "In practice,\bis parameterized by a neural network that maps the BERT output state hiinto the label ( vocabulary ) space .", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "The t(yi\u00001;yi ) = Tyi\u00001;yidenotes the transition score from label yi\u00001toyi whereT2RjVj\u0002jVjis the transition matrix .", "entities": []}, {"text": "Approximation", "entities": []}, {"text": "In the context of text generation , the size of the label space ( vocabulary size ) jVjis typically large , e.g. , 32k .", "entities": [[4, 6, "TaskName", "text generation"]]}, {"text": "Therefore , it is intractable to directly model the transition matrix T and the normalizing factor Z(X ) .", "entities": []}, {"text": "To this end , we adopt the techniques proposed by Sun et al .", "entities": []}, {"text": "( 2019 ) to approximate these two terms .", "entities": []}, {"text": "Speci\ufb01cally , the full transition matrix is approximated by the product of two low - rank matrices T = E1ET 2 , where E1;E22RjVj\u0002danddis much smaller than jVj .", "entities": []}, {"text": "To compute the normalizing factor Z(X ) , at each", "entities": []}, {"text": "237time step , instead of searching through all possible paths , the number of candidates is heuristically truncated to a prede\ufb01ned beam size k. We refer readers to the original paper for further details .", "entities": []}, {"text": "3.2 Output Length Determination In this section , we describe how to let the model determine the output sequence length by itself .", "entities": []}, {"text": "Our basic idea is that we want the model to dynamically stop generation via emitting a special [ eos ] token .", "entities": []}, {"text": "To achieve this , during training , we manually append twoconsecutive", "entities": []}, {"text": "[ eos ] tokens to the end of the target sequence , as shown in the top left part of Figure 2 .", "entities": []}, {"text": "In this way , the model can learn a deterministic transition behaviour between two [ eos ] states , meaning that t([eos];[eos ] )", "entities": []}, {"text": "= maxv2Vt([eos];v ) .", "entities": []}, {"text": "This is because , during training , the model never sees a transition ( [ eos ] , v ) , wherev6=[eos ] .", "entities": []}, {"text": "During inference , the result ~Yis acquired as ~Y= arg maxY0S(X;Y0 ) , where the CRF scoring function S(X;Y0)in Equation ( 8)can be decomposed as : S(X;Y0 ) = TX i=1\by0 i(hi ) + TX i=2t(y0", "entities": [[15, 16, "MethodName", "CRF"]]}, {"text": "i\u00001;y0 i )", "entities": []}, {"text": "= \by0 1(h1 ) |{z } initial state+TX i=2flabel scorez}| { \by0 i(hi )", "entities": []}, {"text": "+ transition scorez}| { t(y0 i\u00001;y0 i ) | { z } state transitiong:(9 )", "entities": []}, {"text": "Once the decoded trajectory enters the [ eos ] state , the state transition term in S(X;Y0 ) will be dominated by the transition score term t([eos];[eos ] ) .", "entities": []}, {"text": "As a result , the model will keep transitioning to [ eos ] in the remaining steps .", "entities": []}, {"text": "An example is provided in the right part of Figure 2 , from which we can see that , at step 5 , the decoded trajectory enters the [ eos ] state and remains at it in the rest of the generation process .", "entities": []}, {"text": "In this way , our model can dynamically control the length of output sequence by entering the [ eos ] state during the generation process .", "entities": []}, {"text": "After the entire generation process is completed , the \ufb01nal output sequence can be obtained by removing all generated [ eos ] tokens .", "entities": []}, {"text": "3.3 Ratio - First Decoding We note that the outputs of BERT can be divided into two subsets .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "The \ufb01rst subset ranges from the beginning to the position where the \ufb01rst [ eos ] is emitted , and the second subset is the rest .", "entities": []}, {"text": "For example , in Figure 2 , the \ufb01rst subset are those corresponding to the output sequence \u201c y(1)y(2)y(3)y(4)[eos ] \u201d .", "entities": []}, {"text": "As for the second part , we can see that it has little effect on the \ufb01nal output and removing it should not change the result .", "entities": []}, {"text": "This indicates that it suf\ufb01ces to only consider the beginning part of BERT outputs for improving the inference speed .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "Especially , for tasks like summarization where the target is known to be shorter than the source sequence , we are safe to only use the \ufb01rst [ \u000b \u0001T ] outputs of BERT to perform inference .", "entities": [[5, 6, "TaskName", "summarization"], [33, 34, "MethodName", "BERT"]]}, {"text": "Here Tdenotes the source length , \u000b 2(0:0;1:0)is set based on the data statistics and [ \u0001]is the integer rounding operation .", "entities": []}, {"text": "Formally , given the source sequence X , the ratio-\ufb01rst decoding is de\ufb01ned as ~Y= arg max Y0F(X;Y0 ; \u000b ) ; = arg max Y0f [ \u000b \u0001T]X i=1\by0 i(hi )", "entities": []}, {"text": "+ [ \u000b \u0001T]X i=2t(y0 i\u00001;y0 i)g : ( 10 )", "entities": []}, {"text": "When \u000b = 1:0 , ratio-\ufb01rst degenerates to the standard decoding strategy in CRF - based models .", "entities": [[13, 14, "MethodName", "CRF"]]}, {"text": "It should be noted that , [ \u000b \u0001T]only constrains the maximum length of the generated result , and the actual output length ( after removing the generated[eos ] tokens ) is still decided by the model itself .", "entities": []}, {"text": "In the experiment section , we demonstrate that ratio-\ufb01rst can notably improve the inference speed whilst maintaining the generation quality .", "entities": []}, {"text": "4", "entities": []}, {"text": "Learning Due to the conditional independence approximation on output tokens , NAG models often tend to generate repeated tokens ( Wang et al . , 2019b ) .", "entities": []}, {"text": "One way to alleviate this problem is to introduce implicit dependencies on the output side .", "entities": []}, {"text": "In this work , we propose to use the unlikelihood formulation of Welleck et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) in the context of NAG , where we de\ufb01ne the set of negative candidate as the surrounding tokens within a prede\ufb01ned context windowc .", "entities": []}, {"text": "Formally , given the source sequence Xand the target sequence Ywith length T0 , the proposed context - aware objective is de\ufb01ned as : LCA(YjX ) = \u0000T0X i=1flogp\u0012(yijhi;X )", "entities": []}, {"text": "+ lCA(i)g ; lCA(i ) = j = i+cX j = i\u0000c;yj6 = yilog(1:0\u0000p\u0012(yjjhi;X ) ) ; ( 11 )", "entities": []}, {"text": "238wherehiis the model output state at position i.", "entities": []}, {"text": "At position i , the proposed objective maximizes the probability of token yiwhile minimizing the probabilities of the surrounding tokens .", "entities": []}, {"text": "In this way , it discourages the model from generating repetitive tokens at different time steps .", "entities": []}, {"text": "The overall learning objective is then de\ufb01ned as LCRF=\u0000logPCRF(YjX ) ;", "entities": []}, {"text": "L = LCRF+\u0015\u0001LCA;(12 ) where\u0015controls the importance of different loss terms andPCRF(YjX)is described in Equation ( 8) . 5 Related Work Non - Autoregressive generation was \ufb01rst introduced by Gu et al .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "( 2018 ) to reduce the inference latency in machine translation .", "entities": [[9, 11, "TaskName", "machine translation"]]}, {"text": "Recent works in this area have investigated ways to mitigate the tradeoff between the decoding speed and generation quality .", "entities": []}, {"text": "Gu et al .", "entities": []}, {"text": "( 2018 ) utilized fertility as latent variables for better translation performance .", "entities": []}, {"text": "Wang et al .", "entities": []}, {"text": "( 2019b ) proposed two auxiliary objectives for better modelling the output states and solving the under - translation problem .", "entities": []}, {"text": "To better model the intermediate alignments between source and target sides , Ma et al .", "entities": []}, {"text": "( 2019 ) proposed a model based on the generative \ufb02ow framework .", "entities": []}, {"text": "Ghazvininejad et al .", "entities": []}, {"text": "( 2019 ) proposed to use a masked language objective to train the NAG model .", "entities": []}, {"text": "During inference , starting from a fully masked sequence , the output is generated in an iterative re\ufb01nement manner .", "entities": []}, {"text": "Recently , Sun et al .", "entities": []}, {"text": "( 2019 ) proposed to incorporate a conditional random \ufb01eld into the decoder of a NAG model for better modelling the outputside dependencies .", "entities": []}, {"text": "Our work is different from prior works in two aspects : ( 1 ) we directly utilize a pretrained language model ( BERT ) to perform nonautoregressive generation ; ( 2 ) our model can dynamically generate the output sequence without the need of prespeci\ufb01ed output length .", "entities": [[22, 23, "MethodName", "BERT"]]}, {"text": "6 Experiments We evaluate the proposed model on three typical text generation tasks : ( 1 ) text summarization ; ( 2 ) sentence compression and ( 3 ) machine translation .", "entities": [[10, 12, "TaskName", "text generation"], [17, 19, "TaskName", "text summarization"], [23, 25, "DatasetName", "sentence compression"], [29, 31, "TaskName", "machine translation"]]}, {"text": "6.1 Experimental Setup We implement the proposed model with PyTorch ( Paszke et al . , 2017 ) .", "entities": []}, {"text": "The BERT model we use is the Huggingface implementation ( Wolf et al . , 2019 ) ( bert - base - uncased ) .", "entities": [[1, 2, "MethodName", "BERT"]]}, {"text": "To approximate the transitionmatrix in the CRF layer , we set the dimension d of matrices E1andE2as 32 .", "entities": [[6, 7, "MethodName", "CRF"]]}, {"text": "For the normalizing factorZ(X ) , we set the prede\ufb01ned beam size kas 256 .", "entities": []}, {"text": "As for the overall learning objective , we set the window size cas3and\u0015as1:0 .", "entities": []}, {"text": "In training , we use Adam optimizer ( Kingma and Ba , 2015 ) .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"]]}, {"text": "To measure the relative speedup , we follow the standard setup which runs inference for each individual example separately .", "entities": []}, {"text": "The model \u2019s inference speed is computed by averaging the results of test cases .", "entities": []}, {"text": "For a fair comparison , we measure the inference speed of all models on the same platform .", "entities": []}, {"text": "6.2 Text Summarization Text summarization aims to automatically generate a compact summary that retains the most important content of the original text document ( Nenkova and McKeown , 2012 ) .", "entities": [[1, 3, "TaskName", "Text Summarization"], [3, 5, "TaskName", "Text summarization"]]}, {"text": "In this experiment , we use the Gigawords dataset ( Rush et al . , 2015 ) as our benchmark .", "entities": []}, {"text": "For evaluation , standard metrics including ROUGE-1 ( R-1 ) , ROUGE-2 ( R-2 ) and ROUGE - L ( R - L ) ( Lin , 2004 ) are reported .", "entities": [[6, 7, "MetricName", "ROUGE-1"], [11, 12, "MetricName", "ROUGE-2"], [16, 19, "MetricName", "ROUGE - L"]]}, {"text": "We compare our model with several representative and the latest NAG models , including NAGNMT ( Gu et al . , 2018 ) , NAR - REG ( Wang et al . , 2019b ) and NAG - CRF ( Sun et al . , 2019 ) .", "entities": [[24, 25, "DatasetName", "NAR"], [38, 39, "MethodName", "CRF"]]}, {"text": "Following previous works , during training , we train a length predictor to predict the output length .", "entities": []}, {"text": "During inference , for each NAG baseline , we adopt the length - parallel decoding strategy ( LPD- k ) ( Wei et al . , 2019 ) , that is , generating kresults using the top - kpossible output length predictions from the length predictor .", "entities": []}, {"text": "The results are then re - ranked by a transformer model to get the \ufb01nal ouput .", "entities": []}, {"text": "In the experiment , we report the results of different NAG baselines using LPD- 9decoding .", "entities": []}, {"text": "In addition , to better examine the effect of using BERT in NAG models , we add a BNAG - CRF baseline which adopts the same structure of the NAG - CRF model but using BERT as the encoder .", "entities": [[10, 11, "MethodName", "BERT"], [20, 21, "MethodName", "CRF"], [31, 32, "MethodName", "CRF"], [35, 36, "MethodName", "BERT"]]}, {"text": "We also compare our model with several strong autoregressive models , which are Luong - NMT ( Luong et al . , 2015 ) , Pointer - Generator ( See et al . , 2017 ) , DRGD ( Li et al . , 2017 ) and Concept Pointer ( Wang et al . , 2019a ) .", "entities": []}, {"text": "To measure the relative inference speedup , we include transformer as a baseline model .", "entities": []}, {"text": "The results are shown in Table 1 , from which we can see that , by using length - parallel decoding , the performance of all NAG baselines can be notably improved .", "entities": []}, {"text": "However , such procedure significantly increases the inference latency .", "entities": []}, {"text": "In contrast ,", "entities": []}, {"text": "239Models R-1 R-2 R - L Speedup Autoregressive Luong - NMT 33.10 14.45 30.71 Pointer - Generator 35.98 15.99 33.33 DRGD 36.25 17.61 33.55 Concept Pointer 36.62 16.40 33.98 Transformer ( b = 4)35.74 16.97 33.43 1.00\u0002 Non - Autoregressive NAG - NMT 27.20 8.96 25.58 9.31\u0002 + LPD-9 29.76 10.03 28.04 5.28\u0002 NAR - REG 28.56 9.79 26.83 8.64\u0002 + LPD-9 31.23 11.14 29.55 4.74\u0002 NAG - CRF 30.29 12.61 28.71 8.07\u0002", "entities": [[29, 30, "MethodName", "Transformer"], [53, 54, "DatasetName", "NAR"], [68, 69, "MethodName", "CRF"]]}, {"text": "+ LPD-9 32.91 14.31 31.03 4.32\u0002 BNAG - CRF 32.63 14.32 30.82 6.13\u0002 + LPD-9 34.56 16.10 32.76 3.21\u0002", "entities": [[8, 9, "MethodName", "CRF"]]}, {"text": "Ours ( \u000b = 0:3 ) 34.67 16.13 32.81 9.31\u0002 Ours ( \u000b = 1:0 ) 35.05 16.48 33.28 6.72\u0002 Table 1 : Results on Gigawords dataset , where bin the transformer baseline stands for beam search size .", "entities": []}, {"text": "our model can self - determine the output length without any re - ranking process .", "entities": []}, {"text": "As shown in the results , our model outperforms the best NAG baseline ( with LPD ) and achieves performances that are comparable with several strong AG models .", "entities": []}, {"text": "Comparing the results of BNAG - CRF and NAGCRF , we can see that incorporating BERT as encoder helps to improve the model performance .", "entities": [[6, 7, "MethodName", "CRF"], [15, 16, "MethodName", "BERT"]]}, {"text": "Nonetheless , our model still outperforms BNAGCRF with LPD-9 decoding .", "entities": []}, {"text": "This is because the dynamic length decoding mechanism allows our model to generate results with optimal length , leading to stronger model performances .", "entities": []}, {"text": "Finally , we analyze the proposed ratio-\ufb01rst decoding .", "entities": []}, {"text": "From the results , we observe a moderate performance drop when using ratio-\ufb01rst ( \u000b = 0:3 ) .", "entities": []}, {"text": "It comes from the fact that , for some input documents with length T , the reference summary is longer than [ \u000b \u0001T ] .", "entities": []}, {"text": "In such cases , ratio-\ufb01rst fails to generate the complete reference summary , leading to the drop of performance .", "entities": []}, {"text": "On the other hand , we can see that , ratio-\ufb01rst can notably improve the inference speedup .", "entities": []}, {"text": "With \u000b = 0:3 , our model achieves the highest inference speedup while still outperforms all compared NAG models .", "entities": []}, {"text": "6.3 Sentence Compression Sentence compression aims at compressing a long sentence into a short one by deleting redundant words .", "entities": [[1, 3, "DatasetName", "Sentence Compression"], [3, 5, "DatasetName", "Sentence compression"]]}, {"text": "In this experiment , we use the Google sentence compression dataset ( Filippova and Altun , 2013 ) as our benchmark .", "entities": [[7, 8, "DatasetName", "Google"], [8, 10, "DatasetName", "sentence compression"]]}, {"text": "For evaluation , we useModels F1 R-1 R-2 R - L Speedup Autoregressive Bi - LSTM - Dep 82.3 81.5 74.1 81.3 Tagger 82.8 81.1 72.4 80.9 Tagger+ILP 79.0 76.1 64.6 75.8 HiSAN - Dep 82.7 82.1 74.9 81.9 HiSAN 83.2 82.9 75.8 82.7 Transformer ( b = 4)82.4 82.0 74.6 81.8 1.00\u0002", "entities": [[5, 6, "MetricName", "F1"], [15, 16, "MethodName", "LSTM"], [44, 45, "MethodName", "Transformer"]]}, {"text": "Non - Autoregressive NAG - NMT 72.5 72.1 59.9 71.8 10.71\u0002 + LPD-9 73.8 73.6 61.0 73.1 6.09\u0002 NAG - REG 73.7 73.1 61.5 73.0 10.00\u0002", "entities": []}, {"text": "+ LPD-9 75.6 75.1 63.4 74.9 5.49\u0002 NAG - CRF 75.1 74.4 66.8 74.2 9.41\u0002", "entities": [[9, 10, "MethodName", "CRF"]]}, {"text": "+ LPD-9 77.3 76.5 69.0 76.3 5.04\u0002 BNAG - CRF 77.1 76.2 68.9 76.0 7.21\u0002", "entities": [[9, 10, "MethodName", "CRF"]]}, {"text": "+ LPD-9 79.3 78.5 71.7 78.2 3.91\u0002 Ours ( \u000b = 0:7 ) 79.5 79.0 72.1 78.7", "entities": []}, {"text": "10.00\u0002", "entities": []}, {"text": "Ours", "entities": []}, {"text": "( \u000b = 1:0 )", "entities": []}, {"text": "80.7 80.3 73.6 80.1 8.42\u0002 Table 2 : Results on sentence compression task the standard token - kept - F1 ( F1 ) score .", "entities": [[10, 12, "DatasetName", "sentence compression"], [19, 20, "MetricName", "F1"], [21, 22, "MetricName", "F1"]]}, {"text": "In addition , We also report the results of other standard metrics including ROUGE-1 , ROUGE-2 and", "entities": [[13, 14, "MetricName", "ROUGE-1"], [15, 16, "MetricName", "ROUGE-2"]]}, {"text": "ROUGE - L. We compare the proposed model with the same NAG baselines as in the previous experiment .", "entities": []}, {"text": "We also compare our model with several strong autoregressive models , including Bi - LSTM - Dep ( Filippova et al . , 2015 ) , Tagger and Tagger+ILP ( Wang et al . , 2017 ) , HiSAN - Dep and HiSAN ( Kamigaito et al . , 2018 ) .", "entities": [[14, 15, "MethodName", "LSTM"]]}, {"text": "To measure the inference speedup , we include transformer as a baseline model .", "entities": []}, {"text": "The results are presented in Table 2 , from which we see that our model outperforms the best reported NAG baseline ( with LPD ) in terms of both the generation quality and inference speed .", "entities": []}, {"text": "Comparing with the strong autoregressive models , our model can achieve competitive performance with a over 8:42\u0002inference speed up .", "entities": []}, {"text": "We also report the results of our model using the ratio-\ufb01rst decoding strategy .", "entities": []}, {"text": "By setting \u000b as0:7 , it achieves a 10:00\u0002 inference speedup while still outperforming other compared NAG baselines .", "entities": []}, {"text": "6.4 Machine Translation Machine translation aims at translating text from the source language to the target language .", "entities": [[1, 3, "TaskName", "Machine Translation"], [3, 5, "TaskName", "Machine translation"]]}, {"text": "In this task , we use the IWSLT14 German - to - English ( DEEN ) dataset as our benchmark .", "entities": []}, {"text": "Following previous works , we use the sequence - level knowledge distillation ( Gu et al . , 2018 ) during training .", "entities": [[10, 12, "MethodName", "knowledge distillation"]]}, {"text": "For evaluation , we report results in BLEU scores ( Papineni et al . , 2002 ) .", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "In this experiment , we use the BERT model in German language .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "We compare our model with a range of strong", "entities": []}, {"text": "240Models BLEU Speedup ( \u0002 ) Autoregressive LSTM - based 28.53 CNN - based 32.84 Transformer ( b = 4 ) 33.31 1.00 Non - Autoregressive ENAG - E 24.13 ( 27.30 ) 15.08 ( 7.39 ) ENAG - P 25.09 ( 28.60 ) 14.48 ( 7.24 ) NAG - REG 23.89 ( 28.04 ) 16.45 ( 9.05 ) NAG - NMT 23.04 ( 26.79 ) 13.92 ( 7.24 ) NAG - CRF 26.39 ( 29.21 ) 11.74 ( 6.03 ) BNAG - CRF 26.73 ( 29.67 ) 9.42 ( 5.01 ) Ours ( \u000b = 0:8 ) 29.71 13.92 Ours ( \u000b = 1:0 ) 30.45 11.31 Table 3 : Results on IWSLT14", "entities": [[1, 2, "MetricName", "BLEU"], [7, 8, "MethodName", "LSTM"], [15, 16, "MethodName", "Transformer"], [72, 73, "MethodName", "CRF"], [83, 84, "MethodName", "CRF"]]}, {"text": "De - En dataset .", "entities": []}, {"text": "The numbers in ( ) are results using length - parallel decoding .", "entities": []}, {"text": "BERT CRF R-1 R-2 R - L X X 35.05 16.48 33.28 \u0002 X 32.41 14.19 30.53 X \u0002 32.16 11.33 30.34 \u0002 \u0002 27.02 8.81 25.25 Table 4 : Ablation study on Gigawords dataset .", "entities": [[0, 1, "MethodName", "BERT"], [1, 2, "MethodName", "CRF"]]}, {"text": "NAG models , including NAG - NMT ( Gu et al . , 2018 ) , ENAG - E and ENAG - P ( Guo et al . , 2019 ) , NAG - REG ( Wang et al . , 2019b ) , NAG - CRF ( Sun et al . , 2019 ) and BNAG - CRF .", "entities": [[46, 47, "MethodName", "CRF"], [58, 59, "MethodName", "CRF"]]}, {"text": "For each NAG baseline , we also report the results using LPD9decoding .", "entities": []}, {"text": "In addition , we compare our model with several strong autoregressive models , including LSTM - based ( Wu et al . , 2016 ) , CNN - based ( Gehring et al . , 2017 ) and transformer model .", "entities": [[14, 15, "MethodName", "LSTM"]]}, {"text": "The results are shown in Table 3 , from which we see that our model outperforms the best NAG baseline ( with LPD ) in terms of both the generation quality and inference speedup .", "entities": []}, {"text": "Additionally , we also report the results using the ratio-\ufb01rst decoding .", "entities": []}, {"text": "By setting \u000b as0:8 , the inference speedup can be further boosted to 13:92\u0002while the generation quality is still higher than the best NAG baseline .", "entities": []}, {"text": "6.5 Further Analysis In this section , we present further discussions and empirical analysis of the proposed model .", "entities": []}, {"text": "BERT & CRF To quantify the importance of each component ( BERT & CRF ) of our model , we evaluate the performance on Gigawords dataset by removing each component iteratively .", "entities": [[0, 1, "MethodName", "BERT"], [2, 3, "MethodName", "CRF"], [11, 12, "MethodName", "BERT"], [13, 14, "MethodName", "CRF"]]}, {"text": "The results are shown in Table 4 , from which we can see that by removing any of these compo - Models rep-1rep-2rep-3rep-4", "entities": []}, {"text": "R - L w/o CA 6.897 2.640 0.741 0.295 32.89 Ours 5.786 1.978 0.427 0.106 33.28", "entities": []}, {"text": "Transformer 4.329 1.348 0.267 0.089 33.43 Table 5 : Evaluation results on n - gram repetitions .", "entities": [[0, 1, "MethodName", "Transformer"]]}, {"text": "nents , the overall performance decreases .", "entities": []}, {"text": "By removing BERT from the model , we observe notable drop across all metrics .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "This shows that the knowledge of BERT is an important factor of the model \u2019s strong performance .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "Comparing with results in Table 1 , it still outperforms vanilla NAG - CRF and performs comparably with NAG - CRF using LPD decoding , which demonstrates the merit of the proposed dynamic length decoding mechanism .", "entities": [[13, 14, "MethodName", "CRF"], [20, 21, "MethodName", "CRF"]]}, {"text": "Another interesting \ufb01nding is that , by only removing the CRF layer , the most notable drop is observed on the bigram - level metric ( ROUGE-2 ) .", "entities": [[10, 11, "MethodName", "CRF"], [26, 27, "MetricName", "ROUGE-2"]]}, {"text": "This shows that the bigram - level dependencies on the output side are mainly captured by the CRF module .", "entities": [[17, 18, "MethodName", "CRF"]]}, {"text": "In addition , by removing both BERT and CRF , all metrics further decrease .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "CRF"]]}, {"text": "This con\ufb01rms that each of these two components positively contributes to the model \u2019s overall performance .", "entities": []}, {"text": "Context - Aware Objective", "entities": []}, {"text": "In this part , we study the effect of the context - aware objective .", "entities": []}, {"text": "As described in Equation ( 11 ) , it aims at alleviating the problem of repetitive generation .", "entities": []}, {"text": "To give a quantitative analysis , we use the measurement of sentencelevel repetition ( Welleck et al . , 2020 ) to compute the ratio of duplicate n - grams ( rep- n ) in the generated result .", "entities": []}, {"text": "This metric is de\ufb01ned as rep - n(Y )", "entities": []}, {"text": "= 100\u0002(1:0\u0000juniquen - grams ( Y)j jn - grams ( Y)j ): ( 13 ) For each generated result , rep-", "entities": []}, {"text": "nis0:0when", "entities": []}, {"text": "it has no repeating n - grams .", "entities": []}, {"text": "The \ufb01nal result is computed by averaging over the entire evaluation set .", "entities": []}, {"text": "We conduct experiments on Gigawords dataset to evaluate the n - gram repetitions ranging from uni - gram to 4 - gram .", "entities": []}, {"text": "The results are shown in Table 5 , where w/o CA means the model is trained without using context - aware objective and", "entities": []}, {"text": "R - L denotes the model \u2019s ROUGE - L score .", "entities": [[7, 10, "MetricName", "ROUGE - L"]]}, {"text": "Additionally , we also show the results from transformer model for a direct comparison .", "entities": []}, {"text": "Comparing the two variants of our model , we see that training with context - aware objective leads to a 42 % drop on rep- 3metric ( 0:427 vs0:741 ) and a 64 % drop on rep- 4metric ( 0:106 vs0:295 ) .", "entities": []}, {"text": "The ROUGE - L results also indicate that", "entities": [[1, 4, "MetricName", "ROUGE - L"]]}, {"text": "241ModelsOurs Length - Parallel Decoding ( \u000b = 1:0)LPD- 1 LPD- 5 LPD- 10 BLEU 30.45 27.15 29.62 30.37 Speedup ( \u0002 ) 11.31 11.84 8.92 6.01 Table 6 : Results comparison on IWSLT14 dataset the reduction in token repetition can effectively improve the model generation quality .", "entities": [[14, 15, "MetricName", "BLEU"]]}, {"text": "Dynamic Length Determination Next , we examine the importance of the model \u2019s ability to dynamically determine the length of the generated output .", "entities": []}, {"text": "To this end , we train another model variant by removing the two [ eos ] tokens from the target sequence .", "entities": []}, {"text": "In this way , the model is not able to self - determine the output length throughout the generation process .", "entities": []}, {"text": "To perform inference , we use length - parallel decoding ( LPD ) with different number of length candidates .", "entities": []}, {"text": "Formally , for each length candidatel , the model generates the result ~Yas ~Y= arg max Y0flX i=1\by0 i(hi )", "entities": []}, {"text": "+ lX i=2t(y0 i\u00001;y0 i)g : ( 14 ) The \ufb01nal result is acquired by re - ranking the generated results with a transformer model .", "entities": []}, {"text": "We conduct experiments on the IWSLT14 DEEN dataset in which we try a different number of length candidates , including top- 1 , top- 5and top10 .", "entities": []}, {"text": "The results are shown in Table 6 , from which we can see , as the number of length candidates increases , the model performance increases as well .", "entities": []}, {"text": "The reason is that a larger candidates set is more likely to contain the best - suited length for the generation model , leading to better performance .", "entities": []}, {"text": "However , such decoding procedure inevitably increases the required computation overhead .", "entities": []}, {"text": "We can see that , when setting kas10 , the inference speedup decreases from 11:84\u0002to6:01\u0002.", "entities": []}, {"text": "In contrast , our proposed model is able to determine the optimal output length by itself .", "entities": []}, {"text": "Without any re - ranking process , it outperforms the model with LPD- 10decoding and achieves the inference speedup that is comparable with the model using LPD- 1decoding .", "entities": []}, {"text": "Ratio - First Decoding We are also interested in the effect of the ratio-\ufb01rst decoding strategy .", "entities": []}, {"text": "To provide a quantitative analysis , we perform inference on the Gigawords dataset using ratio-\ufb01rst with different \u000b .", "entities": []}, {"text": "The experimental results with different \u000b are presented in Figure 3 .", "entities": []}, {"text": "It can be observed that , when \u000b reaches 0:3 , the model approximately Figure 3 : Experiment results on Gigawords dataset using ratio-\ufb01rst decoding with different \u000b .", "entities": []}, {"text": "Figure 4 : The distribution of target / source length ratio of the training and test set in Gigawords dataset .", "entities": []}, {"text": "achieves its optimal performance .", "entities": []}, {"text": "At the same time , a notable improvement can be observed in terms of the inference speedup ( 6:72\u0002 ! 9:31\u0002 ) .", "entities": []}, {"text": "Now we illustrate why the near optimal performance can be achieved when \u000b reaches 0:3 .", "entities": []}, {"text": "In Figure 4 , we present the distribution of the target / source length ratio of every data instance in the Gigawords dataset .", "entities": []}, {"text": "We can see that , for most cases , the ratio between the target length T0and source lengthTis less than 0:3 .", "entities": []}, {"text": "Recall the de\ufb01nition of ratio-\ufb01rst decoding in Equation ( 10 ) , the [ \u000b \u0001T ] constrains the maximum length of the generated result .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "Therefore , once we have a prior knowledge on the data statistic , we can easily choose a proper \u000b that both improves the inference speed whilst maintaining the generation quality .", "entities": []}, {"text": "In this case , a proper \u000b could be 0:3which is demonstrated by the results in Figure 3 and 4 .", "entities": []}, {"text": "By setting different \u000b , ratio-\ufb01rst provides us an explicit way to control the balance between the inference speed and the generation quality .", "entities": []}, {"text": "This property of ratio-\ufb01rst is especially favorable in real - life scenarios where the inference speed is the highest concern .", "entities": []}, {"text": "2427 Conclusion In this work , we explored the potential of BERT in various text generation tasks under the NAG framework .", "entities": [[11, 12, "MethodName", "BERT"], [14, 16, "TaskName", "text generation"]]}, {"text": "To address problems from NAG models previously having a pre\ufb01xed output length , we devised a decoding mechanism which enables the model to determine the output length dynamically .", "entities": []}, {"text": "To reduce errors stemming from the assumption of conditional independence of output tokens , we proposed a context - aware objective as well as using a CRF decoding .", "entities": [[26, 27, "MethodName", "CRF"]]}, {"text": "Furthermore , to maximize the inference speed advantage of our model , we introduced a ratio-\ufb01rst decoding strategy .", "entities": []}, {"text": "We evaluated our model on three benchmark datasets and the results show that our model signi\ufb01cantly outperforms many strong NAG baselines and performs comparably to many strong AG models .", "entities": []}, {"text": "Acknowledgments The authors wish to thank Jialu Xu , Guanlin Li , Xing Wang for their insightful discussions and support .", "entities": []}, {"text": "Many thanks to our anonymous reviewers for their suggestions and comments .", "entities": []}, {"text": "References Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , NAACL - HLT 2019 , Minneapolis , MN , USA , June 2 - 7 , 2019 , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Katja Filippova , Enrique Alfonseca , Carlos A. Colmenares , Lukasz Kaiser , and Oriol Vinyals . 2015 .", "entities": []}, {"text": "Sentence compression by deletion with lstms .", "entities": [[0, 2, "DatasetName", "Sentence compression"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , EMNLP 2015 , Lisbon , Portugal , September 17 - 21 , 2015 , pages 360\u2013368 .", "entities": []}, {"text": "Katja Filippova and Yasemin Altun .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Overcoming the lack of parallel data in sentence compression .", "entities": [[7, 9, "DatasetName", "sentence compression"]]}, {"text": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , EMNLP 2013 , 18 - 21 October 2013 , Grand Hyatt Seattle , Seattle , Washington , USA , A meeting of SIGDAT , a Special Interest Group of the ACL , pages 1481\u20131491 .", "entities": []}, {"text": "Jonas Gehring , Michael Auli , David Grangier , Denis Yarats , and Yann N. Dauphin . 2017 .", "entities": []}, {"text": "Convolutional sequence to sequence learning .", "entities": [[1, 4, "MethodName", "sequence to sequence"]]}, {"text": "In Proceedings of the 34th International Conference on MachineLearning , ICML 2017 , Sydney , NSW , Australia , 6 - 11 August 2017 , pages 1243\u20131252 .", "entities": []}, {"text": "Marjan Ghazvininejad , Omer Levy , Yinhan Liu , and Luke Zettlemoyer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Mask - predict : Parallel decoding of conditional masked language models .", "entities": []}, {"text": "InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , EMNLP - IJCNLP 2019 , Hong Kong , China , November 3 - 7 , 2019 , pages 6111 \u2013 6120 .", "entities": []}, {"text": "Jiatao Gu , James Bradbury , Caiming Xiong , Victor O. K. Li , and Richard Socher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Nonautoregressive neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In 6th International Conference on Learning Representations , ICLR 2018 , Vancouver , BC , Canada , April 30 - May 3 , 2018 , Conference Track Proceedings .", "entities": []}, {"text": "Junliang Guo , Xu Tan , Di He , Tao Qin , Linli Xu , and Tie - Yan Liu . 2019 .", "entities": []}, {"text": "Non - autoregressive neural machine translation with enhanced decoder input .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In The Thirty - Third AAAI Conference on Arti\ufb01cial Intelligence , AAAI 2019 , The Thirty - First Innovative Applications of Arti\ufb01cial Intelligence Conference , IAAI 2019 , The Ninth AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence , EAAI 2019 , Honolulu , Hawaii , USA , January 27 - February 1 , 2019 , pages 3723\u20133730 .", "entities": []}, {"text": "Hidetaka Kamigaito , Katsuhiko Hayashi , Tsutomu Hirao , and Masaaki Nagata . 2018 .", "entities": []}, {"text": "Higher - order syntactic attention network for longer sentence compression .", "entities": [[8, 10, "DatasetName", "sentence compression"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , NAACL - HLT 2018 , New Orleans , Louisiana , USA , June 1 - 6 , 2018 , Volume 1 ( Long Papers ) , pages 1716\u20131726 .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In 3rd International Conference on Learning Representations , ICLR 2015 , San Diego , CA , USA , May 7 - 9 , 2015 , Conference Track Proceedings .", "entities": []}, {"text": "John D. Lafferty , Andrew McCallum , and Fernando C. N. Pereira . 2001 .", "entities": []}, {"text": "Conditional random \ufb01elds : Probabilistic models for segmenting and labeling sequence data .", "entities": []}, {"text": "In Proceedings of the Eighteenth International Conference on Machine Learning ( ICML 2001 ) , Williams College , Williamstown , MA , USA , June 28 - July 1 , 2001 , pages 282\u2013289 .", "entities": []}, {"text": "Piji Li , Wai Lam , Lidong Bing , and Zihao Wang .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Deep recurrent generative decoder for abstractive text summarization .", "entities": [[5, 8, "TaskName", "abstractive text summarization"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , EMNLP 2017 , Copenhagen , Denmark , September 9 - 11 , 2017 , pages 2091\u20132100 .", "entities": []}, {"text": "Chin - Yew Lin .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Rouge :", "entities": []}, {"text": "A package for automatic evaluation of summaries .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "ACL workshop on Text Summarization Branches Out , page 10 .", "entities": [[3, 5, "TaskName", "Text Summarization"]]}, {"text": "243Thang Luong , Hieu Pham , and Christopher D. Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Effective approaches to attention - based neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , EMNLP 2015 , Lisbon , Portugal , September 17 - 21 , 2015 , pages 1412\u20131421 .", "entities": []}, {"text": "Xuezhe Ma , Chunting Zhou , Xian Li , Graham Neubig , and Eduard H. Hovy .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Flowseq : Nonautoregressive conditional sequence generation with generative \ufb02ow .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , EMNLPIJCNLP 2019 , Hong Kong , China , November 3 - 7 , 2019 , pages 4281\u20134291 .", "entities": []}, {"text": "Ani Nenkova and Kathleen R. McKeown .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "A survey of text summarization techniques .", "entities": [[3, 5, "TaskName", "text summarization"]]}, {"text": "In Mining Text Data , pages 43\u201376 .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , July 6 - 12 , 2002 , Philadelphia , PA , USA , pages 311\u2013318 .", "entities": []}, {"text": "Adam Paszke , Sam Gross , Soumith Chintala , Gregory Chanan , Edward Yang , Zachary DeVito , Zeming Lin , Alban Desmaison , Luca Antiga , and Adam Lerer . 2017 .", "entities": [[0, 1, "MethodName", "Adam"], [28, 29, "MethodName", "Adam"]]}, {"text": "Automatic differentiation in pytorch .", "entities": []}, {"text": "InNIPS - W .", "entities": []}, {"text": "Alexander M. Rush , Sumit Chopra , and Jason Weston .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A neural attention model for abstractive sentence summarization .", "entities": [[6, 8, "TaskName", "sentence summarization"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , EMNLP 2015 , Lisbon , Portugal , September 17 - 21 , 2015 , pages 379\u2013389 .", "entities": []}, {"text": "Abigail See , Peter J. Liu , and Christopher D. Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Get to the point : Summarization with pointergenerator networks .", "entities": [[5, 6, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , ACL 2017 , Vancouver , Canada , July 30 August 4 , Volume 1 : Long Papers , pages 1073\u20131083 .", "entities": []}, {"text": "Zhiqing Sun , Zhuohan Li , Haoqing Wang , Di He , Zi Lin , and Zhi - Hong Deng . 2019 .", "entities": []}, {"text": "Fast structured decoding for sequence models .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 32 : Annual Conference on Neural Information Processing Systems 2019 , NeurIPS 2019 , 8 - 14 December 2019 , Vancouver , BC , Canada , pages 3011\u20133020 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 30 : Annual Conference on Neural Information Processing Systems 2017 , 4 - 9 December 2017 , Long Beach , CA , USA , pages 5998\u20136008.Liangguo", "entities": []}, {"text": "Wang , Jing Jiang , Hai Leong Chieu , Chen Hui Ong , Dandan Song , and Lejian Liao . 2017 .", "entities": []}, {"text": "Can syntax help ?", "entities": []}, {"text": "improving an lstm - based sentence compression model for new domains .", "entities": [[2, 3, "MethodName", "lstm"], [5, 7, "DatasetName", "sentence compression"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , ACL 2017 , Vancouver , Canada , July 30 - August 4 , Volume 1 : Long Papers , pages 1385\u20131393 .", "entities": []}, {"text": "Wenbo Wang , Yang Gao , Heyan Huang , and Yuxiang Zhou .", "entities": []}, {"text": "2019a .", "entities": []}, {"text": "Concept pointer network for abstractive summarization .", "entities": [[1, 3, "MethodName", "pointer network"], [5, 6, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , EMNLP - IJCNLP 2019 , Hong Kong , China , November 3 - 7 , 2019 , pages 3074\u20133083 .", "entities": []}, {"text": "Yiren Wang , Fei Tian , Di He , Tao Qin , ChengXiang Zhai , and Tie - Yan Liu . 2019b .", "entities": []}, {"text": "Non - autoregressive machine translation with auxiliary regularization .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In The Thirty - Third AAAI Conference on Arti\ufb01cial Intelligence , AAAI 2019 , The Thirty - First Innovative Applications of Arti\ufb01cial Intelligence Conference , IAAI 2019 , The Ninth AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence , EAAI 2019 , Honolulu , Hawaii , USA , January 27 - February 1 , 2019 , pages 5377\u20135384 .", "entities": []}, {"text": "Bingzhen Wei , Mingxuan Wang , Hao Zhou , Junyang Lin , and Xu Sun . 2019 .", "entities": []}, {"text": "Imitation learning for nonautoregressive neural machine translation .", "entities": [[0, 2, "TaskName", "Imitation learning"], [5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 57th Conference of the Association for Computational Linguistics , ACL 2019 , Florence , Italy , July 28- August 2 , 2019 , Volume 1 : Long Papers , pages 1304\u20131312 .", "entities": [[16, 17, "MethodName", "Florence"]]}, {"text": "Sean Welleck , Ilia Kulikov , Stephen Roller , Emily Dinan , Kyunghyun Cho , and Jason Weston .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Neural text generation with unlikelihood training .", "entities": [[1, 3, "TaskName", "text generation"]]}, {"text": "In 8th International Conference on Learning Representations , ICLR 2020 , Addis Ababa , Ethiopia , April 2630 , 2020 .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , R\u2019emi Louf , Morgan Funtowicz , and Jamie Brew .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Huggingface \u2019s transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "ArXiv , abs/1910.03771 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V .", "entities": []}, {"text": "Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , Jeff Klingner , Apurva Shah , Melvin Johnson , Xiaobing Liu , Lukasz Kaiser , Stephan Gouws , Yoshikiyo Kato , Taku Kudo , Hideto Kazawa , Keith Stevens , George Kurian , Nishant Patil , Wei Wang , Cliff Young , Jason Smith , Jason Riesa , Alex Rudnick , Oriol Vinyals , Greg Corrado , Macduff Hughes , and Jeffrey Dean .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Google \u2019s neural machine translation system : Bridging the gap between human and machine translation .", "entities": [[0, 1, "DatasetName", "Google"], [3, 5, "TaskName", "machine translation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "CoRR , abs/1609.08144 .", "entities": []}]