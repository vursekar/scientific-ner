[{"text": "Proceedings of the 27th International Conference on Computational Linguistics , pages 1156\u20131166 Santa Fe , New Mexico , USA , August 20 - 26 , 2018.1156Adversarial Multi - lingual Neural Relation Extraction Xiaozhi Wang1\u0003 , Xu Han1\u0003 , Yankai Lin1 , Zhiyuan Liu1y , Maosong Sun1;2 1Department of Computer Science and Technology , State Key Lab on Intelligent Technology and Systems , Beijing National Research Center for Information Science and Technology , Tsinghua University , Beijing , China 2Beijing Advanced Innovation Center for Imaging Technology , Capital Normal University , Beijing , China Abstract Multi - lingual relation extraction aims to \ufb01nd unknown relational facts from text in various languages .", "entities": [[30, 32, "TaskName", "Relation Extraction"], [97, 99, "TaskName", "relation extraction"]]}, {"text": "Existing models can not well capture the consistency and diversity of relation patterns in different languages .", "entities": []}, {"text": "To address these issues , we propose an adversarial multi - lingual neural relation extraction ( AMNRE ) model , which builds both consistent and individual representations for each sentence to consider the consistency and diversity among languages .", "entities": [[13, 15, "TaskName", "relation extraction"]]}, {"text": "Further , we adopt an adversarial training strategy to ensure those consistent sentence representations could effectively extract the language - consistent relation patterns .", "entities": []}, {"text": "The experimental results on real - world datasets demonstrate that our AMNRE model signi\ufb01cantly outperforms the state - of - the - art models .", "entities": []}, {"text": "The source code of this paper can be obtained from https://github.com/thunlp/AMNRE .", "entities": []}, {"text": "1 Introduction Relation extraction ( RE ) is a crucial task in NLP , which aims to extract semantic relations between entity pairs from the sentences containing them .", "entities": [[2, 4, "TaskName", "Relation extraction"]]}, {"text": "For example , given an entity pair ( Bill Gates , Microsoft ) and a sentence \u201c Bill Gates is the co - founder and CEO of Microsoft \u201d , we want to \ufb01gure out the relation Founder between the two entities .", "entities": []}, {"text": "RE can potentially bene\ufb01t many applications , such as knowledge base construction ( Zhong et al . , 2015 ; Han et al . , 2018 ) and question answering ( Xiang et al . , 2017 ) .", "entities": [[28, 30, "TaskName", "question answering"]]}, {"text": "Recently , neural models have shown their great abilities in RE .", "entities": []}, {"text": "Zeng et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2014 ) introduce a convolutional neural network ( CNN ) to extract relational facts with automatically learning features from text .", "entities": []}, {"text": "To address the issue of lack of data , Zeng et al .", "entities": []}, {"text": "( 2015 ) incorporate multi - instance learning with a piece - wise convolutional neural network ( PCNN ) to extract relations in distantly supervised data .", "entities": []}, {"text": "Because distant supervision suffer from wrong labeling problems , Lin et al .", "entities": []}, {"text": "( 2016 ) further employ a sentence - level selective attention to \ufb01lter out those noisy sentences in distantly supervised data and achieve state - of - the - art performance .", "entities": []}, {"text": "All these neural relation extraction ( NRE ) models merely focus on extracting relational facts from mono - lingual data , ignoring the rich information in multi - lingual data .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "Lin et al .", "entities": []}, {"text": "( 2017 ) propose a multi - lingual attention - based neural relation extraction ( MNRE ) model , which considers the consistency and complementarity in multi - lingual data .", "entities": [[12, 14, "TaskName", "relation extraction"]]}, {"text": "MNRE builds a sentence representation for each sentence in various languages and employs a multi - lingual attention to capture the pattern consistency and complementarity among languages .", "entities": []}, {"text": "Although MNRE achieves great success in multi - lingual RE , it still has some problems .", "entities": []}, {"text": "MNRE learns a single representation for each sentence in various languages , which can not well capture both the consistency and diversity of relation patterns in different languages .", "entities": []}, {"text": "Moreover , MNRE simply utilizes a multi - lingual attention mechanism and a global relation predictor to capture the consistent relation patterns among multiple languages .", "entities": []}, {"text": "From the experimental data , we \ufb01nd that the sentence representations in different languages are still far from each other and linearly separable .", "entities": []}, {"text": "Therefore , it is hard for the multi\u0003indicates equal contribution", "entities": []}, {"text": "yCorresponding author : Zhiyuan Liu ( liuzy@tsinghua.edu.cn ) .", "entities": []}, {"text": "This work is licensed under a Creative Commons Attribution 4.0 International License .", "entities": []}, {"text": "License details :", "entities": []}, {"text": "http://creativecommons.org/licenses/by/4.0/", "entities": []}, {"text": "1157 x1x2x1x1", "entities": []}, {"text": "1 EI        1 EI        1 EC        1 EC    x2x2", "entities": [[7, 8, "DatasetName", "EC"], [10, 11, "DatasetName", "EC"]]}, {"text": "x1x2", "entities": []}, {"text": "EI        2   EC        2   EC        2   EI        2      \ud835\udc9a 12     \ud835\udc9a11     \ud835\udc9a12     \ud835\udc9a22     \ud835\udc9a21       s1   \ud835\udc94    s2 Language 1 Language 2Individual Semantic Space   ( Language 1)Individual Semantic Space   (", "entities": [[4, 5, "DatasetName", "EC"], [8, 9, "DatasetName", "EC"]]}, {"text": "Language 2)Consistent Semantic Space   Textual Relation   Representation Multi -lingual Attention Sentence Embedding Sentence Encoder Input EmbeddingD \ud835\udc9a 11     \ud835\udc9a 22     \ud835\udc9a 21    Figure 1 : Overall architecture of our adversarial multi - lingual neural relation extraction ( AMNRE ) which contains two languages .", "entities": [[12, 14, "TaskName", "Sentence Embedding"], [39, 41, "TaskName", "relation extraction"]]}, {"text": "lingual attention mechanism and global relation predictor to extract relation consistency from distinct sentence representations .", "entities": []}, {"text": "To address these issues , we propose an adversarial multi - lingual NRE ( AMNRE ) model .", "entities": []}, {"text": "As shown in Figure 1 , for an entity pair , we encode its corresponding sentences in various languages through neural sentence encoders .", "entities": []}, {"text": "For each sentence , we build an individual representation to grasp its individual language features and a consistent representation to encode its substantially consistent features among languages .", "entities": []}, {"text": "Further , we adopt an adversarial training strategy to ensure AMNRE can extract the language - consistent relation patterns from the consistent representations .", "entities": []}, {"text": "Orthogonality constraints are also adopted to enhance differences between individual representations and consistent representations for each language .", "entities": []}, {"text": "In experiments , we take Chinese and English to show the effectiveness of AMNRE .", "entities": []}, {"text": "The experimental results show that AMNRE outperforms all baseline models signi\ufb01cantly by explicitly encoding the consistency and diversity among languages .", "entities": []}, {"text": "And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language - consistent relation patterns .", "entities": []}, {"text": "2 Related Works 2.1 Relation Extraction Traditional supervised RE models ( Zelenko et al . , 2003 ; Socher et al . , 2012 ; Santos et al . , 2015 ) heavily rely on abundant amounts of high - quality annotated data .", "entities": [[4, 6, "TaskName", "Relation Extraction"]]}, {"text": "Hence , Mintz et al .", "entities": []}, {"text": "( 2009 ) propose a distantly supervised model for RE .", "entities": []}, {"text": "Distant supervision aligns knowledge bases ( KBs ) and text to automatically annotate data , and thus distantly supervised models inevitably suffer from wrong labeling problems .", "entities": []}, {"text": "To alleviate the noise issue , Riedel et al .", "entities": []}, {"text": "( 2010 ) and Hoffmann et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2011 ) propose multi - instance learning ( MIL ) mechanisms for single - label and multi - label problems respectively .", "entities": []}, {"text": "Then , Zeng et al . ( 2015 ) attempt to integrate neural models into distant supervision .", "entities": []}, {"text": "Lin et al .", "entities": []}, {"text": "( 2016 ) further propose a sentence - level attention to jointly consider all sentences containing same entity pairs for RE .", "entities": []}, {"text": "The attention - based neural relation extraction ( NRE ) model has become a foundation for some recent works ( Ji et al . , 2017 ; Zeng et al . , 2017 ; Liu et al . , 2017b ; Wu et al . , 2017 ; Feng et al . , 2018 ; Zeng et al . , 2018 ) .", "entities": [[5, 7, "TaskName", "relation extraction"]]}, {"text": "Most existing RE models are devoted to extracting relations from mono - lingual data and ignore information lying in text of multiple languages .", "entities": []}, {"text": "Faruqui and Kumar ( 2015 ) and Verga et al .", "entities": [[2, 3, "DatasetName", "Kumar"]]}, {"text": "( 2016 ) \ufb01rst attempt to adopt multi - lingual transfer learning for RE .", "entities": [[10, 12, "TaskName", "transfer learning"]]}, {"text": "However , both of these works learn predictive", "entities": []}, {"text": "1158models on a new language for existing KBs , without fully leveraging semantic information in text .", "entities": []}, {"text": "Then , Lin et al .", "entities": []}, {"text": "( 2017 ) construct a multi - lingual NRE ( MNRE ) model to jointly represent text of multiple languages to enhance RE .", "entities": []}, {"text": "In this paper , we propose a novel multi - lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces , which can achieve more effective representations for RE . 2.2 Adversarial Training Goodfellow et al .", "entities": []}, {"text": "( 2015 ) propose adversarial training for image classi\ufb01cation tasks .", "entities": []}, {"text": "Afterwards , Goodfellow et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2014 ) propose a mature adversarial training framework and use the framework to train generative models .", "entities": []}, {"text": "Adversarial networks have recently been used as methods to narrow probability distributions and proven effective in some tasks .", "entities": []}, {"text": "In domain adaptation , Ganin et al . ( 2016 ) and Bousmalis et al .", "entities": [[1, 3, "TaskName", "domain adaptation"]]}, {"text": "( 2016 ) adopt adverarial training strategies to transfer the features of one source domain to its corresponding target domain .", "entities": []}, {"text": "Inspired by Ganin et al .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "( 2016 )", "entities": []}, {"text": ", adversarial training has also been explored in some typical NLP tasks for multi - feature fusion .", "entities": []}, {"text": "Park and I m ( 2016 ) propose a multi - modal representation learning model based on adversarial training .", "entities": [[12, 14, "TaskName", "representation learning"]]}, {"text": "Then , Liu et al .", "entities": []}, {"text": "( 2017a ) employ adversarial training to construct a multi - task learning model for text classi\ufb01cation by extending the original binary adversarial training to the multiclass version .", "entities": [[9, 13, "TaskName", "multi - task learning"]]}, {"text": "And a similar adversarial framework is also adapted by Chen et al .", "entities": []}, {"text": "( 2017 ) to learn features from different datasets for chinese word segmentation .", "entities": [[10, 13, "TaskName", "chinese word segmentation"]]}, {"text": "In this paper , we adopt adversarial training to boost feature fusion to grasp the consistency among different languages .", "entities": []}, {"text": "3 Methodology", "entities": []}, {"text": "In this section , we introduce the overall framework of our proposed AMNRE in detail .", "entities": []}, {"text": "As shown in Figure 1 , for each entity pair , AMNRE encodes its corresponding sentences in different languages into several semantic spaces to grasp their individual language patterns .", "entities": []}, {"text": "Meanwhile , a uni\ufb01ed space is also set up to encode consistent features among languages .", "entities": []}, {"text": "By explicitly encoding the consistency and diversity among languages , AMNRE can achieve better extraction results in the multi - lingual scenario .", "entities": []}, {"text": "For each given entity pair , we de\ufb01ne its corresponding sentences in ndifferent languages as T= fS1;:::;Sng , whereSj = fx1 j;:::;xjSjj jgdenotes the sentence set in the j - th language .", "entities": []}, {"text": "All these sentences are labeled with the relation r2R by heuristical labeling algorithms in distant supervision ( Mintz et al . , 2009 ) .", "entities": [[8, 9, "DatasetName", "r2R"]]}, {"text": "Our model aims to learn a relation extractor by maximizing the conditional probabilityp(rjT)with the following three components : Sentence Encoder .", "entities": []}, {"text": "Given a sentence and its target entity pair , we employ neural networks to encode the sentence into a embedding .", "entities": []}, {"text": "In this paper , we implement the sentence encoder with both convolutional ( CNN ) and recurrent ( RNN ) architectures .", "entities": []}, {"text": "Speci\ufb01cally , we set the encoders EI jandEC jto encode each sentence in the j - th language into its individual and consistent embeddings respectively , and expect these embeddings to capture the diversity and consistency among languages .", "entities": []}, {"text": "Multi - lingual Attention .", "entities": []}, {"text": "Since not all sentences are labeled correctly in distant supervision , we adopt multi - lingual attention mechanisms to capture those informative sentences .", "entities": []}, {"text": "In practice , we apply language - individual and language - consistent attentions to compute local and global textual relation representations respectively for \ufb01nal prediction .", "entities": []}, {"text": "Adversarial Training .", "entities": []}, {"text": "Under the framework of AMNRE , we encode the sentences in various languages into a uni\ufb01ed consistent semantic space .", "entities": []}, {"text": "We further adopt adversarial training to ensure these sentences are well fused in the uni\ufb01ed space after encoding so that our model can effectively extract the language - consistent relation patterns .", "entities": []}, {"text": "We will introduce the three components in detail as follows .", "entities": []}, {"text": "3.1 Sentence Encoder Given a sentence x = fw1;w2;:::gcontaining two entities , we apply neural architectures including both CNN and RNN to encode the sentence into a continuous low - dimensional space to capture its implicit semantics .", "entities": []}, {"text": "11593.1.1 Input Layer The input layer transforms all input words in the sentence into corresponding input embeddings by concatenating their word embeddings and position embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "The word embeddings are pre - trained by Skip - Gram ( Mikolov et al . , 2013 ) .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "The position embeddings are a widely - used technique in RE proposed by Zeng et al .", "entities": []}, {"text": "( 2014 ) , representing each word \u2019s relative distances to the two entities into two kp - dimensional vectors .", "entities": [[16, 17, "MethodName", "kp"]]}, {"text": "The input layer represents the input sentence as a ki - dimensional embedding sequence x = fw1;w2;:::g , whereki = kw+kp\u00022,kwandkpare the dimensions of word embeddings and position embeddings respectively .", "entities": [[24, 26, "TaskName", "word embeddings"]]}, {"text": "3.1.2 Encoding Layer After representing the input sentence as a ki - dimensional embedding sequence , we select both CNN ( Zeng et al . , 2014 ) and RNN ( Zhang and Wang , 2015 ) to encode the input embedding sequence x=", "entities": []}, {"text": "fw1;w2;:::gto its sentence embedding .", "entities": [[2, 4, "TaskName", "sentence embedding"]]}, {"text": "CNN slides a convolution kernel with the window size mto extract the kh - dimensional local features , hi = CNN\u0000 wi\u0000m\u00001", "entities": [[3, 4, "MethodName", "convolution"]]}, {"text": "2;:::;wi+m\u00001 2\u0001 : ( 1 ) A max - pooling is then adopted to obtain the \ufb01nal sentence embedding yas follows , [ y]j= maxf[h1]j ; : : : ; [ hn]jg : ( 2 ) RNN is mainly designed for modeling sequential data .", "entities": [[17, 19, "TaskName", "sentence embedding"]]}, {"text": "In this paper , we adopt bidirectional RNN ( Bi - RNN ) to incorporate information from both sides of the sentence sequence as follows , \u0000 !", "entities": []}, {"text": "hi", "entities": []}, {"text": "= RNNf(xi;\u0000 ! hi\u00001 ) ; \u0000hi = RNNb(xi ; \u0000hi+1 ) ; ( 3 ) where\u0000 !", "entities": []}, {"text": "hiand \u0000hiare thekh - dimensional hidden states at the position iof the forward and backward RNN respectively .", "entities": []}, {"text": "RNN ( \u0001)is the recurrent unit and we select gated recurrent unit ( GRU ) ( Cho et al . , 2014 ) as the recurrent unit in this paper .", "entities": [[9, 12, "MethodName", "gated recurrent unit"], [13, 14, "MethodName", "GRU"]]}, {"text": "We concatenate both the forward and backward hidden states as the sentence embedding y , y=", "entities": [[11, 13, "TaskName", "sentence embedding"]]}, {"text": "[ \u0000 !", "entities": []}, {"text": "hn ; \u0000h1 ] : ( 4 ) For simplicity , we denote such a sentence encoding operation as the following equation , y = E(x ): ( 5 ) For each sentence xi j2Sj , we adopt the individual sentence encoder EI jand the consistent sentence encoder EC jto embed the sentence into its individual and consistent representations respectively , fy1 j;y2 j;:::g = fEI j(x1 j);EI j(x2 j);:::g;f\u0016", "entities": [[48, 49, "DatasetName", "EC"]]}, {"text": "y1 j;\u0016 y2 j;:::g = fEC j(x1 j);EC j(x2 j);:::g : ( 6 ) 3.2 Multi - lingual Selective Attention For each given entity pair , AMNRE adopts multi - lingual selective attention mechanisms to exploit informative sentences in T. We explicitly encode languages \u2019 consistency and diversity into individual and consistent representations , thus our attentions are more simple than those proposed in Lin et al .", "entities": []}, {"text": "( 2017 ) .", "entities": []}, {"text": "3.2.1 Language - individual Attention Since it is intuitive that each language has its own characteristic , we set language - individual attention mechanisms for different languages .", "entities": []}, {"text": "In the individual semantic space of the j - th language , we assign a query vector rjto each relation r2R .", "entities": [[20, 21, "DatasetName", "r2R"]]}, {"text": "The attention score for each sentence in Sj = fx1 j;x2 j;:::gis de\ufb01ned as follows , \u000b i j = exp(r > jyi j ) PjSjj k=1exp(r > jyk j ): ( 7 ) The attention scores can be used to compute language - individual textual relation representations , sj = jSjjX k=1 \u000b k jyk j : ( 8)", "entities": []}, {"text": "11603.2.2 Language - consistent Attention Besides language - individual attention mechanisms , we also adopt a language - consistent attention to take all sentences in all languages into consideration .", "entities": []}, {"text": "In the consistent semantic space , we also assign a query vector \u0016 rto each relation r2R and the attention score for each sentence is de\ufb01ned as follows ,", "entities": [[16, 17, "DatasetName", "r2R"]]}, {"text": "i j = exp(\u0016 r>\u0016 yi j )", "entities": []}, {"text": "Pn l=1PjSlj k=1exp(\u0016 r>\u0016 yk l ): ( 9 ) The attention scores can be used to compute language - consistent textual relation representations , \u0016 s = nX l=1jSljX k=1 \f k l\u0016 yk l : ( 10 ) 3.3 Relation Prediction With the language - individual textual relation representations fs1;s2;:::gand the language - consistent textual relation representation \u0016 s , we can estimate the probability p(rjT)over each relation r2R , p(rjT ) = p(rj\u0016 s)nY j=1p(rjsj ): ( 11 ) p(rj\u0016 s)andp(rjsj)can be de\ufb01ned as follows , p(rjsj )", "entities": [[70, 71, "DatasetName", "r2R"]]}, {"text": "= softmax [ Rjsj+dj ] ; p(rj\u0016 s ) = softmax", "entities": [[1, 2, "MethodName", "softmax"], [10, 11, "MethodName", "softmax"]]}, {"text": "[ \u0016R\u0016 s+\u0016d ] ; ( 12 ) where djand\u0016dare bias vectors , Rjis the speci\ufb01c relation matrix of the j - th language , and \u0016Ris the consistent relation matrix .", "entities": []}, {"text": "We de\ufb01ne the objective function to train the relation extractor as follows , min \u0012Lnre(\u0012 )", "entities": []}, {"text": "= \u0000X llogp(rljTl ) ; ( 13 ) where\u0012is all parameters in the framework .", "entities": []}, {"text": "In the training phase , p(rjT)is computed using the labeled relations as the attention queries .", "entities": []}, {"text": "In the test phase , we need to use each possible relation as attention queries to compute p(rjT)for relation prediction since the relations are unknown in advance .", "entities": []}, {"text": "3.4 Adversarial Training In our framework , we encode sentences of various languages into a consistent semantic space to grasp the consistency among languages .", "entities": []}, {"text": "One possible situation is that sentences of different languages are aggregated in different places of the space and linearly separable .", "entities": []}, {"text": "In this case , our purpose of mining substantially consistent relation patterns in different languages is dif\ufb01cult to be reached .", "entities": []}, {"text": "Inspired by Ganin et al .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "( 2016 ) , we adopt adversarial training into our framework to address this problem .", "entities": []}, {"text": "In the adversarial training , we de\ufb01ne a discriminator to estimate which kind of languages the sentences from .", "entities": []}, {"text": "The probability distributions over these sentences are formalized as follows , D(\u0016 si j ) = softmax ( MLP(\u0016 si j ) ) ; ( 14 ) where MLP is a two - layer multilayer perceptron network .", "entities": [[16, 17, "MethodName", "softmax"], [28, 29, "DatasetName", "MLP"]]}, {"text": "Contrary to the discriminator , the consistent sentence encoders are expected to produce sentence embeddings that can not be reliably predicted by the discriminator .", "entities": [[13, 15, "TaskName", "sentence embeddings"]]}, {"text": "Hence , the adversarial training process is a min - max game and can be formalized as follows ,", "entities": []}, {"text": "min \u0012C Emax \u0012DnX j=1jSjjX i=1log[D(EC j(xi j))]j ; ( 15 ) where [ \u0001]jis thej - th value of the vector .", "entities": []}, {"text": "The formula means that given a sentence of any language , the corresponding sentence encoder of its language generates the sentence embedding to confuse the discriminator .", "entities": [[20, 22, "TaskName", "sentence embedding"]]}, {"text": "Meanwhile , the discriminator", "entities": []}, {"text": "1161tries its best to predict the language of the sentence according to the sentence embedding .", "entities": [[13, 15, "TaskName", "sentence embedding"]]}, {"text": "After suf\ufb01cient training , the encoders and the discriminator reach a balance , and sentences of different languages containing similar semantic information can be well encoded into adjacent places of the space .", "entities": []}, {"text": "In training , we optimize the following loss functions instead of Eq . 15 , min \u0012C", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "ELE adv(\u0012C E ) = X lX Sj2TlX xi j2Sjlog[D(EC j(xi j))]j;min \u0012DLD adv(\u0012D )", "entities": []}, {"text": "= \u0000X lX Sj2TlX xi j2Sjlog[D(EC j(xi j))]j ; ( 16 ) where\u0012C Eand\u0012Dare all parameters of the consistent sentence encoders and the discriminator .", "entities": []}, {"text": "We notice that language - individual semantics could be wrongly encoded into the consistent semantic space , and may have negative effects on extracting language - consistent features .", "entities": []}, {"text": "Inspired by Bousmalis et al .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "( 2016 ) , we adopt orthogonality constraints to alleviate this issue .", "entities": []}, {"text": "We minimize the following penalty function : min \u0012ELpenalty ( \u0012E )", "entities": []}, {"text": "= nX j=1", "entities": []}, {"text": "", "entities": []}, {"text": "", "entities": []}, {"text": "IT jCj", "entities": []}, {"text": "", "entities": []}, {"text": "", "entities": []}, {"text": "F ; ( 17 ) where IjandCjare two matrices whose row vectors are the embeddings of sentences in the j - th language encoded by EI jand EC jrespectively .", "entities": [[27, 28, "DatasetName", "EC"]]}, {"text": "\u0012Eis parameters of the all encoders .", "entities": []}, {"text": "And k\u0001kFis the squared Frobenius norm .", "entities": []}, {"text": "3.5 Implementation Details During training process , we combine the extraction and adversarial objective functions as follows , L = Lnre(\u0012 )", "entities": []}, {"text": "+ \u00151LD adv(\u0012D )", "entities": []}, {"text": "+ \u00152LE adv(\u0012C E )", "entities": []}, {"text": "+ \u00153Lpenalty ( \u0012E ) ; ( 18 ) where\u00151,\u00152 , and\u00153are harmonic factors .", "entities": []}, {"text": "All models are optimized using stochastic gradient descent ( SGD ) .", "entities": [[5, 8, "MethodName", "stochastic gradient descent"], [9, 10, "MethodName", "SGD"]]}, {"text": "In practice , we integrate \u00151and\u00152into the alternating ratio among the loss functions , and we calibrate a 1:1:5 ratio among Lnre(\u0012 )", "entities": [[11, 12, "MetricName", "loss"]]}, {"text": "+ \u00153Lpenalty ( \u0012E),LD adv(\u0012D)andLE adv(\u0012C E).\u00153is set as 0:02 .", "entities": []}, {"text": "4 Experiments 4.1 Datasets and Evaluation We evaluate our models on a multi - lingual relation extraction dataset developed by Lin et al . ( 2017 ) .", "entities": [[15, 17, "TaskName", "relation extraction"]]}, {"text": "The dataset consists of English and Chinese data , and has 176relations including a special relation NA indicating that there is no relation between entities .", "entities": []}, {"text": "The whole dataset is divided into three parts for training , validation and test .", "entities": []}, {"text": "The statistics of the dataset are listed in Table 1 .", "entities": []}, {"text": "Dataset #", "entities": []}, {"text": "Rel # Sent # Fact Dataset # Rel # Sent # Fact EnglishTraining 176 1,022,239 47,638 ChineseTraining 176 940,595 42,536 Validation 176 80,191 2,192 Validation 176 82,699 2,192 Test 176 162,018 4,326 Test 176 167,224 4,326 Table 1 : Statistics of the dataset We evaluate all models by the held - out evaluation following previous works ( Mintz et al . , 2009 ; Lin et al . , 2017 ) .", "entities": []}, {"text": "In experiments , we report precision - recall curves of recall under 0:3since we focus more on the performance of those top - ranked results .", "entities": []}, {"text": "To give a complete view of the performance , we also report the area under the curve ( AUC ) .", "entities": [[18, 19, "MetricName", "AUC"]]}, {"text": "4.2 Experiment Settings Following the settings of previous works , we use the pre - trained word embeddings learned by Skip - Gram as the initial word embeddings .", "entities": [[16, 18, "TaskName", "word embeddings"], [26, 28, "TaskName", "word embeddings"]]}, {"text": "We implement the MNRE framework proposed by Lin et al .", "entities": []}, {"text": "( 2017 ) by ourselves .", "entities": []}, {"text": "For fair comparision , we set most of the hyperparameters following Lin et al .", "entities": []}, {"text": "( 2017 ) .", "entities": []}, {"text": "We list the best setting of hyperparameters in Table 2 .", "entities": []}, {"text": "1162Batch SizeB 160 Convolution Kernel Size m 3 Learning Rate \u000b  0.002 Dropout Probability pfor CNN and RNN 0.5 Hidden Layer Dimension khfor CNN", "entities": [[3, 4, "MethodName", "Convolution"], [4, 6, "HyperparameterName", "Kernel Size"], [8, 10, "HyperparameterName", "Learning Rate"], [12, 13, "MethodName", "Dropout"]]}, {"text": "230 Dropout Probability pdfor the Discriminator 0.1 Hidden Layer Dimension khfor RNN 200 Word Dimension kw 50 Hidden Layer Dimension kdfor the Discriminator 2048 Position Dimension kp 5 Table 2 : Parameter settings .", "entities": [[1, 2, "MethodName", "Dropout"], [23, 24, "DatasetName", "2048"], [26, 27, "MethodName", "kp"]]}, {"text": "0.00 0.05 0.10 0.15 0.20 0.25 0.30 Recall0.50.60.70.80.91.0PrecisionAMNRE - CNN MNRE - CNN CNN - Joint CNN - Share CNN - CN CNN - EN 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Recall0.50.60.70.80.91.0PrecisionAMNRE - RNN MNRE - RNN RNN - Joint RNN - Share RNN - CN RNN - EN Figure 2 : The aggregated precision - recall curves for proposed models and various baseline models .", "entities": []}, {"text": "Left : models with CNN as sentence encoders .", "entities": []}, {"text": "Right : models with RNN as sentence encoders .", "entities": []}, {"text": "4.3 Overall Evaluation Results To evaluate the effectiveness of our proposed models AMNRE - CNN andAMNRE - RNN , we compare the proposed models with various neural methods : MNRE - CNN andMNRE - RNN are multi - lingual attention - based NRE models with CNN and RNN sentence encoders respectively ( Lin et al . , 2017 ) ; CNN - EN andRNN - EN are vanilla selective - attention NRE models trained with English data , which are the state - of - the - art models in mono - lingual RE ( Lin et al . , 2016 ) ; CNN - CN andRNN - CN are trained with Chinese data ; CNN - Joint andRNN - Joint are naive joint models which predict relations by directly summing up ranking scores of both English and Chinese ; CNN - Share andRNN - Share are another naive joint models which train English and Chinese models with shared relation embeddings .", "entities": []}, {"text": "The results of precision - recall curves are shown in Figure 2 and the results of AUC are shown in Table 3 .", "entities": [[16, 17, "MetricName", "AUC"]]}, {"text": "From the results , we have the following observations : ( 1 ) Both for CNN and RNN , the models jointly utilizing English and Chinese sentences outperform the models only using mono - lingual sentences .", "entities": []}, {"text": "This demonstrates that the rich information in multi - lingual data is useful and can signi\ufb01cantly enhance existing NRE models .", "entities": []}, {"text": "( 2 ) The -Joint models achieve similar performance with the -Share models , and both of them underperform the MNRE andAMNRE models .", "entities": []}, {"text": "They all bene\ufb01t from the multi - lingual information , but the models with multi - lingual attentions can better take advantage of multi - lingual data .", "entities": []}, {"text": "It indicates that designing targeted schemes to extract rich multi - lingual information is crucial .", "entities": []}, {"text": "( 3)AMNRE achieves the best results among all the baseline models over the entire range of recall in Figure 2 , even as compared with MNRE .AMNRE", "entities": []}, {"text": "also outperforms MNRE with3percentage points increasing in the AUC results .", "entities": [[8, 9, "MetricName", "AUC"]]}, {"text": "It indicates our proposed framework which explicitly encodes languageconsistent and language - individual semantics better extract multi - lingual information , and therefore lead to the signi\ufb01cant improvement in RE performance .", "entities": []}, {"text": "Models CNN - EN CNN - CN CNN - Joint CNN - Share MNRE - CNN AMNRE - CNN AUC 36.6 33.2 37.1 37.0 43.4 46.2 Models RNN - EN RNN - CN RNN - Joint RNN - Share MNRE - RNN AMNRE - RNN AUC 34.5 34.4 36.5 37.6 44.2 47.3 Table 3 : The AUC results of different models ( % ) .", "entities": [[19, 20, "MetricName", "AUC"], [45, 46, "MetricName", "AUC"], [56, 57, "MetricName", "AUC"]]}, {"text": "1163 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Recall0.50.60.70.80.91.0PrecisionAMNRE - CNN - EN MNRE - CNN - EN CNN - EN AMNRE - CNN - CN MNRE - CNN - CN CNN - CN 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Recall0.50.60.70.80.91.0PrecisionAMNRE - RNN - EN MNRE - RNN - EN RNN - EN AMNRE - RNN - CN MNRE - RNN - CN RNN - CNFigure 3 : The aggregated precision - recall curves for proposed models and various baseline models in the mono - lingual scenario .", "entities": []}, {"text": "Left : models with CNN as sentence encoders .", "entities": []}, {"text": "Right : models with RNN as sentence encoders .", "entities": []}, {"text": "Models CNN - EN MNRE - EN AMNRE - EN RNN - EN MNRE - EN AMNRE - EN AUC 36.6 39.6 42.7 34.5 42.2 43.2 Models CNN - CN MNRE - CN AMNRE - CN RNN - CN MNRE - CN AMNRE - CN AUC 33.2 34.6 37.9 33.5 34.8 36.4 Table 4 : The AUC results of different models in the mono - lingual scenario ( % ) .", "entities": [[19, 20, "MetricName", "AUC"], [45, 46, "MetricName", "AUC"], [56, 57, "MetricName", "AUC"]]}, {"text": "4.4 Mono - lingual Evaluation Results To further verify that every mono - lingual RE models can bene\ufb01t from our proposed framework , which explicitly consider language - consistent relation patterns , we train models with multi - lingual data and evaluate the performance of these models in the mono - lingual RE scenario .", "entities": []}, {"text": "To show the results clearly , we report the precision - recall curves in Figure 3 and the AUC results in Table 4 .", "entities": [[18, 19, "MetricName", "AUC"]]}, {"text": "From the results , we can observe that : ( 1 ) As compared with the models directly learned with the mono - lingual data , the models exploiting the multi - lingual information perform better in the mono - lingual scenario .", "entities": []}, {"text": "This demonstrates that there is latent consistency among languages , and grasping this consistency from multi - lingual data can provide additional information for models in each language to enhance their results in the mono - lingual scenario .", "entities": []}, {"text": "( 2 ) Our proposed models achieve the best precision over the entire range of recall and also signi\ufb01cantly improve the AUC results as compared with both MNRE and mono - lingual RE models .", "entities": [[21, 22, "MetricName", "AUC"]]}, {"text": "It indicates that due to the consistent semantic space in our framework , language - consistent information lying in the multi - lingual data is better mined and serve the mono - lingual scenario .", "entities": []}, {"text": "4.5 Effectiveness of Adversarial Training and Orthogonality Constraints We adopt an adversarial training strategy to fuse the features from different languages to extract consistent relation patterns .", "entities": []}, {"text": "Orthogonality constraints are also adopted to separate the consistent and individual feature spaces .", "entities": []}, {"text": "To measure the effectiveness of them , we conduct an ablation study which compares the proposed models with the similar models but without adversarial training strategy ( AMNRE - noA ) , without orthogonality constraints ( AMNRE - noO ) , and without both of them ( AMNRE - noBoth ) .", "entities": []}, {"text": "The AUC results are shown in Table 5 .", "entities": [[1, 2, "MetricName", "AUC"]]}, {"text": "We can observe that both the adversarial training strategy and orthogonality constraints have signi\ufb01cant in\ufb02uence on the performance of our proposed model .", "entities": []}, {"text": "This demonstrates the effectiveness of adversarial training strategy and orthogonality constraints for multi - lingual RE .", "entities": []}, {"text": "To give a more intuitive picture of the effect of these two mechanisms , we visualize the distribution of sentence feature embeddings encoded by the individual and consistent encoders using t - SNE ( Maaten and Hinton , 2008 ) .", "entities": []}, {"text": "The results are shown in Figure 4 .", "entities": []}, {"text": "Figure 4(a ) shows that there are obvious differences between the feature embeddings encoded from the same sentences by individual and consistent encoders .", "entities": []}, {"text": "It indicates the orthogonality constraints are effective to separate the individual and consistent latent spaces .", "entities": []}, {"text": "From the comparison between Figure", "entities": []}, {"text": "1164 ( a ) The same English sentences encoded by the consistent encoder ( yellow ) and individual encoder ( blue ) .", "entities": []}, {"text": "( b ) The English sentences ( yellow ) and the Chinese sentences ( blue ) encoded by their own consistent encoders without adversarial training .", "entities": []}, {"text": "( c ) The English sentences ( yellow ) and the Chinese sentences ( blue ) encoded by their own consistent encoders with adversarial training .", "entities": []}, {"text": "Figure 4 : The visualization of sentence feature embeddings with different mechanisms .", "entities": []}, {"text": "Models AMNRE - CNN AMNRE - CNN - noA AMNRE - CNN - noO AMNRE - CNN - noBoth AUC 46.2 44.1 43.9 41.3 Models AMNRE - RNN AMNRE - RNN - noA AMNRE - RNN - noO AMNRE - RNN - noBoth AUC 47.3 43.5 43.5 42.2 Table 5 : The AUC results of the proposed models and ablated models.(% ) 4(b ) and Figure 4(c ) , we can observe that the feature embeddings from different languages are wellmixed due to the adversarial training strategy .", "entities": [[19, 20, "MetricName", "AUC"], [43, 44, "MetricName", "AUC"], [52, 53, "MetricName", "AUC"]]}, {"text": "We can more easily to grasp latent consistency among languages after multi - feature fusion .", "entities": []}, {"text": "5 Case Study To further show the effectiveness of our proposed model to extract the language - consistent semantic information , we give an example in Table 6 .", "entities": []}, {"text": "We adopt the cosine similarity to measure the similarity between sentence embeddings encoded by consistent encoders .", "entities": [[10, 12, "TaskName", "sentence embeddings"]]}, {"text": "The \ufb01rst sentence in the middle column is the standard Chinese translation of the left sentence , thus they share the same semantic information .", "entities": []}, {"text": "We observe that in our proposed model , the feature embedding similarity between these two sentences are signi\ufb01cantly higher than the other English sentences sharing entity pair and relational fact but differing in semantics .", "entities": []}, {"text": "It indicates that sentences in different languages containing similar semantics can be indeed encoded into adjacent places of the consistent space in our framework .", "entities": []}, {"text": "Relation : Located in Cosine Similarity There are eighteen small glaciers in the North Island onMount Ruapehu .\u5317 \u5317", "entities": []}, {"text": "\u5317\u5c9b \u5c9b \u5c9b\u7684\u9c81 \u9c81 \u9c81\u963f \u963f \u963f\u4f69 \u4f69", "entities": []}, {"text": "\u4f69\u80e1 \u80e1 \u80e1\u5c71 \u5c71 \u5c71\u4e0a\u6709\u5341\u516b\u4e2a\u5c0f\u51b0\u5ddd \u3002 0.584 .", "entities": []}, {"text": ". .", "entities": []}, {"text": "the bottom of the North Island of New Zealand up to the area of Mount Ruapehu .0.3538", "entities": []}, {"text": "It is located on the south - eastern North Island volcanic plateau , . . .", "entities": []}, {"text": "south - east of Mount Ruapehu .0.342", "entities": []}, {"text": "Table 6 : The example highlighting entities for the case study by measuring the cosine similarities between the sentence in the left column and each sentence in the middle column .", "entities": []}, {"text": "6 Conclusion and Future Work", "entities": []}, {"text": "In this paper , we introduce a novel adversarial multi - lingual neural relation extraction model ( AMNRE ) .", "entities": [[13, 15, "TaskName", "relation extraction"]]}, {"text": "AMNRE builds both individual and consistent representations for each sentence to consider the consistency and diversity of relation patterns among languages .", "entities": []}, {"text": "It also employs an adversarial training strategy and orthogonality constraints to ensure the consistent representations could extract the languageconsistent features to extract relations .", "entities": []}, {"text": "The experimental results on real - world datasets demonstrate that", "entities": []}, {"text": "1165our AMNRE could effectively encode the consistency and diversity among languages , and achieves state - of - the - art performance in relation extraction .", "entities": [[23, 25, "TaskName", "relation extraction"]]}, {"text": "We will explore the following directions as our future work : ( 1 ) AMNRE can be also implemented in the scenario of multiple languages , and this paper shows the effectiveness of AMNRE on the dataset with two languages ( English and Chinese ) .", "entities": []}, {"text": "In the future , we will explore AMNRE in much more other languages such as French , Spanish , and so on .", "entities": []}, {"text": "( 2 ) AMNRE simply aligns the sentences with similar semantics in different languages with an adversarial training strategy .", "entities": []}, {"text": "In fact , machine translation is a typical approach to align sentences in various languages .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In the future , we will combine machine translation with our model to further improve the extraction performance .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "Acknowledgments We thank Jiacheng Zhang for his help .", "entities": []}, {"text": "This work is supported by the National Natural Science Foundation of China ( NSFC No . 61621136008 , 61772302 ) and Tsinghua University Initiative Scienti\ufb01c Research Program ( 20151080406 ) .", "entities": []}, {"text": "This research is part of the NExT++ project , supported by the National Research Foundation , Prime Minister \u2019s Of\ufb01ce , Singapore under its IRC@Singapore Funding Initiative .", "entities": []}, {"text": "References Konstantinos Bousmalis , George Trigeorgis , Nathan Silberman , Dilip Krishnan , and Dumitru Erhan .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Domain separation networks .", "entities": []}, {"text": "In Proceedings of NIPS , pages 343\u2013351 .", "entities": []}, {"text": "Xinchi Chen , Zhan Shi , Xipeng Qiu , and Xuanjing Huang . 2017 .", "entities": []}, {"text": "Adversarial multi - criteria learning for chinese word segmentation .", "entities": [[6, 9, "TaskName", "chinese word segmentation"]]}, {"text": "In Proceedings of ACL , pages 1193\u20131203 .", "entities": []}, {"text": "Kyunghyun Cho , Bart Van Merri \u00a8enboer , Dzmitry Bahdanau , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "On the properties of neural machine translation : encoder - decoder approaches .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of SSST-8 .", "entities": []}, {"text": "Manaal Faruqui and Shankar Kumar .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "2015 .", "entities": []}, {"text": "Multilingual open relation extraction using cross - lingual projection .", "entities": [[2, 4, "TaskName", "relation extraction"]]}, {"text": "InProceedings of NAACL , pages 1351\u20131356 .", "entities": []}, {"text": "Jun Feng , Minlie Huang , Li Zhao , Yang Yang , and Xiaoyan Zhu . 2018 .", "entities": []}, {"text": "Reinforcement learning for relation classi\ufb01cation from noisy data .", "entities": []}, {"text": "In Proceedings of AAAI .", "entities": []}, {"text": "Yaroslav Ganin , Evgeniya Ustinova , Hana Ajakan , Pascal Germain , Hugo Larochelle , Mario Marchand , and Victor Lempitsky .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Domain - adversarial training of neural networks .", "entities": []}, {"text": "JMLR , 17(1):2096\u20132030 .", "entities": []}, {"text": "Ian J Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Generative adversarial nets .", "entities": []}, {"text": "In Proceedings of NIPS , pages 2672\u20132680 .", "entities": []}, {"text": "Ian J Goodfellow , Jonathon Shlens , and Christian Szegedy .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Explaining and harnessing adversarial examples .", "entities": []}, {"text": "InProceedings of ICML , pages 1\u201310 .", "entities": []}, {"text": "Xu Han , Zhiyuan Liu , and Maosong Sun .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural knowledge acquisition via mutual attention between knowledge graph and text .", "entities": []}, {"text": "In Proceedings of AAAI .", "entities": []}, {"text": "Raphael Hoffmann , Congle Zhang , Xiao Ling , Luke Zettlemoyer , and Daniel S. Weld .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Knowledge - based weak supervision for information extraction of overlapping relations .", "entities": []}, {"text": "In Proceedings of ACL , pages 541\u2013550 .", "entities": []}, {"text": "Guoliang Ji , Kang Liu , Shizhu He , Jun Zhao , et al . 2017 .", "entities": []}, {"text": "Distant supervision for relation extraction with sentencelevel attention and entity descriptions .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of AAAI , pages 3060\u20133066 .", "entities": []}, {"text": "Yankai Lin , Shiqi Shen , Zhiyuan Liu , Huanbo Luan , and Maosong Sun . 2016 .", "entities": []}, {"text": "Neural relation extraction with selective attention over instances .", "entities": [[1, 3, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of ACL , pages 2124\u20132133 .", "entities": []}, {"text": "Yankai Lin , Zhiyuan Liu , and Maosong Sun . 2017 .", "entities": []}, {"text": "Neural relation extraction with multi - lingual attention .", "entities": [[1, 3, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of ACL , pages 34\u201343 .", "entities": []}, {"text": "Pengfei Liu , Xipeng Qiu , and Xuanjing Huang . 2017a .", "entities": []}, {"text": "Adversarial multi - task learning for text classi\ufb01cation .", "entities": [[1, 5, "TaskName", "multi - task learning"]]}, {"text": "In Proceedings of ACL , pages 1\u201310 .", "entities": []}, {"text": "Tianyu Liu , Kexiang Wang , Baobao Chang , and Zhifang Sui .", "entities": []}, {"text": "2017b .", "entities": []}, {"text": "A soft - label method for noise - tolerant distantly supervised relation extraction .", "entities": [[11, 13, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of EMNLP , pages 1790\u20131795 .", "entities": []}, {"text": "1166Laurens Van Der Maaten and Geoffrey Hinton . 2008 .", "entities": []}, {"text": "Visualizing data using t - sne .", "entities": []}, {"text": "JMLR , 9(12):2579\u20132605 .", "entities": []}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "In Proceedings of ICLR , pages 1\u201312 .", "entities": []}, {"text": "Mintz , Mike , Steven , Jurafsky , and Dan . 2009 .", "entities": []}, {"text": "Distant supervision for relation extraction without labeled data .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of ACL , pages 1003\u20131011 .", "entities": []}, {"text": "Gwangbeen Park and Woobin I m. 2016 .", "entities": []}, {"text": "Image - text multi - modal representation learning by adversarial backpropagation .", "entities": [[6, 8, "TaskName", "representation learning"]]}, {"text": "arXiv preprint arXiv:1612.08354 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sebastian Riedel , Limin Yao , and Andrew Mccallum .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Modeling relations and their mentions without labeled text .", "entities": []}, {"text": "In Proceedings of ECML - PKDD , pages 148\u2013163 .", "entities": []}, {"text": "Cicero Nogueira Dos Santos , Bing Xiang , and Bowen Zhou .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Classifying relations by ranking with convolutional neural networks .", "entities": []}, {"text": "In Proceedings of ACL , pages 626\u2013634 .", "entities": []}, {"text": "Richard Socher , Brody Huval , Christopher D Manning , and Andrew Y Ng . 2012 .", "entities": []}, {"text": "Semantic compositionality through recursive matrix - vector spaces .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 1201\u20131211 .", "entities": []}, {"text": "Patrick Verga , David Belanger , Emma Strubell , Benjamin Roth , and Andrew Mccallum .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Multilingual relation extraction using compositional universal schema .", "entities": [[1, 3, "TaskName", "relation extraction"]]}, {"text": "In NAACL , pages 886\u2013896 .", "entities": []}, {"text": "Yi Wu , David Bamman , and Stuart Russell . 2017 .", "entities": []}, {"text": "Adversarial training for relation extraction .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of EMNLP , pages 1778\u20131783 .", "entities": []}, {"text": "Yang Xiang , Qingcai Chen , Xiaolong Wang , and Yang Qin . 2017 .", "entities": []}, {"text": "Answer selection in community question answering via attentive neural networks .", "entities": [[0, 2, "TaskName", "Answer selection"], [3, 6, "TaskName", "community question answering"]]}, {"text": "IEEE SPL , 24(4):505\u2013509 .", "entities": [[1, 2, "MethodName", "SPL"]]}, {"text": "Dmitry Zelenko , Chinatsu Aone , and Anthony Richardella . 2003 .", "entities": []}, {"text": "Kernel methods for relation extraction .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "JMLR , 3(2):1083\u20131106 .", "entities": []}, {"text": "Daojian Zeng , Kang Liu , Siwei Lai , Guangyou Zhou , and Jun Zhao .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Relation classi\ufb01cation via convolutional deep neural network .", "entities": []}, {"text": "In Proceedings of COLING , pages 2335\u20132344 .", "entities": []}, {"text": "Daojian Zeng , Kang Liu , Yubo Chen , and Jun Zhao . 2015 .", "entities": []}, {"text": "Distant supervision for relation extraction via piecewise convolutional neural networks .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of EMNLP , pages 1753\u20131762 .", "entities": []}, {"text": "Wenyuan Zeng , Yankai Lin , Zhiyuan Liu , and Maosong Sun . 2017 .", "entities": []}, {"text": "Incorporating relation paths in neural relation extraction .", "entities": [[5, 7, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Xiangrong Zeng , Shizhu He , Kang Liu , and Jun Zhao .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Large scaled relation extraction with reinforcement learning .", "entities": [[2, 4, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of AAAI .", "entities": []}, {"text": "Dongxu Zhang and Dong Wang .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Relation classi\ufb01cation via recurrent neural network .", "entities": []}, {"text": "arXiv preprint arXiv:1508.01006 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Huaping Zhong , Jianwen Zhang , Zhen Wang , Hai Wan , and Zheng Chen .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Aligning knowledge and text embeddings by entity descriptions .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 267\u2013272 .", "entities": []}]