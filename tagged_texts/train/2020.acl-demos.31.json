[{"text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 279\u2013286 July 5 - July 10 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics279ADVISER : A Toolkit for Developing Multi - modal , Multi - domain and Socially - engaged Conversational Agents Chia - Yu Li , Daniel Ortega , Dirk V \u00a8ath , Florian Lux , Lindsey Vanderlyn , Maximilian Schmidt , Michael Neumann , Moritz V \u00a8olkel , Pavel Denisov , Sabrina Jenne , Zorica Kacarevic and Ngoc Thang Vu * Institute for Natural Language Processing ( IMS ) , University of Stuttgart thangvu@ims.uni-stuttgart.de Abstract We present ADVISER1- an open - source , multi - domain dialog system toolkit that enables the development of multi - modal ( incorporating speech , text and vision ) , sociallyengaged ( e.g. emotion recognition , engagement level prediction and backchanneling ) conversational agents .", "entities": [[112, 114, "TaskName", "emotion recognition"]]}, {"text": "The \ufb01nal Python - based implementation of our toolkit is \ufb02exible , easy to use , and easy to extend not only for technically experienced users , such as machine learning researchers , but also for less technically experienced users , such as linguists or cognitive scientists , thereby providing a \ufb02exible platform for collaborative research .", "entities": []}, {"text": "1 Introduction Dialog systems or chatbots , both text - based and multi - modal , have received much attention in recent years , with an increasing number of dialog systems in both industrial contexts such as AmazonAlexa , Apple Siri , Microsoft Cortana , Google Duplex , XiaoIce ( Zhou et", "entities": [[45, 46, "DatasetName", "Google"]]}, {"text": "al . , 2018 ) and Furhat2 , as well as academia such as MuMMER ( Foster et al . , 2016 ) and Alana ( Curry et al . , 2018 ) .", "entities": []}, {"text": "However , open - source toolkits and frameworks for developing such systems are rare , especially for developing multi - modal systems comprised of speech , text , and vision .", "entities": []}, {"text": "Most of the existing toolkits are designed for developing dialog systems focused only on core dialog components , with or without the option to access external speech processing services ( Bohus and Rudnicky , 2009 ; Baumann and Schlangen , 2012 ; Lison and Kennington , 2016 ; Ultes et al . , 2017 ; Ortega et al . , 2019 ; Lee et al . , 2019 ) .", "entities": []}, {"text": "To the best of our knowledge , there are only two *", "entities": []}, {"text": "All authors contributed equally .", "entities": []}, {"text": "1Link to open - source code : https://github.com/ DigitalPhonetics / adviser 2https://docs.furhat.iotoolkits , proposed in ( Foster et al . , 2016 ) and ( Bohus et al . , 2017 ) , that support developing dialog agents using multi - modal processing and social signals ( Wagner et al . , 2013 ) .", "entities": []}, {"text": "Both provide a decent platform for building systems , however , to the best of our knowledge , the former is not open - source , and the latter is based on the .NET platform , which could be less convenient for non - technical users such as linguists and cognitive scientists , who play an important role in dialog research .", "entities": []}, {"text": "In this paper , we introduce a new version of ADVISER - previously a text - based , multi - domain dialog system toolkit ( Ortega et al . , 2019 ) - that supports multi - modal dialogs , including speech , text and vision information processing .", "entities": []}, {"text": "This provides a new option for building dialog systems that is open - source and Python - based for easy use and fast prototyping .", "entities": []}, {"text": "The toolkit is designed in such a way that it is modular , \ufb02exible , transparent , and user - friendly for both technically experienced and less technically experienced users .", "entities": []}, {"text": "Furthermore , we add novel features to ADVISER , allowing it to process social signals and to incorporate them into the dialog \ufb02ow .", "entities": []}, {"text": "We believe that these features will be key to developing humanlike dialog systems because it is well - known that social signals , such as emotional states and engagement levels , play an important role in human computer interaction ( McTear et al . , 2016 ) .", "entities": []}, {"text": "However in contrast to open - ended dialog systems ( Weizenbaum , 1966 ) , our toolkit focuses on task - oriented applications ( Bobrow et al . , 1977 ) , such as searching for a lecturer at the university ( Ortega et al . , 2019 ) .", "entities": []}, {"text": "The purpose we envision for dialog systems developed using our toolkit is not the same as the objective of a social chatbot such as XiaoIce ( Zhou et al . , 2018 ) .", "entities": [[21, 22, "TaskName", "chatbot"]]}, {"text": "Rather than promoting \u201c an AI companion with an emotional connection to satisfy the human need for communication , affection , and social belonging \u201d ( Zhou et al . , 2018 ) , ADVISER", "entities": []}, {"text": "280helps develop dialog systems that support users in ef\ufb01ciently ful\ufb01lling concrete goals , while at the same time considering social signals such as emotional states and engagement levels so as to remain friendly and likeable .", "entities": []}, {"text": "2 Objectives The main objective of this work is to develop a multi - domain dialog system toolkit that allows for multi - modal information processing and that provides different modules for extracting social signals such as emotional states and for integrating them into the decision making process .", "entities": [[45, 47, "TaskName", "decision making"]]}, {"text": "The toolkit should be easy to use and extend for users of all levels of technical experience , providing a \ufb02exible collaborative research platform .", "entities": []}, {"text": "2.1 Toolkit Design We extend and substantially modify our previous , text - based dialog system toolkit ( Ortega et al . , 2019 ) while following the same design choices .", "entities": []}, {"text": "This means that our toolkit is meant to optimize the following four criteria : Modularity , Flexibility , Transparency andUser - friendliness at different levels .", "entities": []}, {"text": "This is accomplished by decomposing the dialog system into independent modules ( services ) , which in turn are either rule - based , machine learning - based or both .", "entities": []}, {"text": "These services can easily be combined in different orders / architectures , providing users with \ufb02exible options to design new dialog architectures .", "entities": []}, {"text": "2.2 Challenges & Proposed Solutions Multi - modality The main challenges in handling multi - modality are a ) the design of a synchronization infrastructure and b ) the large range of different latencies from different modalities .", "entities": []}, {"text": "To alleviate the former , we use the publisher / subscriber software pattern presented in section 4 to synchronize signals coming from different sources .", "entities": []}, {"text": "This software pattern also allows for services to run in a distributed manner .", "entities": []}, {"text": "By assigning computationally heavy tasks such as speech recognition and speech synthesis to a more powerful computing node , it is possible to reduce differences in latency when processing different modalities , therefore achieving more natural interactions .", "entities": [[7, 9, "TaskName", "speech recognition"], [10, 12, "TaskName", "speech synthesis"]]}, {"text": "Socially - Engaged Systems Determining the ideal scope of a socially - engaged dialog system is a complex issue , that is which information should be extracted from users and how the system can best Figure 1 : Tracking emotion states and engagement levels using multi - modal information .", "entities": [[39, 40, "DatasetName", "emotion"]]}, {"text": "react to these signals .", "entities": []}, {"text": "Here we focus on two major social signals : emotional states and engagement levels ( see section 3.1 ) , and maintain an internal user state to track them over the course of a dialog .", "entities": []}, {"text": "Note that the toolkit is designed in such a way that any social signal could be extracted and leveraged in the dialog manager .", "entities": []}, {"text": "In order to react to social signals extracted from the user , we provide an initial affective policy module ( see section 3.5 ) and an initial affective NLG module ( see section 3.7 ) , which could be easily extended to more sophisticated behavior .", "entities": []}, {"text": "Furthermore , we provide a backchanneling module that enables the dialog system to give feedback to users during conversations .", "entities": []}, {"text": "Utilizing these features could lead to increased trust and enhance the impression of an empathetic system .", "entities": []}, {"text": "3 Functionalities 3.1 Social Signal Processing We present the three modules of ADVISER for processing social signals : ( a ) emotion recognition , ( b ) engagement level prediction , and ( c ) backchanneling .", "entities": [[21, 23, "TaskName", "emotion recognition"]]}, {"text": "Figure 1 illustrates an example of our system tracking emotion states and engagement levels .", "entities": [[9, 10, "DatasetName", "emotion"]]}, {"text": "Multi - modal Emotion Recognition For recognizing a user \u2019s emotional state , all three available modalities \u2013 text , audio , and vision \u2013 can potentially be exploited , as they can deliver complementary information ( Zeng et al . , 2009 ) .", "entities": [[3, 5, "TaskName", "Emotion Recognition"]]}, {"text": "Therefore , the emotion recognition module can subscribe to the particular input streams of interest ( see section 4 for details ) and apply emotion prediction either in a time - continuous fashion or discretely per turn .", "entities": [[3, 5, "TaskName", "emotion recognition"], [24, 25, "DatasetName", "emotion"]]}, {"text": "In our example implementation in the toolkit , we integrate speech emotion recognition , i.e. using the acoustic signal as features .", "entities": [[10, 13, "TaskName", "speech emotion recognition"]]}, {"text": "Based on the work presented in ( Neumann and Vu , 2017 ) we use log Mel \ufb01lterbank coef\ufb01cients as input to convolutional neural networks ( CNNs ) .", "entities": []}, {"text": "For the sake of", "entities": []}, {"text": "281modularity , three separate models are employed for predicting different types of labels : ( a ) basic emotions fangry , happy , neutral , sad g , ( b ) arousal levels flow , medium , high g , and ( c ) valence levelsfnegative , neutral , positive g.", "entities": []}, {"text": "The models are trained on the IEMOCAP dataset ( Busso et al . , 2008 ) .", "entities": [[6, 7, "DatasetName", "IEMOCAP"]]}, {"text": "The output of the emotion recognition module consists of three predictions per user turn , which can then be used by the user state tracker ( see section 3.4 ) .", "entities": [[4, 6, "TaskName", "emotion recognition"]]}, {"text": "For future releases , we plan to incorporate multiple training datasets as well as visual features .", "entities": []}, {"text": "Engagement Level Prediction User engagement is closely related to states such as boredom and level of interest , with implications for user satisfaction and task success ( Forbes - Riley et", "entities": []}, {"text": "al . , 2012 ; Schuller et al . , 2009 ) .", "entities": []}, {"text": "In ADVISER , we assume that eye activity serves as an indicator of various mental states ( Schuller et al . , 2009 ; Niu et al . , 2018 ) and implement a gaze tracker that monitors the user \u2019s direction of focus via webcam .", "entities": []}, {"text": "Using OpenFace 2.2.0 , a toolkit for facial behavior analysis ( Baltrusaitis et al . , 2018 ) , we extract the features gaze angle xandgaze angle y , which capture left - right and up - down eye movement , for each frame and compute the deviation from the central point of the screen .", "entities": []}, {"text": "If the deviation exceeds a certain threshold for a certain number of seconds , the user is assumed to look away from the screen , thereby disengaging .", "entities": []}, {"text": "Thus , the output of our engagement level prediction module is the binary decision flooking , not looking g.", "entities": []}, {"text": "Both the spatial and temporal sensitivity can be adjusted , such that developers have the option to decide how farandhow long the user \u2019s gaze can stray from the central point until they are considered to be disengaged .", "entities": []}, {"text": "In an adaptive system , this information could be used to select re - engagement strategies , e.g. using an affective template ( see section 3.7 ) .", "entities": []}, {"text": "Backchanneling In a conversation , a backchannel ( BC ) is a soft interjection from the listener to the speaker , with the purpose of signaling acknowledgment or reacting to what was just uttered .", "entities": []}, {"text": "Backchannels contribute to a successful conversation \ufb02ow ( Clark and Krych , 2004 ) .", "entities": []}, {"text": "Therefore , we add an acoustic backchannel module to create a more human - like dialog experience .", "entities": []}, {"text": "For backchannel prediction , we extract 13 Mel - frequency - cepstral coef\ufb01cients from the user \u2019s speech signal , which form the input tothe convolutional neural network based on Ortega et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "The model assigns one of three categories from the proactive backchanneling theory ( Goodwin , 1986 ) to each user utterance fno - backchannel , backchannel - continuer and backchannel - assessment g.", "entities": []}, {"text": "The predicted category is used to add the backchannel realization , such as Right orUh - huh , to the next system response .", "entities": []}, {"text": "3.2 Speech Processing Automatic Speech Recognition ( ASR ) The speech recognition module receives a speech signal as input , which can come from an internal or external microphone , and outputs decoded text .", "entities": [[3, 6, "TaskName", "Automatic Speech Recognition"], [10, 12, "TaskName", "speech recognition"]]}, {"text": "The speci\ufb01c realization of ASR can be interchanged or adapted , for example for new languages or different ASR methods .", "entities": []}, {"text": "We provide an end - to - end ASR model for English based on the Transformer neural network architecture .", "entities": [[15, 16, "MethodName", "Transformer"]]}, {"text": "We use the end - to - end speech processing toolkit ESPnet ( Watanabe et al . , 2018 ) and the IMS - speech English multi - dataset recipe ( Denisov and Vu , 2019 ) , updated to match the LibriSpeech Transformer - based system in ESPnet ( Karita et al . , 2019 ) and to include more training data .", "entities": [[11, 12, "MethodName", "ESPnet"], [42, 43, "DatasetName", "LibriSpeech"], [43, 44, "MethodName", "Transformer"], [48, 49, "MethodName", "ESPnet"]]}, {"text": "Training data comprises the LibriSpeech , Switchboard , TED - LIUM 3 , AMI , WSJ , Common V oice 3 , SWC , V oxForge and M - AILABS datasets with a total amount of 3249 hours .", "entities": [[4, 5, "DatasetName", "LibriSpeech"], [8, 12, "DatasetName", "TED - LIUM 3"]]}, {"text": "As input features , 80 - dimensional log Mel \ufb01lterbank coef\ufb01cients are used .", "entities": []}, {"text": "Output of the ASR model is a sequence of subword units , which include single characters as well as combinations of several characters , making the model lexicon independent .", "entities": []}, {"text": "Speech Synthesis For ADVISER \u2019s voice output , we use the ESPnet - TTS toolkit ( Hayashi et al . , 2019 ) , which is an extension of the ESPnet toolkit mentioned above .", "entities": [[0, 2, "TaskName", "Speech Synthesis"], [11, 12, "MethodName", "ESPnet"], [30, 31, "MethodName", "ESPnet"]]}, {"text": "We use FastSpeech as the synthesis model speeding up mel - spectrogram generation by a factor of 270 and voice generation by a factor of 38 compared to autoregressive Transformer TTS ( Ren et al . , 2019 ) .", "entities": [[29, 30, "MethodName", "Transformer"]]}, {"text": "We use a Parallel WaveGAN ( Yamamoto et al . , 2020 ) to generate waveforms that is computationally ef\ufb01cient and achieves a high mean opinion score of 4.16 .", "entities": [[4, 5, "MethodName", "WaveGAN"]]}, {"text": "The FastSpeech and WaveGAN models were trained with 24 hours of the LJSpeech dataset from a single speaker ( Ito , 2017 ) and are capable of generating voice output in real - time when using a GPU .", "entities": [[3, 4, "MethodName", "WaveGAN"], [12, 13, "DatasetName", "LJSpeech"]]}, {"text": "The synthesis can run on any device in a distributed system .", "entities": []}, {"text": "Additionally , we optimize the synthesizer for abbreviations , such asProf . , Univ . , IMS , NLP , ECTS andPhD , as well", "entities": []}, {"text": "282as for German proper names , such as street names .", "entities": []}, {"text": "These optimizations can be easily extended .", "entities": []}, {"text": "Turn Taking To make interacting with the system more natural , we use a naive end - of - utterance detection .", "entities": []}, {"text": "Users indicate the start of their turn by pressing a hotkey , so they can choose to pause the interaction .", "entities": []}, {"text": "The highest absolute peak of each recording chunk is then compared with a prede\ufb01ned threshold .", "entities": []}, {"text": "If a certain number of sequential chunks do not peak above the threshold , the recording stops .", "entities": []}, {"text": "We are currenlty in the process of planning more sophisticated turn taking models , such as Skantze et al .", "entities": []}, {"text": "( 2015 )", "entities": []}, {"text": ".", "entities": []}, {"text": "3.3 Natural Language Understanding The natural language understanding ( NLU ) unit parses the textual user input ( De Mori et al . , 2008 ) - or the output from the speech recognition system and extracts the user action type , generally referred to as intent in goal - oriented dialog systems ( e.g. Inform andRequest ) , as well as the corresponding slots and values .", "entities": [[1, 4, "TaskName", "Natural Language Understanding"], [5, 8, "TaskName", "natural language understanding"], [32, 34, "TaskName", "speech recognition"], [48, 52, "TaskName", "goal - oriented dialog"]]}, {"text": "The domain - independent , rulebased NLU presented in Ortega et al .", "entities": []}, {"text": "( 2019 ) is integrated into ADVISER and adapted to the new domains presented in section 5 . 3.4 State Tracking Belief State Tracking ( BST ): The BST tracks the history of user informs and the user action types , requests , with one BST entry per turn .", "entities": []}, {"text": "This information is stored in a dictionary structure that is built up , as the user provides more details and the system has a better understanding of user intent .", "entities": []}, {"text": "User State Tracking ( UST ): Similar to the BST , the UST tracks the history of the user \u2019s state over the course of a dialog , with one entry per turn .", "entities": []}, {"text": "In the current implementation , the user state consists of the user \u2019s engagement level , valence , arousal , and emotion category ( details in section 3.1 ) .", "entities": [[21, 22, "DatasetName", "emotion"]]}, {"text": "3.5 Dialog Policies Policies To determine the correct system action , we provide three types of policy services : a handcrafted and a reinforcement learning policy for \ufb01nding entities from a database ( Ortega et al . , 2019 ) , as well as a handcrafted policy for looking up information through an API call .", "entities": []}, {"text": "Both handcrafted policies use a series of rules to help the user \ufb01nd a single entity or , once an entity has been found ( or directly provided by the user ) , \ufb01nd information about that entity .", "entities": []}, {"text": "The reinforcement learning ( RL)policy \u2019s action - value function is approximated by a neural network which outputs a value for each possible system action , given the vectorized representation of a turn \u2019s belief state as input .", "entities": []}, {"text": "The neural network is constructed as proposed in V \u00a8ath and Vu ( 2019 ) following a duelling architecture ( Wang et al . , 2016 ) .", "entities": []}, {"text": "It consists of two separate calculation streams , each with its own layers , where the \ufb01nal layer yields the action - value function .", "entities": []}, {"text": "For off - policy batch - training , we make use of prioritized experience replay ( Schaul et al . , 2015 ) .", "entities": [[12, 15, "MethodName", "prioritized experience replay"]]}, {"text": "Affective Policy", "entities": []}, {"text": "In addition , we have also implemented a rule - based affective policy service that can be used to determine the system \u2019s emotional response .", "entities": []}, {"text": "As this policy is domain - agnostic , predicting the next system emotion output rather than the next system action , it can be used alongside any of the previously mentioned policies .", "entities": [[12, 13, "DatasetName", "emotion"]]}, {"text": "User Simulator To support automatic evaluation and to train the RL policy , we provide a user simulator service outputting at the user acts level .", "entities": []}, {"text": "As we are concerned with task - oriented dialogs here , the user simulator has an agenda - based ( Schatzmann et al . , 2007 ) architecture and is randomly assigned a goal at the beginning of the dialog .", "entities": []}, {"text": "Each turn , it then works to \ufb01rst respond to the system utterance , and then after to ful\ufb01ll its own goal .", "entities": []}, {"text": "When the system utterance also works toward ful\ufb01lling the user goal , the RL policy is rewarded by achieving a shorter total dialog turn count ( Ortega et al . , 2019 ) .", "entities": []}, {"text": "3.6 External Information Resources ADVISER supports three options to access information from external information sources .", "entities": []}, {"text": "In addition to being able to query information from SQL - based databases , we add two new options that includes querying information via APIs and from knowledge bases ( e.g. Wikidata ( Vrande \u02c7ci\u00b4c and Kr \u00a8otzsch , 2014 ) ) .", "entities": []}, {"text": "For example , when a user asks a simple question - Where was Dirk Nowitzki born ?", "entities": []}, {"text": ", our pretrained neural network predicts the topic entity -Dirk", "entities": []}, {"text": "Nowitzki - and the relation - place of birth .", "entities": []}, {"text": "Then , the answer is automatically looked up using Wikidata \u2019s SPARQL endpoint .", "entities": []}, {"text": "3.7 Natural Language Generation ( NLG ) In the NLG service , the semantic representation of the system act is transformed into natural language .", "entities": []}, {"text": "ADVISER currently uses a template - based approach to NLG in which each possible system act is mapped to exactly one utterance .", "entities": []}, {"text": "A special", "entities": []}, {"text": "283 NLG NLG Domain1 NLU NLU Policy ...", "entities": []}, {"text": "Domain BST ...", "entities": []}, {"text": "NLG Domain2NLU PolicyDialogSystemSpeechSynthesis SpeechRecognition Voice InputVideoInputTextInput DomainTrackerInput User State   User Simulator BackchannelingUtility LoggerSpeakerGUI OutputFeature Processing Engagement GazeEmotionRemote   GPU   Server \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd Figure 2 : Example ADVISER toolkit con\ufb01guration : Grey represents backend components , blue represents domain - speci\ufb01c services , and all other colors represent domain - agnostic services .", "entities": []}, {"text": "Two components are run remotely .", "entities": []}, {"text": "syntax using placeholders reduces the number of templates needed and accounts for correct morphological in\ufb02ections ( Ortega et al . , 2019 ) .", "entities": []}, {"text": "Additionally , we developed an affective NLG service , which allows for different templates to be used depending on the user \u2019s emotional state .", "entities": []}, {"text": "This enables a more sensitive / adaptive system .", "entities": []}, {"text": "For example , if the user is sad and the system does not understand the user \u2019s input , it might try to establish common ground to prevent their mood from getting worse due to the bad news .", "entities": []}, {"text": "An example response would be \u201c As much as I would love to help , I am a bit confused \u201d rather than the more neutral \u201c Sorry I am a bit confused \u201d .", "entities": []}, {"text": "One set of NLG templates can be speci\ufb01ed for each possible emotional state .", "entities": []}, {"text": "At runtime , the utterance is then generated from the template associated with the current system emotion and system action .", "entities": [[16, 17, "DatasetName", "emotion"]]}, {"text": "4 Software Architecture 4.1 Dialog as a Collection of Services To allow for maximum \ufb02exibility in combining and reusing components , we consider a dialog system as a group of services which communicate asynchronously by publishing / subscribing to certain topics .", "entities": []}, {"text": "A service is called as soon as at least one message for all its subscribed topics is received and may additionally publish to one or more topics .", "entities": []}, {"text": "Services can elect to receive the most recent message for a topic ( e.g. up - to - date belief state ) ora list of all messages for that topic since the last service call ( e.g. a list of video frames ) .", "entities": []}, {"text": "Constructing a dialog system in this way allows us to break free from a pipeline architecture .", "entities": []}, {"text": "Each step in the dialog process is represented by one or more services which can operate in parallel or sequentially .", "entities": []}, {"text": "For example , tasks like video and speech capture may be performed and processed in parallel before being synchronized by a user state tracking module subscribing to input from both sources .", "entities": []}, {"text": "Figure 2 illustrates the system architecture .", "entities": []}, {"text": "For debugging purposes , we provide a utility to draw the dialog graph , showing the information \ufb02ow between services , including remote services , and any inconsistencies in publish / subscribe connections .", "entities": []}, {"text": "4.2 Support for Distributed Systems Services are location - transparent and may thus be distributed across multiple machines .", "entities": []}, {"text": "A central dialog system discovers local and remote services and provides synchronization guarantees for dialog initialization and termination .", "entities": []}, {"text": "Distribution of services enables , for instance , a more powerful computer to handle tasks such as real - time text - to - speech generation ( see Figure 2 ) .", "entities": []}, {"text": "This is particularly helpful when multiple resource - heavy tasks are combined into a single dialog system .", "entities": []}, {"text": "4.3 Support for Multi - Domain Systems In addition to providing multi - modal support , the publish / subscribe framework also allows for multidomain support by providing a structure which enables arbitrary branching and rejoining of graph structures .", "entities": []}, {"text": "When a service is created , users simply specify which domain(s ) it should publish / subscribe to .", "entities": []}, {"text": "This , in combination with a domain tracking service , allows for seamless integration of domain - agnostic services ( such as speech input / output ) and domain - speci\ufb01c services ( such as NLU / NLG for the lecturers domain ) .", "entities": []}, {"text": "5 Example Use Cases 5.1 Example Domains We provide several example domains to demonstrate ADVISER \u2019s functionalities .", "entities": []}, {"text": "Databases for lecturers and courses at the Institute for Natural Language Processing ( IMS ) , which we used in the previous version of ADVISER , were adapted to the new system architecture .", "entities": []}, {"text": "As example APIs , we implemented a weather domain that makes calls to", "entities": []}, {"text": "284the OpenWeatherMap", "entities": []}, {"text": "API3and a mensa domain for gathering information from the dining hall at the university of Stuttgart .", "entities": []}, {"text": "Note that affective templates were only added to the lecturers and mensa domain .", "entities": []}, {"text": "All domains can be used within the same dialog , simply by switching the topic .", "entities": []}, {"text": "5.2 An Example Implementation Our toolkit allows for easy creation of a dialog system within a few lines of code as follows .", "entities": []}, {"text": "# domains weather = WeatherDomain ( ) mensa = MensaDomain ( ) # create system from desired services ds = DialogSystem(services= [ DomainTracker(domains=[mensa , weather ] ) , SpeechRecorder ( ) , VideoInput ( ) , EngagementTracker ( ) , EmotionTracker ( ) , WeatherNLU(domain = weather ) , MensaNLU(domain = mensa ) , ... , SpeechOutputPlayer ( ) ] ) ds.run_dialog ( ... )", "entities": []}, {"text": "As a \ufb01rst step , a dialog system object is initialized , which is responsible for coordinating the initialization and graceful termination of all dialog services .", "entities": []}, {"text": "Talking about multiple domains in one dialog is enabled by creating a simple keywordbased domain tracker which is introduced as the \ufb01rst argument to the dialog system .", "entities": []}, {"text": "To make the dialog multi - modal , speech and vision modules are introduced next , along with modules to extract engagement and emotion .", "entities": [[23, 24, "DatasetName", "emotion"]]}, {"text": "So far , all of these modules are domain - agnostic and can be used as shared resources between all domains .", "entities": []}, {"text": "Next , domaindependent services such as NLUs , BSTs and NLGs for weather and mensa , are added .", "entities": []}, {"text": "The following shows an example dialog .", "entities": []}, {"text": "SYSTEM : Hello , please let me know how I can help you , I can discuss the following domains : Mensa Food and Weather .", "entities": []}, {"text": "USER : I could have something to eat .", "entities": []}, {"text": "What does the mensa offer today ?", "entities": []}, {"text": "SYSTEM : What type of dish are you looking for ?", "entities": []}, {"text": "USER : I would like a main dish .", "entities": []}, {"text": "SYSTEM : Should the meal be vegan ?", "entities": []}, {"text": "USER :", "entities": []}, {"text": "Yes .", "entities": []}, {"text": "SYSTEM :", "entities": []}, {"text": "The meal mediterranean Ebly wheat is served today , is a main dish and is vegan .", "entities": []}, {"text": "USER : Okay , cool , I will go there now !", "entities": []}, {"text": "What is the weather like ?", "entities": []}, {"text": "3https://openweathermap.org/apiSYSTEM : The weather in Stuttgart on January 28 at 3 PM is 3 degrees celsius with light snow .", "entities": []}, {"text": "USER : Thank you , ADVISER , good", "entities": []}, {"text": "bye !", "entities": []}, {"text": "SYSTEM : Thank you , good bye .", "entities": []}, {"text": "6 Related Work Other tools for building dialog systems include ConvLab ( Lee et al . , 2019 ) , an open - source , textbased dialog system platform that supports both pipelined architectures and an end - to - end neural architecture .", "entities": []}, {"text": "ConvLab also provides reusable components and supports multi - domain settings .", "entities": []}, {"text": "Other systems are largely text - based , but offer the incorporation of external speech components .", "entities": []}, {"text": "InproTK ( Baumann and Schlangen , 2012 ) , for instance , in which modules communicate by networks via con\ufb01guration \ufb01les , uses ASR based on Sphinx-4 and synthesis based on MaryTTS .", "entities": []}, {"text": "Similarly , RavenClaw ( Bohus and Rudnicky , 2009 ) provides a framework for creating dialog managers ; ASR and synthesis components can be supplied , for example , by connecting to Sphinx and Kalliope .", "entities": []}, {"text": "OpenDial ( Lison and Kennington , 2016 ) relies on probabilistic rules and provides options to connect to speech components such as Sphinx .", "entities": []}, {"text": "Multidomain dialog toolkit - PyDial ( Ultes et al . , 2017 ) supports connection to DialPort .", "entities": []}, {"text": "As mentioned in the introduction , Microsoft Research \u2019s npsiis an open and extensible platform that supports the development of multi - modal AI systems ( Bohus et al . , 2017 ) .", "entities": []}, {"text": "It further offers audio and visual processing , such as speech recognition and face tracking , as well as output , such as synthesis and avatar rendering .", "entities": [[10, 12, "TaskName", "speech recognition"]]}, {"text": "And the MuMMER ( multimodal Mall Entertainment Robot ) project ( Foster et al . , 2016 ) is based on the SoftBank Robotics Pepper platform , and thereby comprises processing of audio- , visual- and social signals , with the aim to develop a socially engaging robot that can be deployed in public spaces .", "entities": [[5, 6, "DatasetName", "Mall"]]}, {"text": "7 Conclusions We introduce ADVISER \u2013 an open - source , multidomain dialog system toolkit that allows users to easily develop multi - modal and socially - engaged conversational agents .", "entities": []}, {"text": "We provide a large variety of functionalities , ranging from speech processing to core dialog system capabilities and social signal processing .", "entities": []}, {"text": "With this toolkit , we hope to provide a \ufb02exible platform for collaborative research in multi - domain , multi - modal , socially - engaged conversational agents .", "entities": []}, {"text": "285References Tadas Baltrusaitis , Amir Zadeh , Yao Chong Lim , and Louis - Philippe Morency .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "OpenFace 2.0 : Facial Behavior Analysis Toolkit .", "entities": []}, {"text": "Timo Baumann and David Schlangen . 2012 .", "entities": [[0, 1, "DatasetName", "Timo"]]}, {"text": "The InproTK 2012 Release .", "entities": []}, {"text": "In NAACL - HLT Workshop on Future Directions and Needs in the Spoken Dialog Community : Tools and Data .", "entities": []}, {"text": "Daniel G. Bobrow , Ronald M. Kaplan , Martin Kay , Donald A. Norman , Henry Thompson , and Terry Winograd . 1977 .", "entities": []}, {"text": "Gus , a frame - driven dialog system .", "entities": []}, {"text": "Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Dan Bohus , Sean Andrist , and Mihai Jalobeanu . 2017 .", "entities": []}, {"text": "Rapid Development of Multimodal Interactive Systems : A Demonstration of Platform for Situated Intelligence .", "entities": []}, {"text": "In ICMI \u2019 17 : Proceedings of the 19th ACM International Conference on Multimodal Interaction , pages 493\u2013494 .", "entities": [[9, 10, "DatasetName", "ACM"]]}, {"text": "Dan Bohus and Alexander I Rudnicky .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "The ravenclaw dialog management framework : Architecture and systems .", "entities": []}, {"text": "Computer Speech & Language , 23(3):332\u2013361 .", "entities": []}, {"text": "Carlos Busso , Murtaza Bulut , Chi - Chun Lee , Abe Kazemzadeh , Emily Mower , Samuel Kim , Jeannette N Chang , Sungbok Lee , and Shrikanth S Narayanan . 2008 .", "entities": []}, {"text": "Iemocap :", "entities": [[0, 1, "DatasetName", "Iemocap"]]}, {"text": "Interactive emotional dyadic motion capture database .", "entities": []}, {"text": "Language resources and evaluation , 42(4 ) .", "entities": []}, {"text": "H. H Clark and M. A Krych .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Speaking while monitoring addressees for understanding .", "entities": []}, {"text": "Journal of Memory and Language .", "entities": []}, {"text": "Amanda Cercas Curry , Ioannis Papaioannou , Alessandro Suglia , Shubham Agarwal , Igor Shalyminov , Xinnuo Xu , Ond \u02d8rej", "entities": []}, {"text": "Du \u02d8sek , Arash Eshghi , Ioannis Konstas , Verena Rieser , and Oliver Lemon .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Alana v2 : Entertaining and Informative Opendomain Social Dialogue using Ontologies and Entity Linking .", "entities": [[12, 14, "TaskName", "Entity Linking"]]}, {"text": "In 1st Proceedings of Alexa Prize .", "entities": []}, {"text": "Renato De Mori , Fr \u00b4 ed\u00b4eric Bechet , Dilek Hakkani - Tur , Michael McTear , Giuseppe Riccardi , and Gokhan Tur . 2008 .", "entities": []}, {"text": "Spoken language understanding .", "entities": [[0, 3, "TaskName", "Spoken language understanding"]]}, {"text": "IEEE Signal Processing Magazine .", "entities": []}, {"text": "Pavel Denisov and Ngoc Thang Vu . 2019 .", "entities": []}, {"text": "Imsspeech : A speech to text tool .", "entities": []}, {"text": "Studientexte zur Sprachkommunikation : Elektronische Sprachsignalverarbeitung 2019 , pages 170\u2013177 .", "entities": []}, {"text": "Kate Forbes - Riley , Diane Litman , Heather Friedberg , and Joanna Drummond .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty - Adaptive Spoken Dialogue System .", "entities": []}, {"text": "In Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 91\u2013102 , Montr \u00b4 eal .", "entities": []}, {"text": "Mary Ellen Foster , Rachid Alami , Olli Gestranius , Oliver Lemon , Marketta Niemel \u00a8a , Jean Marc Odobez , and Amit Kumar Pandey .", "entities": [[23, 24, "DatasetName", "Kumar"]]}, {"text": "2016 .", "entities": []}, {"text": "The MuMMER project : Engaging human - robot interaction in real - world public spaces .", "entities": []}, {"text": "In Processings of the Eighth International Conference on Social Robotics ( ICSR 2016 ) , pages 753\u2013763 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Charles Goodwin .", "entities": []}, {"text": "1986 .", "entities": []}, {"text": "Between and within : Alternative sequential treatments of continuers and assessments .", "entities": []}, {"text": "Journal of Human Studies .", "entities": []}, {"text": "Tomoki Hayashi , Ryuichi Yamamoto , Katsuki Inoue , Takenori Yoshimura , Shinji Watanabe , Tomoki Toda , Kazuya Takeda , Yu Zhang , and Xu Tan . 2019 .", "entities": []}, {"text": "Espnet - tts : Uni\ufb01ed , reproducible , and integratable open source end - to - end text - to - speech toolkit .", "entities": [[0, 1, "MethodName", "Espnet"]]}, {"text": "Keith Ito . 2017 .", "entities": []}, {"text": "The lj speech dataset .", "entities": []}, {"text": "https:// keithito.com/LJ-Speech-Dataset/ .", "entities": []}, {"text": "Shigeki Karita , Nanxin Chen , Tomoki Hayashi , Takaaki Hori , Hirofumi Inaguma , Ziyan Jiang , Masao Someki , Nelson Enrique Yalta Soplin , Ryuichi Yamamoto , Xiaofei Wang , et al . 2019 .", "entities": []}, {"text": "A comparative study on transformer vs rnn in speech applications .", "entities": []}, {"text": "In IEEE Automatic Speech Recognition and Understanding Workshop ( ASRU ) .", "entities": [[2, 5, "TaskName", "Automatic Speech Recognition"]]}, {"text": "Sungjin Lee , Qi Zhu , Ryuichi Takanobu , Zheng Zhang , Yaoqin Zhang , Xiang Li , Jinchao Li , Baolin Peng , Xiujun Li , Minlie Huang , and Jianfeng Gao .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "ConvLab : Multi - Domain End - to - End Dialog System Platform .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 64\u201369 .", "entities": []}, {"text": "Pierre Lison and Casey Kennington .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Opendial : A toolkit for developing spoken dialogue systems with probabilistic rules .", "entities": [[6, 9, "TaskName", "spoken dialogue systems"]]}, {"text": "In Proceedings of ACL2016 system demonstrations , pages 67\u201372 .", "entities": []}, {"text": "Michael Frederick McTear , Zoraida Callejas , and David Griol . 2016 .", "entities": []}, {"text": "The conversational interface , volume 6 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Michael Neumann and Ngoc Thang Vu . 2017 .", "entities": []}, {"text": "Attentive convolutional neural network based speech emotion recognition : A study on the impact of input features , signal length , and acted speech .", "entities": [[5, 8, "TaskName", "speech emotion recognition"]]}, {"text": "In Proceedings of Interspeech .", "entities": []}, {"text": "Xuesong Niu , Hu Han , Jiabei Zeng , Xuran Sun , Shiguang Shan , Yan Huang , Songfan Yang , and Xilin Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Automatic Engagement Prediction with GAP Feature .", "entities": [[4, 5, "DatasetName", "GAP"]]}, {"text": "In ICMI , pages 599\u2013603 , Boulder .", "entities": []}, {"text": "Association for Computing Machinery .", "entities": []}, {"text": "Daniel Ortega , Chia - Yu Li , and Thang Vu . 2020 .", "entities": []}, {"text": "Oh , Jeez ! or uh - huh ?", "entities": []}, {"text": "A listener - aware Backchannel predictor on ASR transcriptions .", "entities": []}, {"text": "In ICASSP 2020 - IEEE International Conference on Acoustics , Speech and Signal Processing , pages 8064\u20138068 .", "entities": []}, {"text": "286Daniel Ortega , Dirk V \u00a8ath , Gianna Weber , Lindsey Vanderlyn , Maximilian Schmidt , Moritz V \u00a8olkel , Zorica Karacevic , and Ngoc Thang Vu . 2019 .", "entities": []}, {"text": "Adviser : A dialog system framework for education & research .", "entities": []}, {"text": "InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 93\u201398 .", "entities": []}, {"text": "Yi Ren , Yangjun Ruan , Xu Tan , Tao Qin , Sheng Zhao , Zhou Zhao , and Tie - Yan Liu . 2019 .", "entities": []}, {"text": "Fastspeech : Fast , robust and controllable text to speech .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 3165\u20133174 .", "entities": []}, {"text": "Jost Schatzmann , Blaise Thomson , Karl Weilhammer , Hui Ye , and Steve Young .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Agenda - based user simulation for bootstrapping a pomdp dialogue system .", "entities": []}, {"text": "In Proceedings of NAACL .", "entities": []}, {"text": "Tom Schaul , John Quan , Ioannis Antonoglou , and David Silver . 2015 .", "entities": []}, {"text": "Prioritized experience replay .", "entities": [[0, 3, "MethodName", "Prioritized experience replay"]]}, {"text": "In Proceedings of ICLR .", "entities": []}, {"text": "Bj\u00a8orn Schuller , Ronald M \u00a8uller , Florian Eyben , J \u00a8urgen Gast , Benedikt H \u00a8ornler , Martin W \u00a8ollmer , Gerhard Rigoll , Anja H \u00a8othker , and Hitoshi Konosu .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Being bored ?", "entities": []}, {"text": "Recognising natural interest by extensive audiovisual integration for real - life application .", "entities": []}, {"text": "Image and Vision Computing , 27:1760\u20131774 .", "entities": []}, {"text": "Gabriel Skantze , Martin Johansson , and Jonas Beskow .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Exploring turn - taking cues in multi - party human - robot discussions about objects .", "entities": []}, {"text": "In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction , pages 67\u201374 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Stefan Ultes , Lina M. Rojas Barahona , Pei - Hao Su , David Vandyke , Dongho Kim , I \u02dcnigo Casanueva , Pawe\u0142 Budzianowski , Nikola Mrk \u02c7si\u00b4c , Tsung - Hsien Wen , Milica Gasic , and Steve Young . 2017 .", "entities": []}, {"text": "PyDial : A Multi - domain Statistical Dialogue System Toolkit .", "entities": []}, {"text": "InProceedings of ACL .", "entities": []}, {"text": "Dirk V \u00a8ath and Ngoc Thang Vu . 2019 .", "entities": []}, {"text": "To combine or not to combine ?", "entities": []}, {"text": "a rainbow deep reinforcement learning agent for dialog policies .", "entities": [[5, 6, "DatasetName", "agent"]]}, {"text": "In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue , pages 62\u201367 .", "entities": []}, {"text": "Denny Vrande \u02c7ci\u00b4c and Markus Kr \u00a8otzsch .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Wikidata : a free collaborative knowledgebase .", "entities": []}, {"text": "Communications of the ACM , 57(10):78\u201385 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Johannes Wagner , Florian Lingenfelser , Tobias Baur , Ionut Damian , Felix Kistler , and Elisabeth Andr \u00b4 e. 2013 .", "entities": []}, {"text": "The social signal interpretation ( ssi ) framework : multimodal signal processing and recognition in real - time .", "entities": []}, {"text": "In Proceedings of the 21st ACM international conference on Multimedia .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Ziyu Wang , Tom Schaul , Matteo Hessel , Hado Van Hasselt , Marc Lanctot , and Nando De Freitas .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Dueling network architectures for deep reinforcement learning .", "entities": [[0, 2, "MethodName", "Dueling network"]]}, {"text": "In Proceedings of ICML .Shinji", "entities": []}, {"text": "Watanabe , Takaaki Hori , Shigeki Karita , Tomoki Hayashi , Jiro Nishitoba , Yuya Unno , Nelson Enrique Yalta Soplin , Jahn Heymann , Matthew Wiesner , Nanxin Chen , Adithya Renduchintala , and Tsubasa Ochiai .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Espnet : End - to - end speech processing toolkit .", "entities": [[0, 1, "MethodName", "Espnet"]]}, {"text": "In Interspeech , pages 2207 \u2013 2211 .", "entities": []}, {"text": "Joseph Weizenbaum .", "entities": []}, {"text": "1966 .", "entities": []}, {"text": "ELIZA :", "entities": []}, {"text": "A Computer Program for the Study of Natural Language Communication Between Man and Machine .", "entities": []}, {"text": "Communications of the ACM , 9(1 ) .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Ryuichi Yamamoto , Eunwoo Song , and Jae - Min Kim . 2020 .", "entities": []}, {"text": "Parallel wavegan : A fast waveform generation model based on generative adversarial networks with multi - resolution spectrogram .", "entities": [[1, 2, "MethodName", "wavegan"]]}, {"text": "In ICASSP 2020 - IEEE International Conference on Acoustics , Speech and Signal Processing , pages 6199\u20136203 .", "entities": []}, {"text": "Zhihong Zeng , Maja Pantic , Glenn I Roisman , and Thomas S Huang .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "A survey of affect recognition methods : Audio , visual , and spontaneous expressions .", "entities": []}, {"text": "IEEE transactions on pattern analysis and machine intelligence , 31(1):39\u201358 .", "entities": []}, {"text": "Li Zhou , Jianfeng Gao , Di Li , and Heung - Yeung Shum .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The Design and Implementation of XiaoIce , an Empathetic Social Chatbot .", "entities": [[10, 11, "TaskName", "Chatbot"]]}, {"text": "Computational Linguistics , pages 1\u201362 .", "entities": []}]