[{"text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 2716 - 2725 July 10 - 15 , 2022 \u00a9 2022 Association for Computational Linguistics Do n\u2019t sweat the small stuff , classify the rest : Sample Shielding to protect text classifiers against adversarial attacks Jonathan Rusert , Padmini Srinivasan University of Iowa { jonathan - rusert , padmini-srinivasan}@uiowa.edu", "entities": []}, {"text": "Abstract Deep learning ( DL ) is being used extensively for text classification .", "entities": [[11, 13, "TaskName", "text classification"]]}, {"text": "However , researchers have demonstrated the vulnerability of such classifiers to adversarial attacks .", "entities": []}, {"text": "Attackers modify the text in a way which misleads the classifier while keeping the original meaning close to intact .", "entities": []}, {"text": "State - of - the - art ( SOTA ) attack algorithms follow the general principle of making minimal changes to the text so as to not jeopardize semantics .", "entities": []}, {"text": "Taking advantage of this we propose a novel and intuitive defense strategy called Sample Shielding .", "entities": []}, {"text": "It is attacker and classifier agnostic , does not require any reconfiguration of the classifier or external resources and is simple to implement .", "entities": []}, {"text": "Essentially , we sample subsets of the input text , classify them and summarize these into a final decision .", "entities": []}, {"text": "We shield three popular DL text classifiers with Sample Shielding , test their resilience against four SOTA attackers across three datasets in a realistic threat setting .", "entities": []}, {"text": "Even when given the advantage of knowing about our shielding strategy the adversary \u2019s attack success rate is < = 10 % with only one exception and often < 5 % .", "entities": []}, {"text": "Additionally , Sample Shielding maintains near original accuracy when applied to original texts .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "Crucially , we show that the \u2018 make minimal changes \u2019 approach of SOTA attackers leads to critical vulnerabilities that can be defended against with an intuitive sampling strategy.1 1 Introduction Text classifiers have become ubiquitous .", "entities": []}, {"text": "Unfortunately , they are subject to attacks from adversaries , typically executed using machine learning methods .", "entities": []}, {"text": "Attackers work by making small modifications to the text that mislead the classifier .", "entities": []}, {"text": "Adversarial attackers are now a growing part of the ecosystem .", "entities": []}, {"text": "Like classifiers , attack algorithms have achieved strong success due to advances in machine learning / deep learning .", "entities": []}, {"text": "Current text attackers , like 1Our code and data are available at : https://github.com/JonRusert/SampleShieldingTextFooler ( Jin et al . , 2020 ) and Bert - Attack ( Li et al . , 2020 ) , are able to reduce near perfect classification accuracy down to 5 % .", "entities": [[42, 43, "MetricName", "accuracy"]]}, {"text": "Additionally , these attackers achieve this while perturbing ( changing ) only a small amount of the original text .", "entities": []}, {"text": "This helps preserve the original meaning so that humans are able to understand the original message even though classifiers are duped .", "entities": []}, {"text": "As a counter , classifier shielding techniques are being explored .", "entities": []}, {"text": "One such approach is adversarial training where the classifier , assumed to have access to the attacker , uses it to generate perturbed texts - these are added to the classifier \u2019s training data .", "entities": []}, {"text": "While this leads to model resilience against thatattacker it leaves the classifier open to attacks by new attackers .", "entities": []}, {"text": "Other defenses involve modifying classifier structure to reduce the information an attacker can glean from it ( Goel et al . , 2020 ) .", "entities": []}, {"text": "However , this type of reconfiguration will not be possible if a third party classifier ( e.g. Google Perspective ) is leveraged .", "entities": [[17, 18, "DatasetName", "Google"]]}, {"text": "Even other approaches involve modifying the input text during classification time , but are currently limited to classifiers built from specific masked language models ( Zeng et al . , 2021 ) or rely on external synonym datasets ( Wang et al . , 2021a ) .", "entities": []}, {"text": "We propose a shielding technique which is attacker - agnostic , does not require additional training / reconfiguration to the classifier , can shield any classifier , does not require an external data source , and can be used in a more realistic threat setting .", "entities": []}, {"text": "We refer to this as Sample Shielding .", "entities": []}, {"text": "Sample Shielding takes advantage of current constraints in SOTA attacks .", "entities": []}, {"text": "Mainly , to preserve original meaning , these make the minimal changes needed to deceive the classifier .", "entities": []}, {"text": "For example , BERT - Attack ( Li et al . , 2020 ) only perturbs up to 16 % of text , and often far less ( e.g. 1.1 % ) for some datasets .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "Thus , if we would look at the 84 % to 99 % of text that is untouched our model would be more likely to classify correctly .", "entities": []}, {"text": "Hence , in Sample Shielding we take many samples of the input2716", "entities": []}, {"text": "Classifier    ( W)Original Text   Classifier   ( W \u2019 )   Decision Sample   Shielding   Sample Shielding Website   Attacker   Incorrect   Decision Perturb text   Correct   Decision Figure 1 : Threat model - Attacker modifies text with feedback from its local classifier W\u2032. Dashed box included in path when attacker knows about Sample Shielding employed by website .", "entities": []}, {"text": "When box excluded knowledge of Sample Shielding is unavailable .", "entities": []}, {"text": "text , classify these individually and combine their decisions as an ensemble to classify the text .", "entities": []}, {"text": "Our contributions are as follows :", "entities": []}, {"text": "1 . We propose a new , intuitive shielding technique called Sample Shielding for text classifiers .", "entities": []}, {"text": "2 . We assess Sample Shielding under a realistic threat model where the attacker can not query a website \u2019s classifier hundreds of times since that pattern is easily detectable by the website .", "entities": []}, {"text": "We run experiments under two conditions , when the attacker has knowledge of Sample Shielding and when it does not .", "entities": []}, {"text": "In both cases the attacker uses a local copy of the websites \u2019 classifier .", "entities": []}, {"text": "This is an optimistic assumption favouring the attacker and thus provides a lower bound to our results .", "entities": []}, {"text": "3 . We test against 4 SOTA text attack algorithms , 3 text datasets and 3 classifiers .", "entities": []}, {"text": "When the attacker does not have knowledge of Sample Shielding , our defense reduces attack success rate from near total decimation 90 - 100 % down to 13 - 36 % , while still maintaining accuracy on original texts .", "entities": [[35, 36, "MetricName", "accuracy"]]}, {"text": "When the attacker has knowledge of Sample Shielding , our defense performs even better , reducing attacks down to 1 - 10 % success rate .", "entities": []}, {"text": "This is partially due to Sample Shielding \u2019s random nature providing unreliable feedback to attackers .", "entities": []}, {"text": "Our success with Sample Shielding is good news for classifiers \u2013 and it raises the bar significantly for the next generation attackers .", "entities": []}, {"text": "We share code and our perturbed text collections for future research .", "entities": []}, {"text": "2 Methodology 2.1 Threat model The typical attack strategy perturbing texts with word synonyms or character substitutions assumes to have query access to the target web site \u2019s classifier ( W ) ( Yoo and Qi , 2021 ; Li et", "entities": []}, {"text": "al . , 2021a ; Ren et al . , 2019 ; Jin et al . , 2020 ; Li et al . , 2020 ; Gargand Ramakrishnan , 2020 ; Jia et al . , 2019 ; Li et al . , 2019 ) .", "entities": []}, {"text": "The text is modified by querying Whundreds or thousands of times , each time with a text version differing only slightly from the previous even by just a single word ( Li et al . , 2020 ; Jin et al . , 2020 ) .", "entities": []}, {"text": "Such a querying pattern can be easily identified as adversarial by the website and countered .", "entities": []}, {"text": "Thus , practically the only way in which such an attack can take place is when the attacker owns a local classifier W\u2032which is either an exact copy of Wor a close enough approximation .", "entities": []}, {"text": "We adopt this more realistic threat model , shown in Figure 1 .", "entities": []}, {"text": "In our threat model the attacker uses feedback from its local W\u2032to generate a final perturbed version that defeats W\u2032or is close enough to do so .", "entities": []}, {"text": "The attacker submits only this final version to the website , expecting Wto make the same error .", "entities": []}, {"text": "However , the website defends Wusing Sample Shielding : sample based pre - processing on the input text , prior to applying W. The attacker may or may not be aware of this fact .", "entities": []}, {"text": "Keeping W = W\u2032which is consistent with other defenses , we evaluate our defense under two conditions : 1 ) The attacker does not know that the website employs Sample Shielding pre - processing when classifying text using W. 2 ) The Sample Shielding step is leaked and the attacker incorporates it locally when using W\u2032to generate the final perturbed text .", "entities": []}, {"text": "We present results from experiments exploring both of these attack conditions .", "entities": []}, {"text": "2.2 Sample Shielding approach Intuition .", "entities": []}, {"text": "Current adversarial attackers have two goals : fool the classifier and maintain the original meaning .", "entities": []}, {"text": "Since they make minimal changes , the extent of perturbation is in fact one of the reported statistics .", "entities": []}, {"text": "For example , ( Li et al . , 2020 ) note that their 10 % perturbation rate is far less than in previous attacks .", "entities": []}, {"text": "( Li et al . , 2019 ) also focus on minimal changes ( 4 % ) needed in support of their attack success rate .", "entities": []}, {"text": "Our defense approach capitalizes on this drive to make minimal changes .", "entities": []}, {"text": "Specifically , in Sample Shielding , we take ksamples each composed of p%of the text .", "entities": []}, {"text": "We choose a pwhich minimizes the chance of a sample including attacked ( modified ) words , while maximizing the content available for the classifier to make a correct classification .", "entities": []}, {"text": "We choose a kwhich is large enough to cover key information but small enough to reduce redundancy .", "entities": []}, {"text": "We classify each sample and combine2717", "entities": []}, {"text": "> 0.5   k sample   predictions \u2026 I enjoyed this movie more than I thought I would .", "entities": []}, {"text": "From multiple viewings it becomes   especially clear how much time and energy the director put into this film .", "entities": []}, {"text": "The choice for   lead actor had me worried but it worked well .", "entities": []}, {"text": "The twist was what really had me hooked . ...", "entities": []}, {"text": "Input Text   \u2026 I enjoyed this movie more than I thought I would .", "entities": []}, {"text": "The twist was   what really had me hooked .", "entities": []}, {"text": "...   \u2026   \u2026 From multiple viewings it becomes especially clear how much time   and energy the director put into this film .", "entities": []}, {"text": "The choice for lead actor had   me worried but it worked well . .", "entities": []}, {"text": "... \u2026 \u2026 enjoyed movie more than I thought I would .", "entities": []}, {"text": "From multiple viewings   it clear how much time and the director into this film .", "entities": []}, {"text": "The choice for   had me it worked well .", "entities": []}, {"text": "twist was what really me hooked .", "entities": []}, {"text": "...   \u2026 I enjoyed this movie more   I would .", "entities": []}, {"text": "From viewings clear how much   time and energy   into film .", "entities": []}, {"text": "choice for lead had me worried but it   worked well .", "entities": []}, {"text": "twist really . ...", "entities": []}, {"text": "kSentence Sampling Word Sampling Classifier   k sample probabilities 0.6 0.2 \u2026 0.1 0.41 0 \u2026 0 0Final Prediction Majority Voting Final Prediction Neural Net   Summarizer 0.6 0.4 \u2026 0.2 0.1sortp Figure 2 : Proposed shielding method .", "entities": [[14, 15, "DatasetName", "0"], [16, 17, "DatasetName", "0"]]}, {"text": "Sentences or words are sampled ktimes at a rate of ppercent ( of the input text ) , the ksamples are classified .", "entities": []}, {"text": "The probabilities are used in a majority vote for the final prediction ( solid box ) , or are sorted and given to a Neural Net Summarizer ( NN or NN - BB ) to made the final prediction ( dotted box ) .", "entities": []}, {"text": "their decisions for the final classification .", "entities": []}, {"text": "We explore two sampling and three decision combining methods .", "entities": []}, {"text": "2.2.1 Sampling methods Random Sampling .", "entities": []}, {"text": "We randomly sample pportions of the text .", "entities": []}, {"text": "We explore both sentences and words as sampled units .", "entities": []}, {"text": "A visualization of random sampling is in Figure 2 .", "entities": []}, {"text": "Shifting Sampling .", "entities": []}, {"text": "We sample the text using a moving window of length p\u00d7length _ of_text .", "entities": []}, {"text": "The first starts at the beginning of the text .", "entities": []}, {"text": "The next window starts right after the previous window ends .", "entities": []}, {"text": "If there is insufficient text for the last window , then it wraps back to include the beginning text .", "entities": []}, {"text": "2.2.2", "entities": []}, {"text": "Decision strategy Majority voting .", "entities": []}, {"text": "This is a simple majority vote across the k samples ( Figure 2 ) .", "entities": []}, {"text": "Classifier trained on sample scores from original texts ( NN ) .", "entities": []}, {"text": "We train a neural network summarizer to make a final class prediction based on the ksample probabilities .", "entities": []}, {"text": "Since sample ID does not carry any information , the input to the neural network is a sorted list of sample probabilities .", "entities": []}, {"text": "The intent is to see if the neural network picks up on latent patterns in the probabilities that are not captured by majority voting ( see Figure 2 ) .", "entities": []}, {"text": "It should be emphasized that the neural network summarizer is trained only on probabilities generated from original texts and does not consider probabilities from attacker modified texts .", "entities": []}, {"text": "We use a simple feed forward neural net composed of 2 linear layers ( size 500 and 300 ) as classification summarizer .", "entities": []}, {"text": "Classifier trained on sample scores from original and attacked texts ( NN - BB ) .", "entities": []}, {"text": "This is similar to the previous strategy except that the training dataincludes scores from original texts and texts that have been modified by the attacker .", "entities": []}, {"text": "Because this assumes more knowledge of the attacker we expect NN - BB to perform better than NN .", "entities": []}, {"text": "The ground truth label for these modified texts is the original correct class label .", "entities": []}, {"text": "3 Experimental Setup 3.1 Datasets We examine three standard datasets in our experiments .", "entities": []}, {"text": "Two have binary class labels ( Yelp , IMDB ) and the third has multi class labels ( AG News ) .", "entities": [[8, 9, "DatasetName", "IMDB"], [18, 20, "DatasetName", "AG News"]]}, {"text": "These have been used in adversarial generation and defense research ( Zeng et al . , 2021 ; Li et al . , 2020 ) .", "entities": []}, {"text": "All datasets can be found via huggingface2 .", "entities": []}, {"text": "1 . IMDB - Movie review dataset for binary sentiment classification .", "entities": [[2, 3, "DatasetName", "IMDB"]]}, {"text": "25k examples are provided for training and testing respectively .", "entities": []}, {"text": "2 . Yelp - Yelp dataset for binary sentiment classification on reviews of businesses extracted from the Yelp Dataset Challenge3 .", "entities": []}, {"text": "560k examples are provided for training and 38k for testing .", "entities": []}, {"text": "3 . AG News - News articles from over 2000 news sources annotated by type of news : Sports , World , Business , and Science / Tech .", "entities": [[2, 4, "DatasetName", "AG News"]]}, {"text": "120k training and 7k test sets are provided .", "entities": []}, {"text": "Following previous research , ( Li et al . , 2020 ; Jin et al . , 2020 ) we use all training data , and evaluate our method on random 1k samples of each dataset for the case where the local classifier does not employ Sample Shielding .", "entities": []}, {"text": "Due to the high amount of queries used by the adversaries , we test on a subset of 100 samples for the case where the attacker \u2019s 2huggingface.co/datasets 3www.yelp.com/dataset/challenge/winners2718", "entities": []}, {"text": "local classifier employs Sample Shielding .4", "entities": []}, {"text": "3.2 Adversarial models We test our text classifier shielding strategy against 4 state - of - the - art ( SOTA ) text classifier attack algorithms .", "entities": []}, {"text": "These algorithms have shown excellent performance in causing misclassifications while still producing readable texts .", "entities": []}, {"text": "We defend against 3 word based attacks : TextFooler ( Jin et al . , 2020 ) , Bert - Attack ( Li et al . , 2020 ) , PWWS ( Ren et al . , 2019 ) .", "entities": []}, {"text": "TextFooler leverages word embeddings for word replacements , Bert - Attack leverages BERT itself by masking words and using BERT suggestions , PWWS selects and weights word replacements from WordNet .", "entities": [[2, 4, "TaskName", "word embeddings"], [12, 13, "MethodName", "BERT"], [19, 20, "MethodName", "BERT"]]}, {"text": "All three use some form of greedy selection for determining which words to replace .", "entities": []}, {"text": "We also defend against a character based attack algorithm , TextBugger ( Li et al . , 2019 ) .", "entities": []}, {"text": "3.3 Victim classifier models We test our shielding approach against 3 standard classifiers5used in previous research , e.g. ( Li et al . , 2021a ; Jin et al . , 2020 ; Li et", "entities": []}, {"text": "al . , 2020 ): 1 . CNN - A word based CNN ( Kim , 2014 ) , with three window sizes ( 3,4,5 ) , 100 filters per window with dropout of 0.3 and Glove embeddings .", "entities": [[36, 38, "MethodName", "Glove embeddings"]]}, {"text": "2 . LSTM - A word based bidirectional LSTM with 150 hidden units .", "entities": [[2, 3, "MethodName", "LSTM"], [7, 9, "MethodName", "bidirectional LSTM"]]}, {"text": "As with the CNN a dropout of 0.3 is used and Glove embeddings are leveraged .", "entities": [[11, 13, "MethodName", "Glove embeddings"]]}, {"text": "3 . BERT - The 12 layer BERT base model which has been fine - tuned on the corresponding dataset .", "entities": [[2, 3, "MethodName", "BERT"], [7, 8, "MethodName", "BERT"]]}, {"text": "These are provided by textattack via huggingface6 .", "entities": []}, {"text": "3.4 Experimental design We run experiments on the combination of the three victim classification models , three datasets , and four attack algorithms .", "entities": []}, {"text": "These combinations are run on both threat model conditions ( attacker is aware/ not aware of SampleShielding ) .", "entities": []}, {"text": "This leads to 72 shielding experiments .", "entities": []}, {"text": "For all attacks , we leverage TextAttack framework7which provides classification algorithms and adversarial text generation algorithms implemented as specified in respective papers ( Morris et al . , 2020 ) .", "entities": [[12, 14, "TaskName", "adversarial text"]]}, {"text": "In all experiments where the attacker does not use Sample Shielding 4We share the original and perturbed texts for replicability .", "entities": []}, {"text": "We note that replicability of previous defenses are limited because the identity of their randomly sampled test instances are not provided .", "entities": []}, {"text": "5We calibrated classifier accuracies against previous research ( Li et al . , 2020 ; Jin et al . , 2020 ) 6huggingface.co/textattack 7textattack.readthedocs.io/en/latest/index.html % of Words Perturbed 0102030", "entities": []}, {"text": "TextFooler Bert - Attack PWWSFigure 3 : Average % of perturbed words for each attack .", "entities": []}, {"text": "Percentages estimated by comparing words in original and perturbed texts .", "entities": []}, {"text": "Since TextBugger adds whitespace in words skewing its percentage it is excluded .", "entities": []}, {"text": "we set k= 100 andp= 0.3 .", "entities": []}, {"text": "While better performance was achieved with other values in preliminary experiments , we chose to go with a single combination of pandkfor simplicity .", "entities": []}, {"text": "In experiments where the attacker uses Sample Shielding pre - processing we reduce kto30for efficiency .", "entities": []}, {"text": "Except where otherwise noted , majority voting is used to generate results .", "entities": []}, {"text": "Additionally , shifting sampling ( Section 2.2.1 ) shielding typically achieved 10 - 20 points lower accuracy compared to random , thus we do not include it in the results .", "entities": [[16, 17, "MetricName", "accuracy"]]}, {"text": "3.5 Evaluation measures We examine accuracy and Attack Success Rate : accuracy = # examples _ classified _ correctly # total _ examples ( 1 ) ASR = Original Acc.\u2212Attacked Acc .", "entities": [[5, 6, "MetricName", "accuracy"], [11, 12, "MetricName", "accuracy"], [30, 31, "MetricName", "Acc"]]}, {"text": "Original Acc.(2 ) 4 Results We first present results for the condition where the attacker is not aware of Sample Shielding based pre - processing", "entities": []}, {"text": "and then the results for when the attacker also employs Sample Shielding .", "entities": []}, {"text": "4.1 Condition 1 : Attacker does not know about Sample Shielding Results are in Table 1 .", "entities": []}, {"text": "BERT is the strongest classifier achieving 91 - 100 % accuracy on the original datasets .", "entities": [[0, 1, "MethodName", "BERT"], [10, 11, "MetricName", "accuracy"]]}, {"text": "Attacks are highly successful against unshielded texts .", "entities": []}, {"text": "TextFooler and Bert - Attack are the most successful , dropping accuracies to 0 - 5 % generally .", "entities": [[13, 14, "DatasetName", "0"]]}, {"text": "Attacks were able to achieve strong drops with minimal amount of text perturbed ( about 10 % ) .", "entities": []}, {"text": "Figure 3 shows that the average percent of words perturbed across datasets for each attack are about2719", "entities": []}, {"text": "equal in the mid regions of the plots .", "entities": []}, {"text": "For AG News , attacks are less successful against BERT ; accuracy drops to 19 % in the strongest attack ( TextFooler ) , and only to 49 % in the weakest ( TextBugger ) .", "entities": [[1, 3, "DatasetName", "AG News"], [9, 10, "MethodName", "BERT"], [11, 12, "MetricName", "accuracy"]]}, {"text": "In general , TextBugger , the character - based attacker , is the least effective attacker .", "entities": []}, {"text": "Sample Shielding greatly reduces effectiveness of attacks while maintaining accuracy on original texts .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "The shielded classifier Wmaintains accuracy on original texts to within 7 % of the original accuracy .", "entities": [[4, 5, "MetricName", "accuracy"], [15, 16, "MetricName", "accuracy"]]}, {"text": "Crucially , for attacked texts we see accuracy improve to between 60 and 80 % ( from post attack range of 0 - 5 % generally ) .", "entities": [[7, 8, "MetricName", "accuracy"], [21, 22, "DatasetName", "0"]]}, {"text": "For example , TextFooler causes BERT \u2019s accuracy to drop from 91 % to 1 % for IMDB , however , Sample Shieldingreturns accuracy to 78 % .", "entities": [[5, 6, "MethodName", "BERT"], [7, 8, "MetricName", "accuracy"], [17, 18, "DatasetName", "IMDB"], [23, 24, "MetricName", "accuracy"]]}, {"text": "In other words , the effectiveness of the attack is reduced from 99 % effective to 14 % effective .", "entities": []}, {"text": "Additionally , accuracy on the original texts is maintained ( 91.3 to 91.5 ) .", "entities": [[2, 3, "MetricName", "accuracy"]]}, {"text": "This pattern is seen in the other attack classifier models and dataset combinations as well .", "entities": [[10, 12, "DatasetName", "and dataset"]]}, {"text": "For Yelp , LSTM drops from 92.5 to 0.7 when attacked by BERTAttack , however , Word sampling brings it back up to 66.7 , while achieving an original accuracy of 87.8 .", "entities": [[3, 4, "MethodName", "LSTM"], [29, 30, "MetricName", "accuracy"]]}, {"text": "Overall , accuracy after shielding ranges from 60 to 80 % ( avg : 70 ) , which corresponds to a 13 36 ( avg : 25 ) attack success rate .", "entities": [[2, 3, "MetricName", "accuracy"]]}, {"text": "Sample Shielding effective against both word based and character based attacks .", "entities": []}, {"text": "The results show effectiveness regardless of type of attack ( word or character based ) .", "entities": []}, {"text": "For example , all 4 attacks bring the original accuracy of LSTM from 88.3 down to \u223c0 for IMDB .", "entities": [[9, 10, "MetricName", "accuracy"], [11, 12, "MethodName", "LSTM"], [18, 19, "DatasetName", "IMDB"]]}, {"text": "However , word sampling brings the accuracy back up to \u223c66 .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "This is a great reduction in attack effectiveness .", "entities": []}, {"text": "Again , similar trends are seen for the other classifiers , CNN is reduced from 94.1 to \u22645.5 for Yelp , but word sampling brings it back up to 60 - 70 % .", "entities": []}, {"text": "Word sampling outperforms sentence sampling for LSTM , CNN , sentence sampling better for BERT .", "entities": [[6, 7, "MethodName", "LSTM"], [14, 15, "MethodName", "BERT"]]}, {"text": "For example , for CNN on IMDB , word sampling increases accuracy more than 15 points over sentence sampling ( 69.8 vs 53.3 ) .", "entities": [[6, 7, "DatasetName", "IMDB"], [11, 12, "MetricName", "accuracy"]]}, {"text": "Similar trends hold for LSTM .", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "However , the opposite is seen for BERT classifiers .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "For BERT on IMDB , we see an average of 6.5 higher points for sentence sampling over word sampling .", "entities": [[1, 2, "MethodName", "BERT"], [3, 4, "DatasetName", "IMDB"]]}, {"text": "These results are not surprising as LSTM and CNN leverage word embeddings for classification , while BERT leverages the context of the entire sentence .", "entities": [[6, 7, "MethodName", "LSTM"], [10, 12, "TaskName", "word embeddings"], [16, 17, "MethodName", "BERT"]]}, {"text": "Word sampling is more appropriate for shorttexts .", "entities": []}, {"text": "With AG news , we see a large drop in effectiveness of sentence sampling .", "entities": [[1, 3, "DatasetName", "AG news"]]}, {"text": "The average length of AG News is 43 words compared to 157 and 215 words of Yelp and IMDB respectively ( Li et al . , 2020 ) .", "entities": [[4, 6, "DatasetName", "AG News"], [18, 19, "DatasetName", "IMDB"]]}, {"text": "This shorter length makes it more difficult to sample enough sentences .", "entities": []}, {"text": "For Textfooler - CNN , sentence sampling is only able to increase accuracy from the attacked value of 0.4 to 13.2 .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "However , word sampling is much more effective , increasing accuracy to 77.3 .", "entities": [[10, 11, "MetricName", "accuracy"]]}, {"text": "Text length may be crucial when choosing between the two strategies for a dataset .", "entities": []}, {"text": "Neural Network summarizer shows some improvements over majority voting .", "entities": []}, {"text": "Comparisons of majority voting and the two neural net - based decision strategies are in Table 2 .", "entities": []}, {"text": "We experimented on the two binary datasets8 .", "entities": []}, {"text": "Replacing majority voting with a simple neural net ( NN ) gave somewhat disappointing results - accuracies stay the same or decrease slightly in all cases except for LSTM on the Yelp dataset ( increases ) .", "entities": [[28, 29, "MethodName", "LSTM"]]}, {"text": "However , when the neural nets are trained on perturbed texts ( NN - BB ) , we see increases .", "entities": []}, {"text": "For example , CNN vs TextFooler on Yelp , the neural net increases accuracy from 64.9 to 72.2 , reducing attack success rate from 31 to 23 .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "Possibly a more sophisticated neural net , such as a sequence aware LSTM , might better exploit patterns in the sorted probabilities .", "entities": [[12, 13, "MethodName", "LSTM"]]}, {"text": "4.2 Condition 2 : Attacker knows about Sample Shielding Results are in Table 3 .", "entities": []}, {"text": "As in the previous condition , classifiers perform well on original texts ( Table 1 ) with BERT often achieving the highest accuracies .", "entities": [[17, 18, "MethodName", "BERT"]]}, {"text": "In this setting , every query by an attacker requires ksamples to be processed , which greatly increases attack time .", "entities": []}, {"text": "Thus , we reduce kto 30 for these experiments .", "entities": []}, {"text": "Sample Shielding repels attacks even when attacker uses Sample Shielding .We", "entities": []}, {"text": "see that shielding is extremely successful in almost completely removing the negative effects of the attacks .", "entities": []}, {"text": "For example , on the IMDB - TextFooler combination , attack success rate drops from 100 to 5 for LSTM , 100 to 1 for CNN , and 99 to 6 against BERT .", "entities": [[5, 6, "DatasetName", "IMDB"], [19, 20, "MethodName", "LSTM"], [32, 33, "MethodName", "BERT"]]}, {"text": "The largest protection provided by Sample Shielding ( 100 % ) is for TextBugger vs CNN in IMDB .", "entities": [[17, 18, "DatasetName", "IMDB"]]}, {"text": "The smallest is for 85 % ( PWWS vs LSTM ) .", "entities": [[9, 10, "MethodName", "LSTM"]]}, {"text": "On average the protection is 88.8 % .", "entities": []}, {"text": "The recovered accuracies are only 13 to 0 percent away from the originals .", "entities": [[7, 8, "DatasetName", "0"]]}, {"text": "8AG", "entities": []}, {"text": "News was not included due to the complexity of translating multiple probabilities to a single input.2720", "entities": []}, {"text": "Sample Orig .", "entities": []}, {"text": "TextFooler Bert - Attack TextBugger PWWS Classifier Shielding Acc .", "entities": [[8, 9, "MetricName", "Acc"]]}, {"text": "Acc .", "entities": [[0, 1, "MetricName", "Acc"]]}, {"text": "ASR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "ASR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "ASR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "ASRIMDBLSTMNo Shielding 88.3 0 100 0 100 0.3 100 0.1 100 Shielding - Sentence 85.1 61.4 30 62.0 30 60.3 32 56.2 36 Shielding - Word 85.1 66.0 25 67.0 24 66.0 25 65.7 26 CNNNo Shielding 86.2 0.1 100 0 100 0.3 100 0 100 Shielding - Sentence 84.5 55.3 36 55.2 36 53.6 38 48.9 43 Shielding - Word 84.7 69.8 19 66.7 23 71.6 17 67.8 21 BERTNo Shielding 91.3 1 99 3.7 96 9.2 90 0.7 99 Shielding - Sentence 91.5 78.1 14 79.2 13 80.1 12 78.0 15 Shielding - Word 86.8 74.4 19 71.5 22 78.8 14 63.4 31YelpLSTMNo", "entities": [[3, 4, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [44, 45, "DatasetName", "0"]]}, {"text": "Shielding 92.5 0.3 100 0.7 99 5 95 1.5 98 Shielding - Sentence 90.0 62.3 33 61.1 34 60.5 35 58 37 Shielding - Word 87.8 65.5 29 66.7 28 68.5 26 61.9 33 CNNNo Shielding 94.1 0.8 99 0.4 100 5.5 94 2.4 97 Shielding - Sentence 91.7 58.5 38 54.1 43 57.1 39 50 47 Shielding - Word 88.1 64.9 31 62.2 34 70.4 25 60.2 36 BERTNo Shielding 100 5.9 94 8.3 92 15.5 85 4.9 95 Shielding - Sentence 98.6 74.8 25 72.6 27 79.3 21 68.5 32 Shielding - Word 93.5 69.9 30 75.1 25 78.7 21 71.1 29AG", "entities": []}, {"text": "NewsLSTMNo", "entities": []}, {"text": "Shielding 91.6 1.2 99 0.9 99 16.7 82 15.6 83 Shielding - Sentence 88.8 16.5 82 12.9 86 27.3 70 25.2 72 Shielding - Word 85.1 60.8 34 60.9 34 60.5 34 63.7 30 CNNNo Shielding 91.5 0.4 100 0.3 100 5.2 94 6.3 93 Shielding - Sentence 89.4 13.2 86 13.0 86 17.2 81 15.7 83", "entities": []}, {"text": "Shielding - Word 87.8 77.3 16 67.7 26 74.2 19 80 13 BERTNo Shielding 99.6 18.7 81 22.5 77 49.4 50 38.5 61 Shielding - Sentence 96.4 29.6 70 37.9 62 54.2 46 47.1 53 Shielding - Word 94.5 75.5 24 72.0 28 78.1 22 70.5 29 Table 1 : Results where attacker does not know about Sample Shielding .", "entities": []}, {"text": "Shielding settings : k= 100 , p= 0.3 , majority voting .", "entities": []}, {"text": "Acc : accuracy , ASR : success rate of attack ( % ) , Orig .", "entities": [[0, 1, "MetricName", "Acc"], [2, 3, "MetricName", "accuracy"]]}, {"text": "Acc . : accuracy on original texts .", "entities": [[0, 1, "MetricName", "Acc"], [3, 4, "MetricName", "accuracy"]]}, {"text": "Sampling Orig .", "entities": []}, {"text": "TextFooler Bert - Attack TextBugger PWWS Classifier Strategy Acc .", "entities": [[8, 9, "MetricName", "Acc"]]}, {"text": "Acc .", "entities": [[0, 1, "MetricName", "Acc"]]}, {"text": "SR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "SR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "SR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "SRIMDBLSTMNo Shielding 88.3 0 100 0 100 0.3 100 0.1 100 Maj . V ot . 85.1 66.0 25 67.0 24 66.0 25 65.7 26 NN 85.3 62.5 29 62.1 30 65.4 26 62.4 29 NN - BB 85.3 65.2 26 68.2 23 66.5 25 67.3 24 CNNNo Shielding 86.2 0.1 100 0 100 0.3 100 0 100 Maj . V ot . 84.7 69.8 19 66.7 23 71.6 17 67.8 21 NN 84.8 61.7 28 59.6 31 66.7 23 60.0 30 NN - BB 84.8 69.3 20 67.9 21 72.3 16 69.6 19YelpLSTMNo", "entities": [[3, 4, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [52, 53, "DatasetName", "0"], [56, 57, "DatasetName", "0"]]}, {"text": "Shielding 92.5 0.3 100 0.7 99 5 95 1.5 98 Maj . V ot . 87.8 65.5 29 66.7 28 68.5 26 61.9 33 NN 89.0 68.7 26 68.1 26 73.5 21 63.6 31", "entities": []}, {"text": "NN - BB 89 69.7 25 70.0 24 73.5 21 64.9 30 CNNNo Shielding 94.1 0.8 99 0.4 100 5.5 94 2.4 97 Maj . V ot . 88.1 64.9 31 62.2 34 70.4 25 60.2 36 NN 89.9 63.2 33 57.6 39 69.9 26 57.4 39 NN - BB 89.9 72.2 23 69.7 26 72.9 23 67.6 28 Table 2 : Comparing vote summarizers .", "entities": []}, {"text": "Settings : k= 100 , p= 0.3 , word sampling .", "entities": []}, {"text": "Maj . V ot : majority voting , NN : neural network trained on original texts , NN - BB : neural network trained on original + perturbed texts.2721", "entities": []}, {"text": "Sample Orig .", "entities": []}, {"text": "TextFooler Bert - Attack TextBugger PWWS Classifier Strategy Acc .", "entities": [[8, 9, "MetricName", "Acc"]]}, {"text": "Acc .", "entities": [[0, 1, "MetricName", "Acc"]]}, {"text": "ASR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "ASR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "ASR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "ASRIMDBLSTMNo Shielding 91 0 100 0 100 0 100 0 100 Shielding - Word 94 89 5 87 7 89 5 89 5 CNNNo Shielding 86 0 100 0 100 0 100 0 100 Shielding - Word 89 88 1 88 1 89 0 86 3 BERTNo Shielding 90 1 99 4 96 6 93 2 98 Shielding - Word 85 80 6 80 6 84 1 82 4YelpLSTMNo", "entities": [[3, 4, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [30, 31, "DatasetName", "0"], [32, 33, "DatasetName", "0"], [43, 44, "DatasetName", "0"]]}, {"text": "Shielding 95 0 100 0 100 6 94 0 100 Shielding - Word 87 81 7 79 9 78 10 74 15 CNNNo Shielding 96 0 100 0 100 5 95 3 97 Shielding - Word 88 85 3 81 8 81 8 83 6 BERTNo Shielding 100 3 97 10 90 13 87 7 93 Shielding - Word 92 90 2 88 4 91 1 85 8AG", "entities": [[2, 3, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [27, 28, "DatasetName", "0"]]}, {"text": "NewsLSTMNo", "entities": []}, {"text": "Shielding 93 1 99 0 100 16 83 13 86 Shielding - Word 87 78 10 84 3 78 10 84 3 CNNNo Shielding 92 1 99 0 100 7 92 3 97 Shielding - Word 87 81 7 87 0 84 3 83 5 BERTNo Shielding 99 20 78 11 89 60 39 15 85 Shielding - Word 88 81 8 82 7 83 6 85 3 Table 3 : Results where attacker knows about Sample Shielding .", "entities": [[4, 5, "DatasetName", "0"], [27, 28, "DatasetName", "0"], [40, 41, "DatasetName", "0"]]}, {"text": "Shielding settings : k= 30 , p= 0.3 , majority voting .", "entities": []}, {"text": "Acc : accuracy , ASR : success rate of attack ( % ) , Orig .", "entities": [[0, 1, "MetricName", "Acc"], [2, 3, "MetricName", "accuracy"]]}, {"text": "Acc : accuracy on original texts .", "entities": [[0, 1, "MetricName", "Acc"], [2, 3, "MetricName", "accuracy"]]}, {"text": "p020406080 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Bert - Attack TextFooler TextBugger PWWS Figure 4 : Accuracy with various p values for LSTM on IMDB .", "entities": [[19, 20, "MetricName", "Accuracy"], [25, 26, "MethodName", "LSTM"], [27, 28, "DatasetName", "IMDB"]]}, {"text": "Note that kis fixed to 100 .", "entities": []}, {"text": "These results show the power of Sample Shielding as even with knowledge of both the classifier and Sample Shielding , attacks struggle to perturb the text in a manner that causes Wto fail .", "entities": []}, {"text": "Furthermore , the attacks do worse with feedback from Sample Shielding .", "entities": []}, {"text": "This shows the misleading nature of feedback from Sample Shielding , and unreliability when guiding attacks .", "entities": []}, {"text": "5 Additional Analysis 5.1 Parameter search Increasing praises the risk of samples containing increased amounts of perturbed text .", "entities": []}, {"text": "Decreasing kraises the risk of not covering enough of the unperturbed portions of the original text .", "entities": []}, {"text": "While our settings of p= 0.3andk= 100 for our main results are reasonable values ( Table 1 , Table 2 ) they are not necessarily optimal .", "entities": []}, {"text": "Optimal p. Figure 4 shows the results for all comp020406080 0.2 0.4 0.6 0.8 1.0Bert - Attack TextFooler TextBugger PWWSFigure 5 : Accuracy with various p values for LSTM on AG News .", "entities": [[22, 23, "MetricName", "Accuracy"], [28, 29, "MethodName", "LSTM"], [30, 32, "DatasetName", "AG News"]]}, {"text": "Note that kis fixed to 100 .", "entities": []}, {"text": "binations of attacks against LSTM on IMDB with word shielding as the defense , kfixed at 100 .", "entities": [[4, 5, "MethodName", "LSTM"], [6, 7, "DatasetName", "IMDB"]]}, {"text": "As we increase p , we see a continued drop in accuracy which is consistent with the idea that a higher pis more likely to capture perturbed text .", "entities": [[11, 12, "MetricName", "accuracy"]]}, {"text": "The optimal value range appears to be in 0.2 - 0.4 range , although we do not see large drops until 0.6 onward .", "entities": []}, {"text": "We also examined the same combination on AG News ( Figure 5 ) since it \u2019s texts are considerably shorter and found consistent results .", "entities": [[7, 9, "DatasetName", "AG News"]]}, {"text": "Optimal k. Figure 6 shows results for all attacks against LSTM on IMDB with word sampling as the defense , pfixed at 0.3 .", "entities": [[10, 11, "MethodName", "LSTM"], [12, 13, "DatasetName", "IMDB"]]}, {"text": "The optimal kis not as clear as p. We see clear increases after 30 samples , but then the optimal kvaries depending on attack .", "entities": []}, {"text": "However , we see a leveling off around 90 samples , which gives some credence to our chosen kof 100 .", "entities": []}, {"text": "We also found similar results when examining the same combination on AG News ( Figure 7 ) , however , kstabilized lower ( about 50).2722", "entities": [[11, 13, "DatasetName", "AG News"]]}, {"text": "k55606570 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150Bert - Attack TextFooler TextBugger PWWSFigure 6 : Accuracy with various k values for LSTM on IMDB .", "entities": [[23, 24, "MetricName", "Accuracy"], [29, 30, "MethodName", "LSTM"], [31, 32, "DatasetName", "IMDB"]]}, {"text": "Note that pis fixed to 0.3 .", "entities": []}, {"text": "k54565860626466 25 50 75 100 125 150Bert - Attack TextFooler TextBugger PWWS Figure 7 : Accuracy with various k values for LSTM on AG News .", "entities": [[15, 16, "MetricName", "Accuracy"], [21, 22, "MethodName", "LSTM"], [23, 25, "DatasetName", "AG News"]]}, {"text": "Note that pis fixed to 0.3 .", "entities": []}, {"text": "5.2 Reliability of Sample Shielding Due to the randomness of samples , there may be concern over the consistency of Sample Shielding .", "entities": []}, {"text": "To address this , we ran Sample Shielding 100 times on the IMDB attacked texts from Table 3 against BERT classifier .", "entities": [[12, 13, "DatasetName", "IMDB"], [19, 20, "MethodName", "BERT"]]}, {"text": "Each time 30 random samples were used to vote .", "entities": []}, {"text": "As can be observed from Figure 8,Sample", "entities": []}, {"text": "Shielding consistently protects against attacks .", "entities": []}, {"text": "Median accuracies are above 80 % dropping only to 75 % in the worst case .", "entities": []}, {"text": "This points to Sample Shielding as a consistent , reliable defense .", "entities": []}, {"text": "5.3 Comparison with other SOTA Defenses Comparisons are limited as threat models differ .", "entities": []}, {"text": "As noted earlier , other defenses assume a weaker threat model where the attacker queries the webFigure 8 : Boxplots of accuracies when Sample Shielding is applied 100 times to attacked IMDB texts with BERT as classifier .", "entities": [[31, 32, "DatasetName", "IMDB"], [34, 35, "MethodName", "BERT"]]}, {"text": "Red lines : accuracies reported in Table 3.site \u2019s shielded Wdirectly .", "entities": []}, {"text": "To make ours equivalent we compare SOTA results with our accuracies obtained by the attacker using W\u2032alone ( with W = W\u2032 ) .", "entities": []}, {"text": "We calculate accuracies right after the final perturbed text is generated using W\u2032eliminating a followup round of Wwith Sample Shielding .", "entities": [[14, 15, "DatasetName", "followup"]]}, {"text": "Table 4 provides our full results against this weaker threat model .", "entities": []}, {"text": "With BERT as base classifier for AG News , FreeLB++ , an adversarial training technique ( Li et al . , 2021b ) report accuracies of 51 , 56 , and 42 against TextFooler , TextBugger , and Bert - Attack respectively .", "entities": [[1, 2, "MethodName", "BERT"], [6, 8, "DatasetName", "AG News"]]}, {"text": "RanMask ( Zeng et al . , 2021 ) , which uses random masking of words report accuracies of 38 , 45 , and 49 .", "entities": []}, {"text": "In comparison , Sample Shielding achieves 48 , 55 , and 38 respectively outperforming RanMask in 2 out of 3 , while only a fews point behind FreeLB++ .", "entities": []}, {"text": "For IMDB , FreeLB++ reports 45 , 43 , and 40 and RanMask reports 22 , 18 , and 36 respectively .", "entities": [[1, 2, "DatasetName", "IMDB"]]}, {"text": "Equivalently , Sample Shielding achieves 18 , 34 , and 31 .", "entities": []}, {"text": "With some wins and some losses , Sample Shielding is in the mix with current SOTA defenses in this weaker threat model .", "entities": []}, {"text": "However , when deployed as designed for the realistic threat model , it wins over these other defenses by large margins ( see Table 3 ) .", "entities": []}, {"text": "While we do not know how FreeLB++ , RanMask , and similar defenses would perform with our threat model any deterministic shield would give the exact same results when the classifier is applied once again by the website .", "entities": []}, {"text": "5.4 Limitations / Future work First , in future work we will add in direct comparisons to the two closest methods to Sample Shielding(Zeng et", "entities": []}, {"text": "al . , 2021 ; Wang et al . , 2021a ) .", "entities": []}, {"text": "They are similar in spirit as they also work off samples though these are generated differently .", "entities": []}, {"text": "We have not compared with them because these two papers appeared very recently , one last revised in July ( Zeng et al . , 2021 ) and the other appeared in arXiv in September 2021 ( Wang et al . , 2021a ) .", "entities": [[32, 33, "DatasetName", "arXiv"]]}, {"text": "Second , the neural net summarizer leverages a simple linear layer .", "entities": [[9, 11, "MethodName", "linear layer"]]}, {"text": "Other networks , e.g. , LSTM , maybe better at finding patterns in sequential data .", "entities": [[5, 6, "MethodName", "LSTM"]]}, {"text": "In future work we will also explore layering Sample Shielding onto other defense strategies .", "entities": []}, {"text": "Another limitation of our current method is that we do not measure Sample Shielding \u2019s effectiveness on other common text tasks including Natural Language Understanding .", "entities": [[22, 25, "TaskName", "Natural Language Understanding"]]}, {"text": "Additionally , datasets which contain the shortest texts ( e.g. SST2 ) are2723", "entities": [[10, 11, "DatasetName", "SST2"]]}, {"text": "Sample Orig .", "entities": []}, {"text": "TextFooler Bert - Attack TextBugger PWWS Classifier Strategy Acc .", "entities": [[8, 9, "MetricName", "Acc"]]}, {"text": "Acc .", "entities": [[0, 1, "MetricName", "Acc"]]}, {"text": "ASR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "ASR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "ASR Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "ASRIMDBLSTM Local ( W \u2019 )", "entities": []}, {"text": "Word 87 11 87 31 64 28 68 22 75 CNN Local ( W \u2019 )", "entities": []}, {"text": "Word 91 22 76 22 76 32 65 29 68 BERT Local ( W \u2019 )", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "Word 81 18 78 31 62 34 58 26 68 BERT RanMask * 92 22 75 36 58 18 79 - BERT FreeLB++ * 93 45 51 40 57 43 54 - -AGLSTM Local ( W \u2019 )", "entities": [[10, 11, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"]]}, {"text": "Word 88 42 52 31 65 38 57 55 38 CNN Local ( W \u2019 )", "entities": []}, {"text": "Word 86 45 48 28 67 36 58 54 37 BERT Local ( W \u2019 )", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "Word 88 48 45 38 57 55 38 64 27 BERT RanMask * 92 38 59 49 46 45 51 - BERT FreeLB++ * 95 52 46 42 56 56 41 - Table 4 : Results of attack against local model with knowledge of Sample Shielding .", "entities": [[10, 11, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"]]}, {"text": "For all shielding cases , k= 30 , p= 0.3 , and majority voting is used .", "entities": []}, {"text": "Acc . is accuracy , and ASR is success rate of attack ( % ) and Orig .", "entities": [[0, 1, "MetricName", "Acc"], [3, 4, "MetricName", "accuracy"]]}, {"text": "Acc . is accuracy on the original text .", "entities": [[0, 1, "MetricName", "Acc"], [3, 4, "MetricName", "accuracy"]]}, {"text": "Note that the examples used by RanMask and FreeLB++ is not the set of dataset samples as our paper .", "entities": []}, {"text": "not currently tested in our experiments .", "entities": []}, {"text": "Since sample shielding removes texts , it \u2019s performance could drop for these tasks and short texts .", "entities": []}, {"text": "Thus , future work will include these comparisons .", "entities": []}, {"text": "6 Related Work Defenses using voting .", "entities": []}, {"text": "The most similar methods to our own are RanMask and RS&V both appearing within the last five months .", "entities": []}, {"text": "RanMask ( Zeng et al . , 2021 ) randomly masks tokens in input texts .", "entities": []}, {"text": "This random masking occurs ntimes generating ninputs to be fed to a classifier .", "entities": []}, {"text": "RS&V ( Wang et al . , 2021a ) randomly replaces words in the input with synonyms .", "entities": []}, {"text": "This it does ktimes to produce k samples which are then voted on .", "entities": []}, {"text": "If the samples vote for a different label than the label produced by the unsampled input , then the text is labeled as an adversarial text .", "entities": [[24, 26, "TaskName", "adversarial text"]]}, {"text": "Our method is advantageous since it does not rely on specific models ( i.e. Masked Language Model ) or synonym sources .", "entities": []}, {"text": "Adversarial training .", "entities": []}, {"text": "Classifiers train on perturbed data , learning to identify modified versions of the original input ( Wang and Wang , 2020 ; Wang et al . , 2021b ; Zhu et al . , 2020 ;", "entities": []}, {"text": "Li et", "entities": []}, {"text": "al . , 2021b ) .", "entities": []}, {"text": "As an example , Gil et al .", "entities": []}, {"text": "( 2019 ) propose HotFlip which uses white - box knowledge to generate adversarial attacks to train on .", "entities": []}, {"text": "Specifically , they flip tokens based on the gradients of the one - hot input vectors .", "entities": []}, {"text": "However , adversarial defenses are limited to known attackers .", "entities": []}, {"text": "In contrast , Sample Shielding is \u2018 plug - and - play \u2019 as it is a pre - processing step .", "entities": []}, {"text": "Other defenses .", "entities": []}, {"text": "Several other shielding methods exist ( Keller et al . , 2021 ;", "entities": []}, {"text": "Eger et al . , 2019 ; Zhu et al . , 2021 ) .", "entities": []}, {"text": "For example , Rodriguez and Galeano ( 2018 ) defend Perspective ( Google \u2019s toxicity classification model ) by neutralizing adversarial inputs via a negated predicates list .", "entities": [[12, 13, "DatasetName", "Google"]]}, {"text": "Again , these defensesare restricted to contexts where specific lists may be identified , this is not so with Sample Shielding .", "entities": []}, {"text": "7 Conclusion Sample Shielding , an intuitively designed defense which is attacker and classifier agnostic , protects effectively ; reducing ASR from 90 - 100 % down to 14 - 34 % with minimal accuracy loss ( 3 % ) in original texts .", "entities": [[34, 35, "MetricName", "accuracy"], [35, 36, "MetricName", "loss"]]}, {"text": "The randomness ( through sampling ) provides unreliable feedback for attackers , thus it even thwarts attackers who have query access to classifiers protected with Sample Shielding .", "entities": []}, {"text": "Attack strategies will need to increase the amount of perturbation to make sure a majority of samples fail at classification .", "entities": []}, {"text": "However , this will risk semantic integrity .", "entities": []}, {"text": "Thus , we expect Sample Shielding to cause ripples in future adversarial attack strategies while providing text classifiers with a definite advantage .", "entities": [[11, 13, "TaskName", "adversarial attack"]]}, {"text": "References Steffen Eger , G\u00f6zde G\u00fcl \u00b8 Sahin , Andreas R\u00fcckl\u00e9 , Ji - Ung Lee , Claudia Schulz , Mohsen Mesgar , Krishnkant Swarnkar , Edwin Simpson , and Iryna Gurevych .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Text processing like humans do : Visually attacking and shielding NLP systems .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1634\u20131647 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Siddhant Garg and Goutham Ramakrishnan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Bae : Bert - based adversarial examples for text classification .", "entities": [[8, 10, "TaskName", "text classification"]]}, {"text": "Yotam Gil , Yoav Chai , Or Gorodissky , and Jonathan Berant .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "White - to - black : Efficient distillation of black - box adversarial attacks .", "entities": []}, {"text": "In Proceedings of2724", "entities": []}, {"text": "the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1373\u20131379 , Minneapolis , Minnesota . Association for Computational Linguistics .", "entities": []}, {"text": "Akhil Goel , Akshay Agarwal , Mayank Vatsa , Richa Singh , and Nalini K. Ratha .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Dndnet :", "entities": []}, {"text": "Reconfiguring cnn for adversarial robustness .", "entities": [[3, 5, "TaskName", "adversarial robustness"]]}, {"text": "In 2020 IEEE / CVF Conference on Computer Vision and Pattern Recognition Workshops ( CVPRW ) , pages 103 \u2013 110 .", "entities": []}, {"text": "Robin Jia , Aditi Raghunathan , Kerem G\u00f6ksel , and Percy Liang . 2019 .", "entities": []}, {"text": "Certified robustness to adversarial word substitutions .", "entities": []}, {"text": "Di Jin , Zhijing Jin , Joey Tianyi Zhou , and Peter Szolovits . 2020 .", "entities": []}, {"text": "Is bert really robust ?", "entities": []}, {"text": "a strong baseline for natural language attack on text classification and entailment .", "entities": [[8, 10, "TaskName", "text classification"]]}, {"text": "In Proceedings of the AAAI conference on artificial intelligence , volume 34 , pages 8018\u20138025 .", "entities": []}, {"text": "Yannik Keller , Jan Mackensen , and Steffen Eger . 2021 .", "entities": []}, {"text": "BERT - defense : A probabilistic model based on BERT to combat cognitively inspired orthographic adversarial attacks .", "entities": [[0, 1, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"]]}, {"text": "In Findings of the Association for Computational Linguistics : ACL - IJCNLP 2021 , pages 1616\u20131629 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classification .", "entities": [[4, 6, "TaskName", "sentence classification"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1746\u20131751 , Doha , Qatar .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Dianqi Li , Yizhe Zhang , Hao Peng , Liqun Chen , Chris Brockett , Ming - Ting Sun , and Bill Dolan .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "Contextualized perturbation for textual adversarial attack .", "entities": [[4, 6, "TaskName", "adversarial attack"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 5053\u20135069 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jinfeng Li , Shouling Ji , Tianyu Du , Bo Li , and Ting Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Textbugger : Generating adversarial text against real - world applications .", "entities": [[3, 5, "TaskName", "adversarial text"]]}, {"text": "Proceedings 2019 Network and Distributed System Security Symposium .", "entities": []}, {"text": "Linyang Li , Ruotian Ma , Qipeng Guo , Xiangyang Xue , and Xipeng Qiu . 2020 .", "entities": []}, {"text": "BERT - ATTACK : Adversarial attack against BERT using BERT .", "entities": [[0, 1, "MethodName", "BERT"], [4, 6, "TaskName", "Adversarial attack"], [7, 8, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 6193\u20136202 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zongyi Li , Jianhan Xu , Jiehang Zeng , Linyang Li , Xiaoqing Zheng , Qi Zhang , Kai - Wei Chang , and Cho - Jui Hsieh .", "entities": []}, {"text": "2021b .", "entities": []}, {"text": "Searching for an effiective defender : Benchmarking defense against adversarial word substitution .", "entities": []}, {"text": "In EMNLP .John", "entities": []}, {"text": "Morris , Eli Lifland , Jin Yong Yoo , Jake Grigsby , Di Jin , and Yanjun Qi . 2020 .", "entities": []}, {"text": "TextAttack :", "entities": []}, {"text": "A framework for adversarial attacks , data augmentation , and adversarial training in NLP .", "entities": [[6, 8, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 119\u2013126 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shuhuai Ren , Yihe Deng , Kun He , and Wanxiang Che . 2019 .", "entities": []}, {"text": "Generating natural language adversarial examples through probability weighted word saliency .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1085 \u2013 1097 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Nestor Rodriguez and Sergio Rojas Galeano .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Shielding google \u2019s language toxicity model against adversarial attacks .", "entities": []}, {"text": "CoRR , abs/1801.01828 .", "entities": []}, {"text": "Xiaosen Wang , Yifeng Xiong , and Kun He .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "Randomized substitution and vote for textual adversarial example detection .", "entities": []}, {"text": "ArXiv , abs/2109.05698 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Xiaosen Wang , Yichen Yang , Yihe Deng , and Kun He . 2021b .", "entities": []}, {"text": "Adversarial training with fast gradient projection method against synonym substitution based text attacks .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "Zhaoyang Wang and Hongtao Wang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Defense of word - level adversarial attacks via random substitution encoding .", "entities": []}, {"text": "In KSEM .", "entities": []}, {"text": "Jin Yong Yoo and Yanjun Qi . 2021 .", "entities": []}, {"text": "Towards improving adversarial training of nlp models .", "entities": []}, {"text": "Jiehang Zeng , Xiaoqing Zheng , Jianhan Xu , Linyang Li , Liping Yuan , and Xuanjing Huang . 2021 .", "entities": []}, {"text": "Certified robustness to text adversarial attacks by randomized", "entities": []}, {"text": "[ mask ] .", "entities": []}, {"text": "ArXiv , abs/2105.03743 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Bin Zhu , Zhaoquan Gu , Le Wang , and Zhihong Tian .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Treated : Towards universal defense against textual adversarial attacks .", "entities": []}, {"text": "ArXiv , abs/2109.06176 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Chen Zhu , Yu Cheng , Zhe Gan , Siqi Sun , Tom Goldstein , and Jingjing Liu . 2020 .", "entities": []}, {"text": "Freelb : Enhanced adversarial training for natural language understanding .", "entities": [[6, 9, "TaskName", "natural language understanding"]]}, {"text": "InInternational Conference on Learning Representations .2725", "entities": []}]