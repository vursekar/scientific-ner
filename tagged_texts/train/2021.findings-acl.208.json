[{"text": "Findings of the Association for Computational Linguistics : ACL - IJCNLP 2021 , pages 2355\u20132365 August 1\u20136 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics2355P - Stance : A Large Dataset for Stance Detection in Political Domain Yingjie Li|Tiberiu Sosea|Aditya Sawant|Ajith Jayaraman Nair|", "entities": [[13, 15, "TaskName", "Stance Detection"]]}, {"text": "Diana Inkpen}Cornelia", "entities": []}, {"text": "Caragea|", "entities": []}, {"text": "|Computer Science , University of Illinois at Chicago } Electrical Engineering and Computer Science , University of Ottawa |{yli300,tsosea2,asawan8,anair34,cornelia}@uic.edu } { diana.inkpen}@uottawa.ca Abstract Stance detection determines whether the author of a text is in favor of , against or neutral to a speci\ufb01c target and provides valuable insights into important events such as presidential election .", "entities": [[9, 11, "TaskName", "Electrical Engineering"], [23, 25, "TaskName", "Stance detection"]]}, {"text": "However , progress on stance detection has been hampered by the absence of large annotated datasets .", "entities": [[4, 6, "TaskName", "stance detection"]]}, {"text": "In this paper , we present P - S TANCE , a large stance detection dataset in the political domain , which contains 21,574 labeled tweets .", "entities": [[13, 15, "TaskName", "stance detection"]]}, {"text": "We provide a detailed description of the newly created dataset and develop deep learning models on it .", "entities": []}, {"text": "Our best model achieves a macro - average F1 - score of 80.53 % , which we improve further by using semi - supervised learning .", "entities": [[8, 11, "MetricName", "F1 - score"]]}, {"text": "Moreover , our PSTANCE dataset can facilitate research in the \ufb01elds of cross - domain stance detection such as cross - target stance detection where a classi\ufb01er is adapted from a different but related target .", "entities": [[15, 17, "TaskName", "stance detection"], [22, 24, "TaskName", "stance detection"]]}, {"text": "We publicly release our dataset and code.1 1 Introduction Nowadays , people often express their stances toward speci\ufb01c targets ( e.g. , political events or \ufb01gures , religion , or abortion ) on social media .", "entities": []}, {"text": "These opinions can provide valuable insights into important events , e.g. , presidential election .", "entities": []}, {"text": "The goal of the stance detection task is to determine whether the author of a piece of text is in favor of , against , or neutral toward a speci\ufb01c target ( Mohammad et al . , 2016b ; K\u00fc\u00e7\u00fck and Can , 2020 ; ALDayel and Magdy , 2021 ) .", "entities": [[4, 6, "TaskName", "stance detection"]]}, {"text": "Twitter as a social platform has produced a large quantity of user - generated content , which has become a rich source for mining useful information about various topics such as presidential election .", "entities": []}, {"text": "Political \ufb01gures , who usually receive considerable attention and involve themselves in a large number of political events , are great targets to study stance detection .", "entities": [[24, 26, "TaskName", "stance detection"]]}, {"text": "Therefore , detecting the 1https://github.com/chuchun8/PStancestance expressed toward political \ufb01gures on Twitter has drawn a lot of attention in the NLP community ( Mohammad et al . , 2016a ; Sobhani et al . , 2017 ; Darwish et", "entities": []}, {"text": "al . , 2017 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Even though stance detection has received a lot of attention , the annotated data are usually limited , which poses strong challenges to supervised models .", "entities": [[2, 4, "TaskName", "stance detection"]]}, {"text": "Moreover , a limitation of existing datasets is that explicit mentions of targets and surface - level lexical cues that may expose the stance can be widely observed in the data ( Mohammad et al . , 2016a ; Sobhani et", "entities": []}, {"text": "al . , 2017 ; Swami et al . , 2018 ; Darwish et", "entities": []}, {"text": "al . , 2018 ; Conforti et al . , 2020b ; Lai et al . , 2020 ) , which means a model can detect the stance without extracting effective representations for the meanings of sentences ( i.e. , their lexical and compositional semantics ) .", "entities": []}, {"text": "Another limitation of existing datasets , especially the datasets built on social media , is that the average length of tweets is short , which indicates that the data in these previous datasets are less informative and thus the stance can be detected more easily .", "entities": []}, {"text": "In an effort to minimize these drawbacks , we present P - S TANCE , a dataset for stance detection whose primary goal is to bridge these gaps by making it possible to run large - scale evaluations that require a deeper semantic understanding .", "entities": [[18, 20, "TaskName", "stance detection"]]}, {"text": "This large annotated dataset is composed of 21,574 English tweets in the political domain and each tweet is annotated with a stance toward one of three different targets : \u201c Donald Trump , \u201d \u201c Joe Biden , \u201d and \u201c Bernie Sanders . \u201d", "entities": []}, {"text": "Examples from our dataset and their stance labels are shown in Table 1 .", "entities": []}, {"text": "The main motivation of building this dataset is to provide a new benchmark for in - target stance detection where a classi\ufb01er is trained and validated on the same target .", "entities": [[17, 19, "TaskName", "stance detection"]]}, {"text": "However , we show additional interest in constructing a large corpus to facilitate research on cross - target stance detection where a classi\ufb01er is adapted from different but related target .", "entities": [[18, 20, "TaskName", "stance detection"]]}, {"text": "2356Target Tweet Stance Donald Trump I agree , but not convinced Barr has all the evidence for his opinion .", "entities": []}, {"text": "Although , I have zero worries about POTUS being re - elected , if the evidence is compelling enough on top of the tyrannical Covid lockdowns , I m hopeful more people will wake up .", "entities": []}, {"text": "# GiantRedPill # TrumpFavor Donald Trump Take my kids , for example .", "entities": []}, {"text": "At least , I \u2019m TOLD they \u2019re my kids .", "entities": []}, {"text": "No proof .", "entities": []}, {"text": "Don Jr , Ivanka and Eric were all born to an immigrant woman who WASN\u2019T a US Citizen when they were born .", "entities": []}, {"text": "They should n\u2019t have US Citizenship .", "entities": []}, {"text": "DEPORT THEM ALL !", "entities": []}, {"text": "# TrumpAgainst Bernie Sanders Air borne illnesses will only become more common with climate change .", "entities": []}, {"text": "We need to immediately address this and \ufb01ght for Medicare for All or this could be the new normal .", "entities": []}, {"text": "# BernieSandersFavor Bernie Sanders A meat tax ?", "entities": []}, {"text": "Paying off all medical bills ?", "entities": []}, {"text": "I think # bernie has truly gone off the deep end of the pander cliff .", "entities": []}, {"text": "None of these socialists insane , pie - in - the - sky policies would EVER work , or even come in to fruition yet people continue to fall for it .", "entities": []}, {"text": "Unbelievable .", "entities": []}, {"text": "# foxandfriendsAgainst Joe Biden Robyn Seniors , National HBCU Students for Biden Co - Chair and a @FAMU_1887 student , says that she \u2019s thankful that a \" woman will be Vice President in a Biden administration .", "entities": []}, {"text": "\"Favor Joe Biden", "entities": []}, {"text": "The Ukrainians are smarter than our own democratic party !", "entities": []}, {"text": "Shoot , my Dogs are smarter than our own democratic party ! !", "entities": []}, {"text": "# ImpeachmentHoax # NoQuidProQuo # BidenAgainst Table 1 : Examples from our P - S TANCE dataset .", "entities": []}, {"text": "More interestingly , P - Stance enables a new task in stance detection , which is cross - topic stance detection where a classi\ufb01er is adapted from the same target but with different topics in the past .", "entities": [[11, 13, "TaskName", "stance detection"], [19, 21, "TaskName", "stance detection"]]}, {"text": "These tasks , which use labeled training data of a source target and aim to train a model that generalizes well to a destination target with a shifted distribution , hold great practical value .", "entities": []}, {"text": "Our contributions include the following : 1 ) We present P - S TANCE , a large dataset for stance detection composed of 21,574 tweets sampled from over 2.8 million tweets collected from Twitter .", "entities": [[19, 21, "TaskName", "stance detection"]]}, {"text": "P - S TANCE is more than three times larger than the previous benchmark ( Mohammad et al . , 2016a ) and brings additional challenges such as linguistic complexities .", "entities": []}, {"text": "We provide a detailed description and a comprehensive analysis of this dataset ; 2 ) We conduct experiments on the proposed P - S TANCE dataset and establish a strong baseline based on BERTweet ( Nguyen et al . , 2020 ) .", "entities": []}, {"text": "BERTweet achieves a macro - average F1 - score of 80.53 % , which we improve further by using semisupervised learning ; 3 ) The union of P - S TANCE and previous benchmark datasets provides more opportunities for studying other stance detection tasks , e.g. , cross - target stance detection and crosstopic stance detection .", "entities": [[6, 9, "MetricName", "F1 - score"], [41, 43, "TaskName", "stance detection"], [50, 52, "TaskName", "stance detection"], [54, 56, "TaskName", "stance detection"]]}, {"text": "2 Related Work The most common stance detection task on social media is target - speci\ufb01c stance detection ( ALDayel and Magdy , 2021 ) which aims to identify the stance toward a set of \ufb01gures or topics ( Hasan and Ng , 2014 ; Mohammad et", "entities": [[6, 8, "TaskName", "stance detection"], [16, 18, "TaskName", "stance detection"]]}, {"text": "al . , 2016a ; Xu et", "entities": []}, {"text": "al . , 2016;Taul\u00e9 et al . , 2017 ; Swami et al . , 2018 ; Zotova et al . , 2020 ;", "entities": []}, {"text": "Conforti et al . , 2020b ; Lai et al . , 2020 ; Vamvas and Sennrich , 2020 ; Conforti et al . , 2020a ) .", "entities": []}, {"text": "Besides target - speci\ufb01c stance detection , multi - target stance detection ( Sobhani et al . , 2017 ; Darwish et", "entities": [[4, 6, "TaskName", "stance detection"], [10, 12, "TaskName", "stance detection"]]}, {"text": "al . , 2017 ; Li and Caragea , 2021a ) , and claimbased stance detection ( Qazvinian et al . , 2011 ; Derczynski et al . , 2015 ; Ferreira and Vlachos , 2016 ; Bar - Haim et al . , 2017 ; Rao and Pomerleau , 2017 ; Derczynski et al . , 2017 ; Gorrell et al . , 2019 ) are other popular trends of stance detection .", "entities": [[14, 16, "TaskName", "stance detection"], [24, 25, "DatasetName", "Derczynski"], [52, 53, "DatasetName", "Derczynski"], [71, 73, "TaskName", "stance detection"]]}, {"text": "Multitarget stance detection aims to jointly identify the stance toward two or more targets in the same text .", "entities": [[1, 3, "TaskName", "stance detection"]]}, {"text": "Unlike the target - speci\ufb01c stance detection and multi - target stance detection where the target is usually a prominent \ufb01gure or topic , in claimbased stance detection the target is a claim , which could be an article headline or a rumor \u2019s post .", "entities": [[5, 7, "TaskName", "stance detection"], [11, 13, "TaskName", "stance detection"], [26, 28, "TaskName", "stance detection"]]}, {"text": "Interestingly , despite substantial progress on stance detection , large - scale annotated datasets are limited .", "entities": [[6, 8, "TaskName", "stance detection"]]}, {"text": "We compare our P - S TANCE dataset with some existing stance detection datasets in Table 2 .", "entities": [[11, 13, "TaskName", "stance detection"]]}, {"text": "We can observe that the sizes of existing stance detection datasets are smaller than ours except for the WT - WT dataset ( Conforti et al . , 2020b ) in the \ufb01nancial domain .", "entities": [[8, 10, "TaskName", "stance detection"], [18, 21, "DatasetName", "WT - WT"]]}, {"text": "However , the average tweet length of WT - WT is much shorter when compared with our P - S TANCE .", "entities": [[7, 10, "DatasetName", "WT - WT"]]}, {"text": "Moreover , more explicit mentions of targets and lexical cues of stance appear in the sentences of WT - WT dataset .", "entities": [[17, 20, "DatasetName", "WT - WT"]]}, {"text": "In our work , we focus on the political domain and our P - S TANCE , which contains much longer sentences and less surfacelevel lexical cues , can serve as a new challenging benchmark for stance detection tasks .", "entities": [[36, 38, "TaskName", "stance detection"]]}, {"text": "2357Authors Target(s )", "entities": []}, {"text": "Source Type Size Mohammad et al .", "entities": []}, {"text": "( 2016a)Atheism , Climate change is a real concern , Feminist movement , Hillary Clinton , Legalization of abortion , Donald TrumpTwitter Target - speci\ufb01c 4,870 Ferreira and Vlachos (", "entities": []}, {"text": "2016)Various claims News articles Claim - based 2,595 Sobhani et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2017 ) Trump - Clinton , Trump - Cruz , Clinton - Sanders Twitter Multi - target 4,455 Derczynski et al .", "entities": [[19, 20, "DatasetName", "Derczynski"]]}, {"text": "( 2017)Various claims Twitter Claim - based 5,568 Swami et al .", "entities": []}, {"text": "( 2018 )", "entities": []}, {"text": "Demonetisation in India in 2016", "entities": []}, {"text": "Twitter Target - speci\ufb01c 3,545 Gorrell et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 )", "entities": []}, {"text": "Various claims Twitter , Reddit Claim - based 8,574 Conforti et al .", "entities": [[4, 5, "DatasetName", "Reddit"]]}, {"text": "( 2020b)Merger of companies : Cigna - Express Scripts , Aetna - Humana , CVS - Aetna , Anthem - Cigna , Disney - FoxTwitter Target - speci\ufb01c 51,284 Conforti et al .", "entities": []}, {"text": "( 2020a)Merger of companies : Cigna - Express Scripts , Aetna - Humana , CVS - Aetna , Anthem - CignaNews articles Target - speci\ufb01c 3,291 P - S TANCE Donald Trump , Joe Biden , Bernie Sanders Twitter Target - speci\ufb01c 21,574 Table 2 : Comparison of English stance detection datasets .", "entities": [[49, 51, "TaskName", "stance detection"]]}, {"text": "Different from classifying the stance detection tasks by target type ( i.e. , one speci\ufb01c target , multiple targets , or a claim ) , we can also categorize the stance detection as in - target and cross - target stance detection by the training setting .", "entities": [[4, 6, "TaskName", "stance detection"], [30, 32, "TaskName", "stance detection"], [40, 42, "TaskName", "stance detection"]]}, {"text": "Most previous works focused on the in - target stance detection where a classi\ufb01er is trained and validated on the same target ( Mohammad et al . , 2016b ; Zarrella and Marsh , 2016 ; Wei et al . , 2016 ; Vijayaraghavan et al . , 2016 ;", "entities": [[9, 11, "TaskName", "stance detection"]]}, {"text": "Du et al . , 2017 ; Sun et al . , 2018 ; Wei et al . , 2018 ; Li and Caragea , 2019 , 2021b ) .", "entities": []}, {"text": "However , suf\ufb01cient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets , which motivates the studies of cross - target stance detection ( Augenstein et al . , 2016 ; Xu et", "entities": [[14, 16, "TaskName", "stance detection"], [35, 37, "TaskName", "stance detection"]]}, {"text": "al . , 2018 ; Wei and Mao , 2019 ; Zhang et al . , 2020 ) .", "entities": []}, {"text": "Most previous studies evaluated the cross - target models on the SemEval-2016 dataset ( Mohammad et al . , 2016a ) , which is a small dataset and thus may make the conclusions less convincing .", "entities": []}, {"text": "In this paper , we show that our P - S TANCE dataset can be also used to evaluate the model performance of cross - target stance detection and provides opportunities for exploring more crosstarget tasks by interacting with previous SemEval2016 ( Mohammad et al . , 2016a ) and Multi - Target stance datasets ( Sobhani et al . , 2017 ) .", "entities": [[26, 28, "TaskName", "stance detection"]]}, {"text": "In addition , P - S TANCE enables the exploration of largescale deep learning models including pre - trained language models , e.g. , BERT ( Devlin et al . , 2019 ) and BERTweet ( Nguyen et al . , 2020 ) .", "entities": [[24, 25, "MethodName", "BERT"]]}, {"text": "We \ufb01ne - tune the BERT and BERTweet models on our dataset and compare them with other strong baselines.3 Building the Dataset In this section , we detail the creation and the particularities of P - S TANCE , our large political stance detection dataset composed of 21,574 tweets collected during the 2020 U.S. presidential election .", "entities": [[5, 6, "MethodName", "BERT"], [42, 44, "TaskName", "stance detection"]]}, {"text": "3.1 Data Collection We collected tweets using the Twitter streaming API .", "entities": []}, {"text": "Similar to prior works ( Mohammad et al . , 2016a ; Sobhani et al . , 2017 ) that target presidential candidates , we focus our attention on three political \ufb01gures2 in the presidential race of 2020 : \u201c Donald Trump , \u201d \u201c Joe Biden , \u201d and \u201c Bernie Sanders . \u201d", "entities": []}, {"text": "We used a set of query hashtags as seeds to collect target - related tweets , which can be categorized as favor hashtags , against hashtags andneutral hashtags ( Mohammad et al . , 2016a ) .", "entities": [[8, 9, "DatasetName", "seeds"]]}, {"text": "We show examples of these query hashtags in Table 3 .", "entities": []}, {"text": "In total , we gathered around 2.8 million tweets for all three targets combined .", "entities": []}, {"text": "3.2 Preprocessing To ensure the quality of this dataset , we performed several preprocessing steps : 1)We removed tweets with less than 10 , or more than 128 words .", "entities": []}, {"text": "According to our observations , tweets with less than 10 words are either too easy for detecting the stance or too noisy , and tweets with more than 128 words usually contain duplicate expressions .", "entities": []}, {"text": "2)We removed duplicates and retweets .", "entities": []}, {"text": "Twitter data are noisy not only due to the creative spellings , slang 2We also tried to collect tweets about the woman politician Kamala Harris .", "entities": []}, {"text": "However , we were unable to collect enough data about Harris .", "entities": []}, {"text": "We will look into this in our future work .", "entities": []}, {"text": "2358Target Favor Hashtag Against Hashtag Neutral Hashtag", "entities": []}, {"text": "Trump # Trump2020LandSlide # TrumpCrimeFamily # DonaldTrump # Republican Biden # BidenForPresident # SleepyJoe # JoeBiden # Democrats Sanders # BernieWon # NeverBernie # BernieSanders # Sanders Table 3 : Examples of query hashtags .", "entities": []}, {"text": "Trump Biden Sanders # Raw collection 1,730 K 429 K 654 K # After preprocessing 1,221 K 300 K 465 K Table 4 : Number of unlabeled tweets", "entities": []}, {"text": "before and after preprocessing .", "entities": []}, {"text": "Setup Trump Biden Sanders Average 3 - class 0.62 0.60 0.59 0.60 2 - class 0.86 0.81 0.76 0.81 Table 5 : Krippendorff \u2019s alpha measure of annotator agreement in 3 - class and 2 - class scenarios .", "entities": [[24, 25, "HyperparameterName", "alpha"]]}, {"text": "and URLs , but also because of the duplicate tweets .", "entities": []}, {"text": "Since these duplicate data reduce our ability to build reliable models , we need to clean the dataset by removing duplicates .", "entities": []}, {"text": "3)We kept only the tweets in English because our goal in this work is to build an English stance detection dataset .", "entities": [[18, 20, "TaskName", "stance detection"]]}, {"text": "We leave multilingual stance detection as future work .", "entities": [[3, 5, "TaskName", "stance detection"]]}, {"text": "After data preprocessing , the size of our corpus reduces to around 2 million examples .", "entities": []}, {"text": "In Table 4 , we show the number of tweets before and after preprocessing for each political \ufb01gure .", "entities": []}, {"text": "We will provide this large - scale repository of tweets ( which we call PSTANCE -EXT ) alongside P - STANCE , in hope that it will spur further research in the \ufb01eld of semisupervised learning for stance detection .", "entities": [[37, 39, "TaskName", "stance detection"]]}, {"text": "Finally , we sampled 10,000 tweets for each political \ufb01gure , obtaining 30,000 tweets for annotation in total .", "entities": []}, {"text": "3.3 Data Annotation We gathered stance annotations of three targets through the Amazon Mechanical Turk ( AMT ) crowdsourcing platform .", "entities": []}, {"text": "The AMT workers were asked to annotate each tweet with \u201c Favor , \u201d \u201c Against , \u201d \u201c None , \u201d or \u201c I do n\u2019t know . \u201d", "entities": []}, {"text": "To ensure the annotation quality , we employed strict requirements for the annotators : 1)Many completed tasks ( > 500 ) ; 2)To reside in the USA ; 3)A high acceptance rate ( > 95 % ) .", "entities": []}, {"text": "Moreover , we ran the annotation process in several batches of 1000 examples .", "entities": []}, {"text": "In each batch , we include 100internally annotated examples to measure the quality of the annotators .", "entities": []}, {"text": "If an annotator mislabels more than 25 % of these examples , we discard the annotations of the worker completely , and relabel them .", "entities": []}, {"text": "Interestingly , this process led to a considerable number of reannotations , amounting for more than 20 % of the data .", "entities": []}, {"text": "Each tweet was labeled by three random annotators , and disagreements in the labels were decided by the majority voting among the three annotators .", "entities": []}, {"text": "After obtaining the annotation results , we computed Krippendorff \u2019s alpha ( Krippendorff , 2011 ) as the measure of inter - annotator agreement , as shown in Table 5 .", "entities": [[10, 11, "HyperparameterName", "alpha"]]}, {"text": "Tweets that were annotated with label \u201c I do n\u2019t know \u201d after the majority voting were removed from the dataset .", "entities": []}, {"text": "We observed that annotators had dif\ufb01culties in reaching an agreement on tweets with label \u201c None \u201d and the average of Krippendorff \u2019s alpha values increases from 0.60 to 0.81 when we consider two classes : \u201c Favor \u201d and \u201c Against \u201d .", "entities": [[23, 24, "HyperparameterName", "alpha"]]}, {"text": "Similar to prior work ( Vamvas and Sennrich , 2020 ) , we removed the label \u201c None \u201d from the dataset in our experiments .", "entities": []}, {"text": "3.4 Quality Assurance and Challenges Stance - exposing hashtags that may expose the stance directly , e.g. , # NeverBernie , can be observed in the data .", "entities": []}, {"text": "A model can detect the stance from these hashtags without extracting effective representations for the meanings of sentences , which makes stance detection easier .", "entities": [[21, 23, "TaskName", "stance detection"]]}, {"text": "To remove the stance - exposing hashtags and ensure the data quality , we performed the following steps after the data annotation : 1)We manually built a hashtag lexicon that contains stance - exposing hashtags for each target .", "entities": []}, {"text": "Then we removed all hashtags that are appended at the end of a sentence if they are in the hashtag lexicon .", "entities": []}, {"text": "The reason of only removing the appended hashtags is that a hashtag may serve as a constituent of a sentence , so it would introduce more noise if we simply remove all stance - exposing hashtags .", "entities": []}, {"text": "2)To address the stance - exposing hashtag that is a constituent of a sentence , we replaced stance - exposing hashtags that contain the target name with a neutral hashtag , e.g. , # NeverBernie ! # Bernie .", "entities": []}, {"text": "These steps ensure", "entities": []}, {"text": "2359Trump Biden Sanders Train Favor 2,937 2,552 2,858 Against 3,425 3,254 2,198 Val Favor 365 328 350 Against 430 417 284 Test Favor 361 337 343 Against 435 408 292 Total 7,953 7,296 6,325 Table 6 : Label distribution across different targets for P - S TANCE .", "entities": []}, {"text": "the high quality of our P - S TANCE dataset .", "entities": []}, {"text": "In addition , P - S TANCE is a challenging dataset for the following reasons : 1)Targets in P - S TANCE are referred to in a more implicit way .", "entities": []}, {"text": "Consider the second example in Table 1 , the target name only appears at the end of the sentence and it is hard to correctly identify the stance without any knowledge about the political \ufb01gures mentioned in the content and background immigration policy .", "entities": []}, {"text": "Similarly , for the third example , it is dif\ufb01cult to correctly identify the stance if the classi\ufb01er fails to connect the target with relevant events , i.e. , climate change or medicare for all residents .", "entities": []}, {"text": "2)The average length of tweets in previous datasets is short , and there are more explicit mentions of targets and rich sentiment and emotion words that can easily reveal the stance toward the target .", "entities": [[23, 24, "DatasetName", "emotion"]]}, {"text": "The average tweet length is 17 in", "entities": []}, {"text": "Mohammad et al .", "entities": []}, {"text": "( 2016a ) , 21 in Sobhani et", "entities": []}, {"text": "al . ( 2017 ) and 16 in Conforti et al . ( 2020b ) .", "entities": []}, {"text": "However , our P - S TANCE has a much longer average length of 30 and more implicit mentions of targets and context words , which indicates that our dataset is more dif\ufb01cult .", "entities": []}, {"text": "In addition , PSTANCE covers more target - relevant events .", "entities": []}, {"text": "These characteristics contribute to making P - S TANCE a challenging dataset for stance detection .", "entities": [[13, 15, "TaskName", "stance detection"]]}, {"text": "3.5 Dataset Distribution The \ufb01nal dataset contains 7,953 annotated tweets for \u201c Donald Trump \u201d , 7,296 for \u201c Joe Biden \u201d and 6,325 for \u201c Bernie Sanders \u201d , respectively .", "entities": []}, {"text": "The label distribution of each target is shown in Table 6 .", "entities": []}, {"text": "Each tweet is annotated with a stance label \u201c Favor \u201d or \u201c Against \u201d .", "entities": []}, {"text": "We created the training , validation and testing sets following an 80/10/10 split .", "entities": []}, {"text": "We note thatP - S TANCE is more than 3 times larger than the previous benchmark ( Mohammad et al . , 2016a ) .", "entities": []}, {"text": "4 Experimental Settings In this section , we \ufb01rst introduce two benchmark datasets of stance detection in \u00a7 4.1 .", "entities": [[14, 16, "TaskName", "stance detection"]]}, {"text": "The union ofthese datasets and our P - S TANCE dataset provides opportunities for studying the cross - target stance detection ( \u00a7 5.2 ) and cross - topic stance detection ( \u00a7 5.3 ) .", "entities": [[19, 21, "TaskName", "stance detection"], [29, 31, "TaskName", "stance detection"]]}, {"text": "Then we discuss the evaluation metrics in \u00a7 4.2 and introduce the baseline methods in \u00a7 4.3 .", "entities": []}, {"text": "4.1 Existing Benchmark Datasets SemEval-2016 ( Mohammad et al . , 2016a ) and Multi - Target stance datasets ( Sobhani et al . , 2017 ) are two benchmark datasets in which political \ufb01gures are chosen as the targets .", "entities": []}, {"text": "SemEval-2016 contains six targets : \u201c Atheism , \u201d \u201c Climate Change is a Real Concern , \u201d \u201c Feminist Movement , \u201d \u201c Hillary Clinton , \u201d \u201c Legalization of Abortion , \u201d and \u201c Donald Trump . \u201d", "entities": []}, {"text": "The dataset is annotated for detecting the stance toward a given target .", "entities": []}, {"text": "The data distribution of SemEval-2016 is shown in Table 7 .", "entities": []}, {"text": "Multi - Target stance dataset contains three sets of tweets corresponding to three target pairs : \u201c Donald Trump and Hillary Clinton , \u201d \u201c Donald Trump and Ted Cruz , \u201d \u201c Hillary Clinton and Bernie Sanders \u201d for 2016 U.S. presidential election .", "entities": []}, {"text": "The task aims at detecting the stances toward two targets for each data .", "entities": []}, {"text": "The data distribution of Multi - Target stance dataset is shown in Table 8 .", "entities": []}, {"text": "In the next section , we show how to perform various stance detection tasks with the union of these datasets and our P - S TANCE dataset .", "entities": [[11, 13, "TaskName", "stance detection"]]}, {"text": "4.2 Evaluation Metrics Similar to Mohammad et al .", "entities": []}, {"text": "( 2017 ) and Sobhani et", "entities": []}, {"text": "al . ( 2017 ) , Favgand macro - average of F1 - score ( Fmacro ) are adopted to evaluate the performance of our baseline models .", "entities": [[11, 14, "MetricName", "F1 - score"]]}, {"text": "First , the F1 - score of label \u201c Favor \u201d and \u201c Against \u201d is calculated as follows :", "entities": [[3, 6, "MetricName", "F1 - score"]]}, {"text": "Ffavor = 2PfavorRfavor Pfavor + Rfavor(1 )", "entities": []}, {"text": "Fagainst = 2PagainstRagainst Pagainst + Ragainst(2 ) where P and R are precision and recall , respectively .", "entities": []}, {"text": "After that , the Favgis calculated as : Favg = Ffavor + Fagainst 2(3 ) We compute the Favgfor each target .", "entities": []}, {"text": "Fmacro is calculated by averaging the Favgacross all targets .", "entities": []}, {"text": "4.3 Baseline Methods We run experiments with the following baselines .", "entities": []}, {"text": "2360Target # Train % Favor %", "entities": []}, {"text": "Against % None # Test % Favor % Against % None Atheism 513 17.93 59.26 22.81 220 14.54 72.73 12.73 Climate 395 53.67 3.80 42.53 169 72.78 6.51 20.71 Feminism 664 31.63 49.40 18.97 285 20.35 64.21 15.44 Hillary 689 17.13 57.04 25.83 295 15.25 58.31 26.44 Abortion 653 18.53 54.36 27.11 280 16.43 67.50 16.07 Trump 0 - - - 707 20.93 42.29 36.78 Table 7 : Data distribution of SemEval-2016 dataset .", "entities": [[57, 58, "DatasetName", "0"]]}, {"text": "Target Pair Total Train Dev Test Trump - Clinton 1,722 1,240 177 355 Trump - Cruz 1,317 922 132 263 Clinton - Sanders 1,366 957 137 272 Total 4,455 3,119 446 890 Table 8 : Data distribution of Multi - Target dataset .", "entities": []}, {"text": "BiLSTM ( Schuster and Paliwal , 1997 ): A BiLSTM model that takes tweets as inputs without considering the target information .", "entities": [[0, 1, "MethodName", "BiLSTM"], [9, 10, "MethodName", "BiLSTM"]]}, {"text": "CNN ( Kim , 2014 ): Similar to BiLSTM , the vanilla CNN only takes tweets as inputs and does not consider the target information .", "entities": [[8, 9, "MethodName", "BiLSTM"]]}, {"text": "TAN ( Du et al . , 2017 ): TAN is an attention - based LSTM model that extracts target speci\ufb01c features .", "entities": [[15, 16, "MethodName", "LSTM"]]}, {"text": "BiCE ( Augenstein et al . , 2016 ):", "entities": []}, {"text": "A BiLSTM that uses conditional encoding for stance detection .", "entities": [[1, 2, "MethodName", "BiLSTM"], [7, 9, "TaskName", "stance detection"]]}, {"text": "The target information is \ufb01rst encoded by a BiLSTM , whose hidden representations are then used to initialize another BiLSTM with tweets as inputs .", "entities": [[8, 9, "MethodName", "BiLSTM"], [19, 20, "MethodName", "BiLSTM"]]}, {"text": "BiCE is also a strong baseline for crosstarget stance detection .", "entities": [[8, 10, "TaskName", "stance detection"]]}, {"text": "CrossNet ( Xu et al . , 2018 ): CrossNet is another model for cross - target stance detection .", "entities": [[17, 19, "TaskName", "stance detection"]]}, {"text": "It encodes the target and the tweet by using the same approach with BiCE and add an aspect attention layer to signal the core part of a stance - bearing input .", "entities": []}, {"text": "CrossNet improves BiCE in many cross - target settings .", "entities": []}, {"text": "GCAE ( Xue and Li , 2018 ): A CNN model that utilizes a gating mechanism to block targetunrelated information .", "entities": []}, {"text": "GCAE is a strong baseline for aspect - based sentiment analysis and we apply it to our stance detection task .", "entities": [[6, 11, "TaskName", "aspect - based sentiment analysis"], [17, 19, "TaskName", "stance detection"]]}, {"text": "PGCNN ( Huang and Carley , 2018 ): Similar to GCAE , PGCNN is based on gated convolutional networks and encodes target information by generating target - sensitive \ufb01lters .", "entities": []}, {"text": "BERT ( Devlin et al . , 2019 ): A pre - trained language model that predicts the stance by appendinga linear classi\ufb01cation layer to the hidden representation of [ CLS]token .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "We \ufb01ne - tune the BERT - base on the stance detection task .", "entities": [[5, 6, "MethodName", "BERT"], [10, 12, "TaskName", "stance detection"]]}, {"text": "BERTweet ( Nguyen et al . , 2020 ): BERTweet is another pre - trained language model following the training procedure of RoBERTa ( Liu et al . , 2019 ) .", "entities": [[22, 23, "MethodName", "RoBERTa"]]}, {"text": "Similar to BERT , we \ufb01ne - tune the pretrained BERTweet to predict the stance by appending a linear classi\ufb01cation layer to the hidden representation of the [ CLS]token .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "The pre - trained BERTweet model is \ufb01ne - tuned under the PyTorch framework .", "entities": []}, {"text": "The maximum sequence length is set to 128 and the batch size is 32 .", "entities": [[10, 12, "HyperparameterName", "batch size"]]}, {"text": "We use AdamW optimizer ( Loshchilov and Hutter , 2019 ) and the learning rate is 2e-5 .", "entities": [[2, 3, "MethodName", "AdamW"], [3, 4, "HyperparameterName", "optimizer"], [13, 15, "HyperparameterName", "learning rate"]]}, {"text": "5 Results In this section , we present the set of experiments performed on various stance detection tasks on our dataset and show the results obtained by using the aforementioned baselines .", "entities": [[15, 17, "TaskName", "stance detection"]]}, {"text": "Each result is the average of seven runs with different initializations .", "entities": []}, {"text": "5.1 In - Target Stance Detection In - target stance detection is a stance detection task where a classi\ufb01er is trained and validated on the same target .", "entities": [[4, 6, "TaskName", "Stance Detection"], [9, 11, "TaskName", "stance detection"], [13, 15, "TaskName", "stance detection"]]}, {"text": "Most previous works adopt an \u201c Adhoc \u201d training strategy by training one model for each target and evaluate it on the test set of that target ( i.e. , we train three different models if there are three targets in the dataset ) .", "entities": []}, {"text": "However , the model is more likely to predict the stance by following speci\ufb01c patterns without fully considering the target information and over\ufb01t .", "entities": []}, {"text": "Therefore , to better evaluate the performance of baselines , we propose a \u201c Merged \u201d training strategy by training and validating a model on all targets and testing it on separate targets to be compared with the \u201c Ad - hoc \u201d setting .", "entities": []}, {"text": "Experimental results of these two different settings are shown in Table 9 .", "entities": []}, {"text": "First , we can observe that BERTweet performs best in both settings and signi\ufb01cantly outperforms the second best results ,", "entities": []}, {"text": "2361Method Trump Biden Sanders F macro Drop Ad - hoc BiLSTM 76.92 77.95 69.75 74.87 CNN 76.80 77.22 71.40 75.14 TAN 77.10 77.64 71.60 75.45 BiCE 77.15 77.69 71.24 75.36 PGCNN 76.87 76.60 72.13 75.20 GCAE 78.96 77.95 71.82 76.24 BERT 78.28 78.70 72.45 76.48 BERTweet 82.48y81.02y78.09y80.53 Merged BiLSTM 77.18 75.47 67.43 73.36 1.51 CNN 74.79 74.11 66.68 71.86 3.28 TAN 78.30 75.26 70.67 74.74 0.71 BiCE 77.67 75.69 69.37 74.24 1.12 PGCNN 77.36 74.96 70.29 74.20 1.00 GCAE 79.00 76.32 69.93 75.08 1.16 BERT 79.19 76.02 73.59 76.27 0.21 BERTweet 83.81y79.08y77.75y80.21 0.32 Table 9 : Comparison of different models on the PSTANCE dataset ( % ) .", "entities": [[10, 11, "MethodName", "BiLSTM"], [40, 41, "MethodName", "BERT"], [48, 49, "MethodName", "BiLSTM"], [84, 85, "MethodName", "BERT"]]}, {"text": "y : BERTweet model improves the best baseline at p < 0.05 with paired t - test .", "entities": []}, {"text": "Fmacro is the average of all target pairs .", "entities": []}, {"text": "\u201c Drop \u201d means performance decline between two training strategies for the same model .", "entities": []}, {"text": "Bold scores are best overall .", "entities": []}, {"text": "demonstrating the effectiveness of this model .", "entities": []}, {"text": "Second , performance drops can be observed on all models in the \u201c Merged \u201d setting and models ( BiLSTM and CNN ) that do not consider target information suffer the most severe drops , which means our proposed training strategy can serve as a better evaluation method to test whether the model learns target - speci\ufb01c representations .", "entities": [[19, 20, "MethodName", "BiLSTM"]]}, {"text": "Moreover , we can observe that both BERTweet and BERT perform well and have the minimum performance drops compared with the other baselines , which demonstrates that self - attention mechanism can better capture target - speci\ufb01c representations .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "5.2 Cross - Target Stance Detection Despite substantial progress on the stance detection , suf\ufb01cient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets , which motivates the studies of crosstarget stance detection .", "entities": [[4, 6, "TaskName", "Stance Detection"], [11, 13, "TaskName", "stance detection"], [26, 28, "TaskName", "stance detection"], [45, 47, "TaskName", "stance detection"]]}, {"text": "The model of cross - target stance detection is \ufb01rst trained and validated on a source target , and then tested on a destination target .", "entities": [[6, 8, "TaskName", "stance detection"]]}, {"text": "In this subsection , we show that our P - S TANCE dataset can be also used to evaluate the model performance of cross - target stance detection and provides opportunities for exploring more cross - target tasks by interacting with previous SemEval-2016 and Multi - Target stance datasets .", "entities": [[26, 28, "TaskName", "stance detection"]]}, {"text": "We use \ufb01ve targets for our experiments : \u201c DonaldTarget BiCE CrossNet BERTweet P - S TANCE dataset DT!JB 55.83 56.67 58.88 DT!BS 51.78 50.08 56.50y JB!DT 58.16 60.43 63.64y JB!BS 60.24 60.81 67.04y BS!DT 51.41 52.99 58.75y BS!JB 57.68 62.57 72.99y", "entities": []}, {"text": "DT , JB!BS 52.26 56.26 69.99y DT , BS!JB 53.73 55.57 68.64y JB , BS!DT 53.91 56.44 66.01y P - S TANCE!previous datasets DT!HC 36.12 40.56 34.48 DT!TC 59.37 59.40 63.89y DT!BS 47.73 48.93", "entities": []}, {"text": "51.00y JB!DT", "entities": []}, {"text": "48.90 49.77 56.00y JB!HC 56.77 55.54 57.55 JB!TC 53.47 55.77 62.45y JB!BS 48.11 48.96 51.48y", "entities": []}, {"text": "BS!DT 47.93 46.10 49.96 BS!HC 49.97 50.49 52.81 BS!TC 54.37 52.98 56.91y Table 10 : Comparison of different models for crosstarget stance detection ( % ) .", "entities": [[21, 23, "TaskName", "stance detection"]]}, {"text": "The \ufb01rst half reports the cross - target results on our proposed P - S TANCE dataset .", "entities": []}, {"text": "The second half reports the cross - target results that are trained on the P - S TANCE dataset and tested on the previous datasets .", "entities": []}, {"text": "y : BERTweet model improves the best baseline at p < 0.05 with paired t - test .", "entities": []}, {"text": "Bold scores are best overall .", "entities": []}, {"text": "Trump \u201d ( DT ) , \u201c Joe Biden \u201d ( JB ) , \u201c Bernie Sanders \u201d ( BS ) , \u201c Hillary Clinton \u201d ( HC ) , and \u201c Ted Cruz \u201d ( TC ) .", "entities": []}, {"text": "Experimental results of cross - target stance detection are shown in Table 10 .", "entities": [[6, 8, "TaskName", "stance detection"]]}, {"text": "For the \ufb01rst half of Table 10 , only targets of P - S TANCE dataset are used to evaluate the model performance .", "entities": []}, {"text": "However , for the second half , targets of SemEval-2016 and Multi - Target datasets also serve as destination targets , which makes it a more challenging task since the target - related topics in 2016 are quite different from the ones in 2020 .", "entities": []}, {"text": "More speci\ufb01cally , we train and validate the model on a source target ofP - S TANCE dataset and test it on the data of a destination target , which is a combination of train , validation , and test sets of previous datasets .", "entities": []}, {"text": "Note that we merge the data from SemEval-2016 and Multi - Target datasets if these two datasets share the same target , e.g. , Hillary Clinton .", "entities": []}, {"text": "For the cross - target tasks only on the P - S TANCE dataset , \ufb01rst , we can observe from the Table 10 that BERTweet achieves the best performance on all target con\ufb01gurations , demonstrating its effectiveness .", "entities": []}, {"text": "Moreover , BERTweet shows greater improvement over the best baseline when training on the data of two targets .", "entities": []}, {"text": "The reason is that BERTweet learns", "entities": []}, {"text": "2362more universal representations by leveraging the data from two targets .", "entities": []}, {"text": "Second , we see that CrossNet outperforms BiCE on almost all target con\ufb01gurations , which is consistent with the observations of previous studies ( Xu et al . , 2018 ; Zhang et al . , 2020 ) .", "entities": []}, {"text": "Third , we \ufb01nd that models achieve better performance on JB !", "entities": []}, {"text": "BS and BS ! JB .", "entities": []}, {"text": "One potential explanation is that targets \u201c Joe Biden \u201d and \u201c Bernie Sanders \u201d are from the same party and thus share more similar topics .", "entities": []}, {"text": "For the second half of Table 10 , we observe a signi\ufb01cant drop in performance on all models , which veri\ufb01es that it is more challenging to transfer the knowledge to a destination target with more diverse topics in the past .", "entities": []}, {"text": "BERTweet still achieves the best performance on almost all target con\ufb01gurations , making it a highly competitive model for crosstarget stance detection task .", "entities": [[20, 22, "TaskName", "stance detection"]]}, {"text": "Interestingly , we can observe that both BiCE , CrossNet , and BERTweet show better performance on target \u201c Ted Cruz . \u201d", "entities": []}, {"text": "A possible reason is that the data of \u201c Ted Cruz \u201d contain more universal expressions and topics .", "entities": []}, {"text": "5.3 Cross - Topic Stance Detection Obtaining suf\ufb01cient annotated data of speci\ufb01c target from most recent past is challenging .", "entities": [[4, 6, "TaskName", "Stance Detection"]]}, {"text": "However , sometimes historical annotated data of the same target are available .", "entities": []}, {"text": "Therefore , motivated by a desire to improve the models \u2019 generalization ability to transfer knowledge from historical data , we come up with a new stance detection task , named cross - topic stance detection .", "entities": [[26, 28, "TaskName", "stance detection"], [34, 36, "TaskName", "stance detection"]]}, {"text": "Speci\ufb01cally , in this task , the model of cross - topic stance detection is \ufb01rst trained on the data of a target ( e.g. , Donald Trump ) in 2016 , and then validated and tested on the data of the same target in 2020 .", "entities": [[12, 14, "TaskName", "stance detection"]]}, {"text": "Note that the annotated data of year 2016 are the same with the data used in \u00a7 5.2 .", "entities": []}, {"text": "The results are shown in Table 11 .", "entities": []}, {"text": "Since target \u201c Joe Biden \u201d is absent from the previous stance detection datasets , we use targets \u201c Donald Trump \u201d and \u201c Bernie Sanders \u201d for evaluation .", "entities": [[11, 13, "TaskName", "stance detection"]]}, {"text": "We can observe that BERTweet still performs best on this task and the overall model performance of cross - topic stance detection is better than that of cross - target stance detection due to the use of the same target in evaluation stage .", "entities": [[20, 22, "TaskName", "stance detection"], [30, 32, "TaskName", "stance detection"]]}, {"text": "Moreover , we see that models perform relatively poorly on target \u201c Bernie Sanders \u201d .", "entities": []}, {"text": "One possible explanation is that some topics , e.g. healthcare and climate change , appear rarely in previous datasets .", "entities": []}, {"text": "Target BiCE CrossNet BERTweet DT!DT 58.60 59.41 73.58y BS!BS 59.04 57.66 66.48y Table 11 : Comparison of different models for crosstopic stance detection ( % ) .", "entities": [[21, 23, "TaskName", "stance detection"]]}, {"text": "y : BERTweet model improves the best baseline at p < 0.05 with paired t - test .", "entities": []}, {"text": "Bold scores are best overall .", "entities": []}, {"text": "5.4 Semi - Supervised Stance Detection During elections , there is a considerable amount of data generated by users expressing their opinions about candidates , out of which only a small amount can be annotated and used for supervised stance detection .", "entities": [[4, 6, "TaskName", "Stance Detection"], [39, 41, "TaskName", "stance detection"]]}, {"text": "We explore the potential of the abundant unlabeled tweets and show that we can leverage them to improve the performance of our models .", "entities": []}, {"text": "To this end , we turn to semi - supervised learning , and leverage techniques such as Uncertainty - aware Self - Training ( UST ) .", "entities": []}, {"text": "UST ( Mukherjee and Awadallah , 2020 ) is a semi - supervised approach which uses the standard teacher - student self - training framework , but adds a few powerful changes .", "entities": []}, {"text": "Concretely , UST designs different techniques which leverage the uncertainty of the teacher model to select the unlabeled set of examples in each self - training iteration .", "entities": []}, {"text": "First , we train our teacher model on the labeled examples .", "entities": []}, {"text": "Next , we compute uncertainty estimates of our teacher model on the set of unlabeled examples by performing a few forward passes with dropout enabled .", "entities": []}, {"text": "Finally , we incorporate the uncertainty estimates into our framework as follows : 1)We use these estimates to select the examples for which the teacher is most or least con\ufb01dent about .", "entities": []}, {"text": "2)We incorporate the teacher con\ufb01dence in the student loss by penalizing the student \u2019s misclassi\ufb01ed examples in which the teacher has high con\ufb01dence .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "We use the BERTweet model as teacher and student .", "entities": []}, {"text": "We perform various experiments to show the bene\ufb01ts of using a large amount of unlabeled data from P - S TANCE -EXT alongside our UST model .", "entities": []}, {"text": "We carry out three barely supervised experiments with various number of examples in the training set .", "entities": []}, {"text": "Speci\ufb01cally , we experiment with 30 , 50 , and 100 training examples .", "entities": []}, {"text": "Moreover , we also consider an experiment using the whole training set to investigate the effect of the unlabeled examples when all the training data are available .", "entities": []}, {"text": "We run experiments with different training sets , and report the F1 - scores obtained on the entire testing set .", "entities": [[11, 12, "MetricName", "F1"]]}, {"text": "We show the results of our semi - supervised ex-", "entities": []}, {"text": "2363Method Trump Biden Sanders F macro BERTweet 82.48 81.02 78.09 80.53 UST-30 61.02 64.34 60.45 61.94 UST-50 68.42 73.24 66.12 69.26 UST-100 74.45 79.46 71.67 75.19 UST - ALL 85.50y82.22y79.55y82.42 Table 12 : Semi - supervised learning results .", "entities": []}, {"text": "y : USTALL improves the BERTweet at p < 0.05 with paired t - test .", "entities": []}, {"text": "periments in Table 12 and make the following observations .", "entities": []}, {"text": "First , UST - ALL signi\ufb01cantly outperforms the BERTweet model by 1.89 % in a macroaverage F1 - score when using both the labeled and unlabeled data in a semi - supervised manner .", "entities": [[16, 19, "MetricName", "F1 - score"]]}, {"text": "Second , with only 100 examples ( 2 % of the available training examples ) , UST-100 stays within 1.6 % F1 - score of our best model that leverages the entire training set of target \u201c Joe Biden . \u201d", "entities": [[21, 24, "MetricName", "F1 - score"]]}, {"text": "The results indicate that the bene\ufb01t of using semi - supervised approaches is two - fold .", "entities": []}, {"text": "On one hand , it enables impressive performance in scarce label scenarios , while on the other hand , it still brings gains in scenarios where considerable amounts of labeled data are readily available .", "entities": []}, {"text": "6 Conclusion In this paper , we introduced P - S TANCE , an English stance detection dataset in the political domain , which is larger and more challenging compared with previous datasets for stance detection .", "entities": [[15, 17, "TaskName", "stance detection"], [34, 36, "TaskName", "stance detection"]]}, {"text": "Composed of 21,574 tweets that were collected during the 2020 USA election , P - S TANCE can serve as a new benchmark for stance detection and enable future research in other stance detection tasks , e.g. , cross - target stance detection and cross - topic stance detection .", "entities": [[24, 26, "TaskName", "stance detection"], [32, 34, "TaskName", "stance detection"], [41, 43, "TaskName", "stance detection"], [47, 49, "TaskName", "stance detection"]]}, {"text": "Experimental results show that the BERTweet model signi\ufb01cantly outperforms other strong baselines not only on intarget stance detection , but also on cross - target and cross - topic stance detection .", "entities": [[16, 18, "TaskName", "stance detection"], [29, 31, "TaskName", "stance detection"]]}, {"text": "Moreover , the performance of BERTweet can be further improved by using semi - supervised learning .", "entities": []}, {"text": "Future work includes constructing another large dataset for a more challenging task , i.e. , multi - target stance detection , and studying the multilingual stance detection with the union of P - S TANCE and other multilingual datasets .", "entities": [[18, 20, "TaskName", "stance detection"], [25, 27, "TaskName", "stance detection"]]}, {"text": "Acknowledgments We thank the National Science Foundation and Amazon Web Services for support from grants IIS-1912887 and IIS-1903963 which supported the research and the computation in this study .", "entities": []}, {"text": "We also thank our reviewers for their insightful comments .", "entities": []}, {"text": "References Abeer ALDayel and Walid Magdy .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Stance detection on social media : State of the art and trends .", "entities": [[0, 2, "TaskName", "Stance detection"]]}, {"text": "Information Processing & Management , 58(4):102597 .", "entities": [[3, 4, "TaskName", "Management"]]}, {"text": "Isabelle Augenstein , Tim Rockt\u00e4schel , Andreas Vlachos , and Kalina Bontcheva . 2016 .", "entities": []}, {"text": "Stance detection with bidirectional conditional encoding .", "entities": [[0, 2, "TaskName", "Stance detection"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 876\u2013885 .", "entities": []}, {"text": "Roy Bar - Haim , Indrajit Bhattacharya , Francesco Dinuzzo , Amrita Saha , and Noam Slonim .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Stance classi\ufb01cation of context - dependent claims .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers , pages 251\u2013261 .", "entities": []}, {"text": "Costanza Conforti , Jakob Berndt , Mohammad Taher Pilehvar , Chryssi Giannitsarou , Flavio Toxvaerd , and Nigel Collier . 2020a .", "entities": []}, {"text": "STANDER :", "entities": []}, {"text": "An expertannotated dataset for news stance detection and evidence retrieval .", "entities": [[5, 7, "TaskName", "stance detection"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 4086\u20134101 .", "entities": []}, {"text": "Costanza Conforti , Jakob Berndt , Mohammad Taher Pilehvar , Chryssi Giannitsarou , Flavio Toxvaerd , and Nigel Collier . 2020b .", "entities": []}, {"text": "Will - they - won\u2019t - they : A very large dataset for stance detection on Twitter .", "entities": [[14, 16, "TaskName", "stance detection"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1715 \u2013 1724 .", "entities": []}, {"text": "Kareem Darwish , Walid Magdy , Afshin Rahimi , Timothy Baldwin , and Norah Abokhodair .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Predicting online Islamophobic behavior after # ParisAttacks .", "entities": []}, {"text": "The Journal of Web Science , 4(3):34\u201352 .", "entities": []}, {"text": "Kareem Darwish , Walid Magdy , and Tahar Zanouda . 2017 .", "entities": []}, {"text": "Trump vs. Hillary : What went viral during the 2016 US presidential election .", "entities": []}, {"text": "In 9th International Conference on Social Informatics ( SocInfo 2017 ) , pages 143\u2013161 .", "entities": []}, {"text": "Leon Derczynski , Kalina Bontcheva , Maria Liakata , Rob Procter , Geraldine Wong Sak Hoi , and Arkaitz Zubiaga . 2017 .", "entities": [[1, 2, "DatasetName", "Derczynski"]]}, {"text": "SemEval-2017 task 8 : RumourEval :", "entities": []}, {"text": "Determining rumour veracity and support for rumours .", "entities": []}, {"text": "In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) , pages 69\u201376 .", "entities": []}, {"text": "Leon Derczynski , Kalina Bontcheva , Michal Lukasik , Thierry Declerck , Arno Scharl , Georgi Georgiev , Petya Osenova , Tomas Pariente Lobo , Anna Kolliakou , Robert Stewart , Sara - Jayne Terp , Geraldine", "entities": [[1, 2, "DatasetName", "Derczynski"]]}, {"text": "2364Wong , Christian Burger , Arkaitz Zubiaga , Rob Procter , and Maria Liakata .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "PHEME : Computing Veracity \u2014 the Fourth Challenge of Big Social Data .", "entities": []}, {"text": "InProceedings of the Extended Semantic Web Conference EU Project Networking session ( ESCW - PN ) .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Jiachen Du , Ruifeng Xu , Yulan He , and Lin Gui . 2017 .", "entities": []}, {"text": "Stance classi\ufb01cation with target - speci\ufb01c neural attention networks .", "entities": []}, {"text": "In Proceedings of the 26th International Joint Conference on Arti\ufb01cial Intelligence , pages 3988\u20133994 .", "entities": []}, {"text": "William Ferreira and Andreas Vlachos .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Emergent : A novel dataset for stance classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1163\u20131168 .", "entities": []}, {"text": "Genevieve Gorrell , Elena Kochkina , Maria Liakata , Ahmet Aker , Arkaitz Zubiaga , Kalina Bontcheva , and Leon Derczynski .", "entities": [[20, 21, "DatasetName", "Derczynski"]]}, {"text": "2019 .", "entities": []}, {"text": "SemEval-2019 task 7 : RumourEval , determining rumour veracity and support for rumours .", "entities": []}, {"text": "In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 845\u2013854 .", "entities": []}, {"text": "Kazi Saidul Hasan and Vincent Ng .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Why are you taking this stance ?", "entities": []}, {"text": "Identifying and classifying reasons in ideological debates .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 751\u2013762 .", "entities": []}, {"text": "Binxuan Huang and Kathleen Carley .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Parameterized convolutional neural networks for aspect level sentiment classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1091\u20131096 .", "entities": []}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1746\u20131751 .", "entities": []}, {"text": "Klaus Krippendorff .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Computing krippendorff \u2019s alpha - reliability .", "entities": [[3, 4, "HyperparameterName", "alpha"]]}, {"text": "Dilek K\u00fc\u00e7\u00fck and Fazli Can . 2020 .", "entities": []}, {"text": "Stance detection : A survey .", "entities": [[0, 2, "TaskName", "Stance detection"]]}, {"text": "ACM Comput .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Surv . , 53(1):1\u201337 .", "entities": []}, {"text": "Mirko Lai , Alessandra Teresa Cignarella , Delia Iraz\u00fa Hern\u00e1ndez Far\u00edas , Cristina Bosco , Viviana Patti , and Paolo Rosso .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multilingual stance detection in social media political debates .", "entities": [[1, 3, "TaskName", "stance detection"]]}, {"text": "Computer Speech & Language , 63:101075.Yingjie Li and Cornelia Caragea .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Multi - task stance detection with sentiment and stance lexicons .", "entities": [[3, 5, "TaskName", "stance detection"]]}, {"text": "InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 6298 \u2013 6304 .", "entities": []}, {"text": "Yingjie Li and Cornelia Caragea .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "A multi - task learning framework for multi - target stance detection .", "entities": [[1, 5, "TaskName", "multi - task learning"], [10, 12, "TaskName", "stance detection"]]}, {"text": "InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics : Findings , ACL 2021 , Online Event , August 2 - 4 , 2021 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yingjie Li and Cornelia Caragea .", "entities": []}, {"text": "2021b .", "entities": []}, {"text": "Target - aware data augmentation for stance detection .", "entities": [[3, 5, "TaskName", "data augmentation"], [6, 8, "TaskName", "stance detection"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1850\u20131860 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ilya Loshchilov and Frank Hutter .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Decoupled weight decay regularization .", "entities": [[1, 3, "MethodName", "weight decay"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Saif Mohammad , Svetlana Kiritchenko , Parinaz Sobhani , Xiao - Dan Zhu , and Colin Cherry .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "A dataset for detecting stance in tweets .", "entities": []}, {"text": "In LREC .", "entities": []}, {"text": "Saif Mohammad , Svetlana Kiritchenko , Parinaz Sobhani , Xiaodan Zhu , and Colin Cherry . 2016b .", "entities": []}, {"text": "Semeval-2016 task 6 : Detecting stance in tweets .", "entities": []}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation ( SemEval-2016 ) , pages 31\u201341 .", "entities": []}, {"text": "Saif M Mohammad , Parinaz Sobhani , and Svetlana Kiritchenko .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Stance and sentiment in tweets .", "entities": []}, {"text": "ACM Transactions on Internet Technology ( TOIT ) , 17(3):26 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Subhabrata Mukherjee and Ahmed Awadallah .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Uncertainty - aware self - training for few - shot text classi\ufb01cation .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 33 , pages 21199 \u2013 21212 .", "entities": []}, {"text": "Dat Quoc Nguyen , Thanh Vu , and Anh Tuan Nguyen .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "BERTweet :", "entities": []}, {"text": "A pre - trained language model for English tweets .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 9\u201314 .", "entities": []}, {"text": "Vahed Qazvinian , Emily Rosengren , Dragomir R. Radev , and Qiaozhu Mei . 2011 .", "entities": []}, {"text": "Rumor has it :", "entities": []}, {"text": "Identifying misinformation in microblogs .", "entities": []}, {"text": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , pages 1589\u20131599 .", "entities": []}, {"text": "2365Delip Rao and Dean Pomerleau . 2017 .", "entities": []}, {"text": "Fake News Challenge .", "entities": []}, {"text": "Mike Schuster and Kuldip K Paliwal .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Bidirectional recurrent neural networks .", "entities": []}, {"text": "IEEE Transactions on Signal Processing , 45(11):2673\u20132681 .", "entities": []}, {"text": "Parinaz Sobhani , Diana Inkpen , and Xiaodan Zhu . 2017 .", "entities": []}, {"text": "A dataset for multi - target stance detection .", "entities": [[6, 8, "TaskName", "stance detection"]]}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers , pages 551\u2013557 .", "entities": []}, {"text": "Qingying Sun , Zhongqing Wang , Qiaoming Zhu , and Guodong Zhou .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Stance detection with hierarchical attention network .", "entities": [[0, 2, "TaskName", "Stance detection"]]}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 2399\u20132409 .", "entities": []}, {"text": "Sahil Swami , Ankush Khandelwal , Vinay Singh , Syed Sarfaraz Akhtar , and Manish Shrivastava .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "An English - Hindi code - mixed corpus : Stance annotation and baseline system .", "entities": []}, {"text": "arXiv preprint arXiv:1805.11868 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mariona Taul\u00e9 , Maria Ant\u00f2nia Mart\u00ed , Francisco M. Rangel Pardo , Paolo Rosso , Cristina Bosco , and Viviana Patti . 2017 .", "entities": []}, {"text": "Overview of the task on stance and gender detection in tweets on Catalan independence .", "entities": []}, {"text": "In Proceedings of the Second Workshop on Evaluation of Human Language Technologies for Iberian Languages ( IberEval 2017 ) , pages 157\u2013177 .", "entities": []}, {"text": "Jannis Vamvas and Rico Sennrich .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "X - Stance : A multilingual multi - target dataset for stance detection .", "entities": [[0, 3, "DatasetName", "X - Stance"], [11, 13, "TaskName", "stance detection"]]}, {"text": "InProceedings of the 5th Swiss Text Analytics Conference ( SwissText ) & 16th Conference on Natural Language Processing ( KONVENS ) .", "entities": []}, {"text": "Prashanth Vijayaraghavan , Ivan Sysoev , Soroush V osoughi , and Deb Roy . 2016 .", "entities": []}, {"text": "DeepStance at SemEval-2016 task 6 : Detecting stance in tweets using character and word - level CNNs .", "entities": []}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation ( SemEval-2016 ) , pages 413\u2013419 .", "entities": []}, {"text": "Penghui Wei and Wenji Mao . 2019 .", "entities": []}, {"text": "Modeling transferable topics for cross - target stance detection .", "entities": [[7, 9, "TaskName", "stance detection"]]}, {"text": "In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR\u201919 , page 1173\u20131176 .", "entities": [[6, 7, "DatasetName", "ACM"], [14, 16, "TaskName", "Information Retrieval"]]}, {"text": "Penghui Wei , Wenji Mao , and Daniel Zeng .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A target - guided neural memory model for stance detection in twitter .", "entities": [[8, 10, "TaskName", "stance detection"]]}, {"text": "In 2018 International Joint Conference on Neural Networks , IJCNN 2018 , Rio de Janeiro , Brazil , July 8 - 13 , 2018 , pages 1\u20138 .", "entities": []}, {"text": "Wan Wei , Xiao Zhang , Xuqin Liu , Wei Chen , and Tengjiao Wang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "pkudblab at SemEval-2016 task 6 : A speci\ufb01c convolutional neural network system for effective stance detection .", "entities": [[14, 16, "TaskName", "stance detection"]]}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation ( SemEval-2016 ) , pages 384\u2013388.Chang Xu , C\u00e9cile Paris , Surya Nepal , and Ross Sparks .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Cross - target stance classi\ufb01cation with selfattention networks .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 778 \u2013 783 .", "entities": []}, {"text": "Ruifeng Xu , Yu Zhou , Dongyin Wu , Lin Gui , Jiachen Du , and Yun Xue . 2016 .", "entities": []}, {"text": "Overview of NLPCC shared task 4 : Stance detection in chinese microblogs .", "entities": [[7, 9, "TaskName", "Stance detection"]]}, {"text": "In Natural Language Understanding and Intelligent Applications , pages 907\u2013916 .", "entities": [[1, 4, "TaskName", "Natural Language Understanding"]]}, {"text": "Wei Xue and Tao Li .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Aspect based sentiment analysis with gated convolutional networks .", "entities": [[2, 4, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2514\u20132523 .", "entities": []}, {"text": "Guido Zarrella and Amy Marsh .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "MITRE at SemEval-2016 task 6 : Transfer learning for stance detection .", "entities": [[6, 8, "TaskName", "Transfer learning"], [9, 11, "TaskName", "stance detection"]]}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation ( SemEval-2016 ) , pages 458\u2013463 .", "entities": []}, {"text": "Bowen Zhang , Min Yang , Xutao Li , Yunming Ye , Xiaofei Xu , and Kuai Dai . 2020 .", "entities": []}, {"text": "Enhancing crosstarget stance detection with transferable semanticemotion knowledge .", "entities": [[2, 4, "TaskName", "stance detection"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 3188\u20133197 .", "entities": []}, {"text": "Elena Zotova , Rodrigo Agerri , Manuel Nu\u00f1ez , and German Rigau . 2020 .", "entities": []}, {"text": "Multilingual stance detection in tweets :", "entities": [[1, 3, "TaskName", "stance detection"]]}, {"text": "The Catalonia independence corpus .", "entities": []}, {"text": "In Proceedings of the 12th Language Resources and Evaluation Conference , pages 1368\u20131375 .", "entities": []}]