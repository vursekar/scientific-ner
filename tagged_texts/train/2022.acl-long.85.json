[{"text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 1190 - 1208 May 22 - 27 , 2022 c", "entities": []}, {"text": "2022 Association for Computational Linguistics Explanation Graph Generation via Pre - trained Language Models : An Empirical Study with Contrastive Learning Swarnadeep Saha Prateek Yadav Mohit Bansal UNC Chapel Hill { swarna , prateek , mbansal}@cs.unc.edu", "entities": [[6, 8, "TaskName", "Graph Generation"], [19, 21, "MethodName", "Contrastive Learning"]]}, {"text": "Abstract Pre - trained sequence - to - sequence language models have led to widespread success in many natural language generation tasks .", "entities": []}, {"text": "However , there has been relatively less work on analyzing their ability to generate structured outputs such as graphs .", "entities": []}, {"text": "Unlike natural language , graphs have distinct structural and semantic properties in the context of a downstream NLP task , e.g. , generating a graph that is connected and acyclic can be attributed to its structural constraints , while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts .", "entities": []}, {"text": "In this work , we study pre - trained language models that generate explanation graphs in an end - to - end manner and analyze their ability to learn the structural constraints and semantics of such graphs .", "entities": []}, {"text": "We \ufb01rst show that with limited supervision , pre - trained language models often generate graphs that either violate these constraints or are semantically incoherent .", "entities": []}, {"text": "Since curating large amount of humanannotated graphs is expensive and tedious , we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs .", "entities": []}, {"text": "Next , we leverage these graphs in different contrastive learning models with Max - Margin and InfoNCE losses .", "entities": [[8, 10, "MethodName", "contrastive learning"], [16, 17, "MethodName", "InfoNCE"]]}, {"text": "Our methods lead to signi\ufb01cant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks .", "entities": [[11, 12, "MetricName", "accuracy"], [21, 23, "TaskName", "graph generation"]]}, {"text": "Lastly , we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human - like negative graphs can lead to further improvements.1 1 Introduction Pre - trained sequence - to - sequence language models ( PLMs ) like BART ( Lewis et al . , 2020 ) and 1Our code and models are publicly available at https : //github.com / swarnaHub / ExplagraphGen .", "entities": [[12, 14, "MethodName", "contrastive learning"], [47, 48, "MethodName", "BART"]]}, {"text": "Belief :   Collectivism is terrible for society .", "entities": []}, {"text": "Argument :   Collectivism increases empathy .", "entities": []}, {"text": "Stance :   Counter Increases empathyCollectivism capable of causes Improve human relationship Terrible for societyis not aIncreases empathyCollectivism capable of Terrible for societyis not aEmpathy Societynot part of Gold GraphT5 - generated graph Structurally and Semantically Incorrect Belief :   Since fast foods are greasy and fattening ,               banning them would control obesity .", "entities": []}, {"text": "Argument :   McDonalds has salads .", "entities": []}, {"text": "Stance :   Counter mcdonaldsFast Food part of not has context greasy and fattening Gold Graph T5 - generated graph     Semantically Incorrect   Salads part ofFast Food greasy and fatteningSalads banning them control obesitycapable of has context part of causesFigure 1 : Two representative examples from ExplaGraphs ( Saha et al . , 2021b ) showing the belief , argument , stance , gold explanation graph , and T5 - generated explanation graph .", "entities": [[16, 17, "MethodName", "T5"], [69, 70, "MethodName", "T5"]]}, {"text": "The dashed nodes represent commonsense nodes and the dashed edges are incorrect edges .", "entities": []}, {"text": "The \ufb01rst generated graph is structurally incorrect and the second graph is semantically incorrect .", "entities": []}, {"text": "T5 ( Raffel et al . , 2020 ) have led to signi\ufb01cant advances in many natural language generation tasks like text summarization and machine translation .", "entities": [[0, 1, "MethodName", "T5"], [21, 23, "TaskName", "text summarization"], [24, 26, "TaskName", "machine translation"]]}, {"text": "The models are pre", "entities": []}, {"text": "- trained on massive amounts of text data with self - supervision , thus enabling them to construct coherent natural language sentences for downstream tasks .", "entities": []}, {"text": "This then raises the question whether pre - trained language models , trained on free - form natural language data , can also adapt themselves to generate structured outputs like graphs .", "entities": []}, {"text": "Graphs are common in NLP tasks1190", "entities": []}, {"text": "that involve representing structured knowledge in the form of knowledge bases ( Guarino and Giaretta , 1995 ) , constructing event chains from documents ( Chambers and Jurafsky , 2009 ) , or more recent work on encoding reasoning chains , explanations , or deductive proofs ( Saha et al . , 2020 ; Tafjord et al . , 2021 ; Dalvi et al . , 2021 ) .", "entities": []}, {"text": "Graphs differ from free - form natural language .", "entities": []}, {"text": "In the context of NLP , natural language graphs ( consisting of textual nodes and edges ) can have distinct structural and semantic properties .", "entities": []}, {"text": "For example , consider a recently proposed commonsense explanation graph generation task shown in Fig .", "entities": [[9, 11, "TaskName", "graph generation"]]}, {"text": "1", "entities": []}, {"text": "( Saha et al . , 2021b ) .", "entities": []}, {"text": "Each example shows a belief , an argument and an explanation graph explaining how the argument supports or refutes the belief .", "entities": []}, {"text": "These explanation graphs encode structured knowledge ( augmented with commonsense ) and consist of concepts as nodes and relations from ConceptNet ( Liu and Singh , 2004 ) as edges .", "entities": [[20, 21, "DatasetName", "ConceptNet"]]}, {"text": "For example , the second graph encodes the knowledge that \u201c both salads and fast food are part of mcdonalds and hence mcdonalds is not greasy and fattening \u201d , thus explicitly refuting the belief .", "entities": []}, {"text": "From prior work , the structural constraints enforce the graphs to be connected directed acyclic and the nodes to contain at least two concepts from the belief and two from the argument .", "entities": []}, {"text": "The semantic aspect deals with commonsense and evaluates whether each edge expresses coherent relational knowledge and if the whole graph explains the stance .", "entities": []}, {"text": "Following Saha et al .", "entities": []}, {"text": "( 2021b ) , we represent graphs as strings composed of concatenated edges and \ufb01ne - tune T5 to generate graphs in an autoregressive manner .", "entities": [[17, 18, "MethodName", "T5"]]}, {"text": "We observe that while moderate amount of supervision enables the model to learn valid graph encodings , the graphs frequently violate task - speci\ufb01c structural constraints ( like connectivity ) .", "entities": []}, {"text": "For instance , the \ufb01rst example in Fig .", "entities": []}, {"text": "1 shows a graph generated by T5 that is disconnected and hence structurally incorrect .", "entities": [[6, 7, "MethodName", "T5"]]}, {"text": "Moreover , for the fraction of graphs that are structurally correct , the model also makes commonsense mistakes , a type of semantic error , by inferring wrong or incoherent relations between concepts .", "entities": []}, {"text": "Both T5 - generated graphs shown in Fig . 1 contain incoherent or noncommonsensical edges ( marked by dashed arrows ) like \u201c fast food ; has context ; salads \u201d .", "entities": [[1, 2, "MethodName", "T5"]]}, {"text": "Based on these observations , we study PLMs that generate explanation graphs in an end - to - end manner and analyze their ability to learn the structural constraints aswell as the semantics of such graphs .", "entities": []}, {"text": "While a general recipe towards improving the structural and semantic aspects of graph generation can be via large - scale training with more humanannotated graphs , it is prohibitive under most practical scenarios because of the cognitive load associated with a complex data creation task like graph annotation ( Dalvi et al . , 2021 ; Saha et", "entities": [[12, 14, "TaskName", "graph generation"]]}, {"text": "al . , 2021b ) .", "entities": []}, {"text": "Hence , we propose simple yet effective methods of graph perturbations that perform various kinds of node and edge addition , deletion , and replacement operations to construct structurally and semantically positive ( correct ) and negative ( incorrect ) graphs .", "entities": []}, {"text": "Overall , we leverage three types of negative graphs ( synthetic structural , synthetic semantic , and human - created semantic ) and develop multiple contrastive learning models ( Hjelm et al . , 2018 ; Chen et al . , 2020a ; Khosla et al . , 2020 ; Gunel et al . , 2020 ) for effectively distinguishing between correct and incorrect graphs .", "entities": [[25, 27, "MethodName", "contrastive learning"]]}, {"text": "Our \ufb01rst method is a Generate - and - Re\ufb01ne model that \ufb01rst generates an initial graph and further re\ufb01nes it using another T5 model .", "entities": [[23, 24, "MethodName", "T5"]]}, {"text": "Next , we propose two improved models \u2013 one that uses the negative graphs in a max - margin formulation and another that uses both positive and negative graphs with a InfoNCE ( van den Oord et al . , 2018 ) contrastive loss .", "entities": [[31, 32, "MethodName", "InfoNCE"], [43, 44, "MetricName", "loss"]]}, {"text": "On two real - world tasks of explanation graph generation and temporal graph generation , with varied node and edge semantics , we observe that our proposed methods and graph perturbation techniques generalize well and lead to improvements in both structural and semantic accuracy of graphs .", "entities": [[8, 10, "TaskName", "graph generation"], [12, 14, "TaskName", "graph generation"], [43, 44, "MetricName", "accuracy"]]}, {"text": "Further analysis of different types of negative graphs reveal that the human - error graphs are the hardest , most diverse , and hence the best type of negatives to learn from in contrastive learning .", "entities": [[33, 35, "MethodName", "contrastive learning"]]}, {"text": "Hence , we also develop methods to automatically generate more such human - like semantic negative graphs , which leads to further improvements .", "entities": []}, {"text": "We summarize our contributions as follows .", "entities": []}, {"text": "\u2022We present a detailed analysis of graph structure and semantics for end - to - end explanation graph generation via pre - trained language models .", "entities": [[17, 19, "TaskName", "graph generation"]]}, {"text": "\u2022We propose simple yet effective graph perturbation techniques for constructing positive and negative graphs and use them in different graph contrastive learning models .", "entities": [[20, 22, "MethodName", "contrastive learning"]]}, {"text": "\u2022Our methods lead to signi\ufb01cant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks.1191", "entities": [[11, 12, "MetricName", "accuracy"], [21, 23, "TaskName", "graph generation"]]}, {"text": "2 Related Work Graph Generation from Language Models .", "entities": [[3, 5, "TaskName", "Graph Generation"]]}, {"text": "Representative works on graph generation from language models include knowledge graph completion models like Comet ( Bosselut et al . , 2019 ; Hwang et", "entities": [[3, 5, "TaskName", "graph generation"], [9, 12, "TaskName", "knowledge graph completion"]]}, {"text": "al . , 2021 ) that \ufb01ne - tune GPT ( Radford et al . , 2019 ; Brown et al . , 2020 ) and BART ( Lewis et al . , 2020 ) , generation of event in\ufb02uence graphs ( Tandon et al . , 2019 ; Madaan et al . , 2020 ) , partially ordered scripts ( Sakaguchi et al . , 2021 ) , temporal graphs ( Madaan and Yang , 2021 ) , entailment trees ( Dalvi et al . , 2021 ) , proof graphs ( Saha et al . , 2020 ; Tafjord et al . , 2021 ; Saha et al . , 2021a ) and commonsense explanation graphs ( Saha et al . , 2021b ) .", "entities": [[9, 10, "MethodName", "GPT"], [26, 27, "MethodName", "BART"]]}, {"text": "Linguistic tasks like syntactic parsing ( Zhou et al . , 2020 ; Mohammadshahi and Henderson , 2021 ; Kondratyuk and Straka , 2019 ) and semantic parsing ( Chen et al . , 2020b ; Shin et al . , 2021 ) have also made use of language models .", "entities": [[26, 28, "TaskName", "semantic parsing"]]}, {"text": "There is also a large body of work on building generative models for learning unconditional graph distributions ( You et al . , 2018 ; Simonovsky and Komodakis , 2018 ; Grover et al . , 2019 ; Liao et al . , 2019 ; Shi * et al . , 2020 ) without any semantics attached to the graphs .", "entities": []}, {"text": "Our novelty lies in presenting the \ufb01rst systematic analysis of structure and semantics of graph generation for two downstream NLP tasks using pre - trained language models and improving them via constrastive learning .", "entities": [[14, 16, "TaskName", "graph generation"]]}, {"text": "Data Augmentation and Contrastive Learning .", "entities": [[0, 2, "TaskName", "Data Augmentation"], [3, 5, "MethodName", "Contrastive Learning"]]}, {"text": "Data Augmentation for NLP ( Hedderich et al . , 2020 ; Feng et", "entities": [[0, 2, "TaskName", "Data Augmentation"]]}, {"text": "al . , 2021 ; Chen et al . , 2021 ) has been a powerful tool in low - data settings , ranging from its early usages with synonym replacement ( Kolomiyets et al . , 2011 ; Wang and Yang , 2015 ) to more recent methods of perturbing hidden representations ( Miyato et al . , 2016 ; Shen et al . , 2020 ) .", "entities": []}, {"text": "Contrastive learning , beyond its historical use in learning robust image representations ( Chopra et al . , 2005 ; Hadsell et al . , 2006 ; Gutmann and Hyv\u00e4rinen , 2010 ; Hoffer and Ailon , 2015 ; Hjelm et al . , 2018 ; Chen et al . , 2020a ; He et al . , 2020 ) has been explored in supervised scenarios ( Khosla et al . , 2020 ; Gunel et al . , 2020 ) and for NLP , in training self - supervised language models ( Fang et al . , 2020 ) , learning sentence representations ( Gao et al . , 2021 ) , document clustering ( Zhang et al . , 2021 ) , summarization ( Liu and Liu , 2021 ; Cao and Wang , 2021 ) and generic text generation ( Lee et al . , 2020 ) .", "entities": [[0, 2, "MethodName", "Contrastive learning"], [124, 125, "TaskName", "summarization"], [140, 142, "TaskName", "text generation"]]}, {"text": "It has also been used in unconditional graph representation learning ( You et al . , 2020 ; Hassani andKhasahmadi , 2020 ; Zhu et al . , 2021 ) .", "entities": [[7, 10, "TaskName", "graph representation learning"]]}, {"text": "We follow this rich line of work to explore their applicability in supervised graph generation tasks from pretrained language models in low - resource settings .", "entities": [[13, 15, "TaskName", "graph generation"], [17, 20, "TaskName", "pretrained language models"]]}, {"text": "Generative Commonsense Reasoning .", "entities": []}, {"text": "While traditional commonsense reasoning tasks are discriminative in nature ( Zellers et al . , 2018 ; Talmor et al . , 2019 ; Sap et al . , 2019 ; Bisk et", "entities": []}, {"text": "al . , 2020 ; Sakaguchi et", "entities": []}, {"text": "al . , 2020 ; Talmor et", "entities": []}, {"text": "al . , 2021 ) , recent focus on generative evaluation have led to the development of tasks and benchmarks that explore unstructured commonsense sentence generation ( Lin et al . , 2020 ) , event in\ufb02uence graph generation ( Madaan et al . , 2020 ) , commonsense explanation graph generation ( Saha et al . , 2021b ) , etc .", "entities": [[37, 39, "TaskName", "graph generation"], [50, 52, "TaskName", "graph generation"]]}, {"text": "We experiment with two graph generation tasks , primarily focusing on ExplaGraphs ( Saha et al . , 2021b ) because of the clear distinction in the underlying structural constraints and the semantic aspect dealing with commonsense .", "entities": [[4, 6, "TaskName", "graph generation"]]}, {"text": "3 Motivation and Background Our primary task of interest is a recently proposed commonsense explanation graph generation task called ExplaGraphs ( Saha et al . , 2021b ) .", "entities": [[15, 17, "TaskName", "graph generation"]]}, {"text": "In Sec . 6.4 , we also experiment with another related task of temporal graph generation ( Madaan et al . , 2020 ) .", "entities": [[14, 16, "TaskName", "graph generation"]]}, {"text": "In both these tasks , the structural aspect deals with satisfying certain task - speci\ufb01c constraints on the graph ( like connectivity ) and the semantic aspect deals with the construction of meaningful edges ( that adhere to commonsense ) .", "entities": []}, {"text": "Below we discuss ExplaGraphs brie\ufb02y and analyze pre - trained language models for their ability to generate explanation graphs .", "entities": []}, {"text": "ExplaGraphs ( Saha et al . , 2021b ) .", "entities": []}, {"text": "In this task , given a belief and an argument , an agent has to perform two sub - tasks \u2013 predict the stance ( support / counter ) and also generate an explanation graph explaining the stance .", "entities": [[12, 13, "DatasetName", "agent"]]}, {"text": "Explanation graphs are structured explanations that capture explicit reasoning chains between the belief and the argument , thereby making models more interpretable .", "entities": []}, {"text": "Formally , an explanation graph is a connected DAG with nodes as concepts and edges as commonsense relations between two concepts ( See Fig . 1 ) .", "entities": []}, {"text": "The concepts are either part of the belief or the argument ( represented with solid boxes ) or any external commonsense phrase ( represented with dashed boxes ) .", "entities": []}, {"text": "Each edge in the graph forms a coherent sentence and the graph , when read as a whole,1192", "entities": []}, {"text": "forms reasoning structures explaining why the argument supports or refutes the belief .", "entities": []}, {"text": "Saha et al .", "entities": []}, {"text": "( 2021b ) evaluate explanation graphs by de\ufb01ning two accuracy metrics \u2013 ( 1 ) Structural Correctness Accuracy ( StCA ) :", "entities": [[9, 10, "MetricName", "accuracy"], [17, 18, "MetricName", "Accuracy"]]}, {"text": "Fraction of graphs that satisfy all structural constraints , and ( 2 ) Semantic Correctness Accuracy ( SeCA ) : Fraction of graphs that are both structurally and semantically correct .", "entities": [[15, 16, "MetricName", "Accuracy"]]}, {"text": "A graph is considered structurally correct if it satis\ufb01es the following constraints : ( 1 ) it is connected , ( 2 ) it is a DAG , ( 3 ) the edge relations belong to a pre - de\ufb01ned list , ( 4 ) there are at least two concepts from the belief and two from the argument .", "entities": []}, {"text": "If all these constraints are satis\ufb01ed , the graph is next evaluated for semantic correctness by a model - based metric ( Saha et al . , 2021b ) .", "entities": []}, {"text": "It works on the principle that an explanation graph is semantically correct if the stance inferred from the belief and the graph matches the gold stance .", "entities": []}, {"text": "Refer to Appendix A for a detailed description of all evaluation metrics .", "entities": []}, {"text": "Baseline T5 Model .", "entities": [[1, 2, "MethodName", "T5"]]}, {"text": "Following prior work ( Saha et al . , 2021b ) , we generate explanation graphs as post - hoc explanations by conditioning on the belief , argument and the predicted stance.2The stance prediction model is a \ufb01ne - tuned RoBERTa model ( Liu et", "entities": [[40, 41, "MethodName", "RoBERTa"]]}, {"text": "al . , 2019 ) which we keep unaltered from prior work and focus on the graph generation sub - task .", "entities": [[16, 18, "TaskName", "graph generation"]]}, {"text": "We generate graphs as linearized strings in an endto - end manner by leveraging an encoder - decoder pre - trained language model , T5 ( Raffel et al . , 2020 ) .", "entities": [[24, 25, "MethodName", "T5"]]}, {"text": "The input to the model is the concatenated belief , argument and the stance along with a pre\ufb01x \u201c Generate an Explanation Graph for \u201d .", "entities": []}, {"text": "The graphs are encoded as concatenated bracketed edges , in which the edges are ordered according to the Depth First Search ( DFS ) order of the nodes .", "entities": []}, {"text": "While we choose T5 because of its superior performance ( Saha et al . , 2021b ) , we do not make any model - speci\ufb01c assumptions and graphs can be generated via any encoder - decoder style pre - trained language model ( e.g. , see Appendix E for results with BART ) .", "entities": [[3, 4, "MethodName", "T5"], [52, 53, "MethodName", "BART"]]}, {"text": "Analysis of T5 Baseline .", "entities": [[2, 3, "MethodName", "T5"]]}, {"text": "We analyze the quality of the explanation graphs generated by T5 in Table 1 .", "entities": [[10, 11, "MethodName", "T5"]]}, {"text": "We vary the amount of training data from 500 to 2368 samples ( all ) and report StCA and SeCA along with other metrics like Graph - BertScore ( GBS ) introduced in prior work ( Saha et al . , 2021b ) .", "entities": []}, {"text": "2These are rationalizing models ( Rajani et al . , 2019 ; Hase et al . , 2020 ) that \ufb01rst predict the stance , followed by the graph .", "entities": []}, {"text": "While graphs can also be generated \ufb01rst , followed by the stance , we experiment with one model family for this work .", "entities": []}, {"text": "Count StCA \" SeCA \" G - BS \" GED # EA \" 500 42.5 20.7 36.3 0.68 20.4 1000 49.2 23.7 42.2 0.63 26.2 1500 50.7 33.2 43.4 0.61 28.2 2368 51.0 34.7 43.9 0.61 29.5 Table 1 : Performance of T5 - large with varying amount of training data on ExplaGraphs test set .", "entities": [[9, 10, "DatasetName", "GED"], [42, 43, "MethodName", "T5"]]}, {"text": "While the structural accuracy improves with increase in training data , the gain saturates quickly and even after training on the entire data , we \ufb01nd a signi\ufb01cant fraction of graphs to violate the structural constraints .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "We note that a high 91 % of T5 \u2019s generations are valid graph encodings i.e. , the generated strings can be parsed into graphical structures ( without any post - processing ) , suggesting that T5 is able to learn the graph encoding from a fairly small amount of supervision .", "entities": [[8, 9, "MethodName", "T5"], [36, 37, "MethodName", "T5"]]}, {"text": "However , it fails to satisfy the various structural constraints \u2013 ( 1 ) 20 % of the graphs are disconnected , ( 2 ) 6 % of the graphs contain cycles , and ( 3 ) 14 % of the graphs have less than two concepts from the belief or from the argument .", "entities": []}, {"text": "Note that these constraints are not encoded in the model , thus making them fairly hard to learn from limited supervision .", "entities": []}, {"text": "On the fraction of structurally correct graphs , the model makes further semantic errors and a lower SeCA of 35 % demonstrates that .", "entities": []}, {"text": "In Fig .", "entities": []}, {"text": "1 , we show examples of structurally incorrect and semantically incorrect graphs generated by T5 .", "entities": [[14, 15, "MethodName", "T5"]]}, {"text": "Overall , these results indicate that there is a signi\ufb01cant scope for improvement both on graph structure and semantics , thus motivating us to develop methods with design choices aimed at improving both aspects .", "entities": []}, {"text": "4 Graph Perturbations Most prior works that collect human - annotated graphs for a downstream NLP task have found such collection processes to be quite expensive and tedious ( Tandon et al . , 2019 ;", "entities": []}, {"text": "Dalvi et al . , 2021 ; Saha et al . , 2021b ) .", "entities": []}, {"text": "For instance , Saha et al .", "entities": []}, {"text": "( 2021b ) obtained high - quality data only after multiple rounds of re\ufb01nement and Dalvi et al .", "entities": []}, {"text": "( 2021 ) employ trained expert annotators for entailment tree construction .", "entities": []}, {"text": "The corresponding datasets are also relatively small in size ( 2 - 3k ) , thus limiting the prospect of large - scale training .", "entities": []}, {"text": "Hence , our approach towards improving explanation graph generation is through data augmentation techniques that perturb human - curated graphs to construct positive and negative graphs .", "entities": [[7, 9, "TaskName", "graph generation"], [11, 13, "TaskName", "data augmentation"]]}, {"text": "As noted earlier , we wish to construct graphs that enable better learning of1193", "entities": []}, {"text": "\" Generate an Explanation graph for    Belief :   Banning whaling is humane .", "entities": []}, {"text": "[ SEP ]   Argument :   Banning whaling would harm the workforce , which would be an inhumane act for the people .", "entities": []}, {"text": "[ SEP ]   Stance :   Counter \" T5Positive NegativesGold Graphharm the workforce loss of jobsbanning whaling capable of causes humaneis not aSynthetic Positive Graphharm the workforce going of businessbanning whaling capable of causes humaneis not a Synthetic Semantic ( SySe )", "entities": [[14, 15, "MetricName", "loss"]]}, {"text": "harm the workforce loss of jobsbanning whaling antonym of causes humaneis aHuman Semantic ( HuSe)occupation workforcewhaling part of used for inhumanehas propertybanningnot desires Synthetic Structural ( SySt )    harm the workforce loss of jobsbanning whaling capable of humaneis", "entities": [[3, 4, "MetricName", "loss"], [32, 33, "MetricName", "loss"]]}, {"text": "not a ,   , Figure 2 : Our T5 - based contrastive learning framework for graph generation using positively and three kinds of negatively perturbed graphs .", "entities": [[9, 10, "MethodName", "T5"], [12, 14, "MethodName", "contrastive learning"], [16, 18, "TaskName", "graph generation"]]}, {"text": "structural graph constraints and their semantics .", "entities": []}, {"text": "4.1 Positive Graph Perturbations One simple method to augment existing training data is to create synthetic positive graphs .", "entities": []}, {"text": "These graphs should be created such that all the taskspeci\ufb01c constraints continue to hold upon perturbations .", "entities": []}, {"text": "E.g. , removing a node that makes the graph disconnected is a prohibitive action .", "entities": []}, {"text": "Hence , we choose nodes ( concepts ) that are not part of the belief or the argument ( also termed as commonsense nodes ) and replace them with phrases that are synonymous to the original phrases .", "entities": []}, {"text": "To do so , we select words from the concept with POS tags of Adjective , Noun , Adverb , or Verb and replace them with that synonym from Wordnet ( Miller , 1995 ) for which the cosine similarity of their word2vec representations ( Mikolov et al . , 2013 ) is the highest.3Fig .", "entities": []}, {"text": "2 shows an example of a positive graph perturbation where the node \u201c loss of jobs \u201d is replaced with \u201c going of business \u201d .", "entities": [[13, 14, "MetricName", "loss"]]}, {"text": "Note that our node replacement operations will always lead to structurally similar graphs .", "entities": []}, {"text": "Automatically constructing structurally diverse positive graphs is a challenging problem and we leave that for future work .", "entities": []}, {"text": "4.2 Negative Graph Perturbations In order to enable the model to learn from explicit hard negatives , we construct three diverse types of graphs \u2013 synthetically constructed structural negatives for learning graph constraints and synthetic 3We also tried similar replacement operations with antonyms .", "entities": []}, {"text": "However , they often lead to semantically inconsistent graphs .", "entities": []}, {"text": "E.g. , A causes B does not always imply A not causes not B ornot A not causes not B.and human - created semantic negatives to capture a fairly large space of semantically incorrect graphs .", "entities": []}, {"text": "Below we discuss the construction of these graphs .", "entities": []}, {"text": "Synthetic & Structurally Negative Graphs ( SySt ) .", "entities": []}, {"text": "As shown previously , one common source of errors in the generated explanation graphs is the violation of structural constraints .", "entities": []}, {"text": "To enable learning these constraints , we generate four types of negative graphs by performing the following perturbations on each ground - truth graph : ( 1 ) removing an edge at random such that the resultant graph becomes disconnected , ( 2 ) adding an edge between two randomly chosen nodes such that the resultant graph becomes cyclic , ( 3 ) adding and removing one edge at random such that the resultant graph becomes both disconnected and cyclic , ( 4 ) removing a node randomly such that the resultant graph contains less than two concepts from the belief or argument .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "2 shows an example of a disconnected graph created as part of the structurally negative graphs .", "entities": []}, {"text": "Synthetic & Semantic Negative Graphs ( SySe ) .", "entities": []}, {"text": "We also construct semantically incorrect negative explanation graphs .", "entities": []}, {"text": "While the previous category of negative graphs ( SySt ) captures structural constraints , SySe captures the relational knowledge in graphs .", "entities": []}, {"text": "Semantic incorrectness typically arises from inappropriate relations that do not adhere to human commonsense ( \u201c loss of jobs ; is a ; humane \u201d ) .", "entities": [[16, 17, "MetricName", "loss"]]}, {"text": "We create such negative graphs by selecting a random number of edges and then replacing the relations with some other relations .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "2 shows a semantic negative graph in which the relations marked with dashed lines are perturbed .", "entities": []}, {"text": "Human - created & Semantic Negative Graphs ( HuSe ) .", "entities": []}, {"text": "The space of semantically incorrect graphs is fairly large and in order to augment our synthetic negative graphs with harder structurallydiverse negatives , we make use of human - created incorrect graphs from prior work ( Saha et al . , 2021b).4Humans make subtle errors , thus making them ideal negative candidates for contrastive learning .", "entities": [[53, 55, "MethodName", "contrastive learning"]]}, {"text": "ExplaGraphs was constructed via an iterative framework in which the graphs are iteratively re\ufb01ned ( up to two times ) until they are veri\ufb01ed as correct .", "entities": []}, {"text": "We treat these re\ufb01ned graphs as negatives .", "entities": []}, {"text": "Speci\ufb01cally , in two rounds , if an initial graph G1 4Publicly released by Saha et al .", "entities": []}, {"text": "( 2021b ) at https : //github.com / swarnaHub / ExplaGraphs / blob/ main / data / refinement_graphs_train.tsv .1194", "entities": []}, {"text": "is re\ufb01ned into graphs G2andG3successively , then G1andG2are considered as negative graphs .", "entities": []}, {"text": "Unlike SySe which only perturb the relations , these negatives are structurally diverse ( see Fig . 2 ) and capture semantics not just at the level of each edge but for the graph as a whole ( e.g. , a graph might be re\ufb01ned because it does not explain the stance ) .", "entities": []}, {"text": "Note that human - created graphs can only be semantically incorrect , since their structural correctness is already ensured during construction .", "entities": []}, {"text": "5 Augmentation with Perturbed Graphs Next we propose different methods of leveraging these positive and negative graphs for explanation graph generation .", "entities": [[19, 21, "TaskName", "graph generation"]]}, {"text": "Our models either use only positive graphs as simple data augmentation , only negative graphs in a max - margin model , or both in a Generate & Re\ufb01ne model and a Contrastive model .", "entities": [[9, 11, "TaskName", "data augmentation"]]}, {"text": "5.1 Augmentation with Positive Graphs In this \ufb01rst simple approach , we augment the training data with the synthetically created positive graphs and retrain the baseline T5 model .", "entities": [[26, 27, "MethodName", "T5"]]}, {"text": "5.2 Max - Margin Graph Generation Model Our next model leverages the negatively perturbed graphs in a max - margin formulation .", "entities": [[4, 6, "TaskName", "Graph Generation"]]}, {"text": "During training , given a ( belief , argument , stance ) context x , a ground truth graph G(g)and a negative graph G(n ) , linearized into a sequence of words fy(g ) igk i=1and fy(n ) igl i=1respectively , we de\ufb01ne the loss function Las a linear combination of the standard crossentropy lossLCEand a max - margin loss LMM , de\ufb01ned between a word y(g ) iof the positive graph and a wordy(n ) iof the negative graph .", "entities": [[44, 45, "MetricName", "loss"], [59, 60, "MetricName", "loss"]]}, {"text": "LCE = X i\u0000logP\u0012(y(g ) ijy(g ) < i;x )", "entities": []}, {"text": "LMM = X imax(0;logP\u0012(y(g ) ijy(g ) < i;x ) \u0000logP\u0012(y(n ) ijy(n ) < i;x )", "entities": []}, {"text": "+ \f )", "entities": []}, {"text": "L = LCE+ \u000b LMM where \u000b and \f ( margin ) are hyperparameters .", "entities": []}, {"text": "As noted earlier , the baseline model often makes commonsense mistakes in distinguishing between positive and negative relations ( \u201c causes \u201d vs \u201c not causes \u201d ) and our relation perturbing negative graphs and the max - margin loss component facilitate learning a better boundary between them.5.3 Generate & Re\ufb01ne Graph Generation ExplaGraphs was constructed using a \u201c Re\ufb01nement \u201d phase wherein the initially constructed graphs that are marked incorrect by human veri\ufb01ers are further re\ufb01ned by another set of annotators .", "entities": [[39, 40, "MetricName", "loss"], [51, 53, "TaskName", "Graph Generation"]]}, {"text": "Here we emulate the graph re\ufb01nement phase with the help of a model .", "entities": []}, {"text": "Speci\ufb01cally , our approach is a 2 - stage pipeline \u2013 \ufb01rst , an initial graph is generated by the baseline T5 model and second , an Explanation Graph Re\ufb01nement model conditions on the initial graph , along with the belief , argument and the stance to re\ufb01ne the graph .", "entities": [[21, 22, "MethodName", "T5"]]}, {"text": "The re\ufb01ner is also a T5 model \ufb01ne - tuned with the pre\ufb01x \u201c Re\ufb01ne the Explanation Graph for \u201d on all positive and negative graphs described in Sec .", "entities": [[5, 6, "MethodName", "T5"]]}, {"text": "4 . Note that our approach differs from the actual data collection process in two aspects .", "entities": []}, {"text": "Unlike the human - annotated graphs , which are re\ufb01ned only for semantic correctness , the model - generated graphs can be both structurally and semantically incorrect .", "entities": []}, {"text": "Second , our approach does not involve a graph veri\ufb01cation stage and thus , the re\ufb01ner model acts on all ( correct and incorrect ) graphs generated in stage 1 and is thus trained with both correct and incorrect graphs .", "entities": []}, {"text": "5.4 Contrastive Graph Generation Model Our Contrastive Graph Generation Model ( Fig . 2 ) also leverages both positive and negative graphs but instead of doing so in a 2 - stage Generate & Re\ufb01nemodel , uses a contrastive learning framework ( Khosla et al . , 2020 ; Gunel et al . , 2020 ) .", "entities": [[2, 4, "TaskName", "Graph Generation"], [7, 9, "TaskName", "Graph Generation"], [38, 40, "MethodName", "contrastive learning"]]}, {"text": "Given a ground - truth graph G(g ) , a positive graphG(p)and a set of negative graphs fG(n ) igM i=1 , contrastive learning aims to learn the graph representations such that the gold graph \u2019s representation is close to that of the synthetic positive graph while being distant from those of the negative graphs .", "entities": [[22, 24, "MethodName", "contrastive learning"]]}, {"text": "Similar to Cao and Wang ( 2021 ) , we use the last layer of the decoder in T5 as the representation of each token in the graph and obtain the graph representation by averaging over the constituent token representations .", "entities": [[18, 19, "MethodName", "T5"]]}, {"text": "Let the graph representations be denoted by h(g),h(p)and fh(n ) igM i=1 .", "entities": []}, {"text": "GivenH(g)=fh(p)gSfh(n ) igM i=1 , our overall loss combines the cross - entropy loss LCE and the InfoNCE contrastive loss ( van den Oord et", "entities": [[7, 8, "MetricName", "loss"], [13, 14, "MetricName", "loss"], [17, 18, "MethodName", "InfoNCE"], [19, 20, "MetricName", "loss"]]}, {"text": "al . , 2018)LCLas shown below .", "entities": []}, {"text": "LCL=\u0000logexp(sim(h(g);h(p))= \u001c ) P hi2H(g)exp(sim(h(g);hi)= \u001c )", "entities": []}, {"text": "L = LCE+ \u000b LCL1195", "entities": []}, {"text": "SA\"StCA \" SeCA \" G - BS \" GED # EA \" T5 - Base ( Saha et al . , 2021b ) 87.2 38.7 19.0 33.6 0.71 20.8 T5 - Large 87.2 51.0 34.7 43.9 0.61 29.5 Generate & Re\ufb01ne 87.2 52.5 37.7 45.3 0.60 30.0 Pos Data Aug 87.2 54.5 41.5 46.9 0.58 30.2 Max - Margin 87.2 56.7 43.5 48.6 0.57 30.5 Contrastive 87.2 60.5 42.5 52.1 0.52 33.1 Upper Bound 91.0 91.0 83.5 71.1 0.38 46.8 Table 2 : Comparison of all models across all metrics on the ExplaGraphs ( Saha et al . , 2021b ) test set .", "entities": [[8, 9, "DatasetName", "GED"], [12, 13, "MethodName", "T5"], [29, 30, "MethodName", "T5"]]}, {"text": "Improvement in SeCA is statistically signi\ufb01cant ( computed using Bootstrap test ( Efron and Tibshirani , 1994 ) ) with p<0:005 .", "entities": []}, {"text": "where \u000b and the temperature \u001c are the hyperparameters and sim ( ) denotes the cosine similarity function between the graph representations .", "entities": []}, {"text": "6 Experiments 6.1 Impact of Different Models on Graph Structural and Semantic Accuracy In Table 2 , we compare the various modeling techniques described in Sec . 5 and their effect on the structural and semantic correctness of the generated graphs .", "entities": [[12, 13, "MetricName", "Accuracy"]]}, {"text": "While our primary metrics of interest are Graph Structural Accuracy ( StCA ) and Semantic Accuracy ( SeCA ) , following prior work ( Saha et al . , 2021b ) , we also report Stance Accuracy ( SA ) , Graph - BertScore ( G - BS ) , Graph Edit Distance ( GED ) and Edge Accuracy ( EA ) .", "entities": [[9, 10, "MetricName", "Accuracy"], [15, 16, "MetricName", "Accuracy"], [36, 37, "MetricName", "Accuracy"], [54, 55, "DatasetName", "GED"], [58, 59, "MetricName", "Accuracy"]]}, {"text": "Effect of Model Size and Training Data .", "entities": []}, {"text": "The T5 - Large model uses the same setup as the T5 - Base model experimented with in Saha et al .", "entities": [[1, 2, "MethodName", "T5"], [11, 12, "MethodName", "T5"]]}, {"text": "( 2021b ) .", "entities": []}, {"text": "We observe that using a larger T5 model improves StCA by 12 % and SeCA by 16 % .", "entities": [[6, 7, "MethodName", "T5"]]}, {"text": "This \ufb01nding is in line with other commonsense reasoning tasks ( Lourie et al . , 2021 ; Elazar et al . , 2021 ) which also show that \ufb01ne - tuning a larger language model typically leads to better performance .", "entities": []}, {"text": "Together with the results reported in Table 1 , we conclude that much of the improvement in explanation graph generation comes from increasing the training data and using a larger model .", "entities": [[18, 20, "TaskName", "graph generation"]]}, {"text": "Given its superior performance , we build our proposed models on T5 - large .", "entities": [[11, 12, "MethodName", "T5"]]}, {"text": "Results with Generate & Re\ufb01ne Model .", "entities": []}, {"text": "The Generate & Re\ufb01ne model ( Sec . 5.3 ) improves all metrics ; however the gains are small .", "entities": []}, {"text": "Note that this model re\ufb01nes all graphs ( correct or not ) and can lead to already correct graphs becoming incorrect after re\ufb01nement .", "entities": []}, {"text": "In practice , we observe that most graphs do not change much after re\ufb01nement which we believe stems from the model \u2019s inability to distinguish between correct and incorrect graphs .", "entities": []}, {"text": "Effect of Positive Graph Perturbations .", "entities": []}, {"text": "On retraining T5 augmented with the positively perturbed graphs ( Sec . 5.1 ) , we observe that it obtains signi\ufb01cant improvement over T5 and Generate & Re\ufb01ne both in structural and semantic accuracy .", "entities": [[2, 3, "MethodName", "T5"], [23, 24, "MethodName", "T5"], [33, 34, "MetricName", "accuracy"]]}, {"text": "Note that , by construction , the positive graphs only differ in the commonsense concepts ( not part of the belief or argument ) while keeping the structure intact .", "entities": []}, {"text": "Hence , the model has more supervision about the semantics of the graphs as opposed to the structural constraints .", "entities": []}, {"text": "This is re\ufb02ected in the larger improvement in SeCA .", "entities": []}, {"text": "The positive graphs , being structurally correct , also reinforces the model \u2019s belief about structural correlation with correct graphs , thus leading to some improvement in StCA as well .", "entities": []}, {"text": "Effect of Negative Graph Perturbations .", "entities": []}, {"text": "The Max - Margin model ( Sec . 5.2 ) leverages all structurally and semantically incorrect graphs and obtains up to 6 % and 9 % improvement in StCA and SeCA respectively over the baseline T5 model .", "entities": [[35, 36, "MethodName", "T5"]]}, {"text": "The model implicitly learns the structural constraints through relevant supervision and the margin - based loss enables it to learn a better boundary between correct and incorrect graphs .", "entities": [[15, 16, "MetricName", "loss"]]}, {"text": "Similarly , the semantically perturbed graphs improves the model \u2019s relation prediction capability between concepts .", "entities": []}, {"text": "The Max - Margin model outperforms the Pos Data Aug model because of the former having access to both structural and semantic supervision while the latter is only augmented with structurally similar graphs .", "entities": []}, {"text": "Effect of Positive and Negative Graph Perturbations with Contrastive Learning .", "entities": [[8, 10, "MethodName", "Contrastive Learning"]]}, {"text": "The Contrastive Graph Generation model ( Sec . 5.4 ) leverages both positive and negative graphs and improves StCA to 60 % with comparable SeCA to the Max - Margin model .", "entities": [[2, 4, "TaskName", "Graph Generation"]]}, {"text": "The overall improvements in StCA and SeCA are 9 % and 8 % respectively compared to T5 .", "entities": [[16, 17, "MethodName", "T5"]]}, {"text": "We hypothesize that the constrastive model does not lead to further improvement in SeCA because of the structurally similar positive1196", "entities": []}, {"text": "StCA \" SeCA \" G - BS \" GED # EA \" T5 - Large 46.5 31.6 36.8 0.66 26.7 + SySt", "entities": [[8, 9, "DatasetName", "GED"], [12, 13, "MethodName", "T5"]]}, {"text": "50.2 34.1 40.7 0.64 27.4 + SySe 50.7 35.1 40.8 0.63 27.3 + HuSe 49.5 38.4 39.4 0.64 26.1 Table 3 : Ablation study showing the effect of different types of negative graphs on ExplaGraphs dev set .", "entities": []}, {"text": "Valid \" StCA \" G - BS \" T5 - Base 88.8 88.7 54.4 Max - Margin 89.1 87.7 55.7 Contrastive 97.5 96.9 57.2 Table 4 : Comparison of T5 , Max - Margin and Contrastive models for temporal graph generation .", "entities": [[8, 9, "MethodName", "T5"], [29, 30, "MethodName", "T5"], [39, 41, "TaskName", "graph generation"]]}, {"text": "graphs .", "entities": []}, {"text": "This can potentially be improved by incorporating more structurally diverse graphs .", "entities": []}, {"text": "Finally , our best SeCA is far from perfect and signi\ufb01cant future work can be done in improving the graph semantics .", "entities": []}, {"text": "Further ablations of negative graphs and human evaluation are done on the Max - Margin model , due to its slightly higher SeCA .", "entities": []}, {"text": "6.2 Human Evaluation of Graph Semantics Automatically evaluating graphs for semantic correctness is challenging .", "entities": []}, {"text": "We conduct human evaluation to further validate our \ufb01ndings .", "entities": []}, {"text": "We compare the graphs generated by T5 and our Max - Margin model on Amazon Mechanical Turk where three annotators choose which graph is better or if they are mostly similar ( instructions in Appendix F ) .", "entities": [[6, 7, "MethodName", "T5"]]}, {"text": "For fair comparison , we evaluate only those samples where both models predict the correct stance and the graphs are also structurally correct .", "entities": []}, {"text": "In fact , this lets us evaluate the semantic aspect in isolation when both graphs are structurally correct .", "entities": []}, {"text": "With majority voting on 150 samples , we observe that ourMax - Margin model \u2019s graphs are preferred 13 % more times compared to those of the T5 model ( 43 % vs 30 % and statistically signi\ufb01cant with p < 0.05 ) while in 22 % cases , the graphs are marked similar ( remaining have no majority ) .", "entities": [[27, 28, "MethodName", "T5"]]}, {"text": "6.3 Ablation with Negative Graphs In Table 3 , we show the effect of different types of negative graphs .", "entities": []}, {"text": "We compare the results on the ExplaGraphs validation set by leveraging Synthetic Structural ( SySt ) , Synthetic Semantic ( SySe ) and Human - created Semantic ( HuSe ) graphs with the Max - Margin graph generation model .", "entities": [[36, 38, "TaskName", "graph generation"]]}, {"text": "All types of negatives graphs lead to consistent increase in SeCA .", "entities": []}, {"text": "Leveraging human - created negative graphs leads to a bigger gain in SeCA because of the hardBelief :   Collectivism is terrible for society .", "entities": []}, {"text": "Argument :   Collectivism increases empathy .", "entities": []}, {"text": "Stance :   Counter Increases empathyCollectivism capable of causes Improve human relationship Terrible for societyis not a Increases empathyCollectivism capable of Terrible for societyis not aEmpathy Societynot part ofIncreases empathyCollectivism capable of Terrible for societyis not aGroupthinksynonym ofIncreases empathyCollectivism capable of Good Thingis a Terrible for societynot has    context Gold Graph T5 - generated Graph    Structurally and Semantically IncorrectMax - Margin GraphContrastive GraphFigure 3 : Qualitative analysis of explanation graphs .", "entities": [[52, 53, "MethodName", "T5"]]}, {"text": "ness and diversity in these graphs and hence are the best candidates for contrastive learning .", "entities": [[13, 15, "MethodName", "contrastive learning"]]}, {"text": "6.4 Generalization to Other Graph Generation Tasks We test the generalizability of constructing structurally and semantically perturbed graphs for contrastive learning by also experimenting on a temporal graph generation task ( Madaan and Yang , 2021 ) that requires constructing a temporal graph from a document .", "entities": [[4, 6, "TaskName", "Graph Generation"], [19, 21, "MethodName", "contrastive learning"], [27, 29, "TaskName", "graph generation"]]}, {"text": "The nodes in the graph are events from the document and the edges are temporal relations between events ( \u201c before \u201d , \u201c after \u201d , etc ) .", "entities": []}, {"text": "Following our overall goal of improving graph generation with limited data , we randomly sample 1.3 % of the overall corpus ( \u00189.5k samples ) as the training data such that all graphs are connected DAGs .", "entities": [[6, 8, "TaskName", "graph generation"]]}, {"text": "Similar to ExplaGraphs , we create structurally negative graphs with disconnected and cyclic graphs and semantic negative graphs by perturbating the temporal relations .", "entities": []}, {"text": "E.g. , if an edge relation is \u201c before \u201d , we replace it with \u201c after \u201d .", "entities": []}, {"text": "We construct positive graphs by replacing edges like \u201c A before B \u201d with \u201c B after A \u201d ( more details in Appendix C ) .", "entities": []}, {"text": "In Table 4 , we report structural correctness accuracy ( StCA ) ( percentage of connected DAGs ) and Graph - BertScore ( G - BS ) for measuring approximate semantic correctness wrt gold graphs .", "entities": [[8, 9, "MetricName", "accuracy"]]}, {"text": "We observe that our contrastive model not only generates more valid graph encodings but also improves StCA by 8 % and G - BS by 3%.1197", "entities": []}, {"text": "StCA \" SeCA \" G - BS \" GED # EA \" SySt + SySe + HuSe 49.5 38.4 39.4 0.64 26.1 SySt + SySe + HuSe + HuSe - Gen ( IP ) 53.5 38.7 42.1 0.62 28.1 SySt + SySe + HuSe + HuSe - Gen ( AE ) 52.0 40.2 41.3 0.62 28.2 Table 5 : Effect of training the Max - Margin model with additional Human - like Semantic Negative Graphs on ExplaGraphs dev set .", "entities": [[8, 9, "DatasetName", "GED"], [49, 50, "MethodName", "AE"]]}, {"text": "IP and AE refer to the two thresholding techniques for \ufb01ltering generated negatives .", "entities": [[2, 3, "MethodName", "AE"]]}, {"text": "6.5 Analysis of Generated Graphs Fig .", "entities": []}, {"text": "3 shows an example of the graphs generated by different models ( more examples in Appendix F ) .", "entities": []}, {"text": "Unlike T5 , our models \u2019 graphs are both structurally and semantically correct with diverse commonsense nodes ( \u201c Groupthink \u201d , \u201c Good Thing \u201d ) .", "entities": [[1, 2, "MethodName", "T5"]]}, {"text": "While our models generate more correct graphs , they lack in structural diversity \u2013 the Contrastive model generates 77 % of linear graphs ( i.e. , the nodes are in a linear chain ) which is comparable to 75 % in the T5 model .", "entities": [[42, 43, "MethodName", "T5"]]}, {"text": "This can be attributed to our structurally similar positive graphs as the model does not obtain enough supervision to generate diverse graphs .", "entities": []}, {"text": "Structural diversity is not a measure of graph correctness ; however , like diverse text generation ( Vijayakumar et al . , 2018 ) , generating diverse graphs is an interesting direction for future work .", "entities": [[14, 16, "TaskName", "text generation"]]}, {"text": "6.6 Generating Human - like Semantic Negatives ( HuSe - Gen ) In ExplaGraphs , human - created negatives account for 38 % of the samples for which the initially constructed graph was incorrect and was re\ufb01ned .", "entities": []}, {"text": "Moreover , we see in the previous section that humanerror graphs are the best negative candidates for contrastive learning ( which is intuitive since tricky and subtle errors made by expert human annotators would make for some of the hardest negatives / distractors for a contrastive learning model to learn from ) .", "entities": [[17, 19, "MethodName", "contrastive learning"], [45, 47, "MethodName", "contrastive learning"]]}, {"text": "Hence , in this \ufb01nal section , we further explore whether it is also possible to automatically imitate and generate more of such harder humanlikeincorrect graphs for the remaining samples as well .", "entities": []}, {"text": "Our method consists of the following steps .", "entities": []}, {"text": "Human - like Negative Edge Generation .", "entities": []}, {"text": "We \ufb01rst \ufb01ne - tune a T5 model that conditions on the belief , argument and the stance to generate a set of incorrect edges ( which is the set of edges that are present in the incorrect graph and not in the re\ufb01ned graph ) .", "entities": [[6, 7, "MethodName", "T5"]]}, {"text": "Human - like Negative Graph Construction .", "entities": [[4, 6, "TaskName", "Graph Construction"]]}, {"text": "This generated set of incorrect edges is then added to the correct graph to construct the incorrect graph , such that it is structurally correct and hence representative of human - like erroneous graphs .", "entities": []}, {"text": "Filtering High - quality Negative Graphs .", "entities": []}, {"text": "Con - trastive models will only bene\ufb01t from these negatives if the negative edge generation model is accurate and generates edges that are actually incorrect .", "entities": []}, {"text": "Hence , we control the quality of the generated incorrect graphs by the following two techniques \u2013 ( a)Thresholding via fraction of Acceptable Edges ( AE ): We say that a generated incorrect edge is acceptable if it is not part of the correct graph and can be added to the correct graph without violating any structural constraints .", "entities": [[25, 26, "MethodName", "AE"]]}, {"text": "We compute the fraction of acceptable edges for every generated negative graph and choose only those graphs with AE above a certain threshold \u000e. Intuitively , this ensures that a high fraction of the generated edges are actually incorrect and hence when added to the correct graph , will lead to a suf\ufb01ciently different ( human - like ) incorrect graph .", "entities": [[18, 19, "MethodName", "AE"]]}, {"text": "( b ) Thresholding via Incorrect Probability of a graph ( IP ): We use our SeCA metric model ( that classi\ufb01es a graph into support , counter , or incorrect class ) to compute the probability of the generated graph being incorrect and choose those graphs that are above a certain threshold", "entities": []}, {"text": "of incorrect probability .", "entities": []}, {"text": "We set\u000e= 0:4and", "entities": []}, {"text": "= 0:5(tuned on the dev set ) and train the Max - margin model using these additionally generated human - like negative graphs .", "entities": []}, {"text": "As shown in Table 5 both thresholding approaches lead to further improvements over using just the human - created negative graphs .", "entities": []}, {"text": "These initial promising results for emulating hard / tricky human errors as strong negatives for contrastive learning will hopefully lead to further future work in this interesting direction .", "entities": [[15, 17, "MethodName", "contrastive learning"]]}, {"text": "7 Conclusion We presented an empirical study of graph structure and semantics for end - to - end explanation graph generation from pre - trained language models and showed that the generated graphs often violate structural constraints or are semantically incorrect .", "entities": [[19, 21, "TaskName", "graph generation"]]}, {"text": "We signi\ufb01cantly improve both the structural and semantic accuracy of graph generation by proposing contrastive learning models that leverage simple yet ef\ufb01cient methods of graph perturbations and also generalize to similar graph generation tasks.1198", "entities": [[8, 9, "MetricName", "accuracy"], [10, 12, "TaskName", "graph generation"], [14, 16, "MethodName", "contrastive learning"], [31, 33, "TaskName", "graph generation"]]}, {"text": "Ethical Considerations From an ethics standpoint , we provide a brief overview and show samples from the datasets that our models are trained on throughout the paper and also in the Appendix .", "entities": []}, {"text": "Explanation graph generation improves the interpretability of neural commonsense reasoning systems and could prove to be effective in understanding and debugging such models .", "entities": [[1, 3, "TaskName", "graph generation"]]}, {"text": "Hence we do not foresee any major risks or negative societal impact of our work .", "entities": []}, {"text": "However , like any other ML model , the graphs generated by our models may not always be completely accurate and hence should be used with caution for real - world applications .", "entities": []}, {"text": "Acknowledgements We thank the reviewers for their helpful feedback and the annotators for their time and effort .", "entities": []}, {"text": "This work was supported by DARPA MCS Grant N66001 - 19 - 2 - 4031 , NSF - CAREER Award 1846185 , DARPA YFA17 - D17AP00022 , ONR Grant N00014 - 18 - 1 - 2871 , Microsoft Investigator Fellowship , and Munroe & Rebecca Cobey Fellowship .", "entities": [[5, 6, "DatasetName", "DARPA"], [22, 23, "DatasetName", "DARPA"]]}, {"text": "The views in this article are those of the authors and not the funding agency .", "entities": []}, {"text": "References Yonatan Bisk , Rowan Zellers , Jianfeng Gao , Yejin Choi , et al . 2020 .", "entities": []}, {"text": "Piqa : Reasoning about physical commonsense in natural language .", "entities": [[0, 1, "DatasetName", "Piqa"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 34 , pages 7432\u20137439 .", "entities": []}, {"text": "Antoine Bosselut , Hannah Rashkin , Maarten Sap , Chaitanya Malaviya , Asli Celikyilmaz , and Yejin Choi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Comet : Commonsense transformers for automatic knowledge graph construction .", "entities": [[7, 9, "TaskName", "graph construction"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4762\u20134779 .", "entities": []}, {"text": "Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . 2020 .", "entities": []}, {"text": "Language models are few - shot learners .", "entities": []}, {"text": "Advances in neural information processing systems , 33:1877\u20131901 .", "entities": []}, {"text": "Shuyang Cao and Lu Wang .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "CLIFF : Contrastive learning for improving faithfulness and factuality in abstractive summarization .", "entities": [[2, 4, "MethodName", "Contrastive learning"], [11, 12, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6633\u20136649 , Online and Punta Cana , Dominican Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nathanael Chambers and Dan Jurafsky .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Unsupervised learning of narrative schemas and their participants .", "entities": []}, {"text": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 602\u2013610 .", "entities": []}, {"text": "Jiaao Chen , Derek Tam , Colin Raffel , Mohit Bansal , and Diyi Yang . 2021 .", "entities": []}, {"text": "An empirical survey of data augmentation for limited data learning in nlp .", "entities": [[4, 6, "TaskName", "data augmentation"]]}, {"text": "arXiv preprint arXiv:2106.07499 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ting Chen , Simon Kornblith , Mohammad Norouzi , and Geoffrey Hinton .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "A simple framework for contrastive learning of visual representations .", "entities": [[4, 6, "MethodName", "contrastive learning"]]}, {"text": "InInternational conference on machine learning , pages 1597\u20131607 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Xilun Chen , Asish Ghoshal , Yashar Mehdad , Luke Zettlemoyer , and Sonal Gupta . 2020b .", "entities": []}, {"text": "Lowresource domain adaptation for compositional taskoriented semantic parsing .", "entities": [[1, 3, "TaskName", "domain adaptation"], [6, 8, "TaskName", "semantic parsing"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 5090\u20135100 .", "entities": []}, {"text": "Sumit Chopra , Raia Hadsell , and Yann LeCun . 2005 .", "entities": []}, {"text": "Learning a similarity metric discriminatively , with application to face veri\ufb01cation .", "entities": []}, {"text": "In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ( CVPR\u201905 ) , volume 1 , pages 539\u2013546 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Bhavana Dalvi , Peter Jansen , Oyvind Tafjord , Zhengnan Xie , Hannah Smith , Leighanna Pipatanangkura , and Peter Clark . 2021 .", "entities": []}, {"text": "Explaining answers with entailment trees .", "entities": []}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 7358\u20137370 .", "entities": []}, {"text": "Bradley Efron and Robert J Tibshirani .", "entities": []}, {"text": "1994 .", "entities": []}, {"text": "An introduction to the bootstrap .", "entities": []}, {"text": "CRC press .", "entities": [[0, 1, "DatasetName", "CRC"]]}, {"text": "Yanai Elazar , Hongming Zhang , Yoav Goldberg , and Dan Roth . 2021 .", "entities": []}, {"text": "Back to square one : Artifact detection , training and commonsense disentanglement in the winograd schema .", "entities": [[11, 12, "TaskName", "disentanglement"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 10486\u201310500 .", "entities": []}, {"text": "Hongchao Fang , Sicheng Wang , Meng Zhou , Jiayuan Ding , and Pengtao Xie . 2020 .", "entities": []}, {"text": "Cert : Contrastive self - supervised learning for language understanding .", "entities": [[3, 7, "TaskName", "self - supervised learning"]]}, {"text": "arXiv preprint arXiv:2005.12766 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Steven Y Feng , Varun Gangal , Jason Wei , Sarath Chandar , Soroush V osoughi , Teruko Mitamura , and Eduard Hovy . 2021 .", "entities": []}, {"text": "A survey of data augmentation approaches for nlp .", "entities": [[3, 5, "TaskName", "data augmentation"]]}, {"text": "arXiv preprint arXiv:2105.03075 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tianyu Gao , Xingcheng Yao , and Danqi Chen . 2021 .", "entities": []}, {"text": "SimCSE : Simple contrastive learning of sentence embeddings .", "entities": [[0, 1, "MethodName", "SimCSE"], [3, 5, "MethodName", "contrastive learning"], [6, 8, "TaskName", "sentence embeddings"]]}, {"text": "In Empirical Methods in Natural Language Processing ( EMNLP ) .1199", "entities": []}, {"text": "Aditya Grover , Aaron Zweig , and Stefano Ermon .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Graphite : Iterative generative modeling of graphs .", "entities": []}, {"text": "In International conference on machine learning , pages 2434\u20132444 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Nicola Guarino and Pierdaniele Giaretta .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Ontologies and knowledge bases .", "entities": []}, {"text": "Towards very large knowledge bases , pages 1\u20132 .", "entities": []}, {"text": "Beliz Gunel , Jingfei Du , Alexis Conneau , and Veselin Stoyanov .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Supervised contrastive learning for pre - trained language model \ufb01ne - tuning .", "entities": [[1, 3, "MethodName", "contrastive learning"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Michael Gutmann and Aapo Hyv\u00e4rinen .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Noisecontrastive estimation :", "entities": []}, {"text": "A new estimation principle for unnormalized statistical models .", "entities": []}, {"text": "In Proceedings of the thirteenth international conference on arti\ufb01cial intelligence and statistics , pages 297\u2013304 .", "entities": []}, {"text": "JMLR Workshop and Conference Proceedings .", "entities": []}, {"text": "Raia Hadsell , Sumit Chopra , and Yann LeCun .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Dimensionality reduction by learning an invariant mapping .", "entities": [[0, 2, "TaskName", "Dimensionality reduction"]]}, {"text": "In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ( CVPR\u201906 ) , volume 2 , pages 1735\u20131742 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Peter Hase , Shiyue Zhang , Harry Xie , and Mohit Bansal .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Leakage - adjusted simulatability : Can models generate non - trivial explanations of their behavior in natural language ?", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : Findings , pages 4351\u20134367 .", "entities": []}, {"text": "Kaveh Hassani and Amir Hosein Khasahmadi .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Contrastive multi - view representation learning on graphs .", "entities": [[4, 6, "TaskName", "representation learning"]]}, {"text": "In International Conference on Machine Learning , pages 4116\u20134126 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Kaiming He , Haoqi Fan , Yuxin Wu , Saining Xie , and Ross Girshick .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Momentum contrast for unsupervised visual representation learning .", "entities": [[0, 2, "MethodName", "Momentum contrast"], [5, 7, "TaskName", "representation learning"]]}, {"text": "In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 9729\u20139738 .", "entities": []}, {"text": "Michael A Hedderich , Lukas Lange , Heike Adel , Jannik Str\u00f6tgen , and Dietrich Klakow .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A survey on recent approaches for natural language processing in low - resource scenarios .", "entities": []}, {"text": "arXiv preprint arXiv:2010.12309 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "R Devon Hjelm , Alex Fedorov , Samuel LavoieMarchildon , Karan Grewal , Phil Bachman , Adam Trischler , and Yoshua Bengio .", "entities": [[16, 17, "MethodName", "Adam"]]}, {"text": "2018 .", "entities": []}, {"text": "Learning deep representations by mutual information estimation and maximization .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Elad Hoffer and Nir Ailon .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Deep metric learning using triplet network .", "entities": [[1, 3, "TaskName", "metric learning"]]}, {"text": "In International workshop on similarity - based pattern recognition , pages 84 \u2013 92 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Jena D Hwang , Chandra Bhagavatula , Ronan Le Bras , Jeff Da , Keisuke Sakaguchi , Antoine Bosselut , andYejin Choi . 2021 .", "entities": []}, {"text": "( comet- ) atomic 2020 :", "entities": []}, {"text": "On symbolic and neural commonsense knowledge graphs .", "entities": [[5, 7, "TaskName", "knowledge graphs"]]}, {"text": "InProceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 7 , pages 6384\u20136392 .", "entities": []}, {"text": "Prannay Khosla , Piotr Teterwak , Chen Wang , Aaron Sarna , Yonglong Tian , Phillip Isola , Aaron Maschinot , Ce Liu , and Dilip Krishnan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Supervised contrastive learning .", "entities": [[1, 3, "MethodName", "contrastive learning"]]}, {"text": "Advances in Neural Information Processing Systems , 33 .", "entities": []}, {"text": "Oleksandr Kolomiyets , Steven Bethard , and MarieFrancine Moens . 2011 .", "entities": []}, {"text": "Model - portability experiments for textual temporal analysis .", "entities": []}, {"text": "In Proceedings of the 49th annual meeting of the association for computational linguistics : human language technologies , volume 2 , pages 271\u2013276 .", "entities": []}, {"text": "ACL ; East Stroudsburg , PA .", "entities": []}, {"text": "Dan Kondratyuk and Milan Straka . 2019 .", "entities": []}, {"text": "75 languages , 1 model : Parsing universal dependencies universally .", "entities": [[7, 9, "DatasetName", "universal dependencies"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 2779\u20132795 .", "entities": []}, {"text": "Eleftherios Koutso\ufb01os and Stephen C North .", "entities": []}, {"text": "1996 .", "entities": []}, {"text": "Drawing graphs with dot .", "entities": []}, {"text": "Seanie Lee , Dong Bok Lee , and Sung Ju Hwang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Contrastive learning with adversarial perturbations for conditional text generation .", "entities": [[0, 2, "MethodName", "Contrastive learning"], [6, 9, "TaskName", "conditional text generation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Bart : Denoising sequence - to - sequence pretraining for natural language generation , translation , and comprehension .", "entities": [[2, 3, "TaskName", "Denoising"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871\u20137880 .", "entities": []}, {"text": "Renjie Liao , Yujia Li , Yang Song , Shenlong Wang , Charlie Nash , William L. Hamilton , David Duvenaud , Raquel Urtasun , and Richard Zemel .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Ef\ufb01cient graph generation with graph recurrent attention networks .", "entities": [[1, 3, "TaskName", "graph generation"]]}, {"text": "In NeurIPS .", "entities": []}, {"text": "Bill Yuchen Lin , Wangchunshu Zhou , Ming Shen , Pei Zhou , Chandra Bhagavatula , Yejin Choi , and Xiang Ren . 2020 .", "entities": []}, {"text": "CommonGen :", "entities": [[0, 1, "DatasetName", "CommonGen"]]}, {"text": "A constrained text generation challenge for generative commonsense reasoning .", "entities": [[2, 4, "TaskName", "text generation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : Findings , pages 1823\u20131840 .", "entities": []}, {"text": "Hugo Liu and Push Singh .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Conceptnet \u2014 a practical commonsense reasoning tool - kit .", "entities": [[0, 1, "DatasetName", "Conceptnet"]]}, {"text": "BT technology journal , 22(4):211\u2013226 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "RoBERTa : A robustly optimized bert pretraining approach .", "entities": [[0, 1, "MethodName", "RoBERTa"]]}, {"text": "arXiv preprint arXiv:1907.11692 .1200", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yixin Liu and Pengfei Liu . 2021 .", "entities": []}, {"text": "Simcls : A simple framework for contrastive learning of abstractive summarization .", "entities": [[6, 8, "MethodName", "contrastive learning"], [10, 11, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 1065\u20131072 .", "entities": []}, {"text": "Nicholas Lourie , Ronan Le Bras , Chandra Bhagavatula , and Yejin Choi . 2021 .", "entities": []}, {"text": "Unicorn on rainbow : A universal commonsense reasoning model on a new multitask benchmark .", "entities": []}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 15 , pages 13480 \u2013 13488 .", "entities": []}, {"text": "Aman Madaan , Dheeraj Rajagopal , Yiming Yang , Abhilasha Ravichander , Eduard Hovy , and Shrimai Prabhumoye .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Eigen : Event in\ufb02uence generation using pre - trained language models .", "entities": []}, {"text": "arXiv preprint arXiv:2010.11764 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Aman Madaan and Yiming Yang .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Neural language modeling for contextualized temporal graph generation .", "entities": [[6, 8, "TaskName", "graph generation"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 864\u2013881 .", "entities": []}, {"text": "Tom\u00e1s Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "In 1st International Conference on Learning Representations , ICLR 2013 , Scottsdale , Arizona , USA , May 2 - 4 , 2013 , Workshop Track Proceedings .", "entities": []}, {"text": "George A Miller .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Wordnet : a lexical database for english .", "entities": []}, {"text": "Communications of the ACM , 38(11):39 \u2013 41 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Takeru Miyato , Andrew M Dai , and Ian Goodfellow .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Adversarial training methods for semi - supervised text classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1605.07725 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Alireza Mohammadshahi and James Henderson .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Recursive non - autoregressive graph - to - graph transformer for dependency parsing with iterative re\ufb01nement .", "entities": [[8, 10, "MethodName", "graph transformer"], [11, 13, "TaskName", "dependency parsing"]]}, {"text": "Transactions of the Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , Ilya Sutskever , et al . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "OpenAI blog , 1(8):9 .", "entities": []}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J. Liu . 2020 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the limits of transfer learning with a uni\ufb01ed text - totext transformer .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "Journal of Machine Learning Research , 21(140):1\u201367 .", "entities": []}, {"text": "Nazneen Fatema Rajani , Bryan McCann , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Explain yourself!leveraging language models for commonsense reasoning .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4932\u20134942 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Swarnadeep Saha , Sayan Ghosh , Shashank Srivastava , and Mohit Bansal .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "PRover :", "entities": []}, {"text": "Proof generation for interpretable reasoning over rules .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 122\u2013136 .", "entities": []}, {"text": "Swarnadeep Saha , Prateek Yadav , and Mohit Bansal .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "multiPRover : Generating multiple proofs for improved interpretability in rule reasoning .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 3662\u20133677 .", "entities": []}, {"text": "Swarnadeep Saha , Prateek Yadav , Lisa Bauer , and Mohit Bansal .", "entities": []}, {"text": "2021b .", "entities": []}, {"text": "ExplaGraphs : An explanation graph generation task for structured commonsense reasoning .", "entities": [[4, 6, "TaskName", "graph generation"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 7716\u20137740 .", "entities": []}, {"text": "Keisuke Sakaguchi , Chandra Bhagavatula , Ronan Le Bras , Niket Tandon , Peter Clark , and Yejin Choi . 2021 .", "entities": []}, {"text": "proScript : Partially ordered scripts generation via pre - trained language models .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP .", "entities": []}, {"text": "Keisuke Sakaguchi , Ronan Le Bras , Chandra Bhagavatula , and Yejin Choi . 2020 .", "entities": []}, {"text": "Winogrande :", "entities": [[0, 1, "DatasetName", "Winogrande"]]}, {"text": "An adversarial winograd schema challenge at scale .", "entities": [[2, 5, "DatasetName", "winograd schema challenge"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 34 , pages 8732\u20138740 .", "entities": []}, {"text": "Maarten Sap , Hannah Rashkin , Derek Chen , Ronan LeBras , and Yejin Choi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Socialiqa : Commonsense reasoning about social interactions .", "entities": []}, {"text": "In Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "Dinghan Shen , Mingzhi Zheng , Yelong Shen , Yanru Qu , and Weizhu Chen . 2020 .", "entities": []}, {"text": "A simple but toughto - beat data augmentation approach for natural language understanding and generation .", "entities": [[6, 8, "TaskName", "data augmentation"], [10, 13, "TaskName", "natural language understanding"]]}, {"text": "arXiv preprint arXiv:2009.13818 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Chence Shi * , Minkai Xu * , Zhaocheng Zhu , Weinan Zhang , Ming Zhang , and Jian Tang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Graphaf : a \ufb02ow - based autoregressive model for molecular graph generation .", "entities": [[9, 12, "TaskName", "molecular graph generation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Richard Shin , Christopher H Lin , Sam Thomson , Charles Chen , Subhro Roy , Emmanouil Antonios Platanios , Adam Pauls , Dan Klein , Jason Eisner , and Benjamin Van Durme . 2021 .", "entities": [[20, 21, "MethodName", "Adam"]]}, {"text": "Constrained language models yield few - shot semantic parsers .", "entities": []}, {"text": "arXiv preprint arXiv:2104.08768 .1201", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Martin Simonovsky and Nikos Komodakis .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graphvae : Towards generation of small graphs using variational autoencoders .", "entities": [[9, 10, "MethodName", "autoencoders"]]}, {"text": "In International conference on arti\ufb01cial neural networks , pages 412\u2013422 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Oyvind Tafjord , Bhavana Dalvi , and Peter Clark . 2021 .", "entities": []}, {"text": "ProofWriter : Generating implications , proofs , and abductive statements over natural language .", "entities": [[0, 1, "DatasetName", "ProofWriter"]]}, {"text": "In Findings of the Association for Computational Linguistics : ACL - IJCNLP 2021 , pages 3621\u20133634 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alon Talmor , Jonathan Herzig , Nicholas Lourie , and Jonathan Berant .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Commonsenseqa : A question answering challenge targeting commonsense knowledge .", "entities": [[0, 1, "DatasetName", "Commonsenseqa"], [3, 5, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4149\u20134158 .", "entities": []}, {"text": "Alon Talmor , Ori Yoran , Ronan Le Bras , Chandra Bhagavatula , Yoav Goldberg , Yejin Choi , and Jonathan Berant . 2021 .", "entities": []}, {"text": "Commonsenseqa 2.0 : Exposing the limits of ai through gami\ufb01cation .", "entities": [[0, 1, "DatasetName", "Commonsenseqa"]]}, {"text": "In NeurIPS 2021 Datasets and Benchmarks Track .", "entities": []}, {"text": "Niket Tandon , Bhavana Dalvi , Keisuke Sakaguchi , Peter Clark , and Antoine Bosselut .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Wiqa : A dataset for \u201c what if ... \u201d reasoning over procedural text .", "entities": [[0, 1, "DatasetName", "Wiqa"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 6076\u20136085 .", "entities": []}, {"text": "A\u00e4ron van den Oord , Yazhe Li , and Oriol Vinyals .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Representation learning with contrastive predictive coding .", "entities": [[0, 2, "TaskName", "Representation learning"], [3, 6, "MethodName", "contrastive predictive coding"]]}, {"text": "ArXiv , abs/1807.03748 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Ashwin K Vijayakumar , Michael Cogswell , Ramprasath R Selvaraju , Qing Sun , Stefan Lee , David Crandall , and Dhruv Batra .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Diverse beam search : Decoding diverse solutions from neural sequence models .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "William Yang Wang and Diyi Yang .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "That \u2019s so annoying ! ! ! :", "entities": []}, {"text": "A lexical and frame - semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using # petpeeve tweets .", "entities": [[8, 10, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 2557\u20132563 .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 38\u201345 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jiaxuan You , Rex Ying , Xiang Ren , William Hamilton , and Jure Leskovec .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graphrnn : Generating realistic graphs with deep auto - regressive models .", "entities": []}, {"text": "In International conference on machine learning , pages 5708\u20135717 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Yuning You , Tianlong Chen , Yongduo Sui , Ting Chen , Zhangyang Wang , and Yang Shen . 2020 .", "entities": []}, {"text": "Graph contrastive learning with augmentations .", "entities": [[1, 3, "MethodName", "contrastive learning"]]}, {"text": "Advances in Neural Information Processing Systems , 33:5812 \u2013 5823 .", "entities": []}, {"text": "Rowan Zellers , Yonatan Bisk , Roy Schwartz , and Yejin Choi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Swag : A large - scale adversarial dataset for grounded commonsense inference .", "entities": [[0, 1, "DatasetName", "Swag"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 93\u2013104 .", "entities": []}, {"text": "Dejiao Zhang , Feng Nan , Xiaokai Wei , Shang - Wen Li , Henghui Zhu , Kathleen McKeown , Ramesh Nallapati , Andrew O Arnold , and Bing Xiang . 2021 .", "entities": []}, {"text": "Supporting clustering with contrastive learning .", "entities": [[0, 5, "MethodName", "Supporting clustering with contrastive learning"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 5419\u20135430 .", "entities": []}, {"text": "Tianyi Zhang * , Varsha Kishore * , Felix Wu * , Kilian Q. Weinberger , and Yoav Artzi .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Bertscore : Evaluating text generation with bert .", "entities": [[3, 5, "TaskName", "text generation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Junru Zhou , Zhuosheng Zhang , Hai Zhao , and Shuailiang Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "LIMIT - BERT : Linguistics informed multi - task BERT .", "entities": [[2, 3, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 4450\u20134461 .", "entities": []}, {"text": "Yanqiao Zhu , Yichen Xu , Feng Yu , Qiang Liu , Shu Wu , and Liang Wang .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Graph contrastive learning with adaptive augmentation .", "entities": [[1, 3, "MethodName", "contrastive learning"]]}, {"text": "In Proceedings of the Web Conference 2021 , pages 2069\u20132080 .", "entities": []}, {"text": "A Evaluation Metrics for ExplaGraphs Below we provide brief descriptions of the evaluation metrics used for the ExplaGraphs task .", "entities": []}, {"text": "For further details , we refer readers to prior work ( Saha et al . , 2021b ) .", "entities": []}, {"text": "Structural Correctness Accuracy of Graphs ( StCA ) .", "entities": [[2, 3, "MetricName", "Accuracy"]]}, {"text": "It computes the fraction of graphs where all the structural constraints are satis\ufb01ed .", "entities": []}, {"text": "Semantic Correctness Accuracy of Graphs ( SeCA ) .", "entities": [[2, 3, "MetricName", "Accuracy"]]}, {"text": "SeCA is a model - based metric that computes the fraction of graphs that are both structurally and semantically correct .", "entities": []}, {"text": "For computing SeCA , prior work trains a 3 - way RoBERTa ( Liu1202", "entities": [[11, 12, "MethodName", "RoBERTa"]]}, {"text": "SySt SySe HuSe Total 7522 2368 1336 11226 Table 6 : Count of negative graphs in each category .", "entities": []}, {"text": "et al . , 2019 ) classi\ufb01er that given a belief and a generated explanation graph , infers whether the graph supports the belief , counters the belief or is incorrect ( because of incoherent edges ) .", "entities": []}, {"text": "If it predicts support or counter and this stance matches the gold stance , then the graph is considered semantically correct .", "entities": []}, {"text": "In essense , SeCA works on the principle that an explanation graph is semantically correct if a stance can be unambiguously inferred from it ( by a model in this case or a human ) and that stance is the same as the gold stance .", "entities": []}, {"text": "Note that SeCA is a reference - free metric ( does not use the groundtruth graph ) and hence is invariant to structural variations in explanation graphs .", "entities": []}, {"text": "Graph - BertScore ( G - BS ) .", "entities": []}, {"text": "Graph - BertScore is an extension of BertScore ( Zhang * et al . , 2020 ) for computing the degree of match between the predicted graphs and the ground - truth graphs .", "entities": []}, {"text": "It treats a graph as a set of edges and computes the best match between the gold edges and the predicted edges , where the matching score between a pair of edges is given by the BertScore F1 .", "entities": [[37, 38, "MetricName", "F1"]]}, {"text": "Graph Edit Distance ( GED ) .", "entities": [[4, 5, "DatasetName", "GED"]]}, {"text": "GED is the standard Graph Edit Distance for graphs , measuring the number of edit operations ( addition , deletion , and replacement of nodes and edges ) to transform one graph to the other and further normalized by an appropriate normalizing constant .", "entities": [[0, 1, "DatasetName", "GED"]]}, {"text": "Edge Accuracy ( EA ) .", "entities": [[1, 2, "MetricName", "Accuracy"]]}, {"text": "The \ufb01nal metric , Edge Accuracy ( EA ) measures the fraction of edges in the graph that are important .", "entities": [[5, 6, "MetricName", "Accuracy"]]}, {"text": "An edge is considered important if removing it from the graph leads to a drop in the gold stance prediction con\ufb01dence .", "entities": []}, {"text": "B Statistics of Graph Perturbations We create a total of 11k negative graphs .", "entities": []}, {"text": "Table 6 shows the respective counts of the negative graphs belonging to synthetic structural ( SySt ) , synthetic semantic ( SySe ) and human - created semantic ( HuSe ) categories .", "entities": []}, {"text": "C Temporal Graph Generation", "entities": [[2, 4, "TaskName", "Graph Generation"]]}, {"text": "The task of temporal graph generation requires constructing a temporal graph from a document ( see Fig . 4 ) .", "entities": [[4, 6, "TaskName", "graph generation"]]}, {"text": "The nodes in the graph are events from theDataset Train Dev Test ExplaGraphs 2368 398 400 Temporal ( Sampled ) 9531 953 949 Table 7 : Train , validation and test split sizes of the two datasets .", "entities": []}, {"text": "For Temporal Graph Generation , we randomly sample 1.3 % of the overall corpus ( Madaan and Yang , 2021 ) .", "entities": [[2, 4, "TaskName", "Graph Generation"]]}, {"text": "document ( e.g. , \u201c Markovic jailed \u201d or \u201c Covering up attempted murder \u201d ) and the edges are temporal relations between the events ( e.g. , \u201c Markovic jailed ; before ; Covering up attempted murder \u201d ) .", "entities": []}, {"text": "The authors consider \ufb01ve temporal relations ( \u201c before \u201d , \u201c after \u201d , \u201c simultaneous \u201d , \u201c is included \u201d and \u201c includes \u201d ) and build an automatically constructed large - scale dataset for the task .", "entities": []}, {"text": "Following our overall goal of improving graph generation in limited data settings , we randomly sample 1.3 % of the overall corpus ( \u00189.5k samples ) as the training corpus such that all graphs are connected DAGs.5Following Madaan and Yang ( 2021 ) , we represent graphs in DOT format ( Koutso\ufb01os and North , 1996 ) as shown in Fig .", "entities": [[6, 8, "TaskName", "graph generation"]]}, {"text": "4 .", "entities": []}, {"text": "We \ufb01nd that the speci\ufb01cs of the graph representations do not matter much , as long as all the edges are concatenated in one particular ordering ( either DFS , BFS or Topological order ) .", "entities": []}, {"text": "We construct semantic negative graphs by randomly sampling a fraction of the edges and performing the following operations .", "entities": []}, {"text": "If an edge relation is one of \u201c before \u201d , \u201c after \u201d or \u201c simulatenous \u201d , we replace it with any other relation from this set and if the relation is one of \u201c is included \u201d or \u201c includes \u201d we replace it with the other relation .", "entities": []}, {"text": "Note that these perturbations will always lead to incorrect graphs because \u201c A before B \u201d implies that \u201c A after B \u201d or \u201c A simultaneous B \u201d do not hold .", "entities": []}, {"text": "Finally , we construct positive graphs by randomly sampling a fraction of edges and replacing them using the following rules : ( 1 ) \u201c A before B \u201d with \u201c B after A \u201d and viseversa , ( 2 ) \u201c A simultaneous B \u201d with \u201c B simultaneous A \u201d , ( 3 ) \u201c A includes B \u201d with \u201c B is included A \u201d .", "entities": []}, {"text": "Note that all these operations preserve the temporal meaning of the graph and are done in a way such that the perturbed graph continues to be a connected DAG .", "entities": []}, {"text": "5Since the dataset was constructed automatically , we found about 10 % of the graphs to be disconnected or cyclic.1203", "entities": []}, {"text": "SA\"StCA \" SeCA \" G - BS \" GED # EA \" T5 - Base 86.2 35.4 15.5 27.7 0.75 19.8 T5 - Large 86.2 46.5 31.6 36.8 0.66 26.8 Generate & Re\ufb01ne 86.2", "entities": [[8, 9, "DatasetName", "GED"], [12, 13, "MethodName", "T5"], [21, 22, "MethodName", "T5"]]}, {"text": "46.8 34.4 37.2 0.66 27.2 Pos Data Aug 86.2 50.0 37.6 39.6 0.64 28.4 Max - margin 86.2 49.5 38.4 39.4 0.64 26.1 Contrastive 86.2 52.7 37.9 41.7 0.62 29.8 Table 8 : Comparison of our models with baseline T5 models across all metrics on ExplaGraphs dev set .", "entities": [[39, 40, "MethodName", "T5"]]}, {"text": "SA\"StCA \" SeCA \" G - BS \" GED # EA \" BART - Base 87.2 25.7 13.0 22.0 0.81 12.8 BART - Large 87.2 34.2 22.2 28.9 0.75 20.0 Contrastive 87.2 40.7 26.3 31.3 0.71 22.3 Table 9 : Effect of Contrastive Learning with BART on ExplaGraphs test set .", "entities": [[8, 9, "DatasetName", "GED"], [12, 13, "MethodName", "BART"], [21, 22, "MethodName", "BART"], [42, 44, "MethodName", "Contrastive Learning"], [45, 46, "MethodName", "BART"]]}, {"text": "D Experimental Setup Table 7 shows the number of train , validation and test samples of the two datasets we experiment with .", "entities": []}, {"text": "We build our models on top of the Hugging Face transformers library ( Wolf et al . , 2020).6All models for the ExplaGraphs dataset7(Saha et al . , 2021b ) are trained with a batch size of 8and an initial learning rate of 3\u000310\u00005for a maximum of 15 epochs .", "entities": [[34, 36, "HyperparameterName", "batch size"], [40, 42, "HyperparameterName", "learning rate"]]}, {"text": "The maximum input and output sequence lengths are both set to 150 .", "entities": []}, {"text": "For the max - margin graph generation model , we set both the hyperparameters \u000b ( mixing ratio ) and \f ( margin ) to 1:0 while for the contrastive graph generation model , we set \u000b to0:1 .", "entities": [[5, 7, "TaskName", "graph generation"], [30, 32, "TaskName", "graph generation"]]}, {"text": "For the temporal graph generation task8(Madaan and Yang , 2021 ) , we train all models with a batch size of 4and an initial learning rate of 3\u000310\u00005for a maximum of 10epochs .", "entities": [[3, 5, "TaskName", "graph generation"], [18, 20, "HyperparameterName", "batch size"], [24, 26, "HyperparameterName", "learning rate"]]}, {"text": "The maximum input and output sequence lengths are set to 512and256respectively .", "entities": []}, {"text": "On this task , the hyperparameters \u000b and \f for the max - margin model are again set to 1:0while for the contrastive graph generation model , we set \u000b to0:2 .", "entities": [[23, 25, "TaskName", "graph generation"]]}, {"text": "Across all models and tasks , graphs are generated using beam search decoding with a beam size of 4 .", "entities": []}, {"text": "The batch size and learning rate are manually tuned in the range f4;8;16gand { 10\u00005 , 2\u000310\u00005,3\u000310\u00005 } respectively and the best models are chosen based on the respective validation set performance .", "entities": [[1, 3, "HyperparameterName", "batch size"], [4, 6, "HyperparameterName", "learning rate"]]}, {"text": "Similarly , the mixing ratio hyperparameter \u000b is manually tuned in the range 6https://github.com/huggingface/ transformers 7https://github.com/swarnaHub/ ExplaGraphs 8https://github.com/madaan/ temporal - graph - genStCA \" SeCA \" G - BS \" GED # EA \" Max - Margin 56.7 43.5 48.6 0.57 30.5 + Atomic 58.2 45.0 49.9 0.56 30.9 Table 10 : Effect of \ufb01ne - tuning with additional commonsense knowledge from Atomic .", "entities": [[30, 31, "DatasetName", "GED"]]}, {"text": "f0:1;0:2;0:5;1:0 g.", "entities": []}, {"text": "The random seed is set to 42 in all our experiments .", "entities": []}, {"text": "The total number of parameters in our models is similar to T5 - Base ( 220 M ) or T5 - Large ( 770 M ) depending on the base architecture .", "entities": [[2, 5, "HyperparameterName", "number of parameters"], [11, 12, "MethodName", "T5"], [19, 20, "MethodName", "T5"]]}, {"text": "All our experiments are executed on a single A100 Nvidia GPU .", "entities": []}, {"text": "Each epoch of the contrastive model has an average runtime of 30 mins for ExplaGraphs and 2.5 hours for Temporal Graph Generation .", "entities": [[20, 22, "TaskName", "Graph Generation"]]}, {"text": "E Results Table 8 shows the results of all models on the ExplaGraphs ( Saha et al . , 2021b ) validation set .", "entities": []}, {"text": "Experiments with BART .", "entities": [[2, 3, "MethodName", "BART"]]}, {"text": "In Table 9 , we show the performance of BART ( Lewis et al . , 2020 ) on ExplaGraphs ( Saha et al . , 2021b ) test set .", "entities": [[9, 10, "MethodName", "BART"]]}, {"text": "Unsurprisingly , a larger BART model obtains a much higher StCA and SeCA compared to BART - Base .", "entities": [[4, 5, "MethodName", "BART"], [15, 16, "MethodName", "BART"]]}, {"text": "However , we \ufb01nd T5 to perform much better on this task .", "entities": [[4, 5, "MethodName", "T5"]]}, {"text": "Applying contrastive learning on top of BART leads to improvements across all metrics , thereby showing our method \u2019s generalizability across different pre - trained language models .", "entities": [[1, 3, "MethodName", "contrastive learning"], [6, 7, "MethodName", "BART"]]}, {"text": "Effect of Additional Commonsense Knowledge .", "entities": []}, {"text": "In Table 10 , we explore the impact of integrating additional commonsense knowledge to our MaxMargin model .", "entities": []}, {"text": "Speci\ufb01cally , we \ufb01rst \ufb01ne - tune a1204", "entities": []}, {"text": "contendedbefore He visited island he leftafter place tookafterhis guns lockedbeforeafter afterTests by police laboratory technicians also found traces of Mr. Starkey 's blood under the grip of the weapon , he said .", "entities": []}, {"text": "The police came into possession of the revolver last July 31 , when Mr. Romeo 's father , Ronald , surrendered it to Nassau police officials , explaining that he '' did not believe that Anthony had the emotional capacity to have guns around the house , '' Mr. Wilutis told the court .", "entities": []}, {"text": "When told of these charges , Mr. Scaring said : '' Why has it taken them so many months to come up with these results ?", "entities": []}, {"text": "Later , however , he said he had been with Mr. Starkey on Fire Island , the documents state .", "entities": []}, {"text": "Mr. Romeo told detectives that he visited Fire Island in October and November , but contended that his guns had been safely locked away in Locust Valley then .", "entities": []}, {"text": "Mr. Scaring said his client '' was not on Fire Island at the time of the murder , which the lawyer said '' took place at least two days after he left . ''", "entities": []}, {"text": "Mr. Romeo is 22 or 23 years old and '' employed , though I do n't know in exactly what capacity , '' Mr. Scaring said . '' node01 : he visited island ; node02 : contended ; node03 : he left ; node04 : his guns locked ; node05 : place took ; node01 node02 before ; node03 node04 before ; node01 node04 after ; node02 node04 after ; node03 node05 after ; node04 node05 beforeDocument DOT representation for GraphTemporal GraphFigure 4 : An example of the Temporal Graph Generation Task ( Madaan et al . , 2020 ) showing the source document , the target temporal graph and the corresponding DOT representation .", "entities": [[89, 91, "TaskName", "Graph Generation"]]}, {"text": "Figure 5 : Interface for human evaluation of commonsense explanation graphs .", "entities": []}, {"text": "T5 model on the facts based on ConceptNet relations from ATOMIC-2020 ( Hwang et", "entities": [[0, 1, "MethodName", "T5"], [7, 8, "DatasetName", "ConceptNet"]]}, {"text": "al . , 2021 ) , a large - scale commonsense knowledge base .", "entities": []}, {"text": "The \ufb01ne - tuning objective is to predict the target concept given the source concept and the relation .", "entities": []}, {"text": "Next , we \ufb01ne - tune this model further on the end - task of graph generation which leads to small improvements in both StCA and SeCA .", "entities": [[15, 17, "TaskName", "graph generation"]]}, {"text": "This suggests that better methods of inducing commonsense knowledge in these models can potentially lead to bigger gains with more semantically coherent graphs .", "entities": []}, {"text": "F Human Evaluation In Fig .", "entities": []}, {"text": "5 , we show the interface for human veri\ufb01cation of commonsense explanation graphs onAmazon Mechanical Turk .", "entities": []}, {"text": "We select crowdworkers who are located in the US with a HIT approval rate higher than 96 % and at least 1000 HITs approved .", "entities": []}, {"text": "Since graph evaluation is a challenging task , we \ufb01rst explain how to read the graphs and also provide clear guidelines for comparing the quality of the two graphs.9 G Examples of Generated Explanation Graphs In Fig .", "entities": []}, {"text": "6 , 7 , 8 and 9", "entities": []}, {"text": ", we show various examples of explanation graphs generated by our models .", "entities": []}, {"text": "In Fig . 6 and 7 , our proposed models improve upon 9The payment for each HIT is 0.25 $ at the rate of 12 - 15 $ per hour.1205", "entities": []}, {"text": "Belief :   Since fast foods are greasy and fattening , banning them would control obesity .", "entities": []}, {"text": "Argument :   McDonalds has salads .", "entities": []}, {"text": "Stance :   Counter mcdonaldsFast Food part of not has    context greasy and fattening Gold Graph T5 - generated Graph     Semantically Incorrect   Max - Margin Graph Contrastive GraphSalads part ofFast Food greasy and fatteningSalads banning them control obesitycapable of has context part of causesSalads healthy fast foodhas context not created byMcdonalds greasy control obesitysynonym of not capable of banningnot desireshas property fast food greasy control obesityhas property not capable of saladscreated by Mcdonaldsat locationFigure 6 : Example of explanation graphs generated by different models .", "entities": [[17, 18, "MethodName", "T5"]]}, {"text": "The baseline T5 - generated graph is semantically incorrect ( incoherent relations marked in dashed red ) while our proposed models generate both structurally and semantically correct graphs .", "entities": [[2, 3, "MethodName", "T5"]]}, {"text": "the incorrect semantic relations from the T5 baseline graphs .", "entities": [[6, 7, "MethodName", "T5"]]}, {"text": "Fig .", "entities": []}, {"text": "8 shows an example where all generated graphs , while different , are correct .", "entities": []}, {"text": "Finally , Fig 9 shows an example where although our proposed models improve the semantic aspect compared to the baseline graph , the generated graphs are disconnected and hence structurally incorrect .", "entities": []}, {"text": "Overall , our quantitative results and human evaluation suggest that there is signi\ufb01cant room for improvement on the task of commonsense explanation graph generation.1206", "entities": []}, {"text": "Belief :   Homeschooling is not great for children .", "entities": []}, {"text": "Argument :   There are plenty of ways for children in homeschooling to socialize .", "entities": []}, {"text": "Stance :   counter Gold Graph Max - Margin GraphsocializeHomeschooling capable ofChildren desire remote learningsynonym of great for childrenis asocializeHomeschooling capable of great for childrenis not a plenty of wayshas contextchildrenHomeschooling used for socializecapable of greatis achildrenHomeschooling used for socializecapable of greatis a Contrastive Graph T5 - generated Graph     Semantically Incorrect   Figure 7 : Example of explanation graphs generated by different models .", "entities": [[45, 46, "MethodName", "T5"]]}, {"text": "The baseline T5 - generated graph is semantically incorrect ( incoherent relations marked in dashed red ) while our proposed models generate both structurally and semantically correct graphs .", "entities": [[2, 3, "MethodName", "T5"]]}, {"text": "Belief :   People can relax on a journey when the autonomous car does the driving , allowing them to arrive refreshed .", "entities": []}, {"text": "Argument :   Driving is exhausting .", "entities": []}, {"text": "Stance :   support Gold Graph T5 - generated Graph   Max - Margin Graphautonomous cars capable ofdriving exhaustingnot capable of   can relaxnot has contextdrivingautonomous cars capable of exhaustingis a relaxationdesires Contrastive Graphpeopleused fordriving exhaustingis a relaxationdesires autonomous carscreated by arriving refreshedcapable ofdrivingautonomous cars capable of exhaustingis a peoplehas context arrive refresheddesires Figure 8 : Example of explanation graphs generated by different models .", "entities": [[6, 7, "MethodName", "T5"]]}, {"text": "All models generate structurally and semantically correct graphs while the individual nodes and edges differ.1207", "entities": []}, {"text": "Belief :   Autonomous cars are more dangerous than man - driven cars .", "entities": []}, {"text": "Argument :   Autonomous cars are not safe for humans .", "entities": []}, {"text": "Stance :   support Gold Graph Max - Margin Graph Structurally Incorrect   autonomous cars desireshumans safenot has     context not safeantonym ofsafe for humansautonomous cars capable of dangerousis a man - driven carssynonym of dangeroushas contextnot safeautonomous cars has context dangeroussynonym of man - driven cars safercapable ofsafe for humansautonomous cars not capable of dangerousis a man - driven cars safercapable of Contrastive Graph Structurally Incorrect   T5 - generated Graph     Semantically Incorrect   Figure 9 : Example of explanation graphs generated by different models .", "entities": [[68, 69, "MethodName", "T5"]]}, {"text": "T5 generates a semantically incorrect graph .", "entities": [[0, 1, "MethodName", "T5"]]}, {"text": "Our models generate graphs , which while contain meaningful edges , are disconnected and hence are structurally incorrect.1208", "entities": []}]