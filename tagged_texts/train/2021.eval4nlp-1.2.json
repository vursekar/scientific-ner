[{"text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 11\u201315 July 5\u201310 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_00211Validating Label Consistency in NER Data Annotation Qingkai Zengy , Mengxia Yuy , Wenhao Yuy , Tianwen Jiangz , Meng Jiangy yUniversity of Notre Dame , Notre Dame , IN , USA zHarbin Institute of Technology , Harbin , Heilongjiang , China yfqzeng , myu2 , wyu1 , mjiang2 g@nd.edu zftwjiangg@ir.hit.edu.cn Abstract Data annotation plays a crucial role in ensuring your named entity recognition ( NER ) projects are trained with the correct information to learn from .", "entities": [[9, 10, "TaskName", "NER"], [66, 69, "TaskName", "named entity recognition"], [70, 71, "TaskName", "NER"]]}, {"text": "Producing the most accurate labels is a challenge due to the complexity involved with annotation .", "entities": []}, {"text": "Label inconsistency between multiple subsets of data annotation ( e.g. , training set and test set , or multiple training subsets ) is an indicator of label mistakes .", "entities": []}, {"text": "In this work , we present an empirical method to explore the relationship between label ( in-)consistency and NER model performance .", "entities": [[18, 19, "TaskName", "NER"]]}, {"text": "It can be used to validate the label consistency ( or catch the inconsistency ) in multiple sets of NER data annotation .", "entities": [[19, 20, "TaskName", "NER"]]}, {"text": "In experiments , our method identi\ufb01ed the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) .", "entities": [[13, 14, "DatasetName", "SCIERC"], [15, 16, "DatasetName", "CoNLL03"]]}, {"text": "It validated the consistency in the corrected version of both datasets .", "entities": []}, {"text": "1 Introduction Named entity recognition ( NER ) is one of the foundations of many downstream tasks such as relation extraction , event detection , and knowledge graph construction .", "entities": [[2, 5, "TaskName", "Named entity recognition"], [6, 7, "TaskName", "NER"], [19, 21, "TaskName", "relation extraction"], [22, 24, "TaskName", "event detection"], [27, 29, "TaskName", "graph construction"]]}, {"text": "NER models require vast amounts of labeled data to learn and identify patterns that humans can not continuously .", "entities": [[0, 1, "TaskName", "NER"]]}, {"text": "It is really about getting accurate data to train the models .", "entities": []}, {"text": "When end - to - end neural models achieve excellent performance on NER in various domains ( Lample et al . , 2016 ; Liu et al . , 2018 ; Luan et al . , 2018 ; Zeng et al . , 2020 , 2021 ) , building useful and challenging NER benchmarks , such as CoNLL03 , WNUT16 , and SCIERC , contributes signi\ufb01cantly to the research community .", "entities": [[12, 13, "TaskName", "NER"], [52, 53, "TaskName", "NER"], [57, 58, "DatasetName", "CoNLL03"], [62, 63, "DatasetName", "SCIERC"]]}, {"text": "Data annotation plays a crucial role in building benchmarks and ensuring NLP models are trained with the correct information to learn from ( Luan et al . , 2018 ; Jiang et al . , 2020 ; Yu et al . , 2020 ) .", "entities": []}, {"text": "Producing the necessary annotation from any asset atscale is a challenge , mainly because of the complexity involved with annotation .", "entities": []}, {"text": "Getting the most accurate labels demands time and expertise .", "entities": []}, {"text": "Label mistakes can hardly be avoided , especially when the labeling process splits the data into multiple sets for distributed annotation .", "entities": []}, {"text": "The mistakes cause label inconsistency between subsets of annotated data ( e.g. , training set and test set or multiple training subsets ) .", "entities": []}, {"text": "For example , in the CoNLL03 dataset ( Sang and De Meulder , 2003 ) , a standard NER benchmark that has been cited over 2,300 times , label mistakes were found in 5.38 % of the test set ( Wang et al . , 2019 ) .", "entities": [[5, 6, "DatasetName", "CoNLL03"], [18, 19, "TaskName", "NER"]]}, {"text": "Note that the stateof - the - art results on CoNLL03 have achieved an F1 score of\u0018:93 .", "entities": [[10, 11, "DatasetName", "CoNLL03"], [14, 16, "MetricName", "F1 score"]]}, {"text": "So even if the label mistakes make up a tiny part , they can not be negligible when researchers are trying to improve the results further .", "entities": []}, {"text": "In the work of Wang et al . , \ufb01ve annotators were recruited to correct the label mistakes .", "entities": []}, {"text": "Compared to the original test set results , the corrected test set results are more accurate and stable .", "entities": []}, {"text": "However , two critical issues were not resolved in this process : i)How to identify label inconsistency between the subsets of annotated data ?", "entities": []}, {"text": "ii)How to validate that the label consistency was recovered by the correction ?", "entities": []}, {"text": "Another example is SCIERC ( Luan et al . , 2018 ) ( cited\u001850 times ) which is a multi - task ( including NER ) benchmark in AI domain .", "entities": [[3, 4, "DatasetName", "SCIERC"], [24, 25, "TaskName", "NER"]]}, {"text": "It has 1,861 sentences for training , 455 for dev , and 551 for test .", "entities": []}, {"text": "When we looked at the false predictions given bySCIIE which was a multi - task model released along with the SCIERC dataset , we found that as many as 147 ( 26.7 % of the test set ) sentences were not properly annotated .", "entities": [[20, 21, "DatasetName", "SCIERC"]]}, {"text": "( We also recruited \ufb01ve annotators and counted a mistake when all the annotators report it . )", "entities": []}, {"text": "Three examples are given in Table 1 : two of them have wrong entity types ; the third has a wrong span boundary .", "entities": []}, {"text": "As shown in the experiments section , after the correction , the NER performance becomes more accurate and stable .", "entities": [[12, 13, "TaskName", "NER"]]}, {"text": "12Table 1 : Three examples to compare original and corrected annotation in the test set of the SCIERC dataset .", "entities": [[17, 18, "DatasetName", "SCIERC"]]}, {"text": "If the annotation on the test set consistently followed the \u201c codebook \u201d that was used to annotate training data , the entities in the \ufb01rst two examples would be labelled as \u201c Task \u201d ( not \u201c Method \u201d ) for sure .", "entities": []}, {"text": "Original Examples Corrected Examples Starting from a DP - based solution to the [ traveling salesman problem ] Method , we present a novel technique ...", "entities": []}, {"text": "Starting from a DP - based solution to the [ traveling salesman problem ] Task , we present a novel technique ...", "entities": []}, {"text": "FERRET utilizes a novel approach to [ Q / A ] Method known as predictive questioning which attempts to identify ... FERRET utilizes a novel approach to [ Q / A ] Task known as predictive questioning which attempts to identify ...", "entities": []}, {"text": "The goal of this work is the enrichment of [ human - machine interactions ] Taskin a natural language environment .", "entities": []}, {"text": "The goal of this work is the [ enrichment of human - machine interactions ] Taskin a natural language environment .", "entities": []}, {"text": "Figure 1 : Identifying label inconsistency of test set with training set : We sample three exclusive subsets ( of size x ) from the training set ( orange , green , and blue ) .", "entities": []}, {"text": "We use one subset as the new test set ( orange ) .", "entities": []}, {"text": "We apply the SCIIE NER model on the new test set .", "entities": [[4, 5, "TaskName", "NER"]]}, {"text": "We build three newtraining sets : i)\u201cTrainTest \u201d ( blue - red ) , ii)\u201cPureTrain \u201d ( green - blue ) , iii)\u201cTestTrain \u201d ( red - blue ) .", "entities": []}, {"text": "Results on SCIERC show that the test set ( red ) is less predictive of training samples ( orange ) than the training set itself ( blue or green ) .", "entities": [[2, 3, "DatasetName", "SCIERC"]]}, {"text": "This was not observed on two other datasets .", "entities": []}, {"text": "Besides the signi\ufb01cant correction on the SCIERC dataset , our contributions in this work are as follows :", "entities": [[6, 7, "DatasetName", "SCIERC"]]}, {"text": "i)an empirical , visual method to identify the label inconsistency between subsets of annotated data ( see Figure 1 ) , ii)a method to validate the label consistency of corrected data annotation ( see Figure 2 ) .", "entities": []}, {"text": "Experiments show that they are effective on the CoNLL03 and SCIERC datasets .", "entities": [[8, 9, "DatasetName", "CoNLL03"], [10, 11, "DatasetName", "SCIERC"]]}, {"text": "2 Proposed Methods 2.1 A method to identify label inconsistency Suppose the labeling processes on two parts of annotated data were consistent .", "entities": []}, {"text": "They are likely to be equivalently predictive of each other .", "entities": []}, {"text": "In other words , if we train a model with a set of samples from either part Aor part Bto predict a different set from part A , the performance should be similar .", "entities": []}, {"text": "Take SCIERC as an example .", "entities": [[1, 2, "DatasetName", "SCIERC"]]}, {"text": "We were wondering whether the labels in the test set were consistent with those in the training set .", "entities": []}, {"text": "Our method to identify the inconsistency is presented in Figure 1 .", "entities": []}, {"text": "We sample three exclusive subsets ( of size x ) from the training set .", "entities": []}, {"text": "We set x= 550 according to the size of the original test set .", "entities": []}, {"text": "We use one of the subsets as the new test set .", "entities": []}, {"text": "Then we train the SCIIE NER model ( Luan et al . , 2018 ) to perform on the new test set .", "entities": [[5, 6, "TaskName", "NER"]]}, {"text": "We build three new training sets to feed into the model : \u000f\u201cTrainTest \u201d : \ufb01rst fed with one training subset and then the original test set ; \u000f\u201cPureTrain \u201d : fed with two training subsets ; \u000f\u201cTestTrain \u201d : \ufb01rst fed with the original test set and then one of the training subsets .", "entities": []}, {"text": "Results show that \u201c TestTrain \u201d performed the worst at the early stage because the quality of the", "entities": []}, {"text": "13 Train : Test:(mistakes / corrected)!\"\"#OriginalTrain : Test:\u201cTestTrainMistake\u201d\u201cTestTrainCorrect\u201dw\u201cPureTrainMistake\u201d\u201cPureTrainCorrect\u201d\u201cMistakeTestTrain\u201d\u201cCorrectTestTrain\u201d\u201cMistakePureTrain\u201d\u201cCorrectPureTrain \u201d \u201c Mistake \u201d \u201c Correct\u201d\"w#!\"w#!#\"%!#\"%!Figure 2 : Validating label consistency in corrected test set : We corrected zofy+zsentences in the test set .", "entities": []}, {"text": "We sampled three exclusive subsets of size x , y , andwfrom the training set .", "entities": []}, {"text": "We use the \ufb01rst subset ( of size x ) as the newtest set .", "entities": []}, {"text": "We build four newtraining sets as shown in the \ufb01gure and feed them into the SCIIE model ( at the top of the \ufb01gure ) .", "entities": []}, {"text": "Results show that the label mistakes ( red parts of the curves on the left ) do hurt the performance no matter fed at the beginning or later ; and the corrected test set performs as well as the training set ( on the right ) .", "entities": []}, {"text": "original test set is not reliable .", "entities": []}, {"text": "In \u201c TrainTest \u201d the performance no longer improved when the model started being fed with the original test set .", "entities": []}, {"text": "\u201c PureTrain \u201d performed the best .", "entities": []}, {"text": "All the observations conclude that the original test set is less predictive of training samples than the training set itself .", "entities": []}, {"text": "It may be due to the issue of label inconsistency .", "entities": []}, {"text": "Moreover , we do not have such observations on two other datasets , WikiGold and WNUT16 .", "entities": []}, {"text": "2.2 A method to validate label consistency after correction After we corrected the label mistakes , how could we empirically validate the recovery of label consistency ?", "entities": []}, {"text": "Again , we use a subset of training data as the new test set .", "entities": []}, {"text": "We evaluate the predictability of the original wrong test subset , the corrected test subset , and the rest of the training set .", "entities": []}, {"text": "We expect to see that the wrong test subset delivers weaker performance and the other two sets make comparable good predictions .", "entities": []}, {"text": "Figure 2 illustrates this idea .", "entities": []}, {"text": "Take SCIERC as an example .", "entities": [[1, 2, "DatasetName", "SCIERC"]]}, {"text": "Suppose we corrected zofy+zsentences in the test set .", "entities": []}, {"text": "The original wrong test subset ( \u201c Mistake \u201d ) and the corrected test subset ( \u201c Correct \u201d ) are both of size z. Here z= 147 and the original good test subset y= 404", "entities": []}, {"text": "( \u201c Test \u201d ) .", "entities": []}, {"text": "We sampled three exclusive subsets of size x , y , andw= 804 from the training set ( \u201c Train \u201d ) .", "entities": []}, {"text": "We use the \ufb01rst subset ( of size x ) as the new test set .", "entities": []}, {"text": "We build four new training sets and feed into the SCIIE model .", "entities": []}, {"text": "Each new training set hasy+w+z= 1;355sentences . \u000f\u201cTestTrainMistake\u201d/\u201cTestTrainCorrect \u201d : the original good test subset , the third sampled training subset , and the original wrong test subset ( or the corrected test subset ) ; \u000f\u201cPureTrainMistake\u201d/\u201cPureTrainCorrect \u201d : the second and third sampled training subsets and the original wrong test subset ( or the corrected test subset ) ; \u000f\u201cMistakeTestTrain\u201d/\u201cCorrectTestTrain \u201d : the original wrong test subset ( or the corrected test subset ) , the original good test subset , and the third sampled training subset ; \u000f\u201cMistakePureTrain\u201d/\u201cCorrectPureTrain \u201d : the original wrong test subset ( or the corrected test subset ) and the second and third sampled training subsets .", "entities": []}, {"text": "Results show that the label mistakes ( i.e. , original wrong test subset ) hurt the model performance", "entities": []}, {"text": "14 ( a ) Original with label mistakes ( b ) Corrected Figure 3 : Identifying label inconsistency and validating the consistency in the original & corrected CoNLL03 .", "entities": [[27, 28, "DatasetName", "CoNLL03"]]}, {"text": "whenever being fed at the beginning or later .", "entities": []}, {"text": "The corrected test subset delivers comparable performance with the original good test subset and the training set .", "entities": []}, {"text": "This demonstrates the label consistency of the corrected test set with the training set .", "entities": []}, {"text": "3 Experiments 3.1 Results on SCIERC The visual results of the proposed methods have been presented in Section 2 .", "entities": [[5, 6, "DatasetName", "SCIERC"]]}, {"text": "Here we deploy \ufb01ve state - of - the - art NER models to investigate their performance on the corrected SCIERC dataset .", "entities": [[11, 12, "TaskName", "NER"], [20, 21, "DatasetName", "SCIERC"]]}, {"text": "The NER models are BiLSTM - CRF ( Lample et al . , 2016 ) , LM - BiLSTM - CRF ( Liu et", "entities": [[1, 2, "TaskName", "NER"], [4, 5, "MethodName", "BiLSTM"], [6, 7, "MethodName", "CRF"], [18, 19, "MethodName", "BiLSTM"], [20, 21, "MethodName", "CRF"]]}, {"text": "al . , 2018 ) , singletask and multi - task SCIIE ( Luan et al . , 2018 ) , and", "entities": []}, {"text": "multi - task DyGIE ( Luan et al . , 2019 ) .", "entities": []}, {"text": "As shown in Table 2 , all NER models deliver better performance on the corrected SCIERC than the original dataset .", "entities": [[7, 8, "TaskName", "NER"], [15, 16, "DatasetName", "SCIERC"]]}, {"text": "So the training set is more consistent with the \ufb01xed test set than the original wrong test set .", "entities": []}, {"text": "In future work , we will exploreTable 2 : Five NER models perform consistently better on the corrected SCIERC than on the original dataset .", "entities": [[10, 11, "TaskName", "NER"], [18, 19, "DatasetName", "SCIERC"]]}, {"text": "MethodCorrected SCIERC Original SCIERC P R F1 P R F1 BiLSTM - CRF 58.35 47.95 52.64 56.13 48.07 51.79 LM - BiLSTM - CRF 62.78 58.20 60.40 59.15 57.15 58.13 SCIIE - single 71.20 62.88 66.79 65.77 60.90 63.24 SCIIE - multi 72.66 63.22 67.61 67.66 61.72 64.56 DyGIE - multi 69.64 67.02 68.31 65.09 65.28 65.18 more baselines in the leaderboard .", "entities": [[1, 2, "DatasetName", "SCIERC"], [3, 4, "DatasetName", "SCIERC"], [6, 7, "MetricName", "F1"], [9, 10, "MetricName", "F1"], [10, 11, "MethodName", "BiLSTM"], [12, 13, "MethodName", "CRF"], [21, 22, "MethodName", "BiLSTM"], [23, 24, "MethodName", "CRF"]]}, {"text": "3.2 Results on CoNLL03", "entities": [[3, 4, "DatasetName", "CoNLL03"]]}, {"text": "Based on the correction contributed by ( Wang et al . , 2019 ) , we use the proposed method to justify label inconsistency though the label mistakes take \u201c only \u201d 5.38 % .", "entities": []}, {"text": "It also validates the label consistency after recovery .", "entities": []}, {"text": "Figure 3(a ) shows that starting with the wrong labels in the original test set makes the performance worse than starting with the training set or the good test subset .", "entities": []}, {"text": "After label correction , this issue is \ufb01xed in Figure 3(b ) .", "entities": []}, {"text": "4 Related Work NER is typically cast as a sequence labeling problem and solved by models integrate LSTMs , CRF , and language models ( Lample et al . , 2016 ;", "entities": [[3, 4, "TaskName", "NER"], [19, 20, "MethodName", "CRF"]]}, {"text": "Liu et al . , 2018 ; Zeng et al . , 2019 , 2020 ) .", "entities": []}, {"text": "Another idea is to generate span candidates and predict their type .", "entities": []}, {"text": "Span - based models have been proposed with multitask learning strategies ( Luan et al . , 2018 , 2019 ) .", "entities": []}, {"text": "The multiple tasks include concept recognition , relation extraction , and co - reference resolution .", "entities": [[7, 9, "TaskName", "relation extraction"]]}, {"text": "Researchers notice label mistakes in many NLP tasks ( Manning , 2011 ; Wang et", "entities": []}, {"text": "al . , 2019 ; Eskin , 2000 ; Kv \u02d8eto\u02c7n and Oliva , 2002 )", "entities": []}, {"text": ".", "entities": []}, {"text": "For instance , it is reported that the bottleneck of the POS tagging task is the consistency of the annotation result ( Manning , 2011 ) .", "entities": []}, {"text": "People tried to detect label mistakes automatically and minimize the in\ufb02uence of noise in training .", "entities": []}, {"text": "The mistake re - weighting mechanism is effective in the NER task ( Wang et al . , 2019 ) .", "entities": [[10, 11, "TaskName", "NER"]]}, {"text": "We focus on visually evaluating the label consistency .", "entities": []}, {"text": "5 Conclusion We presented an empirical method to explore the relationship between label consistency and NER model performance .", "entities": [[15, 16, "TaskName", "NER"]]}, {"text": "It identi\ufb01ed the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) .", "entities": [[9, 10, "DatasetName", "SCIERC"], [11, 12, "DatasetName", "CoNLL03"]]}, {"text": "It validated the label consistency in", "entities": []}, {"text": "15multiple sets of NER data annotation on two benchmarks , CoNLL03 and SCIERC .", "entities": [[3, 4, "TaskName", "NER"], [10, 11, "DatasetName", "CoNLL03"], [12, 13, "DatasetName", "SCIERC"]]}, {"text": "Acknowledgements This work is supported by National Science Foundation IIS-1849816 and CCF-1901059 .", "entities": []}, {"text": "References Eleazar Eskin .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "Detecting errors within a corpus using anomaly detection .", "entities": [[6, 8, "TaskName", "anomaly detection"]]}, {"text": "In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference , pages 148\u2013153 . Association for Computational Linguistics .", "entities": []}, {"text": "Tianwen Jiang , Qingkai Zeng , Tong Zhao , Bing Qin , Ting Liu , Nitesh V Chawla , and Meng Jiang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Biomedical knowledge graphs construction from conditional statements .", "entities": [[1, 3, "TaskName", "knowledge graphs"]]}, {"text": "IEEE / ACM transactions on computational biology and bioinformatics , 18(3):823\u2013835 .", "entities": [[2, 3, "DatasetName", "ACM"]]}, {"text": "Pavel Kv \u02d8eto\u02c7n and Karel Oliva .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "( semi-)automatic detection of errors in PoS - tagged corpora .", "entities": []}, {"text": "In COLING 2002 : The 19th International Conference on Computational Linguistics .", "entities": []}, {"text": "Guillaume Lample , Miguel Ballesteros , Sandeep Subramanian , Kazuya Kawakami , and Chris Dyer . 2016 .", "entities": []}, {"text": "Neural architectures for named entity recognition .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}, {"text": "arXiv preprint arXiv:1603.01360 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Liyuan Liu , Jingbo Shang , Xiang Ren , Frank Fangzheng Xu , Huan Gui , Jian Peng , and Jiawei Han . 2018 .", "entities": []}, {"text": "Empower sequence labeling with task - aware neural language model .", "entities": []}, {"text": "InThirty - Second AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Yi Luan , Luheng He , Mari Ostendorf , and Hannaneh Hajishirzi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Multi - task identi\ufb01cation of entities , relations , and coreference for scienti\ufb01c knowledge graph construction .", "entities": [[14, 16, "TaskName", "graph construction"]]}, {"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) .", "entities": []}, {"text": "Yi Luan , Dave Wadden , Luheng He , Amy Shah , Mari Ostendorf , and Hannaneh Hajishirzi . 2019 .", "entities": []}, {"text": "A gen - eral framework for information extraction using dynamic span graphs .", "entities": []}, {"text": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies .", "entities": []}, {"text": "Christopher D Manning . 2011 .", "entities": []}, {"text": "Part - of - speech tagging from 97 % to 100 % : is it time for some linguistics ?", "entities": [[0, 6, "TaskName", "Part - of - speech tagging"]]}, {"text": "InInternational conference on intelligent text processing and computational linguistics .", "entities": []}, {"text": "Erik F Sang and Fien De Meulder .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Introduction to the conll-2003 shared task : Languageindependent named entity recognition .", "entities": [[3, 4, "DatasetName", "conll-2003"], [8, 11, "TaskName", "named entity recognition"]]}, {"text": "arXiv preprint cs/0306050 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zihan Wang , Jingbo Shang , Liyuan Liu , Lihao Lu , Jiacheng Liu , and Jiawei Han . 2019 .", "entities": []}, {"text": "Crossweigh : Training named entity tagger from imperfect annotations .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) .", "entities": []}, {"text": "Wenhao Yu , Mengxia Yu , Tong Zhao , and Meng Jiang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Identifying referential intention with heterogeneous contexts .", "entities": []}, {"text": "In Proceedings of The Web Conference 2020 .", "entities": []}, {"text": "Qingkai Zeng , Jinfeng Lin , Wenhao Yu , Jane ClelandHuang , and Meng Jiang . 2021 .", "entities": []}, {"text": "Enhancing taxonomy completion with concept generation via fusing relational representations .", "entities": []}, {"text": "In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ( KDD ) .", "entities": [[1, 2, "DatasetName", "ACM"]]}, {"text": "Qingkai Zeng , Mengxia Yu , Wenhao Yu , Jinjun Xiong , Yiyu Shi , and Meng Jiang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Faceted hierarchy : A new graph type to organize scienti\ufb01c concepts and a construction method .", "entities": []}, {"text": "In Proceedings of the Thirteenth Workshop on Graph - Based Methods for Natural Language Processing ( TextGraphs-13 ) .", "entities": []}, {"text": "Qingkai Zeng , Wenhao Yu , Mengxia Yu , Tianwen Jiang , Tim Weninger , and Meng Jiang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Tritrain : Automatic pre-\ufb01ne tuning between pretraining and \ufb01ne - tuning for sciner .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : Findings .", "entities": []}]