[{"text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics , pages 2720\u20132726 April 19 - 23 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics2720Complementary Evidence Identi\ufb01cation in Open - Domain Question Answering Xiangyang Mou Rensselaer Polytechnic Institute moux4@rpi.eduMo", "entities": [[9, 14, "TaskName", "Open - Domain Question Answering"]]}, {"text": "Yu IBM Research yum@us.ibm.comShiyu Chang MIT - IBM Watson AI Lab shiyu.chang@ibm.com Yufei Feng Queen \u2019s University feng.yufei@queensu.caLi Zhang Amazon Web Services lzhangza@amazon.comHui Su Fidelity", "entities": []}, {"text": "Hui.Su@fmr.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "This paper proposes a new problem of complementary evidence identi\ufb01cation for opendomain question answering ( QA ) .", "entities": [[12, 14, "TaskName", "question answering"]]}, {"text": "The problem aims to ef\ufb01ciently \ufb01nd a small set of passages that covers full evidence from multiple aspects as to answer a complex question .", "entities": []}, {"text": "To this end , we proposes a method that learns vector representations of passages and models the suf\ufb01ciency and diversity within the selected set , in addition to the relevance between the question and passages .", "entities": []}, {"text": "Our experiments demonstrate that our method considers the dependence within the supporting evidence and signi\ufb01cantly improves the accuracy of complementary evidence selection in QA domain .", "entities": [[17, 18, "MetricName", "accuracy"]]}, {"text": "1 Introduction In recent years , signi\ufb01cant progress has been made in the \ufb01eld of open - domain question answering ( Chen et al . , 2017 ; Wang et al . , 2017 , 2018 ; Clark and Gardner , 2018 ;", "entities": [[15, 20, "TaskName", "open - domain question answering"]]}, {"text": "Min et al . , 2018 ; Asai et al . , 2019 ) .", "entities": []}, {"text": "Very recently , some works turn to deal with a more challenging task of asking complex questions ( Welbl et al . , 2018 ; Clark et al . , 2018 ; Yang et al . , 2018 ) from the open - domain text corpus .", "entities": []}, {"text": "In the open - domain scenario , one critical challenge raised by complex questions is that each question may require multiple pieces of evidence to get the right answer , while the evidence usually scatters in different passages .", "entities": []}, {"text": "Examples in Figure 1 shows two types of questions that require evidence from multiple passages .", "entities": []}, {"text": "To deal with the challenging multi - evidence questions , an open - domain QA system should be able to ( 1 ) ef\ufb01ciently retrieve a small number of passages that cover the full evidence ; and ( 2 ) accurately extract the answer by jointly considering the candidate evidence passages .", "entities": []}, {"text": "While there have been several prior works in the latter direction ( Wang et al . , 2017 ; Clark and Gardner , 2018 ; Figure 1 : Examples of complex questions involving two facts of a person .", "entities": []}, {"text": "Different facts are color - coded .", "entities": []}, {"text": "P#are all relevant passages , while only the ones with solid - line boxes are the true supporting passages .", "entities": []}, {"text": "Lin et", "entities": []}, {"text": "al . , 2018 ) , the solutions to the \ufb01rst problem still rely on traditional or neural information retrieval ( IR ) approaches , which solely measure the relevance between the question and each individual paragraph , and will highly possibly put the wrong evidence to the top.1For example in Figure 1 ( top ) , P1andP2are two candidate evidence passages that are closely related to the question but only cover the same unilateral fact required by the question , therefore leading us to the wrong answer Newton .", "entities": [[18, 20, "TaskName", "information retrieval"]]}, {"text": "This paper formulates a new problem of complementary evidence identi\ufb01cation for answering complex questions .", "entities": []}, {"text": "The key idea is to consider the problem as measuring the properties of the selected passages , more than the individual relevance .", "entities": []}, {"text": "Speci\ufb01cally , we hope the selected passages can serve as a set of spanning bases that supports the 1(Min et", "entities": []}, {"text": "al . , 2019 ) pointed out the shortcut problem in multi - hop QA .", "entities": []}, {"text": "However , as some works ( Wang et al . , 2019 ) show that even a better designed multi - hop model can still bene\ufb01t from full evidence in such situation .", "entities": []}, {"text": "2721question .", "entities": []}, {"text": "The selected passage set thus should satisfy the properties of ( 1 ) relevancy , i.e. , they should be closely related to the question ; ( 2 ) diversity , i.e. , they should cover diverse information given the coverage property is satis\ufb01ed ; ( 3 ) compactness , i.e. , the number of passages to satisfy the above properties should be minimal .", "entities": []}, {"text": "With these three de\ufb01ned properties , we hope to both improve the selective accuracy and encourage the interpretability of the evidence identi\ufb01cation .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "Note that complementary evidence identi\ufb01cation in QA is different from Search Result Diversi\ufb01cation ( SRD ) in IR on their requirement of compactness .", "entities": [[14, 15, "DatasetName", "SRD"]]}, {"text": "The size of the selected set is constrained in QA tasks by the capability of downstream reasoning models and practically needs to be a small value , whereas it is not the case in SRD .", "entities": [[34, 35, "DatasetName", "SRD"]]}, {"text": "To achieve the above goals , a straightforward approach is to train a model that evaluates each subset of the candidate passages , e.g. , by concatenating passages in any subsets .", "entities": []}, {"text": "However , this approach is highly inef\ufb01cient since it requires to encodeO(KL)passage subsets , where Kis the total number of candidates and Lis the maximum size of subsets .", "entities": []}, {"text": "Thus , a practical complementary evidence identi\ufb01cation method needs to be computationally ef\ufb01cient .", "entities": []}, {"text": "This is especially critical when we use heavy models like ELMo ( Peters et al . , 2018 ) and BERT ( Devlin et al . , 2018 ) , where passage encoding is time and memory consuming .", "entities": [[10, 11, "MethodName", "ELMo"], [20, 21, "MethodName", "BERT"]]}, {"text": "To this end , we propose an ef\ufb01cient method to select a set of spanning passages that is suf\ufb01cient and diverse .", "entities": []}, {"text": "The core idea is to represent questions and passages in a vector space and de\ufb01ne the measures of our criterion in the vector space .", "entities": []}, {"text": "For example , in the vector space , suf\ufb01ciency can be de\ufb01ned as a similarity between the question vector and the sum of selected passage vectors , measured by a cosine function with a higher score indicating a closer similarity ; and diversity can be de\ufb01ned as ` 1distance between each pair of passages .", "entities": []}, {"text": "By properly training the passage encoder with a loss function derived by the above terms , we expect the resulted vector space satis\ufb01es the property that the complementary evidence passages lead to large scores .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "In addition , our method only encodes each passage in the candidate set once , which is more ef\ufb01cient than the naive solution mentioned above .", "entities": []}, {"text": "To evaluate the proposed method , we use the multi - hop QA dataset HotpotQA ( the full wiki setting ) since the ground - truth of evidence passages are provided .", "entities": [[14, 15, "DatasetName", "HotpotQA"]]}, {"text": "Experiments show that our method signi\ufb01cantly improves the accuracy of complementary evidence selection .", "entities": [[8, 9, "MetricName", "accuracy"]]}, {"text": "2 Proposed Method 2.1 Task De\ufb01nition Given a question qand a mixture set of paragraphs P = P+[P\u0000with some paragraphs p2P+relevant toqand somep2P\u0000irrelevant .", "entities": []}, {"text": "Our goal is to select a small subset of paragraphs Psel\u001aP , such that every p2P selsatis\ufb01esp2P+(relevancy ) , and all p2 P selcan jointly cover all the information asked by q(complementary ) .", "entities": []}, {"text": "The off - the - shelf models select relevant paragraphs independently , thus usually can not deal with the complementary property .", "entities": []}, {"text": "The inner dependency among the selectedPselneeds to be considered , which will be modeled in the remaining of the section .", "entities": []}, {"text": "2.2 Model and Training Vector Space Modeling We apply BERT model to estimate the likelihood of a paragraph pbeing the supporting evidence to the question q , denoted asP(pjq ) .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "Letqandpidenote the input texts of a question and a passage .", "entities": []}, {"text": "We feed qand the concatenation ofqandpiinto the BERT model , and use the hidden states of the last layer to represent qand piin vector space , denoted as qandpirespectively .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "A fully connected layer f(\u0001)followed by sigmoid activation is added to the end of the BERT model , and outputs a scalar P(pijq)to estimate how relevant the paragraph piis to the question .", "entities": [[6, 8, "MethodName", "sigmoid activation"], [15, 16, "MethodName", "BERT"]]}, {"text": "Note that in our implementation piis based on both qandpi , but we omit the condition on qfor simplicity .", "entities": []}, {"text": "Complementary Conditions Previous works extract evidence paragraphs according to P(pjq ) , which is estimated on each passage separately without considering the dependency among selected paragraphs .", "entities": []}, {"text": "To extract complementary evidence , we propose that the selected passages Pselshould satisfy the following conditions that intuitively encourage each selected passage to be a basis to support the question : \u000fRelevancy : Pselshould have a high probability ofP pi2PselP(pijq ) ; \u000fDiversity : Pselshould cover passages as diverse as possible , which can be measured by the average distance between any pairs in Psel , e.g. , maximizingP i;j2fi;jjpi;pj2Psel;i6 = jg`1(pi;pj ) .", "entities": []}, {"text": "Here", "entities": []}, {"text": "2722`1(\u0001;\u0001)denotesL1distance ; \u000fCompactness : Pselshould optimize the aforementioned conditions while the size being minimal .", "entities": []}, {"text": "In this work we constrain the compactness by \ufb01xingjPseljand meanwhile maximizing cos(P i2fijpi2Pselgpi;q ) .", "entities": []}, {"text": "We use cos(\u0001;\u0001)to encourage the collection of evidence covers what needed by the question .", "entities": []}, {"text": "Training with Complementary Regularization We propose a new supervised training objective to learn the BERT encoder for QA that optimizes the previous conditions .", "entities": [[14, 15, "MethodName", "BERT"]]}, {"text": "Note that in this work we assume a set of labeled training examples are available , i.e. , the ground truth annotations contain complementary supporting paragraphs .", "entities": []}, {"text": "Recently there was a growing in such datasets ( Yang et al . , 2018 ; Yao et", "entities": []}, {"text": "al . , 2019 ) , due to the increasing interest in model explainability .", "entities": []}, {"text": "Also , such supervision signals can also be obtained with distant supervision .", "entities": []}, {"text": "For each training instance ( q;P ) , we de\ufb01ne fpig+=fpig;8i2fijpi2P+g ( 1 ) fpig\u0000=fpig;8i2fijpi2P\u0000g ( 2 ) fpig = fpig+[fpig\u0000(3 ) Denotingypi= 1 ifpi2 P+andypi= 0 if pi2P\u0000 , we have the following training objective function : L(fpig;q;y ) = Lsup(fpig;q;y ) + \u000b Ld(fpig+ ) + \f Lc(fpig;q;y)(4 ) where Lsup(fpig;q;y )", "entities": [[26, 27, "DatasetName", "0"]]}, {"text": "= \u0000X iypilog(f(pi ) ) ; ( 5 ) Ld(fpig+ ) = X pi;pj;i6 = j(1\u0000`1(pi;pj ) ): ( 6 ) Lc(fpig;q;y )", "entities": []}, {"text": "= 8 > > > < > > > : 1\u0000cos(q;P ipi ) ; if\u0005piypi= 1 max(0;cos(q;P ipi)\u0000", "entities": []}, {"text": ") ; if\u0005piypi= 0(7 ) where \u000b and \f are the hyperparameter weights and ` 1(\u0001;\u0001)denotes L1 loss between two input vectors .", "entities": [[17, 18, "MetricName", "loss"]]}, {"text": "Eq 5 is the cross - entropy loss corresponding to relevance condition ; Eq 6 regularizes the diversity condition ; Eq 7 is the cosine - embedding loss2for the compactness condition and", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "> 0is the margin to encourage data samples with better question coverage .", "entities": []}, {"text": "2Refer to CosineEmbeddingLoss in PyTorch.2.3 Inference via Beam Search Score Function During inference , we use the following score function to \ufb01nd the best paragraph combination : g(Psel;q;fpig ) = X piP(pijq )", "entities": [[9, 10, "MetricName", "Score"]]}, {"text": "+ \u000b cos(X pipi;q ) + \f X pi;pj;i6 = j`1(pi;pj)(8 ) where \u000b and \f are hyperparameters similar to Eq 4 .", "entities": []}, {"text": "Note that our approach requires to encode each passage inPonly once for each question , resulting in anO(K)time complexity of encoding ( K= jPj ) ; and the subset selection is performed in the vector space , which is much more ef\ufb01cient than selecting subsets before encoding .", "entities": []}, {"text": "Beam Search In a real - world application , there is usually a large candidate set of P , e.g. , retrieved passages for qvia a traditional IR system .", "entities": []}, {"text": "Our algorithm requires O(K)time encoding , and O(KL ) time scoring in vector space when ranking all the combinations in Lcandidates .", "entities": []}, {"text": "Thus when Kbecomes large , it is still inef\ufb01cient even when L= 2 .", "entities": []}, {"text": "We resort to beam search to deal with scenarios with largeKs .", "entities": []}, {"text": "The details can be found in Appendix A. 3 Experiments 3.1 Settings Datasets Considering the prerequisite of sentence - level evidence annotations , we evaluate our approach on two datasets , a synthetic dataset MNLI-12 and a real application HotpotQA-50 .", "entities": []}, {"text": "Data sampling is detailed in Appendix B. \u000fMNLI-12 is constructed based on the textual entailment dataset MNLI ( Williams", "entities": [[16, 17, "DatasetName", "MNLI"]]}, {"text": "et al . , 2018 ) , in order to verify the ability of our method in \ufb01nding complementary evidence", "entities": []}, {"text": ".", "entities": []}, {"text": "In original MNLI , each premise sentence corresponds to three hypotheses sentences : entailment , neutral and contradiction .", "entities": [[2, 3, "DatasetName", "MNLI"]]}, {"text": "To generate complementary pairs for each premise sentence , we split each hypothesis sentence into two segments .", "entities": []}, {"text": "The goal is to \ufb01nd the segment combination that entails premise sentence , and our dataset , by de\ufb01nition , ensures that only the combination of two segments from the entailment hypothesis can entail the premise , not any of its subset or other combinations .", "entities": []}, {"text": "The original train / dev / test splits from MNLI are used .", "entities": [[9, 10, "DatasetName", "MNLI"]]}, {"text": "2723\u000fHotpotQA-50 is based on the open - domain setting of the multi - hop QA benchmark HotpotQA ( Yang et al . , 2018 ) .", "entities": [[16, 17, "DatasetName", "HotpotQA"]]}, {"text": "The original task requires to \ufb01nd evidence passages from abstract paragraphs of all Wikipedia pages to support a multi - hop question .", "entities": []}, {"text": "For each q , we collect 50 relevant passages based on bigram BM25 ( Godbole et al . , 2019 ) .", "entities": []}, {"text": "Two positive evidence passages to each question are provided by human annotators as the ground truth .", "entities": []}, {"text": "Note that there is no guarantee that P50covers both evidence passages here .", "entities": []}, {"text": "We use the original development set from HotpotQA as our test set and randomly split a subset from the original training set as our development set .", "entities": [[7, 8, "DatasetName", "HotpotQA"]]}, {"text": "3.2 Settings Baseline We compare with the BERT passage ranker ( Nie et al . , 2019 ) that is commonly used on open - domain QA including HotpotQA .", "entities": [[7, 8, "MethodName", "BERT"], [28, 29, "DatasetName", "HotpotQA"]]}, {"text": "The baseline uses the same BERT architecture as our approach described in Section 2.2 , but is trained with only the relevancy loss ( Eq 5 ) and therefore only consider the relevancy when selecting evidence .", "entities": [[5, 6, "MethodName", "BERT"], [22, 23, "MetricName", "loss"]]}, {"text": "We also compare the DRN model from ( Harel et al . , 2019 ) which is designed for the SRD task .", "entities": [[20, 21, "DatasetName", "SRD"]]}, {"text": "Their ensemble system \ufb01rst \ufb01nds the most relevant evidence to the given question , and then select the second diverse evidence using their score function .", "entities": []}, {"text": "The major differences from our method are that ( 1 ) they train two separate models for evidence selection ; ( 2 ) they do not consider the compactness among the evidences .", "entities": []}, {"text": "It is worth mentioning that we replace their LSTM encoder with BERT encoder for fair comparison .", "entities": [[8, 9, "MethodName", "LSTM"], [11, 12, "MethodName", "BERT"]]}, {"text": "Metric During the evaluation we make each method output its top 2 ranked results3(i.e .", "entities": []}, {"text": "the top 1 ranked pair from our method ) as the prediction .", "entities": []}, {"text": "The \ufb01nal performance is evaluated by exact match ( EM ) , i.e. , whether both true evidence passages are covered , and the F1 score on the test sets .", "entities": [[6, 8, "MetricName", "exact match"], [9, 10, "MetricName", "EM"], [24, 26, "MetricName", "F1 score"]]}, {"text": "3.3 Results In the experiments , we have M= 3,N= 4 for MNLI-12 and M= 4,N= 5 for HotpotQA50 with our method .", "entities": []}, {"text": "The values are selected according to development performance .", "entities": []}, {"text": "We follow the settings and hyperparameters used in ( Harel et al . , 2019 ) for the DRN model .", "entities": []}, {"text": "Table 1 shows the performance .", "entities": []}, {"text": "The upper - bound measures how 3There is only one positive pair of evidences for each question .", "entities": []}, {"text": "SystemHotpotQA-50 MNLI-12 EM F1 EM F1 Baseline Ranker 16.67 41.29 41.61 67.57 DRN + BERT 1.03 35.37 6.20 46.07 Our Method 20.15 49.10 53.81 73.18 Upper - Bound 35.49 61.08 100.00 100.00 Table 1 : Model Evaluation ( % ) .", "entities": [[2, 3, "MetricName", "EM"], [3, 4, "MetricName", "F1"], [4, 5, "MetricName", "EM"], [5, 6, "MetricName", "F1"], [14, 15, "MethodName", "BERT"]]}, {"text": "The upper - bound indicates the amount of true evidences contained by all candidate passages .", "entities": []}, {"text": "The baseline ranker is a BERT ranker trained only with relevancy loss .", "entities": [[5, 6, "MethodName", "BERT"], [11, 12, "MetricName", "loss"]]}, {"text": "many pieces of true evidences enclosed by the complete set of candidate passages where our proposed ranker selects from .", "entities": []}, {"text": "For HotpotQA dataset , we use a bi - gram BM25 ranker to collect top 50 relevant passages and build the basis for the experiments4 , which inevitably leads some of the true evidences to be \ufb01ltered out and makes its upper - bound less than100 % .", "entities": [[1, 2, "DatasetName", "HotpotQA"]]}, {"text": "For the arti\ufb01cial MNLI-12 dataset , all the true evidences are guaranteed to be included .", "entities": []}, {"text": "Table 1 shows that our method achieves signi\ufb01cant improvements on both datasets .", "entities": []}, {"text": "On HotpotQA-50 , all systems have low EM scores , because of the relatively low recall of the BM25 retrieval .", "entities": [[7, 8, "MetricName", "EM"]]}, {"text": "Only 35:49 % of the samples in the test set contain both ground - truth evidence passages .", "entities": []}, {"text": "On MNLI-12 , the EM score is around 50 % .", "entities": [[4, 5, "MetricName", "EM"]]}, {"text": "This is mainly because the segments are usually much shorter than a paragraph , with an average length of7words .", "entities": []}, {"text": "Therefore it is more challenging in matching the qwith thepis .", "entities": []}, {"text": "Speci\ufb01cally , both our method and the BERT baseline surpass the DRN model on all datasets and metrics , which results from our question - conditioned passage encoding approach .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "Our de\ufb01ned vector space proves bene\ufb01cial to model the complementation among the evidence with respect to a given question .", "entities": []}, {"text": "The ablation study of our loss function further illustrates that the diversity and the compactness terms ef\ufb01ciently bring additional 20%/30 % increase in EM score on two datasets and consequently raise the F1 score by about 8/6absolute points .", "entities": [[5, 6, "MetricName", "loss"], [23, 24, "MetricName", "EM"], [32, 34, "MetricName", "F1 score"]]}, {"text": "Figure 2 gives examples about how our model improves over the baseline .", "entities": []}, {"text": "Our method can successfully select complementary passages while the baselines only select passages that look similar to the question .", "entities": []}, {"text": "A more interesting example is given at the bottom where the top-50 only covers one supporting passage .", "entities": []}, {"text": "The BERT baseline selects two 4This is the standard setting that starts with BM25 retrieval to make the inference time ef\ufb01cient enough without loss of generality .", "entities": [[1, 2, "MethodName", "BERT"], [23, 24, "MetricName", "loss"]]}, {"text": "2724incorrect passages that cover identical part of facts required by the question and similarly the DRN baseline select a relevant evidence and an irrelevant evidence , while our method scores lower the second passage that does not bring new information , and reaches a supporting selection .", "entities": []}, {"text": "A similar situation contributes to the majority of improvement on one - supporting - evidence data sample in HotpotQA-50 .", "entities": []}, {"text": "Inference Speed Our beam search with score function brings slight overheads to the running time .", "entities": []}, {"text": "On HotpotQA-50 , it takes 1,990 milliseconds ( ms ) on average to obtain the embeddings of all passages for one data sample whereas our vector - based complementary selection only adds an extra 2 ms which can be negligible compared to the encoding time .", "entities": []}, {"text": "3.4 Future Work The latest dense retrieval methods ( Lee et al . , 2019 ; Karpukhin et al . , 2020 ; Guu et al . , 2020 ) show promising results on ef\ufb01cient inference on the full set of Wikipedia articles , which allows to skip the initial standard BM25 retrieval and avoid the signi\ufb01cant loss during the pre - processing step .", "entities": [[57, 58, "MetricName", "loss"]]}, {"text": "Our proposed approach is able to directly cooperate with these methods as we all work in the vector space .", "entities": []}, {"text": "Therefore , the extension to dense retrieval can be naturally the next step of our work .", "entities": []}, {"text": "4 Conclusion In the paper , we propose a new problem of complementary evidence identi\ufb01cation and de\ufb01ne the criterion of complementary evidence in vector space .", "entities": []}, {"text": "We further design an algorithm and a loss function to support ef\ufb01cient training and inference for complementary evidence selection .", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "Compared to the baseline , our approach improves more than 20 % and remains to scale well to the computationally complex cases .", "entities": []}, {"text": "Acknowledgment A special thank to Rensselaer - IBM Arti\ufb01cial Intelligence Research Collaboration ( RPI - AIRC ) for providing externship and other supports .", "entities": []}, {"text": "This work is funded by Cognitive and Immersive Systems Lab ( CISL ) , a collaboration between IBM and RPI , and also a center in IBM \u2019s AI Horizons Network .", "entities": []}, {"text": "Figure 2 : Gain from complementary selection .", "entities": []}, {"text": "In both examples , the DRN baseline \ufb01rst \ufb01nds the most relevant evidence to the question ( left ) and then select a diverse one ( right ) ; the BERT baseline model selected the top-2 most relevant passages ( P1,P2 ) to the question regardless of their complementation ; whereas our model made the selection ( P1,P3 ) with consideration of both relevance and evidence suf\ufb01ciency .", "entities": [[30, 31, "MethodName", "BERT"]]}, {"text": "Note that , in the bottom example , one of the ground - truth supporting passages and the answer were excluded when building the dataset .", "entities": []}, {"text": "References Akari Asai , Kazuma Hashimoto , Hannaneh Hajishirzi , Richard Socher , and", "entities": []}, {"text": "Caiming Xiong .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Learning to retrieve reasoning paths over wikipedia graph for question answering .", "entities": [[9, 11, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1911.10470 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Danqi Chen , Adam Fisch , Jason Weston , and Antoine Bordes . 2017 .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "Reading wikipedia to answer open - domain questions .", "entities": []}, {"text": "arXiv preprint arXiv:1704.00051 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Christopher Clark and Matt Gardner .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Simple and effective multi - paragraph reading comprehension .", "entities": [[6, 8, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 845\u2013855 .", "entities": []}, {"text": "Peter Clark , Isaac Cowhey , Oren Etzioni , Tushar Khot , Ashish Sabharwal , Carissa Schoenick , and Oyvind Tafjord .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Think you have solved question answering ?", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "try arc , the ai2 reasoning challenge .", "entities": []}, {"text": "arXiv preprint arXiv:1803.05457 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep", "entities": []}, {"text": "2725bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ameya Godbole , Dilip Kavarthapu , Rajarshi Das , Zhiyu Gong , Abhishek Singhal , Hamed Zamani , Mo Yu , Tian Gao , Xiaoxiao Guo , Manzil Zaheer , et al . 2019 .", "entities": []}, {"text": "Multi - step entity - centric information retrieval for multi - hop question answering .", "entities": [[6, 8, "TaskName", "information retrieval"], [9, 14, "TaskName", "multi - hop question answering"]]}, {"text": "arXiv preprint arXiv:1909.07598 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kelvin Guu , Kenton Lee , Zora Tung , Panupong Pasupat , and Ming - Wei Chang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Realm : Retrievalaugmented language model pre - training .", "entities": []}, {"text": "arXiv preprint arXiv:2002.08909 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shahar Harel , Se\ufb01 Albo , Eugene Agichtein , and Kira Radinsky .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Learning novelty - aware ranking of answers to complex questions .", "entities": []}, {"text": "In The World Wide Web Conference , pages 2799\u20132805 .", "entities": []}, {"text": "Vladimir Karpukhin , Barlas O \u02d8guz , Sewon Min , Ledell Wu , Sergey Edunov , Danqi Chen , and Wentau Yih . 2020 .", "entities": []}, {"text": "Dense passage retrieval for open - domain question answering .", "entities": [[1, 3, "TaskName", "passage retrieval"], [4, 9, "TaskName", "open - domain question answering"]]}, {"text": "arXiv preprint arXiv:2004.04906 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kenton Lee , Ming - Wei Chang , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Latent retrieval for weakly supervised open domain question answering .", "entities": [[7, 9, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1906.00300 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yankai Lin , Haozhe Ji , Zhiyuan Liu , and Maosong Sun . 2018 .", "entities": []}, {"text": "Denoising distantly supervised open - domain question answering .", "entities": [[0, 1, "TaskName", "Denoising"], [3, 8, "TaskName", "open - domain question answering"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1736 \u2013 1745 .", "entities": []}, {"text": "Sewon Min , Eric Wallace , Sameer Singh , Matt Gardner , Hannaneh Hajishirzi , and Luke Zettlemoyer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Compositional questions do not necessitate multi - hop reasoning .", "entities": []}, {"text": "arXiv preprint arXiv:1906.02900 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sewon Min , Victor Zhong , Richard Socher , and Caiming Xiong .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Ef\ufb01cient and robust question answering from minimal context over documents .", "entities": [[3, 5, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1805.08092 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yixin Nie , Songhe Wang , and Mohit Bansal .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Revealing the importance of semantic retrieval for machine reading at scale .", "entities": [[4, 6, "TaskName", "semantic retrieval"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2553\u20132566 .", "entities": []}, {"text": "Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 2227 \u2013 2237.Haoyu Wang , Mo Yu , Xiaoxiao Guo , Rajarshi Das , Wenhan Xiong , and Tian Gao . 2019 .", "entities": []}, {"text": "Do multi - hop readers dream of reasoning chains ?", "entities": []}, {"text": "arXiv preprint arXiv:1910.14520 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shuohang Wang , Mo Yu , Xiaoxiao Guo , Zhiguo Wang , Tim Klinger , Wei Zhang , Shiyu Chang , Gerry Tesauro , Bowen Zhou , and Jing Jiang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "R 3 : Reinforced ranker - reader for open - domain question answering .", "entities": [[8, 13, "TaskName", "open - domain question answering"]]}, {"text": "In Thirty - Second AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Shuohang Wang , Mo Yu , Jing Jiang , Wei Zhang , Xiaoxiao Guo , Shiyu Chang , Zhiguo Wang , Tim Klinger , Gerald Tesauro , and Murray Campbell . 2017 .", "entities": []}, {"text": "Evidence aggregation for answer re - ranking in open - domain question answering .", "entities": [[8, 13, "TaskName", "open - domain question answering"]]}, {"text": "arXiv preprint arXiv:1711.05116 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Johannes Welbl , Pontus Stenetorp , and Sebastian Riedel .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Constructing datasets for multi - hop reading comprehension across documents .", "entities": [[3, 8, "TaskName", "multi - hop reading comprehension"]]}, {"text": "Transactions of the Association for Computational Linguistics , 6:287\u2013302 .", "entities": []}, {"text": "Adina Williams , Nikita Nangia , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A broad - coverage challenge corpus for sentence understanding through inference .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1112\u20131122 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zhilin Yang , Peng Qi , Saizheng Zhang , Yoshua Bengio , William W Cohen , Ruslan Salakhutdinov , and Christopher D Manning .", "entities": [[16, 17, "DatasetName", "Ruslan"]]}, {"text": "2018 .", "entities": []}, {"text": "Hotpotqa : A dataset for diverse , explainable multi - hop question answering .", "entities": [[0, 1, "DatasetName", "Hotpotqa"], [8, 13, "TaskName", "multi - hop question answering"]]}, {"text": "arXiv preprint arXiv:1809.09600 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yuan Yao , Deming Ye , Peng Li , Xu Han , Yankai Lin , Zhenghao Liu , Zhiyuan Liu , Lixin Huang , Jie Zhou , and Maosong Sun . 2019 .", "entities": []}, {"text": "Docred : A large - scale document - level relation extraction dataset .", "entities": [[0, 1, "DatasetName", "Docred"], [9, 11, "TaskName", "relation extraction"]]}, {"text": "arXiv preprint arXiv:1906.06127 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "2726A Complementary Evidence Selection via Beam Search For ef\ufb01cient inference when L= 2 , we start to select the top- N(N \u001c K ) most relevant passages .", "entities": []}, {"text": "Then we score the combinations between each passage pair in the top- Nset and another top- Mset .", "entities": []}, {"text": "This reduces the complexity from O(K2)to O(MN).Mis a hyperparameter corresponding to the beam size .", "entities": []}, {"text": "In a more general setting with L\u00152 , we have an algorithm with the complexity of O((L\u00001)MN)instead ofO(KL ) , which is shown in algorithm 1 .", "entities": []}, {"text": "Algorithm 1 : Complementary Evidence Selection via Beam Search Data : Vector representation of question ( q ) , vector representation of all the Npassagesfpng(fpng ) ; the maximum number of passage to select ( L ) ; the beam size ( M ) ; a vector of weights for all regularization terms \u0015. Result : The top ranked complementary passages .", "entities": []}, {"text": "/*Predict", "entities": []}, {"text": "the probability P(pi)of being a supporting passage for each passage pigivenq * / 1fori2[1;N]do 2P(pi ) f(q;pi ) ; 3end 4Rank the passages by P(pi ) ; 5Pspan =[ ] 6PickMpassages with top P(pi)intoPspan ; 7fordepth2[2;L]do 8P0 span =[ ] ; 9 forj2[1;M]do /*Pjis a selected subset , sjis the corresponding score * / 10 Pop thej - th tuple ( Pj;sj)fromPspan ; 11 forn2[1;N]do 12 ifThe setPj[fpngis covered byP0 span then 13 continue 14 end /*rnis", "entities": []}, {"text": "the regulation increases by adding pntoPj * / 15 Put(Pj[fpng;sj+P(pn ) + \u0015rn)intoP0 span ; 16 ifMore thanMtuples added based on Pjthen 17 break 18 end 19 end 20 end 21 RankP0 span according to the scores ; 22Pspan P0 span[1 : M ] 23end 24ReturnPspan", "entities": []}, {"text": "[ 0 ] B Data Sampling MNLI-12 In original MNLI , each premise sentence Pcorresponds to one entailment EP , one neutral NPand one contradiction CP .", "entities": [[1, 2, "DatasetName", "0"], [9, 10, "DatasetName", "MNLI"]]}, {"text": "We take the premise Pasq , and split each of its corresponding hypotheses into two segments with a random cutting point near the middle of the sentence , resulting in a total of 6 segments fE1 P;E2 P;N1 P;N2 P;C1 P;C2 Pg .", "entities": []}, {"text": "Mixing them with the 6 segments corresponding to another premise X , we can \ufb01nally have P+=fE1 P;E2 PgandP\u0000= fN1", "entities": []}, {"text": "P;N2 P;C1 P;C2 P;E1 X;E2 X;N1 X;N2 X;C1 X;C2", "entities": []}, {"text": "Xg .", "entities": []}, {"text": "Consequently , we sample one positive and eight negative pairs respectively fromP+andP\u0000.", "entities": []}, {"text": "A pair likefE1 P;C2 Xgis considered as negative .", "entities": []}, {"text": "To ensure the segments are literally meaningful , each segment is guaranteed to be longer than 5 words .", "entities": []}, {"text": "HotpotQA In HotpotQA , the true supporting paragraphs of each question qare given .", "entities": [[0, 1, "DatasetName", "HotpotQA"], [2, 3, "DatasetName", "HotpotQA"]]}, {"text": "Therefore , we can easily form P+ andP\u0000and sample positive and negative pairs of paragraphs respectively from P+andP\u0000.", "entities": []}, {"text": "A special pair that contains one true supporting paragraph and one non - supporting paragraph is considered as a negative pair .", "entities": []}]