[{"text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 3960 - 3975 May 22 - 27 , 2022 c", "entities": []}, {"text": "2022 Association for Computational Linguistics Automatic Error Analysis for Document - level Information Extraction Aliva Das\u0003 , Xinya Du\u0003 , Barry Wang\u0003 , Kejian Shi , Jiayuan Gu , Thomas Porter , Claire Cardie Department of Computer Science , Cornell University { ad677 , xd75 , zw545 , ks2325 , jg844 , tjp78 , ctc9}@cornell.edu Abstract Document - level information extraction ( IE ) tasks have recently begun to be revisited in earnest using the end - to - end neural network techniques that have been successful on their sentence - level IE counterparts .", "entities": [[6, 7, "MetricName", "Error"], [39, 40, "DatasetName", "Cornell"]]}, {"text": "Evaluation of the approaches , however , has been limited in a number of dimensions .", "entities": []}, {"text": "In particular , the precision / recall / F1 scores typically reported provide few insights on the range of errors the models make .", "entities": [[8, 9, "MetricName", "F1"]]}, {"text": "We build on the work of Kummerfeld and Klein ( 2013 ) to propose a transformation - based framework for automating error analysis in document - level event and ( N - ary ) relation extraction .", "entities": [[34, 36, "TaskName", "relation extraction"]]}, {"text": "We employ our framework to compare two state - of - theart document - level template-\ufb01lling approaches on datasets from three domains ; and then , to gauge progress in IE since its inception 30 years ago , vs. four systems from the MUC-4 ( 1992 ) evaluation.1 1 Introduction Although information extraction ( IE ) research has almost uniformly focused on sentence - level relation and event extraction ( Grishman , 2019 ) , the earliest research in the area formulated the task at thedocument level .", "entities": [[43, 44, "DatasetName", "MUC-4"], [67, 69, "TaskName", "event extraction"]]}, {"text": "Consider , for example , the \ufb01rst large - scale ( for the time ) evaluations of IE systems \u2014 e.g. MUC-3 ( 1991 ) and MUC-4 ( 1992 ) .", "entities": [[26, 27, "DatasetName", "MUC-4"]]}, {"text": "Each involved a complex document - level event extraction task : there were 24 types of events , over a dozen event arguments ( or roles ) to be identi\ufb01ed for each event ; documents could contain zero to tens of events , and extracting argument entities ( or role \ufb01llers )", "entities": [[4, 9, "TaskName", "document - level event extraction"]]}, {"text": "required noun phrase coreference resolution to ensure interpretability for the end - user ( e.g. to ensure that multiple distinct mentions of", "entities": [[3, 5, "TaskName", "coreference resolution"]]}, {"text": "the *", "entities": []}, {"text": "These authors contributed equally to this work .", "entities": []}, {"text": "1Our code for the error analysis tool and its output on different model predictions are available at https://github . com / IceJinx33 / auto - err - template - fill/.same entity in the output were not misinterpreted as references to distinct entities ) .", "entities": []}, {"text": "The task was challenging : information relevant for a single event could be scattered across the document or repeated in multiple places ; relevant information might need to be shared across multiple events ; information regarding different events could be intermingled .", "entities": []}, {"text": "In Figure 1 , for example , theDISEASE \" Newcastle \" is mentioned well before its associated event is mentioned ( via the triggering phrase \" the disease has killed \" ) ; that same mention of \" Newcastle \" must again be recognized as the DISEASE in a second event ; and the COUNTRY of the \ufb01rst event ( \" Honduras \" ) appears only in the sentence describing the second event .", "entities": []}, {"text": "In fact , the problem of document - level information extraction has only recently begun to be revisited ( Quirk and Poon , 2017 ; Jain et al . , 2020 ;", "entities": []}, {"text": "Du et al . , 2021b , a ; Li et al . , 2021 ; Du , 2021 ; Yang et al . , 2021 ) in part in an attempt to test the power of end - to - end neural network techniques that have been so successful on their sentence - level counterparts.2Evaluation , however , has been limited in a number of ways .", "entities": []}, {"text": "First , despite the relative complexity of the task , approaches are only evaluated with respect to their overall performance scores ( e.g. precision , recall , and F1 ) .", "entities": [[28, 29, "MetricName", "F1"]]}, {"text": "Even though scores at the role level are sometimes included , no systematic analysis or characterization of the types of errors that occur is typically done .", "entities": []}, {"text": "The latter is needed to determine strategies to improve performance , to obtain more informative cross - system and cross - genre comparisons , and to identify and track broader advances in the \ufb01eld as the underlying approaches evolve .", "entities": []}, {"text": "To date , for example , there has been no attempt to directly compare the error landscape and distribution of 2See , for example , Zhang et al .", "entities": []}, {"text": "( 2019 ) , Du and Cardie ( 2020 ) and Lin et al .", "entities": []}, {"text": "( 2020 ) for within - sentence event extraction ;", "entities": [[7, 9, "TaskName", "event extraction"]]}, {"text": "Akbik et al . ( 2018 ) , and Akbik et al . ( 2019 ) for named entity recognition ( NER ) ; and Zhang et al . ( 2018 ) and Luan et al .", "entities": [[17, 20, "TaskName", "named entity recognition"], [21, 22, "TaskName", "NER"]]}, {"text": "( 2019 ) for sentence - level relation extraction.3960", "entities": []}, {"text": "200006221022 [ Trigger ]", "entities": []}, {"text": "[ Trigger]Input DocumentCountryHonduras                  DiseaseNewcastleVictimsclose to half a million Honduran chickensStatusconfirmedCountryGuatemalaDiseaseNewcastleVictimsStatusconfirmedCountryNewcastle       ( x)Disease ( x)Victimsclose to half a million Honduran chickensStatusconfirmedDesired T emplate(s)Predicted T emplate(s)Our", "entities": []}, {"text": "Automatic Error Analysis SystemMissing Template(s)1 ......", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "Error Statistics The Agriculture ministers of El Salvador and Honduras ... to control the spread of disease affecting poultry , like the virus Newcastle[Disease ] .", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "Urrutia ... to study the Newcastle outbreak .", "entities": []}, {"text": "The disease has killed close to half a million Honduran chickens[Victims ] in recent weeks .", "entities": []}, {"text": "Honduras[Country ] said this week it would halt the importation of chickens and eggs from Guatemala[Country ] , where the disease has been detected earlier , and ..... \u2014 \ufffd Figure 1 : The document - level extraction task from the ProMED dataset on disease outbreaks ( left ) and the automatic error analysis process ( right ) .", "entities": []}, {"text": "Our system performs a set of transformations on the predicted templates to convert them into the corresponding gold standard templates .", "entities": []}, {"text": "Transformation steps are mapped to corresponding error types to produce informative error statistics .", "entities": []}, {"text": "newly developed neural IE methods with that of the largely hand - crafted systems of the 1990s .", "entities": []}, {"text": "In this work , we \ufb01rst introduce a framework for automating error analysis for document - level event and relation extraction , casting both as instances of a general role-\ufb01lling , or template-\ufb01lling task ( Jurafsky and Martin , 2021 ) .", "entities": [[19, 21, "TaskName", "relation extraction"]]}, {"text": "Our approach converts predicted system outputs into their gold standard counterparts through a series of template - level transformations ( Figure 2 ) and then maps combinations of transformations into a collection of IE - based error types .", "entities": []}, {"text": "Examples of errors include duplicates , missing and spurious role \ufb01llers , missing and spurious templates , and incorrect role and template assignments for \ufb01llers .", "entities": []}, {"text": "( See Figure 3 for the full set ) .", "entities": []}, {"text": "Next , we employ the error analysis framework in a comparison of two state - of - the - art documentlevel neural template-\ufb01lling approaches , DyGIE++ ( Wadden et al . , 2019 ) and GTT ( Du et al . , 2021b ) , across three template-\ufb01lling datasets ( SciREX , ProMED ( Patwardhan and Riloff , 2009)3 , and MUC-4 ) .", "entities": [[50, 51, "DatasetName", "SciREX"], [61, 62, "DatasetName", "MUC-4"]]}, {"text": "Finally , in an attempt to gauge progress in the information extraction \ufb01eld over the past 30 years , we employ the framework to compare the performance of four of the original MUC-4 systems with the two newer deep - learning approaches to documentlevel IE.4We \ufb01nd that ( 1 ) the best of the early IE models \u2014 which strikes a better balance between 3http://www.promedmail.org 4The 1992 model outputs are available in the MUC-4 dataset released by NIST , available at https : //www - nlpir.nist.gov / related_projects/ muc / muc_data / muc_data_index.html.precision and recall \u2014 outperforms modern models that exhibit much higher precision and much lower recall ; ( 2 ) the modern neural models make more mistakes on scienti\ufb01c vs. news - oriented texts , and missing role \ufb01llers is universally the largest source of errors ; and ( 3 ) modern models have clear advantages over the early IE systems in terms of accurate span extraction , while the early systems make fewer mistakes assigning role \ufb01llers to their roles .", "entities": [[32, 33, "DatasetName", "MUC-4"], [73, 74, "DatasetName", "MUC-4"]]}, {"text": "2 Related Work Aside from the original MUC-4 evaluation scoring reports ( Chinchor , 1991 ) , which included counts of missing and spurious role \ufb01ller errors , there have been very few attempts at understanding the types of errors made by IE systems and grounding those errors linguistically .", "entities": [[7, 8, "DatasetName", "MUC-4"]]}, {"text": "Valls - Vargas et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2017 ) proposed a framework for studying how different errors propagate through an IE system ; however , the framework can only be used for pipelined systems , not end - to - end ones .", "entities": []}, {"text": "On the other hand , automated error analysis with linguistically motivated error types has been used in other sub-\ufb01elds of NLP such as machinetranslation ( Vilar et al . , 2006 ;", "entities": []}, {"text": "Zhou et", "entities": []}, {"text": "al . , 2008 ; Farr\u00fas et al . , 2010 ; Kholy and Habash , 2011 ; Zeman et al . , 2011 ; Popovi \u00b4 c and Ney , 2011 ) , coreference resolution ( Uryupina , 2008 ; Kummerfeld and Klein , 2013 ; Martschat and Strube , 2014 ; Martschat et al . , 2015 ) and parsing ( Kummerfeld et al . , 2012 )", "entities": [[34, 36, "TaskName", "coreference resolution"]]}, {"text": ".", "entities": []}, {"text": "Recently , generalized automated error analysis frameworks involving human - in - the - loop testing like Errudite ( Wu et al . , 2019 ) , CHECK -3961", "entities": []}, {"text": "LIST(Ribeiro et", "entities": []}, {"text": "al . , 2020 ) , CrossCheck ( Arendt et al . , 2021 ) , and AllenNLP Interpret ( Wallace et al . , 2019 ) have successfully been applied to tasks like machine comprehension and relation extraction ( Alt et al . , 2020 ) .", "entities": [[8, 9, "DatasetName", "Arendt"], [37, 39, "TaskName", "relation extraction"]]}, {"text": "Closest to our work are Kummerfeld et al .", "entities": []}, {"text": "( 2012 ) and Kummerfeld and Klein ( 2013 ) , which use model - agnostic transformationbased mapping approaches to automatically obtain error information in the predicted structured output .", "entities": []}, {"text": "3 Template - Filling Task Speci\ufb01cation and Evaluation As in Jurafsky and Martin ( 2021 ) , we will refer to document - level information extraction tasks as template-\ufb01lling tasks and use the term going forward to refer to both event extraction and documentlevel relation extraction tasks .", "entities": [[40, 42, "TaskName", "event extraction"], [44, 46, "TaskName", "relation extraction"]]}, {"text": "Given a document , D , and an IE template speci\ufb01cation consisting of a predetermined list of roles R1 ; R2 ; : : : ; R iassociated with each type of relevant event for the task of interest , the goal for template \ufb01lling is to extract from D , one output template , Tfor every relevant event / relation e1 ; e2 ; : : : ; e n present in the document .", "entities": []}, {"text": "Notably , in the general case , n\u00150and is not speci\ufb01ed as part of the input .", "entities": []}, {"text": "In each output template , its roles are \ufb01lled with the corresponding role \ufb01ller(s ) , which can be inferred or extracted from the document depending on the predetermined role types .", "entities": []}, {"text": "We consider two role types here:5 Set-\ufb01ll roles , which must be \ufb01lled with exactly one role \ufb01ller from a \ufb01nite set supplied in the template speci\ufb01cation .", "entities": []}, {"text": "An example of a set-\ufb01ll role in Figure 1 is STATUS , which can be confirmed , possible , orsuspected .", "entities": []}, {"text": "String-\ufb01ll roles , whose role \ufb01ller(s ) are spans extracted from the document , or left empty if no corresponding role \ufb01ller is found in the document .", "entities": []}, {"text": "VICTIMS , DISEASE andCOUNTRY are string-\ufb01ll roles in Figure 1 .", "entities": []}, {"text": "Some string-\ufb01ll roles allow multiple \ufb01llers ; for example , there might be more than oneVICTIMS .", "entities": []}, {"text": "Importantly , for document - level template \ufb01lling , exactly one string should be included for each role \ufb01ller entity ( typically a canonical mention of the entity ) , i.e. coreferent mentions of the same entity are not permitted .", "entities": []}, {"text": "Evaluation .", "entities": []}, {"text": "We use the standard ( exact - match ) F1 score ( Chinchor , 1991 ) to evaluate the output 5There are potentially more role types depending on the dataset ( e.g. normalized dates , times , locations ) ; we will not consider those here.produced by a template-\ufb01lling system : F1 = 2\u0001Precision \u0001Recall Precision + Recall 4 Methodology : Automatic Transformations for Error Analysis Similar to the work of Kummerfeld and Klein ( 2013 ) , our error analysis approach is systemagnostic , i.e. it only uses system output and does not consider intermediate system decisions .", "entities": [[9, 11, "MetricName", "F1 score"], [51, 52, "MetricName", "F1"], [55, 56, "MetricName", "Precision"], [57, 58, "MetricName", "Recall"], [64, 65, "MetricName", "Error"]]}, {"text": "This allows for error analysis and comparison across different kinds of systems \u2014 end - to - end or pipeline ; neural or pattern - based .", "entities": []}, {"text": "Given inputs consisting of the system - predicted templates and gold standard templates ( i.e. desired output ) for every document in the target dataset , our error analysis tool operates in three steps .", "entities": []}, {"text": "For each document , 1.Perform an optimized mapping of the associated predicted templates and gold templates .", "entities": []}, {"text": "2.Apply a pre - de\ufb01ned set of transformations to convert each system - predicted template into the desired gold template , keeping track of the transformations applied .", "entities": []}, {"text": "3.Map", "entities": []}, {"text": "the changes made in the conversion process to an IE - based set of error types .", "entities": []}, {"text": "We describe each step in detail in the subsections below .", "entities": []}, {"text": "4.1 Optimized Matching The \ufb01rst stage of the error analysis tool involves matching each system - predicted template to the best - matching gold template for each document in the dataset .", "entities": []}, {"text": "In particular , the overall F1 score for a given document can vary based on how a predicted template is individually matched with a gold template ( or left unmatched ) .", "entities": [[5, 7, "MetricName", "F1 score"]]}, {"text": "Speci\ufb01cally , for each document , we recursively generate all possible template matchings \u2014 where each predicted template is matched ( if possible ) to a gold template .", "entities": []}, {"text": "In particular , for a document with Ppredicted templates and Ggold templates , the total number of possible template matchings is : 1 +   P 1 ! G+   P 2 ! G(G\u00001 ) + : : : + G !", "entities": []}, {"text": "( G\u0000P)!;ifG\u0000P\u00150 1 +   P 1 ! G+   P 2 ! G(G\u00001 )", "entities": []}, {"text": "+ : : : +   P G !", "entities": []}, {"text": "G!;ifG\u0000P <", "entities": []}, {"text": "0", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "= min ( P;G ) X i=0   P i !", "entities": []}, {"text": "G !", "entities": []}, {"text": "( G\u0000i)!3962", "entities": []}, {"text": "R1 : R2 : R3 : R1 : R2 : R3 : R1 : R2 : R3 : R1 : R2 : R3 : Remove Spurious   T emplate(s)Alter", "entities": []}, {"text": "Mention span(s)Dupl.+Desired T emplate(s)Remove Role Filler(s)Introduce Role Filler(s ) R1 : R2 : R3 : Predicted T emplate(s)Alter Role(s)Dupl .", "entities": []}, {"text": "Dupl .", "entities": []}, {"text": "Dupl .", "entities": []}, {"text": "R1 : R2 : R3 : Figure 2 : Automatic transformations to convert predicted templates ( on the left ) to gold templates ( on the right ) .", "entities": []}, {"text": "Arrows represent transformations .", "entities": []}, {"text": "Colored circles represent role \ufb01ller entity mentions .", "entities": []}, {"text": "Dupl . stands for duplicate .", "entities": []}, {"text": "Note that template matching can result in unmatched predicted templates ( Spurious Templates ) , as well as unmatched gold templates ( Missing Templates ) .", "entities": []}, {"text": "Next , for each predicted - gold pair in a template matching , we iterate through all its roles and recursively generate all possible mention matchings , in each of which a predicted role \ufb01ller is matched ( if possible ) to a set of coreferent gold role \ufb01llers .", "entities": []}, {"text": "Similar to template matching , the process of mention matching can also result in unmatched predicted role \ufb01llers ( Spurious Role Fillers ) and unmatched coreferent sets of gold role \ufb01llers ( Missing Role Fillers ) .", "entities": []}, {"text": "Through the process , each predicted role \ufb01ller increases the denominator of the total precision by 1 , and each set of coreferent gold role \ufb01llers increases the denominator of total recall by 1 .", "entities": []}, {"text": "Whenever there is a matched mention pair in which the predicted role \ufb01ller has an exact match to an element of the set of coreferent gold role \ufb01llers , this adds 1 to the numerator of both precision and recall .", "entities": [[15, 17, "MetricName", "exact match"]]}, {"text": "These counts are calculated for each template matching .", "entities": []}, {"text": "Using precision and recall , the total F1 score across all the slots / roles is calculated and the template matching with the highest total F1 score is chosen .", "entities": [[7, 9, "MetricName", "F1 score"], [25, 27, "MetricName", "F1 score"]]}, {"text": "If there are ties , the template matching with the fewest errors is chosen ( see Section 4.3 ) .", "entities": []}, {"text": "4.2 Transformations The second part of the error analysis tool involves changing the predicted templates to the desired gold templates with the help of a \ufb01xed set of transformations as detailed below .", "entities": []}, {"text": "\u2022Alter Span transforms a role \ufb01ller into the gold role \ufb01ller with the lowest span comparison score ( SCS ) .", "entities": []}, {"text": "The tool provides twooptions for computing the SCS between two spans , and each depends only on the starting and ending indices of the spans.6SCS can be interpreted as distance and is 0 between two identical spans , and 1 for non - overlapping spans .", "entities": [[32, 33, "DatasetName", "0"]]}, {"text": "The two modes are given as follows : a)absolute :", "entities": []}, {"text": "This mode captures the ( positive ) distance between the starting ( and ending ) character offsets of spans xandyin the document , and scales that value by the sum of the lengths of xandy , capping it at a maximum of 1 .", "entities": []}, {"text": "SCS = max\u0010 1;jxstart\u0000ystartj+jxend\u0000yendj length ( x)+length ( y)\u0011 b)geometric mean : This mode captures the degree of disjointedness between spans xandyby dividing the length of the overlap between the two spans with respect to each of their lengths , multiplying those two fractions , and subtracting the \ufb01nal result from 1 .", "entities": []}, {"text": "Ifsiis the length of the intersection of xandy , and neither xnoryhave length 0 , SCS is calculated as shown below ; otherwise , SCS is 1 . overlap = min ( xend ; yend)\u0000max ( xstart ; ystart )", "entities": [[13, 14, "DatasetName", "0"]]}, {"text": "si = max ( 0 ; overlap ) SCS = 1\u0000   si2 length ( x)\u0003length ( y ) !", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "Thus , if the predicted role \ufb01ller is an exact match for the gold role \ufb01ller , the SCS is 0 .", "entities": [[9, 11, "MetricName", "exact match"], [20, 21, "DatasetName", "0"]]}, {"text": "If there is some overlap between the spans , the 6This deviates from Kummerfeld and Klein ( 2013 ) , in which incorrect spans are altered to gold mentions that have the same head token , requiring the use of a syntactic parser.3963", "entities": []}, {"text": "Error TypeError ComponentError NameTransformations(s)PredictedGoldMis - placementSpan Errori)Within T emplate \u25a0 Span ErrorAlter SpanPerInd :", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "[ members]PerpInd : [ members of the maoist terrorist organization shining path]ii)Duplicate Role FillerRemove Role FillerTarget : [ electrical appliance store ] , [ store]Target : [ electrical appliance store , store]iii) \u25a0 Duplicate Partially Matched Role FillerAlter Span   + Remove Role FillerTarget : [ store ] , [ electrical]Target : [ store , electrical appliance store]iv ) \u25a0 SpuriousSpurious Role FillerRemove Role FillerPerpOrg : [ fmln]Victim : [ rosa imelda gonzalez medrano]PerpOrg : \u2014 Victim : [ rosa imelda gonzalez medrano]v ) \u25a1 ( Missing)Missing Role FillerIntroduce Role FillerTarget : \u2014 Target : [ local garrison , garrison]vi ) \u25a0 Role ErrorIncorrect RoleAlter RolePerpInd : \u2014 Victim : [ gonzalo rodriguez gacha]PerpInd : [ gonzalo rodriguez gacha ] Victim : \u2014 vii) \u25a0 Incorrect Role + Partially Matched FillerAlter Span   + Alter RolePerpInd : \u2014 Victim : [ gonzalo rodriguez]PerpInd : [ gonzalo rodriguez gacha ] Victim : \u2014 viii)Within and Cross T emplate \u25a0 T emplate ErrorWrong T emplate Role FillerRemove Cross T emplate Spurious Role FillerT1 : Target : [ public bus ] T2 : Target : \u2014 T1 : Target :   \u2014 T2 : Target : [ public bus , bus]ix) \u25a0 Wrong T emplate For Partially Matched Role FillerAlter Span   + Remove Cross T emplate Spurious Role FillerT1 : Target : [ public]T2 : Target : \u2014 T1 : Target : \u2014 T2 : Target : [ public bus , bus]x ) \u25a0 \u25a0 \u2028 Role + T emplate ErrorWrong T emplate + Wrong RoleAlter Role   + Remove Cross T emplate Spurious Role FillerT1 : Victim : \u2014 Weapon : \u2014 T2 : Victim : [ adolfo spezua]Weapon :", "entities": []}, {"text": "[ thomas pellisier]T1 : Victim : [ thomas pellisier]Weapon : \u2014 T2 : Victim : [ adolfo spezua]Weapon : \u2014 xi) \u25a0 Wrong T emplate + Wrong Role + Partially Matched FillerAlter Span   + Alter Role   + Remove Cross T emplate Spurious Role FillerT1 : Victim :   \u2014 Weapon : \u2014 T2 : Victim : [ adolfo spezua ] Weapon :", "entities": []}, {"text": "[ thomas]T1 : Victim : [ thomas pellisier ] Weapon : \u2014 T2 : Victim : [ adolfo spezua ]", "entities": []}, {"text": "Weapon : \u2014 xii)T emplateDetection \u25a0 SpuriousSpurious T emplateRemove T emplateT1 : PerpOrg :", "entities": []}, {"text": "[ fmln]\u2014xiii ) \u25a1 ( Missing)Missing T emplateIntroduce T emplate \u2014 T1 : PerpOrg :", "entities": []}, {"text": "[ fmln ] 1Figure 3 : Error Types with examples from the MUC-4 dataset .", "entities": [[6, 7, "MetricName", "Error"], [12, 13, "DatasetName", "MUC-4"]]}, {"text": "For each template , in every role , the role \ufb01llers in brackets refer to the same entity , while role \ufb01llers in different brackets refer to different entities .", "entities": []}, {"text": "The underlined text indicates the error in the prediction .", "entities": []}, {"text": "SCS is between 0 and 1 ( not inclusive ) , and if there is no overlap between the spans , the SCS is 1 .", "entities": [[3, 4, "DatasetName", "0"]]}, {"text": "The order of comparison of the spans does n\u2019t change the SCS score for both modes .", "entities": []}, {"text": "As the absolute mode is less sensitive to changes in span indices as compared to the geometric mean , we chose geometric mean for our analysis , as tiny changes in index positions result in a bigger change in the SCS score .", "entities": []}, {"text": "\u2022Alter Role changes the role of a role \ufb01ller toanother role within the same template .", "entities": []}, {"text": "\u2022Remove Duplicate Role Filler removes a role \ufb01ller that is coreferent to an already matched role \ufb01ller .", "entities": []}, {"text": "\u2022Remove Cross Template Spurious Role Filler removes a role \ufb01ller that would be correct if present in another template ( in the same role ) .", "entities": []}, {"text": "\u2022Remove Spurious Role Filler removes a role \ufb01ller that has not been mentioned in any of the gold templates for a given document.3964", "entities": []}, {"text": "\u2022Introduce Role Filler introduces a role \ufb01ller that was not present in the predicted template but was required to be present in the matching gold template .", "entities": []}, {"text": "\u2022Remove Template removes a predicted template that could not be matched to any gold template for a given document .", "entities": []}, {"text": "\u2022Introduce Template introduces a template that can be matched to an unmatched gold template for a given document .", "entities": []}, {"text": "For a given document , all singleton Alter Span andAlter Role transformations , as well as sets ofAlter Span + Alter Role transformations , are applied \ufb01rst .", "entities": []}, {"text": "The other transformations are applied in the order in which they were detected , which is dependent on the order of predicted and gold template pairs in the optimized matching and the order of the slots / roles in the template .", "entities": []}, {"text": "4.3 Error Type Mappings", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "The transformations in Section 4.2 are mapped onto a set of IE - speci\ufb01c error types as shown in Figure 3 .", "entities": []}, {"text": "In some cases , a single transformation maps onto a single error , while in others a sequence of transformations is associated with a single error .", "entities": []}, {"text": "Full details are in Appendix A. 5 Document - level IE Datasets Our experiments employ three document - level information extraction datasets .", "entities": []}, {"text": "We brie\ufb02y describe each below .", "entities": []}, {"text": "Dataset statistics are summarized in Table 1 . MUC-4 ( MUC-4 , 1992 ) consists of newswire describing terrorist incidents in Latin America provided by the FBIS ( Federal Broadcast Information Services ) .", "entities": [[8, 9, "DatasetName", "MUC-4"], [10, 11, "DatasetName", "MUC-4"]]}, {"text": "We converted the optional templates to required templates and removed the subtypes of the incidents as done in previous work ( Chambers , 2013 ; Du et al . , 2021b ) so that the dataset is transformed into standardized templates .", "entities": []}, {"text": "The roles chosen from the MUC-4 dataset are PERPIND ( individual perpetrator ) , PERPORG(organization perpetrator ) , TARGET ( physical target ) , VICTIM ( human target ) , and WEAPON which are all string-\ufb01ll roles , as well as INCIDENT TYPE which is a set-\ufb01ll role with six possible role \ufb01llers : attack , kidnapping , bombing , arson , robbery , andforced work stoppage .", "entities": [[5, 6, "DatasetName", "MUC-4"]]}, {"text": "As seen in Table 1 , 44.59 % of the documents have no templates , which makes the identi\ufb01cation of relevant vs. irrelevant documents critical to the success of any IE model for this dataset .", "entities": []}, {"text": "ProMED8(Patwardhan and Riloff , 2009 ) consists of just 125 annotated tuning examples and 120 annotated test examples , describing global disease outbreaks by subject matter experts from ProMED .", "entities": []}, {"text": "We use the tuning data as training data and reserve 10 % of the test data , i.e. 12 examples , to create a development / validation set .", "entities": []}, {"text": "19.83 % of the documents in the dataset have no templates .", "entities": []}, {"text": "The roles that we extract from the dataset are STATUS , COUNTRY , DISEASE , and VICTIMS .DISEASE , VICTIMS , andCOUNTRY are string-\ufb01ll roles9;STATUS is a set-\ufb01ll role with confirmed , possible , andsuspected as the possible role \ufb01ller options .", "entities": []}, {"text": "SciREX ( Jain et al . , 2020 ) consists of annotated computer science articles from Papers with Code10 .", "entities": [[0, 1, "DatasetName", "SciREX"]]}, {"text": "We focus speci\ufb01cally on its 4 - ary relation extraction subtask .", "entities": [[5, 10, "TaskName", "4 - ary relation extraction"]]}, {"text": "The roles present in each relation areMATERIAL ( DATASET ) , METRIC , TASK , and METHOD which are all string-\ufb01lls .", "entities": []}, {"text": "We convert the dataset from its original format to templates for our models , and remove individual role \ufb01llers ( entities ) that have no mentions in the text.11We also remove any duplicate templates.12During preprocessing , we remove malformed words longer than 25 characters , as the majority of these consist of concatenated words that are not present in the corresponding text .", "entities": []}, {"text": "6 IE Modeling Details In our experiments , we train and test two neuralbased IE models , described brie\ufb02y below , on the MUC-4 , ProMED , and SciREX datasets .", "entities": [[23, 24, "DatasetName", "MUC-4"], [28, 29, "DatasetName", "SciREX"]]}, {"text": "Note that 8http://www.promedmail.org 9In the ProMED dataset , COUNTRY is a set-\ufb01ll role , but since countries are explicitly mentioned in most of the documents , we can treat this role as a string-\ufb01ll .", "entities": []}, {"text": "10https://paperswithcode.com 11According to Jain et .", "entities": []}, {"text": "al . , around 50 % of relations in the SciREX dataset contain one or more role \ufb01llers that do not appear in the corresponding text .", "entities": [[10, 11, "DatasetName", "SciREX"]]}, {"text": "These relations are removed during evaluation for our end - to - end task .", "entities": []}, {"text": "https://github.com/allenai/SciREX/blob/master/README.md 12Removing unmentioned entities sometimes eliminates differences between templates .", "entities": []}, {"text": "This results in some templates becoming identical or making some templates contain information that is a subset of the information present in another template .", "entities": []}, {"text": "Thus , we only keep one of these processed templates.3965", "entities": []}, {"text": "# docs ( train / val / test / unannot . ) # tokens per doc ( min / max / avg . ) # templates per relevant doc ( max / avg.)% docs with 0 templates - MUC-4 ( MUC-4 , 1992)1300 / 200 / 200 / 0 31 / 1695 / 362 14 / 1.61 44.59 - ProMED7125 / 12 / 108 / 4979 57 / 4417 / 621 9 / 1.55 19.83 - SciREX ( Jain et al . , 2020)304 / 66 / 66 / 0 1153 / 13155 / 5401 16 / 2.28 0.00 Table 1 : Dataset Statistics .", "entities": [[35, 36, "DatasetName", "0"], [38, 39, "DatasetName", "MUC-4"], [40, 41, "DatasetName", "MUC-4"], [48, 49, "DatasetName", "0"], [76, 77, "DatasetName", "SciREX"], [89, 90, "DatasetName", "0"]]}, {"text": "A relevant document has one or more templates .", "entities": []}, {"text": "to create the training data for both the DyGIE++ and GTT models , we use the \ufb01rst mention of each role \ufb01ller in the document as the mention to be extracted .", "entities": []}, {"text": "DyGIE++ with Clustering We use DyGIE++ \u2014 a span - based , sentence - level extraction model \u2014 to identify role \ufb01llers in the document and associate them with certain role types .", "entities": []}, {"text": "During training , the maximum span length enumerated by the model is set to 8 tokens as in Wadden et al .", "entities": []}, {"text": "( 2019 ) for the SciREX dataset and 11 tokens for the ProMED dataset .", "entities": [[5, 6, "DatasetName", "SciREX"]]}, {"text": "We use bert - base - cased andallenai / scibert_scivocab_uncased for the base BERT and SciBERT models respectively , which both have a maximum input sequence length of 512 tokens .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "To aggregate entities detected by DyGIE++ into templates , we use a clustering algorithm .", "entities": []}, {"text": "For the SciREX dataset , we adopt a heuristic approach that assumes there is only one template per document , and in that template , we assign the named entities predicted by DyGIE++ for a document to the predicted role types .", "entities": [[2, 3, "DatasetName", "SciREX"]]}, {"text": "For the ProMED dataset , we use a different clustering heuristic that ensures that each template has exactly one role \ufb01ller for the COUNTRY andDISEASE roles , as detailed in the dataset annotation guidelines .", "entities": []}, {"text": "Also , since STATUS has the value confirmed in the majority of the templates , every template predicted has its STATUS assigned as confirmed .", "entities": []}, {"text": "GTT is an end - to - end document - level templategenerating model .", "entities": []}, {"text": "For the MUC-4 and SciREX datasets , GTT is run for 20 epochs , while for ProMED it is run for 36 epochs , to adjust for the smaller size of the dataset .", "entities": [[2, 3, "DatasetName", "MUC-4"], [4, 5, "DatasetName", "SciREX"]]}, {"text": "All other hyperparameters are set as in Du et al .", "entities": []}, {"text": "( 2021b ) .", "entities": []}, {"text": "We use the same BERT and SciBERT base models as described in the DyGIE++ architecture above , both with a maximum input sequence length of 512 tokens .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "The computational budget and optimal hyperparameters for these models can be found in Ap - pendix sections D and E , respectively .", "entities": []}, {"text": "7 Experimental Results and Analysis We \ufb01rst discuss the results of DyGIE++ and GTT on SciREX , ProMED , and MUC-4 ; and then examine the performance of these newer neural models on the 1992 MUC-4 dataset vs. a few of the bestperforming IE systems at the time .", "entities": [[15, 16, "DatasetName", "SciREX"], [20, 21, "DatasetName", "MUC-4"], [35, 36, "DatasetName", "MUC-4"]]}, {"text": "7.1 DyGIE++ vs. GTT Table 2 shows the results of evaluating DyGIE++ and GTT on the SciREX , ProMED , and MUC4 datasets .", "entities": [[16, 17, "DatasetName", "SciREX"]]}, {"text": "We can see that all models perform substantially worse on scienti\ufb01c texts ( ProMED , SciREX ) as compared to news ( MUC-4 ) , likely because the model base is pretrained for generalpurpose NLP applications ( BERT ) or there are not enough examples of scienti\ufb01c - style text in the pretraining corpus ( SciBERT ) .", "entities": [[15, 16, "DatasetName", "SciREX"], [22, 23, "DatasetName", "MUC-4"], [37, 38, "MethodName", "BERT"]]}, {"text": "In addition , models seem to perform better on the news - style ProMED dataset than the scienti\ufb01c - paper - based long - text SciREX dataset .", "entities": [[25, 26, "DatasetName", "SciREX"]]}, {"text": "This can be explained by the fact that all four models handle a maximum of 512 tokens as inputs , while the average length of a SciREX document is 5401 tokens .", "entities": [[26, 27, "DatasetName", "SciREX"]]}, {"text": "Thus , a majority of the text is truncated and , hence , unavailable to the models .", "entities": []}, {"text": "Nevertheless , we see an increase in F1 scores for all SciBERT - based models when compared to their BERT counterparts for the SciREX dataset .", "entities": [[7, 8, "MetricName", "F1"], [19, 20, "MethodName", "BERT"], [23, 24, "DatasetName", "SciREX"]]}, {"text": "The same trend is seen for DyGIE++ for ProMED , but not for GTT .", "entities": []}, {"text": "This can be explained by the fact that GTT ( SciBERT ) has more Missing Template errors than GTT ( BERT ) .", "entities": [[20, 21, "MethodName", "BERT"]]}, {"text": "So even if GTT ( SciBERT ) performs better on the scienti\ufb01c slot VICTIMS , i.e. it extracts more scienti\ufb01c information , it does not identify relevant events as well as GTT ( BERT ) , reducing the F1 score across the remaining slots .", "entities": [[33, 34, "MethodName", "BERT"], [38, 40, "MetricName", "F1 score"]]}, {"text": "From the error count results in Figure 4 , we see that GTT makes fewer Missing Template errors than DyGIE++ on the MUC-4 dataset ( 86 vs. 97 ) .", "entities": [[22, 23, "DatasetName", "MUC-4"]]}, {"text": "However , there is no signi\ufb01cant difference3966", "entities": []}, {"text": "SciREX ProMED MUC-4 DyGIE++ ( BERT ) 22.47 % 35.01 % 45.79 % DyGIE++ ( SciBERT ) 25.39 % 38.15 % GTT ( BERT ) 21.54 % 44.64 % 49.00 % GTT ( SciBERT ) 27.68 % 42.96 % Table 2 : F1 Scores for the Neural Models on SciREX , ProMED , and MUC-4 Precision Recall F1 GE NLToolset 56.69 % 52.09 % 54.29 % NYU PROTEUS 34.23 % 31.28 % 32.69 % SRI FASTUS 48.47 % 38.42 % 42.86 % UMass CIRCUS 48.62 % 39.04 % 43.30 % GTT ( BERT ) 63.18 % 40.02 % 49.00 % DyGIE++ ( BERT ) 61.90 % 36.33 % 45.79 % Table 3 : Precision , Recall , and F1 scores for models on the MUC-4 dataset .", "entities": [[0, 1, "DatasetName", "SciREX"], [2, 3, "DatasetName", "MUC-4"], [5, 6, "MethodName", "BERT"], [23, 24, "MethodName", "BERT"], [42, 43, "MetricName", "F1"], [49, 50, "DatasetName", "SciREX"], [54, 55, "DatasetName", "MUC-4"], [55, 56, "MetricName", "Precision"], [56, 57, "MetricName", "Recall"], [57, 58, "MetricName", "F1"], [92, 93, "MethodName", "BERT"], [102, 103, "MethodName", "BERT"], [113, 114, "MetricName", "Precision"], [115, 116, "MetricName", "Recall"], [118, 119, "MetricName", "F1"], [124, 125, "DatasetName", "MUC-4"]]}, {"text": "The \ufb01rst four models were developed in 1992 , while the last two models are recent and use neural - based methods .", "entities": []}, {"text": "in the number of missing templates between the two models on the ProMED and SciREX datasets .", "entities": [[14, 15, "DatasetName", "SciREX"]]}, {"text": "This could be because DyGIE++ is prone to overgeneration \u2014 there are signi\ufb01cantly more Spurious Role Filler and Spurious Template errors as compared to the results of GTT .", "entities": []}, {"text": "Since we use heuristics that create templates based on the extracted role \ufb01llers , this increases the probability that there was a possible match to a gold template , reducing the number of Missing Template Errors .", "entities": []}, {"text": "We can also conclude that DyGIE++ is worse at coreference resolution when compared to GTT as DyGIE++ makes more Duplicate Role Filler errors across all datasets .", "entities": [[9, 11, "TaskName", "coreference resolution"]]}, {"text": "Overall , we \ufb01nd that the major source of error for both GTT and DyGIE++ across all the datasets is missing recall in the form of Missing Role Filler and Missing Template errors .", "entities": []}, {"text": "7.2 Early IE Models vs. DyGIE++ and GTT Table 3 presents the precision , recall , and F1 performance on the MUC-4 dataset for early models from 1992 alongside those of the more recent DyGIE++ and GTT models .", "entities": [[17, 18, "MetricName", "F1"], [21, 22, "DatasetName", "MUC-4"]]}, {"text": "We summarize key \ufb01ndings below .", "entities": []}, {"text": "The best of the early models ( GE NLToolset ) performs better than either of the modern models .", "entities": []}, {"text": "It does so by doing a better job balancing precision and recall , whereas GTT and DyGIE++ exhibit much higher precision and much lower recall .", "entities": []}, {"text": "Predicted Gold Match power lines along the roadpower lines enrique ruiz , retired enrique ruiz maoist shining path groupshining path group of unidenti\ufb01ed individuals who hurled a bomb ... passing vehiclegroup of unidenti\ufb01ed individuals Table 4 : Span Errors in early models .", "entities": []}, {"text": "The differences between the predicted mention and its best gold mention match according to our analysis tool are in bold .", "entities": []}, {"text": "The early models have more span errors than the modern DyGIE++ and GTT models .", "entities": []}, {"text": "The representative kinds of span errors from the 1992 model outputs are shown in Table 4 .", "entities": []}, {"text": "One interesting difference between the span errors in the early models and the modern models is that the predicted mentions include longer spans with more information than is indicated in the best gold mention match .", "entities": []}, {"text": "Some could be due to errors in dataset annotation ; for example , maoist shining path group versus shining path but a signi\ufb01cant number of the span errors occur as the early models seem to extract the entire sentence or clause which contains the desired role \ufb01ller mention .", "entities": []}, {"text": "The modern models tend to leave off parts of the desired spans , and if they do predict larger spans than required , are only off by a few words .", "entities": []}, {"text": "The early models have fewer Missing Template and Missing Role Filler errors as compared to the modern models .", "entities": []}, {"text": "However , the former also have more Spurious Template and Spurious Role Filler errors than the latter , indicating these models mitigate the issue of Missing Templates through over - generation .", "entities": []}, {"text": "The early models have fewer Incorrect Role errors as compared to modern models .", "entities": []}, {"text": "However , since all the models make relatively few such errors , it suggests that role classi\ufb01cation for predicted mentions is not a major problem for modern models .", "entities": []}, {"text": "The main source of error for both early and modern models is missing recall due to missing templates and missing role \ufb01llers .", "entities": []}, {"text": "This strongly suggests future systems can maximize their performance by being less conservative in3967", "entities": []}, {"text": "Error Counts for Models on the MUC - 4 Dataset ModelsDyGIE++   ( BERT)GTT   ( BERT)GENYUSRIUMassNumber of Errors      02505007501000 Span ErrorDuplicate Role FillerDuplicate ParLally Matched Role FillerSpurious Role FillerMissing Role FillerIncorrect RoleIncorrect Role + ParLally Matched FillerWrong Template Role FillerWrong Template For ParLally Matched Role FillerWrong Template +", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "Wrong RoleWrong Template + Wrong Role +", "entities": []}, {"text": "ParLally Matched Filler Spurious TemplateSpurious Template Role FillerMissing TemplateMissing Template Role FillerWithin TemplateWithin + Cross TemplateTemplate Detection Span Error Duplicate Role Filler Duplicate Partially Matched Role Filler Spurious Role Filler Missing Role Filler Incorrect Role Incorrect Role    + Partially Matched Filler Wrong Template Role Filler Wrong Template    For Partially Matched Role Filler Wrong Template +", "entities": [[18, 19, "MetricName", "Error"]]}, {"text": "Wrong Role Wrong Template + Wrong Role    + Partially Matched Filler Spurious Template Spurious Template Role Filler Missing Template Missing Template Role Filler 1Figure 4 : Automated Error Analysis Results ( Error Counts ) for Models on the MUC-4 dataset .", "entities": [[28, 29, "MetricName", "Error"], [32, 33, "MetricName", "Error"], [39, 40, "DatasetName", "MUC-4"]]}, {"text": "role \ufb01ller detection and focusing on improvement of the recall , even at the expense of potentially decreasing some precision .", "entities": []}, {"text": "8 Limitations and Future Work This work explores subtypes of Spurious Role Filler errors extensively , however , we would like to further analyze Missing Role Filler and templatelevel errors for more \ufb01ne - grained error subtypes and the linguistic reasons behind why they occur .", "entities": []}, {"text": "Due to the pairwise comparisons between all predicted and gold mentions in a role for all pairs of predicted and gold templates in an example , the error analysis tool is slow when the number of both the predicted and gold templates as well as the number of role \ufb01llers in the templates is high .", "entities": []}, {"text": "Thus , we would also like to improve the time complexity of our template ( and mention ) matching algorithms using an approach like bipartite matching ( Yang et al . , 2021 ) .", "entities": []}, {"text": "Currently , the error analysis tool reports exact match precision / recall / F1 which is more suitable for string-\ufb01ll roles .", "entities": [[7, 9, "MetricName", "exact match"], [13, 14, "MetricName", "F1"]]}, {"text": "We would like to extend the tool to further analyze set-\ufb01ll roles by implementing metrics such as false - positive rate .", "entities": []}, {"text": "We used a limited number of models in this paper as we aimed to develop and test the usability of our error analysis tool .", "entities": []}, {"text": "In the future , however , we would like to test our tool on a wider range of models , in addition to running more experiments in order to reach more generalizable conclusions about the behavior of IE models.9 Conclusion As new models for information extraction continue to be developed , we \ufb01nd that their predicted error types contain insights regarding their shortcomings .", "entities": []}, {"text": "Analyzing error patterns within model predictions in a more \ufb01ne - grained manner beyond scores provided by commonly used metrics is important for the progress of the \ufb01eld .", "entities": []}, {"text": "We introduce a framework for the automatic categorization of model prediction errors for document - level IE tasks .", "entities": []}, {"text": "We used the tool to analyze the errors of two state - of - theart models on three datasets from varying domains and compared the error pro\ufb01les of these models to four of the earliest systems in the \ufb01eld on a dataset from that era .", "entities": []}, {"text": "We \ufb01nd that state - of - the - art models , when compared to the earlier manual feature - based models , perform better at span extraction but worse at template detection and role assignment .", "entities": []}, {"text": "With a better balance between precision and recall , the best early model outperforms the relatively highprecision , low - recall modern models .", "entities": []}, {"text": "Missing role \ufb01llers remain the main source of errors , and scienti\ufb01c corpora are the most dif\ufb01cult for all systems , suggesting that improvements in these areas should be a priority for future system development .", "entities": []}, {"text": "Acknowledgments We thank the anonymous reviewers and Ellen Riloff for their helpful comments ( ! ) and Sienna Hu for converting the 1992 model outputs to a format compatible with our error analysis tool .", "entities": []}, {"text": "Our research was supported , in part , by NSF CISE Grant 1815455 and the Cornell CS Department CSURP grants for undergraduate research.3968", "entities": [[15, 16, "DatasetName", "Cornell"], [16, 17, "DatasetName", "CS"]]}, {"text": "References Alan Akbik , Tanja Bergmann , and Roland V ollgraf .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Pooled contextualized embeddings for named entity recognition .", "entities": [[4, 7, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 724\u2013728 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alan Akbik , Duncan Blythe , and Roland V ollgraf .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Contextual string embeddings for sequence labeling .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 1638\u20131649 , Santa Fe , New Mexico , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Christoph Alt , Aleksandra Gabryszak , and Leonhard Hennig . 2020 .", "entities": []}, {"text": "TACRED revisited : A thorough evaluation of the TACRED relation extraction task .", "entities": [[0, 1, "DatasetName", "TACRED"], [8, 9, "DatasetName", "TACRED"], [9, 11, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1558 \u2013 1569 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Dustin Arendt , Zhuanyi Shaw , Prasha Shrestha , Ellyn Ayton , Maria Glenski , and Svitlana V olkova . 2021 .", "entities": [[1, 2, "DatasetName", "Arendt"]]}, {"text": "CrossCheck : Rapid , reproducible , and interpretable model evaluation .", "entities": []}, {"text": "In Proceedings of the Second Workshop on Data Science with Human in the Loop : Language Advances , pages 79\u201385 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nathanael Chambers .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Event schema induction with a probabilistic entity - driven model .", "entities": []}, {"text": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1797\u20131807 , Seattle , Washington , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Nancy Chinchor .", "entities": []}, {"text": "1991 .", "entities": []}, {"text": "MUC-3 evaluation metrics .", "entities": []}, {"text": "In Third Message Uunderstanding Conference ( MUC3 ): Proceedings of a Conference Held in San Diego , California , May 21 - 23 , 1991 .", "entities": []}, {"text": "Xinya Du . 2021 .", "entities": []}, {"text": "Towards More Intelligent Extraction of Information from Documents .", "entities": []}, {"text": "Ph.D. thesis , Cornell University .", "entities": [[3, 4, "DatasetName", "Cornell"]]}, {"text": "Copyright - Database copyright ProQuest LLC ; ProQuest does not claim copyright in the individual underlying works .", "entities": []}, {"text": "Xinya Du and Claire Cardie .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Event extraction by answering ( almost ) natural questions .", "entities": [[0, 2, "TaskName", "Event extraction"], [7, 9, "DatasetName", "natural questions"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 671\u2013683 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xinya Du , Alexander Rush , and Claire Cardie .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "GRIT :", "entities": [[0, 1, "DatasetName", "GRIT"]]}, {"text": "Generative role-\ufb01ller transformers for document - level event entity extraction .", "entities": []}, {"text": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics : Main Volume , pages 634\u2013644 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xinya Du , Alexander Rush , and Claire Cardie .", "entities": []}, {"text": "2021b .", "entities": []}, {"text": "Template \ufb01lling with generative transformers .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 909\u2013914 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mireia Farr\u00fas , Marta R. Costa - juss\u00e0 , Jos\u00e9 B. Mari\u00f1o , and Jos\u00e9 A. R. Fonollosa . 2010 .", "entities": []}, {"text": "Linguisticbased evaluation criteria to identify statistical machine translation errors .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 14th Annual conference of the European Association for Machine Translation , Saint Rapha\u00ebl , France .", "entities": [[12, 14, "TaskName", "Machine Translation"]]}, {"text": "European Association for Machine Translation .", "entities": [[3, 5, "TaskName", "Machine Translation"]]}, {"text": "Ralph Grishman . 2019 .", "entities": []}, {"text": "Twenty-\ufb01ve years of information extraction .", "entities": []}, {"text": "Natural Language Engineering , 25(6):677\u2013692 .", "entities": []}, {"text": "Sarthak Jain , Madeleine van Zuylen , Hannaneh Hajishirzi , and Iz Beltagy . 2020 .", "entities": []}, {"text": "SciREX : A challenge dataset for document - level information extraction .", "entities": [[0, 1, "DatasetName", "SciREX"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7506\u20137516 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Daniel Jurafsky and James H. Martin . 2021 .", "entities": []}, {"text": "Speech and language processing , 3rd ed . draft , chapter 17 , information extraction .", "entities": []}, {"text": "Ahmed Kholy and Nizar Habash . 2011 .", "entities": []}, {"text": "Automatic error analysis for morphologically rich languages .", "entities": []}, {"text": "Jonathan K. Kummerfeld , David Hall , James R. Curran , and Dan Klein .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Parser showdown at the Wall Street corral : An empirical investigation of error types in parser output .", "entities": []}, {"text": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pages 1048\u20131059 , Jeju Island , Korea .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jonathan K. Kummerfeld and Dan Klein .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Errordriven analysis of challenges in coreference resolution .", "entities": [[5, 7, "TaskName", "coreference resolution"]]}, {"text": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 265\u2013277 , Seattle , Washington , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Sha Li , Heng Ji , and Jiawei Han . 2021 .", "entities": []}, {"text": "Documentlevel event argument extraction by conditional generation .", "entities": [[1, 4, "TaskName", "event argument extraction"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 894\u2013908 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ying Lin , Heng Ji , Fei Huang , and Lingfei Wu . 2020 .", "entities": []}, {"text": "A joint neural model for information extraction with global features .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7999\u20138009 , Online .", "entities": []}, {"text": "Association for Computational Linguistics.3969", "entities": []}, {"text": "Yi Luan , Dave Wadden , Luheng He , Amy Shah , Mari Ostendorf , and Hannaneh Hajishirzi . 2019 .", "entities": []}, {"text": "A general framework for information extraction using dynamic span graphs .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3036\u20133046 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sebastian Martschat , Thierry G\u00f6ckel , and Michael Strube . 2015 .", "entities": []}, {"text": "Analyzing and visualizing coreference resolution errors .", "entities": [[3, 5, "TaskName", "coreference resolution"]]}, {"text": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics : Demonstrations , pages 6\u201310 , Denver , Colorado . Association for Computational Linguistics .", "entities": []}, {"text": "Sebastian Martschat and Michael Strube .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Recall error analysis for coreference resolution .", "entities": [[0, 1, "MetricName", "Recall"], [4, 6, "TaskName", "coreference resolution"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 2070\u20132081 , Doha , Qatar .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "MUC-3 .", "entities": []}, {"text": "1991 .", "entities": []}, {"text": "Third Message Understanding Conference ( MUC-3 ): Proceedings of a conference held in San Diego , California , May 21 - 23 , 1991 .", "entities": []}, {"text": "MUC-4 .", "entities": [[0, 1, "DatasetName", "MUC-4"]]}, {"text": "1992 .", "entities": []}, {"text": "Fourth message understanding conference ( MUC-4 ) .", "entities": [[5, 6, "DatasetName", "MUC-4"]]}, {"text": "In Proceedings of FOURTH MESSAGE UNDERSTANDING CONFERENCE ( MUC4 ) , McLean , Virginia .", "entities": []}, {"text": "Siddharth Patwardhan and Ellen Riloff .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "A uni\ufb01ed model of phrasal and sentential evidence for information extraction .", "entities": []}, {"text": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 151\u2013160 , Singapore .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Maja Popovi \u00b4 c and Hermann Ney . 2011 .", "entities": []}, {"text": "Towards automatic error analysis of machine translation output .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "Computational Linguistics , 37(4):657\u2013688 .", "entities": []}, {"text": "Chris Quirk and Hoifung Poon . 2017 .", "entities": []}, {"text": "Distant supervision for relation extraction beyond the sentence boundary .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers , pages 1171\u20131182 , Valencia , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Marco Tulio Ribeiro , Tongshuang Wu , Carlos Guestrin , and Sameer Singh .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Beyond accuracy : Behavioral testing of NLP models with CheckList .", "entities": [[1, 2, "MetricName", "accuracy"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4902 \u2013 4912 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Olga Uryupina . 2008 .", "entities": []}, {"text": "Error analysis for learningbased coreference resolution .", "entities": [[0, 1, "MetricName", "Error"], [4, 6, "TaskName", "coreference resolution"]]}, {"text": "In Proceedings of the Sixth International Conference on Language Resources and Evaluation ( LREC\u201908 ) , Marrakech , Morocco .", "entities": []}, {"text": "European Language Resources Association ( ELRA).Josep Valls - Vargas , Jichen Zhu , and Santiago Onta\u00f1\u00f3n . 2017 .", "entities": []}, {"text": "Error analysis in an automated narrative information extraction pipeline .", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "IEEE Transactions on Computational Intelligence and AI in Games , 9(4):342\u2013353 .", "entities": []}, {"text": "David Vilar , Jia Xu , Luis Fernando D\u2019Haro , and Hermann Ney . 2006 .", "entities": []}, {"text": "Error analysis of statistical machine translation output .", "entities": [[0, 1, "MetricName", "Error"], [4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Fifth International Conference on Language Resources and Evaluation ( LREC\u201906 ) , Genoa , Italy .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "David Wadden , Ulme Wennberg , Yi Luan , and Hannaneh Hajishirzi . 2019 .", "entities": []}, {"text": "Entity , relation , and event extraction with contextualized span representations .", "entities": [[5, 7, "TaskName", "event extraction"]]}, {"text": "InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 5784 \u2013 5789 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Eric Wallace , Jens Tuyls , Junlin Wang , Sanjay Subramanian , Matt Gardner , and Sameer Singh .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "AllenNLP interpret : A framework for explaining predictions of NLP models .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ): System Demonstrations , pages 7\u201312 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tongshuang Wu , Marco Tulio Ribeiro , Jeffrey Heer , and Daniel Weld .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Errudite : Scalable , reproducible , and testable error analysis .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 747\u2013763 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Hang Yang , Dianbo Sui , Yubo Chen , Kang Liu , Jun Zhao , and Taifeng Wang .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Document - level event extraction via parallel prediction networks .", "entities": [[0, 5, "TaskName", "Document - level event extraction"]]}, {"text": "InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 6298\u20136308 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Daniel Zeman , Mark Fishel , Jan Berka , and Ondrej Bojar . 2011 .", "entities": []}, {"text": "Addicter : What is wrong with my translations ?", "entities": []}, {"text": "In Prague Bull .", "entities": []}, {"text": "Math .", "entities": []}, {"text": "Linguistics .", "entities": []}, {"text": "Junchi Zhang , Yanxia Qin , Yue Zhang , Mengchi Liu , and Donghong Ji . 2019 .", "entities": []}, {"text": "Extracting entities and events as a single task using a transition - based neural model .", "entities": []}, {"text": "In Proceedings of the Twenty - Eighth International Joint Conference on Arti\ufb01cial Intelligence , IJCAI 2019 , Macao , China , August 10 - 16 , 2019 , pages 5422\u20135428 .", "entities": []}, {"text": "ijcai.org .", "entities": []}, {"text": "Yuhao Zhang , Peng Qi , and Christopher D. Manning .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graph convolution over pruned dependency3970", "entities": [[1, 2, "MethodName", "convolution"]]}, {"text": "trees improves relation extraction .", "entities": [[2, 4, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2205\u20132215 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Ming Zhou , Bo Wang , Shujie Liu , Mu Li , Dongdong Zhang , and Tiejun Zhao . 2008 .", "entities": []}, {"text": "Diagnostic evaluation of machine translation systems using automatically constructed linguistic check - points .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008 ) , pages 1121\u20131128 , Manchester , UK .", "entities": []}, {"text": "Coling 2008", "entities": []}, {"text": "Organizing Committee.3971", "entities": []}, {"text": "A Detailed Error Types Mappings", "entities": [[2, 3, "MetricName", "Error"]]}, {"text": "The speci\ufb01c list of transformations applied in the error correction process .", "entities": []}, {"text": "( 1)Span Error .", "entities": [[2, 3, "MetricName", "Error"]]}, {"text": "Each singleton Alter Span transformation is mapped to a Span Error .", "entities": [[10, 11, "MetricName", "Error"]]}, {"text": "A Span Error occurs when a predicted role \ufb01ller becomes an exact match to the a gold role \ufb01ler only upon span alteration .", "entities": [[2, 3, "MetricName", "Error"], [11, 13, "MetricName", "exact match"]]}, {"text": "( 2)Duplicate Role Filler .", "entities": []}, {"text": "Each singleton Remove Duplicate Role Filler transformation is mapped to a Duplicate Role Filler error .", "entities": []}, {"text": "A Duplicate Role Filler error occurs when a spurious role \ufb01ller is coreferent to an already matched role \ufb01ller and is treated as a separate entity .", "entities": []}, {"text": "This happens when the system fails at coreference resolution .", "entities": [[7, 9, "TaskName", "coreference resolution"]]}, {"text": "( 3)Duplicate Partially Matched Role Filler ( Spurious ) .", "entities": []}, {"text": "Same as ( 2 ) above , but with an added Alter Span transformation applied \ufb01rst to account for partial matching .", "entities": []}, {"text": "This happens when the system fails at correct span extraction and coreference resolution .", "entities": [[11, 13, "TaskName", "coreference resolution"]]}, {"text": "( 4)Spurious Role Filler .", "entities": []}, {"text": "Each singleton Remove Spurious Role Filler transformation is mapped to a Spurious Role Filler error .", "entities": []}, {"text": "A Spurious Role Filler error occurs when a mention is extracted from the text with no connection to the gold templates .", "entities": []}, {"text": "( 5)Missing Role Filler .", "entities": []}, {"text": "Each singleton Introduce Role Filler transformation is mapped to a Missing Role Filler error .", "entities": []}, {"text": "A Missing Role Filler error occurs when a role \ufb01ller is present in the gold template but not the predicted template for a given role .", "entities": []}, {"text": "( 6)Incorrect Role .", "entities": []}, {"text": "Each singleton Alter Role transformation is mapped to an Incorrect Role .", "entities": []}, {"text": "An Incorrect Role occurs when a spurious role \ufb01ller is assigned to the incorrect role within the same template , i.e. the role \ufb01ller would have been correct if present \ufb01lled in another slot / role in the same template .", "entities": []}, {"text": "This happens when the system fails at correct role assignment .", "entities": []}, {"text": "( 7)Incorrect Role + Partially Matched Filler .", "entities": []}, {"text": "Same as ( 4 ) above , but with an added Alter Span transformation applied \ufb01rst to account for partial matching .", "entities": []}, {"text": "This happens when the system fails at correct span extraction and role assignment .", "entities": []}, {"text": "( 8)Wrong Template for Role Filler .", "entities": []}, {"text": "Each singleton Remove Cross Template Spurious Role Filler transformation is mapped to a Wrong Template for Role Filler error .", "entities": []}, {"text": "A Wrong Template for Role Filler occurs when a spurious role \ufb01ller in one template can be assigned to the correct role in another template , i.e. it would be correct if it had been placed in another template .", "entities": []}, {"text": "This happens when the system fails at correct event assignment .", "entities": []}, {"text": "( 9)Wrong Template for Partially Matched Role Filler .", "entities": []}, {"text": "Same as ( 6 ) above , but with an added Alter Span transformation applied \ufb01rst to account for partial matching .", "entities": []}, {"text": "This happens when the system fails at correct span extraction and event assignment .", "entities": []}, {"text": "( 10 ) Wrong Template + Wrong Role .", "entities": []}, {"text": "An Alter Role and Remove Cross Template Spurious Role Filler transformation are applied to the same predicted role \ufb01ller in that order to be mapped to a Wrong Template + Wrong Role error .", "entities": []}, {"text": "A Wrong Template + Wrong Role error occurs when a spurious role \ufb01ller can be assigned to another role in another template .", "entities": []}, {"text": "This happens when the system fails at correct role assignment and event assignment .", "entities": []}, {"text": "( 11 ) Wrong Template + Wrong Role + Partially Matched Filler .", "entities": []}, {"text": "Same as ( 8) above , but with an added Alter Span transformation applied \ufb01rst to account for partial matching .", "entities": []}, {"text": "This happens when the system fails at correct span extraction , role assignment and event assignment .", "entities": []}, {"text": "( 12 ) Spurious Template.13Each singleton Remove Template is mapped to a Spurious Template error .", "entities": []}, {"text": "A Spurious Template error occurs when an extra predicted template is present that can not be matched to a gold template .", "entities": []}, {"text": "( 13 ) Missing Template.14Each singleton Introduce Template transformation is mapped to a Missing Template error .", "entities": []}, {"text": "A Missing Template error occurs when there is a gold template remaining that has no matching predicted template .", "entities": []}, {"text": "13The role \ufb01llers in the Spurious Templates are not added to the Spurious Role Filler error counts but are accounted for in the Spurious Template Role Filler counts .", "entities": []}, {"text": "14The role \ufb01llers in the Missing Templates are not added to the Missing Role Filler error counts but are accounted for in the Missing Template Role Filler counts.3972", "entities": []}, {"text": "B Example Error Types with ProMED", "entities": [[2, 3, "MetricName", "Error"]]}, {"text": "We also provide example error types with the ProMED dataset .", "entities": []}, {"text": "Error Types Transformations(s )", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "Predicted Gold i )", "entities": []}, {"text": "Span Error Alter Span Victims : [ young fattening cattle]Victims :", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "[ young fattening cattle and sheep ] ii ) Duplicate Role Filler Remove Duplicate Role FillerDisease : [ west nile fever ] , [ west nile virus]Disease : [ west nile fever , west nile virus ] iii ) Within Template Incorrect RoleAlter RoleT1 : Disease : [ 2 humans ] Victims : \u2014 T1 : Disease : \u2014 Victims : [ 2 humans ] iv ) Wrong Template For Role FillerRemove Cross Template Spurious Role FillerT1 : Country : [ netherlands ] Victims : [ 770 cases]T1 : Country : [ netherlands ] Victims : [ its 11th case ] T2 : Country : [ united kingdom ] Victims : [ 770 cases ] v ) Spurious Template Remove Spurious TemplateT1 : Country :", "entities": []}, {"text": "[ china ] \u2014 vi ) Missing Template Introduce Missing Template \u2014 T1 : Country : [ germany ] Disease : [ fmd ] Victims : [ 2 pigs ] Table 5 : Some examples of the Error Types taken from the ProMED dataset .", "entities": [[22, 23, "DatasetName", "fmd"], [37, 38, "MetricName", "Error"]]}, {"text": "For each template , in every role , the role \ufb01llers within brackets refer to the same entity , while role \ufb01llers in different brackets refer to different entities .", "entities": []}, {"text": "The text in bold black indicates the error in the prediction .", "entities": []}, {"text": "C Precision , Recall , and F1 Scores for All Models on all Three Datasets We also provide additional precision , recall scores along with the F1 scores .", "entities": [[1, 2, "MetricName", "Precision"], [3, 4, "MetricName", "Recall"], [6, 7, "MetricName", "F1"], [26, 27, "MetricName", "F1"]]}, {"text": "Models SciREX ProMED MUC-4 DyGIE++ ( BERT ) 27.85 / 18.83 / 22.47 51.13 / 26.62 / 35.01 61.90 / 36.33 / 45.79 DyGIE++ ( SciBERT ) 30.47 / 21.76 / 25.39 52.55 / 29.94 / 38.15 GTT ( BERT ) 52.86 / 13.53 / 21.54 68.58 / 33.09 / 44.64 63.18 / 40.02 / 49.00 GTT ( SciBERT ) 53.68 / 18.65 / 27.68 64.68 / 32.16 / 42.96 Table 6 : Precision , Recall and F1 Scores ( % ) .", "entities": [[1, 2, "DatasetName", "SciREX"], [3, 4, "DatasetName", "MUC-4"], [6, 7, "MethodName", "BERT"], [39, 40, "MethodName", "BERT"], [73, 74, "MetricName", "Precision"], [75, 76, "MetricName", "Recall"], [77, 78, "MetricName", "F1"]]}, {"text": "D Computational Budget The GTT ( BERT ) model on the MUC-4 dataset took 1 hour and 21 minutes to train and around 11 minutes to test on Google Colab ( GPU ) .", "entities": [[6, 7, "MethodName", "BERT"], [11, 12, "DatasetName", "MUC-4"], [28, 29, "DatasetName", "Google"]]}, {"text": "The GTT ( BERT ) model on the ProMED dataset took around 24 minutes to train and 4 minutes to test , while the GTT ( SciBERT ) model on the ProMED dataset took around 13 minutes to train and 4 minutes to test , both on Google Colab ( GPU ) .", "entities": [[3, 4, "MethodName", "BERT"], [47, 48, "DatasetName", "Google"]]}, {"text": "The DyGIE++ ( BERT ) model on the ProMED dataset took around 50 minutes to train , while the DyGIE++ ( SciBERT ) model on the ProMED dataset took around 1 hour and 30 minutes to train , both on a NVIDIA V100 GPU .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "For the SciREX dataset , it took around 10 - 20 minutes to run the GTT ( BERT ) and GTT ( SciBERT ) models on a NVIDIA V100 GPU .", "entities": [[2, 3, "DatasetName", "SciREX"], [17, 18, "MethodName", "BERT"]]}, {"text": "It is worth noting that since the GTT model embeds all inputs before training and SciREX documents are extremely long , more than 25 GB of memory needs to be allocated at the embedding phrase .", "entities": [[15, 16, "DatasetName", "SciREX"]]}, {"text": "The training process has normal memory usage .", "entities": []}, {"text": "The DyGIE++ ( BERT ) model took around 2 hours to train , while the DyGIE++ ( SciBERT ) model took around 4 hours to train , both on a NVIDIA V100 GPU .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "Our error analysis tool can be run completely on a CPU and takes a couple of minutes to run , depending on the size of the dataset and the predicted outputs.3973", "entities": []}, {"text": "E Hyperparameters and Model Con\ufb01gurations", "entities": []}, {"text": "We did not run the DyGIE++ model on the MUC-4 dataset as the model output was made available to us by Xinya Du . GTT ( BERT ) Hyperparameter Name", "entities": [[9, 10, "DatasetName", "MUC-4"], [26, 27, "MethodName", "BERT"]]}, {"text": "Value number of gpus 1 number of tpu cores 0 max_grad_norm 1.0 gradient_accumulation_steps 1 seed 1 base_model bert_base_uncased learning_rate 5e-05 weight_decay 0.0 adam_epsilon 1e-08 warmup_steps 0", "entities": [[9, 10, "DatasetName", "0"], [25, 26, "DatasetName", "0"]]}, {"text": "num_train_epochs 20 train_batch_size 1 eval_batch_size 1 max_seq_length_src 435 max_seq_length_tgt 75 threshold 80.0 Table 7 : GTT on the MUC-4 dataset GTT ( BERT ) GTT ( SciBERT ) Hyperparameter Name Value Value number of GPUs 1 1 number of TPU cores 0 0 max_grad_norm 1.0 1.0 gradient_accumulation_steps 1 1 seed 1 1 base_model bert_base_uncased allenai _ scibert _", "entities": [[18, 19, "DatasetName", "MUC-4"], [22, 23, "MethodName", "BERT"], [41, 42, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}, {"text": "scivocab_uncased learning_rate", "entities": []}, {"text": "5e-05 5e-05 weight_decay 0.0 0.0 adam_epsilon 1e-08 1e-08 warmup_steps 0 0", "entities": [[9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}, {"text": "num_train_epochs 36 36 train_batch_size 1 1 eval_batch_size 1 1 max_seq_length_src 435 435 max_seq_length_tgt 75 75 threshold 80.0 80.0 Table 8 : GTT Models on the ProMED dataset3974", "entities": []}, {"text": "GTT ( BERT ) GTT ( SciBERT ) Hyperparameter Name", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "Value Value number of GPUs 1 1 number of TPU cores 0 0 max_grad_norm 1.0 1.0 gradient_accumulation_steps 1 1 seed 1 1 base_model bert_base_uncased allenai _ scibert _", "entities": [[11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}, {"text": "scivocab_uncased learning_rate", "entities": []}, {"text": "5e-05 5e-05 weight_decay 0.0 0.0 adam_epsilon 1e-08 1e-08 warmup_steps 0 0", "entities": [[9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}, {"text": "num_train_epochs", "entities": []}, {"text": "20 20 train_batch_size 1 1 eval_batch_size 1 1 max_seq_length_src 435 435 max_seq_length_tgt 75 75 threshold 80.0 80.0 Table 9 : GTT Models on the SciREX dataset DyGIE++ ( BERT ) DyGIE++ ( SciBERT ) Hyperparameter Name Value Value number of GPUs 1 1 max_span_width 11 11 base_model bert_base_cased allenai _ scibert _ scivocab_cased learning_rate 5e-04 5e-04 patience 5 5 num_train_epochs 20 20 train_batch_size 32 32 num_dataloader_workers 2 2 max seq length", "entities": [[24, 25, "DatasetName", "SciREX"], [28, 29, "MethodName", "BERT"]]}, {"text": "512 512 ner loss weight 1.0 1.0 relation loss weight 0.0 0.0 coreference loss weight 0.2 0.2 events loss weight 0.0 0.0 target task ner ner Table 10 : DyGIE++ Models on the ProMED dataset DyGIE++ ( BERT ) DyGIE++ ( SciBERT ) Hyperparameter Name Value Value number of GPUs 1 1 max_span_width 8 8 base_model bert_base_cased allenai _ scibert _ scivocab_cased learning_rate 5e-04 5e-04 patience 5 5 num_train_epochs 20 20 train_batch_size 32 32 num_dataloader_workers 2 2 max seq length", "entities": [[3, 4, "MetricName", "loss"], [8, 9, "MetricName", "loss"], [13, 14, "MetricName", "loss"], [18, 19, "MetricName", "loss"], [37, 38, "MethodName", "BERT"]]}, {"text": "512 512 ner loss weight 1.0 1.0 relation loss weight 0.0 0.0 coreference loss weight 0.2 0.2 events loss weight 0.0 0.0 target task ner ner Table 11 : DyGIE++ Models on the SciREX dataset3975", "entities": [[3, 4, "MetricName", "loss"], [8, 9, "MetricName", "loss"], [13, 14, "MetricName", "loss"], [18, 19, "MetricName", "loss"], [33, 34, "DatasetName", "SciREX"]]}]