[{"text": "Proceedings of the 16thWorkshop on Innovative Use of NLP for Building Educational Applications , pages 147\u2013157 April 20 , 2021 \u00a9 2021 Association for Computational Linguistics147On the application of Transformers for estimating the dif\ufb01culty of Multiple - Choice Questions from text Luca Benedetto and Paolo Cremonesi Politecnico di Milano , Milan , Italy fname.surnameg@polimi.it Giovanni Aradelli and Andrea Cappelli and Andrea Giussani and Roberto Turrin Cloud Academy Sagl , Mendrisio , Switzerland fname.surnameg@cloudacademy.com", "entities": []}, {"text": "Abstract Classical approaches to question calibration are either subjective or require newly created questions to be deployed before being calibrated .", "entities": []}, {"text": "Recent works explored the possibility of estimating question dif\ufb01culty from text , but did not experiment with the most recent NLP models , in particular Transformers .", "entities": []}, {"text": "In this paper , we compare the performance of previous literature with Transformer models experimenting on a public and a private dataset .", "entities": [[12, 13, "MethodName", "Transformer"]]}, {"text": "Our experimental results show that Transformers are capable of outperforming previously proposed models .", "entities": []}, {"text": "Moreover , if an additional corpus of related documents is available , Transformers can leverage that information to further improve calibration accuracy .", "entities": [[21, 22, "MetricName", "accuracy"]]}, {"text": "We characterize the dependence of the model performance on some properties of the questions , showing that it performs best on questions ending with a question mark and Multiple - Choice Questions ( MCQs ) with one correct choice .", "entities": []}, {"text": "1 Introduction Question Dif\ufb01culty Estimation ( QDE ) , also referred to as \u201c calibration \u201d , is a crucial task in education .", "entities": []}, {"text": "Indeed , since question dif\ufb01culty can be leveraged to assess the skill level of students under examination , wrongly calibrated questions are cause of erroneous estimations .", "entities": []}, {"text": "Also , an accurate calibration enables to detect and discard questions that are too easy or too dif\ufb01cult for certain students .", "entities": []}, {"text": "Traditionally , QDE is performed manually or with pretesting .", "entities": []}, {"text": "Manual calibration is intrinsically subjective and inconsistent .", "entities": []}, {"text": "Pretesting leads indeed to an accurate and consistent calibration , but i ) introduces a long delay between question generation and when the question can be used to scorestudents , and ii ) requires to deploy the new questions before actually using them for scoring .", "entities": [[18, 20, "TaskName", "question generation"]]}, {"text": "To address the issues of the traditional approaches to calibration , recent research tried to leverage Natural Language Processing ( NLP ) techniques to perform QDE from question text : the idea is to use some pretested questions to train a model that performs QDE from text and thus eliminates ( or at least reduces ) the need for pretesting of new questions .", "entities": []}, {"text": "Although such works showed promising results , none of them experimented with the latest NLP research , such as Transformer - based models ( Vaswani et al . , 2017 ) .", "entities": [[19, 20, "MethodName", "Transformer"]]}, {"text": "In this work , we aim at \ufb01lling that gap , studying how different Transformers , also trained with different approaches , compare with the current state of the art .", "entities": []}, {"text": "We evaluate several models built upon the pre - trained BERT ( Devlin et al . , 2019 ) and DistilBERT ( Sanh et al . , 2019 ) language models on the publicly available ASSISTments dataset and the private CloudAcademy dataset .", "entities": [[10, 11, "MethodName", "BERT"], [20, 21, "MethodName", "DistilBERT"]]}, {"text": "By using data from two different domains , we add to the growing body of evidence showing that item text can be used to perform QDE .", "entities": []}, {"text": "More precisely , we show that - after being \ufb01ne - tuned on the task of QDE from text - Transformer models are capable of calibrating newly generated questions more accurately than previously proposed approaches .", "entities": [[20, 21, "MethodName", "Transformer"]]}, {"text": "On top of that , we explore the possibility of leveraging additional textual information which might be available ( e.g. transcript of video lectures ) to perform an additional pre - training of the Transformers before \ufb01ne - tuning them for QDE , and show that such approach can be used to further improve the accuracy .", "entities": [[55, 56, "MetricName", "accuracy"]]}, {"text": "Overall , Transformer - based models are capable of reducing the RMSE by up tp 6.5 % , with respect to previous approaches .", "entities": [[2, 3, "MethodName", "Transformer"], [11, 12, "MetricName", "RMSE"]]}, {"text": "Lastly , we perform an analysis of the best performing model to under-", "entities": []}, {"text": "148stand whether some question characteristics ( e.g. text length , question type ) particularly affect its performance .", "entities": []}, {"text": "Code is available at https://github . com / aradelli / transformers - for - qde .", "entities": []}, {"text": "2 Related Work There is a large interest in understanding how textual features affect item dif\ufb01culty ( El Masri et al . , 2017 ; Hickendorff , 2013 ) , and this is not limited to the educational domain : for instance Wang et al .", "entities": []}, {"text": "( 2014 ) focus on dif\ufb01culty estimation in community question answering systems .", "entities": [[8, 11, "TaskName", "community question answering"]]}, {"text": "The \ufb01rst works addressing QDE from text focused on MCQs and used deterministic approaches based on bag of words and similarities between question , answer , and distractors ( Alsubait et al . , 2013 ; Yaneva et al . , 2018 ; Kurdi et al . , 2016 ) .", "entities": []}, {"text": "Recent works mostly use machine learning approaches .", "entities": []}, {"text": "Huang et al .", "entities": []}, {"text": "( 2017 ) proposed a neural network for QDE of \u201c reading \u201d problems , in which the answer has to be found in a text provided together with the question .", "entities": []}, {"text": "The model receives as input both the text of the question and the document , thus it actually estimates the dif\ufb01culty of \ufb01nding the correct answer in the provided document .", "entities": []}, {"text": "This is a major difference from all other works , in which the dif\ufb01culty depends on the questions only .", "entities": []}, {"text": "Yaneva et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) introduced a model to estimate the p - value of questions from text .", "entities": []}, {"text": "The p - value of a question is de\ufb01ned as the fraction of students who correctly answered it and does not account for different skill levels .", "entities": []}, {"text": "This model was trained using the text of questions and a large dataset of medical documents , thus it can not be used if an analogous dataset is not available .", "entities": []}, {"text": "Similarly , the model proposed in ( Qiu et al . , 2019 ) requires for training a dataset of medical documents in addition to the question texts .", "entities": []}, {"text": "The model is made of two neural architectures to estimate the wrongness ( i.e.1\u0000p - value ) of newly - generated MCQs , considering it as made of two components which indicate i ) how dif\ufb01cult it is to choose between the possible choices , and ii ) how dif\ufb01cult it is to recall the knowledge required to answer the question .", "entities": []}, {"text": "Benedetto et al .", "entities": []}, {"text": "( 2020b ) proposed R2DE , a model that estimates the dif\ufb01culty and discrimination , as de\ufb01ned in Item Response Theory ( IRT ) ( Hambleton et al . , 1991 ) , of newly generated MCQs .", "entities": []}, {"text": "R2DE computes TF - IDF features from the text of the questions and the text of the possible choices , and feeds two Random Forest regressorswith such features .", "entities": []}, {"text": "The performance of R2DE was improved in ( Benedetto et al . , 2020a ) , with the addition of readability and linguistics features .", "entities": []}, {"text": "To the best of our knowledge , ( Xue et al . , 2020 ) is the \ufb01rst work that explored the effects of transfer learning for QDE from text .", "entities": [[24, 26, "TaskName", "transfer learning"]]}, {"text": "Speci\ufb01cally , the authors \ufb01ne - tune pre - trained ELMo embeddings ( Peters et al . , 2018 ) for the task of response time prediction , and subsequently perform a second \ufb01netuning for the task of p - value estimation .", "entities": [[10, 11, "MethodName", "ELMo"]]}, {"text": "Differently from all the other models , this one and R2DE can be trained using only question text , without needing an additional dataset of related documents .", "entities": []}, {"text": "Our approach differs from previous research in several ways .", "entities": []}, {"text": "First of all , we adopt transfer learning on Transformers , which are yet to be explored for QDE from text .", "entities": [[6, 8, "TaskName", "transfer learning"]]}, {"text": "Secondly , similarly to ( Benedetto et al . , 2020b , a ) , we perform the estimation of the IRT dif\ufb01culty which , differently from wrongness andp - value , models students \u2019 skill levels as well .", "entities": []}, {"text": "Speci\ufb01cally , we consider the one - parameter model ( Rasch , 1960 ) , a logistic model that associates a skill level to each student and a dif\ufb01culty level to each question ( both represented as scalars ) .", "entities": []}, {"text": "A brief introduction to the one - parameter IRT model is given in Appendix A. Thirdly , the Transformer models presented here do not necessarily require an additional dataset of documents from the same topics as the questions , and they can be trained using only question texts ; however , if such dataset is available , it can be leveraged to further improve the accuracy of calibration .", "entities": [[18, 19, "MethodName", "Transformer"], [65, 66, "MetricName", "accuracy"]]}, {"text": "Lastly , each previous work experimented on one private dataset ; we experiment on two datasets ( one being publicly available ) , showing that Transformers can be successfully used for QDE in different domains .", "entities": []}, {"text": "3 Introduction to BERT and DistilBERT BERT ( Devlin et", "entities": [[3, 4, "MethodName", "BERT"], [5, 6, "MethodName", "DistilBERT"], [6, 7, "MethodName", "BERT"]]}, {"text": "al . , 2019 ) is a pre - trained language model that reached state of the art performance in many language tasks .", "entities": []}, {"text": "Its key technical innovation was the application of the Transformer , a popular self - attention model ( Vaswani et", "entities": [[9, 10, "MethodName", "Transformer"]]}, {"text": "al . , 2017 ) , to language modeling .", "entities": []}, {"text": "BERT is originally trained to address two tasks : Masked Language Modeling ( MLM ) and Next Sentence Prediction ( NSP ) .", "entities": [[0, 1, "MethodName", "BERT"], [9, 12, "TaskName", "Masked Language Modeling"], [13, 14, "DatasetName", "MLM"]]}, {"text": "MLM consists in removing one word from the input text and asking the model to \ufb01ll the gap , while in NSP the model is asked - given two input sentences - to tell whether the second sentence is a reasonable continuation to the \ufb01rst one .", "entities": [[0, 1, "DatasetName", "MLM"]]}, {"text": "Crucially ,", "entities": []}, {"text": "149 Figure 1 : Fine - tuning of the pre - trained language model for the task of question dif\ufb01culty estimation .", "entities": []}, {"text": "BERT can be used for many different downstream tasks , as we do here for QDE : starting from the pre - trained model , it is suf\ufb01cient to stack an additional layer on top of the original network and then retrain it on the desired task ( process named \u201c \ufb01netuning \u201d ) .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "During \ufb01ne - tuning , the internal weights of the pre - trained model are updated and adapted to the desired task ( together with the weights of the added layer ) , which is both more ef\ufb01cient than training the whole network from scratch and capable of better results , as the knowledge of the pre - trained model is not discarded .", "entities": []}, {"text": "BERT is a large model and therefore requires many resources for training and \ufb01ne - tuning .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "For this reason , we also experiment with DistilBERT ( Sanh et al . , 2019 ) , which is a language model obtained by distilling BERT .", "entities": [[8, 9, "MethodName", "DistilBERT"], [26, 27, "MethodName", "BERT"]]}, {"text": "Knowledge distillation is a compression technique in which a small model is trained to reproduce the full output distribution of a larger model ( Hinton et al . , 2015 ) .", "entities": [[0, 2, "MethodName", "Knowledge distillation"]]}, {"text": "With this approach , DistilBERT is able to retain 95 % of BERT \u2019s performance on several language understanding tasks using about half the number of parameters of BERT .", "entities": [[4, 5, "MethodName", "DistilBERT"], [12, 13, "MethodName", "BERT"], [24, 27, "HyperparameterName", "number of parameters"], [28, 29, "MethodName", "BERT"]]}, {"text": "Similarly to BERT , DistilBERT can be \ufb01ne - tuned on downstream tasks and it is therefore worth exploring for QDE from text .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "DistilBERT"]]}, {"text": "4 Models This section describes how we build the different models which are compared with the current state of the art of QDE from text .", "entities": []}, {"text": "These models are built upon the two pre - trained language models , \ufb01netuning them with two different approaches .", "entities": []}, {"text": "The \ufb01rst approach consists in directly \ufb01ne - tuning the pre - trained model for the task of QDE from text .", "entities": []}, {"text": "The second approach is made of two steps : we i ) further pre - train the pre - trained model on the task of Masked Language Modeling ( MLM ) to improve domain knowledge , and subsequently ii ) \ufb01ne - tune it on the task of QDE from text .", "entities": [[25, 28, "TaskName", "Masked Language Modeling"], [29, 30, "DatasetName", "MLM"]]}, {"text": "This is all done separately for the two datasets : we do not perform any experiments across the two datasets .", "entities": []}, {"text": "4.1 Fine - tuning for QDE from text This is the simplest of the two approaches ; the architecture used for \ufb01ne - tuning is shown in Figure 1 .", "entities": []}, {"text": "Given the pre - trained language model , we stack an additional fully connected layer on top of the network , in order to use that as the new output .", "entities": []}, {"text": "Following the \ufb01ne - tuning guidelines in ( Devlin et al . , 2019 ) , we use only the \ufb01rst output of the pre - trained language model .", "entities": []}, {"text": "This works since the \ufb01rst output correspond to the special token", "entities": []}, {"text": "[ CLS ] which is added at the beginning of the input text and is the only one used for regression and classi\ufb01cation .", "entities": []}, {"text": "Since question calibration is a regression task , the additional output layer has one neuron , and the weights of the connections with the previous layer are randomly initialized .", "entities": []}, {"text": "During \ufb01ne - tuning , both the weights of the additional layer and the internal weights of the pre - trained language model are updated .", "entities": []}, {"text": "For the input , we use the same tokenization and the same encoding as the original models .", "entities": []}, {"text": "That is , all the input samples start with the special token[CLS ] and contain two sentences , separated by the [ SEP ] token ( another special token ): the \ufb01rst sentence is the ( tokenized ) question while the second one contains the ( tokenized ) choices \u2019 text1 .", "entities": []}, {"text": "1It is possible to have a question made of several sentences .", "entities": []}, {"text": "In that case , the whole question is considered as \u201c Sentence 1 \u201d ,", "entities": []}, {"text": "150Speci\ufb01cally , we experiment with three different encodings for the second sentence : i ) Q only , we leave it empty , thus considering only the text of the question , ii ) Q+correct , we use the text of the correct choice(s ) ( as in Figure 1 ) , iii ) Q+all , we use the text of all the possible choices , concatenating them in a single sentence .", "entities": []}, {"text": "For the ASSISTments dataset , only the \ufb01rst encoding is possible as the text of the choices ( correct answer and distractors ) is not available .", "entities": []}, {"text": "We also experimented a fourth approach , considering all the possible choices using several [ SEP ] tokens between each choice ; however , this approach performed largely worse that the others , thus we do not report it here .", "entities": []}, {"text": "We believe that the model , in that case , does not have enough training questions to learn the meaning of the additional separators ( BERT and DistilBERT are pre - trained to use only one [ SEP ] token ) .", "entities": [[25, 26, "MethodName", "BERT"], [27, 28, "MethodName", "DistilBERT"]]}, {"text": "4.2 Pre - training for MLM Masked Language Modeling ( MLM ) is a \ufb01ll - in - theblank task , where a word of the input text is substituted by a [ MASK ] token and the model is trained to use the surrounding words to predict the word that was masked .", "entities": [[5, 6, "DatasetName", "MLM"], [6, 9, "TaskName", "Masked Language Modeling"], [10, 11, "DatasetName", "MLM"]]}, {"text": "We leverage MLM to perform an additional pre - training of the pre - trained language models before the \ufb01ne - tuning on QDE from text .", "entities": [[2, 3, "DatasetName", "MLM"]]}, {"text": "Our goal is to let the model learn the questions \u2019 topics more accurately than how it would do with the \ufb01ne - tuning on QDE only .", "entities": []}, {"text": "In order for MLM to be effective , though , we need an additional dataset of documents about the same topics that are assessed by the questions : this is available only for the CloudAcademy dataset , which contains the transcript of some of the video - lectures on the e - learning platform .", "entities": [[3, 4, "DatasetName", "MLM"]]}, {"text": "In practice , we perform pretraining with MLM as follows .", "entities": [[7, 8, "DatasetName", "MLM"]]}, {"text": "We randomly mask 15 % of the words of the available lectures , then train the language model to predict the masked words sentence by sentence .", "entities": []}, {"text": "The actual prediction is performed by stacking a fully connected layer and a softmax layer on top of the original pre - trained model : for each masked sentence , this additional layer consumes as input the contextual embedding corresponding to the [ MASK ] token , and tries to predict the word that should be inserted in its place .", "entities": [[13, 14, "MethodName", "softmax"]]}, {"text": "After pre - training the model on the task of MLM , the additional dense and softmax layers are removed from the network , thus leaving us with and the [ SEP ] token is still used only to indicate the end of the question .", "entities": [[10, 11, "DatasetName", "MLM"], [16, 17, "MethodName", "softmax"]]}, {"text": "We use this naming ( \u201c Sentence 1 \u201d and \u201c Sentence 2 \u201d ) since it is the one used in the original paper.a pre - trained model which has the same architecture as the original one , with the only difference that all the internal weights were updated during the additional MLM pre - training .", "entities": [[53, 54, "DatasetName", "MLM"]]}, {"text": "The architecture for the \ufb01nal \ufb01ne - tuning for QDE from text is the same as the one shown in Figure 1 . 5 Experimental Datasets", "entities": []}, {"text": "In this work we use the publicly available data collection provided by ASSISTments and the private CloudAcademy data collection .", "entities": []}, {"text": "Both data collections are made of two datasets : i ) the Answers dataset ( referred to as A ) and ii ) the Questions dataset ( Q ) .", "entities": []}, {"text": "It is important to remark that Aand Qare abstract names : we have one Adataset for CloudAcademy and one Adataset for ASSISTments ( similarly for Q).Acontains the students \u2019 answers : for each one , it stores the user ID , the question ID , the correctness of the answer and a timestamp .", "entities": []}, {"text": "Importantly , Acontains only \u201c \ufb01rst timers \u201d , meaning that we consider only the \ufb01rst interaction between a student and a question .", "entities": []}, {"text": "Qcontains the textual information about the items : question ID , question text and , in the case of CloudAcademy , the text of the possible choices .", "entities": []}, {"text": "For the experiments onCloudAcademy data , we also have access to an additional dataset - referred to as Lectures ( L ) which contains the transcripts of some online lectures available on the platform and is used for the additional MLM pre - training .", "entities": [[40, 41, "DatasetName", "MLM"]]}, {"text": "5.1 ASSISTments dataset ASSISTments2is an online tutoring system that provides instructional assistance while assessing students ( Feng et al . , 2009 ) .", "entities": []}, {"text": "In practice , this means that questions - called problems - can be broken down into steps : if the student does not get the originalproblem correctly , he has to answer a sequence ofscaffolding questions that break the problem down into steps .", "entities": []}, {"text": "In the current work , we consider both original and scaffolding problems for QDE from text .", "entities": []}, {"text": "An example problem and the corresponding scaffolding questions are shown in Appendix B.", "entities": []}, {"text": "We \ufb01lter the dataset to keep only questions that are answered by at least 50 students , to improve the reliability of the estimation of ground truth latent traits with IRT ; on average , each item is answered by 151 students and each student answers to 64 different items .", "entities": []}, {"text": "We also remove the questions that require external resources and the system messages 2https://new.assistments.org/", "entities": []}, {"text": "151(e.g .", "entities": []}, {"text": "\u201c Submit your answer from the textbook .", "entities": []}, {"text": "\u201d , \u201c Sorry , that is incorrect .", "entities": []}, {"text": "Let \u2019s go to the next question ! \u201d ) .", "entities": []}, {"text": "After removal of the unsuitable questions , the \ufb01nal dataset used for QDE from text contains 11,393 different items .", "entities": []}, {"text": "Ais publicly available for download3;Qis publicly available under request4 .", "entities": []}, {"text": "5.2 CloudAcademy dataset CloudAcademy5is an e - learning provider offering online courses about IT technologies .", "entities": []}, {"text": "All the questions are MCQ and we have access to the text of the possible choices .", "entities": []}, {"text": "An example question is shown in Appendix B.", "entities": []}, {"text": "The dataset used in our experiments is a sub - sample of the CloudAcademy data collection and it was generated in order to have only questions answered by at least 50 students .", "entities": []}, {"text": "Acontains 7,323,502 interactions , involving 34,696 students and 13,603 unique questions ; on average , each item is answered by 304 students and each student answers to 115 different items .", "entities": []}, {"text": "The overall correctness is 66 % .", "entities": []}, {"text": "Lcontains the transcript of some of the online lectures offered by CloudAcademy about the same topics ( i.e. cloud technologies ) assessed by the questions .", "entities": []}, {"text": "Lcontains a total of 159,563 sentences and 3,228,038 words .", "entities": []}, {"text": "6 Experimental Setup As displayed in Figure 2 , training is performed in two steps , repeated for the two datasets : i ) the IRT model is trained in order to calibrate the questions and obtain the ground truth dif\ufb01culties , then ii ) these ground truth latent traits are used as target values to train the model on QDE from text .", "entities": []}, {"text": "The \ufb01rst step consists in using Ato estimate with IRT the target dif\ufb01culty of all the questions .", "entities": []}, {"text": "Speci\ufb01cally , we use pyirt6for the estimation and consider [ \u00005 ; 5 ] as the possible range of dif\ufb01culties .", "entities": []}, {"text": "Dif\ufb01culties estimated at this stage will later be used as ground truth and therefore are inserted as target values in Q. Then , Qis split into a train dataset ( QTRAIN ) , used to train our model on QDE from text , and a test dataset ( Q TEST ) , which is used for the \ufb01nal evaluation of the model ; we keep 80 % of the questions for training and 20 % for testing .", "entities": []}, {"text": "At 3https://sites.google.com/ site / assistmentsdata / home/ 2012 - 13 - school - data - with - affect 4https://sites.google.com/ site / assistmentsdata / home/ assistments - problems 5https://cloudacademy.com/ 6https://pypi.org/project/pyirt/training time , we keep a portion of Q TRAIN ( 10 % ) as development set , for hyperparameter tuning .", "entities": []}, {"text": "When Lis used for pre - training the model , the setup is very similar .", "entities": []}, {"text": "The only difference is that the regression model , before being \ufb01ne - tuned on the task of QDE from text using Q TRAIN , is pre - trained on the task of MLM on L. Being an unsupervised task , we use the whole Lfor this .", "entities": [[33, 34, "DatasetName", "MLM"]]}, {"text": "Transformers are implemented with the transformers7library from HuggingFace ; \ufb01ne - tuning and pre - training are performed with TensorFlow8 .", "entities": []}, {"text": "Hyperparameters are shown in Appendix C. 7 Results 7.1 Evaluation of the input con\ufb01gurations Before comparing the Transformer models with the state of the art , we show here the performance of the different con\ufb01gurations , both with and without the additional pre - training on MLM .", "entities": [[17, 18, "MethodName", "Transformer"], [46, 47, "DatasetName", "MLM"]]}, {"text": "Table 1 displays the Mean Absolute Error ( MAE ) and the Root Mean Squared Error ( RMSE ) of the different con\ufb01gurations ; the error is the difference between the IRT dif\ufb01culty and the estimation of the Transformer model .", "entities": [[6, 7, "MetricName", "Error"], [8, 9, "MetricName", "MAE"], [15, 16, "MetricName", "Error"], [17, 18, "MetricName", "RMSE"], [38, 39, "MethodName", "Transformer"]]}, {"text": "The input con\ufb01gurations are the ones preTable 1 : Comparison of different Transformer models onCloudAcademy , with ( Y ) and without ( N ) additional MLM pre - training .", "entities": [[12, 13, "MethodName", "Transformer"], [26, 27, "DatasetName", "MLM"]]}, {"text": "Model ( MLM ) Input MAE RMSE DistilBERT ( N ) Q only 0.805 1.017 DistilBERT ( N ) Q + cor .", "entities": [[2, 3, "DatasetName", "MLM"], [5, 6, "MetricName", "MAE"], [6, 7, "MetricName", "RMSE"], [7, 8, "MethodName", "DistilBERT"], [15, 16, "MethodName", "DistilBERT"]]}, {"text": "0.799 1.019 DistilBERT ( N ) Q + all 0.794 1.013 BERT ( N ) Q only 0.807 1.022 BERT ( N ) Q + cor .", "entities": [[2, 3, "MethodName", "DistilBERT"], [11, 12, "MethodName", "BERT"], [19, 20, "MethodName", "BERT"]]}, {"text": "0.789 0.999 BERT ( N ) Q + all 0.811 1.027 DistilBERT ( Y ) Q only 0.801 1.016 DistilBERT ( Y ) Q + cor .", "entities": [[2, 3, "MethodName", "BERT"], [11, 12, "MethodName", "DistilBERT"], [19, 20, "MethodName", "DistilBERT"]]}, {"text": "0.788 0.998 DistilBERT ( Y ) Q + all 0.795 1.009 BERT ( Y ) Q only 0.809 1.023 BERT ( Y ) Q + cor .", "entities": [[2, 3, "MethodName", "DistilBERT"], [11, 12, "MethodName", "BERT"], [19, 20, "MethodName", "BERT"]]}, {"text": "0.774 0.981 BERT ( Y ) Q + all 0.794 1.005 sented in Section 4 : i ) Q only , ii)Q+correct , iii ) Q+all .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "We show the mean of the errors , measured over three independent runs with different random initializations .", "entities": []}, {"text": "The results shown here are obtained on Q TEST forCloudAcademy ; indeed , for ASSISTments the text of the possible choices is not avail7https://huggingface.co/transformers 8https://www.tensorflow.org/", "entities": []}, {"text": "152 Figure 2 : Experimental setup .", "entities": []}, {"text": "able and therefore the only possible input con\ufb01guration is Q only .", "entities": []}, {"text": "Even though there is not one model con\ufb01guration which clearly outperforms all the others , it can be seen that both the additional pre - training and the textual information of the possible choices are helpful in improving the accuracy of the estimation .", "entities": [[39, 40, "MetricName", "accuracy"]]}, {"text": "For DistilBERT without the additional MLM pre - training the best con\ufb01guration isQ+all , while in all the other cases the best input con\ufb01guration is Q+correct .", "entities": [[1, 2, "MethodName", "DistilBERT"], [5, 6, "DatasetName", "MLM"]]}, {"text": "7.2 Comparison with state of the art As baselines , we use : i ) ZeroR : predicts for all the questions the average dif\ufb01culty of the training questions .", "entities": []}, {"text": "ii ) R2DE ( Benedetto et al . , 2020b ): performs dif\ufb01culty estimation in two steps : the input texts are converted into feature arrays with TF - IDF and then the feature arrays are given as input to a Random Forest regression model , that performs the actual estimation .", "entities": []}, {"text": "We implemented R2DE using the available code9 .", "entities": []}, {"text": "iii ) ELMo ( Xue et al . , 2020 ): we re - implement this model to adapt it to our experimental datasets , and show here the results obtained with all the input con\ufb01gurations ( on our dataset , the best performing input con\ufb01guration is different than the one in the original paper ) .", "entities": [[2, 3, "MethodName", "ELMo"]]}, {"text": "Table 2 and Table 3 show the results of the experiments on QDE for CloudAcademy andASSISTments , by displaying the MAE and the RMSE obtained on Q TEST by the Transformer models and the chosen baselines ( all the possible input con\ufb01gurations are considered for the baselines ) .", "entities": [[20, 21, "MetricName", "MAE"], [23, 24, "MetricName", "RMSE"], [30, 31, "MethodName", "Transformer"]]}, {"text": "For each Transformer model , only one input con\ufb01guration is shown , as obtained in subsection 7.1 .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "In order to understand how well the models estimate the dif\ufb01culty of very easy and very challenging questions , we also show the MAE and RMSE measured on the questions whose dif\ufb01culty bis such thatjbj>2 ( also referred to as \u201c extreme \u201d questions ) .", "entities": [[23, 24, "MetricName", "MAE"], [25, 26, "MetricName", "RMSE"]]}, {"text": "The results shown are the mean and the standard deviation of the errors over three independent runs .", "entities": []}, {"text": "9https://github.com/lucabenedetto/ r2de - nlp - to - estimating - irt - parametersTable 2 shows that also R2DE and ELMo are able to leverage the text of the possible choices to improve the accuracy of the estimation ; indeed , the best performing input con\ufb01guration is Q+all for R2DE and Q+correct for ELMo .", "entities": [[18, 19, "MethodName", "ELMo"], [32, 33, "MetricName", "accuracy"], [51, 52, "MethodName", "ELMo"]]}, {"text": "The table shows that the Transformer models generally outperform the proposed baselines on both metrics , both with and without the additional pre - training on MLM .", "entities": [[5, 6, "MethodName", "Transformer"], [26, 27, "DatasetName", "MLM"]]}, {"text": "The model that consistently and significantly outperforms all the others is BERT with Q+correct and the additional MLM pre - training .", "entities": [[11, 12, "MethodName", "BERT"], [17, 18, "DatasetName", "MLM"]]}, {"text": "It is also interesting to remark that ELMo seems to perform estimations a bit biased towards high and low dif\ufb01culties : indeed , considering overall MAE and RMSE it performs at the same level of R2DE", "entities": [[7, 8, "MethodName", "ELMo"], [25, 26, "MetricName", "MAE"], [27, 28, "MetricName", "RMSE"]]}, {"text": "but it is better for the estimation of \u201c extreme \u201d questions .", "entities": []}, {"text": "This might also be the reason why ELMo performs better than DistilBERT without MLM on \u201c extreme \u201d questions but worse overall .", "entities": [[7, 8, "MethodName", "ELMo"], [11, 12, "MethodName", "DistilBERT"], [13, 14, "DatasetName", "MLM"]]}, {"text": "All models have larger errors on \u201c extreme \u201d questions than on general ones , but the increase is different for each of them : the best performing model has an increase in the MAE of 1.19 , which is lower than the other Transformers ( from 1.28 to 1.41 ) , ELMo ( 1.37 ) and R2DE ( 1.52 ) .", "entities": [[34, 35, "MetricName", "MAE"], [52, 53, "MethodName", "ELMo"]]}, {"text": "Results are similar for the RMSE : the increase is 1.19 for the best model , between 1.19 and 1.30 for the other Transformers , 1.23 for ELMo ( Q+correct ) and 1.37 for R2DE ( Q+all ) .", "entities": [[5, 6, "MetricName", "RMSE"], [27, 28, "MethodName", "ELMo"]]}, {"text": "Table 3 shows results similar to Table 2 : BERT is the best performing model and both Transformer models outperform the baselines .", "entities": [[9, 10, "MethodName", "BERT"], [17, 18, "MethodName", "Transformer"]]}, {"text": "However , we can see that the errors are larger than in the previous experiment , thus suggesting that all the models are less capable at estimating the dif\ufb01culty of the questions in ASSISTments .", "entities": []}, {"text": "There could be several reasons for this , but we believe that this limitation is mainly due to two aspects : i ) the platform allows the creation of question with images ( not available to us ) ; ii ) the language used in the dataset is \u201d less natural \u201d than in CloudAcademy ( e.g. many questions are equations with no additional text ) .", "entities": []}, {"text": "153Table 2 : Comparison with the state of the art on CloudAcademy .", "entities": []}, {"text": "Model Input MLM MAE MAE , jbj>2 RMSE RMSE , jbj>2 ZeroR - - 0.845 \u00060.000 2.527\u00060.000 1.069\u00060.000", "entities": [[2, 3, "DatasetName", "MLM"], [3, 4, "MetricName", "MAE"], [4, 5, "MetricName", "MAE"], [7, 8, "MetricName", "RMSE"], [8, 9, "MetricName", "RMSE"]]}, {"text": "2.568\u00060.000 R2DE Q only - 0.826 \u00060.001 2.397\u00060.004 1.051\u00060.001 2.468\u00060.005 R2DE Q + cor .", "entities": []}, {"text": "- 0.819 \u00060.001 2.320\u00060.005 1.033\u00060.002 2.391\u00060.005 R2DE Q + all - 0.813 \u00060.001 2.331\u00060.008 1.034\u00060.001 2.405\u00060.008 ELMo Q only - 0.833 \u00060.002 2.286\u00060.032 1.053\u00060.002 2.373\u00060.025 ELMo Q + cor .", "entities": [[16, 17, "MethodName", "ELMo"], [25, 26, "MethodName", "ELMo"]]}, {"text": "- 0.831 \u00060.008 2.184\u00060.033 1.048\u00060.010 2.276\u00060.018 ELMo Q + all - 0.839 \u00060.004 2.213\u00060.025 1.057\u00060.007 2.308\u00060.015 DistilBERT Q +", "entities": [[6, 7, "MethodName", "ELMo"], [16, 17, "MethodName", "DistilBERT"]]}, {"text": "all N 0.794", "entities": []}, {"text": "\u00060.005", "entities": []}, {"text": "2.203\u00060.044 1.013\u00060.007 2.309\u00060.036 BERT Q + cor .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "N 0.789 \u00060.010 2.118\u00060.130 0.999\u00060.017 2.222\u00060.110 DistilBERT Q + cor .", "entities": [[6, 7, "MethodName", "DistilBERT"]]}, {"text": "Y 0.788 \u00060.005 2.067\u00060.074 0.998\u00060.007 2.187\u00060.061 BERT Q + cor .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "Y 0.774\u00060.011 1.962\u00060.042 0.981\u00060.015 2.079\u00060.047 Table 3 : Comparison with the state of the art on ASSISTments .", "entities": []}, {"text": "Model Input MLM MAE MAE , jbj>2 RMSE RMSE , jbj>2 ZeroR - - 1.066 \u00060.000 2.882\u00060.000 1.424\u00060.000 3.033\u00060.000 R2DE Q only - 0.966 \u00060.001 2.408\u00060.005 1.304\u00060.001 2.700\u00060.003 ELMo Q only - 0.933 \u00060.013 2.025\u00060.052 1.255\u00060.017 2.375\u00060.036 DistilBERT Q only N 0.919 \u00060.009 1.864\u00060.059 1.239\u00060.010 2.259\u00060.037 BERT Q only N 0.911\u00060.003 1.849\u00060.074 1.228\u00060.003 2.243\u00060.038 7.3 Analysis of the best performing model We analyze here some of the characteristics of the best performing model ( i.e. BERT ) , trying to understand whether there are some question properties which particularly in\ufb02uence its accuracy .", "entities": [[2, 3, "DatasetName", "MLM"], [3, 4, "MetricName", "MAE"], [4, 5, "MetricName", "MAE"], [7, 8, "MetricName", "RMSE"], [8, 9, "MetricName", "RMSE"], [28, 29, "MethodName", "ELMo"], [37, 38, "MethodName", "DistilBERT"], [46, 47, "MethodName", "BERT"], [75, 76, "MethodName", "BERT"], [91, 92, "MetricName", "accuracy"]]}, {"text": "We perform the same analysis for R2DE , to understand whether such characteristics are a peculiarity of BERT or are shared among different models ; the choice of R2DE is motivated by the fact that it is the second best performing model on the CloudAcademy dataset ( excluding the other Transformer models ) and it uses TF - IDF to create the features , thus a non - neural approach .", "entities": [[17, 18, "MethodName", "BERT"], [50, 51, "MethodName", "Transformer"]]}, {"text": "We report here the results of three analyses , studying possible differences in the accuracy of QDE depending on i ) input length and question dif\ufb01culty , ii ) number of correct choices , and iii ) whether it is a cloze question .", "entities": [[14, 15, "MetricName", "accuracy"]]}, {"text": "First of all , in Figure 3 we show for the two datasets the distribution of questions depending on the input length and the true dif\ufb01culty ; the number of questions in each bin is represented by its color .", "entities": []}, {"text": "The two \ufb01gures show that the distribution is far for uniform and in many areas there are not enough questions to obtain signi\ufb01cant results from this analysis .", "entities": []}, {"text": "We do not show here the distribution of CloudAcademy for the Q+all input con\ufb01g - uration , which is the one used by R2DE , but it is qualitatively similar to the ones displayed .", "entities": []}, {"text": "Considering this distribution , we decided to focus only on some of the questions while analyzing the error depending on the input length and the true dif\ufb01culty .", "entities": []}, {"text": "Speci\ufb01cally , we keep questions i ) with jbj<3and len<= 110 forCloudAcademy and BERT ( 96.4 % of the questions ) ; ii ) with jbj<3andlen<= 110 forCloudAcademy and R2DE ( 96.9 % ) ; iii ) with jbj<4andlen<= 80 forASSISTments ( 94.3 % ) , there is no difference between BERT and R2DE because they use the same input con\ufb01guration .", "entities": [[13, 14, "MethodName", "BERT"], [51, 52, "MethodName", "BERT"]]}, {"text": "Figure 4 shows how the estimation error ( represented by the color ) of BERT and R2DE depends on the input length and the target dif\ufb01culty of the questions .", "entities": [[14, 15, "MethodName", "BERT"]]}, {"text": "The error heavily depends on the target dif\ufb01culty : this suggests that the two models tend to estimate dif\ufb01culties closer to 0 than the target values ( especially R2DE ) .", "entities": [[21, 22, "DatasetName", "0"]]}, {"text": "Indeed , we have observed that both the target dif\ufb01culties and the estimated dif\ufb01culties follow a Gaussian distribution , with higher variance for the target dif\ufb01culties .", "entities": []}, {"text": "There is no clear correlation between error and input length , but in some cases it seems that the error increases with the input length ( e.g. row b=\u00001 in BERT ) .", "entities": [[30, 31, "MethodName", "BERT"]]}, {"text": "Figure 5 shows the same analysis as Figure 4 for ASSIST-", "entities": []}, {"text": "154 0 50 100 150 200 Input length ( num . of words)4 2 024Difficulty 1020304050607080(a)CloudAcademy , Q+correct con\ufb01guration .", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "0 50 100 150 200 250 300 350 Input length ( num . of words)4 2 024Difficulty 50100150200250 ( b)ASSISTments .", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "Figure 3 : Distribution of questions per input length and dif\ufb01culty .", "entities": []}, {"text": "20 40 60 80 100 Input length ( num . of words)3 2 1 012Difficulty 0.00.51.01.52.02.5 ( a ) BERT . 20 40 60 80 100 120 Input length ( num . of words)3 2 1 012Difficulty 0.00.51.01.52.02.5 ( b ) R2DE .", "entities": [[19, 20, "MethodName", "BERT"]]}, {"text": "Figure 4 : CloudAcademy , estimation error depending on input length and target dif\ufb01culty .", "entities": []}, {"text": "ments .", "entities": []}, {"text": "The \ufb01ndings are very similar , but it is even more evident the fact that R2DE tends to perform predictions close to 0 ; the error of BERT depends less heavily on the dif\ufb01culty .", "entities": [[22, 23, "DatasetName", "0"], [27, 28, "MethodName", "BERT"]]}, {"text": "Again , there are no clear correlations between the input length and the accuracy of the estimation .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "InCloudAcademy , there are i ) cloze questions , in which the correct choice goes in place of an underscore in the text , and ii ) questions with a question mark at the end .", "entities": []}, {"text": "Of the 1259 test questions , 222 are cloze questions .", "entities": []}, {"text": "From the average errors for the different types of questions , we observed that both BERT and R2DE perform slightly worse on cloze questions : BERT \u2019s MAE is 0.804 on cloze questions and 0.756 on the other questions ; similarly , R2DE \u2019s MAE is 0.893 on cloze questions and 0.794 on the other questions .", "entities": [[15, 16, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"], [27, 28, "MetricName", "MAE"], [44, 45, "MetricName", "MAE"]]}, {"text": "Lastly , we looked at the average error depending on the number of correct choices : we comparedquestions with multiple correct choices ( there are 141 of them in the test set ) and the questions with one correct choice .", "entities": []}, {"text": "For BERT , the overall MAE is 0.774 and on the questions with multiple correct choices it is 0.764 ; in the case of R2DE , the MAE is 0.813 overall and 0.750 for questions with multiple correct choices .", "entities": [[1, 2, "MethodName", "BERT"], [5, 6, "MetricName", "MAE"], [27, 28, "MetricName", "MAE"]]}, {"text": "This difference between the two models might be due to the nature of R2DE itself : indeed , it uses a bag of words approach thus it does not care about the position of each word .", "entities": []}, {"text": "Instead , BERT uses contextual embeddings which depend on the position of each word and the encoding of multiple correct choices we performed ( i.e. concatenation ) might not be the better choice , especially considering that probably there are not enough questions to learn the encoding .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "155 0 10 20 30 40 50 60 70 80 Input length ( num . of words)4 3 2 1 01234Difficulty 0.00.51.01.52.02.53.0(a ) BERT . 0", "entities": [[1, 2, "DatasetName", "0"], [23, 24, "MethodName", "BERT"], [25, 26, "DatasetName", "0"]]}, {"text": "10 20 30 40 50 60 70 80 Input length ( num . of words)4 3 2 1 01234Difficulty 0.00.51.01.52.02.53.0", "entities": []}, {"text": "( b ) R2DE .", "entities": []}, {"text": "Figure 5 : ASSISTments , estimation error depending on input length and target dif\ufb01culty .", "entities": []}, {"text": "8 Conclusions In this paper we have performed a study of how Transformer models perform in the task of QDE from text , and have proposed a model which outperforms previous approaches .", "entities": [[12, 13, "MethodName", "Transformer"]]}, {"text": "Speci\ufb01cally , the proposed model is built upon a pre - trained BERT language model , which is \ufb01ne - tuned for the task of QDE from text .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "Previous approaches either require an additional dataset of documents about the same topics assessed by the questions or can not leverage such information ; differently from them the proposed model is capable of outperforming state of the art approaches being trained only on the text of the questions , and can be further improved if such additional dataset is available .", "entities": []}, {"text": "As an outcome from our analysis , we can say that : i ) if an additional dataset is available , BERT with MLM pre - training seems to be the best performing model ; ii ) if the only available data is the text of the questions , DistilBERT might be a better option , as it has basically the same performance as BERT but at a fraction of the computational cost .", "entities": [[21, 22, "MethodName", "BERT"], [23, 24, "DatasetName", "MLM"], [49, 50, "MethodName", "DistilBERT"], [64, 65, "MethodName", "BERT"]]}, {"text": "Furthermore , we studied the effect of some questions characteristics on BERT and R2DE , comparing the two models .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "We have observed that the magnitude of the error naturally increases with the magnitude of the dif\ufb01culty ( especially for R2DE ) , but there is not a clear correlation between the input length and the accuracy of the estimation .", "entities": [[36, 37, "MetricName", "accuracy"]]}, {"text": "We have also observed that both models are less accurate in estimating the dif\ufb01culty of cloze questions , compared to questions that end with a question mark , and that the decrease in accuracy is lower for BERT .", "entities": [[33, 34, "MetricName", "accuracy"], [37, 38, "MethodName", "BERT"]]}, {"text": "We believe that this happens because underscores are not frequent innatural language and thus the model has a chance of learning them only during the \ufb01ne - tuning on QDE , not during MLM pre - training .", "entities": [[33, 34, "DatasetName", "MLM"]]}, {"text": "This is probably not enough data for learning ( from scratch ) the meaning of underscores in exam questions .", "entities": []}, {"text": "Lastly , BERT performs better on questions with only one correct choice than on questions with multiple correct choices ( for the latter , it is also outperformed by R2DE ) .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "This might be due to the encoding we used for multiple correct choices , and it is worth exploring in future research .", "entities": []}, {"text": "Future works could continue to dig deeper into the analysis of the model , as we believe that the accuracy of the estimation could be further improved by using an ensemble model in which different sub - models are used depending on some characteristic of the question under calibration .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "Also , future works will try to explore the attention layers of the proposed model , as it might provide useful information about the reasons why the model works better on some questions .", "entities": [[9, 11, "HyperparameterName", "attention layers"]]}, {"text": "References Tahani Alsubait , Bijan Parsia , and Ulrike Sattler .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "A similarity - based theory of controlling mcq dif\ufb01culty .", "entities": []}, {"text": "In 2013 second international conference on elearning and e - technologies in education ( ICEEE ) , pages 283\u2013288 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Luca Benedetto , Andrea Cappelli , Roberto Turrin , and Paolo Cremonesi . 2020a .", "entities": []}, {"text": "Introducing a framework to assess newly created questions with natural language processing .", "entities": []}, {"text": "In International Conference on Arti\ufb01cial Intelligence in Education , pages 43\u201354 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Luca Benedetto , Andrea Cappelli , Roberto Turrin , and Paolo Cremonesi . 2020b .", "entities": []}, {"text": "R2de : a nlp approach to", "entities": []}, {"text": "156estimating irt parameters of newly generated questions .", "entities": []}, {"text": "In Proceedings of the Tenth International Conference on Learning Analytics & Knowledge , pages 412\u2013421 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Yasmine H El Masri , Steve Ferrara , Peter W Foltz , and Jo - Anne Baird .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Predicting item dif\ufb01culty of science national curriculum tests : the case of key stage 2 assessments .", "entities": []}, {"text": "The Curriculum Journal , 28(1):59\u201382 .", "entities": []}, {"text": "Mingyu Feng , Neil Heffernan , and Kenneth Koedinger . 2009 .", "entities": []}, {"text": "Addressing the assessment challenge with an online system that tutors as it assesses .", "entities": []}, {"text": "volume 19 , pages 243\u2013266 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Ronald K Hambleton , Hariharan Swaminathan , and H Jane Rogers .", "entities": []}, {"text": "1991 .", "entities": []}, {"text": "Fundamentals of item response theory .", "entities": []}, {"text": "Sage .", "entities": []}, {"text": "Marian Hickendorff .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "The language factor in elementary mathematics assessments : Computational skills and applied problem solving in a multidimensional irt framework .", "entities": [[4, 6, "TaskName", "elementary mathematics"]]}, {"text": "Applied Measurement in Education , 26(4):253\u2013278 .", "entities": []}, {"text": "Geoffrey Hinton , Oriol Vinyals , and Jeff Dean . 2015 .", "entities": []}, {"text": "Distilling the knowledge in a neural network .", "entities": []}, {"text": "arXiv preprint arXiv:1503.02531 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhenya Huang , Qi Liu , Enhong Chen , Hongke Zhao , Mingyong Gao , Si Wei , Yu Su , and Guoping Hu . 2017 .", "entities": []}, {"text": "Question dif\ufb01culty prediction for reading problems in standard tests .", "entities": []}, {"text": "In Thirty - First AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Ghader Kurdi , Bijan Parsia , and Uli Sattler .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "An experimental evaluation of automatically generated multiple choice questions from ontologies .", "entities": []}, {"text": "In OWL : Experiences And directions \u2013 reasoner evaluation , pages 24\u201339 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Matthew E Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In Proceedings of NAACL - HLT , pages 2227\u20132237 .", "entities": []}, {"text": "Zhaopeng Qiu , Xian Wu , and Wei Fan . 2019 .", "entities": []}, {"text": "Question dif\ufb01culty prediction for multiple choice problems in medical exams .", "entities": []}, {"text": "In Proceedings of the 28th ACM International Conference on Information and Knowledge Management , pages 139\u2013148 . ACM .", "entities": [[5, 6, "DatasetName", "ACM"], [12, 13, "TaskName", "Management"], [17, 18, "DatasetName", "ACM"]]}, {"text": "Georg Rasch .", "entities": []}, {"text": "1960 .", "entities": []}, {"text": "Probabilistic models for some intelligence and attainment tests .", "entities": []}, {"text": "Danish Institute for Educational Research .", "entities": []}, {"text": "Victor Sanh , Lysandre Debut , Julien Chaumond , and Thomas Wolf .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Distilbert , a distilled version of bert : smaller , faster , cheaper and lighter .", "entities": [[0, 1, "MethodName", "Distilbert"]]}, {"text": "arXiv preprint arXiv:1910.01108 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008 .", "entities": []}, {"text": "Quan Wang , Jing Liu , Bin Wang , and Li Guo .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A regularized competition model for question dif\ufb01culty estimation in community question answering services .", "entities": [[9, 12, "TaskName", "community question answering"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1115\u20131126 .", "entities": []}, {"text": "Kang Xue , Victoria Yaneva , Christopher Runyon , and Peter Baldwin .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Predicting the dif\ufb01culty and response time of multiple choice questions using transfer learning .", "entities": [[11, 13, "TaskName", "transfer learning"]]}, {"text": "In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 193\u2013197 .", "entities": []}, {"text": "Victoria Yaneva , Peter Baldwin , Janet Mee , et al . 2019 .", "entities": []}, {"text": "Predicting the dif\ufb01culty of multiple choice questions in a high - stakes medical exam .", "entities": []}, {"text": "In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 11\u201320 .", "entities": []}, {"text": "Victoria Yaneva et", "entities": []}, {"text": "al .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Automatic distractor suggestion for multiple - choice tests using concept embeddings and information retrieval .", "entities": [[12, 14, "TaskName", "information retrieval"]]}, {"text": "In Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications , pages 389 \u2013 398 .", "entities": []}, {"text": "A Introduction to IRT We model question dif\ufb01culty as de\ufb01ned in the oneparameter IRT model ( also named Rasch model ( Rasch , 1960 ) ) , which associates a skill level \u0012to each student and a dif\ufb01culty level bto each question .", "entities": []}, {"text": "For a given question j , its latent trait bjde\ufb01ne the item response function ( i.r.f . ) , which indicates the probability ( PC ) that a student iwith skill level \u0012i correctly answers the question .", "entities": []}, {"text": "The formula of the i.r.f . is as follows : PC=1 1 + e\u00001:7\u0001(\u0012i\u0000bj)(1 )", "entities": []}, {"text": "According to the intuition , a student with a given skill\u0012ihas a lower probability of correctly answering more dif\ufb01cult questions .", "entities": []}, {"text": "Also , if a question is too dif\ufb01cult or too easy ( i.e. bj!1 or bj ! \u00001 ) , all the students will answer in the same way ( i.e. PC!0orPC!1 ) regardless of \u0012i .", "entities": []}, {"text": "Given some students \u2019 answers to a set of questions , with IRT it is possible to estimate both the", "entities": []}, {"text": "157Table 4 : Example questions from ASSISTments .", "entities": []}, {"text": "ID Question Type 330", "entities": []}, {"text": "The computer game Peter wants to buy will cost at least $ 50 and not more than $ 70 .", "entities": []}, {"text": "He earns $ 3 an hour running errands for his grandmother .", "entities": []}, {"text": "Which inequality shows the number of hours , n , he will have to work to pay for the game?Original 326 What is the minimum cost of the game ?", "entities": []}, {"text": "Scaffolding 327 What is the maximum cost of the game ?", "entities": []}, {"text": "Scaffolding 328 Write an expression that represents the amount of money Peter earns in n hours .", "entities": []}, {"text": "Scaffolding 329 Which inequality shows the number of hours , n , Peter will have to work to pay for the game?Scaffolding Table 5 : Example question from CloudAcademy .", "entities": []}, {"text": "Role Text Question", "entities": []}, {"text": "A user has launched an EBS backed EC2 instance in the US - East-1 region .", "entities": []}, {"text": "The user wants to implement a disaster recovery ( DR ) plan for that instance by creating another instance in a European region .", "entities": []}, {"text": "How can the user accomplish this ?", "entities": []}, {"text": "Correct choice Create an AMI of the instance and copy the AMI to the EU region .", "entities": []}, {"text": "Then launch the instance from the EU AMI .", "entities": []}, {"text": "Distractor Use the \u201c Launch more like this \u201d option to copy the instance from one region to another .", "entities": []}, {"text": "Distractor Copy the instance from the US East region to the EU region .", "entities": []}, {"text": "Distractor Copy the running instance using the \u201c Instance Copy \u201d command to the EU region .", "entities": []}, {"text": "skill levels of the students and the dif\ufb01culty of the questions via likelihood maximization , by selecting the con\ufb01guration ( i.e.the \u0012s andbs ) that maximizes the probability of the observed results .", "entities": []}, {"text": "Also , it is possible to assess the knowledge level ~\u0012iof a studentifrom", "entities": []}, {"text": "the correctness of its answers to a set of calibrated assessment items Q = q1;q2;:::;qNq .", "entities": []}, {"text": "This is done by maximizing the results of the multiplication between the i.r.f . of the questions that were answered correctly and the complementary ( i.e.1\u0000PC ) of the i.r.f . of the questions that were answered erroneously .", "entities": []}, {"text": "B Example questions An example problem from ASSISTments and the corresponding scaffolding questions are shown in Table 4 .", "entities": []}, {"text": "An example question from CloudAcademy , with its correct answer and distractors , is given in Table 5 .", "entities": []}, {"text": "C Hyperparameters For \ufb01ne - tuning on QDE from text we select the hyperparameters from the following pool of candi - dates : \u2022batch size = 16 , 32 , 64 ; \u2022learning rate = 1e-5 , 2e-5 , 3 - 5 ; \u2022patience early stopping = 10 epochs ; \u2022dropout additional layer = 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ; \u2022internal dropout = 0.1 , 0.2 , 0.3 , 0.4 , 0.5 .", "entities": [[44, 46, "MethodName", "early stopping"]]}, {"text": "For the additional pre - training on MLM , the hyperparameters are selected between the following candidates : \u2022batch size = 64 ; \u2022learning rate = 1e-5 ; \u2022number of epochs = 4 , 12 , 24 , 36 ; \u2022dropout = 0.1 .", "entities": [[7, 8, "DatasetName", "MLM"]]}, {"text": "In both cases we use sequence length = 256 and theAdam optimizer .", "entities": [[11, 12, "HyperparameterName", "optimizer"]]}]