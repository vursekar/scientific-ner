[{"text": "Proceedings of the 12th International Workshop on Semantic Evaluation ( SemEval-2018 ) , pages 914\u2013918 New Orleans , Louisiana , June 5\u20136 , 2018 .", "entities": []}, {"text": "\u00a9 2018 Association for Computational Linguistics UMDuluth - CS8761 at SemEval-2018 Task 9 : Hypernym Discovery using Hearst Patterns , Co - occurrence frequencies and Word Embeddings Arshia Z. Hassan andManikya S. Vallabhajosyula andTed Pedersen Department of Computer Science University of Minnesota Duluth , MN 55812 USA { hassa418,valla045,tpederse } @d.umn.edu https://github.com/manikyaswathi/SemEval2018HypernymDiscovery Abstract Hypernym Discovery is the task of identifying potential hypernyms for a given term .", "entities": [[10, 16, "DatasetName", "SemEval-2018 Task 9 : Hypernym Discovery"], [25, 27, "TaskName", "Word Embeddings"], [53, 55, "TaskName", "Hypernym Discovery"]]}, {"text": "A hypernym is a more generalized word that is super - ordinate to more speci\ufb01c words .", "entities": []}, {"text": "This paper explores several approaches that rely on co - occurrence frequencies of word pairs , Hearst Patterns based on regular expressions , and word embeddings created from the UMBC corpus .", "entities": [[24, 26, "TaskName", "word embeddings"]]}, {"text": "Our system Babbage participated in Subtask 1A for English and placed 6th of 19 systems when identifying concept hypernyms , and 12th of 18 systems for entity hypernyms .", "entities": []}, {"text": "1 Introduction Hypernym - hyponym pairs exhibit an is - arelationship where a hypernym is a generalization of a hyponym .", "entities": []}, {"text": "The objective of SemEval\u20132018 Task 9 ( Camacho - Collados et", "entities": []}, {"text": "al . , 2018 ) is to generate a ranked list of hypernyms when given an input hyponym and a vocabulary of candidate hypernyms .", "entities": []}, {"text": "For example , the input hyponym lemongrass could yield the hypernyms [ grass , oil plant , herb ] , where herb would be the best candidate .", "entities": []}, {"text": "This scenario is illustrated in Figure 1 , where the three leaf nodes are hyponyms and the root is a hypernym .", "entities": []}, {"text": "Figure 1 : Hypernym - hyponym example", "entities": []}, {"text": "Note that hypernym discovery is distinct from hypernym detection , where the problem is to detect if a hyponym - hypernym relationship exists between a given pair , such as lemongrass - grass .", "entities": [[2, 4, "TaskName", "hypernym discovery"]]}, {"text": "In our \ufb01rst module , we retrieve candidate hypernyms for an input term using a paragraphlength context - window and calculate their cooccurrence frequencies , which is later used forranking the candidates .", "entities": []}, {"text": "Our second module uses Hearst Patterns ( Hearst , 1992 ) to extract hyponym - hypernym pairs and ranks candidate hypernyms based on co - occurrence frequency of the pairs .", "entities": []}, {"text": "Our \ufb01nal module employs wordembeddings created using word2vec ( Mikolov et al . , 2013 ) .", "entities": []}, {"text": "This paper continues with a more detailed discussion of each module , and then a review of our results .", "entities": []}, {"text": "2 Implementation Babbage begins by pre - processing ( 2.2.1 ) the UMBC Corpus ( 2.1 ) and extracting candidate hypernyms using four different strategies ( 2.2.2 ) .", "entities": []}, {"text": "The \ufb01rst and second module calculates the cooccurrence frequencies between the input term and words in context using the pre - processed UMBC Corpus and the Hearst Pattern set extracted from the UMBC Corpus .", "entities": [[14, 17, "DatasetName", "words in context"]]}, {"text": "The third module uses the IS - A Hearst Pattern set extracted from UMBC Corpus to obtain hypernyms .", "entities": [[5, 8, "DatasetName", "IS - A"]]}, {"text": "The \ufb01nal module constructs a word embedding over the UMBC corpus and uses a distance measure to fetch candidate hypernyms for a given input term .", "entities": []}, {"text": "2.1 UMBC Corpus Our training corpus is the University of Maryland , Baltimore County ( UMBC )", "entities": []}, {"text": "WebBase Corpus ( Han et al . , 2013 ) .", "entities": []}, {"text": "It contains 3 billion words from paragraphs obtained from more than 100 million web pages over various domains .", "entities": []}, {"text": "We use the 28 GB tokenized version of UMBC corpus which is part - of - speech tagged and divided among 408 \ufb01les .", "entities": [[12, 15, "DatasetName", "part - of"]]}, {"text": "There is also a vocabulary \ufb01le with 218,755 unigram , bigram and trigram hypernym terms provided by task organizers .", "entities": []}, {"text": "This \ufb01le de\ufb01nes the set of possible candidate hypernyms.914", "entities": []}, {"text": "2.2 Architecture of System Babbage", "entities": []}, {"text": "The following are the steps involved in constructing our system : 1.Pre - processing the input text corpus", "entities": []}, {"text": "[ 2.2.1 ] : Corpora obtained in this stage include : ( a ) Normalize the input corpus and store as Normalized Corpus ( b ) Fetch Hearst Patterns ( see Figure 2 ) from input corpus and store as Hearst Corpus ( c ) Fetch IS - A Pattern from the input corpus and store as IS - A Corpus ( d ) Creating the word - embedding matrix UMBC Embedding using Normalized Corpus .", "entities": [[46, 49, "DatasetName", "IS - A"], [57, 60, "DatasetName", "IS - A"]]}, {"text": "The Hearst Corpus and the IS - A Corpus patterns are extracted from the original text corpus which has been preprocessed to eliminate punctuation , prepositions , and conjunctions .", "entities": [[5, 8, "DatasetName", "IS - A"]]}, {"text": "All possible combinations of bigram and trigram noun phrases are retained in the Normalized Corpus .", "entities": []}, {"text": "A Word - Embedding matrix is built over this Normalized Corpus .", "entities": []}, {"text": "2.Extracting candidate hypernyms", "entities": []}, {"text": "[ 2.2.2 ] : ( a)Co - occurrence frequencies from Normalized Corpus : A co - occurrence map is built for the input terms with the words in the context of the input term and the frequency of their co - occurrence using the Normalized Corpus .", "entities": []}, {"text": "Words with co - occurrence frequency higher than5are listed as candidate hypernyms for an input term .", "entities": []}, {"text": "This is considered the \ufb01rst module result .", "entities": []}, {"text": "( b)Co - occurrence frequencies from Hearst Corpus : A co - occurrence map similar to the previous step is built by using the Hearst Corpus .", "entities": []}, {"text": "All the words which occur at least once in context of the input term in the Hearst Patterns are listed as candidate hypernyms for this term .", "entities": []}, {"text": "This is considered the second module result .", "entities": []}, {"text": "( c)Co - occurrence frequencies from IS - A Corpus : All the words which occur at least once in the context of the input term in the IS - A Corpus are listed ascandidate hypernyms for this term .", "entities": [[6, 9, "DatasetName", "IS - A"], [28, 31, "DatasetName", "IS - A"]]}, {"text": "If the input term is a concept and is a bigram or trigram term , then part of it is considered as a hypernym for that term .", "entities": []}, {"text": "This is considered the third module result .", "entities": []}, {"text": "( d)Applying word similarity to word embeddings : A \ufb01xed distance value called Phiis used to extract words at this distance to the input term in the UMBC Embedding .", "entities": [[2, 4, "TaskName", "word similarity"], [5, 7, "TaskName", "word embeddings"]]}, {"text": "These words are listed as the candidate hypernyms for an input term .", "entities": []}, {"text": "This is considered our \ufb01nal module result .", "entities": []}, {"text": "3.Merging results from various modules [ 2.3 ] : The order of merging these results is decided by the evaluation scores from these modules for training data .", "entities": []}, {"text": "The same order is applied to the test data .", "entities": []}, {"text": "2.2.1 Pre - processing The task description states that our system should predict candidate hypernyms for an input word which is either a concept or an entity .", "entities": []}, {"text": "Hence , the part - of - speech tag for all candidate hypernyms is noun .", "entities": [[3, 6, "DatasetName", "part - of"]]}, {"text": "This restricts our search space to words with noun part - of - speech tag and bigram or trigram phrases with a noun head word .", "entities": [[9, 12, "DatasetName", "part - of"]]}, {"text": "Our system focuses on concepts , so we do not have any module speci\ufb01c for entities .", "entities": []}, {"text": "To re\ufb01ne the input corpus as per these speci\ufb01cations , the input UMBC Corpus is processed through the following modules : Normalized Corpus : The POS tagged input corpus is processed per paragraph .", "entities": []}, {"text": "Each paragraph is converted to lower - case text .", "entities": []}, {"text": "Then , bigram and trigram noun phrases from each paragraph are obtained using the POS tags given for each word .", "entities": []}, {"text": "It is further \ufb01ltered by removing punctuation marks and words with part - of - speech tags other than noun , verb , adverb oradjective .", "entities": [[11, 14, "DatasetName", "part - of"]]}, {"text": "This \ufb01ltered line is modi\ufb01ed by appending it with bigram and trigram noun phrases obtained earlier .", "entities": []}, {"text": "Hearst Corpus : The original input paragraph is searched for the Hearst Patterns ( shown in Figure 2 ) and all the possible matches are returned in the form of hypernym : one or more hyponyms .", "entities": []}, {"text": "Figure 2 shows the extraction of Hearst Patterns , where NP represents a noun - phrase where the head word is tagged as a noun , the loved - ones such as family and friends is a match for Hearst Patterns ( from915", "entities": []}, {"text": "Figure 2 ) with noun phrases the loved - ones , family andfriends .", "entities": []}, {"text": "IS - A Corpus : A pattern which is not used in the construction of the Hearst Corpus is used here : Hyponym Noun Phrase is ( a|an|the)Hypernym Noun Phrase .", "entities": [[0, 3, "DatasetName", "IS - A"]]}, {"text": "Here the original input paragraph is searched against this pattern and all the possible matches are returned in the form of hyponym : hypernym .a fennel is aplant is a match for this pattern with noun phrases a fennel and plant .", "entities": []}, {"text": "UMBC Embedding : A word embedding matrix is created over the Normalized Corpus using word2vec ( Mikolov et al . , 2013 ) .", "entities": []}, {"text": "The speci\ufb01cations of the model are as follows : ( a)Model : Continuous Bag of Words ( CBOW ) - a term \u2019s embedding value is determined by its context words .", "entities": []}, {"text": "The order of the words in the window size does not matter .", "entities": []}, {"text": "( b)Window Size : 10 .", "entities": []}, {"text": "The context window size for a term which determines its vector value .", "entities": []}, {"text": "( c)Minimum Frequency Count : 5 .", "entities": []}, {"text": "If the frequency of a word is less than this value , the word does not exist in the embedding .", "entities": []}, {"text": "( d)Embedding Dimension Size : 300 .", "entities": []}, {"text": "The number of dimensions for the embedding matrix .", "entities": []}, {"text": "2.2.2", "entities": []}, {"text": "Extracting candidate hypernyms :", "entities": []}, {"text": "Once the UMBC corpus is pre - processed and the three required corpora and an embedding matrix are derived , candidate hypernyms are acquired by applying the below processes .", "entities": []}, {"text": "Co - occurrence frequency from Normalized Corpus : With this module , we hypothesized that a hyponym and its possible hypernyms are more likely to co - occur within a context - window .", "entities": []}, {"text": "The context window of a term is its own paragraph .", "entities": []}, {"text": "We start by creating a map for all the input terms .", "entities": []}, {"text": "If a normalized paragraph 2.2.1 contains any of the input terms , then all the words in the context are added to the map of this particular term which considers them to be hypernyms for this input hyponym term .", "entities": []}, {"text": "Every time a hypernym - hyponym pair co - occurs in one line , their co - occurrence count is increased by one .", "entities": []}, {"text": "Finally , the candidate hypernyms are ranked in descending order of their co - occurrence frequencies .", "entities": []}, {"text": "Co - occurrence frequency from Hearst Corpus :", "entities": []}, {"text": "In the pre - processing step 2.2.1 , we extracted possible hypernym - hyponym mapping data usingHearst Patterns .", "entities": []}, {"text": "Each line of the data is of the form hypernym : hyponym-1 , hyponym-2 , , hyponymn .", "entities": []}, {"text": "In this module , we created a map where each hyponym is a key mapped to hypernyms occurring with that hyponym and their co - occurrence frequencies .", "entities": []}, {"text": "For example , values for keys hyponym1 , hyponym-2 , and hyponym - n are updated with hypernym and the frequencies are increased by 1 .", "entities": []}, {"text": "Finally the top 15 hyponyms ( based on frequencies ) for each key are reported as the result hypernyms .", "entities": []}, {"text": "Co - occurrence frequencies from IS - A Corpus : This module uses hypernym - hyponym pairs from the IS - A Corpus 2.2.1 which are in the form hyponym : hypernym .", "entities": [[5, 8, "DatasetName", "IS - A"], [19, 22, "DatasetName", "IS - A"]]}, {"text": "We use the same strategy as Co - occurrence frequency from Hearst Corpus to obtain the result .", "entities": []}, {"text": "Applying word similarity to word embeddings : We need a general distance vector which represents a hypernym - hyponym distance in the UMBC Embedding .", "entities": [[1, 3, "TaskName", "word similarity"], [4, 6, "TaskName", "word embeddings"]]}, {"text": "We use training data input term ( x ) and the gold data hypernyms ( y ) to calculate this distance ( \u03a6\u2217 ) which is calculated by : \u03a6\u2217= argmin \u03a61 N / summationdisplay ( x , y)\u03a6 / bardblx\u2212y / bardbl2(1 ) \u03a6is used to get candidate hypernyms from the UMBC word embedding matrix for the input terms ( test data ) .", "entities": []}, {"text": "2.3 Merging results from various modules For this task , our system is required to report the 15 most probable hypernyms for each input term .", "entities": []}, {"text": "We have four modules each reporting their top 15 candidate hypernyms .", "entities": []}, {"text": "By looking at the training scores of these modules , we merge the co - occurrence frequencies from IS - A corpus that have higher ranks followed by the co - occurrence frequencies from Normalized corpus and Hearst Pattern corpus .", "entities": [[18, 21, "DatasetName", "IS - A"]]}, {"text": "Results from word embedding module are given the lowest ranks .", "entities": []}, {"text": "3 Experimental Results and Discussion Output candidate hypernym lists are evaluated against gold hypernym lists using the following evaluation criteria : Mean Reciprocal Rank ( MRR ) , Mean Average Precision ( MAP ) and Precision At k ( P@k ) , where k is 1 , 3 , 5 and 15 .", "entities": [[25, 26, "MetricName", "MRR"], [29, 31, "MetricName", "Average Precision"], [32, 33, "DatasetName", "MAP"], [35, 36, "MetricName", "Precision"]]}, {"text": "We ran our model against two sets of data training data916", "entities": []}, {"text": "Figure 2 : Creating Hearst Corpus using 6 Hearst Patterns", "entities": []}, {"text": "Cooc Hearst Phi Is - A Merged MRR .103 .020 .025 .165 .188 MAP .050 .008 .012 .071 .080 P@1 .061 .013 .012 .140 .152 P@3 .055 .008 .012 .076 .087 P@5 .048 .008 .012 .066 .075 P@15 .047 .007 .011", "entities": [[3, 6, "DatasetName", "Is - A"], [7, 8, "MetricName", "MRR"], [13, 14, "DatasetName", "MAP"], [19, 20, "MetricName", "P@1"]]}, {"text": ".062 .070 Table 1 : Test Data 1A English - Concept Scores", "entities": []}, {"text": "Cooc Hearst Phi Is - A Merged MRR", "entities": [[3, 6, "DatasetName", "Is - A"], [7, 8, "MetricName", "MRR"]]}, {"text": ".000 .000 .008 .090 .099 MAP .000 .000 .003 .036 .037 P@1 .000 .000 .004 .069 .081 P@3 .000 .000 .003 .041 .045 P@5 .000 .000 .003 .035 .036 P@15 .000 .000 .003 .031 .030 Table 2 : Test Data 1A English - Entity Scores and test data with 1500 input terms each .", "entities": [[5, 6, "DatasetName", "MAP"], [11, 12, "MetricName", "P@1"]]}, {"text": "These results are shown in Tables 1 and 2 , where it can be clearly observed that our system performs much better for concepts .", "entities": []}, {"text": "However , the IS - A module seemed to fetch good candidates for both entity and concept data .", "entities": [[3, 6, "DatasetName", "IS - A"]]}, {"text": "The gold data provided with the task does not always consider all possible word senses or domains of an input term .", "entities": []}, {"text": "As a result , we observed numerous candidate hypernyms that seem to be plausible solutions that are not considered correct when compared to the gold data .", "entities": []}, {"text": "For example , the input concept navigator has gold standard hypernyms of [ military branch , ex - plorer , military machine , travel , adventurer , seaman ] .", "entities": []}, {"text": "Our system \ufb01nds candidate hypernyms [ browser , web browser , website , application ] .", "entities": []}, {"text": "We also noticed that due to our normalization decisions ( i.e. , using all lower - case characters ) and the contents of the corpus , Babbage performs poorly in some cases .", "entities": []}, {"text": "For example , the gold hypernyms for input entity Hurricane are[video game , software program , computer program ] but our system produced [ storm , windstorm , typhoon , tornado , cyclone ] .", "entities": []}, {"text": "Clearly , our system did not differentiate between the named entity Hurricane and the common noun hurricane while training the word \u2013 embedding models .", "entities": []}, {"text": "On the positive side , our system produced promising results in some cases .", "entities": []}, {"text": "Hyponym liberalism produced [ theory , philosophy , economic policy ] which is very similar to the gold data [ economic theory , theory ] .", "entities": []}, {"text": "It also correctly generated the hyponym person for hypernyms such as collector , moderator , director , senior , and reporter .", "entities": []}, {"text": "For input reporter it produced [ writer , person ] which matches the gold hypernym set .", "entities": []}, {"text": "Acknowledgments This project was carried out as a part of CS 8761 , Natural Language Processing , a graduate level class offered in Fall 2017 at the University of Minnesota , Duluth by Dr. Ted Pedersen .", "entities": [[10, 11, "DatasetName", "CS"]]}, {"text": "All authors of this paper have contributed equally and are listed in alphabetical order by \ufb01rst name.917", "entities": []}, {"text": "References Jose Camacho - Collados , Claudio Delli Bovi , Luis Espinosa - Anke , Sergio Oramas , Tommaso Pasini , Enrico Santus , Vered Shwartz , Roberto Navigli , and Horacio Saggion .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SemEval-2018 Task 9 : Hypernym Discovery .", "entities": [[0, 6, "DatasetName", "SemEval-2018 Task 9 : Hypernym Discovery"]]}, {"text": "In Proceedings of the 12th International Workshop on Semantic Evaluation ( SemEval-2018 ) .", "entities": []}, {"text": "Association for Computational Linguistics , New Orleans , LA , United States .", "entities": []}, {"text": "Lushan Han , Abhay L. Kashyap , Tim Finin , James May\ufb01eld , and Johnathan Weese .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Umbc ebiquity - core : Semantic textual similarity systems .", "entities": [[5, 8, "TaskName", "Semantic textual similarity"]]}, {"text": "In proceedings of the Second Joint Conference on Lexical and Computational Semantics . Association for Computational Linguistics .", "entities": []}, {"text": "Marti A. Hearst . 1992 .", "entities": []}, {"text": "Automatic acquisition of hyponyms from large text corpora .", "entities": []}, {"text": "In Proceedings of the 14th Conference on Computational Linguistics Volume 2 . Association for Computational Linguistics , Stroudsburg , PA , USA , COLING \u2019 92 , pages 539\u2013545 .", "entities": []}, {"text": "https://doi.org/10.3115/992133.992154 .", "entities": []}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "CoRR abs/1301.3781 .", "entities": []}, {"text": "http://arxiv.org/abs/1301.3781.918", "entities": []}]