[{"text": "Proceedings of the Second Workshop on Abusive Language Online ( ALW2 ) , pages 66\u201374 Brussels , Belgium , October 31 , 2018 .", "entities": [[6, 8, "TaskName", "Abusive Language"]]}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics66Aggressive language in an online hacking forum Andrew Caines , Sergio Pastrana , Alice Hutchings & Paula Buttery Department of Computer Science & Technology University of Cambridge Cambridge , U.K. apc38@cam.ac.uk , sp849@cam.ac.uk , alice.hutchings@cl.cam.ac.uk , paula.buttery@cl.cam.ac.uk Abstract We probe the heterogeneity in levels of abusive language in different sections of the Internet , using an annotated corpus of Wikipedia page edit comments to train a binary classi\ufb01er for abuse detection .", "entities": [[30, 31, "DatasetName", "Cambridge"], [31, 32, "DatasetName", "Cambridge"], [49, 51, "TaskName", "abusive language"], [73, 75, "TaskName", "abuse detection"]]}, {"text": "Our test data come from the CrimeBB Corpus of hacking - related forum posts and we \ufb01nd that ( a ) forum interactions are rarely abusive , ( b ) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain , and tends to involve aggressive posturing rather than hate speech or threats of violence .", "entities": [[31, 33, "TaskName", "abusive language"], [59, 61, "DatasetName", "hate speech"]]}, {"text": "We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions , and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments .", "entities": []}, {"text": "Further work remains to be done to compare these results with other inter - domain classi\ufb01cation experiments , and to understand the impact of aggressive language in forum conversations .", "entities": []}, {"text": "1 Introduction The automatic identi\ufb01cation of abusive language online1is of growing interest and concerns have proliferated about aggressive Internet behaviours commonly known as \u2018 trolling \u2019 .", "entities": [[6, 8, "TaskName", "abusive language"]]}, {"text": "From an applications perspective , the accurate detection of vitriolic language is one of the clearest examples of natural language processing for social good , assuming data has been collected ethically and stored legally , and that any intervention is left to the appropriate authorities ( Kennedy et al . , 2017 ; Kumar et al . , 2018 ) .", "entities": [[53, 54, "DatasetName", "Kumar"]]}, {"text": "Meanwhile from a theoretical 1Note that this paper quotes texts which many will \ufb01nd offensive and/or upsetting .", "entities": []}, {"text": "Please contact the authors if you would prefer to read the article with all quotations removed.point of view , there are many outstanding linguistic and sociological research questions surrounding Internet aggression and how it manifests itself in writing ( Pieschl et al . , 2015 ; Waseem et", "entities": []}, {"text": "al . , 2017 ) .", "entities": []}, {"text": "The question we address here is whether online abusive language is of one type or whether there is discernible variation in the level of abuse found in different subsections of the Internet .", "entities": [[8, 10, "TaskName", "abusive language"]]}, {"text": "We do not claim to have the \ufb01nal answer to this nebulous question , but instead we have addressed one small part of the whole : is the level of abuse found in one Internet domain \u2013 namely discussions about English Wikipedia page edits \u2013 similar to that found in another domain , that of an online hacking forum ?", "entities": []}, {"text": "We show that the type of abusive language occurring in the latter is more closely aligned with the milder levels of abuse of those found in Wikipedia discussions , and consider why this might be .", "entities": [[6, 8, "TaskName", "abusive language"]]}, {"text": "We observe that the online hacking forum tends to contain texts aimed at helping or informing other users , whereas the Wikipedia conversations are inherently more adversarial since they relate to recent page edits and disputes arising .", "entities": []}, {"text": "Where abusive language is found in the online hacking forum , it tends to involve profane namecalling , insults and heated disputes , rather than hate speech or threats of violence \u2013 those which have tended to be the more prominent causes for public concern .", "entities": [[1, 3, "TaskName", "abusive language"], [25, 27, "DatasetName", "hate speech"]]}, {"text": "Note here that we make a distinction between aggressive andoffensive language : the former often involves the latter , but not always so .", "entities": []}, {"text": "Offensive language \u2013 identi\ufb01able word tokens such as swearwords and the like \u2013 may offend but is not always used aggressively ; sometimes it is used in a jocular fashion , for example .", "entities": []}, {"text": "Aggressive language , which more often than not is built on the composition of many words , involves a hostile stance from", "entities": []}, {"text": "67one speaker or writer to another .", "entities": []}, {"text": "It is this which might seem to be abusive and which we seek to automatically detect and better understand .", "entities": []}, {"text": "We also distinguish aggressive language from hate speech \u2013 that which might be characterised as prejudicial diatribes to provoke action , perhaps violent , against a group or groups \u2013 and from cyberbullying \u2013 that which involves a sustained period of persecution against an individual or individuals .", "entities": [[6, 8, "DatasetName", "hate speech"]]}, {"text": "Certainly the distinctions are fuzzy at the edges , but these might be thought of as the canonical de\ufb01nitions of these abuse types .", "entities": []}, {"text": "We are dealing with what we deem to be one - off instances of aggression in online communities , though if these were shown to be prejudicial against a group , or sustained against an individual , then the instances start to move into hate speech or cyberbullying behaviours .", "entities": [[44, 46, "DatasetName", "hate speech"]]}, {"text": "In both Wikipedia edits and the online hacking forum , abusive comments are infrequent in the community as a whole and the general objective of gaining reputation in the domain dis - incentivises aggressive behaviour .", "entities": []}, {"text": "Nevertheless we show that aggressive language which does occur may be detected fairly well by training on the Wikipedia edits corpus \u2013 the advantage being that it has been multiply and widely annotated \u2013 and setting the threshold for a binary aggression classi\ufb01er at a fairly moderate level relative to the worst types of abuse found in Wikipedia comments .", "entities": []}, {"text": "Future work remains to be done to more broadly characterise intra - community behaviour in different subsections of the Internet .", "entities": []}, {"text": "2 Related work Offensive language serves many purposes in everyday discourse : from deliberate effect in humour to self - directed profanity to toxic or abusive intent .", "entities": []}, {"text": "We are not concerned here with humorous uses of offensive language or with general profanity .", "entities": []}, {"text": "Instead we are interested in toxic and abusive behaviour , speci\ufb01cally online harassment involving abusive language , aggression and personal attacks .", "entities": [[14, 16, "TaskName", "abusive language"]]}, {"text": "There has been work on other forms of abusive behaviour , such as hate speech ( Warner and Hirschberg , 2012 ; Kwok and Wang , 2013 ; Ribeiro et al . , 2018 ) and cyberbullying ( Xu et", "entities": [[13, 15, "DatasetName", "hate speech"]]}, {"text": "al . , 2013 ; Pieschl et al . , 2015 ) , and we put these aside for now as challenging , distinct topics ( though with the fuzzy edges described above ) .", "entities": []}, {"text": "In terms of online harassment , previous workhas centred around de\ufb01nitions , automatic detection , and dataset creation \u2013 for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpus ( Waseem and Hovy , 2016 ; Wulczyn et al . , 2017 ) .", "entities": [[15, 17, "DatasetName", "and dataset"], [22, 24, "DatasetName", "Hate Speech"]]}, {"text": "Most work has been conducted on English data , with some extensions to other languages ( e.g. Arabic ( Mubarak et al . , 2017 ) , Slovene ( Fi\u0161er et al . , 2017 ) ) .", "entities": []}, {"text": "Automated detection approaches have drawn on classic document classi\ufb01cation methods for spam detection and sentiment analysis , and tend to use lexical and syntactic features ( Nobata et al . , 2016 ; Li et al . , 2017 ; Bourgonje et al . , 2018 ) .", "entities": [[11, 13, "TaskName", "spam detection"], [14, 16, "TaskName", "sentiment analysis"]]}, {"text": "Machine learning techniques range from logistic regression ( Cheng et al . , 2015 ) to support vector machines ( Yin et al . , 2009 ) to neural networks ( Gamb\u00e4ck and Sikdar , 2017 ) .", "entities": [[5, 7, "MethodName", "logistic regression"]]}, {"text": "Our aim here is not especially to push the boundaries on detection techniques \u2013 though naturally we wish our classi\ufb01er to perform fairly well \u2013 but rather we are interested in how to make use of existing labelled training data when predicting personal attacks in other corpora .", "entities": []}, {"text": "In case any persuasion is needed that improved understanding , detection and action on abusive language are desirable , there is evidence that experience of online harassment leads to decreased online participation and is connected with oppression , violence and suicide ( Dinakar et al . , 2011 ; Sood et", "entities": [[14, 16, "TaskName", "abusive language"]]}, {"text": "al . , 2012 ; Wulczyn et al . , 2017 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Of course there may be reasons to be concerned about the perpetrator \u2019s wellbeing along with that of the victims ( Cheng et al . , 2017 ) .", "entities": []}, {"text": "3 Training & test corpora", "entities": []}, {"text": "We have an inter - corpus experimental design , in which a document classi\ufb01er is trained on one dataset and tested on other datasets .", "entities": []}, {"text": "Our training data come from the Wikipedia Comments Corpus ( WikiComments ) ( Wulczyn et al . , 2017 ) , which contains 115,864 discussion posts extracted from an English Wikipedia dump , judged as personal attacks or harassment by crowdworkers .", "entities": []}, {"text": "Ten judgements were collected for each post ; hence we have anattack score from zero to ten for every post2 , and we assume that the higher the attack score the greater the linguistic aggression shown in writing .", "entities": []}, {"text": "This assumption may be challenged , as we accept that there are many reasons why a text may not be unanimously judged to be an attack or ha2Note that the original authors scaled the attack score between 0 and 1 , whereas we re - scale the scores from 0 to 10 .", "entities": [[37, 38, "DatasetName", "0"], [49, 50, "DatasetName", "0"]]}, {"text": "68rassment \u2013 properties of the text such as poor grammar which obfuscates meaning , use of slang insults which are not universally known , or sarcastic phrasing which is not interpreted as an attack by all annotators .", "entities": []}, {"text": "On the other hand , properties of the annotator , such as fatigue or inattention , inexperience with English or the terminology used , or idiosyncratic linguistic thresholds for attacks and harassment , could all play a part in judgement variation as well .", "entities": []}, {"text": "However , over such a large dataset we assume that in terms of aggressive language the texts will be broadly well ordered by their attack scores .", "entities": []}, {"text": "Table 1 shows examples randomly drawn from each attack score , zero to ten , along with the number of posts in each class , and the cumulative size of the corpus in reverse order from attack score ten to zero .", "entities": []}, {"text": "The curators of WikiComments used these annotated discussion posts to train a classi\ufb01er and further label unseen posts in a larger collection of 63 million discussion posts , with a view to largescale analyses of attacks by unregistered users , moderator actions in response to attacks , and more ( Wulczyn et al . , 2017 ) .", "entities": []}, {"text": "They experimented with different thresholds twhere attack scores at or abovetwould be labelled as attacks , and those belowtwould not be attacks .", "entities": []}, {"text": "They found that the optimal value for tbalancing precision and recall was 4.25 .", "entities": []}, {"text": "Our intention is to take the texts and attack scores from WikiComments to train a binary aggression classi\ufb01er for use with other corpora .", "entities": []}, {"text": "The question with such a classi\ufb01er is how to partition the training data for true / false aggression labels : the cut - off could be any attack score value from one to ten .", "entities": []}, {"text": "In the following sections we report on classi\ufb01cation experiments with each attack score cut - off value and a test corpus sourced from Internet forums .", "entities": []}, {"text": "Our test data come from the CrimeBB Corpus3 , a dataset harvested from several hackingrelated websites including HackForums , Antichat and Greysec ( Pastrana et al . , 2018 ) .", "entities": []}, {"text": "The corpus currently contains both English and Russian language data , with plans to incorporate other languages in future .", "entities": []}, {"text": "We opted to work only with posts from the HackForums website4 , it being the most popular English language hacking site worldwide .", "entities": []}, {"text": "Among other author intents such as helpfulness , 3Available by application to the Cambridge Cybercrime Centre , https://www.cambridgecybercrime.uk 4https://hackforums.netdisapproval , sarcasm and gratitude , we manually labelled author aggression as indicated by abusive language in a total of 4123 posts randomly sampled from a selection of HackForums bulletin boards ( themed discussion pages ) from November 2007 to January 2018 .", "entities": [[13, 14, "DatasetName", "Cambridge"], [32, 34, "TaskName", "abusive language"]]}, {"text": "All boards are related to hacking ( such as \u2018 Cryptography , Encryption , and Decryption \u2019 , \u2018 Keyloggers \u2019 , and \u2018 Remote Administration Tools \u2019 ) , as opposed to other interests represented on HackForums such as gaming , entertainment and graphics .", "entities": []}, {"text": "Three annotators labelled 2200 posts and agreed to a \u2018 moderate degree \u2019 according to Landis & Koch \u2019s framework for interpreting Fleiss \u2019s kappa ( Fleiss , 1971 ; Landis and Koch , 1977 ) \u2013 i.e. \u0014=0.4 to 0.6 .", "entities": []}, {"text": "We did not attempt to settle on single annotations for each post , but instead treated all judgements equally , allowing multiple labels both by individual annotators and across different annotators .", "entities": []}, {"text": "A single annotator further labelled the remaining 1923 posts .", "entities": []}, {"text": "Posts with aggressive intent are uncommon on HackForums , with only 100 aggressive posts judged to be aggressive by at least one annotator in the total corpus of 4123 posts ( 2.4 % ) .", "entities": []}, {"text": "Note that profane language is more commonly found \u2013 which is unsurprising given the casual linguistic register \u2013 with 201 posts in this dataset featuring at least one of \u2018 fuck , shit , cunt , jerk , crap , dick \u2019 ( or derived forms ) .", "entities": []}, {"text": "However , the profanity is often used for humorous purposes , or to defuse potentially confrontational conversations , or simply in a casual way for no purpose at all ; it is not always used aggressively ( hence the need for manual annotation ) .", "entities": []}, {"text": "This observation underlines the distinction between offensive and aggressive language .", "entities": []}, {"text": "Table 2 shows the size of the CrimeBB Corpus , the HackForums subset , and the annotated posts , along with examples of aggressive and nonaggressive posts from HackForums .", "entities": []}, {"text": "4 Classi\ufb01cation experiments We trained a binary aggression classi\ufb01er on the WikiComments Corpus setting the true / false thresholdtat each attack score from 1 to 10 and testing the classi\ufb01er on our annotated set of 4123 HackForums posts from the CrimeBB Corpus .", "entities": []}, {"text": "4.1 Evaluation We are interested in the successful classi\ufb01cation of aggressive posts only and therefore , rather than reporting precision , recall and F - measures , we re-", "entities": []}, {"text": "69Attack scoreExample N.postsCumulative n.posts 10fuck u asshole go fuck your own mother fuck fuck fuck fuck a duck fucked a kangaroo you piece of shit go suck your own balls.3223 3223 9drop dead .", "entities": []}, {"text": "you \u2019re a cancer on wikipedia , and i assume you \u2019re a cancer on the rest of the world as well .", "entities": []}, {"text": "your baby will do better without you.2842 6065 8oh", "entities": []}, {"text": "right , like \u201c this article sucks and you know it . \u201d is n\u2019t abusive language .", "entities": [[15, 17, "TaskName", "abusive language"]]}, {"text": "hypocrite .", "entities": []}, {"text": "eat shit and die , loser.2501 8566 7the link is there .", "entities": []}, {"text": "i have an idea .", "entities": []}, {"text": "why not delete the page and then re create it a few days later you fucking retard ?", "entities": []}, {"text": "have you a life at all ?", "entities": []}, {"text": "is it your life to go around and delete everything here ?", "entities": []}, {"text": "why not go outside and build a snowman you loser?2195 10,761 6next time you remove a section without \ufb01rst following procedures , i will have you permanently barred from wikipedia .", "entities": []}, {"text": "i wrote the section and it is completely accurate an unbiased as reported .", "entities": []}, {"text": "you are a fool and a vandal and i will be closely monitoring your activities on anything i write.2706 13,467 5so", "entities": []}, {"text": "this looks like a vendetta .", "entities": []}, {"text": "it is stupid .", "entities": []}, {"text": "you are pissing me off .", "entities": []}, {"text": "this is not what we are here for.2263 15,730 4everybody ! ! !", "entities": []}, {"text": "everybody ! ! ! !", "entities": []}, {"text": "redacted is the undigested part of a giant squid stuck in a sperm whales intestines and testicles4133 19,863 3hello , hoaxer .", "entities": []}, {"text": "thank you for posting on my talk page .", "entities": []}, {"text": "it appears that you enjoy wasting hours \ufb01rst hoaxing and then arguing about it with wikipedia editors on discussion and user pages .", "entities": []}, {"text": "all one needs to is track your ip army to see that you are the hoaxer .", "entities": []}, {"text": "nobody is falling for your nonsense , especially when you do n\u2019t sign your posts.6280 26,143 2i am aware that most bible thumping christians want to burn this guy alive .", "entities": []}, {"text": "i \ufb01nd your assessment far from neutral , i will agf here , but your tone is vitriolic.9408 35,551 1the new title does n\u2019t convey what i wanted the section to be about , think of title that conveys the question not just the general subject matter.22,548 58,099 0 in a legal brief , one might well exclude trial court opinions .", "entities": [[48, 49, "DatasetName", "0"]]}, {"text": "in an encyclopedia article , it \u2019s a different story , especially when the trial court opinion predates the appellate decision by decades.57,765 115,864 Table 1 : Examples , the number of posts , and the cumulative size ( in reverse order ) for each attack score subset of the Wikipedia Comments Corpus ( Wulczyn et al . , 2017 ) .", "entities": []}, {"text": "70Corpus Example N.posts CrimeBB 57,733,219 HackForums 40,152,443 Annotated dataset 4123 Non - aggressivemy bet would be install linux and then use spoo\ufb01ng via", "entities": []}, {"text": "that4023 Aggressivekill yourself .", "entities": []}, {"text": "most retarded advice you could give him100 Table 2 : Examples and the number of posts in subsets of the CrimeBB Corpus ( Pastrana et al . , 2018 ) .", "entities": []}, {"text": "port accuracy as in equation ( 1 ): Accuracy = true positives true positives + false negatives ( 1 ) 4.2 Method All test and training texts were lower - cased and transformed into document - term matrices using thetext2vec package for R ( Selivanov and Wang , 2017 ) .", "entities": [[1, 2, "MetricName", "accuracy"], [8, 9, "MetricName", "Accuracy"]]}, {"text": "For each value of threshold tfrom 1 to 10 , the training texts were assigned true and false labels according to their attack score swhere aggression is true if s\u0015t .", "entities": []}, {"text": "We trained an extreme gradient boosting ( XGBoost ) classi\ufb01er with the R package xgboost", "entities": []}, {"text": "( Chen et al . , 2018 ) .", "entities": []}, {"text": "Boosting is an additive technique whereby new models are added to correct the errors made by existing models thus far : models are added sequentially until no further improvements can be made .", "entities": []}, {"text": "In gradient boosting , new models predict the residuals or errors of prior models using a gradient descent algorithm .", "entities": []}, {"text": "XGBoost is known to work well with sparse matrices , which is the kind of input associated with textual data , and in NLP terms has been shown to perform competitively in sentiment analysis shared tasks ( Nasim , 2017 ; Jabreel and Moreno , 2018 ) .", "entities": [[32, 34, "TaskName", "sentiment analysis"]]}, {"text": "To avoid over-\ufb01tting we set parameters fairly conservatively , with a maximum tree depth of 6 , the number of rounds at 10 and early stopping set to 5 , gamma at 1 , and the learning rate at 0.3 .", "entities": [[24, 26, "MethodName", "early stopping"], [30, 31, "HyperparameterName", "gamma"], [36, 38, "HyperparameterName", "learning rate"]]}, {"text": "We report classi\ufb01er accuracy according to equation ( 1 ) on gold aggression : true labels in our CrimeBB test corpus .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "Recall that we do not compare XGBoost with other classi\ufb01ers , as our focus is on the training data rather than performance .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "In future work we can investigate other models including neural networks , though logistic regression has in somecases out - performed neural nets in the detection of abusive language ( Park and Fung , 2017 ) .", "entities": [[13, 15, "MethodName", "logistic regression"], [27, 29, "TaskName", "abusive language"]]}, {"text": "As the value of tincreases the size of the aggression : true dataset decreases , as seen in Table 1 .", "entities": []}, {"text": "To ensure any change in accuracy is not due to the decrease in aggression : true training instances , we run a second experiment in which for all values oftboth label subsets ( aggression : true and aggression : false ) are randomly reduced to 3223 instances \u2013 the size of the smallest attack score subcorpus ( per the cumulative n.posts column in Table 1 ) .", "entities": [[5, 6, "MetricName", "accuracy"]]}, {"text": "For this latter experiment we report accuracies averaged over one hundred runs to smooth variation in the random sampling process ( identi\ufb01ed as \u2018 Acc . Control \u2019 in Table 3 ) .", "entities": [[24, 25, "MetricName", "Acc"]]}, {"text": "4.3 Results Classi\ufb01cation accuracies are shown in Table 35 .", "entities": []}, {"text": "It is apparent that in both training data settings \u2013 controlled and non - controlled ( \u2018 all \u2019 ) \u2013 the accuracy of aggression identi\ufb01cation reduces as the true / false cut - off threshold tincreases .", "entities": [[22, 23, "MetricName", "accuracy"]]}, {"text": "In the case of the controlled training data setting there is at \ufb01rst a small increase in accuracy as trises from 1 to 3 .", "entities": [[17, 18, "MetricName", "accuracy"]]}, {"text": "This result suggests that the levels in the WikiComments Corpus most closely matching the aggressive posts on HackForums are those in the attack score range 1 to 5 , and that the optimal value oftis between 2 and 3 .", "entities": []}, {"text": "To illustrate the rise and fall in classi\ufb01cation accuracy astincreases , we plot accuracies as boxplots for the 100 runs in the controlled training data setting ( Figure 4.3 ) .", "entities": [[8, 9, "MetricName", "accuracy"]]}, {"text": "The boxplots show medians ( the thick horizontal bars ) , \ufb01rst and third quar5For comparison with the classi\ufb01ers trained by Wulczyn et al ( 2017 ) we also calculated AUC ( area under the curve ) measures in the \u2018 all \u2019 condition .", "entities": [[30, 31, "MetricName", "AUC"]]}, {"text": "Our best AUC was .739 with tat 2 ; Wulczyn et al \u2019s best model was a multi - layered perceptron estimating empirical distributions based on character n - grams and this achieved an AUC of .966 .", "entities": [[2, 3, "MetricName", "AUC"], [34, 35, "MetricName", "AUC"]]}, {"text": "71tN.True postsAcc .", "entities": []}, {"text": "AllAcc .", "entities": []}, {"text": "Control 1 58,099 .80 .76 2 35,551 .63 .77 3 26,143 .54 .78 4 19,863 .41 .75 5 15,730 .35 .72 6 13,467 .32 .70 7 10,761 .27 .64 8 8566 .22 .60 9 6065 .19 .52 10 3223 .09", "entities": []}, {"text": ".42", "entities": []}, {"text": "Table 3 : Classi\ufb01cation accuracy for aggressive posts in the CrimeBB Corpus , with a varying true / false training thresholdtfrom 1 to 10 , the size of the aggression : true set in WikiComments for different values of t , accuracy for all training WikiComments instances , and a controlled experiment sampling 3223 true and false instances ( averaged over 100 runs ) .", "entities": [[4, 5, "MetricName", "accuracy"], [41, 42, "MetricName", "accuracy"]]}, {"text": "tiles ( Q1 , Q3 , shown by the hinges ) , and whiskers extending as far as 1:5\u0003IQR where IQR is the inter - quartile range between Q1 and Q3 .", "entities": [[20, 21, "DatasetName", "IQR"]]}, {"text": "Datapoints beyond the whiskers are outliers and are plotted individually .", "entities": []}, {"text": "4.4", "entities": []}, {"text": "Discussion It is evident from our classi\ufb01cation experiments that levels of linguistic aggression in HackForums tend to be milder than those in WikiComments , if we take the optimal value of tto lie between 2 and 3 ( Table 3 ) whereas for WikiComments it was found to be 4.25 ( Wulczyn et al . , 2017 ) .", "entities": []}, {"text": "A possible explanation for this \ufb01nding may be the difference in purposes of the two sources for our test and training data : discussion of Wikipedia page edits often end up as arguments between contributors .", "entities": []}, {"text": "The fact these arguments may become aggressive or personally offensive at times is unsurprising .", "entities": []}, {"text": "In HackForums , where our test data came from , users often have the intention of educating others , learning from others , buying and selling products , and in many cases discouraging others from acting illegally online ( those with a so - called \u2018 white hat \u2019 hacking ethos \u2013 hackers who identify security vulnerabilities and report them rather than exploit them ) .", "entities": []}, {"text": "HackForums is not an oasis of calm , positive behaviour , however \u2013 on the con - trary , users can often be off - hand in their comments , dismissive of \u2018 noobs \u2019 and \u2018 skids \u2019 ( script kiddies \u2013 a novice or tinkerer ) , sarcastic and rude .", "entities": []}, {"text": "These attitudes , where they do not cross the line into aggressive behaviour , map to our negative label for author intent .", "entities": []}, {"text": "Debates about hacking techniques , authorship of code , and user behaviour ( e.g. spam , posting out - of - date tutorials , offering hacking tools which do n\u2019t work as advertised ) are frequent .", "entities": []}, {"text": "But on the whole , the forum exists for information and technology exchange and the white hat hackers , along with active administrators and a reputation scoring system , help to constrain user behaviour .", "entities": []}, {"text": "Indeed this highly active reputation scoring system may deter aggressive online harassment and allow for users to engender trust in what could otherwise be quite untrustworthy environments ( Holt et al . , 2016 ; D\u00e9cary - H\u00e9tu and Lepp\u00e4nen , 2016 ) .", "entities": []}, {"text": "Furthermore , online deviant communities such as these tend to be rather homogeneous , particularly involving young males ( Hutchings and Chua , 2017 ) .", "entities": []}, {"text": "Therefore the targets for any harassment may be off , rather than on , the forum .", "entities": []}, {"text": "Aside from aggression , we also labelled positive texts ( which answer others \u2019 questions , contain laughter - related word tokens or emoticons , or praise the work of others ) , neutral texts , and negative texts ( including users stating that others can not or should not do something , sarcasm and arguments ) .", "entities": []}, {"text": "These intent types are the majority labels in our 4123 post subset , with 1562 positive , 2566 neutral and 788 negative occurrences ( the posts could be multiply labelled , hence these counts sum to more than 4123 ) .", "entities": []}, {"text": "Minority labels are aggression ( n=100 ) , users posting to moderate discussion ( n=119 ) , and requests to continue discussion in private messaging ( n=238 ) .", "entities": []}, {"text": "We further subdivide our set of 100 aggressive forum posts into seven classes : simply aggressive , personal denigration , alludes to violence , refers to disability , features misogyny , homophobia , racism .", "entities": []}, {"text": "Personal denigration typically involves name - calling \u2013 dismissing someone as an idiot or moron , doubting their technical skills , and so on .", "entities": []}, {"text": "The other classes indicate that the author of the post alludes to violence ( \u201c I \u2019ll cut your neck \u201d ) , disability ( \u201c you \u2019re a retard \u201d ) , misogyny ( \u201c stop bitching \u201d ) , homophobia ( \u201c that \u2019s gay \u201d ) , and racism ( \u201c fucking jew \u201d ) .", "entities": []}, {"text": "Note that , with the exception of \u2018 simply aggressive \u2019 which tends to be", "entities": []}, {"text": "72 Figure 1 : Classifying aggressive posts in the CrimeBB Corpus using controlled training data sizes ; with the true / false training threshold ton thex - axis and accuracy on the y - axis , and each data point being 1 of 100 runs randomly sampling the training data .", "entities": [[29, 30, "MetricName", "accuracy"]]}, {"text": "a fallback if the post falls into no other class , the posts may be assigned multiple labels and that a single annotator undertook labelling .", "entities": []}, {"text": "Label counts are shown in Table 4 .", "entities": []}, {"text": "We \ufb01nd that most aggressive posts are just that \u2013 simply aggressive manners of writing which would be out of place in polite discourse .", "entities": []}, {"text": "For example , authors add emphasis with the f - word , including formulaic phrases in acronym form ( \u2018 gtfo \u2019 , \u2018 wtf \u2019 , \u2018 stfu \u2019 ) .", "entities": []}, {"text": "The next most common aggression type is personal denigration : most often calling the addressee \u2019s intelligence into question , or doubting their motives .", "entities": []}, {"text": "After that , the minority labels are those which might feature in hate speech : discriminating against women , homosexuals and ethnicities .", "entities": [[12, 14, "DatasetName", "hate speech"]]}, {"text": "In addition , the \u2018 refers to dis - Label Count Simply aggressive 48 Personal denigration 37 Refers to disability 7 Includes misogyny 4 Alludes to violence 2 Includes homophobia 1 Includes racism 1 Table 4 : Aggression subclass counts in 100 HackForums posts with aggressive intent from the CrimeBB Corpus .", "entities": []}, {"text": "73ability \u2019 label always involves the words \u2018 retard \u2019 and \u2018 retarded \u2019 in this 100 post sample .", "entities": []}, {"text": "Finally , direct threats of violence are very rare , with only two examples found in this subcorpus .", "entities": []}, {"text": "5 Conclusions & Future work We have shown that abusive language in an online hacking forum is relatively mild compared to that found in Wikipedia page edit comments .", "entities": [[9, 11, "TaskName", "abusive language"]]}, {"text": "We propose that the tendency of forum users to on the whole engage in constructive and informative discourse results in positive behaviour and non - toxic language .", "entities": []}, {"text": "WikiComments , on the other hand , is made up of debates about the rights and wrongs of page edits , and perhaps inevitably this adversarial set up allows more aggressive behaviours to manifest themselves in writing .", "entities": []}, {"text": "In future work we evidently need to annotate more data so that we have more than 100 examples of abusive language from CrimeBB .", "entities": [[19, 21, "TaskName", "abusive language"]]}, {"text": "Due to the low hit rate for abusive language in CrimeBB texts ( 100 in 4123 , for instance ) we can investigate automatic annotation of further chunks of the data , along with supervised sampling from those new annotations to check their quality .", "entities": [[7, 9, "TaskName", "abusive language"]]}, {"text": "These labelled data on a larger scale will allow us to analyse more general patterns of behaviour such as individual and community - wide trends over time , how aggression surfaces and is dealt with by moderators , and linguistic facets of aggressive behaviour such as homophobia , racism , misogyny and so on .", "entities": []}, {"text": "We can also investigate other Internet domains such as social media , other forums and potentially the Dark Web , but also other sections of CrimeBB , such as the reputation voting area within HackForums in which we might expect to \ufb01nd more vitriolic interactions given that votes can be both positive and negative and accompanied by review - like texts .", "entities": []}, {"text": "Finally , we are also interested in applications of our research , including the questions of desired accuracy of any deployed system , the appropriate actions to take , and the ethics of data collection , analysis and intervention ( Kennedy et al . , 2017 ; Thomas et al . , 2017 ) .", "entities": [[17, 18, "MetricName", "accuracy"]]}, {"text": "One option could be to create an alert system for forum moderators , thereby offering real - world impact for our work while allowing the appropriate authorities to take action when necessary ( Kumar et al . , 2018).Acknowledgments", "entities": [[33, 34, "DatasetName", "Kumar"]]}, {"text": "This work was supported by The Alan Turing Institute \u2019s Defence & Security Programme , and the U.K. Engineering & Physical Sciences Research Council .", "entities": []}, {"text": "We thank Emma Lenton , Dr Alastair Beresford , and the anonymous reviewers for their support and advice .", "entities": []}, {"text": "References Peter Bourgonje , Julian Moreno - Schneider , Ankit Srivastava , and Georg Rehm .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Automatic classi\ufb01cation of abusive language and personal attacks in various forms of online communication .", "entities": [[3, 5, "TaskName", "abusive language"]]}, {"text": "In Language Technologies for the Challenges of the Digital Age .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Tianqi Chen , Tong He , Michael Benesty , Vadim Khotilovich , and Yuan Tang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "xgboost :", "entities": []}, {"text": "Extreme Gradient Boosting .", "entities": []}, {"text": "R package version 0.6.4.1 .", "entities": []}, {"text": "Justin Cheng , Michael Bernstein , Cristian DanescuNiculescu - Mizil , and Jure Leskovec . 2017 .", "entities": []}, {"text": "Anyone can become a troll : Causes of trolling behavior in online discussions .", "entities": []}, {"text": "In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Justin Cheng , Cristian Danescu - Niculescu - Mizil , and Jure Leskovec . 2015 .", "entities": []}, {"text": "Antisocial behavior in online discussion communities .", "entities": []}, {"text": "In The 9th International AAAI Conference on Web and Social Media ( ICWSM ) .", "entities": []}, {"text": "David D\u00e9cary - H\u00e9tu and Anna Lepp\u00e4nen .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Criminals and signals : An assessment of criminal performance in the carding underworld .", "entities": []}, {"text": "Security Journal , 29:442\u2013460 .", "entities": []}, {"text": "Karthik Dinakar , Roi Reichart , and Henry Lieberman .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Modeling the detection of textual cyberbullying .", "entities": []}, {"text": "In Fifth International AAAI Conference on Weblogs and Social Media .", "entities": []}, {"text": "Darja Fi\u0161er , Toma\u017e Erjavec , and Nikola Ljube\u0161i \u00b4 c. 2017 .", "entities": []}, {"text": "Legal framework , dataset and annotation schema for socially unacceptable online discourse practices in Slovene .", "entities": []}, {"text": "In Proceedings of the First Workshop on Abusive Language Online .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Joseph Fleiss .", "entities": []}, {"text": "1971 .", "entities": []}, {"text": "Measuring nominal scale agreement among many raters .", "entities": []}, {"text": "Psychological Bulletin , 76:378\u2013382 .", "entities": []}, {"text": "Bj\u00f6rn Gamb\u00e4ck and Utpal Kumar Sikdar . 2017 .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "Using convolutional neural networks to classify hatespeech .", "entities": []}, {"text": "In Proceedings of the First Workshop on Abusive Language Online .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Thomas Holt , Olga Smirnova , and Alice Hutchings .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Examining signals of trust in criminal markets online .", "entities": []}, {"text": "Journal of Cybersecurity , 2:137\u2013145 .", "entities": []}, {"text": "74Alice Hutchings and Yi Ting Chua . 2017 .", "entities": []}, {"text": "Gendering cybercrime .", "entities": []}, {"text": "In T. J. Holt , editor , Cybercrime through an Interdisciplinary Lens .", "entities": []}, {"text": "Oxford : Routledge .", "entities": []}, {"text": "Mohammed Jabreel and Antonio Moreno .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "EiTAKA at SemEval-2018 Task 1 : An ensemble of n - channels ConvNet and XGboost regressors for emotion analysis of tweets .", "entities": [[17, 18, "DatasetName", "emotion"]]}, {"text": "In Proceedings of The 12th International Workshop on Semantic Evaluation .", "entities": []}, {"text": "George Kennedy , Andrew McCollough , Edward Dixon , Alexei Bastidas , John Ryan , Chris Loo , and Saurav Sahay .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Technology solutions to combat online harassment .", "entities": []}, {"text": "In Proceedings of the First Workshop on Abusive Language Online .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Srijan Kumar , William L. Hamilton , Jure Leskovec , and Dan Jurafsky .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "2018 .", "entities": []}, {"text": "Community interaction and con\ufb02ict on the Web .", "entities": []}, {"text": "In Proceedings of the 2018 World Wide Web Conference .", "entities": []}, {"text": "Irene Kwok and Yuzhou Wang .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Locate the hate : Detecting tweets against blacks .", "entities": []}, {"text": "In Twenty - Seventh AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "J. Richard Landis and Gary G. Koch .", "entities": []}, {"text": "1977 .", "entities": []}, {"text": "The measurement of observer agreement for categorical data .", "entities": []}, {"text": "Biometrics , 33:159\u2013174 .", "entities": []}, {"text": "Tai Ching Li , Joobin Gharibshah , Evangelos E. Papalexakis , and Michalis Faloutsos . 2017 .", "entities": []}, {"text": "TrollSpot :", "entities": []}, {"text": "Detecting misbehavior in commenting platforms .", "entities": []}, {"text": "In Proceedings of the 2017 IEEE / ACM International Conference on Advances in Social Networks Analysis and Mining 2017 .", "entities": [[7, 8, "DatasetName", "ACM"]]}, {"text": "Hamdy Mubarak , Kareem Darwish , and Walid Magdy . 2017 .", "entities": []}, {"text": "Abusive language detection on Arabic social media .", "entities": [[0, 2, "TaskName", "Abusive language"]]}, {"text": "In Proceedings of the First Workshop on Abusive Language Online .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Zarmeen Nasim .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "IBA - Sys at SemEval-2017 Task 5 : Fine - grained sentiment analysis on \ufb01nancial microblogs and news .", "entities": [[11, 13, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) .", "entities": []}, {"text": "Chikashi Nobata , Joel Tetreault , Achint Thomas , Yashar Mehdad , and Yi Chang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Abusive language detection in online user content .", "entities": [[0, 2, "TaskName", "Abusive language"]]}, {"text": "In Proceedings of the 25th International Conference on World Wide Web .", "entities": []}, {"text": "Ji Ho Park and Pascale Fung .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "One - step and twostep classi\ufb01cation for abusive language detection on twitter .", "entities": [[7, 9, "TaskName", "abusive language"]]}, {"text": "In Proceedings of the First Workshop on Abusive Language Online .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Sergio Pastrana , Daniel Thomas , Alice Hutchings , and Richard Clayton .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Crimebb : Enabling cybercrime research on underground forums at scale .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on World Wide Web ( WWW\u201918 ) .Stephanie", "entities": []}, {"text": "Pieschl , Christina Kuhlmann , and Torsten Porsch . 2015 .", "entities": []}, {"text": "Beware of publicity !", "entities": []}, {"text": "perceived distress of negative cyber incidents and implications for de\ufb01ning cyberbullying .", "entities": []}, {"text": "Journal of School Violence , 14:111\u2013132 .", "entities": []}, {"text": "Manoel Horta Ribeiro , Pedro H. Calais , Yuri A. Santos , and Wagner Meira J Virg\u00ed\u00cc \u02db Alio A. F. Almeid and .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "\u201c like sheep among wolves \u201d : Characterizing hateful users on Twitter .", "entities": [[8, 12, "DatasetName", "hateful users on Twitter"]]}, {"text": "In Proceedings of WSDM workshop on Misinformation and Misbehavior Mining on the Web ( MIS2 ) .", "entities": [[6, 7, "TaskName", "Misinformation"]]}, {"text": "Dmitriy Selivanov and Qing Wang . 2017 .", "entities": []}, {"text": "text2vec : Modern Text Mining Framework for R .", "entities": []}, {"text": "R package version 0.5.0 .", "entities": []}, {"text": "Sara Owsley Sood , Elizabeth F. Churchill , and Judd Antin .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Automatic identi\ufb01cation of personal insults on social news sites .", "entities": []}, {"text": "Journal of the American Society for Information Science and Technology , 63:270\u2013285 .", "entities": []}, {"text": "Daniel Thomas , Sergio Pastrana , Alice Hutchings , Richard Clayton , and Alastair Beresford . 2017 .", "entities": []}, {"text": "Ethical issues in research using datasets of illicit origin .", "entities": []}, {"text": "InProceedings of the ACM Internet Measurement Conference ( IMC\u201917 ) .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "William Warner and Julia Hirschberg .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Detecting hate speech on the World Wide Web .", "entities": [[1, 3, "DatasetName", "hate speech"]]}, {"text": "In Proceedings of the Second Workshop on Language in Social Media .", "entities": []}, {"text": "Zeerak Waseem , Thomas Davidson , Dana Warmsley , and Ingmar Weber .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Understanding abuse : A typology of abusive language detection subtasks .", "entities": [[6, 8, "TaskName", "abusive language"]]}, {"text": "In Proceedings of the First Workshop on Abusive Language Online .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Zeerak Waseem and Dirk Hovy .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Hateful symbols or hateful people ?", "entities": []}, {"text": "predictive features for hate speech detection on Twitter .", "entities": [[3, 6, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of the NAACL Student Research Workshop .", "entities": []}, {"text": "Ellery Wulczyn , Nithum Thain , and Lucas Dixon . 2017 .", "entities": []}, {"text": "Ex Machina : Personal attacks seen at scale .", "entities": []}, {"text": "InProceedings of the 26th International Conference on World Wide Web .", "entities": []}, {"text": "Jun - Ming Xu , Benjamin Burch\ufb01el , Xiaojin Zhu , and Amy Bellmore .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "An examination of regret in bullying tweets .", "entities": []}, {"text": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies .", "entities": []}, {"text": "Dawei Yin , Zhenzhen Xue , Liangjie Hong , Brian D. Davison , April Kontostathis , and Lynne Edwards .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Detection of harassment on Web 2.0 .", "entities": []}, {"text": "In Proceedings of the Content Analysis in the WEB 2.0 ( CAW2.0 ) Workshop at WWW2009 .", "entities": []}]