[{"text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Short Papers ) , pages 593\u2013598 August 1\u20136 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics593Beyond Laurel / Yanny : An Autoencoder - Enabled Search for Polyperceivable Audio Kartik Chandra Stanford University kach@cs.stanford.eduChuma Kabaghe Stanford University chuma@alumni.stanford.edu", "entities": [[11, 12, "MethodName", "Autoencoder"]]}, {"text": "Gregory Valiant Stanford University gvaliant@cs.stanford.edu", "entities": []}, {"text": "Abstract The famous \u201c laurel / yanny \u201d phenomenon references an audio clip that elicits dramatically different responses from different listeners .", "entities": []}, {"text": "For the original clip , roughly half the population hears the word \u201c laurel , \u201d while the other half hears \u201c yanny . \u201d", "entities": []}, {"text": "How common are such \u201c polyperceivable \u201d audio clips ?", "entities": []}, {"text": "In this paper we apply ML techniques to study the prevalence of polyperceivability in spoken language .", "entities": []}, {"text": "We devise a metric that correlates with polyperceivability of audio clips , use it to ef\ufb01ciently \ufb01nd new \u201c laurel / yanny\u201d-type examples , and validate these results with human experiments .", "entities": []}, {"text": "Our results suggest that polyperceivable examples are surprisingly prevalent , existing for > 2 % of English words.1 1 Introduction How robust is human sensory perception , and to what extent do perceptions differ between individuals ?", "entities": []}, {"text": "In May 2018 , an audio clip of a man speaking the word \u201c laurel \u201d received widespread attention because a signi\ufb01cant proportion of listeners con\ufb01dently reported hearing notthe word \u201c laurel , \u201d but rather the quite different sound \u201c yanny \u201d ( Salam and Victor , 2018 ) .", "entities": []}, {"text": "At \ufb01rst glance , this suggests that the decision boundaries for speech perception vary considerably among individuals .", "entities": []}, {"text": "The reality is more surprising : almost everyone has a decision boundary between the sounds \u201c laurel \u201d and \u201c yanny , \u201d without a signi\ufb01cant \u201c dead zone \u201d separating these classes .", "entities": []}, {"text": "The audio clip in question lies close to this decision boundary , so that if the clip is slightly perturbed ( e.g. by damping certain frequencies or slowing down the playback rate ) , individuals switch from con\ufb01dently perceiving \u201c laurel \u201d to con\ufb01dently perceiving \u201c yanny , \u201d with the exact point of switching varying slightly from person to person .", "entities": []}, {"text": "1This research was conducted under Stanford IRB Protocol", "entities": []}, {"text": "46430.How common is this phenomenon ?", "entities": []}, {"text": "Speci\ufb01cally , what fraction of spoken language is \u201c polyperceivable \u201d in the sense of evoking a multimodal response in a population of listeners ?", "entities": []}, {"text": "In this work , we provide initial results suggesting a signi\ufb01cant density of spoken words that , like the original \u201c laurel / yanny \u201d clip , lie close to unexpected decision boundaries between seemingly unrelated pairs of words or sounds , such that individual listeners can switch between perceptual modes via a slight perturbation .", "entities": []}, {"text": "The clips we consider consist of audio signals synthesized by the Amazon Polly speech synthesis system with a slightly perturbed playback rate ( i.e. a slight slowing - down of the clip ) .", "entities": [[13, 15, "TaskName", "speech synthesis"]]}, {"text": "Though the resulting audio signals are not \u201c natural \u201d stimuli , in the sense that they are very different from the result of asking a human to speak slower ( see Section 5 ) , we \ufb01nd that they are easy to compute and reliably yield compelling polyperceivable instances .", "entities": []}, {"text": "We encourage future work to investigate the power of more sophisticated perturbations , as well as to consider natural , ecologically - plausible perturbations .", "entities": []}, {"text": "To \ufb01nd our polyperceivable instances , we ( 1 ) devise a metric that correlates with polyperceivability , ( 2 ) use this metric to ef\ufb01ciently sample candidate audio clips , and ( 3 ) evaluate these candidates on human subjects via Amazon Mechanical Turk .", "entities": []}, {"text": "We present several compelling new examples of the \u201c laurel / yanny \u201d effect , and we encourage readers to listen to the examples included in the supplementary materials ( also available online at https://theory.stanford.edu/ \u02dcvaliant / polyperceivable / index.html ) .", "entities": []}, {"text": "Finally , we estimate that polyperceivable clips can be made for > 2 % of English words .", "entities": []}, {"text": "5942 Method To investigate polyperceivability in everyday auditory input , we searched for audio clips of single spoken words that exhibit the desired effect .", "entities": []}, {"text": "Our method consisted of two phases : ( 1 ) sample a large number of audio clips that are likely to be polyperceivable , and ( 2 ) collect human perception data on those clips using Amazon Mechanical Turk to identify perceptual modes and con\ufb01rm polyperceivability .", "entities": []}, {"text": "2.1 Sampling clips To sample clips that were likely candidates , we trained a simple autoencoder for audio clips of single words synthesized using the Amazon Polly speech synthesis system .", "entities": [[15, 16, "MethodName", "autoencoder"], [27, 29, "TaskName", "speech synthesis"]]}, {"text": "Treating the autoencoder \u2019s low - dimensional latent space as a proxy forperceptual space , we searched for clips that travel through more of the space as the playback rate is slowed from 1:0\u0002to0:6\u0002. Intuitively , a longer path through encoder space should correspond to a more dramatic change in perception as the clip is slowed down ( Section 3 presents some data supporting this ) .", "entities": [[2, 3, "MethodName", "autoencoder"]]}, {"text": "Concretely , we computed a score Sproportional to the length of the curve swept by the encoder Ein latent space as the clip is slowed down , normalized by the straight - line distance traveled : that is , we de\ufb01ne S(c )", "entities": []}, {"text": "= R0:6\u0002 r=1:0\u0002jjdE(c;r)=drjjdr jjE(c;0:6\u0002)\u0000E(c;1:0\u0002)jj .", "entities": []}, {"text": "Then , with probability proportional to e0:2\u0001S , we importancesampled 200 clips from the set of audio clips of the top 10,000 English words , each spoken by all 16 voices offered by Amazon Polly ( spanning American , British , Indian , Australian , and Welsh accents , and male and female voices ) .", "entities": []}, {"text": "The distributions of Sin the population and our sample is shown in Figure 2 .", "entities": []}, {"text": "Autoencoder details Our autoencoder operates on one - second audio clips sampled at 22,050 Hz , which are converted to spectrograms with a window size of 256 and then \ufb02attened to vectors in R90;000 .", "entities": [[0, 1, "MethodName", "Autoencoder"], [3, 4, "MethodName", "autoencoder"]]}, {"text": "The encoder is a linear map to R512with ReLU activations , and the decoder is a linear map back to R90;000space with pointwise squaring .", "entities": [[8, 9, "MethodName", "ReLU"]]}, {"text": "We used an Adam optimizer with lr=0.01 , training on a corpus of 16,000 clips ( randomly resampled to between 0.6x and 1.0x the original speed ) for 70 epochs with a batch size of 16 ( \u00198 hours on an AWS c5.4xlarge EC2 instance).2.2 Mechanical Turk experiments Each Mechanical Turk worker was randomly assigned 25 clips from our importance - sampled set of 200 .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [32, 34, "HyperparameterName", "batch size"]]}, {"text": "Each clip was slowed to either 0.9x , 0.75x , or 0.6x the original rate .", "entities": []}, {"text": "Workers responded with a perceived word and a con\ufb01dence score for each clip .", "entities": []}, {"text": "We collected responses from 574 workers , all of whom self - identi\ufb01ed as US - based native English speakers .", "entities": []}, {"text": "This yielded 14,370 responses ( \u001972 responses per clip ) .", "entities": []}, {"text": "Next , we manually reviewed these responses and selected the most promising clips for a second round with only 11 of the 200 clips .", "entities": []}, {"text": "Note that because these selections were made by manual review ( i.e. listening to clips ourselves ) , there is a chance we passed over some polyperceivable clips \u2014 this means that our computations in Section 3 are only a conservative lower bound .", "entities": []}, {"text": "For this round , we also included clips of the 5 words identi\ufb01ed by Guan and Valiant ( 2019 ) , 12 potentially - polyperceivable words we had found in earlier experiments , and \u201c laurel \u201d as controls .", "entities": []}, {"text": "We collected an additional 3,950 responses among these 29 clips ( \u0019136 responses per clip ) to validate that they were indeed polyperceivable .", "entities": []}, {"text": "Finally , we took the words associated with these 29 clips and produced a new set of clips using each of the 16 voices , for a total of 464 clips .", "entities": []}, {"text": "We collected 4,125 responses for this last set ( \u00193 responses for each word / voice / rate combination ) .", "entities": []}, {"text": "3 Results Are the words we found polyperceivable ?", "entities": []}, {"text": "To identify cases where words had multiple perceptual \u201c modes , \u201d we looked for clusters in the distribution of responses for each of the 29 candidate words .", "entities": []}, {"text": "Concretely , we treated responses as \u201c bags of phonemes \u201d and then applied K - means .", "entities": []}, {"text": "Though this rough heuristic discards information about the order of phonemes within a word , it works suf\ufb01ciently well for clustering , especially since most of our words have very few syllables ( more sophisticated models of phonetic similarity exist , but they would not change our results ) .", "entities": []}, {"text": "We found that the largest cluster typically contained the original word and rhymes , whereas other clusters represented signi\ufb01cantly different perceptual modes .", "entities": []}, {"text": "Some examples of clusters and their relative frequency are available in Table 1 , and the relative cluster sizes as a function of playback rate are shown in Figure 1 .", "entities": []}, {"text": "As the rate is perturbed ,", "entities": []}, {"text": "595Perceived sound Playback rate 0.90\u0002 0.75\u0002 0.60\u0002 laurel /lauren / moral/\ufb02oral 0.86 0.64 0.19 manly / alley / marry / merry / mary 0.0 0.03 0.35 thrilling 0.63 0.47 0.33 \ufb02owing / throwing 0.34 0.50 0.58 settle 0.65 0.25 0.33 civil 0.32 0.64 0.48 claimed /claim / climbed 0.58 0.34 0.11 framed/\ufb02am(m)ed / friend/ \ufb01nd 0.33 0.52 0.43 leg 0.50 0.31 0.10 lake 0.46 0.34 0.14 growing /rowing 0.50 0.47 0.26 brewing / booing / boeing 0.19 0.23 0.26 third 0.40 0.10 0.10 food / foot 0.18 0.29 0.13 idly / ideally 0.38 0.30 0.03 natalie 0.25 0.27 0.09 \ufb01end 0.22 0.34 0.32 themed 0.11 0.17 0.24 bologna /baloney / bellany 0.26 0.00 0.00 ( good)morning 0.03 0.28 0.77 thumb 0.66 0.74 0.79 fem(me)/\ufb01rm 0.06 0.10 0.12 frank /\ufb02ank 0.72 0.96 0.43 strength 0.08 0.00 0.15 round 0.53 0.38 0.65 world 0.03 0.00 0.14 Table 1 : Some polyperceivable words ( bold ) and their alternate perceptual modes ( below ) .", "entities": []}, {"text": "Each row gives representative elements from the mode , and the proportion of workers whose response fell in that mode .", "entities": []}, {"text": "the prevalance of alternate modes among our clips increases .", "entities": []}, {"text": "How prevalent are polyperceivable words ?", "entities": []}, {"text": "Of our initial sample of 200 words , 11 ultimately yielded compelling demonstrations .", "entities": []}, {"text": "To compute the prevalence of polyperceivable words in the population of the top 10k words , we have to account for the importance sampling weights we used when sampling in Section 2.1 .", "entities": []}, {"text": "After scaling each word \u2019s contribution by the inverse of the probability of including that word in our nonuniform sample of 200 , we conclude that polyperceivable clips exist for at least 2%of the population : that is , of the 16 voices under consideration , at least one yields a polyperceivable clip for > 2 % of the top 10k English words .", "entities": []}, {"text": "We emphasize that this is a conservative lower bound , because it assumes that there were no other polyperceivable words in the 200 words we sampled , besides the 11 that we selected for the second round .", "entities": []}, {"text": "We did not conduct an exhaustive search among those 200 words , instead focusing our Mechanical Turk resources on only the most promising candidates .", "entities": []}, {"text": "potent pearl claimed leg girl idly bailey seat near settle thumb foreman fiend fondly laurel thrilling worlds bologna floral paragraph parallel round third growing early flat frank prologue lady0.000.250.500.751.00Cluster sizes @ 0.90x potent claimed leg idly bailey third seat near pearl laurel bologna worlds foreman thumb girl fondly fiend thrilling settle flat round lady prologue growing floral early frank parallel paragraph0.000.250.500.751.00Cluster sizes @ 0.75x laurel leg third idly pearl near seat settle claimed bailey potent girl thrilling bologna prologue fondly fiend early parallel round worlds thumb lady foreman growing floral frank flat paragraph0.000.250.500.751.00Cluster sizes @ 0.60xFigure 1 : Relative cluster sizes across different playback rates .", "entities": []}, {"text": "When the rate is slightly perturbed , the prevalence of alternate modes increases .", "entities": []}, {"text": "10 20 30 40 50 Autoencoder path length0.000.05DensityClip importance - sampling distribution Population Sample Figure 2 : Distribution of path lengths ( the Smetric ) in the population ( top 10k English words , all 16 voices ) and our sample of 200 .", "entities": [[5, 6, "MethodName", "Autoencoder"]]}, {"text": "IsSa good metric ?", "entities": []}, {"text": "We consider the metric S to be successful because it allowed us to ef\ufb01ciently \ufb01nd several new polyperceivable instances .", "entities": []}, {"text": "If the 200 words were sampled uniformly instead of being importance - sampled based on S , we would only have found 4 polyperceivable words in expectation ( 2 % of 200 ) .", "entities": []}, {"text": "Thus , importance sampling increased our procedure \u2019s recall by almost 3\u0002. For a more quantitative understanding , we analyzed the relationship between \u201c autoencoder path length \u201d Sand \u201c perceptual path length \u201d", "entities": [[24, 25, "MethodName", "autoencoder"]]}, {"text": "T. Our measure Tof \u201c perceptual path length \u201d for a clip ischange in average distance between source word and response as we slow the clip down from 0:75\u0002to0:6\u0002.", "entities": []}, {"text": "As with clustering above , distance", "entities": []}, {"text": "596 claimed lady laurel foreman parallel settle growing fondly potent paragraph thumb third prologue near flat pearl leg fiend bailey idly floral girl frank seat bologna worlds early thrilling round word w0.6 0.4 0.2 0.00.20.40.6r - value of S(w ) vs. T(w)Autoencoder path - length vs. perceptual distanceFigure 3 : Correlation between SandTacross the n= 16 voices for each of our 29 words .", "entities": []}, {"text": "Nearly all words correlate positively , though with varying strengths ( note that \u201c laurel \u201d correlates quite strongly ) .", "entities": []}, {"text": "is measured in bag - of - phonemes space .", "entities": []}, {"text": "For each word , we computed the correlation between Sand Tamong the 16 voices ( both SandTvary signi\ufb01cantly across voices ) .", "entities": []}, {"text": "For all but 5 of our 29 words these metrics correlated positively , though with varying strength ( Figure 3 ) .", "entities": []}, {"text": "This suggests that S indeed correlates with polyperceivability .", "entities": []}, {"text": "4 Discussion : Why study quirks of human perception in an ACL paper ?", "entities": []}, {"text": "Perceptual instability in human sensory systems offers insight into ML systems .", "entities": []}, {"text": "The question of what fraction of natural inputs lie close to decision boundaries for trained ML systems has received enormous attention .", "entities": []}, {"text": "The surprising punchline that has emerged over the past decade is that most natural examples ( including points in the training set ) actually lie extremely close to unexpected decision boundaries .", "entities": []}, {"text": "For most of these points , a tiny but carefully - crafted perturbation can lead the ML system to change the label .", "entities": []}, {"text": "Such perturbations are analogous to the slight perturbation in playback speed for the polyperceivable clips we consider .", "entities": []}, {"text": "In the ML literature , these perturbations , referred to as \u201c adversarial examples \u201d seem pervasive across complex ML systems ( Szegedy et al . , 2013 ; Goodfellow et", "entities": []}, {"text": "al . , 2014 ; Nguyen et al . , 2015 ; Moosavi - Dezfooli et al . , 2016 ; Madry et al . , 2017 ; Raghunathan et al . , 2018 ; Athalye et al . , 2017 ) .", "entities": []}, {"text": "While the initial work on adversarial examples focused on computer vision , more recent work shows the presence of such examples across other settings , including reinforcement learning ( Huang et al . , 2017 ) , reading comprehension ( Jia and Liang , 2017 ) , and speech recognition ( Carlini and Wag - ner , 2018 ; Qin et", "entities": [[37, 39, "TaskName", "reading comprehension"], [48, 50, "TaskName", "speech recognition"]]}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "Studying perceptual illusions would provide a much - needed reference when evaluating ML systems in these domains .", "entities": []}, {"text": "For vision tasks , for example , human vision provides the only evidence that current ML models are far from optimal in terms of robustness to adversarial examples .", "entities": []}, {"text": "However , while humans are certainly not assusceptible to adversarial examples as ML systems , we lack quanti\ufb01ed bounds on human robustness .", "entities": []}, {"text": "More broadly , understanding which systems ( both biological and ML ) have decision boundaries that lie surprisingly close to many natural inputs may inform our sense of what settings are amenable to adversarially robust models , and what settings inherently lead to vulnerable classi\ufb01ers .", "entities": []}, {"text": "Perceptual instability in ML systems offers insight into human sensory systems .", "entities": []}, {"text": "Recent research on adversarial robustness of ML models has provided a trove of new tools and perspectives for probing classi\ufb01ers and exploring the geometry of decision boundaries .", "entities": [[3, 5, "TaskName", "adversarial robustness"]]}, {"text": "These tools can not directly be applied to study the decision boundaries of biological classi\ufb01ers ( e.g. we can not reasonably do \u201c gradient descent \u201d on human subjects ) .", "entities": []}, {"text": "However , using standard data - driven deep learning techniques to model human perceptual systems can allow us to apply these techniques by proxy .", "entities": []}, {"text": "An example can be found in the study of \u201c transferability . \u201d", "entities": []}, {"text": "Adversarial examples crafted to fool a speci\ufb01c model often also fool other models , even those trained on disjoint training sets ( Papernot et al . , 2016a ;", "entities": []}, {"text": "Tram ` er et al . , 2017 ; Liu et al . , 2016 ) .", "entities": []}, {"text": "This prompts the question of whether adversarial examples crafted for an ML model might also transfer to humans .", "entities": []}, {"text": "Recent surprising work by Elsayed et al .", "entities": []}, {"text": "( 2018 ) explores this question for vision .", "entities": []}, {"text": "Humans were shown adversarial examples trained for an image classi\ufb01er for \u001970ms , and asked to choose between the correct label and the classi\ufb01er \u2019s ( incorrect ) predicted label .", "entities": []}, {"text": "Humans selected the incorrect label more frequently when shown adversarial examples than when shown unperturbed images .", "entities": []}, {"text": "Similarly , Hong et al . ( 2014 ) trained a lowdimensional representation of \u201c perceptual space , \u201d and used the decision boundaries of the model to \ufb01nd images that confused human subjects .", "entities": []}, {"text": "5 Related work An enormous body of work from cognitive sciences communities explores the quirks of human / animal sensory systems ( Fahle et al . , 2002 ) .", "entities": []}, {"text": "These works", "entities": []}, {"text": "597often have the explicit goal of exploring isolated \u201c illusions \u201d that provide insights into our perceptual systems ( Davis and Johnsrude , 2007 ;", "entities": []}, {"text": "Fritz et al . , 2005 ) .", "entities": []}, {"text": "However , there are few efforts to quantify the extent to which \u201c typical \u201d instances are polyperceivable or lie close to decision boundaries .", "entities": []}, {"text": "Miller ( 1981 ) studies the effect of speaking rate on how listeners perceive phonemes .", "entities": []}, {"text": "The perceptual shifts studied therein are between phonetically adjacent perceptions ( e.g. \u201c pip \u201d vs. \u201c peep \u201d ) rather than dramatically different perceptions ( e.g. \u201c laurel \u201d vs. \u201c yanny \u201d ) .", "entities": []}, {"text": "The \u201c perturbation \u201d of increasing human speaking rate is much more complex than simply linearly scaling the playback rate of an audio clip .", "entities": []}, {"text": "Speaking - rate induced shifts also seem to hold more universally across voices , as opposed to the polyperceivable instances we examine .", "entities": []}, {"text": "6 Future work Priming effects It is possible to use additional stimuli to alter perceptions of the \u201c laurel / yanny \u201d audio clip .", "entities": []}, {"text": "For example , Bosker ( 2018 ) demonstrates the ability to control a listener \u2019s perception by \u201c priming \u201d them with a carefully crafted recording before the polyperceivable clip is played .", "entities": []}, {"text": "Similarly , Guan and Valiant ( 2019 ) investigated the \u201c McGurk effect \u201d ( McGurk and MacDonald , 1976 ) , where what one \u201c sees \u201d affects what one \u201c hears . \u201d", "entities": []}, {"text": "The work estimated the fraction of spoken words that , when accompanied by a carefully designed video of a human speaker , would be perceived as signi\ufb01cantly different words by listeners .", "entities": []}, {"text": "Such phenomena raise questions about how our autoencoder - based method can be extended to search for \u201c priming - sensitive \u201d polyperceivability .", "entities": [[7, 8, "MethodName", "autoencoder"]]}, {"text": "Security implications Just as adversarial examples for DNNs have security implications ( Papernot et al . , 2016b ; Carlini and Wagner , 2017 ; Liu et al . , 2016 ) , so too might adversarial examples for sensory systems .", "entities": []}, {"text": "For example , if a video clip of a politician happens to be polyperceivable , an adversary could lightly edit it with potentially signi\ufb01cant rami\ufb01cations .", "entities": []}, {"text": "A thorough treatment of such security implications is left to future work .", "entities": []}, {"text": "7 Conclusion In this paper , we leveraged ML techniques to study polyperceivability in humans .", "entities": []}, {"text": "By modeling perceptual space as the latent space of an autoencoder , we were able to discover dozens of new polyper - ceivable instances , which were validated with Mechanical Turk experiments .", "entities": [[10, 11, "MethodName", "autoencoder"]]}, {"text": "Our results indicate that polyperceivability is surprisingly prevalent in spoken language .", "entities": []}, {"text": "More broadly , we suggest that the study of perceptual illusions can offer insight into machine learning systems , and vice - versa .", "entities": []}, {"text": "Acknowledgements We would like to thank Melody Guan for early discussions on this project , and the anonymous reviewers for their thoughtful suggestions .", "entities": []}, {"text": "This research was supported by a seed grant from Stanford \u2019s HAI Institute , NSF award AF-1813049 and ONR Young Investigator Award N00014 - 18 - 1 - 2295 .", "entities": []}, {"text": "References Anish Athalye , Logan Engstrom , Andrew Ilyas , and Kevin Kwok . 2017 .", "entities": []}, {"text": "Synthesizing robust adversarial examples .", "entities": []}, {"text": "arXiv preprint arXiv:1707.07397 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hans Rutger Bosker .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Putting laurel and yanny in context .", "entities": []}, {"text": "The Journal of the Acoustical Society of America , 144(EL503 ) .", "entities": []}, {"text": "Nicholas Carlini and David Wagner .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Towards evaluating the robustness of neural networks .", "entities": []}, {"text": "In 2017 ieee symposium on security and privacy ( sp ) , pages 39\u201357 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Nicholas Carlini and David Wagner .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Audio adversarial examples : Targeted attacks on speech - totext .", "entities": []}, {"text": "In 2018 IEEE Security and Privacy Workshops ( SPW ) , pages 1\u20137 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Matthew H Davis and Ingrid S Johnsrude .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Hearing speech sounds : top - down in\ufb02uences on the interface between audition and speech perception .", "entities": []}, {"text": "Hearing research , 229(1 - 2):132\u2013147 .", "entities": []}, {"text": "Gamaleldin Elsayed , Shreya Shankar , Brian Cheung , Nicolas Papernot , Alexey Kurakin , Ian Goodfellow , and Jascha Sohl - Dickstein .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Adversarial examples that fool both computer vision and time - limited humans .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 3910\u20133920 .", "entities": []}, {"text": "Manfred Fahle , Tomaso Poggio , Tomaso A Poggio , et al . 2002 .", "entities": []}, {"text": "Perceptual learning .", "entities": []}, {"text": "MIT Press .", "entities": []}, {"text": "Jonathan B Fritz , Mounya Elhilali , and Shihab A Shamma .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Differential dynamic plasticity of a1 receptive \ufb01elds during multiple spectral tasks .", "entities": []}, {"text": "Journal of Neuroscience , 25(33):7623\u20137635 .", "entities": []}, {"text": "Ian J Goodfellow , Jonathon Shlens , and Christian Szegedy .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Explaining and harnessing adversarial examples .", "entities": []}, {"text": "arXiv preprint arXiv:1412.6572 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "598Melody Y .", "entities": []}, {"text": "Guan and Gregory Valiant .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A surprising density of illusionable natural speech .", "entities": []}, {"text": "In Proceedings of the 41th Annual Meeting of the Cognitive Science Society ( CogSci ) .", "entities": []}, {"text": "cognitivesciencesociety.org .", "entities": []}, {"text": "Ha Hong , Ethan Solomon , Dan Yamins , and James J DiCarlo .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Large - scale characterization of a universal and compact visual perceptual space .", "entities": []}, {"text": "space ( P - space ) , 10(20):30 .", "entities": []}, {"text": "Sandy Huang , Nicolas Papernot , Ian Goodfellow , Yan Duan , and Pieter Abbeel .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Adversarial attacks on neural network policies .", "entities": []}, {"text": "arXiv preprint arXiv:1702.02284 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Robin Jia and Percy Liang . 2017 .", "entities": []}, {"text": "Adversarial examples for evaluating reading comprehension systems .", "entities": [[4, 6, "TaskName", "reading comprehension"]]}, {"text": "InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , EMNLP .", "entities": []}, {"text": "Yanpei Liu , Xinyun Chen , Chang Liu , and Dawn Song .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Delving into transferable adversarial examples and black - box attacks .", "entities": []}, {"text": "arXiv preprint arXiv:1611.02770 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Aleksander Madry , Aleksandar Makelov , Ludwig Schmidt , Dimitris Tsipras , and Adrian Vladu .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Towards deep learning models resistant to adversarial attacks .", "entities": []}, {"text": "arXiv preprint arXiv:1706.06083 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Harry McGurk and John MacDonald .", "entities": []}, {"text": "1976 .", "entities": []}, {"text": "Hearing lips and seeing voices .", "entities": []}, {"text": "Nature , 264(5588):746\u2013748 .", "entities": []}, {"text": "Joanne L Miller .", "entities": []}, {"text": "1981 .", "entities": []}, {"text": "Effects of speaking rate on segmental distinctions .", "entities": []}, {"text": "Perspectives on the study of speech , pages 39\u201374 .", "entities": []}, {"text": "Seyed - Mohsen Moosavi - Dezfooli , Alhussein Fawzi , and Pascal Frossard .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Deepfool : a simple and accurate method to fool deep neural networks .", "entities": []}, {"text": "In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2574\u20132582 .", "entities": []}, {"text": "Anh Nguyen , Jason Yosinski , and Jeff Clune .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Deep neural networks are easily fooled : High con\ufb01dence predictions for unrecognizable images .", "entities": []}, {"text": "In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 427\u2013436 .", "entities": []}, {"text": "Nicolas Papernot , Patrick McDaniel , and Ian Goodfellow .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "Transferability in machine learning : from phenomena to black - box attacks using adversarial samples .", "entities": []}, {"text": "arXiv preprint arXiv:1605.07277 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Nicolas Papernot , Patrick McDaniel , Somesh Jha , Matt Fredrikson , Z Berkay Celik , and Ananthram Swami .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "The limitations of deep learning in adversarial settings .", "entities": []}, {"text": "In 2016 IEEE European symposium on security and privacy ( EuroS&P ) , pages 372\u2013387 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Yao Qin , Nicholas Carlini , Ian Goodfellow , Garrison Cottrell , and Colin Raffel .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Imperceptible , robust , and targeted adversarial examples for automatic speech recognition .", "entities": [[9, 12, "TaskName", "automatic speech recognition"]]}, {"text": "In International Conference on Machine Learning ( ICML ) .Aditi", "entities": []}, {"text": "Raghunathan , Jacob Steinhardt , and Percy Liang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Certi\ufb01ed defenses against adversarial examples .", "entities": []}, {"text": "arXiv preprint arXiv:1801.09344 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Maya Salam and Daniel Victor . 2018 .", "entities": []}, {"text": "Yanny or laurel ?", "entities": []}, {"text": "how a sound clip divided america .", "entities": []}, {"text": "The New York Times .", "entities": []}, {"text": "Christian Szegedy , Wojciech Zaremba , Ilya Sutskever , Joan Bruna , Dumitru Erhan , Ian Goodfellow , and Rob Fergus .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Intriguing properties of neural networks .", "entities": []}, {"text": "arXiv preprint arXiv:1312.6199 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Florian Tram ` er , Nicolas Papernot , Ian Goodfellow , Dan Boneh , and Patrick McDaniel . 2017 .", "entities": []}, {"text": "The space of transferable adversarial examples .", "entities": []}, {"text": "arXiv preprint arXiv:1704.03453 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}]