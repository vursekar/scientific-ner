[{"text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 5203\u20135215 November 7\u201311 , 2021 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2021 Association for Computational Linguistics5203The Future is not One - dimensional : Complex Event Schema Induction by Graph Modeling for Event", "entities": []}, {"text": "Prediction Manling Li1 , Sha Li1 , Zhenhailong Wang1 , Lifu Huang2 , Kyunghyun Cho3 , Heng Ji1 , Jiawei Han1 , Clare Voss4 1UIUC2Virginia Tech3NYU4US ARL fmanling2,shal2,wangz3,hengji , hanj g@illinois.edu , lifuh@vt.edu , kyunghyun.cho@nyu.edu , clare.r.voss.civ@mail.mil Abstract Event schemas encode knowledge of stereotypical structures of events and their connections .", "entities": []}, {"text": "As events unfold , schemas are crucial to act as a scaffolding .", "entities": []}, {"text": "Previous work on event schema induction focuses either on atomic events or linear temporal event sequences , ignoring the interplay between events via arguments and argument relations .", "entities": []}, {"text": "We introduce a new concept of Temporal Complex Event Schema : a graph - based schema representation that encompasses events , arguments , temporal connections and argument relations .", "entities": []}, {"text": "In addition , we propose a Temporal Event Graph Model that predicts event instances following the temporal complex event schema .", "entities": []}, {"text": "To build and evaluate such schemas , we release a new schema learning corpus containing 6,399 documents accompanied with event graphs , and we have manually constructed gold - standard schemas .", "entities": []}, {"text": "Intrinsic evaluations byschema matching andinstance graph perplexity , prove the superior quality of our probabilistic graph schema library compared to linear representations .", "entities": [[6, 7, "MetricName", "perplexity"]]}, {"text": "Extrinsic evaluation onschema - guided future event prediction further demonstrates the predictive power of our event graph model , signi\ufb01cantly outperforming human schemas and baselines by more than 23.8 % on HITS@1.1 1 Introduction The current automated event understanding task has been overly simpli\ufb01ed to be local and sequential .", "entities": []}, {"text": "Real world events , such as disease outbreaks and terrorist attacks , have multiple actors , complex timelines , intertwined relations and multiple possible outcomes .", "entities": []}, {"text": "Understanding such events requires knowledge in the form of a library of event schemas , capturing the progress of time , and performing global inference for event prediction .", "entities": []}, {"text": "For 1The programs , data and resources are made publicly available for research purpose in https://github.com/ limanling / temporal - graph - schema .example , regarding the 2019 protest in Hong Kong International Airport , a typical question from analysts would be \u201c How long will the \ufb02ights being canceled ? \u201d", "entities": [[33, 34, "DatasetName", "Airport"]]}, {"text": "This requires an event understanding system to match events to schema representations and reason about what might happen next .", "entities": []}, {"text": "The airport protest schema would be triggered by \u201c protest \u201d and \u201c \ufb02ight cancellation \u201d , and evidence of protesters ( e.g. , the number of protesters , the instruments being used , etc ) will suggest a CEO resignation event , or a \ufb02ight rescheduling event , or continuous \ufb02ight cancellation events with respective probabilities .", "entities": []}, {"text": "LearningBuy Transport BuyTransportAssembleTransport Attack DetonateCrash VEHentityPER learner recepientORG instituteWEArecepient entity entity entityartifactartifactagent InjureDie artifactGPEdestinationagent instrument placeplace instrumentagentagent located_inF AClocated_inagent Figure 1 : The example schema of the complex event type car - bombing .", "entities": []}, {"text": "A person learned to make bombs and bought materials as well as a vehicle .", "entities": []}, {"text": "Then the bomb was assembled to the vehicle , and then the attacker drove it to attack people .", "entities": []}, {"text": "People can be hurt by the vehicle , or by the explosion of the bomb , or by the crash of the vehicle .", "entities": []}, {"text": "Comprehending such a news story requires following a timeline , identifying key events and tracking characters .", "entities": []}, {"text": "We refer to such a \u201c story \u201d as a complex event , e.g. , the Kabul ambulance bombingevent .", "entities": []}, {"text": "Its complexity comes from the inclusion of multiple atomic events ( and their arguments ) , relations and temporal order .", "entities": []}, {"text": "A complex event schema can be used to de\ufb01ne the typical structure of a particular type of complex event , e.g. , carbombing .", "entities": []}, {"text": "This leads us to the new task that we address in this paper : temporal complex event", "entities": []}, {"text": "5204schema induction .", "entities": []}, {"text": "Figure 1 shows an example schema about car - bombing with multiple temporal dependencies between events .", "entities": []}, {"text": "Namely , the occurrence of one event may depend on multiple events .", "entities": []}, {"text": "For example , the ASSEMBLE event happens after buying both the bomb materials and the vehicle .", "entities": []}, {"text": "Also , there may be multiple events following an event , such as the multiple consequences of the ATTACK event in Figure 1 .", "entities": []}, {"text": "That is to say , \u201c the future is not one - dimensional \u201d .", "entities": []}, {"text": "Our automatically induced probabilistic complex event schema can be used to forecast event abstractions into the future and thus provide a comprehensive understanding of evolving situations , events , and trends .", "entities": []}, {"text": "For each type of complex event , we aim to induce a schema library that is probabilistic , temporally organized and semantically coherent .", "entities": []}, {"text": "Low level atomic event schemas are abundant , and can be part of multiple , sparsely occurring , higher - level schemas .", "entities": []}, {"text": "We propose a Temporal Event Graph Model , an auto - regressive graph generation model , to reach this goal .", "entities": [[12, 14, "TaskName", "graph generation"]]}, {"text": "Given a currently extracted event graph , we generate the next event type node with its potential arguments , such as the ARREST event in Figure 2 , and then propagate edge - aware information following temporal orders .", "entities": []}, {"text": "After that , we employ a copy mechanism to generate coreferential arguments , such as the DETAINEE argument is theATTACKER of the previous ATTACK event , and build relation edges for them , e.g. , PART WHOLE relation between the PLACE arguments .", "entities": []}, {"text": "Finally , temporal dependencies are determined with argument connections considered , such as the temporal edge showing that A RREST is after A TTACK .", "entities": []}, {"text": "Our generative model serves as both a schema library and a predictive model .", "entities": []}, {"text": "Speci\ufb01cally , we can probe the model to generate event graphs unconditionally to obtain a set of schemas .", "entities": []}, {"text": "We can also pass partially instantiated graphs to the model and \u201c grow \u201d the graph either forward or backward in time to predict missing events , arguments or relations , both from the past and in the future .", "entities": []}, {"text": "We propose a set of schema matching metrics to evaluate the induced schemas by comparing with human - created schemas and show the power of the probabilistic schema in the task of future event prediction as an extrinsic evaluation , to predict event types that are likely to happen next .", "entities": []}, {"text": "We make the following novel contributions : \u000fThis is the \ufb01rst work to induce probabilistic temporal graph schemas for complex eventsSymbol Meaning G2 G Instance graph of a complex event S2S Schema graph of a complex event type e2E Event node in an instance graph v2V Entity node in an instance graph hei;eliTemporal ordering edge between events ei andel , indicatingeiis beforeel hei;a;v jiArgument edge , indicating vjplays argument roleain the eventei hvj;r;vkiRelation edge between entities vjandvk , andris the relation type A(e)Argument role set of event e , de\ufb01ned by the IE ontology \bE The type set of events \bV The type set of entities \u001e ( \u0001 )", "entities": [[38, 39, "DatasetName", "e2E"], [93, 94, "MethodName", "ontology"]]}, {"text": "A mapping function from a node to its type G < iSubgraph of Gcontaining events before ei and their arguments Table 1 : List of symbols across documents , which capture temporal dynamics and connections among individual events through their coreferential or related arguments .", "entities": []}, {"text": "\u000fThis is the \ufb01rst application of graph generation methods to induce event schemas .", "entities": [[6, 8, "TaskName", "graph generation"]]}, {"text": "\u000fThis is the \ufb01rst work to use complex event schemas for event type prediction , and also produce multiple hypotheses with probabilities .", "entities": [[12, 14, "TaskName", "type prediction"]]}, {"text": "\u000fWe have proposed a comprehensive set of metrics for both intrinsic and extrinsic evaluations .", "entities": []}, {"text": "\u000fWe release a new data set of 6,399 documents with gold - standard schemas annotated manually .", "entities": []}, {"text": "2 Problem Formulation From a set of documents describing a complex event , we construct an instance graph Gwhich contains event nodes Eand entity nodes ( argument nodes)V. There are three types of edges in this graph : ( 1 ) event - event edges hei;eliconnecting events that have direct temporal relations ; ( 2 ) evententity edgeshei;a;vjiconnecting arguments to the event ; and ( 3 ) entity - entity edges hvj;r;vkiindicating relations between entities .", "entities": []}, {"text": "We can construct instance graphs by applying Information Extraction ( IE ) techniques on an input text corpus .", "entities": []}, {"text": "In these graphs , the relation edges do not have directions but temporal edges between events are directional , going from the event before to the event after .", "entities": []}, {"text": "For each complex event type , given a set of instance graphsG , the goal of schema induction is to generate a schema library S.", "entities": []}, {"text": "In each schema graphS , the nodes are abstracted to the types of events and entities .", "entities": []}, {"text": "Figure 1 is an example", "entities": []}, {"text": "5205 ( 4 ) Entity Relation Edge Generation \u00a0 Arrest Arrest(5 )", "entities": []}, {"text": "Event T emporal Ordering Prediction \u00a0  ( 3 ) Coreferential Argument Generation \u00a0 ( 2 ) Edge - A ware Graph Neural Network \u00a0  ( 1 ) Event Generation \u00a0  Existing Graph \u00a0  PER T argetAttacker Temporal OrderingPlace PERLOC AttackDie V ictimPlacePER T argetAttacker Temporal OrderingDetainee JailorPlace PlacePERLOC AttackDie V ictimPlace PER T argetAttacker Temporal OrderingDetaineePER", "entities": []}, {"text": "JailorPlace GPEPlaceArrestPERLOC AttackDie V ictimPlacegeneration - mode copy - mode existing nodesvocab   ( entity types)PER T argetAttacker Temporal OrderingDetainee JailorPlace PlaceArrestPERLOC AttackDie V ictimPlace PER T argetAttacker Temporal OrderingDetainee JailorPlace PlaceArrestPERLOC AttackDie V ictimPlacePER T argetAttacker", "entities": []}, {"text": "Temporal OrderingDetainee JailorPlace GPEPlacePERLOC", "entities": []}, {"text": "AttackDie V ictimPlace part_whole part_whole GPE PER PER PERFigure 2 : The generation process of Temporal Event Graph Model .", "entities": []}, {"text": "of schema2for complex event type car - bombing .", "entities": []}, {"text": "Schema graphs can be regarded as a summary abstraction of instance graphs , capturing the reoccurring structures .", "entities": []}, {"text": "3 Our Approach 3.1 Instance Graph Construction", "entities": [[5, 7, "TaskName", "Graph Construction"]]}, {"text": "To induce schemas for a complex event type , such ascar - bombing , we construct a set of instance graphs , where each instance graph is about one complex event , such as Kabul ambulance bombing .", "entities": []}, {"text": "We \ufb01rst identify a cluster of documents that describes the same complex event .", "entities": []}, {"text": "In this paper , we treat all documents linked to a single Wikipedia page as belonging to the same complex event , detailed inx4.1 .", "entities": []}, {"text": "We use OneIE , a state - of - the - art Information Extraction system ( Lin et al . , 2020 ) , to extract entities , relations and events , and then perform crossdocument entity ( Pan et al . , 2015 , 2017 ) and event coreference resolution ( Lai et al . , 2021 ) over the document cluster of each complex event .", "entities": [[48, 51, "TaskName", "event coreference resolution"]]}, {"text": "We further conduct event - event temporal relation extraction ( Ning et al . , 2019 ; Wen et al . , 2021b ) to determine the order of event pairs .", "entities": [[6, 9, "TaskName", "temporal relation extraction"]]}, {"text": "We run the entire 2For simpli\ufb01cation purposes , we mention \u201c schema graphs \u201d as \u201c schemas \u201d , and \u201c events \u201d in schemas are only \u201c event types\u201d.pipeline following ( Wen et al . , 2021a)3 , and the detailed extraction performance is reported in the paper .", "entities": []}, {"text": "After extraction , we construct one instance graph for each complex event , where coreferential events or entities are merged .", "entities": []}, {"text": "We consider the isolated events as irrelevant nodes in schema induction , so they are excluded from the instance graphs during graph construction .", "entities": [[21, 23, "TaskName", "graph construction"]]}, {"text": "Considering schema graphs focus on type - level abstraction , we use type label and node index to represent each node , ignoring the mention level information in these instance graphs .", "entities": []}, {"text": "3.2 Temporal Event Graph Model Overview Given an instance graph G , we regard the schema as the hidden knowledge to guide the generation of these graphs .", "entities": []}, {"text": "To this end , we propose a temporal event graph model that maximizes the probability of each instance graph , parameterized byQ G2Gp(G ) .", "entities": []}, {"text": "At each step , based on the previous graphG < i , we predict one event node eiwith its arguments to generate the next graph Gi , p(G )", "entities": []}, {"text": "= YjEj i=0p(GijG < i ): 3https://github.com/RESIN-KAIROS/ RESIN - pipeline - public", "entities": []}, {"text": "5206We factorize the probability of generating new nodes and edges as : p(GijG < i ) = p(eijG < i)Y aj2A(ei)p(hei;aj;vjijei;aj ) Y", "entities": []}, {"text": "vk2G < ip(hvj;r;vkijvj;vk)Y el2G < ip(hei;elijei;el):(1 )", "entities": []}, {"text": "As shown in Figure 2 , an event node eiis generated \ufb01rst according to the probability p(eijG < i ) .", "entities": []}, {"text": "We then add argument nodes based on the IE ontology .", "entities": [[9, 10, "MethodName", "ontology"]]}, {"text": "We also predict relation hvj;r;vkibetween the newly generated node vjand the existing nodes vk2G < i.", "entities": []}, {"text": "After knowing the shared and related arguments , we add a \ufb01nal step to predict the temporal relations between the new event eiand the existing events el2G < i.", "entities": []}, {"text": "In the traditional graph generation setting , the order of node generation can be arbitrary .", "entities": [[3, 5, "TaskName", "graph generation"]]}, {"text": "However , in our instance graphs , event nodes are connected through temporal relations .", "entities": []}, {"text": "We order events as a directed acyclic graph ( DAG ) .", "entities": []}, {"text": "Considering each event may have multiple events both \u201c before \u201d and \u201c after \u201d , we obtain the generation order by traversing the graph using Breadth - First Search .", "entities": []}, {"text": "We also add dummy START /ENDevent nodes to indicate the starting / ending of the graph generation .", "entities": [[15, 17, "TaskName", "graph generation"]]}, {"text": "At the beginning of the generation process , the graphG0has a single start event node e[SOG ] .", "entities": []}, {"text": "We generatee[EOG]to signal the end of the graph .", "entities": []}, {"text": "3.3 Event Generation To determine the event type of the newly generated event nodeei , we apply a graph pooling over all events to get the current graph representation gi , gi = Pooling ( fe0;\u0001\u0001\u0001;ei\u00001 g ): We use bold to denote the latent representations of nodes and edges , which will be initialized as zeros and updated at each generation step via message passing inx3.4 .", "entities": []}, {"text": "We adopt a mean - pooling operation in this paper .", "entities": []}, {"text": "After that , the event type is predicted through a fully connected layer , p(eijG < i )", "entities": []}, {"text": "= exp(W \u001e ( ei)gi)P \u001e 02\bE[[EOG]exp(W \u001e 0gi ):", "entities": []}, {"text": "Once we know the event type of ei , we add all of its arguments in A(ei)de\ufb01ned in the IE ontology as new entity nodes .", "entities": [[20, 21, "MethodName", "ontology"]]}, {"text": "For example , in Figure 2 , the new event eiis an ARREST event , so we add three argument nodes for DETAINEE , JAILOR , and PLACE respectively .", "entities": []}, {"text": "The edges between these arguments and event eiare also added into the graph.3.4 Edge - Aware Graph Neural Network We use a Graph Neural Network ( GNN ) ( Kipf and Welling , 2017 ) to update node embeddings following the graph structure .", "entities": []}, {"text": "Before we run the GNN on the graph , we \ufb01rst add virtual edges between the newly generated event and all previous events , and between new entities and previous entities , shown as dashed lines in Figure 2 .", "entities": []}, {"text": "The virtual edges enable the representations of new nodes to aggregate the messages from previous nodes , which has been proven effective in ( Liao et al . , 2019 ) .", "entities": []}, {"text": "To capture rich semantics of edge types , we pass edge - aware messages during graph propagation .", "entities": []}, {"text": "An intuitive way is to encode different edge types with different convolutional \ufb01lters , which is similar to RGCN ( Schlichtkrull et al . , 2018 ) .", "entities": [[18, 19, "MethodName", "RGCN"]]}, {"text": "However , the number of RGCN parameters grows rapidly with the number of edge types and easily becomes unmanageable given the large number of relation types and argument roles in the IE ontology.4Instead , we learn a vector representation for each relation type rand argument role a.", "entities": [[5, 6, "MethodName", "RGCN"]]}, {"text": "The message passed through each argument edge hei;a;vjiis : mi;j = ReLU ( Wa((ei\u0000vj)ka ) ) ; wherekdenotes concatenation operation .", "entities": [[11, 12, "MethodName", "ReLU"]]}, {"text": "Similarly , the message between two entities vjandvkis : mj;k = ReLU ( Wr((vj\u0000vk)kr ) ):", "entities": [[11, 12, "MethodName", "ReLU"]]}, {"text": "Considering that the direction of the temporal edge is important , we parametrize the message over this edge by assigning two separate weight matrices to the outgoing and incoming vertices : mi;l = ReLU ( Wbfrei\u0000Waftel ): We aggregate the messages using edge - aware attention following ( Liao et al . , 2019):5 \u000b i;j=\u001b(MLP ( ei\u0000ej ) ) ; where\u001bis the sigmoid function , and MLP contains two hidden layers with ReLU nonlinearities .", "entities": [[33, 34, "MethodName", "ReLU"], [67, 68, "DatasetName", "MLP"], [73, 74, "MethodName", "ReLU"]]}, {"text": "The event node representation eiis then updated using the messages from its local neighbors N(ei ) , similar to entity node representations : ei GRU\u0012 eikX j2N(ei ) \u000b i;jmi;j\u0013 : 4There are 131 edge types according to the \ufb01ne - grained LDC Schema Learning Ontology .", "entities": [[45, 46, "MethodName", "Ontology"]]}, {"text": "5Compared to ( Liao et al . , 2019 ) , we do not use the positional embedding mask because the newly generated nodes have distinct roles .", "entities": []}, {"text": "52073.5 Coreferential Argument Generation After updating the node representations , we detect the entity type of each argument , and also predict whether the argument is coreferential to existing entities .", "entities": []}, {"text": "Inspired by copy mechanism ( Gu et al . , 2016 ) , we classify each argument node vjto either a new entity with entity type \u001e ( vj ) , or an existing entity node in the previous graph G < i.", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "For example , in Figure 2 , the DETAINEE should be classi\ufb01ed to the existing ATTACKER node , while JAILOR node is classi\ufb01ed as P ERSON .", "entities": []}, {"text": "Namely , p(hei;aj;vjijei;aj ) =( p(hei;aj;vji;gjei;aj ) ifvjis new ; p(hei;aj;vji;cjei;aj ) otherwise ; wherep(hei;aj;vji;gjei;aj)is the generation probability , classifying the new node to its entity type \u001e ( vj ): p(hei;aj;vji;gjei;aj )", "entities": []}, {"text": "= exp ( W \u001e ( vj)vj)\u000e Z", "entities": []}, {"text": "The copy probability p(hei;aj;vji;cjei;aj)selects the coreferential entity vfrom the entities in existing graph , denoted by V < i , p(hei;aj;vji;cjei;aj )", "entities": []}, {"text": "= exp ( Wvvj)\u000e Z : Here , Zis the shared normalization term , Z = X \u001e 02\bVexp(W \u001e 0vj)+X v02V < iexp(Wv0vj )", "entities": []}, {"text": "If determined to copy , we merge coreferential entities in the graph .", "entities": []}, {"text": "3.6 Entity Relational Edge Generation", "entities": []}, {"text": "In this phase , we determine the virtual edges to be kept and assign relation types to them , such as PARTWHOLE relation in Figure 2 .", "entities": []}, {"text": "We model the relation edge generation probability as a categorical distribution over relation types , and add [ O ] ( OTHER ) to the typesetRto represent that there is no relation edge : p(hvj;r;vkijvj;vk )", "entities": []}, {"text": "= exp ( MLPr(vj\u0000vk))P r02R [ [ O]exp ( MLPr0(vj\u0000vk ) )", "entities": []}, {"text": "We use two hidden layers with ReLU activation functions to implement the MLP.3.7", "entities": [[6, 7, "MethodName", "ReLU"]]}, {"text": "Event Temporal Ordering Prediction To predict the temporal dependencies between the new events and existing events , we connect them through temporal edges , as shown in Figure 2 .", "entities": []}, {"text": "These edges are critical for message passing in predicting the next event .", "entities": []}, {"text": "We build temporal edges in the last phase of generation , since it relies on the shared and related arguments .", "entities": []}, {"text": "Considering that temporal edges are interdependent , we model the generation probability as a mixture of Bernoulli distributions following ( Liao et al . , 2019 ): p(hei;elijei;el ) = X b", "entities": []}, {"text": "b\u0012b;i;l ;", "entities": []}, {"text": "1;\u0001\u0001\u0001 ;", "entities": []}, {"text": "B = Softmax\u0010X i;lMLP ( ei\u0000el)\u0011 ; \u00121;i;l;\u0001\u0001\u0001;\u0012B;i;l=\u001b(MLP\u0012(ei\u0000el ) ) ; whereBis the number of mixture components .", "entities": []}, {"text": "WhenB= 1 , the distribution degenerates to factorized Bernoulli , which assumes the independence of each potential temporal edge conditioned on the existing graph .", "entities": []}, {"text": "3.8 Training and Schema Decoding We train the model by optimizing the negative loglikelihood loss , L = X G2Gtrain\u0000log2p(G ): To compose the schema library for each complex event scenario , we construct instance graphs from related documents to learn a graph model , and then obtain the schema using greedy decoding .", "entities": [[14, 15, "MetricName", "loss"]]}, {"text": "4 Evaluation Benchmark 4.1 Dataset We conduct experiments on two datasets for both the general scenario and a more speci\ufb01c scenario .", "entities": []}, {"text": "We adopt the DARPA KAIROS6ontology , a newly de\ufb01ned \ufb01ne - grained ontology for Schema Learning , with 24 entity types , 46 relation types , 67 event types , and 85 argument roles.7Our schema induction method does not rely on any speci\ufb01c ontology , only the IE system is trained on a given ontology to create the instance event graphs .", "entities": [[3, 4, "DatasetName", "DARPA"], [12, 13, "MethodName", "ontology"], [43, 44, "MethodName", "ontology"], [54, 55, "MethodName", "ontology"]]}, {"text": "General Schema Learning Corpus : The Schema Learning Corpus , released by LDC ( LDC2020E25 ) , includes 82 types of complex events , such as Disease Outbreak , Presentations and Shop Online .", "entities": [[0, 1, "DatasetName", "General"]]}, {"text": "6https://github.com/NextCenturyCorporation/ kairos - pub / tree / master / data - format / ontology 7The ontology has been released in LDC2020E25 .", "entities": [[13, 14, "MethodName", "ontology"], [15, 16, "MethodName", "ontology"]]}, {"text": "5208Each complex event is associated with a set of source documents .", "entities": []}, {"text": "This data set also includes ground - truth schemas created by LDC annotators , which were used for our intrinsic evaluation .", "entities": []}, {"text": "Dataset Split", "entities": []}, {"text": "# doc # graph # event # arg # rel Train 451 451 6,040 10,720 6,858 General Dev 83", "entities": [[16, 17, "DatasetName", "General"]]}, {"text": "83 1,044 1,762 1,112 Test 83 83 1,211 2,112 1,363 Train 5,247 343 41,672 136,894 122,846 IED Dev 575 42 4,661 15,404 13,320 Test 577 45 5,089 16,721 14,054 Table 2 : Data statistics .", "entities": []}, {"text": "Each instance graph is about one complex event .", "entities": []}, {"text": "IED Schema Learning Corpus : The same type of complex events may have many variants , which depends on the different types of conditions and participants .", "entities": []}, {"text": "In order to evaluate our model \u2019s capability at capturing uncertainty and multiple hypotheses , we decided to dive deeper into one scenario and chose the improvised explosive device ( IED ) as our case study .", "entities": []}, {"text": "We \ufb01rst collected Wikipedia articles that describe 4 types of complex events , i.e. ,Car - bombing IED , Drone Strikes IED , Suicide IED andGeneral IED .", "entities": []}, {"text": "Then we followed ( Li et al . , 2021 ) to exploit the external links to collect the additional news documents with the corresponding complex event type .", "entities": []}, {"text": "The ground - truth schemas for this IED corpus are created manually , through a schema curation tool ( Mishra et al . , 2021 ) .", "entities": []}, {"text": "Only one human schema graph was created for each complex event type , resulting in 4 schemas .", "entities": []}, {"text": "In detail , for each complex event type , we presented example instance graphs and the ranked event sequences to annotators to create human ( ground truth ) schemas .", "entities": []}, {"text": "The event sequences are generated by traversing the instance graphs , and then sorted by frequency and the number of arguments .", "entities": []}, {"text": "Initially we assigned three annotators ( IE experts ) to each create a version of the schema and then the \ufb01nal schema was merged through discussion .", "entities": []}, {"text": "After that , two annotators ( linguists ) performed a two - pass revision .", "entities": []}, {"text": "Human curation focuses on merging and trimming steps by validating them using the reference instance graphs .", "entities": []}, {"text": "Also , temporal dependencies between steps were further re\ufb01ned , and coreferential entities and their relations were added during the curation process .", "entities": []}, {"text": "To avoid bias from the event sequences , linguists in the second round revision were not presented with the event sequences .", "entities": []}, {"text": "All annotators were trainedand disagreements were resolved through discussion .", "entities": []}, {"text": "4.2 Schema Matching Evaluation We compare the generated schemas with the ground truth schemas based on the overlap between them .", "entities": []}, {"text": "The following evaluation metrics were employed:8 Event Match :", "entities": []}, {"text": "A good schema must contain the events crucial to the complex event scenario .", "entities": []}, {"text": "Fscore is used to compute the overlap of event nodes .", "entities": []}, {"text": "Event Sequence Match :", "entities": []}, {"text": "A good schema is able to track events through a timeline .", "entities": []}, {"text": "So we obtain event sequences following temporal order , and evaluate F - score on the overlapping sequences of lengths l= 2andl= 3 .", "entities": []}, {"text": "Event Argument Connection Match :", "entities": []}, {"text": "Our complex event graph schema includes entities and their relations and captures how events are connected through arguments , in addition to their temporal order .", "entities": []}, {"text": "We categorize these connections into three categories : ( 1 ) two events are connected by shared arguments ; ( 2 ) two events have related arguments , i.e. , their arguments are connected through entity relations ; ( 3 ) there are no direct connections between two events .", "entities": []}, {"text": "For every pair of overlapped events , we calculate F - score based on whether these connections are predicted correctly .", "entities": []}, {"text": "The human schemas of the General dataset do not contain arguments and the relations between arguments , so we only compute this metric for the IED dataset .", "entities": [[5, 6, "DatasetName", "General"]]}, {"text": "4.3 Instance Graph Perplexity Evaluation To evaluate our temporal event graph model , we compute the instance graph perplexity by predicting the instance graphs in the test set , PP = 2\u00001 jGtestjP G2Gtestlog2p(G ) : ( 1 ) We calculate the full perplexity for the entire graph using Equation ( 1 ) , and event perplexity using only event nodes , emphasizing the importance of correctly predicting events .", "entities": [[3, 4, "MetricName", "Perplexity"], [18, 19, "MetricName", "perplexity"], [43, 44, "MetricName", "perplexity"], [56, 57, "MetricName", "perplexity"]]}, {"text": "4.4 Schema - Guided Event Prediction To explore schema - guided probabilistic reasoning and prediction , we perform an extrinsic evaluation of event prediction .", "entities": []}, {"text": "Different from traditional event prediction tasks , the temporal event graphs contain arguments with relations , and there are 8We can not use graph matching to compare between baselines and our approach due to the difference in the graph structures being modeled .", "entities": [[23, 25, "TaskName", "graph matching"]]}, {"text": "5209type labels assigned to nodes and edges .", "entities": []}, {"text": "We create a graph - based event prediction dataset using our testing graphs .", "entities": []}, {"text": "The task aims to predict ending events of each graph , i.e. , events that have no future events after it .", "entities": []}, {"text": "An event is predicted correctly if its event type matches one of the ending events in the graph .", "entities": []}, {"text": "Considering that there can be multiple ending events in one instance graph , we rank event type prediction scores and adopt MRR ( Mean Reciprocal Rank ) andHITS@1 as evaluation metrics .", "entities": [[16, 18, "TaskName", "type prediction"], [21, 22, "MetricName", "MRR"]]}, {"text": "5 Experiments 5.1 Experiment Setting Baseline 1 : Event Language Model ( Rudinger et al . , 2015 ; Pichotta and Mooney , 2016 ) is the state - of - the - art event schema induction method .", "entities": []}, {"text": "It learns the probability of temporal event sequences , and the event sequences generated from event language model are considered as schemas .", "entities": []}, {"text": "Baseline 2 : Sequential Pattern Mining ( Pei et al . , 2001 ) is a classic algorithm for discovering common sequences .", "entities": []}, {"text": "We also attach arguments and their relations as extensions to the pattern .", "entities": []}, {"text": "Considering that the event language model baseline can not handle multiple arguments and relations , we add sequential pattern mining for comparison .", "entities": []}, {"text": "The frequent patterns mined are considered as schemas .", "entities": []}, {"text": "Reference : Human Schema is added as a baseline in the extrinsic task of event prediction .", "entities": []}, {"text": "Since human - created schemas are highly accurate but not probabilistic , we want to evaluate their limits at predicting events in the extrinsic task .", "entities": []}, {"text": "We match schemas to instances and \ufb01ll in the matched type .", "entities": []}, {"text": "Ablation Study : Event Graph Model w/o Argument Generation is included as a variant of our model in which we remove argument generation ( x3.5 andx3.6 ) .", "entities": []}, {"text": "It learns to generate a graph containing only event nodes with their temporal relations , aiming to verify whether incorporating argument information helps event modeling .", "entities": []}, {"text": "5.2 Implementation Details Training Details .", "entities": []}, {"text": "For our event graph model , the representation dimension is 128 , and we use a 2 - layer GNN .", "entities": []}, {"text": "The value of Bis 2 .", "entities": []}, {"text": "The number of mixture components in temporal classi\ufb01er is 2 .", "entities": []}, {"text": "The learning rate is 1e-4 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "To train event language model baseline , instead of using LSTM - based architecture following ( Pichotta and Mooney , 2016 ) , we adopt the state - of - the - art auto - regressive language XLNet ( Yang et al . , 2019 ) .", "entities": [[10, 11, "MethodName", "LSTM"], [37, 38, "MethodName", "XLNet"]]}, {"text": "In detail , we\ufb01rst linearize the graph using topological sort , and then train XLNet9using the dimension of 128 ( the same as our temporal event graph model ) , and the number of layers is 3 .", "entities": [[32, 35, "HyperparameterName", "number of layers"]]}, {"text": "The learning rate is 1e-4 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "We select the best model on the validation set .", "entities": []}, {"text": "Both of our model and event language model baseline are trained on one Tesla V100 GPU with 16 GB DRAM .", "entities": []}, {"text": "For sequential pattern mining , we perform random walk , starting from every node in instance graphs and ending at sink nodes , to obtain event type sequences , and then apply Pre\ufb01xSpan ( Pei et al . , 2001)10to rank sequential patterns .", "entities": []}, {"text": "Evaluation Details .", "entities": []}, {"text": "To compose the schema library , we use the \ufb01rst ranked sequence as the schema for these two models .", "entities": []}, {"text": "To perform event prediction using baselines , we traverse the input graph to obtain event type sequences , and conduct prediction on all sequences to produce an averaged score .", "entities": []}, {"text": "For human schemas , we \ufb01rst linearize them and the input graphs , and \ufb01nd the longest common subsequence between them .", "entities": []}, {"text": "5.3 Results and Analysis Intrinsic Evaluation .", "entities": []}, {"text": "In Table 3 , the signi\ufb01cant gain on event match demonstrates the ability of our graph model to keep salient events .", "entities": []}, {"text": "On sequence match , our approach achieves larger performance gain compared to baselines when the path length lis longer .", "entities": []}, {"text": "It implies that the proposed model is capable of capturing longer and wider temporal dependencies .", "entities": []}, {"text": "In the case of connection match , only sequential pattern mining in the baselines can predict connections between events .", "entities": []}, {"text": "When compared against sequential pattern mining , our generation model signi\ufb01cantly performs better since it considers the inter - dependency of arguments and encodes them with graph structures .", "entities": []}, {"text": "Extrinsic Evaluation .", "entities": []}, {"text": "On the task of schemaguided event prediction , our graph model obtains signi\ufb01cant improvement ( see Table 4 . )", "entities": []}, {"text": "The low performance of human schema demonstrates the importance of probabilistically modeling schemas to support downstream tasks .", "entities": []}, {"text": "Take Figure 3 as an example .", "entities": []}, {"text": "Human schemas produce incorrect event types such as TRAIL HEARING , since it matches the sequence ATTACK!DIE!TRAIL HEARING , incapable of capturing the inter - dependencies between sequences .", "entities": []}, {"text": "However , our model is able to customize the prediction to the global context of the input 9https://github.com/huggingface 10https://github.com/chuanconggao/ PrefixSpan - py", "entities": []}, {"text": "5210ModelEvent Sequence Match Connection Event Full Dataset Match l=", "entities": []}, {"text": "2l= 3 Match Perplexity Perplexity GeneralEvent Language Model 54.76 22.87 8.61 - - Sequential Pattern Mining 49.18 20.31 7.37 - - Event Graph Model", "entities": [[3, 4, "MetricName", "Perplexity"], [4, 5, "MetricName", "Perplexity"]]}, {"text": "58.15 24.79 9.18 - 24.25 137.18 w/o ArgumentGeneration 56.96 22.47 8.21 - 68.59 IEDEvent Language Model 49.15 17.77 5.32 - - Sequential Pattern Mining 47.91 18.39 4.79 5.41 - Event Graph Model 59.73 21.51 7.81 10.67 39.39 168.89 w/o ArgumentGeneration 55.01 18.24 6.67 - 51.98 Table 3 : Intrinsic evaluation results , including schema matching F1 score ( % ) and instance graph perplexity .", "entities": [[55, 57, "MetricName", "F1 score"], [63, 64, "MetricName", "perplexity"]]}, {"text": "Dataset Model MRR HITS@1 GeneralEvent Language Model 0.367 0.497 Sequential Pattern Mining 0.330 0.478 Human Schema 0.173 0.205", "entities": [[2, 3, "MetricName", "MRR"], [3, 4, "MetricName", "HITS@1"]]}, {"text": "Event Graph Model 0.401 0.520 w/o ArgumentGeneraion 0.392 0.509 IEDEvent Language Model 0.169 0.513 Sequential Pattern Mining 0.138 0.378 Human Schema 0.072 0.222 Event Graph Model 0.224 0.741 w/o ArgumentGeneraion 0.210 0.734 Table 4 : Schema - guided event prediction performance .", "entities": []}, {"text": "graph , and take into account that there is no ARREST event or justice - related events in the input graph .", "entities": []}, {"text": "Also , the human schema fails to predict INJURE andATTACK , because it relies on the exact match of event sequences of lengths l\u00152 , and can not handle the variants of sequences .", "entities": [[16, 18, "MetricName", "exact match"]]}, {"text": "This problem can be solved by our probabilistic schema , via modeling the prediction probability conditioned on the existing graph .", "entities": []}, {"text": "For example , even though ATTACK mostly happens before DIE , we learn that ATTACK might repeat after DIEevent if there are multiple ATTACK andDETONATE in the existing graph , which means the complex event is about a series of con\ufb02ict events .", "entities": []}, {"text": "Ablation Study .", "entities": []}, {"text": "Removing argument generation ( \u201c w/o ArgumentGeneration \u201d ) generally lowers the performance on all evaluation tasks , since it ignores the coreferential arguments and their relations , but relies solely on the overly simplistic temporal order to connect events .", "entities": []}, {"text": "This is especially apparent from the instance graph perplexity in Table 3 .", "entities": [[8, 9, "MetricName", "perplexity"]]}, {"text": "Learning Corpus Size .", "entities": []}, {"text": "An average of 113 instance graphs is used for each complex event type existing events AttackDetonate Explode InjureAttackDie DieDie InjureContact DieAttackBroadcast ImpedeInterfereW ithevents to be", "entities": []}, {"text": "predictedPrediction Result Human SchemaFireExplosion Die TrialHearing BroadcastTransportation Sentence Graph Temporal SchemaDie Injure Attack Broadcast ArrestInput GraphFigure 3 : An event prediction example ( IED scenario ) .", "entities": []}, {"text": "in the IED scenario , and 383 instance graphs to learn the schema model in the General scenario .", "entities": [[16, 17, "DatasetName", "General"]]}, {"text": "The better performance on the IED dataset in Table 3 shows that the number of instance graphs increases the schema induction performance .", "entities": []}, {"text": "Effect of Information Extraction Errors .", "entities": []}, {"text": "Based on the error analysis for schemas induced in Table 1 , the effect of extraction errors can be categorized into : ( 1 ) temporal ordering errors : 43.3 % ; ( 2 ) missing events : 34.4 % ; ( 3 ) missing coreferential events : 8.8 % ; ( 4 ) incorrect event type : 7.7 % ; ( 5 ) missing coreferential arguments : 5.5 % .", "entities": []}, {"text": "However , even on automatically extracted event graphs with extraction errors , our model signi\ufb01cantly performs better on event prediction compared to humanconstructed schemas , as shown in Table 4 .", "entities": []}, {"text": "It demonstrates that our schema induction method is robust and effective to support downstream tasks , even when only provided with noisy data with extraction errors .", "entities": []}, {"text": "6 Related Work The de\ufb01nition of a complex event schema separates us from related lines of work , namely schema induction andscript learning .", "entities": []}, {"text": "Previous work on schema induction aims to characterize", "entities": []}, {"text": "5211event triggers and participants of individual atomic events ( Chambers , 2013 ; Cheung et al . , 2013 ; Nguyen et al . , 2015 ; Sha et al . , 2016 ; Yuan et", "entities": []}, {"text": "al . , 2018 ) , ignoring inter - event relations .", "entities": []}, {"text": "Work on script learning , on the other hand , originally limited attention to event chains with a single protagonist ( Chambers and Jurafsky , 2008 , 2009 ; Rudinger et al . , 2015 ; Jans et al . , 2012 ; Granroth - Wilding and Clark , 2016 ) and later extended to multiple participants ( Pichotta and Mooney , 2014 , 2016 ; Weber et al . , 2018 ) .", "entities": []}, {"text": "Recent efforts rely on distributed representations encoded from the compositional nature of events ( Modi , 2016 ; Granroth - Wilding and Clark , 2016 ; Weber et al . , 2018 , 2020 ; Zhang et al . , 2020 ) , and language modeling ( Rudinger et al . , 2015 ; Pichotta and Mooney , 2016 ; Peng and Roth , 2016 ) .", "entities": []}, {"text": "All of these methods still assume that events follow linear order in a single chain .", "entities": []}, {"text": "They also overlook the relations between participants which are critical for understanding the complex event .", "entities": []}, {"text": "However , we induce a comprehensive event graph schema , capturing both the temporal dependency and the multi - hop argument dependency across events .", "entities": []}, {"text": "Recent work on event graph schema induction ( Li et al . , 2020 ) only considers the connections between a pair of two events .", "entities": []}, {"text": "Similarly , their event prediction task is designed to automatically generate a missing event ( e.g. , a word sequence ) given a single or a sequence of prerequisite events ( Nguyen et al . , 2017 ;", "entities": []}, {"text": "Hu et al . , 2017 ; Li et al . , 2018b ; Kiyomaru et al . , 2019 ; Lv et al . , 2019 ) , or predict a pre - condition event given the current events ( Kwon et al . , 2020 ) .", "entities": []}, {"text": "In contrast , we leverage the automatically discovered temporal event schema as guidance to forecast the future events .", "entities": []}, {"text": "Existing script annotations ( Chambers and Jurafsky , 2008 , 2010 ; Modi et al . , 2016 ;", "entities": []}, {"text": "Wanzare et al . , 2016 ; Mostafazadeh et al . , 2016a , b ; Kwon et al . , 2020 ) can not support a comprehensive graph schema induction due to the missing of critical event graph structures , such as argument relations .", "entities": []}, {"text": "Furthermore , in real - world applications , complex event schemas are expected to be induced from large - scale historical data , which is not feasible to annotate manually .", "entities": []}, {"text": "We propose a data - driven schema induction approach , and choose to use IE systems instead of using manual annotation , to induce schemas that are robust and can tolerate extraction errors .", "entities": []}, {"text": "Our work is also related to recent advances inmodeling and generation of graphs ( Li et al . , 2018a ;", "entities": []}, {"text": "Jin et al . , 2018 ; Grover et al . , 2019 ; Simonovsky and Komodakis , 2018 ; Liu et al . , 2019 ; Fu et al . , 2020 ; Dai et al . , 2020 ; You et al . , 2018 ; Liao et al . , 2019 ; Yoo et al . , 2020 ; Shi et al . , 2020 ) .", "entities": []}, {"text": "We are the \ufb01rst to perform graph generation on event graphs .", "entities": [[6, 8, "TaskName", "graph generation"]]}, {"text": "7 Conclusions and Future Work We propose a new task to induce temporal complex event schemas , which are capable of representing multiple temporal dependencies between events and their connected arguments .", "entities": []}, {"text": "We induce such schemas by learning an event graph model , a deep auto - regressive model , from the automatically extracted instance graphs .", "entities": []}, {"text": "Experiments demonstrate the model \u2019s effectiveness on both intrinsic evaluation and the downstream task of schema - guided event prediction .", "entities": []}, {"text": "These schemas can guide our understanding and ability to make predictions with respect to what might happen next , along with background knowledge including location- , and participant - speci\ufb01c and temporally ordered event information .", "entities": []}, {"text": "In the future , we plan to extend our framework to hierarchical event schema induction , as well as event and argument instance prediction .", "entities": []}, {"text": "Acknowledgement This research is based upon work supported by U.S. DARPA KAIROS Program Nos .", "entities": [[10, 11, "DatasetName", "DARPA"]]}, {"text": "FA8750 - 19 - 21004 and Air Force No .", "entities": []}, {"text": "FA8650 - 17 - C-7715 .", "entities": []}, {"text": "The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the of\ufb01cial policies , either expressed or implied , of DARPA , or the U.S. Government .", "entities": [[29, 30, "DatasetName", "DARPA"]]}, {"text": "The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein .", "entities": []}, {"text": "References Nathanael Chambers .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Event schema induction with a probabilistic entity - driven model .", "entities": []}, {"text": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1797\u20131807 , Seattle , Washington , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Nathanael Chambers and Dan Jurafsky .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Unsupervised learning of narrative event chains .", "entities": []}, {"text": "In Proceedings of ACL-08 : HLT , pages 789\u2013797 , Columbus , Ohio .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "5212Nathanael Chambers and Dan Jurafsky .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Unsupervised learning of narrative schemas and their participants .", "entities": []}, {"text": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 602\u2013610 , Suntec , Singapore .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nathanael Chambers and Dan Jurafsky .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "A database of narrative schemas .", "entities": []}, {"text": "In Proceedings of the Seventh International Conference on Language Resources and Evaluation ( LREC\u201910 ) , Valletta , Malta .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Jackie Chi Kit Cheung , Hoifung Poon , and Lucy Vanderwende .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Probabilistic frame induction .", "entities": []}, {"text": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 837\u2013846 , Atlanta , Georgia . Association for Computational Linguistics .", "entities": []}, {"text": "Hanjun Dai , Azade Nazi , Yujia Li , Bo Dai , and Dale Schuurmans .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Scalable deep generative modeling for sparse graphs .", "entities": []}, {"text": "In Proceedings of the 37th International Conference on Machine Learning , ICML 2020 , 13 - 18 July 2020 , Virtual Event , volume 119 of Proceedings of Machine Learning Research , pages 2302\u20132312 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Dongqi Fu , Dawei Zhou , and Jingrui He . 2020 .", "entities": []}, {"text": "Local motif clustering on time - evolving graphs .", "entities": []}, {"text": "In KDD \u2019 20 : The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , Virtual Event , CA , USA , August 23 - 27 , 2020 , pages 390\u2013400 .", "entities": [[7, 8, "DatasetName", "ACM"]]}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Mark Granroth - Wilding and Stephen Clark .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "What happens next ?", "entities": []}, {"text": "event prediction using a compositional neural network model .", "entities": []}, {"text": "In Proceedings of the Thirtieth AAAI Conference on Arti\ufb01cial Intelligence , February 12 - 17 , 2016 , Phoenix , Arizona , USA , pages 2727\u20132733 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Aditya Grover , Aaron Zweig , and Stefano Ermon .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Graphite : Iterative generative modeling of graphs .", "entities": []}, {"text": "In Proceedings of the 36th International Conference on Machine Learning , ICML 2019 , 915 June 2019 , Long Beach , California , USA , volume 97 of Proceedings of Machine Learning Research , pages 2434\u20132444 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Jiatao Gu , Zhengdong Lu , Hang Li , and Victor O.K. Li . 2016 .", "entities": []}, {"text": "Incorporating copying mechanism in sequence - to - sequence learning .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1631\u20131640 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Linmei Hu , Juanzi Li , Liqiang Nie , Xiaoli Li , and Chao Shao . 2017 .", "entities": []}, {"text": "What happens next ?", "entities": []}, {"text": "future subevent prediction using contextual hierarchical LSTM .", "entities": [[6, 7, "MethodName", "LSTM"]]}, {"text": "In Proceedings of the Thirty - First AAAI Conference onArti\ufb01cial Intelligence , February 4 - 9 , 2017 , San Francisco , California , USA , pages 3450\u20133456 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Bram Jans , Steven Bethard , Ivan Vuli \u00b4 c , and Marie Francine Moens .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Skip n - grams and ranking functions for predicting script events .", "entities": []}, {"text": "InProceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics , pages 336\u2013344 , Avignon , France .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Wengong Jin , Regina Barzilay , and Tommi S. Jaakkola .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Junction tree variational autoencoder for molecular graph generation .", "entities": [[2, 4, "MethodName", "variational autoencoder"], [5, 8, "TaskName", "molecular graph generation"]]}, {"text": "In Proceedings of the 35th International Conference on Machine Learning , ICML 2018 , Stockholmsm \u00a8assan , Stockholm , Sweden , July 10 - 15 , 2018 , volume 80 of Proceedings of Machine Learning Research , pages 2328\u20132337 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Thomas N. Kipf and Max Welling . 2017 .", "entities": []}, {"text": "Semisupervised classi\ufb01cation with graph convolutional networks .", "entities": []}, {"text": "In 5th International Conference on Learning Representations , ICLR 2017 , Toulon , France , April 24 - 26 , 2017 , Conference Track Proceedings .", "entities": []}, {"text": "OpenReview.net .", "entities": []}, {"text": "Hirokazu Kiyomaru , Kazumasa Omura , Yugo Murawaki , Daisuke Kawahara , and Sadao Kurohashi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Diversity - aware event prediction based on a conditional variational autoencoder with reconstruction .", "entities": [[9, 11, "MethodName", "variational autoencoder"]]}, {"text": "In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing , pages 113\u2013122 , Hong Kong , China . Association for Computational Linguistics .", "entities": []}, {"text": "Heeyoung Kwon , Mahnaz Koupaee , Pratyush Singh , Gargi Sawhney , Anmol Shukla , Keerthi Kumar Kallur , Nathanael Chambers , and Niranjan Balasubramanian .", "entities": [[16, 17, "DatasetName", "Kumar"]]}, {"text": "2020 .", "entities": []}, {"text": "Modeling preconditions in text with a crowd - sourced dataset .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 3818\u20133828 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tuan Lai , Heng Ji , Trung Bui , Quan Hung Tran , Franck Dernoncourt , and Walter Chang .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "A contextdependent gated module for incorporating symbolic semantics into event coreference resolution .", "entities": [[9, 12, "TaskName", "event coreference resolution"]]}, {"text": "In Proc .", "entities": []}, {"text": "The 2021 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies ( NAACLHLT2021 ) .", "entities": []}, {"text": "Manling Li , Qi Zeng , Ying Lin , Kyunghyun Cho , Heng Ji , Jonathan May , Nathanael Chambers , and Clare V oss . 2020 .", "entities": []}, {"text": "Connecting the dots : Event graph schema induction with path language modeling .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 684\u2013695 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sha Li , Heng Ji , and Jiawei Han . 2021 .", "entities": []}, {"text": "Documentlevel event argument extraction by conditional generation .", "entities": [[1, 4, "TaskName", "event argument extraction"]]}, {"text": "In Proceedings of the 2021 Conference of", "entities": []}, {"text": "5213the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 894\u2013908 .", "entities": []}, {"text": "Yujia Li , Oriol Vinyals , Chris Dyer , Razvan Pascanu , and Peter Battaglia . 2018a .", "entities": []}, {"text": "Learning deep generative models of graphs .", "entities": []}, {"text": "In Proceedings of the 35 th International Conference on Machine Learning , Stockholm , Sweden , PMLR 80 .", "entities": []}, {"text": "Zhongyang Li , Xiao Ding , and Ting Liu . 2018b .", "entities": []}, {"text": "Constructing narrative event evolutionary graph for script event prediction .", "entities": []}, {"text": "In Proceedings of the Twenty - Seventh International Joint Conference on Arti\ufb01cial Intelligence , IJCAI 2018 , July 13 - 19 , 2018 , Stockholm , Sweden , pages 4201\u20134207 .", "entities": []}, {"text": "ijcai.org .", "entities": []}, {"text": "Renjie Liao , Yujia Li , Yang Song , Shenlong Wang , William L. Hamilton , David Duvenaud , Raquel Urtasun , and Richard S. Zemel .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Ef\ufb01cient graph generation with graph recurrent attention networks .", "entities": [[1, 3, "TaskName", "graph generation"]]}, {"text": "InAdvances in Neural Information Processing Systems 32 : Annual Conference on Neural Information Processing Systems 2019 , NeurIPS 2019 , December 8 - 14 , 2019 , Vancouver , BC , Canada , pages 4257 \u2013 4267 .", "entities": []}, {"text": "Ying Lin , Heng Ji , Fei Huang , and Lingfei Wu . 2020 .", "entities": []}, {"text": "A joint neural model for information extraction with global features .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7999\u20138009 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jenny Liu , Aviral Kumar , Jimmy Ba , Jamie Kiros , and Kevin Swersky .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Graph normalizing \ufb02ows .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 32 : Annual Conference on Neural Information Processing Systems 2019 , NeurIPS 2019 , December 814 , 2019 , Vancouver , BC , Canada , pages 13556 \u2013 13566 .", "entities": []}, {"text": "Shangwen Lv , Wanhui Qian , Longtao Huang , Jizhong Han , and Songlin Hu . 2019 .", "entities": []}, {"text": "Sam - net : Integrating event - level and chain - level attentions to predict what happens next .", "entities": []}, {"text": "In The Thirty - Third AAAI Conference on Arti\ufb01cial Intelligence , AAAI 2019 , The Thirty - First Innovative Applications of Arti\ufb01cial Intelligence Conference , IAAI 2019 , The Ninth AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence , EAAI 2019 , Honolulu , Hawaii , USA , January 27 - February 1 , 2019 , pages 6802\u20136809 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Piyush Mishra , Akanksha Malhotra , Susan Windisch Brown , Martha Palmer , and Ghazaleh Kazeminejad .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Schema curation interface :", "entities": []}, {"text": "Enhancing user experience in curation tasks .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( ACL - IJCNLP 2021 )", "entities": []}, {"text": "Demo Track .", "entities": []}, {"text": "Ashutosh Modi .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Event embeddings for semantic script modeling .", "entities": []}, {"text": "In Proceedings of The 20thSIGNLL Conference on Computational Natural Language Learning , pages 75\u201383 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Ashutosh Modi , Tatjana Anikina , Simon Ostermann , and Manfred Pinkal .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "InScript : Narrative texts annotated with script information .", "entities": []}, {"text": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC\u201916 ) , pages 3485 \u2013 3493 , Portoro \u02c7z , Slovenia .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Nasrin Mostafazadeh , Nathanael Chambers , Xiaodong He , Devi Parikh , Dhruv Batra , Lucy Vanderwende , Pushmeet Kohli , and James Allen . 2016a .", "entities": []}, {"text": "A corpus and cloze evaluation for deeper understanding of commonsense stories .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 839\u2013849 , San Diego , California .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nasrin Mostafazadeh , Alyson Grealish , Nathanael Chambers , James Allen , and Lucy Vanderwende . 2016b . CaTeRS : Causal and temporal relation scheme for semantic annotation of event structures .", "entities": []}, {"text": "InProceedings of the Fourth Workshop on Events , pages 51\u201361 , San Diego , California .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Dai Quoc Nguyen , Dat Quoc Nguyen , Cuong Xuan Chu , Stefan Thater , and Manfred Pinkal . 2017 .", "entities": []}, {"text": "Sequence to sequence learning for event prediction .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "In Proceedings of the Eighth International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 37\u201342 , Taipei , Taiwan .", "entities": []}, {"text": "Asian Federation of Natural Language Processing .", "entities": []}, {"text": "Kiem - Hieu Nguyen , Xavier Tannier , Olivier Ferret , and Romaric Besanc \u00b8on .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Generative event schema induction with entity disambiguation .", "entities": [[5, 7, "TaskName", "entity disambiguation"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 188 \u2013 197 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Qiang Ning , Sanjay Subramanian , and Dan Roth . 2019 .", "entities": []}, {"text": "An improved neural baseline for temporal relation extraction .", "entities": [[5, 8, "TaskName", "temporal relation extraction"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 6203\u20136209 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xiaoman Pan , Taylor Cassidy , Ulf Hermjakob , Heng Ji , and Kevin Knight .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Unsupervised entity linking with abstract meaning representation .", "entities": [[1, 3, "TaskName", "entity linking"]]}, {"text": "In Proc .", "entities": []}, {"text": "the 2015 Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies ( NAACL - HLT 2015 ) .", "entities": []}, {"text": "5214Xiaoman Pan , Boliang Zhang , Jonathan May , Joel Nothman , Kevin Knight , and Heng Ji . 2017 .", "entities": []}, {"text": "Crosslingual name tagging and linking for 282 languages .", "entities": []}, {"text": "InProceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1946\u20131958 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jian Pei , Jiawei Han , Behzad Mortazavi - Asl , Helen Pinto , Qiming Chen , Umeshwar Dayal , and Meichun Hsu . 2001 .", "entities": [[11, 12, "DatasetName", "Helen"]]}, {"text": "Pre\ufb01xspan : Mining sequential patterns by pre\ufb01x - projected growth .", "entities": []}, {"text": "In Proceedings of the 17th International Conference on Data Engineering , pages 215\u2013224 .", "entities": []}, {"text": "Haoruo Peng and Dan Roth . 2016 .", "entities": []}, {"text": "Two discourse driven language models for semantics .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 290\u2013300 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Karl Pichotta and Raymond Mooney .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Statistical script learning with multi - argument events .", "entities": []}, {"text": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics , pages 220\u2013229 , Gothenburg , Sweden .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Karl Pichotta and Raymond J. Mooney . 2016 .", "entities": []}, {"text": "Learning statistical scripts with LSTM recurrent neural networks .", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "In Proceedings of the Thirtieth AAAI Conference on Arti\ufb01cial Intelligence , February 1217 , 2016 , Phoenix , Arizona , USA , pages 2800\u20132806 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Rachel Rudinger , Pushpendre Rastogi , Francis Ferraro , and Benjamin Van Durme .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Script induction as language modeling .", "entities": []}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1681\u20131686 , Lisbon , Portugal . Association for Computational Linguistics .", "entities": []}, {"text": "Michael Schlichtkrull , Thomas N Kipf , Peter Bloem , Rianne Van Den Berg , Ivan Titov , and Max Welling .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Modeling relational data with graph convolutional networks .", "entities": []}, {"text": "In European Semantic Web Conference , pages 593\u2013607 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Lei Sha , Sujian Li , Baobao Chang , and Zhifang Sui . 2016 .", "entities": []}, {"text": "Joint learning templates and slots for event schema induction .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 428\u2013434 , San Diego , California . Association for Computational Linguistics .", "entities": []}, {"text": "Chence Shi , Minkai Xu , Zhaocheng Zhu , Weinan Zhang , Ming Zhang , and Jian Tang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Graphaf : a \ufb02ow - based autoregressive model for molecular graph generation .", "entities": [[9, 12, "TaskName", "molecular graph generation"]]}, {"text": "In 8th International Conference on Learning Representations , ICLR 2020 , Addis Ababa , Ethiopia , April 26 - 30 , 2020 .", "entities": []}, {"text": "OpenReview.net .", "entities": []}, {"text": "Martin Simonovsky and Nikos Komodakis .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graphvae : Towards generation of small graphs using variational autoencoders .", "entities": [[9, 10, "MethodName", "autoencoders"]]}, {"text": "In International Conference on Arti\ufb01cial Neural Networks , pages 412\u2013422 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Lilian DA Wanzare , Alessandra Zarcone , Stefan Thater , and Manfred Pinkal .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A crowdsourced database of event sequence descriptions for the acquisition of high - quality script knowledge .", "entities": []}, {"text": "Noah Weber , Niranjan Balasubramanian , and Nathanael Chambers .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Event representations with tensor - based compositions .", "entities": []}, {"text": "In Proceedings of the Thirty - Second AAAI Conference on Arti\ufb01cial Intelligence , ( AAAI-18 ) , the 30th innovative Applications of Arti\ufb01cial Intelligence ( IAAI-18 ) , and the 8th AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence ( EAAI-18 ) , New Orleans , Louisiana , USA , February 2 - 7 , 2018 , pages 4946\u20134953 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Noah Weber , Rachel Rudinger , and Benjamin Van Durme .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Causal inference of script knowledge .", "entities": [[0, 2, "MethodName", "Causal inference"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 7583\u20137596 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Haoyang Wen , Ying Lin , Tuan Lai , Xiaoman Pan , Sha Li , Xudong Lin , Ben Zhou , Manling Li , Haoyu Wang , Hongming Zhang , et al . 2021a .", "entities": []}, {"text": "Resin : A dockerized schema - guided cross - document crosslingual cross - media information extraction and event tracking system .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies : Demonstrations , pages 133\u2013143 .", "entities": []}, {"text": "Haoyang Wen , Yanru Qu , Heng Ji , Qiang Ning , Jiawei Han , Avi Sil , Hanghang Tong , and Dan Roth . 2021b .", "entities": []}, {"text": "Event time extraction and propagation via graph attention networks .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "The 2021 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies ( NAACL - HLT2021 ) .", "entities": []}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime G. Carbonell , Ruslan Salakhutdinov , and Quoc V .", "entities": [[13, 14, "DatasetName", "Ruslan"]]}, {"text": "Le . 2019 .", "entities": []}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 32 : Annual Conference on Neural Information Processing Systems 2019 , NeurIPS 2019 , December 8 - 14 , 2019 , Vancouver , BC , Canada , pages 5754\u20135764 .", "entities": []}, {"text": "Sanghyun Yoo , Young - Seok Kim , Kang Hyun Lee , Kuhwan Jeong , Junhwi Choi , Hoshik Lee , and Young Sang Choi .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Graph - aware transformer : Is attention all graphs need ?", "entities": []}, {"text": "arXiv preprint arXiv:2006.05213 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jiaxuan You , Rex Ying , Xiang Ren , William L. Hamilton , and Jure Leskovec .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graphrnn : Generat-", "entities": []}, {"text": "5215ing realistic graphs with deep auto - regressive models .", "entities": []}, {"text": "In Proceedings of the 35th International Conference on Machine Learning , ICML 2018 , Stockholmsm \u00a8assan , Stockholm , Sweden , July 10 - 15 , 2018 , volume 80 of Proceedings of Machine Learning Research , pages 5694\u20135703 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Quan Yuan , Xiang Ren , Wenqi He , Chao Zhang , Xinhe Geng , Lifu Huang , Heng Ji , Chin - Yew Lin , and Jiawei Han . 2018 .", "entities": []}, {"text": "Open - schema event pro\ufb01ling for massive news corpora .", "entities": []}, {"text": "In Proceedings of the 27th ACM International Conference on Information and Knowledge Management , CIKM 2018 , Torino , Italy , October 22 - 26 , 2018 , pages 587\u2013596 .", "entities": [[5, 6, "DatasetName", "ACM"], [12, 13, "TaskName", "Management"]]}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Li Zhang , Qing Lyu , and Chris Callison - Burch . 2020 .", "entities": []}, {"text": "Reasoning about goals , steps , and temporal ordering with WikiHow .", "entities": [[10, 11, "DatasetName", "WikiHow"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 4630\u20134639 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}]