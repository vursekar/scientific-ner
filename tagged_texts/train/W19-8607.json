[{"text": "Proceedings of The 12th International Conference on Natural Language Generation , pages 54\u201364 , Tokyo , Japan , 28 Oct - 1 Nov , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics54Computational Argumentation Synthesis as a Language Modeling Task Roxanne El Baff1Henning Wachsmuth2Khalid Al - Khatib1 Manfred Stede3Benno Stein1 1Bauhaus - Universit\u00e4t Weimar , Weimar , Germany , < first > { .<last > } + @uni - weimar.de 2Paderborn University , Paderborn , Germany , henningw@upb.de 3University of Potsdam , Potsdam , Germany , stede@uni-potsdam.de Abstract Synthesis approaches in computational argumentation so far are restricted to generating claim - like argument units or short summaries of debates .", "entities": []}, {"text": "Ultimately , however , we expect computers to generate whole new arguments for a given stance towards some topic , backing up claims following argumentative and rhetorical considerations .", "entities": []}, {"text": "In this paper , we approach such an argumentation synthesis as a language modeling task .", "entities": []}, {"text": "In our language model , argumentative discourse units are the \u201c words \u201d , and arguments represent the \u201c sentences \u201d .", "entities": []}, {"text": "Given a pool of units for any unseen topic - stance pair , the model selects a set of unit types according to a basic rhetorical strategy ( logos vs. pathos ) , arranges the structure of the types based on the units \u2019 argumentative roles , and \ufb01nally \u201c phrases \u201d an argument by instantiating the structure with semantically coherent units from the pool .", "entities": []}, {"text": "Our evaluation suggests that the model can , to some extent , mimic the human synthesis of strategy - speci\ufb01c arguments .", "entities": []}, {"text": "1 Introduction Existing research on computational argumentation largely focuses on the analysis side .", "entities": []}, {"text": "Various analysis tasks are widely studied including identifying the claims along with their supporting premises ( Stab and Gurevych , 2014 ) , \ufb01nding the relation between argumentative units ( Cocarascu and Toni , 2017 ) , and assessing the persuasiveness of arguments ( Habernal and Gurevych , 2016 ) .", "entities": []}, {"text": "Diverse downstream applications , however , necessitate the development of argumentation synthesistechnologies .", "entities": []}, {"text": "For example , synthesis is needed to produce a summary of arguments for a given topic ( Wang and Ling , 2016 ) or to build a debating system where new arguments are exchanged between the users and the system ( Le et al . , 2018 ) .", "entities": []}, {"text": "As a result , a number of recent studies addresses the argumentation synthesis task .", "entities": []}, {"text": "These studieshave proposed different approaches to generating claims or reasons for a given topic , partly with a particular stance towards the topic ( Bilu and Slonim , 2016 ; Hua and Wang , 2018 ) .", "entities": []}, {"text": "However , the next important synthesis step is still missing in the literature , namely , to generate complete texts including both argumentative and rhetorical considerations .", "entities": []}, {"text": "With the latter , we refer to Aristotle \u2019s three means of persuasion : logos ( providing logical arguments ) , ethos ( demonstrating credibility ) , and pathos ( evoking emotions ) .", "entities": []}, {"text": "As discussed by Wachsmuth et al .", "entities": []}, {"text": "( 2018 ) , following a rhetorical strategy is key to achieving persuasion with argumentative texts .", "entities": []}, {"text": "This paper proposes a new computational approach that synthesizes argumentative texts following a rhetorical strategy .", "entities": []}, {"text": "We do not tackle this task immediately \u201c in the wild \u201d , i.e. , generating an entirely new argumentative text for a freely - chosen topic and a possibly complex strategy .", "entities": []}, {"text": "Rather , we consider a \u201c controlled \u201d synthesis setting , with the goal of successively creating models that are able to deal with more complex settings later on .", "entities": []}, {"text": "In particular , given a pool of argumentative discourse units ( ADUs ) , our approach generates arguments for any unseen pair of topic and stance ( e.g. , \u201c con abortion \u201d ) as well as a basic rhetorical strategy ( i.e. , logos - oriented vs. pathos - oriented).1To abstract from the arguments \u2019 topics during training , we \ufb01rst identify different ADU types using clustering .", "entities": []}, {"text": "Our approach then learns to select unit types matching the given strategy and to arrange them according to their argumentative roles .", "entities": []}, {"text": "Both steps are realized as a language model where ADUs represent words and arguments are sentences .", "entities": []}, {"text": "Finally , our approach \u201c phrases \u201d an argument by predicting the best set of semantically related ADUs for the arranged structure using supervised regression .", "entities": []}, {"text": "Thereby , we ensure that the synthesized texts are 1We consider a single argument to be a sequence of ADUs where each ADU has a speci\ufb01c role : thesis , con , or pro .", "entities": []}, {"text": "55composed of meaningful units , a property that neural generation methods barely achieve so far .", "entities": []}, {"text": "In our evaluation , we utilize the dataset of Wachsmuth et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "This dataset contains 260 argumentative texts on 10 topic - stance pairs , where each text composes \ufb01ve ADUs in a logos - oriented or pathos - oriented manner .", "entities": []}, {"text": "In our experiments , we train our approach on nine topic - stance pairs and then generate an argument for the tenth .", "entities": []}, {"text": "The results demonstrate that our approach successfully manages to combine pairs of ADUs , but its performance on longer sequences of ADUs is limited .", "entities": []}, {"text": "Altogether , our contribution is three - fold : 1.A new view of argumentation synthesis that represents argumentative and rhetorical considerations with language modeling .", "entities": []}, {"text": "2.A novel approach that selects , arranges , and phrases ADUs to synthesize strategy - speci\ufb01c arguments for any topic and stance .", "entities": []}, {"text": "3.First experimental evidence that arguments with basic rhetorical strategies can be synthesized computationally.2 2 Related Work Recently , some researchers have tackled argumentation synthesis statistically with neural networks .", "entities": []}, {"text": "For instance , Wang and Ling ( 2016 ) employed a sequence - to - sequence model to generate summaries of argumentative texts , and Hua and Wang ( 2018 ) did similar to generate counterarguments .", "entities": []}, {"text": "Using neural methods in text generation , it is possible to achieve output that is on topic and grammatically ( more or less ) correct .", "entities": [[4, 6, "TaskName", "text generation"]]}, {"text": "However , when the desired text is to span multiple sentences , the generated text regularly suffers from incoherence and repetitiveness , as for instance discussed by Holtzman et al .", "entities": []}, {"text": "( 2018 ) who examine texts that were produced by RNNs in various domains .", "entities": []}, {"text": "While these problems may be tolerable to some extent in some applications , such as chatbots , bad text can not be accepted in an argumentative or debating scenario , where the goal is to convince or persuade a reader ( rather than to merely inform or entertain ) .", "entities": []}, {"text": "Holtzman et al .", "entities": []}, {"text": "( 2018 ) propose to alleviate incoherence and repetitiveness by training a set of discriminators , which aim to ensure that a text respects the Gricean maxims of quantity , quality , relation , and manner ( Grice , 1975 ) .", "entities": []}, {"text": "To this end , they 2The code for running the experiments is available here : https://github.com/webis-de/ inlg19 - argumentation - synthesisemploy speci\ufb01c datasets , such as one that opposes authentic text continuation to randomly - sampled text .", "entities": []}, {"text": "The discriminators learn optimal weightings for the various models and their combination , such that overall text quality is maximized .", "entities": []}, {"text": "For argumentation , we hypothesize that one needs to go even further and eventually account for the author , implementing her underlying intention in the different parts of an argumentative text as well as in the relations between the parts .", "entities": []}, {"text": "In the past times of rule - based text generation , argumentation synthesis was a popular task ( Zukerman et al . , 2000 ) .", "entities": [[8, 10, "TaskName", "text generation"]]}, {"text": "Approaches involved much handcrafted ( linguistic and domain ) knowledge and user modeling .", "entities": []}, {"text": "For example , the system of Carenini and Moore ( 2006 ) compares attributes of houses ( from a database ) to desired target attributes ( from a user model ) , to then recommend a house to the reader in a convincing text following the Gricean maxims .", "entities": []}, {"text": "To this end , it selected house attributes potentially interesting to the user , arranged , and \ufb01nally phrased them .", "entities": []}, {"text": "The resulting texts resembled the arguments we work with here , which have been manually composed by experts ( Wachsmuth et al . , 2018 ) from the claims , evidence , and objections in the arg - microtext corpus ( Peldszus and Stede , 2016 ) .", "entities": []}, {"text": "To achieve a similar level of output control , today \u2019s text - to - text generation models need to account for the various interdependencies between the text units to be combined .", "entities": [[15, 17, "TaskName", "text generation"]]}, {"text": "Most related to our approach is the system of Sato et al .", "entities": []}, {"text": "( 2015 ) , where a user can enter a claimlike topic along with a stance .", "entities": []}, {"text": "The system then generates argumentative paragraphs on speci\ufb01c aspects of the topic by selecting sentences from 10 million news texts of the Gigaword corpus .", "entities": []}, {"text": "Potentially relevant aspects are those that trigger evaluative judgment in the reader .", "entities": []}, {"text": "The sentences are arranged so that the text starts with a claim sentence and is followed by support sentences , employing the approach of Yanase et al . ( 2015 ) .", "entities": []}, {"text": "The support sentences are ordered by maximizing the semantic connectivity between sentences .", "entities": []}, {"text": "Finally , some rephrasing is done in terms of certain aspects of surface realization .", "entities": []}, {"text": "In a manual evaluation , however , no text was seen as sounding natural , underlining the dif\ufb01culty of the task .", "entities": []}, {"text": "In contrast to Sato et al .", "entities": []}, {"text": "( 2015 ) , we learn directly from input data what argumentative discourse units to combine and how to arrange them .", "entities": []}, {"text": "We leave surface realization aside to keep the focus on the argument composition .", "entities": []}, {"text": "56Role ID Argumentative Discourse Unit Thesis t 1German universities should on no account charge tuition fees t2the universities in Germany should not under any circumstances charge tuition fees t3tuition fees should not generally be charged by universities t4universities should not charge tuition fees in Germany Con c 1one could argue that an increase in tuition fees would allow institutions to be better equipped c2those who study later decide this early on , anyway c3to", "entities": []}, {"text": "oblige non", "entities": []}, {"text": "- academics to \ufb01nance others \u2019 degrees through taxes is not just c4unfortunately sponsoring can lead to disagreeable dependencies in some cases Pro p 1education and training are fundamental rights which the state , the society must provide p2education must not be a question of money in a wealthy society such as Germany p3fees result in longer durations of studies p4funding - wise it ought to be considered how costs incurred by students from other ( federal ) states can be reimbursed p5if a university lacks the funds , sponsors must be found p6longer durations of studies are costly p7studying and taking higher degrees must remain a basic right for everyone p8there are other instruments to motivate tighter discipline while studying p9this would impede or prevent access to those who are \ufb01nancially weaker p10this would mean that only those people with wealthy parents or a previous education and a part - time job while studying would be able to apply for a degree programme in the \ufb01rst place p11universities are for all citizens , independent of their \ufb01nances p12what is the good of a wonderfully out\ufb01tted university if it does n\u2019t actually allow the majority of clever people to broaden their horizons with all that great equipment Topic Should all universities in Germany charge tuition fees ?", "entities": []}, {"text": "Stance Con Table 1 : The candidate thesis , con , and pro units for one topic - stance pair in the dataset of Wachsmuth et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "Some other approaches have been proposed that recompose existing text segments in new arguments .", "entities": []}, {"text": "In particular , Bilu and Slonim ( 2016 ) generated new claims by \u201c recycling \u201d topics and predicates that were found in a database of claims .", "entities": []}, {"text": "Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic .", "entities": []}, {"text": "Egan et al .", "entities": []}, {"text": "( 2016 ) created summaries of the main points in a debate , and", "entities": []}, {"text": "Reisert et al .", "entities": []}, {"text": "( 2015 ) synthesized complete arguments from a set of manually curated topic - stance relations based on the \ufb01ne - grained argument model of Toulmin ( 1958 ) .", "entities": []}, {"text": "However , we are not aware of any approach that synthesizes arguments fully automatically , let alone that follows rhetorical considerations in the synthesis process .", "entities": []}, {"text": "3 Data To develop our model for argumentation synthesis , we exploit the dataset recently developed by Wachsmuth et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "The dataset comprises 260manually generated argumentative texts .", "entities": []}, {"text": "The generation of each text , for one topic - stance pair , has been conducted in a systematic fashion following the three canons of rhetoric ( Aristotle , 2007 ): 1.Inventio\u0018Selecting a subset of argumentative discourse units ( ADUs ) from a pool of given ADUs for a topic - stance pair.2.Dispositio\u0018Arranging the selected ADUs in a sequential order .", "entities": []}, {"text": "3.Elocutio\u0018Phrasing the arranged ADUs by adding connectives at unit - initial or unit-\ufb01nal positions .", "entities": []}, {"text": "Speci\ufb01cally , Wachsmuth et al .", "entities": []}, {"text": "( 2018 ) selected a pool of 200 ADUs for 10 pairs of controversial topic and stance from the English version of the arg - microtexts corpus ( Peldszus and Stede , 2016 ) .", "entities": []}, {"text": "As a preprocessing step , they \u201c decontextualized \u201d these ADUs manually by removing connectives , resolving pronouns , and similar .", "entities": []}, {"text": "Each topic - stance pair comes with 20 such ADUs : four theses , four con units , and 12 pro units .", "entities": []}, {"text": "Table 1 shows the ADU list for one topic - stance pair .", "entities": []}, {"text": "26 participants were asked by Wachsmuth et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) to create short argumentative texts for each topic - stance pair following one of two basic rhetorical strategies : ( 1 ) logos - oriented , i.e. , arguing logically , and ( 2 ) pathos - oriented , i.e. , arguing based on emotional appeals .", "entities": []}, {"text": "For each topic - stance pair they created an argument by selecting one thesis , one con and three pro units that they thought could best form a persuasive argument following the given strategies .", "entities": []}, {"text": "Table 2 shows two samples of generated arguments in the dataset .", "entities": []}, {"text": "The dataset contains 130 logos - oriented and 130", "entities": []}, {"text": "57Strategy ID Text Manually Synthesized From Five Argumentative Discourse Units Logos - oriented c 1one could argue that an increase in tuition fees would allow institutions to be better equipped , t1however German universities should on no account charge tuition fees .", "entities": []}, {"text": "p1education and training are fundamental rights which the state , the society must provide , p12because what is the good of a wonderfully out\ufb01tted university if it does n\u2019t actually allow the majority of clever people to broaden their horizons with all that great equipment .", "entities": []}, {"text": "p4Besides , funding - wise it ought to be considered how costs incurred by students from other ( federal ) states can be reimbursed .", "entities": []}, {"text": "Pathos - oriented p 1education and training are fundamental rights which the state , the society must provide .", "entities": []}, {"text": "t2This is why the universities in Germany should not under any circumstances charge tuition fees .", "entities": []}, {"text": "c1one could argue that an increase in tuition fees would allow institutions to be better equipped , p3however fees result in longer durations of studies p6andlonger durations of studies are costly .", "entities": []}, {"text": "Table 2 : two sample arguments manually synthesized from the ADUs in Table 1 , which are included in the dataset of Wachsmuth et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "The italiced connectives were added by the participants ; they are notpart of the ADUs .", "entities": []}, {"text": "pathos - oriented argumentative texts .", "entities": []}, {"text": "We use these 260 texts to develop and evaluate our computational model for argumentation synthesis .", "entities": []}, {"text": "4 Approach This section presents our computational approach to synthesize arguments for any pair of topic and stance , following one of two basic rhetorical strategies : arguing logically ( logos - oriented ) or arguing emotionally ( pathos - oriented ) .", "entities": []}, {"text": "A black - box view of the approach is shown in Figure 1 .", "entities": []}, {"text": "As input , our approach takes a strategy as well as a pool of argumentative discourse units ( ADUs ) for any speci\ufb01c topic - stance pair x.", "entities": []}, {"text": "Each ADU has the role of a thesis ( in terms of claim with a stance on the topic ) , a conpoint ( objecting the thesis ) , or apropoint ( supporting the thesis ) .", "entities": []}, {"text": "The approach then imitates the human selection , arrangement , and \u201c phrasing \u201d of a sequence of nADUs , in order to synthesize an argument .", "entities": []}, {"text": "Phrasing is done only in terms of picking semantically coherent ADUs for the arranged sequence ; the addition of connectives between ADUs is left to future work .", "entities": []}, {"text": "Below , we detail how we realize each step ( selection , arrangement , and phrasing ) with a topicindependent model .", "entities": []}, {"text": "For each step , we explain how it is trained ( illustrated in Figure 2 ) and how it is applied to an unseen topic - stance pair ( Figure 3 ) .", "entities": []}, {"text": "4.1 Selection Language Model", "entities": []}, {"text": "This model handles the selection of a set of nADUs for a topic - stance pair xand a rhetorical strategy .", "entities": []}, {"text": "We approach the selection as a language modeling task where each ADU is a \u201c word \u201d of our language model and each argument a \u201c sentence \u201d .", "entities": []}, {"text": "To abstract from topic , the model actually selects ADU types , as explained in the following .", "entities": []}, {"text": "Argumentation synthesis The x,3Pro x,4Pro x,2Pro x,5", "entities": []}, {"text": "Con x,1Input", "entities": []}, {"text": "The x,1 The x , t ...", "entities": []}, {"text": "Con x,1", "entities": []}, {"text": "Con x , cPro x,1 Pro x , pPool of ADUs ...... Rhetorical strategy Output Strategy - specific arg\u2019Logos or PathosFigure 1 : Black - box view of our argumentation synthesis approach .", "entities": []}, {"text": "The input is a rhetorical strategy as well as a pool of thesis , con , and pro ADUs for some topicstance pair x.", "entities": []}, {"text": "The approach outputs a strategy - speci\ufb01c sequence of nADUs as an argument for x(here , n=5 ) .", "entities": []}, {"text": "4.1.1 Training of the Model We start from a training set of ADUs for a set of mtopic - stance pairs .", "entities": []}, {"text": "To generalize the language model beyond the covered topics , each ADU is represented using features that aim to capture general emotion - related and logic - related characteristics , accounting for the two given strategies .", "entities": [[21, 22, "DatasetName", "emotion"]]}, {"text": "In particular , we \ufb01rst cluster the pool of all training ADUs based on their feature representation .", "entities": []}, {"text": "As a result , each ADU is represented by a cluster label ( A \u2013 Fin Figure 2 ) , where each label represents one ADU type .", "entities": []}, {"text": "Now , for each of the strategies , we map each manually - generated sequence of ADUs to a sequence of cluster labels .", "entities": []}, {"text": "Using these sequences of labels , we train one separated selection language model for each strategy .", "entities": []}, {"text": "For clustering , we rely on topic - independent features that we expect to implicitly encode logical and emotional strategies : ( 1 ) psychological meaningfulness ( Pennebaker et al . , 2015 ) , ( 2 ) eight basic emotions ( Plutchik , 1980 ; Mohammad and Turney , 2013 ) , and ( 3 ) argumentativeness ( Somasundaran et al . , 2007 ) .", "entities": []}, {"text": "In the following , we elaborate on the concrete features that we extract :", "entities": []}, {"text": "58 \u2192 1 . Selection Language Model\u2192 2 .", "entities": []}, {"text": "Arrangement Language Model\u2192 3 .", "entities": []}, {"text": "Phrasing Regression Model 1 - grams P(A )", "entities": []}, {"text": "= 0.30 2 - gramsP(F ) = 0.25 ... P(A | A ) = 0.00 P(F | A )", "entities": []}, {"text": "= 0.13 ... P(A | F ) = 0.13 P(F | F ) = 0.06 ... \u2026 ...... 1 - grams 2 - grams \u2026 P (    ) = 0.20 P (    )", "entities": []}, {"text": "= 0.60 ...", "entities": []}, {"text": "P (    |    )", "entities": []}, {"text": "= 0.00 P (    |    )", "entities": []}, {"text": "= 0.06 ...", "entities": []}, {"text": "P (    |    )", "entities": []}, {"text": "= 0.19 P (    |    )", "entities": []}, {"text": "= 0.38 ...", "entities": []}, {"text": "...... 1 - grams 2 - grams \u2026 \u21920.12", "entities": []}, {"text": "Pro n,1\u21920.02 ...", "entities": []}, {"text": "Pro 1,2 The 1,2The 1,2 \u21920.04", "entities": []}, {"text": "......", "entities": []}, {"text": "Pro n,1 Con n,8 \u21920.07The 1,2Pro 1,2Pro 1,6Pro 1,7Con 1,2", "entities": []}, {"text": "The 1,3 Pro 1,1Pro 1,6Pro 1,2 Con 1,2 ...", "entities": []}, {"text": "The n,4 Pro n,2 Pro n,1Pro n,3 Con n,2 Con n,2 Pro n,8Pro n,1", "entities": []}, {"text": "The n,2Pro n,7 ... Topic+stance 1 Topic+stance m \u2026 Argument corpus Argumentm,1 Argumentm , jArgument1,1 Argument1,iADU clustering B ACD E F ...... Figure 2 : Illustration of training the three models of our argumentation synthesis approach .", "entities": []}, {"text": "The input is a corpus of argumentative texts for mtopic - stance pairs , each decomposed into a sequence of theses , con units , and pro units .", "entities": []}, {"text": "Initially , the set of all these ADUs is clustered to obtain a set topic - independent ADU types , called A \u2013 Fhere .", "entities": []}, {"text": "( 1)Selection language model : Each argument is converted from a sequence of ADUs to a sequence of ADU types , where a language model is trained on these type sequences .", "entities": []}, {"text": "( 2 ) Arrangement language model : Each argument is converted from a sequence of ADUs to a sequence of ADU roles ( thesis , pro , and con ) where a language model is trained on these ADU role sequences .", "entities": []}, {"text": "( 3 ) Phrasing regression model : A linear regression model is trained which scores each ADU sequence with respect to its semantic coherence .", "entities": [[8, 10, "MethodName", "linear regression"]]}, {"text": "Linguistic Inquiry and Word Count ( LIWC ) LIWC is a lexicon - based text analysis that counts words in psychologically meaningful categories ( Tausczik and Pennebaker , 2010 ) .", "entities": []}, {"text": "We use the version by Pennebaker et al . ( 2015 ) , which contains the following 15 dimensions : 1.Language metrics , e.g. , words per sentence .", "entities": []}, {"text": "2.Function words , e.g. , pronouns and auxiliary verbs .", "entities": []}, {"text": "3.Other grammar , e.g. , common verbs and comparisons .", "entities": []}, {"text": "4.Affect words , e.g. , positive emotion words .", "entities": [[6, 7, "DatasetName", "emotion"]]}, {"text": "5.Social words , e.g. , \u201c family \u201d and \u201c friends \u201d .", "entities": []}, {"text": "6.Cognitive processes , e.g. , \u201c discrepancies \u201d and \u201c certainty \u201d .", "entities": []}, {"text": "7.Perceptual processes , e.g. , \u201c feeling \u201d .", "entities": []}, {"text": "8.Biological processes , e.g. , \u201c health \u201d .", "entities": []}, {"text": "9.Core drives and needs , e.g. , \u201c power \u201d and \u201c reward focused \u201d .", "entities": []}, {"text": "10.Time orientation , e.g. , past - focused .", "entities": []}, {"text": "11.Relativity , e.g. , \u201c time \u201d and \u201c space \u201d .", "entities": []}, {"text": "12.Personal concerns , e.g. , \u201c work \u201d and \u201c leisure \u201d .", "entities": []}, {"text": "13.Informal speech , e.g. , \ufb01llers and non\ufb02uencies .", "entities": []}, {"text": "14.Punctuation , e.g. , periods and commas .", "entities": []}, {"text": "15.Summary variables , as detailed below .", "entities": []}, {"text": "There are four summary variables , each of which is derived from various LIWC dimensions : ( 1 ) analytical thinking ( Pennebaker et al . , 2014 ) , i.e. , the degree to which people use narrative language ( low value ) , or more logical and formal language ( high ) ; ( 2)clout ( Kacewicz et al . , 2014 ) , i.e. , the relative social status , con\ufb01dence , and leadership displayed in a text ; ( 3 ) authenticity ( Newman et al . , 2003 ) , i.e. , the degree to which people reveal themselves in an authentic way ; and ( 4 ) emotional tone ( Cohn et al . , 2004 ) , i.e. , negative for values lower than 50 and positive otherwise .", "entities": []}, {"text": "NRC Emotional and Sentiment Lexicons We use the NRC lexicon of Mohammad and Turney ( 2013 ) .", "entities": []}, {"text": "The lexicon has been compiled manually using crowdsourcing and contains a set of English words and their associations with ( 1 ) sentiment , i.e. , negative and positive polarities , and ( 2 ) emotions , i.e. , the eight basic emotions de\ufb01ned by Plutchik ( 1980 ): anger , anticipation , disgust , fear , joy , surprise , sadness , and trust .", "entities": []}, {"text": "These features are represented as the count of words associated with each category ( e.g. , the count of sad words in an ADU ) .", "entities": []}, {"text": "MPQA Arguing Lexicon Somasundaran et al .", "entities": [[0, 1, "DatasetName", "MPQA"]]}, {"text": "( 2007 ) constructed a lexicon that includes the following arguing patterns : assessments , doubt , authority , emphasis , necessity , causation , generalization , structure , conditionals , inconsistency , possibility , wants , contrast , priority , dif\ufb01culty , inyour-", "entities": []}, {"text": "59", "entities": []}, {"text": "The x,1 The x,2", "entities": []}, {"text": "The x,3 The x , t ... predict A A C ECon x,1", "entities": []}, {"text": "Con x,2", "entities": []}, {"text": "Con x,3", "entities": []}, {"text": "Con x , c ... predict D B ACPro x,1", "entities": []}, {"text": "Pro x,2", "entities": []}, {"text": "Pro x,3", "entities": []}, {"text": "Pro x , p ... predict B C CDRhetorical strategy + ADUs of topic - stance pair x Input 1 .", "entities": []}, {"text": "Selection Language Modelmore probableABCDC ABCDC", "entities": []}, {"text": "ABCDCThe x,3 Pro x,1 Pro x , p Pro x,3 Con x,3", "entities": []}, {"text": "The x,1 Pro x,2 Pro x,3 Pro x , p Con x,2", "entities": []}, {"text": "The x,2 Pro x,2 Pro x,3 Pro x , p Con x,2", "entities": []}, {"text": "2 . Arrangement Language Modelgenerate Candidate argumentsmore probablegenerate ... ...", "entities": []}, {"text": "The x,1 Pro x,2 Pro x,3 Pro x , p Con x,2", "entities": []}, {"text": "The x,2 Pro x,2 Pro x,3 Pro x , p Con x,2 3 .", "entities": []}, {"text": "Phrasing Regression ModelFiltered candidateshigher scorepredict", "entities": []}, {"text": "The x,1 Pro x,2 Pro x,3 Pro x , p Con x,2The", "entities": []}, {"text": "x,2 Pro x,2 Pro x,3 Pro x , p Con x,2", "entities": []}, {"text": "The x,2 Pro x,2 Pro x,3 Pro x , p Con x,2OutputLogos or PathosFigure 3 : Illustration of applying our synthesis approach .", "entities": []}, {"text": "Given the predicted type of each input ADU of the given topic - stance pair x , ( 1 ) the selection generates the most probable type sequence , ( A ; B ; C ; D ; C ) .", "entities": []}, {"text": "From the type sequence , a set of candidate arguments is decoded .", "entities": []}, {"text": "( 2 ) The arrangement \ufb01lters out candidates not matching the most probable ADU role sequence , ( Thesis ; Con ; Pro ; Pro ; Pro ) .", "entities": []}, {"text": "( 3 ) Phrasing scores each remaining argument and outputs the top argument .", "entities": []}, {"text": "shoes , rhetorical question .", "entities": []}, {"text": "We use the count of each arguing pattern in text as one feature ( e.g. , number of assessments patterns in an ADU ) .", "entities": []}, {"text": "4.1.2 Application of the Model As shown in Figure 3 , the selection language model takes the ADUs of an unseen topic - stance xas input .", "entities": []}, {"text": "It then outputs a set of candidate arguments , in terms of sequences of ADUs .", "entities": []}, {"text": "Each ADU is encoded into a cluster label ( representing an ADU type ) .", "entities": []}, {"text": "For example , one might have the following mappings , given the six labels A \u2013 Ffrom Figure 2 : A fThex;1 ; The x;2 ; Con x;3 g B fCon x;2 ; Pro x;1gC fThex;3 ; Con x;c ; Pro x;2 ; Pro x;3 g D fProx;p ; Con x;1 g E fThex;tg F fThex;4 ; Con x;4 ; Pro x;4 g", "entities": []}, {"text": "The language model for either of the two rhetorical strategies generates a set of arguments where each argument is composed of ncluster labels , e.g. , ( A ; B ; C ; D ; C ) forn= 5 in Figure 3 .", "entities": []}, {"text": "This set is ranked by probability of the associated sequence .", "entities": []}, {"text": "For example , assume that ( A ; B ; C ; D ; C ) is most probable .", "entities": []}, {"text": "Then we decode all possible ADU sequences for topic - stance xfrom ( A ; B ; C ; D ; C ) to a set of candidate arguments : ( A ; B ; C ; D ; C ) !", "entities": []}, {"text": "fThex;1 ; The x;2 ; Con x;3 g \u0002fCon x;2 ; Pro x;1 g \u0002fThex;3 ; Con x;c ; Pro x;2 ; Pro x;3 g \u0002fProx;p ; Con x;1 g \u0002fThex;3 ; Con x;c ; Pro x;2 ; Pro x;3 g", "entities": []}, {"text": "The output of the model is a set of candidate arguments , which becomes the input of the arrangement language model .", "entities": []}, {"text": "4.2 Arrangement Language Model", "entities": []}, {"text": "In the arrangement process , we aim to imitate the human behavior of arranging ADUs for a speci\ufb01c topic - stance following a rhetorical strategy ( here , logos orpathos ) .", "entities": []}, {"text": "Again , we approach this problem as a language modeling task .", "entities": []}, {"text": "Each ADU role ( thesis , pro , or con ) is a word of the language model and each argument a sentence .", "entities": []}, {"text": "4.2.1 Training of the Model As sketched in Figure 2 , we \ufb01rst convert the humangenerated arguments from a sequence of ADUs to a sequence of ADU roles .", "entities": []}, {"text": "Then , we use these sequences to train a language model for each strategy .", "entities": []}, {"text": "4.2.2 Application of the Model As shown in Figure 3 , the arrangement language model takes as input the candidate arguments that we get from the selection language model and outputs a set of \ufb01ltered candidate arguments .", "entities": []}, {"text": "The language model for a speci\ufb01c strategy generates a set of argument structures where each such structure is a sequence of nADU roles , e.g. , ( Thesis ; Con ; Pro ; Pro ; Pro ) forn= 5 in Figure 3 .", "entities": []}, {"text": "This set is ranked by the probability of the sequences .", "entities": []}, {"text": "For example , assume that the most frequent sequence is ( Thesis ; Con ; Pro ; Pro ; Pro ) .", "entities": []}, {"text": "60Using the output from the selection language model , we \ufb01lter out all candidate arguments that do not match ( Thesis ; Con ; Pro ; Pro ; Pro ) , ending up with the following \ufb01ltered arguments : fThex;1 ; The x;2g\u0002fCon x;2 g \u0002fProx;2 ; Pro x;3g\u0002fProx;pg \u0002fProx;2 ; Pro x;3 g", "entities": []}, {"text": "The output of the model is a \ufb01ltered set of candidate arguments , which becomes the input of the phrasing regression model .", "entities": []}, {"text": "4.3 Phrasing Regression Model", "entities": []}, {"text": "The set of arguments resulting from the selection and arrangement language models are based on topic - independent features .", "entities": []}, {"text": "The missing step is to entail the topical relationship between the ADUs in each generated argument .", "entities": []}, {"text": "We approach this task with supervised regression .", "entities": []}, {"text": "As indicated above , our model does not really phrase an argument .", "entities": []}, {"text": "Rather , it aims to choose the best among the given set of candidates in terms of semantic coherence .", "entities": []}, {"text": "4.3.1 Training of the Model For each argument , we opt for a feature representation that embeds the content properties of ADUs in order to capture their content relationship .", "entities": []}, {"text": "Concretely , we represent each argument by calculating the semantic similarities of each adjacent bigram in a human - generated argument .", "entities": []}, {"text": "We train a linear regression model where each instance represents the features of one argument .", "entities": [[3, 5, "MethodName", "linear regression"]]}, {"text": "To this end , we set a score to be the sum of the probabilities of ADU bigrams occurring in one argument .", "entities": []}, {"text": "The phrasing model scores each of the \ufb01ltered arguments given as output by the arrangement model .", "entities": []}, {"text": "The argument with the highest score is the \ufb01nal generated argument .", "entities": []}, {"text": "4.3.2 Application of the Model At this point , the phrasing model is provided by the \ufb01ltered arguments from the arrangement model .", "entities": []}, {"text": "For each \ufb01ltered argument , we extract the bigram features ( semantic similarities ) .", "entities": []}, {"text": "Next , using the phrasing model , we predict the score of each sequence .", "entities": []}, {"text": "The sequence with the highest score is the generated argument .", "entities": []}, {"text": "In Figure 3 , this is : ( Thex;2 ; Con x;2 ; Pro x;2 ; Pro x;p ; Pro x;3 ) 5 Experiments In this section , we report the results of evaluating the introduced approach to argumentation synthesisStrategy 2 - grams 3 - grams Logos - oriented 9,110.6 9,466.3 Pathos - oriented 7,939.5 10,279.6 Table 3 : Selection .", "entities": []}, {"text": "Perplexity of the 2 - gram and 3gram language models for each strategy , averaged over 10 leave - one - topic - out runs using Laplace smoothing .", "entities": [[0, 1, "MetricName", "Perplexity"]]}, {"text": "based on the dataset described in Section 3 . 5.1 Experimental Set - up Our experiments are designed in leave - one - topicout cross - validation setting : From the 10 topicstance pairs in the dataset , we use nine for training and the last as the test fold , and we repeat this once for each possible fold .", "entities": []}, {"text": "This way , no topic - speci\ufb01c knowledge can be used in the synthesis process .", "entities": []}, {"text": "For each given basic rhetorical strategy ( logosoriented andpathos - oriented ) , we train one model each for the selection , the arrangement , and the \u201c phrasing \u201d of argumentative discourse units ( ADUs ) on the nine training folds .", "entities": []}, {"text": "The arguments synthesized by their combination are then evaluated against the human - generated arguments in the test folds .", "entities": []}, {"text": "The evaluation covers all three models as well as the \ufb01nal generated argument for each strategy .", "entities": []}, {"text": "We report the average accuracy across all ten folds for each of the models .", "entities": [[3, 5, "MetricName", "average accuracy"]]}, {"text": "5.2 Training : Selection Language Model", "entities": []}, {"text": "In each training / test experiment for one of the two strategies , we \ufb01rst abstract all ADUs across all strategy - speci\ufb01c topic - stance pairs by extracting the LIWC , NRC , and MPQA features , as described in Section 4.1 .", "entities": [[35, 36, "DatasetName", "MPQA"]]}, {"text": "Then , we cluster the given training set using standard k - means ( Ostrovsky et al . , 2012 ) .", "entities": []}, {"text": "After some initial experiments , we decide to set k to 6 , because this best balanced the distribution of arguments over clusters , and showed clear strategyspeci\ufb01c differences.3Using the resulting clustering model , we predicted the type A \u2013 Fof each ADU in the test set ( the tenth topic ) .", "entities": []}, {"text": "Given the ADU types , we next converted the human - generated training and test arguments from a sequence of ADUs to a sequence of ADU types .", "entities": []}, {"text": "After that , we trained one 2 - gram and one 3 - gram selection language.4In Table 3 , we report the mean perplexity of the models for both strategies .", "entities": [[23, 24, "MetricName", "perplexity"]]}, {"text": "3A more thorough evaluation of kis left to future work .", "entities": []}, {"text": "4We did not consider 1 - grams , because arguments are inherently relational , hence requiring at least two ADUs .", "entities": []}, {"text": "61Strategy 2 - grams 3 - grams Logos 54.5 33.5 Pathos 45.9 23.9 Table 4 : Arrangement .", "entities": []}, {"text": "Perplexity of the 2 - gram and 3gram language models for each strategy , averaged over 10 leave - one - topic - out runs using Laplace smoothing .", "entities": [[0, 1, "MetricName", "Perplexity"]]}, {"text": "As shown , the 2 - gram perplexity is lower than the 3 - gram perplexity in both cases .", "entities": [[7, 8, "MetricName", "perplexity"], [15, 16, "MetricName", "perplexity"]]}, {"text": "We assume that the reason lies in the limited size of the dataset and the narrow setting : Only 117 sentences ( ADUs ) are given per strategy for training , with a vocabulary size of 6 ( number of ADU types ) .", "entities": []}, {"text": "Based on the results , we decided to use the 2 - gram selection language model to generate candidate arguments .", "entities": []}, {"text": "5.3 Training : Arrangement Language Model To train arrangement as described in Section 4.2 , we took all arguments of the nine training topics in each experiment .", "entities": []}, {"text": "We converted each argument from a sequence of ADUs to a sequence of ADU roles ( thesis , pro , and con ) .", "entities": []}, {"text": "After that , we trained a 2 - gram and 3 - gram language model for each strategy .", "entities": []}, {"text": "Table 4 lists the mean perplexity values over the 10 folds .", "entities": [[5, 6, "MetricName", "perplexity"]]}, {"text": "Here , the perplexity is lower for 3 - grams than for 2 - grams , which can be expected to yield better performance .", "entities": [[3, 4, "MetricName", "perplexity"]]}, {"text": "Therefore , we used the 3 - gram language model to \ufb01lter the set of candidate arguments .", "entities": []}, {"text": "5.4 Training : Phrasing Regression Model For phrasing ( in terms of choosing the best ADU sequence ) , we \ufb01rst extracted features from each candidate , as described in Section 4.3 .", "entities": []}, {"text": "Then , we calculated the semantic similarities between each pair of adjacent ADUs as follows : 1.We obtained a 300 - dimensional word embedding for each word in an ADU using the pre - trained GloVe common - crawl model ( Pennington et al . , 2014).5 2.We averaged the embeddings of all words in an ADU , resulted in one vector representing the ADU .", "entities": [[35, 36, "MethodName", "GloVe"]]}, {"text": "3.For each adjacent pair of ADUs , we computed the cosine similarity of their vectors .", "entities": []}, {"text": "Figure 4 shows a histogram of the distribution of the cosine similarities of each adjacent pair of 5The used model can be found here :", "entities": []}, {"text": "http://nlp . stanford.edu/data/glove.42B.300d.zip .", "entities": []}, {"text": "Pathos Logos 0.75 0.8 0.85 0.9 0.9524681012 Cosine similarityFigure 4 : Histogram of the cosine similarity of the average word embeddings of adjacent pairs of ADUs in logos - oriented and in pathos - oriented arguments .", "entities": [[19, 21, "TaskName", "word embeddings"]]}, {"text": "ADUs ( i.e. , each ADU 2 - gram ) in logos - oriented arguments and in pathos - oriented arguments .", "entities": []}, {"text": "We observe a generally high similarity between neighboring ADUs for both strategies , with logos - oriented 2 - grams being slightly more similar on average .", "entities": []}, {"text": "Given the ADU 2 - grams , we train a linear regression model that predicts the sum of ADU 2 - gram probabilities in each argument .", "entities": [[10, 12, "MethodName", "linear regression"]]}, {"text": "In case of the logos strategy , the model has a mean squared error ( MSE ) of 0.05 .", "entities": [[15, 16, "MetricName", "MSE"]]}, {"text": "In case of pathos the MSE is 0.03 .", "entities": [[5, 6, "MetricName", "MSE"]]}, {"text": "5.5 Results : Argumentation Synthesis Up to this point , we trained all selection , arrangement , and phrasing models 10 times .", "entities": []}, {"text": "Combining the three models for each strategy , we \ufb01nally generated one argument per strategy for the topic - stance pair left out in each experiments .", "entities": []}, {"text": "Hence , we ended up with 10 computationally synthesized arguments per strategy in total .", "entities": []}, {"text": "We evaluate each of these arguments by checking whether it matches any of the 13 humangenerated ground - truth arguments given per topicstance pair .", "entities": []}, {"text": "The matching is quanti\ufb01ed in terms of n - gram overlap with n = f1 ; : : : ; 5 g. For comparison , we consider a baseline that randomly generates arguments for each topic - stance pair as follows :", "entities": []}, {"text": "1 . Select a random thesis unit from t 1to t4 .", "entities": []}, {"text": "2 . Select a random con unit from c 1to c 4 . 3 .", "entities": []}, {"text": "Select three random pro units from p 1to p 12 .", "entities": []}, {"text": "4 .", "entities": []}, {"text": "Randomly arrange the selected units .", "entities": []}, {"text": "Table 5 presents the accuracy of n - gram overlaps between each of the 13 human - generated arguments per topic - stance pair and the arguments computationally synthesized arguments by our model", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "62Sequential Non - Sequential Strategy Approach 1 - gram 2 - gram 3 - gram 4 - gram 5 - gram 1 - gram 2 - gram 3 - gram 4 - gram 5 - gram Logos Our model 80.0 % 15.0 % 0.0 % 0.0 % 0.0 % 80.0 % 39.0 % 9.0 % 0.0 % 0.0 % Baseline 76.0 % 10.0 % 0.7 % 0.0 % 0.0 % 76.0 % 3.1 % 8.8 % 2.0 % 0.0 % Pathos Our model 88.0 % 20.0 % 0.0 % 0.0 % 0.0 % 88.0 % 48.0 % 17.0 % 4.0 % 0.0 % Baseline 82.0 % 11.5 % 0.7 % 0.0 % 0.0 % 82.0 % 38.9 % 10.7 % 1.6 % 0.0 % Table 5 : Accuracy of n - gram overlaps between the human - generated arguments for each strategy and the arguments computationally synthesized by our model and the baseline .", "entities": [[127, 128, "MetricName", "Accuracy"]]}, {"text": "In the sequential case , the ordering is considered , in thenon - sequential case , it is ignored .", "entities": []}, {"text": "The better result in each experiment is marked bold , if any .", "entities": []}, {"text": "Strategy ID Argument Computationally Synthesized from Five Argumentative Discourse Units Logos t 4universities should not charge tuition fees in Germany .", "entities": []}, {"text": "c3to", "entities": []}, {"text": "oblige non - academics to \ufb01nance others \u2019 degrees through taxes is not just .", "entities": []}, {"text": "p9this would impede or prevent access to those who are \ufb01nancially weaker .", "entities": []}, {"text": "p5if", "entities": []}, {"text": "a university lacks the funds , sponsors must be found .", "entities": []}, {"text": "p8there are other instruments to motivate tighter discipline while studying .", "entities": []}, {"text": "Pathos p 2education must not be a question of money in a wealthy society such as Germany .", "entities": []}, {"text": "c1one could argue that an increase in tuition fees would allow institutions to be better equipped .", "entities": []}, {"text": "p7studying and taking higher degrees must remain a basic right for everyone .", "entities": []}, {"text": "p6longer durations of studies are costly .", "entities": []}, {"text": "t2the universities in Germany should not under any circumstances charge tuition fees .", "entities": []}, {"text": "Table 6 : Comparison of two con arguments computationally synthesized with our model for the topic Should all universities in Germany charge tuition fees ?", "entities": []}, {"text": ", each being a sequence of \ufb01ve ADUs .", "entities": []}, {"text": "A logos - oriented argument ( t4 ; c3 ; p9 ; p5 ; p8)and a pathos - oriented argument ( p2 ; c1 ; p7 ; p6 ; t2 ) .", "entities": [[8, 9, "DatasetName", "c3"]]}, {"text": "The thesis of each argument is marked bold .", "entities": []}, {"text": "and by the baseline , with and without considering the ordering of ADUs .", "entities": []}, {"text": "Our models outperform the baseline for 1 - grams and 2 - grams in all cases .", "entities": []}, {"text": "For sequential 3 - grams , however , it did not achieve any overlap with the human - generated arguments for either strategy .", "entities": []}, {"text": "This may be explained by the fact that the employed selection and phrasing models are based on 2 - grams only .", "entities": []}, {"text": "For n\u00152 , the synthesis generally does not work well anymore .", "entities": []}, {"text": "We believe that the small data size is a main cause behind this , although it may also point to the limitation of composing ADUs based on surface features .", "entities": []}, {"text": "In the non - sequential case , though , our model performs comparably well for 3 - grams , and it even manages to correctly synthesize some ADU 4 - grams .", "entities": []}, {"text": "In Table 6 , we exemplify the top - scored arguments for one topic - stance pair , synthesized by our approach for logos and for pathos respectively .", "entities": []}, {"text": "They indicate that our model was able to learn strategy - speci\ufb01c differences.6In particular , the logos argument starts with the thesis ( t2 ) , as argumentation guidelines suggest .", "entities": []}, {"text": "It then reasons based on consequences and alternatives .", "entities": []}, {"text": "Matching intu6Notice that the coherence of the arguments may be optimized by inserting discourse markers , such as a \u201c but \u201d before p7 in the pathos argument .", "entities": []}, {"text": "As stated above , however , this is beyond the scope of the paper at hand.ition , the pathos argument appeals more to emotion , re\ufb02ected in phrases such as \u201c wealthy society \" and \u201c under any circumstances \u201d .", "entities": [[23, 24, "DatasetName", "emotion"]]}, {"text": "Particularly the thesis ( t4 ) has a more intense tonality than t2 , and putting it at the end creates additional emphasis .", "entities": []}, {"text": "6 Conclusion This paper has presented a topic - independent computational approach to imitate the process of selecting , arranging , and phrasing argumentative discourse units ( ADUs ) \u2014 so to speak , to synthesize arguments .", "entities": []}, {"text": "We have proposed to operationalize the necessary synthesis knowledge in the form of a combined language and regression model that predicts ADU sequences .", "entities": []}, {"text": "So far , we have evaluated our approach on a small dataset only that contains 260 argumentative texts following either of two rhetorical strategies .", "entities": []}, {"text": "For a controlled experiment setting based on this data , we have reported preliminary results of medium effectiveness regarding the imitation of human - generated arguments .", "entities": []}, {"text": "A big challenge for the future is to move from such a controlled setting to a real - world scenario , where arguments have to be formed for a freelychosen topic from material that is mined from the web .", "entities": []}, {"text": "Still , our topic - independent approach de\ufb01nes a \ufb01rst substantial step in this direction .", "entities": []}, {"text": "63References Aristotle . 2007 .", "entities": []}, {"text": "On Rhetoric : A Theory of Civic Discourse ( George A. Kennedy , Translator ) .", "entities": []}, {"text": "Clarendon Aristotle series .", "entities": []}, {"text": "Oxford University Press .", "entities": []}, {"text": "Yonatan Bilu and Noam Slonim .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Claim synthesis via predicate recycling .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 525\u2013530 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Giuseppe Carenini and Johanna D. Moore .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Generating and evaluating evaluative arguments .", "entities": []}, {"text": "Arti\ufb01cial Intelligence , 170(11):925\u2013952 .", "entities": []}, {"text": "Oana Cocarascu and Francesca Toni .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Identifying attack and support argumentative relations using deep learning .", "entities": []}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 1374\u20131379 . Association for Computational Linguistics .", "entities": []}, {"text": "Michael A Cohn , Matthias R Mehl , and James W Pennebaker .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Linguistic markers of psychological change surrounding September 11 , 2001 .", "entities": []}, {"text": "Psychological science , 15(10):687\u2013693 .", "entities": []}, {"text": "Charlie Egan , Advaith Siddharthan , and Adam Wyner . 2016 .", "entities": [[7, 8, "MethodName", "Adam"]]}, {"text": "Summarising the points made in online political debates .", "entities": []}, {"text": "In Proceedings of the Third Workshop on Argument Mining ( ArgMining2016 ) , pages 134\u2013143 , Berlin , Germany .", "entities": [[7, 9, "TaskName", "Argument Mining"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "H. Paul Grice .", "entities": []}, {"text": "1975 .", "entities": []}, {"text": "Logic and conversation .", "entities": []}, {"text": "In Peter Cole and Jerry L. Morgan , editors , Syntax and Semantics , Vol . 3 , pages 41\u201358 .", "entities": []}, {"text": "Academic Press , New York .", "entities": []}, {"text": "Ivan Habernal and Iryna Gurevych . 2016 .", "entities": []}, {"text": "Which argument is more convincing ?", "entities": []}, {"text": "Analyzing and predicting convincingness of web arguments using bidirectional LSTM .", "entities": [[8, 10, "MethodName", "bidirectional LSTM"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1589\u20131599 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ari Holtzman , Jan Buys , Maxwell Forbes , Antoine Bosselut , David Golub , and Yejin Choi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning to write with cooperative discriminators .", "entities": []}, {"text": "Technical Report 1805.06087 , arXiv .", "entities": [[4, 5, "DatasetName", "arXiv"]]}, {"text": "Xinyu Hua and Lu Wang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural argument generation augmented with externally retrieved evidence .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 219\u2013230 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ewa Kacewicz , James W Pennebaker , Matthew Davis , Moongee Jeon , and Arthur C Graesser .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Pronoun use re\ufb02ects standings in social hierarchies .", "entities": []}, {"text": "Journal of Language and Social Psychology , 33(2):125\u2013143.Dieu Thu Le , Cam - Tu Nguyen , and Kim Anh Nguyen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Dave the debater : a retrieval - based and generative argumentative dialogue agent .", "entities": [[12, 13, "DatasetName", "agent"]]}, {"text": "In Proceedings of the 5th Workshop on Argument Mining , pages 121\u2013130 , Brussels , Belgium . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Argument Mining"]]}, {"text": "Saif M Mohammad and Peter D Turney .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Crowdsourcing a word \u2013 emotion association lexicon .", "entities": [[4, 5, "DatasetName", "emotion"]]}, {"text": "Computational Intelligence , 29(3):436\u2013465 .", "entities": []}, {"text": "Matthew L Newman , James W Pennebaker , Diane S Berry , and Jane M Richards .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Lying words : Predicting deception from linguistic styles .", "entities": []}, {"text": "Personality and social psychology bulletin , 29(5):665\u2013675 .", "entities": []}, {"text": "Rafail Ostrovsky , Yuval Rabani , Leonard J Schulman , and Chaitanya Swamy . 2012 .", "entities": []}, {"text": "The effectiveness of lloyd - type methods for the k - means problem .", "entities": []}, {"text": "Journal of the ACM ( JACM ) , 59(6):28 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Andreas Peldszus and Manfred Stede .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "An annotated corpus of argumentative microtexts .", "entities": []}, {"text": "In Argumentation and Reasoned Action : 1st European Conference on Argumentation ( ECA 16 ) .", "entities": []}, {"text": "College Publications .", "entities": []}, {"text": "James W Pennebaker , Ryan L Boyd , Kayla Jordan , and Kate Blackburn .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "The Development and Psychometric Properties of LIWC2015 .", "entities": []}, {"text": "James W Pennebaker , Cindy K Chung , Joey Frazee , Gary M Lavergne , and David I Beaver .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "When small words foretell academic success : The case of college admissions essays .", "entities": []}, {"text": "PloS one , 9(12):e115844 .", "entities": [[0, 1, "DatasetName", "PloS"]]}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher D. Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Robert Plutchik .", "entities": []}, {"text": "1980 .", "entities": []}, {"text": "A general psychoevolutionary theory of emotion .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "In Theories of emotion , pages 3\u201333 .", "entities": [[3, 4, "DatasetName", "emotion"]]}, {"text": "Elsevier .", "entities": []}, {"text": "Paul Reisert , Naoya Inoue , Naoaki Okazaki , and Kentaro Inui . 2015 .", "entities": []}, {"text": "A computational approach for generating Toulmin model argumentation .", "entities": []}, {"text": "In Proceedings of the 2nd Workshop on Argumentation Mining , pages 45\u201355 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Misa Sato , Kohsuke Yanai , Toshinori Miyoshi , Toshihiko Yanase , Makoto Iwayama , Qinghua Sun , and Yoshiki Niwa . 2015 .", "entities": []}, {"text": "End - to - end argument generation system in debating .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "ACL - IJCNLP 2015 System Demonstrations .", "entities": []}, {"text": "Swapna Somasundaran , Josef Ruppenhofer , and Janyce Wiebe .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Detecting arguing and sentiment in meetings .", "entities": []}, {"text": "In Proceedings of the SIGdial Workshop on Discourse and Dialogue , volume 6 .", "entities": []}, {"text": "64Christian Stab and Iryna Gurevych .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Identifying argumentative discourse structures in persuasive essays .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 46\u201356 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yla R Tausczik and James W Pennebaker .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "The psychological meaning of words : LIWC and computerized text analysis methods .", "entities": []}, {"text": "Journal of language and social psychology , 29(1):24\u201354 .", "entities": []}, {"text": "Stephen E. Toulmin .", "entities": []}, {"text": "1958 .", "entities": []}, {"text": "The Uses of Argument .", "entities": []}, {"text": "Cambridge University Press .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Henning Wachsmuth , Manfred Stede , Roxanne El Baff , Khalid Al Khatib , Maria Skeppstedt , and Benno Stein .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Argumentation synthesis following rhetorical strategies .", "entities": []}, {"text": "In Proceedings of COLING 2018 , the 27th International Conference on Computational Linguistics .", "entities": []}, {"text": "The COLING 2018 Organizing Committee .", "entities": []}, {"text": "To appear .", "entities": []}, {"text": "Lu Wang and Wang Ling .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Neural network - based abstract generation for opinions and arguments .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 47\u201357 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Toshihiko Yanase , Toshinori Miyoshi , Kohsuke Yanai , Misa Sato , Makoto Iwayama , Yoshiki Niwa , Paul Reisert , and Kentaro Inui .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Learning sentence ordering for opinion generation of debate .", "entities": [[1, 3, "TaskName", "sentence ordering"]]}, {"text": "In Proceedings of the 2nd Workshop on Argumentation Mining , pages 94\u2013103 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ingrid Zukerman , Richard McConachy , and Kevin B. Korb .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "Using argumentation strategies in automated argument generation .", "entities": []}, {"text": "In First International Conference on Natural Language Generation ( INLG 00 ) , pages 55\u201362 .", "entities": []}]