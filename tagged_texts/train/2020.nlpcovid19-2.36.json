[{"text": "Hate and Toxic Speech Detection in the Context of Covid-19 Pandemic using XAI :", "entities": []}, {"text": "Ongoing Applied Research David Hardage The University of Texas at San Antonio david.hardage@my.utsa.eduPaul Rad , PhD The University of Texas at San Antonio paul.rad@utsa.edu", "entities": [[8, 9, "DatasetName", "Texas"], [19, 20, "DatasetName", "Texas"]]}, {"text": "Abstract As social distancing , self - quarantines , and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech .", "entities": [[27, 29, "DatasetName", "hate speech"]]}, {"text": "While recent machine learning solutions for automated hate and offensive speech identi\ufb01cation are available on Twitter , there are issues with their interpretability .", "entities": []}, {"text": "We propose a novel use of learned feature importance which improves upon the performance of prior state - of - the - art text classi\ufb01cation techniques , while producing more easily interpretable decisions .", "entities": [[7, 9, "TaskName", "feature importance"]]}, {"text": "We also discuss both technical and practical challenges that remain for this task .", "entities": []}, {"text": "1 Introduction In the day and age of social media , a person \u2019s thoughts and feelings can enter the public discourse at the click of a mouse or tap of a screen .", "entities": []}, {"text": "With billions of individuals active on social media , the task of \ufb01nding reviewing and classifying hate speech online quickly grows to a scale not achievable without the use of machine learning .", "entities": [[16, 18, "DatasetName", "hate speech"]]}, {"text": "Additionally , the de\ufb01nition of hate - speech can be broad and include many nuances , but in general hate speech is de\ufb01ned as communication which disparages or incites violence towards an individual or group based on that person or groups \u2019 cultural / ethnic background , gender or sexual orientation .", "entities": [[19, 21, "DatasetName", "hate speech"]]}, {"text": "( Schmidt and Wiegand , 2017 ) .", "entities": []}, {"text": "In the context of Covid-19 , the United Nations has released guidelines on Covid-19 related hatespeech Guidance on COVID-19 related Hate Speech cautioning that Member States and Social Media companies that with the rise of Covid19 cases there has also been an increase of hate speech .", "entities": [[8, 9, "DatasetName", "Nations"], [20, 22, "DatasetName", "Hate Speech"], [44, 46, "DatasetName", "hate speech"]]}, {"text": "The UN warns that such communication could be used for scapegoating , stereotyping , racist and xenophobic purposes .", "entities": []}, {"text": "1UN Guidance Note on Addressing and Countering COVID-19 related Hate Speech 11 May , 20 Figure 1 : The tweets above displays an example of hate speech used for scapegoating by @realDonaldTrump and the response of @ajRAFAEL highlighting the impact this hate speech has on Asian Americans .", "entities": [[9, 11, "DatasetName", "Hate Speech"], [25, 27, "DatasetName", "hate speech"], [41, 43, "DatasetName", "hate speech"]]}, {"text": "The importance of identifying hate speech combined with the magnitude of the data makes this an area in which innovations achieved in NLP and AI research can make an impact .", "entities": [[4, 6, "DatasetName", "hate speech"]]}, {"text": "However , the datasets we use re\ufb02ect their environments and even their annotators ( Waseem , 2016 ) ( Sap et al . , 2019 ) , there are inherent cues contained by the data which can bias the predictions of models developed from these data ( Davidson et al . , 2019 ) .", "entities": []}, {"text": "In the context of detecting hate speech detection , this can lead to predictions be largely the outcome of a few key terms ( Davidson et al . , 2017 ) .", "entities": [[5, 8, "TaskName", "hate speech detection"]]}, {"text": "Being able to explain how underlying data impacts AI decision outcomes has real world applications , and social media companies ignorant to this fact could face a multitude of ethical and legal repercussions ( Samek et al . , 2017 ) .", "entities": []}, {"text": "Our Contribution : In this research , we merge feature importance with text classi\ufb01cation to help decrease false positives .", "entities": [[9, 11, "TaskName", "feature importance"]]}, {"text": "Our method combines the global representation of a term \u2019s feature importance to a predicted class with the local term feature importance of an individual observation .", "entities": [[10, 12, "TaskName", "feature importance"], [20, 22, "TaskName", "feature importance"]]}, {"text": "Each term \u2019s", "entities": []}, {"text": "Figure 2 : This is an example of a Covid-19 Tweet incorrectly classi\ufb01ed by our baseline model as \u201d hate speech towards immigrants \u201d .", "entities": [[19, 21, "DatasetName", "hate speech"]]}, {"text": "After the applying our prediction enhancement method , the tweet was correctly classi\ufb01ed as \u201d not hate speech \u201d .", "entities": [[16, 18, "DatasetName", "hate speech"]]}, {"text": "The \ufb01rst two sentence combinations show differences in local and global term importance impacting the Term Difference Multiplier .", "entities": []}, {"text": "The intensity of grey represents the importance of each term to the denoted label .", "entities": []}, {"text": "The last sentence pair provides the term difference for each local and global term pair as described in our experimental design .", "entities": []}, {"text": "global feature importance is collected from our training dataset and baseline model .", "entities": [[1, 3, "TaskName", "feature importance"]]}, {"text": "Then local feature importance is calculated for each observation on which our trained model makes a prediction .", "entities": [[2, 4, "TaskName", "feature importance"]]}, {"text": "Our algorithm , uses the term level global feature importance to penalize model predictions when an observation \u2019s local term feature importance differs from the global feature importance .", "entities": [[8, 10, "TaskName", "feature importance"], [20, 22, "TaskName", "feature importance"], [26, 28, "TaskName", "feature importance"]]}, {"text": "2 Explainability and Text Classi\ufb01cation", "entities": []}, {"text": "In the same vein of our research , others have leveraged explainability derived with integrated gradients ( Sundararajan et al . , 2017 ) and subject matter experts to create priors for use in text classi\ufb01cation .", "entities": []}, {"text": "In this research , they showed a decrease in undesired model bias and an increase in model performance when using scarce data ( Liu and Avci , 2019 ) .", "entities": []}, {"text": "Overall , our method appears to have similar results and lessens the impacts of speci\ufb01c key terms to the the overall model prediction .", "entities": []}, {"text": "One of the more commonly utilized explainability methods , SHAP provides a framework within the feature contrubutions to a a model \u2019s output can be derived by borrowing Aultman - Shapely values from cooperative game theory ( Lundberg and Lee , 2017 ) .", "entities": [[9, 10, "MethodName", "SHAP"]]}, {"text": "While there are several \u201d explainer \u201d implementations included with SHAP , Gradint Explainer allowed us to leverage our entire training dataset as the background dataset which allows our global average term values described in our experimental design to represent all terms in the training corpus .", "entities": [[10, 11, "MethodName", "SHAP"]]}, {"text": "SHAP \u2019s Gradient Explainer builds off of integrated gradients and leverages what are called expected gradients .", "entities": [[0, 1, "MethodName", "SHAP"]]}, {"text": "This feature attribution methodtakes the integral from integrated gradients and reformulates it as an expectation usable in calculating the Shapely values .", "entities": []}, {"text": "The resulting attributions sum to the difference between the expected and current model output .", "entities": []}, {"text": "However , this method does assume independence of the input features , so it would violate this assumption if we were to leverage any sequence models in classi\ufb01cation .", "entities": []}, {"text": "3 Datasets 3.1 Training and Evaluation Data For this research , our intent to score unlabeled tweets called for a robust dataset which could be generalize to Covid-19 tweets .", "entities": []}, {"text": "This lead us to combining three datasets in the domain of hate and offensive speech : the collection of racist and sexist tweets presented by Waseem and Hovy ( Waseem and Hovy , 2016 ) , the Offensive Language Identi\ufb01cation Dataset ( OLID ) ( Zampieri et al . , 2019 ) , and Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter ( HatEval)(Basile et al . , 2019 ) .", "entities": [[42, 43, "DatasetName", "OLID"], [57, 59, "DatasetName", "Hate Speech"]]}, {"text": "All hate or offensive labels contained within these three datasets were combined and given a sub - classi\ufb01cation based on terms contained within each example .", "entities": []}, {"text": "The sub - classes focus on hate and offensive speech targeting or directed towards immigrants , sexist , or political topics and/or individuals .", "entities": []}, {"text": "The terms which divided positive labeled data into these three sub classes were derived through analysis of the terms contained in positive labeled tweets and through the terms used in extracting the tweets for the original data datasets .", "entities": []}, {"text": "Figure 3 : The architecture of both our baseline model and the prediction enhancement .", "entities": []}, {"text": "Once the baseline model is trained we store the average term feature importance of terms in the training dataset to each predicted class .", "entities": [[11, 13, "TaskName", "feature importance"]]}, {"text": "This is used as the global representation of that term \u2019s importance which we use with an individual observation \u2019s local term feature importance to derive the Term Difference Multiplier .", "entities": [[22, 24, "TaskName", "feature importance"]]}, {"text": "3.2 Covid-19 Tweets In order to leverage this research in the context of Covid-19 , we collected tweets using two different sources .", "entities": []}, {"text": "First , we leveraged data collected by the Texas Advanced Computing Center ( TAAC ) at the University of Texas at Austin .", "entities": [[8, 9, "DatasetName", "Texas"], [19, 20, "DatasetName", "Texas"]]}, {"text": "This dataset was important for us to use due to \u201d Chinese Virus \u201d being one of the term pairs the TACC team used to collect data .", "entities": []}, {"text": "In the context of hate speech in Covid-19 , terms which target countries or ethnicity \u2019s in the labeling of the virus clearly disregard the aforementioned UN guidance on hate speech .", "entities": [[4, 6, "DatasetName", "hate speech"], [29, 31, "DatasetName", "hate speech"]]}, {"text": "The second dataset we leveraged was provided by Georgia State University \u2019s Panacea Lab ( Banda et al . , 2020 ) .", "entities": []}, {"text": "For this dataset , we speci\ufb01cally hydrated tweets from the days following the murder of George Floyd and begging of civil unrest in America .", "entities": []}, {"text": "The intent behind limiting to these dates was to increase the chance of capturing tweets containing racial or ethnic terms .", "entities": []}, {"text": "4 Experimental Design 4.1 Text Classi\ufb01er Since our research focus is to leverage learned feature importance to enhance predictions , we followed proven methods for hate speech classi\ufb01cation ( Gamb \u00a8ack and Sikdar , 2017 ) .", "entities": [[14, 16, "TaskName", "feature importance"], [25, 27, "DatasetName", "hate speech"]]}, {"text": "All tweets were converted into the Glove twitter embeddings ( Pennington et al . , 2014 ) .", "entities": []}, {"text": "These embeddings were passed to a Convolution Neural Network classi\ufb01er 2https://www.tacc.utexas.edu/-/tacc-covid-19-twitterdataset-enables-social-science-research-about-pandemicwhich mirrored the same architecture used by Yoon Kim for sentence classi\ufb01cation ( Kim , 2014 ) .", "entities": [[6, 7, "MethodName", "Convolution"]]}, {"text": "We allowed for parameter tuning with random search , and the \ufb01nal CNN consisted of three convolutional layers of 75 \ufb01lters with kernel sizes of 3 , 4 and 6 .", "entities": [[6, 8, "MethodName", "random search"]]}, {"text": "These all received one dimensional max pooling and a dropout rate of 0.4 was applied .", "entities": [[5, 7, "MethodName", "max pooling"]]}, {"text": "The output layer is a softmax with l2 regularization set at 0.029 .", "entities": [[5, 6, "MethodName", "softmax"], [7, 9, "HyperparameterName", "l2 regularization"]]}, {"text": "These parameters were selected by ranking validation AUC .", "entities": [[7, 8, "MetricName", "AUC"]]}, {"text": "This model served as both our baseline model and the input predictions of our predictions enhanced with XAI .", "entities": []}, {"text": "4.2 Calculating Global Average Feature Importance To achieve this , we apply SHAP \u2019s Gradient Explainer to our baseline model .", "entities": [[4, 6, "TaskName", "Feature Importance"], [12, 13, "MethodName", "SHAP"]]}, {"text": "The Gradient Explainer output has the same dimensions as our Glove embedded data , so to reduce the dimensionality to that of the input text sequence , we sum the expected gradients across the axis corresponding to a term in each sequence .", "entities": []}, {"text": "s1\"xnX x1xi = x1+:::+xn # ! \u0001\u0001\u0001 !", "entities": []}, {"text": "sn\"xnX x1\u0001\u0001\u0001 # Where s is a token in each sequence and xiis the summation of all expected gradients for the embedding dimensions .", "entities": []}, {"text": "The values of these summations can be both positive and negative .", "entities": []}, {"text": "Since our method requires positive inputs to measure the percentage difference , these values for every s step across all sequences in the training dataset are scaled between 0 and 1 via min / max scaling .", "entities": [[28, 29, "DatasetName", "0"]]}, {"text": "We", "entities": []}, {"text": "then create a dictionary of terms from the training corpus and store the \u201d global average \u201d feature importance of each term .", "entities": [[17, 19, "TaskName", "feature importance"]]}, {"text": "This dictionary of global average feature importance values is used to calculate how far a particular prediction strays from the feature importance represented in our training data .", "entities": [[5, 7, "TaskName", "feature importance"], [20, 22, "TaskName", "feature importance"]]}, {"text": "4.3 Enhancing Predictions with Term Difference Multiplier Now that we have the global importance of each term ( feature token ) to each class , we calculate the percentage difference of each term \u2019s local feature importance to that term \u2019s global feature importance by each class .", "entities": [[35, 37, "TaskName", "feature importance"], [42, 44, "TaskName", "feature importance"]]}, {"text": "1\u00000 BB@jsg\u0000slj\u0014(sg+sl ) 2\u00151 CCA As you can see above the percentage difference between each local ( sl ) and global ( sg ) feature importance is subtracted from 1 .", "entities": [[24, 26, "TaskName", "feature importance"]]}, {"text": "This outputs the difference multiplier for each term in a sequence .", "entities": []}, {"text": "These values are averaged for each sequence , and the predicted probability for each class is multiplied by it \u2019s local Term Difference Multiplier for each sequence .", "entities": []}, {"text": "This outputs a new predicted probability score which has been penalized based on how much it \u2019s local attribution values differ from the global mean of each term in the input sequence .", "entities": []}, {"text": "5 Results and Application to Covid 19 Tweets", "entities": []}, {"text": "We found that the enhanced predictions predominately help to correct false positive classi\ufb01cations and shift predictions towards the negative class .", "entities": []}, {"text": "We hypothesize this is due to the diversity of language and relatively neutral feature importance most terms have on the negative class .", "entities": [[13, 15, "TaskName", "feature importance"]]}, {"text": "The relative neutrality in both global and local feature importance scores can be seen in \ufb01gure 2 , and this results in a higher overall average for the aggregation of the Term Difference Multiplier .", "entities": [[8, 10, "TaskName", "feature importance"]]}, {"text": "When we applied our model to Covid-19 tweets we found similar results as described above .", "entities": []}, {"text": "Quite often , we found that tweets providing information about speci\ufb01c ethnic groups or migrants were labeled as hateful or toxic towards immigrants by our baseline model and then correctly labeled as not hateful or offensive speech when we applied the Term Difference Multiplier .", "entities": []}, {"text": "An example of this exact scenario is provided in \ufb01gure 2 .", "entities": []}, {"text": "Figure 4 : A comparison of the classi\ufb01cation outputs from our two implementations .", "entities": []}, {"text": "As you can see above , enhancing predictions with the Term Difference Multiplier shifts predictions to the majority class of no hate or toxic speech .", "entities": []}, {"text": "6 Conclusion and Future Work", "entities": []}, {"text": "Here we have experimented with a novel method to leverage the global feature importance from a model \u2019s training dataset to reinforce or even penalize new predictions when their local feature importance varies from this learned global value .", "entities": [[12, 14, "TaskName", "feature importance"], [30, 32, "TaskName", "feature importance"]]}, {"text": "This novel algorithm marries the \ufb01eld of XAI and NLP in a manner which allows prior knowledge obtained in model training to impact present predictions .", "entities": []}, {"text": "Overall , we believe this technique is especially applicable in scenarios like Covid-19 where little to no pre - existing labeled data are available .", "entities": []}, {"text": "By training this method on a similar corpus it can be used to detract from incorrect predictions made due to a few highly in\ufb02uential terms in Covid-19 datasets .", "entities": []}, {"text": "At present due to this method \u2019s ability to decrease false positives , we believe one application of this research is increasing the ef\ufb01ciency of systems monitoring for hateful and toxic communication .", "entities": []}, {"text": "However , this research is ongoing .", "entities": []}, {"text": "We intend to explore further scenarios such as altering the equation used in our Term Difference Multiplier and the datasets used since the global feature importance can greatly in\ufb02uence the multiplier combined with our model \u2019s original predicted probabilities .", "entities": [[24, 26, "TaskName", "feature importance"]]}, {"text": "References Juan M. Banda , Ramya Tekumalla , Guanyu Wang , Jingyuan Yu , Tuo Liu , Yuning Ding , Katya Artemova , Elena Tutubalin , and Gerardo Chowell .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A large - scale COVID-19", "entities": []}, {"text": "Twitter chatter dataset for open scienti\ufb01c research - an international collabo-", "entities": []}, {"text": "ration .", "entities": []}, {"text": "This dataset will be updated bi - weekly at least with additional tweets , look at the github repo for these updates .", "entities": []}, {"text": "Release : We have standardized the name of the resource to match our pre - print manuscript and to not have to update it every week .", "entities": []}, {"text": "Valerio Basile , Cristina Bosco , Elisabetta Fersini , Debora Nozza , Viviana Patti , Francisco Manuel Rangel Pardo , Paolo Rosso , and Manuela Sanguinetti .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SemEval-2019 task 5 : Multilingual detection of hate speech against immigrants and women in twitter .", "entities": [[7, 9, "DatasetName", "hate speech"]]}, {"text": "In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 54\u201363 , Minneapolis , Minnesota , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Davidson , Debasmita Bhattacharya , and Ingmar Weber .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Racial bias in hate speech and abusive language detection datasets .", "entities": [[3, 5, "DatasetName", "hate speech"], [6, 8, "TaskName", "abusive language"]]}, {"text": "In Proceedings of the Third Workshop on Abusive Language Online , pages 25\u201335 , Florence , Italy .", "entities": [[7, 9, "TaskName", "Abusive Language"], [14, 15, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Davidson , Dana Warmsley , Michael Macy , and Ingmar Weber . 2017 .", "entities": []}, {"text": "Automated hate speech detection and the problem of offensive language .", "entities": [[1, 4, "TaskName", "hate speech detection"]]}, {"text": "In Eleventh international aaai conference on web and social media .", "entities": []}, {"text": "Bj\u00a8orn", "entities": []}, {"text": "Gamb \u00a8ack and Utpal Kumar Sikdar . 2017 .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "Using convolutional neural networks to classify hatespeech .", "entities": []}, {"text": "In Proceedings of the First Workshop on Abusive Language Online , pages 85\u201390 , Vancouver , BC , Canada .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1408.5882 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Frederick Liu and Besim Avci .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Incorporating priors with feature attribution on text classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1906.08286 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Scott M Lundberg and Su - In Lee . 2017 .", "entities": []}, {"text": "A uni\ufb01ed approach to interpreting model predictions .", "entities": []}, {"text": "In I. Guyon , U. V .", "entities": []}, {"text": "Luxburg , S. Bengio , H. Wallach , R. Fergus , S. Vishwanathan , and R. Garnett , editors , Advances in Neural Information Processing Systems 30 , pages 4765\u20134774 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher D Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 conference on empirical methods in natural language processing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Wojciech Samek , Thomas Wiegand , and Klaus - Robert M\u00a8uller . 2017 .", "entities": []}, {"text": "Explainable arti\ufb01cial intelligence : Understanding , visualizing and interpreting deep learning models .", "entities": []}, {"text": "arXiv preprint arXiv:1708.08296 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Maarten Sap , Dallas Card , Saadia Gabriel , Yejin Choi , and Noah A. Smith .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The risk of racial bias in hate speech detection .", "entities": [[6, 9, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of the57th Annual Meeting of the Association for Computational Linguistics , pages 1668\u20131678 , Florence , Italy . Association for Computational Linguistics .", "entities": [[16, 17, "MethodName", "Florence"]]}, {"text": "Anna Schmidt and Michael Wiegand .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A survey on hate speech detection using natural language processing .", "entities": [[3, 6, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of the Fifth International workshop on natural language processing for social media , pages 1\u201310 .", "entities": []}, {"text": "Mukund Sundararajan , Ankur Taly , and Qiqi Yan . 2017 .", "entities": []}, {"text": "Axiomatic attribution for deep networks .", "entities": []}, {"text": "arXiv preprint arXiv:1703.01365 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zeerak Waseem .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Are you a racist or am I seeing things ?", "entities": []}, {"text": "annotator in\ufb02uence on hate speech detection on twitter .", "entities": [[3, 6, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of the First Workshop on NLP and Computational Social Science , pages 138 \u2013 142 , Austin , Texas .", "entities": [[20, 21, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zeerak Waseem and Dirk Hovy .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Hateful symbols or hateful people ?", "entities": []}, {"text": "predictive features for hate speech detection on twitter .", "entities": [[3, 6, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of the NAACL Student Research Workshop , pages 88\u201393 , San Diego , California .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Marcos Zampieri , Shervin Malmasi , Preslav Nakov , Sara Rosenthal , Noura Farra , and Ritesh Kumar .", "entities": [[17, 18, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Predicting the Type and Target of Offensive Posts in Social Media .", "entities": []}, {"text": "In Proceedings of NAACL .", "entities": []}]