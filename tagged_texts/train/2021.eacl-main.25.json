[{"text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics , pages 326\u2013346 April 19 - 23 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics326Evaluating the Evaluation of Diversity in Natural Language Generation Guy Tevet1;2Jonathan Berant1;3 1School of Computer Science , Tel - Aviv University 2Department of Electrical Engineering , Tel - Aviv University 3Allen Institute for AI fguytevet@mail , joberant@cs g.tau.ac.il Abstract", "entities": [[28, 30, "TaskName", "Electrical Engineering"]]}, {"text": "Despite growing interest in natural language generation ( NLG ) models that produce diverse outputs , there is currently no principled method for evaluating the diversity of an NLG system .", "entities": []}, {"text": "In this work , we propose a framework for evaluating diversity metrics .", "entities": []}, {"text": "The framework measures the correlation between a proposed diversity metric and a diversity parameter , a single parameter that controls some aspect of diversity in generated text .", "entities": []}, {"text": "For example , a diversity parameter might be a binary variable used to instruct crowdsourcing workers to generate text with either low or high content diversity .", "entities": []}, {"text": "We demonstrate the utility of our framework by : ( a ) establishing best practices for eliciting diversity judgments from humans , ( b ) showing that humans substantially outperform automatic metrics in estimating content diversity , and ( c ) demonstrating that existing methods for controlling diversity by tuning a \u201c decoding parameter \u201d mostly affect form but not meaning .", "entities": []}, {"text": "Our framework can advance the understanding of different diversity metrics , an essential step on the road towards better NLG systems .", "entities": []}, {"text": "1 Introduction An important desideratum of natural language generation ( NLG ) systems is to produce outputs that are not only correct , but also diverse .", "entities": []}, {"text": "For example , a dialog system ( Adiwardana et al . , 2020 ) should permit many responses for the prompt \u201c How are you today ? \u201d .", "entities": []}, {"text": "Similarly , we expect diverse responses in tasks such as story generation ( Li et al . , 2018 ) , question generation ( Pan et al . , 2019 ) and question answering ( Fan et al . , 2019 ) .", "entities": [[10, 12, "TaskName", "story generation"], [21, 23, "TaskName", "question generation"], [32, 34, "TaskName", "question answering"]]}, {"text": "Despite growing effort to produce more diverse models ( Li et al . , 2016c , a ; Holtzman et al . , 2019 ; Du and Black , 2019 ) , there is no standard evaluation metric for measuring diversity .", "entities": []}, {"text": "Thus , different papers evaluate diversity differently ( if at 0.6 0.8 1.0 Metric Valuesdistinct - n ( averaged ) Set A Set B 2 3 4 Metric ValuesabsHDSQuestion : So what did I miss in the \ufb01rst 20 minutes ?", "entities": []}, {"text": "Set A \u000fPretty much everything .", "entities": []}, {"text": "\u000fNothing , really .", "entities": []}, {"text": "\u000fYou wo n\u2019t believe what happened !", "entities": []}, {"text": "\u000fWhy do you even care ?", "entities": []}, {"text": "\u000fWhat were you doing that was more important than this ?", "entities": []}, {"text": "Set B \u000fNot much .", "entities": []}, {"text": "\u000fIt was pretty dull .", "entities": []}, {"text": "\u000fBlah , you did n\u2019t miss anything .", "entities": []}, {"text": "\u000fNot anything that important .", "entities": []}, {"text": "\u000fVery little , it was uneventful .", "entities": []}, {"text": "Figure 1 : Diversity metric evaluation : we show two sets of responses to the same question , generated by crowdsourcing workers .", "entities": []}, {"text": "While both sets are diverse in terms of form , only set A is diverse in terms of content .", "entities": []}, {"text": "Each graph presents the distribution over a diversity metric for sets with high content diversity ( blue ) and low content diversity ( orange ) .", "entities": []}, {"text": "Distributions are approximated over 200sets .", "entities": []}, {"text": "We observe that the human score metric ( absDHS ) separates the two distributions , while an n - gram based metric ( distinct - n ) fails , illustrating that it does not capture content diversity .", "entities": []}, {"text": "The dotted lines correspond to the speci\ufb01c sets A and B presented above .", "entities": []}, {"text": "all ) , making it dif\ufb01cult to compare competing approaches ( Hashimoto et al . , 2019 ) .", "entities": []}, {"text": "Having a principled and consensual diversity evaluation metric is hence fundamental for the \ufb01eld of NLG .", "entities": []}, {"text": "A key challenge in developing diversity evaluation metrics , is the dif\ufb01culty in determining their ef\ufb01cacy .", "entities": []}, {"text": "Unlike metrics for evaluating the quality of generated text , where one can measure correlation between a metric ( such as BLEU ( Papineni et al . , 2002 ) ) and human judgement ( Zhang et al . , 2019a ; Sagarkar et al . , 2018 ) , it is unknown if hu-", "entities": [[21, 22, "MetricName", "BLEU"]]}, {"text": "327mans can reliably estimate diversity .", "entities": []}, {"text": "In this paper , we propose a framework for evaluating diversity metrics ( Figure 2 ) .", "entities": []}, {"text": "We assume that atester ( human or model ) is generating sets of sentences , conditioned on some diversity parameter that controls the diversity of the output sentences .", "entities": []}, {"text": "We evaluate the diversity of the sentences using a proposed metric , and measure correlation between the metric and the diversity parameter .", "entities": []}, {"text": "High correlation indicates that the metric captures how the diversity parameter affects the model output .", "entities": []}, {"text": "We instantiate this framework with two tests .", "entities": []}, {"text": "As a preliminary step , we introduce the decoding test : the tester is a neural generation model and the diversity parameter is a decoding parameter , such as softmax temperature ( Ackley et al . , 1985 ) .", "entities": [[29, 30, "MethodName", "softmax"]]}, {"text": "This parameter controls the skewness of the distribution in every generated token , and has been shown to affect model diversity ( Holtzman et al . , 2019 ; Caccia et al . , 2018 ) .", "entities": []}, {"text": "Then , we turn the focus tocontent diversity , introducing the content test ( Figure 1 ) .", "entities": []}, {"text": "Here , the tester is a human , and the diversity parameter is a binary variable , where the human is instructed to generate sets of sentences with either high orlowdiversity in content .", "entities": []}, {"text": "We evaluate three families of popular diversity metrics with these tests : ( a ) n - gram - based metrics that estimate diversity based on surface patterns in a set of generated sentences , ( b ) neural metrics : we propose a reduction from evaluating sentence similarity to evaluating diversity , then evaluate diversity using state - of - the - art sentence similarity models , and ( c ) human evaluation : we explore multiple ways in which humans can be asked to estimate diversity , resulting in multiple Human Diversity Score ( HDS ) variations .", "entities": [[94, 95, "MetricName", "Score"]]}, {"text": "Applying our tests leads to several \ufb01ndings : ( i ) In the decoding test , n - gram - based metrics correlate well with decoding parameters , such as softmax temperature .", "entities": [[30, 31, "MethodName", "softmax"]]}, {"text": "While the goal of our framework is to evaluate diversity metrics , this result lets us re\ufb02ect back on the tester itself and conclude that decoding parameters predominantly control the form of text rather than content .", "entities": []}, {"text": "( ii ) Conversely , n - gram - based metrics perform poorly in thecontent test .", "entities": []}, {"text": "While neural metrics outperform n - gram - based metrics , humans are substantially better than any automatic metric at detecting content diversity .", "entities": []}, {"text": "This is illustrated in Figure 1 , where a human clearly distinguishes between sets that have high ( blue ) and low ( orange ) content diver - sity , while n - gram - based metrics fail to do so .", "entities": []}, {"text": "Due to this gap , we construct a large dataset focused on content -diversity metrics .", "entities": []}, {"text": "We release the Metrics for content Diversity ( McDiv ) benchmark , a challenge for research in diversity evaluation .", "entities": []}, {"text": "To conclude , our main contributions are : \u2022 A framework for evaluating diversity metrics .", "entities": []}, {"text": "\u2022 Tests instantiating this framework , measuring the sensitivity of metrics to diversity , with a focus on content diversity .", "entities": []}, {"text": "\u2022 Best practices for obtaining diversity evaluations from crowdsourcing workers .", "entities": []}, {"text": "\u2022 Establishing that humans outperform current automatic metrics in detecting content diversity .", "entities": []}, {"text": "\u2022", "entities": []}, {"text": "The McDiv dataset - a benchmark for content diversity aware metrics .", "entities": []}, {"text": "\u2022", "entities": []}, {"text": "The collected data , test scores and code are publicly available,1and can be used to easily compare new diversity metrics to existing results in our framework .", "entities": []}, {"text": "2 Background : Diversity Evaluation Recently , interest in diversity has increased ( Du and Black , 2019 ; Holtzman et al . , 2019 ) , resulting in multiple proposals for its evaluation .", "entities": []}, {"text": "We describe recent approaches , highlighting the need for a standard way to evaluate metrics .", "entities": []}, {"text": "Perplexity is the standard metric in language modeling , measuring the proximity of a language model ( LM ) , PLM , to the true distribution , Pref , by approximating the cross - entropy H(Pref;PLM ) with held - out data from Pref .", "entities": [[0, 1, "MetricName", "Perplexity"]]}, {"text": "Thus , perplexity captures to some extent diversity .", "entities": [[2, 3, "MetricName", "perplexity"]]}, {"text": "For example , a dialog model that puts all probability mass on the output\u201cI do n\u2019t know \u201d for any given context will obtain in\ufb01nite perplexity once it encounters any other response .", "entities": [[25, 26, "MetricName", "perplexity"]]}, {"text": "This property makes perplexity popular in LM - based NLG models , and often it is the only reported measure for diversity ( Lewis et al . , 2017 ; Fan et al . , 2018 ; Wang et al . , 2019 ; Li et", "entities": [[3, 4, "MetricName", "perplexity"]]}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "However , perplexity does not purely measure diversity , and high perplexity does not entail low diversity .", "entities": [[2, 3, "MetricName", "perplexity"], [11, 12, "MetricName", "perplexity"]]}, {"text": "For example , a LM with a uniform distribution over the vocabulary for each decoded token has high diversity , but its perplexity will be extremely high , due to its low quality .", "entities": [[22, 23, "MetricName", "perplexity"]]}, {"text": "Moreover , perplexity evaluates a LM , while the diversity of a NLG system is also strongly affected by the decoding procedure .", "entities": [[2, 3, "MetricName", "perplexity"]]}, {"text": "For example , Top - k andnucleus 1https://github.com/GuyTevet/ diversity - eval", "entities": []}, {"text": "328sampling are popular decoding schemes that tradeoff quality and diversity by ignoring some of the LM probability mass ( Holtzman et al . , 2019 ) .", "entities": []}, {"text": "Last , some NLG models , such as Generative Adversarial Networks ( GANs ) ( Yu et al . , 2017 ) are not language models .", "entities": []}, {"text": "While one can approximate perplexity for such models ( Tevet et al . , 2019 ) , ideally , a metric should not be tied to a model .", "entities": [[4, 5, "MetricName", "perplexity"]]}, {"text": "N - gram - based metrics A popular metric is distinct n - grams ( Li et al . , 2016b ) , which computes the proportion of unique n - grams out of the total number of n - grams in a set of generated sentences .", "entities": []}, {"text": "Du \u02c7sek et al .", "entities": []}, {"text": "( 2020 ) calculated Shannon entropy ( Manning et al . , 1999 ) based on different n - grams as a measure of lexical diversity .", "entities": []}, {"text": "SelfBLEU", "entities": []}, {"text": "( Zhu et al . , 2018 ; Shu et al . , 2019 ) measures the BLEU score of a generated sentence with respect to another generated sentence ( rather than a gold reference ) .", "entities": [[17, 19, "MetricName", "BLEU score"]]}, {"text": "High average Self - BLEU indicates high similarity between generated sentences and low diversity .", "entities": [[4, 5, "MetricName", "BLEU"]]}, {"text": "In \u00a7 5 we expand this idea and suggest a reduction from any similarity metric to a diversity metric .", "entities": []}, {"text": "By design , n - gram based metrics are sensitive to diversity in the form of language , rather than its meaning .", "entities": []}, {"text": "Embedding - based metrics A new line of metrics suggests to embed generated sentences in latent space , then evaluate them in this space .", "entities": []}, {"text": "Du and Black ( 2019 ) suggest to cluster the embedded sentences with k - means , then use its inertia as a measure for diversity .", "entities": []}, {"text": "Recently , Lai et al .", "entities": []}, {"text": "( 2020 ) suggested to consider the volume induced by the embedded sentences as a diversity metric .", "entities": []}, {"text": "Human evaluation Yang et al .", "entities": []}, {"text": "( 2019 ) asked humans to evaluate the internal diversity of a generated essay .", "entities": []}, {"text": "Ghandeharioun et al .", "entities": []}, {"text": "( 2019 ) let crowdsourcing workers interact with a dialog chat - bot , then asked them to evaluate the diversity of a single conversation .", "entities": []}, {"text": "In contrast , this paper focuses on the diversity of different responses given a context , as in Zhang et al .", "entities": []}, {"text": "( 2019b )", "entities": []}, {"text": ".", "entities": []}, {"text": "To conclude , increasing interest in diversity resulted in multiple proposed diversity metrics .", "entities": []}, {"text": "However , there is no consensus on how to evaluate diversity and what each metric actually measures .", "entities": []}, {"text": "3 Evaluating Diversity Metrics We now describe our framework for evaluating diversity metrics .", "entities": []}, {"text": "Diversity has many facets : for in - Diversity Parameter d\u201cHow are you today ? \u201d", "entities": []}, {"text": "c Tester   / Gd(c )", "entities": []}, {"text": "Diversity Metric mdiv(Sc;d ) Test Score \u001a(mdiv;d)\u201cVery good ! \u201d", "entities": [[5, 6, "MetricName", "Score"]]}, {"text": "\u201c Fine thank you . \u201d", "entities": []}, {"text": "\u201c Could n\u2019t be better . \u201d", "entities": []}, {"text": "Figure 2 : An overview of our diversity metrics evaluation framework .", "entities": []}, {"text": "The tester ( machine or human ) generates a response set ( Sc;d ) given a diversity parameter ( d ) and a context ( c ) .", "entities": []}, {"text": "The test score of a metric mdivis the correlation between the metric score for Sc;dandd . stance , a set of sentences can be diverse in terms of their content , while another may have similar content , but diverse form ( Figure 1 ) .", "entities": []}, {"text": "Our framework provides a way to evaluate metrics for different aspects of diversity under moderate assumptions .", "entities": []}, {"text": "We de\ufb01ne a diversity metric mdiv(Sc)2Ras a function that takes a set of generated responses Sc as an input , and outputs a diversity score .", "entities": []}, {"text": "Each responses2Scis generated for the same input contextc , henceScis a sample from a generative distributionPgen(sjc ) .", "entities": []}, {"text": "The overall diversity score of a generative model can be obtained by averaging mdivover setsScsampled from the model given multiple contexts c2C. To evaluatemdiv(\u0001 ) , we assume access to some deterministic diversity parameter dthat controls an aspect of diversity in Sc .", "entities": []}, {"text": "We test the relation betweenmdivand the parameter d.", "entities": []}, {"text": "By varying d and measuring mdiv , we can compute the correlation\u001abetweenmdivand an aspect of diversity represented byd .", "entities": []}, {"text": "Because our goal is to have metrics thatrank the diversity of generated texts , we use Spearman \u2019s \u001arank correlation as our test score .", "entities": []}, {"text": "Figure 2 illustrates the \ufb02ow of a test in our framework .", "entities": []}, {"text": "In practice , to control the diversity level of Sc usingd , we use a tester : a generative model that takes a context cand a diversity parameter das input , and outputs a response set Sc;d .", "entities": []}, {"text": "We stress that the tester can be either a neural model or a human .", "entities": []}, {"text": "A good tester should reliably represent the diversity level quanti\ufb01ed by d.", "entities": []}, {"text": "As a hypothetical example , ccan be a movie name anddrepresent sentiment diversity , that is ,", "entities": []}, {"text": "329the number of different sentiments in a collection of reviewsSc .", "entities": []}, {"text": "A human tester can observe cand d , and produce reviews accordingly ( such data can be easily mined from IMDB ) .", "entities": [[20, 21, "DatasetName", "IMDB"]]}, {"text": "A collection of such ( d;Sc;d)makes a test , in which the correlation betweenmdiv(Sc;d)anddmeasures the sensitivity of mdivto sentiment diversity .", "entities": []}, {"text": "We now describe two tests that instantiate this framework , roughly corresponding to the two main aspects of diversity : form diversity and content diversity .", "entities": []}, {"text": "3.1 Decoding Test The diversity of a NLG system constructed from a LM depends on both the LM but also the decoding algorithm on top of it .", "entities": []}, {"text": "For example , beam search approximates the most probable output , and dramatically reduces diversity .", "entities": []}, {"text": "Conversely , sampling from the LM leads to high diversity , but low quality output ( Holtzman et al . , 2019 ) .", "entities": []}, {"text": "A popular method to control diversity in NLG systems is to vary some decoding parameter .", "entities": []}, {"text": "Variations include ( a ) softmax temperature ( Ackley et al . , 1985 ) , where a parameter \u001c controls the skewness of the softmax distribution at each step , ( b)Nucleus ( Top- p ) sampling ( Holtzman et al . , 2019 ) , where one samples at each step from the minimal set of most probable tokens whose cumulative probability is at least p , and ( c ) Top - ksampling , which samples from the top- kmost probable tokens at each step .", "entities": [[5, 6, "MethodName", "softmax"], [25, 26, "MethodName", "softmax"]]}, {"text": "All methods skew the LM distribution in a way that avoids low - probability tokens and leads to higher quality ( Holtzman et al . , 2019 ) , providing a decoding parameter that trades off quality and diversity ( Caccia et al . , 2018 ) .", "entities": []}, {"text": "In the decoding test ( decTest ) , we de\ufb01ne the tester to be a LM , such as GPT-2 ( Radford et al . , 2019 ) , and the diversity parameter dto be a decoding parameter such as temperature .", "entities": [[19, 20, "MethodName", "GPT-2"]]}, {"text": "We check how different diversity metrics mdivcorrelate with decoding parameters .", "entities": []}, {"text": "This can shed light on the quality of the metrics , but also on how decoding parameters affect the output of a NLG system .", "entities": []}, {"text": "The decoding test uses automatically - generated data that is cheap to produce , and decoding parameters that are well - known to control diversity .", "entities": []}, {"text": "Thus , we view this test as a warm - up test to explore the strengths of our framework .", "entities": []}, {"text": "3.2 Content Test In the content test ( conTest ) , our goal is to evaluate how different diversity metrics capture the notionofcontent diversity .", "entities": []}, {"text": "Measuring content diversity requires deep understanding of the semantics of responses inSc .", "entities": []}, {"text": "To isolate content from form diversity , we aim to generate response sets with a similar level of form diversity , but where the level of content diversity is controlled by the diversity parameter d.", "entities": []}, {"text": "Thus , we use crowdsourcing workers as testers , and a binary parameter d2 f0;1 g , corresponding to low or high content diversity .", "entities": []}, {"text": "A worker observes a context cand produces a set of responses Scbased on the value of d. We encourage workers to use different words and phrases in different responses regardless of the value of d , such that form diversity is high in all examples .", "entities": []}, {"text": "Examples from this data are in Figure 1 and Appendix B.", "entities": []}, {"text": "In \u00a7 6 , we will focus on whether automatic diversity metrics can perform as well as humans on the task of estimating content diversity .", "entities": []}, {"text": "4 Human Diversity Score One of the core questions we tackle is : Can humans evaluate diversity reliably ?", "entities": [[3, 4, "MetricName", "Score"]]}, {"text": "Although a few papers ( Ghandeharioun et al . , 2019 ; Yang et al . , 2019 ; Zhang et al . , 2019b ) asked humans to evaluate diversity , to the best of our knowledge no work thoroughly investigated this question .", "entities": []}, {"text": "The importance of this question is clear when comparing to quality evaluation .", "entities": []}, {"text": "There , human judgment is the gold standard , and automatic quality metrics are established by showing high correlation with human score .", "entities": []}, {"text": "Thus , understanding if humans can judge diversity is important for improving diversity metrics .", "entities": []}, {"text": "We use crowdsourcing workers2to compute a human diversity score : we show workers a context followed by a set of responses , and ask them to rate the diversity of the set .", "entities": []}, {"text": "To establish best practices , we experiment with multiple variations of HDS ( detailed in \u00a7 6.2 ) , asking humans to rate the diversity of a response set , and evaluating each practice with our framework .", "entities": []}, {"text": "We focus on the following questions : \u2022 Should humans rate diversity of a set or similarity between pairs in the set , from which diversity can be inferred ?", "entities": []}, {"text": "( tl;dr : diversity ) \u2022 Can humans evaluate different aspects of diversity well ?", "entities": []}, {"text": "( tl;dr : not effectively ) \u2022 Should humans rate the absolute diversity score of a set of sentences or rank whether one set is 2Native English speakers , for more details see Appendix A.", "entities": []}, {"text": "330more diverse than another ?", "entities": []}, {"text": "Here , we did not reach a conclusive result , and describe this experiment in the Appendix C.", "entities": []}, {"text": "As a preliminary step , we conducted pilot experiments among a group of NLP graduate students .", "entities": []}, {"text": "The main insights were : ( a ) humans are biased by quality : if a generated set has high diversity but low quality , humans will rate diversity low .", "entities": []}, {"text": "To neutralize this , we explicitly ask workers to evaluate the quality of one of the responses in the setSc , and then instruct them to ignore quality in diversity questions ; ( b ) To make sure a worker reads the context c , we ask them to generate a sentencesbefore they rate diversity ; ( c ) It is dif\ufb01cult for workers to evaluate the diversity of a set with more than 10 responses .", "entities": []}, {"text": "Our crowdsourcing tasks are provided in Appendix A. 5 Diversity to Similarity Reduction We expand the idea from Zhu", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2018 ) and suggest a method to construct a diversity metric from any 2 - sentence similarity metric .", "entities": []}, {"text": "Given msim(s1;s2)2R , a symmetric similarity metric that gets a pair of input sentences ( s1;s2)and returns a similarity score , we can de\ufb01ne a diversity metric ~mdivas the negation of the mean similarity score across all ( unordered ) pairs of Sc : ~mdiv(Sc ) = \u00001\u0000jScj 2\u0001X si;sj2Sc;i > jmsim(si;sj ):", "entities": []}, {"text": "This reduction allows us to easily de\ufb01ne new diversity metrics based on past work on sentence similarity ( Gomaa et al . , 2013 ; Devlin et al . , 2019 ; Zhang et al . , 2019a ; Reimers and Gurevych , 2019 ) .", "entities": []}, {"text": "In \u00a7 6 we show that both n - gram - based similarity metrics and neural semantic similarity metrics provide useful diversity metrics .", "entities": [[16, 18, "TaskName", "semantic similarity"]]}, {"text": "6 Experiments 6.1 NLG Tasks We apply our evaluation procedure on three different English NLG tasks that require diversity .", "entities": []}, {"text": "\u2022Story completion ( storyGen ) ; We use the ROC Stories dataset ( Mostafazadeh et al . , 2016 ) , in which the context cis the \ufb01rst four sentences of a story , and the response sis a single sentence that ends the story .", "entities": []}, {"text": "We use the contexts Cfrom this data and generate response sets Scfor each context using our testers .", "entities": []}, {"text": "The long contexts characterizing this data narrow down the space ofpossible responses , making this a \u201c low - entropy \u201d generation task , where the output is constrained , but diversity is still essential .", "entities": []}, {"text": "\u2022Dialog response generation ( respGen ) ; A comment - response pairs dataset extracted from the website reddit.com and pre - processed by Hashimoto et al .", "entities": [[1, 3, "TaskName", "response generation"]]}, {"text": "( 2019 ) .", "entities": []}, {"text": "We use the comments from their data as contexts Cand generate response setsScfor each context using our testers .", "entities": []}, {"text": "Since comments are single sentences the response is less constrained , making this a \u201c medium - entropy \u201d generation task .", "entities": []}, {"text": "\u20223 - words prompt completion ( promptGen ) ; ContextsCare 3 - words prompts , extracted from the Cornell Movie - Dialogs Corpus ( DanescuNiculescu - Mizil and Lee , 2011 ) by taking the \ufb01rst three words from each original context .", "entities": [[18, 23, "DatasetName", "Cornell Movie - Dialogs Corpus"]]}, {"text": "The response setsScare completions of the prompts , generated by our testers .", "entities": []}, {"text": "This context provides minimal constraints , making this a \u201c highentropy \u201d generation task .", "entities": []}, {"text": "Samples of the contexts extracted for each task , along with generated response sets , are presented in Appendix B.", "entities": []}, {"text": "We intentionally avoid NLG tasks where diversity is not necessarily desired , such as summarization and machine translation .", "entities": [[14, 15, "TaskName", "summarization"], [16, 18, "TaskName", "machine translation"]]}, {"text": "6.2 Evaluated Metrics N - gram - based metrics We evaluate distinct ngrams ( distinct - n ) , as described in \u00a7 2 .", "entities": []}, {"text": "We also evaluate n - grams cosine similarity ( cos - sim ): a similarity measure computing the cosine between the vectors representing two sentences , where each vector is a count vector over the n - grams that appear in the response .", "entities": []}, {"text": "We use the reduction from \u00a7 5 to convert this to a diversity measure .", "entities": []}, {"text": "In both metrics , rather than choosing the order of the ngrams , we average over n2f1 ; : : : ; 5 g , which we found to outperform any single choice of n. Neural metrics We exploit existing BERT - based models ( Devlin et al . , 2019 ) \ufb01ne - tuned for estimating similarity between two sentences ( applying the reduction from \u00a7 5 ) .", "entities": [[40, 41, "MethodName", "BERT"]]}, {"text": "BERT - STS ; A BERT model \ufb01ne - tuned on Semantic Textual Similarity ( Cer et al . , 2017 ): a collection of sentence pairs annotated with scores from 1 - 5 denoting their semantic similarity.3 BERT - Score ( Zhang et al . , 2019a ) ; Originally a quality metric , BERT - Score uses BERT \u2019s embeddings to measure similarity between two sen3https://github.com/swen128/bert-sts", "entities": [[0, 1, "MethodName", "BERT"], [2, 3, "TaskName", "STS"], [5, 6, "MethodName", "BERT"], [11, 14, "TaskName", "Semantic Textual Similarity"], [38, 39, "MethodName", "BERT"], [40, 41, "MetricName", "Score"], [55, 56, "MethodName", "BERT"], [57, 58, "MetricName", "Score"], [59, 60, "MethodName", "BERT"]]}, {"text": "331tences .", "entities": []}, {"text": "We used RoBERTa - large ( Liu et al . , 2019 ) , as suggested by the authors.4 Sentence - BERT ( sent - BERT ) ( Reimers and Gurevych , 2019 ) is a sentence - level embedding model based on BERT .", "entities": [[2, 3, "MethodName", "RoBERTa"], [21, 22, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"], [43, 44, "MethodName", "BERT"]]}, {"text": "We use the cosine similarity between the embeddings of two responses as a similarity metric .", "entities": []}, {"text": "In our experiments we used bert - large - nli - stsb - mean - tokens .5", "entities": []}, {"text": "Human Metrics We examine four methods for evaluating diversity with humans ( see \u00a7 4 ) , to investigate best practices for obtaining diversity judgment from humans .", "entities": []}, {"text": "In all metrics ( except ranking ) , ratings are from 5 ( highest diversity / similarity ) to 1 ( lowest ) .", "entities": []}, {"text": "The original tasks presented to workers are in Appendix A. Absolute HDS ( absHDS ) ; Given a context cand a set of generated responses Sc , rate the level of diversity ofSc .", "entities": []}, {"text": "Ranking HDS ( rnkHDS ) ; Given a context cand twosetsSc;d1;Sc;d2generated with different values of the diversity parameter d , rate which set is more diverse .", "entities": []}, {"text": "Since this metric did not clearly outperform absHDS , we provide results in Appendix C only .", "entities": []}, {"text": "Similarity HDS ( simHDS ) ; Given a context cand a set of generated responses Sc , rate the similarity of each two sentences in Sc , and then apply the reduction from \u00a7 5 .", "entities": []}, {"text": "Aspect HDS ( aspHDS ) ;", "entities": []}, {"text": "Identical to absHDS , except we explicitly ask about a speci\ufb01c aspect of diversity , namely form andcontent .6 6.3 Decoding Test", "entities": []}, {"text": "In decTest we measure the correlation between diversity metrics ( mdiv ) and the softmax temperature decoding parameter ( d ) .", "entities": [[14, 15, "MethodName", "softmax"]]}, {"text": "The tester generating the response sets ( Sc ) is a neural NLG model .", "entities": []}, {"text": "Data and settings For each task , we generated sets of 10responses per context , using a linear temperature sweep with 100 values in the range", "entities": []}, {"text": "[ 0:2;1:2](Caccia et al . , 2018 ) .", "entities": []}, {"text": "We generated 1 K sets in total for each of 1 K contexts ( 10per temperature ) and evaluated 200 ( 2random sets per temperature ) .", "entities": []}, {"text": "For automatic metrics , we repeat this 100 times ( randomly sampling 200out of 1 K sets each time ) , to present the mean and standard 4https://github.com/Tiiiger/bert_score 5https://github.com/UKPLab/ sentence - transformers 6We note that perplexity can not be evaluated as a diversity metric in our framework , because it requires a sample from Pref , while we assume a response set sampled from Pgen .", "entities": [[35, 36, "MetricName", "perplexity"]]}, {"text": "Context Fire next door .", "entities": []}, {"text": "John woke up smelling like something was burning .", "entities": []}, {"text": "He went outside .", "entities": []}, {"text": "He saw the \ufb01re next door .", "entities": []}, {"text": "He called the authorities .", "entities": []}, {"text": "Response set ( \u001c = 0:25 ) \u000fIt was a minor \ufb01re and they put it out .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "Response set ( \u001c = 0:8 )", "entities": []}, {"text": "\u000fThey arrived and put out the \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt turned out to be a \ufb01re .", "entities": []}, {"text": "\u000fIt was a minor \ufb01re night .", "entities": []}, {"text": "Response set ( \u001c = 1:1 ) \u000fIt turned out to be a mechanic .", "entities": []}, {"text": "\u000fBefore the \ufb01re was put out it was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fThey co - worker matter how bad the \ufb01re was .", "entities": []}, {"text": "\u000fSeveral shells , the \ufb01re department came just in time .", "entities": []}, {"text": "Table 1 : An example of the effect of temperature on the response set Scfor a context cfrom ROC Stories . deviation .", "entities": []}, {"text": "HDS metrics are computed over one experiment of 200sets , due to their high cost .", "entities": []}, {"text": "Data for storyGen andrespGen was generated by the MASS model ( Song et al . , 2019 ) , \ufb01ne - tuned on each dataset .", "entities": []}, {"text": "Data for promptGen was generated by GPT-2- large ( Radford et al . , 2019 ) without \ufb01ne - tuning .", "entities": []}, {"text": "We provide examples for how story endings change as a function of temperature in Table 1 .", "entities": []}, {"text": "Examples for all tasks along with additional reproducibility details are in the Appendix B. For each HDS metric , we collected 10 ratings per query from Amazon Mechanical Turk ( AMT ) workers .", "entities": []}, {"text": "While absHDS demands one query per response set , in order to perform simHDS at a reasonable cost , we chose jScj= 5 , resulting in\u00005 2\u0001 = 10 crowdsourcing queries instead of\u000010", "entities": []}, {"text": "2\u0001", "entities": []}, {"text": "= 45 per set .", "entities": []}, {"text": "We evaluate simHDS only for respGen due to the metric \u2019s high cost and low performance .", "entities": []}, {"text": "Results Table 2 presents results of absHDS , simHDS , and all automatic metrics .", "entities": []}, {"text": "In general , ngram based metrics capture the diversity induced by a temperature sweep , beating HDS and neural metrics .", "entities": []}, {"text": "Figure 3 provides a more detailed analysis .", "entities": []}, {"text": "Each point represents a single set of responses generated at some temperature .", "entities": []}, {"text": "While rank correlation for cosine similarity is high , it is", "entities": []}, {"text": "332 0.2 0.4 0.6 0.8 1.0 1.2 Temperature0.00.20.40.60.81.0Metric Score Cosine Similarity 0.2", "entities": [[8, 9, "MetricName", "Score"]]}, {"text": "0.4 0.6 0.8 1.0 1.2 Temperature0.00.10.20.30.4 BERT - STS 0.2 0.4 0.6 0.8 1.0 1.2 Temperature12345 absHDSFigure 3 : decTest : Scatter plot of n - gram - based ( cosine similarity ) , neural ( BERT - STS ) and human ( absHDS ) metrics as a function of temperature for respGen .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "TaskName", "STS"], [36, 37, "MethodName", "BERT"], [38, 39, "TaskName", "STS"]]}, {"text": "Each point corresponds to a single generated set .", "entities": []}, {"text": "Error bars of HDS represent the standard deviation over 10 annotator ratings . storyGen", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "respGen promptGen", "entities": []}, {"text": "distinct - n 0.76 ( 0.03 ) 0.89 ( 0.01 ) 0.91 ( 0.01 ) cos - sim 0.71 ( 0.04 ) 0.89 ( 0.01 ) 0.87 ( 0.02 ) BERT - STS 0.64 ( 0.04 ) 0.81 ( 0.02 ) 0.84 ( 0.02 ) sent - BERT 0.65 ( 0.03 ) 0.80 ( 0.02 ) 0.74 ( 0.03 )", "entities": [[30, 31, "MethodName", "BERT"], [32, 33, "TaskName", "STS"], [47, 48, "MethodName", "BERT"]]}, {"text": "BERT - score 0.69 ( 0.04 ) 0.87 ( 0.01 ) 0.88 ( 0.02 ) absHDS 0.69 0.81 0.79 simHDS - 0.74 Table 2 : decTest results : Spearman \u2019s \u001acorrelation between temperature and each metric score ( mean and standard deviation ) .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "simHDS was tested only on respGen .", "entities": []}, {"text": "far from linear and reaches high values even at low temperatures , scoring 0:6Pearson correlation .", "entities": []}, {"text": "Conversely , the correlation for BERT - STS and absHDS is more linear , scoring 0:75and0:77Pearson correlation respectively .", "entities": [[5, 6, "MethodName", "BERT"], [7, 8, "TaskName", "STS"]]}, {"text": "Thus , Pearson and Spearman correlations disagree on the quality of the different metrics in this case .", "entities": []}, {"text": "While our framework is meant to evaluate diversity metrics , the results of the test let us re\ufb02ect on the decoding parameters themselves .", "entities": []}, {"text": "This result shows that humans perform worse than automatic metrics in this experimental setup , hinting that temperature mostly controls super\ufb01cial changes to the generated text .", "entities": []}, {"text": "Additionally , simHDS performs worse than absHDS although it is 3x more expensive , showing that rating the entire set rather than averaging over pairs is useful .", "entities": []}, {"text": "Other decoding parameters To compare the robustness of our conclusions to other decoding parameters , we repeat it with two additional decoding methods : ( a ) in Nucleus ( Top- p ) sampling we swept linearly over 100 values of pin the range", "entities": []}, {"text": "[ 0:1;1:0 ] ; ( b ) In Top - ksampling we swept kin logarithmic scale over 100 values in the range [ 1;30K]and present the correlation between theTemperature Top - p Top - k distinct - n 0.91 ( 0.01 ) 0.84 ( 0.02 ) 0.61 ( 0.05 ) cos - sim 0.87 ( 0.02 ) 0.78 ( 0.03 ) 0.48 ( 0.05 ) BERT - STS 0.84 ( 0.02 ) 0.74 ( 0.03 ) 0.55 ( 0.05 ) sent - BERT 0.74 ( 0.03 ) 0.63 ( 0.05 ) 0.51 ( 0.05 ) BERT - score 0.88 ( 0.02 ) 0.77 ( 0.03 ) 0.57 ( 0.05 ) Table 3 : decTest results for different decoding parameters : Spearman \u2019s \u001a(mean and standard deviation ) of automatic metrics for promptGen .", "entities": [[65, 66, "MethodName", "BERT"], [67, 68, "TaskName", "STS"], [82, 83, "MethodName", "BERT"], [95, 96, "MethodName", "BERT"]]}, {"text": "metrics and log10(k )", "entities": []}, {"text": ".", "entities": []}, {"text": "While softmax temperature enables skewing PLMto a more diverse Pgenusing \u001c  > 1 , both Top - pand Top - kenable only skewing PLMto a more sharp ( hence less diverse ) Pgen .", "entities": [[1, 2, "MethodName", "softmax"]]}, {"text": "Table 3 presents results for all automatic metrics using the three decoding methods over promptGen .", "entities": []}, {"text": "Results for other tasks are in Appendix C.", "entities": []}, {"text": "We \ufb01nd that Top- pcorrelates well with temperature along all three generation tasks , whereas Topkdoes not correlate with any of them .", "entities": []}, {"text": "6.4 Content Test In conTest , we measure the correlation between diversity metrics ( mdiv ) and content diversity , represented by a binary parameter", "entities": []}, {"text": "d2f0;1 g. The testers are AMT workers , guided to create sets with high level of form diversity and high or low content diversity according to d. Data and settings For each task , we collected 200 sets of 5 responses each ( 100 sets per class ) .", "entities": []}, {"text": "For high content diversity class , we asked workers to give 5 responses per context , with as different content and structure as possible .", "entities": []}, {"text": "Then we asked the same workers to choose a single response they wrote , and rephrase it 5 times such that the original content will be preserved , while changing the form \u2013 this set is used for the low content diversity class .", "entities": []}, {"text": "333 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Metric Valuesdistinct - n ( averaged ) high content diversity low content diversity 0.4 0.6 0.8 1.0 Metric ValuesBERT Score 2.5 3.0 3.5 4.0 4.5 Metric ValuesabsHDSFigure 4 : conTest : histograms of metric values of n - gram ( distinct n - grams ) , neural ( BERT - Score ) and human ( absHDS ) metrics for promptGen .", "entities": [[27, 28, "MetricName", "Score"], [56, 57, "MethodName", "BERT"], [58, 59, "MetricName", "Score"]]}, {"text": "The orange histogram represents the distribution of the low content diversity class , theblue histogram represents the distribution of the high content diversity class andbrown is the intersection between the two .", "entities": []}, {"text": "Pointing down triangles represent the threshold \u0011of the optimal classi\ufb01ers .", "entities": []}, {"text": "The histograms show how each metric separates the two classes .", "entities": []}, {"text": "A sample from this data is in Figure 1 and more samples in Appendix B. For each HDS metric , we collected 10 ratings from crowdsourcing workers , different than the ones who composed the sets .", "entities": []}, {"text": "Results", "entities": []}, {"text": "In addition to Spearman \u2019s \u001a , we report the optimal single - threshold classi\ufb01er accuracy ( OCA ) , i.e. , the best achievable accuracy in predicting the class of a response set ( high or low content diversity ) for any threshold \u0011onmdiv , such that ifmdiv(Sc ) > \u0011", "entities": [[15, 16, "MetricName", "accuracy"], [25, 26, "MetricName", "accuracy"]]}, {"text": "the classi\ufb01er predicts high diversity , and otherwise predicts low diversity .", "entities": []}, {"text": "Table 4 shows the results .", "entities": []}, {"text": "N - gram - based metrics perform poorly , indicating they do not measure content diversity well .", "entities": []}, {"text": "Neural models perform better than n - gram - based metrics ( especially sent - BERT ) , but there is still a clear gap between automatic metrics and humans .", "entities": [[15, 16, "MethodName", "BERT"]]}, {"text": "Figure 4 illustrates the typical distributions of n - gram , neural and human metrics .", "entities": []}, {"text": "Clearly , HDS separates high and low content diversity better than neural metrics .", "entities": []}, {"text": "In addition , n - gram - based metrics saturate both classes to near maximal values , similarly to decTest .", "entities": []}, {"text": "Since conTest isolates content diversity , we used aspHDS to directly rate content and form diversity .", "entities": []}, {"text": "Content aspHDS gets similar scores to absHDS , suggesting little gain in asking directly on the tested aspect .", "entities": []}, {"text": "Form aspHDS gets low scores compared to absHDS , validating that the form diversity of the two classes is similar .", "entities": []}, {"text": "Content Diversity Benchmark We construct theMetrics for content Diversity ( McDiv ) benchmark , focusing on metrics for content diversity .", "entities": []}, {"text": "McDiv is a dataset containing 6Kfc;Scgpairs , ( 2Kfor each storyGen , respGen and promptGen ) collected as described in this section .", "entities": []}, {"text": "Mc - storyGen respGen promptGen", "entities": []}, {"text": "\u001a OCA\u001a OCA\u001a OCA distinct - n 0.57 0.77 0.34 0.67 0.33 0.68 cos - sim 0.56 0.77 0.33 0.66 0.36 0.67 BERT - STS 0.6 0.78 0.46 0.72 0.65 0.82 sent - BERT 0.77 0.90 0.59 0.79 0.68 0.81 BERT - score 0.59 0.77 0.49 0.74 0.4 0.69 absHDS 0.85 0.95 0.63 0.81 0.78 0.89 aspHDS form 0.35 0.65 0.56 0.79 0.4 0.68 aspHDS content 0.84 0.94 0.67 0.83 0.75 0.88 Table 4 : conTest results : Spearman \u2019s ( \u001a ) correlation between a set \u2019s class and each metric score .", "entities": [[22, 23, "MethodName", "BERT"], [24, 25, "TaskName", "STS"], [33, 34, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"]]}, {"text": "Div contains a subset of 3Kexamples , termed McDiv nuggets , in which form diversity was neutralized , providing a dif\ufb01cult meta - evaluation challenge .", "entities": []}, {"text": "McDiv nuggets was sampled to ensure that the correlation of distinct - n ( a form diversity metric ) is zero over this subset .", "entities": []}, {"text": "Applying conTest over the data shows that n - gram based metrics obtain near - zero values on McDiv nuggets as expected , and all neural metrics perform substantially worse on McDiv nuggets than on McDiv .", "entities": []}, {"text": "On conTest , we obtain absHDS annotations for more than 200 random samples from McDiv nuggets and obtain 0.7 Spearman \u2019s \u001afor the respGen task , substantially higher than the best performing neural metric ( sent - BERT ) score at 0.6 .", "entities": [[37, 38, "MethodName", "BERT"]]}, {"text": "Details and conTest results can be found in Appendix C. HDS Stability :", "entities": []}, {"text": "Picking Parameter Values HDS experiments demand expensive human labor .", "entities": []}, {"text": "Thus , we need to carefully choose the number of sets and different ratings we ask per set , to get reliable results in a reasonable budget .", "entities": []}, {"text": "To this end , we conducted two series of experiments , once increasing the number of sets , and again increasing the number of ratings per sets .", "entities": []}, {"text": "By observing results along those two series , we chose to use 200", "entities": []}, {"text": "334 2 4 6 8 10 # ratings per set0.50.60.70.80.91.0Spearman 's   promptGen respGen", "entities": []}, {"text": "storyGen 50 100 150 200 # sets0.50.60.70.80.91.0Figure 5 : conTest absHDS results depends on the number of ratings per set and the number of sets .", "entities": []}, {"text": "sets and 10 ratings per set for all experiments the minimal values in which results are con\ufb01dently stable .", "entities": []}, {"text": "Results are presented in Figure 5 . 7 Aspects of Diversity In this work , we focused on the two primary aspects of diversity : content diversity ( What to say ? )", "entities": []}, {"text": "andform diversity ( How to say it ? ) .", "entities": []}, {"text": "In Figure 1 , Both sets are diverse , but Set B is only form diverse , as all answers deliver the same massage , whereas Set A is diverse in both form and content .", "entities": []}, {"text": "Furthermore , we can observe aspects of diversity as having a tree - like structure , where both content and form diversity can be divided to subaspects : Content diversity ( e.g. answering the question \u201c How are you today ? \u201d ) can be expressed by using different sentiment ( \u201c I \u2019m doing good . \u201d", "entities": []}, {"text": "vs. \u201c I \u2019m so glad you asked !", "entities": []}, {"text": "I \u2019m really doing good . \u201d", "entities": []}, {"text": ") , different relevance ( \u201c I \u2019m \ufb01ne \u201d vs. \u201cDid you watch the game last night ? \u201d ) , and more .", "entities": []}, {"text": "Form diversity can be divided into sub - aspects as well : syntactic diversity ( \u201c Someone took it from me . \u201d", "entities": []}, {"text": "vs. \u201cIt was taken from me . \u201d )", "entities": []}, {"text": "orlexical diversity ( \u201c I feel \ufb01ne . \u201d", "entities": []}, {"text": "vs. \u201cI feel very well . \u201d ) .", "entities": []}, {"text": "Even those sub - aspects can be further divided .", "entities": []}, {"text": "For example , a sub - aspect of lexical diversity is register diversity ( \u201c How are you ? \u201d", "entities": []}, {"text": "vs. \u201cSup bro ? \u201d ) .", "entities": []}, {"text": "Another observation is that different aspects are not orthogonal , that is , changing one aspect may lead to changes in other aspects .", "entities": []}, {"text": "Speci\ufb01cally , we observe that while it is relatively easy to produce high form diversity with low content diversity ( Set Bin Figure 1 ) , it is almost impossible to diversify content without changing form .", "entities": []}, {"text": "This observation was important during the design of conTest .", "entities": []}, {"text": "8 Conclusions This work presents a framework for evaluating diversity metrics as a step toward standardized evaluation .", "entities": []}, {"text": "We limit the scope of this work to differ - ences between form andcontent diversity , which are key towards understanding different aspects of diversity .", "entities": []}, {"text": "Future work can explore other aspects of diversity , e.g. testing sentiment diversity , as proposed in \u00a7 3 .", "entities": []}, {"text": "We urge researchers to use this framework as a platform for developing new diversity metrics and establishing their ef\ufb01ciency .", "entities": []}, {"text": "Acknowledgements We thank Aya Meltzer - Asscher for linguistic advice , and Or Nachmias , Ben Bogin , Mor Geva , Omer Goldman and Ohad Rubin for their useful suggestions and references .", "entities": []}, {"text": "This research was partially supported by The Israel Science Foundation grant 942/16 , The Yandex Initiative for Machine Learning and the European Research Council ( ERC ) under the European Union Horizons 2020 research and innovation programme ( grant ERC DELPHI 802800 ) .", "entities": []}, {"text": "References David H Ackley , Geoffrey E Hinton , and Terrence J Sejnowski .", "entities": []}, {"text": "1985 .", "entities": []}, {"text": "A learning algorithm for boltzmann machines .", "entities": []}, {"text": "Cognitive science , 9(1):147\u2013169 .", "entities": []}, {"text": "Daniel Adiwardana , Minh - Thang Luong , David R", "entities": []}, {"text": "So , Jamie Hall , Noah Fiedel , Romal Thoppilan , Zi Yang , Apoorv Kulshreshtha , Gaurav Nemade , Yifeng Lu , et al . 2020 .", "entities": []}, {"text": "Towards a human - like opendomain chatbot .", "entities": [[6, 7, "TaskName", "chatbot"]]}, {"text": "arXiv preprint arXiv:2001.09977 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Massimo Caccia , Lucas Caccia , William Fedus , Hugo Larochelle , Joelle Pineau , and Laurent Charlin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Language gans falling short .", "entities": []}, {"text": "arXiv preprint arXiv:1811.02549 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Daniel Cer , Mona Diab , Eneko Agirre , I \u02dcnigo LopezGazpio , and Lucia Specia .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Semeval-2017 task 1 : Semantic textual similarity multilingual and crosslingual focused evaluation .", "entities": [[4, 7, "TaskName", "Semantic textual similarity"]]}, {"text": "In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) , pages 1\u201314 .", "entities": []}, {"text": "Cristian Danescu - Niculescu - Mizil and Lillian Lee . 2011 .", "entities": []}, {"text": "Chameleons in imagined conversations : A new approach to understanding coordination of linguistic style in dialogs .", "entities": []}, {"text": "In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics , ACL 2011 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "335Wenchao Du and Alan W Black .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Boosting dialog response generation .", "entities": [[2, 4, "TaskName", "response generation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 38\u201343 .", "entities": []}, {"text": "Ond\u02c7rej Du \u02c7sek , Jekaterina Novikova , and Verena Rieser .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Evaluating the state - of - the - art of end - to - end natural language generation : The e2e nlg challenge .", "entities": [[20, 23, "DatasetName", "e2e nlg challenge"]]}, {"text": "Computer Speech & Language , 59:123\u2013156 .", "entities": []}, {"text": "Angela Fan , Yacine Jernite , Ethan Perez , David Grangier , Jason Weston , and Michael Auli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Eli5 : Long form question answering .", "entities": [[0, 1, "DatasetName", "Eli5"], [2, 6, "TaskName", "Long form question answering"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3558\u20133567 .", "entities": []}, {"text": "Angela Fan , Mike Lewis , and Yann Dauphin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Hierarchical neural story generation .", "entities": [[2, 4, "TaskName", "story generation"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 889\u2013898 .", "entities": []}, {"text": "Asma Ghandeharioun , Judy Hanwen Shen , Natasha Jaques , Craig Ferguson , Noah Jones , Agata Lapedriza , and Rosalind Picard .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Approximating interactive human evaluation with self - play for open - domain dialog systems .", "entities": [[9, 13, "TaskName", "open - domain dialog"]]}, {"text": "In Advances in Neural Information Processing Systems , pages 13658 \u2013 13669 .", "entities": []}, {"text": "Wael H Gomaa , Aly A Fahmy , et al . 2013 .", "entities": []}, {"text": "A survey of text similarity approaches .", "entities": [[3, 5, "TaskName", "text similarity"]]}, {"text": "International Journal of Computer Applications , 68(13):13\u201318 .", "entities": []}, {"text": "Tatsunori Hashimoto , Hugh Zhang , and Percy Liang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unifying human and statistical evaluation for natural language generation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1689\u20131701 .", "entities": []}, {"text": "Ari Holtzman , Jan Buys , Maxwell Forbes , and Yejin Choi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The curious case of neural text degeneration .", "entities": []}, {"text": "arXiv preprint arXiv:1904.09751 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yi - An Lai , Xuan Zhu , Yi Zhang , and Mona Diab .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Diversity , density , and homogeneity : Quantitative characteristic metrics for text collections .", "entities": []}, {"text": "arXiv preprint arXiv:2003.08529 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mike Lewis , Denis Yarats , Yann Dauphin , Devi Parikh , and Dhruv Batra . 2017 .", "entities": []}, {"text": "Deal or no deal ?", "entities": []}, {"text": "end - to - end learning of negotiation dialogues .", "entities": []}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2443\u20132453 .", "entities": []}, {"text": "Jiwei Li , Michel Galley , Chris Brockett , Jianfeng Gao , and Bill Dolan .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "A diversity - promoting objective function for neural conversation models .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 110\u2013119.Jiwei Li , Michel Galley , Chris Brockett , Jianfeng Gao , and Bill Dolan .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "A diversity - promoting objective function for neural conversation models .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 110\u2013119 , San Diego , California . Association for Computational Linguistics .", "entities": []}, {"text": "Jiwei Li , Will Monroe , and Dan Jurafsky .", "entities": []}, {"text": "2016c .", "entities": []}, {"text": "A simple , fast diverse decoding algorithm for neural generation .", "entities": []}, {"text": "arXiv preprint arXiv:1611.08562 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Junyi Li , Wayne Xin Zhao , Ji - Rong Wen , and Yang Song .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Generating long and informative reviews with aspect - aware coarse - to-\ufb01ne decoding .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1969 \u2013 1979 .", "entities": []}, {"text": "Zhongyang Li , Xiao Ding , and Ting Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Generating reasonable and diversi\ufb01ed story ending using sequence to sequence model with adversarial training .", "entities": [[7, 10, "MethodName", "sequence to sequence"]]}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 1033 \u2013 1043 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Christopher D Manning , Christopher D Manning , and Hinrich Sch \u00a8utze . 1999 .", "entities": []}, {"text": "Foundations of statistical natural language processing .", "entities": []}, {"text": "MIT press .", "entities": []}, {"text": "Nasrin Mostafazadeh , Nathanael Chambers , Xiaodong He , Devi Parikh , Dhruv Batra , Lucy Vanderwende , Pushmeet Kohli , and James Allen . 2016 .", "entities": []}, {"text": "A corpus and cloze evaluation for deeper understanding of commonsense stories .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 839\u2013849 , San Diego , California .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Liangming Pan , Wenqiang Lei , Tat - Seng Chua , and Min - Yen Kan. 2019 .", "entities": []}, {"text": "Recent advances in neural question generation .", "entities": [[4, 6, "TaskName", "question generation"]]}, {"text": "arXiv preprint arXiv:1905.08949 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th annual meeting on association for computational linguistics , pages 311\u2013318 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "336Nils Reimers and Iryna Gurevych . 2019 .", "entities": []}, {"text": "Sentencebert : Sentence embeddings using siamese bertnetworks .", "entities": [[2, 4, "TaskName", "Sentence embeddings"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3973\u20133983 .", "entities": []}, {"text": "Manasvi Sagarkar , John Wieting , Lifu Tu , and Kevin Gimpel .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Quality signals in generated stories .", "entities": []}, {"text": "InProceedings of the Seventh Joint Conference on Lexical and Computational Semantics , pages 192 \u2013 202 . Raphael Shu , Hideki Nakayama , and Kyunghyun Cho . 2019 .", "entities": []}, {"text": "Generating diverse translations with sentence codes .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1823\u20131827 .", "entities": []}, {"text": "Kaitao Song , Xu Tan , Tao Qin , Jianfeng Lu , and TieYan Liu . 2019 .", "entities": []}, {"text": "Mass :", "entities": []}, {"text": "Masked sequence to sequence pre - training for language generation .", "entities": [[1, 4, "MethodName", "sequence to sequence"]]}, {"text": "In International Conference on Machine Learning , pages 5926\u20135936 .", "entities": []}, {"text": "Guy Tevet , Gavriel Habib , Vered Shwartz , and Jonathan Berant .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Evaluating text gans as language models .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 2241\u20132247 .", "entities": []}, {"text": "Qingyun Wang , Lifu Huang , Zhiying Jiang , Kevin Knight , Heng Ji , Mohit Bansal , and Yi Luan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Paperrobot : Incremental draft generation of scienti\ufb01c ideas .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1980\u20131991 .", "entities": []}, {"text": "Pengcheng Yang , Lei Li , Fuli Luo , Tianyu Liu , and Xu Sun . 2019 .", "entities": []}, {"text": "Enhancing topic - to - essay generation with external commonsense knowledge .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2002 \u2013 2012 .", "entities": []}, {"text": "Lantao Yu , Weinan Zhang , Jun Wang , and Yong Yu . 2017 .", "entities": []}, {"text": "Seqgan : Sequence generative adversarial nets with policy gradient .", "entities": []}, {"text": "In Thirty - First AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Tianyi Zhang , Varsha Kishore , Felix Wu , Kilian Q Weinberger , and Yoav Artzi . 2019a .", "entities": []}, {"text": "Bertscore : Evaluating text generation with bert .", "entities": [[3, 5, "TaskName", "text generation"]]}, {"text": "arXiv preprint arXiv:1904.09675 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xinyuan Zhang , Yi Yang , Siyang Yuan , Dinghan Shen , and Lawrence Carin . 2019b .", "entities": []}, {"text": "Syntax - infused variational autoencoder for text generation .", "entities": [[3, 5, "MethodName", "variational autoencoder"], [6, 8, "TaskName", "text generation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2069\u20132078.Yaoming Zhu , Sidi Lu , Lei Zheng , Jiaxian Guo , Weinan Zhang , Jun Wang , and Yong Yu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Texygen :", "entities": []}, {"text": "A benchmarking platform for text generation models .", "entities": [[4, 6, "TaskName", "text generation"]]}, {"text": "In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval , pages 1097\u20131100 .", "entities": [[4, 5, "DatasetName", "ACM"], [12, 14, "TaskName", "Information Retrieval"]]}, {"text": "337A HDS Questionnaires All Human scores for HDS metrics were collected using Amazon Mechanical Turk ( AMT ) crowdsourcing platform by English native - speaking workers that were speci\ufb01cally quali\ufb01ed for this task .", "entities": []}, {"text": "Figure 7 presents the warm - up part , common for all HDS questionnaires .", "entities": []}, {"text": "Before asking workers to rate the diversity of each set , we \ufb01rst asked them to generate a response for the context themselves , to make sure they read it .", "entities": []}, {"text": "To neutralize the effect of the responses \u2019 quality on the workers , we also asked the workers to rate the quality of the \ufb01rst response in the set , then explicitly instructed them to ignore quality when rating diversity .", "entities": []}, {"text": "Figures 8 to 11 present the diversity questions of absHDS , aspHDS , rnkHDS and simHDS as appeared in the AMT questionnaires .", "entities": []}, {"text": "Costs For HDS metrics that require one query per response set ( i.e. absHDS , rnkHDS , aspDHS ) , the cost for a single rating was 0:18$.", "entities": []}, {"text": "We collected 10ratings per response set , and conduct each experiment with 200sets , hence the total cost for an experiment was 360 $ .", "entities": []}, {"text": "In the case of simHDS , the response set size was 5 , and the number of queries needed per set is\u00005 2\u0001", "entities": []}, {"text": "= 10 .", "entities": []}, {"text": "The cost of a single rating for this task was 0.056 $ , and with the same multipliers , the total cost for an experiment was 1120 $ , three times more expensive .", "entities": []}, {"text": "B Data Samples B.1 Decoding Test ( decTest ) Tables 11 to 19 present data samples from storyGen , respGen and promptGen with the neural testers of decTest , as detailed in \u00a7 6 .", "entities": []}, {"text": "Each table presents two contexts and three response sets per context .", "entities": []}, {"text": "Each response set was generated with a different value of decoding parameter for the three decoding methods : softmax temperature , Nucleus sampling , and Top - k. B.2 Content Test ( conTest ) Tables 20 to 22 present data samples from storyGen , respGen and promptGen with the human testers of conTest , as detailed in \u00a7 6 .", "entities": [[18, 19, "MethodName", "softmax"]]}, {"text": "Each table presents two contexts and two response sets per context - one for the lowcontent diversity class and one for the high content diversity class .", "entities": []}, {"text": "C Additional Experiments C.1 Decoding Test ( decTest ) Comparing decTest results of storyGen to other tasks ( Table 2 ) , this task is characterised with noisier scores for all metrics ( Figures 3 and 6 ) , hence lower\u001avalues and higher variance .", "entities": []}, {"text": "A possible explanation is larger effect of con the distribution Pgen(sjc)in this task .", "entities": []}, {"text": "Tables 3 , 6 and 7 , present decTest absolute scoring experiment using temperature , nucleus sampling andTop - k decoding parameters as d. Top - k consistently yields lower \u001acompared to other decoding parameters , especially for storyGen task .", "entities": []}, {"text": "This implies that Top - k represents diversity less reliably than other methods .", "entities": []}, {"text": "Ranking experiment To examine whether we can improve correlation by asking humans to rank diversity , rather than providing an absolute score , we designed a ranking version of decTest .", "entities": []}, {"text": "Each context is given along with two sets ( 5 samples each ) , produced with different temperature values .", "entities": []}, {"text": "We sweep over temperature differences instead of the absolute temperature values .", "entities": []}, {"text": "The human metric in this setting is rnkHDS ( see \u00a7 6.2 ) , and the automatic metrics are the difference between the scores each of the two sets got .", "entities": []}, {"text": "We report two measures ; The \ufb01rst is Spearman \u2019s \u001abetween the metric and the temperature difference .", "entities": []}, {"text": "The second is accuracy , i.e. , whether the metric can predict which set has higher temperature ( e.g. , in automatic metrics this is whether the sign of the temperature difference and the sign of metric score difference agree).7 Table 5 summarizes the ranking test results .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "We observe that humans are better at ranking compared to giving absolute scores ( Table 2 ) , and are doing as well as automatic metrics .", "entities": []}, {"text": "However , the scores of all automatic metrics also improve , making it dif\ufb01cult to separate between the different metrics .", "entities": []}, {"text": "C.2 Metrics for Content Diversity ( McDiv ) As elaborated in \u00a7 6.4 , McDiv is a dataset containing6Kfc;Scgpairs , ( 2Kfor each storyGen , respGen and promptGen ) collected as described in \u00a7 6.4 .", "entities": []}, {"text": "McDiv nuggets is a3Ksubset of McDiv , in which form diversity is neutralized , providing a dif\ufb01cult meta - evaluation challenge .", "entities": []}, {"text": "McDiv nuggets was sampled in a manner that causing distinct - n 7We consider ties in the metric difference score as a miss .", "entities": []}, {"text": "338 0.2 0.4 0.6 0.8 1.0 1.2 Temperature0.00.20.40.60.81.0Metric Score Cosine Similarity 0.2", "entities": [[8, 9, "MetricName", "Score"]]}, {"text": "0.4 0.6 0.8 1.0 1.2 Temperature0.00.10.20.30.4 BERT - STS 0.2 0.4 0.6 0.8 1.0 1.2 Temperature012345 absHDSFigure 6 : decTest : Scatter plot of n - gram - based ( cosine similarity ) , neural ( BERT - STS ) and human ( absHDS ) metrics as a function of temperature for storyGen .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "TaskName", "STS"], [36, 37, "MethodName", "BERT"], [38, 39, "TaskName", "STS"]]}, {"text": "Each point corresponds to a single generated set .", "entities": []}, {"text": "Error bars of HDS represent the standard deviation over 10 annotator ratings .", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "storyGen", "entities": []}, {"text": "respGen promptGen", "entities": []}, {"text": "\u001a acc\u001a acc\u001a acc distinct - n 0.88 0.88 0.86 0.9 0.91 0.91 cos - sim", "entities": [[3, 4, "MetricName", "acc"]]}, {"text": "0.86 0.88 0.87 0.91 0.9 0.91 BERT - STS 0.84 0.84 0.85 0.88 0.9 0.89 sent - BERT 0.85 0.86 0.83 0.85 0.85 0.85 BERT - score", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "TaskName", "STS"], [17, 18, "MethodName", "BERT"], [24, 25, "MethodName", "BERT"]]}, {"text": "0.88 0.89 0.88 0.89 0.91 0.9 rnkHDS 0.87 0.89 0.89 0.9 0.89 0.88 Table 5 : decTest ranking results : Spearman \u2019s ( \u001a ) correlation between temperature differences and each metric score .", "entities": []}, {"text": "Accuracy ( acc ) of classifying which set has the higher temperature .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "acc"]]}, {"text": "Standard deviation is up to 0:02for all automatic metrics for both Spearman \u2019s correlation and accuracy .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "metric to score zero correlation in conTest over this subset .", "entities": []}, {"text": "The method of sub - sampling was meant to approximately equalize the distributions of the two classes , lowandhigh content diversity , over the scores of distinct - n metric , and was performed as follows : \u2022 Sort all collected samples ( from both lowand high content diversity classes ) according to their distinct - n score .", "entities": []}, {"text": "\u2022 Divide the sorted samples to groups with \ufb01xed size ( 40samples each in our case ) .", "entities": []}, {"text": "\u2022 From each such group , randomly sample the same amount of samples for each of the two classes .", "entities": []}, {"text": "For example , if a group contains 5lowcontent diversity samples and 35high content diversity samples , we can sample at most 5samples for each class .", "entities": []}, {"text": "Resutls We applied conTest for all the collected data for each of the three NLG tasks ( see Tables 8 and 9 ) .", "entities": []}, {"text": "By design , n - gram based metrics score near - zero correlation on McDiv nuggets , makinghigh andlowcontent diversity classes almostindistinguishable for those metrics , which relay on text surface level features only .", "entities": []}, {"text": "Neural metrics perform strictly worse on McDiv nuggets than McDiv .", "entities": []}, {"text": "In addition , we applied conTest on 200 randomly sampled fc;Scgpairs from McDiv nuggets for respGen task ( see table 10 ) .", "entities": []}, {"text": "Compared to Table 4 , The gap between the best performing neural metrics ( sent - BERT ) and absHDS was increased in favor to HDS ( 0.04 compared to 0.1 difference in Spearman \u2019s \u001a ) .", "entities": [[16, 17, "MethodName", "BERT"]]}, {"text": "D Additional Reproducibility Details Collected data and code All the collected data , metric scores per samples for each of decTest and conTest , as well as code for running and visualizing the tests , are publicly available8 .", "entities": []}, {"text": "The collection methods are elaborated in Section 6 .", "entities": []}, {"text": "Original data", "entities": []}, {"text": "We provide additional data for the original three datasets used in Section 6 . \u2022", "entities": []}, {"text": "ROC Stories dataset9(Mostafazadeh et al . , 2016 ) used for storyGen task contains 96K/1K/1Ktrain / validation / test titles and \ufb01ve - sentence stories .", "entities": []}, {"text": "We used the samples without pre - processing for both \ufb01ne - tuning MASS model and generate samples for our tests .", "entities": []}, {"text": "\u2022 Reddit comment - response dataset used for respGen task contains 37M/1M/1 M train / validation / test comment - response pairs , extracted from the social website reddit.com scraped by pushshift.io followed by the pre - process described in 8https://github.com/GuyTevet/ diversity - eval 9www.cs.rochester.edu/nlp/rocstories/", "entities": [[1, 2, "DatasetName", "Reddit"]]}, {"text": "339(Hashimoto et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "We used the samples without further processing for both \ufb01ne - tuning MASS model and generate samples for our tests .", "entities": []}, {"text": "To the best of our knowledge , this dataset is not publicly available at the moment .", "entities": []}, {"text": "\u2022 CMDC dataset10(Danescu - NiculescuMizil and Lee , 2011 ) contains 108K/30 K train / test sentence - response pairs extracted from movie scripts .", "entities": []}, {"text": "We extracted the \ufb01rst three words from the sentences ( used as contexts for the original task ) to be the context of our task .", "entities": []}, {"text": "We did not use this data for training since we used GPT-2 without \ufb01ne - tuning for promptGen .", "entities": [[11, 12, "MethodName", "GPT-2"]]}, {"text": "Auto - generated data For decTest , we used two pre - trained generative models for generating responses given the contexts : \u2022 For storyGen and respGen tasks , we used MASS11(Song et al . , 2019 ) ( 6L-1024H-8A architecture suggested by the authors ) , pretraind as described in the original paper .", "entities": []}, {"text": "For each task separately , we \ufb01ne - tuned MASS using the training division of the dataset corresponding to the task .", "entities": []}, {"text": "Fine - tuning was done using 200Kexamples over 30epochs , and took23hours using a single TITAN Xp GPU core .", "entities": [[15, 16, "DatasetName", "TITAN"]]}, {"text": "Inference with the \ufb01ne - tuned model takes 65milliseconds on average per response set containing 10responses with the same GPU core .", "entities": []}, {"text": "\u2022 For promptGen task , we used Hugging - Face implementation12of GPT-2 large ( 36 - layer , 1280 - hidden , 20 - heads , 774 M parameters )", "entities": [[11, 12, "MethodName", "GPT-2"]]}, {"text": "( Radford et al . , 2019 ) pre - traind as described in the original paper .", "entities": []}, {"text": "We used this model as - is , without \ufb01ne - tuning .", "entities": []}, {"text": "Inference takes 0:6second on average per response set containing 10responses with a single TITAN Xp GPU core .", "entities": [[13, 14, "DatasetName", "TITAN"]]}, {"text": "Tests Runtime Given metric scores per sample , running each of the tests with 200 samples takes less than a minute on a standard Intel i7 CPU .", "entities": []}, {"text": "10www.cs.cornell.edu/ \u02dccristian / Cornell _ Movie-Dialogs_Corpus.html 11github.com/microsoft/MASS 12github.com/huggingface/transformersTemperature Top - p Top - k distinct - n 0.76 ( 0.03 ) 0.69 ( 0.03 ) 0.2 ( 0.06 )", "entities": [[3, 4, "DatasetName", "Cornell"]]}, {"text": "cos - sim 0.71 ( 0.04 ) 0.66 ( 0.03 ) 0.16 ( 0.06 ) BERT - STS 0.64 ( 0.04 ) 0.58 ( 0.04 ) 0.2 ( 0.07 ) sent - BERT 0.65 ( 0.03 ) 0.59 ( 0.04 ) 0.17 ( 0.06 )", "entities": [[15, 16, "MethodName", "BERT"], [17, 18, "TaskName", "STS"], [32, 33, "MethodName", "BERT"]]}, {"text": "BERT - score 0.69 ( 0.04 ) 0.61 ( 0.04 ) 0.23 ( 0.05 ) Table 6 : decTest results for different decoding parameters : Spearman \u2019s \u001a(mean and standard deviation ) of automatic metrics for storyGen .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "Temperature Top - p Top - k distinct - n 0.89 ( 0.01 ) 0.84 ( 0.02 ) 0.64 ( 0.04 ) cos - sim 0.89 ( 0.01 ) 0.78 ( 0.03 ) 0.62 ( 0.05 ) BERT - STS 0.81 ( 0.02 ) 0.74 ( 0.03 ) 0.56 ( 0.04 ) sent - BERT 0.80 ( 0.02 ) 0.63 ( 0.05 ) 0.51 ( 0.04 )", "entities": [[37, 38, "MethodName", "BERT"], [39, 40, "TaskName", "STS"], [54, 55, "MethodName", "BERT"]]}, {"text": "BERT - score 0.87 ( 0.01 ) 0.77 ( 0.03 ) 0.6 ( 0.05 ) Table 7 : decTest results for different decoding parameters : Spearman \u2019s \u001a(mean and standard deviation ) of automatic metrics for respGen .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "storyGen", "entities": []}, {"text": "respGen promptGen", "entities": []}, {"text": "\u001a OCA\u001a OCA\u001a OCA distinct - n 0.53 0.74 0.52 0.74 0.48 0.75 cos - sim 0.53 0.74 0.52 0.74 0.60 0.77 BERT - STS 0.57 0.74 0.61 0.78 0.78 0.89 sent - BERT 0.75 0.87 0.68 0.83 0.8 0.9 BERT - score 0.60 0.77 0.56 0.78 0.54 0.74 Table 8 : conTest results for McDiv ; Results for automatic metrics over all the samples ( 2 K per task ) .", "entities": [[22, 23, "MethodName", "BERT"], [24, 25, "TaskName", "STS"], [33, 34, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"]]}, {"text": "storyGen", "entities": []}, {"text": "respGen promptGen", "entities": []}, {"text": "\u001a OCA\u001a OCA\u001a OCA distinct - n -0.002 0.49 -0.002 0.49 -0.003 0.49 cos - sim 0.04 0.53 0.08 0.55 0.22 0.60 BERT - STS 0.34 0.64 0.39 0.68 0.68 0.83 sent - BERT 0.63 0.80 0.53 0.76 0.73 0.85 BERT - score 0.35 0.66 0.33 0.65 0.35 0.65 Table 9 : conTest results for McDiv nuggets subset ; Results for automatic metrics over all the samples ( 1 K per task ) .", "entities": [[22, 23, "MethodName", "BERT"], [24, 25, "TaskName", "STS"], [33, 34, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"]]}, {"text": "storyGen", "entities": []}, {"text": "respGen promptGen", "entities": []}, {"text": "\u001a OCA\u001a OCA\u001a OCA distinct - n 0.04 0.57 -0.01 0.46 0.12 0.56 cos - sim 0.05 0.54 0.04 0.54 0.28 0.62 BERT - STS 0.34 0.68 0.39 0.69 0.72 0.9 sent - BERT 0.68 0.85 0.6 0.79 0.75 0.88 BERT - score 0.37 0.69 0.34 0.68 0.38 0.69 absHDS 0.78 0.9 0.7 0.85 0.84 0.94 Table 10 : conTest results for 200 random samples from McDiv nuggets including HDS .", "entities": [[22, 23, "MethodName", "BERT"], [24, 25, "TaskName", "STS"], [33, 34, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"]]}, {"text": "340Context Response set ( \u001c = 0:25 ) Response set ( \u001c = 0:8 )", "entities": []}, {"text": "Response set ( \u001c = 1:1 )", "entities": []}, {"text": "Fire next door .", "entities": []}, {"text": "John woke up smelling like something was burning .", "entities": []}, {"text": "He went outside .", "entities": []}, {"text": "He saw the \ufb01re next door .", "entities": []}, {"text": "He called the authorities.\u000fIt was a minor \ufb01re and they put it out .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a minor \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a minor \ufb01re and they put it out.\u000fThey arrived and put out the \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt turned out to be a \ufb01re .", "entities": []}, {"text": "\u000fIt was a minor \ufb01re night .", "entities": []}, {"text": "\u000fThey arrived and put it out .", "entities": []}, {"text": "\u000fIt was a scary but beautiful \ufb01re next time he went in .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a put out and John was able to put it out .", "entities": []}, {"text": "\u000fIt was a scary place to live.\u000fIt turned out to be a mechanic .", "entities": []}, {"text": "\u000fBefore the \ufb01re was put out it was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fThey co - worker matter how bad the \ufb01re was .", "entities": []}, {"text": "\u000fSeveral shells , the \ufb01re department came just in time .", "entities": []}, {"text": "\u000fThey arrived and put out the \ufb01re .", "entities": []}, {"text": "\u000fIt turned out to be a clear , fact .", "entities": []}, {"text": "\u000fIt was a \ufb01re .", "entities": []}, {"text": "\u000fIt was a scary meal for wet clothes .", "entities": []}, {"text": "\u000fIt was a scary stealing from him .", "entities": []}, {"text": "Charles Shaves His Chest .", "entities": []}, {"text": "Charles decides he no longer wants chest hair .", "entities": []}, {"text": "He shaves his chest hair with a razor .", "entities": []}, {"text": "Although his skin is irritated , he hair is now gone .", "entities": []}, {"text": "Charles is satis\ufb01ed.\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is very happy to have a chest hair .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved.\u000fHe is very happy to have a chest hair .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is very happy that he no longer has chest hair .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that his chest hair is no longer shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved \u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally of shaving .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shashock his chest hair.\u000fHe is glad that he \ufb01nally peaceful his voice again .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaavoided his chest hair .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally downcome .", "entities": []}, {"text": "\u000fHe is ready to start better cooperation his \ufb01replace .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he no longer has chest hair .", "entities": []}, {"text": "\u000fHe is happy to \ufb01nally be shaved .", "entities": []}, {"text": "\u000fHe is very happy to have ster good ride of his life .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally shaved .", "entities": []}, {"text": "\u000fHe is glad that he \ufb01nally has chest hair .", "entities": []}, {"text": "Table 11 : decTest data samples for storyGen task and different temperatures .", "entities": []}, {"text": "Context Response set ( p= 0:208 ) Response set ( p= 0:64 )", "entities": []}, {"text": "Response set ( p= 1 ) Really Bad Decisions .", "entities": []}, {"text": "Jake was nervous about a meeting at work the next day .", "entities": []}, {"text": "He decided to have a drink to relax .", "entities": []}, {"text": "Unfortunately Jake kept drinking .", "entities": []}, {"text": "He was really hung over the next day.\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting.\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting.\u000fHe did not get to the meeting anymore .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe passed out and failing the meeting \u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe passed out and was kicked out of the meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "\u000fHe missed his meeting .", "entities": []}, {"text": "Family Night Food .", "entities": []}, {"text": "Tonight , my mom ordered Mexican food for family night .", "entities": []}, {"text": "She got it from my favorite Mexican place in town .", "entities": []}, {"text": "When it arrived , it was hot and smelled wonderful .", "entities": []}, {"text": "We devoured it with gusto.\u000fAfter a few hours of take it home we all enjoyed its night .", "entities": []}, {"text": "\u000fAfter a few hours of take it home we all enjoyed its night .", "entities": []}, {"text": "\u000fAfter a few hours of take it home we all enjoyed its night .", "entities": []}, {"text": "\u000fAfter a few hours of eating everyone was satis\ufb01ed .", "entities": []}, {"text": "\u000fAfter a few hours of take it home we all enjoyed its night .", "entities": []}, {"text": "\u000fAfter a few hours of eating everyone was satis\ufb01ed .", "entities": []}, {"text": "\u000fAfter a few hours of take it home we all enjoyed its night .", "entities": []}, {"text": "\u000fAfter a few hours of take it home we all enjoyed its night .", "entities": []}, {"text": "\u000fAfter a few hours of take it home we all enjoyed its night .", "entities": []}, {"text": "\u000fAfter a few hours of eating everyone was satis\ufb01ed.\u000fAfter dinner , we all went home to cook Mexican food .", "entities": []}, {"text": "\u000fAfter a few hours of cooking she was tired and ready to eat .", "entities": []}, {"text": "\u000fAfter dinner , I always put got ready for Christmas .", "entities": []}, {"text": "\u000fAfter dinner , I helped her do the dishes .", "entities": []}, {"text": "\u000fAfter a few hours of dinner , the food was amazing .", "entities": []}, {"text": "\u000fAfter a few hours of take it home we all enjoyed margaria .", "entities": []}, {"text": "\u000fAfter dinner , I was ready to take on work the next day of \u000fAfter dinner , I was sad to say goodbye to her \u000fAfter a few hours of take it home we all enjoyed one bite .", "entities": []}, {"text": "\u000fAfter a few hours of eating everyone was satis\ufb01ed.\u000fEven though my stomach was gone , I was sad it was \ufb01nally pockets \u000fAfter dinner , I alone .", "entities": []}, {"text": "\u000fAll in all the family while my mom \ufb01nished the food .", "entities": []}, {"text": "\u000fAfter a few hours of dancing , she pianed .", "entities": []}, {"text": "\u000fAfterwards I \u2019d never \ufb01nish single night a week .", "entities": []}, {"text": "\u000fAfterwards we all went to sleep in my woods .", "entities": []}, {"text": "\u000fAfterwards I helped her do much better than my wife .", "entities": []}, {"text": "\u000fAfterwards my mom helped me \ufb01nish my dinner .", "entities": []}, {"text": "\u000fAfter a few hours we all enjoyed storm blood from the Italy .", "entities": []}, {"text": "\u000fAfter dinner , we all enjoyed some good food together .", "entities": []}, {"text": "Table 12 : decTest data samples for storyGen task and different pvalues ( nucleus sampling ) .", "entities": []}, {"text": "341Context Response set ( k= 3 ) Response set ( k= 32 ) Response set ( k= 318 ) Loud Noise .", "entities": []}, {"text": "Jane was trying to take a nap .", "entities": []}, {"text": "She heard a loud bang in the kitchen .", "entities": []}, {"text": "It woke her up .", "entities": []}, {"text": "A dish had fallen off the counter.\u000fJane pulled over and started to clean herself .", "entities": []}, {"text": "\u000fJane was horri\ufb01ed and dropped her favorite food .", "entities": []}, {"text": "\u000fJane was able to \ufb01nish her car and take a nap .", "entities": []}, {"text": "\u000fJane was able to \ufb01nish her nap .", "entities": []}, {"text": "\u000fJane was able to \ufb01nish her nap .", "entities": []}, {"text": "\u000fJane stopped at the store to buy a new one .", "entities": []}, {"text": "\u000fJane was able to grab a nap afterwards .", "entities": []}, {"text": "\u000fJane was unable to \ufb01nish her nap since it was lost .", "entities": []}, {"text": "\u000fJane pulled over and started to clean up .", "entities": []}, {"text": "\u000fJane was able to \ufb01nish her nap .\u000fJane was able to catch a car using the seat .", "entities": []}, {"text": "\u000fJane stopped at the store to buy a new book .", "entities": []}, {"text": "\u000fJane was sad her cat dropped out of the kitchen .", "entities": []}, {"text": "\u000fJane screamed .", "entities": []}, {"text": "\u000fJane was horri\ufb01ed to \ufb01nd her car broken down on the \ufb02oor .", "entities": []}, {"text": "\u000fJane was horri\ufb01ed and dropped her pay phone .", "entities": []}, {"text": "\u000fJane was easily able to grab a nap .", "entities": []}, {"text": "\u000fJane pulled over and started to cry .", "entities": []}, {"text": "\u000fJane pulled over and started to cry .", "entities": []}, {"text": "\u000fJane stopped at the store to buy a new dish from the store.\u000fJane comes , noticed a lot of food left under it .", "entities": []}, {"text": "\u000fJane was horri\ufb01ed and dropped her book .", "entities": []}, {"text": "\u000fJane remembered to take a nap .", "entities": []}, {"text": "\u000fJane was since she took a nap while she waited for the refun \u000fJane knew she had no time to \ufb01nish her book .", "entities": []}, {"text": "\u000fJane was glad .", "entities": []}, {"text": "\u000fJane was annoyed and began to cry .", "entities": []}, {"text": "\u000fJane stopped at the store to buy a new one .", "entities": []}, {"text": "\u000fJane wanted to have her car back .", "entities": []}, {"text": "\u000fJane was monthed .", "entities": []}, {"text": "Headache .", "entities": []}, {"text": "Kate was wearing big over the ear headphones .", "entities": []}, {"text": "But they were tight and squeezing her head .", "entities": []}, {"text": "She tried to adjust them to relieve the tension .", "entities": []}, {"text": "But nothing really worked.\u000fKate decided to go to the store and buy some ear phones .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some headphones instead .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some ear phones .", "entities": []}, {"text": "\u000fShe decided to go buy a pair of headphones instead .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some headphones instead .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some headphones instead .", "entities": []}, {"text": "\u000fShe decided to go to the doctor and have some rest .", "entities": []}, {"text": "\u000fKate decided to go to the store and buy some headphones instead .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some ear phones .", "entities": []}, {"text": "\u000fShe decided to go buy some ear plugs.\u000fShe decided to go buy a hat so she could enjoy the the long lin \u000fKate decided to go to the store to buy some candy since she was ti \u000fKate decided to go outside to rest .", "entities": []}, {"text": "\u000fKate decided to go to the store and buy some ran phones .", "entities": []}, {"text": "\u000fKate decided to go to the store and buy some headphones instead .", "entities": []}, {"text": "\u000fKate decided to go back to her old dishes .", "entities": []}, {"text": "\u000fShe decided to go buy a big pair of headphones instead .", "entities": []}, {"text": "\u000fKate decided to go to the store and wearing some headphones .", "entities": []}, {"text": "\u000fShe was forced to go to the store to buy some cash .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some headphones instead.\u000fShe decided to go to a ess instead .", "entities": []}, {"text": "\u000fKate decided to go to the store and buy some fake headphones .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some ear phones .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some headphones instead .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some headphones instead .", "entities": []}, {"text": "\u000fShe decided to go buy some ear phones .", "entities": []}, {"text": "\u000fKate decided to go to the gym and use some saw no more watching T \u000fKate decided to go to the store and buy some cute phones .", "entities": []}, {"text": "\u000fShe decided to go buy a fake headphones instead .", "entities": []}, {"text": "\u000fShe decided to go to the store and buy some concert .", "entities": []}, {"text": "Table 13 : decTest data samples for storyGen task and different kvalues ( Top - k ) .", "entities": []}, {"text": "Context Response set ( \u001c = 0:25 )", "entities": []}, {"text": "Response set ( \u001c = 0:8 )", "entities": []}, {"text": "Response set ( \u001c = 1:1 ) it is doing okay", "entities": []}, {"text": "i just want people to talk more\u000fi think you mean", "entities": []}, {"text": "people \u000fi think its working now \u000fi just want people to talk more \u000fi think i just want people to talk more \u000fi do nt think you understand what you re saying \u000fi like to think so \u000fi know that feel \u000fi do nt know what to say \u000fi think its pretty cool \u000fi think its working now\u000fi ca nt believe you did nt think this through i actually have nt had \u000fmay i ask why \u000fi just hope its not going to happen \u000fits alright do nt even want to talk \u000fwhat did it say i was using windows 95 anyway", "entities": []}, {"text": "xd \u000fthanks for reading the thread \u000fi understand and hope they do nt mind that \u000fwant to talk more \u000fits doing ok body language great voice acting and music \u000fall good", "entities": []}, {"text": "i know your pain\u000fim just hawaii everyone wants to be friends lt go talk to people from \u000fits pretty smart we did nt run a year ago and have turned back but \u000fid lick your teeth but tha t sucks smart people \u000fit really did nt stop i think you need to go back to normal now \u000fsounds like a plan to me \u000fwe we can try forever practice beer \u000fokay", "entities": []}, {"text": "but another two more 200 radio views i m going to ore my comment of \u000fteam building looking bro \u000fyep they said that things have changed \u000fyeah", "entities": []}, {"text": "we thank you random stranger girls who say no not like it matters \u000fi do nt like it at all \u000fi do nt like it either \u000fi do nt like it either \u000fi think you mean girls who say no not like it matters \u000fi think that is the most accurate description of this thread \u000fi like this one \u000fgirls who say no do nt like it matter \u000fi do nt like it either \u000fi know a guy who says no and he does nt like it \u000fgirls who say no do nt like it matter\u000fwait", "entities": []}, {"text": "so there are girls who say no like it matters \u000fi have a friend who does nt like it", "entities": []}, {"text": "but i always say no \u000fand you do nt like to play with girls \u000fi say no i do nt want it to \u000fgirls who say no do nt like it care \u000fi see what you did there \u000fgirls who say no need to make eye contact with girls \u000fgirls who say yes do nt like it does \u000fi really do nt care", "entities": []}, {"text": "if its not gay or straight out programs or not \u000fugh that game was just awful", "entities": []}, {"text": "troll\u000fhow about mine like sales \ufb01gure instead of higher \u000fde\ufb01nitely not as stupid as that \u000fagreed is that true \u000frefer to my highest iq you and me \u000fless keep it up \u000fdude", "entities": []}, {"text": "i ve never seen such an complaining member with a legitimate thought \u000fnot that there s anything wrong with that or are more wise than telling want \u000fgirls make videos you ve never met \u000fgtthe girls who say no or a one time thing people that s \u000fwhich is weird since it is a girl i know so sad Table 14 : decTest data samples for respGen task and different temperatures .", "entities": []}, {"text": "342Context Response set ( p= 0:208 ) Response set ( p= 0:64 )", "entities": []}, {"text": "Response set ( p= 1 ) do you know if he still lives in new orleans\u000fi", "entities": []}, {"text": "do nt think so \u000fi do nt think so \u000fi do nt think so \u000fno i do nt think so \u000fno idea i do nt know \u000fi do nt think so \u000fno i do nt think so \u000fno idea", "entities": []}, {"text": "i just saw him live in new orleans \u000fi do nt think so \u000fno idea i just saw him live in new orleans\u000fhe is in nyc \u000fhe does i hope he does \u000fi do nt think so \u000fhe lives in new orleans \u000fyes", "entities": []}, {"text": "he does he is in \ufb02orida \u000fim guessing not that i know of \u000fhes a small town", "entities": []}, {"text": "i do nt know what happened there \u000fnope still in new orleans \u000fi do not i do nt know if he still lives in new orleans \u000fi do nt think so\u000faw screw you guys the gimme a shower and ill send you my money \u000fhe does i m so confused by how this is relevant \u000fthey spent all that much in new orleans up in \ufb01rst place \u000fstill living in new orleans according to most of them it jesus must ve worth \u000fanyone else want to wait for him to go back home \u000fhow many tickets would you say the willing said if he warn you that he s \u000fim guessing he lives there as well \u000fyep rick albeit not \u000fhe lives in", "entities": []}, {"text": "ny \u000fno", "entities": []}, {"text": "i do nt get it any more guess i may canadian its that hanging thing in the back of your mouth\u000fits a tattoo of a hand \u000fits a good thing", "entities": []}, {"text": "its not \u000fi think its a banana \u000fi was thinking the same thing \u000fits a very nice thing to do \u000fits a hole in the bottom of my mouth \u000fi think its a good thing \u000fits a pretty cool thing to do \u000fthats what i thought \u000fi think its a butt\u000fwell it is kind of like a mac \u000fthat would be a good idea \u000fits my cake day \u000fyou can see it in his mouth \u000fits the junk that was pooping in \u000fi was thinking the same thing \u000fi always see that \u000fi did nt know that was possible \u000fthis is my favorite comment of the day \u000fthats not a sticker\u000fi", "entities": []}, {"text": "think you just \ufb02ash \u000fit is and feels good \u000fmy ex", "entities": []}, {"text": "and i reach our arms cheating into the door for almost a minute \u000fmy mind went to side with that \u000fyep terrible for my teeth \u000fexactly what i was thinking \u000fwhy do i look so contest \u000fand where else are you going \u000fjust do nt wear a helmet \u000fand it looks like something some one would give to get in on Table 15 : decTest data samples for respGen task and different pvalues ( nucleus sampling ) .", "entities": []}, {"text": "Context Response set ( k= 3 ) Response set ( k= 32 ) Response set ( k= 318 ) watching curry play in his prime is truly a privilege\u000fi know i just do nt want him to play for us \u000fhe has to be a good center for that \u000fhe is a great center of football in his prime \u000fhes been playing in his prime for a long time \u000fhe was a great back in the day \u000fhes been playing for a while now \u000fi do nt know about that he was pretty damn good at that \u000fi do nt think he was ever in his prime \u000fi do nt think he is a prime minister \u000fi do nt know", "entities": []}, {"text": "why", "entities": []}, {"text": "but i think he is a very good player and\u000fand his career as well \u000fagreed the way he s playing is awesome \u000fit has to be \u000fthis is just called a job \u000fand then being on the \ufb01eld for the \ufb01rst time \u000fi do nt see him doing that often enough \u000fhe just likes to party in the kitchen \u000fat this point he s going to be a great star for the rest of the \u000fonly if he pays well \u000fthe only thing that can make that kind of difference is how much time you\u000fyeah", "entities": []}, {"text": "my feeling i mean we do nt like it", "entities": []}, {"text": "but it happens all the \u000fyou are one for real \u000fthey still have a rule saying they might not be injured yet \u000fit really is a necessary thing to do \u000f\ufb01nally some reason to continue watching him at some point \u000fyet that would be epic \u000fnot to mention eating curry dinner is a privilege \u000fi just do nt want to turn over for this goal like he does in \u000fgt playing in his prime is truly a privilege ftfy \u000fso is saying he is in high school i m going this evening when she usually works\u000fi think you accidentally a word \u000fyou are a good man", "entities": []}, {"text": "\u000fi hope she works \u000fim going to the same time as you when she usually works \u000fi am so sorry to hear that \u000fi hope she works for you \u000fi am so jealous of your work i am so jealous \u000fi hope you have fun", "entities": []}, {"text": "\u000fi hope you get a job at a local bar \u000fi hope she works for you\u000fi see what you did there \u000fwhere in the world are you going to put your socks on \u000fshe usually does", "entities": []}, {"text": "but she has to keep up to date with her in the \u000fawesome what do you want for it \u000fhow does a women have a relationship with someone \u000fdid you get the job \u000fthats where she goes \u000fi know i m also going this friday \u000fthats a great idea \u000fwell make sure you re there too good luck\u000fthats what you like to hear \u000fgo see her later this evening \u000fi read that as she usually fails \u000fokay", "entities": []}, {"text": "ill see you there brah \u000fi always thought that it was for the sake of having the girl play as \u000fthat can be expected here \u000fthats it", "entities": []}, {"text": "i m down now \u000fi do nt see why it would matter if she s married \u000fi will be the ex", "entities": []}, {"text": "gf \u000fshe still does make her phone calls the whole of\ufb01ce Table 16 : decTest data samples for respGen task and different kvalues ( Top - k ) .", "entities": []}, {"text": "343Response set ( \u001c = 0:25 )", "entities": []}, {"text": "Response set ( \u001c = 0:8 )", "entities": []}, {"text": "Response set ( \u001c = 1:1 ) \u000fNot the hacking .", "entities": []}, {"text": "The hacking is the fact that the DNC was hacked . !", "entities": []}, {"text": "\u000fNot the hacking .", "entities": []}, {"text": "The hacking is the real problem .", "entities": []}, {"text": "The hacking is the !", "entities": []}, {"text": "\u000fNot the hacking of the DNC , but the leaks of the emails of the Democratic National Committee . !", "entities": []}, {"text": "\u000fNot the hacking , but the way it was done .", "entities": []}, {"text": "The FBI \u2019s investigation into the !", "entities": []}, {"text": "\u000fNot the hacking of the DNC , but the hacking of the emails of the Democratic National Committee . !", "entities": []}, {"text": "\u000fNot the hacking of the DNC , but the leaking of the emails .", "entities": []}, {"text": "The DNC \u2019s !", "entities": []}, {"text": "\u000fNot the hacking of the DNC .", "entities": []}, {"text": "The hacking of the DNC was a \u201d false \ufb02ag !", "entities": []}, {"text": "\u000fNot the hacking of the DNC .", "entities": []}, {"text": "But the hacking of the RNC .", "entities": []}, {"text": "The DNC hack !", "entities": []}, {"text": "\u000fNot the hacking .", "entities": []}, {"text": "The hacking is the problem .", "entities": []}, {"text": "The hacking is the problem !", "entities": []}, {"text": "\u000fNot the hacking of the DNC , but the leaking of the emails .", "entities": []}, {"text": "The DNC was hacked,!\u000fNot the hacking after all ?", "entities": []}, {"text": "I \u2019m sure the nation - states that are involved in !", "entities": []}, {"text": "\u000fNot the hacking that happened on the internal networks of the Energy Department .", "entities": []}, {"text": "In fact , according to !", "entities": []}, {"text": "\u000fNot the hacking of the American public but rather the fraudulent Heisenberg principle that seemed to be !", "entities": []}, {"text": "\u000fNot the hacking that took place in the DNC last year or the release of hacked emails during the !", "entities": []}, {"text": "\u000fNot the hacking futurists Cardboard inventor and self - described tinkerer Dennis !", "entities": []}, {"text": "\u000fNot the hacking alone .", "entities": []}, {"text": "In the \ufb01rst half of the report , the hackers tried to create fake !", "entities": []}, {"text": "\u000fNot the hacking .", "entities": []}, {"text": "The hacking is the NSA \u2019s new SHIELD technology .", "entities": []}, {"text": "It is !", "entities": []}, {"text": "\u000fNot the hacking and hacking and hacking of the world government .", "entities": []}, {"text": "I know this man is a man !", "entities": []}, {"text": "\u000fNot the hacking aspect , but the pressure exerted by the Trumpistas .", "entities": []}, {"text": "But also the Russia angle !", "entities": []}, {"text": "\u000fNot the hacking , but the willingness . \u201d", "entities": []}, {"text": "The evidence of interest in this case comes in!\u000fNot the hacking experience of a CIA VRO crunch nine months ago \u2014 JumpStart for 2016 jumps !", "entities": []}, {"text": "\u000fNot the hacking , David . )", "entities": []}, {"text": "The directory was \ufb02agged in a document it created in late last year !", "entities": []}, {"text": "\u000fNot the hacking of Democratic Party systems - said the Russian team \u2019s activity represented \u201d just the beginning !", "entities": []}, {"text": "\u000fNot the hacking , of course \u2013 which these sources sounded more concerned about than being attacked 140 times !", "entities": []}, {"text": "\u000fNot the hacking story is over .", "entities": []}, {"text": "But yet there \u2019s another reason not to rush out such statements !", "entities": []}, {"text": "\u000fNot the hacking -either.- These were scattered in the workshop.(Expanded- being guys with !", "entities": []}, {"text": "\u000fNot the hacking of private material of elected of\ufb01cials , e.g. emails , even if the !", "entities": []}, {"text": "\u000fNot the hacking has happened yet ! ! ! ! ! ! ! ! ! ! ! ! ! !", "entities": []}, {"text": "\u000fNot the hacking rumours have cost him any of his followers , least of all the proprietors of !", "entities": []}, {"text": "\u000fNot the hacking group behind the breach of Sony , which has posted the staffer \u2019s information online , !", "entities": []}, {"text": "\u000fHow is our new technology helping us to do that ?", "entities": []}, {"text": "We are using a new technology !", "entities": []}, {"text": "\u000fHow is our system different from that of the United States ?", "entities": []}, {"text": "The United States is a !", "entities": []}, {"text": "\u000fHow is our approach different from that of the other major European countries ?", "entities": []}, {"text": "The European Commission !", "entities": []}, {"text": "\u000fHow is our country going to be able to compete with the rest of the world if we don !", "entities": []}, {"text": "\u000fHow is our country going to be able to compete with China in the future ? \u201d", "entities": []}, {"text": "he asked . !", "entities": []}, {"text": "\u000fHow is our work different from that of other organizations ?", "entities": []}, {"text": "The work of the Center for !", "entities": []}, {"text": "\u000fHow is our work different from other research in this area ?", "entities": []}, {"text": "We are not the \ufb01rst !", "entities": []}, {"text": "\u000fHow is our system of government supposed to work ?", "entities": []}, {"text": "The reason we have a government is !", "entities": []}, {"text": "\u000fHow is our system different from the one that was used in the past ?", "entities": []}, {"text": "The system !", "entities": []}, {"text": "\u000fHow is our country supposed to be a beacon of hope for the world if we have to look!\u000fHow is our government going to catch up with the cyber criminals ? \u201d he said .", "entities": []}, {"text": "\u201d I \u2019m !", "entities": []}, {"text": "\u000fHow is our society selling humanity on slavery ?", "entities": []}, {"text": "The answers to these questions are also important for us !", "entities": []}, {"text": "\u000fHow is our minister giving it to you ?", "entities": []}, {"text": "Is n\u2019t it ?", "entities": []}, {"text": "It \u2019s got a bit of !", "entities": []}, {"text": "\u000fHow is our research different from other studies ?", "entities": []}, {"text": "This study examined the effects of peer- !", "entities": []}, {"text": "\u000fHow is our mission different from Seniors \u2019 Service Corps ( SSC ) other than the fact !", "entities": []}, {"text": "\u000fHow is our challenge different ?", "entities": []}, {"text": "The only difference is that this challenge is about building an ! \u000fHow is our nation governed ? \u201d", "entities": []}, {"text": "As Obama moved into his second term , he is increasingly !", "entities": []}, {"text": "\u000fHow is our recommendation different from what more traditional veterinarians do ?", "entities": []}, {"text": "We do n\u2019t believe !", "entities": []}, {"text": "\u000fHow is our rapid abandonment of critical thinking , knowledge , and values , and the subsequent burial of !", "entities": []}, {"text": "\u000fHow is our education system designed for our futures ?", "entities": []}, {"text": "We are the children of immigrants,!\u000fHow is our Internet even even connected with our corporate tracks ?", "entities": []}, {"text": "Every cell phone on the planet knows !", "entities": []}, {"text": "\u000fHow is our developer name attached to the icon ?", "entities": []}, {"text": "Since the Planetside icon is use internally !", "entities": []}, {"text": "\u000fHow is our food paradise created ?", "entities": []}, {"text": "Arti\ufb01cial chemical fertilizers .", "entities": []}, {"text": "So these are n\u2019t GMOs , but !", "entities": []}, {"text": "\u000fHow is our acquisition * worth - BOARD ROLL ( Least Signi\ufb01cant Equivalents ) !", "entities": []}, {"text": "\u000fHow is our transit plan addressing this problem ?", "entities": []}, {"text": "Under our old plans , Burlington Buses !", "entities": []}, {"text": "\u000fHow is our mind different than any other part of the body ? \u201d", "entities": []}, {"text": "A Broader View !", "entities": []}, {"text": "\u000fHow is our campaign working ?", "entities": []}, {"text": "Bitcoin launches alongside psychological research showing that people pay a lot !", "entities": []}, {"text": "\u000fHow is our mentioning application related to a related method ( # \ufb01ve with two in queue ) page", "entities": []}, {"text": "such ! \u000fHow is our having to resort to roundabout hypotheticals to argue that Stewart may secretly want !", "entities": []}, {"text": "\u000fHow is our blood working out for you ? \u201d", "entities": []}, {"text": "a statewide voter got an outpouring of rename and !", "entities": []}, {"text": "Table 17 : decTest data samples for promptGen task and different temperatures .", "entities": []}, {"text": "Bold text is the 3 - words prompt context .", "entities": []}, {"text": "Figure 7 : Warm - up part , starting each AMT HDS task .", "entities": []}, {"text": "It includes the context , and a single response generated by the tester .", "entities": []}, {"text": "The worker is asked to generate response of hers / his own and rate the quality of the tester \u2019s response .", "entities": []}, {"text": "344Response set ( p= 0:208 ) Response set ( p= 0:64 )", "entities": []}, {"text": "Response set ( p= 1 ) \u000fSo that \u2019s the story of the last few years .", "entities": []}, {"text": "The current political climate is not !", "entities": []}, {"text": "\u000fSo that \u2019s the end of the \ufb01rst part of this series .", "entities": []}, {"text": "I hope you enjoyed it . !", "entities": []}, {"text": "\u000fSo that \u2019s the \ufb01rst thing I want to say .", "entities": []}, {"text": "I \u2019m not going to be the guy !", "entities": []}, {"text": "\u000fSo that \u2019s the thing about being a professional .", "entities": []}, {"text": "You have to be able to handle the criticism !", "entities": []}, {"text": "\u000fSo that \u2019s the way it is .", "entities": []}, {"text": "I do n\u2019t think there \u2019s any way to change it !", "entities": []}, {"text": "\u000fSo that \u2019s the problem .", "entities": []}, {"text": "It \u2019s not just that the government is failing to protect !", "entities": []}, {"text": "\u000fSo that \u2019s the thing about this .", "entities": []}, {"text": "It \u2019s not just about the money .", "entities": []}, {"text": "It \u2019s about !", "entities": []}, {"text": "\u000fSo that \u2019s the end of the story .", "entities": []}, {"text": "The next step is to create a custom !", "entities": []}, {"text": "\u000fSo that \u2019s the case .", "entities": []}, {"text": "So , what \u2019s the problem ?", "entities": []}, {"text": "Well , !", "entities": []}, {"text": "\u000fSo that \u2019s the \ufb01rst time I \u2019ve ever seen a real one .", "entities": []}, {"text": "I \u2019m not!\u000fSo that \u2019s the state of the campaign .", "entities": []}, {"text": "Now , what I do want to talk about is !", "entities": []}, {"text": "\u000fSo that \u2019s the thing : For as much as I love TLC , it \u2019s hard to !", "entities": [[12, 13, "MethodName", "TLC"]]}, {"text": "\u000fSo that \u2019s the idea , anyway .", "entities": []}, {"text": "The last two seasons have been about doing that .", "entities": []}, {"text": "It !", "entities": []}, {"text": "\u000fSo that \u2019s the end of the half - hour segment .", "entities": []}, {"text": "The next half - hour !", "entities": []}, {"text": "\u000fSo that \u2019s the situation we \u2019re in , \u201d he said .", "entities": []}, {"text": "\u201d We \u2019re in the !", "entities": []}, {"text": "\u000fSo that \u2019s the thing , I do n\u2019t know if you know , but in general it \u2019s !", "entities": []}, {"text": "\u000fSo that \u2019s the difference between the kinds of things that people will be talking about on Wednesday , !", "entities": []}, {"text": "\u000fSo that \u2019s the $ 2.3 billion .", "entities": []}, {"text": "Here \u2019s the issue : You \u2019re !", "entities": []}, {"text": "\u000fSo that \u2019s the standard for using memcpy ( ) .", "entities": []}, {"text": "It \u2019s \ufb01ne to use memc !", "entities": []}, {"text": "\u000fSo that \u2019s the next step , and the next step is to try to \ufb01gure out what\u2019s!\u000fSo that \u2019s the \ufb01rst time you want to punch somebody , not miss before . \u201d", "entities": []}, {"text": "The Seahawks would !", "entities": []}, {"text": "\u000fSo that \u2019s the science behind the Broadwell - E processors from Intel that Intel launched last fall !", "entities": []}, {"text": "\u000fSo that \u2019s the instinct from other teams , that they \u2019re a headache .", "entities": []}, {"text": "- Ramsay MacDonald , !", "entities": []}, {"text": "\u000fSo that \u2019s the white whale right there about too much debt .", "entities": []}, {"text": "And then what you !", "entities": []}, {"text": "\u000fSo that \u2019s the end of our discussion about the causes .", "entities": []}, {"text": "What happens when we look at the !", "entities": []}, {"text": "\u000fSo that \u2019s the cover of inhibition against \u201d chronic \u201d or \u201d adaptive \u201d stimulants !", "entities": []}, {"text": "\u000fSo that \u2019s the way the story goes , but exactly how is cloud providers going to restrict Their !", "entities": []}, {"text": "\u000fSo that \u2019s the beginning , the beginning of the show", "entities": []}, {"text": ", I guess \ufb01ve minutes . \u201d !", "entities": []}, {"text": "\u000fSo that \u2019s the Indie Mobile Game Week Honoring Winners ! ! ! ! ! ! ! ! !", "entities": []}, {"text": "\u000fSo that \u2019s the reason I \u2019m writing , that \u2019s why you do n\u2019t understand why people know !", "entities": []}, {"text": "\u000fdo you listen to the music ? \u201d", "entities": []}, {"text": "\u201d I do n\u2019t know .", "entities": []}, {"text": "I do n\u2019t listen !", "entities": []}, {"text": "\u000fdo you listen to them ? \u201d", "entities": []}, {"text": "\u201d I do , \u201d he said .", "entities": []}, {"text": "\u201d I \u2019m not !", "entities": []}, {"text": "\u000fdo you listen to the voices of the people ? \u201d", "entities": []}, {"text": "\u201d I do , \u201d said the king !", "entities": []}, {"text": "\u000fdo you listen to the song ? \u201d", "entities": []}, {"text": "\u201d I do n\u2019t know .", "entities": []}, {"text": "I do n\u2019t know !", "entities": []}, {"text": "\u000fdo you listen to the music ? \u201d", "entities": []}, {"text": "\u201d I do . \u201d", "entities": []}, {"text": "\u201d You \u2019re not !", "entities": []}, {"text": "\u000fdo you listen to the news ?", "entities": []}, {"text": "I do .", "entities": []}, {"text": "I \u2019m a big fan of the !", "entities": []}, {"text": "\u000fdo you listen to me ? \u201d", "entities": []}, {"text": "\u201d Yes , I do . \u201d", "entities": []}, {"text": "\u201d I \u2019m !", "entities": []}, {"text": "\u000fdo you listen to the other side ? \u201d", "entities": []}, {"text": "\u201d I do n\u2019t know .", "entities": []}, {"text": "I do n\u2019t !", "entities": []}, {"text": "\u000fdo you listen to the other side ? \u201d", "entities": []}, {"text": "\u201d I do , \u201d said the boy . \u201d !", "entities": []}, {"text": "\u000fdo you listen to the news ?", "entities": []}, {"text": "No , I do n\u2019t .", "entities": []}, {"text": "I do n\u2019t", "entities": []}, {"text": "listen!\u000fdo", "entities": []}, {"text": "you listen to the current draft ?", "entities": []}, {"text": "I listen to the current draft .", "entities": []}, {"text": "I \u2019m !", "entities": []}, {"text": "\u000fdo you listen to it ? \u201d", "entities": []}, {"text": "It \u2019s easy to hear the \u201d why ? \u201d", "entities": []}, {"text": "but when !", "entities": []}, {"text": "\u000fdo you listen to the people that come here ? \u201d", "entities": []}, {"text": "\u201d No , I \u2019m too busy !", "entities": []}, {"text": "\u000fdo you listen to the thing ? \u201d", "entities": []}, {"text": "\u201d Of course I do .", "entities": []}, {"text": "I \u2019ve been reading !", "entities": []}, {"text": "\u000fdo you listen to those who are opposing it , who want to create a situation in which a !", "entities": []}, {"text": "\u000fdo you listen to music or watch TV ?", "entities": []}, {"text": "How often do you cook or clean ?", "entities": []}, {"text": "How much !", "entities": []}, {"text": "\u000fdo you listen to them ?", "entities": []}, {"text": "It \u2019s like the \ufb01rst time you got into something", "entities": []}, {"text": "and it just !", "entities": []}, {"text": "\u000fdo you listen to your father ?", "entities": []}, {"text": "We \u2019ll leave it to the gods to decide . \u201d !", "entities": []}, {"text": "\u000fdo you listen to music ?", "entities": []}, {"text": "I like to listen to music , but I do n\u2019t really know !", "entities": []}, {"text": "\u000fdo you listen to my story and see if you like it ? \u201d", "entities": []}, {"text": "\u201d I think you!\u000fdo", "entities": []}, {"text": "you listen to Human Fly ? , which YouTuber Nico Perri collaborated on , and Google !", "entities": [[9, 10, "DatasetName", "Nico"], [15, 16, "DatasetName", "Google"]]}, {"text": "\u000fdo you listen to the acapella lyrics out of context and express the feeling ? \u201d", "entities": []}, {"text": "It \u2019s !", "entities": []}, {"text": "\u000fdo you listen to Michael Kiwanuka - Smith who writes , \u201d The American Journalism Review discern !", "entities": []}, {"text": "\u000fdo you listen to my songs as I said , \u201d Ramckhalter said .", "entities": []}, {"text": "\u201d You feel !", "entities": []}, {"text": "\u000fdo you listen to U.S. 90 night at this time of the year ? !", "entities": []}, {"text": "\u000fdo you listen to that as well ? \u201d", "entities": []}, {"text": "\u201d The question was not , \u2019 Who is !", "entities": []}, {"text": "\u000fdo you listen ? \u201d", "entities": []}, {"text": "He asks , leaning forward as he woodenly talks to him .", "entities": []}, {"text": "\u201d Listen !", "entities": []}, {"text": "\u000fdo you listen to those books and sway him so much ?", "entities": []}, {"text": "No .", "entities": []}, {"text": "He was deeply brainwashed !", "entities": []}, {"text": "\u000fdo you listen ? \u2019", "entities": []}, {"text": "Simon(lol).I feel like i \u2019m in a Kurdish Genocide .", "entities": []}, {"text": "I !", "entities": []}, {"text": "\u000fdo you listen to value authenticated queries from your menu when running count ?", "entities": []}, {"text": "And if not , then !", "entities": []}, {"text": "Table 18 : decTest data samples for promptGen task and different pvalues ( nucleus sampling ) .", "entities": []}, {"text": "Bold text is the 3 - words prompt context .", "entities": []}, {"text": "Figure 8 : absHDS question along with the evaluated response set ( conTest in this case ) .", "entities": []}, {"text": "345Response set ( k= 3 ) Response set ( k= 32 ) Response set ( k= 318 ) \u000fI\u2019m kidding .", "entities": []}, {"text": "You ca n\u2019t do that .", "entities": []}, {"text": "\u201d I \u2019ve been trying to \ufb01nd !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You ca n\u2019t be serious . \u201d", "entities": []}, {"text": "The man was a little surprised . !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You ca n\u2019t do that , because the game \u2019s a little bit different . !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re the only one who can do it , you know .", "entities": []}, {"text": "You \u2019re !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You can have it .", "entities": []}, {"text": "\u201d It \u2019s not a bad idea , \u201d !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You do n\u2019t need to be so serious . \u201d", "entities": []}, {"text": "\u201d No , I ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re not .", "entities": []}, {"text": "I \u2019m just a little bit of a dickhead . !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re not .", "entities": []}, {"text": "You \u2019re not .", "entities": []}, {"text": "I know .", "entities": []}, {"text": "I know .", "entities": []}, {"text": "You ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You ca n\u2019t do it .", "entities": []}, {"text": "\u201d I do n\u2019t want to be ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re not going to be able to see it , and you \u2019ll be!\u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re too nice .", "entities": []}, {"text": "We need someone in the government to talk !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You just sit there , I \u2019ll have the other side do it .", "entities": []}, {"text": "What ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You can be my roommate for the holidays in a few weeks .", "entities": []}, {"text": "You don ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You can get the full version of the \ufb01le above , and also the original !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You do n\u2019t look very strong . \u201d", "entities": []}, {"text": "\u201d You look as strong as ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re right about the last guy .", "entities": []}, {"text": "He \u2019s a nice guy .", "entities": []}, {"text": "But ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019ll be happy to know it \u2019s the \ufb01rst time that \u2019s happened since !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You will come down with the same problem , or some other sort of problem !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You might have seen me in a lot of other things .", "entities": []}, {"text": "I \u2019m actually !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You should go and see a doctor .", "entities": []}, {"text": "In fact , I\u2019m!\u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re kidding ? \u201d", "entities": []}, {"text": "\u201d I \u2019m not . \u201d", "entities": []}, {"text": "\u201d Why ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re not . \u201d", "entities": []}, {"text": "\u201d What ?", "entities": []}, {"text": "A dick of the heart ? !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re looking at a new version , \u201d said Zilch , who was !", "entities": []}, {"text": "\u000fI\u2019m", "entities": []}, {"text": "kidding .", "entities": []}, {"text": "You know when someone takes to the streets to protest ?", "entities": []}, {"text": "It \u2019s common for ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You are de\ufb01nitely a complete free agent , \u201d said Caruthers . !", "entities": [[6, 7, "DatasetName", "agent"]]}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You can have another at \ufb01rst , but do n\u2019t start just jumping ahead/ !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You \u2019re just a teenager , are n\u2019t you ? \u201d", "entities": []}, {"text": "It ends there , your ! \u000fI\u2019m kidding .", "entities": []}, {"text": "You were never fully persuaded . \u201d", "entities": []}, {"text": "\u201d Perfect , I am not , \u201d !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You are also in a worse case scenario for someone who was on $ 2500 !", "entities": []}, {"text": "\u000fI\u2019m kidding .", "entities": []}, {"text": "You know . . . \u201d", "entities": []}, {"text": "\u201d I should have stopped him ; I should n\u2019t !", "entities": []}, {"text": "\u000fWhere did he go ? \u201d", "entities": []}, {"text": "I ask , looking at him .", "entities": []}, {"text": "\u201d I \u2019m not sure .", "entities": []}, {"text": "He !", "entities": []}, {"text": "\u000fWhere did he get the idea to do this ?", "entities": []}, {"text": "He had been working on a book !", "entities": []}, {"text": "\u000fWhere did he come from ?", "entities": []}, {"text": "He was born in the city of Karkaros !", "entities": []}, {"text": "\u000fWhere did he go ? \u201d", "entities": []}, {"text": "I asked .", "entities": []}, {"text": "\u201d I do n\u2019t know , \u201d she said . !", "entities": []}, {"text": "\u000fWhere did he go ? \u201d", "entities": []}, {"text": "\u201d I think he went to the hospital , \u201d she said . !", "entities": []}, {"text": "\u000fWhere did he get the idea for the name ?", "entities": []}, {"text": "I think it \u2019s a combination of !", "entities": []}, {"text": "\u000fWhere did he get the idea to make a movie about the Holocaust ?", "entities": []}, {"text": "\u201d I had a lot !", "entities": []}, {"text": "\u000fWhere did he get that idea ?", "entities": []}, {"text": "\u201d I was just trying to make a statement , \u201d !", "entities": []}, {"text": "\u000fWhere did he get that from ?", "entities": []}, {"text": "He \u2019s a very good writer .", "entities": []}, {"text": "I do n\u2019t know what !", "entities": []}, {"text": "\u000fWhere did he go ?", "entities": []}, {"text": "Where was he ?", "entities": []}, {"text": "Where was he ?", "entities": []}, {"text": "He \u2019s gone . !", "entities": []}, {"text": "\u000fWhere did he come back from ?", "entities": []}, {"text": "[ The Doctor is sitting in a chair .", "entities": []}, {"text": "Amy ! \u000fWhere did he \ufb01nd the money ? \u201d asked a reporter from the BBC . \u201d", "entities": []}, {"text": "Is anybody else there !", "entities": []}, {"text": "\u000fWhere did he grow up ? \u201d", "entities": []}, {"text": "But the boy answered , \u201d He always loved to read !", "entities": []}, {"text": "\u000fWhere did he get that idea ? \u201d", "entities": []}, {"text": "he asked .", "entities": []}, {"text": "\u201d I did n\u2019t know .", "entities": []}, {"text": "I \u2019ve never !", "entities": []}, {"text": "\u000fWhere did he come from ? \u201d", "entities": []}, {"text": "You \u2019re looking for that missing piece .", "entities": []}, {"text": "Maybe you \u2019re missing the !", "entities": []}, {"text": "\u000fWhere did he come from ?", "entities": []}, {"text": "He was , I think , from a small island about midway between !", "entities": []}, {"text": "\u000fWhere did he come from , to be sure ? \u201d", "entities": []}, {"text": "he asked , \u201d I know he came from !", "entities": []}, {"text": "\u000fWhere did he go ?", "entities": []}, {"text": "[ A little while later ] I am about to say this !", "entities": []}, {"text": "\u000fWhere did he hear about my story ?", "entities": []}, {"text": "I could n\u2019t tell you .", "entities": []}, {"text": "He \u2019d only heard of !", "entities": []}, {"text": "\u000fWhere did he come from ?", "entities": []}, {"text": "From a place called \u201d the City of the Sun . \u201d!\u000fWhere", "entities": []}, {"text": "did he at the time in his day seek the God he worshipped ?", "entities": []}, {"text": "He said : \u201d !", "entities": []}, {"text": "\u000fWhere did he earn his master \u2019s degree ?", "entities": []}, {"text": "He is part of a class of doctoral students who !", "entities": []}, {"text": "\u000fWhere did he learn to play guitar ? \u201d", "entities": []}, {"text": "I asked , puzzled . \u201d", "entities": []}, {"text": "Before I joined !", "entities": []}, {"text": "\u000fWhere did he come from ? \u201d \u201d Australia , \u201d said Peter .", "entities": []}, {"text": "\u201d How could ! \u000fWhere did he hear this , you might ask ?", "entities": []}, {"text": "Of course , he \u2019d heard of it . !", "entities": []}, {"text": "\u000fWhere did he go ?", "entities": []}, {"text": "He \u2019s probably dead \u2013 or dead and buried within the walls !", "entities": []}, {"text": "\u000fWhere did he earn $ 150 million on his way to a $ 5 billion makeover ? !", "entities": []}, {"text": "\u000fWhere did he learn to make his own sticks , or for that matter , hang a stick on !", "entities": []}, {"text": "\u000fWhere did he learn to skate , anyway ?", "entities": []}, {"text": "Go here and watch this beautiful skater !", "entities": []}, {"text": "\u000fWhere did he get this idea from ?", "entities": []}, {"text": "What do you think about it ?", "entities": []}, {"text": "I get !", "entities": []}, {"text": "Table 19 : decTest data samples for promptGen task and different kvalues ( Top - k ) .", "entities": []}, {"text": "Bold text is the 3 - words prompt context .", "entities": []}, {"text": "Context Response set ( high content diversity )", "entities": []}, {"text": "Response set ( low content diversity ) Sold Out Jane wanted to watch a big new action movie .", "entities": []}, {"text": "She had been waiting a long time for it to come out .", "entities": []}, {"text": "When tickets became available she was too busy .", "entities": []}, {"text": "By the time she had a chance to buy some it was sold out.\u000fJane cried over the fact that she could n\u2019t watch it and just gave up looking for a ticket .", "entities": []}, {"text": "\u000fJane decided to look for a scalper that would sell her the ticket for the movie that she really wanted to see .", "entities": []}, {"text": "\u000fJane thought it was okay since she can still have a chance to watch it once it gets uploaded in video and movie streaming applications .", "entities": []}, {"text": "\u000fJane posted a status on her social media accounts asking her friends for any spare ticket that she is willing to buy .", "entities": []}, {"text": "\u000fJane resorted to contacting her old friend who is working at a huge movie theater hoping she can help her get a ticket.\u000fJane remembered that she has an old friend who is a manager at a big movie theater so she contacted that friend in the hopes that she can buy any spare ticket .", "entities": []}, {"text": "\u000fDesperate to watch the movie , Jane called her friend , who works at a movie theater , asking for a ticket to that movie .", "entities": []}, {"text": "\u000fJane recalled that her friend works at a movie theater and hoped that she can help get a ticket for that movie .", "entities": []}, {"text": "\u000fJane decided to look for her friend who could possibly have access to tickets for that movie since that friend currently works at a movie theater .", "entities": []}, {"text": "\u000fJane realized that her friend might have spare tickets since she is a manager of a movie theater showing that \ufb01lm .", "entities": []}, {"text": "Beavers .", "entities": []}, {"text": "My friend has some beavers in his backyard .", "entities": []}, {"text": "They come up from the creek by his house .", "entities": []}, {"text": "He invites my over and we watch them .", "entities": []}, {"text": "We take pictures of them and send them to our friends.\u000fThey are fascinating animals .", "entities": []}, {"text": "\u000fOur friends love getting the pictures .", "entities": []}, {"text": "\u000fSometimes his dogs chase them .", "entities": []}, {"text": "\u000fThey are building a dam on the creek .", "entities": []}, {"text": "\u000fThey wo n\u2019t let us get too close to them.\u000fThey are busy gathering sticks to make a dam .", "entities": []}, {"text": "\u000fThe dam they are building is almost complete .", "entities": []}, {"text": "\u000fIt \u2019s fascinating to see their workmanship building a dam .", "entities": []}, {"text": "\u000fThey are turning the creek into a pond by building a dam .", "entities": []}, {"text": "\u000fThey all work together with careful engineering to build a dam .", "entities": []}, {"text": "Table 20 : conTest data samples for storyGen task .", "entities": []}, {"text": "Context Response set ( high content diversity )", "entities": []}, {"text": "Response set ( low content diversity ) kill la kill is still going new episode every thursday \u000fThat show sucks", "entities": []}, {"text": "\u000fOMG I ca n\u2019t wait \u000fI thought they canceled it \u000fWhat channel is it on \u000fI only watch nature programs on BBC\u000fLead actor is soooo hot \u000fDid you see the cliffhanger at the end of the season \u000fI\u2019ve been waiting for it to return for weeks \u000fI\u2019m totally gon na binge watch last season", "entities": []}, {"text": "\u000fI just got into this show and ca n\u2019t stop watching places apple slices in a bowl so they \u2019ll stay fresh \u000fOh boy , I love apples .", "entities": []}, {"text": "\u000fI do n\u2019t need you telling me how to keep things fresh , take a hike .", "entities": []}, {"text": "\u000fGirl , you \u2019re the fresh one around here .", "entities": []}, {"text": "\u000fThis post might be better in the life hacks section .", "entities": []}, {"text": "\u000fThis is actually a useful bit of advice.\u000fI \ufb01nd merit in this input .", "entities": []}, {"text": "\u000fThat information will serve me well .", "entities": []}, {"text": "\u000fThanks , that \u2019s really good to know !", "entities": []}, {"text": "\u000fSuch knowledge is certainly bene\ufb01cial .", "entities": []}, {"text": "\u000fWise words , I will heed them .", "entities": []}, {"text": "Table 21 : conTest data samples for respGen task .", "entities": []}, {"text": "346Response set ( high content diversity )", "entities": []}, {"text": "Response set ( low content diversity )", "entities": []}, {"text": "\u000fSuppose there \u2019s an escape plan we have n\u2019t thought of yet .", "entities": []}, {"text": "\u000fSuppose there \u2019s an omelet that is the most amazing ever .", "entities": []}, {"text": "\u000fSuppose there \u2019s an airplane ticket that \u2019s even cheaper .", "entities": []}, {"text": "\u000fSuppose there \u2019s an actual deadline for this paper .", "entities": []}, {"text": "\u000fSuppose there \u2019s an event that we can go to this weekend.\u000fSuppose there \u2019s an airline that costs less .", "entities": []}, {"text": "\u000fSuppose there \u2019s an \ufb02ight that is n\u2019t as expensive .", "entities": []}, {"text": "\u000fSuppose there \u2019s an air travel fare , but does n\u2019t cost as much .", "entities": []}, {"text": "\u000fSuppose there \u2019s an way to \ufb02y there that is low cost .", "entities": []}, {"text": "\u000fSuppose there \u2019s an \ufb02ight going there and it \u2019s not a lot of money \u000fNothing remotely like eating a big breakfast .", "entities": []}, {"text": "\u000fNothing remotely like dancing with your wife at the wedding .", "entities": []}, {"text": "\u000fNothing remotely like singing Justin Bieber \u2019s greatest hits \u000fNothing remotely like falling down a hill \u000fNothing remotely like getting yelled at\u000fNothing remotely like being super full and satis\ufb01ed .", "entities": []}, {"text": "\u000fNothing remotely like getting to taste many different foods .", "entities": []}, {"text": "\u000fNothing remotely like starting the day off right .", "entities": []}, {"text": "\u000fNothing remotely like doing exactly what I want to do .", "entities": []}, {"text": "\u000fNothing remotely like feeding myself with great food .", "entities": []}, {"text": "Table 22 : conTest data samples for promptGen task .", "entities": []}, {"text": "Bold text is the 3 - words prompt context .", "entities": []}, {"text": "Figure 9 : aspHDS question ( content in this case ) .", "entities": []}, {"text": "The response set is the same as presented for absHDS question .", "entities": []}, {"text": "Figure 10 : rnkHDS question along with the two evaluated response sets .", "entities": []}, {"text": "Figure 11 : simHDS question along with the two evaluated responses .", "entities": []}]