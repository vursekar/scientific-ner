[{"text": "Proceedings of the Third Conference on Machine Translation ( WMT ) , Volume 2 : Shared Task Papers , pages 667\u2013670 Belgium , Brussels , October 31 - Novermber 1 , 2018 .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64076 Neural Machine Translation with the Transformer and Multi - Source Romance Languages for the Biomedical WMT 2018", "entities": [[7, 9, "TaskName", "Machine Translation"], [11, 12, "MethodName", "Transformer"], [21, 23, "DatasetName", "WMT 2018"]]}, {"text": "task Brian Tubay and Marta R. Costa - juss ` a TALP Research Center , Universitat Politcnica de Catalunya , Barcelona brian.alcides.tubay.alvarez@alu-etsetb.upc.edu,marta.ruiz@upc.edu", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "The Transformer architecture has become the state - of - the - art in Machine Translation .", "entities": [[1, 2, "MethodName", "Transformer"], [14, 16, "TaskName", "Machine Translation"]]}, {"text": "This model , which relies on attention - based mechanisms , has outperformed previous neural machine translation architectures in several tasks .", "entities": [[15, 17, "TaskName", "machine translation"]]}, {"text": "In this system description paper , we report details of training neural machine translation with multi - source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task .", "entities": [[12, 14, "TaskName", "machine translation"], [22, 23, "MethodName", "Transformer"], [32, 34, "DatasetName", "WMT 2018"]]}, {"text": "Using multi - source languages from the same family allows improvements of over 6 BLEU points .", "entities": [[14, 15, "MetricName", "BLEU"]]}, {"text": "1 Introduction Neural Machine Translation ( NMT ) ( Bahdanau et al . , 2015 ) proved to be competitive with the encoder - decoder architecture based on recurrent neural networks and attention .", "entities": [[3, 5, "TaskName", "Machine Translation"]]}, {"text": "After this architecture , new proposals based on convolutional neural networks ( Gehring et al . , 2017 ) or only attention - based mechanisms ( Vaswani et al . , 2017 ) appeared .", "entities": []}, {"text": "The latter architecture has achieved great success in Machine Translation ( MT ) and it has already been extended to other tasks such as Parsing ( Kaiser et al . , 2017 ) , Speech Recognition 1 , Speech Translation ( Cros et al . , 2018 ) , Chatbots ( Costa - juss ` a et al . , 2018 ) among others .", "entities": [[8, 10, "TaskName", "Machine Translation"], [34, 36, "TaskName", "Speech Recognition"], [39, 40, "TaskName", "Translation"]]}, {"text": "However , training with low resources is still a big drawback for neural architectures and NMT is not an exception ( Koehn and Knowles , 2017 ) .", "entities": []}, {"text": "To face low resource scenarios , several techniques have been proposed , like using multi - source ( Zoph and Knight , 2016 ) , multiple languages ( Johnson et al . , 2017 ) or unsupervised techniques ( Lample et al . , 2018 ;", "entities": []}, {"text": "Artetxe et al . , 2018 ) , among many others .", "entities": []}, {"text": "1https://tensorflow.github.io/ tensor2tensor / tutorials\\/asr$_$with$ _ $ transformer.htmlIn this paper , we use the Transformer enhanced with the multi - source technique to participate in the Biomedical WMT 2018 task , which can be somehow considered a low - resourced task , given the large quantity of data that it is required for NMT .", "entities": [[13, 14, "MethodName", "Transformer"], [26, 28, "DatasetName", "WMT 2018"]]}, {"text": "Our multi - source enhancement is done only with Romance languages .", "entities": []}, {"text": "The fact of using similar languages in a multi - source system may be a factor towards improving the \ufb01nal system which ends up with over 6 BLEU points of improvement over the single source system .", "entities": [[27, 28, "MetricName", "BLEU"]]}, {"text": "2", "entities": []}, {"text": "The Transformer architecture The Transformer model is the \ufb01rst NMT model relying entirely on self - attention to compute representations of its input and output without using recurrent neural networks ( RNN ) or convolutional neural networks ( CNN ) .", "entities": [[1, 2, "MethodName", "Transformer"], [4, 5, "MethodName", "Transformer"]]}, {"text": "RNNs read one word at a time , having to perform multiple steps before generating an output that depends on words that are far away .", "entities": []}, {"text": "But it has been demonstrated that the more steps required , the harder it is to the network to learn how to make these decisions ( Bahdanau et al . , 2015 ) .", "entities": []}, {"text": "In addition , given the sequential nature of the RNNs , it is dif\ufb01cult to fully take advantage of modern computing devices such as Tensor Processing Units ( TPUs ) or Graphics Processing Units ( GPUs ) which rely on parallel processing .", "entities": []}, {"text": "The Transformer is an encoder - decoder model that was conceived to solve these problems .", "entities": [[1, 2, "MethodName", "Transformer"]]}, {"text": "The encoder is composed of three stages .", "entities": []}, {"text": "In the \ufb01rst stage input words are projected into an embedded vector space .", "entities": []}, {"text": "In order to capture the notion of token position within the sequence , a positional encoding is added to the embedded input vectors .", "entities": []}, {"text": "Without positional encodings , the output of the multi - head attention network would be the same for the sentences \u201c I love you more than her\u201d667", "entities": [[8, 12, "MethodName", "multi - head attention"]]}, {"text": "and \u201c I love her more than you \u201d .", "entities": []}, {"text": "The second stage is a multi - head self - attention .", "entities": []}, {"text": "Instead of computing a single attention , this stage computes multiple attention blocks over the source , concatenates them and projects them linearly back onto a space with the initial dimensionality .", "entities": []}, {"text": "The individual attention blocks compute the scaled dot - product attention with different linear projections .", "entities": [[6, 11, "MethodName", "scaled dot - product attention"]]}, {"text": "Finally a position - wise fully connected feed - forward network is used , which consists of two linear transformations with a ReLU activation ( Vinod Nair , 2010 ) in between .", "entities": [[22, 23, "MethodName", "ReLU"]]}, {"text": "Embedding    &   Positional Encoding   Embedding    &   Positional Encoding   Multi - Head    Self - Attention   Masked Multi - Head    Self - Attention   Feed ForwardFeed Forward Multi - Head    Attention   Softmax Inputs TargetsOutput Probabilities Encoder Decoder Figure 1 : Simpli\ufb01ed diagram of the Transformer model", "entities": [[40, 41, "MethodName", "Softmax"], [53, 54, "MethodName", "Transformer"]]}, {"text": "The decoder operates similarly , but generates one word at a time , from left to right .", "entities": []}, {"text": "It is composed of \ufb01ve stages .", "entities": []}, {"text": "The \ufb01rst two are similar to the encoder : embedding and positional encoding and a masked multi - head self - attention , which unlike in the encoder , forces to attend only to past words .", "entities": []}, {"text": "The third stage is a multi - head attention that not only attends to these past words , but also to the \ufb01nal representations generated by the encoder .", "entities": [[5, 9, "MethodName", "multi - head attention"]]}, {"text": "The fourth stage is another position - wise feed - forward network .", "entities": []}, {"text": "Finally , a softmax layer allows to map target word scores into target word probabilities .", "entities": [[3, 4, "MethodName", "softmax"]]}, {"text": "For more speci\ufb01c details about the architecture , refer to the original paper ( Vaswani et al . , 2017 ) .", "entities": []}, {"text": "3 Multi - Source translation Multi - source translation consists in exploiting multiple text inputs to improve NMT ( Zoph andKnight , 2016 ) .", "entities": []}, {"text": "In our case , we are using this approach in the Transformer architecture described above and using only inputs from the same language family .", "entities": [[11, 12, "MethodName", "Transformer"]]}, {"text": "4 Experiments In this section we report details on the database , training parameters and results .", "entities": []}, {"text": "4.1 Databases and Preprocessing The experimental framework is the Biomedical Translation Task ( WMT18)2 .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "The corpus used to train the model are the one provided for the task for the selected languages pairs : Spanishto - English ( es2en ) , French - to - English ( fr2en ) and Portuguese - to - English ( pt2en ) .", "entities": []}, {"text": "Sources are mainly from Scielo and Medline and detailed in Table 3 .", "entities": []}, {"text": "Training Scielo Medline Total es2en 713127 285358 998485 fr2en 9127 612645 621772 pt2en 634438 74267 708705 all2en 1356692 972270 2328962", "entities": []}, {"text": "Table", "entities": []}, {"text": "3 : Corpus Statistics ( number of segments ) .", "entities": []}, {"text": "Validation sets were taken from Khresmoi development data3 , as recommended in the task description .", "entities": []}, {"text": "Each validation dataset contains 500 sentence pairs .", "entities": [[1, 3, "DatasetName", "validation dataset"]]}, {"text": "Test sets were the ones provides by the task for the previous year competition ( WMT174 ) .", "entities": []}, {"text": "Preprocessing relied on three basic steps : tokenization , truecasing and limiting sentence length to 80 words .", "entities": []}, {"text": "Words were segmented by means of Byte - Pair Encoding ( BPE ) ( Sennrich et al . , 2015 ) .", "entities": [[11, 12, "MethodName", "BPE"]]}, {"text": "4.2 Parameters The system was implemented using OpenNMT in PyTorch ( Klein et al . , 2017 ) with the hyperparameters suggested in the website5 .", "entities": []}, {"text": "Other parameters used in training are de\ufb01ned in Table 4 .", "entities": []}, {"text": "Both single - language systems and multi - source system 2http://www.statmt.org/wmt18/ biomedical-translation-task.html 3https://lindat.mff.cuni.cz/ repository / xmlui / handle/11234/1 - 2122 4http://www.statmt.org/wmt17/ biomedical-translation-task.html 5http://opennmt.net/OpenNMT-py/FAQ .", "entities": []}, {"text": "html668", "entities": []}, {"text": "System es2en pt2en fr2en WMT17 WMT18", "entities": []}, {"text": "WMT17 WMT18", "entities": []}, {"text": "WMT17", "entities": []}, {"text": "WMT18", "entities": []}, {"text": "Best performing system 37.49 43.31 43.88 42.58 - 25.78 Single - Language 39.35 39.06 44.31 38.54 31.75 19.42 Multi - Language 40.11 40.49 45.55 39.49 38.31 25.78 Table 1 : Trained systems results for WMT17 and WMT18 of\ufb01cial test sets .", "entities": []}, {"text": "SpanishUtilizando la base de datos Epistemonikos , la cual es mantenida mediante bsquedas realizadas en 30 bases de datos , identi\ufb01camos seis revisiones sistemticas que en conjunto incluyen 36 estudios aleatorizados pertinentes a la pregunta .", "entities": []}, {"text": "Single - LanguageUsing the Epistemonikos database , which is maintained through searches in 30 databases , we identi\ufb01ed six systematic reviews including 36 randomized studies relevant to the question .", "entities": []}, {"text": "Multi - LanguageUsing the Epistemonikos database , which is maintained through searches in 30 databases , we identi\ufb01ed six systematic reviews that altogether include 36 randomized studies relevant to the question .", "entities": []}, {"text": "PortugueseOs resultados dos modelos de regresso mostraram associao entre os fatores de correo estimados e os indicadores de adequao propostos Single - Language Regression models showed an association between estimated correction factors and the proposed adequacy indicators .", "entities": []}, {"text": "Multi - LanguageThe results of the regression models showed an association between the estimated correction factors and the proposed adequacy indicators .", "entities": []}, {"text": "French ( Traduit par Docteur Serge Messier ) .", "entities": []}, {"text": "Single - Language [ Doctor Serge Messier ] .", "entities": []}, {"text": "Multi - Language [ ( Translated by Doctor Serge Messier ) ] .", "entities": []}, {"text": "Table 2 : Spanish / Portuguese / French to English examples for WMT18 were trained with same architecture and parameters .", "entities": []}, {"text": "Hparam Text - to - Text Encoder layers 6 Decoder layers 6 Batch size 4096 Adam optimizer \u03b21= 0.9\u03b22= 0.998 Attention heads 8 Table 4 : Training parameters .", "entities": [[12, 14, "HyperparameterName", "Batch size"], [15, 16, "MethodName", "Adam"], [16, 17, "HyperparameterName", "optimizer"]]}, {"text": "We trained three single - language systems , one for each language pair .", "entities": []}, {"text": "We required 14 epochs for the Spanish - to - English system ( 7 hours of training ) , 16 epochs for the French - to - English system ( 9 hours of training ) , and 17 epochs for the Portuguese - to - English system ( 7 hours of training ) .", "entities": []}, {"text": "For the multi - source system , which concatenated the three parallel corpus together , we required 11 epochs ( 23 hours of training ) .", "entities": []}, {"text": "We stopped training when the validation accuracy did not increase in two consecutive epochs .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "4.3 Results Best ranking systems from WMT17 and WMT18 are shown in Table 1 , except for French - to - EnglishWMT17 since the references for this set are not available .", "entities": []}, {"text": "For this pair , we used 1000 sentences from the Khresmoi development data .", "entities": []}, {"text": "Table 1 shows BLEU results for the baseline systems , the single - language and multi - source approaches .", "entities": [[3, 4, "MetricName", "BLEU"]]}, {"text": "The Transformer architecture outperforms WMT17 best system .", "entities": [[1, 2, "MethodName", "Transformer"]]}, {"text": "Results become even better with the system is trained with the common corpus of Romance languages , what we call the multi - source approach .", "entities": []}, {"text": "The latter is consistent with the universal truth that more data equals better results , even if the source language is not the same .", "entities": []}, {"text": "Finally , Table 2 shows some examples of the output translations .", "entities": []}, {"text": "5 Conclusions The main conclusions of our experiments are that the multi - source inputs of the same family applied to the Transformer architecture can improve the single input .", "entities": [[22, 23, "MethodName", "Transformer"]]}, {"text": "Best improvements achieve an increase of 6 BLEU points in translation quality .", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "Acknowledgments Authors would like to thank Noe Casas for his valuable comments .", "entities": []}, {"text": "This work is supported in669", "entities": []}, {"text": "part by the Spanish Ministerio de Econom \u00b4 \u0131a y Competitividad , the European Regional Development Fund and the Agencia Estatal de Investigaci \u00b4 on , through the postdoctoral senior grant Ram \u00b4 on y Cajal , the contract TEC2015 - 69266 - P ( MINECO / FEDER , EU ) and the contract PCIN2017 - 079 ( AEI / MINECO ) .", "entities": []}, {"text": "References Mikel Artetxe , Gorka Labaka , Eneko Agirre , and Kyunghyun Cho . 2018 .", "entities": []}, {"text": "Unsupervised neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio . 2015 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "CoRR , abs/1409.0473 .", "entities": []}, {"text": "Marta R. Costa - juss ` a,\u00b4Alvaro Nuez , and Carlos Segura .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Experimental research on encoder - decoder architectures with attention for chatbots .", "entities": []}, {"text": "Computaci \u00b4 on y Sistemas .", "entities": []}, {"text": "Laura Cros , Carlos Escolano , Jos \u00b4 e A. R. Fonollosa , and Marta R. Costa - juss ` a. 2018 .", "entities": []}, {"text": "End - to - end speech translation with the transformer .", "entities": []}, {"text": "Submitted to IberSpeech .", "entities": []}, {"text": "Jonas Gehring , Michael Auli , David Grangier , Denis Yarats , and Yann N. Dauphin . 2017 .", "entities": []}, {"text": "Convolutional sequence to sequence learning .", "entities": [[1, 4, "MethodName", "sequence to sequence"]]}, {"text": "CoRR , abs/1705.03122 .", "entities": []}, {"text": "Melvin Johnson , Mike Schuster , Quoc V .", "entities": []}, {"text": "Le , Maxim Krikun , Yonghui Wu , Zhifeng Chen , Nikhil Thorat , Fernanda Vi \u00b4 egas , Martin Wattenberg , Greg Corrado , Macduff Hughes , and Jeffrey Dean . 2017 .", "entities": []}, {"text": "Google \u2019s multilingual neural machine translation system : Enabling zero - shot translation .", "entities": [[0, 1, "DatasetName", "Google"], [4, 6, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 5:339\u2013351.Lukasz Kaiser , Aidan N Gomez , Noam Shazeer , Ashish Vaswani , Niki Parmar , Llion Jones , and Jakob Uszkoreit . 2017 .", "entities": []}, {"text": "One model to learn them all.arXiv preprint arXiv:1706.05137 .", "entities": []}, {"text": "Guillaume Klein , Yoon Kim , Yuntian Deng , Jean Senellart , and Alexander Rush . 2017 .", "entities": []}, {"text": "OpenNMT : Open - source toolkit for neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "InProceedings of ACL 2017 , System Demonstrations , pages 67\u201372 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Philipp Koehn and Rebecca Knowles .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Six challenges for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proc .", "entities": []}, {"text": "of the 1st Workshop on Neural Machine Translation , pages 28\u201339 , Vancouver .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "Guillaume Lample , Ludovic Denoyer , and Marc\u2019Aurelio Ranzato .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Unsupervised machine translation using monolingual corpora only .", "entities": [[0, 3, "TaskName", "Unsupervised machine translation"]]}, {"text": "InProceedings of the International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2015 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1508.07909 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In I. Guyon , U. V .", "entities": []}, {"text": "Luxburg , S. Bengio , H. Wallach , R. Fergus , S. Vishwanathan , and R. Garnett , editors , Advances in Neural Information Processing Systems 30 , pages 6000\u20136010 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Geoffrey E. Hinton Vinod Nair .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Recti\ufb01ed linear units improve restricted boltzmann machines .", "entities": []}, {"text": "In 27th International Conference on Machine Learning .", "entities": []}, {"text": "Barret Zoph and Kevin Knight .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Multi - source neural translation .", "entities": []}, {"text": "In NAACL - HLT 2016 , pages 30 \u2013 34.670", "entities": []}]