[{"text": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers , pages 590\u2013600 , Valencia , Spain , April 3 - 7 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics The ContrastMedium Algorithm :", "entities": []}, {"text": "Taxonomy Induction From Noisy Knowledge Graphs With Just a Few Links Stefano Faralli1 , Alexander Panchenko2 , Chris Biemann2and Simone Paolo Ponzetto1 1Data and Web Science Group , University of Mannheim , Germany 2Language Technology Group , University of Hamburg , Germany { stefano , simone } @informatik.uni - mannheim.de { panchenko , biemann } @informatik.uni - hamburg.de Abstract In this paper , we present ContrastMedium , an algorithm that transforms noisy semantic networks into full-\ufb02edged , clean taxonomies .", "entities": [[4, 6, "TaskName", "Knowledge Graphs"]]}, {"text": "ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as , for instance , a set of manually selected input root and leaf concepts .", "entities": []}, {"text": "This is achieved by leveraging structural information from a companion reference taxonomy , to which the input knowledge graph is linked ( either automatically or manually ) .", "entities": []}, {"text": "When used in conjunction with methods for hypernym acquisition and knowledge base linking , our methodology provides a complete solution for end - to - end taxonomy induction .", "entities": []}, {"text": "We conduct experiments using automatically acquired knowledge graphs , as well as a SemEval benchmark , and show that our method is able to achieve high performance on the task of taxonomy induction .", "entities": [[6, 8, "TaskName", "knowledge graphs"]]}, {"text": "1 Introduction Recent years have witnessed an impressive amount of work on automatic construction of wide - coverage knowledge resources .", "entities": []}, {"text": "Web - scale open information extraction systems like NELL ( Carlson et al . , 2010 ) or ReVerb ( Fader et al . , 2011 ) have been successful in acquiring massive amounts of machine - readable knowledge by effectively tapping large amounts of text from Web pages .", "entities": [[3, 6, "TaskName", "open information extraction"], [8, 9, "DatasetName", "NELL"], [18, 19, "DatasetName", "ReVerb"]]}, {"text": "However , the output of these systems does not consist of a clean , fully - semanti\ufb01ed output .", "entities": []}, {"text": "Such output , on the other hand , could be provided by the vocabulary of large - scale ontologies like DBpedia ( Bizer et al . , 2009 ) or YAGO ( Hoffart et al . , 2013 ) and the integration of open andclosed information extraction approaches ( Dutta et al . , 2014 ) .", "entities": [[20, 21, "DatasetName", "DBpedia"], [30, 31, "DatasetName", "YAGO"]]}, {"text": "The use of an encyclopedia - centric ( e.g. , Wikipedia - based ) dictionary of entities leads to poor coverage of domain - speci\ufb01c terminologies ( Faralli and Navigli , 2013 ) .", "entities": []}, {"text": "This can be alleviated by constructing knowledge bases of ever increasing coverage and complexity from the Web ( Wu et al . , 2012 ;", "entities": []}, {"text": "Gupta et al . , 2014 ; Dong et al . , 2014 ) or by community efforts ( Bollacker et al . , 2008 ) .", "entities": []}, {"text": "However , the focus on large size and wide coverage at entity level has led all these resources to avoid the complementary problem of curating and maintaining a clean taxonomic backbone with as minimal supervision as possible .", "entities": []}, {"text": "That is , no resource , to date , integrates structured information from existing wide - coverage knowledge graphs with empirical evidence from text for the explicit goal of building full-\ufb02edged taxonomies consisting of a clean and fully - connected directed acyclic graph ( DAG ) .", "entities": [[17, 19, "TaskName", "knowledge graphs"]]}, {"text": "This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain - speci\ufb01c knowledge with dozens of scienti\ufb01c , industrial and social applications ( Glass and Vessey , 1995 ) .", "entities": []}, {"text": "In taxonomy induction , the required domain knowledge can be acquired with many different methods for hypernym extraction , ranging from simple lexical patterns ( Hearst , 1992 ; Oakes , 2005 ; Kozareva and Hovy , 2010 ) to statistical and machine learning techniques ( Caraballo , 1999 ; Agirre et al . , 2000 ; Ritter et al . , 2009 ; Velardi et al . , 2013 ) .", "entities": []}, {"text": "Recent efforts , such as Microsoft \u2019s Probase ( Wu et al . , 2012 ) or the WebIsaDB ( Seitner et al . , 2016 ) similarly focus on \u2018 local \u2019 extraction of single hypernym relations , and do not address the problem of how to combine these single relations into a coherent taxonomy .", "entities": []}, {"text": "When taxonomies are automatically acquired , their cleaning ( also called \u201c pruning \u201d ) becomes a mandatory step ( Velardi et", "entities": []}, {"text": "al . , 2013).590", "entities": []}, {"text": "The contributions of this paper are two - fold : 1 .", "entities": []}, {"text": "We introduce a new algorithm , named ContrastMedium , which , given a noisy knowledge graph and its ( possibly automatically generated ) links to a companion taxonomy , is able to output a full-\ufb02edged taxonomy .", "entities": []}, {"text": "Information from the reference taxonomy is projected onto the input noisy graph to automatically acquire topological clues , which are then used to drive the cleaning process .", "entities": []}, {"text": "The reference taxonomy provides us with ground - truth taxonomic relations that make our knowledge - based method not truly unsupervised sensu stricto .", "entities": []}, {"text": "However , the availability of resources like , for instance , WordNet ( Fellbaum , 1998 ) or BabelNet ( Navigli and Ponzetto , 2012 ) implies that these requirements are trivially satis\ufb01ed ; 2 .", "entities": []}, {"text": "We combine our approach with an unsupervised framework for knowledge acquisition from text ( Faralli et al . , 2016 ) to provide a full end - to - end pipeline for taxonomy induction from scratch .", "entities": []}, {"text": "2 Related Work Knowledge Bases ( KBs ) can be created in many different ways depending on the availability of external resources and speci\ufb01c application needs .", "entities": []}, {"text": "Recently , much work in Natural Language Processing focused on Knowledge Base Completion ( Nickel et al . , 2016a , KBC ) , the task of enriching and re\ufb01ning existing KBs .", "entities": [[10, 13, "TaskName", "Knowledge Base Completion"]]}, {"text": "Many different methods have been explored for KBC , including exploitation of resources such as text corpora ( Snow et al . , 2006 ; Mintz et al . , 2009 ; Aprosio et al . , 2013 ) or other KBs ( Wang et al . , 2012 ; Bryl and Bizer , 2014 ) for acquiring additional knowledge .", "entities": []}, {"text": "Alternative approaches , in contrast , primarily rely on existing information from the KB itself ( Socher et al . , 2013 ; Nickel et al . , 2016b ) used as ground - truth to simultaneously learn continuous representations of KB concepts and relations , which are used to infer additional KB relations .", "entities": []}, {"text": "Finally , Open Information Extraction methods looked at ways to extract large amounts of facts from Web - scale corpora in order to acquire open - domain KBs ( Etzioni et al . , 2011 ; Faruqui and Kumar , 2015 , inter alia ) ;", "entities": [[2, 5, "TaskName", "Open Information Extraction"], [38, 39, "DatasetName", "Kumar"]]}, {"text": "In this paper , we focus on a different , yet complementary task , which is a necessary step when inducing novel KBs from scratch , namely extracting clean taxonomies from noisy knowl - edge graphs .", "entities": []}, {"text": "State - of - the - art algorithms differ by the amount of human supervision required and their ability to respect some topological properties while pruning .", "entities": []}, {"text": "Approaches like those of Kozareva and Hovy ( 2010 ) , Velardi et al .", "entities": []}, {"text": "( 2013 ) and Kapanipathi et al .", "entities": []}, {"text": "( 2014 ) , for instance , apply different topological pruning strategies that require to specify the root and leaf concept nodes of the KB in advance \u2013 i.e. , a prede\ufb01ned set of abstract toplevel concepts and lower terminological nodes , respectively .", "entities": []}, {"text": "The approach of Faralli et al . ( 2015 ) avoids such supervision on the basis of an iterative method that uses an ef\ufb01cient variant of topological sorting ( Tarjan , 1972 ) for cycle pruning .", "entities": []}, {"text": "Such lack of supervision , however , comes at the cost of not being able to preserve the original connectivity between the top ( abstract ) and the bottom ( instance ) concepts .", "entities": []}, {"text": "Random edge removal ( Faralli et al . , 2015 ) , in fact , can lead to disconnected components , a problem shared with the OntoLearn Reloaded approach ( Velardi et al . , 2013 ) , which can not ensure such property when used to approximate the solution for a large noisy graph .", "entities": []}, {"text": "Our work goes one step beyond the previous contributions by presenting a new ef\ufb01cient algorithm that is able to extract a clean taxonomy from a noisy knowledge graph without needing to know in advance \u2013 that is , having to manually specify \u2013 the top - level and leaf concepts of the taxonomy , while preserving the overall connectivity of the graph .", "entities": []}, {"text": "We achieve this by projecting the information from a reference KB such as , for instance , WordNet ( Fellbaum , 1998 ) , onto the input noisy KB on the basis of pre - existing KB links \u2013 which in turn can be automatically generated with high precision using any of the existing solutions for KB mapping ( Navigli and Ponzetto , 2012 ; Faralli et al . , 2016 , inter alia ) or by relying on ground truth information from the Linguistic Linked Open Data cloud ( Chiarcos et al . , 2012 ) .", "entities": []}, {"text": "Some aspects of the proposed approach \u2013 namely , the propagation of the nodes \u2019 weights through the graph , which we metaphorically represent as the \ufb02ow of a contrast medium across nodes ( Section 3.3 ) \u2013 are somewhat similar in spirit to spreading activation ( Collins and Loftus , 1975 ) and random walks on graphs ( Lov \u00b4 asz , 1993 ) approaches .", "entities": []}, {"text": "However , in contrast to spreading activation approaches we leverage the graph directionality in order to reach all the possible nodes within the same connected components .", "entities": []}, {"text": "More-591", "entities": []}, {"text": "over , in contrast to random walks on graphs our method is deterministic in nature .", "entities": []}, {"text": "Here , we argue for the choice of a deterministic approach , like ours , that does not require tuning of parameters : its termination is guaranteed by the number of iterations , which we bind by the maximal diameter |E|for a graph G= ( V , E ) .", "entities": [[29, 32, "HyperparameterName", "number of iterations"]]}, {"text": "Generally , random walk algorithms would provide an approximation that may lead to a less precise estimation of the order induced by the contrast medium level .", "entities": []}, {"text": "3", "entities": []}, {"text": "The ContrastMedium Algorithm 3.1 Problem Statement Our work builds upon the notion of a noisy knowledge graph ( NKG ) , which consists of a directed graphG= ( V , E)whereVis a set of concepts andEthe set of labelled binary semantic relations \u2013 e.g. , those found between synsets like , for instance , hypernymy or meronymy within a semantic network like WordNet .", "entities": []}, {"text": "In a NKG we assume bothVandEto have been acquired automatically , e.g. , in order to induce a domain - aware or a general purpose knowledge base .", "entities": []}, {"text": "Additionally , we consider for our purposes the hypernymy graph T= ( TV , TE)ofG , the subgraph made up of the hypernymy ( i.e. , isa - labeled ) edges of E. SinceT is a subgraph of G , we can expect that the former inherits a certain amount of noise from the latter .", "entities": []}, {"text": "Noise within hypernymy graphs can be further classi\ufb01ed into : i ) noisy nodes , the concepts that do not belong to a speci\ufb01c target vocabulary , e.g. , domain concepts for domain - speci\ufb01c KBs , such asJaguar Cars within a zoological taxonomy ; ii)noisy edges , the wrongly - acquired relations between unrelated concepts or out - of - domain relations , e.g. , Jaguar Cars isaFeline ; iii ) cycles of hypernymy relations , such as those derived from counts over very large corpora ( Seitner et al . , 2016 ) , e.g. , jaguar ( Panthera onca ) \u2192 feline\u2192animal\u2192jaguar ( Panthera onca ) .", "entities": []}, {"text": "We accordingly de\ufb01ne the task of extracting a clean taxonomy from a NKG as that of pruning the cycles , as well as the noisy edges and nodes , from the hypernymy subgraph TofG. 3.2 Resources Used In order to enable end - to - end taxonomy induction from scratch , we combine our general approach with existing KBs that have been automatically induced from text and linked to reference lexical knowledge bases on the basis of unsuper - vised methods .", "entities": []}, {"text": "To this end , we use the linked disambiguated distributional KBs from Faralli et al .", "entities": []}, {"text": "( 2016)1 , which are built in three steps : 1)Learning a JoBimText model .", "entities": []}, {"text": "Initially , a sense inventory is created from a large text collection using the pipeline of the JoBimText project ( Biemann and Riedl , 2013).2The resulting structure contains disambiguated protoconcepts ( i.e. , senses ) , their similar and related terms , as well as aggregated contextual clues per proto - concept .", "entities": []}, {"text": "2)Disambiguation of related terms .", "entities": []}, {"text": "Similar terms and hypernyms associated with a protoconcept are fully disambiguated based on the partial disambiguation from step ( 1 ) .", "entities": []}, {"text": "The result is a proto - conceptualization ( PCZ ) , where all terms have a sense identi\ufb01er .", "entities": []}, {"text": "3)Linking to a lexical resource .", "entities": []}, {"text": "The PCZ is automatically aligned with an existing lexical resource ( LR ) such as WordNet or BabelNet .", "entities": []}, {"text": "For example , bridge : NN:3 is linked to the Babel synset bn:00013077n ( the \u2018 infrastructure \u2019 sense ) .", "entities": []}, {"text": "That is , a mapping between the two sense inventories is created to combine them into a new extended sense inventory , a hybrid aligned resource .", "entities": []}, {"text": "Table 1 shows the proto - conceptualization entries for the polysemous terms bridge andlink , namely their \ufb01gurative ( \u201c bridge : NN:2 \u201d and \u201c link : NN:1 \u201d ) and concrete \u2018 infrastructure \u2019 ( \u201c bridge : NN:3 \u201d and \u201c link : NN:0 \u201d ) senses , respectively .", "entities": []}, {"text": "JoBimText models provide sense distinctions that are only partially disambiguated : the list of similar and hypernyms terms of each sense , in fact , does not carry sense information .", "entities": []}, {"text": "Consequently , a semantic closure procedure is applied in order to obtain a PCZ and arrive at sense representation in which all terms get assigned a unique , best-\ufb01tting sense identi\ufb01er ( see Faralli et al .", "entities": []}, {"text": "( 2016 ) for details ) .", "entities": []}, {"text": "PCZs consist of a rich , yet noisy , disambiguated semantic network automatically induced from large amounts of text : links to existing lexical resources provide us a source of external supervision that can be leveraged to clean them and turn them into full-\ufb02edged taxonomies .", "entities": []}, {"text": "Steps 1\u20133 are unsupervised by nature .", "entities": []}, {"text": "Consequently , when 1https://madata.bib.uni-mannheim.de/171/ 2http://www.jobimtext.org592", "entities": []}, {"text": "entry similar terms hypernyms bridge : NN:2 gap : NN:2 , divide : NN:2 , link : NN:1 , ... issue : NN:2 , topic : NN:3 , ... bridge : NN:3 road : NN:0 , highway : NN:1 , overpass : NN:3 ... infrastructure : NN:1 , project : NN:1 , ... link : NN:0 connection : NN:3 , correlation : NN:1 , linkage : NN:1 ... service : NN:6 , feature : NN:0 , ... link : NN:1 relationship : NN:1 , interaction : NN:1 , divide : NN:0 ... problem : NN:1 , topic : NN:3 ...", "entities": []}, {"text": "Table 1 : Excerpt of a proto - conceptualization ( PCZ ) for the words \u201c bridge : NN \u201d and \u201c link : NN \u201d .", "entities": []}, {"text": "combined with our algorithm they provide a complete framework for fully unsupervised taxonomy induction from scratch .", "entities": []}, {"text": "Note , however , that our approach offers a general solution to the problem of taxonomy cleaning .", "entities": []}, {"text": "In an additional set of experiments , we apply it to different automatically generated taxonomies from a SemEval task in a more controlled setting where we rely on a few manually created KB links only .", "entities": []}, {"text": "3.3", "entities": []}, {"text": "The ContrastMedium Algorithm At its core , our algorithm relies on the notion of alinked noisy knowledge graph ( LNKG ) .", "entities": []}, {"text": "This consists of a quintuple ( G , KB , KBroot , \u03bb , M ) where : i)G= ( VG , EG)is a noisy knowledge graph ; ii)KB = ( VKB , EKB)is a companion knowledge base providing a ground - truth taxonomy ; iii)KBrootis the root node of the reference knowledge base KB ( if several top - level nodes exist , an arti\ufb01cial root can be created by connecting them all ) ; iv ) \u03bbis a conventional symbol to represent the \u201c unde\ufb01ned concept \u201d , i.e. , a place - holder for empty mappings ; v ) M : VG\u2192 VKB\u222a{\u03bb}is the function , which maps nodes of VGinto nodes of VKBor into the unde\ufb01ned concept\u03bb .", "entities": []}, {"text": "The key ideas behind ContrastMedium are : \u2022Identi\ufb01cation of important topological clues from the companion knowledge base KB in order to hierarchically sort the concepts in G. For our purposes , KB is expected to be able to provide ground - truth taxonomic relations that can be safely projected onto Gto guide the cleaning process : that is , we assume it to be reasonably clean .", "entities": []}, {"text": "In contrast , we do not make any assumption on how KB has been created : our approach can be used with either manually created taxonomies like WordNet or ( semi)automatically induced ones , provided they are of suf\ufb01cient quality .", "entities": []}, {"text": "Hence , our method is knowledge - based without the need of further supervision other than that contained in KB ; \u2022Projection of topological clues from KB back onto the LNKG", "entities": []}, {"text": "Gon the basis of the links Pruning of T clean taxonomy   T\u2019(G , KB , KBroot , \u03bb , M )", "entities": []}, {"text": "A contrast medium drop enters   the KB through the root node   KBroot CKB", "entities": []}, {"text": "The drop flows through the   common / linked node of T 2 initial CT final CTShaking T by turning it up ,   down and up .", "entities": []}, {"text": "3 41 KB KB T T T\u2019M Figure 1 : ContrastMedium : algorithm work\ufb02ow .", "entities": []}, {"text": "found in the mapping M. Similarly to the case of the reference knowledge base , we do not make any assumption on how the links between GandKB have been created : while there exists different methods to automatically link ( lexical ) knowledge bases ( Navigli and Ponzetto , 2012 ; Faralli et al . , 2016 ) , we later show that it is also possible to achieve state - of - the - art performance with a few manually given links ; \u2022Propagation of the topological clues across the entire NKG G. That is , to cope with the partial coverage of automatic mappings , as well as the need to reduce the number of manually created KB links , we apply a signal propagation technique that solely relies on the structure of G ; \u2022To make use of the resulting topological clues to drive the taxonomy pruning process .", "entities": []}, {"text": "That is , propagated topological clues from KB are additionally leveraged to ensure that the output593", "entities": []}, {"text": "ALGORITHM 1 : The ContrastMedium algorithm .", "entities": []}, {"text": "Input : ( G= ( V , E ) , KB = ( VKB , EKB),KB root , \u03bb , M ) Output : hypernymy graph T / primeofG , s.t . T / primehas no cycles .", "entities": []}, {"text": "//Estimating clues from KB ( Fig . 1 , step 1 )", "entities": []}, {"text": "1\u2200x\u2208VKB : CKB(x ) = 0 ; 2injectContrastMedium ( KB , KBroot ) ;", "entities": [[5, 6, "DatasetName", "0"]]}, {"text": "//Transferring clues from KB to G ( Fig .", "entities": []}, {"text": "1 , step 2 ) 3T= ( VT , ET)\u2190\u2212 hypernymyGraph ( G ) ;", "entities": []}, {"text": "4\u2200x\u2208VT : CT(x ) = 0 ; 5transferClues ( M , KB , T , CKB , CT ) ; //Shaking the graph T(Fig .", "entities": [[5, 6, "DatasetName", "0"]]}, {"text": "1 , step 3 ) 6shake(UP , T , CT);//propagate through in - edges 7shake(DOWN , T , CT);//propagate through out - edges 8shake(UP , T , CT);//propagate through in - edges //Pruning the graph T(Fig .", "entities": []}, {"text": "1 , step 4 ) 9T /", "entities": []}, {"text": "prime= prune(T , CT ) ; 10returnT / prime ; results in a proper taxonomic structure .", "entities": []}, {"text": "We rely on the metaphor of a contrast medium ( CM ) to describe our approach , which is summarized in Figure 1 .", "entities": []}, {"text": "In the context of clinical analysis , a CM is injected into the human body to highlight speci\ufb01c complex internal body structures ( in general , the venuous system ) .", "entities": []}, {"text": "In a similar fashion , we detect the topological structure of a graph by propagating a certain amount of CM that we initially inject through the node KBroot of the companion knowledge base KB .", "entities": []}, {"text": "The highlighted structure indicates the distance of a node with respect to the node KBroot .", "entities": []}, {"text": "Then the lowest values of contrast medium indicate the leaf terminological nodes .", "entities": []}, {"text": "The observed quantities are then transferred to corresponding nodes of the noisy graph by the mapping M. Next , the medium is propagated by \u2019 shaking \u2019 the noisy graph .", "entities": []}, {"text": "We let the \ufb02uid reach all the components Gby alternating two phases of propagation : letting the CM to \ufb02ow through both incoming ( \u2018 shake up \u2019 ) ; and outgoing ( \u2018 shake down \u2019 ) edges .", "entities": []}, {"text": "At the end , we use the partial order induced by the observed node level of CM to drive the pruning phase , and \u2018 stretch \u2019 the original NKG Ginto a proper DAG .", "entities": []}, {"text": "Our approach is presented in Algorithms 1 and 2.3It consists of the following main steps : 1 ) CM injection Cf .", "entities": []}, {"text": "Figure 1 , block 1 and Algorithm 1 , lines 1 - 2 .", "entities": []}, {"text": "We initially de\ufb01ne the functionCKB : VKB\u2192[0.0\u22121.0]and assign a zero contrast medium level to all the nodes of the KB graphCKB(x )", "entities": []}, {"text": "= 0,x\u2208VKB(line 1 ) .", "entities": []}, {"text": "Next , 3A demo is available at http://web.informatik .", "entities": []}, {"text": "uni-mannheim.de/faralli/cm.html with examples of the application of ContrastMedium to a few simple LNKGs .", "entities": []}, {"text": "ALGORITHM 2 : The Shake routine .", "entities": []}, {"text": "Input : direction may be UP or DOWN , graph = ( Vgraph , Egraph ) , Cgraph Output : the updatedCgraph 1foreachx\u2208Vgraph do 2Current graph ( x )", "entities": []}, {"text": "= Cgraph ( x);Flown graph = 0.0 ; //iteratively propagates the CM 3fori= 0to|Egraph|\u22121do 4 foreachx\u2208Vgraph", "entities": []}, {"text": "do 5InOut graph ( x ) = 0.0 ; 6 foreachxs.t .", "entities": []}, {"text": "Current graph ( x)>0.0do 7CMlevel = Current graph ( x ) ; 8 ifdirection = = DOWN then 9 O = outgoingEdges ( x , graph ) ; 10 foreach ( x , y)\u2208Ido 11 InOut graph ( y)+ = CMlevel max ( |O|,1 ) ; 12 else 13 I = incomingEdges ( x , graph ) ; 14 foreach ( y , x)\u2208Ido 15 InOut graph ( y)+ = CMlevel max ( |I|,1 ) ; 16 Flown graph ( x)+ = CMlevel ; 17 foreachx\u2208Vgraph do 18 Current graph ( x ) = InOut graph ( x ) ; 19foreachx\u2208Vgraph do 20Cgraph ( x ) = Flown graph ( x ) ; we call the routine \u2018 injectContrastMedium \u2019 which : 1 ) assigns an initial contrast level equals to 1.0 to the node KBroot of theKB graph ; ii ) uses the routine \u201c Shake \u201d with the direction parameter equals to \u201c DOWN \u201d ( see Algorithm 2 and Step 3 \u201c Graph shaking \u201d for more details ) to let the CM drop through KB .", "entities": []}, {"text": "In practice , the shaking routine implements a node contrast medium level propagation algorithm following the outgoing ( \u2018 down \u2019 ) or the incoming ( \u2018 up \u2019 ) edges of the graph .", "entities": []}, {"text": "2 ) CM transfer Cf .", "entities": []}, {"text": "Figure 1 , block 2 and Algorithm 1 , lines 3 - 5 .", "entities": []}, {"text": "In the next phase , we \ufb01rst extract the hypernymy subgraph T= ( VT , ET ) ofG(see Section 3.1 ) and then follow the links in the mapping Mto transfer the contrast medium levels , i.e. ,CT(y )", "entities": []}, {"text": "= CKB(x)(s.t.x\u2208VKB , y\u2208 VT,(y\u2192x)\u2208M ) .", "entities": []}, {"text": "3 ) Graph shaking Cf .", "entities": []}, {"text": "Figure 1 , block 3 and Algorithm 1 , lines 6 - 8 .", "entities": []}, {"text": "After having transferred the CM to the target hypernym graph TofG , we shakeTto let the CM \ufb02ow by traversing the incoming , the outgoing , and \ufb01nally the incoming edges again \u2013 see Algorithm 2 for details on the \u2018 Shake \u2019 routine .", "entities": []}, {"text": "Note that these two kinds of propagation are needed since the CM needs to be propagated through all the nodes of the graph to highlight the topological clues we are searching for .", "entities": []}, {"text": "In particular , in Algorithm 2 at each iterationtfor each node x\u2208Vgraph , depending on594", "entities": []}, {"text": "the value of the parameter direction ( line 8 and line 12 ): i ) we observe a CM level for the node x ( line 7 ) ; ii ) if direction = = DOWN ( lines 9 - 11 ) we traverse all the outgoing edges ( x , y)ofxand", "entities": []}, {"text": "propagate the observed CM level of x , otherwise ( direction = = UP , lines 13 - 15 ) we traverse the incoming edges ( y , x)and propagate the CM level to the nodes y ; iii ) the value of Flown graph(x)is incremented by the observed CM level ( line 16 ) ; iv ) for each node xwe reset the current observed value of the CM level with the portion of the liquid which has \ufb02own from the incoming or the outgoing edges during the propagation ( lines 17 - 18 ) .", "entities": []}, {"text": "Depending on the propagation direction , we have two different behaviours for the CM .", "entities": []}, {"text": "When exiting a node xthrough out the outgoing edges ( direction = = DOWN ) we increment the level of contrast medium of the reached nodes by the observed value of xdivided by number of outgoing edges of x. By converse , when we climb ( direction = = UP ) across the incoming edges of a nodexwe increment the CM level of the reached node by the observed CM quantity of xdivided by the number of incoming edges of x. Note that the sequence UP / DOWN / UP and the specular DOWN / UP / DOWN are the only ones from the 8 possible combinations which can guarantee the contrast medium to \ufb02ow on the entire graph .", "entities": [[40, 41, "DatasetName", "converse"]]}, {"text": "We simply selected the \ufb01rst sequence since the \ufb01nal rank places candidate root nodes on the top ( and candidate leaf nodes on the bottom ) .", "entities": []}, {"text": "4 ) Pruning Cf .", "entities": []}, {"text": "Figure 1 , block 4 and Algorithm 1 , lines 9 .", "entities": []}, {"text": "Finally , we create a clean taxonomy T / prime by pruning the graph Ton the basis of the contrast levels found in CT .", "entities": []}, {"text": "CM levels in CTcan be used to induce a order of the nodes that , intuitively , captures the level of conceptual abstraction for the nodes inT.", "entities": []}, {"text": "We use them to produce a clean taxonomy as follows .", "entities": []}, {"text": "We \ufb01rst sort the nodes v\u2208VT in a listS = s0,s1, ... ,s|VT|\u22121by the decreasing resulting CM level value in CT .", "entities": []}, {"text": "The nodes with a higher level of contrast medium are candidates to be at the top level while the ones at the end of the list are candidates to be leaf nodes of the output taxonomy .", "entities": []}, {"text": "Next , the pruning routine starts from a graph T / prime= ( VT / prime = VT , ET / prime=\u2205)and for each nodes\u2208S(from the last node to the \ufb01rst ) add toET / primeall the edges of the kind e= ( y , s ) where a path from ytosdoes not exists in Tand withybelonging to one of the following : i ) the set of peers{x\u2208Ss.t . CT(x )", "entities": []}, {"text": "= CT(s ) } ; ii ) theascending ordered list of preceding ( x\u2208Ss.t . CT(x ) >", "entities": []}, {"text": "C T(s ) ) ; iii ) the ascending ordered list of following ( x\u2208Ss.t . CT(x)<CT(s ) )", "entities": []}, {"text": "Complexity analysis .", "entities": []}, {"text": "The propagation step ( Figure 1 , blocks 1 and 3 ; Algorithm 2 ) costs O(|E|\u2217|V|)since we iteratively analyze all the nodes ofVfor a number|E|of iterations .", "entities": []}, {"text": "The \ufb01nal step of pruning ( Figure 1 , block 4 ) , instead , can have a time cost O(|V|2\u2217(|E|+|V| ) ) , since , in the worst case , the algorithm must analyse all the possible pairs of vertices , and then test the existence of a directed path between the candidate pairs of nodes .", "entities": []}, {"text": "4 Experiments We perform two sets of experiments .", "entities": []}, {"text": "We \ufb01rst evaluate our approach when applied to large , automatically induced noisy knowledge graphs ( Section 4.1 ) and then quantify the impact it can have to further improve the quality of the output of state - ofthe - art taxonomy induction systems ( Section 4.2 ) .", "entities": [[13, 15, "TaskName", "knowledge graphs"]]}, {"text": "4.1 Experiment 1 : Pruning existing LNKG", "entities": []}, {"text": "We \ufb01rst apply ContrastMedium to a variety of knowledge graphs that have been automatically acquired and linked to reference KBs like WordNet and BabelNet using unsupervised methods ( Section 3.2 ) .", "entities": [[8, 10, "TaskName", "knowledge graphs"]]}, {"text": "Our research questions ( RQs ) are : RQ1 Can we use ContrastMedium as component of a complete framework for fully unsupervised taxonomy induction from scratch ?", "entities": []}, {"text": "RQ2 What is the quality of the resulting taxonomies ?", "entities": []}, {"text": "4.1.1", "entities": []}, {"text": "Experimental Setting We apply our pruning algorithm to the automatically acquired KBs presented by Faralli et al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": ".", "entities": []}, {"text": "These noisy knowledge graphs have been induced from large text corpora and include both taxonomic and other ( i.e. , related , topically associative ) semantic relations ( cf .", "entities": [[2, 4, "TaskName", "knowledge graphs"]]}, {"text": "Table 1 ) , as well as automatically induced mappings to lexical knowledge bases like WordNet and BabelNet .", "entities": []}, {"text": "These NKGs have been induced from a 100 million sentence news corpus ( news ) and from a 35 million sentence Wikipedia corpus ( wiki ) , using different parameter values to generate sense inventories of different granularities ( e.g. , 1.8 vs. 6.0 average senses per term for the wiki - p1.8 and wiki - p6.0 datasets , respectively ) .", "entities": []}, {"text": "Table 2 shows some of595", "entities": []}, {"text": "senses polysemy hypernyms links hypernymy graph dataset # avg .", "entities": []}, {"text": "max # avg . # |VT| |ET| news - p1.6 332k 1.6 18 15k 6.9 60k 170k 1.538k news - p2.3 461k 2.3 17 15k 5.8 95k 225k 1.871k wiki - p1.8 368k 1.8 15 15k 4.4 67k 185k 1.167k wiki - p6.0 1.5 M 6.0 36 52k 1.7 279k 394k 1.901k Table 2 : Dimensions of the four datasets adopted as linked noisy knowledge graphs ( Faralli et al . , 2016 ) .", "entities": [[64, 66, "TaskName", "knowledge graphs"]]}, {"text": "the dimensions for each of the four NKGs \u2013 number of senses , average and maximum sense polysemy , number and average hypernyms per sense , the number of linked senses to WordNet concepts ( i.e. , \u201c links \u201d ) , and the number of nodes and edges for the corresponding hypernymy graph .", "entities": []}, {"text": "Since our algorithm primarily focuses on conceptual hierarchical ( taxonomic ) structures \u2013 referred to as the TBox in Knowledge Representation \u2013 we use the WordNet mappings only , since the manual inspection of the BabelNet mappings revealed that they are focused primarily on instances ( that is , ABox statements ) .", "entities": []}, {"text": "In order to have a complete quintuple for each NKG , we selected , for the companion KB , the top KBrootconcept entity of the WordNet taxonomy ( SynsetID SID-00001740 - N ) .", "entities": []}, {"text": "4.1.2 Measures We benchmark ContrastMedium using a variety of metrics that are meant to capture structural properties of the output taxonomies ( to describe the impact of pruning on the original NKGs ) , as well as an estimation of their overall quality .", "entities": []}, {"text": "Edge compression : the ratio of the number of pruned edges over the total number of edges : CEG , G / prime=|EG|\u2212|EG / prime| |EG|", "entities": []}, {"text": "whereEGandEG / primerepresent the number of edges found within the input ( G ) and pruned ( G / prime ) taxonomy , respectively .", "entities": []}, {"text": "Pruning accuracy : the performance on a 3 - way classi\ufb01cation task to automatically detect the level of granularity of a concept as a proxy to quantify the overall quality of the output taxonomies .", "entities": [[1, 2, "MetricName", "accuracy"]]}, {"text": "Pruning accuracy is estimated using gold - standard annotations that are created from a random sample of 1,000 nodes for each NKG .", "entities": [[1, 2, "MetricName", "accuracy"]]}, {"text": "Two annotators with previous experience in knowledge acquisition and engineering were asked to provide for eachconcept whether it can be classi\ufb01ed as : i ) a root , top - level abstract concept \u2013 i.e. , any of entity , object , etc . and more in general nodes that correspond to abstract concepts that we can expect to be part of a core ontology such as , for instance , DOLCE ( Gangemi et al . , 2002 ) ; ii ) a leaf terminological node ( i.e. , instances such as Lady Gaga orPorsche 911 ) ; iii ) or a middle - level concept ( e.g. , celebrity orcars , concepts not \ufb01tting into any of the previous classes ) .", "entities": [[64, 65, "MethodName", "ontology"]]}, {"text": "An adjudication procedure was used to resolve any discrepancy between the two annotators : the inter - annotator agreement after adjudication is \u03ba= 0.657(Fleiss , 1971 ) , with most disagreement occurring on the identi\ufb01cation of abstract , core ontology concepts .", "entities": [[39, 40, "MethodName", "ontology"]]}, {"text": "A local 3 - way classi\ufb01cation task provides a rather crude way to estimate the performance on inducing hierarchical structures like taxonomies .", "entities": []}, {"text": "Here , we use it primarily to benchmark how well ContrastMedium compares against other , structure - agnostic algorithms used within state - ofthe - art solutions such as , for instance , Tarjan \u2019s topological sorting ( Section 2 ) , which only break cycles in a random fashion .", "entities": []}, {"text": "Given ground - truth concept granularity judgements , we compute standard accuracy for each of the three classes .", "entities": [[11, 12, "MetricName", "accuracy"]]}, {"text": "That is , we compare the system outputs against the gold standards and obtain three accuracy measures : one for the root nodes ( AR ) , one for the nodes \u2018 in the middle \u2019 ( AM ) and \ufb01nally one for the leaf nodes ( AL ) .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "For example a true positive root node is a node annotated as a root node in the gold standard and having no incoming edges in the pruned graph .", "entities": []}, {"text": "Error Reduction ( ER ): \ufb01nally , we compute the relative error reduction of ContrastMedium against other , baseline approaches as : Baseline errors/|sample|\u2212CMerrors/|sample|", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "Baseline errors/|sample| Asbaseline we use the approach of Faralli et al.596", "entities": []}, {"text": "Pruned Knowledge Graph Pruning accuracy ER ContrastMedium Tarjan ( baseline ) ContrastMedium Tarjan ( baseline ) dataset|VG / prime| |EG / prime|CEG , G / prime|VG / prime| |EG / prime|CEG , G / primeARAMALARAMAL news - p1.6 170k 1.536k 0.15 % 170k 1.535k 0.18 % 98.9 98.3 99.3 93.3 94.6 95.3 0.62 news - p2.3 225k 1.867k 0.19 % 225k 1.866k 0.23 % 98.7 98.7 99.9 95.7 94.7 95.6 0.50 wiki - p1.8 183k 1.165k 0.18 % 183k 1.164k 0.22 % 97.6 94.7 97.3 93.1 87.3 94.1 0.41 wiki - p6.0 394k 1.897k 0.18 % 394k 1.896k 0.21 % 95.9 94.3 98.3 89.5 90.1 92.8 0.50", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "Table", "entities": []}, {"text": "3 : Structural analysis , pruning accuracies and error reduction ( ER ) for the four LNKGs .", "entities": []}, {"text": "animal   great    apes feline   lion animal   great    apes feline   lionanimal   great    apes feline   lion1   2 3 animal   great    apes feline   lionXX XXBaseline   ContrastMedium   Figure 2 : An example noisy graph and the different solutions provided by ContrastMedium and the baseline .", "entities": []}, {"text": "( 2015 ) based on Tarjan \u2019s topological sorting ( Section 2 ) , which iteratively searches for a cycle ( until no cycle can be found ) and randomly removes an edge from it .", "entities": []}, {"text": "To the best of our knowledge , this is the only algorithm that we can fairly compare with , since alternative solutions all need to know the sets of root and leaf nodes in advance .", "entities": []}, {"text": "4.1.3 Results and discussion Table 3 summarizes the performance of ContrastMedium on the four automatically acquired NKGs .", "entities": []}, {"text": "The results show that the pruning impact of our approach is lower than that of the baseline ( an average of 1 K edges of difference , cf . columns 3 and 6 ) , which also determines higher edge compressionCEG , G / primevalues for the baseline method .", "entities": []}, {"text": "Despite being less aggressive in terms of the number of edges pruned , ContrastMedium outperforms the Tarjan - based algorithm on all datasets in terms of accuracy .", "entities": [[26, 27, "MetricName", "accuracy"]]}, {"text": "Thanks to our method , in fact , we are able to achieve , even despite the baseline already reaching very high performance levels ( well above 90 % accuracy ) , improvements of up to 6 points , with an overall error reduction between around 40 % and 60 % .", "entities": [[29, 30, "MetricName", "accuracy"]]}, {"text": "To provide anintuition of why ContrastMedium clearly outperforms the baseline approach , we provide in Figure 2 an exempli\ufb01ed depiction of a typical case on which the baseline fails ( based on a manually inspected random sample ) .", "entities": []}, {"text": "In our example , the Tarjan baseline \ufb01rst detects the cycle C1=(lion\u2192 animal\u2192feline\u2192lion ) and randomly decides to break it by removing the edge ( animal \u2192feline ) .", "entities": []}, {"text": "Next , it detects the cycle C2= ( animal\u2192great apes\u2192animal ) and randomly decides to break it by removing the edge ( animal\u2192great apes ) .", "entities": []}, {"text": "ContrastMedium , instead , after the shaking of the graph can leverage the partial ordering of the nodes ( based on the concept granularity of the corresponding concepts ) to select the edges ( animal , feline ) , ( feline , lion ) and ( animal , great apes ) , while removing all remaining wrong and redundant edges .", "entities": []}, {"text": "4.2 Experiment 2 : SemEval-15 task 17", "entities": []}, {"text": "We next evaluate the overall impact of our approach within an existing benchmark for the taxonomy induction task .", "entities": []}, {"text": "Intuitively , most of the bene\ufb01ts from our method derive from the \u201c gold standard \u201d information of the companion KB , and its linking to the NKG , which act as a source of supervision .", "entities": []}, {"text": "Consequently , we address the research question of how much ( pseudo-)supervision our method needs in terms of KB links , and whether it can be used to improve the state - of - the - art on the task of taxonomy induction .", "entities": []}, {"text": "4.2.1 Experimental Setting We use the benchmark data from the SemEval-15 task 17 \u201c Taxonomy Extraction Evaluation : TExEval \u201d ( Bordea et al . , 2015 ) , since it provides us with gold - standard datasets and system outputs within a standard , easy - to - reproduce setting .", "entities": [[5, 7, "DatasetName", "the benchmark"]]}, {"text": "Initially , we select from the participating systems4the two best performing taxonomies based on the Cumula4Cf .", "entities": []}, {"text": "Table \u201c Comparative Evaluation \u201d at http://alt .", "entities": []}, {"text": "qcri.org/semeval2015/task17/index.php?id=evaluation597", "entities": []}, {"text": "0.48 0.5 0.52 0.54 0.56 0.58 0.6 1 2 3 4 5 10Cumulative Fowlkes and Mallows number of linksEquipments baseline SemEval15 - 17 ContrastMedium   0.48 0.5 0.52 0.54 0.56 0.58 0.6 1 2 3 4 5 10Cumulative Fowlkes and Mallows number of linksSciences baseline SemEval15 - 17 ContrastMediumFigure 3 : Performance on the SemEval-15 TExEval dataset ( Cumulative Fowlkes&Mallows measure ) .", "entities": []}, {"text": "tive Fowlkes&Mallows ( CF&M ) measure ( Velardi et al . , 2012 ) , the Equipments andSciences taxonomies from the INRIASAC and the LT3 teams respectively .", "entities": []}, {"text": "We next apply our approach to these taxonomies , in order to clean them in a postprocessing fashion .", "entities": []}, {"text": "By selecting the top - systems we can see how far we can advance the state - ofthe - art overall .", "entities": []}, {"text": "Besides , these two taxonomies are also the ones containing the highest number of cycles , giving the application of our cleaning algorithm a more challenging ( and meaningful ) setting .", "entities": []}, {"text": "To remove the effects of automatic linking and quantify the amount of manual efforts needed by our approach , 10 random concepts from each of these resources are manually linked to WordNet , and the taxonomies subsequently pruned using ContrastMedium and the baseline .", "entities": []}, {"text": "We then evaluate performance following the task \u2019s experimental setting and compute the CF&M measure for different levels of manually - created KB links .", "entities": []}, {"text": "4.2.2 Results and discussion In Table 3 , we report the performance on the SemEval task for the two selected input taxonomies .", "entities": []}, {"text": "Results on the structural similarities of the pruned taxonomies with the gold standard ones , computed using the CF&M measure , indicate that , thanks to ContrastMedium and with a minimal human effort \u2013 the creation of just a few KB links ( up to 10 ) , which are needed only when automatic linking is not available \u2013 it is possible to boost the quality of taxonomies using state - of - art methods by a large margin .", "entities": []}, {"text": "For instance , in the case of the Equipments taxonomy , we improve up to 7 points .", "entities": []}, {"text": "The baseline , which only breaks cycles , is not able to reassess the graph structure and only provides very small improvements to the submitted NKGs .", "entities": []}, {"text": "Overall , the results show that ContrastMedium leads to competitive performance on a hard , realistic benchmark such as TExEval , achieving the best overall results for both taxonomies .", "entities": []}, {"text": "That is , our algorithm is able to improve the state - of - theart on taxonomy induction by additionally boosting the quality of existing top - performing systems for this task : this is achieved on the basis of a minimally supervised approach that only requires a few links to a reference KB , which is used to provide ground - truth taxonomic relations and guide the cleaning process .", "entities": []}, {"text": "5 Conclusions In this paper , we presented ContrastMedium , a novel algorithm that can be applied to automatically linked noisy knowledge graphs to provide an end - to - end solution for fully unsupervised taxonomy induction from scratch , i.e. , without any human effort .", "entities": [[21, 23, "TaskName", "knowledge graphs"]]}, {"text": "Our results indicate that ContrastMedium can be successfully applied to a wide range of automatically acquired KBs , ranging from large linked noisy knowledge graphs all the way to small - scale induced taxonomies to produce high - quality isahierarchies that achieve state - ofthe - art results on SemEval benchmarks .", "entities": [[23, 25, "TaskName", "knowledge graphs"]]}, {"text": "As future work , we plan to improve the scalability of the algorithm , in particular its time complexity order , and apply it to Web - scale resources like the WebIsaDB ( Seitner et al . , 2016 ) or state - of - the - art approaches like TAXI ( Panchenko et al . , 2016 ) , as well as to publicly release the created resources .", "entities": []}, {"text": "Acknowledgments We acknowledge the support of the Deutsche Forschungsgemeinschaft ( DFG ) under the JOINT project.598", "entities": []}, {"text": "References Eneko Agirre , Olatz Ansa , Eduard H. Hovy , and David Mart \u00b4 \u0131nez . 2000 .", "entities": []}, {"text": "Enriching very large ontologies using the WWW .", "entities": []}, {"text": "In Proceedings of the ECAI 2000 Workshop on Ontology Learning .", "entities": [[8, 9, "MethodName", "Ontology"]]}, {"text": "Alessio P. Aprosio , Claudio Giuliano , and Alberto Lavelli .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Extending the coverage of DBpedia properties using distant supervision over Wikipedia .", "entities": [[4, 5, "DatasetName", "DBpedia"]]}, {"text": "InProceedings of the NLP & DBpedia workshop colocated with the 12th International Semantic Web Conference ( ISWC 2013 ) , pages 20\u201331 .", "entities": [[5, 6, "DatasetName", "DBpedia"]]}, {"text": "Chris Biemann and Martin Riedl .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Text : Now in 2D !", "entities": []}, {"text": "A Framework for Lexical Expansion with Contextual Similarity .", "entities": []}, {"text": "Journal of Language Modelling , 1(1):55\u201395 .", "entities": [[2, 4, "TaskName", "Language Modelling"]]}, {"text": "Christian Bizer , Jens Lehmann , Georgi Kobilarov , S\u00a8oren Auer , Christian Becker , Richard Cyganiak , and Sebastian Hellmann .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "DBpedia \u2013 A crystallization point for the web of data .", "entities": [[0, 1, "DatasetName", "DBpedia"]]}, {"text": "Journal Web Semantics , 7(3):154\u2013165 .", "entities": []}, {"text": "Kurt D. Bollacker , Colin Evans , Praveen Paritosh , Tim Sturge , and Jamie Taylor .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Freebase : a collaboratively created graph database for structuring human knowledge .", "entities": []}, {"text": "In Proceedings of the ACM SIGMOD International Conference on Management of Data , pages 1247\u20131250 .", "entities": [[4, 5, "DatasetName", "ACM"], [9, 10, "TaskName", "Management"]]}, {"text": "Georgeta Bordea , Paul Buitelaar , Stefano Faralli , and Roberto Navigli . 2015 .", "entities": []}, {"text": "Semeval-2015 task 17 : Taxonomy extraction evaluation ( TExEval ) .", "entities": []}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 902\u2013910 .", "entities": []}, {"text": "V olha Bryl and Christian Bizer .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Learning con\ufb02ict resolution strategies for cross - language wikipedia data fusion .", "entities": []}, {"text": "In Proceedings of the 23rd International World Wide Web Conference , pages 1129\u20131134 .", "entities": []}, {"text": "Sharon A. Caraballo .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Automatic construction of a hypernym - labeled noun hierarchy from text .", "entities": []}, {"text": "In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , pages 120 \u2013 126 .", "entities": []}, {"text": "Andrew Carlson , Justin Betteridge , Bryan Kisiel , Burr Settles , Estevam R. Hruschka , and Tom M. Mitchell .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Toward an architecture for never - ending language learning .", "entities": []}, {"text": "In Proceedings of the 24th AAAI Conference on Arti\ufb01cial Intelligence , pages 1306 \u2013 1313 .", "entities": []}, {"text": "Christian Chiarcos , Sebastian Hellmann , and Sebastian Nordhoff .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Linking linguistic resources : Examples from the Open Linguistics Working Group .", "entities": []}, {"text": "In Christian Chiarcos , Sebastian Nordhoff , and Sebastian Hellmann , editors , Linked Data in Linguistics - Representing and Connecting Language Data and Language Metadata , pages 201\u2013216 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Allan M. Collins and Elizabeth F. Loftus .", "entities": []}, {"text": "1975 .", "entities": []}, {"text": "A spreading - activation theory of semantic processing .", "entities": []}, {"text": "Psychological Review , 82(6):407 \u2013 428 .", "entities": []}, {"text": "Xin Dong , Evgeniy Gabrilovich , Geremy Heitz , Wilko Horn , Ni Lao , Kevin Murphy , Thomas Strohmann , Shaohua Sun , and Wei Zhang .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Knowledge vault : a web - scale approach to probabilistic knowledge fusion .", "entities": []}, {"text": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 601\u2013610 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Arnab Dutta , Christian Meilicke , and Simone Paolo Ponzetto .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A probabilistic approach for integrating heterogeneous knowledge sources .", "entities": []}, {"text": "In Proceedings of the 11th Extended Semantic Web Conference , pages 286\u2013301 .", "entities": []}, {"text": "Oren Etzioni , Anthony Fader , Janara Christensen , Stephen Soderland , and Mausam . 2011 .", "entities": []}, {"text": "Open information extraction : The second generation .", "entities": [[0, 3, "TaskName", "Open information extraction"]]}, {"text": "In Proceedings of the 22nd International Joint Conference on Arti\ufb01cial Intelligence , pages 3\u201310 .", "entities": []}, {"text": "Anthony Fader , Stephen Soderland , and Oren Etzioni . 2011 .", "entities": []}, {"text": "Identifying relations for open information extraction .", "entities": [[3, 6, "TaskName", "open information extraction"]]}, {"text": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , pages 1535\u20131545 .", "entities": []}, {"text": "Stefano Faralli and Roberto Navigli .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Growing Multi - Domain Glossaries from a Few Seeds using Probabilistic Topic Models .", "entities": [[11, 13, "TaskName", "Topic Models"]]}, {"text": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 170\u2013181 .", "entities": []}, {"text": "Stefano Faralli , Giovanni Stilo , and Paola Velardi .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Large scale homophily analysis in twitter using a twixonomy .", "entities": []}, {"text": "In Proceedings of the 24th International Joint Conference on Arti\ufb01cial Intelligence , pages 2334\u20132340 .", "entities": []}, {"text": "Stefano Faralli , Alexander Panchenko , Chris Biemann , and Simone P. Ponzetto . 2016 .", "entities": []}, {"text": "Linked disambiguated distributional semantic networks .", "entities": []}, {"text": "In The Semantic Web \u2013 ISWC 2016 : 15th International Semantic Web Conference , pages 56\u201364 .", "entities": []}, {"text": "Manaal Faruqui and Shankar Kumar .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "2015 .", "entities": []}, {"text": "Multilingual open relation extraction using cross - lingual projection .", "entities": [[2, 4, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1351\u20131356 .", "entities": []}, {"text": "Christiane Fellbaum , editor .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "WordNet :", "entities": []}, {"text": "An Electronic Database .", "entities": []}, {"text": "MIT Press , Cambridge , MA .", "entities": [[3, 4, "DatasetName", "Cambridge"]]}, {"text": "Joseph L Fleiss .", "entities": []}, {"text": "1971 .", "entities": []}, {"text": "Measuring nominal scale agreement among many raters .", "entities": []}, {"text": "Psychological bulletin , 76(5):378 .", "entities": []}, {"text": "Aldo Gangemi , Nicola Guarino , Claudio Masolo , Alessandro Oltramari , and Luc Schneider . 2002 .", "entities": []}, {"text": "Sweetening ontologies with DOLCE .", "entities": []}, {"text": "In Proceedings of the 13th International Conference on599", "entities": []}, {"text": "Knowledge Engineering and Knowledge Management , pages 166\u2013181 .", "entities": [[4, 5, "TaskName", "Management"]]}, {"text": "Robert L. Glass and Iris Vessey .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Contemporary application - domain taxonomies .", "entities": []}, {"text": "IEEE Software , 12(4):63\u201376 .", "entities": []}, {"text": "Rahul Gupta , Alon Y .", "entities": []}, {"text": "Halevy , Xuezhi Wang , Steven Euijong Whang , and Fei Wu . 2014 .", "entities": []}, {"text": "Biperpedia : An ontology for search applications .", "entities": [[3, 4, "MethodName", "ontology"]]}, {"text": "In Proceedings of the 40th International Conference on Very Large Data Bases , pages 505\u2013516 .", "entities": []}, {"text": "Marti A. Hearst . 1992 .", "entities": []}, {"text": "Automatic acquisition of hyponyms from large text corpora .", "entities": []}, {"text": "In Proceedings of the 14th Conference on Computational Linguistics Volume 2 , pages 539\u2013545 .", "entities": []}, {"text": "Johannes Hoffart , Fabian Suchanek , Klaus Berberich , and Gerhard Weikum .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "YAGO2 :", "entities": []}, {"text": "A spatially and temporally enhanced knowledge base from Wikipedia .", "entities": []}, {"text": "Arti\ufb01cial Intelligence , 194(28):28 \u2013 61 .", "entities": []}, {"text": "Pavan Kapanipathi , Prateek Jain , Chitra Venkataramani , and Amit Sheth .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "User interests identi\ufb01cation on Twitter using a hierarchical knowledge base .", "entities": []}, {"text": "In The Semantic Web : Trends and Challenges , pages 99\u2013113 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Zornitsa Kozareva and Eduard Hovy .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "A semi - supervised method to learn and construct taxonomies using the web .", "entities": []}, {"text": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , pages 1110\u20131118 .", "entities": []}, {"text": "L\u00b4aszl\u00b4o Lov \u00b4 asz .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "Random walks on graphs : A survey .", "entities": []}, {"text": "Combinatorics , 2:146 .", "entities": []}, {"text": "Mike Mintz , Steven Bills , Rion Snow , and Dan Jurafsky . 2009 .", "entities": []}, {"text": "Distant supervision for relation extraction without labeled data .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 1003\u20131011 .", "entities": []}, {"text": "Roberto Navigli and Simone Paolo Ponzetto .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Babelnet : The automatic construction , evaluation and application of a wide - coverage multilingual semantic network .", "entities": []}, {"text": "Arti\ufb01cial Intelligence , 193:217 \u2013 250 .", "entities": []}, {"text": "Maximilian Nickel , Kevin Murphy , V olker Tresp , and Evgeniy Gabrilovich . 2016a .", "entities": []}, {"text": "A review of relational machine learning for knowledge graphs .", "entities": [[7, 9, "TaskName", "knowledge graphs"]]}, {"text": "Proceedings of the IEEE , 104(1):11\u201333 .", "entities": []}, {"text": "Maximilian Nickel , Lorenzo Rosasco , and Tomaso A. Poggio .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "Holographic embeddings of knowledge graphs .", "entities": [[3, 5, "TaskName", "knowledge graphs"]]}, {"text": "In Proceedings of the 30th AAAI Conference on Arti\ufb01cial Intelligence , pages 1955\u20131961 .", "entities": []}, {"text": "Michael P. Oakes .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Using Hearst \u2019s rules for the automatic acquisition of hyponyms for mining a pharmaceutical corpus .", "entities": []}, {"text": "In Proceedings of the RANLP 2005 Text Mining Workshop , pages 63\u201367.Alexander Panchenko , Stefano Faralli , Eugen Ruppert , Steffen Remus , Hubert Naets , C \u00b4 edrick Fairon , Simone Paolo Ponzetto , and Chris Biemann . 2016 .", "entities": []}, {"text": "TAXI at SemEval-2016 Task 13 : A taxonomy induction method based on lexico - syntactic patterns , substrings and focused crawling .", "entities": []}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation ( SemEval 2016 ) , pages 1320\u20131327 .", "entities": []}, {"text": "Alan Ritter , Stephen Soderland , and Oren Etzioni .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "What is this , anyway : Automatic hypernym discovery .", "entities": [[7, 9, "TaskName", "hypernym discovery"]]}, {"text": "In Proceedings of the AAAI Spring Symposium on Learning by Reading and Learning to Read , pages 88\u201393 .", "entities": []}, {"text": "Julian Seitner , Christian Bizer , Kai Eckert , Stefano Faralli , Robert Meusel , Heiko Paulheim , and Simone Paolo Ponzetto . 2016 .", "entities": []}, {"text": "A large database of hypernymy relations extracted from the web .", "entities": []}, {"text": "In Proceedings of the 10th International Conference on Language Resources and Evaluation , pages 360 \u2013 367 .", "entities": []}, {"text": "Rion Snow , Daniel Jurafsky , and Andrew Y .", "entities": []}, {"text": "Ng . 2006 .", "entities": []}, {"text": "Semantic taxonomy induction from heterogenous evidence .", "entities": []}, {"text": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics , pages 801\u2013808 .", "entities": []}, {"text": "Richard Socher , Danqi Chen , Christopher D Manning , and Andrew Ng . 2013 .", "entities": []}, {"text": "Reasoning with neural tensor networks for knowledge base completion .", "entities": [[6, 9, "TaskName", "knowledge base completion"]]}, {"text": "In Advances in Neural Information Processing Systems , pages 926\u2013934 .", "entities": []}, {"text": "Robert Tarjan . 1972 .", "entities": []}, {"text": "Depth-\ufb01rst search and linear graph algorithms .", "entities": []}, {"text": "SIAM Journal on Computing , 1:146\u2013160 .", "entities": []}, {"text": "Paola Velardi , Roberto Navigli , Stefano Faralli , and Juana Mar \u00b4 \u0131a Ruiz - Mart \u00b4 \u0131nez .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "A new method for evaluating automatically learned terminological taxonomies .", "entities": []}, {"text": "In Proceedings of the 8th International Conference on Language Resources and Evaluation , pages 1498\u20131504 .", "entities": []}, {"text": "Paola Velardi , Stefano Faralli , and Roberto Navigli .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "OntoLearn Reloaded : A Graph - Based Algorithm for Taxonomy Induction .", "entities": []}, {"text": "Computational Linguistics , 39(3):665\u2013707 .", "entities": []}, {"text": "Zhichun Wang , Juanzi Li , Zhigang Wang , and Jie Tang .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Cross - lingual knowledge linking across wiki knowledge bases .", "entities": []}, {"text": "In Proceedings of the 21st International World Wide Web Conference , pages 459 \u2013 468 .", "entities": []}, {"text": "Wentao Wu , Hongsong Li , Haixun Wang , and Kenny Q Zhu . 2012 .", "entities": []}, {"text": "Probase : a probabilistic taxonomy for text understanding .", "entities": []}, {"text": "In Proceedings of the ACM SIGMOD International Conference on Management of Data , pages 481\u2013492.600", "entities": [[4, 5, "DatasetName", "ACM"], [9, 10, "TaskName", "Management"]]}]