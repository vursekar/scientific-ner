[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 3790\u20133796 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics3790Neural Topic Modeling by", "entities": []}, {"text": "Incorporating Document Relationship Graph Deyu Zhou\u0003Xuemeng Hu", "entities": []}, {"text": "Rui Wang School of Computer Science and Engineering , Key Laboratory of Computer Network and Information Integration , Ministry of Education , Southeast University , China fd.zhou , xuemenghu , rui wangg@seu.edu.cn Abstract Graph Neural Networks ( GNNs ) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community .", "entities": []}, {"text": "In this paper , we propose Graph Topic Model ( GTM ) , a GNN based neural topic model that represents a corpus as a document relationship graph .", "entities": []}, {"text": "Documents and words in the corpus become nodes in the graph and are connected based on document - word cooccurrences .", "entities": []}, {"text": "By introducing the graph structure , the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution .", "entities": [[35, 36, "MethodName", "convolution"]]}, {"text": "Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach .", "entities": []}, {"text": "1 Introduction Probabilistic topic models ( Blei , 2012 ) are tools for discovering main themes from large corpora .", "entities": [[3, 5, "TaskName", "topic models"]]}, {"text": "The popular Latent Dirichlet Allocation ( LDA ) ( Blei et al . , 2003 ) and its variants ( Lin and He , 2009 ; Zhao et al . , 2010 ; Zhou et", "entities": [[6, 7, "MethodName", "LDA"]]}, {"text": "al . , 2014 ) are effective in extracting coherent topics in an interpretable manner , but usually at the cost of designing sophisticated and model - speci\ufb01c learning algorithm .", "entities": []}, {"text": "Recently , neural topic modeling that utilizes neuralnetwork - based black - box inference has been the main research direction in this \ufb01eld .", "entities": []}, {"text": "Notably , NVDM ( Miao et al . , 2016 ) employs variational autoencoder ( V AE ) ( Kingma and Welling , 2013 ) to model topic inference and document generation .", "entities": [[12, 14, "MethodName", "variational autoencoder"], [16, 17, "MethodName", "AE"]]}, {"text": "Speci\ufb01cally , NVDM consists of an encoder inferring topics from documents and a decoder generating documents from topics , where the latent topics are constrained by a Gaussian prior .", "entities": []}, {"text": "Srivastava and \u0003Corresponding author .", "entities": []}, {"text": "Sutton ( 2017 ) argued that Dirichlet distribution is a more appropriate prior for topic modeling than Gaussian in NVDM and proposed ProdLDA that approximates the Dirichlet prior with logistic normal .", "entities": []}, {"text": "There are also attempts that directly enforced a Dirichlet prior on the document topics .", "entities": []}, {"text": "W - LDA ( Nan et al . , 2019 ) models topics in the Wasserstein autoencoders ( Tolstikhin et al . , 2017 ) framework and achieves distribution matching by minimizing their Maximum Mean Discrepancy ( MMD ) ( Gretton et al . , 2012 ) , while adversarial topic model ( Wang et al . , 2019a , b , 2020 ) directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network ( GAN ) ( Goodfellow et", "entities": [[2, 3, "MethodName", "LDA"], [16, 17, "MethodName", "autoencoders"], [37, 38, "DatasetName", "MMD"], [85, 88, "MethodName", "Generative Adversarial Network"], [89, 90, "MethodName", "GAN"]]}, {"text": "al . , 2014 ) .", "entities": []}, {"text": "Recently , due to the effectiveness of Graph Neural Networks ( GNNs ) ( Li et al . , 2015 ; Kipf and Welling , 2016 ; Zhou et", "entities": []}, {"text": "al . , 2018 ) in embedding graph structures , there is a surge of interests of applying GNN to natural language processing tasks ( Yasunaga et al . , 2017 ; Song et al . , 2018 ; Yao et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "For example , GraphBTM ( Zhu et al . , 2018 ) is a neural topic model that incorporates the graph representation of a document to capture biterm cooccurrences in the document .", "entities": []}, {"text": "To construct the graph , a sliding window over the document is employed and all word pairs in the window are connected .", "entities": []}, {"text": "A limitation of GraphBTM is that only word relationships are considered while ignoring document relationships .", "entities": []}, {"text": "Since a topic is possessed by a subset of documents in the corpus , we believe that the topical neighborhood of a document , i.e. , documents with similar topics , would help determine the topics of a document .", "entities": []}, {"text": "To this end , we propose Graph Topic Model ( GTM ) , a neural topic model that a corpus is represented as a document relationship graph where documents and words in the corpus are nodes and they are connected based on", "entities": []}, {"text": "3791document - word co", "entities": []}, {"text": "-", "entities": []}, {"text": "occurrences", "entities": []}, {"text": ".", "entities": []}, {"text": "In GTM , the topical representation of a document node is aggregated from its multi - hop neighborhood , including both document and word nodes , using Graph Convolutional Network ( GCN ) ( Kipf and Welling , 2016 ) .", "entities": [[27, 30, "MethodName", "Graph Convolutional Network"], [31, 32, "MethodName", "GCN"]]}, {"text": "As GCN is able to capture high - order neighborhood relationships , GTM is essentially capable of modeling both word - word and doc - doc relationships .", "entities": [[1, 2, "MethodName", "GCN"]]}, {"text": "In speci\ufb01c , the relationships between relevant documents are established by their shared words , which is desirable for topic modeling as documents belonging to one topic typically have similar word distributions .", "entities": []}, {"text": "The main contributions of the paper are : \u2022We propose GTM , a novel topic model that incorporates document relationship graph to enrich document and word representations .", "entities": []}, {"text": "\u2022We extensively experimented on three datasets and the results demonstrate the effectiveness of the proposed approach .", "entities": []}, {"text": "2 Graph Topic Model 2.1 Graph Representation of the Corpus We represent the whole corpus Dwith an undirected graphG= ( N;E ) , whereNandEare nodes and edges in the graph respectively .", "entities": []}, {"text": "To model both words and documents , each of them is represented as a node ni2N , which gives rise to N = V+Dnodes in total , where Vis the size of vocabularyVandDis the number of documents in corpusD. An edge ( ni;nj)indicates the relevance of nodeniandnj , whose weight is determined by Ai;j=8 >", "entities": []}, {"text": "> > > <", "entities": []}, {"text": "> > > > : TF - IDF ij ; i2D andj2V TF - IDF ji ; i2V andj2D 1 ; i = j 0 ; otherwise(1 )", "entities": [[24, 25, "DatasetName", "0"]]}, {"text": "where Ais the adjacency matrix of Gand TF - IDF ij denotes the max - normalized TF - IDF ( Term Frequency \u2013 Inverse Document Frequency ) weight of wordjin document i. Besides self - connections , we only apply positive weights to edges between documents and words , while rely on the model to capture higher - order relationships , e.g. doc - doc and word - word relationships , by applying graph convolutions on graph G. IX XTI\uf8ee \uf8f0\uf8f9 \uf8fbE \u02c6Z Z Dirichlet(\u03b1)G \u02c6XLrec(X,\u02c6X )", "entities": []}, {"text": "MMD ( PZ , Q\u02c6Z)Figure 1 : The framework of GTM .", "entities": [[0, 1, "DatasetName", "MMD"]]}, {"text": "Circles denote neural networks .", "entities": []}, {"text": "X , I,^Z,^X , Zare the TF - IDF matrix of the corpus , an identity matrix , latent topics of all documents , reconstructed word weights and topic distributions drawn from the Dirichlet prior respectively .", "entities": []}, {"text": "Lrec(X;^X)and MMD ( PZ;QZ)are training objectives .", "entities": [[1, 2, "DatasetName", "MMD"]]}, {"text": "2.2 Model Architecture The proposed GTM consists of an encoder Eand a decoderG.", "entities": []}, {"text": "The framework is shown in Figure 1 , and we detail the architecture in the following .", "entities": []}, {"text": "The encoder network Emaps nodes inGto their topic distributions by iteratively applying graph convolution to the node features .", "entities": [[13, 14, "MethodName", "convolution"]]}, {"text": "Following ( Kipf and Welling , 2016 ) , the layer - wise propagation rule of the graph convolution at layer l+ 12[1;L]is de\ufb01ned as H(l+1)=\u001b(D\u00001 2AD\u00001 2H(l)W(l))(2 ) where A2RN\u0002Nis the adjacency matrix of G , Dii = P jAij , W(l)2Rd(l)\u0002d(l+1)is a layerspeci\ufb01c weight matrix where d(l)is the output size of layerl , and\u001bdenotes an activation function that is LeakyReLU", "entities": [[18, 19, "MethodName", "convolution"], [57, 59, "HyperparameterName", "activation function"]]}, {"text": "(", "entities": []}, {"text": "Maas et al . , 2013 ) in this paper .", "entities": []}, {"text": "H(l)2RN\u0002d(l)is the activations of all nodes at layerlandH(0 ) iis the embedding of node i. At each encoder layer , what the graph convolution does is aggregating node features from a node \u2019s \ufb01rst - order neighborhood , which consequently enlarges the receptive \ufb01eld of the central node and enables the information propagation between relevant nodes .", "entities": [[23, 24, "MethodName", "convolution"]]}, {"text": "After successively applying Lgraph convolution layers , the encoding of a node essentially involves its Lth - order neighborhood .", "entities": [[4, 5, "MethodName", "convolution"]]}, {"text": "With L\u00152 , doc - doc and word - word relationships are naturally captured in the topic inference process .", "entities": []}, {"text": "We also add a batch normalization ( Ioffe and Szegedy , 2015 ) after each graph convolution .", "entities": [[4, 6, "MethodName", "batch normalization"], [16, 17, "MethodName", "convolution"]]}, {"text": "After the graph encoding , a softmax is further applied to the node features of a document to produce a multinomial topic distribution ^z2RK , whereK is the topic number .", "entities": [[6, 7, "MethodName", "softmax"]]}, {"text": "3792Based on the inferred topic distribution ^z , the decoder network Gtries to restore the original document representations .", "entities": []}, {"text": "To achieve this goal , we employ a 2 - layer MLP with LeakyReLU activation and batch normalization in the \ufb01rst layer .", "entities": [[11, 12, "DatasetName", "MLP"], [16, 18, "MethodName", "batch normalization"]]}, {"text": "The output of the MLP decoder is then softmax - normalized to generate a word distribution ^x2RV .", "entities": [[4, 5, "DatasetName", "MLP"], [8, 9, "MethodName", "softmax"]]}, {"text": "The decoder is also used to interpret topics .", "entities": []}, {"text": "In this case , we feed to the decoder an identity matrix I2RK\u0002K , and the decoder output G(I)iis the word distribution of the i - th topic .", "entities": []}, {"text": "2.3 Training Objective Based on the Wasserstein Autoencoder ( Tolstikhin et al . , 2017 ) framework , the training objective of GTM is to minimize the document reconstruction loss when the latent topic space is constrained by a prior distribution .", "entities": [[7, 8, "MethodName", "Autoencoder"], [29, 30, "MetricName", "loss"]]}, {"text": "The reconstruction loss is de\ufb01ned as Lrec(X;^X )", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "= \u0000E(xlog^x ) ; ( 3 ) wherexdenotes the TF - IDF of a document and ^xis the reconstructed word distribution corresponding tox .", "entities": []}, {"text": "we use TF - IDF as the reconstruction target since TF - IDF basically preserves the relative importance of words and reduces some background noise that may hurt topic modeling , e.g. , stop words .", "entities": []}, {"text": "We impose a Dirichlet prior , the conjugate prior of the multinomial distribution , to the latent topic distributions .", "entities": []}, {"text": "Following W - LDA ( Nan et al . , 2019 ) , we achieve this goal by minimizing the Maximum Mean Discrepancy ( MMD ) ( Gretton et al . , 2012 ) between the distribution Q^Zof inferred topic distributions ^zand the Dirichlet prior PZfrom which we draw multinomial noises z : MMD ( PZ ; Q^Z ) = 1 m(m\u00001)X i6 = jk(z(i);z(j))+ 1 n(n\u00001)X i6 = jk(^z(i);^z(j))\u00002 mnX i;jk(z(i);^z(j));(4 ) wheremandnare the number of samples from Z and^Zrespectively ( mandnare batch sizes and they are equal in our experiments ) , and k : Z\u0002Z !", "entities": [[3, 4, "MethodName", "LDA"], [24, 25, "DatasetName", "MMD"], [53, 54, "DatasetName", "MMD"], [75, 78, "HyperparameterName", "number of samples"]]}, {"text": "Ris the kernel function .", "entities": []}, {"text": "We use the information diffusion kernel ( Lebanon and Lafferty , 2003 ) as in W - LDA : k(z;z0 )", "entities": [[17, 18, "MethodName", "LDA"]]}, {"text": "= exp(\u0000arccos2(KX i=1q ziz0 i));(5 ) which is sensitive to points near the simplex boundary and thus more suitable for the sparse topic distributions.3 Experiments We evaluate our model on three datasets : 20Newsgroups consisting of 11,259 documents , Grolier consisting of 29,762 documents , and NYTimes consisting of 99,992 documents .", "entities": []}, {"text": "We use the preprocessed 20Newsgroups of ( Srivastava and Sutton , 2017 ) , and preprocessed Grolier and NYTimes of ( Wang et al . , 2019a ) .", "entities": []}, {"text": "We compare the performance of our model with LDA ( Blei et al . , 2003 ) , NVDM ( Miao et al . , 2016 ) , ProdLDA ( Srivastava and Sutton , 2017 ) , GraphBTM ( Zhu et al . , 2018 ) , ATM ( Wang et al . , 2019a ) and W - LDA ( Nan et al . , 2019 ) using topic coherence measures ( R \u00a8oder et al . , 2015 ) .", "entities": [[8, 9, "MethodName", "LDA"], [59, 60, "MethodName", "LDA"]]}, {"text": "To quantify the understandability of the extracted topics , a topic coherence measure aggregates the relatedness scores of the topic words ( topweighted words ) of each topic , where the word relatedness scores are estimated based on word co - occurrence statistics on a large external corpus .", "entities": []}, {"text": "For example , the NPMI coherence measure ( Aletras and Stevenson , 2013 ) applies a sliding window of size 10 over the Wikipedia corpus to calculate NPMI ( Bouma , 2009 ) for word pairs .", "entities": []}, {"text": "We use three topic coherence measures in our experiments : C A ( Aletras and Stevenson , 2013 ) , C P ( R \u00a8oder et al . , 2015 ) , and NPMI .", "entities": []}, {"text": "The topic coherence scores are calculated using Palmetto ( R \u00a8oder et al . , 2015)1 .", "entities": []}, {"text": "Dataset Model C A C P NPMI 20NewsgroupsLDA", "entities": []}, {"text": "0:1769", "entities": []}, {"text": "0:2362 0:0524", "entities": []}, {"text": "NVDM 0:1432\u00000:2558\u00000:0984 ProdLDA", "entities": []}, {"text": "0:2155", "entities": []}, {"text": "0:1859\u00000:0083 GraphBTM 0:2195", "entities": []}, {"text": "0:2152 0:0082", "entities": []}, {"text": "ATM 0:1720 0:1914 0:0207", "entities": []}, {"text": "W - LDA 0:2065 0:2501", "entities": [[2, 3, "MethodName", "LDA"]]}, {"text": "0:0400", "entities": []}, {"text": "GTM 0:2465 0 : 3451 0 : 0629 GrolierLDA 0:2009", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "DatasetName", "0"]]}, {"text": "0:1908 0:0498", "entities": []}, {"text": "NVDM 0:1457\u00000:1877\u00000:0619 ProdLDA 0:1734\u00000:0374\u00000:0193 ATM 0:2189 0:2104", "entities": []}, {"text": "0:0582 W - LDA 0:2354 0:2579", "entities": [[3, 4, "MethodName", "LDA"]]}, {"text": "0:0725", "entities": []}, {"text": "GTM 0:2464 0 : 3251 0 : 0950 NYTimesLDA 0:2128 0:3083", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "DatasetName", "0"]]}, {"text": "0:0773 NVDM 0:1342\u00000:4131\u00000:1437 ProdLDA 0:1964\u00000:0035\u00000:0282", "entities": []}, {"text": "ATM 0:2375 0:3568", "entities": []}, {"text": "0:0899 W - LDA 0:2253 0:3352 0:0783", "entities": [[3, 4, "MethodName", "LDA"]]}, {"text": "GTM 0:2443 0 : 3776 0 : 0911 Table 1 : Average topic coherence of 5 topic number settings ( 20 , 30 , 50 , 75 , 100 ) .", "entities": [[2, 3, "DatasetName", "0"], [5, 6, "DatasetName", "0"]]}, {"text": "Bold values indicate the best performing model under the corresponding dataset / metric setting .", "entities": []}, {"text": "1https://github.com/AKSW/Palmetto", "entities": []}, {"text": "3793 20 30 50 75 1000.3 0.2 0.1 0.00.10.20.30.4C_P on 20NG 20 30 50 75 1000.140.160.180.200.220.240.26C_A on 20NG", "entities": []}, {"text": "20 30 50 75 1000.10 0.05 0.000.05NPMI on 20NG 20 30 50 75 1000.4 0.2 0.00.20.4C_P on NYT 20 30 50 75 1000.1000.1250.1500.1750.2000.2250.250C_A on NYT 20 30 50 75 1000.15 0.10 0.05 0.000.050.10NPMI", "entities": []}, {"text": "on NYT 20 30 50 75 1000.3 0.2 0.1 0.00.10.20.30.4C_P on Grolier 20 30 50 75 1000.140.160.180.200.220.240.26C_A on Grolier 20 30 50 75 1000.05 0.000.050.10NPMI on Grolier GTM", "entities": []}, {"text": "WLDA GraphBTM ATM LDA ProdLDA NVDMFigure 2 : Topic coherence scores ( C P , C A , NPMI ) w.r.t .", "entities": [[3, 4, "MethodName", "LDA"]]}, {"text": "topic numbers on 20Newsgroups ( 20NG ) , NYTimes ( NYT ) , and Grolier .", "entities": []}, {"text": "We use 2 graph convolution layers with output dimensions of 100andKrespectively in the encoder .", "entities": [[4, 5, "MethodName", "convolution"]]}, {"text": "The hidden size of the decoder is also set to100 .", "entities": []}, {"text": "We use the RMSProp ( Hinton et al . , 2012 ) optimizer with a learning rate of 0:01to train the model for 100epochs .", "entities": [[3, 4, "MethodName", "RMSProp"], [12, 13, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"]]}, {"text": "Since the training datasets scale up to 100 K documents , i.e. , 100 K document nodes in the graph , it is hard to do batch training on a single GPU given the large memory requirements .", "entities": []}, {"text": "We solve this issue by mini - batching the datasets and feeding to the model a subgraph consisting of 1000 document nodes and all word nodes at a training step , which results in ef\ufb01cient training ( The training time increases almost linearly with the number of documents ) and makes it possible to apply our model to even bigger datasets .", "entities": []}, {"text": "The topic coherence results on the three datasets are shown in Table 1 , where each value is the average of 5 topic number settings : 20 , 30 , 50 , 75 , 100 .", "entities": []}, {"text": "From Table 1 , we can observe that our proposed GTM is the best - performing model under all dataset / metric settings .", "entities": []}, {"text": "W - LDA , ATM , LDA , andGraphBTM alternately achieve the second - best but they are always under - performed compared to our model .", "entities": [[2, 3, "MethodName", "LDA"], [6, 7, "MethodName", "LDA"]]}, {"text": "As described in section 2 , GTM is an extension to W - LDA with the main difference that GTM models topics in a larger context and incorporates more global information with the graph encoder .", "entities": [[13, 14, "MethodName", "LDA"]]}, {"text": "Therefore the improvements of GTM over W - LDA indicate the effectiveness of such information for topic modeling .", "entities": [[8, 9, "MethodName", "LDA"]]}, {"text": "We only experimented GraphBTM on 20Newsgroups because only 20Newsgroups preserves the sequential information that is necessary for GraphBTM to build graphs .", "entities": []}, {"text": "GraphBTM performs well on the C A metric , which is reasonable since C A is a coherence measure based on a small sliding window of size 5 and consequently prefers models concentrating on a smaller context like GraphBTM .", "entities": []}, {"text": "However , GraphBTM fails to achieve a high C P or NPMI score , which uses a bigger window ( 70 and 10 respectively ) .", "entities": []}, {"text": "To explore how topic coherence results vary w.r.t .", "entities": []}, {"text": "different topic numbers , we present in Figure 2 the topic coherence scores under different topic", "entities": []}, {"text": "3794Model Topics GTMcancer medicine patient treatment medical disease md health hospital investigation satellite mission space launch lunar spacecraft shuttle orbit nasa \ufb02ight car honda bmw engine ford saturn dealer turbo rear model ticket send mail price credit sale offer receive list customer GraphBTMcancer hus md medical health disease patient mission laboratory culture probe mission spacecraft lunar shuttle orbit nasa solar satellite space car bike coproad hit gas insurance fbi guy lot carbuymouse scsi engine card audio pc windows faster village turkish armenia azerbaijan troops militia greek lebanon armenian greece W - LDAmsg food patient disease study science onetreatment doctor scienti\ufb01c space launch nasa satellite ground mission shuttle userocket orbit cardogroad ride speed light drive bike gofront condition sale offer shipping sell excellent car speaker cd include LDAusedrug cause effect medical study disease patient doctor treatment space launch earth nasa mission system orbit satellite design moon carbuy price sale new engine offer model dealer carbuy sell price sale new engine offer model dealer Table 2 : Discovered topics that are most similar to 4 ground - truth categories ( sci.med , sci.space , rec.autos , misc.forsale ) on 20Newsgroups with topic number 50 .", "entities": []}, {"text": "Italics are manually labeled off - topic words .", "entities": []}, {"text": "numbers settings .", "entities": []}, {"text": "It can be observed in Figure 2 that GTM enjoys the best overall performance , achieving the highest scores in most settings .", "entities": []}, {"text": "LDA has a slightly higher NPMI score on 20Newsgroups dataset with 75 and 100 topics , nevertheless , GTM outperforms all baseline models with a relatively large margin on other settings of 20Newsgroups .", "entities": [[0, 1, "MethodName", "LDA"]]}, {"text": "NVDM is apparently the worst - performing model , while performances of models other than GTM and NVDM are not so consistent .", "entities": []}, {"text": "Notably , WLDA , GraphBTM , and LDA obtain the second - best overall C P , C A , and NPMI scores respectively .", "entities": [[7, 8, "MethodName", "LDA"]]}, {"text": "Another observation from Figure 2 is that GTM performs better on smaller topics , probably due to the fact that topics become more discriminative against each other when the topic number is small .", "entities": []}, {"text": "To gain an intuitive impression on the discovered topics , we present in Table 2 4 topics corresponding to 4 out of 20 ground - truth categories of 20Newsgroups .", "entities": []}, {"text": "It can be observed that the topics discovered by GTM are more coherent and interpretable , containing few off - topic words .", "entities": []}, {"text": "As a comparison , GraphBTM \u2019s rec.autos topic mixes up automobiles and criminals , W - LDA \u2019s misc.forsale topic is dif\ufb01cult to identify with too many offtopic words , while LDA can not distinguish between rec.autos and misc.forsale well thus recog - nizes them as the same topic .", "entities": [[16, 17, "MethodName", "LDA"], [31, 32, "MethodName", "LDA"]]}, {"text": "It can be observed that GTM learns more discriminative topics by examining topic words from overlapping topics , e.g. rec.autos and misc.forsale .", "entities": []}, {"text": "4 Conclusion We have introduced Graph Topic Model , a neural topic model that incorporates corpus - level neighboring context using graph convolutions to enrich document representations and facilitate the topic inference .", "entities": []}, {"text": "Both quantitative and qualitative results are presented in the experiments to demonstrate the effectiveness of the proposed approach .", "entities": []}, {"text": "In the future , we would like to extend GTM to corpora with explicit doc - doc interactions , e.g. , scienti\ufb01c documents with citations or social media posts with user relationships .", "entities": []}, {"text": "Replacing GCN in GTM with more advanced graph neural networks is another promising research direction .", "entities": [[1, 2, "MethodName", "GCN"]]}, {"text": "Acknowledgments The authors would like to thank the anonymous reviewers for insightful comments and helpful suggestions .", "entities": []}, {"text": "This work was funded in part by the National Key Research and Development Program of China ( 2016YFC1306704 ) and the National Natural Science Foundation of China ( 61772132 ) .", "entities": []}, {"text": "3795References Nikolaos Aletras and Mark Stevenson .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Evaluating topic coherence using distributional semantics .", "entities": []}, {"text": "InProceedings of the 10th International Conference on Computational Semantics ( IWCS 2013 ) \u2013 Long Papers , pages 13\u201322 , Potsdam , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "David M. Blei .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Probabilistic topic models .", "entities": [[1, 3, "TaskName", "topic models"]]}, {"text": "Communications of the ACM , 55(4):77 . David M. Blei , Andrew Y .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Ng , and Michael I. Jordan .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Latent dirichlet allocation .", "entities": []}, {"text": "J. Mach .", "entities": []}, {"text": "Learn .", "entities": []}, {"text": "Res . , 3:993\u20131022 .", "entities": []}, {"text": "G. Bouma .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Normalized ( pointwise ) mautual information in collocation extraction .", "entities": []}, {"text": "In From Form to Meaning : Processing Texts Automatically , Proceedings of the Biennial GSCL Conference 2009 , volume Normalized , pages 31\u201340 , T \u00a8ubingen .", "entities": []}, {"text": "Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Generative adversarial nets .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 27 , pages 2672\u20132680 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Arthur Gretton , Karsten M. Borgwardt , Malte J. Rasch , Bernhard Sch \u00a8olkopf , and Alexander Smola .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "A kernel two - sample test .", "entities": []}, {"text": "J. Mach .", "entities": []}, {"text": "Learn .", "entities": []}, {"text": "Res . , 13:723\u2013773 .", "entities": []}, {"text": "Geoffrey Hinton , Nitish Srivastava , and Kevin Swersky .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Neural networks for machine learning lecture 6a overview of mini - batch gradient descent .", "entities": []}, {"text": "Sergey Ioffe and Christian Szegedy .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Batch normalization : Accelerating deep network training by reducing internal covariate shift .", "entities": [[0, 2, "MethodName", "Batch normalization"]]}, {"text": "In Proceedings of the 32nd International Conference on Machine Learning , volume 37 , pages 448\u2013456 , Lille , France .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Diederik P Kingma and Max Welling .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Autoencoding variational bayes .", "entities": []}, {"text": "arXiv preprint arXiv:1312.6114 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Thomas N Kipf and Max Welling .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semisupervised classi\ufb01cation with graph convolutional networks .", "entities": []}, {"text": "arXiv preprint arXiv:1609.02907 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Guy Lebanon and John D. Lafferty .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Information diffusion kernels .", "entities": []}, {"text": "In S. Becker , S. Thrun , and K. Obermayer , editors , Advances in Neural Information Processing Systems 15 , pages 391\u2013398 .", "entities": []}, {"text": "MIT Press .", "entities": []}, {"text": "Yujia Li , Daniel Tarlow , Marc Brockschmidt , and Richard Zemel . 2015 .", "entities": []}, {"text": "Gated graph sequence neural networks .", "entities": [[0, 5, "MethodName", "Gated graph sequence neural networks"]]}, {"text": "arXiv preprint arXiv:1511.05493 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Chenghua Lin and Yulan He . 2009 .", "entities": []}, {"text": "Joint sentiment / topic model for sentiment analysis .", "entities": [[6, 8, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 18th ACM Conference on Information and Knowledge Management , CIKM \u2019 09 , pages 375\u2013384 , New York , NY , USA .", "entities": [[5, 6, "DatasetName", "ACM"], [11, 12, "TaskName", "Management"]]}, {"text": "ACM.Andrew L Maas , Awni Y Hannun , and Andrew Y Ng . 2013 .", "entities": []}, {"text": "Recti\ufb01er nonlinearities improve neural network acoustic models .", "entities": []}, {"text": "In ICML Workshop on Deep Learning for Audio , Speech and Language Processing .", "entities": []}, {"text": "Citeseer .", "entities": [[0, 1, "DatasetName", "Citeseer"]]}, {"text": "Yishu Miao , Lei Yu , and Phil Blunsom .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Neural variational inference for text processing .", "entities": [[1, 3, "MethodName", "variational inference"]]}, {"text": "In Proceedings of The 33rd International Conference on Machine Learning , volume 48 , pages 1727\u20131736 , New York , New York , USA .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Feng Nan , Ran Ding , Ramesh Nallapati , and Bing Xiang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Topic modeling with Wasserstein autoencoders .", "entities": [[4, 5, "MethodName", "autoencoders"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6345\u20136381 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Michael R \u00a8oder , Andreas", "entities": []}, {"text": "Both , and Alexander Hinneburg .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Exploring the space of topic coherence measures .", "entities": []}, {"text": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining , WSDM \u2019 15 , pages 399\u2013408 , New York , NY , USA . ACM .", "entities": [[5, 6, "DatasetName", "ACM"], [29, 30, "DatasetName", "ACM"]]}, {"text": "Linfeng Song , Yue Zhang , Zhiguo Wang , and Daniel Gildea .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A graph - to - sequence model for AMRto - text generation .", "entities": [[1, 6, "TaskName", "graph - to - sequence"], [10, 12, "TaskName", "text generation"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1616 \u2013 1626 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Akash Srivastava and Charles Sutton .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Autoencoding variational inference for topic models .", "entities": [[1, 3, "MethodName", "variational inference"], [4, 6, "TaskName", "topic models"]]}, {"text": "arXiv preprint arXiv:1703.01488 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ilya Tolstikhin , Olivier Bousquet , Sylvain Gelly , and Bernhard Schoelkopf .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Wasserstein autoencoders .", "entities": [[1, 2, "MethodName", "autoencoders"]]}, {"text": "arXiv preprint arXiv:1711.01558 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rui Wang , Xuemeng Hu , Deyu Zhou , Yulan He , Yuxuan Xiong , Chenchen Ye , and Haiyang Xu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Neural topic modeling with bidirectional adversarial training .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 340\u2013350 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rui Wang , Deyu Zhou , and Yulan He . 2019a .", "entities": []}, {"text": "ATM : Adversarial - neural topic model .", "entities": []}, {"text": "Information Processing & Management , 56(6):102098 .", "entities": [[3, 4, "TaskName", "Management"]]}, {"text": "Rui Wang , Deyu Zhou , and Yulan He . 2019b .", "entities": []}, {"text": "Open event extraction from online text using a generative adversarial network .", "entities": [[1, 3, "TaskName", "event extraction"], [8, 11, "MethodName", "generative adversarial network"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 282\u2013291 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Liang Yao , Chengsheng Mao , and Yuan Luo . 2019 .", "entities": []}, {"text": "Graph convolutional networks for text classi\ufb01cation .", "entities": []}, {"text": "Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 33:7370\u20137377 .", "entities": []}, {"text": "3796Michihiro Yasunaga , Rui Zhang , Kshitijh Meelu , Ayush Pareek , Krishnan Srinivasan , and Dragomir Radev . 2017 .", "entities": []}, {"text": "Graph - based neural multi - document summarization .", "entities": [[4, 8, "TaskName", "multi - document summarization"]]}, {"text": "In Proceedings of the 21st Conference on Computational Natural Language Learning ( CoNLL 2017 ) , pages 452\u2013462 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xin Zhao , Jing Jiang , Hongfei Yan , and Xiaoming Li .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Jointly modeling aspects and opinions with a MaxEnt - LDA hybrid .", "entities": [[9, 10, "MethodName", "LDA"]]}, {"text": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , pages 56\u201365 , Cambridge , MA .", "entities": [[17, 18, "DatasetName", "Cambridge"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Deyu Zhou , Liangyu Chen , and Yulan He .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A simple Bayesian modelling approach to event extraction from twitter .", "entities": [[6, 8, "TaskName", "event extraction"]]}, {"text": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 700 \u2013 705 , Baltimore , Maryland .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jie Zhou , Ganqu Cui , Zhengyan Zhang , Cheng Yang , Zhiyuan Liu , Lifeng Wang , Changcheng Li , and Maosong Sun .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graph neural networks : A review of methods and applications .", "entities": []}, {"text": "arXiv preprint arXiv:1812.08434 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Qile Zhu , Zheng Feng , and Xiaolin Li .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "GraphBTM : Graph enhanced autoencoded variational inference for biterm topic model .", "entities": [[5, 7, "MethodName", "variational inference"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 4663\u20134672 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}]