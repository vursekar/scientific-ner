[{"text": "Findings of the Association for Computational Linguistics : ACL - IJCNLP 2021 , pages 2152\u20132161 August 1\u20136 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics2152Attending via both Fine - tuning and Compressing Jie Zhou1,2 , Y uanbin Wu2 , Qin Chen2 , Xuanjing Huang3 , and Liang He1,2 1Shanghai Key Laboratory of Multidimensional Information Processing 2School of Computer Science and Technology , East China Normal University jzhou@ica.stc.sh.cn , { ybwu , qchen , lhe } @cs.ecnu.edu.cn 3School of Computer Science ,", "entities": []}, {"text": "Fudan University xjhuang@fudan.edu.cn", "entities": []}, {"text": "Abstract Though being a primary trend for enhancing interpretability of neural networks , attention mechanism \u2019s reliability and validity are still under debate .", "entities": []}, {"text": "In this paper , we try to pu - rify attention scores to obtain a more faithfulexplanation of downstream models .", "entities": []}, {"text": "Speci\ufb01-cally , we propose a framework consisting of alearner and a compressor , which performs \ufb01netuning and compressing iteratively to enhancethe performance and interpretability of the attention mechanism .", "entities": []}, {"text": "The learner focuses onlearning better text representations to achievegood decisions by \ufb01ne - tuning , while the com - pressor aims to perform compressions over the representations to retain the most useful clues for explanations with a V ariational in - formation bottleneck A Ttention ( V A T ) mecha - nism .", "entities": []}, {"text": "Extensive experiments on eight bench - mark datasets show the great advantages ofour proposed approach in terms of both performance and interpretability .", "entities": []}, {"text": "1 Introduction Attention mechanisms ( Bahdanau et al . , 2014 ) h a v e achieved great success in various natural language processing ( NLP ) tasks .", "entities": []}, {"text": "They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels .", "entities": []}, {"text": "The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models ( Mullenbach et al . , 2018 ; Xie et al . , 2017 ; Xu et", "entities": []}, {"text": "al . , 2015 ) .", "entities": []}, {"text": "Li et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2016 ) pointed the view : \u201c Attention provides an important way to explain the workings of neural models \u201d .", "entities": []}, {"text": "Additionally , Wiegreffe and Pinter ( 2019 ) showed that attention mechanisms could help understand the inner workings of a model .", "entities": []}, {"text": "The basic assumption of understanding of models with attention scores is that the inputs ( e.g. , words ) with high attentive weights are essential formaking decisions .", "entities": []}, {"text": "However , as far as we know , it has not been formally veri\ufb01ed .", "entities": []}, {"text": "Existing research ( Jain and Wallace , 2019 ) also shows that attention is not explicable , and there are a lot of controversy regarding to the result explanations ( Wiegreffe and Pinter , 2019 ; Jain and Wallace , 2019 ) .", "entities": []}, {"text": "Moreover , we \ufb01nd that though the attention mechanism can help improve the performance for text classi\ufb01cation in our experiments , it may focus on the irrelevant information .", "entities": []}, {"text": "For example , in the sentence \u201c A very funny movie . \u201d , the long short - term memory model with standard attention ( LSTM - A TT ) infers a correct sentiment label while pays more attention to the irrelevant word \u201c movie \u201d , making the result dif\ufb01cult to explain .", "entities": [[15, 20, "MethodName", "long short - term memory"], [25, 26, "MethodName", "LSTM"]]}, {"text": "In general , the attention weights are only optimized to encode the task - relevant information while are not restricted to imitate human behavior .", "entities": []}, {"text": "In order to enhance the interpretability of the attention mechanism , recent studies turn to integrate the human provided explanation signals into the attention models .", "entities": []}, {"text": "Rei and S\u00f8gaard ( 2018 ) regularized the attention weights with a small amount of word - level annotations .", "entities": []}, {"text": "Barrett et al .", "entities": []}, {"text": "( 2018 )", "entities": []}, {"text": "; Bao et al .", "entities": []}, {"text": "( 2018 ) improved the explanation of attention by aligning explanations with human - provided rationales .", "entities": []}, {"text": "These methods rely on additional labour consuming labelling for enhancing explanations , which is hard to extend to other datasets or tasks .", "entities": []}, {"text": "In this paper , we aim to train a more ef\ufb01cient and effective interpretable attention model without any pre - de\ufb01ned annotations or pre - collected explanations .", "entities": []}, {"text": "Speci\ufb01cally , we propose a framework consisting of a learner and a compressor , which enhances the performance and interpretability ofthe attention model for text classi\ufb01cation 1 .", "entities": []}, {"text": "The learner learns text representations by \ufb01ne - tuning 1We focus on the task of text classi\ufb01cation , but our method can be easily extended to other NLP or CV tasks with attention mechanisms .", "entities": []}, {"text": "2153the encoder .", "entities": []}, {"text": "Regarding to the compressor , we are motivated by the effectiveness of the information bottleneck ( IB ) ( Tishby et al . , 1999 ) to enhance performance ( Li and Eisner , 2019 ) or detect important features ( Bang et al . , 2019 ; Chen and Ji , 2020 ; Jiang et al . , 2020 ; Schulz et al . , 2020 ) , and present a V ariational information bottleneck A Ttention ( V A T ) mechanism using IB to keep the most relevant clues and forget the irrelevant ones forbetter attention explanations .", "entities": []}, {"text": "In particular , IB isintegrated into attention to minimize the mutualinformation ( MI ) with the input while preserving as much MI as possible with the output , which provides more accurate and reliable explanations by controlling the information \ufb02ow .", "entities": []}, {"text": "To evaluate the effectiveness of our proposed approach , we adapt two advanced neural models ( LSTM and BERT ) within the framework and conduct experiments on eight benchmark datasets .", "entities": [[16, 17, "MethodName", "LSTM"], [18, 19, "MethodName", "BERT"]]}, {"text": "The experimental results show that our adapted models outperform the standard attention - based models over all the datasets .", "entities": []}, {"text": "Moreover , they exhibit great advantages with respect to interpretability by both qualitative and quantitative analyses .", "entities": []}, {"text": "Speci\ufb01cally , we obtain signi\ufb01cant improvements by applying our model to the semi - supervised word - level sentiment detection task , which detects the sentiment words based on attention weights via only sentencelevel sentiment label .", "entities": []}, {"text": "In addition , we provide the case studies and text representation visualization to have an insight into how our model works .", "entities": []}, {"text": "The main contributions of this work are summarized as follows .", "entities": []}, {"text": "\u2022We propose a novel framework to enhance the performance and interpretability of the attention models , where a learner is used to learn good representations by \ufb01ne - tuning and a compressor is used to obtain good attentive weights by compressing iteratively .", "entities": []}, {"text": "\u2022We present a V ariational information bottleneck A Ttention ( V A T ) mechanism for the compressor , which performs compression over the text representation to keep the task related information while reduce the irrelevant noise via information bottleneck .", "entities": []}, {"text": "\u2022Extensive experiments show the great advantages of our models within the proposed framework , and we perform various qualitative and quantitative analyses to shed light on why our models work in both performance and interpretability.2 Related Work In this section , we survey related attention mech - anisms ( Bahdanau et al . , 2014 ) and review the most relevant studies on information bottleneck ( IB ) ( Tishby et al . , 1999 ) .", "entities": []}, {"text": "Attention has been proved can help explain the internals of neural models ( Li et al . , 2016 ; Wiegreffe and Pinter , 2019 ) though it is limited ( Jain and Wallace , 2019 ) .", "entities": []}, {"text": "Many researchers try to improve the interpretability of the attention mechanisms .", "entities": []}, {"text": "Rei and S\u00f8gaard ( 2018 ) leveraged small amounts of word - level annotations to regularize attention .", "entities": []}, {"text": "Kim et al .", "entities": []}, {"text": "( 2017 ) introduced a structured attention mechanism to learn attention variants from explicit probabilistic semantics .", "entities": []}, {"text": "Barrett et al .", "entities": []}, {"text": "( 2018 )", "entities": []}, {"text": "; Bao et al .", "entities": []}, {"text": "( 2018 ) aligned explanations with human - provided rationales to improve the explanation of attention .", "entities": []}, {"text": "Unlike these methods that require prior attributions or human explanations , the V A T method enforces the attention to learn the vital information while \ufb01lter the noise via IB .", "entities": []}, {"text": "A series of studies motivate us to utilize IB to improve the explanations of attention mechanisms .", "entities": []}, {"text": "Li and Eisner ( 2019 ) compressed the pre - trained embedding ( e.g. , BERT , ELMO ) , remaining onlythe information that helps a discriminative parserthrough variational IB .", "entities": [[15, 16, "MethodName", "BERT"], [17, 18, "MethodName", "ELMO"]]}, {"text": "Zhmoginov et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) utilized the IB approach to discover the salient region .", "entities": []}, {"text": "Some works ( Jiang et al . , 2020 ; Chen et al . , 2018 ; Guan et al . , 2019 ; Schulz et al . , 2020 ; Bang et al . , 2019 ) proposed to identify vital features or attributions via IB .", "entities": []}, {"text": "Moreover , Chen and Ji ( 2020 ) designed a variational mask strategy to delete the useless words in the text .", "entities": []}, {"text": "As far as we are aware , we are the \ufb01rst ones to leverage IB into attention mechanisms to train more interpretable attention with better accuracy .", "entities": [[25, 26, "MetricName", "accuracy"]]}, {"text": "3", "entities": []}, {"text": "Our Approach In this section , we introduce our framework consist - ing of a learner and a compressor with a V ariational information bottleneck A Ttenttion ( V A T ) mecha - nism .", "entities": []}, {"text": "Given an attention - based neural network model , we formulate our idea within the frameworkof variational information bottleneck ( VIB ) ( Tishby et al . , 1999 ) .", "entities": []}, {"text": "Our framework aims to improve the attention \u2019s interpretalility with better performance by restricting the attention to capture the crucial words while \ufb01lter the useless information .", "entities": []}, {"text": "2154X R \u0de0\u0707\u08bbX R Z \u0de0\u0707\u08bb p(Z|R)\u0d4cp\u123a\u0886\u0201\u08bb\u01e1\u0884\u123b q(\u0de1\u0707|Z)Learner Compressor Figure 1 : The framework .", "entities": []}, {"text": "The learner aims to learn the good text representation Xby \ufb01ne - tuning , and the compressor aims to learn good attention weights by com - pressing the attentive representations to capture the im - portant words while forget the redundant information via V", "entities": []}, {"text": "A T.", "entities": []}, {"text": "The blue circles mean the corresponding parameters of the modules are \ufb01xed .", "entities": []}, {"text": "3.1 Overview Our framework is composed of a learner and a compressor , which performs \ufb01ne - tuning and compressing iteratively ( Figure 1 ) .", "entities": []}, {"text": "The learner aims to learn a task - speci\ufb01c contextual word representation by \ufb01ne - tuning .", "entities": []}, {"text": "The compressor enforces the model to learn task - relevant information while reduce irrelevant information via IB .", "entities": []}, {"text": "We iteratively perform the learner and compressor ( \ufb01ne - tuning and compressing ) to improve each other .", "entities": []}, {"text": "Learner .", "entities": []}, {"text": "We adopt a basic attention - based neural network model as a learner to learn representations of the words based on the good attention weights learned by the compressor .", "entities": []}, {"text": "The model is optimized by cross - entropy loss to learn the label - relevantinformation .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "In this phase , we \ufb01x the attention \u2019s parameters so that the model will focus on updating the encoder to learn word representations .", "entities": []}, {"text": "Compressor .", "entities": []}, {"text": "To restrict the attention to capture the vital information while reduce the noise , we integrate IB into attention mechanisms to compress the text attentive representation .", "entities": []}, {"text": "We \ufb01x the encoder \u2019s parameters so that the model will focus on learning the attention weights based on current representations obtained from the learner .", "entities": []}, {"text": "3.2 Basic Attention Model ( Learner )", "entities": []}, {"text": "In this section , we describe our learner , which is an attention - based neural network model .", "entities": []}, {"text": "First , given at e x tT \u201c tw1,w2, ... ,w |T|u , where |T|is the length of text T , we feed it into an encoder with aEncoder \u2026 \u2026 X\ud835\udf48 \ud835\udc96MLP\ud835\udc18 RZ \ud835\udf36Min I(Z ; R)Max I(Z ; Y)q(\ud835\udc18|Z )", "entities": []}, {"text": "Z = u+\ud835\udf48\u2299\ud835\udedc     \ud835\udedc~\ud835\udc75\u123a\ud835\udfce , \ud835\udc70\u123b   p(Z|R ) VAT T Figure 2 : The architecture of our V A T ( Compressor ) .", "entities": []}, {"text": "First , we obtain the input text \u2019s word representations X via an encoder trained by the learner .", "entities": []}, {"text": "Then , we calculateZby compressing the text representation Rthat is the weighted sum of Xbased on the attention \u03b1 , while remaining the maximum information to judge Yby inputtingZinto a MLP classi\ufb01er for predicting .", "entities": [[18, 19, "HyperparameterName", "\u03b1"], [30, 31, "DatasetName", "MLP"]]}, {"text": "word embedding layer .", "entities": []}, {"text": "We adopt LSTM and BERT models as our encoder , and other models can also be applied to our framework .", "entities": [[2, 3, "MethodName", "LSTM"], [4, 5, "MethodName", "BERT"]]}, {"text": "We obtain the contextaware word representations x\u201crx1,x2, ... ,x |T|s , wherexiis the hidden vector of the word wi .", "entities": []}, {"text": "x\u201cencoder pT , \u03b8encoder q , ( 1 ) where\u03b8encoder is the parameters of the encoder .", "entities": []}, {"text": "Based on the contextual word representations , attention mechanism ( Bahdanau et al . , 2014 ) 2is utilized to capture the important parts in the text and obtain the text representation R , which is calculated as , R\u201cn\u00ff i\u201c1\u03b1ixi \u03b1i\u201csoftmax pvJ atanh pWaxiqq(2 ) where\u03b8attention \u201c tva , Wauis the trainable parameters of the attention , which is not updatedin this step to learn the word representation x based the good attention learned by the compressor.\u03b1\u201cr\u03b11,\u03b12, ... ,\u03b1 |T|sis the attention weights .", "entities": [[81, 82, "HyperparameterName", "\u03b1"]]}, {"text": "Finally , we input the text representation Rinto a 2In this paper , we only explore the local attention mechanism on our framework , other attention mechanisms ( e.g. , multi - head attention ( V aswani et al . , 2017 ) ) can also be applied .", "entities": [[30, 34, "MethodName", "multi - head attention"]]}, {"text": "We would like to explore it in future work .", "entities": []}, {"text": "2155multi - layer perceptron ( MLP ) to predict the probability .", "entities": [[5, 6, "DatasetName", "MLP"]]}, {"text": "The cross - entropy loss is used to optimize the model .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "3.3 Variational Information Bottleneck Attention ( Compressor ) The learner optimizes the sentence representations by minimizing the cross - entropy loss , which does not restrict the model to ignore the useless information .", "entities": [[20, 21, "MetricName", "loss"]]}, {"text": "Thus , we compress sentence representations Rinto a latent representation Zthat retains most useful information to infer the label Y. We propose to accomplish this by integrating VIB into the attention mechanism ( Figure 2 ) .", "entities": []}, {"text": "To ensure Zcontains maximum ability to predict Y(IpZ;Yq ) while has the least redundant information form R(\u00b4IpZ;Rq ) , we use the standard IB theory ( Tishby et al . , 1999 ) and de\ufb01ne the objective function as : max \u03b1IpZ;Yq\u00b4\u03b2\u00a8IpZ;Rq ( 3 ) whereIp\u00a8;\u00a8qmeans the mutual information and \u03b2 is a coef\ufb01cient to balance two components .", "entities": [[50, 51, "HyperparameterName", "\u03b2"]]}, {"text": "The main challenge is to estimate the lower bound for IpZ;Yqand", "entities": []}, {"text": "the upper bound for IpZ;Rq.3", "entities": []}, {"text": "The joint probability p\u03b8pr , y , z qcan be factored aspprq\u00a8ppy|rq\u00a8p\u03b8pz|rqbased on the independence assumption4 .", "entities": []}, {"text": "By replacing the conditional distribution p\u03b8py|zqwith a variational approximationq\u03c6py|zq , we obtain a lower bound of IpZ;Yq.q\u03c6py|zqis a simple classi\ufb01er that runs on a compressed text representation z. IpZ;Yqhkkkkkkkkkkkkkikkkkkkkkkkkkkj Ep\u03b8py , z qrlogp\u03b8py|zq ppyqs\u00b4lower bound hkkkkkkkkkkkkkikkkkkkkkkkkkkj Ep\u03b8py , z qrlogq\u03c6py|zq ppyqs \u201c Ep\u03b8pzqrKLpp\u03b8py|zq}q\u03c6py|zqqs \u011b 0(4 ) whereKL r\u00a8}\u00a8s represents Kullback - Leibler divergence .", "entities": []}, {"text": "Speci\ufb01cally , we regard ppyqas constant and then minimize Ep\u03b8py , z qrlogq\u03c6py|zqs .", "entities": []}, {"text": "Since we must \ufb01rst sample rto sample y , z fromp\u03b8pr , y , z q , the lower bound of IpZ;Yqis computed as , IpZ;Yq\u011bEppr , y qrEp\u03b8pz|rqrlogq\u03c6py|zqss ( 5 ) We calculate the upper bound of IpZ;Rqby replacingp\u03b8pzqwith a variational distribution r\u03c8pzq , 3We give the main steps as follows and the detailed derivation is provided in supplementary materials .", "entities": []}, {"text": "4Y\u00d1R\u00d1Z : YandZare independent given R.upper boundhkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj EpprqrEp\u03b8pz|rqrlogp\u03b8pz|rq r\u03c8pzqss \u00b4 IpZ;Rqhkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj EpprqrEp\u03b8pz|rqrlogp\u03b8pz|rq ppzqss \u201c EpprqrKLpppzq}r\u03c8pzqqs \u011b 0 ( 6 ) The upper bound of IpZ;Rqis computed as , IpZ;Rq\u010fEpprqrEp\u03b8pz|rqrlogp\u03b8pz|rq r\u03c8pzqss \u201c EpprqrKLrp\u03b8pz|rq}r\u03c8pzqss(7 )", "entities": [[16, 17, "DatasetName", "0"]]}, {"text": "Then , we obtain the lower bound Lof IB by substituting Equation 5and 7into Equation 3 : L\u201cEppr , y qrEp\u03b8pz|rqrlogq\u03c6py|zqs \u00b4 \u03b2\u00a8KLrp\u03b8pz|rq}r\u03c8pzqss(8 )", "entities": []}, {"text": "The \ufb01rst component in Lis to keep the most useful information in p\u03b8pz|rqfor inferring y , while the second one is to regularize p\u03b8pz|rqwith a prede\ufb01ned prior distribution r\u03c8pzq(e.g . , Gaussian distribution ) .", "entities": []}, {"text": "To compute p\u03b8pz|rq , we adopt the reparametrization trick for multivariate Gaussians ( Rezende et al . , 2014 ) , which obtains the gradient of parameters that derive zfrom a random noise /epsilon1 .", "entities": []}, {"text": "z\u201cu`\u03c3d / epsilon1,/epsilon1 \u201e Np0,Iq ( 9 ) where dmeans element - wise multiplication .", "entities": []}, {"text": "uand \u03c3denote the mean and covariance de\ufb01ned by two functions of R , whereR \u201c \u03b1\u00a8xthat is learned based on attention .", "entities": []}, {"text": "In particular , two MLP are used to predict uand\u03c3 .", "entities": [[4, 5, "DatasetName", "MLP"]]}, {"text": "Finally , we input the zinto a MLP to predict q\u03c6py|zqand optimize the attention \u2019s parameter via Equation 8 . 4 Experiment Setup We adopt two typical neural network models , attention - based LSTM ( Hochreiter and Schmidhuber , 1997 ) and BERT ( Devlin et al . , 2019 ) , to explore our V A T algorithm .", "entities": [[7, 8, "DatasetName", "MLP"], [34, 35, "MethodName", "LSTM"], [43, 44, "MethodName", "BERT"]]}, {"text": "4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our V A T model , we conduct the experiments over eight benchmark datasets : IMDB ( Maas et al . , 2011 ) , Stanford Sentiment Treebank with ( includes SST 1 and its binary version SST -2 ) ( Socher et al . , 2013 ) , Y elp ( Zhang et al . , 2015 ) ,", "entities": [[25, 26, "DatasetName", "IMDB"], [41, 42, "DatasetName", "SST"], [47, 48, "DatasetName", "SST"]]}, {"text": "AG News ( Zhang et al . , 2015 ) , TREC ( Li and Roth , 2002 ) , subjective / objective classi\ufb01cation Subj ( Pang and Lee , 2005 ) and Twitter ( Rosenthal et al . , 2015 , 2014 ) .", "entities": [[0, 2, "DatasetName", "AG News"], [11, 12, "DatasetName", "TREC"], [24, 25, "DatasetName", "Subj"]]}, {"text": "The statistics information of these datasets are shown in Table 1 .", "entities": []}, {"text": "2156IMDB", "entities": []}, {"text": "SST -1", "entities": [[0, 1, "DatasetName", "SST"]]}, {"text": "SST -2", "entities": [[0, 1, "DatasetName", "SST"]]}, {"text": "Y elp AG News", "entities": [[2, 4, "DatasetName", "AG News"]]}, {"text": "Trec Subj Twitter Class 2", "entities": [[0, 1, "DatasetName", "Trec"], [1, 2, "DatasetName", "Subj"]]}, {"text": "5 2 2 4 6 2 3 Length 268 18 19 138 32 10 23 22#train 20,000 8,544 6,920 500,000 114,000 5,000 8,000 7,969#dev 5,000 1,1101 872 60,000 6,000 452 1,000 1,375#test 25,000 2,210 1,821 38,000 7,600 500 1,000 3,795 Table 1 : The statistics information of the datasets , where Class is the number of the class", "entities": []}, {"text": ", Length is average text length , and # train/#dev/#test counts the number of samples in the train / dev / test sets .", "entities": [[12, 15, "HyperparameterName", "number of samples"]]}, {"text": "IMDB SST -1", "entities": [[0, 1, "DatasetName", "IMDB"], [1, 2, "DatasetName", "SST"]]}, {"text": "SST -2", "entities": [[0, 1, "DatasetName", "SST"]]}, {"text": "Y elp AG News", "entities": [[2, 4, "DatasetName", "AG News"]]}, {"text": "Trec Subj Twitter Average LSTM - base 88.79 45.20 85.45 95.10 91.91 90.00 89.00 71.25 82.09 LSTM - A TT 88.16 46.29 84.73 95.06 91.88 91.00 90.80 70.75 82.33 LSTM - V", "entities": [[0, 1, "DatasetName", "Trec"], [1, 2, "DatasetName", "Subj"], [4, 5, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"], [29, 30, "MethodName", "LSTM"]]}, {"text": "A T 88.98 47.42 86.22 95.32 92.04 92.80 91.10 71.62 83.19 BERT -base 91.90 51.44 91.60 96.07 93.52 96.60 96.50 75.28 86.61 BERT -A TT", "entities": [[11, 12, "MethodName", "BERT"], [22, 23, "MethodName", "BERT"]]}, {"text": "91.81 51.13 91.16 97.20 93.41 96.40 96.20 74.84 86.52 BERT -V A T 92.11 51.99 91.98 97.36 93.71 97.20 96.70 77.13 87.27 Table 2 : The main results of text classi\ufb01cation .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "Baselines We compare our model with two kinds of models , basic models ( LSTM / BERT -base ) and attention - based models ( LSTM / BERT -A TT ) .", "entities": [[14, 15, "MethodName", "LSTM"], [16, 17, "MethodName", "BERT"], [25, 26, "MethodName", "LSTM"], [27, 28, "MethodName", "BERT"]]}, {"text": "LSTM - base takes the max - pooling of the LSTM \u2019s hidden vectors as text representation .", "entities": [[0, 1, "MethodName", "LSTM"], [10, 11, "MethodName", "LSTM"]]}, {"text": "For BERT base , the \u201c [ CLS ] \u201d representation is obtained as the sentence representation .", "entities": [[1, 2, "MethodName", "BERT"]]}, {"text": "LSTM - A TT model is a standard attention - based LSTM model that has the same structure as the learner .", "entities": [[0, 1, "MethodName", "LSTM"], [11, 12, "MethodName", "LSTM"]]}, {"text": "We obtain the BERT A TT by replacing the LSTM encoder with BERT in LSTM - A TT .", "entities": [[3, 4, "MethodName", "BERT"], [9, 10, "MethodName", "LSTM"], [12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "LSTM"]]}, {"text": "Our models are marked with V A T ( LSTM - V A T , BERT -V A T ) , which integrate VIB into attention - based neural models .", "entities": [[9, 10, "MethodName", "LSTM"], [15, 16, "MethodName", "BERT"]]}, {"text": "4.2 Implementation Details For LSTM - based models , we use GloV e embedding ( Pennington et al . , 2014 ) with 300 - dimension to initialize the word embedding and \ufb01ne - tune it during the training .", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "We randomly initialize all outof - vocabulary words and weights with the uniform distribution Up\u00b40.1,0.1q .", "entities": []}, {"text": "For the BERT -based models , we \ufb01ne - tune pre - trained BERT -base model .", "entities": [[2, 3, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"]]}, {"text": "The dimension of hidden state vectors of LSTM is 100 and the max sentence length is 256 in ourexperiments .", "entities": [[7, 8, "MethodName", "LSTM"]]}, {"text": "Adam ( Kingma and Ba , 2014 ) i s utilized as the optimizer with learning rate 0.001 ( for LSTM - based model ) and 0.00001 ( for BERT based model ) .", "entities": [[0, 1, "MethodName", "Adam"], [13, 14, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"], [20, 21, "MethodName", "LSTM"], [29, 30, "MethodName", "BERT"]]}, {"text": "We also search different values \u03b2Pt0.01,0.1,1,10u .", "entities": []}, {"text": "5 Experiments First , we perform our models and baselines on eight benchmark datasets and visualize the text representation to verify the effectiveness of V A T ( Section 5.1 ) .", "entities": []}, {"text": "Second , to further investigate our V A T model , we adopt two popular explanation metrics for quantitative evaluation ( Section 5.2 ) .", "entities": []}, {"text": "Third , we apply our models to semi - supervision sentiment detection task to evaluate the explanation of our model ( Section 5.3 ) .", "entities": []}, {"text": "Fourth , we explore the in\ufb02uence of our iteration strategy in Section 5.4 and provide case studies in Section 5.5 .", "entities": []}, {"text": "For the limitation of the space , we may only list the results on parts of the datasets in some cases since the conclusions are similar for other datasets .", "entities": []}, {"text": "The complete results are presented in the supplementary materials .", "entities": []}, {"text": "5.1 Main Results We report the accuracy of our V A T and baselines based on LSTM and BERT ( Table 2 ) .", "entities": [[6, 7, "MetricName", "accuracy"], [16, 17, "MethodName", "LSTM"], [18, 19, "MethodName", "BERT"]]}, {"text": "From these results , we \ufb01nd the following observations : 1)our models ( LSTM / BERT -V A T ) outperform all the corresponding baselines over all the eight datasets , which denotes the effectiveness of our V A T on both LSTM and BERT -based models ; 2)compared with attention - based models ( LSTM / BERT -A TT ) , our models obtain better results .", "entities": [[13, 14, "MethodName", "LSTM"], [15, 16, "MethodName", "BERT"], [42, 43, "MethodName", "LSTM"], [44, 45, "MethodName", "BERT"], [55, 56, "MethodName", "LSTM"], [57, 58, "MethodName", "BERT"]]}, {"text": "It indicates reducing the irrelevant information in input via V", "entities": []}, {"text": "A T can improve the performance of the models .", "entities": []}, {"text": "Furthermore , we visualize the sentence representations obtained from LSTM / BERT -A TT and -V A T models ( Figure 3 ) .", "entities": [[9, 10, "MethodName", "LSTM"], [11, 12, "MethodName", "BERT"]]}, {"text": "We randomly select 1000 samples from the test set for each dataset .", "entities": []}, {"text": "We can \ufb01nd that our V A T model can reduce the distance of the samples in a class and add the distance ofthe samples in different classes .", "entities": []}, {"text": "For example , itis hard to split the positive samples from the neg - ative ones based on the representations obtained from LSTM - A TT for the IMDB dataset , while the divider line based on our V A T is clear .", "entities": [[22, 23, "MethodName", "LSTM"], [28, 29, "DatasetName", "IMDB"]]}, {"text": "These ob-", "entities": []}, {"text": "2157(a ) IMDB ( LSTM ) ( b ) Subj ( LSTM ) ( c ) Twitter ( LSTM ) ( d ) IMDB ( BERT ) ( e ) Subj ( BERT ) ( f ) Twitter ( BERT ) Figure 3 : Visualization of text representation obtained from LSTM / BERT -A TT and LSTM / BERT -V", "entities": [[2, 3, "DatasetName", "IMDB"], [4, 5, "MethodName", "LSTM"], [9, 10, "DatasetName", "Subj"], [11, 12, "MethodName", "LSTM"], [18, 19, "MethodName", "LSTM"], [23, 24, "DatasetName", "IMDB"], [25, 26, "MethodName", "BERT"], [30, 31, "DatasetName", "Subj"], [32, 33, "MethodName", "BERT"], [39, 40, "MethodName", "BERT"], [50, 51, "MethodName", "LSTM"], [52, 53, "MethodName", "BERT"], [56, 57, "MethodName", "LSTM"], [58, 59, "MethodName", "BERT"]]}, {"text": "A T. We use t - SNE to transfer 100/768 - dimensional feature space into two - dimensional space .", "entities": []}, {"text": "IMDB SST -1", "entities": [[0, 1, "DatasetName", "IMDB"], [1, 2, "DatasetName", "SST"]]}, {"text": "SST -2", "entities": [[0, 1, "DatasetName", "SST"]]}, {"text": "Y elp AG News", "entities": [[2, 4, "DatasetName", "AG News"]]}, {"text": "Trec Subj Twitter Accuracy LSTM - base 88.79 45.20 85.45 95.10 91.91 90.00 89.00 71.25 AOPCRandom 0.30 5.97 7.58 1.02 1.87 19.40 1.50 4.72 LSTM - A TT 5.27 12.94 20.54 6.64 5.99 31.00 2.10 19.10 LSTM - V A T 6.13 14.34 21.58 7.12 6.59 37.20 6.30 20.37 Accuracy BERT -base 91.90 51.44 91.60 96.07 93.52 96.60 96.50 75.28 AOPCRandom 0.60 33.26 41.46 3.60 44.20 65.80 45.70 59.21 BERT -A TT 2.81", "entities": [[0, 1, "DatasetName", "Trec"], [1, 2, "DatasetName", "Subj"], [3, 4, "MetricName", "Accuracy"], [4, 5, "MethodName", "LSTM"], [24, 25, "MethodName", "LSTM"], [36, 37, "MethodName", "LSTM"], [49, 50, "MetricName", "Accuracy"], [50, 51, "MethodName", "BERT"], [69, 70, "MethodName", "BERT"]]}, {"text": "33.98 41.52 4.73 52.22 71.60 45.70 59.39 BERT -V A T 3.17 34.03 41.52 6.64 54.70 72.20 45.80 59.45 Table 3 : The results of AOPC .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "( a ) IMDB ( LSTM ) ( b ) IMDB ( BERT ) Figure 4 : The in\ufb02uence of Top- Kfor LSTM / BERT based models in terms of AOPC .", "entities": [[3, 4, "DatasetName", "IMDB"], [5, 6, "MethodName", "LSTM"], [10, 11, "DatasetName", "IMDB"], [12, 13, "MethodName", "BERT"], [22, 23, "MethodName", "LSTM"], [24, 25, "MethodName", "BERT"]]}, {"text": "servations show our V A T model can learn a better task - speci\ufb01c representation by enforcing the model to reduce the task - irrelevant information .", "entities": []}, {"text": "5.2 Quantitative Evaluation In this section , we evaluate our V A T model using two metrics , AOPC and post - hoc accuracy , which are widely used for explanations ( Chen and Ji,2020 ) .", "entities": [[23, 24, "MetricName", "accuracy"]]}, {"text": "Note that well - trained LSTM / BERT -base is used for evaluating the performance of classi\ufb01cation .", "entities": [[5, 6, "MethodName", "LSTM"], [7, 8, "MethodName", "BERT"]]}, {"text": "AOPC .", "entities": []}, {"text": "To evaluate the faithfulness of explanations to our models , we adopt the area over theperturbation curve ( AOPC ) ( Nguyen , 2018 ; Samek et al . , 2016 ) metric .", "entities": []}, {"text": "It calculates the average change of accuracy over test data by deleting top K words via attentive weights .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "The larger the value of AOPC , the better the explanations of the models .", "entities": []}, {"text": "Table 3displays", "entities": []}, {"text": "the results with K \u201c 5 .", "entities": []}, {"text": "We compare our models with random and basic attention - based models .", "entities": []}, {"text": "From the results , we observe that : 1)basic attention - based models ( LSTM / BERT -A TT ) can \ufb01nd the important words in the sentence to some extent .", "entities": [[14, 15, "MethodName", "LSTM"], [16, 17, "MethodName", "BERT"]]}, {"text": "Comparing with random ( Random ) , LSTM / BERT -A TT obtains signi\ufb01cant improvement ; 2)Our models ( LSTM / BERT V A T ) outperform the standard attention - based models .", "entities": [[7, 8, "MethodName", "LSTM"], [9, 10, "MethodName", "BERT"], [19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "BERT"]]}, {"text": "It indicates that integrating VIB into the attention mechanism can help improve the interpretability of the models by \ufb01ltering the useless informa - tion ; 3)BERT model is sensitive to the context ; deleting the words will destroy the semantic information of the sentence and signi\ufb01cantly affect the model \u2019s performance .", "entities": []}, {"text": "We also explore the in\ufb02uence of top- K(Figure 4 ) .", "entities": []}, {"text": "Intuitively , the more words we delete , the larger accuracy the models reduce .", "entities": [[10, 11, "MetricName", "accuracy"]]}, {"text": "Our models reduce more performance than random and attention - based", "entities": []}, {"text": "2158IMDB", "entities": []}, {"text": "SST -1", "entities": [[0, 1, "DatasetName", "SST"]]}, {"text": "SST -2", "entities": [[0, 1, "DatasetName", "SST"]]}, {"text": "Y elp AG News", "entities": [[2, 4, "DatasetName", "AG News"]]}, {"text": "Trec Subj Twitter Accuracy LSTM - base 88.79 45.20 85.45 95.10 91.91 90.00 89.00 71.25 Post - hocRandom 58.48 34.21 71.33 64.74 62.45 71.40 78.40 54.07 LSTM - A TT 83.96 40.56 82.70 87.80 78.96 73.60 87.40 70.20 LSTM - V A T 84.41 43.39 84.35 88.82 81.43 79.20 89.10 71.23 Accuracy BERT -base 91.90 51.44 91.60 96.07 93.52 96.60 96.50 75.28 Post - hocRandom 51.50 20.27 50.52 50.21 26.74 26.60 50.60 40.50 BERT -A TT 51.72 29.19 58.92 53.63 37.53 34.20 61.90 53.68 BERT -V A T 53.40 30.23 61.34 56.58 43.08 36.80 65.40 56.05 Table 4 : The results of post - hoc accuracy .", "entities": [[0, 1, "DatasetName", "Trec"], [1, 2, "DatasetName", "Subj"], [3, 4, "MetricName", "Accuracy"], [4, 5, "MethodName", "LSTM"], [26, 27, "MethodName", "LSTM"], [38, 39, "MethodName", "LSTM"], [51, 52, "MetricName", "Accuracy"], [52, 53, "MethodName", "BERT"], [73, 74, "MethodName", "BERT"], [84, 85, "MethodName", "BERT"], [105, 106, "MetricName", "accuracy"]]}, {"text": "( a ) IMDB ( LSTM ) ( b ) IMDB ( BERT ) Figure 5 : The in\ufb02uence of Top- K for LSTM - based models in terms of post - hoc . models .", "entities": [[3, 4, "DatasetName", "IMDB"], [5, 6, "MethodName", "LSTM"], [10, 11, "DatasetName", "IMDB"], [12, 13, "MethodName", "BERT"], [23, 24, "MethodName", "LSTM"]]}, {"text": "For the IMDB dataset , when deleting top 20words ( average length is 268 ) , the accuracy reduces about 19 points for our LSTM - V A T model while it is about 2 points for the random model .", "entities": [[2, 3, "DatasetName", "IMDB"], [17, 18, "MetricName", "accuracy"], [24, 25, "MethodName", "LSTM"]]}, {"text": "Post - hoc Accuracy .", "entities": [[3, 4, "MetricName", "Accuracy"]]}, {"text": "We also adopt the post - hoc accuracy ( Chen et al . , 2018 ) to evaluate the in\ufb02uence of task - speci\ufb01c essential words on the performance of LSTM - based and BERT -based models .", "entities": [[7, 8, "MetricName", "accuracy"], [30, 31, "MethodName", "LSTM"], [34, 35, "MethodName", "BERT"]]}, {"text": "For each test sample , we select the top Kwords based on their attentive weights as input to make a prediction and compare it with the ground truth .", "entities": []}, {"text": "Table 4presents the performance with K \u201c 5 .", "entities": []}, {"text": "First , it is interesting to \ufb01nd that the post - hoc accuracy with \ufb01ve most important words on Sbuj dataset ( 89.10 ) is even better than the original sentence ( 89.00 ) .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "Additionally , we obtain comparable results with only \ufb01ve words for SST -1 , SST -2 , and Twitter datasets .", "entities": [[11, 12, "DatasetName", "SST"], [14, 15, "DatasetName", "SST"]]}, {"text": "These show that our model can reduce the noise information since most of the words are useless for predictions in some cases .", "entities": []}, {"text": "Second , for BERT -based models , the context words are also important for classi\ufb01cation even though they may not be task - speci\ufb01c .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "Similarly , we investigate the in\ufb02uence of top- K for post - hoc ( Figure 5 ) .", "entities": []}, {"text": "The LSTM - base model with top- 10 words selected by our LSTM - V A T model can achieve comparable results with the original samples in most cases .", "entities": [[1, 2, "MethodName", "LSTM"], [12, 13, "MethodName", "LSTM"]]}, {"text": "Additionally , for theIMDB dataset , the accuracy of LSTM - base with one word selected by our V A T model is even better than the one with 20 words selected randomly.5.3 Semi - Supervised Word - Level Sentiment Detection We perform semi - supervised word - level sentiment detection in Twitter ( Rosenthal et al . , 2015 , 2014 ) to evaluate the interpretability of our V A T.", "entities": [[7, 8, "MetricName", "accuracy"], [9, 10, "MethodName", "LSTM"]]}, {"text": "This task requires to detect the sentiment words in a tweet via the sentiment polarity of the whole tweet .", "entities": []}, {"text": "In the following example from the dataset , positive words ( \u201c good \u201d and \u201c fantastic \u201d ) are marked with a bold font and the overall polarity of the tweet is positive : Good morning becky !", "entities": []}, {"text": "Thursday is going to be fantastic !", "entities": []}, {"text": "We use the SemEval 2013 Twitter dataset , which contains word - level sentiment annotation .", "entities": [[3, 5, "DatasetName", "SemEval 2013"]]}, {"text": "We remove the samples with the neutral sentiment .", "entities": []}, {"text": "We report word - level precision , recall , and F - measure for evaluating the models ( Table 5 ) , the same as ( Rei and S\u00f8gaard , 2018 ) .", "entities": [[10, 13, "MetricName", "F - measure"]]}, {"text": "Note that we select the top - K(we set it as 1 and 5 here ) words according to the attention weights as the sentiment words .", "entities": []}, {"text": "We compare our V A T model with random and attention - based models .", "entities": []}, {"text": "The results show attentionbased models can capture the important words in the text , to a certain extent .", "entities": []}, {"text": "Since our V A T can reduce irrelevant information , it performs better than the standard attention model .", "entities": []}, {"text": "Also , LSTM - based models outperform BERT -based models for this task in most cases .", "entities": [[2, 3, "MethodName", "LSTM"], [7, 8, "MethodName", "BERT"]]}, {"text": "It is because that BERT learns much semantic information from the text , and context information plays a vital role in prediction .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "5.4 In\ufb02uence of Iteration We propose to train the learner and compressor iteratively so that the learner optimizes the wordrepresentations based on the good attention , and the compressor optimizes the attention based on the good word representations .", "entities": []}, {"text": "To have a deep look at how it works , we \ufb01rst provide our V A T model \u2019s accuracy with different iterations ( Table 6 ) .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "From the results , we can \ufb01nd that the model \u2019s performance will improve at \ufb01rst , then it will converge .", "entities": []}, {"text": "2159Positive Negative P@1 R@1 F1@1 P@5 R@5", "entities": [[2, 3, "MetricName", "P@1"], [3, 4, "MetricName", "R@1"], [6, 7, "MetricName", "R@5"]]}, {"text": "F1@5 P@1 R@1", "entities": [[1, 2, "MetricName", "P@1"], [2, 3, "MetricName", "R@1"]]}, {"text": "F1@1 P@5 R@5", "entities": [[2, 3, "MetricName", "R@5"]]}, {"text": "F1@5 Random 14.88 4.78 6.56 14.59 23.34 16.06 20.52 5.61 8.19 17.18 23.68 17.97 LSTM - A TT 58.70 26.04 32.73 30.30 54.17 34.70 47.13 15.74 21.39 28.24 42.04 30.33 LSTM - V", "entities": [[14, 15, "MethodName", "LSTM"], [30, 31, "MethodName", "LSTM"]]}, {"text": "A T 65.20 29.38 36.60 33.04 58.40 37.77 60.00 21.42 28.76 32.70 49.19 35.35 BERT -A TT 46.44 16.52 21.82 33.13 52.52 35.66 37.74 9.19 13.46 30.82 39.65 30.23 BERT -V A T 55.24 20.62 26.90 37.26 58.39 40.09 43.83 11.15 16.20 36.42 44.55 35.30 Table 5 : The results of semi - supervision word - level sentiment detection in twitter .", "entities": [[14, 15, "MethodName", "BERT"], [29, 30, "MethodName", "BERT"]]}, {"text": "( a ) AG News ( LSTM ) ( b ) AG News ( BERT ) Figure 6 : Visualization of text representation obtained from LSTM / BERT -V A T with different iterations .", "entities": [[3, 5, "DatasetName", "AG News"], [6, 7, "MethodName", "LSTM"], [11, 13, "DatasetName", "AG News"], [14, 15, "MethodName", "BERT"], [25, 26, "MethodName", "LSTM"], [27, 28, "MethodName", "BERT"]]}, {"text": "We use t - SNE to transfer 100/768 - dimensional feature space into two - dimensional space .", "entities": []}, {"text": "Method Text Prediction LSTM -ATT", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "I admired this work a lot .", "entities": []}, {"text": "Positive   \u221a LSTM -VAT I admired this work a lot .", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "Positive   \u221a LSTM -ATT", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "That sucks if you have to take the sats tomorrow .", "entities": []}, {"text": "Neutral    \u7064 LSTM -VAT", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "That sucks if you have to take the sats tomorrow .", "entities": []}, {"text": "Negative \u221a Figure 7 : Two examples of attention visualization .", "entities": []}, {"text": "Red denotes the attentive weights of the words .", "entities": []}, {"text": "A deeper color indicates a larger value .", "entities": []}, {"text": "Dataset 012345 LSTM - V A TTwitter 70.75 71.62 70.96 70.67 71.06 70.98 IMDB 88.16 88.98 88.22 88.84 88.14 88.60 BERT -V", "entities": [[2, 3, "MethodName", "LSTM"], [13, 14, "DatasetName", "IMDB"], [20, 21, "MethodName", "BERT"]]}, {"text": "A TTwitter 74.84 75.26 77.71 77.13 76.68 76.76 IMDB 91.81 92.06 92.11 92.09 91.92 91.96 Table 6 : The accuracy with different iteration number with our LSTM / BERT -V A T model .", "entities": [[8, 9, "DatasetName", "IMDB"], [19, 20, "MetricName", "accuracy"], [26, 27, "MethodName", "LSTM"], [28, 29, "MethodName", "BERT"]]}, {"text": "Also , we draw change of the sentence representation with different iterations ( Figure 6 ) .", "entities": []}, {"text": "Similarly , we observe that \ufb01ne - tuning and compressing iteratively can improve the sentence representations .", "entities": []}, {"text": "The samples with the same class are close , and the samples with different classes have a large distance .", "entities": []}, {"text": "5.5 Case Studies To understand why our proposed V A T model is more effective than the standard attention - based model , we visualize two examples of LSTM - based models using attention heatmaps ( Figure 7).First , the standard attention - based LSTM model focuses on the wrong words ( e.g. , \u201c this \u201d , \u201c work \u201d ) even though it predicts the right sentiment while our V A T model \ufb01nds the correct words ( e.g. , \u201c admired \u201d , \u201c lot \u201d ) .", "entities": [[28, 29, "MethodName", "LSTM"], [44, 45, "MethodName", "LSTM"]]}, {"text": "It indicates integrating IB into attention can help it focus on the key words and reduce the noisyinformation .", "entities": []}, {"text": "Second , our proposed model can also improve the attention \u2019s performance by capturing the critical words accurately .", "entities": []}, {"text": "For example , in thesentence \u201c That sucks if you have to take the sats tomorrow . \u201d , our model predicts the right class label by attending the words \u201c sucks \u201d and \u201c have to . \u201d", "entities": []}, {"text": "6 Conclusions and Future Work", "entities": []}, {"text": "This paper proposes a V A T -based framework to improve the performance and interpretability of attentions via both \ufb01ne - tuning and compressing .", "entities": []}, {"text": "The experimental results on eight benchmark datasets for text classi\ufb01cation verify the effectiveness of", "entities": []}, {"text": "2160our models within this framework .", "entities": []}, {"text": "In addition , we apply the framework for sentiment detection , which further demonstrates the superiority in terms of interpretability .", "entities": []}, {"text": "It is also interesting to \ufb01nd that training the models by \ufb01ne - tuning and compressing iteratively is effective to improve the text representations .", "entities": []}, {"text": "In the future , we will investigate the effectiveness of our proposed attention framework for other tasks and areas , such as machine translation and visual question answering .", "entities": [[22, 24, "TaskName", "machine translation"], [25, 28, "DatasetName", "visual question answering"]]}, {"text": "Acknowledgement", "entities": []}, {"text": "The authors wish to thank the reviewers for their helpful comments and suggestions .", "entities": []}, {"text": "This research is ( partially ) supported by NSFC ( 62076097),STCSM ( 18ZR1411500 ) , the Fundamental Research Funds for the Central Universities .", "entities": []}, {"text": "This research is also funded by the Science and Tech - nology Commission of Shanghai Municipality ( 19511120200 & 20511105102 ) .", "entities": []}, {"text": "The computation is performed in ECNU Multifunctional Platformfor Innovation ( 001 ) .", "entities": []}, {"text": "The corresponding authors are Y uanbin Wu and Liang He .", "entities": []}, {"text": "References Dzmitry Bahdanau , Kyunghyun Cho , and Y oshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Neural machine translation by jointlylearning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1409.0473 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Seojin Bang , Pengtao Xie , Heewook Lee , Wei Wu , and Eric Xing .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Explaining a black - box using deep variational information bottleneck approach .", "entities": []}, {"text": "arXiv preprint arXiv:1902.06918 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Y ujia Bao , Shiyu Chang , Mo Y u , and Regina Barzilay .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deriving machine attention from human ra - tionales .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1903\u20131913 .", "entities": []}, {"text": "Maria Barrett , Joachim Bingel , Nora Hollenstein , Marek Rei , and Anders S\u00f8gaard .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Sequenceclassi\ufb01cation with human attention .", "entities": []}, {"text": "In Proceedings of the 22nd Conference on Computational Natural Language Learning , pages 302\u2013312 .", "entities": []}, {"text": "Hanjie Chen and Y angfeng Ji . 2020 .", "entities": []}, {"text": "Learning variational word masks to improve the interpretability ofneural text classi\ufb01ers .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 4236\u20134251 .", "entities": []}, {"text": "Jianbo Chen , Le Song , Martin Wainwright , and Michael Jordan .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning to explain : Aninformation - theoretic perspective on model interpretation .", "entities": []}, {"text": "In International Conference on Machine Learning , pages 883\u2013892.Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deepbidirectional transformers for language understand - ing .", "entities": []}, {"text": "In NAACL - HLT ( 1 ) .", "entities": []}, {"text": "Chaoyu Guan , Xiting Wang , Quanshi Zhang , Runjin Chen , Di He , and Xing Xie . 2019 .", "entities": []}, {"text": "Towards a deep and uni\ufb01ed understanding of deep neural models in nlp .", "entities": []}, {"text": "In International conference on machine learning , pages 2454\u20132463 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Sepp Hochreiter and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural computation , 9(8):1735\u20131780 .", "entities": []}, {"text": "Sarthak Jain and Byron C Wallace .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Attention is not explanation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , V olume 1", "entities": []}, {"text": "( Long and Short Pa - pers ) , pages 3543\u20133556 .", "entities": []}, {"text": "Zhiying Jiang , Raphael Tang , Ji Xin , and Jimmy Lin . 2020 .", "entities": []}, {"text": "Inserting information bottleneck for attribu - tion in transformers .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : Findings , pages 3850\u20133857 .", "entities": []}, {"text": "Y oon Kim , Carl Denton , Luong Hoang , and Alexander M. Rush .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Structured attention networks .", "entities": []}, {"text": "In5th International Conference on Learning Representations , ICLR 2017 , Toulon , France , April 24 - 26 , 2017 , Conference Track Proceedings .", "entities": []}, {"text": "Diederik P Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jiwei Li , Will Monroe , and Dan Jurafsky .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Understanding neural networks through representation erasure .", "entities": []}, {"text": "arXiv preprint arXiv:1612.08220 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xiang Lisa Li and Jason Eisner .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Specializing word embeddings ( for parsing ) by information bottleneck .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , EMNLP - IJCNLP 2019 , Hong Kong , China , November 3 - 7 , 2019 , pages 2744\u20132754 .", "entities": []}, {"text": "Xin Li and Dan Roth . 2002 .", "entities": []}, {"text": "Learning question classi\ufb01ers .", "entities": []}, {"text": "In COLING 2002 : The 19th International Conference on Computational Linguistics .", "entities": []}, {"text": "Andrew Maas , Raymond E Daly , Peter T Pham , Dan Huang , Andrew Y Ng , and Christopher Potts . 2011 .", "entities": []}, {"text": "Learning word vectors for sentiment analysis .", "entities": [[4, 6, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 49th annual meeting of the association for computational linguistics : Human language technologies , pages 142\u2013150 .", "entities": []}, {"text": "James Mullenbach , Sarah Wiegreffe , Jon Duke , Jimeng Sun , and Jacob Eisenstein .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Explainable pre - diction of medical codes from clinical text .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North Amer - ican Chapter of the Association for Computational", "entities": []}, {"text": "2161Linguistics : Human Language Technologies , V olume 1 ( Long Papers ) , pages 1101\u20131111 .", "entities": []}, {"text": "Dong Nguyen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Comparing automatic and human evaluation of local explanations for text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , V olume 1 ( Long Papers ) , pages 1069\u20131078 .", "entities": []}, {"text": "Bo Pang and Lillian Lee . 2005 .", "entities": []}, {"text": "Seeing stars : exploiting class relationships for sentiment categorization with respect to rating scales .", "entities": []}, {"text": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics , pages 115\u2013124 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher D Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word rep - resentation .", "entities": []}, {"text": "In Proceedings of the 2014 conference on empirical methods in natural language process - ing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Marek Rei and Anders S\u00f8gaard .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Zero - shot sequence labeling : Transferring knowledge from sentences to tokens .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Asso - ciation for Computational Linguistics : Human Language Technologies , V olume 1 ( Long Papers ) , pages 293\u2013302 .", "entities": []}, {"text": "Danilo Jimenez Rezende , Shakir Mohamed , and Daan Wierstra .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Stochastic backpropagation and approximate inference in deep generative models .", "entities": []}, {"text": "InInternational Conference on Machine Learning , pages 1278\u20131286 .", "entities": []}, {"text": "Sara Rosenthal , Preslav Nakov , Svetlana Kiritchenko , Saif Mohammad , Alan Ritter , and V eselin Stoyanov . 2015 .", "entities": []}, {"text": "SemEval-2015 task 10 : Sentiment analysisin Twitter .", "entities": []}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 451\u2013463 .", "entities": []}, {"text": "Sara Rosenthal , Alan Ritter , Preslav Nakov , and V eselin Stoyanov .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "SemEval-2014 task 9 : Sentiment analysis in Twitter .", "entities": [[4, 6, "TaskName", "Sentiment analysis"]]}, {"text": "In Proceedings of the 8th International Workshop on Semantic Evaluation ( SemEval 2014 ) , pages 73\u201380 .", "entities": []}, {"text": "Wojciech Samek , Alexander Binder , Gr \u00b4 egoire Montavon , Sebastian Lapuschkin , and Klaus - RobertM\u00a8uller .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Evaluating the visualization of what a deep neural network has learned .", "entities": []}, {"text": "IEEE transactions on neural networks and learning systems , 28(11):2660\u20132673 .", "entities": []}, {"text": "Karl Schulz , Leon Sixt , Federico Tombari , and Tim Landgraf . 2020 .", "entities": []}, {"text": "Restricting the \ufb02ow : Informationbottlenecks for attribution .", "entities": []}, {"text": "In 8th International Conference on Learning Representations , ICLR 2020 , Addis Ababa , Ethiopia , April 26 - 30 , 2020 .", "entities": []}, {"text": "Richard Socher , Alex Perelygin , Jean Wu , Jason Chuang , Christopher D Manning , Andrew Y Ng , and Christopher Potts . 2013 .", "entities": []}, {"text": "Recursive deep mod - els for semantic compositionality over a sentimenttreebank .", "entities": []}, {"text": "In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631\u20131642 .", "entities": []}, {"text": "Naftali Tishby , Fernando C Pereira , and William Bialek .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "The information bottleneck method .", "entities": []}, {"text": "InProceedings of the 37th annual Allerton Conference on Communication , Control , and Computing , page 368\u2013377 .", "entities": []}, {"text": "Ashish V aswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukaszKaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008 .", "entities": []}, {"text": "Sarah Wiegreffe and Y uval Pinter .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Attention is not not explanation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 11\u201320 .", "entities": []}, {"text": "Qizhe Xie , Xuezhe Ma , Zihang Dai , and Eduard Hovy . 2017 .", "entities": []}, {"text": "An interpretable knowledge transfer modelfor knowledge base completion .", "entities": [[5, 8, "TaskName", "knowledge base completion"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( V olume 1 : Long Papers ) , pages 950\u2013962 .", "entities": []}, {"text": "Kelvin Xu , Jimmy Ba , Ryan Kiros , Kyunghyun Cho , Aaron Courville , Ruslan Salakhudinov , Rich Zemel , and Y oshua Bengio . 2015 .", "entities": [[15, 16, "DatasetName", "Ruslan"]]}, {"text": "Show , attend and tell : Neural image caption generation with visual atten - tion .", "entities": []}, {"text": "In International conference on machine learning , pages 2048\u20132057 .", "entities": []}, {"text": "Xiang Zhang , Junbo Zhao , and Y ann LeCun . 2015 .", "entities": []}, {"text": "Character - level convolutional networks for text clas - si\ufb01cation .", "entities": []}, {"text": "Advances in neural information processing systems , 28:649\u2013657 .", "entities": []}, {"text": "Andrey Zhmoginov , Ian Fischer , and Mark Sandler . 2019 .", "entities": []}, {"text": "Information - bottleneck approach to salient re - gion discovery .", "entities": []}, {"text": "arXiv preprint arXiv:1907.09578 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}]