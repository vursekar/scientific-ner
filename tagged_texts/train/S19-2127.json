[{"text": "Proceedings of the 13th International Workshop on Semantic Evaluation ( SemEval-2019 ) , pages 722\u2013726 Minneapolis , Minnesota , USA , June 6\u20137 , 2019 .", "entities": []}, {"text": "\u00a9 2019 Association for Computational Linguistics722nlpUP at SemEval-2019 Task 6 : A Deep Neural Language Model for Offensive Language Detection Jelena Mitrovi \u00b4 c , Bastian Birkeneder , Michael Granitzer Faculty of Computer Science and Mathematics University of Passau , Germany jelena.mitrovic@uni-passau.de |", "entities": []}, {"text": "birkeneder@fim.uni-passau.de michael.granitzer@uni-passau.de", "entities": []}, {"text": "Abstract This paper presents our submission for the SemEval shared task 6 , sub - task A on the identi\ufb01cation of offensive language .", "entities": []}, {"text": "Our proposed model , C - BiGRU , combines a Convolutional Neural Network ( CNN ) with a bidirectional Recurrent Neural Network ( RNN ) .", "entities": [[6, 7, "MethodName", "BiGRU"]]}, {"text": "We utilize word2vec to capture the semantic similarities between words .", "entities": []}, {"text": "This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non - offensive tweets .", "entities": []}, {"text": "In addition , we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets .", "entities": []}, {"text": "Our model achieved a macro F1 - score of 79.40 % on the SemEval dataset .", "entities": [[5, 8, "MetricName", "F1 - score"]]}, {"text": "1 Introduction The ever - increasing amount of user - generated data introduces new challenges in terms of automatic content moderation , especially regarding hate speech and offensive language detection .", "entities": [[24, 29, "DatasetName", "hate speech and offensive language"]]}, {"text": "User content mostly consists of microposts , where the context of a post can be missing or inferred only from current events .", "entities": []}, {"text": "The challenge of automatic identi\ufb01cation and detection of online aggressiveness has therefore gained increasing popularity in the scienti\ufb01c community over the last years .", "entities": []}, {"text": "Several recent workshops and conferences such as TRAC ( Kumar et al . , 2018 ) , ALW2 ( Fi \u02c7ser et al . , 2018 ) , and GermEval ( Wiegand et al . , 2018 ) show the growing importance of this subject .", "entities": [[9, 10, "DatasetName", "Kumar"]]}, {"text": "The SemEval 2019 shared task 6 ( Zampieri et al . , 2019b ) further addresses this topic by introducing the Offensive Language Identi\ufb01cation Dataset ( OLID ) , which consists of tweets , labeled with a three - level annotation model ( Zampieri et al . , 2019a ) .", "entities": [[26, 27, "DatasetName", "OLID"]]}, {"text": "Sub - task A is composed of a binary classi\ufb01cation problem of whether a tweet in the dataset is offensive or not .", "entities": []}, {"text": "Sub - task B focuses on different categories of offensive language and the goalof sub - task C is to identify the targeted individual of an offensive tweet .", "entities": []}, {"text": "In the following paper , we present our contribution to sub - task A.", "entities": []}, {"text": "After the related work section , we outline our conducted experiments in section 3 and further describe the used baseline model , as well as the submitted model .", "entities": []}, {"text": "In section 4 we report the results of our experiments on the OLID dataset and the additionally used GermEval dataset .", "entities": [[12, 13, "DatasetName", "OLID"]]}, {"text": "Section 5 discusses our results and section 6 concludes our work and describes possible future work .", "entities": []}, {"text": "2 Related Work Several methods and models have been presented in literature over the last decade to address the predicament of identifying hate speech , offensive language , and online aggressiveness .", "entities": [[22, 24, "DatasetName", "hate speech"]]}, {"text": "In the following section , we present the most notable contributions related to our work .", "entities": []}, {"text": "The tweets collected by Davidson et al . ( 2017 ) were divided into Hate , Offensive , and Neither .", "entities": []}, {"text": "Their proposed algorithm uses unigram , bigram , and trigram tokens as features , weighted by the respective TF - IDF , as well as Part - of - Speech ( POS ) tagging and different metrics to determine the readability and sentiment of a tweet .", "entities": [[25, 28, "DatasetName", "Part - of"]]}, {"text": "Logisticregression and linear SVM result in the best performance for a wide range of assessed classi\ufb01ers .", "entities": [[3, 4, "MethodName", "SVM"]]}, {"text": "Nobata et al .", "entities": []}, {"text": "( 2016 ) collected comments from Yahoo !", "entities": []}, {"text": "Finance and News articles over a time period of one year and labeled them as either \u2019 Abusive \u2019 or \u2019 Clean \u2019 .", "entities": []}, {"text": "They experimented with various different features , including n - gram , linguistic , syntactic , and distributional semantics features .", "entities": []}, {"text": "Various approaches utilized deep learning models for text categorization .", "entities": [[7, 9, "TaskName", "text categorization"]]}, {"text": "Zhang et al .", "entities": []}, {"text": "( 2015 ) proposed a character - level convolutional network for text classi\ufb01cation on large - scale datasets .", "entities": []}, {"text": "Their network uses 1 - dimensional convolutional \ufb01lters to extract features from different character embed-", "entities": []}, {"text": "723dings .", "entities": []}, {"text": "Gamb \u00a8ack and Sikdar ( 2017 ) further experimented with convolutional networks in the context of online hate speech classi\ufb01cation .", "entities": [[17, 19, "DatasetName", "hate speech"]]}, {"text": "Their research work compares different types of convolutional models , namely character - level , word vectors with a pretrained word2vec ( w2v ) model , randomly generated word vectors , and w2v in combination with character n - grams .", "entities": []}, {"text": "The results of their experiments suggest that w2v embeddings are the most suitable for this task .", "entities": []}, {"text": "Zhang et al .", "entities": []}, {"text": "( 2018 ) suggest an architecture similar to our network , where a convolutional \ufb01lter extracts features from pretrained word embeddings .", "entities": [[19, 21, "TaskName", "word embeddings"]]}, {"text": "After max pooling , the feature maps are processed using a unidirectional GRU .", "entities": [[1, 3, "MethodName", "max pooling"], [12, 13, "MethodName", "GRU"]]}, {"text": "Their model is compared to a bag - of - n - gram model on various multi - class hate speech datasets and shows promising results .", "entities": [[19, 21, "DatasetName", "hate speech"]]}, {"text": "A detailed survey on different architectures , methods and features for offensive language detection is provided by Schmidt and Wiegand ( 2017 ) .", "entities": []}, {"text": "3 System Description", "entities": []}, {"text": "In addition to Twitter data provided by the organizers of the SemEval shared task , we further evaluate our approach on German tweets from the GermEval ( 2018 ) shared task .", "entities": []}, {"text": "The OLID dataset contains 13,240 tweets , with 4,400 offensive and 8,840 non - offensive tweets ( 66.77 % offensive , 33.23 % non - offensive ) .", "entities": [[1, 2, "DatasetName", "OLID"]]}, {"text": "Similarly , the GermEval dataset contains 5,009 tweets , divided into 1,688 offensive and 3,321 non - offensive tweets ( 66.30 % offensive , 33.70 % non - offensive ) .", "entities": []}, {"text": "To compensate for the imbalanced class distributions and weigh each class equally , we choose the macro averaged F1 - score of both classes as our main evaluation metric .", "entities": [[18, 21, "MetricName", "F1 - score"]]}, {"text": "From both data sets we use 10 % of our tweets as test set .", "entities": []}, {"text": "The remaining tweets are split into 90 % training set and 10 % validation set .", "entities": []}, {"text": "We conduct a strati\ufb01ed 10 - fold cross - validation on the training and validation set to prevent over\ufb01tting and to validate our model .", "entities": []}, {"text": "The pretrained w2v model , which is used to initialize the weights of our embedding layer , resulted from the work of Godin et al . ( 2015 ) .", "entities": []}, {"text": "The w2v model for the GermEval dataset originates from our previous work ( 2018 ) .", "entities": []}, {"text": "For comparison to our proposed model , a token bag - of - n - gram model composed of unigrams , bigrams , and trigrams weighted by their TF - IDF is used as baseline approach .", "entities": []}, {"text": "We subsequently analyze the performance of different classi\ufb01ers on theresulting feature space .", "entities": []}, {"text": "We have used the packages keras , scikit - learn , gensim , and nltkfor preprocessing and the implementation of our models .", "entities": []}, {"text": "3.1 Preprocessing Tweets are \ufb01rst tokenized and converted to lowercase .", "entities": []}, {"text": "We constrain repeated character sequences to length 3 and replace all longer character sequences .", "entities": []}, {"text": "HTML character encodings are replaced by their corresponding literal or token representation ( e.g. \u2018 & amp ; \u2019 translates to \u2018 and \u2019 ) .", "entities": []}, {"text": "Tokens are further split if they enclose a set of special characters ( \u2018 n \u2019 , \u2018 / \u2019 , \u2018 & \u2019 , \u2018 - \u2019 ) .", "entities": []}, {"text": "Since hashtags are often used to replace contextually important words mid - sentence , we split hashtags in the actual hashsymbol and the following string to keep the semantic information of a hashtag ( e.g. \u2018 Brainless # Liberal Stooge Ocasio - Cortez \u2019 ) .", "entities": []}, {"text": "3.2 Baseline Model A TF - IDF bag - of - words model as baseline approach is chosen to evaluate the performance of our model .", "entities": []}, {"text": "We limit our feature space to the 10,000 most frequently used unigrams , bigrams , and trigrams in a corpus .", "entities": []}, {"text": "Furthermore , we stem each token in the preprocessing phase and remove stopwords .", "entities": []}, {"text": "We compare the performance of several classi\ufb01ers , namely multinomial Naive Bayes ( NB ) , SVM , Decision Tree ( DT ) , and Logistic Regression ( LogR ) and conduct a grid search to optimize our hyper - parameters .", "entities": [[16, 17, "MethodName", "SVM"], [25, 27, "MethodName", "Logistic Regression"]]}, {"text": "3.3 C - BiGRU After the preprocessing step , we construct a dictionary which maps all unique tokens to their number of occurrences in the respective corpus .", "entities": [[3, 4, "MethodName", "BiGRU"]]}, {"text": "Tokens which appear only once in a corpus are disregarded and treated as unknown token .", "entities": []}, {"text": "As a next step , we construct the weighting matrix Wm\u0002dim for our embedding layer , where dim is the dimension of the used w2v model and mthe number of unique tokens ti ; i2f1 ; : : : ; mg .", "entities": []}, {"text": "The word vector oftiis stored in Wif the token is represented in the w2v model .", "entities": []}, {"text": "If tihas no pretrained word vector , we generate a random vector drawn from the uniform distribution withinh \u0000q 6 dim;q 6 dimi as suggested by He et al . ( 2015 ) .", "entities": []}, {"text": "We \ufb01x the maximum length of a sentence to 150 tokens , longer sequences are clipped at the end and shorter sequences are padded with a masking token .", "entities": []}, {"text": "724The convolutional layer of our classi\ufb01er consists of ( k\u0002128 ) 1 - dimensional \ufb01lters , where k is the number of different window sizes .", "entities": []}, {"text": "These window sizes range from 2 to 5 and allow the extraction of n - gram features .", "entities": []}, {"text": "The padding of the input is kept constant , resulting in the same output sequence length as the input .", "entities": []}, {"text": "We further choose ReLu as activation function .", "entities": [[3, 4, "MethodName", "ReLu"], [5, 7, "HyperparameterName", "activation function"]]}, {"text": "The resulting feature maps are concatenated and passed towards the recurrent layer .", "entities": []}, {"text": "Gated Recurrent Units ( GRU ) as initially proposed by Cho et al .", "entities": [[4, 5, "MethodName", "GRU"]]}, {"text": "( 2014 ) are used in RNNs to capture long - term dependencies of input sequences .", "entities": []}, {"text": "Similar to Long Short - Term Memory ( LSTM ) units ( Hochreiter and Schmidhuber , 1997 ) GRU are able to overcome the vanishing gradient problem by using a gating mechanism .", "entities": [[2, 7, "MethodName", "Long Short - Term Memory"], [8, 9, "MethodName", "LSTM"], [18, 19, "MethodName", "GRU"]]}, {"text": "GRU have shown to achieve comparable results to LSTM in sequence modeling tasks and are able to outperform the latter on smaller data sets ( Chung et al . , 2014 ) .", "entities": [[0, 1, "MethodName", "GRU"], [8, 9, "MethodName", "LSTM"]]}, {"text": "The recurrent layer in our model consists of a bidirectional GRU , where the concatenated feature maps , which resulted from the convolutional layer , are used as input for the GRU layer .", "entities": [[9, 11, "MethodName", "bidirectional GRU"], [31, 32, "MethodName", "GRU"]]}, {"text": "Simultaneously , the reversed copy of the input sequence is used for the second GRU layer .", "entities": [[14, 15, "MethodName", "GRU"]]}, {"text": "Both GRU layers return a hidden state for each processed feature map .", "entities": [[1, 2, "MethodName", "GRU"]]}, {"text": "The output of both layers is then concatenated .", "entities": []}, {"text": "We set the length of the returned hidden states to 64for both layers , resulting in an output space of ( 150\u0002128 ) neurons .", "entities": []}, {"text": "Afterwards , a global max pooling layer reduces the output space to ( 1\u0002128 ) nodes .", "entities": [[4, 6, "MethodName", "max pooling"]]}, {"text": "The following fully - connected layer consists of 32neurons , which connect to a single output neuron .", "entities": []}, {"text": "The output neuron utilizes the sigmoid activation function .", "entities": [[5, 7, "MethodName", "sigmoid activation"]]}, {"text": "To additionally prevent over\ufb01tting , we include two dropout layers with a dropout rate of 0.2 ; oneafter the embedding layer and another one after the fully - connected layer .", "entities": []}, {"text": "Furthermore , we adopt early stopping and use 10 % of the training data as validation split .", "entities": [[4, 6, "MethodName", "early stopping"]]}, {"text": "We use cross entropy as error function for our model and the optimizer \u2018 adam \u2019 to update our network weights ( Kingma and Ba , 2014 ) .", "entities": [[12, 13, "HyperparameterName", "optimizer"]]}, {"text": "The batch size for the gradient update is set to 32 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "A schema of our proposed model is illustrated in Figure 1 . 4 Results For the comparison model , the SVM performs best on the OLID dataset with an F1 - score of 70.22 % averaged over a 10 - fold cross - validation .", "entities": [[20, 21, "MethodName", "SVM"], [25, 26, "DatasetName", "OLID"], [29, 32, "MetricName", "F1 - score"]]}, {"text": "The SVM also shows the best results on the GermEval dataset with an F1 - score of 66.61 % .", "entities": [[1, 2, "MethodName", "SVM"], [13, 16, "MetricName", "F1 - score"]]}, {"text": "The evaluation on the test set results in 66.78 % F1 - score for the GermEval gold test set .", "entities": [[10, 13, "MetricName", "F1 - score"]]}, {"text": "The evaluation of the baseline model for the OLID gold test set is not possible at the time of writing , since the gold test data have not yet been released .", "entities": [[8, 9, "DatasetName", "OLID"]]}, {"text": "The C - BiGRU achieved a 76.28 % F1 - score on the OLID and a 71.13 % F1 - score on the GermEval dataset on average over a 10 - fold cross - validation .", "entities": [[3, 4, "MethodName", "BiGRU"], [8, 11, "MetricName", "F1 - score"], [13, 14, "DatasetName", "OLID"], [18, 21, "MetricName", "F1 - score"]]}, {"text": "On the OLID gold test set , our model achieved an F1 - score of 79.40 % .", "entities": [[2, 3, "DatasetName", "OLID"], [11, 14, "MetricName", "F1 - score"]]}, {"text": "The evaluation on the GermEval gold test data resulted in a 72.41 % F1score .", "entities": []}, {"text": "An overview of all results can be found in Table 1 .", "entities": []}, {"text": "Figure 2 shows the confusion matrix of our submitted predictions for the SemEval shared task .", "entities": []}, {"text": "Baseline C - BiGRU CV gold CV gold OLID 70.22 % - 76.28 % 79.40 % GermEval 66.61 % 66.78 % 71.13 % 72.41 % Table 1 : All results in table form ( CV = crossvalidation ; gold = gold test set ) .", "entities": [[3, 4, "MethodName", "BiGRU"], [8, 9, "DatasetName", "OLID"]]}, {"text": "Figure 1 : Representation of the proposed classi\ufb01er .", "entities": []}, {"text": "725 Figure 2 : Confusion Matrix of the OLID gold test set , sub - task A. Depicted are instances and normalized values .", "entities": [[8, 9, "DatasetName", "OLID"]]}, {"text": "5 Discussion The presented model continues our work on the identi\ufb01cation of offensive German tweets ( 2018 ) .", "entities": []}, {"text": "We were able to improve our proposed model by adjusting the architecture of the recurrent layer in our neural network .", "entities": []}, {"text": "By using a bidirectional GRU instead of a unidirectional LSTM , we are able to capture past and future information about the input sequence and exploit the better performance of GRU networks on smaller datasets .", "entities": [[3, 5, "MethodName", "bidirectional GRU"], [9, 10, "MethodName", "LSTM"], [30, 31, "MethodName", "GRU"]]}, {"text": "Furthermore , we return the hidden states for each feature map instead of returning only the last hidden state .", "entities": []}, {"text": "This allows us to extract higher - level sequentially dependent features from each concatenated feature map .", "entities": []}, {"text": "Our experiments show that our suggested model outperforms the baseline model on both datasets .", "entities": []}, {"text": "The difference between the F1 - scores for the English and German dataset might be attributed to the smaller size of the German training set , which contains only about 5,000 tweets .", "entities": [[4, 5, "MetricName", "F1"]]}, {"text": "The discrepancy between the results of our cross - validation and achieved score on the OLID test set might be explained by the small amount of test tweets , which may lead to imprecise results for the submitted runs .", "entities": [[15, 16, "DatasetName", "OLID"]]}, {"text": "By utilizing w2v as features , we are able to limit extensive and language speci\ufb01c preprocessing .", "entities": []}, {"text": "\u201c @USER Lolol God he is such an a**hole . \u201d", "entities": []}, {"text": "In this example , the vector representation of \u201c a**hole \u201d has a high cosine similarity ( 0.63 ) to thevector representation of \u201c asshole \u201d , which allows our model to classify this tweet as offensive .", "entities": []}, {"text": "On the contrary , our approach falls short when confronted with indirect insults .", "entities": []}, {"text": "\u201c @USER @USER I m sure the air that he is breathing is also bad . \u201d", "entities": []}, {"text": "Our model wrongly predicts a non - offensive tweet in this instance .", "entities": []}, {"text": "The detection of offensive , hateful , racist , and/or sexist user behavior in social media still proves to be a challenge .", "entities": []}, {"text": "Even for humans , it can be problematic to identify offensive microposts , since these posts can be ambiguous and dependant on the personal mindset of a reader .", "entities": []}, {"text": "Ross et al .", "entities": []}, {"text": "( 2017 ) show that it can be dif\ufb01cult to measure the agreement of annotators about hate speech in the light of the European refugee crisis .", "entities": [[16, 18, "DatasetName", "hate speech"]]}, {"text": "They conclude that instead of a classi\ufb01cation problem , a regression model with an average offensiveness score of multiple annotators might be more suitable for this task .", "entities": []}, {"text": "Furthermore , it can be dif\ufb01cult to grasp the full context of an arbitrary tweet .", "entities": []}, {"text": "With only excerpts of a conversation , the context and true intention of the author may be dif\ufb01cult to determine .", "entities": []}, {"text": "6 Conclusion and Future Work", "entities": []}, {"text": "In this paper , we describe our submitted model for the SemEval shared task 6 and evaluation methods for the identi\ufb01cation of online aggressiveness in social media microposts .", "entities": []}, {"text": "Our model achieves good results in the two evaluated datasets .", "entities": []}, {"text": "For the OLID dataset which contains English tweets , a macro F1 - score of 79.40 % is reached , while our network resulted in an F1 - score of 72.41 % on the GermEval dataset , which consists of German tweets .", "entities": [[2, 3, "DatasetName", "OLID"], [11, 14, "MetricName", "F1 - score"], [26, 29, "MetricName", "F1 - score"]]}, {"text": "We plan to evaluate our approach on more datasets to further investigate the potential of our model for different languages .", "entities": []}, {"text": "One such set is the TRAC dataset , which contains aggressionannotated Facebook posts and comments in Hindi .", "entities": []}, {"text": "Furthermore , we want to examine whether additional features such as character - level embeddings or POS tagging will improve our results .", "entities": []}, {"text": "Inclusion of \ufb01gurative language detection has proved to enhance many NLP tasks , such as argument mining and so - called hidden hate speech ( Mitrovi \u00b4 c et al . , 2017 ) , which is also one of our future directions .", "entities": [[15, 17, "TaskName", "argument mining"], [22, 24, "DatasetName", "hate speech"]]}, {"text": "726References Bastian Birkeneder , Jelena Mitrovi \u00b4 c , Julia Niemeier , Leon Teubert , and Siegfried Handschuh . 2018 .", "entities": []}, {"text": "upInf - Offensive Language Detection in German Tweets .", "entities": []}, {"text": "In Proceedings of the GermEval 2018 Workshop , pages 71 \u2013 78 .", "entities": []}, {"text": "Kyunghyun Cho , Bart Van Merri \u00a8enboer , Caglar Gulcehre , Dzmitry Bahdanau , Fethi Bougares , Holger Schwenk , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Learning phrase representations using rnn encoder - decoder for statistical machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1406.1078v3 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Junyoung Chung , Caglar Gulcehre , KyungHyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Empirical evaluation of gated recurrent neural networks on sequence modeling.arXiv preprint arXiv:1412.3555 .", "entities": []}, {"text": "Thomas Davidson , Dana Warmsley , Michael Macy , and Ingmar Weber . 2017 .", "entities": []}, {"text": "Automated hate speech detection and the problem of offensive language .", "entities": [[1, 4, "TaskName", "hate speech detection"]]}, {"text": "arXiv preprint arXiv:1703.04009 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Darja Fi \u02c7ser , Ruihong Huang , Vinodkumar Prabhakaran , Rob V oigt , Zeerak Waseem , and Jacqueline Wernimont .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Proceedings of the 2nd workshop on abusive language online ( alw2 ) .", "entities": [[6, 8, "TaskName", "abusive language"]]}, {"text": "In Proceedings of the 2nd Workshop on Abusive Language Online ( ALW2 ) .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Bj\u00a8orn", "entities": []}, {"text": "Gamb \u00a8ack and Utpal Kumar Sikdar . 2017 .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "Using convolutional neural networks to classify hatespeech .", "entities": []}, {"text": "In Proceedings of the First Workshop on Abusive Language Online , pages 85\u201390 .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Fr\u00b4ederic Godin , Baptist Vandersmissen , Wesley De Neve , and Rik Van de Walle .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Multimedia lab@acl wnut ner shared task : Named entity recognition for twitter microposts using distributed word representations .", "entities": [[7, 10, "TaskName", "Named entity recognition"]]}, {"text": "In Proceedings of the Workshop on Noisy User - generated Text , pages 146\u2013153 .", "entities": []}, {"text": "Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . 2015 .", "entities": []}, {"text": "Delving deep into recti\ufb01ers : Surpassing human - level performance on imagenet classi\ufb01cation .", "entities": [[11, 12, "DatasetName", "imagenet"]]}, {"text": "In Proceedings of the IEEE international conference on computer vision , pages 1026\u20131034 .", "entities": []}, {"text": "Sepp Hochreiter and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural computation , 9(8):1735\u20131780 .", "entities": []}, {"text": "Diederik P Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ritesh Kumar , Atul Kr .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "Ojha , Marcos Zampieri , and Shervin Malmasi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Proceedings of the \ufb01rst workshop on trolling , aggression and cyberbullying ( trac-2018 ) .", "entities": []}, {"text": "In Proceedings of the First Workshop on Trolling , Aggression and Cyberbullying ( TRAC2018 ) .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jelena Mitrovi \u00b4 c , Cliff O\u2019Reilly , Miljana Mladenovi \u00b4 c , and Siegfried Handschuh . 2017 .", "entities": []}, {"text": "Ontological representations of rhetorical \ufb01gures for argument mining .", "entities": [[6, 8, "TaskName", "argument mining"]]}, {"text": "Argument & Computation , 8(3):267\u2013287 .", "entities": []}, {"text": "Chikashi Nobata , Joel Tetreault , Achint Thomas , Yashar Mehdad , and Yi Chang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Abusive language detection in online user content .", "entities": [[0, 2, "TaskName", "Abusive language"]]}, {"text": "In Proceedings of the 25th international conference on world wide web , pages 145\u2013153 .", "entities": []}, {"text": "International World Wide Web Conferences Steering Committee .", "entities": []}, {"text": "Bj\u00a8orn Ross , Michael Rist , Guillermo Carbonell , Benjamin Cabrera , Nils Kurowsky , and Michael Wojatzki . 2017 .", "entities": []}, {"text": "Measuring the reliability of hate speech annotations : The case of the european refugee crisis .", "entities": [[4, 6, "DatasetName", "hate speech"]]}, {"text": "arXiv preprint arXiv:1701.08118 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Anna Schmidt and Michael Wiegand .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A survey on hate speech detection using natural language processing .", "entities": [[3, 6, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media , pages 1\u201310 .", "entities": []}, {"text": "Michael Wiegand , Melanie Siegel , and Josef Ruppenhofer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Overview of the germeval 2018 shared task on the identi\ufb01cation of offensive language .", "entities": []}, {"text": "Austrian Academy of Sciences , Vienna September 21 , 2018 .", "entities": []}, {"text": "Marcos Zampieri , Shervin Malmasi , Preslav Nakov , Sara Rosenthal , Noura Farra , and Ritesh Kumar . 2019a .", "entities": [[17, 18, "DatasetName", "Kumar"]]}, {"text": "Predicting the Type and Target of Offensive Posts in Social Media .", "entities": []}, {"text": "In Proceedings of NAACL .", "entities": []}, {"text": "Marcos Zampieri , Shervin Malmasi , Preslav Nakov , Sara Rosenthal , Noura Farra , and Ritesh Kumar . 2019b .", "entities": [[17, 18, "DatasetName", "Kumar"]]}, {"text": "SemEval-2019 Task 6 : Identifying and Categorizing Offensive Language in Social Media ( OffensEval ) .", "entities": []}, {"text": "In Proceedings of The 13th International Workshop on Semantic Evaluation ( SemEval ) .", "entities": []}, {"text": "Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 .", "entities": []}, {"text": "Character - level convolutional networks for text classi\ufb01cation .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 649\u2013657 .", "entities": []}, {"text": "Ziqi Zhang , David Robinson , and Jonathan Tepper .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Detecting hate speech on twitter using a convolution - gru based deep neural network .", "entities": [[1, 3, "DatasetName", "hate speech"], [7, 8, "MethodName", "convolution"], [9, 10, "MethodName", "gru"]]}, {"text": "In European Semantic Web Conference , pages 745\u2013760 .", "entities": []}, {"text": "Springer .", "entities": []}]