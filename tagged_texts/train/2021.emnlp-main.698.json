[{"text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 8862\u20138874 November 7\u201311 , 2021 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2021 Association for Computational Linguistics8862MLEC - QA : A Chinese Multi - Choice Biomedical Question Answering Dataset Jing Li , Shangping Zhong andKaizhi Chen College of Computer and Data Science , Fuzhou University , Fuzhou , China { N190320027 , spzhong , ckz}@fzu.edu.cn Abstract Question Answering ( QA ) has been successfully applied in scenarios of human - computer interaction such as chatbots and search engines .", "entities": [[14, 16, "TaskName", "Question Answering"], [44, 46, "TaskName", "Question Answering"]]}, {"text": "However , for the speci\ufb01c biomedical domain , QA systems are still immature due to expert - annotated datasets being limited by category and scale .", "entities": []}, {"text": "In this paper , we present MLEC - QA , the largest - scale Chinese multi - choice biomedical QA dataset , collected from the National Medical Licensing Examination in China .", "entities": []}, {"text": "The dataset is composed of \ufb01ve subsets with 136,236 biomedical multi - choice questions with extra materials ( images or tables ) annotated by human experts , and \ufb01rst covers the following biomedical sub-\ufb01elds : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Traditional Chinese Medicine Combined with Western Medicine .", "entities": []}, {"text": "We implement eight representative control methods and open - domain QA methods as baselines .", "entities": []}, {"text": "Experimental results demonstrate that even the current best model can only achieve accuracies between 40 % to 55 % on \ufb01ve subsets , especially performing poorly on questions that require sophisticated reasoning ability .", "entities": []}, {"text": "We hope the release of the MLEC - QA dataset can serve as a valuable resource for research and evaluation in open - domain QA , and also make advances for biomedical QA systems.1 1 Introduction As a branch of the QA task , Biomedical Question Answering ( BQA ) enables effectively perceiving , accessing , and understanding complex biomedical knowledge by innovative applications , which makes BQA an important QA application in the biomedical domain ( Jin et al . , 2021 ) .", "entities": [[45, 47, "TaskName", "Question Answering"]]}, {"text": "Such a task has recently attracted considerable attention from the NLP community ( Zweigenbaum , 2003 ; He et al . , 2020b ; Jin et al . , 2020 ) , but is still confronted with the following three key challenges : 1https://github.com/Judenpech/MLEC-QA(1 ) Most work attempt to build BQA systems with deep learning and neural network techniques ( Ben Abacha et al . , 2017 , 2019b ; Pampari et", "entities": []}, {"text": "al . , 2018 ) and are thus data - hungry .", "entities": []}, {"text": "However , annotating large - scale biomedical question - answer pairs with high quality is prohibitively expensive .", "entities": []}, {"text": "As a result , current expert - annotated BQA datasets are small in size .", "entities": []}, {"text": "( 2 ) Multi - choice QA is a typical format type of BQA dataset .", "entities": []}, {"text": "Most previous work focus on such format type of datasets in which contents are in the \ufb01eld of clinical medicine ( Zhang et al . , 2018b ; Jin et al . , 2020 ) and consumer health ( Zhang et al . , 2017 , 2018a ; He et al . , 2019 ; Tian et al . , 2019 ) .", "entities": []}, {"text": "However , there are many other specialized sub-\ufb01elds in biomedicine that have not been studied before ( e.g. , Stomatology ) .", "entities": []}, {"text": "( 3 ) Ideal BQA systems should not only focus on raw text data , but also fully utilize various types of biomedical resources , such as images and tables .", "entities": []}, {"text": "Unfortunately , most BQA datasets are either texts ( Tsatsaronis et al . , 2015 ; Pampari et al . , 2018 ; Jin et al . , 2019 ) or images ( Lau et al . , 2018 ; Ben Abacha et al . , 2019a ; He et al . , 2020a ) ; as a result , BQA datasets that are composed by fusing different biomedical resources are relatively limited .", "entities": []}, {"text": "To push forward the variety of BQA datasets , we present MLEC - QA , the largest - scale Chinese multi - choice BQA dataset .", "entities": []}, {"text": "Questions in MLECQA are collected from the National Medical Licensing Examination in China ( NMLEC ) 2 , which are carefully designed by human experts to evaluate professional knowledge and skills for those who want to be medical practitioners in China .", "entities": []}, {"text": "The NMLEC has a total number of 24 categories of exams , but only \ufb01ve of them have the written exams in Chinese .", "entities": []}, {"text": "Every year , only around 18 - 22 % of applicants can pass one of these exams , showing the complexity and dif\ufb01culty of passing 2http://www.nmec.org.cn/Pages/ArticleList-12-0-01.html", "entities": []}, {"text": "8863them even for skilled humans .", "entities": []}, {"text": "There are three main properties of MLEC - QA : ( 1 ) MLEC - QA is the largest - scale Chinese multichoice BQA dataset , containing 136,236 questions with extra materials ( images or tables ) , Table 1 shows an example .", "entities": []}, {"text": "( 2 ) MLEC - QA \ufb01rst covers the following biomedical sub-\ufb01elds : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Traditional Chinese Medicine Combined with Western Medicine ( denoted as Chinese Western Medicine ) .", "entities": []}, {"text": "Only one ( Clinic ) of them has been studied in previous research .", "entities": []}, {"text": "( 3 ) MLEC - QA provides extra labels of \ufb01ve question types ( A1 , A2 , A3 / A4 and B1 ) for each question , and an in - depth analysis of the most frequent reasoning types of the questions in MLEC - QA , such as lexical matching , multi - sentence reading and concept summary , etc .", "entities": []}, {"text": "Detailed analysis can be found in Section 3.2 .", "entities": []}, {"text": "Examples of sub-\ufb01elds and question types are summarized in Table 2 .", "entities": []}, {"text": "We set each example of \ufb01ve question types corresponding to one of the sub\ufb01elds due to page limits .", "entities": []}, {"text": "[ Question ] \u0b33\u011163\u0ddfb3\u0f6c\u0d48\u0c2d\u04c9\u0b9d\u07aa\u0e68\u0e65\u03f4\u0b7c\u0e73b \u04b0\u0e38\u011f\u04b0\u0e38 \u0482 \u0786\u1254\u0111\u0d5f\u0da4\u0111\u0da0\u0e5c\u0964\u073b\u0613 \u0651\u0d1d\u0569\u111da\u08e0\u0f5b\u0c3c\u11b0\u0111\u0db9\u11aa\u05ee\u0803\u1231\u0cc2\u0111 \u0814\u1166\u09ef\u0a7b\u06dab\u0da0\u04a7 Babinski \u1198\u0afc\u0f41\u0111\u0e68 \u0a54CT\u0cc2\u0e6d\u0111\u07d1\u06be\u1180\u05a5\u1190 \u060e \u0d5e ( ) Male , 63 years old .", "entities": []}, {"text": "Had headache with vomiting after long - distance running 3 hours ago .", "entities": []}, {"text": "Physical Examination : not cooperative , somnolence , double pupillary light re\ufb02ex exists , neck rigidity , free movement of limbs , muscle tension slightly high .", "entities": []}, {"text": "Bilateral Babinski sign is evident , CT of the head as shown in the \ufb01gure .", "entities": []}, {"text": "Which option is the correct diagnosis of the patient :", "entities": []}, {"text": "[ Options ] A\u011f\u11ea\u0ea9\u0b07\u0f2f\u0c38\u051b\u0fd3 / Subarachnoid hemorrhage B\u011f\u0b37\u0d69\u11d6\u0a3f / Tumor of ventricle C\u011f\u0b37\u0d69\u0b35\u11d6 / Ventricular cyst D\u011f\u0b37\u0d69\u051b\u0fd3 / Ventricular hemorrhage E\u011f\u0a9d\u0a8e\u0b07\u07c4\u06c1/ Choroidal calci\ufb01cation [ Answer ] D Table 1 : An example of questions with additional images in MLEC - QA dataset .", "entities": []}, {"text": "As an attempt to solve MLEC - QA and provide strong baselines , we implement eight representative control methods and open - domain QA methods by a two - stage retriever - reader framework : ( 1 ) A retriever \ufb01nding documents that ( might ) contain an answer from a large collection of documents .", "entities": []}, {"text": "We adopt Chinese Wikipedia dumps3as our information sources , and use a distributed search and 3https://dumps.wikimedia.org/analytics engine , ElasticSearch4 , as the document store and document retriever .", "entities": []}, {"text": "( 2 ) A reader \ufb01nding the answer in given documents retrieved by the retriever .", "entities": []}, {"text": "We \ufb01ne - tune \ufb01ve pre - trained language models for machine reading comprehension as the reader .", "entities": [[11, 14, "TaskName", "machine reading comprehension"]]}, {"text": "Experimental results show that even the current best model can only achieve accuracies of 53 % , 44 % , 40 % , 55 % , and 50 % on the \ufb01ve categories of subsets : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Chinese Western Medicine , respectively .", "entities": []}, {"text": "The models especially perform poorly on questions that require understanding comprehensive biomedical concepts and handling complex reasoning .", "entities": []}, {"text": "Question Type Example ( \u2217represents the correct answer ) A1(Public Health )", "entities": []}, {"text": "What should be used to compare the results of two samples ?", "entities": []}, {"text": "A. T test B. x2test C. \u00b5test D. F test \u2217E. Rank sum test B1(Chinese Western Medicine ) A. Heart and liver B. Spleen and lungs C. Liver and kidneys D. Heart and kidneys E. Spleen and kidneys 1 .", "entities": []}, {"text": "What is the viscera that \" Yikui \" in Yikui homologous refers to ?", "entities": []}, {"text": "( E ) 2 .", "entities": []}, {"text": "What is the viscera that \" water and \ufb01re \" in harmonization of water and \ufb01re refers to ?", "entities": []}, {"text": "( D ) A2(Clinic )", "entities": []}, {"text": "Primipara , 29 years old , at 37 weeks of gestation .", "entities": []}, {"text": "Had jet vomiting once this morning , suddenly convulsed an hour ago and then went to hospital in a coma .", "entities": []}, {"text": "Physical examination : BP 180/120mmhg , urine protein ( + + + ) .", "entities": []}, {"text": "The most likely diagnosis of this patient is : \u2217A. Eclampsia B. Hematencephalon C. Hysteria D. Epilepsy E. Cerebral thrombosis A3(Traditional Chinese Medicine )", "entities": []}, {"text": "Female , 28 years old .", "entities": []}, {"text": "In the recent month , has oral ulcer attacks repeatedly , upset , dif\ufb01culties to sleep at night , dry stool , defecates every 1 - 2 days , has dry mouth , does not like drinking water , and has yellow urine , red tongue , greasy fur , and rapid pulse .", "entities": []}, {"text": "1 .", "entities": []}, {"text": "The drug of choice for the treatment of the pattern is : A. Mirabilite B. Arctium lappa \u2217C. Bamboo leaf D. Baikal skullcap E. Gypsum 2 .", "entities": [[18, 19, "DatasetName", "Bamboo"]]}, {"text": "The appropriate compatible drug for the treatment of the disease is :", "entities": []}, {"text": "A. Angelica dahurica B. Cassia twig C. Rhizoma Zingiberis \u2217D. Rheum of\ufb01cinale E. Ash bark 3 .", "entities": []}, {"text": "Which of the following drugs should be used with caution during menstruation ?", "entities": []}, {"text": "A. Semen sojae praeparatum \u2217B. Rheum of\ufb01cinale C. Coptis chinensis D. Lophatherum gracile E. Rhizoma phragmitis A4(Stomatology )", "entities": []}, {"text": "A 50 - year - old patient comes to the outpatient clinic 2 years after the end of radiotherapy for nasopharyngeal carcinoma outside hospital .", "entities": []}, {"text": "Examination : full mouth multiple teeth dental surfaces with different degrees of caries , some of the affected teeth have become residual crowns , residual roots , less intraoral saliva , more soft scaling of the dental surfaces and sulci .", "entities": []}, {"text": "1 . The diagnosis of this patient is : A. Acute caries \u2217B. Rampant caries C. Chronic caries D. Secondary caries E. Smooth surface caries 2 .", "entities": []}, {"text": "There are several treatment designs as follows , except : A. Design treatment of full mouth caries B. Endodontic treatment of teeth with hypodontia \u2217C. Filling metal material D. Remineralization adjunctive therapy E. Regular review Table 2 : Examples of sub-\ufb01elds and question types in MLEC - QA .", "entities": []}, {"text": "The Chinese version is in Appendix D. In summary , the major contributions of this paper are threefold : \u2022We present MLEC - QA , the largest - scale Chinese multi - choice BQA dataset with extra materials , and it \ufb01rst covers \ufb01ve biomedical sub-\ufb01elds , only one of which has been studied in previous research .", "entities": []}, {"text": "4https://www.elastic.co/", "entities": []}, {"text": "8864\u2022We conduct an in - depth analysis on MLECQA , revealing that both comprehensive biomedical knowledge and sophisticated reasoning ability are required to answer questions .", "entities": []}, {"text": "\u2022We implement eight representative methods as baselines and show the performance of existing methods on MLEC - QA , and provide an outlook for future research directions .", "entities": []}, {"text": "2 Related Work Open - Domain BQA The Text REtrieval Conference ( TREC ) ( Voorhees and Tice , 2000 ) has triggered the open - domain BQA research .", "entities": [[12, 13, "DatasetName", "TREC"]]}, {"text": "At the time , most traditional BQA systems were employing complex pipelines with question processing , document / passage retrieval , and answer processing modules .", "entities": [[18, 20, "TaskName", "passage retrieval"]]}, {"text": "Examples of such systems include EPoCare ( Niu et al . , 2003 ) , MedQA ( Yu et al . , 2007 ;", "entities": []}, {"text": "Terol et al . , 2007 ; Wang et al . , 2007 ) and AskHERMES ( Cao et al . , 2011 ) .", "entities": []}, {"text": "With the introduction of various BQA datasets that are focused on speci\ufb01c biomedical topics , such as BioASQ ( Tsatsaronis et al . , 2015 ) , emrQA ( Pampari et al . , 2018 ) and PubMedQA ( Jin et al . , 2019 ) , pioneered by Chen et al .", "entities": [[17, 18, "DatasetName", "BioASQ"], [27, 28, "DatasetName", "emrQA"], [37, 38, "DatasetName", "PubMedQA"]]}, {"text": "( 2017 ) , the modern open - domain BQA systems largely simpli\ufb01ed the traditional BQA pipeline to a two - stage retriever - reader framework by combining information retrieval and machine reading comprehension models ( Ben Abacha et al . , 2017 , 2019b ) .", "entities": [[28, 30, "TaskName", "information retrieval"], [31, 34, "TaskName", "machine reading comprehension"]]}, {"text": "Moreover , the extensive use of medical images ( e.g. , CT ) and tables ( e.g. , laboratory examination ) has improved results in real - world clinical scenarios , making the BQA a task lying at the intersection of Computer Vision ( CV ) and NLP .", "entities": []}, {"text": "However , most BQA models focus on either texts or images ( Lau et al . , 2018 ; Ben Abacha et al . , 2019a ; He et al . , 2020a )", "entities": []}, {"text": "; as a result , BQA datasets that are composed by fusing different biomedical resources are relatively limited .", "entities": []}, {"text": "Open - Domain Multi - Choice BQA Datasets With rapidly increasing numbers of consumers asking health - related questions on online medical consultation websites , cMedQA ( Zhang et", "entities": []}, {"text": "al . , 2017 , 2018a ) , webMedQA ( He et al . , 2019 ) and ChiMed ( Tian et al . , 2019 ) exploit patient - doctor QA data to build consumer health QA datasets .", "entities": []}, {"text": "However , the quality problems in such datasets are that the answers are written by online - doctors andthe data itself has intrinsic noise .", "entities": []}, {"text": "By contrast , medical licensing examinations , which are designed by human medical experts , often take the form of multi - choice questions , and contain a signi\ufb01cant number of questions that require comprehensive biomedical knowledge and multiple reasoning ability .", "entities": []}, {"text": "Such exams are the perfect data source to push the development of BQA systems .", "entities": []}, {"text": "Several datasets have been released that exploit such naturally existing BQA data , which are summarized in Table 3 .", "entities": []}, {"text": "Collecting from the Spain public healthcare specialization examination , HEAD - QA ( Vilares and G\u00f3mez - Rodr\u00edguez , 2019 ) contains multichoice questions from six biomedical categories , including Medicine , Pharmacology , Psychology , Nursing , Biology and Chemistry .", "entities": []}, {"text": "NLPEC ( Li et al . , 2020 ) collects 21.7k multi - choice questions with human - annotated answers from the National Licensed Pharmacist Examination in China , but only a small number of sample data is available for public use .", "entities": []}, {"text": "Last but not least , clinical medicine , as one of the 24 categories in NMLEC , has been previously studied by MedQA ( Zhang et al . , 2018b ) and MEDQA ( Jin et al . , 2020 ) .", "entities": []}, {"text": "However , the former did not release any data or code , and the latter only focused on clinical medicine with 34k questions in their cross - lingual studies , questions with images or tables were not included , and none of the remaining categories in MLEC - QA were studied .", "entities": []}, {"text": "Dataset Size Content Metric Available Language Extra cMedQA v1.0 54k Consumer Health P@1", "entities": [[12, 13, "MetricName", "P@1"]]}, {"text": "Yes Chinese", "entities": []}, {"text": "No cMedQA v2.0 108k Consumer Health P@1 Yes Chinese", "entities": [[6, 7, "MetricName", "P@1"]]}, {"text": "No webMedQA 63k Consumer Health P@1 & MAP Yes Chinese No ChiMed 24.9k", "entities": [[5, 6, "MetricName", "P@1"], [7, 8, "DatasetName", "MAP"]]}, {"text": "Consumer Health Acc Yes Chinese No NLPEC 21k", "entities": [[2, 3, "MetricName", "Acc"]]}, {"text": "Pharmacology Acc No * Chinese No MedQA 235k Clinical Medicine Acc", "entities": [[1, 2, "MetricName", "Acc"], [10, 11, "MetricName", "Acc"]]}, {"text": "No Chinese No MEDQA 61k Clinical Medicine Acc Yes Chinese & English", "entities": [[7, 8, "MetricName", "Acc"]]}, {"text": "No HEAD - QA 6.8k", "entities": []}, {"text": "Multi - Category Acc Yes Spanish & English", "entities": [[3, 4, "MetricName", "Acc"]]}, {"text": "Yes MLEC - QA 136k Multi - Category Acc", "entities": [[8, 9, "MetricName", "Acc"]]}, {"text": "Yes Chinese Yes Table 3 : Comparison of MLEC - QA with existing opendomain multi - choice BQA datasets .", "entities": []}, {"text": "No * indicates a small number of sample data is available .", "entities": []}, {"text": "Extra indicates if the dataset provides extra material to answer questions .", "entities": []}, {"text": "3 MLEC - QA Dataset 3.1 Data Collection We collect 155,429 multi - choice questions from the 2006 to 2020 NMLEC and practice exercises from the Internet .", "entities": []}, {"text": "Except for the categories that do not use Chinese in examinations , all categories are included in MLEC - QA : Clinic ( Cli ) , Stomatology ( Sto ) , Public Health ( PH ) , Traditional Chinese", "entities": []}, {"text": "8865Medicine ( TCM ) , and Chinese Western Medicine ( CWM ) .", "entities": []}, {"text": "After removing duplicated or incomplete questions ( e.g. , some options missing ) , there are 136,236 questions in MLEC - QA , and each question contains \ufb01ve candidate options with one correct / best option and four incorrect or partially correct options .", "entities": []}, {"text": "We describe in detail the JSON data structure of MLEC - QA in Appendix B. MLEC - QA contains 1,286 questions with extra materials that provide additional information to answer correctly .", "entities": []}, {"text": "As shown in Figure 1 , the extra materials are all in a graphical format with various types , such as ECG , table of a patient \u2019s condition record , formula , CT , line graph , explanatory drawing , etc .", "entities": []}, {"text": "We include these questions with extra materials in MLEC - QA to facilitate future BQA explorations on the crossover studies of CV and NLP , although we will not exploit them in this work due to the various speci\ufb01cs involved in extra materials .", "entities": []}, {"text": "( a ) ECG   ( b ) Table ( c ) Explanatory Drawing   ( d ) CT ( e ) Line Graph   ( f ) Formula Figure 1 : Examples of extra materials .", "entities": []}, {"text": "Basically , as shown in Table 2and", "entities": []}, {"text": "Table 4", "entities": []}, {"text": ", the questions in MLEC - QA are divided into \ufb01ve types including : \u2022A1 : single statement question ; \u2022B1 : similar to A1 , with a group of options shared in multiple questions;\u2022A2 : questions accompanied by a clinical scenario ; \u2022A3 : similar to A2 , with information shared among multiple independent questions ; \u2022A4 : similar to A3 , with information shared among multiple questions , new information can be gradually added .", "entities": []}, {"text": "We further classify these questions into Knowledge Questions ( KQ ) and Case Questions ( CQ ) , where KQ ( A1+B1 ) focus on the de\ufb01nition and comprehension of biomedical knowledge , while CQ ( A2+A3 / A4 ) require analysis and practical application for real - world medical scenarios .", "entities": []}, {"text": "Both types of questions require multiple reasoning ability to answer .", "entities": []}, {"text": "SubsetKnowledge Questions Case Questions TotalA1 ( Extra ) B1 ( Extra ) A2 ( Extra ) A3 / A4 ( Extra ) Cli 16,996 ( 19 ) 5,327 ( 21 ) 6,823 ( 7 ) 4,495 ( 26 ) 33,641 Sto 14,796 ( 269 ) 5,084 ( 84 ) 3,528 ( 325 ) 3,041 ( 311 ) 26,449 PH 10,413 ( 14 ) 3,949 ( 2 ) 2,469 ( 25 ) 1,693 ( 55 ) 18,524 TCM 15,235 ( 10 ) 8,045 ( 14 ) 6,044 ( 48 ) 1,541 ( 9 ) 30,865 CWM 14,051 ( 9 ) 7,336 ( 22 ) 5,370 ( 16 ) 0 26,757 Total 71,491 29,741 24,234 10,770 136,236 Table 4 : Statistics of question types in MLEC - QA , where \" Extra \" indicates number of questions with extra materials .", "entities": [[107, 108, "DatasetName", "0"]]}, {"text": "Only A1 , A2 , and B1 are used in the examination of Chinese Western Medicine .", "entities": []}, {"text": "For the Train / Dev / Test split , randomly splitting may cause data imbalance because the number of the \ufb01ve question types are various from each other ( e.g. , A1 is far more than others ) .", "entities": []}, {"text": "To ensure that the subsets have the same distribution of the question types , we split the data based on the question types , with 80 % training , 10 % development , and 10 % test .", "entities": []}, {"text": "The overall statistics of the MLEC - QA dataset are summarized in Table 5 .", "entities": []}, {"text": "We can see that the length of the questions and the vocabulary size in Clinic are larger than the rest of the subsets , explaining that clinical medicine may involve more medical subjects than other specialties .", "entities": []}, {"text": "Metric Cli Sto PH TCM CWM # of options per question 5 5 5 5 5 Avg./Max .", "entities": []}, {"text": "question len 46.51 / 332 37.12 / 341 36.76 / 352 32.24 / 340 30.63 / 280 Avg./Max .", "entities": []}, {"text": "option len 9.07 / 100 9.71 / 101 9.72 / 125 7.25 / 200 7.77 / 130 Vocabulary size 46,175 41,178 35,790 38,904 38,187 # of extra materials 73 989 96 81 47 # of questions Train 26,913 21,159 14,818 24,692 21,406 Dev 3,365 2,645 1,852 3,086 2,676 Test 3,363 2,645 1,854 3,087 2,675 Total 33,641 26,449 18,524 30,865 26,757 Table 5 : The overall statistics of MLEC - QA .", "entities": []}, {"text": "Question / option length is calculated in characters .", "entities": []}, {"text": "Vocabulary size is measured by Pkuseg ( Luo et al . , 2019 ) in words .", "entities": []}, {"text": "88663.2 Reasoning Types of the Questions Since the annual examination papers are designed by a team of healthcare experts who try to follow the similar reasoning types distribution .", "entities": []}, {"text": "To better understand our dataset , we manually inspected 10 sets of examination papers ( 2 sets for each sub\ufb01eld ) , and summarize the most frequent reasoning types of the questions from MLEC - QA and previous works ( Lai et al . , 2017 ; Zhong et al . , 2020 ) .", "entities": []}, {"text": "The examples are shown in Table 6 .", "entities": []}, {"text": "Notably , the \" Evidence \" is well - organized by us to show how models need to handle these reasoning issues to achieve promising performance in MLEC - QA .", "entities": []}, {"text": "The de\ufb01nition of reasoning types of the questions are as follows : Lexical Matching This type of question is common and the simplest .", "entities": []}, {"text": "The retrieved documents are highly matched with the question , the correct answer exactly matches a span in the document .", "entities": []}, {"text": "As shown in the example , the model only needs to check which option is matched with .", "entities": []}, {"text": "Multi - Sentence Reading Unlike lexical matching , where questions and correct answers can be found within a single sentence , multi - sentence reading requires models reading multiple sentences to gather enough information to generate answers .", "entities": []}, {"text": "Concept Summary The correct options for this type of question do not appear directly in the documents .", "entities": []}, {"text": "It requires the model to understand and summarize the question relevant concepts after reading the documents .", "entities": []}, {"text": "As shown in the example , the model needs to understand and summarize the relevant mechanism of \" Thermoregulation \" , and infer that when an obstacle arises in thermoregulation , the body temperature will not be able to maintain a relatively constant level , that is , it will rise with the increase of ambient temperature .", "entities": []}, {"text": "Numerical Calculation", "entities": []}, {"text": "This type of question involves logical reasoning and arithmetic operations related to mathematics .", "entities": []}, {"text": "As shown in the example , the model \ufb01rst needs to judge the approximate age of month according to the height of the infant , and then reverse calculate the age of months according to the height formula of infants 7~12 months old to obtain the age in months : ( 68 - 65 ) / 1.5 + 6 = 8 .", "entities": []}, {"text": "Multi - Hop Reasoning This type of question requires several steps of logical reasoning over mul - tiple documents to answer .", "entities": []}, {"text": "As shown in the example , the patient \u2019s hemoglobin ( HB ) value is low , indicating that the patient has anemia , and the supply of iron should be increased in their diet .", "entities": []}, {"text": "The model needs to compare the iron content of each option : the iron content of C , D and E is low and that of A , B is high , but B is not easily absorbed , so the best answer is A. Reasoning Type Example ( \u2217represents the correct answer )", "entities": []}, {"text": "Lexical MatchingThe main hallmark of peritonitis is : A. Signi\ufb01cant abdominal distension B. Abdominal mobility dullness C. Bowel sounds were reduced or absent D. Severe abdominal cramping \u2217E. Peritoneal irritation signs Evidence : The hallmark signs of peritonitis are peritoneal irritation signs , i.e. , tenderness , muscle tension , and rebound tenderness .", "entities": []}, {"text": "Multi - Sentence ReadingWhich is wrong in the following narrative relating to the appendix : A.", "entities": []}, {"text": "The appendiceal artery is the terminal artery B. Appendiceal tissues contain abundant lymphoid follicles C. Periumbilical pain at appendicitis onset visceral pain \u2217D. Resection of the appendix in adults will impair the body \u2019s immune function", "entities": []}, {"text": "E. There are argyrophilic cells in the deep part of the appendiceal mucosa , which are associated with carcinoid tumorigenesis Evidence : ( 1 ) The appendiceal artery is a branch of the ileocolic artery and is a terminal artery without collaterals ; ( 2 ) The appendix is a lymphoid organ[ ...", "entities": []}, {"text": "]Therefore , resection of the adult appendix does not compromise the body \u2019s immune function ; ( 3 ) The nerves of the appendix are supplied by sympathetic \ufb01bers[ ... ]belonging to visceral pain ; ( 4 ) Argyrophilic cells are found in the appendiceal mucosa and are the histological basis for the development of appendiceal carcinoids .", "entities": []}, {"text": "Concept SummaryThe main hallmark of thermoregulatory disorders in hyperthermic environments is : A. Developed syncope B. Developed shock C. Dry heat of skin \u2217D. Increased body temperature E. Decreased body temperature Evidence : The purpose of thermoregulation is to maintain body temperature in the normal range .", "entities": []}, {"text": "In hyperthermic environments , the thermoregulatory center is dysfunctional and can not maintain the body \u2019s balance of heat production and heat dissipation , so the body temperature is increased by the in\ufb02uence of ambient temperature .", "entities": []}, {"text": "Numerical CalculationA normal infant , weighing 7.5 kg and measuring 68 cm in length .", "entities": []}, {"text": "Bregma 1.0 cm , head circumference 44 cm .", "entities": []}, {"text": "Teething 4 .", "entities": []}, {"text": "Can sit alone and can pick up pellets with a hallux and fore\ufb01nger .", "entities": []}, {"text": "The most likely age of the infant is : \u2217A. 8 months B. 24 months C. 18 months D. 12 months E. 5 months Evidence : A normal infant measured 65 cm at 6 months and 75 cm at 1 year of age .", "entities": []}, {"text": "The infant \u2019s 7 to 12 month length is calculated as : length = 65 + ( months of age - 6 ) x 1.5 .", "entities": []}, {"text": "Multi - Hop Reasoning6 - month - old female infant , arti\ufb01cial feeding mainly , physical examination revealed a low hemoglobin ( HB ) value , the dietary supplement that should be mainly added is : \u2217A. Liver paste B. Egg yolk paste C. Tomato paste D. Rice paste E. Apple puree Evidence : ( 1 ) Low HB value indicates anemia tendency .", "entities": []}, {"text": "Iron de\ufb01ciency anemia is the most important and common type of anemia in China .", "entities": []}, {"text": "( 2 ) Iron supply should be increased in diet .", "entities": []}, {"text": "( 3 ) Liver paste is rich in iron .", "entities": []}, {"text": "( 4 ) The iron content of egg yolk paste is lower than that of liver paste , and it is not easy to be absorbed .", "entities": []}, {"text": "( 5 ) The iron content of tomato paste , rice paste and apple puree is lower than that of liver paste .", "entities": []}, {"text": "Table 6 : Examples of reasoning types of the questions in MLEC - QA .", "entities": []}, {"text": "The Chinese version is in Appendix E. 4 Methods Notation We represent MLEC - QA task as : ( D , Q , O , A ) , where Qirepresents the ith Question , Direpresents the collection of retrieved question relevant Documents , Oi= { OiA , OiB , OiC , OiD , OiE}are the candidate Options , Airepresents Answer , and we use A\u2032 ito denote the Predicted Answer .", "entities": []}, {"text": "88674.1 Document Retriever Both examination counseling books and Wikipedia have been used as the source of supporting materials in previous research ( Zhong et al . , 2020 ; Jin et al . , 2020 ; Vilares and G\u00f3mezRodr\u00edguez , 2019 ) .", "entities": []}, {"text": "However , because examination counseling books are designed to help examinees pass the examination , knowledge is highly simpli\ufb01ed and summarized ; even the easily confused knowledge points are compared .", "entities": []}, {"text": "Using examination counseling books as information sources may make the retriever - reader more likely to exploit shallow text matching , and complex reasoning is seldom involved .", "entities": [[18, 20, "TaskName", "text matching"]]}, {"text": "Therefore , to help better understand the improvement coming from future models , we adopt Chinese Wikipedia dumps as our information sources , which contain a wealth of information ( over 1 million articles ) of real - world facts .", "entities": []}, {"text": "Building upon the whole Chinese Wikipedia data , we use a distributed search and analytics engine , ElasticSearch , as the document store and document retriever , which supports very fast full - text searches .", "entities": []}, {"text": "The similarity scoring function used in Elasticsearch is the BM25 algorithm ( Robertson and Zaragoza , 2009 ) , which measures the relevance of documents to a given search query .", "entities": []}, {"text": "As de\ufb01ned in Appendix C , the larger this BM25 score , the stronger the relevance between document and query .", "entities": []}, {"text": "Speci\ufb01cally , for each question Qiand each candidate option Oijwhere j\u2208 { A , B , C , D , E } , we de\ufb01ne QiOij = Qi+Oijas a search query to Elasticsearch and is repeated for all options .", "entities": []}, {"text": "The document with the highest BM25 score returned by each query is selected as supporting materials for the next stage machine reading comprehension task .", "entities": [[20, 23, "TaskName", "machine reading comprehension"]]}, {"text": "4.2 Control Methods In general , each option should have the same correct rate for multi - choice questions , but in fact , the order in which the correct options appear is not completely random , and the more the number of options , the lower the degree of randomization ( Poundstone , 2014 ) .", "entities": []}, {"text": "Given the complex nature of multi - choice tasks , we employ three control methods to ensure a fair comparison among various open - domain QA models .", "entities": []}, {"text": "Random A\u2032=Random ( O ) .", "entities": []}, {"text": "For each question , an option is randomly chosen as the answerfrom \ufb01ve candidate options .", "entities": []}, {"text": "We perform this experiment \ufb01ve times and average the results as the baseline of the Random method .", "entities": []}, {"text": "Constant A\u2032=Constant j(O ) , where j\u2208 { A , B , C , D , E } .", "entities": []}, {"text": "For each question , the jthoption is always chosen as the answer to obtain the accuracy distribution of \ufb01ve candidate options .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "Mixed A\u2032=Mixed ( O ) .", "entities": []}, {"text": "Incorporating the previous experiences of NMLEC and multi - choice task work ( Vilares and G\u00f3mez - Rodr\u00edguez , 2019 ) , the Mixed method simulates how humans solving uncertain questions , and consists of the following three strategies : ( 1 ) the correct rate of choosing \" All of the options above is correct / incorrect \" is much higher than the other options .", "entities": []}, {"text": "( 2 ) Supposing the length of options is roughly equal , only one option is obviously longer with more detailed and speci\ufb01c descriptions , or is obviously shorter than the other options , then choose this option .", "entities": []}, {"text": "( 3 ) The correct option tends to appear in the middle of candidate options .", "entities": []}, {"text": "The three strategies are applied in turn .", "entities": []}, {"text": "If any strategy matches , then the option that matches the strategy is chosen as the answer .", "entities": []}, {"text": "4.3 Fine - Tuning Pre - Trained Language Models We apply an uni\ufb01ed framework UER - py ( Zhao et al . , 2019 ) to \ufb01ne - tuning pre - trained language models on the machine reading comprehension task as our reader .", "entities": [[36, 39, "TaskName", "machine reading comprehension"]]}, {"text": "We consider the following \ufb01ve pre - trained language models : Chinese BERT - Base ( denoted as BERT - Base ) and Multilingual Uncased BERT - Base ( denoted as BERT - BaseMultilingual ) ( Devlin et al . , 2019 ) , Chinese BERTBase with whole word masking and pre - trained over larger corpora ( denoted as BERT - wwm - ext ) ( Cui et al . , 2019 ) , and the robustly optimized BERTs : Chinese RoBERTa - wwm - ext and Chinese RoBERTa - wwm - ext - large ( Cui et al . , 2019 ) .", "entities": [[12, 13, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"], [31, 32, "MethodName", "BERT"], [60, 61, "MethodName", "BERT"], [82, 83, "MethodName", "RoBERTa"], [89, 90, "MethodName", "RoBERTa"]]}, {"text": "Speci\ufb01cally , given the ithquestion Qi , retrieved question relevant documents Di , and a candidate option Oij , where j\u2208 { A , B , C , D , E } .", "entities": []}, {"text": "The input sequence for the framework is constructed by concatenating [ CLS ] , tokens in Di , [ SEP ] , tokens in Qi , [ SEP ] , tokens in an option Oij , and [ SEP ] , where [ CLS ] is the classi\ufb01er token , and [ SEP ] is the sentence separator in pre - trained language models .", "entities": []}, {"text": "We pass each of the \ufb01ve options in turn , and the model outputs the hidden state representation Sij\u2208R1\u00d7Hof the input sequence ,", "entities": []}, {"text": "8868then performs the classi\ufb01cation and output an unnormalized log probability Pij\u2208Rof each optionOijbeing correct by Pij = SijWT , where W\u2208R1\u00d7His the weight matrix .", "entities": []}, {"text": "Finally , we pass the unnormalized log probabilities of each option through a softmax layer and obtain the option with the highest probability as the predicted answer A\u2032 i. 5 Experiments 5.1 Experimental Settings We conduct detailed experiments and analyses to investigate the performance of control methods and open - domain QA methods on MLEC - QA .", "entities": [[13, 14, "MethodName", "softmax"]]}, {"text": "As shown in Figure 2 , we implement a two - stage retriever - reader framework : ( 1 ) a retriever \ufb01rst retrieves question relevant documents from Chinese Wikipedia using ElasticSearch , ( 2 ) and then a reader employs machine reading comprehension models to generate answers in given documents retrieved by the retriever .", "entities": [[41, 44, "TaskName", "machine reading comprehension"]]}, {"text": "For the reader , all machine reading comprehension models are trained with 12 epochs , an initial learning rate of 2e-6 , a maximum sequence length of 512 , a batch size of 5 .", "entities": [[5, 8, "TaskName", "machine reading comprehension"], [17, 19, "HyperparameterName", "learning rate"], [30, 32, "HyperparameterName", "batch size"]]}, {"text": "The parameters are selected based on the best performance on the development set , and we keep the default values for the other hyper - parameters ( Devlin et al . , 2019 ) .", "entities": []}, {"text": "We use accuracy as the metric to evaluate different methods , and provide baseline results , as well as human pass mark ( 60 % ) instead of human performance due to the wide variations exist in human performance , from almost full marks to can not even pass the exam .", "entities": [[2, 3, "MetricName", "accuracy"]]}, {"text": "Figure 2 : Overview of the two - stage retriever - reader framework on MLEC - QA .", "entities": []}, {"text": "5.2 Retrieval Performance The main drawbacks of the Chinese Wikipedia database in biomedicine are that it is not comprehensive and thorough , that is , it may not provide complete coverage of all subjects .", "entities": []}, {"text": "To evaluate whether retrieved documents can cover enough evidence to answer questions , we sampled 5%(681 ) questions from the development sets of \ufb01ve categories using strati\ufb01ed random sampling , and manually annotate each question by \ufb01ve medical experts with 3 labels : ( 1 ) Exactly Match ( EM ): the retrieved documents exactly match the question .", "entities": [[49, 50, "MetricName", "EM"]]}, {"text": "( 2 ) Partial Match ( PM ): the retrieved documents partially match the question , can be confused with the correct options or are incomplete .", "entities": []}, {"text": "( 3 ) Mismatch ( MM ): the retrieved documents do not match the question at all .", "entities": []}, {"text": "Table 7lists the performance of the retrieval strategy as well as the results of the annotation for KQ and CQ questions on \ufb01ve subsets .", "entities": []}, {"text": "Subset EM ( KQ / CQ ) PM ( KQ / CQ ) MM ( KQ / CQ ) Cli 15.63 ( 18.75 / 12.5 ) 75 ( 68.75 / 81.25 ) 9.38 ( 12.5 / 6.25 )", "entities": [[1, 2, "MetricName", "EM"]]}, {"text": "Sto 20.83 ( 16.67 / 25 ) 54.17 ( 50 / 58.33 ) 25 ( 33.33 / 16.67 ) PH 6.25 ( 0 / 12.5 ) 43.75 ( 50 / 37.5 ) 50 ( 50 / 50 ) TCM 10.71 ( 14.29 / 7.14 ) 50 ( 42.86 / 57.14 ) 39.29 ( 42.86 / 35.71 ) CWM 20.83 ( 8.33 / 33.33 ) 54.17 ( 58.33 / 50 ) 25 ( 33.33 / 16.67 ) Table 7 : Matching rate ( % ) of retrieved documents that exactly match , partial match or mismatch with the questions in the MLEC - QA dataset .", "entities": [[22, 23, "DatasetName", "0"]]}, {"text": "From the table , we make the following observations .", "entities": []}, {"text": "First , most retrieved documents indicate PM with the questions , while the matching rates of EM and MM achieve maximums of 20.83 % ( CWM ) and 50 % ( PH ) , respectively .", "entities": [[16, 17, "MetricName", "EM"]]}, {"text": "Second , the matching rate of CQ is higher than KQ in most subsets as CQ are usually related to simpler concepts , and use more words to describe questions , which leads to easier retrieval .", "entities": []}, {"text": "By contrast , KQ usually involve more complex concepts that may not be included in the Chinese Wikipedia database .", "entities": []}, {"text": "Therefore , the mismatching rate of KQ is signi\ufb01cantly higher than that of CQ .", "entities": []}, {"text": "Third , among different subsets , the performance in the subset Cli achieves the best as clinical medicine is more \" general \" to retrieve compare with other specialties .", "entities": []}, {"text": "Whereas the performance in the subset PH achieves the worst as the Public Health is usually related to \" confusing concepts \" , which leads to poor retrieval performance .", "entities": []}, {"text": "5.3 Baseline Results Tables 8and", "entities": []}, {"text": "Figure 3show the performance of baselines as well as the performance on KQ and CQ questions .", "entities": []}, {"text": "As we can see , among control methods , the correct option has a slight tendency to appear in the middle ( C and D ) of candidate options , but the margins are small .", "entities": []}, {"text": "The performance of the Mixed method is slightly better than a random guess , which indicates that the \ufb02exible use of", "entities": []}, {"text": "8869MethodCli Sto PH TCM CWM Dev Test Dev Test Dev Test Dev Test Dev Test Random 19.73 19.61 19.41 19.43 19.70 20.13 20.17 20.11 19.69 20.16 ConstantOption [ A ] 17.12 18.05 16.82 17.01 16.09 17.80 19.05 20.60 18.68 20.56 Option [ B ] 20.30 20.93 20.95 20.53 21.49 21.36 22.52 20.70 20.70 21.20 Option [ C ] 21.66 21.23 22.31 20.95 21.65 20.23 22.29 22.35 23.39 21.27", "entities": []}, {"text": "Option [ D ] 22.53 21.38 20.64 22.76 22.68 21.84 19.80 20.67 21.00 19.51 Option [ E ] 18.40 18.41 19.28 18.75 18.09 18.77 16.33 15.68 16.22 17.46 Mixed 24.40 24.68 24.46 24.42 23.97 23.68 22.07 22.38 23.62 23.25 BERT - Base 47.26 48.30 40.53 40.08 38.99 37.40 48.51 49.14 44.32 45.14 BERT - wwm - ext 50.27 50.89 43.26 42.05 41.75 40.04 54.57 54.94 49.89 50.04 BERT - Base - Multilingual 46.61 47.68 39.85 38.76 36.61 36.70 45.50 46.61 42.26 42.86 RoBERTa - wwm - ext 49.94 51.97 41.14 40.88 38.40 38.91 50.45 49.82 47.38 46.00 RoBERTa - wwm - ext - large 53.25 53.22 44.92 43.75 39.10 38.75 47.99 48.65 50.49 50.11 Human Pass Mark 60 Table 8 : Performance of baselines in accuracy ( % ) on the MLEC - QA dataset .", "entities": [[39, 40, "MethodName", "BERT"], [52, 53, "MethodName", "BERT"], [67, 68, "MethodName", "BERT"], [82, 83, "MethodName", "RoBERTa"], [97, 98, "MethodName", "RoBERTa"], [125, 129, "MetricName", "accuracy ( % )"]]}, {"text": "guessing skills may add wings to the tiger as humans can exclude some certain wrong options , but if the cart before the horse is reversed , it is impossible to pass the exam only through opportunistic guessing .", "entities": []}, {"text": "RoBERTa - wwm - ext - large and BERTwwm - ext perform better than other models on \ufb01ve subsets .", "entities": [[0, 1, "MethodName", "RoBERTa"]]}, {"text": "However , even the best - performing model can only achieve accuracies between 40 % to 55 % on \ufb01ve subsets , so there is still a gap to pass the exams .", "entities": []}, {"text": "Figure 3 : Performance in accuracy ( % ) on KQ ( solid lines ) and CQ ( dashed lines ) questions .", "entities": [[5, 9, "MetricName", "accuracy ( % )"]]}, {"text": "Comparing the performance between KQ and CQ questions , most models achieve better performance on CQ , which is positively correlated with CQ \u2019s better retrieval performance .", "entities": []}, {"text": "Among different subsets , the subset TCM is the easiest ( 54.95 % ) one to answer across the board , while the subset PH is the hardest ( 40.04 % ) , which does not totally correspond to their retrieval performance asshown in Table 7 .", "entities": []}, {"text": "The possible reason is that the diagnosis and treatment of diseases in traditional Chinese medicine are characterized by \" Homotherapy for Heteropathy \" , that is , treating different diseases with the same method , which may result in some patterns or mechanisms that can be used by the models to reach such results .", "entities": []}, {"text": "5.4 Comparative Analysis Given that we use the Chinese Wikipedia database as our information sources and apply a two - stage retriever - reader framework , the reason for such poor baseline performance could come from both our information sources and the retriever - reader framework .", "entities": []}, {"text": "Information Sources Both books and Wikipedia have been used as the information sources in previous research .", "entities": []}, {"text": "One of our subsets , Clinic , has been studied by MEDQA ( Jin et al . , 2020 ) as a subset ( MCMLE ) for cross - lingual research .", "entities": []}, {"text": "MEDQA uses 33 medical textbooks as their information sources and the evaluation result shows that their collected text materials can provide enough information to answer all the questions in MCMLE .", "entities": []}, {"text": "We compare the best model ( RoBERTa - wwm - ext - large ) performance on both datasets as shown in Table 9 .", "entities": [[6, 7, "MethodName", "RoBERTa"]]}, {"text": "Notably , questions in MCMLE have four candidate options due to one of the wrong options being deleted .", "entities": []}, {"text": "Therefore , the random accuracy on MCMLE is higher than ours .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "From the results we can see that even with 100 % covered materials , the best model can only", "entities": []}, {"text": "8870DatasetAccuracy ( % ) Dev Test MEDQA ( MCMLE ) 69.30 70.10 MLEC - QA ( Clinic ) 53.25 53.22 Table 9 : Comparison of best model ( RoBERTa - wwmext - large ) performance on MEDQA and our MLECQA dataset .", "entities": [[28, 29, "MethodName", "RoBERTa"]]}, {"text": "achieve 16.88 % higher accuracy on the test set than ours , which indicates that using Wikipedia as information sources is not that terrible compared with medical books , and the main reason for baseline performance may come from machine reading comprehension models that lack sophisticated reasoning ability .", "entities": [[4, 5, "MetricName", "accuracy"], [39, 42, "TaskName", "machine reading comprehension"]]}, {"text": "Retriever - Reader We also perform an experiment that sampled 5 % ( 92 ) questions from the development set of Public Health , and manually annotate each question by a medical expert to determine whether that can exactly or partially match with the top K retrieved documents , as shown in Table 10 .", "entities": []}, {"text": "Notably , the actual number of retrieved documents is 5\u00d7Kas we de\ufb01ne QiOij = Qi+Oij as a search query and is repeated for all options .", "entities": []}, {"text": "From the results , we can see that more documents even bring more noise instead , as the best match documents have already been fetched in the top 1 documents .", "entities": []}, {"text": "It indicates that the poor performance of machine reading comprehension models is coming from the insuf\ufb01ciency of reasoning ability rather than the number of retrieved documents .", "entities": [[7, 10, "TaskName", "machine reading comprehension"]]}, {"text": "Top K 1 2 3 4 5 Match 52.08 42.19 36.25 32.29 29.46 Table 10 : Matching rate ( % ) of Top K retrieved documents that exactly or partial match with the questions in the Public Health subset .", "entities": []}, {"text": "6 Conclusion We present the largest - scale Chinese multi - choice BQA dataset , MLEC - QA , which contains \ufb01ve biomedical sub\ufb01elds with extra materials ( images or tables ) annotated by human experts : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Chinese Western Medicine .", "entities": []}, {"text": "Such questions correspond to examinations ( NMLEC ) to access the quali\ufb01cations of medical practitioners in the Chinese healthcare system , and requirespecialized domain knowledge and multiple reasoning abilities to be answered .", "entities": []}, {"text": "We implement eight representative control methods and opendomain QA methods by a two - stage retrieverreader framework as baselines .", "entities": []}, {"text": "The experimental results demonstrate that even the current best approaches can not achieve good performance on MLEC - QA .", "entities": []}, {"text": "We hope MLEC - QA can bene\ufb01t researchers on improving the open - domain QA models , and also make advances for BQA systems .", "entities": []}, {"text": "Acknowledgements We thank the anonymous reviewers for their insightful comments and suggestions .", "entities": []}, {"text": "This work is supported by the National Natural Science Foundation of China ( NSFC No . 61972187 ) .", "entities": []}, {"text": "References Asma Ben Abacha , Eugene Agichtein , Yuval Pinter , and Dina Demner - Fushman . 2017 .", "entities": []}, {"text": "Overview of the medical question answering task at TREC 2017 LiveQA .", "entities": [[4, 6, "TaskName", "question answering"], [8, 9, "DatasetName", "TREC"], [10, 11, "DatasetName", "LiveQA"]]}, {"text": "In Proceedings of the Twenty - Sixth Text REtrieval Conference , TREC 2017 , Gaithersburg , Maryland , USA , November 15 - 17 , 2017 , volume 500 - 324 of NIST Special Publication .", "entities": [[11, 12, "DatasetName", "TREC"]]}, {"text": "National Institute of Standards and Technology ( NIST ) .", "entities": []}, {"text": "Asma Ben Abacha , Sadid A. Hasan , Vivek V. Datla , Joey Liu , Dina Demner - Fushman , and Henning M\u00fcller . 2019a .", "entities": []}, {"text": "VQA - Med : Overview of the medical visual question answering task at ImageCLEF 2019 .", "entities": [[0, 1, "TaskName", "VQA"], [8, 11, "DatasetName", "visual question answering"]]}, {"text": "In Working Notes of CLEF 2019 - Conference and Labs of the Evaluation Forum , Lugano , Switzerland , September 9 - 12 , 2019 , volume 2380 of CEUR Workshop Proceedings .", "entities": []}, {"text": "CEUR-WS.org .", "entities": []}, {"text": "Asma Ben Abacha , Yassine Mrabet , Mark Sharp , Travis R. Goodwin , Sonya E. Shooshan , and Dina Demner - Fushman .", "entities": []}, {"text": "2019b .", "entities": []}, {"text": "Bridging the gap between consumers \u2019 medication questions and trusted answers .", "entities": []}, {"text": "InMEDINFO 2019 : Health and Wellbeing e - Networks for All - Proceedings of the 17th World Congress on Medical and Health Informatics , Lyon , France , 25 - 30 August 2019 , volume 264 of Studies in Health Technology and Informatics , pages 25\u201329 .", "entities": []}, {"text": "IOS Press .", "entities": []}, {"text": "Yonggang Cao , Feifan Liu , Pippa Simpson , Lamont D. Antieau , Andrew S. Bennett , James J. Cimino , John W. Ely , and Hong Yu . 2011 .", "entities": []}, {"text": "AskHERMES : An online question answering system for complex clinical questions .J. Biomed .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "Informatics , 44(2):277 \u2013 288 .", "entities": []}, {"text": "Danqi Chen , Adam Fisch , Jason Weston , and Antoine Bordes . 2017 .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "Reading Wikipedia to answer opendomain questions .", "entities": []}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational", "entities": []}, {"text": "8871Linguistics ( Volume 1 : Long Papers ) , pages 1870 \u2013 1879 , Vancouver , Canada . Association for Computational Linguistics .", "entities": []}, {"text": "Yiming Cui , W. Che , T. Liu , B. Qin , Ziqing Yang , S. Wang , and G. Hu . 2019 .", "entities": []}, {"text": "Pre - Training with Whole Word Masking for Chinese BERT .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "ArXiv .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Junqing He , Mingming Fu , and Manshu Tu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Applying deep matching networks to Chinese medical question answering : A study and a dataset .", "entities": [[7, 9, "TaskName", "question answering"]]}, {"text": "BMC Medical Informatics and Decision Making , 19(2):52 .", "entities": [[4, 6, "TaskName", "Decision Making"]]}, {"text": "Xuehai He , Yichen Zhang , Luntian Mou , Eric P. Xing , and Pengtao Xie . 2020a .", "entities": []}, {"text": "PathVQA : 30000 + questions for medical visual question answering .CoRR , abs/2003.10286 .", "entities": [[0, 1, "DatasetName", "PathVQA"], [7, 10, "DatasetName", "visual question answering"]]}, {"text": "Yun He , Ziwei Zhu , Yin Zhang , Qin Chen , and James Caverlee .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Infusing Disease Knowledge into BERT for Health Question Answering , Medical Inference and Disease Name Recognition .", "entities": [[4, 5, "MethodName", "BERT"], [7, 9, "TaskName", "Question Answering"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 4604\u20134614 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Di Jin , Eileen Pan , Nassim Oufattole , Wei - Hung Weng , Hanyi Fang , and Peter Szolovits . 2020 .", "entities": []}, {"text": "What Disease does this Patient Have ?", "entities": []}, {"text": "A Large - scale Open Domain Question Answering Dataset from Medical Exams", "entities": [[6, 8, "TaskName", "Question Answering"]]}, {"text": ".arXiv:2009.13081", "entities": []}, {"text": "[ cs ] .", "entities": []}, {"text": "Qiao Jin , Bhuwan Dhingra , Zhengping Liu , William Cohen , and Xinghua Lu . 2019 .", "entities": []}, {"text": "PubMedQA :", "entities": [[0, 1, "DatasetName", "PubMedQA"]]}, {"text": "A dataset for biomedical research question answering .", "entities": [[5, 7, "TaskName", "question answering"]]}, {"text": "InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2567 \u2013 2577 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Qiao Jin , Zheng Yuan , Guangzhi Xiong , Qianlan Yu , Chuanqi Tan , Mosha Chen , Songfang Huang , Xiaozhong Liu , and Sheng Yu . 2021 .", "entities": []}, {"text": "Biomedical Question Answering :", "entities": [[1, 3, "TaskName", "Question Answering"]]}, {"text": "A Comprehensive Review .", "entities": []}, {"text": "arXiv:2102.05281", "entities": []}, {"text": "[ cs ] .", "entities": []}, {"text": "Guokun Lai , Qizhe Xie , Hanxiao Liu , Yiming Yang , and Eduard Hovy . 2017 .", "entities": []}, {"text": "RACE : Large - scale ReAding comprehension dataset from examinations .", "entities": [[0, 1, "DatasetName", "RACE"], [5, 7, "TaskName", "ReAding comprehension"]]}, {"text": "InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages785\u2013794 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Jason J. Lau , Soumya Gayen , Asma Ben Abacha , and Dina Demner - Fushman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A dataset of clinically generated visual questions and answers about radiology images .Scienti\ufb01c", "entities": []}, {"text": "Data , 5(1):180251 .", "entities": []}, {"text": "Dongfang Li , Baotian Hu , Qingcai Chen , Weihua Peng , and Anqi Wang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Towards medical machine reading comprehension with structural knowledge and plain text .", "entities": [[2, 5, "TaskName", "machine reading comprehension"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1427\u20131438 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ruixuan Luo , Jingjing Xu , Yi Zhang , Xuancheng Ren , and Xu Sun . 2019 .", "entities": []}, {"text": "PKUSEG : A toolkit for multi - domain chinese word segmentation .CoRR , abs/1906.11455 .", "entities": [[8, 11, "TaskName", "chinese word segmentation"]]}, {"text": "Yun Niu , Graeme Hirst , Gregory McArthur , and Patricia Rodriguez - Gianolli .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Answering clinical questions with role identi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the ACL 2003 Workshop on Natural Language Processing in Biomedicine , pages 73\u201380 , Sapporo , Japan . Association for Computational Linguistics .", "entities": []}, {"text": "Anusri Pampari , Preethi Raghavan , Jennifer Liang , and Jian Peng .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "emrQA :", "entities": [[0, 1, "DatasetName", "emrQA"]]}, {"text": "A large corpus for question answering on electronic medical records .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2357\u20132368 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "William Poundstone .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Rock Breaks Scissors .", "entities": []}, {"text": "Little Brown & amp ; Co. Stephen Robertson and Hugo Zaragoza .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "The Probabilistic Relevance Framework : BM25 and Beyond .Foundations and Trends in Information Retrieval , 3(4):333\u2013389 .", "entities": [[12, 14, "TaskName", "Information Retrieval"]]}, {"text": "Rafael M. Terol , Patricio Mart\u00ednez - Barco , and Manuel Palomar .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "A knowledge based method for the medical question answering problem .Comput .", "entities": [[7, 9, "TaskName", "question answering"]]}, {"text": "Biol .", "entities": []}, {"text": "Medicine , 37(10):1511\u20131521 .", "entities": []}, {"text": "Yuanhe Tian , Weicheng Ma , Fei Xia , and Yan Song .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "ChiMed : A Chinese medical corpus for question answering .", "entities": [[7, 9, "TaskName", "question answering"]]}, {"text": "InProceedings of the 18th BioNLP Workshop and Shared Task , pages 250\u2013260 , Florence , Italy .", "entities": [[13, 14, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "George Tsatsaronis , Georgios Balikas , Prodromos Malakasiotis , Ioannis Partalas , Matthias Zschunke , Michael R. Alvers , Dirk Weissenborn , Anastasia Krithara , Sergios Petridis , Dimitris Polychronopoulos , Yannis Almirantis , John Pavlopoulos , Nicolas Baskiotis , Patrick Gallinari , Thierry Arti\u00e8res , Axel - Cyrille Ngonga Ngomo , Norman Heino , \u00c9ric Gaussier , Liliana Barrio - Alvers , Michael Schroeder , Ion Androutsopoulos , and Georgios Paliouras . 2015 .", "entities": []}, {"text": "8872An overview of the BIOASQ large - scale biomedical semantic indexing and question answering competition .", "entities": [[4, 5, "DatasetName", "BIOASQ"], [12, 14, "TaskName", "question answering"]]}, {"text": "BMC Bioinform . , 16:138:1\u2013138:28 .", "entities": []}, {"text": "David Vilares and Carlos G\u00f3mez - Rodr\u00edguez .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "HEAD - QA : A healthcare dataset for complex reasoning .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 960\u2013966 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Ellen M. Voorhees and Dawn M. Tice .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "The TREC-8 question answering track .", "entities": [[2, 4, "TaskName", "question answering"]]}, {"text": "In Proceedings of the Second International Conference on Language Resources and Evaluation ( LREC\u201900 ) , Athens , Greece .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Weiming Wang , Dawei Hu , Min Feng , and Liu Wenyin .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Automatic clinical question answering based on UMLS relations .", "entities": [[2, 4, "TaskName", "question answering"], [6, 7, "DatasetName", "UMLS"]]}, {"text": "In Third International Conference on Semantics , Knowledge and Grid , Xian , Shan Xi , China , October 29 - 31 , 2007 , pages 495 \u2013 498 . IEEE Computer Society .", "entities": []}, {"text": "Hong Yu , Minsuk Lee , David R. Kaufman , John W. Ely , Jerome A. Osheroff , George Hripcsak , and James J. Cimino .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Development , implementation , and a cognitive evaluation of a de\ufb01nitional question answering system for physicians .J. Biomed .", "entities": [[11, 13, "TaskName", "question answering"]]}, {"text": "Informatics , 40(3):236\u2013251 . S. Zhang , X. Zhang , H. Wang , L. Guo , and S. Liu . 2018a .", "entities": []}, {"text": "Multi - scale attentive interaction networks for chinese medical question answer selection .", "entities": [[10, 12, "TaskName", "answer selection"]]}, {"text": "IEEE Access , 6:74061\u201374071 .", "entities": []}, {"text": "Sheng Zhang , Xin Zhang , Hui Wang , Jiajun Cheng , Pei Li , and Zhaoyun Ding .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Chinese medical question answer matching using end - to - end characterlevel multi - scale CNNs .Applied", "entities": []}, {"text": "Sciences , 7(8):767 .", "entities": []}, {"text": "Xiao Zhang , Ji Wu , Zhiyang He , Xien Liu , and Ying Su . 2018b .", "entities": []}, {"text": "Medical exam question answering with large - scale reading comprehension .", "entities": [[2, 4, "TaskName", "question answering"], [8, 10, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of the Thirty - Second AAAI Conference on Arti\ufb01cial Intelligence , ( AAAI-18 ) , the 30th innovative Applications of Arti\ufb01cial Intelligence ( IAAI-18 ) , and the 8th AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence ( EAAI-18 ) , New Orleans , Louisiana , USA , February 2 - 7 , 2018 , pages 5706 \u2013 5713 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Zhe Zhao , Hui Chen , Jinbin Zhang , Xin Zhao , Tao Liu , Wei Lu , Xi Chen , Haotang Deng , Qi Ju , and Xiaoyong Du . 2019 .", "entities": []}, {"text": "UER : An open - source toolkit for pretraining models .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ): System Demonstrations , pages 241\u2013246 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Haoxi Zhong , Chaojun Xiao , Cunchao Tu , Tianyang Zhang , Zhiyuan Liu , and Maosong Sun . 2020 .", "entities": []}, {"text": "JECQA :", "entities": []}, {"text": "A Legal - Domain Question Answering Dataset .", "entities": [[4, 6, "TaskName", "Question Answering"]]}, {"text": "InThe Thirty - Fourth AAAI Conference on Arti\ufb01cial Intelligence , AAAI 2020 , The Thirty - Second Innovative Applications of Arti\ufb01cial Intelligence Conference , IAAI 2020 , The Tenth AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence , EAAI 2020 , New York , NY , USA , February 7 - 12 , 2020 .", "entities": []}, {"text": ", pages 9701\u20139708 .", "entities": []}, {"text": "Pierre Zweigenbaum . 2003 .", "entities": []}, {"text": "Question answering in biomedicine .", "entities": [[0, 2, "TaskName", "Question answering"]]}, {"text": "In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics .", "entities": []}, {"text": "8873A Source of Data Collection For \ufb01ve subsets in MLEC - QA , we collect 2006 to 2020 Sprint Paper for the National Medical Licensing Examination - Tianjin Science and Technology Press in PDF format , and then converted them into digital format via Optical Character Recognition ( OCR ) .", "entities": [[44, 47, "TaskName", "Optical Character Recognition"]]}, {"text": "We manually checked and corrected the OCR results with con\ufb01dence less than 0.99 to ensure the quality of our dataset .", "entities": []}, {"text": "We also scraped practice exercises from offcn ( http://www.offcn.com/yixue/yszg/ ) , which are freely accessible online for public usage .", "entities": []}, {"text": "B Data Structure The data structure below describe the JSON \ufb01le representation in MLEC - QA .", "entities": []}, {"text": "{ \" qid\":The question ID , \" qtype\":[\"A1 \u0f98\u0e35 \" , \" B1\u0f98\u0e35 \" , \" A2\u0f98\u0e35 \" , \" A3 / A4 \u0f98\u0e35 \" ] , \" qtext\":Description of the question , \" qimage\":Image or table path ( if any ) , \" options \" : { \" A\":Description of the option A , \" B\":Description of the option B , \" C\":Description of the option C , \" D\":Description of the option D , \" E\":Description of the option E } , \" answer\":[\"A \" , \" B \" , \" C \" , \" D \" , \" E \" ] } C BM25 Score Function The BM25 algorithm is de\ufb01ned as : s(D , Q )", "entities": [[107, 108, "MetricName", "Score"]]}, {"text": "= n\u2211 i=1IDF ( qi)\u00b7f(qi , D)\u00b7(k1 + 1 ) f(qi , D ) + k1 \u00b7 ( 1\u2212b+b\u00b7|D| avgdl),(1 ) where qiis the ithquery term of a query Q , f(qi , D)isqi \u2019s term frequency in the document D,|D|is the length of the document Din words , andavgdl is the average document length in the text collection from which documents are drawn .", "entities": []}, {"text": "bdetermines the effects of the length of the document on the average length .", "entities": []}, {"text": "k1is a variable which helps determine term frequency saturation characteristics .", "entities": []}, {"text": "By default , b , k1has a value of 0.75 , 1.2 in Elasticsearch , respectively .", "entities": []}, {"text": "IDF ( qi)is the Inverse Document Frequency ( IDF ) weight of the query term qi .", "entities": []}, {"text": "It is usually computed as : IDF ( qi ) = ln ( N\u2212n(qi )", "entities": []}, {"text": "+ 0 .5 n(qi ) + 0", "entities": [[1, 2, "DatasetName", "0"], [6, 7, "DatasetName", "0"]]}, {"text": ".5 + 1 ) , ( 2)where Nis the total number of documents in the collection , and n(qi)is the number of documents containing qi .", "entities": []}, {"text": "D Chinese version of examples of sub-\ufb01elds and question types Question Type Example ( * represents the correct answer ) A1\u010d\u070b\u0704\u0ecf\u0d33\u010e \u111d\u0431\u08a0\u0a06\u1022\u0427\u0829\u05a9\u1227\u0a18\u0754\u08b2\u10b5\u0eed\u04b5\u0459\u0d48\u0111\u1052\u10a8\u010d \u010e A. t\u085f\u1012 B.x2\u085f\u1012 C.\u00b5\u085f\u1012 D. F\u085f\u1012 \u2217E.\u11c7\u085f\u0784\u1012 B1\u010d\u11cf\u0f06\u1044\u0786\u08b2\u010e A.\u0f8fa\u0bc1 B.\u06c9a\u066b", "entities": []}, {"text": "C.\u0bc1a\u0d2f", "entities": []}, {"text": "D.\u0f8fa\u0d2f E.\u06c9a\u0d2f 1.\u1059\u0748\u0e5d\u10f7\u05a5\u1059\u0748\u0dee\u11b7\u05a5\u1123\u0d5e\u010d E\u010e 2.\u0da3\u05a5\u0836\u083b\u0805\u0da3\u0805\u0dee\u11b7\u05a5\u1123\u0d5e\u010d D\u010e A2\u010d\u0a22\u0535\u010e \u051a\u1113\u06b9\u0111 29\u0ddfb\u0cac\u0d28 37\u11db\u0111\u08c2\u04e4\u0ba8\u0d1d\u0f9f\u0b7c\u0e73 1\u0551\u01111\u0f6c\u0d48\u0c2d\u0e6c\u0c96\u050e\u0527 \u0469\u0ddb\u07fd\u0827\u0ad9\u0cc6\u10fdb\u04b0\u0e38 : BP 180/120mmHg \u0111\u0b55\u0591\u03e2 ( + + + ) b\u07d1\u06be\u1180\u124b\u0956 \u0b3f\u05a5\u1190 \u060e \u0d5e\u010d \u010e \u2217A.\u1230 \u1b75 B.\u0b37\u051b\u0fd3 C.\u1b93\u11a1 D.\u1b96 \u1b75 E.\u0b37\u0fd3\u0d9d\u0f99\u04ee A3\u010d\u11cf\u1044\u010e \u0b6f\u0f9f\u0111 28\u0ddfb\u08cd\u1042\u1105\u105b\u099f\u0111\u0967\u0c38\u098e\u101a\u063f\u06af\u0651\u1254\u0111\u0f8f\u0650\u0111\u1040\u0e9e\u0b34\u105b\u0cc6\u0da4\u0111 \u0576\u044c\u06c4\u0111 1j2\u0cb0\u1042\u0f9b\u0111\u0967\u06c4 \u0482 \u0f1f \u1082\u0111\u0f6c\u044c\u07db\u0111\u0d19\u11c9\u07a3\u0111\u0df9\u0b48\u0111\u0a9d\u0d94b 1.\u11cd\u0a0f\u054e\u11a3\u1052\u0d6e \u0fca \u05a5\u1030\u0efe\u0d5e\u010d \u010e A.\u0aa7\u0f62 B.\u0b64\u147e\u1230 \u2217C.\u11f0\u103d D.\u07db\u13f9 E.\u0d46\u06db 2.\u11cd\u0a0f\u054e\u11a3\u0956\u105b\u0d61\u0592\u0ba5\u0ef8\u05a5\u1030\u0efe\u0d5e\u010d \u010e A.\u03e2\u13f4 B.\u0749\u11a5 C.\u087b\u06c4 \u2217D.\u0576\u07db E.\u0c55\u0bc3 3.\u07d1\u1180\u1105\u08dc\u0bf9\u0857\u0111\u0f2f\u0a19\u0785\u1030\u1052\u0d30\u10a8\u010d \u010e A. \u058e \u05f8\u1cf7 \u2217B.\u0576\u07db C.\u07db\u09f5", "entities": []}, {"text": "D. \u058e \u11f0\u103d E.\u0a52\u06f4 A4\u010d\u0967\u0c38\u010e \u07d1\u1180 50\u0ddf\u0111\u1079\u0430\u0ff3\u03b4\u0e93\u10fd\u0662\u0a0f\u08b2\u0d8f\u07aa 2\u0b4d\u0111\u099f\u0aca\u1190\u08fc\u1190b\u085f\u04b0\u011f\u0c86 \u0967\u06f1\u061f\u0fe9\u0502\u0fe9\u0aeb \u0482 \u0e5d\u04f1\u0607\u0c7e\u0111\u0486\u07d1\u0673\u0fe9\u1058\u04ee\u0497\u0733\u0111\u0497\u06f4\u0111\u0967\u0b3d\u0e8a\u1041\u08a0 \u0d12\u0111\u0fe9\u0aeb\u0823\u1d72\u070e\u0cc8\u061f\u0711b 1.\u07d1\u06be\u1180\u05a5\u1190 \u060e \u0eb9\u010d \u010e A.\u0824\u0f9f\u0c7e \u2217B.\u0ad2\u0f9f\u0c7e", "entities": []}, {"text": "C.\u0aa4\u0f9f\u0c7e D.\u063f\u083f\u0c7e E.\u0bdc\u07c1\u0aeb\u0c7e 2.\u11cd\u0a0f\u0d21\u0839\u10b5\u105b\u0f2f\u082b\u0f5b\u0111\u0522\u0a14\u010d \u010e", "entities": []}, {"text": "A.\u0d21\u0839\u11cd\u0a0f\u0c86\u0967\u0c7e\u0502 B.\u0fe9\u0ddd\u0468\u044d\u05a5\u0fe9\u0502\u0fe9\u0ddd\u11cd\u0a0f \u2217C.\u0509\u0e41\u08c1\u0d8b\u048b\u0a18 D.\u111c\u0980\u06a3\u07c4\u11f9\u11cd\u0a0f E.\u05e7\u0bf9\u06af\u04b0 Table 11 : Chinese version of Table 2 .", "entities": []}, {"text": "E Chinese version of examples of reasoning types Reasoning Type Example ( \u2217represents the correct answer )", "entities": []}, {"text": "Lexical Matching\u06b4\u0b07\u1000\u05a5\u11f6\u1031\u0453\u11bd\u0d5e\u010d \u010e A.\u0afc\u0f41\u06b4\u05a5\u116e B.\u06b4\u0486\u104d\u05ee\u0f9f\u1224\u107b C.\u04cb\u0afe\u107b\u0f68\u0d3e\u0868\u0807\u0cd0 D.\u0916\u0a1b\u06b4\u05a5\u0486\u089b\u0e65 \u2217E.\u06b4\u0b07\u054f\u0817\u1198 Evidence : \u06b4\u0b07\u1000\u05a5\u0453\u11bd\u0f9f\u0e38\u1198\u0d5e\u06b4\u0b07\u054f\u0817\u1198\u0111\u0827\u0fe2\u0e65a\u08c5\u0814\u1166\u0651\u0784\u0e4b\u0e65b Multi - Sentence Reading\u0f2f\u0a19\u10d0\u09a6\u0ec1\u0f4d\u05a5\u0731\u0fbb\u0d8d\u0111\u0570 \u0f02 \u05a5\u0d5e\u010d \u010e A.\u09a6\u0ec1\u05ee\u0a9d\u0d5e\u11d4\u0b0c\u05ee\u0a9d B.\u09a6\u0ec1\u1246\u11ae\u11cf\u0763\u10b5\u05a5\u06b6\u067e\u0a25\u03d8\u0a72\u0b9e C.\u09a6\u0ec1 \u1000\u063f\u0468\u0d48\u05a5\u0c09\u11db\u0e65\u0d8b\u0b3d\u1123\u0f9f\u0e2e\u0e65 \u2217D.\u04ee\u0ca6\u0c4d\u0522\u09a6\u0ec1\u087c\u0de5\u080f\u075d\u0e38\u05a5\u0ae7\u1066 \u06ff\u0b3f E.\u09a6\u0ec1\u1b2a\u0b07\u0d27\u0486\u10b5\u0d5f\u107f\u0f25\u0406\u0111\u10d0\u09cb\u03b4\u063f\u0d33\u10b5\u0731 Evidence : ( 1)\u09a6\u0ec1\u05ee\u0a9d\u0d5e\u08b2\u07ed\u04cb\u05ee\u0a9d\u0673\u05a5\u11a6\u0111\u0d5e\u0eed\u04a7\u11a6\u05a5\u11d4\u0b0c\u05ee\u0a9d\u0120 ( 2)\u09a6\u0ec1\u0d5e\u1042 \u06f1\u0a25\u03d8\u0c16\u0732 [ ... ] \u0723\u0c4d\u0522\u04ee\u0ca6\u09a6\u0ec1\u0eed\u0de5\u10bf\u080f\u0e38\u05a5\u0ae7\u1066\u06ff\u0b3f\u0120 ( 3)\u09a6\u0ec1\u05a5\u0d2a \u08dc\u10ae\u06cb\u088c\u0d2a\u08dc \u0f38 \u0ebb", "entities": []}, {"text": "[ ... ] \u0d8b\u0b3d\u1123\u0f9f\u0e2e\u0e65\u0120 ( 4)\u09a6\u0ec1\u1b2a\u0b07\u0486\u10b5\u0d5f\u107f\u0f25\u0406\u0111\u0d5e\u063f \u0d33\u09a6\u0ec1\u09cb\u03b4\u05a5\u1246\u11ae\u0fd0\u080e\u0524b Concept Summary\u06da\u0ed1\u08e2\u07cc\u11cf\u0e38\u0ed1\u08ab\u05d8\u1170\u03b8\u05a5\u11f6\u1031\u0453\u11bd\u0d5e\u010d \u010e A.\u051b\u0f43\u1111\u1280 B.\u051b\u0f43\u0fa8\u0958 C.\u0bc3\u06c4\u0691\u0ca3 \u2217D.\u0e38\u0ed1\u0d36\u06da E.\u0e38\u0ed1\u05ae\u0886", "entities": []}, {"text": "Evidence : \u0e38\u0ed1\u05a5\u08ab\u05d8\u0b22\u05a5\u111d\u10bf\u0ebb\u04fb\u0e38\u0ed1\u111d\u119e\u04c8\u0653\u0eb6b\u06da\u0ed1\u08e2\u07cc\u11cf\u0111\u0e38\u0ed1\u08ab\u05d8\u11cf\u0d77 \u06ff\u0b3f\u1170\u03b8\u0111 \u0482 \u0b3f\u0ebb\u04fb\u080f\u0e38\u05a5\u04c1\u0ca3\u0784\u0cdb\u0ca3\u0bdc\u0799\u0111\u0e38\u0ed1\u0d73\u08e2\u07cc\u0ed1\u0607\u1095\u0f59\u0637\u0d36 \u06dab Numerical Calculation\u119e\u04c8\u1089\u0638\u0111\u0e38\u11d7", "entities": []}, {"text": "7.5kg\u0111\u0d26\u04c9 68cmb\u0c2d\u126c 1.0cm\u0111\u0e68\u0eb6 44cmb\u051b\u0fe9 4\u06f1b \u0b3f\u05ff\u1255\u0469\u0b3f\u105b\u0b17a\u0d4a\u11b7\u0b26\u0c7c\u0f6c\u0c6fb\u0638\u06be\u124b\u0956\u0b3f\u05a5\u1105\u0a2d\u0d5e\u010d \u010e \u2217A. 8\u06f1\u1105", "entities": []}, {"text": "B. 24\u06f1\u1105 C. 18\u06f1\u1105 D. 12\u06f1\u1105", "entities": []}, {"text": "E. 5\u06f1\u1105 Evidence : \u119e\u04c8\u1089\u0638 6\u06f1\u1105\u0d48\u0d26\u04c9 65cm\u01111\u0ddf\u0d48\u0d26\u04c9", "entities": []}, {"text": "75cmb\u1089\u0638 7\u223c12\u06f1\u1105\u0d26\u04c9 \u0839\u0dd8\u0704\u0d54\u0eb9\u011f\u0d26\u04c9 = 65 + ( \u1105\u0a2d - 6 ) x 1.5b Multi - Hop Reasoning6\u06f1\u1105\u0b6f\u1089\u0111\u0ca6 \u06fd \u0ec8\u1021\u0eb9\u11f6\u0111\u0e38\u085f\u06ec\u04b0\u063f\u0f43\u0fd3\u0591\u07a3\u03e2\u010d Hb\u010e\u11b4\u0bca\u05ae\u0111\u108b \u0e40\u06a3\u05a5\u0846\u0d4a\u11f6\u1031\u0d5e\u010d \u010e \u2217A.\u06c9\u0b43 B.\u07db\u0591\u0b43 C.\u0f06\u07a3\u0d58\u0b43 D.\u0adc\u0678 E.\u0bda\u0754\u0b43 Evidence : ( 1)Hb\u11b4\u0bca\u05ae\u0456\u0afc\u10b5\u0bd5\u0fd3\u0c60\u0f5f\u0111\u0c8c\u0e4d\u0f9f\u0bd5\u0fd3\u0d5e\u0ee1\u0753\u124b\u11f6\u1031a\u124b\u04c8\u05a5\u086e\u0bd5 \u0fd3\u09cb\u0f98b ( 2)\u1082\u0d4a\u108b\u1139\u0846\u0e4d\u06f3\u0702\u05a5b ( 3)\u06c9\u0b43\u0763\u10b5\u05a5\u06b6\u067e\u0e4db ( 4)\u07db\u0591\u0b43\u0e4d \u0763\u0a08\u0431\u06c9\u0b43\u0e4d\u0763\u0a08\u05ae\u0111\u0c4f \u0482 \u105e\u0f0b\u0d6cb ( 5)\u0f06\u07a3\u0d58\u0b43a\u0adc\u0784\u0678\u0bda\u0754\u0b43\u05a5\u0e4d \u0763\u0a08\u0928 \u0482 \u0cc2\u06c9\u0b43\u06dab Table 12 : Chinese version of Table 6 .", "entities": []}, {"text": "8874F Data Statement F.1 CURATION RATIONALE", "entities": []}, {"text": "In order to bene\ufb01t researchers on improving the open - domain QA models , and also make advances forBiomedical Question Answering ( BQA ) systems , we present MLEC - QA , the largest - scale Chinese multi - choice BQA dataset to date .", "entities": [[19, 21, "TaskName", "Question Answering"]]}, {"text": "F.2 LANGUAGE VARIETY", "entities": []}, {"text": "The data is represented in simpli\ufb01ed Chinese ( zhHans - CN ) , and collect from the 2006 to 2020 NMLEC , as well as practice exercises from the Internet .", "entities": []}, {"text": "F.3 SPEAKER DEMOGRAPHIC Since the data is designed by a team of anonymous human healthcare experts , we are not able to directly reach them for inclusion in this dataset and thus could not be asked for demographic information .", "entities": []}, {"text": "It is expected that most of the speakers come from China with professionals working in the area of biomedicine , and speak Chinese as a native language .", "entities": []}, {"text": "No direct information is available about age and gender distribution .", "entities": [[6, 9, "DatasetName", "age and gender"]]}, {"text": "F.4 ANNOTATOR DEMOGRAPHIC", "entities": []}, {"text": "The experiments involve annotations from 5 medical experts with at least have a master \u2019s degree and have passed the NMLEC .", "entities": []}, {"text": "They ranged in age from 28 \u26b6 45 years , included 3 men and 2 women , all come from China and speak Chinese as a native language .", "entities": []}, {"text": "F.5 SPEECH SITUATION All questions in MLEC - QA are collected from theNational Medical Licensing Examination in China ( NMLEC ) , which are carefully designed by human experts to evaluate professional knowledge and skills for those who want to be medical practitioners in China .", "entities": []}, {"text": "F.6 TEXT CHARACTERISTICS", "entities": []}, {"text": "The topics include in MLEC - QA are in 5 biomedical sub-\ufb01elds : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Traditional Chinese Medicine Combined with Western Medicine .", "entities": []}, {"text": "F.7 RECORDING QUALITY N / A.F.8 OTHER N / A. F.9", "entities": []}, {"text": "PROVENANCE APPENDIX N / A.", "entities": []}]