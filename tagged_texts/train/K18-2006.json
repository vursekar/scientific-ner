[{"text": "Proceedings of the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies , pages 65\u201373 Brussels , Belgium , October 31 \u2013 November 1 , 2018 .", "entities": [[14, 16, "DatasetName", "Universal Dependencies"]]}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics https://doi.org/10.18653/v1/K18-200665Joint Learning of POS and Dependencies for Multilingual Universal Dependency Parsing Zuchao Li1;2;\u0003 , Shexia He1;2;\u0003 , Zhuosheng Zhang1;2 , Hai Zhao1;2;y 1Department of Computer Science and Engineering , Shanghai Jiao Tong University 2Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering , Shanghai Jiao Tong University , Shanghai , China fcharlee , heshexia , zhangzs g@sjtu.edu.cn , zhaohai@cs.sjtu.edu.cn", "entities": [[14, 16, "TaskName", "Dependency Parsing"]]}, {"text": "Abstract", "entities": []}, {"text": "This paper describes the system of team LeisureX in the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies .", "entities": [[21, 23, "DatasetName", "Universal Dependencies"]]}, {"text": "Our system predicts the part - of - speech tag and dependency tree jointly .", "entities": [[4, 7, "DatasetName", "part - of"]]}, {"text": "For the basic tasks , including tokenization , lemmatization and morphology prediction , we employ the of\ufb01cial baseline model ( UDPipe ) .", "entities": [[8, 9, "TaskName", "lemmatization"]]}, {"text": "To train the low - resource languages , we adopt a sampling method based on other richresource languages .", "entities": []}, {"text": "Our system achieves a macro - average of 68.31 % LAS F1 score , with an improvement of 2.51 % compared with the UDPipe .", "entities": [[11, 13, "MetricName", "F1 score"]]}, {"text": "1 Introduction The goal of Universal Dependencies ( UD ) ( Nivre et al . , 2016 ; Zeman et al . , 2017 ) is to develop multilingual treebank , whose annotations of morphology and syntax are cross - linguistically consistent .", "entities": [[5, 7, "DatasetName", "Universal Dependencies"], [8, 9, "DatasetName", "UD"]]}, {"text": "In this paper , we describe our system for the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies ( Zeman et al . , 2018 ) , and we focus only on the subtasks of part - of - speech ( POS ) tagging and dependency parsing .", "entities": [[21, 23, "DatasetName", "Universal Dependencies"], [40, 43, "DatasetName", "part - of"], [50, 52, "TaskName", "dependency parsing"]]}, {"text": "For the intermediate steps , including tokenization , lemmatization and morphology prediction , we tackle them by the of\ufb01cial baseline model ( UDPipe)1 . \u0003These authors made equal contribution.yCorresponding author .", "entities": [[8, 9, "TaskName", "lemmatization"]]}, {"text": "This paper was partially supported by National Key Research and Development Program of China ( No . 2017YFB0304100 ) , National Natural Science Foundation of China ( No . 61672343 and No . 61733011 ) , Key Project of National Society Science Foundation of China ( No . 15ZDA041 ) , The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University ( No . 14JCRZ04 ) .", "entities": []}, {"text": "1https://ufal.mff.cuni.cz/udpipe/Dependency parsing that aims to predict the existence and type of linguistic dependency relations between words , is a fundamental part in natural language processing ( NLP ) tasks ( Li et al . , 2018c ; He et al . , 2018 ) .", "entities": []}, {"text": "Many referential natural language processing studies ( Zhang et al . , 2018 ; Bai and Zhao , 2018 ; Cai et al . , 2018 ; Li et al . , 2018b ; Wang et al . , 2018 ; Qin et al . , 2017 ) can also contribute to the universal dependency parsing system .", "entities": [[54, 56, "TaskName", "dependency parsing"]]}, {"text": "Universal dependency parsing focuses on learning syntactic dependency structure over many typologically different languages , even low - resource languages in a real - world setting .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "Within the dependency parsing literature , there are two dominant techniques , graph - based ( McDonald et al . , 2005 ; Ma and Zhao , 2012 ; Kiperwasser and Goldberg , 2016 ; Dozat and Manning , 2017 ) and transition - based parsing ( Nivre , 2003 ; Dyer et al . , 2015 ; Zhang et al . , 2017 ) .", "entities": [[2, 4, "TaskName", "dependency parsing"]]}, {"text": "Graph - based dependency parsers enjoy the advantage of the global search which learns the scoring functions for all possible parsing trees to \ufb01nd the globally highest scoring one while transition - based dependency parsers build dependency trees from left to right incrementally , which makes the series of multiple choice decisions locally .", "entities": []}, {"text": "In our system , we adopt the transition - based dependency parsing in view of its relatively lower time complexity .", "entities": [[7, 12, "TaskName", "transition - based dependency parsing"]]}, {"text": "Our system implements universal dependency parsing based on the stack - pointer networks ( STACKPTR ) parser introduced by ( Ma et al . , 2018 ) .", "entities": [[4, 6, "TaskName", "dependency parsing"]]}, {"text": "Furthermore , previous work ( Straka et al . , 2016 ; Nguyen et al . , 2017 ) showed that POS tags are helpful to dependency parsing .", "entities": [[26, 28, "TaskName", "dependency parsing"]]}, {"text": "In particular , ( Nguyen et al . , 2017 ) pointed out that parsing performance could be improved by the merit of accurate POS tags and the context of syntactic parse tree could help resolve POS ambiguities .", "entities": []}, {"text": "Therefore , we seek to jointly learn POS tagging and dependency parsing .", "entities": [[10, 12, "TaskName", "dependency parsing"]]}, {"text": "66As Long short - term memory ( LSTM ) networks ( Hochreiter and Schmidhuber , 1997 ) have shown signi\ufb01cant representational effectiveness to a wide range of NLP tasks , we leverage bidirectional LSTMs ( BiLSTM ) to learn shared representations for both POS tagging and dependency parsing .", "entities": [[1, 6, "MethodName", "Long short - term memory"], [7, 8, "MethodName", "LSTM"], [35, 36, "MethodName", "BiLSTM"], [46, 48, "TaskName", "dependency parsing"]]}, {"text": "In addition , to train the low - resource languages , we adopt a sampling method based on other richresource languages .", "entities": []}, {"text": "In terms of all the above model improvement , compared to the UDPipe baseline , our system achieves a macro - average of 68.31 % LAS F1 score , with an improvement of 2.51 % in this task .", "entities": [[26, 28, "MetricName", "F1 score"]]}, {"text": "2", "entities": []}, {"text": "Our Model In this section , we describe our joint model2 for POS tagging and dependency parsing in the CoNLL 2018 Shared Task , which is built on the STACKPTR parser introduced by ( Ma et al . , 2018 ) .", "entities": [[15, 17, "TaskName", "dependency parsing"]]}, {"text": "Our model is mainly composed of three components , the representation ( Section 2.1 ) , POS tagger ( Section 2.2 ) and dependency parser ( Section 2.3 ) .", "entities": []}, {"text": "Figure 1 illustrates the overall model .", "entities": []}, {"text": "2.1 Representation Representation is a key component in various NLP models , and good representations should ideally model both complex characteristics and linguistic contexts .", "entities": []}, {"text": "In our system , we follow the bidirectional LSTM - CNN architecture ( BiLSTMCNNs ) ( Chiu and Nichols , 2016 ; Ma and Hovy , 2016 ) , where CNNs encode word information into character - level representation and BiLSTM models context information of each word .", "entities": [[7, 9, "MethodName", "bidirectional LSTM"], [40, 41, "MethodName", "BiLSTM"]]}, {"text": "Character Level Representation", "entities": []}, {"text": "Though word embedding is popular in many existing parsers , they are not ideal for languages with high out - ofvocabulary ( OOV ) ratios .", "entities": []}, {"text": "Hence , our system introduces the character - level ( Li et al . , 2018a ) representation to address the challenge .", "entities": []}, {"text": "Formally , given a word w = fBOW;c 1;c2;:::;cn;EOWg , where two special BOW ( begin - of - word ) and EOW ( end - of - word ) tags indicate the begin and end positions respectively , we use the CNN to extract character - level representation as follows : ec = MaxPool ( Conv ( w ) ) 2Our code will be available here : https://github . com / bcmi220 / joint_stackptr .where", "entities": []}, {"text": "the CNN is similar to the one in ( Chiu and Nichols , 2016 ) , but we use only characters as the inputs to CNN , without character type features .", "entities": []}, {"text": "Word Level Representation Word embedding is a standard component of most state - of - the - art NLP architectures .", "entities": []}, {"text": "Due to their ability to capture syntactic and semantic information of words from large scale unlabeled texts , we pre - train the word embeddings from the given training dataset by word2vec ( Mikolov et al . , 2013 ) toolkit .", "entities": [[23, 25, "TaskName", "word embeddings"]]}, {"text": "For low - resource languages without available training data , we sample the training dataset from similar languages to generate a mixed dataset .", "entities": []}, {"text": "2.2 POS Tagger To enrich morphological information , we also incorporate UPOS tag embeddings into the representation .", "entities": []}, {"text": "Therefore , we jointly predict the UPOS tag in our system .", "entities": []}, {"text": "The architecture for the POS tagger in our model is almost identical to that of the parser ( Dozat et al . , 2017 ) .", "entities": []}, {"text": "The tagger uses a BiLSTM over the concatenation of word embeddings and character embeddings : spos i = BiLSTMpos(ew i \f ec i )", "entities": [[4, 5, "MethodName", "BiLSTM"], [9, 11, "TaskName", "word embeddings"]]}, {"text": "Then we calculate the probability of tag for each type using af\ufb01ne classi\ufb01ers as follows : hpos i = MLPpos(spos i ) rpos", "entities": []}, {"text": "i = Wposhpos i+bpos ypos i = argmax(ri )", "entities": []}, {"text": "The tag classi\ufb01er is trained jointly using crossentropy losses that are summed together with the dependency parser loss during optimization .", "entities": [[17, 18, "MetricName", "loss"]]}, {"text": "Context - sensitive Representation In order to integrate contextual information , we concatenate the character embedding ec , pre - trained word embeddingewand UPOS tag embedding epos , then feed them into the BiLSTM .", "entities": [[33, 34, "MethodName", "BiLSTM"]]}, {"text": "We take the bidirectional vectors at the \ufb01nal layer as the contextsensitive representation : \u0000 !", "entities": []}, {"text": "si = LSTMforward ( ew i \f ec", "entities": []}, {"text": "i \f epos i )   \u0000si = LSTMbackward (", "entities": []}, {"text": "ew i \f ec", "entities": []}, {"text": "i \f epos i ) si=\u0000 !", "entities": []}, {"text": "si \f  \u0000si Notably , we use the UPOS tag from the output of our POS tagging model .", "entities": []}, {"text": "67 RepresentationLSTM LSTMLSTM LSTMLSTM LSTMLSTM LSTM+", "entities": []}, {"text": "+ + + \u2534 .", "entities": []}, {"text": ".+ .", "entities": []}, {"text": "+ BiLSTMBiaffine Scorer BiLSTMAffine classifier ... headchild root rootStack - Pointer Network Word Character POS tag Word Characterhead ...", "entities": [[10, 12, "MethodName", "Pointer Network"]]}, {"text": "POS   taggingGrandchildFigure 1 : The joint model for POS tagging and dependency parsing .", "entities": [[12, 14, "TaskName", "dependency parsing"]]}, {"text": "2.3 Dependency Parsing The universal dependency parsing component of our system is built on the current state - of - the - art approach STACKPTR , which combines pointer networks ( Vinyals et al . , 2015 ) with an internal stack for tracking the status of depth-\ufb01rst search .", "entities": [[1, 3, "TaskName", "Dependency Parsing"], [5, 7, "TaskName", "dependency parsing"]]}, {"text": "It bene\ufb01ts from the global information of the sentence and all previously derived subtree structures , and removes the left - to - right restriction in classical transition - based parsers .", "entities": []}, {"text": "The STACKPTR parser mainly consists of two parts : encoder and decoder .", "entities": []}, {"text": "The encoder based on BiLSTM - CNNs architecture takes the sequence of tokens and their POS tags as input , then encodes it into encoder hidden state si .", "entities": [[4, 5, "MethodName", "BiLSTM"]]}, {"text": "The internal stack \u001bis initialized with dummy ROOT .", "entities": []}, {"text": "For decoder ( a uni - directional RNN ) , it receives the input from last step and outputs decoder hidden state ht .", "entities": []}, {"text": "The pointer neural network takes the top element wh in the stack \u001bat each timestep tas current head to select a speci\ufb01c child wcwith biaf\ufb01ne attentionmechanism ( Dozat and Manning , 2017 ) for attention score function in all possible head - dependent pairs .", "entities": []}, {"text": "Then the child wcwill be pushed onto the stack\u001bfor next step when c6 = h , otherwise it indicates that all children of the current head h have been selected , therefore the head whwill be popped out of the stack \u001b.", "entities": []}, {"text": "The attention scoring function used is given as follows and the pointer neural network uses atas pointer to select the child element : et i = hT tWsi+UTht+VTsi+b at = softmax ( et ) More speci\ufb01cally , the decoder maintains a list of available words in test phase .", "entities": [[30, 31, "MethodName", "softmax"]]}, {"text": "For each head hat each decoding step , the selected child will be removed from the list to make sure that it can not be selected as a child of other head words .", "entities": []}, {"text": "Given a dependency tree , there may be multiple children for a speci\ufb01c head .", "entities": []}, {"text": "This results in more than one valid selection for each time step ,", "entities": []}, {"text": "68which might confuse the decoder .", "entities": []}, {"text": "To address this problem , the parser introduces an inside - outside order to utilize second - order sibling information , which has been proven to be an important feature for parsing process ( McDonald and Pereira , 2006 ; Koo and Collins , 2010 ) .", "entities": []}, {"text": "To utilize the secondorder information , the parser replaces the input of decoder from sias follows :", "entities": []}, {"text": "i = ss\u000esh\u000esi wheresandhindicate the sibling and head index of nodei,\u000eis the element - wise sum operation to ensure no additional model parameters .", "entities": []}, {"text": "2.4 Loss Function The training objective of pur system is to learn the probability of UPOS tags P\u0012pos(yposjx)and", "entities": []}, {"text": "the dependency trees P\u0012dep(ydepjx;y0 pos ) .", "entities": []}, {"text": "Given a sentencex , the probabilities are factorized as : P\u0012pos(yposjx ) = kX i=1P\u0012pos(pijx )", "entities": []}, {"text": "y0 pos= arg max ypos2Ypos(P\u0012pos(yposjx )", "entities": []}, {"text": ") P\u0012dep(ydepjx;y0 pos )", "entities": []}, {"text": "= kX i=1P\u0012dep(pijp < i;x;y0 pos )", "entities": []}, {"text": "= kY i=1liY j=1P\u0012dep(ci;jjci;<j;p < i;x;y0 pos ) where\u0012posand\u0012deprepresent the model parameters respectively .", "entities": []}, {"text": "p < idenotes the preceding dependency paths that have already been generated .", "entities": []}, {"text": "ci;jrepresents the jthword inpiandci;jdenotes all the proceeding words on the path pi .", "entities": []}, {"text": "Therefore , the whole loss is the sum of three objectives :", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "Loss = Losspos+Lossarc+Losslabel where theLosspos , LossarcandLosslabel are the conditional likehood of their corresponding target , using the cross - entropy loss .", "entities": [[21, 22, "MetricName", "loss"]]}, {"text": "Speci\ufb01cally , we train a dependency label classi\ufb01er following Dozat and Manning ( 2017 ) , which takes the dependency head - child pair as input features .", "entities": []}, {"text": "3 System Implements Our system focuses on three targets : the UPOS tag , dependency arc and dependency relation .", "entities": []}, {"text": "Therefore , we rely on the UDPipe model ( StrakaTreebank Sampling Breton KEB English ,", "entities": []}, {"text": "Irish Czech PUD Czech PDT English PUD", "entities": []}, {"text": "English EWT Faroese OFT Norwegian , English , Danish , Swedish , German ,", "entities": []}, {"text": "Dutch Finnish PUD Finnish TDT Japanese Modern Japanese GSD Naija NSC English Swedish PUD Swedish Talbanken Thai PUD English , Chinese , Hindi , Vietnamese Table 1 : Language substitution for treebanks without training data et al . , 2016 ) to provide a pipeline from raw text to basic dependency structures , including a tokenizer , tagger and the dependency predictor .", "entities": []}, {"text": "For treebanks with non - empty training dataset ( including treebanks whose training set is very small ) , we utilize the baseline model UDPipe trained on corresponding treebank , which has been provided by the organizer .", "entities": []}, {"text": "For the remaining nine treebanks without training data , we construct the train dataset by sampling from the other training datasets according to the language similarity inspired by ( Zhao et al . , 2009 , 2010 ; Wang et al . , 2015 , 2016 ) , as detailed in Table 1 .", "entities": []}, {"text": "Our system adopts the hyper - parameter con\ufb01guration in ( Ma et al . , 2018 ) , with a few exceptions .", "entities": []}, {"text": "We initialize word vectors with 50 - dimensional pretrained word embeddings , 100 - dimensional tag embeddings and 512 - dimensional recurrent states ( in each direction ) .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "Our system drops embeddings and hidden states independently with 33 % probability .", "entities": []}, {"text": "We optimize with Adam ( Kingma and Ba , 2015 ) , setting the learning rate to 1e\u00003and \f 1=", "entities": [[3, 4, "MethodName", "Adam"], [14, 16, "HyperparameterName", "learning rate"]]}, {"text": "2= 0:9 .", "entities": []}, {"text": "Moreover , we train models for up to 100 epochs with batch size 32 on 3 NVIDIA GeForce GTX 1080Ti GPUs with 200 to 500 sentences per second and occupying 2 to 3 GB graphic memory each model .", "entities": [[11, 13, "HyperparameterName", "batch size"]]}, {"text": "A full run over the test datasets on the TIRA virtual machine ( Potthast et al . , 2014 ) takes about 12 hours .", "entities": []}, {"text": "4 Results Table 2 reports the of\ufb01cial evaluation results of our system in several metrics of treebanks from the CoNLL 2018 shared task ( ? ) .", "entities": []}, {"text": "For dependency parsing , our model outperforms the baseline", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "69Results Ours Baseline Best LAS 68.31 65.80 75.84 MLAS 53.70 52.42 61.25 BLEX 58.42 55.80 66.09 UAS 74.03 71.64 80.51 CLAS 63.85 60.77 72.36 UPOS 87.15 87.32 90.91 XPOS 83.91 85.00 86.67 Morphological features 83.46 83.74 87.59 Morphological tags 76.68 77.62 80.30 Lemmas 87.77 87.84 91.24 Sentence segmentation 83.01 83.01 83.87 Word segmentation 96.97 96.97 98.18 Tokenization 97.39 97.39 98.42 Table 2 : Results on all treebanks .", "entities": [[46, 48, "TaskName", "Sentence segmentation"]]}, {"text": "with absolute gains ( 1.28 - 3.08 % ) on average LAS , UAS , MLAS and CLAS .", "entities": []}, {"text": "These results show that our joint model could improve the performance of universal dependency parsing .", "entities": [[13, 15, "TaskName", "dependency parsing"]]}, {"text": "Surprisingly , in the case of POS tagging , our joint model obtains lower averaged accuracy in both UPOS and XPOS .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "The possible reason for performance degradation may be that we select all hyper - parameters based on English and do not tune them individually .", "entities": []}, {"text": "Furthermore , we also compare the performance of our system with the baseline and the best scorer on big treebanks ( Table 3 ) , PUD treebanks ( Table 4 ) , low - resource languages ( Table 5 ) , respectively .", "entities": []}, {"text": "Since our model applies the baseline model for tokenization and segmentation , we show all results of focused metrics on each treebank in Table 6 .", "entities": []}, {"text": "In addition , we compare our model with the best and the average results of top ten models on each treebank , using LAS F1 for the evaluation metric , as shown in Figure 2 . 5 Conclusion In this paper , we describe our system in the CoNLL 2018 shared task on UD parsing .", "entities": [[24, 25, "MetricName", "F1"], [53, 54, "DatasetName", "UD"]]}, {"text": "Our system uses a transition - based neural network architecture for dependency parsing , which predicts the UPOS tag and dependencies jointly .", "entities": [[11, 13, "TaskName", "dependency parsing"]]}, {"text": "Combining pointer networks with an internal stack to track the status of the top - down , depth-\ufb01rst search in the parsing decoding procedure , the STACKPTR parser is able to capture information from the whole sentence and all the previously derived subtrees , removing the left - to - right restriction in classical transition - based parsers , while maintainingResults Ours Baseline Best LAS 77.98 74.14 84.37 MLAS 63.79 61.27 72.67 BLEX 68.55 64.67 75.83 UAS 82.27 78.78 87.61 CLAS 73.59 69.13 81.29 UPOS 93.71 93.71 96.23 XPOS 91.81 91.81 95.16 Morphological features 90.85 90.85 94.14 Morphological tags 87.56 87.56 91.50 Lemmas 93.34 93.34 96.08 Sentence segmentation 86.09 86.09 89.52 Word segmentation 98.81 98.81 99.21 Tokenization 99.24 99.24 99.51 Table 3 : Results on big treebank only .", "entities": [[106, 108, "TaskName", "Sentence segmentation"]]}, {"text": "Results Ours Baseline Best LAS 61.05 66.63 74.20 MLAS 41.95 51.75 58.75 BLEX 50.60 54.87 63.25 UAS 67.88 71.22 78.42 CLAS 57.34 61.29 69.86 UPOS 82.45 85.23 87.51 XPOS 35.66 54.27 55.98 Morphological features 78.89 83.41 87.05 Morphological tags 34.68 50.32 51.90 Lemmas 82.24 83.37 85.76 Sentence segmentation 75.53 75.53 76.04 Word segmentation 92.61 92.61 94.57 Tokenization 92.61 92.61 94.57 Table 4 : Results on PUD treebank only .", "entities": [[46, 48, "TaskName", "Sentence segmentation"]]}, {"text": "Results Ours Baseline Best LAS 17.16 17.17 27.89 MLAS 3.43 3.44 6.13 BLEX 7.63 7.63 13.98 UAS 30.07 30.08 39.23 CLAS 13.42 13.42 22.18 UPOS 45.17 45.20 61.07 XPOS 54.68 54.23 54.73 Morphological features 38.03 38.03 48.95 Morphological tags 25.86 25.72 25.91 Lemmas 54.25 54.25 64.42 Sentence segmentation 65.99 65.99 67.50 Word segmentation 84.95 84.95 93.38 Tokenization 85.76 85.76 93.34 Table 5 : Results on low - resource languages only .", "entities": [[46, 48, "TaskName", "Sentence segmentation"]]}, {"text": "70 af_afribooms   ar_padt   bg_btb br_keb   bxr_bdt   ca_ancora   cs_cac cs_fictree   cs_pdt cs_pud   cu_proiel   da_ddt   de_gsd el_gdt en_ewt", "entities": []}, {"text": "en_gum   en_lines   en_pud   es_ancora   et_edt eu_bdt fa_seraji   fi_ftb fi_pud fi_tdt fo_oft fro_srcmf   fr_gsd fr_sequoia   fr_spoken   ga_idt gl_ctg gl_treegal   got_proiel   grc_perseus   grc_proiel   he_htb hi_hdtb   hr_set   hsb_ufal   hu_szeged020406080Best", "entities": []}, {"text": "Ours Avg . Top10hy_armtdp   id_gsd it_isdt it_postwita   ja_gsd ja_modern   kk_ktb   kmr_mg   ko_gsd ko_kaist la_ittb la_perseus   la_proiel   lv_lvtb nl_alpino   nl_lassysmall   no_bokmaal   no_nynorsk   no_nynorsklia   pcm_nsc   pl_lfg pl_sz pt_bosque   ro_rrt ru_syntagrus   ru_taiga   sk_snk sl_ssjsl_sst sme_giella   sr_set sv_lines   sv_pud sv_talbanken   th_pud tr_imst ug_udt uk_iu ur_udtb vi_vtb zh_gsd020406080Best Ours Avg . Top10Figure 2 : LAS F1 score per treebank .", "entities": [[69, 71, "MetricName", "F1 score"]]}, {"text": "For comparison , we include the best of\ufb01cial result and the average of the top ten results on each treebank .", "entities": []}, {"text": "linear parsing steps .", "entities": []}, {"text": "Furthermore , our model is single instead of ensemble , and it does not utilize lemmas or morphological features .", "entities": []}, {"text": "Results show that our system achieves 68.31 % in macroaveraged LAS F1 - score on the of\ufb01cial blind test .", "entities": [[11, 14, "MetricName", "F1 - score"]]}, {"text": "Further improvements could be obtained by multilingual embeddings and adopting ensemble methods .", "entities": []}, {"text": "References Hongxiao Bai and Hai Zhao .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep enhanced representation for implicit discourse relation recognition .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics .", "entities": []}, {"text": "pages 571\u2013583 .", "entities": []}, {"text": "Jiaxun Cai , Shexia He , Zuchao Li , and Hai Zhao .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A full end - to - end semantic role labeler , syntacticagnostic over syntactic - aware ?", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics .", "entities": []}, {"text": "pages 2753\u20132765 .", "entities": []}, {"text": "Jason PC Chiu and Eric Nichols .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Named entity recognition with bidirectional LSTM - CNNs .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 6, "MethodName", "bidirectional LSTM"]]}, {"text": "Transactions of the Association for Computational Linguistics 4:357\u2013370 .", "entities": []}, {"text": "Timothy Dozat and Christopher D Manning .", "entities": []}, {"text": "2017.Deep biaf\ufb01ne attention for neural dependency parsing .", "entities": [[5, 7, "TaskName", "dependency parsing"]]}, {"text": "ICLR .", "entities": []}, {"text": "Timothy Dozat , Peng Qi , and Christopher D Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Stanford \u2019s graph - based neural dependency parser at the conll 2017 shared task .", "entities": []}, {"text": "Proceedings of the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies pages 20\u201330 .", "entities": [[14, 16, "DatasetName", "Universal Dependencies"]]}, {"text": "Chris Dyer , Miguel Ballesteros , Wang Ling , Austin Matthews , and Noah A. Smith .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Transitionbased dependency parsing with stack long shortterm memory pages 334\u2013343 .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "Shexia He , Zuchao Li , Hai Zhao , and Hongxiao Bai . 2018 .", "entities": []}, {"text": "Syntax for semantic role labeling , to be , or not to be .", "entities": [[2, 5, "TaskName", "semantic role labeling"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) .", "entities": []}, {"text": "pages 2061\u20132071 .", "entities": []}, {"text": "Sepp Hochreiter and Jrgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural Computation 9(8):1735\u20131780 .", "entities": []}, {"text": "Diederik P Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In ICLR .", "entities": []}, {"text": "Eliyahu Kiperwasser and Yoav Goldberg . 2016 .", "entities": []}, {"text": "Simple and accurate dependency parsing using bidirectional LSTM feature representations .", "entities": [[3, 5, "TaskName", "dependency parsing"], [6, 8, "MethodName", "bidirectional LSTM"]]}, {"text": "Transactions of the Association for Computational Linguistics4:313\u2013327 .", "entities": []}, {"text": "71UPOS UAS LAS MLAS UPOS UAS LAS MLAS afafribooms 95.12 84.64 80.75 66.96 ar padt 89.34 74.45 70.11 57.21 bgbtb 97.72 91.24 87.69 77.56 br keb 30.74 27.80 10.25 0.37 bxrbdt 41.66 29.20 12.61 2.09 ca ancora 98.00 91.87 89.38 80.87 cscac 98.32 91.07 88.46 74.28 cs \ufb01ctree 97.28 91.07 87.12 71.98 cspdt 98.21 91.59 89.37 78.20 cs pud 94.67 84.09 78.17 59.57 cuproiel 93.70 75.18 68.68 55.36 da ddt 95.44 82.21 78.74 67.34 degsd 91.58 80.31 75.73 36.39 el gdt 95.63 86.64 83.17 65.02 enewt 93.62 83.32 80.46 70.58 en gum 93.24 81.09 76.68 63.05 enlines 94.71 80.71 75.26 65.04 en pud 94.15 86.77 83.49 70.23 esancora 98.14 91.35 89.09 81.01 et edt 95.50 84.18 80.59 70.39 eubdt 92.34 81.06 76.49 60.75 fa seraji 96.01 86.76 82.78 75.38 \ufb01ftb 92.28 84.23 79.83 66.53 \ufb01 pud 84.86 62.87 50.67 36.39 \ufb01tdt 94.37 84.72 80.88 70.42 fo oft 44.66 42.64 25.19 0.36 frosrcmf 94.30 90.32 85.15 75.66 fr gsd 95.75 87.25 84.08 74.58 frsequoia 95.84 85.16 82.50 71.23 fr spoken 92.94 71.81 65.30 52.73 gaidt 89.21 72.66 62.93 37.66 gl ctg 96.26 81.60 78.60 65.00 gltreegal 91.09 71.61 66.16 49.13 got proiel 94.31 69.71 62.62 48.19 grcperseus 82.37 70.08 63.68 33.28 grc proiel 95.87 75.19 71.05 52.44 hehtb 80.87 64.90 60.53 46.03 hi hdtb 95.75 94.18 90.83 72.03 hrset 96.33 88.39 83.06 60.93 hsb ufal 65.75 35.02 23.64 3.55 huszeged 90.59 73.91 66.23 50.36 hy armtdp 65.40 36.81 21.79 6.84 idgsd 92.99 83.49 77.12 64.70 it isdt 97.05 91.01 88.91 79.66 itpostwita 93.94 72.74 67.48 54.38 ja gsd 87.85 76.14 74.43 60.32 jamodern 48.44 29.36 22.71 8.10 kk ktb 48.94 39.45 24.21 7.62 kmr mg 59.31 32.86 23.92 5.47 ko gsd 93.44 80.91 76.27 68.93 kokaist 93.32 87.43 85.11 76.91 la ittb 97.21 86.64 83.96 73.55 laperseus 83.34 58.45 47.61 30.16 la proiel 94.84 68.02 62.62 49.11 lvlvtb 91.70 78.74 73.13 55.05 nl alpino 94.04 87.76 83.91 68.47 nllassysmall 94.06 82.34 78.13 64.55 no bokmaal 96.51 90.30 88.11 78.94 nonynorsk 96.07 89.67 87.26 76.85 no nynorsklia 85.15 57.92 48.95 37.60 pcm nsc 44.44 26.11 12.18 4.60 pl lfg 96.77 93.67 90.94 74.89 plsz 95.50 89.64 85.83 64.03 pt bosque 95.99 88.48 85.80 70.70 rorrt 96.62 89.06 83.94 74.60 ru syntagrus 97.84 92.09 90.28 80.63 rutaiga 86.53 63.58 55.51 36.79 sk snk 93.15 83.42 79.43 55.02 slssj 94.46 84.01 81.18 65.00 sl sst 88.50 54.16 46.95 34.19 sme giella 87.69 63.80 56.98 46.05 sr set 96.84 89.50 84.90 70.68 svlines 93.97 81.32 76.04 59.25 sv pud 90.12 76.30 70.19 35.44 svtalbanken 95.36 85.27 81.57 71.64 th pud 5.65 0.71 0.62 0.01 trimst 91.64 64.02 56.07 44.49 ug udt 87.48 71.29 57.89 37.46 ukiu 94.80 81.43 77.01 56.96 ur udtb 92.13 86.14 79.99 51.65 vivtb 75.29 47.32 41.77 34.18 zh gsd 83.47 66.45 63.05 51.64 Table 6 : Performances of focused metrics on each treebank .", "entities": [[390, 391, "DatasetName", "sst"]]}, {"text": "72Terry Koo and Michael Collins .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Ef\ufb01cient thirdorder dependency parsers .", "entities": []}, {"text": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics , pages 1\u201311 .", "entities": []}, {"text": "Haonan Li , Zhisong Zhang , Yuqi Ju , and Hai Zhao .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Neural character - level dependency parsing for Chinese .", "entities": [[4, 6, "TaskName", "dependency parsing"]]}, {"text": "In The Thirty - Second AAAI Conference on Arti\ufb01cial Intelligence ( AAAI-18 ) .", "entities": []}, {"text": "Zuchao Li , Jiaxun Cai , Shexia He , and Hai Zhao . 2018b .", "entities": []}, {"text": "Seq2seq dependency parsing .", "entities": [[0, 1, "MethodName", "Seq2seq"], [1, 3, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics .", "entities": []}, {"text": "pages 3203\u20133214 .", "entities": []}, {"text": "Zuchao Li , Shexia He , Jiaxun Cai , Zhuosheng Zhang , Hai Zhao , Gongshen Liu , Linlin Li , and Luo Si .", "entities": []}, {"text": "2018c .", "entities": []}, {"text": "A uni\ufb01ed syntax - aware framework for semantic role labeling .", "entities": [[7, 10, "TaskName", "semantic role labeling"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "Xuezhe Ma and Eduard Hovy .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "End - to - end sequence labeling via bi - directional LSTM - CNNsCRF .", "entities": [[11, 12, "MethodName", "LSTM"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) .", "entities": []}, {"text": "volume 1 , pages 1064\u20131074 .", "entities": []}, {"text": "Xuezhe Ma , Zecong Hu , Jingzhou Liu , Nanyun Peng , Graham Neubig , and Eduard Hovy .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Stackpointer networks for dependency parsing .", "entities": [[3, 5, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) .", "entities": []}, {"text": "pages 1403\u20131414 .", "entities": []}, {"text": "Xuezhe Ma and Hai Zhao .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Fourth - order dependency parsing .", "entities": [[3, 5, "TaskName", "dependency parsing"]]}, {"text": "In 24th International Conference on Computational Linguistics .", "entities": []}, {"text": "page 785 .", "entities": []}, {"text": "Ryan McDonald , Koby Crammer , and Fernando Pereira . 2005 .", "entities": []}, {"text": "Online large - margin training of dependency parsers .", "entities": []}, {"text": "In Proceedings of the 43rd annual meeting on association for computational linguistics .", "entities": []}, {"text": "Association for Computational Linguistics , pages 91\u201398 .", "entities": []}, {"text": "Ryan McDonald and Fernando Pereira . 2006 .", "entities": []}, {"text": "Online learning of approximate dependency parsing algorithms .", "entities": [[0, 2, "TaskName", "Online learning"], [4, 6, "TaskName", "dependency parsing"]]}, {"text": "In 11th Conference of the European Chapter of the Association for Computational Linguistics .", "entities": []}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "In ICLR Workshop .", "entities": []}, {"text": "Dat Quoc Nguyen , Mark Dras , and Mark Johnson .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A novel neural network model for joint pos tagging and graph - based dependency parsing .", "entities": [[13, 15, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies .", "entities": [[15, 17, "DatasetName", "Universal Dependencies"]]}, {"text": "Vancouver , Canada , pages 134\u2013142 .", "entities": []}, {"text": "Joakim Nivre .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "An ef\ufb01cient algorithm for projective dependency parsing .", "entities": [[5, 7, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT ) .", "entities": []}, {"text": "Citeseer , pages 149\u2013160.Joakim", "entities": [[0, 1, "DatasetName", "Citeseer"]]}, {"text": "Nivre , Marie - Catherine de Marneffe , Filip Ginter , Yoav Goldberg , Jan Haji \u02c7c , Christopher Manning , Ryan McDonald , Slav Petrov , Sampo Pyysalo , Natalia Silveira , Reut Tsarfaty , and Daniel Zeman .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Universal Dependencies v1 : A multilingual treebank collection .", "entities": [[0, 2, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proceedings of the 10th International Conference on Language Resources and Evaluation ( LREC 2016 ) .", "entities": []}, {"text": "Portoro , Slovenia , pages 1659\u20131666 .", "entities": []}, {"text": "Martin Potthast , Tim Gollub , Francisco Rangel , Paolo Rosso , Efstathios Stamatatos , and Benno Stein .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Improving the reproducibility of PAN \u2019s shared tasks : Plagiarism detection , author identi\ufb01cation , and author pro\ufb01ling .", "entities": []}, {"text": "In Evangelos Kanoulas , Mihai Lupu , Paul Clough , Mark Sanderson , Mark Hall , Allan Hanbury , and Elaine Toms , editors , Information Access Evaluation meets Multilinguality , Multimodality , and Visualization .", "entities": []}, {"text": "5th International Conference of the CLEF Initiative ( CLEF 14 ) .", "entities": []}, {"text": "Berlin Heidelberg New York , pages 268\u2013299 .", "entities": []}, {"text": "Lianhui Qin , Zhisong Zhang , Hai Zhao , Zhiting Hu , and Eric Xing .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Adversarial connectiveexploiting networks for implicit discourse relation classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) .", "entities": []}, {"text": "volume 1 , pages 1006\u20131017 .", "entities": []}, {"text": "Milan Straka , Jan Haji \u02c7c , and Jana Strakov \u00b4 a. 2016 .", "entities": []}, {"text": "UDPipe : trainable pipeline for processing CoNLL - U \ufb01les performing tokenization , morphological analysis , POS tagging and parsing .", "entities": [[13, 15, "TaskName", "morphological analysis"]]}, {"text": "In Proceedings of the 10th International Conference on Language Resources and Evaluation ( LREC 2016 ) .", "entities": []}, {"text": "Portoro , Slovenia .", "entities": []}, {"text": "Oriol Vinyals , Meire Fortunato , and Navdeep Jaitly .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Pointer networks .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems .", "entities": []}, {"text": "pages 2692\u20132700 .", "entities": []}, {"text": "Rui Wang , Masao Utiyama , Isao Goto , Eiichiro Sumita , Hai Zhao , and Bao - Liang Lu . 2016 .", "entities": []}, {"text": "Converting continuous - space language models into ngram language models with ef\ufb01cient bilingual pruning for statistical machine translation .", "entities": [[16, 18, "TaskName", "machine translation"]]}, {"text": "ACM Transactions on Asian and Low - Resource Language Information Processing 15(3):11 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Rui Wang , Hai Zhao , Bao - Liang Lu , Masao Utiyama , and Eiichiro Sumita . 2015 .", "entities": []}, {"text": "Bilingual continuousspace language model growing for statistical machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "IEEE / ACM Transactions on Audio , Speech , and Language Processing 23(7):1209 \u2013 1220 .", "entities": [[2, 3, "DatasetName", "ACM"]]}, {"text": "Rui Wang , Hai Zhao , Sabine Ploux , Bao - Liang Lu , Masao Utiyama , and Eiichiro Sumita .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graphbased bilingual word embedding for statistical machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "ACM Transactions on Asian and Low - Resource Language Information Processing ( TALLIP ) 17(4):31 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Daniel Zeman , Jan Haji \u02c7c , Martin Popel , Martin Potthast , Milan Straka , Filip Ginter , Joakim Nivre , and", "entities": []}, {"text": "73Slav Petrov .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies .", "entities": [[11, 13, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proceedings of the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies .", "entities": [[15, 17, "DatasetName", "Universal Dependencies"]]}, {"text": "Association for Computational Linguistics , Brussels , Belgium , pages 1\u201320 .", "entities": []}, {"text": "Daniel Zeman , Martin Popel , Milan Straka , Jan Haji\u02c7c , Joakim Nivre , Filip Ginter , Juhani Luotolahti , Sampo Pyysalo , Slav Petrov , Martin Potthast , Francis Tyers , Elena Badmaeva , Memduh G\u00a8ok\u0131rmak , Anna Nedoluzhko , Silvie Cinkov \u00b4 a , Jan Haji\u02c7c jr . , Jaroslava Hlav \u00b4 a\u02c7cov\u00b4a , V\u00b4aclava Kettnerov \u00b4 a , Zde\u02c7nka Ure \u02c7sov\u00b4a , Jenna Kanerva , Stina Ojala , Anna Missil \u00a8a , Christopher Manning , Sebastian Schuster , Siva Reddy , Dima Taji , Nizar Habash , Herman Leung , Marie - Catherine de Marneffe , Manuela Sanguinetti , Maria Simi , Hiroshi Kanayama , Valeria de Paiva , Kira Droganova , H \u00b4 ector Mart \u00b4 \u0131nez Alonso , C \u00b8 a\u02d8gr\u0131 C \u00b8 \u00a8oltekin , Umut Sulubacak , Hans Uszkoreit , Vivien Macketanz , Aljoscha Burchardt , Kim Harris , Katrin Marheinecke , Georg Rehm , Tolga Kayadelen , Mohammed Attia , Ali Elkahky , Zhuoran Yu , Emily Pitler , Saran Lertpradit , Michael Mandl , Jesse Kirchner , Hector Fernandez Alcalde , Jana Strnadova , Esha Banerjee , Ruli Manurung , Antonio Stella , Atsuko Shimada , Sookyoung Kwak , Gustavo Mendonc \u00b8a , Tatiana Lando , Rattima Nitisaroj , and Josie Li .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies .", "entities": [[11, 13, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proceedings of the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies .", "entities": [[15, 17, "DatasetName", "Universal Dependencies"]]}, {"text": "Vancouver , Canada , pages 1\u201319 .", "entities": []}, {"text": "Zhirui Zhang , Shujie Liu , Mu Li , Ming Zhou , and Enhong Chen . 2017 .", "entities": []}, {"text": "Stack - based multi - layer attention for transition - based dependency parsing .", "entities": [[8, 13, "TaskName", "transition - based dependency parsing"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "Copenhagen , Denmark , pages 1677\u20131682 .", "entities": []}, {"text": "Zhuosheng Zhang , Yafang Huang , and Hai Zhao .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Subword - augmented embedding for cloze reading comprehension .", "entities": [[6, 8, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics .", "entities": []}, {"text": "pages 1802\u20131814 .", "entities": []}, {"text": "Hai Zhao , Yan Song , and Chunyu Kit .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "How large a corpus do we need : Statistical method versus rulebased method .", "entities": []}, {"text": "Training ( M ) 8(2.71):0\u201383 .", "entities": []}, {"text": "Hai Zhao , Yan Song , Chunyu Kit , and Guodong Zhou .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Cross language dependency parsing using a bilingual lexicon .", "entities": [[2, 4, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP : Volume 1Volume 1 .", "entities": []}, {"text": "Association for Computational Linguistics , pages 55\u201363 .", "entities": []}]