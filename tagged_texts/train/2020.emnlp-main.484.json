[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 6008\u20136018 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics6008XGLUE :", "entities": []}, {"text": "A New Benchmark Dataset for Cross - lingual Pre - training , Understanding and Generation Yaobo Liang , \u0000Nan Duan , Yeyun Gong , Ning Wu , Fenfei Guo , Weizhen Qi , Ming Gong , Linjun Shou , Daxin Jiang , Guihong Cao , Xiaodong Fan , Ruofei Zhang , Rahul Agrawal , Edward Cui , Sining Wei , Taroon Bharti , Ying Qiao , Jiun - Hung Chen , Winnie Wu , Shuguang Liu , Fan Yang , Daniel Campos , Rangan Majumder , Ming Zhou fyalia , nanduan , yegong , t - niwu , v - fengu , v - weqi , migon , lisho , djiang , gucao , xiafan , bzhang , rahul.agrawal , edwac , sinwei , tbharti , yiqia , jiuche , winniew , shuguanl , fanyang , Campos .", "entities": []}, {"text": "Daniel , ranganm , mingzhou g@microsoft.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "In this paper , we introduce XGLUE , a new benchmark dataset that can be used to train large - scale cross - lingual pre - trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross - lingual tasks .", "entities": [[6, 7, "DatasetName", "XGLUE"]]}, {"text": "Comparing to GLUE ( Wang et al . , 2019 ) , which is labeled in English for natural language understanding tasks only , XGLUE has two main advantages : ( 1 ) it provides 11 diversi\ufb01ed tasks that cover both natural language understanding and generation scenarios ; ( 2 ) for each task , it provides labeled data in multiple languages .", "entities": [[2, 3, "DatasetName", "GLUE"], [18, 21, "TaskName", "natural language understanding"], [24, 25, "DatasetName", "XGLUE"], [41, 44, "TaskName", "natural language understanding"]]}, {"text": "We extend a recent cross - lingual pre - trained model Unicoder ( Huang et al . , 2019 ) to cover both understanding and generation tasks , which is evaluated on XGLUE as a strong baseline .", "entities": [[32, 33, "DatasetName", "XGLUE"]]}, {"text": "We also evaluate the base versions ( 12 - layer ) of Multilingual BERT , XLM and XLM - R for comparison.1 1 Introduction Pre - training + Fine - tuning has become a new NLP paradigm , where the general knowledge are \ufb01rstly learnt from large - scale corpus by self - supervised learning and then transferred to downstream tasks by task - speci\ufb01c \ufb01ne - tuning .", "entities": [[13, 14, "MethodName", "BERT"], [15, 16, "MethodName", "XLM"], [17, 18, "MethodName", "XLM"], [40, 42, "TaskName", "general knowledge"], [51, 55, "TaskName", "self - supervised learning"]]}, {"text": "Three different types of pre - trained models are explored recently , includingmonolingual pre - trained models ( Radford et al . , 2018 ; Devlin et", "entities": []}, {"text": "al . , 2019 ; Liu et al . , 2019 ; Yang et al . , 2019b ;", "entities": []}, {"text": "Dong et al . , 2019 ; Lewis et al . , 2019a ) , multilingual and cross - lingual pre - trained models ( Devlin et al . , 2019 ; Conneau and Lample , 2019 ; Huang et al . , 2019 ; Conneau et al . , 2019 ) and multimodal pre - trained models ( Lu et al . , 2019 ; Li et al . , 2020 ; Chen et al . , 2019 ; Zhou et al . , 2020 ) .", "entities": []}, {"text": "In this paper , we focus on the cross - lingual pretrained models , due to their importance to alleviating the low - resource issue among languages , 1The dataset is available at https://microsoft .", "entities": []}, {"text": "github.io/XGLUE/ , The code and model is available at https://github.com/microsoft/Unicoderwhere an NLP task often has rich training data in one language ( such as English ) but has few or no training data in other languages ( such as French and German ) .", "entities": []}, {"text": "In order to further advance the development of cross - lingual pre - trained models for various downstream tasks in different languages , this paper introduces XGLUE , a new benchmark dataset that can be used to : ( i ) train large - scale cross - lingual pre - trained models using multilingual and bilingual corpora , ( ii ) evaluate generalization capabilities of the cross - lingual pre - trained models across a diverse set of cross - lingual tasks .", "entities": [[26, 27, "DatasetName", "XGLUE"]]}, {"text": "The contribution of XGLUE is two - fold .", "entities": [[3, 4, "DatasetName", "XGLUE"]]}, {"text": "First , it provides 11 diversi\ufb01ed cross - lingual tasks covering both understanding and generation scenarios .", "entities": []}, {"text": "XTREME ( Hu et al . , 2020 ) is a concurrent work of XGLUE .", "entities": [[0, 1, "DatasetName", "XTREME"], [14, 15, "DatasetName", "XGLUE"]]}, {"text": "But it includes cross - lingual understanding tasks only .", "entities": []}, {"text": "Besides , XGLUE introduces 6 new tasks selected from Search , Ads and News scenarios , which makes XGLUE have more practical values .", "entities": [[2, 3, "DatasetName", "XGLUE"], [18, 19, "DatasetName", "XGLUE"]]}, {"text": "Second , an extended version of Unicoder ( Huang et al . , 2019 ) is described and evaluated as a strong cross - lingual pre - trained model baseline on XGLUE for both understanding and generation tasks .", "entities": [[31, 32, "DatasetName", "XGLUE"]]}, {"text": "We also evaluate the base versions ( 12 - layer ) of Multilingual BERT ( Devlin et al . , 2019 ) , XLM ( Conneau and Lample , 2019 ) and XLM - R ( Conneau et al . , 2019 ) for comparison .", "entities": [[13, 14, "MethodName", "BERT"], [23, 24, "MethodName", "XLM"], [32, 33, "MethodName", "XLM"]]}, {"text": "2 XGLUE Benchmark 2.1 Pre - training Corpus We collect two corpora , Small Corpus andLarge Corpus , with different sizes for cross - lingual pretraining .", "entities": [[1, 2, "DatasetName", "XGLUE"]]}, {"text": "Table 1 lists the data statistics .", "entities": []}, {"text": "2.1.1 Small Corpus ( SC ) Multilingual Corpus We extract raw sentences from Wikipedia using WikiExtractor .", "entities": []}, {"text": "It leads to a 101 G multilingual corpus covering 100 languages .", "entities": []}, {"text": "6009Type # of Languages Size Small CorpusMultilingual 100 101 G Bilingual 27 146 G Large CorpusMultilingual 100 2,500G+101 G Bilingual 27 146 G Table 1 : The statistics of two pre - training corpora .", "entities": []}, {"text": "Bilingual Corpus We use an in - house pipeline to extract bilingual sentence pairs from the Web , which leads to a 146 G bilingual corpus covering 27 languages , including Arabic , Bulgarian , Danish , German , Greek , English , Spanish , Finnish , French , Hebrew , Hindi , Hungarian , Indonesian , Italian , Japanese , Korean , Dutch , Polish , Portuguese , Russian , Swedish , Swahili , Thai , Turkish , Urdu , Vietnamese andChinese .", "entities": [[79, 80, "DatasetName", "Urdu"]]}, {"text": "All the bilingual pairs are English to another language .", "entities": []}, {"text": "2.1.2 Large Corpus ( LC ) Multilingual Corpus Following Wenzek et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) , we construct a clean version of Common Crawl ( CC)2as the multilingual corpus .", "entities": [[10, 12, "DatasetName", "Common Crawl"]]}, {"text": "First , we use a language identi\ufb01cation model trained based on Wikipedia to classify the language of each page in CC .", "entities": []}, {"text": "Then , we train a language model for each language using the corresponding part of the Wikipedia corpus , and use it to \ufb01lter documents as Wenzek et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) did .", "entities": []}, {"text": "We use one CC dump for English and twelve CC dumps for other languages .", "entities": []}, {"text": "It leads to a 2,500 G multilingual corpus covering 89 languages .", "entities": []}, {"text": "We also include the 101 G multilingual corpus described in Section 2.1.1 .", "entities": []}, {"text": "Bilingual Corpus We reuse the bilingual corpus described in Section 2.1.1 .", "entities": []}, {"text": "We will add CCMatrix ( Schwenk et al . , 2019 ) in the future .", "entities": [[3, 4, "DatasetName", "CCMatrix"]]}, {"text": "2.2 Downstream Tasks We select 11 cross - lingual tasks in XGLUE , which are categorized into 3 groups : single - input understanding tasks , pair - input understanding tasks , and generation tasks .", "entities": [[11, 12, "DatasetName", "XGLUE"]]}, {"text": "For each task , training set is only available in English .", "entities": []}, {"text": "In order to obtain a good performance on XGLUE , a model should be able to learn how to do a task well using its English training set , and then transfer this ability to test sets in other languages .", "entities": [[8, 9, "DatasetName", "XGLUE"]]}, {"text": "Table 2 gives the dataset statistics and Table 3 lists languages covered by all tasks .", "entities": []}, {"text": "2https://commoncrawl.org/.2.2.1 Single - input Understanding Tasks NER We select a subset of the following two NER tasks , CoNLL-2002 NER ( Sang , 2002 ) and CoNLL-2003 NER ( Sang and De Meulder , 2003 ) , to form this cross - lingual NER dataset .", "entities": [[6, 7, "TaskName", "NER"], [15, 16, "TaskName", "NER"], [19, 20, "TaskName", "NER"], [26, 27, "DatasetName", "CoNLL-2003"], [27, 28, "TaskName", "NER"], [40, 44, "TaskName", "cross - lingual NER"]]}, {"text": "It covers 4 languages , including English , German , Spanish andDutch , and 4 types of named entities , including Person , Location , Organization andMiscellaneous entities that do not belong to the previous three types .", "entities": []}, {"text": "F1 score is used as the metric .", "entities": [[0, 2, "MetricName", "F1 score"]]}, {"text": "POS Tagging ( POS ) Following ( Kim et al . , 2017 ) , we select a subset of Universal Dependencies ( UD ) Treebanks ( v2.5 ) ( Zeman et al . , 2019 ) , which covers 18 languages .", "entities": [[20, 22, "DatasetName", "Universal Dependencies"], [23, 24, "DatasetName", "UD"]]}, {"text": "Accuracy ( ACC ) of the predicted POS tags is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"]]}, {"text": "News Classi\ufb01cation ( NC )", "entities": []}, {"text": "This task aims to predict the category given a news article .", "entities": []}, {"text": "It covers 5 languages , including English , Spanish , French , German andRussian .", "entities": []}, {"text": "Each labeled instance is a 3 - tuple:<news title , news body , category > .", "entities": []}, {"text": "The category number is 10 .", "entities": []}, {"text": "We crawl this dataset from Microsoft News ( MSN ) .", "entities": []}, {"text": "Accuracy ( ACC ) of the multi - class classi\ufb01cation is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"]]}, {"text": "2.2.2", "entities": []}, {"text": "Pair - input Understanding Tasks MLQA The MLQA ( Lewis et al . , 2019b ) is a multilingual machine reading comprehension task , which contains QA annotations labeled in 7 languages , including English , Arabic , German , Spanish , Hindi , Vietnamese andChinese .", "entities": [[5, 6, "DatasetName", "MLQA"], [7, 8, "DatasetName", "MLQA"], [19, 22, "TaskName", "machine reading comprehension"]]}, {"text": "F1 score of the predicted answers is used as the metric .", "entities": [[0, 2, "MetricName", "F1 score"]]}, {"text": "XNLI", "entities": [[0, 1, "DatasetName", "XNLI"]]}, {"text": "We reuse the original XNLI dataset ( Conneau et al . , 2018 ) in XGLUE .", "entities": [[4, 5, "DatasetName", "XNLI"], [15, 16, "DatasetName", "XGLUE"]]}, {"text": "PA WS - X The PAWS - X ( Yang et al . , 2019a ) is a paraphrase identi\ufb01cation dataset , which extends the Wikipedia portion of the PAWS ( Zhang et al . , 2019 ) evaluation to more languages .", "entities": [[5, 8, "DatasetName", "PAWS - X"], [29, 30, "DatasetName", "PAWS"]]}, {"text": "We select 4 languages , including English , Spanish , French and German , from the original dataset and use them in XGLUE .", "entities": [[22, 23, "DatasetName", "XGLUE"]]}, {"text": "Accuracy ( ACC ) of the binary classi\ufb01cation is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"]]}, {"text": "Query - Ad Matching ( QADSM )", "entities": []}, {"text": "This task aims to predict whether an advertisement ( ad ) is relevant to an input query .", "entities": []}, {"text": "It covers 3 languages , includingEnglish , French andGerman .", "entities": []}, {"text": "Each labeled instance is a 4 - tuple : < query , ad title , ad description , label > .", "entities": []}, {"text": "The label indicates whether the ad is relevant to the query ( Good ) , or not ( Bad ) .", "entities": []}, {"text": "We con-", "entities": []}, {"text": "6010Task # of Languages jTrainjenjDevjavgjTestjavgMetric Data Source NER 4 15.0 K 2.8 K 3.4 K F1 ECI Multilingual Text Corpus POS 18 25.4 K 1.0 K 0.9 K ACC UD Tree - banks ( v2.5 ) NC\u00035", "entities": [[7, 8, "TaskName", "NER"], [15, 16, "MetricName", "F1"], [28, 29, "MetricName", "ACC"], [29, 30, "DatasetName", "UD"]]}, {"text": "100 K 10 K 10 K ACC MSN MLQA 7 87.6 K 0.6 K 5.7 K F1 Wikipedia XNLI 15 433 K 2.5 K 5 K ACC MultiNLI Corpus PAWS - X 4 49.4 K 2 K 2 K ACC Wikipedia QADSM\u00033", "entities": [[6, 7, "MetricName", "ACC"], [8, 9, "DatasetName", "MLQA"], [16, 17, "MetricName", "F1"], [18, 19, "DatasetName", "XNLI"], [26, 27, "MetricName", "ACC"], [27, 28, "DatasetName", "MultiNLI"], [29, 32, "DatasetName", "PAWS - X"], [39, 40, "MetricName", "ACC"]]}, {"text": "100 K 10 K 10 K ACC Bing WPR\u00037 100 K 10 K 10 K nDCG Bing QAM\u00033", "entities": [[6, 7, "MetricName", "ACC"]]}, {"text": "100 K 10 K 10 K ACC Bing QG\u00036 100 K 10 K 10 K BLEU-4 Bing NTG\u00035", "entities": [[6, 7, "MetricName", "ACC"], [15, 16, "MetricName", "BLEU-4"]]}, {"text": "300 K 10 K 10 K BLEU-4 MSN Table 2 : 11 downstream tasks in XGLUE .", "entities": [[6, 7, "MetricName", "BLEU-4"], [15, 16, "DatasetName", "XGLUE"]]}, {"text": "For each task , training set is only available in English .", "entities": []}, {"text": "jTrainjendenotes the number of labeled instances in the training set .", "entities": []}, {"text": "jDevjavgandjTestjavgdenote the average numbers of labeled instances in the dev sets and test sets , respectively .", "entities": []}, {"text": "\u0003denotes the corresponding dataset is constructed by this paper .", "entities": []}, {"text": "Task ar bg de el en es", "entities": []}, {"text": "fr", "entities": []}, {"text": "hi it", "entities": []}, {"text": "nl pl pt ru sw th tr ur vi zh NER X X X X POS X X X X X X X X X X X X X X X X X X NC\u0003X X X X X MLQA X X X X X X X XNLI X X X X X X X X X X X X X X X PAWS - X X X X X QADSM\u0003X X X WPR\u0003X X X X X X X QAM\u0003X X X QG\u0003X X X X X X NTG\u0003X X X X X Table 3 : The 19 languages covered by the 11 downstream tasks : Arabic ( ar ) , Bulgarian ( bg ) , German ( de ) , Greek ( el),English ( en ) , Spanish ( es ) , French ( fr),Hindi ( hi ) , Italian ( it),Dutch ( nl ) , Polish ( pl ) , Portuguese ( pt ) , Russian ( ru ) , Swahili ( sw ) , Thai ( th ) , Turkish ( tr),Urdu ( ur ) , Vietnamese ( vi ) , and Chinese ( zh ) .", "entities": [[10, 11, "TaskName", "NER"], [39, 40, "DatasetName", "MLQA"], [47, 48, "DatasetName", "XNLI"], [63, 66, "DatasetName", "PAWS - X"]]}, {"text": "All these 6 new tasks with \u0003 are labeled by human , except es , it and pt datasets in QG ( 80+% accuracy ) are obtained by an in - house QA ranker .", "entities": [[23, 24, "MetricName", "accuracy"]]}, {"text": "struct this dataset based on Bing .", "entities": []}, {"text": "Accuracy ( ACC ) of the binary classi\ufb01cation is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"]]}, {"text": "Web Page Ranking ( WPR ) This task aims to predict whether a web page is relevant to an input query .", "entities": []}, {"text": "It covers 7 languages , including English , German , French , Spanish , Italian , Portuguese andChinese .", "entities": []}, {"text": "Each labeled instance is a 4 - tuple : < query , web page title , web page snippet , label > .", "entities": []}, {"text": "The relevance label contains 5 ratings : Perfect ( 4 ) , Excellent ( 3 ) , Good ( 2 ) , Fair ( 1 ) and Bad ( 0 ) .", "entities": [[29, 30, "DatasetName", "0"]]}, {"text": "We construct this dataset based on Bing .", "entities": []}, {"text": "Normalize Discounted Cumulative Gain ( nDCG ) is used as the metric .", "entities": []}, {"text": "QA Matching ( QAM )", "entities": []}, {"text": "This task aims to predict whether a < question , passage > pair is a QA pair .", "entities": []}, {"text": "It covers 3 languages , including English , French andGerman .", "entities": []}, {"text": "Each labeled instance is a 3 - tuple : < question , passage , label > .", "entities": []}, {"text": "The label indicates whether the passage is the answer of the question(1 ) , or not ( 0 ) .", "entities": [[17, 18, "DatasetName", "0"]]}, {"text": "We construct this dataset based on Bing .", "entities": []}, {"text": "Accuracy ( ACC ) of the binary classi\ufb01cation is used as the metric .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "MetricName", "ACC"]]}, {"text": "2.2.3 Generation Tasks Question Generation ( QG )", "entities": [[3, 5, "TaskName", "Question Generation"]]}, {"text": "This task aims to generate a question for a given passage .", "entities": []}, {"text": "We collect < passage , question > pairs from Bing .", "entities": []}, {"text": "It covers 6 languages , including English , French , German , Spanish , Italian andPortuguese .", "entities": []}, {"text": "BLEU-4 score is used as the metric .", "entities": [[0, 1, "MetricName", "BLEU-4"]]}, {"text": "News Title Generation ( NTG )", "entities": []}, {"text": "This task aims to generate a proper title for a given news body .", "entities": []}, {"text": "We collect < news body , news title > pairs from Microsoft News ( MSN ) .", "entities": []}, {"text": "It covers 5 languages , includingGerman , English , French , Spanish andRussian .", "entities": []}, {"text": "BLEU-4 score is used as the metric .", "entities": [[0, 1, "MetricName", "BLEU-4"]]}, {"text": "60113 Pre - train Unicoder for Cross - lingual Understanding Tasks We select Unicoder ( Huang et al . , 2019 ) as the backbone model .", "entities": []}, {"text": "Section 3 introduces a simpli\ufb01ed version of Unicoder using two pre - training tasks ( MLN and TLM ) for cross - lingual understanding tasks .", "entities": []}, {"text": "Section 4 describes how to extend Unicoder to cover cross - lingual generation tasks .", "entities": []}, {"text": "The original Unicoder ( Huang et al . , 2019 ) includes more pre - training tasks besides MLM and TLM .", "entities": [[18, 19, "DatasetName", "MLM"]]}, {"text": "But to keep the baseline pre - trained model simple and to reduce the experimental cost , we just use MLM and TLM in this paper .", "entities": [[20, 21, "DatasetName", "MLM"]]}, {"text": "It means for understanding tasks , Unicoder is almost equal to XLM , except some hyper - parameter differences .", "entities": [[11, 12, "MethodName", "XLM"]]}, {"text": "3.1 Masked Language Model ( MLM ) Following Devlin et al . ( 2019 ) , this task extends the masked language model task to multiple languages .", "entities": [[5, 6, "DatasetName", "MLM"]]}, {"text": "At each iteration , a batch is composed of sentences sampled from different languages .", "entities": []}, {"text": "The sampling probability of a language liis de\ufb01ned as \u0015li = p \u000b  li = P lip \u000b  li , wherepliis the percentage of the language liin the entire corpus , the smoothing factor \u000b is set to 0.3 .", "entities": []}, {"text": "For each batch , we randomly sample 15 % of the words and replace them with ( i ) a special symbol [ MASK ] , ( ii ) a random token or ( iii ) keep them unchanged with probability 80 % , 10 % and 10 % , respectively .", "entities": []}, {"text": "For each token , we only use its token embedding and position embedding , and discard segment embedding and language embedding .", "entities": []}, {"text": "3.2 Translation Language Model ( TLM ) Following Conneau and Lample ( 2019 ) , this task extends the MLM task to bilingual corpus .", "entities": [[1, 2, "TaskName", "Translation"], [19, 20, "DatasetName", "MLM"]]}, {"text": "Given a bilingual sentence pair , TLM \ufb01rst concatenates them into a single sentence , and then masks words using the same strategy of MLM .", "entities": [[24, 25, "DatasetName", "MLM"]]}, {"text": "The pre - trained model learns to recover each masked word based on the bilingual context .", "entities": []}, {"text": "We follow MLM to sample language pairs in each batch with \u000b = 0:3 .", "entities": [[2, 3, "DatasetName", "MLM"]]}, {"text": "4 Pre - train Unicoder for Cross - lingual Generation Tasks", "entities": []}, {"text": "The encoder - decoder architecture is employed to extend Unicoder to generation tasks , where the BPE embeddings are shared between encoder and decoder .", "entities": [[16, 17, "MethodName", "BPE"]]}, {"text": "Two separate generative tasks are proposed for Unicoder pre - training : Multilingual De - noising Auto - Encoding ( xDAE ) andMultilingual Future N - gram Prediction ( xFNP ) .", "entities": []}, {"text": "4.1 Multilingual Denoising Auto - Encoding ( xDAE ) Motivated by BART ( Lewis et al . , 2019a ) , xDAE aims to predict the original text X= ( x1;x2;:::;xjXj)2lifrom a language libased on its corrupted form c(X ) , wherec(X)is a noising function that corrupts an input text Xas its output .", "entities": [[2, 3, "TaskName", "Denoising"], [11, 12, "MethodName", "BART"]]}, {"text": "Four different text noising strategies for c(\u0001)are explored in this paper .", "entities": []}, {"text": "( 1 ) Shuf\ufb02e the input text X by adding a noise \u000b \u0018U(0;3)to the input indices and then re - ordering Xbased on the rank of the noised indices .", "entities": []}, {"text": "( 2 ) Drop words with a probability of 0.1 .", "entities": []}, {"text": "( 3 ) Replace 10 % of the input words in X with the [ MASK ] symbol .", "entities": []}, {"text": "( 4 ) Sample a number of token spans from Xwith span lengths drawn from a Poisson distribution ( \u0015= 3 ) , and then replace each token span with a single [ MASK ] token .", "entities": []}, {"text": "Here , 0 - length spans correspond to the insertion of [ MASK ] tokens .", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "Based on the performance of different noising strategies ( Table 10 ) , we select ( 4 ) and use it in pre - training .", "entities": []}, {"text": "We leave \ufb01nding better text noising strategies for future work .", "entities": []}, {"text": "We train Unicoder using this task by maximizing the following loss function LxDAE : LxDAE = X li2LX X2lijXjX t=1logp(xtjx < t;c(X ) )", "entities": [[10, 11, "MetricName", "loss"]]}, {"text": "whereL = l1;:::;lNdenotesNlanguages , Xis an instance in the ithlanguageli , p(xtjx < t;c(X ) ) denotes the probability of generating a single token xtat time steptgivenc(X)andx < t. 4.2 Multilingual Future N - gram Prediction ( xFNP ) Motivated by ProphetNet ( Yan et al . , 2020 ) , xFNP introduces a future n - gram prediction mechanism to natural language generation .", "entities": [[41, 42, "MethodName", "ProphetNet"], [43, 46, "DatasetName", "Yan et al"]]}, {"text": "It encourages the model to plan for the future tokens explicitly and prevents over-\ufb01tting on strong local correlations .", "entities": []}, {"text": "Given an input text X= ( x1;x2;:::;xjXj)2li from a language li , we randomly mask ktoken spans ofXto generate the masked text X0as", "entities": []}, {"text": "the input , and concatenate all masked token spans into Yas the output .", "entities": []}, {"text": "Details of this mask strategy are described in Section 6.1 .", "entities": []}, {"text": "After this , xFNP", "entities": []}, {"text": "\ufb01rst encodesX0toHencwith", "entities": []}, {"text": "the encoder : Henc= Encoder ( X0 )", "entities": []}, {"text": "6012Then , instead of predicting the next token only at each time step , xFNP generates nfuture tokens simultaneously at time step twith the decoder : p(ytjy < t;X0);:::;p ( yt+n\u00001jy < t;X0 ) = Decoder ( y < t;Henc ) Following Yan", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2020 ) , we set n=", "entities": []}, {"text": "2 .", "entities": []}, {"text": "We train Unicoder using this task by maximizing the following loss function LxFNP :", "entities": [[10, 11, "MetricName", "loss"]]}, {"text": "LxFNP = X li2LX X2lif \u000b 0\u0001jYjX t=1logp(ytjy < t;X0 ) + \u000b 1\u0001jYj\u00001X t=1logp(yt+1jy < t;X0)g whereX0andYare generated from Xbased on the method mentioned above .", "entities": []}, {"text": "Following Yan et al .", "entities": [[1, 4, "DatasetName", "Yan et al"]]}, {"text": "( 2020 ) , we set \u000b 0= \u000b 1= 1 . 5 Experiments 5.1 Data Labeling For tasks QADSM , WPR , QAM and QG , we label the data on an Microsoft internal crowdsourcing platform .", "entities": []}, {"text": "Each labeler must learn the guideline and pass the labeling test .", "entities": []}, {"text": "Each sample is labeled by three labeler .", "entities": []}, {"text": "We only keep the samples with two or three labeler have same label .", "entities": []}, {"text": "For tasks NC and NTG , we directly use the category label on MSN website .", "entities": []}, {"text": "All the category label on MSN is review by human .", "entities": []}, {"text": "5.2 Experimental Settings Understanding Tasks The hyper - parameters are set as follows : 768 hidden units , 12 heads , GELU activation , a dropout rate of 0.1 , 512 max input length , 12 layers in encoder .", "entities": [[21, 22, "MethodName", "GELU"]]}, {"text": "In the pre - training stage , we \ufb01rst initialize UnicoderLCwith XLM - R base ( Conneau et al . , 2019 ) , and then run continue pre - training with the accumulated 8,192 batch size with gradients accumulation .", "entities": [[11, 12, "MethodName", "XLM"], [35, 37, "HyperparameterName", "batch size"]]}, {"text": "We use Adam Optimizer with a linear warm - up and set the learning rate to 3e-5 .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "Optimizer"], [13, 15, "HyperparameterName", "learning rate"]]}, {"text": "We select different understanding tasks randomly in different batches .", "entities": []}, {"text": "This costed 12 days on 16 V100 .", "entities": []}, {"text": "In the \ufb01ne - tuning stage , the batch size is set to 32 .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}, {"text": "We use Adam Optimizer ( Kingma and Ba , 2014 ) with warm - up and set the learning rate to 5e-6 .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "Optimizer"], [18, 20, "HyperparameterName", "learning rate"]]}, {"text": "For all sentence classi\ufb01cation tasks , we \ufb01netune 10 epochs .", "entities": []}, {"text": "For POS Tagging and NER , we\ufb01ne - tune 20 epochs .", "entities": [[4, 5, "TaskName", "NER"]]}, {"text": "And for POS Tagging , we set the learning rate to 2e-5 .", "entities": [[8, 10, "HyperparameterName", "learning rate"]]}, {"text": "For MLQA , we set the learning rate to 3e-5 , batch size to 12 and train 2 epochs following BERT for SQuAD .", "entities": [[1, 2, "DatasetName", "MLQA"], [6, 8, "HyperparameterName", "learning rate"], [11, 13, "HyperparameterName", "batch size"], [20, 21, "MethodName", "BERT"], [22, 23, "DatasetName", "SQuAD"]]}, {"text": "After each epoch , we test the \ufb01ne - tuned model on the dev sets of all languages .", "entities": []}, {"text": "We select the model with the best average result on the dev sets of all languages .", "entities": []}, {"text": "Generation Tasks We evaluate UnicoderxDAE SC and UnicoderxFNP SC as two separate models .", "entities": []}, {"text": "For UnicoderxDAE SC , the hyper - parameters are set as follows : 768 hidden units , 12 heads , GELU activation , a dropout rate of 0.1 , 512 max input length , 12 layers in encoder , 12 layers in decoder .", "entities": [[20, 21, "MethodName", "GELU"]]}, {"text": "In the pre - training stage , we \ufb01rst initialize encoder and decoder with XLM - R , and then run continue pre - training with 1,024 batch size .", "entities": [[14, 15, "MethodName", "XLM"], [27, 29, "HyperparameterName", "batch size"]]}, {"text": "We use Adam optimizer with warm - up and set the learning rate to 2e-4 .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [11, 13, "HyperparameterName", "learning rate"]]}, {"text": "This costed 10 days on 16 V100 .", "entities": []}, {"text": "In the \ufb01ne - tuning stage , the batch size is 1024 .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}, {"text": "We use Adam Optimizer with learning rate 1e-5 and warm - up steps 2000 .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "Optimizer"], [5, 7, "HyperparameterName", "learning rate"]]}, {"text": "For UnicoderxFNP SC , the hyper - parameters are set as follows : 1,024 hidden size , 12 layers in encoder , 12 layers in decoder , 512 max input length .", "entities": []}, {"text": "In the pre - training stage , we pre - train the model from scratch and follow ProphetNet ( Yan et al . , 2020 ) to randomly mask a continuous span ( with a \ufb01xed length 9 ) in every 64 tokens .", "entities": [[17, 18, "MethodName", "ProphetNet"], [19, 22, "DatasetName", "Yan et al"]]}, {"text": "About 15 % of the tokens in original sequence are masked in this step .", "entities": []}, {"text": "We use a special symbol [ MASK ] to replace 80 % of the masked tokens , keep 10 % unchanged , and random replace 10 % of the masked tokens .", "entities": []}, {"text": "We set the batch size to 1,024 , training steps to 350,000 .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}, {"text": "The learning rate is set to 1e-4 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "We set the number of future tokens nto 2 .", "entities": []}, {"text": "In the \ufb01ne - tuning stage , we use Adam Optimizer and set the learning rate to 1e-4 .", "entities": [[9, 10, "MethodName", "Adam"], [10, 11, "HyperparameterName", "Optimizer"], [14, 16, "HyperparameterName", "learning rate"]]}, {"text": "We set the batch size to 64 and the warm - up steps to 1,000 .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}, {"text": "5.3 Main Result 7 cross - lingual pre - trained models are evaluated on XGLUE and compared in Table 4 : 12 - layer MBERT ( Devlin et al . , 2019 ) trained on Wikipedia corpus for 102 languages , 12 - layer XLM ( Conneau and Lample , 2019 ) trained on Wikipedia and bilingual corpora for 15 languages , 12 - layer XLM - Rbase(Conneau et al . , 2019 ) trained on Common Crawl corpus for 100 languages , 12 - layer UnicoderSCtrained on small corpus for 100 languages , 12 - layer Unicoder LCtrained on large corpus for 100 languages , 12 - layer UnicoderxDAE SC and", "entities": [[14, 15, "DatasetName", "XGLUE"], [24, 25, "MethodName", "MBERT"], [44, 45, "MethodName", "XLM"], [65, 66, "MethodName", "XLM"], [76, 78, "DatasetName", "Common Crawl"]]}, {"text": "6013Task Model ar bg de el en es", "entities": []}, {"text": "fr", "entities": []}, {"text": "hi it", "entities": []}, {"text": "nl pl pt ru sw th tr ur vi zh A VG NERM - BERT - - 69.2 - 90.6 75.4 - - - 77.9 - - - - - - - - - 78.2 XLM - R base - - 70.4 - 90.9 75.2 - - - 79.5 - - - - - - - - - 79.0", "entities": [[14, 15, "MethodName", "BERT"], [35, 36, "MethodName", "XLM"]]}, {"text": "Unicoder LC - - 71.8 - 91.1 74.4 - - - 81.6 - - - - - - - - - 79.7 POSM - BERT 52.4 85.0 88.7 81.5 95.6 86.8 87.6 58.4 91.3 88.0 81.8 88.3 78.8 - 43.3 69.2 53.8 54.3 58.3 74.7 XLM - R base 67.3 88.8 92.2 88.2 96.2 89.0 89.9 74.5 92.6 88.5 85.4 89.7 86.9 - 57.9 72.7 62.1 55.2 60.4 79.8 Unicoder LC 68.6 88.5 92.0 88.3 96.1 89.1 89.4 69.9 92.5 88.9 83.6 89.8 86.7 - 57.6 75.0 59.8 56.3 60.2 79.6 NCM - BERT - - 82.6 - 92.2 81.6 78.0 - - - - - 79.0 - - - - - - 82.7 XLM - R base - - 84.5 - 91.8 83.2 78.2 - - - - - 79.4 - - - - - - 83.4 Unicoder LC - - 84.2 - 91.7 83.5 78.5 - - - - - 79.7 - - - - - - 83.5 MLQAM - BERT 50.9 - 63.8 - 80.5 67.1 - 47.9 - - - - - - - - - 59.5 55.4 60.7 XLM - R base 56.4 - 62.1 - 80.1 67.9 - 60.5 - - - - - - - - - 67.1 61.4 65.1 Unicoder LC 57.8 - 62.7 - 80.6 68.6 - 62.7 - - - - - - - - - 67.5 62.1 66.0 XNLIM - BERT 64.9 68.9 71.1 66.4 82.1 74.3 73.8 60.0 - - - - 69.0 50.4 55.8 61.6 58.0 69.5 69.3 66.3 XLMy 73.1 77.4 77.8 76.6 85.0 78.9 78.7 69.6 - - - - 75.3 68.4 73.2 72.5 67.3 76.1 76.5 75.1 XLM - R base", "entities": [[24, 25, "MethodName", "BERT"], [45, 46, "MethodName", "XLM"], [93, 94, "MethodName", "BERT"], [114, 115, "MethodName", "XLM"], [162, 163, "MethodName", "BERT"], [183, 184, "MethodName", "XLM"], [231, 232, "MethodName", "BERT"], [273, 274, "MethodName", "XLM"]]}, {"text": "72.1 77.5 77.0 75.9 84.6 79.2 78.2 69.8 - - - - 75.5 64.7 71.6 72.9 65.1 74.8 73.7 74.2 Unicoder SC 68.5 73.2 71.6 71.6 82.9 75.0 74.7 66.0 - - - - 70.6 64.1 67.0 68.7 62.5 71.2 69.7 70.5 Unicoder LC 73.9 78.5 78.2 77.3 85.4 79.8 79.2 70.1 - - - - 76.7 67.4 71.8 73.8 66.3 75.9 74.7 75.3 PAWS - XM - BERT - - 82.9 - 94.0 85.9 86.0 - - - - - - - - - - - - 87.2 XLM - R base - - 86.9 - 94.4 88.0 88.7 - - - - - - - - - - - - 89.5 Unicoder LC - - 87.4 - 94.9 88.8 89.3 - - - - - - - - - - - - 90.1 QADSMM - BERT - - 60.3 - 68.3 - 64.1 - - - - - - - - - - - - 64.2 XLM - R base - - 65.8 - 71.7 - 68.3 - - - - - - - - - - - - 68.6 Unicoder LC - - 64.6 - 71.8 - 68.7 - - - - - - - - - - - - 68.4 WPRM - BERT - - 76.6 - 78.1 75.3 74.2 - 70.1 - - 76.6 - - - - - - 64.5 73.5 XLM - R base - - 77.6 - 78.2 76.0 74.4 - 70.7 - - 77.3 - - - - - - 63.9 73.8 Unicoder LC - - 77.2 - 78.4 75.7 74.9 - 70.3 - - 77.4 - - - - - - 64.4 73.9 QAMM - BERT - - 64.7 - 67.5 - 66.0 - - - - - - - - - - - - 66.1 XLM - R base - - 68.1 - 69.3 - 67.8 - - - - - - - - - - - - 68.4 Unicoder LC - - 68.4 - 69.9 - 68.4 - - - - - - - - - - - - 68.9 A VG2 UM - BERT 72.6 XLM - R base", "entities": [[64, 65, "DatasetName", "PAWS"], [68, 69, "MethodName", "BERT"], [89, 90, "MethodName", "XLM"], [137, 138, "MethodName", "BERT"], [158, 159, "MethodName", "XLM"], [206, 207, "MethodName", "BERT"], [227, 228, "MethodName", "XLM"], [275, 276, "MethodName", "BERT"], [296, 297, "MethodName", "XLM"], [346, 347, "MethodName", "BERT"], [348, 349, "MethodName", "XLM"]]}, {"text": "75.8 Unicoder LC 76.2 QGM - BERT - - 0.1 - 7.8 0.1 0.1 - 0.2 - - 0.1 - - - - - - - 1.4 XLM - R base - - 0.1 - 6.0 0.0 0.0 - 0.1 - - 0.0 - - - - - - - 1.0 UnicoderxDAE SC - - 3.0 - 14.0 12.4 4.2 - 15.8 - - 8.3 - - - - - - - 9.6 UnicoderxFNP SC - - 3.7 - 13.9 14.8 4.9 - 17.0 - - 9.5 - - - - - - - 10.6 NTGM - BERT - - 0.7 - 9.0 0.4 0.4 - - - - - 0.0 - - - - - - 2.1 XLM - R base - - 0.6 - 8.1 0.4 0.3 - - - - - 0.0 - - - - - - 1.9 UnicoderxDAE SC - - 6.8 - 15.6 9.0 8.7 - - - - - 7.7 - - - - - - 9.6 UnicoderxFNP SC - - 7.5 - 15.8 11.9 9.9 - - - - - 8.4 - - - - - - 10.7 A VG2 GM - BERT 1.8 XLM - R base 1.5 UnicoderxDAE SC 9.6 UnicoderxFNP SC 10.7 Table 4 : The overall evaluation results on XGLUE .", "entities": [[6, 7, "MethodName", "BERT"], [27, 28, "MethodName", "XLM"], [97, 98, "MethodName", "BERT"], [118, 119, "MethodName", "XLM"], [190, 191, "MethodName", "BERT"], [192, 193, "MethodName", "XLM"], [211, 212, "DatasetName", "XGLUE"]]}, {"text": "We use M - BERT ( Devlin et al . , 2019 ) , XLM ( Conneau and Lample , 2019 ) and XLM - R base(Conneau et al . , 2019 ) as baselines .", "entities": [[4, 5, "MethodName", "BERT"], [14, 15, "MethodName", "XLM"], [23, 24, "MethodName", "XLM"]]}, {"text": "Unicoder SCand Unicoder LCare pre", "entities": []}, {"text": "- trained using small corpus and large corpus , respectively .", "entities": []}, {"text": "UnicoderxDAE SC and UnicoderxFNP SC are pre - trained by xDAE and xFNP", "entities": []}, {"text": "for 100 languages , respectively .", "entities": []}, {"text": "For the results of M - BERT / XLM - R on generation tasks , we initialize the encoder - decoder model with M - BERT / XLM - R and \ufb01ne - tune it on each downstream task without pre - training .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "XLM"], [25, 26, "MethodName", "BERT"], [27, 28, "MethodName", "XLM"]]}, {"text": "All models are ( 12 - layer ) based ones .", "entities": []}, {"text": "Given a task , each pre - trained model is \ufb01ne - tuned using its English training set only , and then applied to all test sets in different languages .", "entities": []}, {"text": "A VG2 Uand A VG2 Gdenote the average score of the average scores on 9 understanding tasks and 2 generation tasks , respectively .", "entities": []}, {"text": "Pivot en fr es de el bg ru tr ar vi th zh hi sw", "entities": []}, {"text": "ur", "entities": []}, {"text": "A VG en 85.4 79.2 79.8 78.2 77.3 78.5 76.7 73.8 73.9 75.9 71.8 74.7 70.1 67.4 66.3 75.3 fr 84.0 79.9 80.3 78.8 77.4 79.2 77.0 73.6 73.7 76.7 72.7 75.3 73.0 67.4 68.3 75.8 es 84.5 80.2 81.2 79.7 78.2 79.2 77.6 74.5 74.8 77.0 72.8 76.2 73.2 67.7 69.6 76.4 de 83.5 79.1 80.1 80.2 77.9 78.6 77.0 74.9 74.6 76.1 73.3 76.2 73.1 67.7 68.9 76.1 el 83.8 80.1 81.0 78.6 79.6 79.3 77.0 74.2 74.9 77.1 73.5 75.9 72.7 69.1 69.1 76.4 bg 83.5 79.6 80.4 79.1 77.9 80.5 77.9 74.9 73.9 76.5 73.9 75.6 72.8 68.6 68.9 76.3 ru 84.1 79.9 79.9 78.8 77.5 79.9 78.1 73.9 74.5 77.1 73.8 75.7 73.1 68.5 69.0 76.2 tr 83.3 78.4 79.6 78.4 77.5 79.2 77.5 77.1 74.2 77.1 74.5 76.5 73.7 69.3 70.3 76.4 ar 83.2 78.9 79.5 77.6 77.4 78.6 77.0 75.4 76.8 76.8 74.0 76.0 73.0 69.5 69.3 76.2 vi 83.2 78.6 79.1 77.7 76.6 78.9 77.5 75.3 74.7 78.5 73.5 76.8 73.1 67.8 69.0 76.0 th 82.5 78.5 79.1 77.8 77.1 78.3 76.7 75.0 74.3 76.9 76.4 76.2 72.9 68.4 69.7 76.0 zh 81.6 78.2 77.9 77.1 76.0 77.9 76.2 73.7 73.7 75.8 73.6 76.6 71.7 67.4 68.3 75.1 hi 81.8 78.5 79.2 76.7 77.2 78.2 76.2 74.5 73.9 76.4 71.7 75.2 73.8 68.2 68.5 75.3 sw 82.0 77.6 78.8 77.2 76.5 77.7 76.2 74.4 74.3 76.3 74.0 75.2 72.2 71.4 69.5 75.6 ur 76.7 72.5 74.1 72.6 72.1 73.9 72.7 69.7 69.7 72.8 70.1 72.4 69.0 66.0 67.5 71.5 Table 5 : Impacts of different pivot languages on XNLI .", "entities": [[266, 267, "DatasetName", "XNLI"]]}, {"text": "Given each pivot language , the corresponding \ufb01ne - tuned XNLI results on all languages are listed in the same row .", "entities": [[10, 11, "DatasetName", "XNLI"]]}, {"text": "Each bolded number is the best result in that column .", "entities": []}, {"text": "6014Pivot en es fr de ru A VG en 15.6/15.8 9.0/11.9 8.7/9.9 6.8/7.5 7.7/8.4 9.6/10.7 es 7.8/8.8 17.1/17.1 10.6/10.9 7.6/8.0 8.0/8.6 10.2/10.7 fr 8.2/8.7 11.4/12.5 19.4/20.9 8.3/8.2 7.6/7.8 11.0/11.6 de 8.2/8.6 9.9/11.2 9.5/10.2 14.1/13.7 8.4/8.0 10.0/10.3 ru 6.9/7.4 9.3/10.8 8.8/9.9 6.9/7.0 16.6/16.7 9.7/10.4", "entities": []}, {"text": "Table 6 : Impacts of different pivot languages on NTG .", "entities": []}, {"text": "UnicoderxDAE SC /UnicoderxFNP SC evaluated by BLEU-4 .", "entities": [[6, 7, "MetricName", "BLEU-4"]]}, {"text": "12 - layer UnicoderxFNP SC trained on Wikipedia corpus for 100 languages .", "entities": []}, {"text": "Given a downstream task , each pre - trained model is \ufb01ne - tuned using its English training set and then applied to all test sets in different languages .", "entities": []}, {"text": "Note that , all results are reproduced by this paper , except the XLM yresults on XNLI are from Conneau and Lample ( 2019 ) .", "entities": [[13, 14, "MethodName", "XLM"], [16, 17, "DatasetName", "XNLI"]]}, {"text": "We \ufb01nd ( 1 ) Unicoder LCperforms slightly better than M - BERT and XLM - R baseon the 9 understanding tasks , as it is pre - trained based on multilingual and bilingual corpora at the same time and uses TLM ; ( 2 ) Unicoder LCperforms better than UnicoderSC , as it is pre - trained based on the larger corpus ; ( 3 ) UnicoderxDAE SC and UnicoderxFNP SC show good cross - lingual transfer capabilities and perform signi\ufb01cantly better than M - BERT and XLM - Rbaseon the 2 generation tasks .", "entities": [[12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "XLM"], [74, 78, "TaskName", "cross - lingual transfer"], [86, 87, "MethodName", "BERT"], [88, 89, "MethodName", "XLM"]]}, {"text": "It proves the importance of introducing generation tasks into pre - training for cross - lingual text generation ; ( 4 ) UnicoderxFNP SC performs slightly better than UnicoderxDAE SC .", "entities": [[16, 18, "TaskName", "text generation"]]}, {"text": "But it is not a fair comparison , because they use different text denoising tasks ( sentence prediction vs. span prediction ) and different generation mechanisms ( single - token prediction vs. multi - token prediction ) .", "entities": [[13, 14, "TaskName", "denoising"]]}, {"text": "We leave combining these two tasks for future work .", "entities": []}, {"text": "5.4 Ablation Study 5.4.1 Pivot - language Fine - tuning We de\ufb01ne pivot - language ( pl ) \ufb01ne - tuning as \ufb01netune a pre - trained model for a downstream task using its labeled data in a pivot language ( e.g. English ) and then apply the \ufb01ne - tuned model to all languages .", "entities": []}, {"text": "Table 4 chooses English as the pivot language , as all tasks in XGLUE have labeled data in English .", "entities": [[13, 14, "DatasetName", "XGLUE"]]}, {"text": "But is English always the optimal choice ?", "entities": []}, {"text": "Will the results become better , if we do \ufb01ne - tuning using other pivot languages ?", "entities": []}, {"text": "To answer these questions , we evaluate Unicoder on XNLI and NTG using different pivot languages in \ufb01ne - tuning and list comparison results in Table 5 and Table 6 , respectively .", "entities": [[9, 10, "DatasetName", "XNLI"]]}, {"text": "( 1 ) For each test set in languageliin Table 5 and Table 6 , its best result isoften achieved when the model is \ufb01ne - tuned using lias the pivot language ; ( 2 ) For XNLI in Table 5 , the best pivot languages are Spanish ( es ) , Greek ( el ) and Turkish ( tr ) , rather than English ( en ) .", "entities": [[37, 38, "DatasetName", "XNLI"]]}, {"text": "For NTG in Table 6 , the best pivot language is French ( fr ) for both UnicoderxDAE SC and UnicoderxFNP SC .", "entities": []}, {"text": "It means the average quality of a cross - lingual pre - trained model could be further improved on a downstream task , by selecting a speci\ufb01c pivot language in \ufb01netuning .", "entities": []}, {"text": "5.4.2", "entities": []}, {"text": "Multi - language Fine - tuning We de\ufb01ne multi - language ( ml ) \ufb01ne - tuning as \ufb01netune a pre - trained model for a downstream task using all its available labeled data in different languages .", "entities": []}, {"text": "We evaluate Unicoder on XNLI and NTG using this \ufb01ne - tuning method and list evaluation results in Table 7 and Table 8 , respectively .", "entities": [[4, 5, "DatasetName", "XNLI"]]}, {"text": "We \ufb01nd multi - language \ufb01ne - tuning can achieve better results than pivot - language \ufb01ne - tuning on both XNLI and NTG .", "entities": [[21, 22, "DatasetName", "XNLI"]]}, {"text": "It means the average quality of a cross - lingual pre - trained model could be signi\ufb01cantly improved on a downstream task , by using combined labeled data in multiple languages .", "entities": []}, {"text": "5.4.3 Multi - task Fine - tuning We de\ufb01ne multi - task ( mt ) \ufb01ne - tuning as \ufb01ne - tune a pre - trained model for multiple downstream tasks using their combined labeled data .", "entities": []}, {"text": "To reduce the experimental cost , we evaluate Unicoder on 5 understanding tasks : XNLI , PAWS - X , NC , QAM and QADSM , using their merged English labeled data in \ufb01ne - tuning .", "entities": [[14, 15, "DatasetName", "XNLI"], [16, 19, "DatasetName", "PAWS - X"]]}, {"text": "Results are listed in Table 9 .", "entities": []}, {"text": "We \ufb01nd PAWS - X and QADSM can bene\ufb01t from the joint \ufb01ne - tuning strategy , but XNLI , NC and QAM can not .", "entities": [[2, 5, "DatasetName", "PAWS - X"], [18, 19, "DatasetName", "XNLI"]]}, {"text": "We leave discovering relationships between different tasks for better downstream task \ufb01ne - tuning for future work .", "entities": []}, {"text": "5.4.4 Impacts of Text Noising Strategies We investigate the impacts of different text noising strategies ( Section 4.1 ) in UnicoderxDAE SC , and list comparison results in Table 10 , where ( 1)+(2)+(3 ) denotes the result of using the \ufb01rst three strategies in pre - training , ( 4 ) denotes the result of using the last strategy in pre - training , ( 1)+(2)+(3)+(4 ) denotes the result of using all strategies in pretraining .", "entities": []}, {"text": "To reduce experiment cost , we set max sequence length to 256 and only train 60 K steps .", "entities": []}, {"text": "We \ufb01nd that ( 4 ) can achieve the best average result on NTG .", "entities": []}, {"text": "So all results of UnicoderxDAE SC reported in this paper is pre - trained using ( 4 ) only .", "entities": []}, {"text": "6015en fr es de el bg ru tr ar vi th zh hi sw ur", "entities": []}, {"text": "A VG XLM - R base ( pl ) 84.6 78.2 79.2 77.0 75.9 77.5 75.5 72.9 72.1 74.8 71.6 73.7 69.8 64.7 65.1 74.2 XLM - R base ( ml)85.7 81.5 82.5 81.2 79.7 81.7 80.0 79.0 77.1 80.1 77.9 79.2 76.5 73.0 71.3 79.1 Unicoder LC ( pl ) 85.4 79.2 79.8 78.2 77.3 78.5 76.7 73.8 73.9 75.9 71.8 74.7 70.1", "entities": [[2, 3, "MethodName", "XLM"], [25, 26, "MethodName", "XLM"]]}, {"text": "67.4 66.3 75.3", "entities": []}, {"text": "Unicoder LC ( ml)85.8 81.9 82.3 81.5 80.8 82.0 79.9 78.7 78.1 80.2 78.4 79.3 76.2 73.2 72.4 79.4 Table 7 : Impact of multi - language \ufb01ne - tuning on XNLI .", "entities": [[31, 32, "DatasetName", "XNLI"]]}, {"text": "plandmldenote pivot - language \ufb01ne - tuning ( English as pivot ) and multi - language \ufb01ne - tuning , respectively .", "entities": []}, {"text": "Model en es fr de ru A VG UnicoderxDAE SC ( pl ) 15.6 9.0 8.7 6.8 7.7 9.6 UnicoderxDAE SC ( ml)18.5 18.3 28.2 15.5 33.4 22.8 UnicoderxFNP SC ( pl ) 15.8 11.9 9.9 7.5 8.4 10.7 UnicoderxFNP SC ( ml)15.6 17.1 19.1 13.9 15.8 16.3 Table 8 : Impact of multi - language \ufb01ne - tuning on NTG .", "entities": []}, {"text": "plandmldenote pivot - language \ufb01ne - tuning ( English as pivot ) and multi - language \ufb01ne - tuning , respectively .", "entities": []}, {"text": "BLUE-4 is the metric .", "entities": []}, {"text": "Model XNLI PAWS - X NC QAM QADSM A VG Unicoder LC ( pl ) 75.3 90.1", "entities": [[1, 2, "DatasetName", "XNLI"], [2, 5, "DatasetName", "PAWS - X"]]}, {"text": "83.5 68.9 68.4 77.2 Unicoder LC ( mt ) 74.4 90.2 83.4 68.7 69.0 77.1 Table 9 : Impacts of multi - task \ufb01ne - tuning on XNLI , PAWS - X , NC , QAM and QADSM .", "entities": [[27, 28, "DatasetName", "XNLI"], [29, 32, "DatasetName", "PAWS - X"]]}, {"text": "plandmtdenote pivot - language \ufb01ne - tuning ( English as pivot ) on each task and multi - task \ufb01ne - tuning , respectively .", "entities": []}, {"text": "We also compare UnicoderxDAE SC with XNLG ( Chi et al . , 2019 ) on the Abstractive Summarization task .", "entities": [[18, 19, "TaskName", "Summarization"]]}, {"text": "For fairly comparison , we implement xDAE in same code base and use same pre - training languages as XNLG .", "entities": []}, {"text": "The zero - shot comparison results are listed in Table 11 .", "entities": []}, {"text": "We can see that by using xDAE only in pre - training , UnicoderxDAE SC can outperform XNLG signi\ufb01cantly , which is pre - trained using 4 tasks including MLM , DAE , XMLM and XAE .", "entities": [[29, 30, "DatasetName", "MLM"]]}, {"text": "It veri\ufb01es the effectiveness of the fourth text noising strategy described in Section 4.1 for generation tasks .", "entities": []}, {"text": "6 Related Work Dataset GLUE ( Wang et al . , 2019 ) includes 9 natural language understanding tasks that are labeled in English only .", "entities": [[4, 5, "DatasetName", "GLUE"], [15, 18, "TaskName", "natural language understanding"]]}, {"text": "Comparing to GLUE , XGLUE not only expands task annotations to multiple languages , but also includes natural language generation tasks .", "entities": [[2, 3, "DatasetName", "GLUE"], [4, 5, "DatasetName", "XGLUE"]]}, {"text": "XNLI ( Conneau et al . , 2018 ) , NER ( Sang , 2002 ; Sang and De Meulder , 2003 ) , POS Tagging ( Kim et al . , 2017 ) , MLQA ( Lewis et al . , 2019b ) and PAWS - X ( Yang et al . , 2019a ) are 5 multilingual datasets built for speci\ufb01c tasks .", "entities": [[0, 1, "DatasetName", "XNLI"], [10, 11, "TaskName", "NER"], [35, 36, "DatasetName", "MLQA"], [45, 48, "DatasetName", "PAWS - X"]]}, {"text": "Text Noising Strategy en es fr de ru A VG ( 1)+(2)+(3 ) 14.6 8.5 7.4 6.0 7.4 8.8 ( 4 ) 14.8 8.7 7.5 6.7 8.2 9.2 ( 1)+(2)+(3)+(4 ) 15.2 7.9 7.3 6.2 7.7 8.9 Table 10 : Impact of different text noising strategies on NTG using pivot - language \ufb01ne - tuning ( English as pivot ) .", "entities": []}, {"text": "BLUE-4 is the metric .", "entities": []}, {"text": "Model fr zh", "entities": []}, {"text": "A VG XNLG ( Chi et al . , 2019 ) 36.3 38.9 37.6 UnicoderxDAE SC 37.9 42.2 40.1 Table 11 : The zero - shot results on Abstractive Summarization .", "entities": [[29, 30, "TaskName", "Summarization"]]}, {"text": "UnicoderxDAE SC and XNLG are \ufb01ne - tuned using English labeled data .", "entities": []}, {"text": "ROUGE - L is the metric .", "entities": [[0, 3, "MetricName", "ROUGE - L"]]}, {"text": "XGLUE not only includes these 5 existing tasks , but also introduces 6 new tasks selected from real - world scenarios ( i.e. , Search , Ads and News ) .", "entities": [[0, 1, "DatasetName", "XGLUE"]]}, {"text": "This makes XGLUE have more practical values .", "entities": [[2, 3, "DatasetName", "XGLUE"]]}, {"text": "XTREME ( Hu et al . , 2020 ) is a concurrent work of XGLUE .", "entities": [[0, 1, "DatasetName", "XTREME"], [14, 15, "DatasetName", "XGLUE"]]}, {"text": "Comparing to it , XGLUE includes both understanding and generation tasks , which , to the best of our knowledge , is the \ufb01rst attempt in the cross - lingual dataset construction efforts .", "entities": [[4, 5, "DatasetName", "XGLUE"]]}, {"text": "Cross - lingual Pre - trained Model Multilingual BERT ( M - BERT )", "entities": [[8, 9, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"]]}, {"text": "( Devlin et al . , 2019 ) performs pre - training based on the multilingual corpus with the masked language model task .", "entities": []}, {"text": "By sharing the model parameters and the vocabulary for all languages , M - BERT can obtain the cross - lingual capability over 102 languages .", "entities": [[14, 15, "MethodName", "BERT"]]}, {"text": "XLM ( Conneau and Lample , 2019 ) performs cross - lingual pre - training based on multilingual corpus and bilingual corpus , by introducing the translation language model task into pre - training .", "entities": [[0, 1, "MethodName", "XLM"]]}, {"text": "Based on XLM , Unicoder ( Huang et al . , 2019 ) uses more cross - lingual pretraining tasks and achieves better results on XNLI .", "entities": [[2, 3, "MethodName", "XLM"], [25, 26, "DatasetName", "XNLI"]]}, {"text": "XLM - R ( Conneau et al . , 2019 ) is a RoBERTa ( Liu et al . , 2019)-version XLM without using translation language model in pre - training .", "entities": [[0, 1, "MethodName", "XLM"], [13, 14, "MethodName", "RoBERTa"], [21, 22, "MethodName", "XLM"]]}, {"text": "It is trained based on a much larger multilingual corpus ( i.e. Com-", "entities": []}, {"text": "6016mon Crawl ) and become the new state - of - the - art on XNLI .", "entities": [[15, 16, "DatasetName", "XNLI"]]}, {"text": "In this paper , we use both the Common Crawl corpus and the bilingual corpus , aiming to build a stronger baseline model on XGLUE .", "entities": [[8, 10, "DatasetName", "Common Crawl"], [24, 25, "DatasetName", "XGLUE"]]}, {"text": "BART ( Lewis et al . , 2019a ) and ProphetNet ( Yan et al . , 2020 ) are two latest generative pre - trained models .", "entities": [[0, 1, "MethodName", "BART"], [10, 11, "MethodName", "ProphetNet"], [12, 15, "DatasetName", "Yan et al"]]}, {"text": "We borrow ideas from these two works and extend Unicoder to cross - lingual generation tasks , which goes a step further to verify and explore different text generation approaches in the cross - lingual scenario .", "entities": [[27, 29, "TaskName", "text generation"]]}, {"text": "7 Conclusion We present XGLUE as a new cross - lingual benchmark and conduct comprehensive evaluations with interesting \ufb01ndings observed .", "entities": [[4, 5, "DatasetName", "XGLUE"]]}, {"text": "We thank STC - A NLP , Bing Answers , Bing Ads , Bing Relevance and Microsoft News for providing the datasets .", "entities": []}, {"text": "References Yen - Chun Chen , Linjie Li , Licheng Yu , Ahmed El Kholy , Faisal Ahmed , Zhe Gan , Yu Cheng , and Jingjing Liu . 2019 .", "entities": []}, {"text": "Uniter : Learning universal image - text representations .", "entities": [[0, 1, "MethodName", "Uniter"]]}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zewen Chi , Li Dong , Furu Wei , Wenhui Wang , XianLing Mao , and Heyan Huang . 2019 .", "entities": []}, {"text": "Cross - lingual natural language generation via pre - training .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , Francisco Guzm \u00b4 an , Edouard Grave , Myle Ott , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unsupervised cross - lingual representation learning at scale .", "entities": [[4, 6, "TaskName", "representation learning"]]}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Alexis Conneau and Guillaume Lample .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crosslingual language model pretraining .", "entities": []}, {"text": "In NeurIPS .", "entities": []}, {"text": "Alexis Conneau , Guillaume Lample , Ruty Rinott , Adina Williams , Samuel R Bowman , Holger Schwenk , and Veselin Stoyanov .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Xnli : Evaluating crosslingual sentence representations .", "entities": [[0, 1, "DatasetName", "Xnli"]]}, {"text": "arXiv preprint arXiv:1809.05053 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In NAACL .", "entities": []}, {"text": "Li Dong , Nan Yang , Wenhui Wang , Furu Wei , Xiaodong Liu , Yu Wang , Jianfeng Gao , Ming Zhou , and Hsiao - Wuen Hon . 2019 .", "entities": []}, {"text": "Uni\ufb01ed language model pre - training for natural language understanding and generation .", "entities": [[7, 10, "TaskName", "natural language understanding"]]}, {"text": "NeurIPS .", "entities": []}, {"text": "Junjie Hu , Sebastian Ruder , Aditya Siddhant , Graham Neubig , Orhan Firat , and Melvin Johnson .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Xtreme : A massively multilingual multi - taskbenchmark for evaluating cross - lingual generalization .", "entities": []}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Haoyang Huang , Yaobo Liang , Nan Duan , Ming Gong , Linjun Shou , Daxin Jiang , and Ming Zhou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unicoder :", "entities": []}, {"text": "A universal language encoder by pre - training with multiple cross - lingual tasks .", "entities": []}, {"text": "In EMNLP .", "entities": []}, {"text": "Joo - Kyung Kim , Young - Bum Kim , Ruhi Sarikaya , and Eric Fosler - Lussier .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Cross - lingual transfer learning for POS tagging without cross - lingual resources .", "entities": [[0, 4, "TaskName", "Cross - lingual transfer"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2832\u20132838 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Diederik P Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Ves Stoyanov , and Luke Zettlemoyer . 2019a .", "entities": []}, {"text": "Bart : Denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension .", "entities": [[2, 3, "TaskName", "Denoising"]]}, {"text": "arXiv preprint arXiv:1910.13461 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Patrick Lewis , Barlas O \u02d8guz , Ruty Rinott , Sebastian Riedel , and Holger Schwenk . 2019b .", "entities": []}, {"text": "Mlqa : Evaluating cross - lingual extractive question answering .", "entities": [[0, 1, "DatasetName", "Mlqa"], [7, 9, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1910.07475 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Gen Li , Nan Duan , Yuejian Fang , Ming Gong , Daxin Jiang , and Zhou Zhou .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unicoder - vl : A universal encoder for vision and language by cross - modal pre - training .", "entities": []}, {"text": "AAAI .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jiasen Lu , Dhruv Batra , Devi Parikh , and Stefan Lee . 2019 .", "entities": []}, {"text": "Vilbert : Pretraining task - agnostic visiolinguistic representations for vision - and - language tasks .", "entities": [[0, 1, "MethodName", "Vilbert"]]}, {"text": "NeurIPS .", "entities": []}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training .", "entities": []}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Erik Tjong Kim Sang and Fien De Meulder .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Introduction to the conll-2003 shared task : Languageindependent named entity recognition .", "entities": [[3, 4, "DatasetName", "conll-2003"], [8, 11, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the Seventh Conference on Natural Language Learning at HLT - NAACL 2003 , pages 142\u2013147 .", "entities": []}, {"text": "Tjong Kim Sang .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Ef : Introduction to the conll2002 shared task .", "entities": [[5, 6, "DatasetName", "conll2002"]]}, {"text": "In Proceedings of the 6th Conference on Natural Language Learning .", "entities": []}, {"text": "Holger Schwenk , Guillaume Wenzek , Sergey Edunov , Edouard Grave , and Armand Joulin .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Ccmatrix : Mining billions of high - quality parallel sentences on the web .", "entities": [[0, 1, "DatasetName", "Ccmatrix"]]}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "6017Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Glue : A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[11, 14, "TaskName", "natural language understanding"]]}, {"text": "ICLR .", "entities": []}, {"text": "Guillaume Wenzek , Marie - Anne Lachaux , Alexis Conneau , Vishrav Chaudhary , Francisco Guzman , Armand Joulin , and Edouard Grave .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Ccnet :", "entities": [[0, 1, "MethodName", "Ccnet"]]}, {"text": "Extracting high quality monolingual datasets from web crawl data .", "entities": []}, {"text": "arXiv preprint arXiv:1911.00359 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yu Yan , Weizhen Qi , Yeyun Gong , Dayiheng Liu , Nan Duan , Jiusheng Chen , Ruofei Zhang , and Ming Zhou .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Prophetnet :", "entities": [[0, 1, "MethodName", "Prophetnet"]]}, {"text": "Predicting future ngram for sequence - to - sequence pre - training .", "entities": []}, {"text": "arXiv preprint arXiv:2001.04063 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yinfei Yang , Yuan Zhang , Chris Tar , and Jason Baldridge . 2019a .", "entities": []}, {"text": "Paws - x : A cross - lingual adversarial dataset for paraphrase identi\ufb01cation .", "entities": [[0, 3, "DatasetName", "Paws - x"]]}, {"text": "arXiv preprint arXiv:1908.11828 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Carbonell , Ruslan Salakhutdinov , and Quoc V .", "entities": [[12, 13, "DatasetName", "Ruslan"]]}, {"text": "Le . 2019b .", "entities": []}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "NeurIPS .", "entities": []}, {"text": "Daniel Zeman , Joakim Nivre , Mitchell Abrams , No \u00a8emi Aepli , \u02c7Zeljko Agi \u00b4 c , Lars Ahrenberg , Gabriel \u02d9e Aleksandravi \u02c7ci\u00afut\u02d9e , Lene Antonsen , Katya Aplonova , Maria Jesus Aranzabe , Gashaw Arutie , Masayuki Asahara , Luma Ateyah , Mohammed Attia , Aitziber Atutxa , Liesbeth Augustinus , Elena Badmaeva , Miguel Ballesteros , Esha Banerjee , Sebastian Bank , Verginica Barbu Mititelu , Victoria Basmov , Colin Batchelor , John Bauer , Sandra Bellato , Kepa Bengoetxea , Yevgeni Berzak , Irshad Ahmad Bhat , Riyaz Ahmad Bhat , Erica Biagetti , Eckhard Bick , Agn\u02d9e Bielinskien \u02d9e , Rogier Blokland , Victoria Bobicev , Lo \u00a8\u0131c Boizou , Emanuel Borges V \u00a8olker , Carl B\u00a8orstell , Cristina Bosco , Gosse Bouma , Sam Bowman , Adriane Boyd , Kristina Brokait \u02d9e , Aljoscha Burchardt , Marie Candito , Bernard Caron , Gauthier Caron , Tatiana Cavalcanti , G \u00a8uls \u00b8en Cebiro \u02d8glu Eryi \u02d8git , Flavio Massimiliano Cecchini , Giuseppe G. A. Celano , Slavom \u00b4 \u0131r\u02c7C\u00b4epl\u00a8o , Savas Cetin , Fabricio Chalub , Jinho Choi , Yongseok Cho , Jayeol Chun , Alessandra T. Cignarella , Silvie Cinkov \u00b4 a , Aur\u00b4elie Collomb , C \u00b8 a \u02d8gr\u0131 C \u00b8 \u00a8oltekin , Miriam Connor , Marine Courtin , Elizabeth Davidson , MarieCatherine de Marneffe , Valeria de Paiva , Elvis de Souza , Arantza Diaz de Ilarraza , Carly Dickerson , Bamba Dione , Peter Dirix , Kaja Dobrovoljc , Timothy Dozat , Kira Droganova , Puneet Dwivedi , Hanne Eckhoff , Marhaba Eli , Ali Elkahky , Binyam Ephrem , Olga Erina , Toma \u02c7z Erjavec , Aline Etienne , Wograine Evelyn , Rich \u00b4 ard Farkas , Hector Fernandez Alcalde , Jennifer Foster , Cl \u00b4 audia Freitas , Kazunori Fujita , Katar \u00b4 \u0131na Gajdo \u02c7sov\u00b4a , Daniel Galbraith , Marcos Garcia , Moa G \u00a8ardenfors , Sebastian Garza , Kim Gerdes , Filip Ginter , Iakes Goenaga , Koldo Gojenola , Memduh G \u00a8ok\u0131rmak , Yoav Goldberg , Xavier G \u00b4 omez Guinovart , Berta Gonz \u00b4 alez Saavedra , Bernadeta Grici \u00afut\u02d9e , Matias Grioni , Normunds Gr \u00afuz\u00af\u0131tis , Bruno Guillaume , C \u00b4 elineGuillot - Barbance , Nizar Habash , Jan Haji \u02c7c , Jan Haji\u02c7c jr . , Mika H \u00a8am\u00a8al\u00a8ainen , Linh H ` a M\u02dcy , Na - Rae Han , Kim Harris , Dag Haug , Johannes Heinecke , Felix Hennig , Barbora Hladk \u00b4 a , Jaroslava Hlav \u00b4 a\u02c7cov\u00b4a , Florinel Hociung , Petter Hohle , Jena Hwang , Takumi Ikeda , Radu Ion , Elena Irimia , O .l\u00b4aj\u00b4\u0131d\u00b4e", "entities": [[33, 34, "TaskName", "Jesus"]]}, {"text": "Ishola , Tom \u00b4 a\u02c7s Jel \u00b4 \u0131nek , Anders Johannsen , Fredrik J\u00f8rgensen , Markus Juutinen , H \u00a8uner Kas \u00b8\u0131kara , Andre Kaasen , Nadezhda Kabaeva , Sylvain Kahane , Hiroshi Kanayama , Jenna Kanerva , Boris Katz , Tolga Kayadelen , Jessica Kenney , V \u00b4 aclava Kettnerov \u00b4 a , Jesse Kirchner , Elena Klementieva , Arne K\u00a8ohn , Kamil Kopacewicz , Natalia Kotsyba , Jolanta Kovalevskait \u02d9e , Simon Krek , Sookyoung Kwak , Veronika Laippala , Lorenzo Lambertino , Lucia Lam , Tatiana Lando , Septina Dian Larasati , Alexei Lavrentiev , John Lee , Phuong L \u02c6e H`\u02c6ong , Alessandro Lenci , Saran Lertpradit , Herman Leung , Cheuk Ying Li , Josie Li , Keying Li , KyungTae Lim , Maria Liovina , Yuan Li , Nikola Ljube \u02c7si\u00b4c , Olga Loginova , Olga Lyashevskaya , Teresa Lynn , Vivien Macketanz , Aibek Makazhanov , Michael Mandl , Christopher Manning , Ruli Manurung , C \u02d8at\u02d8alina", "entities": []}, {"text": "M\u02d8ar\u02d8anduc , David Mare \u02c7cek , Katrin Marheinecke , H\u00b4ector Mart \u00b4 \u0131nez Alonso , Andr \u00b4 e Martins , Jan Ma \u02c7sek , Yuji Matsumoto , Ryan McDonald , Sarah McGuinness , Gustavo Mendonc \u00b8a , Niko Miekka , Margarita Misirpashayeva , Anna Missil \u00a8a , C \u02d8at\u02d8alin", "entities": []}, {"text": "Mititelu , Maria Mitrofan , Yusuke Miyao , Simonetta Montemagni , Amir More , Laura Moreno Romero , Keiko Sophie Mori , Tomohiko Morioka , Shinsuke Mori , Shigeki Moro , Bjartur Mortensen , Bohdan Moskalevskyi , Kadri Muischnek , Robert Munro , Yugo Murawaki , Kaili M \u00a8u\u00a8urisep , Pinkey Nainwani , Juan Ignacio Navarro Hor \u02dcniacek , Anna Nedoluzhko , Gunta Ne \u02c7spore - B \u00aferzkalne , Luong Nguy\u02dc \u02c6en Thi . , Huy ` \u02c6en Nguy\u02dc \u02c6en Thi .Minh , Yoshihiro Nikaido , Vitaly Nikolaev , Rattima Nitisaroj , Hanna Nurmi , Stina Ojala , Atul Kr .", "entities": []}, {"text": "Ojha , Ad \u00b4 edayo .", "entities": []}, {"text": "Ol\u00b4u`okun , Mai Omura , Petya Osenova , Robert \u00a8Ostling , Lilja \u00d8vrelid , Niko Partanen , Elena Pascual , Marco Passarotti , Agnieszka Patejuk , Guilherme Paulino - Passos , Angelika Peljak - \u0141api \u00b4 nska , Siyao Peng , Cenel - Augusto Perez , Guy Perrier , Daria Petrova , Slav Petrov , Jason Phelan , Jussi Piitulainen , Tommi A Pirinen , Emily Pitler , Barbara Plank , Thierry Poibeau , Larisa Ponomareva , Martin Popel , Lauma Pretkalnin \u00b8a , Sophie Pr \u00b4 evost , Prokopis Prokopidis , Adam Przepi \u00b4 orkowski , Tiina Puolakainen , Sampo Pyysalo , Peng Qi , Andriela R\u00a8a\u00a8abis , Alexandre Rademaker , Loganathan Ramasamy , Taraka Rama , Carlos Ramisch , Vinit Ravishankar , Livy Real , Siva Reddy , Georg Rehm , Ivan Riabov , Michael Rie\u00dfler , Erika Rimkut \u02d9e , Larissa Rinaldi , Laura Rituma , Luisa Rocha , Mykhailo Romanenko , Rudolf Rosa , Davide Rovati , Valentin Ros , ca , Olga Rudina , Jack Rueter , Shoval Sadde , Beno \u02c6\u0131t Sagot , Shadi Saleh , Alessio Salomoni , Tanja Samard \u02c7zi\u00b4c , Stephanie Samson , Manuela Sanguinetti , Dage S \u00a8arg , Baiba Saul \u00af\u0131te , Yanin Sawanakunanon , Nathan Schneider , Sebastian Schuster , Djam \u00b4 e Seddah , Wolfgang Seeker , Mojgan Seraji , Mo Shen , Atsuko Shimada , Hiroyuki Shirasu , Muh Shohibus-", "entities": [[92, 93, "MethodName", "Adam"]]}, {"text": "6018sirri , Dmitry Sichinava , Aline Silveira , Natalia Silveira , Maria Simi , Radu Simionescu , Katalin Simk \u00b4 o , M\u00b4aria", "entities": []}, {"text": "\u02c7Simkov", "entities": []}, {"text": "\u00b4 a , Kiril Simov , Aaron Smith , Isabela Soares - Bastos , Carolyn Spadine , Antonio Stella , Milan Straka , Jana Strnadov \u00b4 a , Alane Suhr , Umut Sulubacak , Shingo Suzuki , Zsolt Sz \u00b4 ant\u00b4o , Dima Taji , Yuta Takahashi , Fabio Tamburini , Takaaki Tanaka , Isabelle Tellier , Guillaume Thomas , Liisi Torga , Trond Trosterud , Anna Trukhina , Reut Tsarfaty , Francis Tyers , Sumire Uematsu , Zde \u02c7nka Ure\u02c7sov\u00b4a , Larraitz Uria , Hans Uszkoreit , Andrius Utka , Sowmya Vajjala , Daniel van Niekerk , Gertjan van Noord , Viktor Varga , Eric Villemonte de la Clergerie , Veronika Vincze , Lars Wallin , Abigail Walsh , Jing Xian Wang , Jonathan North Washington , Maximilan Wendt , Seyi Williams , Mats Wir \u00b4 en , Christian Wittern , Tsegay Woldemariam , Tak - sum Wong , Alina Wr \u00b4 oblewska , Mary Yako , Naoki Yamazaki , Chunxiao Yan , Koichi Yasuoka , Marat M. Yavrumyan , Zhuoran Yu , Zden \u02c7ek\u02c7Zabokrtsk", "entities": []}, {"text": "\u00b4 y , Amir Zeldes , Manying Zhang , and Hanzhi Zhu . 2019 .", "entities": []}, {"text": "Universal dependencies 2.5 .", "entities": [[0, 2, "DatasetName", "Universal dependencies"]]}, {"text": "LINDAT / CLARIAHCZ digital library at the Institute of Formal and Applied Linguistics ( \u00b4 UFAL ) , Faculty of Mathematics and Physics , Charles University .", "entities": []}, {"text": "Yuan Zhang , Jason Baldridge , and Luheng He . 2019 .", "entities": []}, {"text": "Paws : Paraphrase adversaries from word scrambling .", "entities": []}, {"text": "arXiv preprint arXiv:1904.01130 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Luowei Zhou , Hamid Palangi , Lei Zhang , Houdong Hu , Jason Corso , and Jianfeng Gao .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Uni\ufb01ed vision - language pre - training for image captioning and vqa .", "entities": [[8, 10, "TaskName", "image captioning"], [11, 12, "TaskName", "vqa"]]}, {"text": "AAAI .", "entities": []}, {"text": "A The \ufb01ne - tune parameters of Unicoder on XGLUE .", "entities": [[9, 10, "DatasetName", "XGLUE"]]}, {"text": "Task batch size epoch number learning rate NER 32 20 5e-6 POS 32 20 5e-6 NC 32 10 5e-6 MLQA 12 2 3e-5 XNLI 32 10 5e-6 PAWS - X 32 10 5e-6 QADSM 32 10 5e-6 WPR 32 10 5e-6 QAM 32 10 5e-6 Table 12 : The \ufb01ne - tune parameters of understanding tasks .", "entities": [[1, 3, "HyperparameterName", "batch size"], [3, 5, "HyperparameterName", "epoch number"], [5, 7, "HyperparameterName", "learning rate"], [7, 8, "TaskName", "NER"], [19, 20, "DatasetName", "MLQA"], [23, 24, "DatasetName", "XNLI"], [27, 30, "DatasetName", "PAWS - X"]]}, {"text": "Task Model batch size learning rate warm up steps QG UnicoderxDAE SC 64 1e-4 1000 NTG UnicoderxDAE SC 64 1e-4 1000 QG UnicoderxFNP SC 1024", "entities": [[2, 4, "HyperparameterName", "batch size"], [4, 6, "HyperparameterName", "learning rate"]]}, {"text": "1e-5 2000 NTG UnicoderxFNP SC 1024 1e-5 2000 Table 13 : The \ufb01ne - tune parameters of generation tasks .", "entities": []}]