[{"text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 8449\u20138456 November 7\u201311 , 2021 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2021 Association for Computational Linguistics8449An Empirical Investigation of Word Alignment Supervision for Zero - Shot Multilingual Neural Machine Translation Alessandro Raganato , Ra\u00fal V\u00e1zquez , Mathias Creutz andJ\u00f6rg Tiedemann University of Helsinki , { name.surname}@helsinki.\ufb01 Abstract Zero - shot translations is a fascinating feature of Multilingual Neural Machine Translation ( MNMT ) systems .", "entities": [[8, 10, "TaskName", "Word Alignment"], [17, 19, "TaskName", "Machine Translation"], [47, 49, "TaskName", "Machine Translation"]]}, {"text": "These MNMT models are usually trained on English - centric data , i.e. English either as the source or target language , and with a language label prepended to the input indicating the target language .", "entities": []}, {"text": "However , recent work has highlighted several \ufb02aws of these models in zero - shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly unstable results .", "entities": []}, {"text": "In this paper , we investigate the bene\ufb01ts of an explicit alignment to language labels in Transformer - based MNMT models in the zero - shot context , by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label .", "entities": [[16, 17, "MethodName", "Transformer"], [36, 38, "TaskName", "word alignment"]]}, {"text": "We compare and evaluate several MNMT systems on three multilingual MT benchmarks of different sizes , showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language , improving the zero - shot performance overall .", "entities": []}, {"text": "Moreover , as an additional advantage , we \ufb01nd that our alignment supervision leads to more stable results across different training runs .", "entities": []}, {"text": "1 Introduction Multilingual Neural Machine Translation ( MNMT ) focuses on translation between multiple language pairs through a single optimized neural model , and has been explored from different angles witnessing a rapid progress in recent years ( Arivazhagan et al . , 2019b ;", "entities": [[4, 6, "TaskName", "Machine Translation"]]}, {"text": "Wang et al . , 2020 ; Dabre et al . , 2020 ; Lin et al . , 2021 ) .", "entities": []}, {"text": "Besides the great \ufb02exibility MNMT models offer , they are also highlighted by their so called zero - shot translation capabilities , i.e. , translating between all combinations of languages available in the training data , including those with no parallel data seen at training time ( Ha et al . , 2016 ;", "entities": []}, {"text": "Firat et al . , 2016 ; Johnson et al . ,2017 ) .", "entities": []}, {"text": "Many studies have investigated this feature , focusing on the impact of both , the model architecture design ( Arivazhagan et al . , 2019a ; Pham et al . , 2019 ) and data pre - processing ( Lee et al . , 2017 ; Wang et al . , 2019 ; Rios et al . , 2020 ; Wu et al . , 2021 ) .", "entities": []}, {"text": "Broadly speaking , MNMT architectures are categorized according to their degree of parameter sharing , from fully shared ( Johnson et al . , 2017 ) to the use of language - speci\ufb01c components ( V\u00e1zquez et al . , 2020 ; Escolano et al . , 2021 ; Zhang et al . , 2021 ) .", "entities": []}, {"text": "The Johnson et al .", "entities": []}, {"text": "( 2017 ) MNMT model is widely used , due to its simplicity and good translation quality .", "entities": []}, {"text": "It uses the fully shared parameters setting , and relies on appending an arti\ufb01cial language label to each input sentence to indicate the target language .", "entities": []}, {"text": "While this method allows for zeroshot translation , several works have highlighted two major \ufb02aws : i ) its failure to reliably generalize to unseen language pairs , ending up with the so called off - target issue , where the language label is ignored and the wrong target language is produced as a result ( Zhang et al . , 2020 ) , ii ) its lack of stability in translation results between different training runs ( Rios et al . , 2020 ) .", "entities": []}, {"text": "In this work , we investigate the role of guided alignment in the Johnson et al .", "entities": []}, {"text": "( 2017 ) setting , by jointly training one cross attention head to explicitly focus on the target language label .", "entities": []}, {"text": "We show that alignment supervision mitigates the off - target translation issue in the zero - shot case .", "entities": []}, {"text": "Our method improves the zero - shot translation performance and results in more stable results across different training runs .", "entities": []}, {"text": "2 Methodology Alignment Methods .", "entities": []}, {"text": "Given a bitext Bsrc= ( s1;:::;s j;:::;s N)andBtrg= ( t1;:::;t i;:::;t M ) whereBsrcis a sentence in the source language andBtrgis", "entities": []}, {"text": "its translation in the target language , an alignmentAis a mapping of words between Bsrc andBtrg(Tiedemann , 2011 ) , formally de\ufb01ned as", "entities": []}, {"text": "8450 Figure 1 : English!German example sentence with different alignment methods .", "entities": []}, {"text": "Alignments in ( a ) show word alignments between corresponding words in the two languages , ( b ) our introduced alignments between all target words and the input language label , and ( c ) the union of the two .", "entities": []}, {"text": "a subset of the Cartesian product of the word positions ( Och and Ney , 2003 ): A\u0012f(j;i ) : j= 1;:::;N ; i= 1;:::;Mg(1 ) We study three different settings : ( a ) standard word alignment between corresponding words , ( b ) alignments between all target words and the language label in the input string , and ( c ) the union between the former two .", "entities": [[36, 38, "TaskName", "word alignment"]]}, {"text": "Figure 1 shows an example of those approaches .", "entities": []}, {"text": "To produce word alignments between parallel sentences , i.e. , Figure 1 ( a ) , we use the awesome - align tool ( Dou and Neubig , 2021 ) , a recent work that leverages multilingual BERT ( Devlin et al . , 2019 ) to extract the links.1 Models .", "entities": [[37, 38, "MethodName", "BERT"]]}, {"text": "To train Many - to - Many MNMT models , we use a 6 - layer Transformer architecture ( Vaswani et al . , 2017 ) , prepending a language label in the input to indicate the target language ( Johnson et al . , 2017 ) .", "entities": [[16, 17, "MethodName", "Transformer"]]}, {"text": "Following Garg et al .", "entities": []}, {"text": "( 2019 ) , given an alignment matrix AMM;N and an attention matrix computed by a cross attention head", "entities": []}, {"text": "AHM;N , for each target word i , we use the following cross - entropy lossLato minimize the Kullback - Leibler divergence between AH andAM : La(AH;AM )", "entities": []}, {"text": "= \u00001 MMX i=1NX j=1AMi;jlog(AHi;j ) ( 2 ) The overall lossLis : L = Lt+", "entities": []}, {"text": "La(AH;AM ) ( 3 ) whereLtis the standard NLL translation loss , and", "entities": [[8, 9, "MetricName", "NLL"], [10, 11, "MetricName", "loss"]]}, {"text": "is a hyperparameter .", "entities": []}, {"text": "We use", "entities": []}, {"text": "= 0:05 , supervising only one cross attention head at the third last 1We use the bert - base - multilingual - cased checkpoint , without \ufb01ne - tuning , and with softmax as a extraction function.layer.2Given", "entities": [[32, 33, "MethodName", "softmax"]]}, {"text": "the sparse nature of the alignments , we replace the softmax operator in the cross attention head with the \u000b -entmax function ( Peters et al . , 2019 ; Correia et al . , 2019 ) .", "entities": [[10, 11, "MethodName", "softmax"]]}, {"text": "Entmax allows sparse attention weights for any \u000b > 1 .", "entities": []}, {"text": "Following Peters et al .", "entities": []}, {"text": "( 2019 ) , we use \u000b = 1.5 .", "entities": []}, {"text": "3 Experimental Setup We use three highly multilingual MT benchmarks : \u2022TED Talks ( Qi et al . , 2018 ) .", "entities": []}, {"text": "An Englishcentric parallel corpus with 10 M training sentences across 116 translation directions .", "entities": []}, {"text": "Following Aharoni et al .", "entities": []}, {"text": "( 2019 ) , we evaluate on a total of 16 language directions , while as zeroshot test we evaluate on 4 language pairs .", "entities": []}, {"text": "\u2022WMT-2018 ( Bojar et al . , 2018).3A parallel dataset provided by the WMT-2018 shared task on news translation .", "entities": []}, {"text": "We use all available language pairs , i.e. 14 , up to 5 M training sentences for each language pair .", "entities": []}, {"text": "We evaluate the models on the test sets of the shared task , i.e. newstest2018 .", "entities": []}, {"text": "As there are no zero - shot test sets provided by the competition , we use the test portion from the Tatoeba - challenge ( Tiedemann , 2020),4 in all possible language pair combinations included in the challenge .", "entities": [[21, 22, "DatasetName", "Tatoeba"]]}, {"text": "\u2022OPUS-100 ( Zhang et al . , 2020 ) .", "entities": []}, {"text": "An Englishcentric multi - domain benchmark , built upon the OPUS parallel text collection ( Tiedemann , 2012 ) .", "entities": []}, {"text": "It covers a total of 198 language directions , with up to 1 M training sentence per 2As we use the OpenNMT - py ( Klein et al . , 2017 ) toolkit , it is recommended to supervise the third last layer .", "entities": []}, {"text": "Seehttps://github.com/OpenNMT/OpenNMT-py/ issues/1843 .", "entities": []}, {"text": "3http://data.statmt.org/wmt18/ translation - task / preprocessed/ 4release v2020 - 07 - 28 .", "entities": []}, {"text": "8451ID Model # Param .", "entities": []}, {"text": "EN ! X ( 16 ) X!EN ( 16)BLEU zero(4)ACC zero(4 )", "entities": []}, {"text": "Aharoni et al .", "entities": []}, {"text": "( 2019)-103 473 M 20.11 29.97 9.17 Aharoni et al .", "entities": []}, {"text": "( 2019 ) 93 M 19.54 28.03 - 1 Transformer 93 M 18.93 \u00060.15 27.56\u00060.25 6.81\u00060.86 72.38\u00067.18 2 1 + 1.5- entmax 93 M 18.90\u00060.25 27.21\u00060.38 10.02\u00061.50 87.81\u00068.80 3 2 + ( a ) 93 M 18.99 \u00060.07 27.58\u00060.12 8.38\u00065.37 73.12\u000641.14 4 2 + ( b ) 93 M 18.98 \u00060.08 27.48\u00060.13", "entities": [[9, 10, "MethodName", "Transformer"]]}, {"text": "6.35\u00060.87 65.01\u00066.10 5 2 + ( c ) 93 M 19.06 \u00060.11", "entities": []}, {"text": "27.37\u00060.19 11.94\u00060.86 97.25\u00062.66 Table 1 : Results on the Many - to - Many TED Talks benchmark .", "entities": []}, {"text": "The baselines consist of 1our replication of the standard 6 - layer Transformer model by Aharoni et al .", "entities": [[12, 13, "MethodName", "Transformer"]]}, {"text": "( 2019 ) , and 2its variant with a 1.5- entmax function on the cross attention heads as in Correia et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "The labels ( a ) , ( b ) , ( c ) denote the use of different alignment supervision ( see Section 2 ) .", "entities": []}, {"text": "\u201c # Param . \u201d : trainable parameter number .", "entities": []}, {"text": "\u201c EN - > X ( 16 ) \u201d and \u201c X- > EN ( 16 ) \u201d : average BLEU scores for English to Non - English languages and for Non - English languages to English on 16 language pairs respectively .", "entities": [[20, 21, "MetricName", "BLEU"]]}, {"text": "\u201c BLEU zero ( 4 ) \u201d and \u201c ACC zero ( 4 ) \u201d : average BLEU scores and target language identi\ufb01cation accuracy over 4 zero - shot language directions .", "entities": [[1, 2, "MetricName", "BLEU"], [9, 10, "MetricName", "ACC"], [17, 18, "MetricName", "BLEU"], [23, 24, "MetricName", "accuracy"]]}, {"text": "We report average BLEU and accuracy scores , plus the standard deviation over 3 training runs with different random seeds .", "entities": [[3, 4, "MetricName", "BLEU"], [5, 6, "MetricName", "accuracy"], [19, 20, "DatasetName", "seeds"]]}, {"text": "language pair .", "entities": []}, {"text": "It provides supervised translation test data for 188 language pairs , and zero - shot evaluation data for 30 pairs .", "entities": []}, {"text": "Following related work ( Aharoni et al . , 2019 ; Zhang et al . , 2020 ) , we apply joint Byte - Pair Encoding ( BPE ) segmentation ( Sennrich et al . , 2016 ; Kudo and Richardson , 2018 ) , with a shared vocabulary size of 32 K symbols for TED Talks and 64 K for WMT-2018 and OPUS-100 .", "entities": [[27, 28, "MethodName", "BPE"], [63, 64, "DatasetName", "OPUS-100"]]}, {"text": "As evaluation measure , we use tokenized BLEU ( Papineni et al . , 2002 ) to be comparable with Aharoni et al .", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "( 2019 ) for the TED Talks benchmark , and SACRE BLEU 5(Post , 2018 ) for WMT-2018 and OPUS-100.6 As an additional evaluation , we report the target language identi\ufb01cation accuracy score for the zeroshot cases ( Zhang et al . , 2020 ) , called ACC zero .", "entities": [[11, 12, "MetricName", "BLEU"], [31, 32, "MetricName", "accuracy"], [47, 48, "MetricName", "ACC"]]}, {"text": "We use fasttext as a language identi\ufb01cation tool ( Joulin et al . , 2017 ) , counting how many times the translation language matches the reference target language .", "entities": [[2, 3, "MethodName", "fasttext"]]}, {"text": "The Transformer models follow the base setting of Vaswani et", "entities": [[1, 2, "MethodName", "Transformer"]]}, {"text": "al . ( 2017 ) , with three different random seeds in each run .", "entities": [[10, 11, "DatasetName", "seeds"]]}, {"text": "All of them are trained on the Many - to - Many English - centric scenario , i.e. , on the concatenation of the training data having English either as the source or target language .", "entities": []}, {"text": "Details about data and model settings in the Appendix .", "entities": []}, {"text": "5Signature : BLEU+case.mixed+numrefs.1+smooth.exp+ tok.{13a , ja - mecab-0.996 - IPA , zh}+version.1.5.0 6We report average BLEU over all test sets .", "entities": [[15, 16, "MetricName", "BLEU"]]}, {"text": "Scores for each language pair are available in the supplementary material.4 Results and Discussion Throughout this section we refer to our baseline MNMT models by the labels 1and 2 , while 3,4 , and 5mark the models trained with the auxiliary alignment supervision task , ( a ) , ( b ) , ( c ) from Figure 1 respectively ( see Section 2 ) .", "entities": []}, {"text": "TED Talks .", "entities": []}, {"text": "Table 1 shows the results on the TED Talks benchmark .", "entities": []}, {"text": "Regarding translation quality on the language pairs seen during training ( EN ! X and X!EN columns ) , average BLEU scores from all models end up in the same ballpark .", "entities": [[20, 21, "MetricName", "BLEU"]]}, {"text": "In contrast , zero - shot results vary across the board , with 5attaining the best performance , with almost 2 BLEU points better than its baseline 2 .", "entities": [[21, 22, "MetricName", "BLEU"]]}, {"text": "Moreover , 5considerably improves target language identi\ufb01cation accuracy ( ACC zero ) , with more stable results , i.e. lower standard deviation , than counterparts .", "entities": [[7, 8, "MetricName", "accuracy"], [9, 10, "MetricName", "ACC"]]}, {"text": "Surprisingly , the addition of alignment supervision ( a ) and ( b ) as an auxiliary task has an overall detrimental effect on the zero - shot performance , even though model 4results in more stable results than 2 .", "entities": []}, {"text": "WMT-2018 .", "entities": []}, {"text": "Table 2 reports the results on the WMT-2018 benchmark .", "entities": []}, {"text": "As expected , in a highresource scenario bilingual baselines are hard to beat .", "entities": []}, {"text": "Among multilingual models , the overall performance follows a similar trend as before .", "entities": []}, {"text": "Enriching the model with alignment supervision ( c ) results in the best system overall , with an improvement of more than 3 BLEU points in the zero - shot", "entities": [[23, 24, "MetricName", "BLEU"]]}, {"text": "8452ID Model # Param .", "entities": []}, {"text": "EN ! X ( 7 ) X!EN ( 7)BLEU zero(24)ACC zero(24 ) Transformer , Bilingual 127 M 18.28 19.25 - 1 Transformer 127 M 15.18 \u00060.54 18.39\u00060.65 9.78\u00060.61 74.17\u00064.78 2 1 + 1.5- entmax 127 M 15.17\u00060.41 18.33\u00060.56", "entities": [[12, 13, "MethodName", "Transformer"], [21, 22, "MethodName", "Transformer"]]}, {"text": "8.55\u00060.61 65.31\u00064.46 3 2 + ( a ) 127 M 11.99 \u00060.37 16.42\u00060.73 6.38\u00060.83 73.78\u00067.84 4 2 + ( b ) 127 M 15.46 \u00060.16", "entities": []}, {"text": "18.66\u00060.31 11.72\u00060.76 85.64\u00063.37 5 2 + ( c ) 127 M 15.50 \u00060.18 18.70\u00060.23 11.98\u00060.12 85.68\u00060.82 Table 2 : Results on the Many - to - Many WMT-2018 benchmark .", "entities": []}, {"text": "Average BLEU , target language identi\ufb01cation accuracy and standard deviation of 3 training runs .", "entities": [[1, 2, "MetricName", "BLEU"], [6, 7, "MetricName", "accuracy"]]}, {"text": "ID Model # Param .", "entities": []}, {"text": "EN ! X ( 94 ) X!EN ( 94 ) EN!X ( 4 ) X!EN ( 4)BLEU zero(30)ACC zero(30 ) Transformer , Bilingualy110 M - - 20.28 21.23 - Transformer+MA TTy141 M 20.77 29.15 16.08 24.15 4.71 39.40 MA TT+LALN+LALTy173 M 22.86 29.49 19.25 24.53 5.41 51.40 1 Transformer 142 M 18.50 \u00060.08 26.85\u00060.13 18.37\u00060.39 25.70\u00060.05 4.59\u00060.21 30.91\u00062.05 2 1 + 1.5- entmax 142 M 18.47\u00060.15 26.83\u00060.14 18.42\u00060.38 25.67\u00060.10", "entities": [[20, 21, "MethodName", "Transformer"], [48, 49, "MethodName", "Transformer"]]}, {"text": "4.39\u00060.86 30.51\u00065.62 3 2 + ( a ) 142 M 17.80 \u00060.23", "entities": []}, {"text": "26.21\u00060.40 17.53\u00060.34 25.18\u00060.39 3.96\u00060.43 28.95\u00062.61 4 2 + ( b ) 142 M 18.56 \u00060.04 26.91\u00060.18 18.32\u00060.36 25.47\u00060.10 4.63\u00060.48 31.05\u00065.93 5 2 + ( c ) 142 M 18.63 \u00060.07 26.69\u00060.09 18.51\u00060.18 25.39\u00060.01 4.73\u00060.16 32.00\u00060.96 Table 3 : Results on the Many - to - Many OPUS-100 benchmark .", "entities": [[46, 47, "DatasetName", "OPUS-100"]]}, {"text": "Results marked with yare taken from Zhang et al . ( 2020 ) .", "entities": []}, {"text": "MA TTdenotes the use of merged attention ( Zhang et al . , 2019 ) .", "entities": []}, {"text": "L ALNand L ALTindicate the use of language - aware components .", "entities": []}, {"text": "Average BLEU , target language identi\ufb01cation accuracy and standard deviation of 3 training runs .", "entities": [[1, 2, "MetricName", "BLEU"], [6, 7, "MetricName", "accuracy"]]}, {"text": "testbed compared to baseline 2 , and with stable results across three training runs ( standard deviations of 0.12 and 0.82 ) .", "entities": []}, {"text": "OPUS-100 .", "entities": [[0, 1, "DatasetName", "OPUS-100"]]}, {"text": "As one can see from Table 3 , we con\ufb01rm the positive effect of adding the alignment strategy ( c ) both as translation quality and as a mechanism to produce stable results even in a highly multilingual setup , i.e. , training on 198 language directions .", "entities": []}, {"text": "The average score over 30 zeroshot language pairs is low but the individual results range from 0.3 to 17.5 BLEU showing the potentials of multilingual models in this challenging data set as well.7Even though the results from our best model still lag behind models with languagespeci\ufb01c components , i.e. MA TT+LALN+LALT from Zhang et al .", "entities": [[19, 20, "MetricName", "BLEU"]]}, {"text": "( 2020 ) , we note that our results demonstrate the positive effect of alignment on zero - shot translation.8 Overall , our experiments show consistent results across different benchmarks , providing quantitative evidence on the utility of guided alignment in highly multilingual MT scenarios .", "entities": []}, {"text": "Supervising 7Individual scores available in the supplementary material .", "entities": [[6, 8, "DatasetName", "supplementary material"]]}, {"text": "8Also note that Zhang et al .", "entities": []}, {"text": "( 2020 ) average the last 5 checkpoints whereas we report single checkpoints per run.a single cross attention head with the alignment method ( c ) substantially reduces the instability between training runs , mitigating the off - target translation issue in the zero - shot evaluation .", "entities": []}, {"text": "Zero - shot improvements , i.e. BLEU zeroandACC zero , are large in two benchmarks out of three , i.e. Ted Talks and WMT-2018 , and with a similar trend in OPUS100 .", "entities": [[6, 7, "MetricName", "BLEU"], [31, 32, "DatasetName", "OPUS100"]]}, {"text": "We also note that performance differences may be related to the different data sizes ( see Appendix A ) .", "entities": []}, {"text": "TED Talks is a rather small and imbalanced multilingual dataset with 116 language directions with a total of 10 M training sentences , while WMT-2018 and OPUS-100 comprise 14 language pairs for a total of 47.8 M training sentences , and 110 M training sentences for 198 language pairs , respectively .", "entities": [[26, 27, "DatasetName", "OPUS-100"]]}, {"text": "We plan on investigating the impact of the training size and the resulting alignments on the zero - shot test sets further in future work .", "entities": []}, {"text": "Limitations Finally , we highlight that we have focused on a quantitative evaluation on Englishcentric MNMT benchmarks only , therefore we lack a comprehensive evaluation on complete MNMT benchmarks including training data without English as source and target language ( Freitag and Firat , 2020 ;", "entities": []}, {"text": "Rios et al . , 2020 ; Tiedemann , 2020 ;", "entities": []}, {"text": "8453Goyal et al . , 2021 ) .", "entities": []}, {"text": "5 Conclusions and Future Work", "entities": []}, {"text": "In this work we present an empirical comparative evaluation of integrating different alignment methods in Transformer - based models for highly multilingual English - centric MT setups .", "entities": [[15, 16, "MethodName", "Transformer"]]}, {"text": "Our extensive evaluation over three alignment variants shows that adding alignment supervision between corresponding words and the language label consistently improves the stability of the models , resulting in stable performance across different runs and mitigating the off - target translation issue in the zero - shot scenario .", "entities": []}, {"text": "We believe that our work will pave the way for designing new and better multilingual MT models to improve their generalization in zero - shot setups .", "entities": []}, {"text": "As future work , we intend to analyze the quality of the learned alignments and their effect on the other attention weights in both supervised and zeroshot evaluation data ( Raganato and Tiedemann , 2018 ; Tang et al . , 2018 ; Mare \u02c7cek and Rosa , 2019 ;", "entities": []}, {"text": "V oita et al . , 2019 ) .", "entities": []}, {"text": "Finally , we plan to explore other mechanisms to inject prior knowledge to better handle zero - shot translations ( Deshpande and Narasimhan , 2020 ;", "entities": []}, {"text": "Raganato et al . , 2020 ; Song et al . , 2020 ) .", "entities": []}, {"text": "Acknowledgments This work is part of the FoTran project , funded by the European Research Council ( ERC ) under the European Union \u2019s Horizon 2020 research and innovation programme ( grant agreement No 771113 ) .", "entities": []}, {"text": "The authors gratefully acknowledge the support of the CSC \u2013 IT Center for Science , Finland , for computational resources .", "entities": []}, {"text": "Finally , We would also like to acknowledge NVIDIA and their GPU grant .", "entities": []}, {"text": "References Roee Aharoni , Melvin Johnson , and Orhan Firat .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Massively multilingual neural machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3874\u20133884 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Naveen Arivazhagan , Ankur Bapna , Orhan Firat , Roee Aharoni , Melvin Johnson , and WolfgangMacherey . 2019a .", "entities": []}, {"text": "The missing ingredient in zeroshot neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1903.07091 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Naveen Arivazhagan , Ankur Bapna , Orhan Firat , Dmitry Lepikhin , Melvin Johnson , Maxim Krikun , Mia Xu Chen , Yuan Cao , George Foster , Colin Cherry , et al . 2019b .", "entities": []}, {"text": "Massively multilingual neural machine translation in the wild : Findings and challenges .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1907.05019 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ond\u02c7rej Bojar , Christian Federmann , Mark Fishel , Yvette Graham , Barry Haddow , Philipp Koehn , and Christof Monz .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Findings of the 2018 conference on machine translation ( WMT18 ) .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Shared Task Papers , pages 272\u2013303 , Belgium , Brussels . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Gon\u00e7alo M. Correia , Vlad Niculae , and Andr\u00e9 F. T. Martins .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Adaptively sparse transformers .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2174 \u2013 2184 , Hong Kong , China . Association for Computational Linguistics .", "entities": []}, {"text": "Raj Dabre , Chenhui Chu , and Anoop Kunchukuttan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A survey of multilingual neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "ACM Computing Surveys ( CSUR ) , 53(5):1\u201338 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Ameet Deshpande and Karthik Narasimhan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Guiding attention for self - supervised learning with transformers .", "entities": [[3, 7, "TaskName", "self - supervised learning"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 4676\u20134686 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zi - Yi Dou and Graham Neubig .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Word alignment by \ufb01ne - tuning embeddings on parallel corpora .", "entities": [[0, 2, "TaskName", "Word alignment"]]}, {"text": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics : Main Volume , pages 2112\u20132128 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Carlos Escolano , Marta R. Costa - juss\u00e0 , Jos\u00e9 A. R. Fonollosa , and Mikel Artetxe .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Multilingual machine translation : Closing the gap between shared and language - speci\ufb01c encoder - decoders .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics : Main Volume , pages 944\u2013948 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "8454Orhan Firat , Baskaran Sankaran , Yaser Al - onaizan , Fatos T. Yarman Vural , and Kyunghyun Cho . 2016 .", "entities": []}, {"text": "Zero - resource translation with multi - lingual neural machine translation .", "entities": [[9, 11, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 268\u2013277 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Markus Freitag and Orhan Firat .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Complete multilingual neural machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Fifth Conference on Machine Translation , pages 550\u2013560 , Online .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sarthak Garg , Stephan Peitz , Udhyakumar Nallasamy , and Matthias Paulik .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Jointly learning to align and translate with transformer models .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 4453\u20134462 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Naman Goyal , Cynthia Gao , Vishrav Chaudhary , PengJen Chen , Guillaume Wenzek , Da Ju , Sanjana Krishnan , Marc\u2019Aurelio Ranzato , Francisco Guzman , and Angela Fan . 2021 .", "entities": []}, {"text": "The \ufb02ores-101 evaluation benchmark for low - resource and multilingual machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:2106.03193 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Thanh - Le Ha , Jan Niehues , and Alexander Waibel .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Toward multilingual neural machine translation with universal encoder and decoder .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1611.04798 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Melvin Johnson , Mike Schuster , Quoc V .", "entities": []}, {"text": "Le , Maxim Krikun , Yonghui Wu , Zhifeng Chen , Nikhil Thorat , Fernanda Vi\u00e9gas , Martin Wattenberg , Greg Corrado , Macduff Hughes , and Jeffrey Dean . 2017 .", "entities": []}, {"text": "Google \u2019s multilingual neural machine translation system : Enabling zero - shot translation .", "entities": [[0, 1, "DatasetName", "Google"], [4, 6, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 5:339\u2013351 .", "entities": []}, {"text": "Armand Joulin , Edouard Grave , Piotr Bojanowski , and Tomas Mikolov .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Bag of tricks for ef\ufb01cient text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers , pages 427\u2013431 , Valencia , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Diederik P Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Guillaume Klein , Yoon Kim , Yuntian Deng , Jean Senellart , and Alexander Rush . 2017 .", "entities": []}, {"text": "OpenNMT : Opensource toolkit for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of ACL 2017 , System Demonstrations , pages 67\u201372 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Taku Kudo and John Richardson .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SentencePiece : A simple and language independent subword tokenizer and detokenizer for neural text processing .", "entities": [[0, 1, "MethodName", "SentencePiece"]]}, {"text": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 66\u201371 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jason Lee , Kyunghyun Cho , and Thomas Hofmann . 2017 .", "entities": []}, {"text": "Fully character - level neural machine translation without explicit segmentation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 5:365\u2013378 .", "entities": []}, {"text": "Zehui Lin , Liwei Wu , Mingxuan Wang , and Lei Li . 2021 .", "entities": []}, {"text": "Learning language speci\ufb01c sub - network for multilingual machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 293\u2013305 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "David Mare \u02c7cek and Rudolf Rosa . 2019 .", "entities": []}, {"text": "From balustrades to pierre vinken : Looking for syntax in transformer self - attentions .", "entities": []}, {"text": "In Proceedings of the 2019 ACL Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 263 \u2013 275 , Florence , Italy .", "entities": [[22, 23, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Franz Josef Och and Hermann Ney .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "A systematic comparison of various statistical alignment models .", "entities": []}, {"text": "Computational Linguistics , 29(1):19\u201351 .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "BLEU :", "entities": [[0, 1, "MetricName", "BLEU"]]}, {"text": "A method for automatic evaluation of machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311\u2013318 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ben Peters , Vlad Niculae , and Andr\u00e9 F. T. Martins .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Sparse sequence - to - sequence models .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1504 \u2013 1519 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Ngoc - Quan Pham , Jan Niehues , Thanh - Le Ha , and Alexander Waibel .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Improving zero - shot translation with language - independent constraints .", "entities": []}, {"text": "In Proceedings of the Fourth Conference on Machine Translation ( Volume 1 : Research Papers ) , pages 13 \u2013 23 , Florence , Italy .", "entities": [[7, 9, "TaskName", "Machine Translation"], [22, 23, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Matt Post . 2018 .", "entities": []}, {"text": "A call for clarity in reporting BLEU scores .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 186 \u2013 191 , Brussels , Belgium .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ye Qi , Devendra Sachan , Matthieu Felix , Sarguna Padmanabhan , and Graham Neubig .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "When and why are pre - trained word embeddings useful for neural machine translation ?", "entities": [[7, 9, "TaskName", "word embeddings"], [12, 14, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the", "entities": []}, {"text": "8455Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 529\u2013535 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alessandro Raganato , Yves Scherrer , and J\u00f6rg Tiedemann .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Fixed encoder self - attention patterns in transformer - based machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 556\u2013568 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alessandro Raganato and J\u00f6rg Tiedemann .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "An analysis of encoder representations in transformerbased machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 287\u2013297 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Annette Rios , Mathias M\u00fcller , and Rico Sennrich .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Subword segmentation and a single bridge language affect zero - shot neural machine translation .", "entities": [[12, 14, "TaskName", "machine translation"]]}, {"text": "InProceedings of the Fifth Conference on Machine Translation , pages 528\u2013537 , Online .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Kai Song , Kun Wang , Heng Yu , Yue Zhang , Zhongqiang Huang , Weihua Luo , Xiangyu Duan , and Min Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Alignment - enhanced transformer for constraining nmt with pre - speci\ufb01ed translations .", "entities": []}, {"text": "Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 34(05):8886\u20138893 .", "entities": []}, {"text": "Gongbo Tang , Rico Sennrich , and Joakim Nivre .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "An analysis of attention mechanisms : The case of word sense disambiguation in neural machine translation .", "entities": [[9, 12, "TaskName", "word sense disambiguation"], [14, 16, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 26 \u2013 35 , Brussels , Belgium .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "J\u00f6rg Tiedemann .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Bitext alignment .", "entities": []}, {"text": "Synthesis Lectures on Human Language Technologies , 4(2):1 \u2013 165 .", "entities": []}, {"text": "J\u00f6rg Tiedemann .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Parallel data , tools and interfaces in OPUS .", "entities": []}, {"text": "In Proceedings of the Eighth International Conference on Language Resources and Evaluation ( LREC\u201912 ) , pages 2214\u20132218 , Istanbul , Turkey .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "J\u00f6rg Tiedemann .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "The tatoeba translation challenge \u2013 realistic data sets for low resource and multilingual MT .", "entities": [[1, 4, "DatasetName", "tatoeba translation challenge"]]}, {"text": "In Proceedings of the Fifth Conference on Machine Translation , pages 1174\u20131182 , Online .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 5998\u20136008 .", "entities": []}, {"text": "Ra\u00fal V\u00e1zquez , Alessandro Raganato , Mathias Creutz , and J\u00f6rg Tiedemann .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A systematic study of inner - attention - based sentence representations in multilingual neural machine translation .", "entities": [[14, 16, "TaskName", "machine translation"]]}, {"text": "Computational Linguistics , 46(2):387\u2013424 .", "entities": []}, {"text": "Elena V oita , David Talbot , Fedor Moiseev , Rico Sennrich , and Ivan Titov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Analyzing multi - head self - attention : Specialized heads do the heavy lifting , the rest can be pruned .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5797\u20135808 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Xinyi Wang , Hieu Pham , Philip Arthur , and Graham Neubig .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Multilingual neural machine translation with soft decoupled encoding .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Yiren Wang , ChengXiang Zhai , and Hany Hassan . 2020 .", "entities": []}, {"text": "Multi - task learning for multilingual neural machine translation .", "entities": [[0, 4, "TaskName", "Multi - task learning"], [7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1022\u20131034 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Liwei Wu , Shanbo Cheng , Mingxuan Wang , and Lei Li .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Language tags matter for zero - shot neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In Findings of the Association for Computational Linguistics : ACL - IJCNLP 2021 , pages 3001\u20133007 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Biao Zhang , Ankur Bapna , Rico Sennrich , and Orhan Firat . 2021 .", "entities": []}, {"text": "Share or not ? learning to schedule language - speci\ufb01c capacity for multilingual translation .", "entities": []}, {"text": "In International Conference on Learning Representations ( ICLR ) 2021 .", "entities": []}, {"text": "Biao Zhang , Ivan Titov , and Rico Sennrich .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Improving deep transformer with depth - scaled initialization and merged attention .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 898\u2013909 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Biao Zhang , Philip Williams , Ivan Titov , and Rico Sennrich .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Improving massively multilingual neural machine translation and zero - shot translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1628 \u2013 1639 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "8456A Data and Model details A.1 Data TED Talks ( Qi et al . , 2018 ) .", "entities": []}, {"text": "This parallel corpus includes 59 language pairs from and to English .", "entities": []}, {"text": "It is a highly imbalanced benchmark , ranging from less than 4 K up to 215 K training sentences .", "entities": []}, {"text": "We use the same languages as Aharoni et al .", "entities": []}, {"text": "( 2019 ) for both supervised testing and zero - shot evaluation .", "entities": []}, {"text": "As supervised test sets , we use { Azerbeijani , Belarusian , Galician , Slovak , Arabic , German , Hebrew , Italian}$English .", "entities": []}, {"text": "As zero - shot test sets , we use Arabic$French , and Ukrainian $ Russian .", "entities": []}, {"text": "WMT-2018 ( Bojar et al . , 2018 ) .", "entities": []}, {"text": "We use training and testing data as provided by the WMT 2018 news translation task organizers .", "entities": [[10, 13, "DatasetName", "WMT 2018 news"]]}, {"text": "The benchmark contains a total of 14 language pairs : { Chinese , Czech , Estonian , Finnish , German , Russian , Turkish}$English .", "entities": [[0, 2, "DatasetName", "The benchmark"]]}, {"text": "For training , we use up to 5 M parallel sentences per language pair , with Turkish$English , Estonian$English , and Finnish$English , having only 200 K , 1 M , and 2.7 M training sentences , respectively .", "entities": []}, {"text": "For zeroshot test sets , we use the test data from Tiedemann ( 2020 ) , using the following 24 language directions : Czech$German , German $ Russian , German$Chinese , Finnish $ German , Finnish$Turkish , Russian $ Finnish , Russian$Chinese , Turkish $ Chinese , Czech$Russian , German $ Turkish , Estonian$Russian , Russian $ Turkish OPUS-100 ( Zhang et al . , 2020 ) .", "entities": [[58, 59, "DatasetName", "OPUS-100"]]}, {"text": "OPUS-100 is a recent benchmark consisting of 55 M Englishcentric sentence pairs covering 100 languages .", "entities": [[0, 1, "DatasetName", "OPUS-100"]]}, {"text": "The data is collected from movie subtitles , GNOME documentation , and the Bible .", "entities": []}, {"text": "Out of 99 language pairs , 44 have 1 M sentences , 73 have at least 100 K sentences , and 95 at least 10K. It provides also zero - shot test sets , pairing the following languages : Arabic , Chinese , Dutch , French , German , and Russian .", "entities": []}, {"text": "A.2 Model hyperparameters We use the OpenNMT - py framework ( Klein et al . , 2017 ) , and the Transformer base model setting ( Vaswani et al . , 2017 ) .", "entities": [[21, 22, "MethodName", "Transformer"]]}, {"text": "Speci\ufb01cally , we use 6 layers for the encoder and the decoder , 512 as model dimension , and 2048 as hidden dimension.#Lang .", "entities": [[19, 20, "DatasetName", "2048"]]}, {"text": "# Train .", "entities": []}, {"text": "# Zero - shot pairs sent .", "entities": []}, {"text": "lang . pairs TED Talks 116 10 M 4 WMT-2018 14 47 M 24 OPUS-100 198 110 M 30 Table 4 : Benchmark statistics : number of language pairs used for training , total number of training sentences , and number of language pairs for zero - shot evaluation .", "entities": [[14, 15, "DatasetName", "OPUS-100"]]}, {"text": "We applied 0.1 as dropout for both residual layers and attention weights , using the Adam optimizer ( Kingma and Ba , 2015 ) with \f 1 = 0:9 , and \f 2 = 0:998 , with learning rate set at 3 and 40 K warmup steps as in Aharoni et al .", "entities": [[15, 16, "MethodName", "Adam"], [16, 17, "HyperparameterName", "optimizer"], [37, 39, "HyperparameterName", "learning rate"]]}, {"text": "( 2019 ) .", "entities": []}, {"text": "We train the models with three random seeds each , for 200 K training steps for the TED Talks and WMT-2018 benchmarks , while for 500 K training steps for the OPUS-100 .", "entities": [[7, 8, "DatasetName", "seeds"], [31, 32, "DatasetName", "OPUS-100"]]}, {"text": "To speed up training , we use halfprecision , i.e. , FP16 .", "entities": []}]