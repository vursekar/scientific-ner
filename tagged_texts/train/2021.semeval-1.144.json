[{"text": "Proceedings of the 15th International Workshop on Semantic Evaluation ( SemEval-2021 ) , pages 1045\u20131050 Bangkok , Thailand ( online ) , August 5\u20136 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics1045YNU - HPCC at SemEval-2021 Task 6 : Combining ALBERT and Text - CNN for Persuasion Detection in Texts and Images Xingyu Zhu , Jin Wang and Xuejie Zhang School of Information Science and Engineering Yunnan University Kunming , China Contact : xyzhu@mail.ynu.edu.cn , fwangjin , xjzhang g@ynu.edu.cn", "entities": [[14, 15, "MethodName", "ALBERT"]]}, {"text": "Abstract In recent years , memes combining image and text have been widely used in social media , and memes are one of the most popular types of content used in online disinformation campaigns .", "entities": []}, {"text": "In this paper , our study on the detection of persuasion techniques in texts and images in SemEval-2021 Task 6 is summarized .", "entities": []}, {"text": "For propaganda technology detection in text , we propose a combination model of both ALBERT and Text - CNN for text classi\ufb01cation , as well as a BERT - based multi - task sequence labeling model for propaganda technology coverage span detection .", "entities": [[14, 15, "MethodName", "ALBERT"], [27, 28, "MethodName", "BERT"]]}, {"text": "For the meme classi\ufb01cation task involved in text understanding and visual feature extraction , we designed a parallel channel model divided into text and image channels .", "entities": []}, {"text": "Our method1achieved a good performance on subtasks 1 and 3 .", "entities": []}, {"text": "The micro F1scores of 0.492 , 0.091 , and 0.446 achieved on the test sets of the three subtasks ranked 12th , 7th , and 11th , respectively , and all are higher than the baseline model .", "entities": []}, {"text": "1 Introduction The intentional shaping of information to promote a predetermined agenda is called propaganda .", "entities": []}, {"text": "Propaganda uses psychological and rhetorical techniques to achieve its purpose .", "entities": []}, {"text": "Propaganda techniques generally include the use of logical fallacies and appeal to the emotions of the audience .", "entities": [[7, 9, "TaskName", "logical fallacies"]]}, {"text": "In recent years , memes combining images and text have been widely used in social media , and the use of memes can easily and effectively attract a large number of users on social platforms .", "entities": []}, {"text": "Memes are one of the most popular types of content used in online disinformation campaigns ( Martino et al . , 2020 ) , and memes applied in a disinformation campaign achieve their purpose of in\ufb02uencing users through 1The code of this paper is availabled at : https:// github.com/zxyqujing/SemEval-2021-task6rhetorical and psychological techniques .", "entities": []}, {"text": "Therefore , it is meaningful to research computational techniques for automatically detecting propaganda in particular content .", "entities": []}, {"text": "The SemEval 2021 Task 6 ( Dimitrov et al . , 2021 ) consists of three subtasks : \u2022Subtask 1 - Given only the \u201c textual content \u201d of a meme , identify which of the 20 techniques are used .", "entities": []}, {"text": "The 20 techniques include appeal to authority , loaded language , and name calling or labeling .", "entities": []}, {"text": "\u2022Subtask 2 : Given only the \u201c textual content \u201d of a meme , identify which of the 20 techniques are used along with the span(s ) of the text covered by each technique .", "entities": []}, {"text": "\u2022Subtask 3 : Given a meme , identify which of the 22 techniques are used for both the textual and visual content of the meme .", "entities": []}, {"text": "These 22 technologies include the 20 technologies in subtasks 1 and 2 , and 2 technologies , i.e. , transfer and appeal to ( strong ) emotions , are added .", "entities": []}, {"text": "The detection of propaganda techniques in texts is similar to a text sentiment analysis , and both can be attributed to text classi\ufb01cation tasks .", "entities": [[12, 14, "TaskName", "sentiment analysis"]]}, {"text": "In a previous study , Peng et al . ( 2020 ) used the adversarial learning of sentiment word representations for a sentiment analysis .", "entities": [[22, 24, "TaskName", "sentiment analysis"]]}, {"text": "A tree - structured regional CNN - LSTM ( Wang et al . , 2020 ) and dynamic routing in a tree - structured LSTM ( Wang et al . , 2019 ) were used for a dimensional sentiment analysis .", "entities": [[7, 8, "MethodName", "LSTM"], [24, 25, "MethodName", "LSTM"], [38, 40, "TaskName", "sentiment analysis"]]}, {"text": "In previous SemEval competitions , Dao et al .", "entities": []}, {"text": "( 2020 ) used GloVe - LSTM and BERT - LSTM models , and Paraschiv et", "entities": [[4, 5, "MethodName", "GloVe"], [6, 7, "MethodName", "LSTM"], [8, 9, "MethodName", "BERT"], [10, 11, "MethodName", "LSTM"]]}, {"text": "al .", "entities": []}, {"text": "( 2020 ) used an ensemble model containing BERT and BiLSTM to detect both spans and categories of propaganda techniques in news articles ( Da San Martino et al . , 2020 ) .", "entities": [[8, 9, "MethodName", "BERT"], [10, 11, "MethodName", "BiLSTM"]]}, {"text": "In addition , in multimodal analysis combining images and", "entities": []}, {"text": "1046 \u2026 ALBERT Hidden Representation ( 512,768)w1,w2,w3,w4, \u2026 \u2026 ,wn-1,wnInput", "entities": [[2, 3, "MethodName", "ALBERT"]]}, {"text": "Text Conv1 Kernel_size = 3Conv2 Kernel_size = 4Conv3 Kernel_size = 5 MaxPool_1 MaxPool_2 MaxPool_2 Flatten Flatten Flatten Concatenate Dense1 ( 768 units relu ) ( 20 units sigmoid ) \u2026 Text - CNN Dense2 Output ( 1,20 )    Figure 1 : ALBERT - Text - CNN model architecture .", "entities": [[22, 23, "MethodName", "relu"], [42, 43, "MethodName", "ALBERT"]]}, {"text": "text , Yuan et al .", "entities": []}, {"text": "( 2020 ) proposed a parallel channel ensemble model combining BERT embedding , BiLSTM , attention and CNN , and ResNet for a sentiment analysis of memes .", "entities": [[10, 11, "MethodName", "BERT"], [13, 14, "MethodName", "BiLSTM"], [20, 21, "MethodName", "ResNet"], [23, 25, "TaskName", "sentiment analysis"]]}, {"text": "Li et al .", "entities": []}, {"text": "( 2019 ) proposed a Visual BERT model that aligns and fuses text and image information using transformers ( Vaswani et al . , 2017 ) .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "In this paper , we propose three different systems for the three subtasks in SemEval-2021 Task 6 .", "entities": []}, {"text": "For subtask 1 , we added a Text - CNN layer after the pre - trained model ALBERT to \ufb01ne - tune it for a multi - label classi\ufb01cation of text .", "entities": [[17, 18, "MethodName", "ALBERT"]]}, {"text": "For subtask 2 , we used the idea of partitioning to transform the problem into the detection of 20 techniques for each text separately .", "entities": []}, {"text": "BERT was used in the model for text feature extraction followed by multi - task sequence labeling , and the results of each task were combined to obtain the \ufb01nal results .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "For subtask 3 , we built the system using a parallel channel model containing text and image channels .", "entities": []}, {"text": "The text channel used both the ALBERT and Text - CNN models to extract features of text in the meme , and the image channel used ResNet and VGGNet for image feature extraction .", "entities": [[6, 7, "MethodName", "ALBERT"], [26, 27, "MethodName", "ResNet"]]}, {"text": "The information extracted by the two parallel channels was then combined through a fully connected layer after concatenation .", "entities": []}, {"text": "Using micro F1 - scores as metrics , the results of the proposed model in subtasks 1 , 2 , and 3 were 0.625 , 0.215 , and 0.636 , respectively , on the dev set .", "entities": [[1, 3, "MetricName", "micro F1"]]}, {"text": "The remainder of this paper is organized as follows .", "entities": []}, {"text": "First , section 2 describes the details of the w1,w2,w3,w4, \u2026 \u2026 ,wn-1,wnInput Text BERT \u2026 Hidden Representation ( 512,768 ) Dense1 Dense2 Dense3 Dense20 \u2026   Ensemblesoftmax softmax softmax softmax \u2026   Intermediate Results ( 512 , 41 ) Output ( 20 , 512 , 41 ) Figure 2 : Architecture of multi - task sequence labeling model .", "entities": [[16, 17, "MethodName", "BERT"], [30, 31, "MethodName", "softmax"], [31, 32, "MethodName", "softmax"], [32, 33, "MethodName", "softmax"]]}, {"text": "ALBERT and Text - CNN used in our system .", "entities": [[0, 1, "MethodName", "ALBERT"]]}, {"text": "Section 3 then presents the experimental results .", "entities": []}, {"text": "Finally , some concluding remarks are presented in section 4 . 2 System Overview 2.1 Subtask 1 Subtask 1 requires a detection model that uses only the textual features of the meme content and detects which of the 20 propaganda techniques were used .", "entities": []}, {"text": "This is a multi - label classi\ufb01cation problem for text , based on the pre - trained ALBERT model and added a Text - CNN layer .", "entities": [[17, 18, "MethodName", "ALBERT"]]}, {"text": "As illustrated in Figure 2 , the proposed model includes an ALBERT layer , a TextCNN layer , a fully connected layer , and an output layer .", "entities": [[11, 12, "MethodName", "ALBERT"]]}, {"text": "\u2022ALBERT ( Lan et al . , 2020 ) is a lite BERT for self - supervised learning of language representations , which uses layer - to - layer parameter sharing to reduce the number of parameters of the model , which not only speeds up the model training but also outperforms BERT on certain datasets .", "entities": [[12, 13, "MethodName", "BERT"], [14, 18, "TaskName", "self - supervised learning"], [34, 37, "HyperparameterName", "number of parameters"], [52, 53, "MethodName", "BERT"]]}, {"text": "With our model , the pretrained ALBERT model is \ufb01ne - tuned to obtain a 512 \u00d7768 hidden representation matrix for subsequent multi - label classi\ufb01cation of text .", "entities": [[6, 7, "MethodName", "ALBERT"]]}, {"text": "\u2022Text - CNN ( Kim , 2014 ) is a convolutional neural network applied to a text classi\ufb01cation task , using multiple kernels of different sizes to extract key information in sentences , and is thus able to better capture the local relevance .", "entities": []}, {"text": "In", "entities": []}, {"text": "1047    Image input size(244,244,3)Text input ALBERT \u2026 Hidden Representation ( 512,768)ResNet18 / VGG16 Text - CNNw1,w2,w3,w4, \u2026 \u2026 ,wn-1,wn", "entities": [[6, 7, "MethodName", "ALBERT"]]}, {"text": "Flatten Text Feature \uff08 1,768\uff09Image Feature ( 1,512 ) Concatenate(1,1280 ) Dense ( 22 units , sigmoid ) \u2026 Output ( 1,22)Image   Figure 3 : Architecture of parallel channel model .", "entities": []}, {"text": "this layer , we used three different sizes of onedimensional convolution kernels , i.e. , 3 , 4 , and 5 , to extract information from the hidden representation matrix output from the ALBERT layer for the \ufb01nal multi - label text classi\ufb01cation .", "entities": [[10, 11, "MethodName", "convolution"], [33, 34, "MethodName", "ALBERT"]]}, {"text": "2.2 Subtask 2 Subtask 2 was a multi - label sequence - labeling task .", "entities": []}, {"text": "We built the model by converting the problem to detect the coverage of each propagation technique separately for the input sequence , and built a multi - task sequence labeling model based on a \ufb01ne - tuning of BERT .", "entities": [[38, 39, "MethodName", "BERT"]]}, {"text": "As illustrated in Figure 3 , the input sequence was \ufb01rst obtained using the pre - trained BERT ( Devlin et al . , 2019 ) model with a hidden representation matrix with dimensions of 512 \u00d7768 .", "entities": [[17, 18, "MethodName", "BERT"]]}, {"text": "Subsequently , 20 parallel fully connected layers were input separately for the detection of each propaganda technique coverage span ( For each propagation technique , the sequence labeling task is performed separately for the input text ) .", "entities": []}, {"text": "For each technique , the intermediate result of each parallel channel output is a 512 \u00d741 matrix , and the ensemble layer represents the stacking of 20 matrices from 20 parallel channels , the dimensions of the \ufb01nal output were 20 \u00d7512\u00d741 , which denote the propaganda technique category , maximum sentence length , and code corresponding to each technique , respectively.2.3 Subtask 3 For subtask 3 , we modeled the problem as a multilabel classi\ufb01cation task of the meme text and image content .", "entities": []}, {"text": "We used a parallel channel model of text and image channels , and then concatenated the text and image features extracted by the two parallel channels to apply multi - label meme classi\ufb01cation .", "entities": []}, {"text": "The architecture of the proposed model is shown in Figure 4 .", "entities": []}, {"text": "Text Channel .", "entities": []}, {"text": "In the text channel , we used the ALBERT \u2013 Text - CNN model used in subtask 1 , taking the text part of the meme content as an input to obtain a 768 - dimensional text feature vector as the output .", "entities": [[8, 9, "MethodName", "ALBERT"]]}, {"text": "Image Channel .", "entities": []}, {"text": "In the image channel , we used ResNet and VGGNet , taking the image part of the meme content as input to obtain a 512 - dimensional image feature vector as the output .", "entities": [[7, 8, "MethodName", "ResNet"]]}, {"text": "The ResNet model ( He et al . , 2016 ) is a deep residual learning model for image recognition , and presents the interlayer residual jump connection and solves the deep vanishing gradient problem .", "entities": [[1, 2, "MethodName", "ResNet"], [18, 20, "TaskName", "image recognition"]]}, {"text": "VGGNet ( Simonyan and Zisserman , 2015 ) is a deep convolutional neural network with small - sized convolutional kernels and a regular network structure , in which the size of the convolution kernels used in VGG16 in our experiment is 3 \u00d73 , and the pooling kernels is 2 \u00d7 2 .", "entities": [[32, 33, "MethodName", "convolution"]]}, {"text": "Furthermore , only the structures of the ResNet and VGGNet were used in our experiment , and the pre - training weights were not applied .", "entities": [[7, 8, "MethodName", "ResNet"]]}, {"text": "10483 Experimental Results 3.1 Dataset The organizer provided a dataset containing 687 memes for the training set , 63 memes for the development set , and 200 memes for the test set .", "entities": []}, {"text": "The dataset of subtask 1 provides the ID , text of the meme , and the corresponding propaganda techniques used , and the dataset of subtask 3 also contains the corresponding meme image .", "entities": []}, {"text": "The dataset of subtask 2 provides the ID , text of the meme , and the corresponding propaganda techniques used in a certain text fragment , in which the scope covered by the propaganda technology in the text is marked as \u201c start , \u201d \u201c end , \u201d and \u201c text fragment , \u201d respectively .", "entities": []}, {"text": "The datasets were preprocessed using the following procedures before model training : \u2022In subtasks 1 and 3 , we \ufb01rst used one - hot encoding to encode the label into a vector whose length is the total number of technology categories .", "entities": []}, {"text": "\u2022In subtask 2 , we labeled each token in the text as \u201c I - technique \u201d and \u201c O - technique \u201d based on the 20 propaganda technology terms .", "entities": []}, {"text": "\u201c Itechnique \u201d indicates that the publicity technique was used and \u201c O - technique \u201d indicates that it was not , e.g. , O - Smears and I - Smears .", "entities": []}, {"text": "For 20 different propaganda techniques there are 40 different codes , and then add another padding code , so the label code length is 41 .", "entities": []}, {"text": "\u2022In subtask 3 , we normalized the meme image size to 224 \u00d7 224 \u00d7 3 . 3.2 Evaluation Metrics", "entities": []}, {"text": "The of\ufb01cial evaluation measure for all subtasks is the micro F1 - score , which is de\ufb01ned as follows : F1\u0000score = 2\u0003Prec\u0003Rec Prec", "entities": [[10, 13, "MetricName", "F1 - score"]]}, {"text": "+ Rec(1 )", "entities": []}, {"text": "where Prec andRecdenote the precision and recall scores of all samples , respectively .", "entities": []}, {"text": "For subtask 2 , the standard micro F1 - score was slightly modi\ufb01ed to account for partial matching between spans ( Dimitrov et al . , 2021 ) .", "entities": [[7, 10, "MetricName", "F1 - score"]]}, {"text": "In addition , the macro F1 - score was also reported for each type of propaganda .", "entities": [[5, 8, "MetricName", "F1 - score"]]}, {"text": "3.3 Implementation Details All models used the TensorFlow2 backend , and all BERT - based models were implemented usingthe HuggingFace Transformers toolkit(Wolf et al . , 2020 ) .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "The Adam optimizer ( Ba and Kingma , 2015 ) was used to update all trainable parameters .", "entities": [[1, 2, "MethodName", "Adam"], [2, 3, "HyperparameterName", "optimizer"]]}, {"text": "The loss functions in subtasks 1 and 3 were binary cross - entropy , and subtask 2 was categorical cross - entropy .", "entities": [[1, 2, "MetricName", "loss"]]}, {"text": "The hyper - parameters in the model training process were obtained using a grid - search strategy , as shown in Table 1 .", "entities": []}, {"text": "Once the optimal settings of the parameters were obtained , they were used for classi\ufb01cation on the test sets of different corpora .", "entities": []}, {"text": "Hyper - parameter Values Learning rate 5e-6 Adam epsilon 1e-8 Text - CNN dropout 0.3", "entities": [[4, 6, "HyperparameterName", "Learning rate"], [7, 8, "MethodName", "Adam"], [8, 9, "HyperparameterName", "epsilon"]]}, {"text": "Text - CNN \ufb01lters 64 Classi\ufb01er dropout 0.3 Batch size 16 Table 1 : Hyper - parameters in our models .", "entities": [[8, 10, "HyperparameterName", "Batch size"]]}, {"text": "3.4 Results Table 2 presents the results of Subtask 1 .", "entities": []}, {"text": "We conducted experiments on several pre - trained models including BERT , RoBERTa(Liu et al . , 2019 ) , and ALBERT combined with the Text - CNN layer , and observed that the ALBERT and Text - CNN models achieved the best performance , the reason for which may be that the training datasets are small , and a serious over\ufb01tting will occur by directly \ufb01netuning BERT .", "entities": [[10, 11, "MethodName", "BERT"], [21, 22, "MethodName", "ALBERT"], [34, 35, "MethodName", "ALBERT"], [67, 68, "MethodName", "BERT"]]}, {"text": "Furthermore , the experiments show that the ALBERT model has fewer parameters and performs better on small datasets .", "entities": [[7, 8, "MethodName", "ALBERT"]]}, {"text": "Adding a TextCNN layer after the BERT - based model can better extract the local relevance information of the text , which not only effectively alleviates the over\ufb01tting phenomenon it also effectively improves the model performance .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "In subtask 2 , the results of our proposed multitask sequence labeling model on the dev set are F1 - score of 0.215 , Precision of 0.378 , and Recall of 0.151 .", "entities": [[18, 21, "MetricName", "F1 - score"], [24, 25, "MetricName", "Precision"], [29, 30, "MetricName", "Recall"]]}, {"text": "The results on the test set are F1 - score of 0.091 , Precision of 0.186 , and Recall of 0.061 .", "entities": [[7, 10, "MetricName", "F1 - score"], [13, 14, "MetricName", "Precision"], [18, 19, "MetricName", "Recall"]]}, {"text": "Table 3 shows the results of Subtask 3 .", "entities": []}, {"text": "It can be observed that ResNet18 works better than VGG16 when using both ALBERT and ALBERT - TextCNN models .", "entities": [[13, 14, "MethodName", "ALBERT"], [15, 16, "MethodName", "ALBERT"]]}, {"text": "The performance was improved by adding a Text - CNN layer to the text channel .", "entities": []}, {"text": "Considering that the micro F1 - scores are relatively close , we selected the models with the top - three F1-", "entities": [[3, 5, "MetricName", "micro F1"]]}, {"text": "1049Model F1 - Macro F1 - Micro BERT 0.302 0.509 RoBERTa - Text - CNN 0.385 0.500 BERT - Text - CNN 0.414 0.560 ALBERT - Text - CNN 0.472 0.625 Table 2 : Scores of different models for subtask 1 on dev set .", "entities": [[1, 2, "MetricName", "F1"], [3, 5, "MetricName", "Macro F1"], [7, 8, "MethodName", "BERT"], [10, 11, "MethodName", "RoBERTa"], [17, 18, "MethodName", "BERT"], [24, 25, "MethodName", "ALBERT"]]}, {"text": "Model F1 - Macro F1 - Micro ALBERT - VGG16 0.240 0.577 ALBERT - ResNet18 0.272 0.605 ALBERT - Text - CNN+VGG16 0.346 0.606 ALBERT - Text - CNN+ResNet18 0.247 0.636 Hard V oting 0.245 0.625 Table 3 : Experimental results of different models for subtask 3 on dev set .", "entities": [[1, 2, "MetricName", "F1"], [3, 5, "MetricName", "Macro F1"], [7, 8, "MethodName", "ALBERT"], [12, 13, "MethodName", "ALBERT"], [17, 18, "MethodName", "ALBERT"], [24, 25, "MethodName", "ALBERT"]]}, {"text": "scores and used hard voting to generate the results for comparison .", "entities": []}, {"text": "For all three subtasks , the proposed systems achieved micro F1 - scores of 0.492 , 0.091 , and 0.446 on the test set , respectively .", "entities": [[9, 11, "MetricName", "micro F1"]]}, {"text": "The results of all models exceeded the baseline .", "entities": []}, {"text": "However , there is a considerable decrease compared to the scores of 0.625 , 0.215 , and 0.636 achieved on the dev set .", "entities": []}, {"text": "4 Conclusions In this paper , we presented our system for the SemEval-2021 Task 6 , the experimental results in subtasks 1 and 3 show that our proposed ALBERTText - CNN model and the parallel channel model achieved a good performance in the detection of persuasion techniques in texts and images .", "entities": []}, {"text": "We participated in all three subtasks and achieved the 12th , 7th , and 11th places in the test set , respectively .", "entities": []}, {"text": "In a future study , to improve the generalization ability of the model , we will focus on how to deal with the problems caused by unbalanced training data .", "entities": []}, {"text": "Acknowledgements This work was supported by the National Natural Science Foundation of China ( NSFC ) under Grants Nos . 61702443 , 61966038 and 61762091 .", "entities": []}, {"text": "References Jimmy Lei Ba and Diederik P. Kingma .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In Proceedings of the 3rd International Conference on Learning Representations ( ICLR-2015 ) , pages 1\u201315.Giovanni", "entities": []}, {"text": "Da San Martino , Alberto Barr \u00b4 on - Cede \u02dcno , Henning Wachsmuth , Rostislav Petrov , and Preslav Nakov .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "SemEval-2020 task 11 : Detection of propaganda techniques in news articles .", "entities": []}, {"text": "In Proceedings of the Fourteenth Workshop on Semantic Evaluation ( SemEval-2020 ) , pages 1377\u20131414 , Barcelona ( online ) .", "entities": []}, {"text": "Jiaxu Dao , Jin Wang , and Xuejie Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "YNUHPCC at SemEval-2020 task 11 : LSTM network for detection of propaganda techniques in news articles .", "entities": [[6, 7, "MethodName", "LSTM"]]}, {"text": "In Proceedings of the Fourteenth Workshop on Semantic Evaluation ( SemEval-2020 ) , pages 1509 \u2013 1515 , Barcelona ( online ) .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL-2019 ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Dimiter Dimitrov , Bishr Bin Ali , Shaden Shaar , Firoj Alam , Fabrizio Silvestri , Hamed Firooz , Preslav Nakov , and Giovanni Da San Martino . 2021 .", "entities": []}, {"text": "Task 6 at SemEval-2021 : Detection of persuasion techniques in texts and images .", "entities": []}, {"text": "In Proceedings of the 15th International Workshop on Semantic Evaluation ( SemEval-2021 ) , SemEval \u2019 21 , Bangkok , Thailand .", "entities": []}, {"text": "Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . 2016 .", "entities": []}, {"text": "Deep residual learning for image recognition .", "entities": [[4, 6, "TaskName", "image recognition"]]}, {"text": "In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR-2016 ) , pages 770\u2013778 .", "entities": []}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP-2014 ) , pages 1746 \u2013 1751 , Doha , Qatar .", "entities": []}, {"text": "1050Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , and Radu Soricut .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "ALBERT :", "entities": [[0, 1, "MethodName", "ALBERT"]]}, {"text": "A lite BERT for self - supervised learning of language representations .", "entities": [[2, 3, "MethodName", "BERT"], [4, 8, "TaskName", "self - supervised learning"]]}, {"text": "In Proceedings of the 8th International Conference on Learning Representations ( ICLR-2020 ) .", "entities": []}, {"text": "Liunian Harold Li , Mark Yatskar , Da Yin , Cho - Jui Hsieh , and Kai - Wei Chang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "VisualBERT : A simple and performant baseline for vision and language .", "entities": [[0, 1, "MethodName", "VisualBERT"]]}, {"text": "CoRR , abs/1908.03557 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "RoBERTa : A robustly optimized BERT pretraining approach .", "entities": [[0, 1, "MethodName", "RoBERTa"], [5, 6, "MethodName", "BERT"]]}, {"text": "CoRR , abs/1907.11692 .", "entities": []}, {"text": "Giovanni Da San Martino , Stefano Cresci , Alberto Barr\u00b4on - Cede \u02dcno , Seunghak Yu , Roberto Di Pietro , and Preslav Nakov .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A survey on computational propaganda detection .", "entities": []}, {"text": "In Proceedings of the Twenty - Ninth International Joint Conference on Arti\ufb01cial Intelligence ( IJCAI-2020 ) , pages 4826\u20134832 .", "entities": []}, {"text": "Andrei Paraschiv , Dumitru - Clementin Cercel , and Mihai Dascalu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "UPB at SemEval-2020 task 11 : Propaganda detection with domain - speci\ufb01c trained BERT .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "In Proceedings of the Fourteenth Workshop on Semantic Evaluation ( SemEval-2020 ) , pages 1853\u20131857 , Barcelona ( online ) .", "entities": []}, {"text": "Bo Peng , Jin Wang , and Xuejie Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Adversarial learning of sentiment word representations for sentiment analysis .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}, {"text": "Information Sciences , 541:426 \u2013 441 .", "entities": []}, {"text": "Karen Simonyan and Andrew Zisserman .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Very deep convolutional networks for large - scale image recognition .", "entities": [[8, 10, "TaskName", "image recognition"]]}, {"text": "In Proceedings of the 3rd International Conference on Learning Representations ( ICLR-2015 ) .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Proceedings of the Advances in Neural Information Processing Systems 30 : Annual Conference on Neural Information Processing Systems ( NIPS-2017 ) , pages 5998\u20136008 .", "entities": []}, {"text": "Jin Wang , Liang - Chih Yu , K. Robert Lai , and Xuejie Zhang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Investigating dynamic routing in treestructured LSTM for sentiment analysis .", "entities": [[5, 6, "MethodName", "LSTM"], [7, 9, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP-2019 ) , pages 3432\u20133437 , Hong Kong , China .", "entities": []}, {"text": "Jin Wang , Liang - Chih Yu , K. Robert Lai , and Xuejie Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Tree - structured regional cnnlstm model for dimensional sentiment analysis .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}, {"text": "IEEE / ACM Transactions on Audio , Speech , and Language Processing , 28:581\u2013591.Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , R \u00b4 emi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander M. Rush .", "entities": [[2, 3, "DatasetName", "ACM"]]}, {"text": "2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations ( EMNLP-2020 ) , pages 38 \u2013 45 .", "entities": []}, {"text": "Li Yuan , Jin Wang , and Xuejie Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "YNUHPCC at SemEval-2020 task 8 : Using a parallelchannel model for memotion analysis .", "entities": [[11, 13, "DatasetName", "memotion analysis"]]}, {"text": "In Proceedings of the Fourteenth Workshop on Semantic Evaluation ( SemEval-2020 ) , pages 916\u2013921 , Barcelona ( online ) .", "entities": []}]