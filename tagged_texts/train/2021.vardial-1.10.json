[{"text": "Proceedings of the 8th VarDial Workshop on NLP for Similar Languages , Varieties and Dialects , pages 84\u201395 April 20 , 2021 \u00a9 2021 Association for Computational Linguistics84UnibucKernel : Geolocating Swiss German Jodels Using Ensemble Learning Mihaela G \u02d8aman1 , Sebastian Cojocariu1 , Radu Tudor Ionescu1;2;\u0003 1Department of Computer Science,2Romanian Young Academy University of Bucharest 14 Academiei , Bucharest , Romania \u0003raducu.ionescu@gmail.com", "entities": [[34, 36, "TaskName", "Ensemble Learning"]]}, {"text": "Abstract", "entities": []}, {"text": "In this work , we describe our approach addressing the Social Media Variety Geolocation task featured in the 2021 VarDial Evaluation Campaign .", "entities": []}, {"text": "We focus on the second subtask , which is based on a data set formed of approximately 30 thousand Swiss German Jodels .", "entities": []}, {"text": "The dialect identi\ufb01cation task is about accurately predicting the latitude and longitude of test samples .", "entities": []}, {"text": "We frame the task as a double regression problem , employing an XGBoost metalearner with the combined power of a variety of machine learning approaches to predict both latitude and longitude .", "entities": []}, {"text": "The models included in our ensemble range from simple regression techniques , such as Support Vector Regression , to deep neural models , such as a hybrid neural network and a neural transformer .", "entities": []}, {"text": "To minimize the prediction error , we approach the problem from a few different perspectives and consider various types of features , from lowlevel character n - grams to high - level BERT embeddings .", "entities": [[32, 33, "MethodName", "BERT"]]}, {"text": "The XGBoost ensemble resulted from combining the power of the aforementioned methods achieves a median distance of 23:6 km on the test data , which places us on the third place in the ranking , at a difference of6:05 km and 2:9 km from the submissions on the \ufb01rst and second places , respectively .", "entities": []}, {"text": "1 Introduction The Social Media Variety Geolocation ( SMG ) task was proposed , for the second year consecutively , in the 2021 edition of the VarDial Evaluation Campaign ( Chakravarthi et al . , 2021 ) .", "entities": []}, {"text": "This task is aimed at geolocation prediction based on short text messages exchanged by the users of social media platforms such as Twitter or Jodel .", "entities": []}, {"text": "The location from where a short text was posted on a certain social media platform is expressed by two components : the latitude and the longitude .", "entities": []}, {"text": "Naturally , the geolocation task is formulated as a double regressionproblem .", "entities": []}, {"text": "Twitter and Jodel are the platforms used for data collection , and similar to the previous spin of SMG at VarDial 2020 ( G \u02d8aman et al . , 2020 ) , the task is divided into three subtasks , by language area , namely : \u2022Standard German Jodels ( DE - AT ) - which targets conversations initiated in Germany and Austria in regional dialectal forms ( Hovy and Purschke , 2018 ) .", "entities": []}, {"text": "\u2022Swiss German Jodels ( CH ) - containing a smaller number of Jodel conversations from the German speaking half of Switzerland ( Hovy and Purschke , 2018 ) .", "entities": []}, {"text": "\u2022BCMS Tweets - from the area of Bosnia and Herzegovina , Croatia , Montenegro and Serbia where the macro - language is BCMS , with both similarities and a fair share of variation among the component languages ( Ljube \u02c7si\u00b4c et", "entities": []}, {"text": "al . , 2016 ) .", "entities": []}, {"text": "The focus of our work falls only on the second subtask , SMG - CH , tackled via a variety of handcrafted and deep learning models .", "entities": []}, {"text": "We propose a single ensemble model joining the power of several individual models through meta - learning based on Extreme Gradient Boosting ( XGBoost ) ( Chen and Guestrin , 2016 ) .", "entities": [[14, 17, "TaskName", "meta - learning"]]}, {"text": "We trained two independent ensemble models , each predicting one of the components that form the geographical coordinates ( latitude and longitude ) .", "entities": []}, {"text": "The \ufb01rst model plugged into our meta - learner is a Support Vector Regression ( SVR ) model ( Chang and Lin , 2002 ) based on string kernels .", "entities": []}, {"text": "Previous usage in dialect identi\ufb01cation has proved the ef\ufb01ciency of this technique in the task of interest ( Butnaru and Ionescu , 2018b ; G \u02d8aman and Ionescu , 2020 ; Ionescu and Butnaru , 2017 ; Ionescu and Popescu , 2016 ) .", "entities": []}, {"text": "85The second model included in the ensemble is a hybrid convolutional neural network ( CNN ) ( Liang et al . , 2017 ) that combines , in the same architecture , character - level ( Zhang et al . , 2015 ) and word - level representations .", "entities": []}, {"text": "The ability of capturing morphological relationships at the character level and using them as features for CNNs is also known to give promising results in dialect identi\ufb01cation ( Butnaru and Ionescu , 2019 ; Tudoreanu , 2019 ) .", "entities": []}, {"text": "Different from works using solely character - level CNNs for dialect identi\ufb01cation ( Butnaru and Ionescu , 2019 ; Tudoreanu , 2019 ) , we believe that the addition of words might bring the bene\ufb01t of learning dialectspeci\ufb01c multi - word expressions that are hard to capture at the character level ( Dhingra et al . , 2016 ;", "entities": []}, {"text": "Ling et al . , 2015 ) .", "entities": []}, {"text": "Bidirectional Encoder Representations from Transformers ( BERT ) ( Devlin et al . , 2019 ) is a top performing technique used in recent years for solving mainstream NLP problems .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "Thus , it seems \ufb01t to also include the outputs of a \ufb01ne - tuned German version of BERT in our XGBoost meta - learner .", "entities": [[18, 19, "MethodName", "BERT"]]}, {"text": "We conducted experiments on the development set provided by the shared task organizers ( Hovy and Purschke , 2018 ) in order to decide which model to choose as our submission for the SMGCH subtask .", "entities": []}, {"text": "Our results indicate that the ensemble model attains the best performance .", "entities": []}, {"text": "With median distances that are 5 - 6 km higher , all the other models , tested individually on the development set , provide slightly worse predictions .", "entities": []}, {"text": "The remainder of this paper is organized as follows .", "entities": []}, {"text": "We present related work on dialect identi\ufb01cation and geolocation of short texts in Section 2 .", "entities": []}, {"text": "Our approach is described in detail in Section 3 .", "entities": []}, {"text": "We present the experiments and empirical results in Section 4 .", "entities": []}, {"text": "Finally , our conclusions are drawn in Section 5 . 2", "entities": []}, {"text": "Related Work Our study of the related work starts with a brief overview of geotagging based on text .", "entities": []}, {"text": "Then , we look at more speci\ufb01c methods studying geotagging in social media and we also investigate the amount of research done , from a computational linguistics perspective , in geotagging by dialect , with focus on last year \u2019s approaches for the same subtask , namely SMG - CH at VarDial 2020 .", "entities": []}, {"text": "Text - based geotagging .", "entities": []}, {"text": "The works based on text to perform geotagging can be divided into three generic categories by the approaches taken in orderto predict location .", "entities": []}, {"text": "The \ufb01rst type of approach relies mainly on gazetteers as the source of the location mappings .", "entities": []}, {"text": "This tool is adopted in a number of works ( Cheng et al . , 2010 ;", "entities": []}, {"text": "Lieberman et al . , 2010 ; Quercini et al . , 2010 ) , from ruled - based methods ( Bilhaut et al . , 2003 ) to various machine learning techniques that rely on named entity recognition ( Ding et al . , 2000 ; Gelernter and Mushegian , 2011 ; Qin et al . , 2010 ) .", "entities": [[36, 39, "TaskName", "named entity recognition"]]}, {"text": "This category of methods brings the disadvantage of relying on speci\ufb01c mentions of locations in text , rather than inferring them in a less straightforward manner .", "entities": []}, {"text": "These direct mentions of places are not a safe assumption , especially when it comes to social media platforms which represent the data source in some of these studies ( Cheng et al . , 2010 ) .", "entities": []}, {"text": "The other two main categories of approaches for text - based geolocation rely on either supervised ( Kinsella et al . , 2011 ; Wing and Baldridge , 2011 ) or unsupervised ( Ahmed et al . , 2013 ; Eisenstein et al . , 2010 ; Hong et al . , 2012 ) learning .", "entities": []}, {"text": "The latter methods usually employ clustering techniques based on topic models .", "entities": [[9, 11, "TaskName", "topic models"]]}, {"text": "Geolocation in social media .", "entities": []}, {"text": "A number of works ( Rout et al . , 2013 ) look at this task from a supervised learning perspective .", "entities": []}, {"text": "However , in these studies , other details ( e.g. social ties ) in the users pro\ufb01le are considered rather than their written content .", "entities": []}, {"text": "Other works in this area are related to our current interest in studying language variation for the geolocation of social media posts ( Doyle , 2014 ; Eisenstein et al . , 2010 ; Han et al . , 2014 ; Rahimi et al . , 2017 ; Roller et al . , 2012 ) .", "entities": []}, {"text": "Among these , various machine learning techniques are employed in location prediction , ranging from probabilistic graphical models ( Eisenstein et al . , 2010 ) and adaptive grid search ( Roller et al . , 2012 ) to Bayesian methods ( Doyle , 2014 ) and neural networks ( Rahimi et al . , 2017 ) .", "entities": []}, {"text": "Dialect - based geolocation .", "entities": []}, {"text": "Many dialects are covered in the text - based geotagging research to date , including Dutch ( Wieling et al . , 2011 ) , British ( Szmrecsanyi , 2008 ) , American ( Eisenstein et al . , 2010 ; Huang et al . , 2016 ) and African American Vernacular English ( Jones , 2015 ) .", "entities": []}, {"text": "Out of all the languages that were subject to location detection by dialect , we are interested in German .", "entities": []}, {"text": "In this direction , the study that is the most relevant to our work is that of Hovy and Purschke ( 2018 ) which targets the German language and its variations .", "entities": []}, {"text": "Approximately 16.8 million online posts from the German - speaking area of Europe are employed in", "entities": []}, {"text": "86this study with the aim of learning document representations of cities .", "entities": []}, {"text": "A small fraction of these posts are Jodels collected from the German speaking side of Switzerland , and these are also used in the SMGCH subtask that we are addressing in this paper .", "entities": []}, {"text": "The assumption here is that the methods should manage to capture enough regional variations in the written language , which can serve as the means to automatically distinguish the geographical region of social media posts .", "entities": []}, {"text": "The veri\ufb01cation performed in this direction in the original paper ( Hovy and Purschke , 2018 ) used clustering to determine larger regions covering a given dialect .", "entities": []}, {"text": "However , given the shared task formulation , we take a different approach and use the provided data in a double regression setup , addressing the problem both from a shallow and a deep learning perspective .", "entities": []}, {"text": "As previously mentioned , this is the second consecutive year in which SMG - CH is featured at VarDial ( Chakravarthi et al . , 2021 ) , with a similar format , although an updated data set .", "entities": []}, {"text": "The participants of the 2020 SMG - CH shared task ( G \u02d8aman et al . , 2020 ) studied this task from a variety of angles .", "entities": []}, {"text": "Some techniques are based on deep neural networks such as the popular BERT architecture ( G\u02d8aman and Ionescu , 2020 ; Scherrer and Ljube \u02c7si\u00b4c , 2020 ) , bidirectional Long Short - Term Memory ( LSTM ) networks applied on FastText embeddings ( Mishra , 2020 ) or character - level CNNs ( G \u02d8aman and Ionescu , 2020 ) .", "entities": [[12, 13, "MethodName", "BERT"], [30, 35, "MethodName", "Long Short - Term Memory"], [36, 37, "MethodName", "LSTM"], [41, 42, "MethodName", "FastText"]]}, {"text": "Other techniques are based on shallow or handcrafted features such as a gridbased prediction using an n - gram language model ( Jauhiainen et al . , 2020 ) or a clustering technique that shifts the problem into a discrete space , then uses an SVM for the classi\ufb01cation of posts into regions .", "entities": [[45, 46, "MethodName", "SVM"]]}, {"text": "Our best submission ( G \u02d8aman and Ionescu , 2020 ) in last year \u2019s campaign was an ensemble based on XGBoost as meta - learner over the predictions of three different models : \u0017-SVR with string kernels , a character - level CNN and an LSTM based on BERT embeddings .", "entities": [[46, 47, "MethodName", "LSTM"], [49, 50, "MethodName", "BERT"]]}, {"text": "This year , we acknowledge that our deep learning models had sub - optimal results in our previous participation at SMG - CH .", "entities": []}, {"text": "Consequently , for this year , we bring stronger neural networks into the XGBoost ensemble .", "entities": []}, {"text": "Instead of using an LSTM with BERT embeddings , we \ufb01netune the cased version of German BERT and use it directly for double regression in a setup that is more suitable to the data set size , compared to our previous endeavour .", "entities": [[4, 5, "MethodName", "LSTM"], [6, 7, "MethodName", "BERT"], [16, 17, "MethodName", "BERT"]]}, {"text": "Instead of a character - level CNN , we use a hybrid CNN which learns end - to - end rep - resentations for both words and characters .", "entities": []}, {"text": "The aforementioned deep learning models are plugged into an XGBoost ensemble alongside a retrained version of the \u0017-SVR model employed in our past participation ( G \u02d8aman and Ionescu , 2020 ) .", "entities": []}, {"text": "Our current changes introduce a signi\ufb01cant improvement in median distance compared to our previous results , on a similar ( and perhaps more challenging ) data set .", "entities": []}, {"text": "3 Method The only system that our team submitted for the SMG - CH subtask is an ensemble model based on the XGBoost meta - learner , as illustrated in Figure 1 .", "entities": []}, {"text": "In this section , we describe the three machine learning techniques that provide their predictions as input for the meta - learner , as well as the gradient boosting method that combines the independent models .", "entities": []}, {"text": "3.1 Support Vector Regression with String Kernels String Kernels ( Lodhi et al . , 2001 ) provide a way of comparing two documents , based on the inner product generated by all substrings of length n , typically known as character n - grams .", "entities": []}, {"text": "Being relatively simple to use and implement , this technique has many applications according to the literature ( Cozma et al . , 2018 ; Gim \u00b4 enez - P \u00b4 erez et al . , 2017 ; Masala et al . , 2017 ; Ionescu and Butnaru , 2018 ; Ionescu et al . , 2014 , 2016 ; Popescu and Ionescu , 2013 ) , with emphasis on dialect identi\ufb01cation and the good results obtained for this task in previous VarDial evaluation campaigns ( Butnaru and Ionescu , 2018b ; Ionescu and Butnaru , 2017 ; Ionescu and Popescu , 2016 ) .", "entities": []}, {"text": "Similar to our last year \u2019s submission for the SMG - CH subtask ( G \u02d8aman and Ionescu , 2020 ) , we employ the string kernels computed by the ef\ufb01cient algorithm introduced by Popescu et al . ( 2017 ) .", "entities": []}, {"text": "This gives us a dual representation of the data , through a kernel matrix where the cell on row iand columnj represents the similarity between two text samples xiandxj .", "entities": []}, {"text": "In our experiments , we consider the presence bits string kernel ( Popescu and Ionescu , 2013 ) as the similarity function .", "entities": []}, {"text": "For two strings xi andxjover a set of characters S , the presence bits string kernel is de\ufb01ned as follows : k0=1(xi;xj ) = X g2Sn#(xi;g)\u0001#(xj;g ) ; ( 1 )", "entities": []}, {"text": "87 Figure 1 : Ensemble proposed by UnibucKernel for the SMG - CH shared task .", "entities": []}, {"text": "Best viewed in color .", "entities": []}, {"text": "wherenis the length of n - grams and # ( x;g)is a function that returns 1 when the number of occurrences of n - gram ginxis greater than 1 , and 0 otherwise .", "entities": [[31, 32, "DatasetName", "0"]]}, {"text": "The resulting kernel matrix is plugged into a \u0017Support Vector Regression ( \u0017-SVR ) model .", "entities": []}, {"text": "SVR ( Drucker et al . , 1997 ) is a modi\ufb01ed Support Vector Machines ( SVM ) ( Cortes and Vapnik , 1995 ) model that is repurposed for regression .", "entities": [[16, 17, "MethodName", "SVM"]]}, {"text": "Similar to SVM , SVR uses the notion of support vectors and margin in order to \ufb01nd an optimal estimator .", "entities": [[2, 3, "MethodName", "SVM"]]}, {"text": "However , instead of a separating hyperplane , SVR aims to \ufb01nd a hyperplane that estimates the data points ( support vectors ) within the margin with minimal error .", "entities": []}, {"text": "In our experiments , we employ an equivalent SVR formulation known as \u0017-SVR ( Chang and Lin , 2002 ) , where\u0017is the con\ufb01gurable proportion of support vectors to keep with respect to the number of samples in the data set .", "entities": [[34, 37, "HyperparameterName", "number of samples"]]}, {"text": "Using \u0017-SVR , the optimal solution can converge to a sparse model , with only a few support vectors .", "entities": []}, {"text": "This is especially useful inour case , as the data set provided for the SMG - CH subtask does not contain too many samples .", "entities": []}, {"text": "Another reason to employ \u0017-SVR in our regression task is that it was found to surpass other regression methods for other use cases , such as complex word identi\ufb01cation ( Butnaru and Ionescu , 2018a ) .", "entities": []}, {"text": "3.2 Hybrid Convolutional Neural Network Characters are the base units in building words that exist in the vocabulary of most languages .", "entities": []}, {"text": "Among the advantages of working at the character level , we enumerate ( i)the neutrality with respect to language theory ( independence of word boundaries , semantic structure or syntax ) and ( ii)the robustness to spelling errors and words that are outside the vocabulary ( Ballesteros et al . , 2015 ) .", "entities": []}, {"text": "These explain the growing interest in using characters as features in various language modeling setups ( AlRfou et al . , 2019 ; Ballesteros et al . , 2015 ; Sutskever et al . , 2011 ; Wood et al . , 2009 ; Zhang et al . , 2015 ) .", "entities": []}, {"text": "Word embeddings are vectorial word representations that associate similar vectors to semanti-", "entities": [[0, 2, "TaskName", "Word embeddings"]]}, {"text": "88cally related words , allowing us to express semantic relations mathematically in the generated embedding space .", "entities": []}, {"text": "From the initial works of Bengio et al . ( 2003 ) and Sch \u00a8utze ( 1993 ) to the recent improvements in the quality of the embedding and the training time ( Collobert and Weston , 2008 ; Mikolov et al . , 2013a , b ; Pennington et al . , 2014 ) , generating meaningful representations of words became a hot topic in the NLP research community .", "entities": []}, {"text": "These improvements , and many others not mentioned here , have been extensively used in various NLP tasks ( Garg et al . , 2018 ; Glorot et al . , 2011 ; Ionescu and Butnaru , 2019 ; Musto et al . , 2016 ) .", "entities": []}, {"text": "Considering the sometimes orthogonal bene\ufb01ts of character and word embeddings , an intuitive idea has emerged , namely that of combining the character and word representations , which should complement each other in various aspects and provide better meaningful cues in the learning process of hybrid neural architectures ( Liang et al . , 2017 ) .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "Thus , throughout the experiments performed in this work , we choose to employ a hybrid convolutional neural network working at both the character level ( Zhang et al . , 2015 ) and the word level ( Kim , 2014 ) .", "entities": []}, {"text": "The hybrid architecture concatenates two CNNs , out of which one is equipped with a character embedding layer and the other has an analogous word embeddings layer .", "entities": [[24, 26, "TaskName", "word embeddings"]]}, {"text": "The networks are able to automatically learn a 2D representation of text formed of either character or word embedding vectors , that are further processed by convolutional and fullyconnected layers .", "entities": []}, {"text": "The last convolutional activation maps of our two CNNs sharing similar architectural choices are concatenated in what we call a hybrid network ( Liang et al . , 2017 ) , with the aim of accurately and simultaneously predicting the two location components required for the geolocation task .", "entities": []}, {"text": "The \ufb01rst component of the hybrid network is a characterlevel CNN , which takes the \ufb01rst and last 250characters in the input and encodes each character with its position in the alphabet , then learns end - to - end embeddings for each character , as vectors of 128 components .", "entities": []}, {"text": "The second CNN used as part of the hybrid network operates at the word level and it receives as input each sample encoded , initially , as an array of 100indexes , corresponding to the position of each word in the vocabulary .", "entities": []}, {"text": "As part of the pre - processing for the word - level CNN , we split the initial text into words , keeping the \ufb01rst 50words and the last 50words in the sample .", "entities": []}, {"text": "Inthe end , we employ the German Snowball Stemmer ( Weissweiler and Fraser , 2018 ) to reduce each word to its stem , in an effort to reduce the vocabulary size by mapping variations of the same word to a single vocabulary entry .", "entities": []}, {"text": "The word - level CNN is also equipped with an embedding layer , learning end - to - end word representations as vectors of length 128 .", "entities": []}, {"text": "Each of the two CNNs has three convolutional ( conv ) layers placed after the initial embedding layer .", "entities": []}, {"text": "The number of \ufb01lters decreases from 1024 for the \ufb01rst conv layer to 728for the second conv layer and to 512for the third conv layer .", "entities": []}, {"text": "Each conv layer is equipped with Recti\ufb01ed Linear Units ( ReLU ) ( Nair and Hinton , 2010 ) as the activation function .", "entities": [[10, 11, "MethodName", "ReLU"], [21, 23, "HyperparameterName", "activation function"]]}, {"text": "The convolutional \ufb01lter sizes differ across the two convolutional architectures .", "entities": []}, {"text": "Hence , we use kernels of sizes 9,7and7for the char CNN .", "entities": []}, {"text": "In the same time , we choose 7,5and3as appropriate \ufb01lter sizes for the conv layers of the word CNN .", "entities": []}, {"text": "In the char CNN , each conv layer is followed by a max - pooling layer with \ufb01lters of size 3 .", "entities": []}, {"text": "In the word CNN , we add max - pooling layers only after the \ufb01rst two conv layers .", "entities": []}, {"text": "The pooling \ufb01lter sizes are3for the \ufb01rst pooling layer and 2for the second pooling layer .", "entities": []}, {"text": "The activation maps resulting after the last conv blocks of the char and the word CNNs are concatenated and the hybrid network continues with four fully - connected ( fc ) layers with ReLU activations .", "entities": [[33, 34, "MethodName", "ReLU"]]}, {"text": "The fc layers are formed of 512,256 , 128and64individual neural units , respectively .", "entities": []}, {"text": "3.3 Fine - Tuned German BERT Transformers ( Vaswani et al . , 2017 ) represent an important advance in NLP , with many bene\ufb01ts over the traditional sequential neural architectures .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "Based on an encoder - decoder architecture with attention , transformers proved to be better at modeling long - term dependencies in sequences , while being effectively trained as the sequential dependency of previous tokens is removed .", "entities": []}, {"text": "Unlike other contemporary attempts at using transformers in language modeling ( Radford et al . , 2018 ) , BERT ( Devlin et al . , 2019 ) builds deep language representations in a self - supervised fashion and incorporates context from both directions .", "entities": [[19, 20, "MethodName", "BERT"]]}, {"text": "The masked language modeling technique enables BERT to pretrain these deep bidirectional representations , that can be further \ufb01ne - tuned and adapted for a variety of downstream tasks , without signi\ufb01cant architectural updates .", "entities": [[1, 4, "TaskName", "masked language modeling"], [6, 7, "MethodName", "BERT"]]}, {"text": "We also make use of this property", "entities": []}, {"text": "89 in the current work , employing the Hugging Face ( Wolf et al . , 2020 ) version of the cased German BERT model1 .", "entities": [[23, 24, "MethodName", "BERT"]]}, {"text": "The model was initially trained on the latest German Wikipedia dump , the OpenLegalData dump and a collection of news articles , summing up to a total of 12 GB of text \ufb01les .", "entities": []}, {"text": "We \ufb01ne - tune this pre - trained German BERT model for the geolocation of Swiss German short texts , in a regression setup .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "The choice of hyperparameters is , in part , inspired by the winning system in the last year \u2019s SMG - CH subtask at VarDial ( Scherrer and Ljube \u02c7si\u00b4c , 2020 ) .", "entities": []}, {"text": "3.4 Ensemble Learning Gradient tree boosting ( Friedman , 2001 ) is based on training a tree ensemble model in an additive fashion .", "entities": [[1, 3, "TaskName", "Ensemble Learning"]]}, {"text": "This technique has been successfully used in classi\ufb01cation ( Li , 2010 ) and ranking ( Burges , 2010 ) problems , obtaining notable results in reputed competitions such as the Net\ufb02ix Challenge ( Bennett and Lanning , 2007 ) .", "entities": []}, {"text": "Furthermore , gradient tree boosting is the ensemble method of choice in some real - world pipelines running in production ( He et al . , 2014 ) .", "entities": []}, {"text": "XGBoost ( Chen and Guestrin , 2016 ) is a tree boosting model targeted at solving large - scale tasks with limited computational resources .", "entities": []}, {"text": "This approach aims at parallelizing tree learning while also trying to handle various sparsity patterns .", "entities": []}, {"text": "Over\ufb01tting is addressed through shrinkage and column subsampling .", "entities": []}, {"text": "Shrinkage acts as a learning rate , reducing the in\ufb02uence of each individual tree .", "entities": [[4, 6, "HyperparameterName", "learning rate"]]}, {"text": "Column subsampling is borrowed from Random Forests ( Breiman , 2001 ) , bearing the advantage of speeding up the computations .", "entities": []}, {"text": "In the experiments , we employ XGBoost as a metalearner over the individual predictions of each of the models described above .", "entities": []}, {"text": "We opted for XGBoost in detriment of average voting and a \u0017-SVR meta - learner , both providing comparatively lower performance levels in a set of preliminary ensemble experiments .", "entities": []}, {"text": "4 Experiments 4.1 Data Set The SMG - CH subtask ( Hovy and Purschke , 2018 ) offers , as support , a training set of 25,261 Jodel posts , provided in plain text format .", "entities": []}, {"text": "Each textual input is associated with a pair of coordinates , i.e. latitude and longitude , representing the position on 1https://huggingface.co/ bert - base - german - casedEarth from where the text was posted .", "entities": []}, {"text": "The development set is provided in an identical format and it is composed of 2,416 samples that we use to perform hyperparameter tuning and validate the results of our models .", "entities": []}, {"text": "The test set has 2,438 samples without the corresponding coordinates , in order to avoid cheating or over\ufb01tting .", "entities": []}, {"text": "Additionally , the proposed evaluation metric is the median distance between the predicted and the reference coordinates .", "entities": []}, {"text": "A baseline median distance of 53:13 km is also included in the speci\ufb01cations .", "entities": []}, {"text": "4.2 Parameter Tuning SVR based on string kernels .", "entities": []}, {"text": "We compute the presence bits string kernel using the ef\ufb01cient algorithm proposed by Popescu et al . ( 2017 ) .", "entities": []}, {"text": "In order to \ufb01nd the optimal range of n - grams , we experiment with multiple blended spectrum string kernels based on various n - gram ranges that include n - grams from 3 to 7 characters long .", "entities": []}, {"text": "The best performance in terms of both mean absolute error ( MAE ) and mean squared error ( MSE ) was attained by a string kernel based on the blended spectrum of 3 to 5 character n - grams .", "entities": [[11, 12, "MetricName", "MAE"], [18, 19, "MetricName", "MSE"]]}, {"text": "These results are consistent with those reported by Ionescu and Butnaru ( 2017 ) and G \u02d8aman and Ionescu ( 2020 ) , suggesting that the 3 - 5 n - gram range is optimal for German dialect identi\ufb01cation .", "entities": []}, {"text": "The resulting kernel matrix is used as input for two \u0017-SVR models , optimized for predicting the latitude and longitude ( in degrees ) , respectively .", "entities": []}, {"text": "For each of the two regressors , we tune the regularization penalty C , in a range of values from 10\u00004to104 .", "entities": []}, {"text": "Similarly , for the proportion of support vectors \u0017 , we consider 10values uniformly covering the interval ( 0;1]with a step of0:1 .", "entities": []}, {"text": "To search for the optimal combination of hyperparameters , we apply grid search .", "entities": []}, {"text": "Consistent with our previous study that inspired this choice of model and features ( G \u02d8aman and Ionescu , 2020 ) , for both regression models , the best hyperparameter combination is C= 10 and\u0017= 0:5 .", "entities": []}, {"text": "Hybrid CNN .", "entities": []}, {"text": "As regularization for each conv block of the two CNNs , we introduce Gaussian noise and spatial dropout .", "entities": []}, {"text": "We try many different magnitudes in the two regularization techniques , obtaining a few slightly different variations of the same model .", "entities": []}, {"text": "These are compared against each other using grid search , selecting the best model to be included in our \ufb01nal ensemble .", "entities": []}, {"text": "The rates used for the spatial dropout ( Tompson et al . , 2015 ) are in the range", "entities": []}, {"text": "[ 0:001;0:2 ] , with a step of 0:001 , with", "entities": []}, {"text": "90Table 1 : The preliminary results of our team ( UnibucKernel ) obtained on the development set of the SMG - CH subtask .", "entities": []}, {"text": "Method Median ( km ) Mean ( km ) Hybrid CNN 30.05 35.75 \u0017-SVR with string kernels 30.31 34.82 BERT # 1(L2 ) 30.63 35.78 BERT # 2(L1 , truncated ) 33.86 38.85 BERT # 3(L1 ) 30.17 36.16 XGBoost ensemble 25.11 30.75 the best dropout rate deemed to be 0:007 .", "entities": [[19, 20, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"], [33, 34, "MethodName", "BERT"]]}, {"text": "For the Gaussian noise , we consider standard deviations in the range [ 0:0001;0:01 ] , with the best results obtained with a standard deviation of 0:001 .", "entities": []}, {"text": "After each of the four fully - connected layers , we employ plain dropout ( Srivastava et al . , 2014 ) with a rate of 0:005 , deemed the best choice in the range [ 0:001;0:5 ] .", "entities": []}, {"text": "In the optimization phase , common evaluation metrics such as the mean squared error ( MSE ) and the mean absolute error ( MAE ) are typically used to measure performance .", "entities": [[15, 16, "MetricName", "MSE"], [23, 24, "MetricName", "MAE"]]}, {"text": "In addition to these two metrics , we consider another error function as candidate for the loss to be minimized , namely the Huber loss .", "entities": [[16, 17, "MetricName", "loss"], [24, 25, "MetricName", "loss"]]}, {"text": "Although minimizing the Huber loss tends to give optimal values for the classical evaluation metrics , we \ufb01nally use MSE as loss , given that it seems to converge to better results in terms of the median distance , which is the of\ufb01cial evaluation metric in the SMG shared task .", "entities": [[4, 5, "MetricName", "loss"], [19, 20, "MetricName", "MSE"], [21, 22, "MetricName", "loss"]]}, {"text": "We optimize the hybrid CNN using the Adam optimization algorithm ( Kingma and Ba , 2015 ) with an initial learning rate of10\u00003 , chosen from the range", "entities": [[7, 8, "MethodName", "Adam"], [20, 22, "HyperparameterName", "learning rate"]]}, {"text": "[ 10\u00005;10\u00002 ] , a weight decay of 10\u00007 , selected in the initial range [ 10\u00009;10\u00006 ] , and the learning rate decay of 0:999 .", "entities": [[5, 7, "MethodName", "weight decay"], [21, 23, "HyperparameterName", "learning rate"]]}, {"text": "We train the hybrid architecture on mini - batches of96samples for 1000 epochs with early stopping .", "entities": [[14, 16, "MethodName", "early stopping"]]}, {"text": "The network included in the \ufb01nal ensemble converged in 136epochs .", "entities": []}, {"text": "Fine - Tuned German BERT .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "We \ufb01ne - tune three BERT models in slightly different setups .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "The pretrained base model used as starting point is the cased German BERT ( Wolf et al . , 2020 ) .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "We set the maximum sequence length to 128 , applying zero - padding to reach the \ufb01xed length for the samples that are shorter .", "entities": []}, {"text": "The batch size , unanimously used , is 32 , with a training that goes on for a maximum of 50 epochs .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "We optimize the models using Adam , with an initial learning rate \u000b = 5 \u000110\u00005 .", "entities": [[5, 6, "MethodName", "Adam"], [10, 12, "HyperparameterName", "learning rate"]]}, {"text": "The division by zero is prevented by the introduction of\u000f= 10\u00008 .", "entities": []}, {"text": "Moving to the particularities ofeach of the three \ufb01ne - tuned BERT models , we note a difference in the choice of the loss function , such that the \ufb01rst model employs the L2loss ( MSE ) , while the other two models use the L1loss ( MAE ) .", "entities": [[11, 12, "MethodName", "BERT"], [23, 24, "MetricName", "loss"], [35, 36, "MetricName", "MSE"], [47, 48, "MetricName", "MAE"]]}, {"text": "Another variation is introduced by the text truncation choices , such that for the \ufb01rst and third models , we do not enforce truncation , while for the second model , all the samples are enforced to align to the provided maximum sequence length .", "entities": []}, {"text": "These differences are re\ufb02ected in the number epochs required for complete convergence : 20,18and33 , respectively .", "entities": []}, {"text": "We monitor the median distance for early stopping and observe that the best performance upon convergence is obtained by the third model , with a median distance of 30:17 km on the validation set , followed by the results of the \ufb01rst ( 30:63 km ) and second ( 33:86 km ) models , respectively .", "entities": [[6, 8, "MethodName", "early stopping"]]}, {"text": "In the ensemble , we include only the top scoring BERT model .", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "Extreme Gradient Boosting .", "entities": []}, {"text": "We employed XGBoost as a meta - learner , training it over the predictions of all the other models described in Section 3 .", "entities": []}, {"text": "We selected XGBoost as the submission candidate , as it provided the best results .", "entities": []}, {"text": "The XGBoost regressor , deemed optimal by the hyperparameter tuning , has default values for most parameters2 , except for the number of estimators and the maximum tree depth .", "entities": [[21, 24, "HyperparameterName", "number of estimators"]]}, {"text": "We set the number of estimators to 100for the latitude regressor and to 1000 for the longitude regressor .", "entities": [[3, 6, "HyperparameterName", "number of estimators"]]}, {"text": "Similarly , the maximum depth of the trees is 10for the latitude model and 20for the longitude one .", "entities": [[3, 5, "HyperparameterName", "maximum depth"]]}, {"text": "4.3 Preliminary Results We train three different models which rely on different learning methods and types of features to perform the required double regression .", "entities": []}, {"text": "Thus , we have the hybrid CNN relying on both words and characters as features , the shallow \u0017-SVR based on 2https://xgboost.readthedocs.io/en/ latest/", "entities": []}, {"text": "91Table 2 : The \ufb01nal results obtained on the test set of the SMG - CH subtask by our ensemble model against the baseline and the top scoring methods .", "entities": []}, {"text": "Method Rank Median ( km ) Mean ( km ) AUC HeLju uc 1st 17.55 25.84 74907.5 HeLju c 2nd 20.70 29.62 67237.5 UnibucKernel 3rd 23.60 29.75 63347.0 baseline - 53.13 51.50 28637.0 string kernels and three \ufb01ne - tuned German BERT models looking at higher - level features and understanding dependencies in a bidirectional manner .", "entities": [[10, 11, "MetricName", "AUC"], [41, 42, "MethodName", "BERT"]]}, {"text": "Table 1 shows the preliminary results obtained on the development set by each individual model as well as the results of the submitted XGBoost ensemble .", "entities": []}, {"text": "The individual models provide quite similar results in terms of the median distance between the predicted and ground - truth locations .", "entities": []}, {"text": "These results stay around a value of 30 km for the median distance and 35 km for the mean distance .", "entities": []}, {"text": "Among the independent models , the hybrid CNN obtains slightly better results in terms of the median distance ( 30:05 km ) , whereas the second attempt at \ufb01ne - tuning BERT gives the worst distances , namely 33:86 km for the median distance and 38:85 km for the mean distance .", "entities": [[31, 32, "MethodName", "BERT"]]}, {"text": "\u0017-SVR surpasses all the other models , by a small margin , in terms of the mean distance ( 34:82 km ) .", "entities": []}, {"text": "The results of the submitted XGBoost ensemble model stand proof that our intuition was correct , namely that all these individual models have the potential to complement each other if put together in an ensemble .", "entities": []}, {"text": "Indeed , the submitted system clearly surpasses the best individual model by approximately 5 km in terms of both the median and the mean distance metrics .", "entities": []}, {"text": "4.4 Final Results For our \ufb01nal submission , we trained all the individual models on both the provided training and development data sets .", "entities": []}, {"text": "Then , we also retrained the submitted ensemble model , in a hope that this will give an even smaller median distance on the test set , compared to what we have obtained in the preliminary validation phase .", "entities": []}, {"text": "Table 2 shows an improvement in terms of the median distance on the test set compared to the one obtain on the development data .", "entities": []}, {"text": "We can not be sure that this improvement is solely due to the retraining that involves the development set .", "entities": []}, {"text": "However , we conjecture that this endeavour played its part in the slightly better results .", "entities": []}, {"text": "We outperform the baseline model by 29:53 km in terms of the median distanceand by 21:75 km in terms of the mean distance , obtaining the third place in the competition .", "entities": []}, {"text": "The constrained submission proposed by the organizers of the SMG - CH shared task surpasses our model by2:9 km in terms of the median distance and by0:13 km in terms of the mean distance .", "entities": []}, {"text": "The unconstrained system on the \ufb01rst place , which was also proposed by the organizers of the SMG - CH shared task , distances itself by larger margins , with a difference of 6:05 km for the median distance and a difference of 3:91 km for the mean distance .", "entities": []}, {"text": "5 Conclusion In this paper , we proposed an ensemble learning model for the geolocation of Swiss German social media posts .", "entities": [[9, 11, "TaskName", "ensemble learning"]]}, {"text": "The ensemble is based on an XGBoost meta - learner applied on top of three individual models : a hybrid CNN , an approach based on string kernels and a \ufb01ne - tuned German BERT model .", "entities": [[34, 35, "MethodName", "BERT"]]}, {"text": "Given the \ufb01nal results obtained in the SMGCH subtask , we conclude that predicting the location of Swiss German social media posts is a challenging task , the median distance being higher than 20 km .", "entities": []}, {"text": "Using external data sources to build a language model seems to be a more promising path towards success , as shown by the \ufb01nal standings of the VarDial 2020 ( G \u02d8aman et al . , 2020 ) and 2021 ( Chakravarthi et al . , 2021 )", "entities": []}, {"text": "SMG shared tasks .", "entities": []}, {"text": "In future work , we aim to study the applicability of our ensemble on other geolocation tasks , perhaps taking into consideration future VarDial challenges .", "entities": []}, {"text": "Acknowledgments The authors thank reviewers for their useful remarks .", "entities": []}, {"text": "This work was supported by a grant of the Romanian Ministry of Education and Research , CNCS - UEFISCDI , project number PN - III - P1 - 1.1TE-2019 - 0235 , within PNCDI III .", "entities": []}, {"text": "This article has also bene\ufb01ted from the support of the Romanian Young Academy , which is funded by Stiftung Mercator and the Alexander von Humboldt Foundation for the period 2020 - 2022 .", "entities": []}, {"text": "92References Amr Ahmed , Liangjie Hong , and Alex J. Smola .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Hierarchical geographical modeling of user locations from social media posts .", "entities": []}, {"text": "In Proceedings of WWW , pages 25\u201336 .", "entities": []}, {"text": "Rami Al - Rfou , Dokook Choe , Noah Constant , Mandy Guo , and Llion Jones .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Character - Level Language Modeling with Deeper Self - Attention .", "entities": []}, {"text": "In Proceedings of AAAI , pages 3159\u20133166 .", "entities": []}, {"text": "Miguel Ballesteros , Chris Dyer , and Noah A. Smith . 2015 .", "entities": []}, {"text": "Improved Transition - Based Parsing by Modeling Characters instead of Words with LSTMs .", "entities": []}, {"text": "In Proceedings of EMNLP 2015 , pages 349\u201359 .", "entities": []}, {"text": "Yoshua Bengio , R \u00b4 ejean Ducharme , Pascal Vincent , and Christian Janvin .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "A Neural Probabilistic Language Model .", "entities": [[1, 5, "MethodName", "Neural Probabilistic Language Model"]]}, {"text": "Journal of Machine Learning Research , 3:1137\u20131155 .", "entities": []}, {"text": "James Bennett and Stan Lanning .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "The Net\ufb02ix Prize .", "entities": []}, {"text": "In Proceedings of KDD , page 35 .", "entities": []}, {"text": "Fr\u00b4ed\u00b4erik Bilhaut , Thierry Charnois , Patrice Enjalbert , and Yann Mathet .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Geographic reference analysis for geographic document querying .", "entities": []}, {"text": "In Proceedings of GEOREF , pages 55\u201362 .", "entities": []}, {"text": "Leo Breiman .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Random Forests .", "entities": []}, {"text": "Machine Learning , 45(1):5\u201332 .", "entities": []}, {"text": "Christopher J.C. Burges .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "From RankNet to LambdaRank to LambdaMART : An Overview .", "entities": []}, {"text": "Technical Report MSR - TR-2010 - 82 , Microsoft .", "entities": []}, {"text": "Andrei Butnaru and Radu Tudor Ionescu .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "UnibucKernel : A kernel - based learning method for complex word identi\ufb01cation .", "entities": []}, {"text": "In Proceedings of BEA-13 , pages 175\u2013183 .", "entities": []}, {"text": "Andrei Butnaru and Radu Tudor Ionescu .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "UnibucKernel Reloaded : First Place in Arabic Dialect Identi\ufb01cation for the Second Year in a Row .", "entities": []}, {"text": "In Proceedings of VarDial , pages 77\u201387 .", "entities": []}, {"text": "Andrei Butnaru and Radu Tudor Ionescu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "MOROCO :", "entities": [[0, 1, "DatasetName", "MOROCO"]]}, {"text": "The Moldavian and Romanian Dialectal Corpus .", "entities": []}, {"text": "In Proceedings of ACL , pages 688\u2013698 .", "entities": []}, {"text": "Bharathi Raja Chakravarthi , Mihaela G \u02d8aman , Radu Tudor Ionescu , Heidi Jauhiainen , Tommi Jauhiainen , Krister Lind \u00b4 en , Nikola Ljube \u02c7si\u00b4c , Niko Partanen , Ruba Priyadharshini , Christoph Purschke , Eswari Rajagopal , Yves Scherrer , and Marcos Zampieri .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Findings of the VarDial Evaluation Campaign 2021 .", "entities": []}, {"text": "In Proceedings of the Eighth Workshop on NLP for Similar Languages , Varieties and Dialects ( VarDial ) .", "entities": []}, {"text": "Chih - Chung Chang and Chih - Jen Lin .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Training\u0017-Support Vector Regression : Theory and Algorithms .", "entities": []}, {"text": "Neural Computation , 14:1959\u20131977.Tianqi Chen and Carlos Guestrin .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "XGBoost : A scalable tree boosting system .", "entities": []}, {"text": "In Proceedings of KDD , pages 785\u2013794 .", "entities": []}, {"text": "Zhiyuan Cheng , James Caverlee , and Kyumin Lee . 2010 .", "entities": []}, {"text": "You are where you tweet : a content - based approach to geo - locating Twitter users .", "entities": []}, {"text": "In Proceedings of CIKM , pages 759\u2013768 .", "entities": []}, {"text": "Ronan Collobert and Jason Weston .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "A Uni\ufb01ed Architecture for Natural Language Processing :", "entities": []}, {"text": "Deep Neural Networks with Multitask Learning .", "entities": []}, {"text": "In Proceedings of ICML , pages 160\u2013167 .", "entities": []}, {"text": "Corinna Cortes and Vladimir Vapnik . 1995 .", "entities": []}, {"text": "Supportvector networks .", "entities": []}, {"text": "Machine Learning , 20(3):273 \u2013 297 .", "entities": []}, {"text": "M\u02d8ad\u02d8alina Cozma , Andrei Butnaru , and Radu Tudor Ionescu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Automated essay scoring with string kernels and word embeddings .", "entities": [[0, 3, "TaskName", "Automated essay scoring"], [7, 9, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of ACL , pages 503\u2013509 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of NAACL , pages 4171 \u2013 4186 .", "entities": []}, {"text": "Bhuwan Dhingra , Zhong Zhou , Dylan Fitzpatrick , Michael Muehl , and William W. Cohen . 2016 .", "entities": []}, {"text": "Tweet2Vec :", "entities": []}, {"text": "Character - Based Distributed Representations for Social Media .", "entities": []}, {"text": "In Proceedings of ACL , pages 269\u2013274 .", "entities": []}, {"text": "Junyan Ding , Luis Gravano , and Narayanan Shivakumar . 2000 .", "entities": []}, {"text": "Computing geographical scopes of web resources .", "entities": []}, {"text": "In Proceedings of VLDV .", "entities": []}, {"text": "Gabriel Doyle .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Mapping dialectal variation by querying social media .", "entities": []}, {"text": "In Proceedings of EACL 2014 , pages 98\u2013106 .", "entities": []}, {"text": "Harris Drucker , Christopher J.C. Burges , Linda Kaufman , Alex J. Smola , and Vladimir Vapnik .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Support vector regression machines .", "entities": []}, {"text": "In Proceedings of NIPS , pages 155\u2013161 .", "entities": []}, {"text": "Jacob Eisenstein , Brendan O\u2019Connor , Noah A. Smith , and Eric Xing .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "A latent variable model for geographic lexical variation .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 1277\u20131287 .", "entities": []}, {"text": "Jerome H. Friedman .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Greedy function approximation : a gradient boosting machine .", "entities": []}, {"text": "Annals of statistics , pages 1189\u20131232 .", "entities": []}, {"text": "Nikhil Garg , Londa Schiebinger , Dan Jurafsky , and James Zou .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Word embeddings quantify 100 years of gender and ethnic stereotypes .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}, {"text": "Proceedings of the National Academy of Sciences , 115 : E3635 \u2013 E3644 .", "entities": []}, {"text": "Judith Gelernter and Nikolai Mushegian .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Geoparsing messages from microtext .", "entities": []}, {"text": "Transactions in GIS , 15(6):753\u2013773 .", "entities": []}, {"text": "93Rosa M. Gim \u00b4 enez - P \u00b4 erez , Marc Franco - Salvador , and Paolo Rosso .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Single and Cross - domain Polarity Classi\ufb01cation using String Kernels .", "entities": []}, {"text": "In Proceedings of EACL , pages 558\u2013563 .", "entities": []}, {"text": "Xavier Glorot , Antoine Bordes , and Yoshua Bengio .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Domain Adaptation for Large - Scale Sentiment Classi\ufb01cation : A Deep Learning Approach .", "entities": [[0, 2, "TaskName", "Domain Adaptation"]]}, {"text": "In Proceedings of ICML , pages 513\u2013520 .", "entities": []}, {"text": "Mihaela G \u02d8aman , Dirk Hovy , Radu Tudor Ionescu , Heidi Jauhiainen , Tommi Jauhiainen , Krister Lind \u00b4 en , Nikola Ljube \u02c7si\u00b4c , Niko Partanen , Christoph Purschke , Yves Scherrer , and Marcos Zampieri .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A Report on the VarDial Evaluation Campaign 2020 .", "entities": []}, {"text": "In Proceedings of VarDial , pages 1\u201314 .", "entities": []}, {"text": "Mihaela G \u02d8aman and Radu Tudor Ionescu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Combining Deep Learning and String Kernels for the Localization of Swiss German Tweets .", "entities": []}, {"text": "In Proceedings of VarDial , pages 242\u2013253 .", "entities": []}, {"text": "Bo Han , Paul Cook , and Timothy Baldwin .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "TextBased Twitter User Geolocation Prediction .", "entities": []}, {"text": "Journal of Arti\ufb01cial Intelligence Research , 49:451\u2013500 .", "entities": []}, {"text": "Xinran He , Junfeng Pan , Ou Jin , Tianbing Xu , Bo Liu , Tao Xu , Yanxin Shi , Antoine Atallah , Ralf Herbrich , Stuart Bowers , and Joaquin Q. Candela .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Practical Lessons from Predicting Clicks on Ads at Facebook .", "entities": []}, {"text": "In Proceedings of ADKDD , pages 1\u20139 .", "entities": []}, {"text": "Liangjie Hong , Amr Ahmed , Siva Gurumurthy , Alex J. Smola , and Kostas Tsioutsiouliklis .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Discovering Geographical Topics in the Twitter Stream .", "entities": []}, {"text": "In Proceedings of WWW , pages 769\u2013778 .", "entities": []}, {"text": "Dirk Hovy and Christoph Purschke .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Capturing Regional Variation with Distributed Place Representations and Geographic Retro\ufb01tting .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 4383\u20134394 .", "entities": []}, {"text": "Yuan Huang , Diansheng Guo , Alice Kasakoff , and Jack Grieve .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Understanding US regional linguistic variation with Twitter data analysis .", "entities": []}, {"text": "Computers , Environment and Urban Systems , 59:244\u2013255 .", "entities": []}, {"text": "Radu Tudor Ionescu and Andrei Butnaru . 2017 .", "entities": []}, {"text": "Learning to Identify Arabic and German Dialects using Multiple Kernels .", "entities": []}, {"text": "In Proceedings of VarDial , pages 200\u2013209 .", "entities": []}, {"text": "Radu Tudor Ionescu and Andrei Butnaru .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Improving the results of string kernels in sentiment analysis and Arabic dialect identi\ufb01cation by adapting them to your test set .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of EMNLP , pages 1084\u20131090 .", "entities": []}, {"text": "Radu Tudor Ionescu and Andrei Butnaru .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Vector of Locally - Aggregated Word Embeddings ( VLAWE ): A Novel Document - level Representation .", "entities": [[5, 7, "TaskName", "Word Embeddings"]]}, {"text": "InProceedings of NAACL , pages 363\u2013369 .", "entities": []}, {"text": "Radu Tudor Ionescu and Marius Popescu .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "UnibucKernel : An Approach for Arabic Dialect Identi\ufb01cation based on Multiple String Kernels .", "entities": []}, {"text": "In Proceedings of VarDial , pages 135\u2013144.Radu Tudor Ionescu , Marius Popescu , and Aoife Cahill .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Can characters reveal your native language ?", "entities": []}, {"text": "A language - independent approach to native language identi\ufb01cation .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 1363\u20131373 .", "entities": []}, {"text": "Radu Tudor Ionescu , Marius Popescu , and Aoife Cahill .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "String kernels for native language identi\ufb01cation : Insights from behind the curtains .", "entities": []}, {"text": "Computational Linguistics , 42(3):491\u2013525 .", "entities": []}, {"text": "Tommi Jauhiainen , Heidi Jauhiainen , and Krister Lind \u00b4 en . 2020 .", "entities": []}, {"text": "Experiments in language variety geolocation and dialect identi\ufb01cation .", "entities": []}, {"text": "In Proceedings of VarDial , pages 220\u2013231 .", "entities": []}, {"text": "Taylor Jones . 2015 .", "entities": []}, {"text": "Toward a description of African American Vernacular English dialect regions using \u201c Black Twitter \u201d .", "entities": []}, {"text": "American Speech , 90(4):403\u2013440 .", "entities": []}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional Neural Networks for Sentence Classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 1746\u20131751 .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In Proceedings of ICLR .", "entities": []}, {"text": "Sheila Kinsella , Vanessa Murdock , and Neil O\u2019Hare . 2011 .", "entities": []}, {"text": "\u201c I \u2019m eating a sandwich in Glasgow \u201d modeling locations with tweets .", "entities": []}, {"text": "In Proceedings of SMUC , pages 61\u201368 .", "entities": []}, {"text": "Ping Li .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Robust Logitboost and Adaptive Base Class ( ABC ) Logitboost .", "entities": [[7, 8, "MethodName", "ABC"]]}, {"text": "In Proceedings of UAI , pages 302\u2013311 .", "entities": []}, {"text": "Dongyun Liang , Weiran Xu , and Yinge Zhao . 2017 .", "entities": []}, {"text": "Combining Word - Level and Character - Level Representations for Relation Classi\ufb01cation of Informal Text .", "entities": []}, {"text": "In Proceedings of RepL4NLP , pages 43\u201347 .", "entities": []}, {"text": "Michael D. Lieberman , Hanan Samet , and Jagan Sankaranarayanan .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Geotagging with local lexicons to build indexes for textually - speci\ufb01ed spatial data .", "entities": []}, {"text": "In Proceedings of ICDE , pages 201\u2013212 .", "entities": []}, {"text": "Wang Ling , Tiago Lu", "entities": []}, {"text": "\u00b4 \u0131s , Lu \u00b4 \u0131s Marujo , Ram \u00b4 on Fernandez Astudillo , Silvio Amir , Chris Dyer , Alan W. Black , and Isabel Trancoso .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Finding Function in Form : Compositional Character Models for Open V ocabulary Word Representation .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 1520\u20131530 .", "entities": []}, {"text": "Nikola Ljube \u02c7si\u00b4c , Tanja Samard \u02c7zi\u00b4c , and Curdin Derungs .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "TweetGeo - A Tool for Collecting , Processing and Analysing Geo - encoded Linguistic Data .", "entities": []}, {"text": "In Proceedings of COLING , pages 3412 \u2013 3421 .", "entities": []}, {"text": "Huma Lodhi , John Shawe - Taylor , Nello Cristianini , and Christopher J.C.H. Watkins .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Text Classi\ufb01cation Using String Kernels .", "entities": []}, {"text": "In Proceedings of NIPS , pages 563\u2013569 .", "entities": []}, {"text": "94Mihai Masala , Stefan Ruseti , and Traian Rebedea . 2017 .", "entities": []}, {"text": "Sentence selection with neural networks using string kernels .", "entities": []}, {"text": "In Proceedings of KES , pages 1774\u20131782 .", "entities": []}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013a .", "entities": []}, {"text": "Ef\ufb01cient Estimation of Word Representations in Vector Space .", "entities": []}, {"text": "In Proceedings of ICLR Workshops .", "entities": []}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S. Corrado , and Jeff Dean .", "entities": []}, {"text": "2013b .", "entities": []}, {"text": "Distributed Representations of Words and Phrases and their Compositionality .", "entities": []}, {"text": "In Proceedings of NIPS , pages 3111\u20133119 .", "entities": []}, {"text": "Piyush Mishra .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Geolocation of Tweets with a BiLSTM Regression Model .", "entities": [[5, 6, "MethodName", "BiLSTM"]]}, {"text": "In Proceedings of VarDial , pages 283\u2013289 .", "entities": []}, {"text": "Cataldo Musto , Giovanni Semeraro , Marco Degemmis , and Pasquale Lops . 2016 .", "entities": []}, {"text": "Learning Word Embeddings from Wikipedia for Content - Based Recommender Systems .", "entities": [[1, 3, "TaskName", "Word Embeddings"]]}, {"text": "In Proceedings of ECIR , pages 729\u2013734 .", "entities": []}, {"text": "Vinod Nair and Geoffrey E. Hinton .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Recti\ufb01ed Linear Units Improve Restricted Boltzmann Machines .", "entities": [[4, 5, "DatasetName", "Restricted"]]}, {"text": "In Proceedings of ICML , pages 807\u2013814 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher D. Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "GloVe : Global Vectors for Word Representation .", "entities": [[0, 1, "MethodName", "GloVe"]]}, {"text": "In Proceedings of EMNLP , pages 1532\u20131543 .", "entities": []}, {"text": "Marius Popescu , Cristian Grozea , and Radu Tudor Ionescu . 2017 .", "entities": []}, {"text": "HASKER : An ef\ufb01cient algorithm for string kernels .", "entities": []}, {"text": "Application to polarity classi\ufb01cation in various languages .", "entities": []}, {"text": "In Proceedings of KES , pages 1755\u20131763 .", "entities": []}, {"text": "Marius Popescu and Radu Tudor Ionescu .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "The Story of the Characters , the DNA and the Native Language .", "entities": []}, {"text": "In Proceedings of BEA-8 , pages 270 \u2013 278 .", "entities": []}, {"text": "Teng Qin , Rong Xiao , Lei Fang , Xing Xie , and Lei Zhang .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "An ef\ufb01cient location extraction algorithm by leveraging web contextual information .", "entities": []}, {"text": "In Proceedings of GIS , pages 53\u201360 .", "entities": []}, {"text": "Gianluca Quercini , Hanan Samet , Jagan Sankaranarayanan , and Michael D. Lieberman .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Determining the spatial reader scopes of news sources using local lexicons .", "entities": []}, {"text": "In Proceedings of GIS , pages 43\u201352 .", "entities": []}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving language understanding with unsupervised learning .", "entities": []}, {"text": "Technical report , OpenAI .", "entities": []}, {"text": "Afshin Rahimi , Trevor Cohn , and Timothy Baldwin . 2017 .", "entities": []}, {"text": "A Neural Model for User Geolocation and Lexical Dialectology .", "entities": []}, {"text": "In Proceedings of ACL , pages 209\u2013216.Stephen Roller , Michael Speriosu , Sarat Rallapalli , Benjamin Wing , and Jason Baldridge .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Supervised Text - based Geolocation Using Language Models on an Adaptive Grid .", "entities": []}, {"text": "In Proceedings of EMNLP , pages 1500\u20131510 .", "entities": []}, {"text": "Dominic Rout , Kalina Bontcheva , Daniel Preot \u00b8iucPietro , and Trevor Cohn .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Where \u2019s @wally ?", "entities": []}, {"text": "A Classi\ufb01cation Approach to Geolocating Users Based on their Social Ties .", "entities": []}, {"text": "In Proceedings of HT , pages 11\u201320 .", "entities": []}, {"text": "Yves Scherrer and Nikola Ljube \u02c7si\u00b4c .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "HeLju@VarDial 2020 : Social Media Variety Geolocation with BERT Models .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "In Proceedings of VarDial , pages 202\u2013211 .", "entities": []}, {"text": "Hinrich Sch \u00a8utze .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "Word Space .", "entities": []}, {"text": "In Proceedings of NIPS , pages 895\u2013902 .", "entities": []}, {"text": "Nitish Srivastava , Geoffrey E. Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov .", "entities": [[14, 15, "DatasetName", "Ruslan"]]}, {"text": "2014 .", "entities": []}, {"text": "Dropout :", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "A Simple Way to Prevent Neural Networks from Over\ufb01tting .", "entities": []}, {"text": "Journal of Machine Learning Research , 15(1):1929\u20131958 .", "entities": []}, {"text": "Ilya Sutskever , James Martens , and Geoffrey E. Hinton .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Generating Text with Recurrent Neural Networks .", "entities": []}, {"text": "In Proceedings of ICML , pages 1017\u20131024 .", "entities": []}, {"text": "Benedikt Szmrecsanyi . 2008 .", "entities": []}, {"text": "Corpus - based dialectometry : Aggregate morphosyntactic variability in british english dialects .", "entities": []}, {"text": "International Journal of Humanities and Arts Computing , 2(1 - 2):279\u2013296 .", "entities": []}, {"text": "Jonathan Tompson , Ross Goroshin , Arjun Jain , Yann LeCun , and Christoph Bregler . 2015 .", "entities": []}, {"text": "Ef\ufb01cient object localization using convolutional networks .", "entities": [[1, 3, "TaskName", "object localization"]]}, {"text": "In Proceedings of CVPR , pages 648\u2013656 .", "entities": []}, {"text": "Diana Tudoreanu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "DTeam@VarDial 2019 :", "entities": []}, {"text": "Ensemble based on skip - gram and triplet loss neural networks for Moldavian vs. Romanian cross - dialect topic identi\ufb01cation .", "entities": [[7, 9, "MethodName", "triplet loss"]]}, {"text": "In Proceedings of VarDial , pages 202\u2013208 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Proceedings of NIPS , pages 5998 \u2013 6008 .", "entities": []}, {"text": "Leonie Weissweiler and Alexander Fraser .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Developing a Stemmer for German Based on a Comparative Analysis of Publicly Available Stemmers .", "entities": []}, {"text": "In Proceedings of GSCL , pages 81\u201394 .", "entities": []}, {"text": "Martijn Wieling , John Nerbonne , and R. Harald Baayen .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Quantitative social dialectology : Explaining linguistic variation geographically and socially .", "entities": []}, {"text": "PloS One , 6(9):e23613 .", "entities": [[0, 1, "DatasetName", "PloS"]]}, {"text": "Benjamin Wing and Jason Baldridge . 2011 .", "entities": []}, {"text": "Simple supervised document geolocation with geodesic grids .", "entities": []}, {"text": "InProceedings of ACL , pages 955\u2013964 .", "entities": []}, {"text": "95Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 .", "entities": []}, {"text": "Transformers : State - of - the - Art Natural Language Processing .", "entities": []}, {"text": "In Proceedings of EMNLP : System Demonstrations , pages 38\u201345 .", "entities": []}, {"text": "Frank Wood , C \u00b4 edric Archambeau , Jan Gasthaus , Lancelot James , and Yee Whye Teh . 2009 .", "entities": []}, {"text": "A Stochastic Memoizer for Sequence Data .", "entities": []}, {"text": "In Proceedings of ICML , pages 1129\u20131136 .", "entities": []}, {"text": "Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 .", "entities": []}, {"text": "Character - level Convolutional Networks for Text Classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of NIPS , pages 649 \u2013 657 .", "entities": []}]