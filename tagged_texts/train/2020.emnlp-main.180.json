[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 2302\u20132315 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics2302UDapter :", "entities": []}, {"text": "Language Adaptation for Truly Universal Dependency Parsing Ahmet \u00a8Ust\u00a8un Arianna Bisazza Gosse Bouma Gertjan van Noord University of Groningen fa.ustun , a.bisazza , g.bouma , g.j.m.van.noord g@rug.nl Abstract Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality .", "entities": [[5, 7, "TaskName", "Dependency Parsing"], [32, 34, "TaskName", "dependency parsing"]]}, {"text": "However , crosslanguage interference and restrained model capacity remain major obstacles .", "entities": []}, {"text": "To address this , we propose a novel multilingual task adaptation approach based on contextual parameter generation andadapter modules .", "entities": []}, {"text": "This approach enables to learn adapters via language embeddings while sharing model parameters across languages .", "entities": []}, {"text": "It also allows for an easy but effective integration of existing linguistic typology features into the parsing network .", "entities": []}, {"text": "The resulting parser , UDapter , outperforms strong monolingual and multilingual baselines on the majority of both high - resource and lowresource ( zero - shot ) languages , showing the success of the proposed adaptation approach .", "entities": []}, {"text": "Our in - depth analyses show that soft parameter sharing via typological features is key to this success.1 1 Introduction Monolingual training of a dependency parser has been successful when relatively large treebanks are available ( Kiperwasser and Goldberg , 2016 ; Dozat and Manning , 2017 ) .", "entities": []}, {"text": "However , for many languages , treebanks are either too small or unavailable .", "entities": []}, {"text": "Therefore , multilingual models leveraging Universal Dependency annotations ( Nivre et al . , 2018 ) have drawn serious attention ( Zhang and Barzilay , 2015 ; Ammar et al . , 2016 ; de Lhoneux et al . , 2018 ; Kondratyuk and Straka , 2019 ) .", "entities": []}, {"text": "Multilingual approaches learn generalizations across languages and share information between them , making it possible to parse a target language without supervision in that language .", "entities": []}, {"text": "Moreover , multilingual models can be faster to train and easier to maintain than a large set of monolingual models .", "entities": []}, {"text": "1Our code for UDapter is publicly available at https://github.com/ahmetustun/udapterHowever , scaling a multilingual model over a high number of languages can lead to sub - optimal results , especially if the training languages are typologically diverse .", "entities": []}, {"text": "Often , multilingual neural models have been found to outperform their monolingual counterparts on low- and zero - resource languages due to positive transfer effects , but underperform for high - resource languages ( Johnson et al . , 2017 ; Arivazhagan et al . , 2019 ; Conneau et al . , 2020 ) , a problem also known as \u201c the curse of multilinguality \u201d .", "entities": []}, {"text": "Generally speaking , a multilingual model without language - speci\ufb01c supervision is likely to suffer from over - generalization and perform poorly on high - resource languages due to limited capacity compared to the monolingual baselines , as veri\ufb01ed by our experiments on parsing .", "entities": []}, {"text": "In this paper , we strike a good balance between maximum sharing and language - speci\ufb01c capacity in multilingual dependency parsing .", "entities": [[19, 21, "TaskName", "dependency parsing"]]}, {"text": "Inspired by recently introduced parameter sharing techniques ( Platanios et al . , 2018 ; Houlsby et al . , 2019 ) , we propose a new multilingual parser , UDapter , that learns to modify its language - speci\ufb01c parameters including the adapter modules , as a function of language embeddings .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "This allows the model to share parameters across languages , ensuring generalization and transfer ability , but also enables language - speci\ufb01c parameterization in a single multilingual model .", "entities": []}, {"text": "Furthermore , we propose not to learn language embeddings from scratch , but to leverage a mix of linguistically curated and predicted typological features as obtained from the URIEL language typology database ( Littell et al . , 2017 ) which supports 3718 languages including all languages represented in UD .", "entities": [[49, 50, "DatasetName", "UD"]]}, {"text": "While the importance of typological features for cross - lingual parsing is known for both non - neural ( Naseem et al . , 2012 ; T \u00a8ackstr \u00a8om et al . , 2013 ; Zhang and Barzilay , 2015 ) and neural approaches ( Ammar et al . , 2016 ;", "entities": []}, {"text": "Scholivet et al . , 2019 ) , we are the \ufb01rst to use them", "entities": []}, {"text": "2303effectively as direct input to a neural parser , without manual selection , over a large number of languages in the context of zero - shot parsing where gold POS labels are not given at test time .", "entities": []}, {"text": "In our model , typological features are crucial , leading to a substantial LAS increase on zero - shot languages and no loss on high - resource languages when compared to the language embeddings learned from scratch .", "entities": [[22, 23, "MetricName", "loss"]]}, {"text": "We train and test our model on the 13 syntactically diverse high - resource languages that were used by Kulmizev et al .", "entities": []}, {"text": "( 2019 ) , and also evaluate it on 30 genuinely low - resource languages .", "entities": []}, {"text": "Results show that UDapter signi\ufb01cantly outperforms stateof - the - art monolingual ( Straka , 2018 ) and multilingual ( Kondratyuk and Straka , 2019 ) parsers on most high - resource languages and achieves overall promising improvements on zero - shot languages .", "entities": []}, {"text": "Contributions We conduct several experiments on a large set of languages and perform thorough analyses of our model .", "entities": []}, {"text": "Accordingly , we make the following contributions : 1 ) We apply the idea of adapter tuning ( Rebuf\ufb01 et al . , 2018 ; Houlsby et al . , 2019 ) to the task of universal dependency parsing .", "entities": [[37, 39, "TaskName", "dependency parsing"]]}, {"text": "2 ) We combine adapters with the idea of contextual parameter generation ( Platanios et al . , 2018 ) , leading to a novel language adaptation approach with state - of - the art UD parsing results .", "entities": [[35, 36, "DatasetName", "UD"]]}, {"text": "3 ) We provide a simple but effective method for conditioning the language adaptation on existing typological language features , which we show is crucial for zero - shot performance .", "entities": []}, {"text": "2 Previous Work This section presents the background of our approach .", "entities": []}, {"text": "Multilingual Neural Networks", "entities": []}, {"text": "Early models in multilingual neural machine translation ( NMT ) designed dedicated architectures ( Dong et al . , 2015 ;", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "Firat et al . , 2016 ) whilst subsequent models , from Johnson et al . ( 2017 ) onward , added a simple language identi\ufb01er to the models with the same architecture as their monolingual counterparts .", "entities": []}, {"text": "More recently , multilingual NMT models have focused on maximizing transfer accuracy for low - resource language pairs , while preserving high - resource language accuracy ( Platanios et al . , 2018 ; Neubig and Hu , 2018 ; Aharoni et al . , 2019 ; Arivazhagan et al . , 2019 ) , known as the ( positive ) transfer - ( negative ) interference trade - off .", "entities": [[11, 12, "MetricName", "accuracy"], [25, 26, "MetricName", "accuracy"]]}, {"text": "Another line of work builds massively multilingual pre - trained language mod - els to produce contextual representation to be used in downstream tasks ( Devlin et al . , 2019 ; Conneau et al . , 2020 ) .", "entities": []}, {"text": "As the leading model , multilingual BERT ( mBERT)2(Devlin et al . , 2019 ) which is a deep self - attention network , was trained without language - speci\ufb01c signals on the 104 languages with the largest Wikipedias .", "entities": [[6, 7, "MethodName", "BERT"], [19, 23, "MethodName", "self - attention network"]]}, {"text": "It uses a shared vocabulary of 110 K WordPieces ( Wu et al . , 2016 ) , and has been shown to facilitate cross - lingual transfer in several applications ( Pires et al . , 2019 ; Wu and Dredze , 2019 ) .", "entities": [[24, 28, "TaskName", "cross - lingual transfer"]]}, {"text": "Concurrently to our work , Pfeiffer et al .", "entities": []}, {"text": "( 2020 ) have proposed to combine language and task adapters , small bottleneck layers ( Rebuf\ufb01 et al . , 2018 ; Houlsby et al . , 2019 ) , to address the capacity issue which limits multilingual pre - trained models for cross - lingual transfer .", "entities": [[44, 48, "TaskName", "cross - lingual transfer"]]}, {"text": "Cross - Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages ( McDonald et al . , 2013 ; Nivre et al . , 2018 ) has provided an opportunity for the study of cross - lingual parsing .", "entities": [[3, 5, "TaskName", "Dependency Parsing"]]}, {"text": "Early studies trained a delexicalized parser ( Zeman and Resnik , 2008 ; McDonald et al . , 2013 ) on one or more source languages by using either gold or predicted POS labels ( Tiedemann , 2015 ) and applied it to target languages .", "entities": []}, {"text": "Building on this , later work used additional features such as typological language properties ( Naseem et al . , 2012 ) , syntactic embeddings ( Duong et al . , 2015 ) , and cross - lingual word clusters ( T \u00a8ackstr \u00a8om et al . , 2012 ) .", "entities": []}, {"text": "Among lexicalized approaches , Vilares et al .", "entities": []}, {"text": "( 2016 ) learns a bilingual parser on a corpora obtained by merging harmonized treebanks .", "entities": []}, {"text": "Ammar et al .", "entities": []}, {"text": "( 2016 ) trains a multilingual parser using multilingual word embeddings , token - level language information , language typology features and \ufb01ne - grained POS tags .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "More recently , based on mBERT ( Devlin et al . , 2019 ) , zero - shot transfer in dependency parsing was investigated ( Wu and Dredze , 2019 ; Tran and Bisazza , 2019 ) .", "entities": [[5, 6, "MethodName", "mBERT"], [20, 22, "TaskName", "dependency parsing"]]}, {"text": "Finally Kondratyuk and Straka ( 2019 ) trained a multilingual parser on the concatenation of all available UD treebanks .", "entities": [[17, 18, "DatasetName", "UD"]]}, {"text": "Language Embeddings and Typology Conditioning a multilingual model on the input language is studied in NMT ( Ha et al . , 2016 ; Johnson et al . , 2017 ) , syntactic parsing ( Ammar et al . , 2016 ) and language modeling ( \u00a8Ostling and Tiedemann , 2017 ) .", "entities": []}, {"text": "The goal is to embed language information in real2https://github.com/google-research/ bert / blob / master / multilingual.md", "entities": []}, {"text": "2304valued vectors in order to enrich internal representations with input language for multilingual models .", "entities": []}, {"text": "In dependency parsing , several previous studies ( Naseem et al . , 2012 ; T \u00a8ackstr \u00a8om et al . , 2013 ; Zhang and Barzilay , 2015 ; Ammar et", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "al . , 2016 ; Scholivet et al . , 2019 ) have suggested that typological features are useful for the selective sharing of transfer information .", "entities": []}, {"text": "Results , however , are mixed and often limited to a handful of manually selected features ( Fisch et al . , 2019 ; Ponti et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "As the most similar work to ours , Ammar et al .", "entities": []}, {"text": "( 2016 ) uses typological features to learn language embeddings as part of training , by augmenting each input token and parsing action representation .", "entities": []}, {"text": "Unfortunately though , this technique is found to underperform the simple use of randomly initialized language embeddings ( \u2018 language IDs \u2019 ) .", "entities": []}, {"text": "Authors also reported that language embeddings hurt the performance of the parser in zero - shot experiments ( Ammar et al . , 2016 , footnote 30 ) .", "entities": []}, {"text": "Our work instead demonstrates that typological features can be very effective if used with the right adaptation strategy in both supervised and zero - shot settings .", "entities": []}, {"text": "Finally , Lin et al .", "entities": []}, {"text": "( 2019 ) use typological features , along with properties of the training data , to choose optimal transfer languages for various tasks , including UD parsing , in a hard manner .", "entities": [[25, 26, "DatasetName", "UD"]]}, {"text": "By contrast , we focus on a soft parameter sharing approach to maximize generalizations within a single universal model .", "entities": []}, {"text": "3 Proposed Model", "entities": []}, {"text": "In this section , we present our truly universal dependency parser , UDapter .", "entities": []}, {"text": "UDapter consists of a biaf\ufb01ne attention layer stacked on top of the pretrained Transformer encoder ( mBERT ) .", "entities": [[13, 14, "MethodName", "Transformer"], [16, 17, "MethodName", "mBERT"]]}, {"text": "This is similar to ( Wu and Dredze , 2019 ; Kondratyuk and Straka , 2019 ) , except that our mBERT layers are interleaved with special adapter layers inspired by Houlsby et al .", "entities": [[21, 22, "MethodName", "mBERT"]]}, {"text": "( 2019 ) .", "entities": []}, {"text": "While mBERT weights are frozen , biaf\ufb01ne attention and adapter layer weights are generated by a contextual parameter generator ( Platanios et al . , 2018 ) that takes a language embedding as input and is updated while training on the treebanks .", "entities": [[1, 2, "MethodName", "mBERT"]]}, {"text": "Note that the proposed adaptation approach is not restricted to dependency parsing and is in principle applicable to a range of multilingual NLP tasks .", "entities": [[10, 12, "TaskName", "dependency parsing"], [21, 23, "TaskName", "multilingual NLP"]]}, {"text": "We will now describe the components of our model.3.1 Biaf\ufb01ne Attention Parser", "entities": []}, {"text": "The top layer of UDapter is a graph - based biaf\ufb01ne attention parser proposed by Dozat and Manning ( 2017 ) .", "entities": []}, {"text": "In this model , an encoder generates an internal representation rifor each word ; the decoder takes riand passes it through separate feedforward layers ( MLP ) , and \ufb01nally uses deep biaf\ufb01ne attention to score arcs connecting a head and a tail : h(head ) i = MLP(head)(ri ) ( 1 ) h(tail ) i = MLP(tail)(ri ) ( 2 ) s(arc)=Biaffine ( H(head);H(tail ) ) ( 3 ) Similarly , label scores are calculated by using a biaf\ufb01ne classi\ufb01er over two separate feedforward layers .", "entities": [[25, 26, "DatasetName", "MLP"]]}, {"text": "Finally , the Chu - Liu / Edmonds algorithm ( Chu , 1965 ; Edmonds , 1967 ) is used to \ufb01nd the highest scoring valid dependency tree .", "entities": []}, {"text": "3.2 Transformer Encoder with Adapters To obtain contextualized word representations , UDapter uses mBERT .", "entities": [[1, 2, "MethodName", "Transformer"], [13, 14, "MethodName", "mBERT"]]}, {"text": "For a token iin sentenceS , BERT builds an input representation wicomposed by summing a WordPiece embedding xi(Wu et al . , 2016 ) and a position embedding fi .", "entities": [[6, 7, "MethodName", "BERT"], [15, 16, "MethodName", "WordPiece"]]}, {"text": "Each wi2Sis then passed to a stacked self - attention layers ( SA ) to generate the \ufb01nal encoder representation ri :", "entities": [[9, 11, "HyperparameterName", "attention layers"]]}, {"text": "wi = xi+fi ( 4 ) ri = SA(wi ; \u0002(ad ) ) ( 5 ) where \u0002(ad)denotes the adapter modules .", "entities": []}, {"text": "During training , instead of \ufb01ne - tuning the whole encoder network together with the task - speci\ufb01c top layer , we use adapter modules ( Rebuf\ufb01 et al . , 2018 ; Stickland and Murray , 2019 ; Houlsby et al . , 2019 ) , or simply adapters , to capture both task - speci\ufb01c and language - speci\ufb01c information .", "entities": []}, {"text": "Adapters are small modules added between layers of a pre - trained network .", "entities": []}, {"text": "In adapter tuning , the weights of the original network are kept frozen , whilst the adapters are trained for a downstream task .", "entities": []}, {"text": "Tuning with adapters was mainly suggested for parameter ef\ufb01ciency but they also act as an information module for the task or the language to be adapted ( Pfeiffer et al . , 2020 ) .", "entities": []}, {"text": "In this way , the original network serves as a memory for the language(s ) .", "entities": []}, {"text": "In UDapter , following Houlsby et al .", "entities": []}, {"text": "( 2019 ) , two bottleneck adapters with two feedforward projections and a GELU nonlinearity ( Hendrycks and Gimpel , 2016 ) are inserted into each transformer layer as shown in", "entities": [[13, 14, "MethodName", "GELU"]]}, {"text": "2305 F \u00a0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 Biaffine \t Attention \t ( for \t Dependency \t Parsing ) BERT \t Encoder \t\t \t\t I \t\t\t\t have \t\t\t a \t\t\t banana \t\t in \t\t\t\t my \t\t\t\t ear < eng > Feed - forward Layer Multi - headed Self Attention LayerLayer Norm2x Feed - forward layerAdapter LayerLayer Norm Adapter Layer PParameter \t Generator \t with \t Language \t Embeddings P LTrainable \t Layers Frozen \t Layers Trainable \t variables Parameter \t Tensor Language \t Feature \t VectorLanguage \t EmbeddingP L FComputed \t Values \t ( Tensor)Parameters are generated by using language embedding ( L ) with \u00a0 dot product ( . )", "entities": [[5, 6, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [14, 15, "DatasetName", "0"], [28, 29, "MethodName", "BERT"], [65, 66, "MethodName", "Adapter"]]}, {"text": "as simple linear transform Language features ( F ) are transformed to language embeddings ( L ) by a MLP network \u00a0 \u00a0  Transformer \t Layer \t  with \t Adapters Feedforward   down - projectFeedforward   up - project Nonlinearity Adapter \t Layer Figure 1 : UDapter architecture with contextual parameter generator ( CPG ) and adapter layers .", "entities": [[19, 20, "DatasetName", "MLP"], [22, 23, "MethodName", "Transformer"], [39, 40, "MethodName", "Adapter"]]}, {"text": "CPG takes languages embeddings projected from typological features as input and generates parameters of adapter layers and biaf\ufb01ne attention .", "entities": []}, {"text": "Figure 1 .", "entities": []}, {"text": "We apply adapter tuning for two reasons : 1 ) Each adapter module consists of only few parameters and allows to use contextual parameter generation ( CPG ; seex3.3 ) with a reasonable number of trainable parameters.32 )", "entities": []}, {"text": "Adapters enable taskspeci\ufb01c as well as language - speci\ufb01c adaptation via CPG since it keeps backbone multilingual representations as memory for all languages in pre - training , which is important for multilingual transfer .", "entities": []}, {"text": "3.3 Contextual Parameter Generator To control the amount of sharing across languages , we generate trainable parameters of the model using a contextual parameter generator ( CPG ) function inspired by Platanios et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "CPG enables UDapter to retain high multilingual quality without losing performance on a single language , during multi - language training .", "entities": []}, {"text": "We de\ufb01ne CPG as a function of language embeddings .", "entities": []}, {"text": "Since we only train adapters and the biaf\ufb01ne attention ( i.e. adapter tuning ) , the parameter generator is formalized asf\u0012(ad ) ; \u0012(bf)g , g(m)(le)whereg(m)denotes the parameter generator with language embedding le , and\u0012(ad)and\u0012(bf)denote the parameters of adapters and biaf\ufb01ne attention respectively .", "entities": []}, {"text": "We implement CPG as a simple linear transform of a language embedding , similar to Platanios et al .", "entities": []}, {"text": "( 2018 ) , so that weights of adapters in the encoder and biaf\ufb01ne attention are generated by the dot product of language embeddings : g(m)(le )", "entities": []}, {"text": "= ( W(ad);W(bf))\u0001le ( 6 ) 3Due to CPG , the number of adapter parameters is multiplied by language embedding size , resulting in a larger model compared to the baseline ( more details in Appendix A.1).where le2RM , W(ad)2RP(ad)\u0002M , W(bf)2 RP(bf)\u0002M , M is the language embedding size , P(ad)andP(bf)are the number of parameters for adapters and biaf\ufb01ne attention respectively.4An important advantage of CPG is the easy integration of existing task or language features .", "entities": [[53, 56, "HyperparameterName", "number of parameters"]]}, {"text": "3.4 Typology - Based Language Embeddings Soft sharing via CPG enables our model to modify its parsing decisions depending on a language embedding .", "entities": []}, {"text": "While this allows UDapter to perform well on the languages in training , even if they are typologically diverse , information sharing is still a problem for languages not seen during training ( zero - shot learning ) as a language embedding is not available .", "entities": [[33, 37, "TaskName", "zero - shot learning"]]}, {"text": "Inspired by Naseem et al .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "( 2012 ) and Ammar et al .", "entities": []}, {"text": "( 2016 ) , we address this problem by de\ufb01ning language embeddings as a function of a large set of language typological features , including syntactic and phonological features .", "entities": []}, {"text": "We use a multi - layer perceptron MLP(lang)with two feedforward layers and a ReLU nonlinear activation to compute a language embedding le : le = MLP(lang)(lt ) ( 7 ) where ltis a typological feature vector for a language consisting of all103 syntactic , 28 phonological and 158 phonetic inventory features from the URIEL language typology database ( Littell et al . , 2017 ) .", "entities": [[13, 14, "MethodName", "ReLU"]]}, {"text": "URIEL is a collection of binary features 4Platanios et al .", "entities": []}, {"text": "( 2018 ) also suggest to apply parameter grouping .", "entities": []}, {"text": "We have not tried that yet , but one may learn separate low - rank projections of language embeddings for the adapter parameters group and the biaf\ufb01ne parameters group .", "entities": []}, {"text": "2306ar en eu \ufb01", "entities": []}, {"text": "he hi it", "entities": []}, {"text": "ja ko ru sv tr zh HR - AVG LR", "entities": []}, {"text": "-AVG", "entities": []}, {"text": "Previous work : uuparser - bert [ 1 ] 81.8 87.6 79.8 83.9 85.9 90.8 91.7 92.1 84.2 91.0 86.9 64.9 83.4 84.9 udpipe [ 2 ] 82.9 87.0 82.9 87.5 86.9 91.8 91.5 93.7 84.2 92.3 86.6 67.6 80.5 85.8 udify [ 3 ] 82.9 88.5 81.0 82.1 88.1 91.5 93.7 92.1 74.3 93.1 89.1 67.4 83.8 85.2 34.1 Monolingually trained ( one model per language ): mono - udify 83.5 89.4 81.3 87.3 87.9 91.1 93.1 92.5 84.2 91.9 88.0 66.0 82.4 86.0 Multilingually trained ( one model for all languages ): multi - udify 80.1 88.5 76.4 85.1 84.4 89.3 92.0 90.0 78.0 89.0 86.2 62.9 77.8 83.0 35.3 adapter - only 82.8 88.3 80.2 86.9 86.2 90.6 93.1 91.6 81.3 90.8 88.4 66.0 79.4 85.0 32.9 udapter 84.4 89.7 83.3 89.0 88.8 92.0 93.5 92.8 85.9 92.2 90.3 69.6 83.2 87.3 36.5 Table 1 : Labelled attachment scores ( LAS ) on high - resource languages for baselines and UDapter .", "entities": []}, {"text": "Last two columns show average LAS of 13 high - resource ( HR - AVG ) and 30 low - resource ( LR - AVG ) languages respectively .", "entities": []}, {"text": "Previous work results are reported from ( Kulmizev et al . , 2019 )", "entities": []}, {"text": "[ 1 ] and ( Kondratyuk and Straka , 2019 )", "entities": []}, {"text": "[ 2,3 ] .", "entities": []}, {"text": "be br * bxr * cy fo * gsw * hsb * kk koi * krl * mdf * mr olo * pcm * sa * tl yo * yue * AVG multi - udify 80.1", "entities": []}, {"text": "60.5 26.1 53.6 68.6 43.6 53.2 61.9 20.8 49.2 24.8 46.4 42.1 36.1 19.4 62.7 41.2 30.5 45.2 udapter - proxy 69.9 - - - 64.1 23.7 44.4 45.1 - 45.6 - 29.6 41.1 - 15.1 - - 24.5 udapter 79.3 58.5 28.9 54.4 69.2 45.5 54.2 60.7 23.1 48.4 26.6 44.4 43.3 36.7 22.2 69.5 42.7 32.8 46.2 Table 2 : Labelled attachment scores ( LAS ) on a subset of 30 low - resource languages .", "entities": []}, {"text": "Languages with \u2018 * \u2019 are not included in mBERT training corpus .", "entities": [[9, 10, "MethodName", "mBERT"]]}, {"text": "( Results for all low - resource languages , together with the chosen proxy , are given in Appendix A.2 . ) extracted from multiple typological and phylogenetic databases such as WALS ( World Atlas of Language Structures ) ( Dryer and Haspelmath , 2013 ) , PHOIBLE ( Moran and McCloy , 2019 ) , Ethnologue ( Lewis et al . , 2015 ) and Glottolog ( Hammarstr \u00a8om et al . , 2020 ) .", "entities": [[34, 35, "DatasetName", "Atlas"]]}, {"text": "As many feature values are not available for each language , we use the values predicted by Littell et al .", "entities": []}, {"text": "( 2017 ) using a k - nearest neighbors approach based on average of genetic , geographical and feature distances between languages .", "entities": [[5, 9, "MethodName", "k - nearest neighbors"]]}, {"text": "4 Experiments Data and Training Details For our training languages , we follow Kulmizev et al .", "entities": []}, {"text": "( 2019 ) , who selected from UD 2.3 ( Nivre et al . , 2018 ) 13 treebanks \u201c from different language families , with different morphological complexity , scripts , character set sizes , training sizes , domains , and with good annotation quality \u201d ( see codes in Table 1).5During training , a language identi\ufb01er is added to each sentence , and gold word segmentation is provided .", "entities": [[7, 8, "DatasetName", "UD"]]}, {"text": "We test our models on the training languages ( highresource set ) , and on 30 languages that have no or very little training data ( low - resource set ) in a 5To reduce training time we cap the very large Russian Syntagrus treebank ( 48 K sentences ) to a random 15 K sample.zero - shot setup , i.e , without any training data.6The detailed treebank list is provided in Appendix A.3 .", "entities": []}, {"text": "For evaluation , the of\ufb01cial CoNLL 2018 Shared Task script7is used to obtain LAS scores on the test set of each treebank .", "entities": []}, {"text": "For the encoder , we use BERT - multilingualcased together with its WordPiece tokenizer .", "entities": [[6, 7, "MethodName", "BERT"], [12, 13, "MethodName", "WordPiece"]]}, {"text": "Since dependency annotations are between words , we pass the BERT output corresponding to the \ufb01rst wordpiece per word to the biaf\ufb01ne parser .", "entities": [[10, 11, "MethodName", "BERT"], [16, 17, "MethodName", "wordpiece"]]}, {"text": "We apply the same hyper - parameter settings as Kondratyuk and Straka ( 2019 ) .", "entities": []}, {"text": "Additionally , we use 256 and 32 for adapter size and language embedding size respectively .", "entities": []}, {"text": "In our approach , pre - trained BERT weights are frozen , and only adapters and biaf\ufb01ne attention are trained , thus we use the same learning rate for the whole network by applying an inverse square root learning rate decay with linear warmup ( Howard and Ruder , 2018 ) .", "entities": [[7, 8, "MethodName", "BERT"], [26, 28, "HyperparameterName", "learning rate"], [38, 40, "HyperparameterName", "learning rate"], [42, 44, "MethodName", "linear warmup"]]}, {"text": "Appendix A.1 gives the hyper - parameter details .", "entities": []}, {"text": "Baselines We compare UDapter to the current state of the art in UD parsing :", "entities": [[12, 13, "DatasetName", "UD"]]}, {"text": "[ 1 ] UUparser+BERT ( Kulmizev et al . , 2019 ) , a graph - based BLSTM 6For this reason , the terms \u2018 zero - shot \u2019 and \u2018 low - resource \u2019 are used interchangeably in this paper .", "entities": []}, {"text": "7https://universaldependencies.org/ conll18 / evaluation.html", "entities": []}, {"text": "2307parser ( de Lhoneux et", "entities": []}, {"text": "al . , 2017 ; Smith et al . , 2018 ) using mBERT embeddings as additional features .", "entities": [[13, 14, "MethodName", "mBERT"]]}, {"text": "[ 2 ] UDpipe ( Straka , 2018 ) , a monolingually trained multi - task parser that uses pretrained word embeddings and character representations .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "[ 3 ] UDify ( Kondratyuk and Straka , 2019 ) , the mBERTbased multi - task UD parser on which our UDapter is based , but originally trained on alllanguage treebanks from UD .", "entities": [[17, 18, "DatasetName", "UD"], [33, 34, "DatasetName", "UD"]]}, {"text": "UDPipe scores are taken from Kondratyuk and Straka ( 2019 ) .", "entities": []}, {"text": "To enable a direct comparison , we also re - train UDify on our set of 13 high - resource languages both monolingually ( one treebank at a time ; monoudify ) and multilingually ( on the concatenation of languages ; multi - udify ) .", "entities": []}, {"text": "Finally , we evaluate two variants of our model : 1 ) Adapter - only has only task - speci\ufb01c adapter modules and no languagespeci\ufb01c adaptation , i.e. no contextual parameter generator ; and 2 ) UDapter - proxy is trained without typology features : a separate language embedding is learnt from scratch for each in - training language , and for low - resource languages we use one from the same language family , if available , as proxy representation .", "entities": [[12, 13, "MethodName", "Adapter"]]}, {"text": "Importantly , all baselines are either trained for a single language , or multilingually without any language - speci\ufb01c adaptation .", "entities": []}, {"text": "By comparing UDapter to these parsers , we highlight its unique character that enables language speci\ufb01c parameterization by typological features within a multilingual framework for both supervised and zero - shot learning setup .", "entities": [[28, 32, "TaskName", "zero - shot learning"]]}, {"text": "4.1 Results Overall , UDapter outperforms the monolingual and multilingual baselines on both high - resource and zero - shot languages .", "entities": []}, {"text": "Below , we elaborate on the detailed results .", "entities": []}, {"text": "High - resource Languages Labelled Attachement Scores ( LAS ) on the high - resource set are given in Table 1 .", "entities": []}, {"text": "UDapter consistently outperforms both our monolingual and multilingual baselines in all languages , and beats the previous work , setting a new state of the art , in 9 out of 13 languages .", "entities": []}, {"text": "Statistical signi\ufb01cance testing8applied between UDapter andmulti / mono - udify con\ufb01rms that UDapter \u2019s performance is signi\ufb01cantly better than the baselines in 11 out of 13 languages ( all except enandit ) .", "entities": []}, {"text": "8We used paired bootstrap resampling to check whether the difference between two models is signi\ufb01cant ( p < 0.05 ) by using Udapi ( Popel et al . , 2017 ) .", "entities": []}, {"text": "ko eu tr zh he ar sv fi ru ja", "entities": []}, {"text": "hi it en0246810 difference ( udapter , multi - udify ) 048121620 treebank size ( K)Figure 2 : Difference in LAS between UDapter and multi - udify in the high - resource setting .", "entities": []}, {"text": "Diamonds indicate the amount of sentences in the corresponding treebank .", "entities": []}, {"text": "Among directly comparable baselines , multiudify gives the worst performance in the typologically diverse high - resource setting .", "entities": []}, {"text": "This multilingual model is clearly worse than its monolingually trained counterparts mono - udify : 83.0 vs86.0 .", "entities": []}, {"text": "This result resounds with previous \ufb01ndings in multilingual NMT ( Arivazhagan et al . , 2019 ) and highlights the importance of language adaptation even when using high - quality sentence representations like those produced by mBERT .", "entities": [[36, 37, "MethodName", "mBERT"]]}, {"text": "To understand the relevance of adapters , we also evaluate a model which has almost the same architecture as multi - udify except for the adapter modules and the tuning choice ( frozen mBERT weights ) .", "entities": [[33, 34, "MethodName", "mBERT"]]}, {"text": "Interestingly , this adapter - only model considerably outperforms multi - udify ( 85.0 vs 83.0 ) , indicating that adapter modules are also effective in multilingual scenarios .", "entities": []}, {"text": "Finally , UDapter achieves the overall best results , with consistent gains over both multi - udify andadapter - only , showing the importance of linguistically informed adaptation even for in - training languages .", "entities": []}, {"text": "Low - Resource Languages Average LAS on the 30 low - resource languages are shown in column lr - avg of Table 1 .", "entities": []}, {"text": "Overall , UDapter slightly outperforms the multi - udify baseline ( 36.5 vs36.3 ) , which shows the bene\ufb01ts of our approach on both in - training and zero - shot languages .", "entities": []}, {"text": "For a closer look , Table 2 provides individual results for the 18 representative languages in our low - resource set .", "entities": []}, {"text": "Here we \ufb01nd a mixed picture : UDapter outperforms multi - udify on 13 out of 18 languages9 .", "entities": []}, {"text": "Achieving improvements in the zero - shot parsing 9LAS scores for all 30 languages are given in Appendix A.2 .", "entities": []}, {"text": "By signi\ufb01cance testing , UDapter is signi\ufb01cantly better than multi - udify on 16/30 low - resource languages , which is shown in Table 4", "entities": []}, {"text": "2308 HR LR30405060708090 multi - udify adapter - only ( 1024 ) adapter - only ( 2048 ) udapter(a ) high - resource low - resource ( zero - shot)30405060708090 adapter - only ( 1024 ) cpg ( adapters ) cpg ( adap.+biaf . ) *", "entities": [[16, 17, "DatasetName", "2048"]]}, {"text": "( b ) Figure 3 : Impact of different UDapter components on parsing performance ( LAS ): ( a ) adapters and adapter layer size , ( b ) application of contextual parameter generation to different portions of the network .", "entities": []}, {"text": "In ( b ) the model named \u2018 cpg ( adap.+biaf . ) \u2019 coincides with the full UDapter .", "entities": []}, {"text": "setup is very dif\ufb01cult , thus we believe this result is an important step towards overcoming the problem of positive / negative transfer trade - off .", "entities": []}, {"text": "Indeed , UDapter - proxy results show that choosing a proxy language embedding from the same language family underperforms UDapter , apart from not being available for many languages .", "entities": []}, {"text": "This indicates the importance of typological features in our approach ( seex5.2 for further analysis ) .", "entities": []}, {"text": "5 Analysis In this section , we further analyse UDapter to understand its impact on different languages , and the importance of its various components .", "entities": []}, {"text": "5.1 Which languages improve most ?", "entities": []}, {"text": "Figure 2 presents the LAS gain of UDapter over the multi - udify baseline for each high - resource language along with the respective treebank training size .", "entities": []}, {"text": "To summarize , the gains are higher for languages with less training data .", "entities": []}, {"text": "This suggests that in UDapter , useful knowledge is shared among intraining languages , which bene\ufb01ts low resource languages without hurting high resource ones .", "entities": []}, {"text": "For zero - shot languages , the difference between the two models is small compared to high - resource languages ( +1.2 LAS ) .", "entities": []}, {"text": "While it is harder to \ufb01nd a trend here , we notice that UDapter is typically bene\ufb01cial for the languages notpresent in the mBERT training corpus : it outperforms multi - udify in 13 out of 22 ( non - mBERT ) languages .", "entities": [[23, 24, "MethodName", "mBERT"], [40, 41, "MethodName", "mBERT"]]}, {"text": "This suggests that typological feature - based adaptation leads to improved sentence representations when the pretrained encoder has not been exposed to a language .", "entities": []}, {"text": "high - resource low - resource ( zero - shot)0102030405060708090 From scratch & Centroid Typological features(a ) syntax phonology inventory0.500.510.520.530.540.550.560.570.58 Language - Typology Features ( b ) Figure 4 : ( a ) Impact of language typology features on parsing performance ( LAS ) .", "entities": []}, {"text": "( b ) Average of normalized feature weights obtained from linear projection layer of the language embedding network .", "entities": []}, {"text": "5.2 How much gain from typology ?", "entities": []}, {"text": "UDapter learns language embeddings from syntactic , phonological and phonetic inventory features .", "entities": []}, {"text": "A natural alternative to this choice is to learn language embeddings from scratch .", "entities": []}, {"text": "For a comparison , we train a model where , for each in - training language , a separate language embedding ( of the same size : 32 ) is initialized randomly and learned end - toend .", "entities": []}, {"text": "For the zero - shot languages we use the average , or centroid , of all in - training language embeddings .", "entities": []}, {"text": "As shown in Figure 4a , on the high - resource set , the models with and without typological features achieve very similar average LAS ( 87.3 and 87.1 respectively ) .", "entities": []}, {"text": "On zero - shot languages , however , the use of centroid embedding performs very poorly : 9.0 vs36.5 average LAS score over 30 languages .", "entities": []}, {"text": "As already discussed in x4.1 ( Table 2 ) , using a proxy language embedding belonging to the same family as the test language , when available , also clearly underperforms UDapter .", "entities": []}, {"text": "These results con\ufb01rm our expectation that a model can learn reliable language embeddings for in - training languages , however typological signals are required to obtain a robust parsing quality on zero - shot languages .", "entities": []}, {"text": "5.3 How does UDapter represent languages ?", "entities": []}, {"text": "We start by analyzing the projection weights assigned to different typological features by the \ufb01rst layer of the language embedding network ( see eq . 7 ) .", "entities": []}, {"text": "Figure 4b shows the averages of normalized syntactic , phonological and phonetic inventory feature weights .", "entities": []}, {"text": "Although dependency parsing is a syntactic task , the network does not only utilize syntactic features , as also observed by Lin et al .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "( 2019 ) , but exploits all available typological features to learn its representations .", "entities": []}, {"text": "2309 A B C Figure 5 : Vector spaces for ( A ) language - typology feature vectors taken from URIEL , ( B ) language embeddings learned from typological features by UDapter , and ( C ) language embeddings learned without typological features .", "entities": []}, {"text": "High- and low - resource languages are indicated by red and blue dots respectively .", "entities": []}, {"text": "Highlighted clusters in A and B denote sets of genetically related languages .", "entities": []}, {"text": "Next , we plot the language representations learned in UDapter by using t - SNE ( van der Maaten and Hinton , 2008 ) , which is similar to the analysis carried out by Ponti et al .", "entities": []}, {"text": "( 2019 , \ufb01gure 8) using the language vectors learned by Malaviya", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2017 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Figure 5 illustrates 2D vector spaces generated for the typological feature vectors lt(A ) and the language embeddings lelearned by UDapter with or without typological features ( BandCrespectively ) .", "entities": []}, {"text": "The bene\ufb01ts of using typological features can be understood by comparing AandB :", "entities": []}, {"text": "During training , UDapter learns to project URIEL features to language embeddings in a way that is optimal for in - training language parsing quality .", "entities": []}, {"text": "This leads to a different placement of the high - resource languages ( red points ) in the space , where many linguistic similarities are preserved ( e.g. Hebrew and Arabic ; European languages except Basque ) but others are overruled ( Japanese drifting away from Korean ) .", "entities": []}, {"text": "Looking at the low - resource languages ( blue points ) we \ufb01nd that typologically similar languages tend to have similar embeddings to the closest highresource language in both AandB. In fact , most groupings of genetically related languages , such as the Indian languages ( hi - cluster ) or the Uralic ones ( \ufb01-cluster ) are largely preserved across these two spaces .", "entities": []}, {"text": "Comparing Band Cwhere language embeddings are learned from scratch , the absence of typological features leads to a seemingly random space with no linguistic similarities ( e.g. Arabic far away from Hebrew , Korean closer to English than to Japanese , etc . )", "entities": []}, {"text": "and , therefore , no principled wayto represent additional languages .", "entities": []}, {"text": "Taken together with the parsing results of x4.1 , these plots suggest that UDapter embeddings strike a good balance between a linguistically motivated representation space and one solely optimized for in - training language accuracy .", "entities": [[34, 35, "MetricName", "accuracy"]]}, {"text": "5.4 Is CPG really essential ?", "entities": []}, {"text": "In section 4.1 we observed that adapter tuning alone ( that is , without CPG ) improved the multilingual baseline in the high - resource languages , but worsened it considerably in the zero - shot setup .", "entities": []}, {"text": "By contrast , the addition of CPG with typological features led to the best results over all languages .", "entities": []}, {"text": "But could we have obtained similar results by simply increasing the adapter size ?", "entities": []}, {"text": "For instance , in multilingual MT , increasing overall model capacity of an already very large and deep architecture can be a powerful alternative to more sophisticated parameter sharing approaches ( Arivazhagan et al . , 2019 ) .", "entities": []}, {"text": "To answer this question we train another adapteronly model with doubled size ( 2048 instead of the 1024 used in the main experiments ) .", "entities": [[13, 14, "DatasetName", "2048"]]}, {"text": "As seen in 3a , increase in model size brings a slight gain to the high - resource languages , but actually leads to a small loss in the zero - shot setup .", "entities": [[26, 27, "MetricName", "loss"]]}, {"text": "This shows that adapters enlarge the per - language capacity for in - training languages , but at the same time they hurt generalization and zero - shot transfer .", "entities": []}, {"text": "By contrast , UDapter including CPG which increases the model size by language embeddings ( see Appendix A.1 for details ) , outperforms both adapter - only models , con\ufb01rming once more the", "entities": []}, {"text": "2310importance of this component .", "entities": []}, {"text": "For our last analysis ( Fig .", "entities": []}, {"text": "3b )", "entities": []}, {"text": ", we study soft parameter sharing via CPG on different portions of the network , namely : only on the adapter modules \u2018 cpg ( adapters ) \u2019 versus on both adapters and biaf\ufb01ne attention \u2018 cpg ( adap.+biaf . ) \u2019 corresponding to the full UDapter .", "entities": []}, {"text": "Results show that most of the gain in the high - resource languages is obtained by only applying CPG on the multilingual encoder .", "entities": []}, {"text": "On the other hand , for the low - resource languages , typological feature based parameter sharing is most important in the biaf\ufb01ne attention layer .", "entities": []}, {"text": "We leave further investigation of this result to future work .", "entities": []}, {"text": "6 Conclusion We have presented UDapter , a multilingual dependency parsing model that learns to adapt languagespeci\ufb01c parameters on the basis of adapter modules ( Rebuf\ufb01 et al . , 2018 ; Houlsby et al . , 2019 ) and the contextual parameter generation ( CPG ) method ( Platanios et al . , 2018 ) which is in principle applicable to a range of multilingual NLP tasks .", "entities": [[9, 11, "TaskName", "dependency parsing"], [65, 67, "TaskName", "multilingual NLP"]]}, {"text": "While adapters provide a more general tasklevel adaptation , CPG enables language - speci\ufb01c adaptation , de\ufb01ned as a function of language embeddings projected from linguistically curated typological features .", "entities": []}, {"text": "In this way , the model retains high per - language performance in the training data and achieves better zero - shot transfer .", "entities": []}, {"text": "UDapter , trained on a concatenation of typologically diverse languages ( Kulmizev et al . , 2019 ) , outperforms strong monolingual andmultilingual baselines on the majority of both high - resource and low - resource ( zero - shot ) languages , which re\ufb02ects its strong balance between per - language capacity and maximum sharing .", "entities": []}, {"text": "Finally , the analyses we performed on the underlying characteristics of our model show that typological features are crucial for zero - shot languages .", "entities": []}, {"text": "Acknowledgements Arianna Bisazza was partly funded by the Netherlands Organization for Scienti\ufb01c Research ( NWO ) under project number 639.021.646 .", "entities": []}, {"text": "We would like to thank the Center for Information Technology of the University of Groningen for providing access to the Peregrine HPC cluster and the anonymous reviewers for their helpful comments .", "entities": []}, {"text": "References Roee Aharoni , Melvin Johnson , and Orhan Firat .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Massively multilingual neural machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3874 \u2013 3884 .", "entities": []}, {"text": "Waleed Ammar , George Mulcaire , Miguel Ballesteros , Chris Dyer , and Noah A. Smith . 2016 .", "entities": []}, {"text": "Many languages , one parser .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 4:431\u2013444 .", "entities": []}, {"text": "Naveen Arivazhagan , Ankur Bapna , Orhan Firat , Dmitry Lepikhin , Melvin Johnson , Maxim Krikun , Mia Xu Chen , Yuan Cao , George Foster , Colin Cherry , Wolfgang Macherey , Zhifeng Chen , and Yonghui Wu . 2019 .", "entities": []}, {"text": "Massively multilingual neural machine translation in the wild : Findings and challenges .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "CoRR , abs/1907.05019 .", "entities": []}, {"text": "Yoeng - Jin Chu . 1965 .", "entities": []}, {"text": "On the shortest arborescence of a directed graph .", "entities": []}, {"text": "Scientia Sinica , 14:1396\u20131400 .", "entities": []}, {"text": "Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , Francisco Guzm \u00b4 an , Edouard Grave , Myle Ott , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unsupervised cross - lingual representation learning at scale .", "entities": [[4, 6, "TaskName", "representation learning"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8440 \u2013 8451 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Daxiang Dong , Hua Wu , Wei He , Dianhai Yu , and Haifeng Wang . 2015 .", "entities": []}, {"text": "Multi - task learning for multiple language translation .", "entities": [[0, 4, "TaskName", "Multi - task learning"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 1723\u20131732 .", "entities": []}, {"text": "Timothy Dozat and Christopher D Manning . 2017 .", "entities": []}, {"text": "Deep biaf\ufb01ne attention for neural dependency parsing .", "entities": [[5, 7, "TaskName", "dependency parsing"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Matthew S. Dryer and Martin Haspelmath , editors .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "WALS", "entities": []}, {"text": "Online .", "entities": []}, {"text": "Max Planck Institute for Evolutionary Anthropology , Leipzig .", "entities": []}, {"text": "Long Duong , Trevor Cohn , Steven Bird , and Paul Cook . 2015 .", "entities": []}, {"text": "A neural network model for low - resource Universal Dependency parsing .", "entities": [[9, 11, "TaskName", "Dependency parsing"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 339\u2013348 .", "entities": []}, {"text": "2311Jack Edmonds .", "entities": []}, {"text": "1967 .", "entities": []}, {"text": "Optimum branchings .", "entities": []}, {"text": "Journal of Research of the national Bureau of Standards B , 71(4):233\u2013240 .", "entities": []}, {"text": "Orhan Firat , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Multi - way , multilingual neural machine translation with a shared attention mechanism .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 866\u2013875 .", "entities": []}, {"text": "Adam Fisch , Jiang Guo , and Regina Barzilay .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "2019 .", "entities": []}, {"text": "Working hard or hardly working :", "entities": []}, {"text": "Challenges of integrating typology into neural dependency parsers .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 5714 \u2013 5720 .", "entities": []}, {"text": "Thanh - Le Ha , Jan Niehues , and Alex Waibel .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Toward multilingual neural machine translation with universal encoder and decoder .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "ArXiv preprint .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Harald Hammarstr \u00a8om , Robert Forkel , Martin Haspelmath , and Sebastian Bank .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Glottolog 4.3 .", "entities": []}, {"text": "Max Planck Institute for the Science of Human History , Jena .", "entities": []}, {"text": "Dan Hendrycks and Kevin Gimpel .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units .", "entities": [[6, 10, "MethodName", "gaussian error linear units"]]}, {"text": "International Conference on Learning Representations .", "entities": []}, {"text": "Neil Houlsby , Andrei Giurgiu , Stanislaw Jastrzebski , Bruna Morrone , Quentin De Laroussilhe , Andrea Gesmundo , Mona Attariyan , and Sylvain Gelly .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Parameter - ef\ufb01cient transfer learning for nlp .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "InInternational Conference on Machine Learning , pages 2790\u20132799 .", "entities": []}, {"text": "Jeremy Howard and Sebastian Ruder .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Universal language model \ufb01ne - tuning for text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 328\u2013339 .", "entities": []}, {"text": "Melvin Johnson , Mike Schuster , Quoc V .", "entities": []}, {"text": "Le , Maxim Krikun , Yonghui Wu , Zhifeng Chen , Nikhil Thorat , Fernanda Vi \u00b4 egas , Martin Wattenberg , Greg Corrado , Macduff Hughes , and Jeffrey Dean . 2017 .", "entities": []}, {"text": "Google \u2019s multilingual neural machine translation system : Enabling zero - shot translation .", "entities": [[0, 1, "DatasetName", "Google"], [4, 6, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 5:339\u2013351 .", "entities": []}, {"text": "Eliyahu Kiperwasser and Yoav Goldberg . 2016 .", "entities": []}, {"text": "Simple and accurate dependency parsing using bidirectional LSTM feature representations .", "entities": [[3, 5, "TaskName", "dependency parsing"], [6, 8, "MethodName", "bidirectional LSTM"]]}, {"text": "Transactions of the Association for Computational Linguistics , 4:313\u2013327 .", "entities": []}, {"text": "Dan Kondratyuk and Milan Straka . 2019 .", "entities": []}, {"text": "75 languages , 1 model : Parsing Universal Dependencies universally .", "entities": [[7, 9, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 2779\u20132795 .", "entities": []}, {"text": "Artur Kulmizev , Miryam de Lhoneux , Johannes Gontrum , Elena Fano , and Joakim Nivre .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Deep contextualized word embeddings in transitionbased and graph - based dependency parsing - a tale of two parsers revisited .", "entities": [[2, 4, "TaskName", "word embeddings"], [10, 12, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 2755\u20132768 .", "entities": []}, {"text": "M Paul Lewis , Gary F Simons , and CD Fennig .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Ethnologue : Languages of the world [ eighteenth .", "entities": []}, {"text": "Dallas , Texas : SIL International .", "entities": [[2, 3, "DatasetName", "Texas"]]}, {"text": "Miryam de Lhoneux , Johannes Bjerva , Isabelle Augenstein , and Anders S\u00f8gaard .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Parameter sharing between dependency parsers for related languages .", "entities": []}, {"text": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 4992\u20134997 .", "entities": []}, {"text": "Miryam de Lhoneux , Yan Shao , Ali Basirat , Eliyahu Kiperwasser , Sara Stymne , Yoav Goldberg , and Joakim Nivre .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "From raw text to Universal Dependencies - look , no tags !", "entities": [[4, 6, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proceedings of the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies , pages 207\u2013217 .", "entities": [[15, 17, "DatasetName", "Universal Dependencies"]]}, {"text": "Yu - Hsiang Lin , Chian - Yu Chen , Jean Lee , Zirui Li , Yuyan Zhang , Mengzhou Xia , Shruti Rijhwani , Junxian He , Zhisong Zhang , Xuezhe Ma , Antonios Anastasopoulos , Patrick Littell , and Graham Neubig .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Choosing transfer languages for cross - lingual learning .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3125\u20133135 .", "entities": []}, {"text": "Patrick Littell , David R. Mortensen , Ke Lin , Katherine Kairis , Carlisle Turner , and Lori Levin . 2017 .", "entities": []}, {"text": "URIEL and lang2vec : Representing languages as typological , geographical , and phylogenetic vectors .", "entities": []}, {"text": "InProceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers , pages 8\u201314 .", "entities": []}, {"text": "Laurens van der Maaten and Geoffrey Hinton .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Visualizing data using t - sne .", "entities": []}, {"text": "Journal of Machine Learning Research , 9:2579\u20132605 .", "entities": []}, {"text": "Chaitanya Malaviya , Graham Neubig , and Patrick Littell . 2017 .", "entities": []}, {"text": "Learning language representations for typology prediction .", "entities": []}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2529\u20132535 .", "entities": []}, {"text": "Ryan McDonald , Joakim Nivre , Yvonne QuirmbachBrundage , Yoav Goldberg , Dipanjan Das , Kuzman Ganchev , Keith Hall , Slav Petrov , Hao Zhang , Oscar T \u00a8ackstr \u00a8om , Claudia Bedini , N \u00b4 uria Bertomeu Castell \u00b4 o , and Jungmee Lee .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Universal Dependency annotation for multilingual parsing .", "entities": []}, {"text": "2312InProceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 92\u201397 .", "entities": []}, {"text": "Steven Moran and Daniel McCloy , editors .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "PHOIBLE 2.0 .", "entities": []}, {"text": "Max Planck Institute for the Science of Human History , Jena .", "entities": []}, {"text": "Tahira Naseem , Regina Barzilay , and Amir Globerson .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Selective sharing for multilingual dependency parsing .", "entities": [[4, 6, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 629\u2013637 .", "entities": []}, {"text": "Graham Neubig and Junjie Hu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Rapid adaptation of neural machine translation to new languages .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 875\u2013880 .", "entities": []}, {"text": "Joakim Nivre , Mitchell Abrams , \u02c7Zeljko Agi \u00b4 c , Lars Ahrenberg , Lene Antonsen , Katya Aplonova , Maria Jesus Aranzabe , Gashaw Arutie , Masayuki Asahara , Luma Ateyah , Mohammed Attia , Aitziber Atutxa , Liesbeth Augustinus , Elena Badmaeva , Miguel Ballesteros , Esha Banerjee , Sebastian Bank , Verginica Barbu Mititelu , Victoria Basmov , John Bauer , Sandra Bellato , Kepa Bengoetxea , Yevgeni Berzak , Irshad Ahmad Bhat , Riyaz Ahmad Bhat , Erica Biagetti , Eckhard Bick , Rogier Blokland , Victoria Bobicev , Carl B \u00a8orstell , Cristina Bosco , Gosse Bouma , Sam Bowman , Adriane Boyd , Aljoscha Burchardt , Marie Candito , Bernard Caron , Gauthier Caron , G \u00a8uls \u00b8en Cebiro \u02d8glu Eryi \u02d8git , Flavio Massimiliano Cecchini , Giuseppe G. A. Celano , Slavom \u00b4 \u0131r\u02c7C\u00b4epl\u00a8o , Savas Cetin , Fabricio Chalub , Jinho Choi , Yongseok Cho , Jayeol Chun , Silvie Cinkov \u00b4 a , Aur \u00b4 elie Collomb , C \u00b8 a\u02d8gr\u0131 C \u00b8 \u00a8oltekin , Miriam Connor , Marine Courtin , Elizabeth Davidson , Marie - Catherine de Marneffe , Valeria de Paiva , Arantza Diaz de Ilarraza , Carly Dickerson , Peter Dirix , Kaja Dobrovoljc , Timothy Dozat , Kira Droganova , Puneet Dwivedi , Marhaba Eli , Ali Elkahky , Binyam Ephrem , Toma \u02c7z Erjavec , Aline Etienne , Rich \u00b4 ard Farkas , Hector Fernandez Alcalde , Jennifer Foster , Cl \u00b4 audia Freitas , Katar \u00b4 \u0131na Gajdo \u02c7sov\u00b4a , Daniel Galbraith , Marcos Garcia , Moa G \u00a8ardenfors , Sebastian Garza , Kim Gerdes , Filip Ginter , Iakes Goenaga , Koldo Gojenola , Memduh G \u00a8ok\u0131rmak , Yoav Goldberg , Xavier G \u00b4 omez Guinovart , Berta Gonz \u00b4 ales Saavedra , Matias Grioni , Normunds Gr \u00afuz\u00af\u0131tis , Bruno Guillaume , C \u00b4 eline Guillot - Barbance , Nizar Habash , Jan Haji \u02c7c , Jan Haji \u02c7c jr . , Linh H ` a M \u02dcy , Na - Rae Han , Kim Harris , Dag Haug , Barbora Hladk \u00b4 a , Jaroslava Hlav \u00b4 a\u02c7cov\u00b4a , Florinel Hociung , Petter Hohle , Jena Hwang , Radu Ion , Elena Irimia , O .l\u00b4aj\u00b4\u0131d\u00b4e", "entities": [[21, 22, "TaskName", "Jesus"]]}, {"text": "Ishola , Tom \u00b4 a\u02c7s Jel \u00b4 \u0131nek , Anders Johannsen , Fredrik J\u00f8rgensen , H \u00a8uner Kas \u00b8\u0131kara , Sylvain Kahane , Hiroshi Kanayama , Jenna Kanerva , Boris Katz , Tolga Kayadelen , Jessica Kenney , V \u00b4 aclava Kettnerov \u00b4 a , Jesse Kirchner , Kamil Kopacewicz , Natalia Kotsyba , Simon Krek , Sookyoung Kwak , Veronika Laippala , Lorenzo Lambertino , Lucia Lam , Ta - tiana Lando , Septina Dian Larasati , Alexei Lavrentiev , John Lee , Ph \u02c6o\u02c6eng L \u02c6e H ` \u02c6ong , Alessandro Lenci , Saran Lertpradit , Herman Leung , Cheuk Ying Li , Josie Li , Keying Li , KyungTae Lim , Nikola Ljube \u02c7si\u00b4c , Olga Loginova , Olga Lyashevskaya , Teresa Lynn , Vivien Macketanz , Aibek Makazhanov , Michael Mandl , Christopher Manning , Ruli Manurung , C \u02d8at\u02d8alina M \u02d8ar\u02d8anduc , David Mare \u02c7cek , Katrin Marheinecke , H \u00b4 ector Mart \u00b4 \u0131nez Alonso , Andr \u00b4 e Martins , Jan Ma \u02c7sek , Yuji Matsumoto , Ryan McDonald , Gustavo Mendonc \u00b8a ,", "entities": []}, {"text": "Niko Miekka , Margarita Misirpashayeva , Anna Missil \u00a8a , C \u02d8at\u02d8alin", "entities": []}, {"text": "Mititelu , Yusuke Miyao , Simonetta Montemagni , Amir More , Laura Moreno Romero , Keiko Sophie Mori , Shinsuke Mori , Bjartur Mortensen , Bohdan Moskalevskyi , Kadri Muischnek , Yugo Murawaki , Kaili M \u00a8u\u00a8urisep , Pinkey Nainwani , Juan Ignacio Navarro Hor \u02dcniacek , Anna Nedoluzhko , Gunta Ne\u02c7spore - B \u00aferzkalne , L \u02c6o\u02c6eng Nguy\u02dc \u02c6en Thi . , Huy ` \u02c6en Nguy\u02dc \u02c6en Thi .Minh , Vitaly Nikolaev , Rattima Nitisaroj , Hanna Nurmi , Stina Ojala , Ad \u00b4 edayo .`o Ol\u00b4u`okun , Mai Omura , Petya Osenova , Robert \u00a8Ostling , Lilja \u00d8vrelid , Niko Partanen , Elena Pascual , Marco Passarotti , Agnieszka Patejuk , Guilherme PaulinoPassos , Siyao Peng , Cenel - Augusto Perez , Guy Perrier , Slav Petrov , Jussi Piitulainen , Emily Pitler , Barbara Plank , Thierry Poibeau , Martin Popel , Lauma Pretkalnin \u00b8a , Sophie Pr \u00b4 evost , Prokopis Prokopidis , Adam Przepi \u00b4 orkowski , Tiina Puolakainen , Sampo Pyysalo , Andriela R \u00a8a\u00a8abis , Alexandre Rademaker , Loganathan Ramasamy , Taraka Rama , Carlos Ramisch , Vinit Ravishankar , Livy Real , Siva Reddy , Georg Rehm , Michael Rie\u00dfler , Larissa Rinaldi , Laura Rituma , Luisa Rocha , Mykhailo Romanenko , Rudolf Rosa , Davide Rovati , Valentin Ros , ca , Olga Rudina , Jack Rueter , Shoval Sadde , Beno \u02c6\u0131t Sagot , Shadi Saleh , Tanja Samard \u02c7zi\u00b4c , Stephanie Samson , Manuela Sanguinetti , Baiba Saul\u00af\u0131te , Yanin Sawanakunanon , Nathan Schneider , Sebastian Schuster , Djam \u00b4 e Seddah , Wolfgang Seeker , Mojgan Seraji , Mo Shen , Atsuko Shimada , Muh Shohibussirri , Dmitry Sichinava , Natalia Silveira , Maria Simi , Radu Simionescu , Katalin Simk \u00b4 o , M \u00b4 aria", "entities": [[157, 158, "MethodName", "Adam"], [299, 300, "MethodName", "aria"]]}, {"text": "\u02c7Simkov \u00b4 a , Kiril Simov , Aaron Smith , Isabela Soares - Bastos , Carolyn Spadine , Antonio Stella , Milan Straka , Jana Strnadov \u00b4 a , Alane Suhr , Umut Sulubacak , Zsolt Sz \u00b4 ant\u00b4o , Dima Taji , Yuta Takahashi , Takaaki Tanaka , Isabelle Tellier , Trond Trosterud , Anna Trukhina , Reut Tsarfaty , Francis Tyers , Sumire Uematsu , Zde \u02c7nka Ure \u02c7sov\u00b4a , Larraitz Uria , Hans Uszkoreit , Sowmya Vajjala , Daniel van Niekerk , Gertjan van Noord , Viktor Varga , Eric Villemonte de la Clergerie , Veronika Vincze , Lars Wallin , Jing Xian Wang , Jonathan North Washington , Seyi Williams ,", "entities": []}, {"text": "Mats Wir \u00b4 en , Tsegay Woldemariam , Tak - sum Wong , Chunxiao Yan , Marat M. Yavrumyan , Zhuoran Yu , Zden \u02c7ek\u02c7Zabokrtsk", "entities": []}, {"text": "\u00b4 y , Amir Zeldes , Daniel Zeman , Manying Zhang , and Hanzhi Zhu . 2018 .", "entities": []}, {"text": "Universal dependencies 2.3 .", "entities": [[0, 2, "DatasetName", "Universal dependencies"]]}, {"text": "LINDAT / CLARIAH - CZ digital library at the Institute of Formal and Applied Linguistics ( \u00b4 UFAL ) , Faculty of Mathematics and Physics , Charles University .", "entities": []}, {"text": "2313Robert \u00a8Ostling and J \u00a8org Tiedemann .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Continuous multilinguality with language vectors .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers , pages 644\u2013649 .", "entities": []}, {"text": "Jonas Pfeiffer , Ivan Vuli \u00b4 c , Iryna Gurevych , and Sebastian Ruder .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Mad - x : An adapter - based framework for multi - task cross - lingual transfer .", "entities": [[13, 17, "TaskName", "cross - lingual transfer"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "Telmo Pires , Eva Schlinger , and Dan Garrette .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "How multilingual is multilingual BERT ?", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4996 \u2013 5001 .", "entities": []}, {"text": "Emmanouil Antonios Platanios , Mrinmaya Sachan , Graham Neubig , and Tom Mitchell .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Contextual parameter generation for universal neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 425\u2013435 .", "entities": []}, {"text": "Edoardo Maria Ponti , Helen O\u2019Horan , Yevgeni Berzak , Ivan Vuli \u00b4 c , Roi Reichart , Thierry Poibeau , Ekaterina Shutova , and Anna Korhonen .", "entities": [[4, 5, "DatasetName", "Helen"]]}, {"text": "2019 .", "entities": []}, {"text": "Modeling language variation and universals : A survey on typological linguistics for natural language processing .", "entities": []}, {"text": "Computational Linguistics , 45(3):559\u2013601 .", "entities": []}, {"text": "Martin Popel , Zden \u02c7ek\u02c7Zabokrtsk \u00b4 y , and Martin V ojtek . 2017 .", "entities": []}, {"text": "Udapi : Universal API for Universal Dependencies .", "entities": [[5, 7, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies ( UDW 2017 ) , pages 96\u2013101 .", "entities": [[8, 10, "DatasetName", "Universal Dependencies"]]}, {"text": "Sylvestre - Alvise Rebuf\ufb01 , Hakan Bilen , and Andrea Vedaldi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Ef\ufb01cient parametrization of multidomain deep neural networks .", "entities": []}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8119\u20138127 .", "entities": []}, {"text": "Manon Scholivet , Franck Dary , Alexis Nasr , Benoit Favre , and Carlos Ramisch .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Typological features for multilingual delexicalised dependency parsing .", "entities": [[5, 7, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3919\u20133930 .", "entities": []}, {"text": "Aaron Smith , Bernd Bohnet , Miryam de Lhoneux , Joakim Nivre , Yan Shao , and Sara Stymne .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "82 treebanks , 34 models : Universal Dependency parsing with multi - treebank models .", "entities": [[7, 9, "TaskName", "Dependency parsing"]]}, {"text": "In Proceedings of the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies , pages 113\u2013123 .", "entities": [[15, 17, "DatasetName", "Universal Dependencies"]]}, {"text": "Asa Cooper Stickland and Iain Murray .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bert and pals : Projected attention layers for ef\ufb01cient adaptation in multi - task learning .", "entities": [[5, 7, "HyperparameterName", "attention layers"], [11, 15, "TaskName", "multi - task learning"]]}, {"text": "In International Conference on Machine Learning , pages 5986\u20135995.Milan Straka .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "UDPipe 2.0 prototype at CoNLL 2018 UD shared task .", "entities": [[6, 7, "DatasetName", "UD"]]}, {"text": "In Proceedings of the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies , pages 197\u2013207 .", "entities": [[15, 17, "DatasetName", "Universal Dependencies"]]}, {"text": "Oscar T \u00a8ackstr \u00a8om , Ryan McDonald , and Joakim Nivre .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Target language adaptation of discriminative transfer parsers .", "entities": []}, {"text": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1061\u20131071 .", "entities": []}, {"text": "Oscar T \u00a8ackstr \u00a8om , Ryan McDonald , and Jakob Uszkoreit .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Cross - lingual word clusters for direct transfer of linguistic structure .", "entities": []}, {"text": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 477\u2013487 .", "entities": []}, {"text": "J\u00a8org Tiedemann .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Cross - lingual dependency parsing with Universal Dependencies and predicted PoS labels .", "entities": [[3, 5, "TaskName", "dependency parsing"], [6, 8, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proceedings of the Third International Conference on Dependency Linguistics ( Depling 2015 ) , pages 340\u2013349 .", "entities": []}, {"text": "Ke Tran and Arianna Bisazza .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Zero - shot dependency parsing with pre - trained multilingual sentence representations .", "entities": [[3, 5, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 2nd Workshop on Deep Learning Approaches for LowResource NLP ( DeepLo 2019 ) , pages 281\u2013288 .", "entities": []}, {"text": "David Vilares , Carlos G \u00b4 omez - Rodr \u00b4 \u0131guez , and Miguel A. Alonso . 2016 .", "entities": []}, {"text": "One model , two languages : training bilingual parsers with harmonized treebanks .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 425\u2013431 .", "entities": []}, {"text": "Shijie Wu and Mark Dredze .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Beto , bentz , becas : The surprising cross - lingual effectiveness of BERT .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 833\u2013844 .", "entities": []}, {"text": "Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , et al . 2016 .", "entities": []}, {"text": "Google \u2019s neural machine translation system : Bridging the gap between human and machine translation .", "entities": [[0, 1, "DatasetName", "Google"], [3, 5, "TaskName", "machine translation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "ArXiv preprint .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Daniel Zeman and Philip Resnik . 2008 .", "entities": []}, {"text": "Crosslanguage parser adaptation between related languages .", "entities": []}, {"text": "In Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages .", "entities": []}, {"text": "Yuan Zhang and Regina Barzilay .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Hierarchical low - rank tensors for multilingual transfer parsing .", "entities": []}, {"text": "InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1857\u20131867 .", "entities": []}, {"text": "2314Hyper - Parameter Value Dependency tag dimension 256 Dependency arc dimension 768 Optimizer Adam \f 1 ; \f 2 0.9 , 0.99 Weight decay 0.01 Label smoothing 0.03 Dropout 0.5 BERT dropout 0.2", "entities": [[12, 13, "HyperparameterName", "Optimizer"], [13, 14, "MethodName", "Adam"], [22, 24, "MethodName", "Weight decay"], [25, 27, "MethodName", "Label smoothing"], [28, 29, "MethodName", "Dropout"], [30, 31, "MethodName", "BERT"]]}, {"text": "Mask probability 0.2 Batch size 32 Epochs 80 Base learning rate 1e\u00003 BERT learning rate 5e\u00005 LR warm up ratio", "entities": [[3, 5, "HyperparameterName", "Batch size"], [9, 11, "HyperparameterName", "learning rate"], [12, 13, "MethodName", "BERT"], [13, 15, "HyperparameterName", "learning rate"]]}, {"text": "1 = 80 Adapter size 256 Language embedding size 32 Table 3 : Hyper - parameter setting A Appendix A.1 Experimental Details Implementation UDapter \u2019s implementation is based on UDify ( Kondratyuk and Straka , 2019 ) .", "entities": [[3, 4, "MethodName", "Adapter"]]}, {"text": "We use the same hyper - parameters setting optimized in UDify without applying a new hyperparameter search .", "entities": []}, {"text": "Together with the additional adapter size andlanguage embedding size that are picked manually by parsing accuracy , hyperparameters are given in Table 3 .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "Note that , to give a fair chance to the adapter - only baseline ( see x4 ) , we used 1024 as adapter size unlike that of the \ufb01nal UDapter ( 256 ) .", "entities": []}, {"text": "For fair comparison , mono - udify and multi - udify are re - trained on the concatenation of 13 high - resource languages for only dependency parsing .", "entities": [[26, 28, "TaskName", "dependency parsing"]]}, {"text": "Besides , we did not use a layer attention for both our model and the baselines .", "entities": []}, {"text": "Training Time and Model size Comparing to UDify , UDapter has a similar training time .", "entities": []}, {"text": "An epoch over the full training set takes approximately 27 and 30 minutes in UDify and UDapter respectively on a Tesla V100 GPU .", "entities": []}, {"text": "In terms of number of trainable parameters , UDify has 191 M total number of parameters whereas UDapter uses 550 M parameters in total , 302 M for adapters ( 32x9.4 M ) and 248 M for biaf\ufb01ne attention ( 32x7.8 M ) , since the parameter generator network ( CPG ) multiplies the tensors with language embedding size ( 32 ) .", "entities": [[13, 16, "HyperparameterName", "number of parameters"]]}, {"text": "Note that for multilingual training , UDapter \u2019s parameter cost depends only on language embedding size regardless of number of languages , therefore it highly scalable with an increasing number of languages for larger experiments .", "entities": []}, {"text": "Finally , monolingual UDifyorig.udify multi - udify udapter udap.-proxy aii * 9.1 8.4 14.3 8.2 ( ar ) akk * 4.4 4.5 8.2 9.1 ( ar ) am * 2.6 2.8 5.9 1.1 ( ar ) be 81.8 80.1 79.3 69.9 ( ru ) bho * ( y ) 35.9 37.2 37.3 35.9 ( hi ) bm * 7.9 8.9 8.1 3.1 ( CTR ) br * 39.0 60.5 58.5 14.3 ( CTR ) bxr * 26.7 26.1 28.9 9.1 ( CTR ) cy 42.7 53.6 54.4 9.8 ( CTR ) fo * 59.0 68.6 69.2 64.1 ( sv ) gsw * 39.7 43.6 45.5 23.7 ( en ) gun * ( y ) 6.0 8.5 8.4 2.1 ( CTR ) hsb * 62.7 53.2 54.2 44.4 ( ru ) kk 63.6 61.9 60.7 45.1 ( tr ) kmr * ( y ) 20.2 11.2 12.1 4.7 ( CTR ) koi * 22.6 20.8 23.1 6.5 ( CTR ) kpv *", "entities": []}, {"text": "( y ) 12.9 12.4 12.5 4.7 ( CTR ) krl * 41.7 49.2 48.4 45.6 ( \ufb01 ) mdf * 19.4 24.7 26.6 8.7 ( CTR )", "entities": []}, {"text": "mr 67.0 46.4 44.4 29.6 ( hi ) myv * ( y ) 16.6 19.1 19.2 6.3 ( CTR ) olo * 33.9 42.1 43.3 41.1 ( \ufb01 ) pcm * ( y ) 31.5 36.1 36.7 5.6 ( CTR ) sa * 19.4 19.4 22.2 15.1 ( hi ) ta(y )", "entities": []}, {"text": "71.4 46.0 46.1 12.3 ( CTR ) te(y ) 83.4 71.2 71.1 23.1 ( CTR )", "entities": []}, {"text": "tl 41.4 62.7 69.5 14.1 ( CTR ) wbp * 6.7 9.6 12.1 4.8 ( CTR ) yo 22.0 41.2 42.7 10.5 ( CTR ) yue * 31.0 30.5 32.8 24.5 ( zh ) avg 34.1 35.3 36.5 20.4 Table 4 : LAS results of UDapter and UDify models ( Kondratyuk and Straka , 2019 ) for all low - resource languages .", "entities": []}, {"text": "\u2018 * \u2019 shows languages not present in mBERT training data .", "entities": [[8, 9, "MethodName", "mBERT"]]}, {"text": "Additionally , ( y)indicates languages where no signi\ufb01cant difference between UDapter and multi - udify by signi\ufb01cance testing .", "entities": []}, {"text": "For udapter - proxy , chosen proxy language is given between brackets .", "entities": []}, {"text": "CTR means centroid language embedding .", "entities": []}, {"text": "models are trained separately so the total number of parameters for 13 languages is 2.5B ( 13x191 M ) .", "entities": [[7, 10, "HyperparameterName", "number of parameters"]]}, {"text": "A.2 Zero - Shot Results Table 4 shows LAS scores on all 30 low - resouce languages for UDapter , original UDify ( Kondratyuk and Straka , 2019 ) , and re - trained \u2018 multiudify \u2019 .", "entities": []}, {"text": "Languages with \u2018 * \u2019 are not included in mBERT training data .", "entities": [[9, 10, "MethodName", "mBERT"]]}, {"text": "Note that original UDify is trained on all available UD treebanks from 75 languages .", "entities": [[9, 10, "DatasetName", "UD"]]}, {"text": "For the zero - shot languages , we obtained original UDify scores by running the pre - trained model .", "entities": []}, {"text": "A.3 Language Details Details of training and zero - shot languages such as language code , data size ( number of sentences ) , and family are given in Table 5 and Table 6 .", "entities": []}, {"text": "2315Language Code Treebank Family Word Order Train Test Arabic ar PADT Afro - Asiatic , Semitic VSO 6.1k 680 Basque eu BDT Basque SOV 5.4k 1799 Chinese zh GSD Sino - Tibetan SVO 4.0k 500 English en EWT IE , Germanic SVO 12.5k 2077", "entities": []}, {"text": "Finnish \ufb01 TDT Uralic , Finnic SVO 12.2k 1555 Hebrew he HTB Afro - Asiatic , Semitic SVO 5.2k 491", "entities": []}, {"text": "Hindi hi HDTB IE , Indic SOV 13.3k 1684 Italian it ISDT IE , Romance SVO 13.1k 482 Japanese ja GSD Japanese SOV 7.1k 551 Korean ko GSD Korean SOV 4.4k 989 Russian ru SynTagRus IE , Slavic SVO 15k * 6491 Swedish sv Talbanken IE , Germanic SVO 4.3k 1219 Turkish tr IMST Turkic , Southwestern SOV 3.7k 975 Table 5 : Training languages that are from UD 2.3 ( Nivre et al . , 2018 ) with the details including treebank name , family , word order and data size of training and test sets .", "entities": [[68, 69, "DatasetName", "UD"]]}, {"text": "Language Code Treebank(s )", "entities": []}, {"text": "Family Test Akkadian akk PISANDUB Afro - Asiatic ,", "entities": []}, {"text": "Semitic 1074", "entities": []}, {"text": "Amharic am ATT Afro - Asiatic , Semitic 101 Assyrian aii AS Afro - Asiatic , Semitic 57 Bambara bm CRB Mande", "entities": []}, {"text": "1026 Belarusian be HSE IE , Slavic 253 Bhojpuri bho BHTB IE , Indic 254 Breton br KEB IE ,", "entities": []}, {"text": "Celtic 888 Buryat bxr BDT Mongolic", "entities": []}, {"text": "908 Cantonese yue HK Sino - Tibetan 1004 Erzya myv JR Uralic , Mordvin 1550", "entities": []}, {"text": "Faroese fo OFT IE , Germanic 1207 Karelian krl KKPP Uralic , Finnic 228 Kazakh kk KTB Turkic , Northwestern 1047 Komi Permyak koi UH Uralic , Permic 49 Komi Zyrian kpv LATTICE , IKDP Uralic , Permic 210", "entities": []}, {"text": "Kurmanji kmr MG IE , Iranian 734 Livvi olo KKPP Uralic , Finnic 106 Marathi mr UFAL IE , Indic 47 Mbya Guarani gun THOMAS , DOOLEY Tupian 98 Moksha mdf JR Uralic , Mordvin 21 Naija pcm NSC Creole 948", "entities": []}, {"text": "Sanskrit sa UFAL IE , Indic 230 Swiss G. gsw UZH IE , Germanic 100 Tagalog tl TRG Austronesian , Central Philippine 55 Tamil ta TTB Dravidian , Southern 120 Telugu te MTG Dravidian , South Central 146 Upper Sorbian hsb UFAL IE , Slavic 623 Warlpiri wbp UFAL Pama - Nyungan 54", "entities": []}, {"text": "Welsh cy CCG IE , Celtic 956 Yoruba yo YTB Niger - Congo , Defoid 100 Table 6 : Zero - shot languages are selected from UD 2.5 to increase the number of languages in the experiments .", "entities": [[26, 27, "DatasetName", "UD"]]}, {"text": "Language details include treebank name , family and test size for zero - shot experiments .", "entities": []}]