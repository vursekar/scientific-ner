[{"text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 10285\u201310299 November 7\u201311 , 2021 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2021 Association for Computational Linguistics10285Discretized Integrated Gradients for Explaining Language Models Soumya Sanyal University of Southern California soumyasa@usc.eduXiang Ren University of Southern California xiangren@usc.edu", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "As a prominent attribution - based explanation algorithm , Integrated Gradients ( IG ) is widely adopted due to its desirable explanation axioms and the ease of gradient computation .", "entities": []}, {"text": "It measures feature importance by averaging the model \u2019s output gradient interpolated along a straight - line path in the input data space .", "entities": [[2, 4, "TaskName", "feature importance"]]}, {"text": "However , such straight - line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space .", "entities": []}, {"text": "This questions the faithfulness of the gradients computed at the interpolated points and consequently , the quality of the generated explanations .", "entities": []}, {"text": "Here we propose Discretized Integrated Gradients ( DIG ) , which allows effective attribution along non - linear interpolation paths .", "entities": []}, {"text": "We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space , yielding more faithful gradient computation .", "entities": []}, {"text": "We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classi\ufb01cation datasets .", "entities": []}, {"text": "We provide the source code of DIG to encourage reproducible research1 .", "entities": []}, {"text": "1 Introduction In the past few years , natural language processing has seen tremendous progress , largely due to strong performances yielded by pre - trained language models ( Devlin et al . , 2019 ; Radford et al . , 2019 ; Brown et al . , 2020 ) .", "entities": []}, {"text": "But even with this impressive performance , it can still be dif\ufb01cult to understand the underlying reasoning for the preferred predictions leading to distrust among end - users ( Lipton , 2018 ) .", "entities": []}, {"text": "Hence , improving model interpretability has become a central focus in the community with an increasing effort in developing methods that can explain model behaviors ( Ribeiro et al . , 2016 ; Binder et al . , 2016 ; Li et al . , 2016 ; Sundararajan et al . , 1https://github.com/INK-USC/DIG decentniceclubokaysmarthomedull < pad > Input : the movie was good !", "entities": []}, {"text": "goodFigure 1 : An illustration of paths used in IG and DIG .", "entities": []}, {"text": "IG uses a straight line interpolation with points as depicted by green squares .", "entities": []}, {"text": "In contrast , DIG uses a nonlinear path ( shown in blue ) with interpolation points ( red stars ) lying close to words in the embedding space .", "entities": []}, {"text": "2017 ; Shrikumar et al . , 2017 ; Lundberg and Lee , 2017 ; Murdoch et al . , 2018 ) .", "entities": []}, {"text": "Explanations in NLP are typically represented at a word - level or phrase - level by quantifying the contributions of the words or phrases to the model \u2019s prediction by a scalar score .", "entities": []}, {"text": "These explanation methods are commonly referred as attributionbased methods ( Murdoch et al . , 2018 ; Ancona et al . , 2018 ) .", "entities": []}, {"text": "Integrated Gradients ( IG ) ( Sundararajan et al . , 2017 ) is a prominent attribution - based explanation method used due to the many desirable explanation axioms and ease of gradient computation .", "entities": []}, {"text": "It computes the partial derivatives of the model output with respect to each input feature as the features are interpolated along a straight - line path from the given input to a baseline value .", "entities": []}, {"text": "For example , say we want to compute the attribution for the word \u201c good \u201d in the sentence \u201c the movie was good ! \u201d", "entities": []}, {"text": "using IG .", "entities": []}, {"text": "The straight - line interpolation path used by IG is depicted in green in Figure 1 .", "entities": []}, {"text": "Here , the baseline word is de\ufb01ned as the \u201c < pad > \u201d embedding and the green squares are the intermediate interpolation points in the embedding space .", "entities": []}, {"text": "While this method can be used for attributing inputs in both continuous ( e.g. , image , audio , etc . )", "entities": []}, {"text": "and discrete ( e.g. , text , molecules , etc . ) domains ( Sundararajan et al . , 2017 ) , their usage in the dis-", "entities": []}, {"text": "10286crete domain has some limitations .", "entities": []}, {"text": "Since the interpolation is done along a straight - line path joining the input word embedding and the baseline embedding ( \u201c < pad > \u201d in Figure 1 ) , the interpolated points are not necessarily representative of the discrete word embedding distribution .", "entities": []}, {"text": "Speci\ufb01cally , let a dummy word embedding space be de\ufb01ned by the words represented by black dots in Figure 1 .", "entities": []}, {"text": "Then we can see that some of the green squares can be very far - off from any original word in the embedding space .", "entities": []}, {"text": "Since the underlying language model is trained to effectively work with the speci\ufb01c word embedding space as input , using these out - of - distribution green interpolated samples as intermediate inputs to calculate gradients can lead to sub - optimal attributions .", "entities": []}, {"text": "To mitigate these limitations , we propose a Discretized integrated gradients ( DIG ) formulation by relaxing the constraints of searching for interpolation points along a straight - line path .", "entities": []}, {"text": "Relaxing this linear - path constraint leads to a new constraint on the interpolation paths in DIG that points along the path should be monotonically situated between the input word embedding and the baseline embedding .", "entities": []}, {"text": "Hence , in DIG , our main objective is to monotonically interpolate between the input word embedding and baseline such that the intermediate points are close to real data samples .", "entities": []}, {"text": "This would ensure that the interpolated points are more representative of the word embedding distribution , enabling more faithful model gradient computations .", "entities": []}, {"text": "To this end , we propose two interpolation strategies that search for an optimal anchor word embedding in the real data space and then modify it such that it lies monotonically between the input word and baseline ( see Fig . 1 for an illustration ) .", "entities": []}, {"text": "We apply DIG using our proposed interpolation algorithms to generate attributions for three pre - trained language models - BERT ( Devlin et al . , 2019 ) , DistilBERT ( Sanh et al . , 2020 ) , and RoBERTa ( Liu et al . , 2019 ) , each \ufb01ne - tuned separately on three sentiment classi\ufb01cation datasets - SST2 ( Socher et al . , 2013 ) , IMDB ( Maas et al . , 2011 ) , and Rotten Tomatoes ( Pang and Lee , 2005 ) .", "entities": [[19, 20, "MethodName", "BERT"], [29, 30, "MethodName", "DistilBERT"], [40, 41, "MethodName", "RoBERTa"], [61, 62, "DatasetName", "SST2"], [71, 72, "DatasetName", "IMDB"]]}, {"text": "We \ufb01nd that our proposed interpolation strategies achieve a superior performance compared to integrated gradients and other gradient - based baselines on eight out of the nine settings across different metrics .", "entities": []}, {"text": "Further , we also observe that on average , end - users \ufb01nd explanations provided by DIG to be more plausible justi\ufb01cations of model behaviorthan the explanations from other baselines .", "entities": []}, {"text": "2 Method In this section , we \ufb01rst describe our proposed Discretized integrated gradients ( DIG ) and the desirable explanation axioms satis\ufb01ed by it .", "entities": []}, {"text": "Then we describe an interpolation algorithm that leverages our DIG in discrete textual domains .", "entities": []}, {"text": "Please refer to Appendix A for a brief introduction of the attribution - based explanation setup and the integrated gradients method .", "entities": []}, {"text": "2.1 Discretized integrated gradients Below , we de\ufb01ne our DIG formulation that allows interpolations along non - linear paths : DIGi(x )", "entities": []}, {"text": "= Zxi xk i = x0 i@F\u0000", "entities": []}, {"text": "xk\u0001 @xidxk i : ( 1 ) Here , xk irefers to the ithdimension of the kth interpolated point between input xand baseline", "entities": [[16, 17, "DatasetName", "kth"]]}, {"text": "x0 andFis a neural network .", "entities": []}, {"text": "The only constraint on", "entities": []}, {"text": "xk i \u2019s is that each interpolation should be monotonic betweenxiandx0", "entities": []}, {"text": "i , i.e. ,8j;k2f1;:::;mg ; j < k , x0 i\u0014xj i\u0014xk i\u0014xiifx0 i\u0014xi ; x0 i\u0015xj i\u0015xk i\u0015xiotherwise:(2 )", "entities": []}, {"text": "Heremis the total number of steps for interpolation .", "entities": []}, {"text": "This constraint is essential because it allows approximating the integral in Eq . 1 using Riemann summation2which requires monotonic paths .", "entities": []}, {"text": "We note that the interpolation points used by IG naturally satisfy this constraint since they lie along a straight line joining xandx0 .", "entities": []}, {"text": "The key distinction of our formulation from IG is that DIG is agnostic of any \ufb01xed step size parameter \u000b and thus allows non - linear interpolation paths in the embedding space .", "entities": [[16, 18, "HyperparameterName", "step size"]]}, {"text": "The integral approximation of DIG is de\ufb01ned as follows : DIGapprox i(x )", "entities": []}, {"text": "= \u0006m k=1@F\u0000 xk\u0001 @xi\u0002\u0000 xk+1 i\u0000xk i\u0001 ; ( 3 ) wheremis the total number of steps considered for the approximation .", "entities": []}, {"text": "2.2 Axioms satis\ufb01ed by DIG As described in prior works ( Sundararajan et al . , 2017 ; Shrikumar et al . , 2017 ) , a good explanation 2https://en.wikipedia.org/wiki/ Riemann_sum", "entities": []}, {"text": "10287 ww2w6w1w5w3w4w\u2019c(a ) DIG - G REEDY ww2", "entities": []}, {"text": "[ 7]w6 [ 8]w1 [ 5]w5 [ 12]w3 [ 10]w4 [ 20]w\u2019c ( b ) DIG - M AXCOUNT", "entities": []}, {"text": "Figure 2 : Overview of paths used in DIG and IG .", "entities": []}, {"text": "The gray region is the neighborhood of w. Green line depicts the straight - line path used by IG .", "entities": []}, {"text": "Left :", "entities": []}, {"text": "In DIG - G REEDY , we \ufb01rst monotonize each word in the neighborhood ( red arrow ) and the word closest to its corresponding monotonic point is selected as the anchor ( w5since the red arrow ofw5has the smallest magnitude ) .", "entities": []}, {"text": "Right :", "entities": []}, {"text": "In DIG - M AXCOUNT we select the word with the highest number of monotonic dimensions ( count shown in [ :] ) as the anchor word ( w4 ) , followed by changing the non - monotonic dimensions of w4(red arrow to c ) .", "entities": []}, {"text": "Repeating this iteratively gives the non - linear blue path for DIG with the red stars as interpolation points .", "entities": []}, {"text": "Please refer to Section 2.1 for more details .", "entities": []}, {"text": "Figure best viewed in color .", "entities": []}, {"text": "algorithm should satisfy certain desirable axioms which justify the use of the algorithm for generating model explanations .", "entities": []}, {"text": "Similar to IG , DIG also satis\ufb01es many such desirable axioms .", "entities": []}, {"text": "First , DIG satis\ufb01es Implementation Invariance which states that attributions should be identical for two functionally equivalent models .", "entities": []}, {"text": "Two models are functionally equivalent if they have the same output for the same input , irrespective of any differences in the model \u2019s internal implementation design .", "entities": []}, {"text": "Further , DIG satis\ufb01es Completeness which states that the sum of the attributions for an input should add up to the difference between the output of the model at the input and the baseline , i.e. ,P iDIGi(x )", "entities": []}, {"text": "= F(x)\u0000F(x0 ) .", "entities": []}, {"text": "This ensures that if F(x0)\u00190then the output is completely attributed to the inputs .", "entities": []}, {"text": "Thirdly , DIG satis\ufb01es Sensitivity which states that attributions of inputs should be zero if the model does not depend ( mathematically ) on the input .", "entities": []}, {"text": "Please refer to Appendix B for further comparisons of DIG with IG . 2.3 Interpolation algorithm Here , we describe our proposed interpolation algorithm that searches for intermediate interpolation points between the input word embedding and the baseline embedding .", "entities": []}, {"text": "Once we have the desired interpolation points , we can use Equation 3 to compute the word attributions similar to the IG algorithm .", "entities": []}, {"text": "Please refer to Section A.2 for more details about application of IG to text .", "entities": []}, {"text": "Design Consideration .", "entities": []}, {"text": "First , we discuss the key design considerations we need to consider of our interpolation algorithm .", "entities": []}, {"text": "Clearly , our interpolation points need to satisfy the monotonicity constraints de\ufb01ned in Equation 2 so that we can use the Riemann sum approximation of DIG .", "entities": []}, {"text": "Hence , we need to ensure that every intermediate point lies in a monotonic path .", "entities": []}, {"text": "Also , the interpolation points should lie close to the original words in the embedding space to ensure that the model gradients faithfully de\ufb01ne the model behavior .", "entities": []}, {"text": "Now , we de\ufb01ne the notion of closeness for our speci\ufb01c use - case of explaining textual models .", "entities": []}, {"text": "To calculate how far the interpolated words are from some true word embedding in the vocabulary , we can compute the distance of the interpolated point from the nearest word in the vocabulary .", "entities": []}, {"text": "We de\ufb01ne this as the word - approximation error ( WAE ) .", "entities": []}, {"text": "More speci\ufb01cally , if wkdenotes the kthinterpolation point for a word w , then its word - approximation error along the interpolated path is de\ufb01ned as : WAEw=1 mmX k=1min x2Vdist(wk\u0000x);(4 )", "entities": []}, {"text": "whereVis the embedding matrix of all the words in the vocabulary .", "entities": []}, {"text": "WAE of a sentence is the average WAE of all words in the sentence .", "entities": []}, {"text": "Intuitively , minimizing WAE will ensure that the interpolated points are close to some real word embedding in the vocabulary which in turn ensures that output gradients of Fare not computed for some out - ofdistribution unseen embedding points .", "entities": []}, {"text": "10288We observe that to minimize WAE without the monotonic constraints de\ufb01ned in Section 2.1 , one can de\ufb01ne some heuristic to search for interpolation points that belong to the set V(i.e . ,", "entities": []}, {"text": "select words from the vocabulary as interpolation points ) , leading to a zero WAE .", "entities": []}, {"text": "Motivated by this , for a given input word embedding , we \ufb01rst search for an anchor word from the vocabulary that can be considered as the next interpolation point .", "entities": []}, {"text": "Since the anchor point need not be monotonic w.r.t .", "entities": []}, {"text": "the given input , we then optimally perturb the dimensions of the anchor word so that they satisfy the monotonicity constraints in Equation 2 .", "entities": []}, {"text": "This perturbed point becomes our \ufb01rst interpolation .", "entities": []}, {"text": "For subsequent interpolation points , we repeat the above steps using the previous anchor and perturbed points .", "entities": []}, {"text": "Formally , we break our interpolation algorithm into two parts : ( i)ANCHOR SEARCH :", "entities": []}, {"text": "In this step , given the initial word embedding w , we search for an anchor word embedding a2V. ( ii)MONOTONIZE : This step takes the anchor embeddingaand modi\ufb01es its dimensions to create a new embedding csuch that all dimensions ofcare monotonic between the input w and the baseline w0 .", "entities": []}, {"text": "Overall , given an initial input word embedding w and a baseline embedding w0 , our interpolation algorithm interpolates points from wtow0(which is in decreasing order of kin Eq . 3 ) .", "entities": []}, {"text": "It proceeds by calling ANCHOR SEARCH onwto get an anchor word a.", "entities": []}, {"text": "Then , it applies MONOTONIZE onato get the monotonic embedding c.", "entities": []}, {"text": "This is our \ufb01rst interpolated point ( in reverse order ) , i.e. ,c = wm\u00001 .", "entities": []}, {"text": "Now , theabecomes the new w for the next iteration and the process continues tillmsteps .", "entities": []}, {"text": "Next , we describe in detail our speci\ufb01c formulations of the MONOTONIZE and ANCHOR SEARCH algorithms .", "entities": []}, {"text": "MONOTONIZE :", "entities": []}, {"text": "In this step , given an anchor word embeddinga , we modify the non - monotonic dimensions of asuch that they become monotonic w.r.t.wandw0 .", "entities": []}, {"text": "The monotonic dimensions of a vectorais given by : Ma = fjjw0 j\u0014aj\u0014wj;j2f1;:::;Dgg [ fjjw0 j\u0015aj\u0015wj;j2f1;:::;Dgg ; whereDis the word embedding dimension .", "entities": [[19, 22, "HyperparameterName", "word embedding dimension"]]}, {"text": "The number of monotonic dimensions is given bythe size of the set de\ufb01ned as jMaj .", "entities": []}, {"text": "Thus , the non - monotonic dimensions Mais the set complement of the monotonic dimensions , i.e. , Ma= f1;:::;Dg\u0000Ma , where the subtraction is the setdiff operation .", "entities": []}, {"text": "Let the \ufb01nal monotonic vector be c. We de\ufb01ne the MONOTONIZE operations as follows : c[Ma ] a[Ma ] ; c[Ma ] w[Ma]\u00001 m\u0002(w[Ma]\u0000w0[Ma ] ) ; wheremis the total number of interpolation points we want to select in the path .", "entities": []}, {"text": "It can be easily seen thatcis monotonic w.r.t .", "entities": []}, {"text": "wandw0according to the de\ufb01nition in Equation 2 . ANCHOR SEARCH :", "entities": []}, {"text": "First , we preprocess the word embedding in Vto \ufb01nd the top- Knearest neighbor for each word .", "entities": []}, {"text": "We consider this neighborhood for candidate anchor selection .", "entities": []}, {"text": "Let us denote the Kneighbors for a word wbyKNNV(w ) .", "entities": []}, {"text": "We de\ufb01ne two heuristics to search for the next anchor word : GREEDY and M AXCOUNT .", "entities": []}, {"text": "In the GREEDY heuristic , we \ufb01rst compute the monotonic embedding corresponding to each word in the neighborhood KNNV(w)using the MONOTONIZE step .", "entities": []}, {"text": "Then , we select the anchor word a that is closest to its corresponding monotonic embedding obtained from the above step .", "entities": []}, {"text": "This can be thought of as minimizing the WAE metric for a single interpolated word .", "entities": []}, {"text": "The key intuition here is to locally optimize for smallest perturbations at each iterative selection step .", "entities": []}, {"text": "This heuristic is depicted in Figure 2a and the algorithm is presented in Algorithm 1 in Appendix .", "entities": []}, {"text": "In the MAXCOUNT heuristic , we select the anchoraas the word in KNNV(w)with the highest number of monotonic dimensions .", "entities": []}, {"text": "Precisely , the anchor is given by : a= arg max a02KNN V(w)jMa0j : The intuition of this heuristic is that the vector with highest number of monotonic dimensions would require the minimum number of dimensions being perturbed in the MONOTONIZE step and hence , would be close to a word in the vocabulary .", "entities": []}, {"text": "This heuristic is depicted in Figure 2b and the algorithm is presented in Algorithm 2 in Appendix .", "entities": []}, {"text": "3 Experimental Setup In this section , we describe the datasets and models used for evaluating our proposed algorithm .", "entities": []}, {"text": "10289MethodDistilBERT RoBERTa BERT LO#Comp\"Suff#WAE#LO#Comp\"Suff#WAE#LO#Comp\"Suff#WAE # Grad*Inp -0.402 0.112 0.375 - -0.318 0.085 0.398 - -0.502 0.168 0.366 DeepLIFT -0.196 0.053 0.489 - -0.300 0.078 0.432 - -0.175 0.063 0.470 GradShap -0.778 0.216 0.308 - -0.523 0.168 0.347 - -0.686 0.225 0.333 IG -0.950 0.248 0.275 0.344 -0.738 0.222 0.250 0.669 -0.670 0.237 0.396 0.302", "entities": [[1, 2, "MethodName", "RoBERTa"], [2, 3, "MethodName", "BERT"]]}, {"text": "DIG - G REEDY -1.222 0.310 0.237 0.229 -0.756 0.218 0.215 0.460 -0.879 0.292 0.374 0.249 DIG - M AXCOUNT -1.259 0.307 0.241 0.227 -0.826 0.227 0.238 0.439 -0.777 0.272 0.377 0.173 Table 1 : Comparison of variants of DIG with baselines on three LMs \ufb01ne - tuned on SST2 dataset .", "entities": [[49, 50, "DatasetName", "SST2"]]}, {"text": "\u2018 - \u2019 denotes that the WAE metric is not computable for that setting .", "entities": []}, {"text": "We observe that DIG outperforms the baselines on all three LMs .", "entities": []}, {"text": "Please refer to Section 4.1 for more details .", "entities": []}, {"text": "MethodDistilBERT RoBERTa BERT LO#Comp\"Suff#WAE#LO#Comp\"Suff#WAE#LO#Comp\"Suff#WAE # Grad*Inp -0.197 0.081 0.212 - -0.195 0.043 0.279 - -0.731 0.102 0.231 DeepLIFT -0.021 -0.009 0.534 - -0.157 0.028 0.340 - -0.335 0.023 0.486 GradShap -0.473 0.185 0.154 - -0.416 0.129 0.196 - -0.853 0.190 0.255 IG -0.446 0.182 0.224 0.379 -0.733 0.226 0.084 0.708 -0.641 0.107 0.295 0.333 DIG - G REEDY -0.878 0.319 0.133 0.256 -0.683 0.198 0.100 0.484 -1.152 0.221 0.240 0.287 DIG - M AXCOUNT -0.795 0.296 0.152 0.255 -0.470 0.121 0.213 0.470 -0.995 0.195 0.245 0.190", "entities": [[1, 2, "MethodName", "RoBERTa"], [2, 3, "MethodName", "BERT"]]}, {"text": "Table 2 : Comparison of variants of DIG with baselines on three LMs \ufb01ne - tuned on IMDB dataset .", "entities": [[17, 18, "DatasetName", "IMDB"]]}, {"text": "We observe that DIG outperforms the baselines on DistilBERT and BERT models .", "entities": [[8, 9, "MethodName", "DistilBERT"], [10, 11, "MethodName", "BERT"]]}, {"text": "Please refer to Section 4.1 for more details .", "entities": []}, {"text": "Datasets .", "entities": []}, {"text": "The SST2 ( Socher et al . , 2013 ) dataset has 6920/872/1821 example sentences in the train / dev / test sets .", "entities": [[1, 2, "DatasetName", "SST2"]]}, {"text": "The task is binary classi\ufb01cation into positive / negative sentiment .", "entities": []}, {"text": "The IMDB ( Maas et al . , 2011 ) dataset has 25000/25000 example reviews in the train / test sets with similar binary labels for positive and negative sentiment .", "entities": [[1, 2, "DatasetName", "IMDB"]]}, {"text": "Similarly , the Rotten Tomatoes ( RT ) ( Pang and Lee , 2005 ) dataset has 5331 positive and 5331 negative review sentences .", "entities": []}, {"text": "We use the processed dataset made available by HuggingFace Dataset library3", "entities": []}, {"text": "( Wolf et al . , 2020b ) .", "entities": []}, {"text": "Language Models .", "entities": []}, {"text": "We use pre - trained BERT ( Devlin et al . , 2019 ) , DistilBERT ( Sanh et al . , 2020 ) , and RoBERTa ( Liu et al . , 2019 ) text classi\ufb01cation models individually \ufb01ne - tuned for SST2 , IMDB , and RT datasets.4The \ufb01ne - tuned checkpoints used are provided by the HuggingFace library ( Wolf et al . , 2020a ) .", "entities": [[5, 6, "MethodName", "BERT"], [15, 16, "MethodName", "DistilBERT"], [26, 27, "MethodName", "RoBERTa"], [43, 44, "DatasetName", "SST2"], [45, 46, "DatasetName", "IMDB"]]}, {"text": "Evaluation Metrics .", "entities": []}, {"text": "Following prior literature , we use the following three automated metrics : \u2022Log - odds ( LO ) score ( Shrikumar et al . , 2017 ) is de\ufb01ned as the average difference of the 3https://github.com/huggingface/ datasets 4Note that the vocabulary matrix Vthat is used in DIG is theword_embeddings layer of the language models in HuggingFace ( Wolf et al . , 2020a).negative logarithmic probabilities on the predicted class before and after masking the top k%words with zero padding .", "entities": []}, {"text": "Lower scores are better .", "entities": []}, {"text": "\u2022Comprehensiveness ( Comp ) score ( DeYoung et al . , 2020 ) is the average difference of the change in predicted class probability before and after removing the top k%words .", "entities": []}, {"text": "Similar to Log - odds , this measures the in\ufb02uence of the top - attributed words on the model \u2019s prediction .", "entities": []}, {"text": "Higher scores are better .", "entities": []}, {"text": "\u2022Suf\ufb01ciency ( Suff ) score ( DeYoung et al . , 2020 ) is de\ufb01ned as the average difference of the change in predicted class probability before and after keeping only the top k%words .", "entities": []}, {"text": "This measures the adequacy of the top k% attributions for model \u2019s prediction .", "entities": []}, {"text": "Please refer to Appendix C for more details about the evaluation metrics .", "entities": []}, {"text": "We use k= 20 % in our experiments .", "entities": []}, {"text": "In Appendix D we further analyze the effect of changing top - k% on the metrics .", "entities": []}, {"text": "Additionally , we use our proposed word - approximation error ( WAE ) metric to compare DIG with IG .", "entities": []}, {"text": "10290MethodDistilBERT RoBERTa BERT LO#Comp\"Suff#WAE#LO#Comp\"Suff#WAE#LO#Comp\"Suff#WAE # Grad*Inp -0.152 0.068 0.315 - -0.158 0.054 0.406 - -0.801 0.204 0.398 DeepLIFT -0.077 0.017 0.372 - -0.150 0.050 0.413 - -0.388 0.096 0.438 GradShap -0.298 0.156 0.270 - -0.290 0.128 0.338 - -0.809 0.235 0.388 IG -0.424 0.208 0.189 0.348 -0.368 0.149 0.317 0.677 -0.789 0.203 0.418 0.305", "entities": [[1, 2, "MethodName", "RoBERTa"], [2, 3, "MethodName", "BERT"]]}, {"text": "DIG - G REEDY -0.501 0.257 0.184 0.329 -0.393 0.148 0.294 0.465 -1.056 0.267 0.416 0.251 DIG - M AXCOUNT", "entities": []}, {"text": "-0.467 0.231 0.190 0.230 -0.361 0.133 0.332 0.444 -0.874 0.237 0.430 0.178 Table 3 : Comparison of variants of DIG with baselines on three LMs \ufb01ne - tuned on Rotten Tomatoes dataset .", "entities": []}, {"text": "We observe that DIG outperforms the baselines on all three LMs .", "entities": []}, {"text": "Please refer to Section 4.1 for more details .", "entities": []}, {"text": "4 Results 4.1 Performance Comparison We compare DIG with four representative gradientbased explanation methods - Gradient*Input ( Grad*Inp ) ( Shrikumar et al . , 2016 ) , DeepLIFT ( Shrikumar et al . , 2017 ) , GradShap ( Lundberg and Lee , 2017 ) , and integrated gradients ( Sundararajan et al . , 2017 ) .", "entities": []}, {"text": "For the IMDB and RT datasets , we randomly sample a subset of 2,000 reviews from the public test sets to compare the different methods , due to computation costs .", "entities": [[2, 3, "DatasetName", "IMDB"]]}, {"text": "For the SST2 dataset , we use the complete set of 1,821 test sentences .", "entities": [[2, 3, "DatasetName", "SST2"]]}, {"text": "The results are shown in Tables 1 , 2 , and 3 for SST2 , IMDB , and Rotten Tomatoes respectively .", "entities": [[13, 14, "DatasetName", "SST2"], [15, 16, "DatasetName", "IMDB"]]}, {"text": "Comparison with baselines .", "entities": []}, {"text": "First , we observe that across the nine different settings we studied ( three language models per dataset ) , DIG consistently outperforms the baselines on eight of the settings .", "entities": []}, {"text": "This is valid for all the metrics .", "entities": []}, {"text": "We also note that the WAE metric is lower for all variants of DIG compared to IG .", "entities": []}, {"text": "This validates that our proposed interpolation strategies for DIG is able to considerably reduce the word - approximation error in the interpolated paths and consistently improving performance on all three explanation evaluation metrics considered .", "entities": []}, {"text": "Comparison between variants of DIG .", "entities": []}, {"text": "Second , we observe that on average , DIG- GREEDY performs better than DIG- MAXCOUNT .", "entities": []}, {"text": "Speci\ufb01cally , we \ufb01nd that DIG- MAXCOUNT does n\u2019t outperform DIG- GREEDY by signi\ufb01cantly large margins on any setting ( while the opposite is true for one setting - RoBERTa \ufb01ne - tuned on IMDB dataset ) .", "entities": [[29, 30, "MethodName", "RoBERTa"], [34, 35, "DatasetName", "IMDB"]]}, {"text": "This could be because the DIG- GREEDY strategy ensures that the monotonic point cis always close to the anchor adue to the locally greedy selection at each step which is not explicitly guaranteed by DIG- MAXCOUNT .", "entities": []}, {"text": "But overall , we do not \ufb01nd any 2.372.352.101.991.551.62Mean RankSST2RT1.001.502.002.503.00GradShapIGDIGFigure 3 : Result of human evaluation on DistilBERT model \ufb01ne - tuned on SST2 dataset and BERT model \ufb01ne - tuned on Rotten Tomatoes dataset .", "entities": [[17, 18, "MethodName", "DistilBERT"], [23, 24, "DatasetName", "SST2"], [26, 27, "MethodName", "BERT"]]}, {"text": "A lower mean rank means higher trustworthy explanation algorithm .", "entities": []}, {"text": "For more details , refer to Section 4.2 speci\ufb01c performance trend between the two proposed variants and plan to study the in\ufb02uence of the embedding distribution in future works .", "entities": []}, {"text": "Analysis .", "entities": []}, {"text": "Finally , though we are able to achieve good reductions in WAE , we note that the WAE for our interpolation algorithms are not close to zero yet .", "entities": []}, {"text": "This leaves some scope to design better interpolation algorithms in future .", "entities": []}, {"text": "Moreover , we \ufb01nd that the average Pearson correlation between log - odds and WAE is 0.32 and the correlation is 0.45 if we consider the eight settings where we outperform IG .", "entities": [[7, 9, "MetricName", "Pearson correlation"]]}, {"text": "We discuss the correlations of all the settings in Appendix E. While this suggests a weak correlation between the two metrics , it is hard to comment if there is a causality between the two .", "entities": []}, {"text": "This is partially because we believe selection of interpolation points should also take the semantics of the perturbed sentences into consideration , which we do n\u2019t strongly enforce in our strategies .", "entities": []}, {"text": "Hence , we think that constraining interpolations in a semantically meaningful way is a promising direction to explore .", "entities": []}, {"text": "10291Method SST2 IMDB RT IG", "entities": [[1, 2, "DatasetName", "SST2"], [2, 3, "DatasetName", "IMDB"]]}, {"text": "-0.950 -0.446 -0.424 DIG - R ANDOM ANCHOR", "entities": []}, {"text": "-1.217\u00060.024 -0.834\u00060.021 -0.474\u00060.003 DIG - R ANDOM NEIGHBOR -1.247\u00060.013 -0.854\u00060.015 -0.460\u00060.010 DIG ( best ) -1.259 -0.878 -0.501 Table 4 : Comparison of DIG with two ablation variants - DIG - R ANDOM ANCHOR and DIGRANDOM NEIGHBOR on the DistilBERT model .", "entities": [[39, 40, "MethodName", "DistilBERT"]]}, {"text": "We report 5 - seed average log - odds score for the randomized methods .", "entities": []}, {"text": "Please refer to Section 4.3 for more details .", "entities": []}, {"text": "4.2 Human Evaluation To further understand the impact of our algorithm on end users , we conduct human evaluations of explanations from our method and the two top baselines - IG and GradShap .", "entities": []}, {"text": "We perform the study on the DistilBERT model \ufb01ne - tuned on SST2 dataset and the BERT model \ufb01ne - tuned on Rotten Tomatoes dataset .", "entities": [[6, 7, "MethodName", "DistilBERT"], [12, 13, "DatasetName", "SST2"], [16, 17, "MethodName", "BERT"]]}, {"text": "Further , we select the best variant of DIG on each dataset for explanation comparisons .", "entities": []}, {"text": "First , we pick 50 sample sentences from each dataset with lengths between 5 and 25 words for easier visualizations .", "entities": []}, {"text": "Then , we convert the attributions from each method into word highlights , whose intensity is determined by the magnitude of the attributions .", "entities": []}, {"text": "Finally , we show the highlighted sentence and the model \u2019s predicted label to the annotators and ask them to rank the explanations on a scale of 1 - 3 , \u201c 1 \u201d being the most comprehensive explanation that best justi\ufb01es the prediction .", "entities": []}, {"text": "Figure 3 shows the mean rank of each explanation algorithm across the two datasets .", "entities": []}, {"text": "We \ufb01nd that DIG has a signi\ufb01cantly lower mean rank compared to IG ( p < : 001on both SST2 and Rotten Tomatoes5 ) .", "entities": [[19, 20, "DatasetName", "SST2"]]}, {"text": "Thus , we conclude that explanations generated by DIG are also trustworthy according to humans .", "entities": []}, {"text": "Please refer to Appendix G for visualizations and discussion on explanations generated by our methods .", "entities": []}, {"text": "4.3 Performance Analysis In this section , we report the ablation of ANCHOR SEARCH and the effect of path density on DIG .", "entities": []}, {"text": "Please refer to Appendix F for ablations on neighborhood size and discussions on computational complexity .", "entities": []}, {"text": "Ablation Study on A NCHOR SEARCH .We ablate our methods with two random variants - DIG- RANDOM ANCHOR and DIG5We compute the p\u0000value using Wilcoxon signed - rank test .", "entities": []}, {"text": "# Interpolation PointsDelta % 024681012 1030100300IGDIG+MaxCount", "entities": []}, {"text": "Figure 4 : Effect of increasing number of interpolation pointsmon IG and DIG .", "entities": []}, {"text": "RANDOM NEIGHBOR , in which the ANCHOR SEARCH step uses a random anchor selection heuristic .", "entities": []}, {"text": "Speci\ufb01cally , in DIG- RANDOM ANCHOR , the anchor is selected randomly from the complete vocabulary .", "entities": []}, {"text": "Thus , this variant just ensures that the selected anchor is close to some word in the vocabulary which is not necessarily in the neighborhood .", "entities": []}, {"text": "In contrast , the DIG- RANDOM NEIGHBOR selects the anchor randomly from the neighborhood without using our proposed heuristics MAXCOUNT orGREEDY .", "entities": []}, {"text": "The log - odds metrics of IG , the two ablations , and our best variant of DIG for DistilBERT \ufb01ne - tuned individually on all three datasets are reported in Table 4 .", "entities": [[19, 20, "MethodName", "DistilBERT"]]}, {"text": "We report 5 - seed average for the randomized baselines .", "entities": []}, {"text": "We observe that DIG- RANDOM ANCHOR improves upon IG on all three datasets .", "entities": []}, {"text": "This shows that generating interpolation points close to the words in the vocabulary improve the explanation quality .", "entities": []}, {"text": "Further , we observe that DIG- RANDOM NEIGHBOR improves upon DIG- RANDOM ANCHOR on log - odds metric .", "entities": []}, {"text": "One reason could be that the words in a neighborhood are more semantically relevant to the original word , leading to more coherent perturbations for evaluating model gradients .", "entities": []}, {"text": "Finally , we observe that , on average , our proposed method is better compared to selecting a random anchor in the neighborhood .", "entities": []}, {"text": "This shows that our search strategies MAXCOUNT andGREEDY are indeed helpful .", "entities": []}, {"text": "Effect of Increasing Path Density .", "entities": []}, {"text": "In integrated gradients , the completeness axiom ( Section 2.2 ) is used to estimate if the integral approximation ( Equation 6 ) error is low enough .", "entities": []}, {"text": "This error is denoted as the Delta % error .", "entities": []}, {"text": "If the error is high , users can increase the number of interpolation points m. While DIG also satis\ufb01es the completeness axiom ,", "entities": []}, {"text": "10292FactorfLog - Odds#WAE#Delta % # 0 -1.259 0.227 4.926 1 -1.229 0.230 3.728 2 -1.184 0.232 2.752 3 -1.181 0.233 1.862 Table 5 : Effect of up - sampling a path by a factor fon Delta % for DIG using m= 30 .", "entities": [[5, 6, "DatasetName", "0"]]}, {"text": "error reduction by increasing mis infeasible .", "entities": []}, {"text": "This is because increasing min Equation 3 implicitly changes the integral path rather than increasing the density .", "entities": []}, {"text": "Hence , to achieve an error reduction in DIG , we up - sample the interpolation path P= fw;w 1;w2;:::;wm\u00002;w0gwith an up - sampling factor ( f ) of one as follows : P1 = fw;w+w1 2;w1;w1+w2 2;:::;wm\u00002+w0 2;w0 g ; i.e. , we insert the mean of two consecutive points to the path .", "entities": []}, {"text": "This essentially doubles the density of points in the path .", "entities": []}, {"text": "Similarly , P2can be obtained by up - sampling P1 , etc .", "entities": []}, {"text": "DIG(m;f = 0 ) refers to the standard DIG with no up - sampling .", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "Given that we have two hyperparameters mand fthat determine the overall path density , we analyze the effect of each of these in Figure 4 and Table 5 respectively .", "entities": []}, {"text": "The results are shown for DIG- MAXCOUNT applied on DistilBERT model \ufb01netuned on SST2 dataset .", "entities": [[9, 10, "MethodName", "DistilBERT"], [13, 14, "DatasetName", "SST2"]]}, {"text": "In Figure 4 , we observe that asmincreases , the Delta % of IG decreases as expected .", "entities": []}, {"text": "But the trend is opposite for DIG .", "entities": []}, {"text": "As discussed above , for DIG , the path length increases with increasing m , and hence , we attribute this trend to increasing dif\ufb01culty in effectively approximating the integral for longer paths .", "entities": []}, {"text": "Next , in Table 5 , we observe that as the up - sampling factorfincreases , the Delta % consistently decreases .", "entities": []}, {"text": "We also \ufb01nd that our up - sampling strategy does not increase the WAE by a signi\ufb01cant amount with increasingf , which is desirable .", "entities": []}, {"text": "Thus , this con\ufb01rms that our up - sampling strategy is a good substitute of increasing mfor IG to effectively reduce the integral approximation error Delta % .", "entities": []}, {"text": "Following Sundararajan et al .", "entities": []}, {"text": "( 2017 ) , we choose a threshold of 5 % average Delta to select the hyperparameters .", "entities": []}, {"text": "For more discussions , please refer to Appendix F.1 .", "entities": []}, {"text": "5 Related Works There has been an increasing effort in developing interpretability algorithms that can help understand a neural network model \u2019s behavior by explaining their predictions ( Doshi - Velez and Kim,2017 ;", "entities": []}, {"text": "Gilpin et al . , 2019 ) .", "entities": []}, {"text": "Attributions are a post - hoc explanation class where input features are quanti\ufb01ed by scalar scores indicating the magnitude of contribution of the features toward the predicted label .", "entities": []}, {"text": "Explanation algorithms that generate attributions can be broadly classi\ufb01ed into two categories - model - agnostic algorithms , like LIME ( Ribeiro et al . , 2016 ) , Input occlusion ( Li et al . , 2016 ) , Integrated gradients6(Sundararajan et al . , 2017 ) , SHAP ( Lundberg and Lee , 2017 ) , etc . and model - dependent algorithms , like LRP ( Binder et al . , 2016 ) , DeepLIFT ( Shrikumar et al . , 2017 ) , CD ( Murdoch et al . , 2018 ) , ACD ( Singh et al . , 2019 ) , SOC ( Jin et al . , 2020 ) , etc .", "entities": [[19, 20, "MethodName", "LIME"], [49, 50, "MethodName", "SHAP"], [107, 108, "DatasetName", "SOC"]]}, {"text": "While the model - agnostic algorithms can be used as blackbox explanation tools that can work for any neural network architecture , for the latter , one needs to understand the network \u2019s architectural details to implement the explanation algorithm .", "entities": []}, {"text": "Typically , model - dependent algorithms require speci\ufb01c layer decomposition rules ( Ancona et al . , 2018 ; Murdoch et al . , 2018 ) which needs to be de\ufb01ned for all the components in the model .", "entities": []}, {"text": "Model - agnostic methods usually work directly with the model outputs and gradients which are universally available .", "entities": []}, {"text": "Due to the many desirable explanation axioms and ease of gradient computation , there has been several extensions of integrated gradients .", "entities": []}, {"text": "For example , Miglani et al . ( 2020 ) study the effect of saturation in the saliency maps generated by integrated gradients .", "entities": []}, {"text": "Merrill et al .", "entities": []}, {"text": "( 2019 ) extend integrated gradients to certain classes of discontinuous functions in \ufb01nancial domains .", "entities": []}, {"text": "Further , Jha et al .", "entities": []}, {"text": "( 2020 ) use KNNs and auto - encoders to learn latent paths for RNAs .", "entities": []}, {"text": "Different from prior work , our focus here is to improve integrated gradients speci\ufb01cally for the discrete textual domain .", "entities": []}, {"text": "While the idea of learning latent paths for text data is quite interesting , it brings a signi\ufb01cant amount of challenge in successfully modeling such a complex latent space and hence , we leave this for future work .", "entities": []}, {"text": "6 Conclusion In this paper , we proposed Discretized integrated gradients ( DIG ) which is effective in explaining models working with discrete text data .", "entities": []}, {"text": "Further , we proposed two interpolation strategies - DIGGREEDY and DIG- MAXCOUNT that generate non6Note that IG is strictly not a model - agnostic algorithm since it is de\ufb01ned for neural networks , but we still classify it as one since the scope of this work is limited to working on neural networks .", "entities": []}, {"text": "10293linear interpolation paths for word embedding space .", "entities": []}, {"text": "Finally , we established the effectiveness of DIG over integrated gradients and other gradientbased baselines through experiments on multiple language models and datasets .", "entities": []}, {"text": "We also conduct human evaluations and \ufb01nd that DIG enhances human trust on model predictions .", "entities": []}, {"text": "Acknowledgments This research is supported in part by the Of\ufb01ce of the Director of National Intelligence ( ODNI ) , Intelligence Advanced Research Projects Activity ( IARPA ) , via Contract No . 2019 - 19051600007 , the DARPA MCS program under Contract No .", "entities": [[38, 39, "DatasetName", "DARPA"]]}, {"text": "N660011924033 , the Defense Advanced Research Projects Agency with award W911NF-19 - 20271 , NSF IIS 2048211 , NSF SMA 1829268 , USC Annenberg Graduate Fellowship , and gift awards from Google , Amazon , JP Morgan and Sony .", "entities": [[19, 20, "MethodName", "SMA"], [22, 23, "DatasetName", "USC"], [31, 32, "DatasetName", "Google"]]}, {"text": "We would like to thank all the collaborators in USC INK research lab for their constructive feedback on the work .", "entities": [[9, 10, "DatasetName", "USC"]]}, {"text": "References Marco Ancona , Enea Ceolini , Cengiz \u00d6ztireli , and Markus Gross . 2018 .", "entities": []}, {"text": "Towards better understanding of gradient - based attribution methods for deep neural networks .", "entities": []}, {"text": "In 6th International Conference on Learning Representations , ICLR 2018 , Vancouver , BC , Canada , April 30 - May 3 , 2018 , Conference Track Proceedings .", "entities": []}, {"text": "OpenReview.net .", "entities": []}, {"text": "Alexander Binder , Gr\u00e9goire Montavon , Sebastian Lapuschkin , Klaus - Robert M\u00fcller , and Wojciech Samek .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Layer - wise relevance propagation for neural networks with local renormalization layers .", "entities": []}, {"text": "InInternational Conference on Arti\ufb01cial Neural Networks .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Tom B. Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - V oss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel M. Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Language models are few - shot learners .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 33 : Annual Conference on Neural Information Processing Systems 2020 , NeurIPS 2020 , December 6 - 12 , 2020 , virtual .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training ofdeep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jay DeYoung , Sarthak Jain , Nazneen Fatema Rajani , Eric Lehman , Caiming Xiong , Richard Socher , and Byron C. Wallace .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "ERASER :", "entities": []}, {"text": "A benchmark to evaluate rationalized NLP models .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4443\u20134458 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Finale Doshi - Velez and Been Kim . 2017 .", "entities": []}, {"text": "Towards a rigorous science of interpretable machine learning .", "entities": [[5, 8, "TaskName", "interpretable machine learning"]]}, {"text": "Leilani H. Gilpin , David Bau , Ben Z. Yuan , Ayesha Bajwa , Michael Specter , and Lalana Kagal .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Explaining explanations : An overview of interpretability of machine learning .", "entities": []}, {"text": "Anupama Jha , Joseph K. Aicher , Matthew R. Gazzara , Deependra Singh , and Yoseph Barash .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Enhanced integrated gradients : improving interpretability of deep learning models using splicing codes as a case study .", "entities": []}, {"text": "Genome Biology , ( 1 ) .", "entities": []}, {"text": "Xisen Jin , Zhongyu Wei , Junyi Du , Xiangyang Xue , and Xiang Ren . 2020 .", "entities": []}, {"text": "Towards hierarchical importance attribution : Explaining compositional semantics for neural sequence models .", "entities": []}, {"text": "In 8th International Conference on Learning Representations , ICLR 2020 , Addis Ababa , Ethiopia , April 26 - 30 , 2020 .", "entities": []}, {"text": "OpenReview.net .", "entities": []}, {"text": "Jiwei Li , Will Monroe , and Dan Jurafsky .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Understanding neural networks through representation erasure .", "entities": []}, {"text": "ArXiv preprint , abs/1612.08220 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Zachary C. Lipton .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The mythos of model interpretability :", "entities": []}, {"text": "In machine learning , the concept of interpretability is both important and slippery .", "entities": []}, {"text": "Queue , ( 3 ) .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , and Jingfei Du an . 2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "ArXiv preprint , abs/1907.11692 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Scott M. Lundberg and Su - In Lee . 2017 .", "entities": []}, {"text": "A uni\ufb01ed approach to interpreting model predictions .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 30 : Annual Conference on Neural Information Processing Systems 2017 , December 4 - 9 , 2017 , Long Beach , CA , USA , pages 4765\u20134774 .", "entities": []}, {"text": "Andrew L. Maas , Raymond E. Daly , Peter T. Pham , Dan Huang , Andrew Y .", "entities": []}, {"text": "Ng , and Christopher Potts . 2011 .", "entities": []}, {"text": "Learning word vectors for sentiment analysis .", "entities": [[4, 6, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , pages 142\u2013150 , Portland , Oregon , USA . Association for Computational Linguistics .", "entities": []}, {"text": "10294John Merrill , Geoff Ward , Sean Kamkar , Jay Budzik , and Douglas Merrill . 2019 .", "entities": []}, {"text": "Generalized integrated gradients : A practical method for explaining diverse ensembles .", "entities": []}, {"text": "Vivek Miglani , Narine Kokhlikyan , Bilal Alsallakh , Miguel Martin , and Orion Reblitz - Richardson . 2020 .", "entities": []}, {"text": "Investigating saturation effects in integrated gradients .", "entities": []}, {"text": "W. James Murdoch , Peter J. Liu , and Bin Yu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Beyond word importance : Contextual decomposition to extract interactions from lstms .", "entities": []}, {"text": "In 6th International Conference on Learning Representations , ICLR 2018 , Vancouver , BC , Canada , April 30 - May 3 , 2018 , Conference Track Proceedings .", "entities": []}, {"text": "OpenReview.net .", "entities": []}, {"text": "Bo Pang and Lillian Lee . 2005 .", "entities": []}, {"text": "Seeing stars : Exploiting class relationships for sentiment categorization with respect to rating scales .", "entities": []}, {"text": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ( ACL\u201905 ) , pages 115 \u2013 124 , Ann Arbor , Michigan . Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Jeff Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Marco T\u00falio Ribeiro , Sameer Singh , and Carlos Guestrin . 2016 .", "entities": []}, {"text": "\" why should I trust you ? \" : Explaining the predictions of any classi\ufb01er .", "entities": []}, {"text": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , San Francisco , CA , USA , August 13 - 17 , 2016 , pages 1135\u20131144 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Victor Sanh , Lysandre Debut , Julien Chaumond , and Thomas Wolf .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Distilbert , a distilled version of bert : smaller , faster , cheaper and lighter .", "entities": [[0, 1, "MethodName", "Distilbert"]]}, {"text": "Avanti Shrikumar , Peyton Greenside , and Anshul Kundaje . 2017 .", "entities": []}, {"text": "Learning important features through propagating activation differences .", "entities": []}, {"text": "In Proceedings of the 34th International Conference on Machine Learning , ICML 2017 , Sydney , NSW , Australia , 6 - 11 August 2017 , volume 70 of Proceedings of Machine Learning Research , pages 3145\u20133153 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Avanti Shrikumar , Peyton Greenside , Anna Shcherbina , and Anshul Kundaje . 2016 .", "entities": []}, {"text": "Not just a black box : Learning important features through propagating activation differences .", "entities": []}, {"text": "ArXiv preprint , abs/1605.01713 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Chandan Singh , W. James Murdoch , and Bin Yu . 2019 .", "entities": []}, {"text": "Hierarchical interpretations for neural network predictions .", "entities": []}, {"text": "In 7th International Conference on Learning Representations , ICLR 2019 , New Orleans , LA , USA , May 6 - 9 , 2019 .", "entities": []}, {"text": "OpenReview.net .", "entities": []}, {"text": "Richard Socher , Alex Perelygin , Jean Wu , Jason Chuang , Christopher D. Manning , Andrew Ng , and Christopher Potts . 2013 .", "entities": []}, {"text": "Recursive deep modelsfor semantic compositionality over a sentiment treebank .", "entities": []}, {"text": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1631\u20131642 , Seattle , Washington , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Mukund Sundararajan , Ankur Taly , and Qiqi Yan . 2017 .", "entities": []}, {"text": "Axiomatic attribution for deep networks .", "entities": []}, {"text": "In Proceedings of the 34th International Conference on Machine Learning , ICML 2017 , Sydney , NSW , Australia , 6 - 11 August 2017 , volume 70 of Proceedings of Machine Learning Research , pages 3319\u20133328 . PMLR .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 38\u201345 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Wolf , Quentin Lhoest , Patrick von Platen , Yacine Jernite , Mariama Drame , Julien Plu , Julien Chaumond , Clement Delangue , Clara Ma , Abhishek Thakur , Suraj Patil , Joe Davison , Teven Le Scao , Victor Sanh , Canwen Xu , Nicolas Patry , Angie McMillan - Major , Simon Brandeis , Sylvain Gugger , Fran\u00e7ois Lagunas , Lysandre Debut , Morgan Funtowicz , Anthony Moi , Sasha Rush , Philipp Schmidd , Pierric Cistac , Victor Mu\u0161tar , Jeff Boudier , and Anna Tordjmann . 2020b .", "entities": []}, {"text": "Datasets .", "entities": []}, {"text": "GitHub .", "entities": []}, {"text": "Note : https://github.com/huggingface/datasets .", "entities": []}, {"text": "10295A Preliminaries A.1 Attribution - based Explanations Attribution - based explanations generate a scalar score for a given input feature that indicates the contribution ( or importance ) of that feature towards particular label ( Ancona et al . , 2018 ) .", "entities": []}, {"text": "Formally , letx=", "entities": []}, {"text": "[ x1;:::;xN]2RNbe an input to a model which produces an output y=", "entities": []}, {"text": "[ y1;:::;yC ] , whereCis the total number of labels .", "entities": []}, {"text": "For a given label ( usually the label predicted by the model ) , attribution - based explanation methods compute the contribution Rc= [ Rc 1;:::;Rc N]2RNof each feature .", "entities": []}, {"text": "A.2 Integrated gradients Integrated gradients ( IG ) ( Sundararajan et al . , 2017 ) for an inputxalong theithdimension is de\ufb01ned as follows : IGi(x ) = ( xi\u0000x0 i)\u0002R1 \u000b = 0@F(x0 + \u000b \u0002(x\u0000x0 ) ) @xid \u000b : ( 5 ) Here , Fis the neural network , x0is a baseline embedding , and \u000b is the step size .", "entities": [[61, 63, "HyperparameterName", "step size"]]}, {"text": "Simply put , integrated gradients algorithm works by sampling points at a uniform spacing along a straight - line between the input and the baseline , and summing the model \u2019s gradient at the inputs for each interpolated points .", "entities": []}, {"text": "To compute this integral ef\ufb01ciently , the authors propose a Riemann summation approximation de\ufb01ned below : IGapprox i(x )", "entities": []}, {"text": "= ( xi\u0000x0 i)\u0002\u0006m k=1@F(x0+k m\u0002(x\u0000x0 ) ) )", "entities": []}, {"text": "@xi\u00021 m ; ( 6 ) wheremis the total number of steps considered for the approximation .", "entities": []}, {"text": "Next , we brie\ufb02y describe how IG is used to explain a model \u2019s prediction which takes a sentence as input ( for example , the model can be a text classi\ufb01cation network ) .", "entities": []}, {"text": "Let S= [ w0::wn]be a sentence of lengthnandwibe theithword embedding of the sentence .", "entities": []}, {"text": "Also , let Fbe a text - classi\ufb01cation model , i.e. ,y = F(S ) .", "entities": []}, {"text": "Then , IG calculates the attribution for each dimension of a word embedding wi .", "entities": []}, {"text": "The interpolation points required for Equation 6 are generated by linearly interpolating the word embedding between wiand a baseline word embedding ( usually chosen as the pad embedding ) .", "entities": []}, {"text": "Then , using Eq . 6 , the attribution for the ithdimension ofwis calculated .", "entities": []}, {"text": "The \ufb01nal word attribution is the sum of the attributions for each dimension of the word embedding .", "entities": []}, {"text": "B Comparison with Integrated gradients and Path methods It is easy to see that the approximation of integrated gradients is a special case of DIG .", "entities": []}, {"text": "Note that the kth linear interpolation of the ithdimension of input x for IG can be represented as :", "entities": [[3, 4, "DatasetName", "kth"]]}, {"text": "xk i = x0 i+k", "entities": []}, {"text": "m\u0002(xi\u0000x0 i ): ( 7 ) Substituting Eq . 7 in Eq .", "entities": []}, {"text": "3 gives us Eq .", "entities": []}, {"text": "6 .", "entities": []}, {"text": "Sundararajan et al .", "entities": []}, {"text": "( 2017 ) de\ufb01ne path methods as the general form of integrated gradients that are applicable for all monotonic paths between the input and the baseline .", "entities": []}, {"text": "Our DIG approach is a reformulation of the path method where the paths are not necessarily parameterized by \u000b , making it more applicable for discrete data domain .", "entities": []}, {"text": "Hence , DIG also satis\ufb01es all the theoretical properties applicable for path methods - Implementation Invariance , Sensitivity , Linearity , and Completeness .", "entities": []}, {"text": "We refer the readers to Proposition 2 in Sundararajan et al .", "entities": []}, {"text": "( 2017 ) for more technical details .", "entities": []}, {"text": "C Evaluation Metrics", "entities": []}, {"text": "In this section , we rede\ufb01ne the evaluation metrics and state the formulations for each of them .", "entities": []}, {"text": "In this work , we use the following three automated metrics : \u2022Log - odds ( LO ) score ( Shrikumar et al . , 2017 ) is de\ufb01ned as the average difference of the negative logarithmic probabilities on the predicted class before and after masking the top k%features with zero padding .", "entities": []}, {"text": "Given the attribution scores generated by an explanation algorithm , we select the top k% words based on their attributions replace them with zero padding .", "entities": []}, {"text": "More concretely , for a dataset with Nsentences , it is de\ufb01ned as : log\u0000odds(k )", "entities": []}, {"text": "= 1 NNX i=1logp\u0010 ^yjx(k ) i\u0011 p(^yjxi ) ; where ^yis the predicted class , xiis theith sentence , and x(k ) iis the modi\ufb01ed sentence with top k% words replaced with zero padding .", "entities": []}, {"text": "Lower scores are better .", "entities": []}, {"text": "\u2022Comprehensiveness ( Comp ) score ( DeYoung et al . , 2020 ) is the average difference of the change in predicted class probability", "entities": []}, {"text": "10296Dataset DistilBERT RoBERTa BERT SST2 1.00 0.00 0.42 IMDB 0.98 -0.68 0.51 Rotten Tomatoes 0.21 0.22 0.23 Table 6 : Pearson correlation between log - odds and WAE metrics for different dataset+LM settings .", "entities": [[1, 2, "MethodName", "DistilBERT"], [2, 3, "MethodName", "RoBERTa"], [3, 4, "MethodName", "BERT"], [4, 5, "DatasetName", "SST2"], [8, 9, "DatasetName", "IMDB"], [20, 22, "MetricName", "Pearson correlation"]]}, {"text": "Please refer to Appendix E for more details .", "entities": []}, {"text": "before and after removing the top k%features .", "entities": []}, {"text": "Similar to Log - odds , this measures the in\ufb02uence of the top - attributed words on the model \u2019s prediction .", "entities": []}, {"text": "It is de\ufb01ned as : Comp(k ) = 1 NNX i=1p(^yjx(k ) i)\u0000p(^yjxi ): Herex(k ) idenotes the modi\ufb01ed sentence with topk%words deleted from the sentence .", "entities": []}, {"text": "Higher scores are better .", "entities": []}, {"text": "\u2022Suf\ufb01ciency ( Suff ) score ( DeYoung et al . , 2020 ) is de\ufb01ned as the average difference of the change in predicted class probability before and after keeping only the top k%features .", "entities": []}, {"text": "This measures the adequacy of the top k%attributions for model \u2019s prediction .", "entities": []}, {"text": "It is de\ufb01ned in a similar fashion as comprehensiveness , except the x(k ) iis de\ufb01ned as the sentence containing only the top k%words .", "entities": []}, {"text": "Lower scores are better .", "entities": []}, {"text": "D Effect of top - k in evaluation metrics In Figure 5 , we visualize the effect of changing top - k% on log - odds , comprehensiveness , and suf\ufb01ciency metrics for DistilBERT model \ufb01ne - tuned on the SST2 dataset .", "entities": [[33, 34, "MethodName", "DistilBERT"], [40, 41, "DatasetName", "SST2"]]}, {"text": "We compare the two variants of our method : DIG- GREEDY and DIG- MAXCOUNT with Integrated Gradients .", "entities": []}, {"text": "We observe that our method outperforms IG for all values of k. Speci\ufb01cally , we note that the gap between DIG and IG is initially non - existent but then gradually increases with increasing k in Figure 5 ( a ) and eventually saturates .", "entities": []}, {"text": "This shows that although IG might be equally good as DIG at \ufb01nding the top-5 % important words , the explanations from IG are significantly misaligned from true model behavior for higher top - k values .", "entities": []}, {"text": "Top - kLog - odds -1.500 - 1.000 - 0.5000.000 10 20 30 40 50IG", "entities": []}, {"text": "DIG - MaxCount DIG - Greedy(a ) Log - odds # Top - kComp 0.0000.1000.2000.3000.4000.500 10 20 30 40 50IG DIG - MaxCount DIG - Greedy ( b ) Comprehensiveness \" Top - kSuff 0.0000.1000.2000.3000.400 10 20 30 40 50IG DIG - MaxCount DIG - Greedy ( c )", "entities": []}, {"text": "Suf\ufb01ciency # Figure 5 : Effect of changing top - k% in log - odds , comprehensiveness , and suf\ufb01ciency metric for the DistilBERT model \ufb01ne - tuned on SST2 dataset .", "entities": [[23, 24, "MethodName", "DistilBERT"], [29, 30, "DatasetName", "SST2"]]}, {"text": "10297mIG DIG Log - Odds#Delta % # Log - Odds#Delta", "entities": []}, {"text": "% # 10 -0.984 8.064 -1.252", "entities": []}, {"text": "2.263 30 -0.950 3.394 -1.259 4.926 100 -0.933 1.235 -1.258 9.849 300 -0.940 0.703 -1.242 10.955 Table 7 : Effect of increasing number of interpolation pointsmon Delta % for IG and DIG .", "entities": []}, {"text": "Please refer to Appendix F.1 for more details .", "entities": []}, {"text": "Up - sampling factor fLog - Odds#WAE#Delta % # DIG ( m= 30;f= 0 ) -1.259 0.227 4.926 DIG ( m= 30;f= 1 ) -1.229 0.230 3.728 DIG ( m= 30;f= 2 ) -1.184 0.232 2.752 DIG ( m= 30;f= 3 ) -1.181 0.233 1.862 Table 8 : Effect of up - sampling a path by a factor fon", "entities": [[13, 14, "DatasetName", "0"]]}, {"text": "Delta % for DIG .", "entities": []}, {"text": "For more details , refer to Appendix F.1 .", "entities": []}, {"text": "E Correlation between Log - odds and WAE", "entities": []}, {"text": "We compute the Pearson correlation between logodds and WAE for each dataset + LM pair .", "entities": [[3, 5, "MetricName", "Pearson correlation"]]}, {"text": "For this , we consider the metric values for IG , DIGGREEDY , and DIG- MAXCOUNT and report the correlations for each setting in Table 6 .", "entities": []}, {"text": "We observe that , there is a strong correlation on average for DistilBERT .", "entities": [[12, 13, "MethodName", "DistilBERT"]]}, {"text": "For BERT and RoBERTa we \ufb01nd a weak positive and negative correlation respectively .", "entities": [[1, 2, "MethodName", "BERT"], [3, 4, "MethodName", "RoBERTa"]]}, {"text": "F Ablation Studies F.1 Effect of increasing path density Here , we report the detailed analysis of the effect of increasingmandfin Tables 7 and 8 respectively .", "entities": []}, {"text": "In Table 7 , we report the Log - odds score along with Delta % .", "entities": []}, {"text": "We do not note any consistent trend in Log - odds with increasing mfor both IG and DIG .", "entities": []}, {"text": "The results of IG suggest that , as long as the Delta % is suf\ufb01ciently low , decreasing Delta % any further does n\u2019t impact the explanations very signi\ufb01cantly .", "entities": []}, {"text": "Further , in Table 8 , we report the WAE metrics to emphasize that our up - sampling strategy does n\u2019t increase the WAE by a signi\ufb01cant amount , which is desirable .", "entities": []}, {"text": "Also , we note a consistent increase ( although marginally ) in Log - odds with decreasing Delta % .", "entities": []}, {"text": "But per our previous observations on IG , we believe these changes do not imply a causal relation between the two .", "entities": []}, {"text": "K Log - odds#WAE#Delta % # 10 -1.258 0.276 21.405 30 -1.263 0.310 12.228 100 -1.277 0.276 14.155 200 -1.194 0.295 10.647 300 -1.216 0.286 8.523 500 -1.259 0.227 4.926 Table 9 : Effect of increasing the neighborhood size K ofKNN Vfor DIG .", "entities": []}, {"text": "Please refer to Appendix F.2 for more details .", "entities": []}, {"text": "F.2 Effect of increasing neighborhood size", "entities": []}, {"text": "In this section , we study the effect of increasing the neighborhood size in DIG .", "entities": []}, {"text": "The results are shown in Table 9 .", "entities": []}, {"text": "We observe a clear decreasing trend in Delta % with increasing neighborhood size , but there is no clear trend on Log - odds or WAE .", "entities": []}, {"text": "Hence , we believe that the neighborhood size has little impact on the explanation quality , but we should still ensure suf\ufb01ciently low Delta .", "entities": []}, {"text": "F.3 Discussion on computational complexity In this section", "entities": []}, {"text": ", we brie\ufb02y discuss the computational complexity of our proposed interpolation strategies .", "entities": []}, {"text": "The algorithms for DIG- GREEDY and DIG- MAXCOUNT are presented in Algorithms 1 and 2 respectively .", "entities": []}, {"text": "From there , we observe that both our algorithms have a running time complexity ofO(nmK ) , wherenis the number of words , mis the number of interpolation points , and Kis theKNNVneighborhood size .", "entities": []}, {"text": "While it is computationally feasible to parallelize the loops corresponding to nandK , the same can not be said for the loop corresponding to mbecause we select the interpolation points iteratively .", "entities": []}, {"text": "Although we empirically \ufb01nd in Section F.1 that a small number of interpolation points are suf\ufb01cient to calculate the explanations , we believe this bottleneck can be further tackled through ef\ufb01cient design of noniterative search algorithms .", "entities": []}, {"text": "We leave this for future works .", "entities": []}, {"text": "G Visualizations of explanations In this section , we present some interesting sentence visualizations based on explanations from DIG and IG for SST2 dataset in Figure 6 .", "entities": [[22, 23, "DatasetName", "SST2"]]}, {"text": "We show the sentence visualization and the model \u2019s predicted sentiment for the sentence for each explanation algorithm .", "entities": []}, {"text": "In the visualizations , the red highlighted words denote positive attributions and", "entities": []}, {"text": "10298blue denotes negative attributions .", "entities": []}, {"text": "That is , the explanation model suggests that the red highlighted words support the predicted label whereas the blue ones oppose ( or undermine ) the prediction .", "entities": []}, {"text": "We observe that in many cases , DIG is able to highlight more plausible explanations .", "entities": []}, {"text": "For example , in sentence pairs 1 - 7 , clearly the DIG highlights are more inline with the model prediction .", "entities": []}, {"text": "But we want to emphasize that it does not mean that our method always produces more plausible highlights .", "entities": []}, {"text": "For example , for sentences 8 - 10 , we observe that highlights from IG are more plausible than those of DIG .", "entities": []}, {"text": "Hence , this shows that , while it could be a good exercise to visualize the attributions as a sanity check , we should rely more on automated metrics and human evaluations to correctly compare explanation algorithms .", "entities": []}, {"text": "Algorithm 1 : DIG - G REEDY Input : Sentences=", "entities": []}, {"text": "[ w1;w2;:::wn ] , k - nearest neighbor graph for the vocabularyKNNV , number of interpolation points m Output : Interpolations 1points", "entities": []}, {"text": "= [ ] n\u0003m 2fori 1tondo 3 forj 1tomdo 4dists = fg 5 fork 1toKdo 6 nbr KNNV(wi)[k ] 7 c0 MO N O T", "entities": []}, {"text": "O", "entities": []}, {"text": "N I Z E ( nbr;wi ) 8 dists [ nbr ]   Distance ( nbr;c0 ) 9 end for 10a arg mina02distsdists", "entities": []}, {"text": "[ a0 ] 11c MO N O T", "entities": []}, {"text": "O", "entities": []}, {"text": "N I Z E ( a;wi ) 12points", "entities": []}, {"text": "[ i;j ] c 13 end for 14end for 15returnpointsAlgorithm 2 : DIG - M AXCOUNT", "entities": []}, {"text": "Input : Sentences=", "entities": []}, {"text": "[ w1;w2;:::wn ] , k - nearest neighbor graph for the vocabularyKNNV , number of interpolation points m Output : Interpolations 1points", "entities": []}, {"text": "= [ ] n\u0003m 2fori 1tondo 3 forj 1tomdo 4a arg maxa02KNN V(wi)jMa0j", "entities": []}, {"text": "5c", "entities": []}, {"text": "MO N O T", "entities": []}, {"text": "O", "entities": []}, {"text": "N I Z E ( a;wi ) 6points", "entities": []}, {"text": "[ i;j ] c 7 end for 8end for 9returnpoints", "entities": []}, {"text": "10299 Figure 6 : Some example visualizations of attributions from DIG and IG for the DistilBERT model \ufb01ne - tuned on SST2 dataset .", "entities": [[15, 16, "MethodName", "DistilBERT"], [21, 22, "DatasetName", "SST2"]]}, {"text": "The sentence visualization is followed by model \u2019s sentiment prediction for the sentence .", "entities": []}, {"text": "Here , the red highlighted words denote positive attributions and blue denotes negative attributions .", "entities": []}, {"text": "For more details , please refer to Appendix G", "entities": []}]