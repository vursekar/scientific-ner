[{"text": "Proceedings of the 19th International Conference on Spoken Language Translation ( IWSLT 2022 ) , pages 277 - 285 May 26 - 27 , 2022 c", "entities": [[9, 10, "TaskName", "Translation"]]}, {"text": "2022 Association for Computational Linguistics CUNI - KIT System for Simultaneous Speech Translation Task at IWSLT 2022 Peter Pol\u00e1k1and Ngoc - Quan Ngoc2and Tuan - Nam Nguyen2and Danni Liu3 Carlos Mullov2and Jan Niehues2and", "entities": [[12, 13, "TaskName", "Translation"]]}, {"text": "Ond \u02c7rej Bojar1and Alexander Waibel2,4 polak@ufal.mff.cuni.cz 1Charles University 2Karlsruhe Institute of Technology", "entities": []}, {"text": "3Maastricht University 4Carnegie Mellon University", "entities": []}, {"text": "Abstract In this paper , we describe our submission to the Simultaneous Speech Translation at IWSLT 2022 .", "entities": [[13, 14, "TaskName", "Translation"]]}, {"text": "We explore strategies to utilize an offline model in a simultaneous setting without the need to modify the original model .", "entities": []}, {"text": "In our experiments , we show that our onlinization algorithm is almost on par with the offline setting while being 3\u00d7faster than offline in terms of latency on the test set .", "entities": []}, {"text": "We also show that the onlinized offline model outperforms the best IWSLT2021 simultaneous system in medium and high latency regimes and is almost on par in the low latency regime .", "entities": []}, {"text": "We make our system publicly available.1 1 Introduction This paper describes the CUNI - KIT submission to the Simultaneous Speech Translation task at IWSLT 2022 ( Anastasopoulos et al . , 2022 ) by Charles University ( CUNI ) and Karlsruhe Institute of Technology ( KIT ) .", "entities": [[20, 21, "TaskName", "Translation"]]}, {"text": "Recent work on end - to - end ( E2E ) simultaneous speech - to - text translation ( ST ) is focused on training specialized models specifically for this task .", "entities": [[9, 10, "DatasetName", "E2E"], [12, 18, "TaskName", "speech - to - text translation"]]}, {"text": "The disadvantage is the need of storing an extra model , usually a more difficult training and inference setup , increased computational complexity ( Han et al . , 2020 ; Liu et al . , 2021 ) and risk of performance degradation if used in offline setting ( Liu et al . , 2020a ) .", "entities": []}, {"text": "In this work , we base our system on a robust multilingual offline ST model that leverages pretrained wav2vec 2.0 ( Baevski et al . , 2020 ) and mBART ( Liu et al . , 2020b ) .", "entities": [[29, 30, "MethodName", "mBART"]]}, {"text": "We revise the onlinization approach by Liu et al . ( 2020a ) and propose an improved technique with a fully controllable qualitylatency trade - off .", "entities": []}, {"text": "We demonstrate that without any change to the offline model , our simultaneous system in the mid- and high - latency regimes is on par 1https://hub.docker.com/repository/ docker / polape7 / cuni - kit - simultaneouswith the offline performance .", "entities": []}, {"text": "At the same time , the model outperforms previous IWSLT systems in medium and high latency regimes and is almost on par in the low latency regime .", "entities": []}, {"text": "Finally , we observe a problematic behavior of the average lagging metric for speech translation ( Ma et al . , 2020 ) when dealing with long hypotheses , resulting in negative values .", "entities": []}, {"text": "We propose a minor change to the metric formula to prevent this behavior .", "entities": []}, {"text": "Our contribution is as follows : \u2022We revise and generalize onlinization proposed by Liu et al . ( 2020a ) ; Nguyen et al . ( 2021 ) and discover parameter enabling quality - latency trade - off , \u2022We demonstrate that one multilingual offline model can serve as simultaneous ST for three language pairs , \u2022We demonstrate that an improvement in the offline model leads also to an improvement in the online regime , \u2022We propose a change to the average lagging metric that avoids negative values .", "entities": []}, {"text": "2 Related Work Simultaneous speech translation can be implemented either as a ( hybrid ) cascaded system ( Kolss et al . , 2008 ; Niehues et al . , 2016 ; Elbayad et al . , 2020 ;", "entities": []}, {"text": "Liu et al . , 2020a ; Bahar et al . , 2021 ) or an end - to - end model ( Han et al . , 2020 ; Liu et al . , 2021 ) .", "entities": []}, {"text": "Unlike for the offline speech translation where cascade seems to have the best quality , the end - to - end speech translation offers a better qualitylatency trade - off ( Ansari et al . , 2020 ; Liu et al . , 2021 ; Anastasopoulos et al . , 2021 ) .", "entities": []}, {"text": "End - to - end systems use different techniques to perform simultaneous speech translation .", "entities": []}, {"text": "Han et al .", "entities": []}, {"text": "( 2020 ) uses wait- k(Ma et", "entities": []}, {"text": "al . , 2019 ) model and metalearning ( Indurthi et al . , 2020 ) to alleviate277", "entities": []}, {"text": "the data scarcity .", "entities": []}, {"text": "Liu et al .", "entities": []}, {"text": "( 2020a ) uses a unidirectional encoder with monotonic cross - attention to limit the dependence on future context .", "entities": []}, {"text": "Other work ( Liu et al . , 2021 ) proposes Cross Attention augmented Transducer ( CAAT ) as an extension of RNN - T ( Graves , 2012 ) .", "entities": []}, {"text": "Nguyen et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2021 ) proposed a hypothesis stability detection for automatic speech recognition ( ASR ) .", "entities": [[9, 12, "TaskName", "automatic speech recognition"]]}, {"text": "The shared prefix strategy finds the longest common prefix in all beams .", "entities": []}, {"text": "Liu et al .", "entities": []}, {"text": "( 2020a ) explore such strategies in the context of speech recognition and translation .", "entities": [[10, 12, "TaskName", "speech recognition"]]}, {"text": "The most promising is the longest common prefix of two consecutive chunks .", "entities": []}, {"text": "The downside of this approach is the inability to parametrize the quality - latency trade - off .", "entities": []}, {"text": "We directly address this in our work .", "entities": []}, {"text": "3 Onlinization In this section , we describe the onlinization of the offline model and propose two ways to control the quality - latency trade - off .", "entities": []}, {"text": "3.1 Incremental Decoding Depending on the language pair , translation tasks may require reordering or a piece of information that might not be apparent until the source utterance ends .", "entities": []}, {"text": "In the offline setting , the model processes the whole utterance at once , rendering the strategy most optimal in terms of quality .", "entities": []}, {"text": "If applied in online mode , this ultimately leads to a large latency .", "entities": []}, {"text": "One approach to reducing the latency is to break the source utterance into chunks and perform the translation on each chunk .", "entities": []}, {"text": "In this paper , we follow the incremental decoding framework described by Liu et al .", "entities": []}, {"text": "( 2020a )", "entities": []}, {"text": ".", "entities": []}, {"text": "We break the input utterance into small fixed - size chunks and decode each time after we receive a new chunk .", "entities": []}, {"text": "After each decoding step , we identify a stable part of the hypothesis using stable hypothesis detection .", "entities": []}, {"text": "The stable part is sent to the user ( \u201c committed \u201d in the following ) and is no longer changed afterward ( i.e. , no retranslation).2Our current implementation assumes that the whole speech input fits into memory , in other words , we are only adding new chunks as they are arriving .", "entities": []}, {"text": "This simplification is possible because the evaluation of the shared task is performed on segmented input , on individual utterances .", "entities": []}, {"text": "With each newly arrived input chunk , the decoding starts with forced decoding of 2This is a requirement for the evaluation in the Simultaneous Speech Translation task at IWSLT 2022.the already committed tokens and continues with beam search decoding .", "entities": [[25, 26, "TaskName", "Translation"]]}, {"text": "3.2 Chunk Size Speech recognition and translation use chunking for simultaneous inference with various chunk sizes ranging from 300 ms to 2 seconds ( Liu , 2020 ; Nguyen et al . , 2021 ) although the literature suggests that the turn - taking in conversational speech is shorter , around 200 ms ( Levinson and Torreira , 2015 ) .", "entities": [[3, 5, "TaskName", "Speech recognition"], [8, 9, "TaskName", "chunking"]]}, {"text": "We investigate different chunk sizes in combination with various stable hypothesis detection strategies .", "entities": []}, {"text": "As we document later , the chunk size is the principal factor that controls the quality - latency trade - off .", "entities": []}, {"text": "3.3 Stable Hypothesis Detection Committing hypotheses from incomplete input presents a possible risk of introducing errors .", "entities": []}, {"text": "To reduce the instability and trade time for quality , we employ a stable hypothesis detection .", "entities": []}, {"text": "Formally , we define a function prefix ( W)that , given a set of hypotheses ( i.e. , Wc allif we want to consider the whole beam or Wc bestfor", "entities": []}, {"text": "the single best hypothesis obtained during the beam search decoding of the c - th chunk ) , outputs a stable prefix .", "entities": []}, {"text": "We investigate several functions : Hold- n(Liu et al . , 2020a ) Hold- nstrategy selects the best hypothesis in the beam and deletes the last ntokens from it : prefix ( Wc best )", "entities": []}, {"text": "= W0 : max(0 , |W|\u2212n ) , ( 1 ) where Wc bestis the best hypothesis obtained in the beam search of c - th chunk .", "entities": []}, {"text": "If the hypothesis has onlynor fewer tokens , we return an empty string .", "entities": []}, {"text": "LA - nLocal agreement ( Liu et al . , 2020a ) displays the agreeing prefixes of the two consecutive chunks .", "entities": []}, {"text": "Unlike the hold- nstrategy , the local agreement does not offer any explicit quality - latency trade - off .", "entities": []}, {"text": "We generalize the strategy to take the agreeing prefixes of nconsecutive chunks .", "entities": []}, {"text": "During the first n\u22121chunks , we do not output any tokens .", "entities": []}, {"text": "From the n - th chunk on , we identify the longest common prefix of the best hypothesis of the nconsecutive chunks : prefix ( Wc best )", "entities": []}, {"text": "= /braceleft\uf8ecigg \u2205 , ifc < n , LCP ( Wc\u2212n+1 best , ... , Wc best),otherwise , ( 2)278", "entities": []}, {"text": "where LCP ( \u00b7 ) is longest common prefix of the arguments .", "entities": []}, {"text": "SP - nShared prefix ( Nguyen et al . , 2021 ) strategy displays the longest common prefix of all the items in the beam of a chunk .", "entities": []}, {"text": "Similarly to the LA- n strategy , we propose a generalization to the longest common prefix of all items in the beams of the n consecutive chunks : prefix ( Wc all )", "entities": []}, {"text": "= /braceleft\uf8ecigg \u2205 , ifc < n , LCP ( Wc\u2212n+1 beam 1 ... B , ... , Wc beam 1 ... B),otherwise , ( 3 ) i.e. , all beam hypotheses 1 , ... , B ( where Bis the beam size ) of all chunks c\u2212n+ 1 , ... , c .", "entities": []}, {"text": "3.4 Initial Wait The limited context of the early chunks might result in an unstable hypothesis and an emission of erroneous tokens .", "entities": []}, {"text": "The autoregressive nature of the model might cause further performance degradation in later chunks .", "entities": []}, {"text": "One possible solution is to use longer chunks , but it inevitably leads to a higher latency throughout the whole utterance .", "entities": []}, {"text": "To mitigate this issue , we explore a lengthening of the first chunk .", "entities": []}, {"text": "We call this strategy an initial wait .", "entities": []}, {"text": "4 Experiments Setup In this section , we describe the onlinization experiments .", "entities": []}, {"text": "4.1 Evaluation Setup We use the SimulEval toolkit ( Ma et al . , 2020 ) .", "entities": []}, {"text": "The toolkit provides a simple interface for evaluation of simultaneous ( speech ) translation .", "entities": []}, {"text": "It reports the quality metric BLEU ( Papineni et al . , 2002 ; Post , 2018 ) and latency metrics Average Proportion ( AP , Cho and Esipova 2016 ) , Average Lagging ( AL , Ma et al . 2019 ) , and Differentiable Average Lagging ( DAL , Cherry and Foster 2019 ) modified for speech source .", "entities": [[5, 6, "MetricName", "BLEU"], [24, 25, "DatasetName", "AP"]]}, {"text": "Specifically , we implement an Agent class .", "entities": [[5, 6, "DatasetName", "Agent"]]}, {"text": "We have to implement two important functions : policy(state ) and predict(state ) , where state is the state of the agent ( e.g. , read processed input , emitted tokens , ... ) .", "entities": [[21, 22, "DatasetName", "agent"]]}, {"text": "The policy function returns the action of the agent : ( 1 ) READ to request more input , ( 2 ) WRITE to emit new hypothesis tokens .", "entities": [[8, 9, "DatasetName", "agent"]]}, {"text": "We implement the policy as specified in Algorithm 1 .", "entities": []}, {"text": "The default action is READ .", "entities": []}, {"text": "If there is a new chunk , we perform the inference and use the prefix ( Wc)function to find the stable prefix .", "entities": []}, {"text": "If there are new tokens to display ( i.e. , |prefix ( Wc)|>|prefix ( Wc\u22121)| ) , we return the WRITE action .", "entities": []}, {"text": "As soon as our agent emits an endof - sequence ( EOS ) token , the inference of the utterance is finished by the SimulEval .", "entities": [[4, 5, "DatasetName", "agent"]]}, {"text": "We noticed that our model was emitting the EOS token quite often , especially in the early chunks .", "entities": []}, {"text": "Hence , we ignore the EOS if returned by our model and continue the inference until the end of the source.3 Algorithm 1 Policy function Require : state ifstate .new_input", "entities": []}, {"text": "> chunk _ size then hypothesis \u2190predict ( state ) if|hypothesis |>0then", "entities": []}, {"text": "return WRITE end if end if return READ 4.2 Speech Translation Models In our experiments , we use two different models .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "First , we do experiments with a monolingual Model A , then for the submission , we use a multilingual and more robust Model B .4", "entities": []}, {"text": "Model A is the KIT IWSLT 2020 model for the Offline Speech Translation task .", "entities": [[12, 13, "TaskName", "Translation"]]}, {"text": "Specifically , it is an end - to - end English to German Transformer model with relative attention .", "entities": [[13, 14, "MethodName", "Transformer"]]}, {"text": "For more described description , refer to Pham et al .", "entities": []}, {"text": "( 2020b ) .", "entities": []}, {"text": "4.2.1 Multilingual Model", "entities": []}, {"text": "For the submission , we use a multilingual Model B. We construct the SLT architecture with the encoder based on the wav2vec 2.0 ( Baevski et al . , 2020 ) and the decoder based on the autoregressive language model pretrained with mBART50 ( Tang et al . , 2020 ) .", "entities": []}, {"text": "wav2vec 2.0 is a Transformer encoder model which receives raw waveforms as input and generates high - level representations .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "The architecture consists of two main components : first , a 3This might cause an unnecessary increase in latency , but it might be partially prevented by voice activity detection .", "entities": [[28, 30, "TaskName", "activity detection"]]}, {"text": "4We also did experiments with a dedicated EnglishGerman model similar to Model B ( i.e. , based on wav2vec and mBART ) , but it performed worse both in offline and online setting compared to the multilingual version.279", "entities": [[20, 21, "MethodName", "mBART"]]}, {"text": "convolution - based feature extractor downsamples long audio waveforms into features that have similar lengths with spectrograms .", "entities": [[0, 1, "MethodName", "convolution"]]}, {"text": "After that , a deep Transformer encoder uses self - attention and feedforward neural network blocks to transform the features without further downsampling .", "entities": [[5, 6, "MethodName", "Transformer"]]}, {"text": "During the self - supervised training process , the network is trained with a contrastive learning strategy ( Baevski et al . , 2020 ) , in which the already downsampled features are randomly masked and the model learns to predict the quantized latent representation of the masked time step .", "entities": [[14, 16, "MethodName", "contrastive learning"]]}, {"text": "During the supervised learning step , we freeze the feature extraction weights to save memory since the first layers are among the largest ones .", "entities": []}, {"text": "We fine - tune all of the weights in the Transformer encoder .", "entities": [[10, 11, "MethodName", "Transformer"]]}, {"text": "Moreover , to make the model more robust to the fluctuation in absolute positions and durations when it comes to audio signals , we added the relative position encodings ( Dai et al . , 2019 ; Pham et al . , 2020a ) to alleviate this problem.5 Here we used the same pretrained model with the speech recognizer , with the large architecture pretrained with 53khours of unlabeled data .", "entities": [[26, 29, "MethodName", "relative position encodings"]]}, {"text": "mBART50 is an encoder - decoder Transformerbased language model .", "entities": []}, {"text": "During training , instead of the typical language modeling setting of predicting the next word in the sequence , this model is trained to reconstruct a sequence from its noisy version ( Lewis et al . , 2019 ) and later extended to a multilingual version ( Liu et al . , 2020b ; Tang et al . , 2020 ) in which the corpora from multiple languages are combined during training .", "entities": []}, {"text": "mBART50 is the version that is pretrained on 50languages .", "entities": []}, {"text": "The mBART50 model follows the Transformer encoder and decoder ( Vaswani et al . , 2017 ) .", "entities": [[5, 6, "MethodName", "Transformer"]]}, {"text": "During fine - tuning , we combine the mBART50 decoder with the wav2vec 2.0 encoder , where both encoder and decoder know one modality .", "entities": []}, {"text": "The crossattention layers connecting the decoder with the encoder are the parts that require extensive finetuning in this case , due to the modality mismatch between pretraining and fine - tuning .", "entities": []}, {"text": "Finally , we use the model in a multilingual setting , i.e. , for English to Chinese , German , and Japanese language pairs by training on the combination of the datasets .", "entities": []}, {"text": "The mBART50 vocabulary contains language tokens for all three languages 5This has the added advantage of better generalization in situations where training and testing data are segmented differently.and can be used to control the language output ( Ha et al . , 2016 ) .", "entities": []}, {"text": "For more details on the model refer to Pham et al . ( 2022 ) .", "entities": []}, {"text": "4.3 Test Data For the onlinization experiments , we use MuST - C ( Cattoni et al . , 2021 ) tst - COMMON from the v2.0 release .", "entities": [[10, 13, "DatasetName", "MuST - C"]]}, {"text": "We conduct all the experiments on the English - German language pair .", "entities": []}, {"text": "5 Experiments and Results In this section , we describe the experiments and discuss the results .", "entities": []}, {"text": "5.1 Chunks Size We experiment with chunk sizes of 250 ms , 500 ms , 1s , and 2 s. We combine the sizes of the chunks with different partial hypothesis selection strategies .", "entities": []}, {"text": "The results are shown in Figure 1 .", "entities": []}, {"text": "The results document that the chunk size parameter has a stronger influence on the trade - off than different prefix strategies .", "entities": []}, {"text": "Additionally , this enables constant trade - off strategies ( e.g. , LA-2 ) to become flexible .", "entities": []}, {"text": "0 0.5 1 1.5 2 2.5 3202530 25050010002000 25050010002000 50010002000 Average Lagging ( seconds)BLEULA-2 SP-2 Hold-6", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "Offline Figure 1 : Quality - latency trade - off of different chunk sizes combined with different stable hypothesis detection strategies .", "entities": []}, {"text": "The number next to the marks indicates chunk size in milliseconds .", "entities": []}, {"text": "5.2 Stable Hypothesis Detection Strategies We experiment with three strategies : hold- n(withholds last ntokens ) , shared prefix ( SP- n ; finds the longest common prefix of all beams in nconsecutive chunks ) and local agreement ( LA- n ; finds the longest common prefix of the best hypothesis in nconsecutive chunks ) .", "entities": []}, {"text": "For hold- n , we selectn= 3,6,12 ; for SP- n , we select n= 1,2 ( n= 1corresponds to the strategy by Nguyen et al .", "entities": []}, {"text": "( 2021 ) )", "entities": []}, {"text": "; for LA- nwe select n= 2,3,4(n= 2280", "entities": []}, {"text": "corresponds to the strategy by Liu et al . ( 2020a ) ) .", "entities": []}, {"text": "The results are in Figures 2 and 3 . 0 0.5 1 1.5 2 2.5 315202530 3612 3612 3612 Average Lagging ( seconds)BLEU500 ms 1000 ms 2000 ms Offline Figure 2 : Quality - latency trade - off of hold- nstrategy with different values of n.", "entities": [[9, 10, "DatasetName", "0"]]}, {"text": "The number next to the marks indicates n. Colored lines connect results with equal chunk size .", "entities": []}, {"text": "Hold- nThe results suggest ( see Figure 2 ) that the hold- nstrategy can use either nor chunk size to control the quality - latency trade - off with equal effect .", "entities": []}, {"text": "The only exception seems to be too low n < = 3 , which slightly underperforms the options with higher nand shorter chunk size .", "entities": []}, {"text": "Local agreement ( LA- n)The local agreement seems to outperform all other strategies ( see Figure 3 ) .", "entities": []}, {"text": "LA- nfor all nfollows the same qualitylatency trade - off line .", "entities": []}, {"text": "The advantage of LA-2 is in reduced computational complexity compared to the other LA- nstrategies with n > 2 .", "entities": []}, {"text": "Shared prefix ( SP- n)SP-1 strongly underperforms other strategies in quality ( see Figure 3 ) .", "entities": []}, {"text": "While the SP-1 strategy performs well in the ASR task ( Nguyen et al . , 2021 ) , it is probably too lax for the speech translation task .", "entities": []}, {"text": "The generalized and more conservative SP-2 performs much better .", "entities": []}, {"text": "Although , the more relaxed LA-2 , which considers only the best item in the beam , has a better qualitylatency trade - off curve than the more conservative SP-2 .", "entities": []}, {"text": "5.3 Initial Wait As we could see in Section 5.1 , the shorter chunk sizes tend to perform worse .", "entities": []}, {"text": "One of the reasons might be the limited context of the early chunks.6 To increase the early context , we prolong the first chunk to 2 seconds .", "entities": []}, {"text": "The results are in Table 1 .", "entities": []}, {"text": "We see a slight ( 0.3 BLEU ) increase in quality for a chunk size of 250 6If we translated a non - pre - segmented input , this problem would be limited only onetime to the beginning of the input .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "Initial wait Chunk size BLEU AL AP DAL 0250 16.34 -35.97 0.66 1435.06 500 25.40 727.55 0.73 1791.21 1000 30.29 1660.59 0.83 2662.18 2000250 16.60 358.35 0.74 2121.54 500 25.42 952.15 0.77 2142.53 1000 30.29 1654.77 0.83 2657.48 Table 1 : Quality - latency trade - off of the LA-2 strategy with and without the initial wait .", "entities": [[4, 5, "MetricName", "BLEU"], [6, 7, "DatasetName", "AP"]]}, {"text": "ms , though the initial wait does not improve the BLEU and a considerable increase in the latency .", "entities": [[10, 11, "MetricName", "BLEU"]]}, {"text": "The performance seems to be influenced mainly by the chunk size .", "entities": []}, {"text": "The reason for smaller chunks \u2019 under - performance might be caused by ( 1 ) acoustic uncertainty towards the end of a chunk ( e.g. , words often get cut in the middle ) , or ( 2 ) insufficient information difference between two consecutive chunks .", "entities": []}, {"text": "This is supported by the observation in Figure 3 .", "entities": []}, {"text": "Increasing the number of consecutive chunks ( i.e. , increasing the context for the decision ) considered in the local agreement strategy ( LA- 2,3,4 ) , improves the quality , while it adds latency .", "entities": []}, {"text": "5.4 Negative Average Lagging Interestingly , we noticed that some of the strategies achieved negative average lagging ( e.g. , LA-2 in Section 5.1 ) with a chunk size of 250 ms has AL of -36 ms ) .", "entities": []}, {"text": "After a closer examination of the outputs , we found that the negative AL is in utterances where the hypothesis is significantly longer than the reference .", "entities": []}, {"text": "Recall the average latency for speech input defined by Ma et al .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "( 2020 ): ALspeech = 1 \u03c4\u2032(|X|)\u03c4\u2032(|X|)/summationdisplay i=1di\u2212d\u2217 i , ( 4 ) where di=/summationtextj k=1Tk , jis the index of raw audio segment that has been read when generating yi , Tkis duration of raw audio segment , \u03c4\u2032(|X| )", "entities": []}, {"text": "=", "entities": []}, {"text": "min{i|di=/summationtext|X| j=1Tj}andd\u2217 iare the delays of an ideal policy : d\u2217 i= ( i\u22121)\u00d7|X|/summationdisplay j=1Tj/|Y\u2217| , ( 5 ) whereY\u2217is reference translation .", "entities": []}, {"text": "If the hypothesis is longer than the reference , thend\u2217 i > di , making the sum argument in Equation ( 4 ) negative .", "entities": []}, {"text": "On the other hand , if we use the length of the hypothesis instead , then a shorter281", "entities": []}, {"text": "0 0.5 1 1.5 2 2.5 315202530 2505001000200025050010002000 25050010002000 2505001000 250500 3612 Average Lagging ( seconds)BLEUSP-1 SP-2 LA-2 LA-3 LA-4 Hold- n500 Offline Figure 3 : Quality - latency trade - off of shared prefix ( SP- n ) and local agreement ( LA- n ) with different nand chunk size .", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "hypothesis would benefit.7We , therefore , suggest using the maximum of both to prevent the advantage of either a shorter or a longer hypothesis : d\u2217 i= ( i\u22121)\u00d7|X|/summationdisplay j=1Tj / max ( |Y|,|Y\u2217|).(6 ) 6 Submitted System In this section , we describe the submitted system .", "entities": []}, {"text": "We follow the allowed training data and pretrained models and therefore our submission is constrained ( see Section 4.2.1 for model description ) .", "entities": []}, {"text": "For stable hypothesis detection , we decided to use the local agreement strategy with n= 2 .", "entities": []}, {"text": "As shown in Section 5.2 , the LA-2 has the best latencyquality trade - off along with other LA- nstrategies .", "entities": []}, {"text": "To achieve the different latency regimes , we use various chunk sizes , depending on the language pair .", "entities": []}, {"text": "We decided not to use larger n > 2to control the latency , as it increases the computation complexity while having the same effect as using a different chunk size .", "entities": []}, {"text": "The results on MuST - C tst - COMMON are in Table 2 .", "entities": [[3, 6, "DatasetName", "MuST - C"]]}, {"text": "The quality - latency trade - off is in Figure 4 .", "entities": []}, {"text": "From Table 2 and Figure 4 , we can see that the proposed method works well on two different models and various language pairs .", "entities": []}, {"text": "We see that an improvement in the offline model ( offline BLEU of 31.36 and 33.14 for Model A and B , respectively ) leads to improvement in the online regime .", "entities": [[11, 12, "MetricName", "BLEU"]]}, {"text": "7Ma et al .", "entities": []}, {"text": "( 2019 ) originally used the hypothesis length in the Equation ( 5 ) and then Ma et al .", "entities": []}, {"text": "( 2020 ) suggested to use the reference length instead.1 1.5 2 2.5 3 3.5283032 Average Lagging ( seconds)BLEU Model A Model B Best IWSLT21 Figure 4 : Quality - latency trade - off on English - German tst - COMMON of our two models : a dedicated EnglishGerman model trained from scratch ( Model A ) and a multilingual model based on wav2vec and mBART ( Model B ) .", "entities": [[65, 66, "MethodName", "mBART"]]}, {"text": "We also include the best IWSLT 2021 system ( USTC - NELSLIP ( Liu et al . , 2021 ) ) .", "entities": []}, {"text": "Finally , we see that our method beats the best IWSLT 2021 system ( USTC - NELSLIP ( Liu et al . , 2021 ) ) in medium and high latency regimes using both models ( i.e. , a model trained from scratch and a model based on pretrained wav2vec and mBART ) , and is almost on par in the low latency regime ( Model A is losing 0.35 BLEU and Model B is losing 0.47 BLEU ) .", "entities": [[51, 52, "MethodName", "mBART"], [70, 71, "MetricName", "BLEU"], [77, 78, "MetricName", "BLEU"]]}, {"text": "6.1 Computationally Aware Latency In this paper , we do not report any computationally aware metrics , as our implementation of Transformers is slow .", "entities": []}, {"text": "Later , we implemented the same onlinization approach using wav2vec 2.0 and mBART from Huggingface Transformers ( Wolf et al . , 2020 ) .", "entities": [[12, 13, "MethodName", "mBART"]]}, {"text": "The new implementation reaches faster than realtime inference speed.282", "entities": []}, {"text": "Model Language pair Latency regime Chunk size BLEU AL AP DAL Best IWSLT21 system En - DeLow - 27.40 920 0.68 1420 Medium - 29.68 1860 0.82 2650 High - 30.75 2740 0.90 3630 Model A En - DeLow 600 27.05 947 0.76 1993", "entities": [[7, 8, "MetricName", "BLEU"], [9, 10, "DatasetName", "AP"]]}, {"text": "Medium 1000 30.30 1660 0.84 2662 High 2000 31.41 2966 0.93 3853 Offline - 31.36 5794 1.00 5794 Model BEn - DeLow 500 26.93 945 0.77 2052 Medium 1000 31.60 1906 0.86 2945 High 2500 32.98 3663 0.96 4452 Offline - 33.14 5794 1.00 5794", "entities": []}, {"text": "En - JaLow 1000 16.84 2452 0.90 3212 Medium 2400 16.99 3791 0.97 4296 High 3000 16.97 4140 0.98 4536 Offline - 16.88 5119 1.00 5119 En - ZhLow 800 23.69 1761 0.85 2561 Medium 1500 24.29 2788 0.93 3500 High 2500 24.56 3669 0.97 4212 Offline - 24.54 5119 1.00 5119 Table 2 : Results of the older model used for the experiments ( Model A ) and the submitted system ( Model B ) on the MuST - C v2 tst - COMMON .", "entities": [[78, 81, "DatasetName", "MuST - C"]]}, {"text": "We also include the best IWSLT 2021 system ( USTC - NELSLIP ( Liu et al . , 2021 ) ) .", "entities": []}, {"text": "7 Conclusion In this paper , we reviewed onlinization strategies for end - to - end speech translation models .", "entities": []}, {"text": "We identified the optimal stable hypothesis detection strategy and proposed two separate ways of the qualitylatency trade - off parametrization .", "entities": []}, {"text": "We showed that the onlinization of the offline models is easy and performs almost on par with the offline run .", "entities": []}, {"text": "We demonstrated that an improvement in the offline model leads to improved online performance .", "entities": []}, {"text": "We also showed that our method outperforms a dedicated simultaneous system .", "entities": []}, {"text": "Finally , we proposed an improvement in the average latency metric .", "entities": []}, {"text": "Acknowledgments This work has received support from the project \u201c Grant Schemes at CU \u201d ( reg .", "entities": []}, {"text": "no . CZ.02.2.69/0.0/0.0/19_073/0016935 ) , the grant 1926934X ( NEUREM3 ) of the Czech Science Foundation , the European Union \u2019s Horizon 2020 Research and Innovation Programme under Grant Agreement No 825460 ( ELITR ) , and partly supported by a Facebook Sponsored Research Agreement \u201c Language Similarity in Machine Translation \u201d .", "entities": [[49, 51, "TaskName", "Machine Translation"]]}, {"text": "References Antonios Anastasopoulos , Luisa Bentivogli , Marcely Z. Boito , Ond \u02c7rej Bojar , Roldano Cattoni , Anna Currey , Georgiana Dinu , Kevin Duh , Maha Elbayad , Marcello Federico , Christian Federmann , Hongyu Gong , Roman Grundkiewicz , Barry Haddow , Benjamin Hsu , D\u00e1vid Javorsk\u00fd , V \u02c7era Kloudov\u00e1 , Surafel M. Lakew , Xutai Ma , Prashant Mathur , Paul McNamee , Kenton Murray , Maria N \u02d8adejde , Satoshi Nakamura , Matteo Negri , Jan Niehues , Xing Niu , Juan Pino , Elizabeth Salesky , Jiatong Shi , Sebastian St\u00fcker , Katsuhito Sudoh , Marco Turchi , Yogesh Virkar , Alex Waibel , Changhan Wang , and Shinji Watanabe . 2022 .", "entities": []}, {"text": "FINDINGS OF THE IWSLT 2022 EV ALUATION CAMPAIGN .", "entities": []}, {"text": "In Proceedings of the 19th International Conference on Spoken Language Translation ( IWSLT 2022 ) , Dublin , Ireland .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Antonios Anastasopoulos , Ond \u02c7rej Bojar , Jacob Bremerman , Roldano Cattoni , Maha Elbayad , Marcello Federico , Xutai Ma , Satoshi Nakamura , Matteo Negri , Jan Niehues , Juan Pino , Elizabeth Salesky , Sebastian St\u00fcker , Katsuhito Sudoh , Marco Turchi , Alexander Waibel , Changhan Wang , and Matthew Wiesner . 2021 .", "entities": []}, {"text": "FINDINGS OF THE IWSLT 2021 EV ALUATION CAMPAIGN .", "entities": []}, {"text": "In Proceedings of the 18th International Conference on Spoken Language Translation ( IWSLT 2021 ) , pages 1\u201329 , Bangkok , Thailand ( online ) .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ebrahim Ansari , Amittai Axelrod , Nguyen Bach , Ond\u02c7rej Bojar , Roldano Cattoni , Fahim Dalvi , Nadir Durrani , Marcello Federico , Christian Federmann , Jiatao Gu , Fei Huang , Kevin Knight , Xutai Ma , Ajay Nagesh , Matteo Negri , Jan Niehues , Juan Pino , Elizabeth Salesky , Xing Shi , Sebastian St\u00fcker , Marco Turchi , Alexander Waibel , and Changhan Wang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "FINDINGS OF THE IWSLT 2020 EV ALUATION CAMPAIGN .", "entities": []}, {"text": "In Proceedings of the 17th International Conference on Spoken Language Translation , pages 1\u201334 , Online .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alexei Baevski , Yuhao Zhou , Abdelrahman Mohamed,283", "entities": []}, {"text": "and Michael Auli .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "wav2vec 2.0 : A framework for self - supervised learning of speech representations .", "entities": [[6, 10, "TaskName", "self - supervised learning"]]}, {"text": "Advances in Neural Information Processing Systems , 33:12449\u201312460 .", "entities": []}, {"text": "Parnia Bahar , Patrick Wilken , Mattia A. Di Gangi , and Evgeny Matusov . 2021 .", "entities": []}, {"text": "Without further ado : Direct and simultaneous speech translation by AppTek in 2021 .", "entities": []}, {"text": "In Proceedings of the 18th International Conference on Spoken Language Translation ( IWSLT 2021 ) , pages 52\u201363 , Bangkok , Thailand ( online ) .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Roldano Cattoni , Mattia Antonino Di Gangi , Luisa Bentivogli , Matteo Negri , and Marco Turchi .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Mustc : A multilingual corpus for end - to - end speech translation .", "entities": []}, {"text": "Computer Speech & Language , 66:101155 .", "entities": []}, {"text": "Colin Cherry and George Foster .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Thinking slow about latency evaluation for simultaneous machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1906.00048 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kyunghyun Cho and Masha Esipova .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Can neural machine translation do simultaneous translation ?", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1606.02012 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zihang Dai , Zhilin Yang , Yiming Yang , Jaime Carbonell , Quoc Le , and Ruslan Salakhutdinov .", "entities": [[16, 17, "DatasetName", "Ruslan"]]}, {"text": "2019 .", "entities": []}, {"text": "Transformer - XL : Attentive language models beyond a fixed - length context .", "entities": [[0, 3, "MethodName", "Transformer - XL"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ( ACL ) .", "entities": []}, {"text": "Maha Elbayad , Ha Nguyen , Fethi Bougares , Natalia Tomashenko , Antoine Caubri\u00e8re , Benjamin Lecouteux , Yannick Est\u00e8ve , and Laurent Besacier . 2020 .", "entities": []}, {"text": "ON - TRAC consortium for end - to - end and simultaneous speech translation challenge tasks at IWSLT 2020 .", "entities": []}, {"text": "In Proceedings of the 17th International Conference on Spoken Language Translation , pages 35 \u2013 43 , Online .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alex Graves . 2012 .", "entities": []}, {"text": "Sequence transduction with recurrent neural networks .", "entities": []}, {"text": "arXiv preprint arXiv:1211.3711 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Thanh - Le Ha , Jan Niehues , and Alexander Waibel .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Toward multilingual neural machine translation with universal encoder and decoder .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 13th International Workshop on Spoken Language Translation ( IWSLT 2016 ) , Seattle , USA .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Hou Jeung Han , Mohd Abbas Zaidi , Sathish Reddy Indurthi , Nikhil Kumar Lakumarapu , Beomseok Lee , and Sangha Kim .", "entities": [[13, 14, "DatasetName", "Kumar"]]}, {"text": "2020 .", "entities": []}, {"text": "End - to - end simultaneous translation system for IWSLT2020 using modality agnostic meta - learning .", "entities": [[13, 16, "TaskName", "meta - learning"]]}, {"text": "In Proceedings of the 17th International Conference on Spoken Language Translation , pages 62\u201368 , Online .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sathish Indurthi , Houjeung Han , Nikhil Kumar Lakumarapu , Beomseok Lee , Insoo Chung , Sangha Kim , and Chanwoo Kim .", "entities": [[7, 8, "DatasetName", "Kumar"]]}, {"text": "2020 .", "entities": []}, {"text": "End - end speech - to - text translation with modality agnostic meta - learning .", "entities": [[3, 9, "TaskName", "speech - to - text translation"], [12, 15, "TaskName", "meta - learning"]]}, {"text": "InICASSP 2020 - 2020 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pages 7904\u20137908 .", "entities": []}, {"text": "Muntsin Kolss , Stephan V ogel , and Alex Waibel . 2008 .", "entities": []}, {"text": "Stream decoding for simultaneous spoken language translation .", "entities": []}, {"text": "In Ninth Annual Conference of the International Speech Communication Association .", "entities": []}, {"text": "Stephen C Levinson and Francisco Torreira .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Timing in turn - taking and its implications for processing models of language .", "entities": []}, {"text": "Frontiers in psychology , 6:731 .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Ves Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bart : Denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension .", "entities": [[2, 3, "TaskName", "Denoising"]]}, {"text": "arXiv preprint arXiv:1910.13461 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Dan Liu , Mengge Du , Xiaoxi Li , Yuchen Hu , and Lirong Dai . 2021 .", "entities": []}, {"text": "The USTC - NELSLIP systems for simultaneous speech translation task at IWSLT 2021 .", "entities": []}, {"text": "In Proceedings of the 18th International Conference on Spoken Language Translation ( IWSLT 2021 ) , pages 30\u201338 , Bangkok , Thailand ( online ) .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Danni Liu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Low - latency end - to - end speech recognition with enhanced readability .", "entities": [[8, 10, "TaskName", "speech recognition"]]}, {"text": "Master \u2019s thesis , Maastricht University .", "entities": []}, {"text": "Danni Liu , Gerasimos Spanakis , and Jan Niehues . 2020a .", "entities": []}, {"text": "Low - Latency Sequence - to - Sequence Speech Recognition and Translation by Partial Hypothesis Selection .", "entities": [[8, 10, "TaskName", "Speech Recognition"], [11, 12, "TaskName", "Translation"]]}, {"text": "In Proc .", "entities": []}, {"text": "Interspeech 2020 , pages 3620 \u2013 3624 .", "entities": []}, {"text": "Yinhan Liu , Jiatao Gu , Naman Goyal , Xian Li , Sergey Edunov , Marjan Ghazvininejad , Mike Lewis , and Luke Zettlemoyer .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Multilingual denoising pre - training for neural machine translation .", "entities": [[1, 2, "TaskName", "denoising"], [7, 9, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 8:726\u2013742 .", "entities": []}, {"text": "Mingbo Ma , Liang Huang , Hao Xiong , Renjie Zheng , Kaibo Liu , Baigong Zheng , Chuanqiang Zhang , Zhongjun He , Hairong Liu , Xing Li , et al . 2019 .", "entities": []}, {"text": "Stacl :", "entities": []}, {"text": "Simultaneous translation with implicit anticipation and controllable latency using prefix - to - prefix framework .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3025\u20133036 .", "entities": []}, {"text": "Xutai Ma , Mohammad Javad Dousti , Changhan Wang , Jiatao Gu , and Juan Pino . 2020 .", "entities": []}, {"text": "Simuleval : An evaluation toolkit for simultaneous translation .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 144\u2013150 .", "entities": []}, {"text": "Thai - Son Nguyen , Sebastian St\u00fcker , and Alex Waibel . 2021 .", "entities": []}, {"text": "Super - Human Performance in Online LowLatency Recognition of Conversational Speech .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "Interspeech 2021 , pages 1762\u20131766.284", "entities": []}, {"text": "Jan Niehues , Thai Son Nguyen , Eunah Cho , Thanh - Le Ha , Kevin Kilgour , Markus M\u00fcller , Matthias Sperber , Sebastian St\u00fcker , and Alex Waibel . 2016 .", "entities": []}, {"text": "Dynamic Transcription for Low - Latency Speech Translation .", "entities": [[7, 8, "TaskName", "Translation"]]}, {"text": "InProc .", "entities": []}, {"text": "Interspeech 2016 , pages 2513\u20132517 .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics , pages 311\u2013318 .", "entities": []}, {"text": "Ngoc - Quan Pham , Thanh - Le Ha , Tuan - Nam Nguyen , Thai - Son Nguyen , Elizabeth Salesky , Sebastian St\u00fcker , Jan Niehues , and Alex Waibel . 2020a .", "entities": []}, {"text": "Relative Positional Encoding for Speech Recognition and Direct Translation .", "entities": [[4, 6, "TaskName", "Speech Recognition"], [8, 9, "TaskName", "Translation"]]}, {"text": "In Proc .", "entities": []}, {"text": "Interspeech 2020 , pages 31\u201335 .", "entities": []}, {"text": "Ngoc - Quan Pham , Tuan - Nam Nguyen , Thai - Binh Nguyen , Danni Liu , Carlos Mullov , Jan Niehues , and Alexander Waibel . 2022 .", "entities": []}, {"text": "Effective combination of pretrained models - KIT@IWSLT2022 .", "entities": []}, {"text": "In Proceedings of the 19th International Conference on Spoken Language Translation ( IWSLT 2022 ) , Dublin , Ireland .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ngoc - Quan Pham , Felix Schneider , Tuan Nam Nguyen , Thanh - Le Ha , Thai - Son Nguyen , Maximilian Awiszus , Sebastian St\u00fcker , and Alex Waibel .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Kit \u2019s iwslt 2020 slt translation system .", "entities": []}, {"text": "In Proceedings of the 17th International Conference on Spoken Language Translation , pages 55\u201361 .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Matt Post . 2018 .", "entities": []}, {"text": "A call for clarity in reporting bleu scores .", "entities": [[6, 7, "MetricName", "bleu"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 186 \u2013 191 . Yuqing Tang , Chau Tran , Xian Li , Peng - Jen Chen , Naman Goyal , Vishrav Chaudhary , Jiatao Gu , and Angela Fan .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "2020 .", "entities": []}, {"text": "Multilingual translation with extensible multilingual pretraining and finetuning .", "entities": []}, {"text": "arXiv preprint arXiv:2008.00401 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 5998\u20136008 .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , R\u00e9mi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander M. Rush .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 38\u201345 , Online .", "entities": []}, {"text": "Association for Computational Linguistics.285", "entities": []}]