[{"text": "Proceedings of SemEval-2016 , pages 394\u2013400 , San Diego , California , June 16 - 17 , 2016 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2016 Association for Computational Linguistics IUCL at SemEval-2016 Task 6 : An Ensemble Model for Stance Detection in Twitter Can Liu , Wen Li , Bradford Demarest , Yue Chen , Sara Couture , Daniel Dakota , Nikita Haduong , Noah Kaufman , Andrew Lamont , Manan Pancholi , Kenneth Steimel , Sandra K \u00a8ubler Indiana University { liucan , wl9,bdemares , yc59,scouture , ddakota , nhaduong , nokaufma , alamont , mpanchol , ksteimel , skuebler } @indiana.edu", "entities": [[15, 17, "TaskName", "Stance Detection"]]}, {"text": "Abstract We present the IUCL system , based on supervised learning , for the shared task on stance detection .", "entities": [[17, 19, "TaskName", "stance detection"]]}, {"text": "Our of\ufb01cial submission , the random forest model , reaches a score of 63.60 , and is ranked 6th out of 19 teams .", "entities": []}, {"text": "We also use gradient boosting decision trees and SVM and merge all classi\ufb01ers into an ensemble method .", "entities": [[8, 9, "MethodName", "SVM"]]}, {"text": "Our analysis shows that random forest is good at retrieving minority classes and gradient boosting majority classes .", "entities": []}, {"text": "The strengths of different classi\ufb01ers wrt . precision and recall complement each other in the ensemble .", "entities": []}, {"text": "1 Introduction Stance detection is a dif\ufb01cult task since it often requires reasoning in order to determine whether an utterance is in favor of or against a speci\ufb01c issue .", "entities": [[2, 4, "TaskName", "Stance detection"]]}, {"text": "In the shared task ( see Mohammad et al .", "entities": []}, {"text": "( 2016 ) for details about the shared task ) , we interpret it as a variant of sentiment analysis and adopt an approach that combines shallow lexical features with an ensemble of different supervised machine learning classi\ufb01ers .", "entities": [[18, 20, "TaskName", "sentiment analysis"]]}, {"text": "Previous work has shown that using \u201c arguing \u201d features based on an arguing lexicon along with modal verbs and targets identi\ufb01ed via syntactic rules ( Somasundaran and Wiebe , 2010 ) ; \ufb01nding polarized relations between aspects and topics ( Somasundaran and Wiebe , 2009 ) ; adding semantic frames ( Hasan and Ng , 2013 ) and contextual features ( Anand et al . , 2011 ) generally improve results .", "entities": []}, {"text": "Since some of these features do not generalize across targets ( Anand et al . , 2011 ) , and since we have an additional challenge in processing Twitter data , we rely on unigram features and word vectors .", "entities": []}, {"text": "This means that ourapproach is incapable of handling sarcasm or humor .", "entities": []}, {"text": "Instead , it provides a robust basis on which we can later add more informative features .", "entities": []}, {"text": "Our approach consists of classi\ufb01ers with a bag of words ( unigrams ) or with word vectors as features .", "entities": []}, {"text": "We use three separate classi\ufb01ers ( SVMs , random forest , gradient boosting decision trees ) and an ensemble classi\ufb01er ( TiMBL ) .", "entities": []}, {"text": "Our of\ufb01cial submission is the random forest classi\ufb01er with word unigrams .", "entities": []}, {"text": "2 Methods We use the data sets provided by the SemEval-2016 shared task 6 ( Mohammad et al . , 2016 ) .", "entities": []}, {"text": "2.1 Preprocessing Preprocessing mostly consists of tokenization .", "entities": []}, {"text": "During tokenization , we normalize capitalization , and all punctuation signs are separated except for @ and # , as these symbols indicate hashtags and handles .", "entities": []}, {"text": "We extract frequency counts of each token in the entire corpus and in each stance ( Favor , Against , None ) per target for use in the feature selection process .", "entities": [[28, 30, "MethodName", "feature selection"]]}, {"text": "We experimented with TWEEBOPARSER ( Kong et al . , 2014 ) , a dependency parser speci\ufb01cally designed for Twitter data , to extract dependency relations among words .", "entities": []}, {"text": "We extract POS tags , multiword expressions , and dependency triples from the parses .", "entities": []}, {"text": "However , due to the feature sparsity , none of them improved over unigrams .", "entities": []}, {"text": "Thus , they are not used in the \ufb01nal systems .", "entities": []}, {"text": "2.2 Features One of the major decisions in developing a machine learning system for stance detection lies in394", "entities": [[14, 16, "TaskName", "stance detection"]]}, {"text": "Model Features GBDT GloVe word vectors random forest unigrams + IG SVM unigrams + IG ensembleG three classi\ufb01ers + global ensembleNG three classi\ufb01ers only Table 1 : Summary of features for each model .", "entities": [[3, 4, "MethodName", "GloVe"], [11, 12, "MethodName", "SVM"]]}, {"text": "The random forest model constitutes our of\ufb01cial submission .", "entities": []}, {"text": "the choice of features and of feature representations .", "entities": []}, {"text": "Detecting stance in political tweets can be regarded as a form of sentiment analysis for short text , and we assume that different stances of tweets are partially expressed by the choice of words .", "entities": [[12, 14, "TaskName", "sentiment analysis"]]}, {"text": "For example , not mentioning any words that express a polarized attitude indicates that a tweet is most likely a None stance .", "entities": []}, {"text": "Tweets are relatively short documents , we use bag of words ( unigrams ) since in this case bigrams and trigrams are likely to be too sparse to be informative .", "entities": []}, {"text": "Another possibility would be to follow approaches in sentiment analysis and use sentiment lexicons .", "entities": [[8, 10, "TaskName", "sentiment analysis"]]}, {"text": "However , such lexicons are normally general purpose resources , and domain speci\ufb01c information is not included .", "entities": []}, {"text": "In contrast , we need such domain speci\ufb01c knowledge , for example to capture the fact that \u201c dear lord \u201d is an indication of a negative stance towards the target Atheism while it may have a different meaning when it occurs for the target Hillary Clinton .", "entities": []}, {"text": "Since unigrams include a high number of irrelevant features and also constitute a rather impoverished representation , we use feature selection as well as word vectors in our experiments .", "entities": [[19, 21, "MethodName", "feature selection"]]}, {"text": "Table 1 summarizes the features used for each of our models .", "entities": []}, {"text": "We use information gain ( IG ) for feature selection on unigrams .", "entities": [[8, 10, "MethodName", "feature selection"]]}, {"text": "Global refers to global features ( see section 2.2.3 ) .", "entities": []}, {"text": "The three classi\ufb01ers are GBDT , random forest , and SVM ; the ensemble uses their output ( predicted label and its probability ) .", "entities": [[10, 11, "MethodName", "SVM"]]}, {"text": "2.2.1 Feature Selection There are issues resulting from the large number of bag - of - words features : 1 ) Not all words are good indicators for stance ; some words occur evenly across the data set .", "entities": [[1, 3, "MethodName", "Feature Selection"]]}, {"text": "2 ) Rare words , which are less likely to occur in the test data , do not contribute much .", "entities": []}, {"text": "To alleviate these problems , we perform feature selection using information gain ( IG ) .", "entities": [[7, 9, "MethodName", "feature selection"]]}, {"text": "IG esti - mates the amount of information a word gives for the decision on the stance .", "entities": []}, {"text": "We choose IG because it has been shown to be robust across different sentiment analysis data sets and across different skewing ratios , compared to other feature selection methods ( Liu et al . , 2014 ) .", "entities": [[13, 15, "TaskName", "sentiment analysis"], [26, 28, "MethodName", "feature selection"]]}, {"text": "Note that different from its use in decision trees , we use IG as an external \ufb01lter to select a subset of features , before and independent of any classi\ufb01ers .", "entities": []}, {"text": "2.2.2", "entities": []}, {"text": "Word Vector Features One limitation of bag - of - words features is that they are very sparse , and they can not handle out - ofvocabulary words properly .", "entities": []}, {"text": "Since tweets are relatively short , and the amount of of\ufb01cial training data is small , it is likely that the out - of - vocabulary rate is high .", "entities": []}, {"text": "Thus we also build models using word vectors , which represent each word with a vector of continuous values .", "entities": []}, {"text": "Word vectors have been shown to capture the similarity among words and thus alleviate data sparseness ( Collobert et al . , 2011 ) .", "entities": []}, {"text": "We have experimented with two different word vector models , word2vec ( Mikolov et al . , 2013 ) and GloVe ( Pennington et al . , 2014 ) .", "entities": [[20, 21, "MethodName", "GloVe"]]}, {"text": "We have used the pre - trained word2vec obtained from the Google News dataset , which contains a 300 - dimensional vector representation for 3 million words and phrases1 , and the pre - trained GloVe , which is obtained from 2 billion tweets and has a 250dimensional vector representation for 1.2 million words and phrases2 .", "entities": [[11, 12, "DatasetName", "Google"], [35, 36, "MethodName", "GloVe"]]}, {"text": "To construct a representation for a tweet , we look up a word in the word vectors model , then average all vectors for words to produce a vector representation for the tweet .", "entities": []}, {"text": "For example , to represent a 15 word tweet using word2vec , we \ufb01rst obtain a 300dimensional vector for each word , then average all 15 vectors .", "entities": []}, {"text": "This means that the word order is lost and the representation constitutes a \u201c bag of vectors \u201d .", "entities": []}, {"text": "Comparing Word Vectors We have performed a comparison of both word vector variants in a 5 - fold cross validation experiment on the training data .", "entities": []}, {"text": "Table 2 summarizes the results .", "entities": []}, {"text": "We can see that GloVe performs consistently better than word2vec except for Feminist where word2vec is 0.6 % better than 1https://code.google.com/archive/p/word2vec/ 2http://nlp.stanford.edu/projects/glove/395", "entities": [[4, 5, "MethodName", "GloVe"]]}, {"text": "Target Word2vec GloVe Abortion 61.4 62.4 Atheism 62.6 66.4 Climate 69.9 71.1 Feminist 53.8 53.2 Hillary 59.5 61.1 Table 2 : Comparing word2vec and GloVe .", "entities": [[2, 3, "MethodName", "GloVe"], [24, 25, "MethodName", "GloVe"]]}, {"text": "GloVe .", "entities": [[0, 1, "MethodName", "GloVe"]]}, {"text": "We assume that this performance gap is mainly caused by the domain difference from which the word vectors are obtained : We used GloVe pretrained on tweets and word2vec pre - trained on news .", "entities": [[23, 24, "MethodName", "GloVe"]]}, {"text": "This leads to a higher number of out - of - vocabulary words for the word2vec model .", "entities": []}, {"text": "In other words , GloVe provides a broader coverage for this data set .", "entities": [[4, 5, "MethodName", "GloVe"]]}, {"text": "2.2.3 Global Features The bag - of - words features used in the classi\ufb01ers ( see section 3 ) assume that the words are considered independently .", "entities": []}, {"text": "However , in many situations , it is the distributions of positively and negatively oriented words that determine the \ufb01nal stance of a tweet .", "entities": []}, {"text": "A low coverage of words from these two distributions is a strong indicator for None stance as well .", "entities": []}, {"text": "This is especially important for the ensemble classi\ufb01er .", "entities": []}, {"text": "For this reason , we have developed two additional features for the ensemble , which capture information from these two distributions : one feature for positive orientation and one for negative orientation .", "entities": []}, {"text": "The feature is a numeric score , representing the association of a tweet with positive or negative stance respectively .", "entities": []}, {"text": "The positive orientation is calculated based on the following equation : scorepos T=1 |T|/summationdisplay", "entities": []}, {"text": "w\u2282Tfreq ( w)in POS / summationtext w / prime\u2282Vfreq ( w / prime)in POS where Tis a tweet , |T|is the tweet length excluding stop words .", "entities": []}, {"text": "Vis the entire vocabulary .", "entities": []}, {"text": "Freq ( w)is the frequency count of win the following set .", "entities": []}, {"text": "POS is the set of all positive tweets .", "entities": []}, {"text": "This score measures for each word ( its lemma ) the association with positive stance , sums up all words in the tweet , and normalizes the score by the tweet length .", "entities": [[8, 9, "DatasetName", "lemma"]]}, {"text": "The score for the negative orientation is calculated accordingly .", "entities": []}, {"text": "The None orientation is not calculated since it is already represented by the absence of positively ornegatively oriented words .", "entities": []}, {"text": "I.e. , we assume that if a tweet has low positive and negative orientations , it indicates a None stance .", "entities": []}, {"text": "2.3 Adding Manually Annotated Data We mined additional tweets for each of the \ufb01ve targets in Nov. 2015 by searching for hashtags relevant to the targets .", "entities": []}, {"text": "These tweets are not included in the \ufb01nal systems since they increased the class imbalance .", "entities": []}, {"text": "We will investigate better options for including the data in the future .", "entities": []}, {"text": "Hashtags for Abortion include # abortion , # abortionrights , and # prolife ; Atheism includes # atheism , # atheist , and # theist ; Climate includes # actionclimate and # climatechange ; Feminist includes # feminism , # feminist , # heforshe , and # womensrights ; and Hillary includes # HillaryClinton .", "entities": []}, {"text": "Tweets were then annotated for stance , following the guidelines used for the annotation of the of\ufb01cial shared task data3 .", "entities": []}, {"text": "Two annotators participated in the annotation process .", "entities": []}, {"text": "The number of additional tweets ranged between 260 and 2,400 per target .", "entities": []}, {"text": "3 Classi\ufb01ers Since there is little research on determining the best \ufb01tting bias for stance detection , we explore three different classi\ufb01ers for the stance classi\ufb01cation , support vector machines ( SVM ) , random forest , and gradient boosting decision trees ( GBDT ) .", "entities": [[14, 16, "TaskName", "stance detection"], [31, 32, "MethodName", "SVM"]]}, {"text": "For all three classi\ufb01ers , we use the implementations in ScikitLearn ( Pedregosa et al . , 2011 ) .", "entities": []}, {"text": "We choose SVM because it is the most widely used machine learning model for text classi\ufb01cation and sentiment analysis ( e.g. , ( Pil \u00b4 aszy , 2005 ) ) .", "entities": [[2, 3, "MethodName", "SVM"], [17, 19, "TaskName", "sentiment analysis"]]}, {"text": "Additionally , it has been shown to be robust with high dimensional features ( e.g. , ( Joachims , 1998 ) ) .", "entities": []}, {"text": "Random forest is adopted because of its capability of reducing over\ufb01tting by performing sampling on data points and on feature subspaces .", "entities": []}, {"text": "GBDT is selected because it works well with continuous numerical features such as word vectors .", "entities": []}, {"text": "We train individual classi\ufb01ers for each target .", "entities": []}, {"text": "Parameters are optimized in a 5 - fold cross - validation over the training data .", "entities": []}, {"text": "SVM and random forest are trained on different numbers of selected unigrams 3See http://alt.qcri.org/semeval2016/ task6 / data / uploads / stance - question.pdf .396", "entities": [[0, 1, "MethodName", "SVM"]]}, {"text": "for each target : 1,700 for Abortion , 1,535 for Atheism , 1,381 for Climate , 1,749 for Feminist , and 1,704 for Hillary .", "entities": []}, {"text": "GBDT is trained on the word vectors : 300 dimensions for word2vec and 250 dimensions for GloVe .", "entities": [[16, 17, "MethodName", "GloVe"]]}, {"text": "Additional experiments are performed with a standard feed - forward neural network on word vectors .", "entities": []}, {"text": "These showed better performance on the training set for some targets , but overall , GBDT prove to be more reliable .", "entities": []}, {"text": "SVM Our initial experiments using cross validation on training data showed that linear kernel performed better than non - linear ones , and that the LinearSVC implementation ( one - vs - rest strategy for multi - class ) outperformed SVC ( one - vs - one strategy ) .", "entities": [[0, 1, "MethodName", "SVM"]]}, {"text": "The optimal parameters differ for each target : 0.015 - 0.3 for the slack variable ; standard hinge or squared hinge for the loss function ; and L2 norm for the penalty term .", "entities": [[23, 24, "MetricName", "loss"]]}, {"text": "Random Forest", "entities": []}, {"text": "The parameters for random forest are : 50 , 70 , or 90 for the number of trees ; 500 or All for the number of features to consider when looking for the best split ; 200 , 500 , or unlimited for the maximum depth of trees .", "entities": [[44, 46, "HyperparameterName", "maximum depth"]]}, {"text": "GBDT", "entities": []}, {"text": "The gradient boosting decision trees ( GBDT ) classi\ufb01er is used in combination with word vector features .", "entities": []}, {"text": "Our initial experiments showed that GBDT handles word vector features better than SVM and random forest .", "entities": [[12, 13, "MethodName", "SVM"]]}, {"text": "The optimal parameter range for different targets are : 80 - 100 for number of estimators ; 0.05 - 0.3 for learning rate ; false for warm start ; and 0.5 - 1.0 for subsample ratio .", "entities": [[13, 16, "HyperparameterName", "number of estimators"], [21, 23, "HyperparameterName", "learning rate"]]}, {"text": "Ensemble Classi\ufb01er", "entities": []}, {"text": "Since initial experiments with the three classi\ufb01ers showed considerable differences across targets and stances , we investigate whether an ensemble classi\ufb01er would bene\ufb01t from aggregating their predictions .", "entities": []}, {"text": "For the ensemble classi\ufb01er , we choose a memory - based learner , TiMBL , because of the need to operate on a small set of rather abstract features : stance predictions and con\ufb01dence scores from the three classi\ufb01ers along with the global features ( see section 2.2.3 ) .", "entities": []}, {"text": "We use TiMBL ( Daelemans et al . , 2009 ) version 6.4.2 , and perform 5 - fold jackkni\ufb01ng to generate the training set for this ensemble classi\ufb01er .", "entities": []}, {"text": "Parameter optimization is performed on the \ufb01ve folds .", "entities": []}, {"text": "The best parameters are different in each target : 7 - 29Team Of\ufb01cial Metric MITRE 67.82 IUCL - RF 63.60 Table 3 : Of\ufb01cial results of the IUCL - RF system in comparison to the best system .", "entities": []}, {"text": "Model Of\ufb01cial Metric GBDT 64.64 Random Forest 63.60 SVM 61.93 EnsembleG 62.46 EnsembleNG 66.14 Table 4 : Overall comparison of all IUCL systems .", "entities": [[8, 9, "MethodName", "SVM"]]}, {"text": "The best accuracy of an individual classi\ufb01er is shown in italics , the best overall result in bold .", "entities": [[2, 3, "MetricName", "accuracy"]]}, {"text": "for the number of neighbors ; default minority voting for class voting in most cases ; Modi\ufb01ed Value Distance , Jeffrey divergence , and cosine distance for distance metric ; and gain ratio for feature weight in most cases .", "entities": [[27, 29, "HyperparameterName", "distance metric"]]}, {"text": "4 Results 4.1 Of\ufb01cial Result Since the ensemble classi\ufb01er was not completed in time for submission , we had to decide which individual classi\ufb01er to submit .", "entities": []}, {"text": "The random forest model is selected based on a \ufb01ve - fold cross validation on the training set .", "entities": []}, {"text": "This system reaches a score of 63.60 ( macro - averaged F ) , as shown in table 3 , the sixth best result out of 19 participating systems .", "entities": []}, {"text": "This result is approximately 4 percent points lower than that of the highest performing system .", "entities": []}, {"text": "4.2 Additional Results 4.2.1 Overview of All Classi\ufb01ers Table 4 shows the results of the three individual classi\ufb01ers as well as of the two ensemble model variants , one combining only the individual classi\ufb01ers \u2019 outputs ( EnsembleNG ) , the other one ( EnsembleG ) including also the global features ( see section 2.2.3 ) .", "entities": []}, {"text": "These results show that the GBDT approach using GloVe reaches the highest result ( 64.64 ) among the individual classi\ufb01ers .", "entities": [[8, 9, "MethodName", "GloVe"]]}, {"text": "The random forest classi\ufb01er , which constitutes our of\ufb01cial submission is about 1 percentage point lower ( 63.60 ) , and the397", "entities": []}, {"text": "Abortion Atheism Favor Against None Favor Against None Model Acc F", "entities": [[9, 10, "MetricName", "Acc"]]}, {"text": "Prec Rec Prec Rec Prec Rec Acc FPrec Rec Prec Rec Prec Rec GBDT 65.0 53.6 52.6 21.7 75.6 77.2 38.2 57.8 67.3 56.4 37.0 31.2 82.3 75.6 37.0 60.7 RF 65.0 57.6 43.6 37.0 83.8 68.3 41.4 80.0 70.5 57.9 45.0 28.1 81.2 81.2 40.0 57.1 SVM 60.7 58.6 43.6 52.2 81.6 60.8 36.9 68.9 59.1 51.9 26.1 37.5 81.5 66.2 27.0 42.9 EnsembleG 62.9 46.3 55.6 10.9 75.1 73.5 37.2 71.1 69.1 45.9 50.0 6.2 76.1 85.6 36.1 46.4 EnsembleNG 66.8 60.2 50.0 39.1 80.2 73.0 43.1 68.9 69.1 50.6 57.1", "entities": [[6, 7, "MetricName", "Acc"], [47, 48, "MethodName", "SVM"]]}, {"text": "12.5 74.0 88.7 28.6 21.4 Climate Feminist Favor Against None Favor Against None Model Acc F", "entities": [[14, 15, "MetricName", "Acc"]]}, {"text": "Prec Rec Prec Rec Prec Rec Acc FPrec Rec Prec Rec Prec Rec GBDT 72.8 41.8 82.0 85.4 0.00 0.00 43.9 51.4 57.9 51.6 30.3 34.5 69.3 72.7 44.4 27.3 RF 68.0 39.1 82.7 74.0 0.00 0.00 40.7 68.6 57.2 51.1 31.1 39.7 73.9 61.7 46.6 61.4", "entities": [[6, 7, "MetricName", "Acc"]]}, {"text": "SVM", "entities": [[0, 1, "MethodName", "SVM"]]}, {"text": "68.6 39.8 79.7 79.7 0.00 0.00 39.1 51.4 55.4 54.6 33.9 67.2 76.5 55.2 47.4 40.9 EnsembleG 69.2 39.6 81.2 77.2 0.00 0.00 42.3 62.9 65.6 44.9 57.1 6.9 68.9 88.5 48.8 47.7 EnsembleNG 72.2 40.5 84.2 78.0 0.00 0.00 47.3 74.3 62.8 57.9 39.4 44.8 75.1 72.7 47.6 45.5 Hillary Favor Against None Model Acc F", "entities": [[55, 56, "MetricName", "Acc"]]}, {"text": "Prec Rec Prec Rec Prec Rec GBDT 64.4 48.7 40.0 13.3 66.0 93.6 63.9 29.5 RF 70.2 49.8 75.0 13.3 70.5 84.9 68.8 70.5 SVM 62.0 55.3 36.8 46.7 70.2 68.6 62.9 56.4 EnsembleG 63.4 44.1 100.0 8.9 66.0 79.1 55.3 60.3 EnsembleNG 67.8 51.6 80.0 17.8 71.1 77.3 60.2 75.6 Table 5 : Detailed comparison .", "entities": [[24, 25, "MethodName", "SVM"]]}, {"text": "Best accuracies of individual classi\ufb01ers are shown in italics , best overall results in bold .", "entities": []}, {"text": "( F = macro - averaged F over Favor and Against ; of\ufb01cial score . )", "entities": []}, {"text": "SVM classi\ufb01er is about 1.5 percentage points below that ( 61.93 ) .", "entities": [[0, 1, "MethodName", "SVM"]]}, {"text": "A closer look at the ensemble variants shows that using the global features has a detrimental effect across all targets , most likely because this information is too coarse .", "entities": []}, {"text": "The other ensemble classi\ufb01er improves over GBDT by 1.5 percentage points ( 66.14 ) .", "entities": []}, {"text": "This shows that we can bene\ufb01t from important information from all individual classi\ufb01ers .", "entities": []}, {"text": "4.2.2 Further Analysis While the of\ufb01cial scorer averages the results over all \ufb01ve targets , we are interested in whether our classi\ufb01ers show a stable performance across targets , and why the ensemble model bene\ufb01ts from combining all individual classi\ufb01ers .", "entities": []}, {"text": "For this reason , we modi\ufb01ed the scorer so that it would calculate accuracy , precision , and recall for individual stances per target separately .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "The results are shown in table 5 .", "entities": []}, {"text": "The of\ufb01cial metric is the macro - averaged F - measure on Favor and Against while accuracy is equivalent to the micro - averaged F - measure based on all classes .", "entities": [[8, 11, "MetricName", "F - measure"], [16, 17, "MetricName", "accuracy"], [24, 27, "MetricName", "F - measure"]]}, {"text": "The results show a more diverse picture : For the individual classi\ufb01ers , GBDT reaches the highest ac - curacies for the targets Climate and Feminist , random forest for Atheism and Hillary , and they tie for Abortion .", "entities": []}, {"text": "For the ensembles , the version without global features reaches higher accuracies for Abortion , Climate , and Hillary , the version with global features has a higher accuracy for Feminist , and they tie for Atheism .", "entities": [[28, 29, "MetricName", "accuracy"]]}, {"text": "EnsembleNG , which reaches the best score across all targets , only reaches the best score for two targets : Abortion and Feminist .", "entities": []}, {"text": "It reaches lower results than the best individual classi\ufb01er for 3 targets : Atheism , Climate , and Hillary .", "entities": []}, {"text": "However , since the best results for the latter 3 targets are reached by different individual classi\ufb01ers ( random forest for Atheism and Hillary ; GBDT for Climate ) , we assume that the ensemble provides the best compromise .", "entities": []}, {"text": "In order to obtain a better understanding of the differences in performance of classi\ufb01ers across targets , we have analyzed the distribution of stances per target .", "entities": []}, {"text": "Table 6 shows the distribution in training and test data .", "entities": []}, {"text": "If we combine the information from table 5 with the stance distributions , we notice that a major advantage of the random forest classi\ufb01er is its398", "entities": []}, {"text": "Data Set Stance Abortion Atheism Climate Feminist Hillary Train Favor 18 18 54 32 17 Against 55 59 4 49 57 None 27 23 42 19 26 Test Favor 17 14 73 20 15 Against 67 73 7 64 58 None 16 13 20 16 27 Table 6 : Class distribution across targets in percentage .", "entities": []}, {"text": "high recall on the None stance , which is generally ( one of ) the minority class(es ) .", "entities": []}, {"text": "For the second minority class ( Favor for Abortion , Atheism , Hillary , and Feminist ; and Against for Climate ) , the picture is less clear : For Climate , none of the classi\ufb01ers manage to identify any of the Against tweets .", "entities": []}, {"text": "For Abortion and Feminist , random forest also shows a high recall for Favor , but for Atheism and Hillary , its precision is considerably higher .", "entities": []}, {"text": "In contrast , GBDT reaches a higher recall for the majority class ( with Atheism as the only exception ) .", "entities": []}, {"text": "SVM generally has precision and recall values between or below the other classi\ufb01ers .", "entities": [[0, 1, "MethodName", "SVM"]]}, {"text": "The only exception is the target Feminist , where SVM reaches the highest precision for all three stances .", "entities": [[9, 10, "MethodName", "SVM"]]}, {"text": "One hypothesis that could be drawn from the analysis above is that the GBDT model is better suited for \ufb01nding examples of the majority classes while random forest is better at \ufb01nding minority class examples .", "entities": []}, {"text": "However , when we compare the targets Abortion and Atheism , the class distribution is similar , but the performance of the two classi\ufb01ers is vastly different : For Abortion , GBDT reaches higher recall for the majority class ( Against ) and higher precision for Favor .", "entities": []}, {"text": "For Atheism , it has a higher precision for the majority class and a higher recall for Favor .", "entities": []}, {"text": "The reasons for these different behaviors need to be determined in future work .", "entities": []}, {"text": "5 Conclusion In this shared task , we regard stance detection as a special case of sentiment analysis , using supervised classi\ufb01ers and bag of unigrams and word vectors as features .", "entities": [[9, 11, "TaskName", "stance detection"], [16, 18, "TaskName", "sentiment analysis"]]}, {"text": "Our submitted system is based on a random forest classi\ufb01er because of its capability to handle over\ufb01tting and to generalize over the test data .", "entities": []}, {"text": "Since the amount of available training data is small , random forest \u2019s ability to sample data points and fea - ture subspaces reduces data sparsity .", "entities": []}, {"text": "The submitted system has an of\ufb01cial score of 63.60 and ranked 6th out of 19 teams .", "entities": []}, {"text": "We also experimented with other single models ( SVM and GBDT ) and with an ensemble model built on a memory - based classi\ufb01er .", "entities": [[8, 9, "MethodName", "SVM"]]}, {"text": "The GBDT model using GloVe word vectors reaches a higher score of 64.64 , which may be a result of the word vectors \u2019 capability to capture similarities among words , which helps in dealing with out - of - vocabulary words .", "entities": [[4, 5, "MethodName", "GloVe"]]}, {"text": "The ensemble model that aggregates information from the three individual classi\ufb01ers reaches the highest performance of 66.14 .", "entities": []}, {"text": "Our hypothesis is that different strengths ( e.g. , good performance for minority / majority classes ) from individual models complement each other in the ensemble .", "entities": []}, {"text": "However a closer look at the performance of all classi\ufb01ers and ensembles across individual targets shows that no system reaches consistently good results across all targets .", "entities": []}, {"text": "The best performing ensemble ( EnsembleNG ) outperforms individual classi\ufb01ers only for Abortion and Feminist ; for the other targets , random forest or GBDT reach higher accuracies .", "entities": []}, {"text": "Some of the variation in system performance can be explained by the class imbalance present in the data sets for the different targets , but further work is required to identify other factors .", "entities": []}, {"text": "Finally , it is worth pointing out that our approach to stance detection utilizes very surface oriented features .", "entities": [[11, 13, "TaskName", "stance detection"]]}, {"text": "To boost performance , we may need to develop methods that incorporate inference , entailment , and world knowledge , for example , to handle cases such as \u201c keep H. out of the white house \u201d .", "entities": []}, {"text": "Acknowledgement This work is based on research supported by the U.S. Of\ufb01ce of Naval Research ( ONR ) via grant # N0001410 - 1 - 0140.399", "entities": []}, {"text": "References Pranav Anand , Marilyn Walker , Rob Abbott , Jean E Fox Tree , Robeson Bowmani , and Michael Minor .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Cats rule and dogs drool ! :", "entities": []}, {"text": "Classifying stance in online debate .", "entities": []}, {"text": "In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis , pages 1\u20139 , Portland , OR .", "entities": [[12, 14, "TaskName", "Sentiment Analysis"]]}, {"text": "Ronan Collobert , Jason Weston , L \u00b4 eon Bottou , Michael Karlen , Koray Kavukcuoglu , and Pavel Kuksa . 2011 .", "entities": []}, {"text": "Natural language processing ( almost ) from scratch .", "entities": []}, {"text": "Journal of Machine Learning Research , 12:2493 \u2013 2537 .", "entities": []}, {"text": "Walter Daelemans , Jakub Zavrel , Ko van der Sloot , and Antal van den Bosch . 2009 .", "entities": []}, {"text": "TiMBL : Tilburg memory based learner \u2013 version 6.2 \u2013 reference guide .", "entities": []}, {"text": "Technical Report ILK 09 - 01 , Induction of Linguistic Knowledge , Computational Linguistics , Tilburg University .", "entities": []}, {"text": "Kazi Saidul Hasan and Vincent Ng . 2013 .", "entities": []}, {"text": "Stance classi\ufb01cation of ideological debates : Data , models , features , and constraints .", "entities": []}, {"text": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing ( IJCLNP 2013 ) , pages 1348\u20131356 , Nagoya , Japan .", "entities": []}, {"text": "Thorsten Joachims .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "Text Categorization with Support Vector Machines : Learning with Many Relevant Features .", "entities": [[0, 2, "TaskName", "Text Categorization"]]}, {"text": "Springer .", "entities": []}, {"text": "Lingpeng Kong , Nathan Schneider , Swabha Swayamdipta , Archna Bhatia , Chris Dyer , and Noah A Smith .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A dependency parser for tweets .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP 2014 ) , pages 1001\u20131012 , Doha , Qatar .", "entities": []}, {"text": "Can Liu , Sandra K \u00a8ubler , and Ning Yu .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Feature selection for highly skewed sentiment analysis tasks .", "entities": [[0, 2, "MethodName", "Feature selection"], [5, 7, "TaskName", "sentiment analysis"]]}, {"text": "InProceedings of the Second Workshop on Natural Language Processing for Social Media ( SocialNLP ) , pages 2\u201311 , Dublin , Ireland .", "entities": []}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "arXiv preprint arXiv:1301.3781 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Saif M. Mohammad , Svetlana Kiritchenko , Parinaz Sobhani , Xiaodan Zhu , and Colin Cherry .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semeval2016 task 6 : Detecting stance in tweets .", "entities": []}, {"text": "In Proceedings of the International Workshop on Semantic Evaluation , SemEval\u201916 , San Diego , CA , June .", "entities": []}, {"text": "Fabian Pedregosa , Ga \u00a8el Varoquaux , Alexandre Gramfort , Vincent Michel , Bertrand Thirion , Olivier Grisel , Mathieu Blondel , Peter Prettenhofer , Ron Weiss , Vincent Dubourg , Jake Vanderplas , Alexandre Passos , David Cournapeau , Matthieu Brucher , Matthieu Perrot , and \u00b4 Edouard Duchesnay .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Scikit - learn : Machine learning in Python .", "entities": []}, {"text": "Journal of Machine Learning Research , 12:2825\u20132830.Jeffrey Pennington , Richard Socher , and Christopher D Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "GloVe : Global vectors for word representation .", "entities": [[0, 1, "MethodName", "GloVe"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 , Doha , Qatar .", "entities": []}, {"text": "Istv\u00b4an Pil \u00b4 aszy .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Text categorization and support vector machines .", "entities": [[0, 2, "TaskName", "Text categorization"]]}, {"text": "In Proceedings of the 6th International Symposium of Hungarian Researchers on Computational Intelligence , Budapest , Hungary .", "entities": []}, {"text": "Swapna Somasundaran and Janyce Wiebe .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Recognizing stances in online debates .", "entities": []}, {"text": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 226\u2013234 , Suntec , Singapore .", "entities": []}, {"text": "Swapna Somasundaran and Janyce Wiebe .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Recognizing stances in ideological on - line debates .", "entities": []}, {"text": "In Proceedings of the NAACL HLT 2010", "entities": []}, {"text": "Workshop on Computational Approaches to Analysis and Generation of Emotion in Text , pages 116\u2013124 , Los Angeles , CA.400", "entities": []}]