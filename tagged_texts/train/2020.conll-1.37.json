[{"text": "Proceedings of the 24th Conference on Computational Natural Language Learning , pages 455\u2013475 Online , November 19 - 20 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17455On the Computational Power of Transformers and its Implications in Sequence Modeling Satwik Bhattamishra Arkil Patel Navin Goyal Microsoft Research India ft - satbh , t - arkpat , navingo g@microsoft.com", "entities": []}, {"text": "Abstract Transformers are being used extensively across several sequence modeling tasks .", "entities": []}, {"text": "Signi\ufb01cant research effort has been devoted to experimentally probe the inner workings of Transformers .", "entities": []}, {"text": "However , our conceptual and theoretical understanding of their power and inherent limitations is still nascent .", "entities": []}, {"text": "In particular , the roles of various components in Transformers such as positional encodings , attention heads , residual connections , and feedforward networks , are not clear .", "entities": []}, {"text": "In this paper , we take a step towards answering these questions .", "entities": []}, {"text": "We analyze the computational power as captured by Turing - completeness .", "entities": []}, {"text": "We \ufb01rst provide an alternate and simpler proof to show that vanilla Transformers are Turing - complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing - complete .", "entities": []}, {"text": "We further analyze the necessity of each component for the Turing - completeness of the network ; interestingly , we \ufb01nd that a particular type of residual connection is necessary .", "entities": [[26, 28, "MethodName", "residual connection"]]}, {"text": "We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "1 Introduction Transformer ( Vaswani et al . , 2017 ) is a recent selfattention based sequence - to - sequence architecture which has led to state of the art results across various NLP tasks including machine translation ( Ott et al . , 2018 ) , language modeling ( Radford et al . , 2018 ) and question answering ( Devlin et al . , 2019 ) .", "entities": [[2, 3, "MethodName", "Transformer"], [36, 38, "TaskName", "machine translation"], [58, 60, "TaskName", "question answering"]]}, {"text": "Although a number of variants of Transformers have been proposed , the original architecture still underlies these variants .", "entities": []}, {"text": "While the training and generalization of machine learning models such as Transformers are the central goals in their analysis , an essential prerequisite to this end is characterization of the computational POS(1 ) POS(2 ) POS(3 ) ( a ) ( b)Figure 1 : ( a ) Self - Attention Network with positional encoding , ( b ) Self - Attention Network with positional masking without any positional encoding power of the model : training a model for a certain task can not succeed if the model is computationally incapable of carrying out the task .", "entities": [[47, 51, "MethodName", "Self - Attention Network"], [58, 62, "MethodName", "Self - Attention Network"]]}, {"text": "While the computational capabilities of recurrent networks ( RNNs ) have been studied for decades ( Kolen and Kremer , 2001 ; Siegelmann , 2012 ) , for Transformers we are still in the early stages .", "entities": []}, {"text": "The celebrated work of Siegelmann and Sontag ( 1992 ) showed , assuming arbitrary precision , that RNNs are Turing - complete , meaning that they are capable of carrying out any algorithmic task formalized by Turing machines .", "entities": []}, {"text": "Recently , P \u00b4 erez et al .", "entities": []}, {"text": "( 2019 ) have shown that vanilla Transformers with hard - attention can also simulate Turing machines given arbitrary precision .", "entities": []}, {"text": "However , in contrast to RNNs , Transformers consist of several components and it is unclear which components are necessary for its Turing - completeness and thereby crucial to its computational expressiveness .", "entities": []}, {"text": "The role of various components of the Transformer in its ef\ufb01cacy is an important question for further improvements .", "entities": [[7, 8, "MethodName", "Transformer"]]}, {"text": "Since the Transformer does not process the input sequentially , it requires some form of positional information .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "Various positional encoding schemes have been proposed to capture order information ( Shaw et al . , 2018 ; Dai et al . , 2019 ; Huang et al . , 2018 ) .", "entities": []}, {"text": "At the same time , on", "entities": []}, {"text": "456machine translation , Yang et al .", "entities": []}, {"text": "( 2019 ) showed that the performance of Transformers with only positional masking ( Shen et al . , 2018 ) is comparable to that with positional encodings .", "entities": []}, {"text": "In case of positional masking ( Fig . 1 ) , as opposed to explicit encodings , the model is only allowed to attend over preceding inputs and no additional positional encoding vector is combined with the input vector .", "entities": []}, {"text": "Tsai et al .", "entities": []}, {"text": "( 2019 ) raised the question of whether explicit encoding is necessary if positional masking is used .", "entities": []}, {"text": "Additionally , since P \u00b4 erez et al . ( 2019 ) \u2019s", "entities": []}, {"text": "Turingcompleteness proof relied heavily on residual connections , they asked whether these connections are essential for Turing - completeness .", "entities": []}, {"text": "In this paper , we take a step towards answering such questions .", "entities": []}, {"text": "Below , we list the main contributions of the paper , \u000fWe provide an alternate and arguably simpler proof to show that Transformers are Turingcomplete by directly relating them to RNNs .", "entities": []}, {"text": "\u000fMore importantly , we prove that Transformers with positional masking and without positional encoding are also Turing - complete .", "entities": []}, {"text": "\u000fWe analyze the necessity of various components such as self - attention blocks , residual connections and feedforward networks for Turing - completeness .", "entities": []}, {"text": "Figure 2 provides an overview .", "entities": []}, {"text": "\u000fWe explore implications of our results on machine translation and synthetic tasks.1 2 Related Work Computational Power of neural networks has been studied since the foundational paper McCulloch and Pitts ( 1943 ) ; in particular , among sequence - to - sequence models , this aspect of RNNs has long been studied ( Kolen and Kremer , 2001 ) .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "The seminal work by Siegelmann and Sontag ( 1992 ) showed that RNNs can simulate a Turing machine by using unbounded precision .", "entities": []}, {"text": "Chen et al .", "entities": []}, {"text": "( 2018 ) showed that RNNs with ReLU activations are also Turing - complete .", "entities": [[7, 8, "MethodName", "ReLU"]]}, {"text": "Many recent works have explored the computational power of RNNs in practical settings .", "entities": []}, {"text": "Several works ( Merrill et al . , 2020 ) , ( Weiss et al . , 2018 ) recently studied the ability of RNNs to recognize counter - like languages .", "entities": []}, {"text": "The capability of RNNs to recognize strings of balanced 1We have made our source code available at https://github.com/satwik77/Transformer-ComputationAnalysis.parantheses has also been studied ( Sennhauser and Berwick , 2018 ; Skachkova et al . , 2018 ) .", "entities": []}, {"text": "However , such analysis on Transformers has been scarce .", "entities": []}, {"text": "Theoretical work on Transformers was initiated by P\u00b4erez et al .", "entities": []}, {"text": "( 2019 ) who formalized the notion of Transformers and showed that it can simulate a Turing machine given arbitrary precision .", "entities": []}, {"text": "Concurrent to our work , there have been several efforts to understand self - attention based models ( Levine et al . , 2020 ;", "entities": []}, {"text": "Kim et al . , 2020 ) .", "entities": []}, {"text": "Hron et al .", "entities": []}, {"text": "( 2020 ) show that Transformers behave as Gaussian processes when the number of heads tend to in\ufb01nity .", "entities": [[8, 10, "TaskName", "Gaussian processes"]]}, {"text": "Hahn ( 2020 ) showed some limitations of Transformer encoders in modeling regular and context - free languages .", "entities": [[8, 9, "MethodName", "Transformer"]]}, {"text": "It has been recently shown that Transformers are universal approximators of sequence - tosequence functions given arbitrary precision ( Yun et al . , 2020 ) .", "entities": []}, {"text": "However , these are not applicable2 to the complete Transformer architecture .", "entities": [[9, 10, "MethodName", "Transformer"]]}, {"text": "With a goal similar to ours , Tsai et al .", "entities": []}, {"text": "( 2019 ) attempted to study the attention mechanism via a kernel formulation .", "entities": []}, {"text": "However , a systematic study of various components of Transformers has not been done .", "entities": []}, {"text": "3 De\ufb01nitions and Preliminaries All the numbers used in our computations will be from the set of rational numbers denoted Q.", "entities": []}, {"text": "For a sequenceX= ( x1;:::;xn ) , we setXj:= ( x1;:::;xj)for1\u0014j\u0014n .", "entities": []}, {"text": "We will work with an alphabet \u0006of sizem , with special symbols#and$signifying the beginning and end of the input sequence , respectively .", "entities": []}, {"text": "The symbols are mapped to vectors via a given \u2018 base \u2019 embedding fb:\u0006!Qdb , wheredbis the dimension of the embedding .", "entities": []}, {"text": "E.g. , this embedding could be the one used for processing the symbols by the RNN .", "entities": []}, {"text": "We setfb ( # ) = 0dbandfb($ ) = 0db .", "entities": []}, {"text": "Positional encoding is a function pos : N!Qdb .", "entities": []}, {"text": "Together , these provide embedding for a symbol sat positionigiven byf(fb(s);pos(i ) ) , often taken to be simply fb(s ) + pos(i ) .", "entities": []}, {"text": "Vector JsK2Qm denotes one - hot encoding of a symbol s2\u0006. 3.1 RNNs We follow Siegelmann and Sontag ( 1992 ) in our de\ufb01nition of RNNs .", "entities": []}, {"text": "To feed the sequences 2Hahn ( 2020 ) and Yun et al .", "entities": []}, {"text": "( 2020 ) study encoder - only seqto - seq models with \ufb01xed length outputs in which the computation halts as soon as the last symbol of the input is processed .", "entities": []}, {"text": "Our work is about the full Transformer ( encoder and decoder ) which is a seq - to - seq model with variable length sequence output in which the decoder starts operating sequentially after the encoder .", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "457s1s2:::sn2\u0006\u0003to the RNN , these are converted to the vectors x1;x2;:::;xnwherexi= fb(si ) .", "entities": []}, {"text": "The RNN is given by the recurrence ht= g(Whht\u00001+Wxxt+b ) , wheret\u00151 , function g(\u0001)is a multilayer feedforward network ( FFN ) with activation \u001b , bias vectorb2Qdh , matrices Wh2Qdh\u0002dhandWx2Qdh\u0002db , andht2Qdh is the hidden state with given initial hidden state h0;dhis the hidden state dimension .", "entities": [[17, 19, "MethodName", "feedforward network"]]}, {"text": "After the last symbol snhas been fed , we continue to feed the RNN with the terminal symbol fb($)until it halts .", "entities": []}, {"text": "This allows the RNN to carry out computation after having read the input .", "entities": []}, {"text": "A class of seq - to - seq neural networks is Turingcomplete if the class of languages recognized by the networks is exactly the class of languages recognized by Turing machines .", "entities": []}, {"text": "Theorem 3.1 .", "entities": []}, {"text": "( Siegelmann and Sontag , 1992 )", "entities": []}, {"text": "Any seq - to - seq function \u0006\u0003!\u0006\u0003computable by a Turing machine can also be computed by an RNN .", "entities": []}, {"text": "For details please see section B.1 in appendix .", "entities": []}, {"text": "3.2 Transformer Architecture Vanilla Transformer .", "entities": [[1, 2, "MethodName", "Transformer"], [4, 5, "MethodName", "Transformer"]]}, {"text": "We describe the original Transformer architecture with positional encoding ( Vaswani et al . , 2017 ) as formalized by P \u00b4 erez et al .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "( 2019 ) , with some modi\ufb01cations .", "entities": []}, {"text": "All vectors in this subsection are from Qd .", "entities": []}, {"text": "The transformer , denoted Trans , is a seq - to - seq architecture .", "entities": []}, {"text": "Its input consists of ( i ) a sequence X= ( x1;:::;xn)of vectors , ( ii ) a seed vector y0 .", "entities": []}, {"text": "The output is a sequence Y= ( y1;:::;yr ) of vectors .", "entities": []}, {"text": "The sequence Xis obtained from the sequence ( s1;:::;sn)2\u0006nof symbols by using the embedding mentioned earlier : xi= f(fb(si);pos(i ) ) .", "entities": []}, {"text": "The transformer consists of composition of transformer encoder andtransformer decoder .", "entities": []}, {"text": "For the feedforward networks in the transformer layers we use the activation as in Siegelmann and Sontag ( 1992 ) , namely the saturated linear activation function\u001b(x)which takes value 0forx < 0 , valuex for0 < x < 1and value 1forx>1 .", "entities": [[31, 32, "DatasetName", "0"]]}, {"text": "This activation can be easily replaced by the standard ReLU activation via \u001b(x )", "entities": [[9, 10, "MethodName", "ReLU"]]}, {"text": "= ReLU(x)\u0000ReLU(x\u00001 ) .", "entities": []}, {"text": "Self - attention .", "entities": []}, {"text": "The self - attention mechanism takes as input ( i ) a query vectorq , ( ii ) a sequence ofkeyvectorsK= ( k1;:::;kn ) , and ( iii ) a sequence of value vectorsV= ( v1;:::;vn ) .", "entities": []}, {"text": "The q - attention over KandV , denoted Att(q;K;V ) , is a vectora= \u000b 1v1 + \u000b 2v2+\u0001\u0001\u0001+ \u000b nvn , where ( i ) ( \u000b 1 ; : : : ; \u000b n ) = \u001a(fatt(q;k1);:::;fatt(q;kn ) ) .", "entities": []}, {"text": "( ii ) The normalization function \u001a:Qn!Qn \u00150is hardmax : forx= ( x1;:::;xn)2Qn , if the maximum value occurs rtimes among x1;:::;xn , then hardmax ( x)i:= 1 = rifxiis a maximum value andhardmax ( x)i:= 0otherwise .", "entities": []}, {"text": "In practice , the softmax is often used but its output values are in general not rational .", "entities": [[4, 5, "MethodName", "softmax"]]}, {"text": "( iii ) For vanilla transformers , the scoring function fattused is a combination of multiplicative attention ( Vaswani et", "entities": [[15, 17, "MethodName", "multiplicative attention"]]}, {"text": "al . , 2017 ) and a non - linear function : fatt(q;ki )", "entities": []}, {"text": "= \u0000 \f\f hq;kii \f\f .", "entities": []}, {"text": "This was also used by P\u00b4erez et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Transformer encoder .", "entities": [[0, 1, "MethodName", "Transformer"]]}, {"text": "Asingle - layer encoder is a function Enc(X;\u0012 ) , with input X= ( x1;:::;xn)a sequence of vectors in Qd , and parameters\u0012.", "entities": []}, {"text": "The output is another sequence Z= ( z1;:::;zn)of vectors in Qd .", "entities": []}, {"text": "The parameters\u0012specify functions Q(\u0001);K(\u0001);V(\u0001 ) , andO(\u0001 ) , all of type Qd!Qd .", "entities": []}, {"text": "The functions Q(\u0001);K(\u0001 ) ; andV(\u0001)are linear transformations and O(\u0001)an FFN .", "entities": []}, {"text": "For 1\u0014i\u0014n , the output of the self - attention block is produced by ai= Att(Q(xi);K(X);V(X ) )", "entities": []}, {"text": "+ xi ( 1 ) This operation is also referred to as the encoderencoder attention block .", "entities": []}, {"text": "The output Zis computed byzi = O(ai ) + aifor1\u0014i\u0014n .", "entities": []}, {"text": "The addition operations + xiand+aiare the residual connections .", "entities": []}, {"text": "The complete L - layer transformer encoder TEnc(L)(X;\u0012 ) = ( Ke;Ve)has the same inputX= ( x1;:::;xn)as the single - layer encoder .", "entities": []}, {"text": "In contrast , its output Ke= ( ke 1;:::;ke n ) andVe= ( ve 1;:::ve n)contains two sequences .", "entities": []}, {"text": "TEnc(L)is obtained by composition of Lsinglelayer encoders : let X(0):=X , and for 0\u0014`\u0014 L\u00001 , letX(`+1)= Enc(X(`);\u0012`)and \ufb01nally , Ke = K(L)(X(L));Ve = V(L)(X(L ) ):", "entities": []}, {"text": "Transformer decoder .", "entities": [[0, 2, "MethodName", "Transformer decoder"]]}, {"text": "The input to a singlelayer decoder is ( i ) ( Ke;Ve)output by the encoder , and ( ii ) sequence Y= ( y1;:::;yk)of vectors fork\u00151 .", "entities": []}, {"text": "The output is another sequence Z= ( z1;:::;zk ) .", "entities": []}, {"text": "Similar to the single - layer encoder , a singlelayer decoder is parameterized by functions Q(\u0001);K(\u0001);V(\u0001)andO(\u0001)and is de\ufb01ned by pt= Att(Q(yt);K(Yt);V(Yt ) )", "entities": []}, {"text": "+ yt;(2 ) at= Att(pt;Ke;Ve )", "entities": []}, {"text": "+ pt ; ( 3 ) zt = O(at ) + at ; where 1\u0014t\u0014k .", "entities": []}, {"text": "The operation in ( 2 ) will be", "entities": []}, {"text": "458referred to as the decoder - decoder attention block and the operation in ( 3 ) as the decoder - encoder attention block .", "entities": []}, {"text": "In ( 2 ) , positional masking is applied to prevent the network from attending over symbols which are ahead of them .", "entities": []}, {"text": "AnL - layer Transformer decoder TDecL((Ke;Ve);Y;\u0012 ) = zis obtained by repeated application of Lsingle - layer decoders each with its own parameters , and a transformation functionF : Qd!Qdapplied to the last vector in the sequence of vectors output by the \ufb01nal decoder .", "entities": [[3, 5, "MethodName", "Transformer decoder"]]}, {"text": "Formally , for 0\u0014`\u0014L\u00001andY0:=Ywe have Y`+1= Dec((Ke;Ve);Y`;\u0012`);z = F(yL k ): Note that while the output of a single - layer decoder is a sequence of vectors , the output of an L - layer Transformer decoder is a single vector .", "entities": [[35, 37, "MethodName", "Transformer decoder"]]}, {"text": "The complete Transformer .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "The output Trans(X;y0 )", "entities": []}, {"text": "= Yis computed by the recurrence ~yt+1= TDec(TEnc ( X);(y0;y1;:::;yt ) ) , for0\u0014t\u0014r\u00001 .", "entities": []}, {"text": "We getyt+1by adding positional encoding : yt+1=~yt+1 + pos(t+ 1 ) .", "entities": []}, {"text": "Directional Transformer .", "entities": [[1, 2, "MethodName", "Transformer"]]}, {"text": "We denote the Transformer with only positional masking and no positional encodings as Directional Transformer and use them interchangeably .", "entities": [[3, 4, "MethodName", "Transformer"], [14, 15, "MethodName", "Transformer"]]}, {"text": "In this case , we use standard multiplicative attention as the scoring function in our construction , i.e , fatt(q;ki ) = hq;kii .", "entities": [[7, 9, "MethodName", "multiplicative attention"]]}, {"text": "The general architecture is the same as for the vanilla case ; the differences due to positional masking are the following .", "entities": []}, {"text": "There are no positional encodings .", "entities": []}, {"text": "So the input vectors xionly involve fb(si ) .", "entities": []}, {"text": "Similarly , yt=~yt .", "entities": []}, {"text": "In ( 1 ) , Att(\u0001)is replaced byAtt(Q(xi);K(Xi);V(Xi))whereXi:= ( x1;:::;xi)for1\u0014i\u0014n .", "entities": []}, {"text": "Similarly , in ( 3 ) , Att(\u0001)is replaced by Att(pt;Ke t;Ve t ) .", "entities": []}, {"text": "Remark 1 .", "entities": []}, {"text": "Our de\ufb01nitions deviate slightly from practice , hard - attention being the main one since hardmax keeps the values rational whereas softmax takes the values to irrational space .", "entities": [[21, 22, "MethodName", "softmax"]]}, {"text": "Previous studies have shown that soft - attention behaves like hard - attention in practice and Hahn ( 2020 ) discusses its practical relevance .", "entities": []}, {"text": "Remark 2 .", "entities": []}, {"text": "Transformer Networks with positional encodings are not necessarily equivalent in terms of their computational expressiveness ( Yun et al . , 2020 ) to those with only positional masking when considering the encoder only model ( as used in BERT and GPT-2 ) .", "entities": [[0, 1, "MethodName", "Transformer"], [39, 40, "MethodName", "BERT"], [41, 42, "MethodName", "GPT-2"]]}, {"text": "Our results in Section 4.1 show their equivalence in terms of expressiveness for the complete seq - to - seq architecture.4 Primary Results 4.1 Turing - Completeness Results In light of Theorem 3.1 , to prove that Transformers are Turing - complete , it suf\ufb01ces to show that they cansimulate RNNs .", "entities": []}, {"text": "We say that a Transformer simulates an RNN ( as de\ufb01ned in Sec . 3.1 ) if on every input s2\u0006\u0003 , at each step t , the vectoryt contains the hidden state htas a subvector , i.e. yt=", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "[ ht;\u0001 ] , and halts at the same step as the RNN .", "entities": []}, {"text": "Theorem 4.1 .", "entities": []}, {"text": "The class of Transformers with positional encodings is Turing - complete .", "entities": []}, {"text": "Proof Sketch .", "entities": [[1, 2, "DatasetName", "Sketch"]]}, {"text": "The inputs0;:::;sn2\u0006\u0003is provided to the transformer as the sequence of vectors x0;:::;xn , wherexi= [ 0dh;fb(si);0dh;i;1 ] , which has as sub - vector the given base embedding fb(si)and the positional encoding i , along with extra coordinates set to constant values and will be used later .", "entities": []}, {"text": "The basic observation behind our construction of the simulating Transformer is that the transformer decoder can naturally implement the recurrence operations of the type used by RNNs .", "entities": [[9, 10, "MethodName", "Transformer"], [13, 15, "MethodName", "transformer decoder"]]}, {"text": "To this end , the FFNOdec(\u0001)of the decoder , which plays the same role as the FFN component of the RNN , needs sequential access to the input in the same way as RNN .", "entities": []}, {"text": "But the Transformer receives the whole input at the same time .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "We utilize positional encoding along with the attention mechanism to isolate xt", "entities": []}, {"text": "at timetand feed it to Odec(\u0001 ) , thereby simulating the RNN .", "entities": []}, {"text": "As stated earlier , we append the input s1;:::;sn of the RNN with $ \u2019s until it halts .", "entities": []}, {"text": "Since the Transformer takes its input all at once , appending by $ \u2019s is not possible ( in particular , we do not know how long the computation would take ) .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "Instead , we append the input with a single $ .", "entities": []}, {"text": "After encountering a$once , the Transformer will feed ( encoding of ) $ toOdec(\u0001)in subsequent steps until termination .", "entities": [[5, 6, "MethodName", "Transformer"]]}, {"text": "Here we con\ufb01ne our discussion to the case t\u0014n ; thet > n case is slightly different but simpler .", "entities": []}, {"text": "The construction is straightforward : it has only one head , one encoder layer and one decoder layer ; moreover , the attention mechanisms in the encoder and the decoder - decoder attention block of the decoder are trivial as described below .", "entities": []}, {"text": "The encoder attention layer does trivial computation in that it merely computes the identity function :", "entities": []}, {"text": "zi = xi , which can be easily achieved , e.g. by using the residual connection and setting the value vectors to 0 .", "entities": [[14, 16, "MethodName", "residual connection"], [22, 23, "DatasetName", "0"]]}, {"text": "The \ufb01-", "entities": []}, {"text": "459 Encoder - Encoder   Attention HeadFeed Forward   Network Input EmbeddingDecoder - Decoder Attention HeadDecoder - Encoder Attention HeadFeed Forward   Network Output EmbeddingPositional   EncodingPositional   EncodingFigure 2 : Transformer network with various components highlighted .", "entities": [[31, 32, "MethodName", "Transformer"]]}, {"text": "The components marked red are essential for the Turing - completeness whereas for the pairs of blocks and residual connections marked green , either one of the component is enough .", "entities": []}, {"text": "The dashed residual connection is not necessary for Turingcompleteness of the network .", "entities": [[2, 4, "MethodName", "residual connection"]]}, {"text": "nalK(1)(\u0001)andV(1)(\u0001)functions bring ( Ke;Ve ) into useful forms by appropriate linear transformations : ki=", "entities": []}, {"text": "[ 0db;0db;0db;\u00001;i]andvi= [ 0db;fb(si);0db;0;0 ] .", "entities": []}, {"text": "Thus , the key vectors only encode the positional information and the value vectors only encode the input symbols .", "entities": []}, {"text": "The output sequence of the decoder is y1;y2 ; : : : .", "entities": []}, {"text": "Our construction will ensure , by induction on t , thatytcontains the hidden states htof the RNN as a sub - vector along with positional information :", "entities": []}, {"text": "yt=", "entities": []}, {"text": "[ ht;0db;0db;t+1;1 ] .", "entities": []}, {"text": "This is easy to arrange fort= 0 , and assuming it for twe prove it for t+1 .", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "As for the encoder , the decoder - decoder attention block acts as the identity : pt = yt .", "entities": []}, {"text": "Now , using the last but one coordinate in ytrepresenting the time t+ 1 , the attention mechanism Att(pt;Ke;Ve ) can retrieve the embedding of the t - th input symbolxt .", "entities": []}, {"text": "This is possible because in the key vector kimentioned above , almost all coordinates other than the one representing the position iare set to 0 , allowing the mechanism to only focus on the positional information and not be distracted by the other contents of pt = yt : the scoring function has valuefatt(pt;ki )", "entities": [[24, 25, "DatasetName", "0"]]}, {"text": "= \u0000jhpt;kiij=\u0000ji\u0000(t+ 1)j .", "entities": []}, {"text": "For a given t , it is maximized at i = t+ 1 fort < n and ati = nfort\u0015n .", "entities": []}, {"text": "This use of scoring function is similar to P \u00b4 erez et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "At this point , Odec(\u0001)has at its disposal the hidden stateht(coming from ytviaptand the residual connection ) and the input symbol xt(coming via the attention mechanism and the residual connection ) .", "entities": [[14, 16, "MethodName", "residual connection"], [28, 30, "MethodName", "residual connection"]]}, {"text": "Hence O(\u0001)can act just like the FFN ( Lemma C.4 ) underlying the RNN to compute ht+1 and thusyt+1 , proving the induction hypothesis .", "entities": [[8, 9, "DatasetName", "Lemma"]]}, {"text": "The complete construction can be found in Sec .", "entities": []}, {"text": "C.2 in the appendix .", "entities": []}, {"text": "Theorem 4.2 .", "entities": []}, {"text": "The class of Transformers with positional masking and no explicit positional encodings is Turing - complete .", "entities": []}, {"text": "Proof Sketch .", "entities": [[1, 2, "DatasetName", "Sketch"]]}, {"text": "As before , by Theorem 3.1 it suf\ufb01ces to show that Transformers can simulate RNNs .", "entities": []}, {"text": "The inputs0;:::;snis provided to the transformer as the sequence of vectors x0;:::;xn , where xi=", "entities": []}, {"text": "[ 0dh;0dh;fb(si);JsiK;0;0m;0m;0 m ] .", "entities": []}, {"text": "The general goal for the directional case is similar to the vanilla case , namely we would like the FFN Odec(\u0001)of the decoder to directly simulate the computation in the underlying RNN .", "entities": []}, {"text": "In the vanilla case , positional encoding and the attention mechanism helped us feed input xtat thet - th iteration of the decoder toOdec(\u0001 ) .", "entities": []}, {"text": "However , we no longer have explicit positional information in the input xtsuch as a coordinate with value t.", "entities": []}, {"text": "The key insight is that we do not need the positional information explicitly to recover xtat stept : in our construction , the attention mechanism with masking will recover xt in an indirect manner even though it \u2019s not able to \u201c zero in \u201d on the t - th position .", "entities": []}, {"text": "Let us \ufb01rst explain this without details of the construction .", "entities": []}, {"text": "We maintain in vector !", "entities": []}, {"text": "t2Qm , with a coordinate each for symbols in \u0006 , the fraction of times the symbol has occurred up to step t.", "entities": []}, {"text": "Now , at a stept\u0014n , for the difference !", "entities": []}, {"text": "t\u0000!t\u00001(which is part of the query vector ) , it can be shown easily that only the coordinate corresponding to stis positive .", "entities": []}, {"text": "Thus after applying the linearized sigmoid \u001b(!t\u0000!t\u00001 ) , we can isolate the coordinate corresponding to st . Now using this query vector , the ( hard ) attention mechanism will be able to retrieve the value vectors for all indices jsuch thatsj = st and output their average .", "entities": []}, {"text": "Crucially , the value vector for an index jis essentiallyxjwhich depends only onsj .", "entities": []}, {"text": "Thus , all these vectors are equal to xt , and so is their average .", "entities": []}, {"text": "This recovers xt , which", "entities": []}, {"text": "460can now be fed to Odec(\u0001 ) , simulating the RNN .", "entities": []}, {"text": "We now outline the construction and relate it to the above discussion .", "entities": []}, {"text": "As before , for simplicity we restrict to the case t\u0014n .", "entities": []}, {"text": "We use only one head , one layer encoder and two layer decoder .", "entities": []}, {"text": "The encoder , as in the vanilla case , does very little other than pass information along .", "entities": []}, {"text": "The vectors in ( Ke;Ve)are obtained by the trivial attention mechanism followed by simple linear transformations : ke i=", "entities": []}, {"text": "[ 0dh;0dh;0db;JsiK;0;0m;0m;0 m ] andve i=", "entities": []}, {"text": "[ 0dh;0dh;fb(si);0m;0;0m;JsiK;0 m ] .", "entities": []}, {"text": "Our construction ensures that at step twe have yt=", "entities": []}, {"text": "[ ht\u00001;0dh;0db;0m;1 2t;0m;0m;!t\u00001 ] .", "entities": []}, {"text": "As before , the proof is by induction on t. In the \ufb01rst layer of decoder , the decoderdecoder attention block is trivial : p(1 ) t = yt .", "entities": []}, {"text": "In the decoder - encoder attention block , we give equal attention to all the t+ 1 values , which along with Oenc(\u0001 ) , leads toz(1 ) t=", "entities": []}, {"text": "[ ht\u00001;0dh;0db;\u000et;1 2t+1;0m;0m;!t ] , where essentially\u000et=\u001b(!t\u0000!t\u00001 ) , except with a change for the last coordinate due to special status of the last symbol $ in the processing of RNN .", "entities": []}, {"text": "In the second layer , the decoder - decoder attention block is again trivial with p(2 ) t = z(1 )", "entities": []}, {"text": "t. We remark that in this construction , the scoring function is the standard multiplicative attention3 .", "entities": []}, {"text": "Now hp(2 ) t;ke ji = h\u000et;JsjKi=\u000et;j , which is positive if and only if sj = st , as mentioned earlier .", "entities": []}, {"text": "Thus attention weights in Att(p(2 ) t;Ke t;Ve t)satisfy hardmax ( hp(2 ) t;ke 1i;:::;hp(2 ) t;ke ti ) = 1 \u0015t(I(s0= st);I(s1 = st);:::;I(st = st ) ) , where\u0015tis a normalization constant and I(\u0001)is the indicator .", "entities": []}, {"text": "See Lemma D.3 for more details .", "entities": [[1, 2, "DatasetName", "Lemma"]]}, {"text": "At this point , Odec(\u0001)has at its disposal the hidden stateht(coming from z(1 ) tviap(2 ) tand the residual connection ) and the input symbol xt(coming via the attention mechanism and the residual connection ) .", "entities": [[18, 20, "MethodName", "residual connection"], [32, 34, "MethodName", "residual connection"]]}, {"text": "Hence Odec(\u0001)can act just like the FFN underlying the RNN to compute ht+1and", "entities": []}, {"text": "thusyt+1 , proving the induction hypothesis .", "entities": []}, {"text": "The complete construction can be found in Sec .", "entities": []}, {"text": "D in the Appendix .", "entities": []}, {"text": "In practice , Yang et al .", "entities": []}, {"text": "( 2019 ) found that for NMT , Transformers with only positional masking achieve comparable performance compared to the ones with positional encodings .", "entities": []}, {"text": "Similar evidence 3Note that it is closer to practice than the scoring function \u0000jhq;kijused in P \u00b4 erez", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2019 ) and Theorem 4.1was found by Tsai et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Our proof for directional transformers entails that there is no loss of order information if positional information is only provided in the form of masking .", "entities": [[10, 11, "MetricName", "loss"]]}, {"text": "However , we do not recommend using masking as a replacement for explicit encodings .", "entities": []}, {"text": "The computational equivalence of encoding and masking given by our results implies that any differences in their performance must come from differences in learning dynamics .", "entities": []}, {"text": "4.2 Analysis of Components The results for various components follow from our construction in Theorem 4.1 .", "entities": []}, {"text": "Note that in both the encoder and decoder attention blocks , we need to compute the identity function .", "entities": []}, {"text": "We can nullify the role of the attention heads by setting the value vectors to zero and making use of only the residual connections to implement the identity function .", "entities": []}, {"text": "Thus , even if we remove those attention heads , the model is still Turing - complete .", "entities": []}, {"text": "On the other hand , we can remove the residual connections around the attention blocks and make use of the attention heads to implement the identity function by using positional encodings .", "entities": []}, {"text": "Hence , either the attention head or the residual connection is suf\ufb01cient to achieve Turing - completeness .", "entities": [[8, 10, "MethodName", "residual connection"]]}, {"text": "A similar argument can be made for the FFN in the encoder layer : either the residual connection or the FFN is suf\ufb01cient for Turing - completeness .", "entities": [[16, 18, "MethodName", "residual connection"]]}, {"text": "For the decoder - encoder attention head , since it is the only way for the decoder to obtain information about the input , it is necessary for the completeness .", "entities": []}, {"text": "The FFN is the only component that can perform computations based on the input and the computations performed earlier via recurrence and hence , the model is not Turing - complete without it .", "entities": []}, {"text": "Figure 2 summarizes the role of different components with respect to the computational expressiveness of the network .", "entities": []}, {"text": "Proposition 4.3 .", "entities": []}, {"text": "The class of Transformers without residual connection around the decoderencoder attention block is not Turing - complete .", "entities": [[5, 7, "MethodName", "residual connection"]]}, {"text": "Proof Sketch .", "entities": [[1, 2, "DatasetName", "Sketch"]]}, {"text": "We con\ufb01ne our discussion to singlelayer decoder ; the case of multilayer decoder is similar .", "entities": []}, {"text": "Without the residual connection , the decoder - encoder attention block produces at= Att(pt;Ke;Ve )", "entities": [[2, 4, "MethodName", "residual connection"]]}, {"text": "= Pn i=1", "entities": []}, {"text": "i ve ifor some \u000b i \u2019s such", "entities": []}, {"text": "thatPn i \u000b i= 1 .", "entities": []}, {"text": "Note that , without residual connectionatcan take on at most 2n\u00001values .", "entities": []}, {"text": "This is because by the de\ufb01nition of hard attention the vector ( \u000b 1 ; : : : ; \u000b n)is characterized by the set of zero coordinates and there are at most 2n\u00001", "entities": []}, {"text": "461such sets ( all coordinates can not be zero ) .", "entities": []}, {"text": "This restriction on the number of values on atholds regardless of the value of pt .", "entities": []}, {"text": "If the task requires the network to produce values of atthat come from a set with size at least 2n , then the network will not be able to perform the task .", "entities": []}, {"text": "Here \u2019s an example task : given a number \u00012(0;1 ) , the network must produce numbers 0;\u0001;2\u0001;:::;k", "entities": []}, {"text": "\u0001 , wherek is the maximum integer such that k\u0001\u00141 .", "entities": []}, {"text": "If the network receives a single input \u0001 , then it is easy to see that the vector atwill be a constant ( ve 1 ) at any step and hence the output of the network will also be constant at all steps .", "entities": []}, {"text": "Thus , the model can not perform such a task .", "entities": []}, {"text": "If the input is combined with n\u00001auxiliary symbols ( such as # and$ ) , then in the network , each attakes on at most 2n\u00001values .", "entities": []}, {"text": "Hence , the model will be incapable of performing the task if \u0001<1=2n .", "entities": []}, {"text": "Such a limitation does not exist with a residual connection since the vector at = Pn i=1 \u000b i ve i+ptcan take arbitrary number of values depending on its prior computations in pt .", "entities": [[8, 10, "MethodName", "residual connection"]]}, {"text": "For further details , see Sec . C.1 in the Appendix .", "entities": []}, {"text": "Discussion .", "entities": []}, {"text": "It is perhaps surprising that residual connection , originally proposed to assist in the learning ability of very deep networks , plays a vital role in the computational expressiveness of the network .", "entities": [[5, 7, "MethodName", "residual connection"]]}, {"text": "Without it , the model is limited in its capability to make decisions based on predictions in the previous steps .", "entities": []}, {"text": "We explore practical implications of this result in section 5 . 5 Experiments In this section , we explore the practical implications of our results .", "entities": []}, {"text": "Our experiments are geared towards answering the following questions : Q1.Are there any practical implications of the limitation of Transformers without decoder - encoder residual connections ?", "entities": []}, {"text": "What tasks can they do or not do compared to vanilla Transformers ?", "entities": []}, {"text": "Q2 .", "entities": []}, {"text": "Is there any additional bene\ufb01t of using positional masking as opposed to absolute positional encoding ( Vaswani et al . , 2017 ) ?", "entities": []}, {"text": "Although we showed that Transformers without decoder - encoder residual connection are not Turing complete , it does not imply that they are incapable of performing all the tasks .", "entities": [[9, 11, "MethodName", "residual connection"]]}, {"text": "Our results suggest that they are limited in their capability to make inferences based on their previous computations , which is required for tasks such as counting and language modeling .", "entities": []}, {"text": "However , it can be shown that the modelis capable of performing tasks which rely only on information provided at a given step such as copying and mapping .", "entities": []}, {"text": "For such tasks , given positional information at a particular step , the model can look up the corresponding input and map it via the FFN .", "entities": []}, {"text": "We evaluate these hypotheses via our experiments .", "entities": []}, {"text": "Model Copy Task Counting Vanilla Transformers 100.0 100.0 - Dec - Enc Residual 99.7 0.0 - Dec - Dec Residual 99.7 99.8 Table 1 : BLEU scores ( \" ) for copy and counting task .", "entities": [[25, 26, "MetricName", "BLEU"]]}, {"text": "Please see Section 5 for details For our experiments on synthetic data , we consider two tasks , namely the copy task and the counting task .", "entities": []}, {"text": "For the copy task , the goal of a model is to reproduce the input sequence .", "entities": []}, {"text": "We sample sentences of lengths between 5 - 12 words from Penn Treebank and create a train - test split of 40k-1k with all sentences belonging to the same range of length .", "entities": [[11, 13, "DatasetName", "Penn Treebank"]]}, {"text": "In the counting task , we create a very simple dataset where the model is given one number between 0 and 100 as input and its goal is to predict the next \ufb01ve numbers .", "entities": [[19, 20, "DatasetName", "0"]]}, {"text": "Since only a single input is provided to the encoder , it is necessary for the decoder to be able to make inferences based on its previous predictions to perform this task .", "entities": []}, {"text": "The bene\ufb01t of conducting these experiments on synthetic data is that they isolate the phenomena we wish to evaluate .", "entities": []}, {"text": "For both these tasks , we compare vanilla Transformer with the one without decoder - encoder residual connection .", "entities": [[8, 9, "MethodName", "Transformer"], [16, 18, "MethodName", "residual connection"]]}, {"text": "As a baseline we also consider the model without decoder - decoder residual connection , since according to our results , that connection does not in\ufb02uence the computational power of the model .", "entities": [[12, 14, "MethodName", "residual connection"]]}, {"text": "We implement a single layer encoderdecoder network with only a single attention head in each block .", "entities": []}, {"text": "We then assess the in\ufb02uence of the limitation on Machine Translation which requires a model to do a combination of both mapping and inferring from computations in previous timesteps .", "entities": [[9, 11, "TaskName", "Machine Translation"]]}, {"text": "We evaluate the models on IWSLT\u201914 German - English dataset and IWSLT\u201915 English - Vietnamese dataset .", "entities": []}, {"text": "We again compare vanilla Transformer with the ones without decoder - encoder and decoder - decoder residual connection .", "entities": [[4, 5, "MethodName", "Transformer"], [16, 18, "MethodName", "residual connection"]]}, {"text": "While tuning the models , we vary the number of layers from 1 to 4 , the learning rate , warmup steps and the number of heads .", "entities": [[8, 11, "HyperparameterName", "number of layers"], [17, 19, "HyperparameterName", "learning rate"]]}, {"text": "Speci\ufb01cations of the models , experimental setup , datasets and sample outputs can be found in Sec .", "entities": []}, {"text": "E", "entities": []}, {"text": "462Model De - En En - Vi Vanilla Transformers 32.9 28.8 - Dec - Enc Residual 24.1 21.8 - Dec - Dec Residual 30.6 27.2 Table 2 : BLEU scores ( \" ) for translation task .", "entities": [[28, 29, "MetricName", "BLEU"]]}, {"text": "Please see Section 5 for details .", "entities": []}, {"text": "in the Appendix .", "entities": []}, {"text": "Results on the effect of residual connections on synthetic tasks can be found in Table 1 .", "entities": []}, {"text": "As per our hypothesis , all the variants are able to perfectly perform the copy task .", "entities": []}, {"text": "For the counting task , the one without decoder - encoder residual connection is incapable of performing it .", "entities": [[11, 13, "MethodName", "residual connection"]]}, {"text": "However , the other two including the one without decoder - decoder residual connection are able to accomplish the task by learning to make decisions based on their prior predictions .", "entities": [[12, 14, "MethodName", "residual connection"]]}, {"text": "Table 3 provides some illustrative sample outputs of the models .", "entities": []}, {"text": "For the MT task , results can be found in Table 2 .", "entities": []}, {"text": "While the drop from removing decoder - encoder residual connection is signi\ufb01cant , it is still able to perform reasonably well since the task can be largely ful\ufb01lled by mapping different words from one sentence to another .", "entities": [[8, 10, "MethodName", "residual connection"]]}, {"text": "For positional masking , our proof technique suggests that due to lack of positional encodings , the model must come up with its own mechanism to make order related decisions .", "entities": []}, {"text": "Our hypothesis is that , if it is able to develop such a mechanism , it should be able to generalize to higher lengths and not over\ufb01t on the data it is provided .", "entities": []}, {"text": "To evaluate this claim , we simply extend the copy task upto higher lengths .", "entities": []}, {"text": "The training set remains the same as before , containing sentences of length 5 - 12 words .", "entities": []}, {"text": "We create 5 different validation sets each containing 1k sentences each .", "entities": []}, {"text": "The \ufb01rst set contains sentences within the same length as seen in training ( 5 - 12 words ) , the second set contains sentences of length 13 - 15 words while the third , fourth and \ufb01fth sets contain sentences of lengths 15 - 20 , 21 - 25 and 26 - 30 words respectively .", "entities": []}, {"text": "We consider two models , one which is provided absolute positional encodings and one where only positional masking is applied .", "entities": []}, {"text": "Figure 3 shows the performance of these models across various lengths .", "entities": []}, {"text": "The model with positional masking clearly generalizes up to higher lengths although its performance too degrades at extreme lengths .", "entities": []}, {"text": "We found that the model with absolute positional encodings during training over\ufb01ts on the fact that the 13th token is always the terminal symbol .", "entities": []}, {"text": "Hence , when evaluFigure 3 : Performance of the two models on the copy task across varying lengths of test inputs .", "entities": []}, {"text": "DiSAN refers to Transformer with only positional masking .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "SAN refers to vanilla Transformers .", "entities": []}, {"text": "ated on higher lengths it never produces a sentence of length greater than 12 .", "entities": []}, {"text": "Other encoding schemes such as relative positional encodings ( Shaw et al . , 2018 ; Dai et al . , 2019 ) can generalize better , since they are inherently designed to address this particular issue .", "entities": []}, {"text": "However , our goal is not to propose masking as a replacement of positional encodings , rather it is to determine whether the mechanism that the model develops during training is helpful in generalizing to higher lengths .", "entities": []}, {"text": "Note that , positional masking was not devised by keeping generalization or any other bene\ufb01t in mind .", "entities": []}, {"text": "Our claim is only that , the use of masking does not limit the model \u2019s expressiveness and it may bene\ufb01t in other ways , but during practice one should explore each of the mechanisms and even a combination of both .", "entities": []}, {"text": "Yang et al .", "entities": []}, {"text": "( 2019 ) showed that a combination of both masking and encodings is better able to learn order information as compared to explicit encodings .", "entities": []}, {"text": "SOURCE \u2013 42 REFERENCE \u2013 43 44 45 46 47 VANILLA TRANSFORMER \u2013 43 44 45 46 47 - DEC - ENCRESIDUAL \u2013 27 27 27 27 27 - DEC - DECRESIDUAL \u2013 43 44 45 46 47 Table 3 : Sample outputs by the models on the counting task .", "entities": []}, {"text": "Without the residual connection around DecoderEncoder block , the model is incapable of predicting more than one distinct output .", "entities": [[2, 4, "MethodName", "residual connection"]]}, {"text": "6 Discussion and Final Remarks We showed that the class of languages recognized by Transformers and RNNs are exactly the same .", "entities": []}, {"text": "This implies that the difference in performance of both the networks across different tasks can be attributed only to their learning abilities .", "entities": []}, {"text": "In contrast to RNNs , Transformers are composed of multiple components which are not essential for their com-", "entities": []}, {"text": "463putational expressiveness .", "entities": []}, {"text": "However , in practice they may play a crucial role .", "entities": []}, {"text": "Recently , V oita et al .", "entities": []}, {"text": "( 2019 ) showed that the decoder - decoder attention heads in the lower layers of the decoder do play a signi\ufb01cant role in the NMT task and suggest that they may be helping in language modeling .", "entities": []}, {"text": "This indicates that components which are not essential for the computational power may play a vital role in improving the learning and generalization ability .", "entities": []}, {"text": "Take - Home Messages .", "entities": []}, {"text": "We showed that the order information can be provided either in the form of explicit encodings or masking without affecting computational power of Transformers .", "entities": []}, {"text": "The decoder - encoder attention block plays a necessary role in conditioning the computation on the input sequence while the residual connection around it is necessary to keep track of previous computations .", "entities": [[20, 22, "MethodName", "residual connection"]]}, {"text": "The feedforward network in the decoder is the only component capable of performing computations based on the input and prior computations .", "entities": [[1, 3, "MethodName", "feedforward network"]]}, {"text": "Our experimental results show that removing components essential for computational power inhibit the model \u2019s ability to perform certain tasks .", "entities": []}, {"text": "At the same time , the components which do not play a role in the computational power may be vital to the learning ability of the network .", "entities": []}, {"text": "Although our proofs rely on arbitrary precision , which is common practice while studying the computational power of neural networks in theory ( Siegelmann and Sontag , 1992 ; P \u00b4 erez et al . , 2019 ; Hahn , 2020 ; Yun et al . , 2020 ) , implementations in practice work over \ufb01xed precision settings .", "entities": []}, {"text": "However , our construction provides a starting point to analyze Transformers under \ufb01nite precision .", "entities": []}, {"text": "Since RNNs can recognize all regular languages in \ufb01nite precision ( Korsky and Berwick , 2019 ) , it follows from our construction that Transformer can also recognize a large class of regular languages in \ufb01nite precision .", "entities": [[24, 25, "MethodName", "Transformer"]]}, {"text": "At the same time , it does not imply that it can recognize all regular languages given the limitation due to the precision required to encode positional information .", "entities": []}, {"text": "We leave the study of Transformers in \ufb01nite precision for future work .", "entities": []}, {"text": "Acknowledgements We thank the anonymous reviewers for their constructive comments and suggestions .", "entities": []}, {"text": "We would also like to thank our colleagues at Microsoft Research and Michael Hahn for their valuable feedback and helpful discussions .", "entities": []}, {"text": "References Yining Chen , Sorcha Gilroy , Andreas Maletti , Jonathan May , and Kevin Knight .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Recurrent neural networks as weighted language recognizers .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 2261\u20132271 , New Orleans , Louisiana . Association for Computational Linguistics .", "entities": []}, {"text": "Zihang Dai , Zhilin Yang , Yiming Yang , Jaime Carbonell , Quoc Le , and Ruslan Salakhutdinov .", "entities": [[16, 17, "DatasetName", "Ruslan"]]}, {"text": "2019 .", "entities": []}, {"text": "Transformer - XL : Attentive language models beyond a \ufb01xed - length context .", "entities": [[0, 3, "MethodName", "Transformer - XL"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2978\u20132988 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Michael Hahn .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Theoretical limitations of selfattention in neural sequence models .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 8:156\u2013171 .", "entities": []}, {"text": "Jiri Hron , Yasaman Bahri , Jascha Sohl - Dickstein , and Roman Novak .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "In\ufb01nite attention : Nngp and ntk for deep attention networks .", "entities": [[5, 6, "MethodName", "ntk"], [7, 9, "TaskName", "deep attention"]]}, {"text": "arXiv preprint arXiv:2006.10540 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Cheng - Zhi Anna Huang , Ashish Vaswani , Jakob Uszkoreit , Noam Shazeer , Curtis Hawthorne , Andrew M. Dai , Matthew D. Hoffman , and Douglas Eck . 2018 .", "entities": []}, {"text": "An improved relative self - attention mechanism for transformer with application to music generation .", "entities": [[12, 14, "TaskName", "music generation"]]}, {"text": "ArXiv , abs/1809.04281 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Hyunjik Kim , George Papamakarios , and Andriy Mnih . 2020 .", "entities": []}, {"text": "The lipschitz constant of self - attention .", "entities": []}, {"text": "arXiv preprint arXiv:2006.04710 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Guillaume Klein , Yoon Kim , Yuntian Deng , Jean Senellart , and Alexander Rush . 2017 .", "entities": []}, {"text": "OpenNMT : Opensource toolkit for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of ACL 2017 , System Demonstrations , pages 67\u201372 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "John F Kolen and Stefan C Kremer .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "A \ufb01eld guide to dynamical recurrent networks .", "entities": []}, {"text": "John Wiley & Sons .", "entities": []}, {"text": "Samuel A Korsky and Robert C Berwick .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "On the computational power of rnns .", "entities": []}, {"text": "arXiv preprint arXiv:1906.06349 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "464Yoav Levine , Noam Wies , Or Sharir , Ho\ufb01t Bata , and Amnon Shashua . 2020 .", "entities": []}, {"text": "Limits to depth ef\ufb01ciencies of self - attention .", "entities": []}, {"text": "arXiv preprint arXiv:2006.12467 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Minh - Thang Luong and Christopher D Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Stanford neural machine translation systems for spoken language domains .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the International Workshop on Spoken Language Translation , pages 76\u201379 .", "entities": [[9, 10, "TaskName", "Translation"]]}, {"text": "Warren S McCulloch and Walter Pitts .", "entities": []}, {"text": "1943 .", "entities": []}, {"text": "A logical calculus of the ideas immanent in nervous activity .", "entities": []}, {"text": "The bulletin of mathematical biophysics , 5(4):115 \u2013 133 .", "entities": []}, {"text": "William Merrill , Gail Weiss , Yoav Goldberg , Roy Schwartz , Noah A. Smith , and Eran Yahav .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A formal hierarchy of RNN architectures .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 443\u2013459 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Myle Ott , Sergey Edunov , David Grangier , and Michael Auli .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Scaling neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 1\u20139 , Brussels , Belgium .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jorge P \u00b4 erez , Javier Marinkovi \u00b4 c , and Pablo Barcel \u00b4 o. 2019 .", "entities": []}, {"text": "On the turing completeness of modern neural network architectures .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training .", "entities": []}, {"text": "URL https://s3 - us - west-2 .", "entities": []}, {"text": "amazonaws .", "entities": []}, {"text": "com / openaiassets / researchcovers / languageunsupervised / language understanding paper .", "entities": []}, {"text": "pdf .", "entities": []}, {"text": "Alexander Rush .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The annotated transformer .", "entities": []}, {"text": "InProceedings of Workshop for NLP Open Source Software ( NLP - OSS ) , pages 52\u201360 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Luzi Sennhauser and Robert Berwick .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Evaluating the ability of LSTMs to learn context - free grammars .", "entities": []}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 115\u2013124 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Peter Shaw , Jakob Uszkoreit , and Ashish Vaswani . 2018 .", "entities": []}, {"text": "Self - attention with relative position representations .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 464\u2013468 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tao Shen , Tianyi Zhou , Guodong Long , Jing Jiang , Shirui Pan , and Chengqi Zhang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Disan : Directional self - attention network for rnn / cnn - free language understanding .", "entities": [[3, 7, "MethodName", "self - attention network"]]}, {"text": "In Thirty - Second AAAI Conference on Arti\ufb01cial Intelligence", "entities": []}, {"text": ".Hava", "entities": []}, {"text": "T Siegelmann .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Neural networks and analog computation : beyond the Turing limit .", "entities": []}, {"text": "Springer Science & Business Media .", "entities": []}, {"text": "Hava T Siegelmann and Eduardo D Sontag .", "entities": []}, {"text": "1992 .", "entities": []}, {"text": "On the computational power of neural nets .", "entities": []}, {"text": "In Proceedings of the \ufb01fth annual workshop on Computational learning theory , pages 440\u2013449 .", "entities": []}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Natalia Skachkova , Thomas Trost , and Dietrich Klakow .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Closing brackets with recurrent neural networks .", "entities": []}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 232\u2013239 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yao - Hung Hubert Tsai , Shaojie Bai , Makoto Yamada , Louis - Philippe Morency , and Ruslan Salakhutdinov .", "entities": [[18, 19, "DatasetName", "Ruslan"]]}, {"text": "2019 .", "entities": []}, {"text": "Transformer dissection : An uni\ufb01ed understanding for transformer \u2019s attention via the lens of kernel .", "entities": [[0, 1, "MethodName", "Transformer"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 4344\u20134353 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008 .", "entities": []}, {"text": "Elena V oita , David Talbot , Fedor Moiseev , Rico Sennrich , and Ivan Titov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Analyzing multi - head self - attention : Specialized heads do the heavy lifting , the rest can be pruned .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5797\u20135808 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Gail Weiss , Yoav Goldberg , and Eran Yahav .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "On the practical computational power of \ufb01nite precision RNNs for language recognition .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 740\u2013745 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Baosong Yang , Longyue Wang , Derek F. Wong , Lidia S. Chao , and Zhaopeng Tu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Assessing the ability of self - attention networks to learn word order .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3635\u20133644 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Chulhee Yun , Srinadh Bhojanapalli , Ankit Singh Rawat , Sashank Reddi , and Sanjiv Kumar .", "entities": [[15, 16, "DatasetName", "Kumar"]]}, {"text": "2020 .", "entities": []}, {"text": "Are transformers universal approximators of sequence - to - sequence functions ?", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "465A Roadmap We begin with various de\ufb01nitions and results .", "entities": []}, {"text": "We de\ufb01ne simulation of Turing machines by RNNs and state the Turing - completeness result for RNNs .", "entities": []}, {"text": "We de\ufb01ne vanilla and directional Transformers and what it means for Transformers to simulate RNNs .", "entities": []}, {"text": "Many of the de\ufb01nitions from the main paper are reproduced here , but in more detail .", "entities": []}, {"text": "In Sec .", "entities": []}, {"text": "C.1 we discuss the effect of removing a residual connection on computational power of Transformers .", "entities": [[8, 10, "MethodName", "residual connection"]]}, {"text": "Sec .", "entities": []}, {"text": "C.2 contains the proof of Turing completeness of vanilla Transformers and Sec .", "entities": []}, {"text": "D the corresponding proof for directional Transformers .", "entities": []}, {"text": "Finally , Sec . 5 has further details of experiments .", "entities": []}, {"text": "B De\ufb01nitions", "entities": []}, {"text": "Denote the setf1;2;:::;ngby[n ] .", "entities": []}, {"text": "Functions de\ufb01ned for scalars are extended to vectors in the natural way : for a function Fde\ufb01ned on a set A , for a sequence ( a1;:::;an)of elements in A , we setF(a1;:::;an):= ( F(a1);:::;F ( an ) ) .", "entities": []}, {"text": "IndicatorI(P)is1 , if predicate Pis true and is 0 otherwise .", "entities": [[8, 9, "DatasetName", "0"]]}, {"text": "For a sequence X= ( xn0;:::;xn ) for somen0\u00150 , we setXj:= ( xn0;:::;xj)for j2fn0;i+1;:::;ng .", "entities": []}, {"text": "We will work with an alphabet\u0006 = f \f 1 ; : : : ; \f mg , with \f 1= # and \f m= $ .", "entities": []}, {"text": "The special symbols # and$correspond to the beginning and end of the input sequence , resp .", "entities": []}, {"text": "For a vectorv , by0vwe mean the all- 0vector of the same dimension as v. Let \u0016t:= minft;ng B.1 RNNs and Turing - completeness Here we summarize , somewhat informally , the Turing - completeness result for RNNs due to ( Siegelmann and Sontag , 1992 ) .", "entities": []}, {"text": "We recall basic notions from computability theory .", "entities": []}, {"text": "In the main paper , for simplicity we stated the results for total recursive functions \u001e : f0;1g\u0003!f0;1g\u0003 , i.e. a function that is de\ufb01ned on every s2 f0;1g\u0003 and whose values can be computed by a Turing machine .", "entities": []}, {"text": "While total recursive functions form a satisfactory formalization of seq - to - seq tasks , here we state the more general result for partial recursive functions .", "entities": []}, {"text": "Let \u001e : f0;1g\u0003!f0;1g\u0003be partial recursive .", "entities": []}, {"text": "A partial recursive function is one that need not be de\ufb01ned for every s2 f0;1g\u0003 , and there exists a Turing Machine Mwith the following property .", "entities": []}, {"text": "The input sis initially written on the tape of the Turing Machine Mand the output \u001e ( s ) is the content of the tape upon acceptance whichis indicated by halting in a designated accept state .", "entities": []}, {"text": "Onsfor which \u001e is unde\ufb01ned , Mdoes not halt .", "entities": []}, {"text": "We now specify how Turing machine Mis simulated by RNN R(M ) .", "entities": []}, {"text": "In the RNNs in ( Siegelmann and Sontag , 1992 ) the hidden state hthas the form ht=", "entities": []}, {"text": "[ qt ; \t 1 ; \t 2 ] ; whereqt= [ q1;:::;qs]denotes the state of M one - hot form .", "entities": []}, {"text": "Numbers \t 1 ; \t 22Q , called stacks , store the contents of the tape in a certain Cantor set like encoding ( which is similar to , but slightly more involved , than binary representation ) at each step .", "entities": []}, {"text": "The simulating RNN R(M ) , gets as input encodings of s1s2:::snin the \ufb01rstnsteps , and from then on receives the vector 0as input in each step .", "entities": []}, {"text": "If \u001e is de\ufb01ned on s , thenMhalts and accepts with the output \u001e ( s)the content of the tape .", "entities": []}, {"text": "In this case , R(M)enters a special accept state , and \t 1 encodes \u001e ( s)and \t 2= 0 .", "entities": [[19, 20, "DatasetName", "0"]]}, {"text": "IfMdoes not halt then R(M)also does not enter the accept state .", "entities": []}, {"text": "Siegelmann and Sontag ( 1992 ) further show that fromR(M)one can further explicitly produce the \u001e ( s)as its output .", "entities": []}, {"text": "In the present paper , we will not deal with explicit production of the output but rather work with the de\ufb01nition of simulation in the previous paragraph .", "entities": []}, {"text": "This is for simplicity of exposition , and the main ideas are already contained in our results .", "entities": []}, {"text": "If the Turing machine computes \u001e ( s ) in timeT(s ) , the simulation takes O(jsj)time to encode the input sequence sand4T(s)to compute \u001e ( s ) .", "entities": []}, {"text": "Theorem B.1 ( ( Siegelmann and Sontag , 1992 ) ) .", "entities": []}, {"text": "Given any partial recursive function \u001e : f0;1g\u0003 ! f0;1g\u0003computed by Turing machine M \u001e , there exists a simulating RNN R(M \u001e ) .", "entities": []}, {"text": "In view of the above theorem , for establishing Turing - completeness of Transformers , it suf\ufb01ces to show that RNNs can be simulated by Transformers .", "entities": []}, {"text": "Thus , in the sequel we will only talk about simulating RNNs .", "entities": []}, {"text": "B.2 Vanilla Transformer Architecture", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "Here we describe the original transformer architecture due to ( Vaswani et al . , 2017 ) as formalized by ( P \u00b4 erez et al . , 2019 ) .", "entities": []}, {"text": "While our notation and de\ufb01nitions largely follow ( P \u00b4 erez et al . , 2019 ) , they are not identical .", "entities": []}, {"text": "The transformer here makes use of positional encoding ; later we will discuss the transformer variant using directional attention but without using positional encoding .", "entities": []}, {"text": "466The transformer , denoted Trans , is a sequenceto - sequence architecture .", "entities": []}, {"text": "Its input consists of ( i ) a sequence X= ( x1;:::;xn)of vectors in Qd , ( ii ) a seed vector y02Qd .", "entities": []}, {"text": "The output is a sequenceY= ( y1;:::;yr)of vectors in Qd .", "entities": []}, {"text": "The sequence Xis obtained from the sequence ( s0;:::;sn)2\u0006n+1of symbols by using the embedding mentioned earlier : xi = f(fb(si);pos(i ) ) for0\u0014i\u0014n .", "entities": []}, {"text": "The transformer consists of composition of transformer encoder and a transformer decoder .", "entities": [[10, 12, "MethodName", "transformer decoder"]]}, {"text": "The transformer encoder is obtained by composing one or more single - layer encoders and similarly the transformer decoder is obtained by composing one or more single - layer decoders .", "entities": [[17, 19, "MethodName", "transformer decoder"]]}, {"text": "For the feed - forward networks in the transformer layers we use the activation as in ( Siegelmann and Sontag , 1992 ) , namely the saturated linear activation function : \u001b(x )", "entities": [[28, 30, "HyperparameterName", "activation function"]]}, {"text": "= 8 > < > :0 ifx<0 ; x if0\u0014x\u00141 ; 1 ifx>1:(4 ) As mentioned in the main paper , we can easily work with the standard ReLU activation via \u001b(x )", "entities": [[28, 29, "MethodName", "ReLU"]]}, {"text": "= ReLU ( x)\u0000ReLU ( x\u00001 ) .", "entities": [[1, 2, "MethodName", "ReLU"]]}, {"text": "In the following , after de\ufb01ning these components , we will put them together to specify the full transformer architecture .", "entities": []}, {"text": "But we begin with self - attention mechanism which is the central feature of the transformer .", "entities": []}, {"text": "Self - attention .", "entities": []}, {"text": "The self - attention mechanism takes as input ( i ) a query vectorq , ( ii ) a sequence ofkeyvectorsK= ( k1;:::;kn ) , and ( iii ) a sequence of value vectorsV= ( v1;:::;vn ) .", "entities": []}, {"text": "All vectors are in Qd .", "entities": []}, {"text": "Theq - attention over keys Kand valuesV , denoted by Att(q;K;V ) , is a vectoragiven by ( \u000b 1 ; : : : ; \u000b n ) = \u001a(fatt(q;k1);:::;fatt(q;kn ) ) ; a= \u000b 1v1 + \u000b 2v2+\u0001\u0001\u0001+ \u000b nvn : The above de\ufb01nition uses two functions \u001aand", "entities": []}, {"text": "fattwhich we now describe .", "entities": []}, {"text": "For the normalization function \u001a:Qn!Qn \u00150we will use hardmax : forx= ( x1;:::;xn)2Qn , if the maximum value occurs rtimes among x1;:::;xn , then hardmax ( x)i:= 1 = rifxiis a maximum value andhardmax ( x)i:= 0otherwise .", "entities": []}, {"text": "In practice , the softmax is often used but its output values are in general not rational .", "entities": [[4, 5, "MethodName", "softmax"]]}, {"text": "The names soft - attention andhard - attention are used for the attention mechanism depending on which normalization function is used .", "entities": []}, {"text": "For the Turing - completeness proof of vanilla transformers , the scoring function fattused is a combination of multiplicative attention ( Vaswani et", "entities": [[18, 20, "MethodName", "multiplicative attention"]]}, {"text": "al . , 2017 ) and a non - linear function : fatt(q;ki )", "entities": []}, {"text": "= \u0000 \f\f hq;kii \f\f .", "entities": []}, {"text": "For directional transformers , the standard multiplicative attention is used , that is , fatt(q;ki ) = hq;kii .", "entities": [[6, 8, "MethodName", "multiplicative attention"]]}, {"text": "Transformer encoder .", "entities": [[0, 1, "MethodName", "Transformer"]]}, {"text": "Asingle - layer encoder is a function Enc(X;\u0012 ) , where\u0012is the parameter vector and the input X= ( x1;:::;xn)is a sequence of vector in Qd .", "entities": []}, {"text": "The output is another sequenceZ= ( z1;:::;zn)of vectors in Qd .", "entities": []}, {"text": "The parameters\u0012specify functions Q(\u0001);K(\u0001);V(\u0001 ) , andO(\u0001 ) , all of type Qd!Qd .", "entities": []}, {"text": "The functions Q(\u0001);K(\u0001);andV(\u0001)are usually linear transformations and this will be the case in our constructions : Q(xi )", "entities": []}, {"text": "= xT iWQ ; K(xi )", "entities": []}, {"text": "= xT iWK ; V(xi )", "entities": []}, {"text": "= xT iWV ; whereWQ;WK;WV2Qd\u0002d .", "entities": []}, {"text": "The function O(\u0001 ) is a feed - forward network .", "entities": []}, {"text": "The single - layer encoder is then de\ufb01ned by ai= Att(Q(xi);K(X);V(X ) )", "entities": []}, {"text": "+ xi;(5 ) zi = O(ai )", "entities": []}, {"text": "+ ai : The addition operations + xiand+aiare the residual connections .", "entities": []}, {"text": "The operation in ( 5 ) is called the encoder - encoder attention block .", "entities": []}, {"text": "The complete L - layer transformer encoder TEnc(L)(X;\u0012)has the same input X = ( x1;:::;xn)as the single - layer encoder .", "entities": []}, {"text": "By contrast , its output consists of two sequences ( Ke;Ve ) , each a sequence of nvectors in Qd .", "entities": []}, {"text": "The encoder TEnc(L)(\u0001)is obtained by repeated application of single - layer encoders , each with its own parameters ; and at the end , two trasformation functionsKL(\u0001)andVL(\u0001)are applied to the sequence of output vectors at the last layer .", "entities": []}, {"text": "Functions K(L)(\u0001)andV(L)(\u0001)are linear transformations in our constructions .", "entities": []}, {"text": "Formally , for 1\u0014`\u0014L\u00001 andX1:=X , we have X`+1= Enc(X`;\u0012 ` ) ;", "entities": []}, {"text": "Ke = K(L)(XL ) ; Ve = V(L)(XL ):", "entities": []}, {"text": "467The output of the L - layer Transformer encoder ( Ke;Ve ) = TEnc(L)(X)is fed to the Transformer decoder which we describe next .", "entities": [[7, 8, "MethodName", "Transformer"], [17, 19, "MethodName", "Transformer decoder"]]}, {"text": "Transformer decoder .", "entities": [[0, 2, "MethodName", "Transformer decoder"]]}, {"text": "The input to a singlelayer decoder is ( i ) ( Ke;Ve ) , the sequences of key and value vectors output by the encoder , and ( ii ) a sequence Y= ( y1;:::;yk)of vectors in Qd .", "entities": []}, {"text": "The output is another sequence Z= ( z1;:::;zk ) of vectors in Qd .", "entities": []}, {"text": "Similar to the single - layer encoder , a singlelayer decoder is parameterized by functions Q(\u0001);K(\u0001);V(\u0001)andO(\u0001)and is de\ufb01ned by pt= Att(Q(yt);K(Yt);V(Yt ) )", "entities": []}, {"text": "+ yt;(6 ) at= Att(pt;Ke;Ve )", "entities": []}, {"text": "+ pt ; ( 7 ) zt = O(at )", "entities": []}, {"text": "+ at : The operation in ( 6 ) will be referred to as the decoder - decoder attention block and the operation in ( 7 ) as the decoder - encoder attention block .", "entities": []}, {"text": "In the decoder - decoder attention block , positional masking is applied to prevent the network from attending over symbols which are ahead of them .", "entities": []}, {"text": "AnL - layer Transformer decoder is obtained by repeated application of Lsingle - layer decoders each with its own parameters and a transformation functionF : Qd!Qdapplied to the last vector in the sequence of vectors output by the \ufb01nal decoder .", "entities": [[3, 5, "MethodName", "Transformer decoder"]]}, {"text": "Formally , for 1\u0014`\u0014L\u00001andY1 = Ywe have Y`+1= Dec((Ke;Ve);Y`;\u0012 ` ) ; z = F(yL t ):", "entities": []}, {"text": "We usez= TDecL((Ke;Ve);Y;\u0012)to denote anL - layer Transformer decoder .", "entities": [[7, 9, "MethodName", "Transformer decoder"]]}, {"text": "Note that while the output of a single - layer decoder is a sequence of vectors , the output of an L - layer Transformer decoder is a single vector .", "entities": [[24, 26, "MethodName", "Transformer decoder"]]}, {"text": "The complete Transformer .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "ATransformer network receives an input sequence X , a seed vector y0 , andr2N. Fort\u00150its output is a sequence Y= ( y1;:::;yr)de\ufb01ned by ~yt+1= TDec\u0000 TEnc(X);(y0;y1;:::;yt)\u0001 : We getyt+1by adding positional encoding : yt+1=~yt+1 + pos(t+ 1 ) .", "entities": []}, {"text": "We denote the complete Transformer by Trans(X;y0 )", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "= Y. The Transformer \u201c halts \u201d when yT2H , whereHis a prespeci\ufb01ed halting set .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "Simulation of RNNs by Transformers .", "entities": []}, {"text": "We say that a Transformer simulates an RNN ( as de\ufb01ned in Sec . B.1 ) if on input s2\u0006\u0003 , at each step t , the vectorytcontains the hidden state htas a subvector : yt=", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "[ ht;\u0001 ] , and halts at the same step as RNN .", "entities": []}, {"text": "C Results on Vanilla Transformers C.1 Residual Connections Proposition C.1 .", "entities": []}, {"text": "The Transformer without residual connection around the Decoder - Encoder Attention block in the Decoder is not Turing Complete Proof .", "entities": [[1, 2, "MethodName", "Transformer"], [3, 5, "MethodName", "residual connection"]]}, {"text": "Recall that the vectors atis produced from the Encoder - Decoder Attention block in the following way , at= Att(pt;Ke;Ve )", "entities": [[0, 1, "MetricName", "Recall"], [4, 5, "DatasetName", "atis"]]}, {"text": "+ pt", "entities": []}, {"text": "The result follows from the observation that without the residual connections , at= Att(pt;Ke;Ve ) , which leads to at = Pn i=1 \u000b i ve i for some \u000b is such", "entities": []}, {"text": "thatPn i \u000b i= 1 .", "entities": []}, {"text": "Sinceve iis produced from the encoder , the vector atwill have no information about its previous hidden state values .", "entities": []}, {"text": "Since the previous hidden state information was computed and stored in pt , without the residual connection , the information in atdepends solely on the output of the encoder .", "entities": [[15, 17, "MethodName", "residual connection"]]}, {"text": "One could argue that since the attention weights \u000b is depend on the query vector pt , it could still use it gain the necessary information from the vectors ve is .", "entities": []}, {"text": "However , note that by de\ufb01nition of hard attention , the attention weights \u000b iinat", "entities": []}, {"text": "= Pn i=1", "entities": []}, {"text": "i ve i can either be zero or some nonzero value depending on the attention logits .", "entities": []}, {"text": "Since the attention weights \u000b iare such thatPn i \u000b i= 1 and all the nonzero weights are equal to each other .", "entities": []}, {"text": "Thus given the constraints there are 2n\u00001ways to attend over ninputs excluding the case where no input is attended over .", "entities": []}, {"text": "Hence , the network without decoder - encoder residual connection with ninputs can have at most 2n\u00001 distinctatvalues .", "entities": [[8, 10, "MethodName", "residual connection"]]}, {"text": "This implies that the model will be unable to perform a task that takes ninputs and has to produce more than 2n\u00001outputs .", "entities": []}, {"text": "Note that , such a limitation will not exist with a residual connection since the vector at= \u0006n i=1 \u000b i ve i+pt can take arbitrary number of values depending on its prior computations in pt .", "entities": [[11, 13, "MethodName", "residual connection"]]}, {"text": "As an example to illustrate the limitation , consider the following simple problem , given a value \u0001 , where 0\u0014\u0001\u00141 , the network must produce", "entities": []}, {"text": "468the values 0;\u0001;2\u0001;:::;k", "entities": []}, {"text": "\u0001 , wherekis the maximum integer such that k\u0001\u00141 .", "entities": []}, {"text": "If the network receives a single input \u0001 , the encoder will produce only one particular output vector and regardless of what the value of the query vector ptis , the vector atwill be constant at every timestep .", "entities": []}, {"text": "Since atis fed to feedforward network which maps it to zt , the output of the decoder will remain the same at every timestep and it can not produce distinct values .", "entities": [[1, 2, "DatasetName", "atis"], [4, 6, "MethodName", "feedforward network"]]}, {"text": "If the input is combined with n\u00001auxiliary symbols ( such as # and$ ) , then the network can only produce 2n\u00001outputs .", "entities": []}, {"text": "Hence , the model will be incapable of performing the task if \u0001<1=2n .", "entities": []}, {"text": "Thus the model can not perform the task de\ufb01ned above which RNNs and Vanilla Transformers can easily do with a simple counting mechanism via their recurrent connection .", "entities": []}, {"text": "For the case of multilayer decoder , consider anyLlayer decoder model .", "entities": []}, {"text": "If the residual connection is removed , the output of decoder - encoder attention block at each layer is a ( ` ) t = Pn i=1 \u000b ( ` ) i ve i for1\u0014`\u0014L. Observe , that since output of the decoder - encoder attention block in the last ( L - th ) layer of the decoder is a(L ) t = Pn i=1 \u000b ( L ) i ve i. Since the output of the Llayer decoder will be a feedforward network over a(L ) t , the computation reduces to the single layer decoder case .", "entities": [[2, 4, "MethodName", "residual connection"], [83, 85, "MethodName", "feedforward network"]]}, {"text": "Hence , similar to the single layer case , if the task requires the network to produce values of atthat come from a set with size at least 2n , then the network will not be able to perform the task .", "entities": []}, {"text": "This implies that the model without decoderencoder residual connection is limited in its capability to perform tasks which requires it to make inferences based on previously generated outputs .", "entities": [[7, 9, "MethodName", "residual connection"]]}, {"text": "C.2 Simulation of RNNs by Transformers with positional encoding Theorem C.2 .", "entities": []}, {"text": "RNNs can be simulated by vanilla Transformers and hence the class of vanilla Transformers is Turing - complete .", "entities": []}, {"text": "Proof .", "entities": []}, {"text": "The construction of the simulating transformer is simple : it uses a single head and both the encoder and decoder have one layer .", "entities": []}, {"text": "Moreover , the encoder does very little and most of the action happens in the decoder .", "entities": []}, {"text": "The main task for the simulation is to design the input embedding ( building on the given base embedding fb ) , the feedforward networkO(\u0001)and", "entities": []}, {"text": "the matrices corresponding to functionsQ(\u0001);K(\u0001);V(\u0001).Input embedding .", "entities": []}, {"text": "The input embedding is obtained by summing the symbol and positional encodings which we next describe .", "entities": []}, {"text": "These encodings have dimension d= 2dh+db+ 2 , wheredhis the dimension of the hidden state of the RNN and db is the dimension of the given encoding fbof the input symbols .", "entities": []}, {"text": "We will use the symbol encoding fsymb:\u0006!Qdwhich is essentially the same as fbexcept that the dimension is now larger : fsymb(s ) =", "entities": []}, {"text": "[ 0dh;fe(s);0dh;0;0 ] : The positional encoding pos : N!Qdis simply pos(i ) =", "entities": []}, {"text": "[ 0dh;0db;0dh;i;1 ] : Together , these de\ufb01ne the combined embedding f for a given input sequence s0s1\u0001\u0001\u0001sn2\u0006\u0003by f(si )", "entities": []}, {"text": "= fsymb(si)+pos(i )", "entities": []}, {"text": "=", "entities": []}, {"text": "[ 0dh;fb(si);0dh;i;1 ] : The vectorsv2Qdused in the computation of our transformer are of the form v=", "entities": []}, {"text": "[ h1;s;h2;x1;x2 ] ; whereh1;h22Qdh;s2Qde;andx1;x22Q. The coordinates corresponding to the hi \u2019s are reserved for computation related to hidden states of theRNN , the coordinates corresponding to sare reserved for base embeddings , and those for x1 andx2are reserved for scalar values related to positional operations .", "entities": []}, {"text": "The \ufb01rst two blocks , corresponding toh1andsare reserved for computation of the RNN .", "entities": []}, {"text": "During the computation of the Transformer , the underlying RNN will get the input s\u0016tat steptfor t= 0;1 ; : : : , where recall that \u0016t= minft;ng : This sequence leads to the RNN getting the embedding of the input sequence s0;:::;snin the \ufb01rstn+ 1 steps followed by the embedding of the symbol $ for the subsequent steps , which is in accordance with the requirements of ( Siegelmann and Sontag , 1992 ) .", "entities": [[5, 6, "MethodName", "Transformer"]]}, {"text": "Similar to ( P \u00b4 erez et al . , 2019 ) we use the following scoring function in the attention mechanism in our construction , fatt(qi;kj )", "entities": []}, {"text": "= \u0000jhqi;kjij ( 8) Construction of TEnc .As", "entities": []}, {"text": "previously mentioned , our transformer encoder has only one layer , and the computation in the encoder is very simple : the attention mechanism is not utilized , only the residual connections are .", "entities": []}, {"text": "This is done by setting", "entities": []}, {"text": "469the matrix for V(\u0001)to the all - zeros matrix , and the feedforward networks to always output 0 .", "entities": [[17, 18, "DatasetName", "0"]]}, {"text": "The application of appropriately chosen linear transformations for the \ufb01nal K(\u0001)andV(\u0001)give the following lemma about the output of the encoder .", "entities": [[13, 14, "DatasetName", "lemma"]]}, {"text": "Lemma C.3 .", "entities": [[0, 1, "DatasetName", "Lemma"]]}, {"text": "There exists a single layer encoder denoted by TEnc that takes as input the sequence ( x1;:::;xn;$)and generates the tuple ( Ke;Ve ) whereKe= ( k1;:::;kn)andVe= ( v1;:::;vn ) such that , ki= [ 0h;0s;0h;\u00001;i ] ; vi= [ 0h;si;0h;0;0 ] : Construction of TDec .As", "entities": []}, {"text": "in the construction ofTEnc , our TDec has only one layer .", "entities": []}, {"text": "Also likeTEnc , the decoder - decoder attention block just computes the identity : we set V(1)(\u0001 )", "entities": []}, {"text": "= 0 identically , and use the residual connection so that pt = yt .", "entities": [[1, 2, "DatasetName", "0"], [7, 9, "MethodName", "residual connection"]]}, {"text": "Fort\u00150 , at thet - th step we denote the input to the decoder as yt=~yt+ pos(t ) .", "entities": []}, {"text": "Leth0= 0hand ~ y0=0 .", "entities": []}, {"text": "We will show by induction that at thet - th timestep we have yt=", "entities": []}, {"text": "[ ht;0s;0h;t+ 1;1 ] : ( 9 ) By construction , this is true for t= 0 :", "entities": [[16, 17, "DatasetName", "0"]]}, {"text": "y0=", "entities": []}, {"text": "[ 0h;0s;0h;1;1 ] :", "entities": []}, {"text": "Assuming that it holds for t , we show it for t+ 1 .", "entities": []}, {"text": "By Lemma C.5 Att(pt;Ke;Ve )", "entities": [[1, 2, "DatasetName", "Lemma"]]}, {"text": "=", "entities": []}, {"text": "[ 0h;vt+1;0h;0;0]:(10 ) Lemma C.5 basically shows how we retrieve the inputst+1at the relevant step for further computation in the decoder .", "entities": [[3, 4, "DatasetName", "Lemma"]]}, {"text": "It follows that at= Att(pt;Ke;Ve )", "entities": []}, {"text": "+", "entities": []}, {"text": "pt", "entities": []}, {"text": "=", "entities": []}, {"text": "[ ht;st+1;0h;t+ 1;1 ] : In the \ufb01nal block of the decoder , the computation for RNN takes place : Lemma C.4 .", "entities": [[20, 21, "DatasetName", "Lemma"]]}, {"text": "There exists a function O(\u0001)de\ufb01ned by feed - forward network such that , O(at )", "entities": []}, {"text": "=", "entities": []}, {"text": "[ ( ht+1\u0000ht);\u0000st+1;0h;\u0000(t+ 1);\u00001 ] ; whereWh;Wxandbdenote the parameters of the RNN under consideration .", "entities": []}, {"text": "This leads to zt = O(at )", "entities": []}, {"text": "+ at=", "entities": []}, {"text": "[ ht+1;0s;0h;0;0 ] : We choose the function Ffor our decoder to be the identity function , therefore ~yt+1=", "entities": []}, {"text": "[ ht+1;0s;0h;0;0 ] , which means yt+1=~yt+1 + pos(i+ 1 ) =", "entities": []}, {"text": "[ ht+1;0s;0h;t+ 2;1 ] , proving our induction hypothesis .", "entities": []}, {"text": "C.3 Technical Lemmas Proof of Lemma C.3 .", "entities": [[5, 6, "DatasetName", "Lemma"]]}, {"text": "We construct a single - layer encoder achieving the desired KeandVe .", "entities": []}, {"text": "We make use of the residual connections and via trivial selfattention we get that zi = xi .", "entities": []}, {"text": "More speci\ufb01cally fori2[n]we have V(1)(xi )", "entities": []}, {"text": "= 0 ; ai=0+xi ; O(ai ) = 0 ; zi=0+ai = xi : V(1)(xi )", "entities": [[1, 2, "DatasetName", "0"], [8, 9, "DatasetName", "0"]]}, {"text": "= 0can be achieved by setting the weight matrix as the all- 0matrix .", "entities": []}, {"text": "Recall that xiis de\ufb01ned as xi= [ 0h;si ; 0h;i;1 ] :", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "We then apply linear transformations in K(zi ) = ziWkandV(zi ) = ziWv , where WT k=2 66666640 0\u0001\u0001\u0001 0 0 ............ 0 0\u0001\u0001\u0001 0 0 0 0\u0001\u0001\u0001 0 1 0 0\u0001\u0001\u0001 \u0000 1 03 7777775 ; andWk2Qd\u0002d , and similarly one can obtain vi by setting the submatrix of Wv2Qd\u0002dformed by the \ufb01rstd\u00002rows and columns to the identity matrix , and the rest of the entries to zeros .", "entities": [[19, 20, "DatasetName", "0"], [20, 21, "DatasetName", "0"], [22, 23, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [28, 29, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}, {"text": "Lemma C.5 .", "entities": [[0, 1, "DatasetName", "Lemma"]]}, {"text": "Letqt2Qdbe a query vector such thatq= [ \u0001;:::;\u0001;t+ 1;1]wheret2Nand \u2018 \u0001 \u2019 denotes an arbitrary value .", "entities": []}, {"text": "Then we have Att(qt;Ke;Ve )", "entities": []}, {"text": "=", "entities": []}, {"text": "[ 0h;st+1;0h;0;0]:(11 )", "entities": []}, {"text": "470Proof .", "entities": []}, {"text": "Recall thatpt = yt=", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "[ ht;0 ; : : : ; 0;t+ 1;1]andki=", "entities": []}, {"text": "[ 0;0 ; : : : ; 0;\u00001;i]and hence hpt;kii = i\u0000(t+ 1 ) ; fatt(pt;ki )", "entities": []}, {"text": "= \u0000ji\u0000(t+ 1)j :", "entities": []}, {"text": "Thus , fori2[n ] , the scoring function fatt(pt;ki ) has the maximum value 0at", "entities": []}, {"text": "indexi = t+ 1 if t < n ; fort\u0015n , the maximum value t+ 1\u0000nis achieved for i = n. Therefore Att(pt;Ke;Ve )", "entities": []}, {"text": "= st+1 : Proof of Lemma C.4 .", "entities": [[5, 6, "DatasetName", "Lemma"]]}, {"text": "Recall that at=", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "[ ht;st+1 ; 0h;t+ 1;1 ] NetworkO(at)is of the form O(at ) = W2\u001b(W1at+b1 ) ; where Wi2Qd\u0002dandb2Qdand W1 = dhdedh2 dh de dh 22 6664WhWx00 0", "entities": [[26, 27, "DatasetName", "0"]]}, {"text": "I00", "entities": []}, {"text": "I 000 0 00I3 7775 andb1=", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "[ bh;0s;0h;0;0 ] .", "entities": []}, {"text": "Hence \u001b(W1at+b1 )", "entities": []}, {"text": "=", "entities": []}, {"text": "[ \u001b(Whht+Wxst+1+b ) ; st+1;ht;t+ 1;1 ]", "entities": []}, {"text": "Next we de\ufb01ne W2by W2 = dhdedh2 dh de dh 22 6664I0\u0000I0 0\u0000I00", "entities": []}, {"text": "0000 000\u0000I3 7775 :", "entities": []}, {"text": "This leads to O(at ) = W2\u001b(W1at+b1 ) =", "entities": []}, {"text": "[ \u001b(Whht+Wxst+1+b)\u0000ht;\u0000st+1 ; 0h;\u0000(t+ 1);\u00001 ] ; which is what we wanted to prove .", "entities": []}, {"text": "D Completeness of Directional Transformers There are a few changes in the architecture of the Transformer to obtain directional Transformer .", "entities": [[15, 16, "MethodName", "Transformer"], [19, 20, "MethodName", "Transformer"]]}, {"text": "The \ufb01rst change is that there are no positional encodings and", "entities": []}, {"text": "thus the input vector xionly consists of si .", "entities": []}, {"text": "Similarly , there are no positional encodings in the decoder inputs and hence yt=~yt .", "entities": []}, {"text": "The vector ~yis the output representation produced at the previous step and the \ufb01rst input vector to the decoder ~y0=0 .", "entities": []}, {"text": "Instead of using positional encodings , we apply positional masking to the inputs and outputs of the encoder .", "entities": []}, {"text": "Thus the encoder - encoder attention in ( 5 ) is rede\ufb01ned as a(`+1 ) i = Att(Q(z ( ` ) i);K(Z ( ` ) i);V(Z ( ` ) i))+z ( ` ) i ; whereZ(0)=X. Similarly the decoder - encoder attention in ( 7 ) is rede\ufb01ned by a ( ` ) t= Att(p ( ` ) t;Ke t;Ve t ) + p ( ` ) t ; where`ina ( ` ) tdenotes the layer ` and we use v(`;b)to denote any intermediate vector being used in`-th layer and b - th block in cases where the same symbol is used in multiple blocks in the same layer .", "entities": []}, {"text": "Theorem D.1 .", "entities": []}, {"text": "RNNs can be simulated by vanilla Transformers and hence the class of vanilla Transformers is Turing - complete .", "entities": []}, {"text": "Proof .", "entities": []}, {"text": "The Transformer network in this case will be more complex than the construction for the vanilla case .", "entities": [[1, 2, "MethodName", "Transformer"]]}, {"text": "The encoder remains very similar , but the decoder is different and has two layers .", "entities": []}, {"text": "Embedding .", "entities": []}, {"text": "We will construct our Transformer to simulate an RNN of the form given in the de\ufb01nition with the recurrence ht = g(Whht\u00001+Wxxt+b ): The vectors used in the Transformer layers are of dimensiond= 2dh+de+ 4j\u0006j+ 1 .", "entities": [[4, 5, "MethodName", "Transformer"], [28, 29, "MethodName", "Transformer"]]}, {"text": "Wheredhis the dimension of the hidden state of the RNN and deis the dimension of the input embedding .", "entities": []}, {"text": "All vectorv2Qdused during the computation of the network are of the form v=", "entities": []}, {"text": "[ h1;h2;s1;Js1K;x1;Js2KJs3K;Js4 K ] wherehi2Qdh;s2Qdeandxi2Q.", "entities": []}, {"text": "These blocks reserved for different types of objects .", "entities": []}, {"text": "The", "entities": []}, {"text": "471vectorshis are reserved for computation related to hidden states of RNN s , sis are reserved for input embeddings and xis are reserved for scalar values related to positional operations .", "entities": []}, {"text": "Given an input sequence s0s1s2\u0001\u0001\u0001sn2\u0006\u0003 wheres0= # andsn= $ , we use an embedding functionf:\u0006!Qdde\ufb01ned as f(si )", "entities": []}, {"text": "= xi=", "entities": []}, {"text": "[ 0h;0h;si ; JsiK;0;0!;0!;0 ! ]", "entities": []}, {"text": "Unlike ( P \u00b4 erez et al . , 2019 ) , we use the dot product as our scoring function as used in Vaswani et al .", "entities": []}, {"text": "( 2017 ) in the attention mechanism in our construction , fatt(qi;kj )", "entities": []}, {"text": "= hqi;kji : For the computation of the Transformer , we also use a vector sequence in Qj\u0006jde\ufb01ned by !", "entities": [[8, 9, "MethodName", "Transformer"]]}, {"text": "t=1 t+ 1tX j=0JstK ; where 0\u0014t\u0014n .", "entities": []}, {"text": "The vector ! t=", "entities": []}, {"text": "( ! t;1;:::;!t;j\u0006j)contains the proportion of each input symbol till step tfor0\u0014t\u0014n .", "entities": []}, {"text": "Set ! \u00001=0 .", "entities": []}, {"text": "From the de\ufb01ntion of ! t , it follows that at any step 1\u0014k\u0014j\u0006jwe have ! t;k= \u001e t;k t+ 1 ; ( 12 ) where \u001e t;kdenotes the number of times the k - th symbol \f kin\u0006has appeared till the t - th step .", "entities": []}, {"text": "Note that!t;0=1 t+1since the \ufb01rst coordinate corresponds to the proportion of the start symbol # which appears only once at t= 0 .", "entities": [[21, 22, "DatasetName", "0"]]}, {"text": "Similarly , ! t;j\u0006j= 0", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "for0\u0014t <", "entities": []}, {"text": "n and!t;j\u0006j= 1=(t+ 1 ) fort\u0015n , since the end symbol $ does n\u2019t appear till the end of the input and it appears only once at t", "entities": []}, {"text": "= n. We de\ufb01ne two more sequences of vectors in Qj\u0006j for0\u0014t\u0014n : \u0001t=\u001b(!t\u0000!t\u00001 ) ; \u000et= ( \u0001t;1;:::;\u0001t;j\u0006j\u00001;1=2t+1 ): Here\u0001tdenotes the difference in the proportion of symbols between the t - th and ( t\u00001)-th steps , with the applicatin of sigmoid activation .", "entities": [[42, 44, "MethodName", "sigmoid activation"]]}, {"text": "In vector \u000et , the last coordinate of \u0001thas been replaced with 1=2t+1 .", "entities": []}, {"text": "The last coordinate in ! tindicates the proportion of the terminal symbol $ and hence the last value in \u0001tdenotes the change in proportion of $ .We set the last coordinate in \u000etto an exponentially decreasing sequence so that after nsteps we always have a nonzero score for the terminal symbol and it is taken as input in the underlying RNN .", "entities": []}, {"text": "Different and perhaps simpler choices for the last coordinate of\u000etmay be possible .", "entities": []}, {"text": "Note that 0\u0014\u0001t;k\u00141and 0\u0014\u000et;k\u00141for0\u0014t\u0014nand1\u0014k\u0014j\u0006j .", "entities": []}, {"text": "Construction of TEnc", "entities": []}, {"text": ".The", "entities": []}, {"text": "input to the network DTransMis the sequence ( s0;s1;:::;sn\u00001;sn ) wheres0= # andsn= $ .", "entities": []}, {"text": "Our encoder is a simple single layer network such that TEnc(x0;x1;:::;xn ) = ( Ke;Ve)whereKe= ( ke 0;:::;ke n)andVe= ( ve 0;:::;ve n)such that , ke i= [ 0h;0h;0s ; JsiK;0;0!;0!;0!];(13 ) ve i= [ 0h;0h;si ; 0!;0;0!;JsiK;0 ! ] : Similar to our construction of the encoder for vanilla transformer ( Lemma C.3 ) , the above Ke andVecan be obtained by making the output of Att(\u0001 ) = 0 by choosing the V(\u0001)to always evaluate to 0and similarly for O(\u0001 ) , and using residual connections .", "entities": [[52, 53, "DatasetName", "Lemma"], [70, 71, "DatasetName", "0"]]}, {"text": "Then one can produce KeandVe via simple linear transformations using K(\u0001)and V(\u0001 ) .", "entities": []}, {"text": "Construction of TDec .At", "entities": []}, {"text": "thet - th step we denote the input to the decoder as yt=~yt , where 0\u0014t\u0014r , whereris the step where the decoder halts .", "entities": []}, {"text": "Leth\u00001=0handh0=0h .", "entities": []}, {"text": "We will prove by induction on tthat for 0\u0014t\u0014rwe have yt=", "entities": []}, {"text": "[ ht\u00001;0h;0s ; 0!;1 2t;0!;0!;!t\u00001]:(14 )", "entities": []}, {"text": "This is true for t= 0by", "entities": []}, {"text": "the choice of seed vector : y0= [ 0h;0h;0s ; 0!;1;0!;0!;0 ! ] : Assuming the truth of ( 14 ) for t , we show it for t+ 1 . Layer 1 . Similar to the construction in Lemma C.3 , in the decoder - decoder attention block we set V(1)(\u0001 )", "entities": [[39, 40, "DatasetName", "Lemma"]]}, {"text": "= 0dand use the residual connections to set p(1 ) t = yt .", "entities": []}, {"text": "At thet - th step in the decoder - encoder attention block of layer 1 we have Att(p(1 ) t;Ke\u0016t;Ve\u0016t ) = \u0016tX j=0^ \u000b ( 1;2 ) t;jve j ;", "entities": []}, {"text": "472where ( ^ \u000b ( 2;2 ) t;1 ; : : : ; ^ \u000b ( 2;2 ) t;\u0016t )", "entities": []}, {"text": "= hardmax\u0010 hp(1 ) t;ke 1i;:::;hp(1 ) t;ke\u0016ti\u0011 = hardmax ( 0 ; : : : ; 0 )", "entities": [[11, 12, "DatasetName", "0"], [17, 18, "DatasetName", "0"]]}, {"text": "= \u00121 \u0016t+ 1;:::;1 \u0016t+ 1\u0013 : Therefore P\u0016t j=0^ \u000b ( 1;2 ) t;jve j= [ 0h;0h;s0 : t ; 0!;0;0!;!\u0016t;0 ! ] where s0 : t=1", "entities": []}, {"text": "\u0016t+", "entities": []}, {"text": "1\u0016tX j=0sj :", "entities": []}, {"text": "Thus , a(1 ) t= Att(p(1 ) t;Ke\u0016t;Ve\u0016t ) + p(1 ) t =", "entities": []}, {"text": "[ ht\u00001;0h;s0 : t;0!;1 2t;0!;!\u0016t;!t\u00001 ] : In Lemma D.2 we construct feed - forward network O(1)(\u0001)such that O(1)(a(1 ) t ) =", "entities": [[8, 9, "DatasetName", "Lemma"]]}, {"text": "[ 0h;0h;\u0000s0 : t;\u000e\u0016t;\u00001 2t+1 2t+1 ; 0!;\u0000!\u0016t;\u0000!t\u00001+!\u0016t ] : Hence z(1 ) t = O(1)(a(1 ) t ) + a(1 ) t ( 15 ) =", "entities": []}, {"text": "[ ht\u00001;0h;0s;\u000e\u0016t;1 2t+1;0!;0!;!\u0016t ] : Layer 2 .", "entities": []}, {"text": "In the \ufb01rst block of layer 2 , we set the value transformation function to identically zero similar to Lemma C.3 , i.e. V(2)(\u0001 ) = 0which leads to the output of Att(\u0001)to be0and then using the residual connection we get p(2 ) t = z(1 ) t.", "entities": [[19, 20, "DatasetName", "Lemma"], [37, 39, "MethodName", "residual connection"]]}, {"text": "It follows by Lemma D.3 that Att(p(2 ) t;Ke\u0016t;Ve\u0016t ) =", "entities": [[3, 4, "DatasetName", "Lemma"]]}, {"text": "[ 0h;0h;s\u0016t;0!;0;0!;JstK;0 ! ] : Thus , a(2 ) t= Att(p(2 ) t;Ke\u0016t;Ve\u0016t ) + p(2 ) t", "entities": []}, {"text": "=", "entities": []}, {"text": "[ ht\u00001;0h;s\u0016t;\u000e\u0016t;1 2t+1;0!;JstK;!\u0016t ] :", "entities": []}, {"text": "In the \ufb01nal block of the decoder in the second layer , the computation for RNN takes place .", "entities": []}, {"text": "InLemma D.4 below we construct the feed - forward networkO(2)(\u0001)such that O(2)(a(2 ) t ) =", "entities": []}, {"text": "[ \u001b(Whht\u00001+Wxs\u0016t+b)\u0000ht\u00001 0h;\u0000s\u0016t;\u0000\u000et;0;0!;\u0000JstK;0 ! ]", "entities": []}, {"text": "and hence z(2 ) t = O(2)(a(2 ) t ) + a(2 ) t =[ \u001b(Whht\u00001+Wxs\u0016t+b);0h;0s ; 0!;1 2t+1;0!;0!;!\u0016t ] ; which gives yt+1= [ ht;0h;0s ; 0!;1 2t+1;0!;0!;!\u0016t ] ; proving the induction hypothesis ( 14 ) for t+1 , and completing the simulation of RNN .", "entities": []}, {"text": "D.1 Technical Lemmas Lemma D.2 .", "entities": [[3, 4, "DatasetName", "Lemma"]]}, {"text": "There exists a function O(1)(:)de\ufb01ned by feed - forward network such that , O(1)(a(1 ) t ) =", "entities": []}, {"text": "[ 0h;0h;\u0000s0 : t;\u000et ; \u00001 2t+1 2t+1;0!;\u0000!t;\u0000!t\u00001+!t ] Proof .", "entities": []}, {"text": "We de\ufb01ne the feed - forward network O(1 ) (: ) such that O(1)(a(1 ) t ) =", "entities": []}, {"text": "[ 0h;0h;\u0000s0 : t;\u000et\u0000!t ; \u00001 2t+1 2t+1;0!;0!;\u0000!t\u00001+!t ] where \u000et= ( \u0001t;1;:::;\u0001t;n\u00001;1=2t+1);0\u0014\u000et\u00141 Recall that , a(1 ) t=", "entities": [[13, 14, "MetricName", "Recall"]]}, {"text": "[ ht\u00001;0h;s0 : t ; ! t;1", "entities": []}, {"text": "2t;0!;0!;!t\u00001 ] We de\ufb01ne the feed - forward network O(at)as follows , O(1)(at )", "entities": []}, {"text": "= W2\u001b(W1a(1 ) t+b1 )", "entities": []}, {"text": "473whereWi2Qd\u0002dandb12Qd .", "entities": []}, {"text": "De\ufb01neW1as 2dhded!1d!d!d !", "entities": []}, {"text": "2dh de d!\u00001 1 1 d !", "entities": []}, {"text": "d ! d!2 6666666666640000000 0I00000 00000I\u0000I 0001 2000 0001 2000 00I0000 00000I0 000000I3 777777777775 andb1=0 , then \u001b(W1a(1 ) t+b1 ) =", "entities": []}, {"text": "[ 0h;0h;s0 : t;\u0001t;1 2t+1 ; !", "entities": []}, {"text": "t;!t\u00001;!t\u00001 ]", "entities": []}, {"text": "We de\ufb01neW2as 2dhded!\u000012d!d!d !", "entities": []}, {"text": "2dh de d!\u00001 1 1 d !", "entities": []}, {"text": "d ! d!2", "entities": []}, {"text": "6666666666640000000 0\u0000I00000 00I0000 0001;0000 000\u00002;1000 00I0000", "entities": []}, {"text": "00000\u0000I0 00000I\u0000I3", "entities": []}, {"text": "777777777775", "entities": []}, {"text": "This leads to O(1)(a(1 ) t ) =", "entities": []}, {"text": "[ 0h;0h;s0 : t;\u000et ; \u00001 2t+1 2t+1;0!;\u0000!t;\u0000!t\u00001+!t ] which is what we wanted to prove .", "entities": []}, {"text": "Lemma D.3 .", "entities": [[0, 1, "DatasetName", "Lemma"]]}, {"text": "Letp(2 ) t2Qdbe a query vector such that p(2 ) t=", "entities": []}, {"text": "[ \u0001;\u0001;\u0001 ; \u000et;\u0001;\u0001;\u0001;\u0001 ] wheret\u00150and \u2018 \u0001 \u2019 denotes an arbitrary value .", "entities": []}, {"text": "Then we have Att(p(2 ) t;Ke\u0016t;Ve\u0016t ) = [ 0h;0h;s\u0016t ; 0!;0;0!;JstK;0 ! ] : ( 16 ) Proof .", "entities": []}, {"text": "Let ( ^ \u000b ( 2;2 ) t;1 ; : : : ; ^ \u000b ( 2;2 ) t;\u0016t )", "entities": []}, {"text": "= hardmax\u0010 hp(2 ) t;ke 1i;:::;hp(2 )", "entities": []}, {"text": "t;ke\u0016ti\u0011be", "entities": []}, {"text": "the vector of normalized attention scores in the decoder - encoder attention block of layer 2 at time t.", "entities": []}, {"text": "Then Att(p(2 ) t;Ke\u0016t;Ve\u0016t ) = \u0016tX j=0^ \u000b ( 2;2 ) t;jve j : We claim that Claim 1 .", "entities": []}, {"text": "Fort\u00150we have ( ^ \u000b ( 2;2 ) t;1 ; : : : ; ^ \u000b ( 2;2 ) t;\u0016t )", "entities": []}, {"text": "= 1 \u0015\u0016t\u0000 I(s0 = st);I(s1 = st);:::;I(s\u0016t = st)\u0001 ; where\u0015tis a normalization factor given by \u0015t = Pn\u00001 j=0I(sj = st ) .", "entities": []}, {"text": "We now prove the lemma assuming the claim above .", "entities": [[4, 5, "DatasetName", "lemma"]]}, {"text": "Denote the L.H.S. in ( 16 ) by", "entities": []}, {"text": "t. Note that ifsj = st , thenve j=", "entities": []}, {"text": "t. Now we have \u0016tX j=0^ \u000b ( 2;2 ) t;jve j=1 \u0015t\u0016tX j=0I(sj = st)ve j = 1 \u0015t0 @\u0016tX j=0I(sj = st)1 A", "entities": []}, {"text": "t =", "entities": []}, {"text": "t ; completing the proof of the lemma modulo the proof of the claim , which we prove next .", "entities": [[7, 8, "DatasetName", "lemma"]]}, {"text": "Proof .", "entities": []}, {"text": "( of Claim 1 ) For 0 < t\u0014n , the vector ! t\u0000!t\u00001has the form   \u00121 t+ 1\u00001 t\u0013 ; : : : ; \u0012 \u001e t;k t+ 1\u0000 \u001e t\u00001;k t\u0013 ; : : : ; 0 ! :", "entities": [[6, 7, "DatasetName", "0"], [39, 40, "DatasetName", "0"]]}, {"text": "Ifst= \f k , then ( ! t\u0000!t\u00001)k ( 17 ) = \u0012 \u001e t;k t+ 1\u0000 \u001e t\u00001;k t\u0013 ( 18 ) = \u0012 \u001e t\u00001;k+ 1 t+ 1\u0000 \u001e t\u00001;k t\u0013 ( 19 ) = t\u0000 \u001e t\u00001;k t(t+ 1)(20 ) \u00151 t(t+ 1 ): ( 21 ) The last inequality used our assumption that s0= # and that # does not occur at any later time and", "entities": []}, {"text": "474therefore \u001e t\u00001;j < t.", "entities": []}, {"text": "On the other hand , if st6= \f k , then ( ! t\u0000!t\u00001)k=\u0012 \u001e t;k t+ 1\u0000 \u001e t\u00001;k t\u0013", "entities": []}, {"text": "= \u0012 \u001e t\u00001;k t+ 1\u0000 \u001e t\u00001;k", "entities": []}, {"text": "t\u0013", "entities": []}, {"text": "= \u0000 \u001e t\u00001;j t(t+ 1)(22 ) \u00140 : This leads to , ( ! t\u0000!t\u00001)k>0 ifst= \f k ; ( !", "entities": []}, {"text": "t\u0000!t\u00001)k\u00140 otherwise : In words , the change in the proportion of a symbol is positive from step t\u00001totif and only if it is the input symbol at the t - th step .", "entities": []}, {"text": "For 0\u0014t\u0014n and1\u0014k\u0014j\u0006j , this leads to \u0001t;k=\u001b(!t\u0000!t\u00001)k>0 ifst= \f k ; \u0001t;k=\u001b(!t\u0000!t\u00001)k= 0 otherwise ; Fort > n , \u0001t=0 : Recall thatp(2 ) t = z(1 ) twhich comes from ( 15 ) , andke jis de\ufb01ned in ( 13 ) .", "entities": [[13, 14, "DatasetName", "0"], [22, 23, "MetricName", "Recall"]]}, {"text": "We reproduce these for convenience : p(2 ) t=", "entities": []}, {"text": "[ ht\u00001;0h;0s ; \u000e\u0016t;1 2t+1;0!;0!;!\u0016t ] ; ke j= [ 0h;0h;0s ; JsjK;0;0!;0!;0 ! ] :", "entities": []}, {"text": "It now follows that for 0 < t < n , if0\u0014j\u0014tis such thatsj6 = st , then hp(2 ) t;ke ji = h\u000et;JsjKi=\u000et;i= 0 :", "entities": [[5, 6, "DatasetName", "0"], [24, 25, "DatasetName", "0"]]}, {"text": "And for 0 < t < n , if0\u0014j\u0014tis such that sj = st= \f i , then hp(2 ) t;ke ji = h\u000et;JsjKi=\u000et;i ( 23 ) = t\u0000 \u001e t\u00001;j t(t+", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "1)\u00151 t(t+ 1 ): ( 24 ) Thus , for 0\u0014t < n , in the vector \u0010 hp(2 ) t;ke 0i;:::;hp(2 ) t;ke ti\u0011 , the largest coordinates are the ones indexed by jwithsj = st and they all equalt\u0000 \u001e t\u00001;i t(t+1 ) .", "entities": []}, {"text": "All other coordinates are 0 .", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "Fort\u0015n , only the last coordinate hp(2 ) t;ke ni = h\u000et;J$Ki=1 2t+1is non - zero .", "entities": []}, {"text": "Now the claim follows immediately by the de\ufb01nition of hardmax .Lemma", "entities": []}, {"text": "D.4 .", "entities": []}, {"text": "There exists a function O(2)(:)de\ufb01ned by feed - forward network such that , for t\u00150 , O(2)(a(2 ) t ) =", "entities": []}, {"text": "[ \u001b(Whht\u00001+Wxs\u0016t+b)\u0000ht\u00001 ; 0h;\u0000s\u0016t;\u0000\u000et;0;0!;\u0000JstK;0 ! ]", "entities": []}, {"text": "whereWh;Wxandbdenote the parameters of the RNN under consideration .", "entities": []}, {"text": "Proof .", "entities": []}, {"text": "Proof is very similar to proof of lemma C.4 .", "entities": [[7, 8, "DatasetName", "lemma"]]}, {"text": "E Details of Experiments In this section , we describe the speci\ufb01cs of our experimental setup .", "entities": []}, {"text": "This includes details about the dataset , models , setup and some sample outputs .", "entities": []}, {"text": "E.1 Impact of Residual Connections The models under consideration are the vanilla Transformer , the one without decoder - encoder residual connection and the one without decoderdecoder residual connection .", "entities": [[12, 13, "MethodName", "Transformer"], [20, 22, "MethodName", "residual connection"], [27, 29, "MethodName", "residual connection"]]}, {"text": "For the synthetic tasks , we implement a single layer encoder - decoder network with only a single attention head in each block .", "entities": []}, {"text": "Our implementation of the Transformer is adapted from the implementation of ( Rush , 2018 ) .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "Table 4 provides some illustrative sample outputs of the models for the copy task .", "entities": []}, {"text": "SOURCE & REFERENCE \u2013 there was no problem at all says douglas ford chief executive of\ufb01cer of the futures exchange DIRECTIONAL TRANS FORMER \u2013 there was no problem at all says douglas ford chief executive of\ufb01cer of the futures exchange VANILLA TRANS FORMER \u2013 there was no problem at all says douglas ford chief executive of\ufb01cer Table 4 : Sample outputs by the models on the copy task on length 16 .", "entities": []}, {"text": "With absolute positional encodings the model over\ufb01ts on terminal symbol at position 13 and generates sequence of length 12 .", "entities": []}, {"text": "For the machine translation task , we use OpenNMT ( Klein et al . , 2017 ) for our implementation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "For preprocessing the German - English dataset we used the script from fairseq .", "entities": []}, {"text": "The dataset contains about 153k training sentences , 7k development sentences and 7k test sentences .", "entities": []}, {"text": "The hyperparameters to train the vanilla Transformer were obtained from fairseq \u2019s guidelines .", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "We tuned the parameters on the validation set for the two baseline model .", "entities": []}, {"text": "To preprocess the English - Vietnamese dataset , we follow Luong and Manning ( 2015 ) .", "entities": []}, {"text": "The dataset contains about 133k training sentences .", "entities": []}, {"text": "We use", "entities": []}, {"text": "475the tst2012 dataset containing 1.5k sentences for validation and tst2013 containing 1.3k sentences as test set .", "entities": []}, {"text": "We use noam optimizer in all our experiments .", "entities": [[3, 4, "HyperparameterName", "optimizer"]]}, {"text": "While tuning the network , we vary the number of layer from 1 to 4 , the learning rate , the number of heads , the warmup steps , embedding size and feedforward embedding size .", "entities": [[17, 19, "HyperparameterName", "learning rate"]]}, {"text": "E.2 Masking and Encodings Our implementation for directional transformer is based on ( Yang et al . , 2019 ) but we use only unidirectional masking as opposed to bidirectional used in their setup .", "entities": []}, {"text": "While tuning the models , we vary the layers from 1 to 4 , the learning rate , warmup steps and the number of heads .", "entities": [[15, 17, "HyperparameterName", "learning rate"]]}]