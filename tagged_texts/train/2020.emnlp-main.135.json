[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 1728\u20131736 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics1728Tired of Topic Models ?", "entities": [[6, 8, "TaskName", "Topic Models"]]}, {"text": "Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too !", "entities": [[3, 5, "TaskName", "Word Embeddings"]]}, {"text": "Suzanna Sia Ayush Dalmia Sabrina J. Mielke Department of Computer Science Johns Hopkins University Baltimore , MD , USA ssia1@jhu.edu , adalmia1@jhu.edu , sjmielke@jhu.edu Abstract Topic models are a useful analysis tool to uncover the underlying themes within document collections .", "entities": [[25, 27, "TaskName", "Topic models"]]}, {"text": "The dominant approach is to use probabilistic topic models that posit a generative story , but in this paper we propose an alternative way to obtain topics : clustering pretrained word embeddings while incorporating document information for weighted clustering and reranking top words .", "entities": [[7, 9, "TaskName", "topic models"], [30, 32, "TaskName", "word embeddings"]]}, {"text": "We provide benchmarks for the combination of different word embeddings and clustering algorithms , and analyse their performance under dimensionality reduction with PCA .", "entities": [[8, 10, "TaskName", "word embeddings"], [19, 21, "TaskName", "dimensionality reduction"], [22, 23, "MethodName", "PCA"]]}, {"text": "The best performing combination for our approach performs as well as classical topic models , but with lower runtime and computational complexity .", "entities": [[12, 14, "TaskName", "topic models"]]}, {"text": "1 Introduction Topic models are the standard approach for exploratory document analysis ( Boyd - Graber et al . , 2017 ) , which aims to uncover main themes and underlying narratives within a corpus .", "entities": [[2, 4, "TaskName", "Topic models"]]}, {"text": "But in times of distributed and even contextualized embeddings , are they the only option ?", "entities": []}, {"text": "This work explores an alternative to topic modeling by casting \u2018 key themes \u2019 or \u2018 topics \u2019 as clusters of word types under the modern distributed representation learning paradigm : unsupervised pre - trained word embeddings provide a representation for each word type as a vector , allowing us to cluster them based on their distance in high - dimensional space .", "entities": [[27, 29, "TaskName", "representation learning"], [35, 37, "TaskName", "word embeddings"]]}, {"text": "The goal of this work is not to strictly outperform , but rather to benchmark standard clustering of modern embedding methods against the classical approach of Latent Dirichlet Allocation ( LDA ; Blei et al . , 2003 ) .", "entities": [[30, 31, "MethodName", "LDA"]]}, {"text": "We restrict our study to in\ufb02uential embedding methods and focus on centroid - based clustering algorithms as they provide a natural way to obtainthe top words in each cluster based on distance from the cluster center.1 Aside from reporting the best performing combination of word embeddings and clustering algorithm , we are also interested in whether there are consistent patterns : embeddings which perform consistently well across clustering algorithms might be good representations for unsupervised document analysis , clustering algorithms that perform consistently well are more likely to generalize to future word embedding methods .", "entities": [[44, 46, "TaskName", "word embeddings"]]}, {"text": "To make our approach reliably work as well as LDA , we incorporate corpus frequency statistics directly into the clustering algorithm , and quantify the effects of two key methods , 1 ) weighting terms during clustering and 2 ) reranking terms for obtaining the top Jrepresentative words .", "entities": [[9, 10, "MethodName", "LDA"]]}, {"text": "Our contributions are as follows : \u2022We systematically apply centroid - based clustering algorithms on top of a variety of pretrained word embeddings and embedding methods for document analysis .", "entities": [[21, 23, "TaskName", "word embeddings"]]}, {"text": "\u2022Through weighted clustering and reranking of top words we obtain sensible topics ; the best performing combination is comparable with LDA , but with smaller time complexity and empirical runtime .", "entities": [[20, 21, "MethodName", "LDA"]]}, {"text": "\u2022We show that further speedups are possible by reducing the embedding dimensions by up to 80 % using PCA .", "entities": [[18, 19, "MethodName", "PCA"]]}, {"text": "2 Related Work and Background Analyzing documents by clustering word embeddings is a natural idea \u2014 clustering has been used 1We found that using non - centroid - based hierarchical , or density based clustering algorithms like DBScan resulted in worse performance and more hyperparameters to tune .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "1729for readability assessment ( Cha et al . , 2017 ) , argument mining ( Reimers et al . , 2019 ) , document classi\ufb01cation and document clustering ( Sano et al . , 2017 ) , inter alia .", "entities": [[12, 14, "TaskName", "argument mining"]]}, {"text": "So far , however , clustering word embeddings has not seen much success for the purposes of topic modeling .", "entities": [[6, 8, "TaskName", "word embeddings"]]}, {"text": "While many modern efforts have attempted to incorporate word embeddings intothe probabilistic LDA framework ( Liu et al . , 2015 ; Nguyen et al . , 2015 ; Das et al . , 2015 ; Zhao et al . , 2017 ; Batmanghelich et al . , 2016 ; Xun et al . , 2017 ; Dieng et al . , 2019 ) , relatively little work has examined the feasibility of clustering embeddings directly .", "entities": [[8, 10, "TaskName", "word embeddings"], [12, 13, "MethodName", "LDA"]]}, {"text": "Xie and Xing ( 2013 ) and Viegas et al .", "entities": []}, {"text": "( 2019 ) \ufb01rst cluster documents and subsequently \ufb01nd words within each cluster for document analysis .", "entities": []}, {"text": "Sridhar ( 2015 ) targets short texts where LDA performs poorly in particular , \ufb01tting GMMs to learned word2vec representations .", "entities": [[8, 9, "MethodName", "LDA"]]}, {"text": "De Miranda et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) cluster using self - organising maps , but provide only qualitative results .", "entities": []}, {"text": "In contrast , our proposed approach is straightforward to implement , feasible for regular length documents , requires no retraining of embeddings , and yields qualitatively and quantitatively convincing results .", "entities": []}, {"text": "We focus on centroid based k - means ( KM ) , Spherical k - means ( SK ) , and k - medoids ( KD ) for hard clustering , and von Mises - Fisher Models ( VMFM ) and Gaussian Mixture Models ( GMM ) for soft clustering ; as pre - trained embeddings we consider word2vec ( Mikolov et al . , 2013 ) , GloVe ( Pennington et al . , 2014 ) , FastText ( Bojanowski et al . , 2017 ) , Spherical ( Meng et al . , 2019 ) , ELMo ( Peters et al . , 2018 ) , and BERT ( Devlin et al . , 2018 ) .", "entities": [[68, 69, "MethodName", "GloVe"], [78, 79, "MethodName", "FastText"], [98, 99, "MethodName", "ELMo"], [109, 110, "MethodName", "BERT"]]}, {"text": "3 Methodology After preprocessing and extracting the vocabulary from our training documents , each word type is converted to its embedding representation ( averaging all of its tokens for contextualized embeddings ; details in \u00a7 5.3 ) .", "entities": []}, {"text": "Following this we apply the various clustering algorithms on the entire training corpus vocabulary to obtain kclusters , using weighted ( \u00a7 3.2 ) or unweighted word types .", "entities": []}, {"text": "After the clustering algorithm has converged , we obtain the top J words ( \u00a7 3.1 ) from each cluster for evaluation .", "entities": []}, {"text": "Note that one potential shortcoming of our approach is the possibility of outliers forming their own cluster , which we leave to future work .", "entities": []}, {"text": "Figure 1 : The \ufb01gure on the left shows the cluster center ( ? ) without weighting , while the \ufb01gure on the right shows that after weighting ( larger points have higher weight ) a hopefully more representative cluster center is found .", "entities": []}, {"text": "Note that top words based on distance from the cluster center could still very well be low frequency word types , motivating reranking ( \u00a7 3.3 ) .", "entities": []}, {"text": "3.1 Obtaining top - J words In traditional topic modeling ( LDA ) , the top J words are those with highest probability under each topic - word distribution .", "entities": [[11, 12, "MethodName", "LDA"]]}, {"text": "For centroid based clustering algorithms , the top words of some cluster iare naturally those closest to the cluster center c(i ) , or with highest probability under the cluster parameters .", "entities": []}, {"text": "Formally , this means choosing the set of typesJas argmin J : jJj=10X j2J8 > < > :", "entities": []}, {"text": "kc(i)\u0000xjk2 2 for KM / KD ; cos(c(i);xj ) for SK ; f(xjjc(i);\u0006i)for GMM / VMFM :", "entities": []}, {"text": "Our results in \u00a7 6 focus on KM and GMM , as we observe that k - medoids , spherical KM and von Mises - Fisher tend to perform worse than KM and GMM ( see App .", "entities": []}, {"text": "A , App . B ) .", "entities": []}, {"text": "Note that it is possible to extend this approach to obtain the top topics given a document : compute similarity scores between learned topic cluster centers and all word embeddings from that particular document , and normalize them using softmax to obtain a ( non - calibrated ) probability distribution .", "entities": [[28, 30, "TaskName", "word embeddings"], [39, 40, "MethodName", "softmax"]]}, {"text": "Crucial to our method is the incorporation of corpus statistics on top of vanilla clustering algorithms , which we will describe in the remainder of this section .", "entities": []}, {"text": "3.2 Weighting while clustering The intuition of weighted clustering is based on the formulation of classical LDA which models the probability of the word type tbelonging to a topic i asNt;i+ \f tP t0Nt0i+ \f t0 , whereNt;irefers to the number of times word type thas been assigned to topic i , and", "entities": [[16, 17, "MethodName", "LDA"]]}, {"text": "1730 \f is a parameter of the Dirichlet prior on the pertopic word distribution .", "entities": []}, {"text": "In our case , illustrated by the schematic in Fig .", "entities": []}, {"text": "1 , weighting is a natural way to account for the frequency effects of vocabulary terms during clustering .", "entities": []}, {"text": "3.3 Reranking when obtaining topics When obtaining the top- Jwords that make up a cluster \u2019s topic , we also consider reranking terms , as there is no guarantee that words closest to cluster centers are important word types .", "entities": []}, {"text": "We will show in Table 2 that without reranking , clustering yields \u201c sensible \u201d topics but low NPMI scores .", "entities": []}, {"text": "3.4 Which corpus statistics ?", "entities": []}, {"text": "To incorporate corpus statistics into the clustering algorithm , we examine three different schemes2to assign weights to word types , where ntis the count of word type tin corpusD , anddis a document : tf = ntP t0nt0(1 ) tf - df = tf\u0001jfd2Djt2dgj jDj(2 ) tf -", "entities": []}, {"text": "idf = tf\u0001log\u0012jDj jfd2Djt2dgj+ 1\u0013 ( 3 ) These scores can now be used for weighting word types when clustering ( \u0005w),reranking top 100 words ( \u0005r ) after , both ( \u0005w r ) , or neither ( simply \u0005 ) .", "entities": []}, {"text": "We \ufb01nd that simply using tfoutperforms the other weighting schemes ( App . C ) .", "entities": []}, {"text": "Our results and subsequent analysis in \u00a7 6 uses tffor weighting and reranking .", "entities": []}, {"text": "4 Computational Complexity The complexity of KM is O(tknm ) , and of GMM isO(tknm3 ) , fortiterations,3kclusters ( topics ) , nword types ( unique vocabulary ) , and membedding dimensions .", "entities": []}, {"text": "Weighted variants have a oneoff cost of weight initialization , and contribute a constant factor when recalulculating the centroid during clustering .", "entities": []}, {"text": "Reranking has an additional O(n\u0001log(nk))factor , where nkis the average number of elements in a cluster .", "entities": []}, {"text": "In contrast , LDA via collapsed Gibbs sampling has a complexity of 2We also experimented with various scaling methods such as robust scaling , logistic - sigmoid , and log transform but found that these do not improve performance .", "entities": [[3, 4, "MethodName", "LDA"]]}, {"text": "3In general , trequired for convergence differs for clustering algorithm and embedding representation .", "entities": []}, {"text": "However we can specify the maximum number of iterations as a constant factor for worst case analysis .", "entities": [[6, 9, "HyperparameterName", "number of iterations"]]}, {"text": "O(tkN )", "entities": []}, {"text": ", whereNis the number of all tokens , so whenN \u001d n , clustering methods can potentially achieve better performance - complexity tradeoffs .", "entities": []}, {"text": "Note that running ELMo and BERT over documents also requires iterating over all tokens , but only once , and not for every topic and iteration .", "entities": [[3, 4, "MethodName", "ELMo"], [5, 6, "MethodName", "BERT"]]}, {"text": "4.1 Cost of obtaining Embeddings For readily available pretrained word embeddings such as word2vec , FastText , GloVe and Spherical , the embeddings can be considered as \u2018 given \u2019 as the practioner does not need to generate these embeddings from scratch .", "entities": [[9, 11, "TaskName", "word embeddings"], [15, 16, "MethodName", "FastText"], [17, 18, "MethodName", "GloVe"]]}, {"text": "However for contextual embeddings such as ELMo and BERT , there is additional computational cost in obtaining these embeddings before clustering , which requires passing through RNN and transformer layers respectively .", "entities": [[6, 7, "MethodName", "ELMo"], [8, 9, "MethodName", "BERT"]]}, {"text": "This can be trivially parallelised by batching the context window ( usually a sentence ) .", "entities": []}, {"text": "We use standard pretrained ELMo and BERT models in our experiments and therefore do not consider the runtime of training these models from scratch .", "entities": [[4, 5, "MethodName", "ELMo"], [6, 7, "MethodName", "BERT"]]}, {"text": "5 Experimental Setup Our implementation is freely available online.4 5.1 Datasets We use the 20 newsgroup dataset ( 20NG ) which contains around 18000 documents and 20categories,5and a subset of Reuters215786which contains around 10000 documents .", "entities": []}, {"text": "5.2 Evaluation ( Topic Coherence )", "entities": []}, {"text": "We adopt a standard 60 - 40 train - test split for 20NG and 70 - 30 for Reuters .", "entities": []}, {"text": "The top 10words ( \u00a7 3.1 ) were evaluated using normalized pointwise mutual information ( NPMI ; Bouma , 2009 ) which has been shown to correlate with human judgements ( Lau et al . , 2014 ) .", "entities": []}, {"text": "NPMI ranges from [ \u00001;1]with1indicating perfect association .", "entities": []}, {"text": "The train split is used to obtain the top topic words in an unsupervised fashion ( we do not use any document labels ) , and the test split is used to evaluate the \u201c topic coherence \u201d of these top words .", "entities": []}, {"text": "NPMI scores are averaged across all topics .", "entities": []}, {"text": "For both datasets we use 20topics ; which gives best NPMI out of 20,50,100topics for Reuters , and is the ground truth number for 20NG .", "entities": []}, {"text": "The 4https://github.com/adalmia96/ Cluster - Analysis 5http://qwone.com/ \u02dcjason/20Newsgroups/ 6https://www.nltk.org/book/ch02.html", "entities": []}, {"text": "1731Reuters 20 Newsgroups \u0005\u0005w\u0005r\u0005w r\u0005\u0005w\u0005r\u0005w r KM GMM KM GMM KM GMM KM GMM KM GMM KM GMM KM GMM KM GMM Word2vec -0.39", "entities": [[1, 3, "DatasetName", "20 Newsgroups"]]}, {"text": "-0.47 -0.21 -0.09 0.02 0.01 0.03 0.08 -0.21 -0.10 -0.11 0.13 0.18 0.16 0.19 0.20 ELMo -0.73 -0.55 -0.43 0.00 -0.10 -0.08 -0.02 0.06 -0.56 -0.13 -0.38 0.18 0.13 0.14 0.16 0.19", "entities": [[15, 16, "MethodName", "ELMo"]]}, {"text": "GloVe -0.67 -0.59 -0.04 0.01 -0.27 -0.03 0.01 0.05 -0.18 -0.12 0.06 0.24 0.22 0.23 0.23 0.23 Fasttext -0.68 -0.70 -0.46 -0.08 0.00 0.00 0.06 0.11 -0.32 -0.20 -0.18 0.21 0.24 0.23 0.25 0.24 Spherical -0.53 -0.65 -0.07 0.09 0.01 -0.05 0.10 0.12 -0.05 -0.24 0.24 0.23 0.25 0.22 0.26 0.24 BERT -0.43 -0.19 -0.07 0.12 0.00 -0.01 0.12 0.15 0.04 0.14 0.25 0.25 0.17 0.19 0.25 0.25 average -0.57 -0.52 -0.21 0.01 -0.06 -0.03 0.05 0.10 -0.21 -0.11 -0.02 0.21 0.20 0.20 0.23", "entities": [[0, 1, "MethodName", "GloVe"], [17, 18, "MethodName", "Fasttext"], [51, 52, "MethodName", "BERT"]]}, {"text": "0.23 std . dev . 0.14 0.18 0.19 0.09 0.12 0.03 0.05 0.04 0.21 0.13 0.25 0.05 0.04 0.04 0.04 0.02 Table 1 : NPMI Results ( higher is better ) for pre - trained word embeddings and k - means ( KM ) , and Gaussian Mixture Models ( GMM ) .", "entities": [[35, 37, "TaskName", "word embeddings"]]}, {"text": "\u0005windicates weighted and \u0005rindicates reranking of top words .", "entities": []}, {"text": "For Reuters ( left table ) , LDA has an NPMI score of 0.12 , while GMMw rBERT achieves 0.15 .", "entities": [[7, 8, "MethodName", "LDA"]]}, {"text": "For 20NG ( right ) , both LDA and KMw r Spherical achieve a score of 0.26 .", "entities": [[7, 8, "MethodName", "LDA"]]}, {"text": "All results are averaged across 5 random seeds .", "entities": [[7, 8, "DatasetName", "seeds"]]}, {"text": "NPMI scores presented in Table 1 are averaged across cluster centers initialized using 5 random seeds .", "entities": [[15, 16, "DatasetName", "seeds"]]}, {"text": "5.3 Preprocessing We lowercase tokens , remove stopwords , punctuation and digits , and exclude words that appear in less than 5 documents and appear in long sentences of more than 50 words , removing email artifacts and noisy token sequences which are not valid sentences .", "entities": []}, {"text": "An analysis on the effect of rare word removal can be found in \u00a7 6.2 .", "entities": []}, {"text": "For contextualized word embeddings ( BERT and ELMo ) , sentences served as the context window to obtain the token representations .", "entities": [[2, 4, "TaskName", "word embeddings"], [5, 6, "MethodName", "BERT"], [7, 8, "MethodName", "ELMo"]]}, {"text": "Subword representations were averaged for BERT , which performs better than just using the \ufb01rst subword .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "6 Results and Discussion Our main results are shown in Table 1 . 6.1 Runtime Running LDA with MALLET ( McCallum , 2002 ) takes a minute , but performs no better than KMw r , which takes little more than 10 seconds on CPU using sklearn ( Pedregosa et al . , 2011 ) , and 3 - 4 seconds using a simple implementation using JAX ( Bradbury et al . , 2018 ) on GPU .", "entities": [[16, 17, "MethodName", "LDA"]]}, {"text": "6.2 Weighting From Table 1 , we see that reranking and weighting greatly improves clustering performance across different embeddings .", "entities": []}, {"text": "As a \ufb01rst step to uncover why , we investigate how sensitive our methods are to restricting the clustering to only frequently appearing word types .", "entities": []}, {"text": "Visualized in Fig .", "entities": []}, {"text": "3 , we \ufb01nd that as we vary the cutoff term frequency , thus changing the vocabulary size and allowing more rare words onBERT ( topic12 ) Spherical ( topic19 ) KM KM r KM KM r vram drive detector earth vesa hard electromagnetic nasa cmos card magnetic satellite portable computer spectrometer orbit micron chip infrared surface nubus machine optical energy digital video velocity radar machine hardware radiation solar motherboards clipper solar spacecraft hardware controller telescope electrical NPMI :", "entities": []}, {"text": "-0.36 NPMI : 0.15 NPMI :", "entities": []}, {"text": "-0.01 NPMI : 0.36 Table 2 : Top 10 words in a topic on 20NG and overall NPMI , for k - means ( KM ) before and after reranking ( KM r ): reranking clearly improves NPMI for BERT and Spherical .", "entities": [[39, 40, "MethodName", "BERT"]]}, {"text": "the x - axis , NPMI is more affected for the models without reweighting .", "entities": []}, {"text": "This suggests that reweighting using term frequency is effective for clustering without the need for ad - hoc restriction of infrequent terms \u2014 without it , all combinations perform poorly compared to LDA .", "entities": [[32, 33, "MethodName", "LDA"]]}, {"text": "In general , GMM outperforms KM for both weighted and unweighted variants averaged across all embedding methods ( p<0:05).7 6.3 Reranking For KM , extracted topics before reranking results in reasonable looking themes , but scores poorly on NPMI .", "entities": []}, {"text": "Reranking strongly improves KM on average ( p<0:02 ) for both Reuters and 20NG .", "entities": []}, {"text": "Examples before and after reranking are provided in Table 2 .", "entities": []}, {"text": "This indicates that while cluster centers are centered around valid themes , they are surrounded by low frequency word types .", "entities": []}, {"text": "7Two - tailed t - test for GMMwvs KMw .", "entities": []}, {"text": "1732 50 100 200 300 500 800 Embedding dimensionality\u22120.2\u22120.10.00.10.20.3NPMI ( higher is better)KMw 50 100 200 300 500 800 Embedding dimensionalityGMMw 50 100 200 300 500 800 Embedding dimensionalityKMw r Word2Vec Fasttext GloVe Spherical BERT ElmoFigure 2 : Plots showing the effect of PCA dimension reduction on different embedding and clustering algorithms .", "entities": [[31, 32, "MethodName", "Fasttext"], [32, 33, "MethodName", "GloVe"], [34, 35, "MethodName", "BERT"], [43, 44, "MethodName", "PCA"]]}, {"text": "KMw rwhich we advocate over GMMs for ef\ufb01ciency , allows for dimension reduction of up to 80 % .", "entities": []}, {"text": "We observe that when applying reranking to GMMwthe gains are much less pronounced than KMw .", "entities": []}, {"text": "The top topic words before and after reranking for BERT - GMMwhave an average Jaccard similarity score of 0.910 , indicating that the cluster centers learned by weighted GMMs are already centered at word types of high frequency in the training corpus .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "6.4 Embeddings Spherical embeddings and BERT perform consistently well across both datasets .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "For 20NG , KMw r Spherical and LDA both achieve 0:26NPMI .", "entities": [[7, 8, "MethodName", "LDA"]]}, {"text": "For Reuters , GMMw rBERT achieves the top NPMI score of 0:15compared to 0:12of LDA .", "entities": [[14, 15, "MethodName", "LDA"]]}, {"text": "Word2vec and ELMo ( using only the last layer8 ) perform poorly compared to the other embeddings .", "entities": [[2, 3, "MethodName", "ELMo"]]}, {"text": "FastText and GloVe can achieve similar performance to BERT on 20NG but are slightly inferior on Reuters .", "entities": [[0, 1, "MethodName", "FastText"], [2, 3, "MethodName", "GloVe"], [8, 9, "MethodName", "BERT"]]}, {"text": "Training or \ufb01ne - tuning embeddings on the given data prior to clustering could potentially achieve better performance , but we leave this to future work .", "entities": []}, {"text": "6.5 Qualitative results We \ufb01nd that our approach yields a greater diversity within topics as compared to LDA while achieving comparable coherence scores ( App . D ) .", "entities": [[17, 18, "MethodName", "LDA"]]}, {"text": "Such topics are arguably more valuable for exploratory analysis .", "entities": []}, {"text": "6.6 Dimensionality Reduction We apply PCA to the word embeddings before clustering to investigate the amount of redundancy in the dimensions of large embeddings , which impact clustering complexity ( \u00a7 4 ) .", "entities": [[1, 3, "TaskName", "Dimensionality Reduction"], [5, 6, "MethodName", "PCA"], [8, 10, "TaskName", "word embeddings"]]}, {"text": "With reranking , the 8Selected as best performing by manually testing 13 different mixing ratios .", "entities": []}, {"text": "104 4\u00d71036\u00d71032\u00d71043\u00d7104 V ocabulary size0.000.050.100.150.200.250.30NPMI ( higher is better)LDA BERT KMw r BERT KM r Spherical KMw r Spherical KM rFigure 3 : NPMI as a function of vocabulary size reduced by term frequency on 20NG .", "entities": [[9, 10, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"]]}, {"text": "Embeddings are more sensitive to noisy vocabulary ( infrequent terms ) than LDA , but reweighting ( \u0005w ) helps to alleviate this .", "entities": [[12, 13, "MethodName", "LDA"]]}, {"text": "dimensions of all embeddings can be reduced by more than 80 % ( Fig . 2 ) .", "entities": []}, {"text": "We observe that KMw rcan consistently reduce the number of dimensions across different embedding types without loss of performance .", "entities": [[16, 17, "MetricName", "loss"]]}, {"text": "Although GMMwdoes not require reranking for good performance , it \u2019s cubic complexity indicates that KMw r might be preferred in practical settings .", "entities": []}, {"text": "7 Conclusion We outlined a methodology for clustering word embeddings for unsupervised document analysis , and presented a systematic comparison of various in\ufb02uential embedding methods and clustering algorithms .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "Our experiments suggest that pretrained word embeddings ( both contextualized and non - contextualized ) , combined with tf - weighted k - means and tf - based reranking , provide a viable alternative to traditional topic modeling at lower complexity and runtime .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "1733Acknowledgments We thank Aaron Mueller , Pamela Shapiro , Li Ke , Adam Poliak , Kevin Duh and the anonymous reviewers for their feedback .", "entities": [[12, 13, "MethodName", "Adam"]]}, {"text": "References Kayhan Batmanghelich , Ardavan Saeedi , Karthik Narasimhan , and Sam Gershman .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Nonparametric spherical topic modeling with word embeddings .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the conference .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Meeting , volume 2016 , page 537 .", "entities": []}, {"text": "NIH Public Access .", "entities": []}, {"text": "David M Blei , Andrew Y Ng , and Michael I Jordan .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Latent dirichlet allocation .", "entities": []}, {"text": "Journal of machine Learning research , 3(Jan):993\u20131022 .", "entities": []}, {"text": "Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Enriching word vectors with subword information .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 5:135\u2013146 .", "entities": []}, {"text": "Gerlof Bouma .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Normalized ( pointwise ) mutual information in collocation extraction .", "entities": []}, {"text": "Proceedings of GSCL , pages 31\u201340 .", "entities": []}, {"text": "Jordan Boyd - Graber , Yuening Hu , David Mimno , et al . 2017 .", "entities": []}, {"text": "Applications of topic models .", "entities": [[2, 4, "TaskName", "topic models"]]}, {"text": "Foundations and Trends \u00ae in Information Retrieval , 11(2 - 3):143 \u2013 296 .", "entities": [[5, 7, "TaskName", "Information Retrieval"]]}, {"text": "James Bradbury , Roy Frostig , Peter Hawkins , Matthew James Johnson , Chris Leary , Dougal Maclaurin , and Skye Wanderman - Milne .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "JAX : composable transformations of Python+NumPy programs .", "entities": []}, {"text": "Miriam Cha , Youngjune Gwon , and HT Kung . 2017 .", "entities": []}, {"text": "Language modeling by clustering with word embeddings for text readability assessment .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management , pages 2003\u20132006 .", "entities": [[5, 6, "DatasetName", "ACM"], [12, 13, "TaskName", "Management"]]}, {"text": "Rajarshi Das , Manzil Zaheer , and Chris Dyer . 2015 .", "entities": []}, {"text": "Gaussian lda for topic models with word embeddings .", "entities": [[3, 5, "TaskName", "topic models"], [6, 8, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 795\u2013804 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Adji B Dieng , Francisco JR Ruiz , and David M Blei .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Topic modeling in embedding spaces .", "entities": []}, {"text": "arXiv preprint arXiv:1907.04907 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jey Han Lau , David Newman , and Timothy Baldwin .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Machine reading tea leaves : Automaticallyevaluating topic coherence and topic model quality .", "entities": []}, {"text": "InProceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics , pages 530\u2013539 .", "entities": []}, {"text": "Yang Liu , Zhiyuan Liu , Tat - Seng Chua , and Maosong Sun . 2015 .", "entities": []}, {"text": "Topical word embeddings .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In TwentyNinth AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Andrew Kachites McCallum .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Mallet : A machine learning for language toolkit .", "entities": []}, {"text": "Http://mallet.cs.umass.edu .", "entities": []}, {"text": "Yu Meng , Jiaxin Huang , Guangyuan Wang , Chao Zhang , Honglei Zhuang , Lance Kaplan , and Jiawei Han . 2019 .", "entities": []}, {"text": "Spherical text embedding .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 8206\u20138215 .", "entities": []}, {"text": "Tomas Mikolov , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient estimation of word representations in vector space .", "entities": []}, {"text": "arXiv preprint arXiv:1301.3781 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Guilherme Raiol de Miranda , Rodrigo Pasti , and Leandro Nunes de Castro . 2019 .", "entities": []}, {"text": "Detecting topics in documents by clustering word vectors .", "entities": []}, {"text": "In International Symposium on Distributed Computing and Arti\ufb01cial Intelligence , pages 235\u2013243 .", "entities": [[4, 6, "TaskName", "Distributed Computing"]]}, {"text": "Springer .", "entities": []}, {"text": "Dat Quoc Nguyen , Richard Billingsley , Lan Du , and Mark Johnson .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Improving topic models with latent feature word representations .", "entities": [[1, 3, "TaskName", "topic models"]]}, {"text": "Transactions of the Association for Computational Linguistics , 3:299\u2013313 .", "entities": []}, {"text": "F. Pedregosa , G. Varoquaux , A. Gramfort , V .", "entities": []}, {"text": "Michel , B. Thirion , O. Grisel , M. Blondel , P. Prettenhofer , R. Weiss , V .", "entities": []}, {"text": "Dubourg , J. Vanderplas , A. Passos , D. Cournapeau , M. Brucher , M. Perrot , and E. Duchesnay . 2011 .", "entities": []}, {"text": "Scikit - learn : Machine learning in Python .", "entities": []}, {"text": "Journal of Machine Learning Research , 12:2825\u20132830 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher D. Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Matthew E. Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of NAACL .", "entities": []}, {"text": "Nils Reimers , Benjamin Schiller , Tilman Beck , Johannes Daxenberger , Christian Stab , and Iryna Gurevych .", "entities": [[4, 5, "DatasetName", "Schiller"]]}, {"text": "2019 .", "entities": []}, {"text": "Classi\ufb01cation and clustering of arguments with contextualized word embeddings .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "arXiv preprint arXiv:1906.09821 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Motoki Sano , Austin J Brockmeier , Georgios Kontonatsios , Tingting Mu , John Y Goulermas , Jun\u2019ichi Tsujii , and Sophia Ananiadou . 2017 .", "entities": []}, {"text": "Distributed document and phrase co - embeddings for descriptive clustering .", "entities": []}, {"text": "In Proceedings of the 15th Conference of", "entities": []}, {"text": "1734the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers , pages 991\u20131001 .", "entities": []}, {"text": "Vivek Kumar Rangarajan Sridhar .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "2015 .", "entities": []}, {"text": "Unsupervised topic modeling for short texts using distributed representations of words .", "entities": []}, {"text": "In Proceedings of the 1st workshop on vector space modeling for natural language processing , pages 192\u2013200 .", "entities": []}, {"text": "Felipe Viegas , S \u00b4 ergio Canuto , Christian Gomes , Washington Luiz , Thierson Rosa , Sabir Ribas , Leonardo Rocha , and Marcos Andr \u00b4 e Gonc \u00b8alves .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Cluwords : exploiting semantic word clustering representation for enhanced topic modeling .", "entities": []}, {"text": "In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining , pages 753\u2013761 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Pengtao Xie and Eric P Xing .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Integrating document clustering and topic modeling .", "entities": []}, {"text": "arXiv preprint arXiv:1309.6874 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Guangxu Xun , Yaliang Li , Wayne Xin Zhao , Jing Gao , and Aidong Zhang .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A correlated topic model using word embeddings .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "In IJCAI , pages 4207 \u2013 4213 .", "entities": []}, {"text": "He Zhao , Lan Du , and Wray Buntine . 2017 .", "entities": []}, {"text": "A word embeddings informed focused topic model .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Asian Conference on Machine Learning , pages 423\u2013438.A k - means ( KM ) vs k - medoids ( KD ) To further understand the effect of other centroid based algorithms on topic coherence , we also applied the k - medoids ( KD ) clustering algorithm .", "entities": []}, {"text": "KD is a hard clustering algorithm similar to KM but less sensitive to outliers .", "entities": []}, {"text": "As we can see in Table 3 , in all cases KD usually did as well or worse than KM .", "entities": []}, {"text": "KD also did relatively poorly after frequency reranking .", "entities": []}, {"text": "Where KD did do better than KM , the difference is not very striking and the NPMI scores were still quite below the other top performing models .", "entities": []}, {"text": "B Results for Spherical k - means and Von Mises - Fisher Mixture Table 4 shows the overall bad performance of spherical clustering methods , speci\ufb01cally Spherical k - Means ( SKM ) and von - Mises - Fisher mixtures ( VMFM ) .", "entities": []}, {"text": "C Comparing Different Reranking Schemes We present the results for using different reranking schemes for KM ( Table 5 ) and Weighted KM for Frequency ( Table 6 ) .", "entities": []}, {"text": "We can see that compared to the TF results in the main paper , other schemes for reranking such as aggregated TF - IDF and TF - DF improve over the original hard clustering , but fare worse in comparison with reranking with TF .", "entities": []}, {"text": "D Qualitative Comparison of Topics Generated We present the different topics generated using LDA ( Table 7 ) and topics generated using BERT KMw rfor the Reuters dataset ( Table 8) .", "entities": [[13, 14, "MethodName", "LDA"], [22, 23, "MethodName", "BERT"]]}, {"text": "Note that KM KD KMrKDr Word2Vec", "entities": []}, {"text": "-0.21 -0.32 0.18 0.12 FastText -0.33 -0.39 0.24 0.19 GloVe -0.18 -0.43 0.22 0.08 BERT 0.04 -0.06 0.17 0.15 ELMo -0.56 -0.56 0.13 0.12 Spherical -0.05 -0.07 0.25 0.22 average -0.22 -0.31 0.20 0.15 std . dev . 0.21 0.20 0.04 0.05 Table 3 : Results for pre - trained word embeddings and k - means ( KM ) and k - medoids ( KD ) .", "entities": [[4, 5, "MethodName", "FastText"], [9, 10, "MethodName", "GloVe"], [14, 15, "MethodName", "BERT"], [19, 20, "MethodName", "ELMo"], [50, 52, "TaskName", "word embeddings"]]}, {"text": "rindicates reranking of top words using term frequency .", "entities": []}, {"text": "1735Reuters \u0005\u0005w\u0005r\u0005w r SKM VMFM SKM VMFM SKM VMFM SKM VMFM Word2vec", "entities": []}, {"text": "-0.70 -0.85 -0.43 -0.88 -0.16 -0.05 -0.19 -0.05 ELMo -0.74 -0.88", "entities": [[8, 9, "MethodName", "ELMo"]]}, {"text": "-0.37 -0.87 -0.14 -0.10 0.00 -0.12 GloVe -0.52 -0.88 -0.11 -0.88 0.00 -0.18 0.06 -0.17 Fasttext -0.85 -0.89 -0.65 -0.87 -0.18 -0.08 -0.18 -0.10 Spherical -0.50 -0.81 -0.08 -0.82 0.01 -0.07 0.10 -0.09 BERT -0.40 -0.88 -0.06 -0.65 -0.03 -0.14 0.11 -0.16 average", "entities": [[6, 7, "MethodName", "GloVe"], [15, 16, "MethodName", "Fasttext"], [33, 34, "MethodName", "BERT"]]}, {"text": "-0.62 -0.87 -0.28", "entities": []}, {"text": "-0.83", "entities": []}, {"text": "-0.08", "entities": []}, {"text": "-0.10", "entities": []}, {"text": "-0.02 -0.12 std . dev . 0.17 0.03 0.24 0.09 0.09 0.05 0.14 0.04 20 Newsgroups \u0005\u0005w\u0005r\u0005w r SKM VMFM SKM VMFM SKM VMFM SKM VMFM Word2vec -0.37 -0.59 -0.17 -0.88 0.15 0.17 0.14 0.16 ELMo -0.52 -0.66 -0.30 -0.87 0.16 0.10 0.20 0.12 GloVe 0.00 -0.62 0.23 -0.88 0.25 0.13 0.24 0.14 Fasttext -0.60 -0.58 -0.26 -0.54 0.12 0.19 0.14 0.19 Spherical -0.04 -0.54 0.22 -0.82 0.25 0.22 0.25 0.21 BERT 0.06 -0.62 0.22 -0.65 0.23 0.11 0.25 0.10 average -0.24 -0.60", "entities": [[14, 16, "DatasetName", "20 Newsgroups"], [35, 36, "MethodName", "ELMo"], [44, 45, "MethodName", "GloVe"], [53, 54, "MethodName", "Fasttext"], [71, 72, "MethodName", "BERT"]]}, {"text": "-0.01 -0.77 0.19 0.15 0.20 0.15 std . dev . 0.29 0.04 0.26 0.14 0.06 0.05 0.05 0.04 Table 4 : NPMI Results ( higher is better ) for pre - trained word embeddings and Spherical k - means ( SKM ) , and von Mises - Fisher Mixtures ( VMFM ) .", "entities": [[32, 34, "TaskName", "word embeddings"]]}, {"text": "\u0005windicates weighted and \u0005rindicates reranking of top words .", "entities": []}, {"text": "TF TF - IDF TF - DF Word2Vec 0.18 0.15 0.17 FastText 0.24 0.23 0.23 GloVe 0.22 0.17 0.21 BERT 0.17 0.15 0.17 ELMo 0.13 0.09 0.14 Spherical 0.25 0.22 0.24 average 0.20 0.17 0.19 std . dev . 0.04 0.05 0.04 Table 5 : Results for k - means ( without weighting ) with pre - trained word embeddings using different reranking metrics : TF , TF - IDF , and TF - DF .", "entities": [[11, 12, "MethodName", "FastText"], [15, 16, "MethodName", "GloVe"], [19, 20, "MethodName", "BERT"], [23, 24, "MethodName", "ELMo"], [58, 60, "TaskName", "word embeddings"]]}, {"text": "unlike LDA , which uses the highest posterior probability allowing duplicate words to appear in duplicate topics , using a hard clustering algorithm for assignment mean that each word is assigned to one topic only .", "entities": [[1, 2, "MethodName", "LDA"]]}, {"text": "We can see compared to the LDA topics which tend to contain topics mostly regarding wealth and pro\ufb01ts , clustering with BERT KMw r introduces new topics in involving locations and corporate positions .", "entities": [[6, 7, "MethodName", "LDA"], [21, 22, "MethodName", "BERT"]]}, {"text": "We see overall that using clustering allows for a discovery for a greater diversity\u0005wTF\u0005wTF - IDF\u0005wTF - DF Word2Vec 0.19 0.17 0.20 FastText 0.25 0.25 0.25 GloVe 0.23 0.21 0.23 BERT 0.25 0.24 0.25 ELMo 0.16 0.15 0.16 Spherical 0.26 0.24 0.25 average 0.23 0.21 0.22 std .", "entities": [[22, 23, "MethodName", "FastText"], [26, 27, "MethodName", "GloVe"], [30, 31, "MethodName", "BERT"], [34, 35, "MethodName", "ELMo"]]}, {"text": "dev . 0.04 0.04 0.04 Table 6 : Results for k - means ( weighted ) pre - trained word embeddings using different reranking metrics : TF , TF - IDF and TF - DF weighted with term frequency .", "entities": [[19, 21, "TaskName", "word embeddings"]]}, {"text": "of topics due to the greater diversity of words over all the topics .", "entities": []}, {"text": "1736Top 10 Word for Each Topic NPMI dollar rate rates exchange currency market dealers central interest point 0.369 year growth rise government economic economy expected domestic in\ufb02ation report 0.355 gold reserves year tons company production exploration ounces feet mine 0.290 billion year rose dlrs fell marks earlier \ufb01gures surplus rise -0.005 year tonnes crop production week grain sugar estimated expected area 0.239 dlrs company sale agreement unit acquisition assets agreed subsidiary sell -0.043 bank billion banks money interest market funds credit debt loans 0.239 tonnes wheat export sugar tonne exports sources shipment sales week 0.218 plan bill industry farm proposed government administration told proposal change 0.212 prices production price crude output barrels barrel increase demand industry 0.339 group company investment stake \ufb01rm told companies capital chairman president 0.191 trade countries foreign of\ufb01cials told of\ufb01cial world government imports agreement 0.298 offer company shares share dlrs merger board stock tender shareholders 0.074 shares stock share common dividend company split shareholders record outstanding 0.277 dlrs year quarter earnings company share sales reported expects results -0.037 market analysts time added long analyst term noted high back 0.316 coffee meeting stock producers prices export buffer quotas market price 0.170 loss dlrs pro\ufb01t shrs includes year gain share mths excludes -0.427 spokesman today government strike union state yesterday workers of\ufb01cials told 0.201 program corn dlrs prior futures price loan contract contracts cents -0.287 Table 7 : NPMI Scores and Top 10 words for the topics generated using LDA for the Reuters dataset Top 10 Word for Each Topic NPMI rise increase growth fall change decline drop gains cuts rising 0.238 president chairman minister house baker administration secretary executive chief washington 0.111 make continue result include reduce open support work raise remain 0.101 january march february april december june september october july friday 0.043 year quarter week month earlier months years time period term 0.146 rose fell compared reported increased estimated revised adjusted unchanged raised 0.196 today major made announced recent full previously strong \ufb01nal additional 0.125 share stock shares dividend common cash stake shareholders outstanding preferred 0.281 dlrs billion tonnes marks francs barrels cents tonne barrel tons -0.364 sales earnings business operations companies products markets assets industries operating 0.115 sale acquisition merger sell split sold owned purchase acquire held 0.003 board meeting report general commission annual bill committee association council 0.106 loss pro\ufb01t revs record note oper prior shrs gain includes 0.221 company corp group unit \ufb01rm management subsidiary trust paci\ufb01c holdings 0.058 prices price current total lower higher surplus system high average 0.198 offer agreement agreed talks tender plan terms program proposed issue 0.138 bank trade market rate exchange dollar foreign interest rates banks 0.327 told of\ufb01cial added department analysts of\ufb01cials spokesman sources statement reuters 0.181 production export exports industry wheat sugar imports output crude domestic 0.262 japan government international world countries american japanese national states united 0.251 Table 8 : NPMI Scores and Top 10 words for the topics generated using BERT KMw rfor the Reuters dataset", "entities": [[194, 195, "MetricName", "loss"], [202, 203, "DatasetName", "mths"], [241, 242, "MethodName", "LDA"], [385, 386, "MetricName", "loss"], [487, 488, "MethodName", "BERT"]]}]