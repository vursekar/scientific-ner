[{"text": "Proceedings of the 25th Conference on Computational Natural Language Learning ( CoNLL ) , pages 148\u2013157 November 10\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics148Enriching Language Models with Visually - grounded Word Vectors and the Lancaster Sensorimotor Norms Casey Kennington Department of Computer Science Boise State University caseykennington@boisestate.edu", "entities": []}, {"text": "Abstract Language models are trained only on text despite the fact that humans learn their \ufb01rst language in a highly interactive and multimodal environment where the \ufb01rst set of learned words are largely concrete , denoting physical entities and embodied states .", "entities": []}, {"text": "To enrich language models with some of this missing experience , we leverage two sources of information : ( 1 ) the Lancaster Sensorimotor norms , which provide ratings ( means and standard deviations ) for over 40,000 English words along several dimensions of embodiment , and which capture the extent to which something is experienced across 11 different sensory modalities , and ( 2 ) vectors from coef\ufb01cients of binary classi\ufb01ers trained on images for the BERT vocabulary .", "entities": [[77, 78, "MethodName", "BERT"]]}, {"text": "We pre - trained the ELECTRA model and \ufb01ne - tuned the RoBERTa model with these two sources of information then evaluate using the established GLUE benchmark and the Visual Dialog benchmark .", "entities": [[5, 6, "MethodName", "ELECTRA"], [12, 13, "MethodName", "RoBERTa"], [25, 26, "DatasetName", "GLUE"], [29, 31, "TaskName", "Visual Dialog"]]}, {"text": "We \ufb01nd that enriching language models with the Lancaster norms and image vectors improves results in both tasks , with some implications for robust language models that capture holistic linguistic meaning in a language learning context .", "entities": []}, {"text": "1 Introduction Children learn their \ufb01rst spoken language in a highly interactive setting where generally the \ufb01rst words children learn are concrete words that denote physical objects , which is an important developmental step in child \ufb01rst language acquisition ( Kuperman et al . , 2012a ; McCune , 2008 ; Clark , 2013 ) .", "entities": [[37, 39, "TaskName", "language acquisition"]]}, {"text": "This is partly because handling the Symbol Grounding Problem \u2013 the ablity to connect symbolic knowledge of language with representations of the physical world ( Harnad , 1990)\u2013must take place before children learn more abstract concepts later in their cognitive development ( Borghi et al . , 2019 ; Ponari et al . , 2018 ) .", "entities": []}, {"text": "Importantly , the physical world is not just the visual world ; children learn thatwords ground into proprioperceptive states ( e.g. , a hand grasp around an object has speci\ufb01c muscle activations tied to the word grab ) , interoceptive states ( i.e. , affect and valence ) , as well as all other sensory modalities ( e.g. , the word stinky grounds into olfactory , the word loud grounds into auditory ) .", "entities": []}, {"text": "These claims are evidenced in a large body of child development and cognitive science literature .", "entities": []}, {"text": "Smith and Gasser ( 2005 ) , for example , identi\ufb01ed that babies \u2019 experience of the world is profoundly multimodal : babies live in a physical world full of rich regularities that organize perception , action and thought ; babies learn in a social world to learn a shared linguistic communicative system that is symbolic .", "entities": []}, {"text": "Furthermore , a growing body of literature from linguistics and computational linguistics makes a strong case that the process of language learning ( indeed , general human cognition ) is embodied , interactive , and enacted ; i.e. , movement in the world is required ( Pulverm \u00a8uller , 1999 ; Lakoff and Johnson , 1999 ; Barsalou , 2008 ; Johnson , 2008 ; Smith and Samuelson , 2009 ; Di Paolo et al . , 2018 ;", "entities": []}, {"text": "Bisk et al . , 2020 ) ; see also the prior work in developmental robotics research ; e.g. , Cangelosi and Schlesinger ( 2015 ) , Chapter 7.1 Taken together , it is clear that aspects of the physical world are necessary for holistic knowledge of semantic meaning , which has implications for how language is modeled computationally .", "entities": []}, {"text": "In particular , what does this mean for language models that are trained purely on text ( likely largely written by adults ) , such as BERT ( Devlin et al . , 2018 ) or GPT-3 ?", "entities": [[26, 27, "MethodName", "BERT"], [36, 37, "MethodName", "GPT-3"]]}, {"text": "These models have clearly led to important advances for natural language processing tasks and applications , but it is also clear that language models trained only on text are missing critical semantic information ( Bender and Koller , 2020 ) .", "entities": []}, {"text": "1Here we follow Dourish ( 2001 ) that emodiment is possessing and acting through a physical manifestation in the world ; i.e. , having sensory input is only part of emobodiment \u2013 the ability to actin the world is essential .", "entities": []}, {"text": "149In this paper , we contribute to a growing body of recent work that attempts to addresses these limitations by ( 1 ) leveraging multimodal and sensorimotor knowledge of the Lancaster Sensorimotor Norms ( Lynott et al . , 2019 ) and ( 2 ) using vectorized representations of images by treating both ( 1 ) and ( 2 ) as embeddings of language models for GLUE and Visual Dialog benchmarks .", "entities": [[66, 67, "DatasetName", "GLUE"], [68, 70, "TaskName", "Visual Dialog"]]}, {"text": "In the following section , we explain related work \u2013 a growing body of literature that is adding multimodal information to language models , then we explain our two embeddings that we will use .", "entities": []}, {"text": "We explore how these embeddings can be used to enrich the ELECTRA language model \u2019s pre - training and \ufb01ne - tuning , and evaluate on the GLUE benchmark ( Experiment 1 , Section 4 ) , and how they can be used to replace input embeddings for a pre - trained RoBERTa model for the Visual Dialog task ( Experiment 2 , Section 5 ) .", "entities": [[11, 12, "MethodName", "ELECTRA"], [27, 28, "DatasetName", "GLUE"], [52, 53, "MethodName", "RoBERTa"], [56, 58, "TaskName", "Visual Dialog"]]}, {"text": "Our experiments shed light on how useful multimodal information can be in a task that is text - only ( Experiment 1 ) and a task that is multimodal ( Experiment 2 ) .", "entities": []}, {"text": "Our results show that our parsimonious method to unifying vision ( and sensorimotor knowledge ) in existing language models shows improvements in multimodal benchmarks with accessible hardware ( i.e. , a single GPU ) as a step towards models that can be trained in settings similar to that of child language learners .", "entities": []}, {"text": "2 Related Work Language models are trained on text .", "entities": []}, {"text": "G \u00a8unther et al .", "entities": []}, {"text": "( 2018 ) took up the question do words inherit sensorimotor activation from purely linguistic context ?", "entities": []}, {"text": "and showed that experience is necessary for reactivating experiential traces , but this reactivation is not a necessary condition for understanding the corresponding aspects of word meaning .", "entities": []}, {"text": "We take this to mean that humans are very adept at learning new concepts from language exposure alone ( i.e. , abstract concepts ) ; e.g. , someone who has never seen a zebra before , but hears them described as \u201c horses with vertical black and white stripes \u201d can compose a connotation of what zebra denotes without direct visual exposure .", "entities": []}, {"text": "However , this only works if an agent that has learned the language has the knowledge of horses , black , white , stripes , and vertical concepts \u2013 i.e. , via direct experience , not just through linguistic exposure or encyclopedic de\ufb01nitions .", "entities": [[7, 8, "DatasetName", "agent"]]}, {"text": "These claims are further backed up by neuroscience research that showed that neural assemblies encode concrete content words ( i.e. , wordsthat denote visual objects ) and verbs ( i.e. , words that denote actions ) are learned and represented in different brain regions ( Pulverm \u00a8uller , 1999 ;", "entities": []}, {"text": "Borghesani et al . , 2019 ) .", "entities": []}, {"text": "Rogers et al .", "entities": []}, {"text": "( 2020 ) provides a recent primer and overview of research that has attempted to uncover strengths and weaknesses of BERT and related language models ( so - called BERTology ) .", "entities": [[20, 21, "MethodName", "BERT"]]}, {"text": "While our work does \ufb01t into that growing body of literature , our criticisms on current language models speci\ufb01cally lies in the fact that they are only trained on easy - to - obtain text .", "entities": []}, {"text": "This criticism is born out in Forbes et al .", "entities": []}, {"text": "( 2019 ) which showed that BERT can guess affordances and properties of objects because that information can be found in text ( e.g. , a typical chair has the affordance of being sittable , and a property of having legs ) , but has no notion of how objects are related semantically to each other , and Da and Kasai ( 2019 ) further showed that real - world perceptual properties are likely to be assumed instead of inferred .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "Furthermore , Bender and Koller ( 2020 ) make a strong case that BERT learns form instead of meaning , and while the fact that BERT performs so well on many tasks is dif\ufb01cult to dispute , models trained on text are missing semantic information crucial for holistic language understanding .", "entities": [[13, 14, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"]]}, {"text": "Since before BERT which has proven powerful in many language processing tasks , efforts have been made to encode multimodal ( i.e. , more than just text as a learning modality ) information into embeddings and language models ( Takano and Utsumi , 2016 ; Kiros et al . , 2014 ; Zellers et al . , 2021 ) and recent , continued efforts towards bridging grounded visual representations to distributional representations of word meanings give credence to the claim that text - only models like BERT are missing crucial semantic information because enriching BERT with visual information improves performance in several known tasks ( Kim et al . , 2019 ; Lu et", "entities": [[2, 3, "MethodName", "BERT"], [86, 87, "MethodName", "BERT"], [94, 95, "MethodName", "BERT"]]}, {"text": "al . , 2019 ; Li et al . , 2019 ) .", "entities": []}, {"text": "These models usually treat language and vision as separate pipelines ; our method directly endows the language model with visual and sensorimotor knowledge .", "entities": []}, {"text": "3 Data In this section , we motivate and introduce of multimodal information we will use in our experiments .", "entities": []}, {"text": "The Lancaster Sensorimotor Norms The Lancaster Sensorimotor norms ( Lynott et al . , 2019 ) provide ratings ( means and standard deviations ) for", "entities": []}, {"text": "15040,000 English words along dimensions of embodiment which capture the extent to which a concept is experienced across 11 different sensory modalities , and measures derived from those categories , listed below ( each has an example word that rates highly for that modalitiy ): \u000fAuditory - sound ; ping \u000fGustatory - having to do with eating ; cream \u000fHaptic - muscle movement ; handshake \u000fInteroceptive - having to do with affect or emotion ; headache \u000fOlfactory - smell ; incense \u000fVisual - visual ; barcode \u000fFoot - leg - haptics for foot / leg ; run \u000fHand - arm - haptics for hand / arm ; pointing \u000fHead - having to do with the head ; eye \u000fMouth - haptics for mouth ; kiss \u000fTorso - haptics for torso ; breath \u000fMax - strength.perceptual - the highest rating across the 11 sensorimotor dimensions \u000fMinkowski3.perceptual - treating the 11 modalities as a vector , this represents the distance of the vector from the origin with in\ufb02uence of weaker dimensions attenuated \u000fExclusivity.perceptual - the extent to which a concept ( out of the 11 ) which is experienced through a single perceptual modalitiy", "entities": [[73, 74, "DatasetName", "emotion"]]}, {"text": "The last three can be seen as aggregates from the 11 modalities ; they also have .action values representing the extent to which a concept is experienced as an action ( as opposed to .perceptual ) , and.sensorimotor values representing the extent a concept is experience as sensorimotor .", "entities": []}, {"text": "As these norms were derived from surveys given to adults , these norms represent the degree to which the survey participants assigned those words to those categories .", "entities": []}, {"text": "Though this does not represent a neurophysiological grounding of words to those modalities learned through interaction and embodiment , this serves as a useful approximation .", "entities": []}, {"text": "The \ufb01nal set is a vocabulary of 39,707 words ( after removing rows which had null values ) , each represented as a vector of length 39 ( i.e. , 11 mean , 11 stdev columns ; Max - strength , Minkowski , and Exclusivity columns for different ways of aggregating the modalities ) .", "entities": []}, {"text": "We normalize each value in the vector independently to a value between 0 - 1 by dividing each value over its max value .", "entities": [[12, 13, "DatasetName", "0"]]}, {"text": "We call this the Lancaster vectors .", "entities": []}, {"text": "We performed t - SNE on the Lancaster vectors ( mapping to 2 dimensions ) to determine if clus - ters would reveal any intuitions about the kinds of semantic relatedness that the words might have with each other .", "entities": []}, {"text": "Some clusters emerged such as foods ( presumably because they have similar gustatoryratings ) , leg - movement verbs ( e.g. , walk , jump , sit ) , colors with eye - related words ( e.g. , purple , green , blue , dark , see , eyes ) , soft things ( e.g. , hug , tummy , pillow , clothes ) , audio - related words ( e.g. , talk , story , sound , music , lie , say ) , among others .", "entities": []}, {"text": "Figure 1 : The red WAC classi\ufb01ers are trained on positive and negative examples of images from Google Images for the word red ; each image is then passed through the CLIP model .", "entities": [[17, 18, "DatasetName", "Google"], [31, 32, "MethodName", "CLIP"]]}, {"text": "We train a binary logistic regression classi\ufb01er , then extract the coef\ufb01cients for the redvector .", "entities": [[4, 6, "MethodName", "logistic regression"]]}, {"text": "Words - as - Classi\ufb01ers Image Vectors The Words - as - Classi\ufb01ers ( WAC ) approach to grounded semantics is quite simple : train a binary classi\ufb01er for each word in a corpus where the features to that classi\ufb01er are derived from images ( Kennington and Schlangen , 2015 ) .", "entities": []}, {"text": "Each classi\ufb01er is given positive and negative examples of visual denotations of each word by the images and learns a \u201c \ufb01tness \u201d score by the classi\ufb01er .", "entities": []}, {"text": "For example , the red classi\ufb01er is given images of objects that are referred to as red in a corpus , and randomly assigned negative examples of things that are not referred to as red , as depicted in Figure 1 .", "entities": []}, {"text": "We follow Kiros et al .", "entities": []}, {"text": "( 2018 ) and use Google Image Search to \ufb01nd images using the BERT vocabulary , resulting in 27,152 words and corresponding images ( some words did not result in images , and we did not download images for \ufb01ller words ) .", "entities": [[5, 6, "DatasetName", "Google"], [13, 14, "MethodName", "BERT"]]}, {"text": "For each word , we perform an image search and download the top 100 images .", "entities": []}, {"text": "We then follow Schlangen et al .", "entities": []}, {"text": "( 2016 ) and process each image by passing them through the recent CLIP ( Jia", "entities": [[13, 14, "MethodName", "CLIP"]]}, {"text": "151et al . , 2021 ) convolutional neural network ( trained on ImageNet , using CLIP \u2019s ViT - B/32 model ) , yielding a vector of size 512 for each image .", "entities": [[12, 13, "DatasetName", "ImageNet"], [15, 16, "MethodName", "CLIP"]]}, {"text": "We use the 100 images as positive examples for each term in our vocabulary and randomly select three negative examples for each positive example .", "entities": []}, {"text": "We then use a logistic regression classi\ufb01er ( C=0.25 , max iterations=1000 ) , one for each word , trained on the images for each word .", "entities": [[4, 6, "MethodName", "logistic regression"]]}, {"text": "After training , we then follow Moro et al .", "entities": []}, {"text": "( 2019 ) and extract the coef\ufb01cients to arrive at a vector of size 513 ( all coef\ufb01cients plus the bias term ) which we use in our evaluations below .", "entities": []}, {"text": "We call these the WAC vectors .", "entities": []}, {"text": "The WAC model is useful because , as explained in Kennington and Schlangen ( 2015 ) , the classi\ufb01ers can actually identify objects ( something that language models can not do on their own ) , the coef\ufb01cients represent a computed word intension , new words in a vocabulary can easily be added without retraining all other classi\ufb01ers including adjectives likeredwhich are often missing from pre - trained object classi\ufb01ers , and the classi\ufb01ers are effectively learned with only a few examples , making it effective for fast learning of concrete , grounded concepts .", "entities": []}, {"text": "However , the WAC model suffers from two assumptions : \ufb01rst , that all words have concrete , visual denotations even though many abstract words likeutopia clearly do not , and that all words are independent of each other in terms of linguistic context .", "entities": []}, {"text": "We hypothesize in both experiments below that these coef\ufb01cients used as vectorized embeddings will be useful to a text - only language model because they add necessary visual information ; the language model complements WAC by using linguistic context ( i.e. , text ) for training , overcoming WAC \u2019s assumptions .", "entities": []}, {"text": "4 Experiment 1 : Tying embedding weights and pre - training ELECTRA , \ufb01ne - tuning on GLUE In this experiment , and crucially for our ongoing work that aligns with child - inspired language acquisition , we use ELECTRA ( Clark et al . , 2020 ) as a language model because it has been shown to be trainable with smaller amounts of data than other language models , yet yield respectable results and can be trained using a single GPU .", "entities": [[11, 12, "MethodName", "ELECTRA"], [17, 18, "DatasetName", "GLUE"], [34, 36, "TaskName", "language acquisition"], [39, 40, "MethodName", "ELECTRA"]]}, {"text": "Task & Procedure Wang et al .", "entities": []}, {"text": "( 2018 ) introduced the GLUE benchmark which consists of nine English sentence understanding tasks covering several domains ( e.g. , movie reviews and online Figure 2 : Image regions ( objects ) represented as CLIP vectors are positive and negative train examples for WAC classi\ufb01er .", "entities": [[5, 6, "DatasetName", "GLUE"], [35, 36, "MethodName", "CLIP"]]}, {"text": "WAC classi\ufb01er weights are tied to the embedding layer for the Generator and Discriminator for ELECTRA .", "entities": [[15, 16, "MethodName", "ELECTRA"]]}, {"text": "Dimensionality Reduction ( DR ) maps higher dimensional vectors to lower dimensions as needed .", "entities": [[0, 2, "TaskName", "Dimensionality Reduction"]]}, {"text": "Lancaster vectors are represented directly .", "entities": []}, {"text": "question answering ) .", "entities": [[0, 2, "TaskName", "question answering"]]}, {"text": "We opt for this benchmark because of its coverage over several domains and to show that adding multimodal knowledge improves tasks that are based on text.2Our aim is to achieve improved results over the text - only baseline with a speci\ufb01ed number of training steps using the openwebtext data for training.3We report results on the development set , as done in Wu et al . ( 2021 ) .", "entities": [[47, 48, "DatasetName", "openwebtext"]]}, {"text": "We only report the results for the MRPC ( a paraphrase task that uses accuracy and f1 metrics ) , COLA ( a grammatical acceptibility task ; uses Matthew \u2019s Correlation ) , and WNLI ( ambiguity resolution ; uses an accuracy metric ) tasks because they are suf\ufb01cient to illustrate the utility of our method when applied to ELECTRA .", "entities": [[7, 8, "DatasetName", "MRPC"], [14, 15, "MetricName", "accuracy"], [20, 21, "MethodName", "COLA"], [34, 35, "DatasetName", "WNLI"], [41, 42, "MetricName", "accuracy"], [59, 60, "MethodName", "ELECTRA"]]}, {"text": "To give ELECTRA knowledge about additional modalities from the Lancaster and WAC vectors , we tie the vectors to the the weights of the generator and discriminator of ELECTRA depicted in Figure 2 , and vary whether the embeddings are 2GLUE has a public leader board found at https:// gluebenchmark.com/leaderboard 3We build off of the implementation of https:// github.com/lucidrains/electra-pytorch", "entities": [[2, 3, "MethodName", "ELECTRA"], [28, 29, "MethodName", "ELECTRA"]]}, {"text": "152frozen or not during pre - training , then train for 100,000 steps.4We then \ufb01ne - tune the resulting ELECTRA model on the GLUE tasks using the multimodal vectors following standard \ufb01ne - tuning protocols ; that is , we add a linear layer with a softmax to the pre - trained model and use the ADAM solver with a learning rate of 2e-5 for 3 epochs .", "entities": [[19, 20, "MethodName", "ELECTRA"], [23, 24, "DatasetName", "GLUE"], [42, 44, "MethodName", "linear layer"], [46, 47, "MethodName", "softmax"], [56, 57, "DatasetName", "ADAM"], [60, 62, "HyperparameterName", "learning rate"]]}, {"text": "As the WAC vectors were larger than ELECTRA \u2019s expected embedding size of 128 , we applied UMAP to reduce the dimensionality to 128 ; similarly for the WAC and Lancaster concatenated embeddings .", "entities": [[7, 8, "MethodName", "ELECTRA"]]}, {"text": "For cases where there was no vector for a word ( e.g. , the [ unmapped ] words or words outside of the vocabulary of the Lancaster vectors ) , we simply used zero vectors .", "entities": []}, {"text": "For Lancaster vectors , we set the ELECTRA embedding size to 39 .", "entities": [[7, 8, "MethodName", "ELECTRA"]]}, {"text": "We explored freezing the embeddings ; our hypothesis is that not freezing the embeddings will lead to better results because the training regime can overpower the embeddings , but retain the multimodal knowledge .", "entities": []}, {"text": "For a broader comparison , we also compared to GloVE ( Pennington et al . , 2014 ) and several ablations where we concatenate multimodal vectors with the GloVe vectors ( we used the evaluation script for GloVE provided by Wang et al . ( 2018 ) ) .", "entities": [[28, 29, "MethodName", "GloVe"]]}, {"text": "We also use the same training and evaluation regime for the WAC and Lancaster vectors , and a concatenation of the two , on their own treating them as word - level embeddings similar to GloVe .", "entities": [[35, 36, "MethodName", "GloVe"]]}, {"text": "Results Table 1 shows the results on the GLUE benchmark .", "entities": [[8, 9, "DatasetName", "GLUE"]]}, {"text": "The word - level embeddings of GloVe , WAC , and Lancaster are shown in the top 5 rows of the table .", "entities": [[6, 7, "MethodName", "GloVe"]]}, {"text": "As expected , these word - level embeddings are not state - of - the - art , but we notice that both Lancaster and WAC vectors perform comparably against the GloVE vectors despite only being trained on images ( WAC ) or derived from the Lancaster norms .", "entities": []}, {"text": "Of note is a signi\ufb01cant advantage of using the Lancaster vectors alone compared to using any other embedding or combination for the WNLI task which is co - reference and natural language inference for \ufb01ction books .", "entities": [[22, 23, "DatasetName", "WNLI"], [30, 33, "TaskName", "natural language inference"]]}, {"text": "This suggests that inference on \ufb01ction is helped by knowing which modalities affect each word .", "entities": []}, {"text": "Interestingly , the best performing model for COLA was GloVE and Lancaster word - level embeddings ; COLA is 4This takes about 12 hours of training on our 12 GB GPU , which we opted for because it represents more data and train time than ELECTRA - small , but still a small enough amount of time to establish using this model in a co - located , interactive learning setting similar to the setting where children learn their \ufb01rst language .", "entities": [[7, 8, "MethodName", "COLA"], [17, 18, "MethodName", "COLA"], [45, 46, "MethodName", "ELECTRA"]]}, {"text": "MRPC MRPC COLA WNLI acc f1 corr acc GloVE 0.745 0.807 0.691 0.563 GloVE+lan 0.735 0.799 0.449 0.563 lan 0.711 0.778 0.691 0.596 wac 0.748 0.812 0.313 0.563 lan+wac 0.619 0.670 0.382 0.535 ELECTRA 0.730 0.835 0.449 0.563 EL - wac 0.730 0.835 0.440 0.563 EL - wacf0.760 0.833 0.0 0.563 EL - lan 0.708 0.819 0.39 0.535 EL - lan - wacf0.792 0.859 0.459 0.563 Table 1 : GLUE development set results with GloVe , ELECTRA , Lancaster ( lan ) , WAC models and several combinations , with ( fand without weight freezing during ELECTRA pre - training .", "entities": [[0, 1, "DatasetName", "MRPC"], [1, 2, "DatasetName", "MRPC"], [2, 3, "MethodName", "COLA"], [3, 4, "DatasetName", "WNLI"], [4, 5, "MetricName", "acc"], [7, 8, "MetricName", "acc"], [33, 34, "MethodName", "ELECTRA"], [69, 70, "DatasetName", "GLUE"], [74, 75, "MethodName", "GloVe"], [76, 77, "MethodName", "ELECTRA"], [96, 97, "MethodName", "ELECTRA"]]}, {"text": "a grammaticality test , which is important in language understanding , but arguably less critical in early - stage child language acquisition .", "entities": [[20, 22, "TaskName", "language acquisition"]]}, {"text": "All other rows show the ELECTRA baseline and ELECTRA that uses some variation of WAC , Lancaster , or both as embeddings ( denoted with the EL- pre\ufb01x ) .", "entities": [[5, 6, "MethodName", "ELECTRA"], [8, 9, "MethodName", "ELECTRA"]]}, {"text": "The bottom part of the table compares ELECTRA with a variant of ELECTRA that uses WAC embeddings ( both with and without freezing the embedding weights ) , ELECTRA with lancaster embeddings and ELECTRA with WAC embeddings concatenated with the Lancaster embeddings ( where the length of the WAC embeddings plus the size of ELECTRA is 128 ) .", "entities": [[7, 8, "MethodName", "ELECTRA"], [12, 13, "MethodName", "ELECTRA"], [28, 29, "MethodName", "ELECTRA"], [33, 34, "MethodName", "ELECTRA"], [54, 55, "MethodName", "ELECTRA"]]}, {"text": "Contrary to our hypothesis , we observe that when ELECTRA uses WAC with frozen weights , the performance on the benchmark performs better than all others , including the ELECTRA baseline .", "entities": [[9, 10, "MethodName", "ELECTRA"], [19, 21, "DatasetName", "the benchmark"], [29, 30, "MethodName", "ELECTRA"]]}, {"text": "This could suggest that ELECTRA can make effective use of the visual and Lancaster embeddings by adjusting weights in the other layers of the model .", "entities": [[4, 5, "MethodName", "ELECTRA"]]}, {"text": "The EL - lan - wac variant performed well above the ELECTRA baseline , substantiating the hypothesis that enriching the model with multimodal knowledge can improve results .", "entities": [[11, 12, "MethodName", "ELECTRA"]]}, {"text": "Taken together , we \ufb01nd the results encouraging because the relatively short training regime still yielded respectable results , suggesting that ELECTRA with a visual or other multimodal embedding can be useful with less training as is the case when children learn language .", "entities": [[21, 22, "MethodName", "ELECTRA"]]}, {"text": "1535 Experiment 2 : Replacing RoBERTa embeddings , \ufb01ne - tuninng on Visual Dialog", "entities": [[5, 6, "MethodName", "RoBERTa"], [12, 14, "TaskName", "Visual Dialog"]]}, {"text": "The evaluation in Experiment 1 was made up of text - based tasks .", "entities": []}, {"text": "In this experiment , we use an evaluation that requires knowledge of the visual world by evaluating the Lancaster and WAC vectors on the Visual Dialog task ( Das et al . , 2019 ) , termed visdial .", "entities": [[24, 26, "TaskName", "Visual Dialog"], [37, 38, "DatasetName", "visdial"]]}, {"text": "Moreover , Experiment 1 used pre - training on a subset of the data for only 100,000 steps .", "entities": []}, {"text": "In this experiment , we evaluate using a fully pre - trained RoBERTa model by replacing its embeddings with theWAC and Lancaster vectors .", "entities": [[12, 13, "MethodName", "RoBERTa"]]}, {"text": "Task Following Murahari", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2019 ) , given an image , dialogue history consisting of questionanswer pairs , and a follow - up question about the image , the task of visdial is to predict a free - form natural language answer to the question .", "entities": [[28, 29, "DatasetName", "visdial"]]}, {"text": "The visdial dataset introduced in Das et al .", "entities": [[1, 2, "DatasetName", "visdial"]]}, {"text": "( 2019 ) also includes evaluation metrics and human - annotated answers to the natural language queries about the image .", "entities": []}, {"text": "Five human annotators identi\ufb01ed which responses out of 100 candidates could be considered correct .", "entities": []}, {"text": "This allows multiple answers to be correct ( e.g. , yesandyeah are semantically identical ) .", "entities": []}, {"text": "Metrics We report the following metrics : \u000fR@1 Rate of times the top - ranked answer is a correct one ; i.e. , accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}, {"text": "\u000fR@5 Rate of times correct answers are in the top-\ufb01ve ranked answers .", "entities": []}, {"text": "\u000fMRR Mean Reciprocal Rank is the multiplicative inverse of the rank of the \ufb01rst correct answer .", "entities": []}, {"text": "\u000fNDCG Normalized Discount Accumulative Gain is a measure of ranking quality that takes the top K ranked options , where K is the number of answers marked as correct by a least one annotator ; in this measure , the fraction of annotators that marked a particular answer as correct is taken into account .", "entities": []}, {"text": "Baseline and Procedure", "entities": []}, {"text": "We report the values for the model described in Murahari et", "entities": []}, {"text": "al . ( 2019 ) for our baseline \u2013 work which builds on VilBERT ( Lu et al . , 2019 ) , a parallel model of vision and language used for the visual dialogue task \u2013 and leverage their model with our custom , multimodal embeddings .", "entities": [[13, 14, "MethodName", "VilBERT"]]}, {"text": "Their model uses two transformers , one for the language modality and one for the visual modality .", "entities": []}, {"text": "As explained in Lu et al .", "entities": []}, {"text": "( 2019),the interaction between the two transformers is mediated by two co - attention layers where attention in one modality is conditioned on inputs from the other modality .", "entities": [[13, 15, "HyperparameterName", "attention layers"]]}, {"text": "Murahari et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) adapted the VilBERT model for the visdial task by using a pre - trained language model trained on English Wikipedia and the BooksCorpus ( Zhu et al . , 2015 ) using masked language modeling and next sentence prediction losses .", "entities": [[5, 6, "MethodName", "VilBERT"], [9, 10, "DatasetName", "visdial"], [35, 38, "TaskName", "masked language modeling"]]}, {"text": "They then frame the task as a next - sentence prediction task ( whereas the original VilBERT was modeled to generate descriptions of images ) .", "entities": [[16, 17, "MethodName", "VilBERT"]]}, {"text": "They then use the Conceptual Captions ( Sharma et al . , 2018 ) and Visual Question Answering ( VQA ) ( Antol et al . , 2015 ) datasets ( using masked image region , masked language modeling , and next sentence prediction losses ) to train the two transformers .", "entities": [[4, 6, "DatasetName", "Conceptual Captions"], [15, 18, "DatasetName", "Visual Question Answering"], [19, 20, "TaskName", "VQA"], [36, 39, "TaskName", "masked language modeling"]]}, {"text": "They then \ufb01ne - tune on the visdial task ( also using masked image region , masked language modeling , and next sentence prediction losses ) .", "entities": [[7, 8, "DatasetName", "visdial"], [16, 19, "TaskName", "masked language modeling"]]}, {"text": "The underlying architecture uses a pre - trained BERT language model ( i.e. , bert - base - uncased ) as a starting point before training on the Wikipedia , BooksCorpus , Conceptual Captions , and VQA datasets .", "entities": [[8, 9, "MethodName", "BERT"], [32, 34, "DatasetName", "Conceptual Captions"], [36, 37, "TaskName", "VQA"]]}, {"text": "This constitutes our baseline .", "entities": []}, {"text": "We do n\u2019t consider the dense representations from Murahari", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) due to hardware limitations .", "entities": []}, {"text": "We altered their architecture by replacing the RoBERTa pre - trained embedding layer with the Lancaster and WAC vectors , as depicted in Figure 3 .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}, {"text": "We then \ufb01ne - tuned on the visdial task using their training regime.5We explain how we made vectors compatible with their architecture .", "entities": [[7, 8, "DatasetName", "visdial"]]}, {"text": "Vocabulary : RoBERTa & AoA", "entities": [[2, 3, "MethodName", "RoBERTa"]]}, {"text": "Abstract words do not have concrete , visual denotations , such as utopia orjustice , so it does not make theoretical sense to include a WAC embedding for words that are clearly abstract because whatever set of images represents those concepts may not have useful semantic information .", "entities": []}, {"text": "Moreover , children learning their \ufb01rst language learn concrete concepts before they learn more abstract concepts ( Borghi et al . , 2019 ; Ponari et al . , 2018 ) .", "entities": []}, {"text": "To explore if RoBERTa could make use of a WAC embedding that uses words that are more aimed at a child vocabulary , we report results of \ufb01ltering out words not in the the Age - of - Acquisition ( AoA ) list ( Kuperman et al . , 2012b ) .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}, {"text": "AoA a list of 30,000 English words rated for the average age when children \ufb01rst speak those words ( avg 11 years ; std 3.0 , most common words 5https://github.com/vmurahari3/visdial-bert", "entities": []}, {"text": "154 Figure 3 : Adapted from Murahari et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Our approach uses the same pre - training datasets , architecture , and losses .", "entities": []}, {"text": "During the \ufb01nal \ufb01ne - tuning on the Visual Dialog data , however , we replace the RoBERTa embeddings with the Lancaster and WAC embeddings .", "entities": [[8, 10, "TaskName", "Visual Dialog"], [17, 18, "MethodName", "RoBERTa"]]}, {"text": "are for ages 2 - 14 ) .", "entities": []}, {"text": "This resulted in 9,627 remaining words in the vocabulary ; all other words were set to an embedding of zeros .", "entities": []}, {"text": "Lancaster vectors Similar to AoA , the Lancaster Norms has a prede\ufb01ned vocabulary , which , when compared to the RoBERTa vocabulary results in 11,402 words in both .", "entities": [[20, 21, "MethodName", "RoBERTa"]]}, {"text": "For each word in the RoBERTa vocabulary that was also in the Lancaster norms , we replaced the RoBERTa embedding with the Lancaster vector for that word ; otherwise words retained the original RoBERTa embedding .", "entities": [[5, 6, "MethodName", "RoBERTa"], [18, 19, "MethodName", "RoBERTa"], [33, 34, "MethodName", "RoBERTa"]]}, {"text": "As their model expects vectors of size 768 ( the embedding size for RoBERTa ) , but the Lancaster vectors are only size 39 , we padded the rest of the vector with zeros .", "entities": [[13, 14, "MethodName", "RoBERTa"]]}, {"text": "WAC vectors We use the vocabulary from the RoBERTa tokenizer as with the Lancaster Vectors , which results in a a 27,152 - word overlap with the WAC vectors .", "entities": [[8, 9, "MethodName", "RoBERTa"]]}, {"text": "As the WAC vectors have a dimensionality of 513 , smaller than the required size of RoBERTa \u2019s 768 , we padded zeros after each vector .", "entities": [[16, 17, "MethodName", "RoBERTa"]]}, {"text": "All vectors that did not exist in the WAC set were zero vectors of size 768 .", "entities": []}, {"text": "We followed a training regime for two settings : \u000fno - freeze The embedding layer was not frozen so as to allow weight changes duringtraining .", "entities": []}, {"text": "\u000f2 - freeze The embedding layer was frozen for two epochs , then the weights were unfrozen for the rest of training ; prior work has shown that freezing layers after a certain number of epochs can improve results ( Liu et al . , 2021 ) ; we opt for two because it still allows the \ufb01netuning to overpower the exiting embeddings if needed and preliminary results showed that freezing the weights for all epochs resulted in poor model performance .", "entities": [[33, 36, "HyperparameterName", "number of epochs"]]}, {"text": "We trained each model for 20 epochs in total , which is the default training setting for this task .", "entities": []}, {"text": "We used the settings that were used to train the baseline model ( i.e. , learning rate of 2e-5 ) .", "entities": [[15, 17, "HyperparameterName", "learning rate"]]}, {"text": "We report the results of the baseline model and the variants of our above changes.6Compared to Experiment 1 with the GLUE benchmark , the approach taken in this section fundamentally changes how the Lancaster and WAC embeddings are applied to RoBERTa ; here the Lancaster and WAC embeddings are used on a pre - trained model .", "entities": [[20, 21, "DatasetName", "GLUE"], [40, 41, "MethodName", "RoBERTa"]]}, {"text": "We hy6Note that the baseline we are comparing to here is lower than what is reported on the leaderboard https://eval.ai/web/challenges/ challenge - page/518 / leaderboard .", "entities": []}, {"text": "This is partially due to the fact that our training regime was altered due to hardware limitations ( i.e. , we could only use a batch size of 8 on a single 12 GB GPU ) .", "entities": [[25, 27, "HyperparameterName", "batch size"]]}, {"text": "155no freeze MRR R@1 R@5 NDCG baseline 64.92 50.52 82.98 56.82 lan 64.61 49.98 82.6 58.10 2 - freeze MRR R@1 R@5", "entities": [[2, 3, "MetricName", "MRR"], [3, 4, "MetricName", "R@1"], [4, 5, "MetricName", "R@5"], [19, 20, "MetricName", "MRR"], [20, 21, "MetricName", "R@1"], [21, 22, "MetricName", "R@5"]]}, {"text": "NDCG lan 63.77 49.03 81.63 57.53 wac 66.38 52.25 83.8 61.47 lanwac 63.93 49.025 82.65 57.73 wac - aoa 66.79 52.75 84.05 60.44 Table 2 : Experiment 2 results for the visdial task : baseline RoBERTa embedding , Lancaster norms ( lan ) , WAC vectors , and concatenated ( lanwac ) , not frozen , and only frozen for 2 epochs ( bottom section ) .", "entities": [[31, 32, "DatasetName", "visdial"], [35, 36, "MethodName", "RoBERTa"]]}, {"text": "pothesize that RoBERTa will improve with WAC embeddings , as well as the Lancaster concatenated toWAC ( denoted lanwac ) , though the Lancaster embedding on its own may be too small to make a difference .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}, {"text": "As words that are learned earlier in a child \u2019s life are generally more concrete , we hypothesize that RoBERTa will improve when WAC only uses words from the AoA data as more abstract terms are represented by zero vectors .", "entities": [[19, 20, "MethodName", "RoBERTa"]]}, {"text": "Results Table 2 shows the results for the visdial task .", "entities": [[8, 9, "DatasetName", "visdial"]]}, {"text": "Though it is clear that RoBERTa is doing the heavy lifting , when added to RoBERTa , the Lancaster and WAC vectors show improvements over the RoBERTa baseline for some metrics .", "entities": [[5, 6, "MethodName", "RoBERTa"], [15, 16, "MethodName", "RoBERTa"], [26, 27, "MethodName", "RoBERTa"]]}, {"text": "As noted in Murahari et al .", "entities": []}, {"text": "( 2019 ) , the NDCG metric is actually counter to MRR , but is important because it takes multiple dialogue response annotations into account .", "entities": [[11, 12, "MetricName", "MRR"]]}, {"text": "For cases where the Lancaster and WAC models yield better performance , these results suggest that a pre - trained language model can make use of adding multimodal knowledge in the form of vectors derived from multimodal knowledge ( Lancaster ) and visual ( WAC ) for the visdial task .", "entities": [[48, 49, "DatasetName", "visdial"]]}, {"text": "RoBERTA that uses the WAC embedding especially shows respectable results in the visdial task , particularly when the embedding uses the AoA vocabulary ( we only considered AoA for WAC because WAC peformed better than lanwac in this experiment ) .", "entities": [[12, 13, "DatasetName", "visdial"]]}, {"text": "The WAC vectors were trained on very noisy data , yet despite the noise and the parsimonious model , there is some information about the visual world that enriches the baseline model .", "entities": []}, {"text": "The variant trained with frozen weights for 2 epochs , then unfrozen for the remaining 18 epochs had respectable performance across all metrics.7 7As a sanity check , we also evaluated using randomly generated embeddings which performed only slightly worse6", "entities": []}, {"text": "Conclusion The main contribution of this paper is to explore using the Lancaster Sensorimotor Norms and the Words - as - Classi\ufb01ers model as vectorized knowledge from the physical world on the GLUE and Visual Dialog tasks .", "entities": [[32, 33, "DatasetName", "GLUE"], [34, 36, "TaskName", "Visual Dialog"]]}, {"text": "Lancaster norms performed well on their own in one GLUE task compared to other word embeddings like GloVe , and coupled with the WAC vectors as the embedding in an ELECTRA model , they performed respectably on the GLUE task .", "entities": [[9, 10, "DatasetName", "GLUE"], [14, 16, "TaskName", "word embeddings"], [17, 18, "MethodName", "GloVe"], [30, 31, "MethodName", "ELECTRA"], [38, 39, "DatasetName", "GLUE"]]}, {"text": "The WAC vectors , when used as embeddings in the RoBERTa model performed well on the Visual Dialog task , particularly when the vocabulary was more restricted to the Age of Acquisition vocabulary .", "entities": [[10, 11, "MethodName", "RoBERTa"], [16, 18, "TaskName", "Visual Dialog"]]}, {"text": "Crucially , this work differs from other visually grounded models because the grounded knowledge is part of the language model itself ( i.e. , the embeddings ) rather than computed in parallel and added for a task - speci\ufb01c purpose .", "entities": []}, {"text": "Moreover , standard language models can not actually identify denotations when they are present ; i.e. , ELECTRA and RoBERTa are not actually capable of determining if an object is redorsoftfrom observing that object \u2013 a basic ability for a language learning child \u2013 simply because those models can not observe the world outside of text , though the purpose of the WAC ( and models like VilBERT ) model is to do just that : identify denotations ; by coupling WAC with ELECTRA and RoBERTa , both models can make use of that capability .", "entities": [[17, 18, "MethodName", "ELECTRA"], [19, 20, "MethodName", "RoBERTa"], [67, 68, "MethodName", "VilBERT"], [83, 84, "MethodName", "ELECTRA"], [85, 86, "MethodName", "RoBERTa"]]}, {"text": "This work is critical in our ongoing efforts towards a model that learns language in a co - located setting in an embodied platform .", "entities": []}, {"text": "In particular , our knowledge from this paper informs us that the ELECTRA model with embeddings tied to WAC classi\ufb01er weights is a good candidate for live interaction of a robot that is learning words from a human collaborator because the ELECTRA- WAC model can function with small amounts of data and the embedding layer can successfully be tied to weights of the WAC classi\ufb01ers .", "entities": [[12, 13, "MethodName", "ELECTRA"]]}, {"text": "We leave implementation and evaluation of this model on a robotic platform for future work .", "entities": []}, {"text": "Acknowledgements Thanks to the anonymous reviewers whose comments really helped strengthen the paper .", "entities": []}, {"text": "Also thanks to NVIDIA for donating that GPU that was used for the experiments .", "entities": []}, {"text": "than baseline when frozen for 2 epochs , but the results of wac - aoa are signi\ufb01cantly better .", "entities": []}, {"text": "156References Stanislaw Antol , Aishwarya Agrawal , Jiasen Lu , Margaret Mitchell , Dhruv Batra , C. Lawrence Zitnick , and Devi Parikh . 2015 .", "entities": []}, {"text": "VQA : Visual question answering .", "entities": [[0, 1, "TaskName", "VQA"], [2, 5, "DatasetName", "Visual question answering"]]}, {"text": "In Proceedings of the IEEE International Conference on Computer Vision .", "entities": []}, {"text": "Lawrence W Barsalou .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Grounded Cognition .", "entities": []}, {"text": "Annual Review of Psychology , ( 59):617\u2013645 .", "entities": []}, {"text": "Emily M Bender and Alexander Koller .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Climbing towards NLU : On Meaning , Form , and Understanding in the Age of Data .", "entities": []}, {"text": "In Association for Computational Linguistics , pages 5185\u20135198 .", "entities": []}, {"text": "Yonatan Bisk , Ari Holtzman , Jesse Thomason , Jacob Andreas , Yoshua Bengio , Joyce Chai , Mirella Lapata , Angeliki Lazaridou , Jonathan May , Aleksandr Nisnevich , Nicolas Pinto , and Joseph Turian .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Experience Grounds Language .", "entities": []}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Valentina Borghesani , Marco Buiatti , Evelyn Eger , and Manuela Piazza .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Conceptual and Perceptual Dimensions of Word Meaning Are Recovered Rapidly and in Parallel during Reading .", "entities": []}, {"text": "Journal of Cognitive Neuroscience , 31(1):95\u2013108 .", "entities": []}, {"text": "Anna M Borghi , Laura Barca , Ferdinand Binkofski , Cristiano Castelfranchi , Giovanni Pezzulo , and Luca Tummolini .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Words as social tools : Language , sociality and inner grounding in abstract concepts .", "entities": []}, {"text": "Phys .", "entities": []}, {"text": "Life Rev. , 29:120\u2013153 .", "entities": []}, {"text": "Angelo Cangelosi and Matthew Schlesinger . 2015 .", "entities": []}, {"text": "Developmental robotics : From babies to robots .", "entities": []}, {"text": "MIT press .", "entities": []}, {"text": "Eve V Clark .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "First language acquisition .", "entities": [[1, 3, "TaskName", "language acquisition"]]}, {"text": "Cambridge University Press .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Kevin Clark , Minh - Thang Luong , Quoc V Le , and Christopher D Manning .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "ELECTRA :", "entities": [[0, 1, "MethodName", "ELECTRA"]]}, {"text": "Pretraining text encoders as discriminators rather than generators .", "entities": []}, {"text": "Jeff Da and Jungo Kasai .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Cracking the Contextual Commonsense Code : Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations .", "entities": []}, {"text": "In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Abhishek Das , Satwik Kottur , Khushi Gupta , Avi Singh , Deshraj Yadav , Stefan Lee , Jose M.F. Moura , Devi Parikh , and Dhruv Batra .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Visual Dialog .", "entities": [[0, 2, "TaskName", "Visual Dialog"]]}, {"text": "IEEE Transactions on Pattern Analysis and Machine Intelligence , 41(5 ) .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "Ezequiel A Di Paolo , Elena Clare Cuffari , and Hanne De Jaegher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Linguistic bodies : The continuity between life and language .", "entities": []}, {"text": "Mit Press .", "entities": []}, {"text": "Paul Dourish .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Where the Action Is : The Foundations of Embodied Interaction .", "entities": []}, {"text": "Where the action is the foundations of embodied interaction .", "entities": []}, {"text": "Maxwell Forbes , Ari Holtzman , Yejin Choi , and G Allen . 2019 .", "entities": []}, {"text": "Do Neural Language Representations Learn Physical Commonsense ?", "entities": []}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Fritz G \u00a8unther , Carolin Dudschig , and Barbara Kaup .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Symbol Grounding Without Direct Experience : Do Words Inherit Sensorimotor Activation From Purely Linguistic Context ?", "entities": []}, {"text": "Cognitive Science , 42:336\u2013374 .", "entities": []}, {"text": "Stevan Harnad .", "entities": []}, {"text": "1990 .", "entities": []}, {"text": "The symbol grounding problem .", "entities": []}, {"text": "Physica D : Nonlinear Phenomena , 42(13):335\u2013346 .", "entities": []}, {"text": "Chao Jia , Yinfei Yang , Ye Xia , Yi - Ting Chen , Zarana Parekh , Hieu Pham , Quoc V Le , Yunhsuan Sung , Zhen Li , and Tom Duerig . 2021 .", "entities": []}, {"text": "Scaling up visual and Vision - Language representation learning with noisy text supervision .", "entities": [[7, 9, "TaskName", "representation learning"]]}, {"text": "Mark Johnson .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "The meaning of the body : Aesthetics of human understanding .", "entities": []}, {"text": "University of Chicago Press .", "entities": []}, {"text": "Casey Kennington and David Schlangen . 2015 .", "entities": []}, {"text": "Simple Learning and Compositional Application of Perceptually Grounded Word Meanings for Incremental Reference Resolution .", "entities": []}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 292\u2013301 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Donghyun Kim , Kuniaki Saito , Kate Saenko , Stan Sclaroff , and Bryan A. Plummer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "MULE : Multimodal Universal Language Embedding .", "entities": []}, {"text": "Jamie Ryan Kiros , William Chan , and Geoffrey E Hinton .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Illustrative Language Understanding : Large - Scale Visual Grounding with Image Search .", "entities": [[7, 9, "TaskName", "Visual Grounding"]]}, {"text": "InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Long Papers ) , pages 922\u2013933 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ryan Kiros , Ruslan Salakhutdinov , and Richard S Zemel .", "entities": [[3, 4, "DatasetName", "Ruslan"]]}, {"text": "2014 .", "entities": []}, {"text": "Unifying Visual - Semantic Embeddings with Multimodal Neural Language Models .", "entities": []}, {"text": "In arXiv preprint arXiv:1411.2539 , pages 1\u201313 .", "entities": [[1, 2, "DatasetName", "arXiv"]]}, {"text": "Victor Kuperman , Hans Stadthagen - Gonzalez , and Marc Brysbaert . 2012a .", "entities": []}, {"text": "Age - of - acquisition ratings for 30,000 English words .", "entities": []}, {"text": "Behavior Research Methods , 44(4):978\u2013990 .", "entities": []}, {"text": "157Victor Kuperman , Hans Stadthagen - Gonzalez , and Marc Brysbaert . 2012b .", "entities": []}, {"text": "Age - of - acquisition ratings for 30,000 english words .", "entities": []}, {"text": "Behav .", "entities": []}, {"text": "Res .", "entities": []}, {"text": "Methods , 44(4):978\u2013990 .", "entities": []}, {"text": "George Lakoff and Mark Johnson .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Philosophy in the \ufb02esh : The embodied mind and its challenge to western thought , volume 640 .", "entities": [[0, 1, "TaskName", "Philosophy"]]}, {"text": "Basic books New York .", "entities": []}, {"text": "Liunian Harold Li , Mark Yatskar , Da Yin , Cho Jui Hsieh , and Kai Wei Chang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Visualbert :", "entities": [[0, 1, "MethodName", "Visualbert"]]}, {"text": "A simple and performant baseline for vision and language .", "entities": []}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yuhan Liu , Saurabh Agarwal , and Shivaram Venkataraman . 2021 .", "entities": []}, {"text": "AutoFreeze :", "entities": []}, {"text": "Automatically freezing model blocks to accelerate \ufb01ne - tuning .", "entities": []}, {"text": "Jiasen Lu , Dhruv Batra , Devi Parikh , and Stefan Lee . 2019 .", "entities": []}, {"text": "ViLBERT : Pretraining TaskAgnostic Visiolinguistic Representations for Visionand - Language Tasks .", "entities": [[0, 1, "MethodName", "ViLBERT"]]}, {"text": "Dermot Lynott , Louise Connell , Marc Brysbaert , James Brand , and James Carney . 2019 .", "entities": []}, {"text": "The Lancaster Sensorimotor Norms : multidimensional measures of perceptual and action strength for 40,000 English words .", "entities": []}, {"text": "Behavior Research Methods , pages 1\u201321 .", "entities": []}, {"text": "Lorraine McCune . 2008 .", "entities": []}, {"text": "How Children Learn to Learn Language .", "entities": []}, {"text": "Oxford University Press .", "entities": []}, {"text": "Daniele Moro , Stacy Black , and Casey Kennington .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Composing and embedding the words - asclassi\ufb01ers model of grounded semantics .", "entities": []}, {"text": "Vishvak Murahari , Dhruv Batra , Devi Parikh , and Abhishek Das . 2019 .", "entities": []}, {"text": "Large - scale pretraining for visual dialog : A simple state - of - the - art baseline .", "entities": [[5, 7, "TaskName", "visual dialog"]]}, {"text": "arXiv preprint arXiv:1912.02379 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Marta Ponari , Courtenay Frazier Norbury , and Gabriella Vigliocco .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Acquisition of abstract concepts is in\ufb02uenced by emotional valence .", "entities": []}, {"text": "Dev .", "entities": []}, {"text": "Sci . , 21(2 ) .", "entities": []}, {"text": "Friedemann Pulverm \u00a8uller .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Words in the brain \u2019s language .", "entities": []}, {"text": "Behavioral and Brain Sciences .Anna Rogers , Olga Kovaleva , and Anna Rumshisky .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A Primer in BERTology : What we know about how BERT works .", "entities": [[1, 2, "MethodName", "Primer"], [10, 11, "MethodName", "BERT"]]}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "David Schlangen , Sina Zarriess , and Casey Kennington . 2016 .", "entities": []}, {"text": "Resolving References to Objects in Photographs using the Words - As - Classi\ufb01ers Model .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , pages 1213 \u2013 1223 .", "entities": []}, {"text": "Piyush Sharma , Nan Ding , Sebastian Goodman , and Radu Soricut .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Conceptual captions : A cleaned , hypernymed , image alt - text dataset for automatic image captioning .", "entities": [[0, 2, "DatasetName", "Conceptual captions"], [15, 17, "TaskName", "image captioning"]]}, {"text": "In ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics , Proceedings of the Conference ( Long Papers ) .", "entities": []}, {"text": "L B Smith and L Samuelson .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Objects in Space and Mind : From Reaching to Words .", "entities": []}, {"text": "In The Spatial Foundations of Language and Cognition .", "entities": []}, {"text": "Linda Smith and Michael Gasser .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "The Development of Embodied Cognition : Six Lessons from Babies .", "entities": []}, {"text": "Arti\ufb01cial Life , ( 11):13\u201329 .", "entities": []}, {"text": "Katsumi Takano and Akira Utsumi . 2016 .", "entities": []}, {"text": "Grounded Distributional Semantics for Abstract Words .", "entities": []}, {"text": "CogSci , pages 2171\u20132176 .", "entities": []}, {"text": "Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel R Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "GLUE :", "entities": [[0, 1, "DatasetName", "GLUE"]]}, {"text": "A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[9, 12, "TaskName", "natural language understanding"]]}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 353\u2013355 .", "entities": []}, {"text": "Zhaofeng Wu , Hao Peng , Noah A Smith , and Paul G Allen . 2021 .", "entities": []}, {"text": "Infusing Finetuning with Semantic Dependencies .", "entities": []}, {"text": "Transactions of ACL .", "entities": []}, {"text": "Rowan Zellers , Ari Holtzman , Matthew Peters , Roozbeh Mottaghi , Aniruddha Kembhavi , Ali Farhadi , and Yejin Choi . 2021 .", "entities": []}, {"text": "PIGLeT :", "entities": []}, {"text": "Language grounding through Neuro - Symbolic interaction in a 3D world .", "entities": []}, {"text": "Yukun Zhu , Ryan Kiros , Rich Zemel , Ruslan Salakhutdinov , Raquel Urtasun , Antonio Torralba , and Sanja Fidler .", "entities": [[9, 10, "DatasetName", "Ruslan"]]}, {"text": "2015 .", "entities": []}, {"text": "Aligning books and movies : Towards story - like visual explanations by watching movies and reading books .", "entities": []}, {"text": "In Proceedings of the IEEE International Conference on Computer Vision .", "entities": []}]