[{"text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 5375 - 5388 May 22 - 27 , 2022 c", "entities": []}, {"text": "2022 Association for Computational Linguistics IMPLI : Investigating NLI Models \u2019 Performance on Figurative Language Kevin Stowe}Prasetya Ajie Utamay\u0003Iryna Gurevych } } Ubiquitous Knowledge Processing Lab ( UKP ) Department of Computer Science Technical University of Darmstadt yBloomberg , London , United Kingdom www.ukp.tu-darmstadt.de Abstract Natural language inference ( NLI ) has been widely used as a task to train and evaluate models for language understanding .", "entities": [[27, 28, "DatasetName", "UKP"], [45, 48, "TaskName", "Natural language inference"]]}, {"text": "However , the ability of NLI models to perform inferences requiring understanding of \ufb01gurative language such as idioms and metaphors remains understudied .", "entities": []}, {"text": "We introduce the IMPLI ( Idiomatic and Metaphoric Paired Language Inference ) dataset , an English dataset consisting of paired sentences spanning idioms and metaphors .", "entities": []}, {"text": "We develop novel methods to generate 24k semiautomatic pairs as well as manually creating 1.8k gold pairs .", "entities": []}, {"text": "We use IMPLI to evaluate NLI models based on RoBERTa \ufb01ne - tuned on the widely used MNLI dataset .", "entities": [[9, 10, "MethodName", "RoBERTa"], [17, 18, "DatasetName", "MNLI"]]}, {"text": "We then show that while they can reliably detect entailment relationship between \ufb01gurative phrases with their literal counterparts , they perform poorly on similarly structured examples where pairs are designed to be non - entailing .", "entities": []}, {"text": "This suggests the limits of current NLI models with regard to understanding \ufb01gurative language and this dataset serves as a benchmark for future improvements in this direction.1 1 Introduction Understanding \ufb01gurative language ( i.e. , that in which the intended meaning of the utterance differs from the literal compositional meaning ) is a particularly di \u000ecult area in NLP ( Shutova , 2011 ; Veale et al . , 2016 ) , but is essential for proper natural language understanding .", "entities": [[77, 80, "TaskName", "natural language understanding"]]}, {"text": "We consider here two types of \ufb01gurative language : idioms and metaphors .", "entities": []}, {"text": "Idioms can be viewed as non - compositional multiword expressions ( Jochim et al . , 2018 ) , and have been historically di \u000ecult for NLP systems .", "entities": []}, {"text": "For instance , sentiment systems struggle with multiword expressions in which individual words do not directly contribute to the sentiment ( Sag et al . , 2002 ) .", "entities": [[21, 22, "MethodName", "Sag"]]}, {"text": "\u0003The work was done while the second author was still a\u000eliated with the UKP Lab at TU Darmstadt .", "entities": [[13, 14, "DatasetName", "UKP"]]}, {"text": "1Dataset and all related resources are publicly available at https://github.com/UKPLab/acl2022-impli .IdiomsJamie was pissed o \u000b this afternoon . !", "entities": []}, {"text": "Jamie was irritated this afternoon There \u2019s a marina down in the docks .", "entities": []}, {"text": "9There \u2019s a marina down under scrutiny .", "entities": []}, {"text": "MetaphorsThehearts of men were softened .", "entities": []}, {"text": "!", "entities": []}, {"text": "Themen were made kindler and gentler .", "entities": []}, {"text": "The gun kicked into my shoulder .", "entities": []}, {"text": "9Themule kicked into my shoulder .", "entities": []}, {"text": "Table 1 : Examples of entailment ( ! )", "entities": []}, {"text": "and nonentailment pairs ( 9 ) from the IMPLI dataset .", "entities": []}, {"text": "Metaphors involve linking conceptual properties of two or more domains , and are known to be pervasive in everyday language ( Lako \u000b and Johnson , 1980 ; Stefanowitsch and Gries , 2008 ; Steen et al . , 2010 ) .", "entities": []}, {"text": "Recent work has shown that these types of \ufb01gurative language are impactful across a broad array of NLP tasks ( see \u00a7 2.1 ) .", "entities": []}, {"text": "Large - scale pre - training and transformer - based architectures have yielded increasingly powerful language models ( Vaswani et al . , 2017 ;", "entities": []}, {"text": "Devlin et al . , 2019 ; Liu et al . , 2019 ) .", "entities": []}, {"text": "However , relatively little work has explored these models \u2019 representations of \ufb01gurative and creative language .", "entities": []}, {"text": "NLI datasets have widely been used for evaluating the performance of language models ( Dagan et al . , 2006 ;", "entities": []}, {"text": "Bowman et al . , 2015a ; Williams et al . , 2018 ) , but there are insu \u000ecient \ufb01gurative language datasets in which a literal sentence is linked to a corresponding \ufb01gurative counterpart that are large enough to be suitable for evaluating NLI .", "entities": []}, {"text": "Due to the creative nature of human language , creating a dataset of diverse , high - quality literal /\ufb01gurative pairs is time - consuming and di \u000ecult .", "entities": []}, {"text": "To address this gap , we build a new English dataset of paired expressions designed to be leveraged to explore model performance via NLI .", "entities": []}, {"text": "Our dataset , IMPLI ( Idiomatic /Metaphoric Paired Language Inference ) , is comprised of both silver pairs , which are built using semi - automated5375", "entities": []}, {"text": "methods ( \u00a7 3.1 ) , as well as hand - written gold pairs ( \u00a7 3.4 ) , crafted to re\ufb02ect both entailment and nonentailment scenarios .", "entities": []}, {"text": "Each pair consists of a sentence containing a \ufb01gurative expression ( idioms / metaphors ) and a literal counterpart , designed to be either entailed or non - entailed by the \ufb01gurative expression ( Table 1 shows some examples ) .", "entities": []}, {"text": "Our contribution thus consists of three key parts : \u0088We create a new IMPLI dataset consisting of 24,029 silver and 1,831 gold sentence pairs consisting of idiomatic and metaphoric phrases that result in both entailment and nonentailment relationship ( see Table 2 ) .", "entities": []}, {"text": "\u0088We evaluate language models in an NLI setup , showing that metaphoric language is surprisingly easy , while non - entailing idiomatic relationships remain extremely di \u000ecult .", "entities": []}, {"text": "\u0088We evaluate model performance in a number of experiments , showing that incorporating idiomatic expressions into the training data is less helpful than expected , and that idioms that can occur more in more \ufb02exible syntactic contexts tend to be easier to classify .", "entities": []}, {"text": "2 Background 2.1 Figurative Language and NLP Figurative language includes idioms , metaphors , metonymy , hyperbole , and more .", "entities": []}, {"text": "Critically , \ufb01gurative language is that in which speaker meaning ( what the speaker intends to accomplish through an utterance ) di \u000b ers from the literal meaning of that utterance .", "entities": []}, {"text": "This leads to problems in NLP systems if they are trained mostly on literal data , as their representations for particular words and /or phrases will not re\ufb02ect their \ufb01gurative intended meanings .", "entities": []}, {"text": "Figurative language has a signi\ufb01cant impact on many NLP tasks .", "entities": []}, {"text": "Metaphoric understanding has been shown to be necessary for proper machine translation ( Mao et al . , 2018 ; Mohammad et al . , 2016 ) .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "Sentiment analysis also relies critically on \ufb01gurative language : irony and sarcasm can reverse the polarity of a sentence , while metaphors and idioms may make more subtle changes in the speaker meaning ( Ghosh et al . , 2015 ) .", "entities": [[0, 2, "TaskName", "Sentiment analysis"]]}, {"text": "Political discourse tasks including bias , misinformation , and political framing detection bene\ufb01t from joint learning with metaphoricity ( Huguet Cabot et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Figurative language engendered by creativity on social media also poses di \u000eculty for many NLP tasks including identifying depression symptoms ( Yadavet al . , 2020 ;", "entities": []}, {"text": "Iyer et al . , 2019 ) and hate speech detection ( Lemmens et al . , 2021 ) .", "entities": [[8, 11, "TaskName", "hate speech detection"]]}, {"text": "We are here focused on idioms and metaphors .", "entities": []}, {"text": "There is currently a gap in diagnostic datasets for idioms , and our work \ufb01lls this gap .", "entities": []}, {"text": "There exist some relevant metaphoric resources ( see x2:2 ) ; metaphors are known to be extremely common and important to understanding \ufb01gurative language , our resource serves to build upon this work .", "entities": []}, {"text": "2.2 NLI and related challenges Natural language inference is the task of predicting , given two fragments of text , whether the meaning of one ( premise ) entails the other ( hypothesis ) ( Dagan et al . , 2006 ) .", "entities": [[5, 8, "TaskName", "Natural language inference"]]}, {"text": "The task is formulated as a 3 - way classi\ufb01cation problem , in which the premise and hypothesis pairs are labeled as entailment , contradiction , orneutral , if their relationship could not be directly inferred ( Bowman et al . , 2015b ) .", "entities": []}, {"text": "NLI has been widely used as an evaluation task for language understanding , and there have been a large number of challenging datasets , which have been used to further our understanding of the capabilities of language models ( Wang et al . , 2018 , 2019 ) .", "entities": []}, {"text": "Paired data for \ufb01gurative language is relatively sparse , and there is a gap in the diagnostic datasets used for NLI in this area .", "entities": []}, {"text": "Previous work includes the literal /metaphoric paraphrases of Mohammad et al .", "entities": []}, {"text": "( 2016 ) and Bizzoni and Lappin ( 2018 ) , although both contain only hundreds of samples , insu\u000ecient for proper model training and evaluation .", "entities": []}, {"text": "With regard to NLI , early work proposed the task of textual entailment as a way of understanding metaphor processing capabilities ( Agerri et al . , 2008 ; Agerri , 2008 ) .", "entities": []}, {"text": "Poliak et al .", "entities": []}, {"text": "( 2018 ) build a dataset for diverse NLI , which includes some creative language such as puns , albeit making no claims with regard to \ufb01gurativeness .", "entities": []}, {"text": "Zhou", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2021 ) build a dataset consisting of paired idiomatic and literal expressions .", "entities": []}, {"text": "They begin with a set of 823 idiomatic expressions yielding 5,170 sentences , and had annotators manually rewrite sentences containing these idioms as literal expressions .", "entities": []}, {"text": "We expand on this methodology by having annotators only correct de\ufb01nitions for the idioms themselves and use these de\ufb01nitions to automatically generate the literal interpretations of the idioms by replacing them into appropriate contexts : this allows us to scale up to over 24k silver sentences .", "entities": []}, {"text": "We also expand beyond paraphrasing by incorporating both entailment and non - entailment5376", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "Type Ent .", "entities": []}, {"text": "Gold /silver Description Count Idioms !", "entities": []}, {"text": "Silver Replace idiom used in \ufb01gurative context with de\ufb01nition 16652 9 Silver Replace idiom used in literal context with de\ufb01nition 886 9 Silver Replace idiom used in \ufb01gurative context with adversarial de\ufb01nition 6116 !", "entities": []}, {"text": "Gold Hand written literal de\ufb01nition of idiom 532 9 Gold Manual replacement of key words in de\ufb01nition w /antonyms 375 9 Gold Hand written non - entailed sentence 254 Metaphors !", "entities": []}, {"text": "Silver Replace metaphoric construction with literal construction 375 !", "entities": []}, {"text": "Gold Hand written literal paraphrase of metaphor 388 9 Gold Hand written non - entailed sentence 282 Table 2 : Dataset Summary : Overview of entailments /non - entailment types in IMPLI .", "entities": []}, {"text": "( ! ) denotes entailments , ( 9 ) non - entailments .", "entities": []}, {"text": "Note that the descriptions are simpli\ufb01ed : some intermediate steps are omitted ( see \u00a7 3.1 ) .", "entities": []}, {"text": "pairs to enable NLI - based evaluation .", "entities": []}, {"text": "Similar to this work , Chakrabarty et al .", "entities": []}, {"text": "( 2021a ) build a dataset for NLI based on \ufb01gurative language .", "entities": []}, {"text": "Their dataset consists of \ufb01gurative /literal pairs recast from previously developed simile and metaphor datasets , along with a parallel dataset between ironic and non - ironic rephrasing .", "entities": []}, {"text": "This sets the groundwork for \ufb01gurative NLI , but the dataset is relatively small outside of the irony domain , and the non - entailments are generated purely by replacing words with their antonyms , restricting the novelty of the hypotheses .", "entities": []}, {"text": "Their dataset is relatively easy for NLI models ; here we show that \ufb01gurative language can be challenging , particularly with regard to non - entailments .", "entities": []}, {"text": "Zhou et", "entities": []}, {"text": "al . ( 2021 ) and Chakrabarty et al .", "entities": []}, {"text": "( 2021a ) provide invaluable resources for \ufb01gurative NLI ; our works aims to covers gaps in a number of areas .", "entities": []}, {"text": "First , we generate a large number of both entailment and non - entailment pairs , allowing for better evaluation of adversarial non - entailing examples .", "entities": []}, {"text": "Second , our silver methods allow for rapid development of larger scale data , allowing for model training and evaluation .", "entities": []}, {"text": "We show that while entailment pairs are relatively easy ( accuracy scores ranging from .86 to .89 ) , the non - entailment pairs are exceedingly challenging , with the roberta - large model achieving accuracy scores ranging from .311 to .539 .", "entities": [[10, 11, "MetricName", "accuracy"], [35, 36, "MetricName", "accuracy"]]}, {"text": "3 Building a Dataset Our IMPLI dataset is built from idiomatic and metaphoric sentences paired with entailing and nonentailing counterparts , from both silver pairs ( \u00a7 3.1 ) and manually written sentences ( \u00a7 3.4 ) .", "entities": []}, {"text": "For our purposes , we follow McCoy et al .", "entities": []}, {"text": "( 2019 ) in con\ufb02ating the neutral and contradiction categories into a nonentailment label .", "entities": []}, {"text": "We then label every pair as either entailment ( ! ) or non - entailment ( 9).Due to the di \u000ecult nature of the task and to avoid issues with crowdsourcing ( Bowman et al . , 2020 ) , we employed expert annotators .", "entities": []}, {"text": "We used two \ufb02uent English speakers , both graduate students in linguistics with strong knowledge in \ufb01gurative language , paid at a rate of $ 20 /hr .", "entities": []}, {"text": "For each method below , we ran pilot studies , incorporated annotator feedback and iteratively assessed the viability of identifying and generating appropriate expressions .", "entities": []}, {"text": "As the annotators were working on generating new expressions , agreement was not calculated : we instead assessed the quality of the resulting expressions ( see Section 3.3 ) .", "entities": []}, {"text": "Table 2 contains an overview of the di \u000b erent entailment and non - entailment types collected ( Detail examples are also provided in Appendix D ) .", "entities": []}, {"text": "3.1 Silver pairs First , we explore a method for generating silver pairs using annotators to create phrase de\ufb01nitions which can be inserted automatically into relevant contexts , yielding a large number of possible entailment and non - entailment pairs that di \u000b er only with regard to the relevant phrase .", "entities": []}, {"text": "Our procedure hinges on a key assumption : for any given \ufb01gurative phrase , we can generate a contextually independent literal paraphrase .", "entities": []}, {"text": "We then replace the original expression with the literal paraphrase , following the assumption that the \ufb01gurative expression necessarily entails its literal paraphrase : He \u2019s stuck in bed , which is his hard cheese .!He \u2019s stuck in bed , which is hisbad luck .", "entities": []}, {"text": "Conversely , in contexts where the original phrase is used literally , replacing it with the literal paraphrase should yield a non - entailment relation .", "entities": []}, {"text": "Switzerland is famous for six cheeses , sometimes referred to as hard cheeses .5377", "entities": []}, {"text": "Idiom Corpus   in the docks   under   scrutiny Figurative sentences Idiom   \udb82\udc01 Annotator   Corrections Literal sentences    \ud83d\udcd6 Idiom Dictionary   Lookup   under   accusation ,   scrutiny   Entailments Non - entailments The sailors all worked   under scrutiny .The", "entities": []}, {"text": "sailors all worked   in the docks Those   in the docks   face multiple charges   Those under   scrutiny   face multiple   charges   Entailments Non - entailments Figure 1 : Idiomatic de\ufb01nition replacement .", "entities": []}, {"text": "Pairs are generated using corrected dictionary de\ufb01nitions , substituted into \ufb01gurative ( left ) and literal ( center ) sentences .", "entities": []}, {"text": "9Switzerland is famous for six cheeses , sometimes referred to as bad luck .", "entities": []}, {"text": "3.1.1 Idioms To build idiomatic pairs , we use three corpora that contain sentences with idiomatic expressions ( IEs ) labelled as either \ufb01gurative or literal.2These are the MAGPIE Corpus ( Haagsma et al . , 2020 ) , the PIE Corpus ( Adewumi et al . , 2021 ) , and the SemEval 2013", "entities": [[40, 41, "DatasetName", "PIE"], [53, 55, "DatasetName", "SemEval 2013"]]}, {"text": "Task 5 ( Korkontzelos et al . , 2013 ) .", "entities": []}, {"text": "We collect the total set of IEs that are present in these corpora .", "entities": []}, {"text": "We then extract de\ufb01nitions for these using freely available online idiom dictionaries.3 These de\ufb01nitions are often faulty , incomplete , or improperly formatted .", "entities": []}, {"text": "We employed annotators to make manual corrections .", "entities": []}, {"text": "The annotators were given the original IE as well as the de\ufb01nition extracted from the dictionary .", "entities": []}, {"text": "The annotators were asked to ensure that the dictionary de\ufb01nition given was ( 1 ) a correct literal interpretation and ( 2 ) \ufb01t syntactically in the same environments as the original IE .", "entities": []}, {"text": "If the de\ufb01nition met both of these criteria , the IE can be replaced by its de\ufb01nition to yield an entailment pair .", "entities": []}, {"text": "If either criterion was not met , annotators were asked to minimally update the de\ufb01nition so that it satis\ufb01ed the requirements .", "entities": []}, {"text": "In total this process yielded 697 IE de\ufb01nitions .", "entities": []}, {"text": "We then used the above corpora , replacing these de\ufb01nitions into the original sentences ( see Figure 1 ) .", "entities": []}, {"text": "We use the \ufb01gurative /literal labels from the 2We here use \" idiomatic expression \" or \" IE \" to refer to the speci\ufb01c idiom in question ( ie .", "entities": []}, {"text": "\" kick the bucket \" , \" spill the beans \" ) , as opposed to the sentence /context containing it .", "entities": []}, {"text": "3www.theidioms.com , www.wiktionary.org as right as rain Figurative sentences Idiom     Non - entailments But when he got down there   he was as reliable as rain", "entities": []}, {"text": "But when he got down there   he was as right as rain   as reliable as rain Adversarial Definitions Idiom Corpus   Figure 2 : Adversarial Pair Generation .", "entities": []}, {"text": "Nonentailing pairs are generated by replacing adversarial de\ufb01nitions into \ufb01gurative contexts .", "entities": []}, {"text": "Original IE Adversarial De\ufb01nition man of the cloth tailor heart of gold cold , mean heart come clean bathe turn a trick do a magic trick Table 3 : Sampled hand - written adversarial de\ufb01nitions .", "entities": []}, {"text": "original corpora : replacing them into \ufb01gurative contexts yields entailment relations , while replacing them into contexts where the phrase is meant literally then yields non - entailments .", "entities": []}, {"text": "3.1.2 Adversarial De\ufb01nitions As a second method for generating non - entailment pairs , we asked annotators to write novel , adversarial de\ufb01nitions for IEs .", "entities": []}, {"text": "Given a particular phrase , they were instructed to invent a new meaning for the IE that was not entailed by the true meaning , but which seemed reasonable presuming they had never heard the original IE .", "entities": []}, {"text": "Some examples of this process are shown in Table 3 .", "entities": []}, {"text": "We then replace these adversarial de\ufb01nitions into \ufb01gurative sentences from the corpora .", "entities": []}, {"text": "This yields pairs where the premise is an idiom used \ufb01guratively , and the hypothesis is a sentence that attempts to rephrase the idiom literally , but does so incorrectly , thus yielding non - entailments ( Figure 2 ) .", "entities": []}, {"text": "3.1.3 Metaphors Metaphors are handled in a similar way : we start with a collection of minimal metaphoric expressions ( MEs ) .", "entities": []}, {"text": "These are subject - verb - object and adjective - noun constructions from Tsvetkov et al .", "entities": []}, {"text": "( 2014 ) .", "entities": []}, {"text": "Each is annotated as being either literal or metaphoric , along with an example sentence .", "entities": []}, {"text": "We passed these MEs directly to annotators , who were then instructed to replace a word in the ME so that it would be considered literal in a neutral context .", "entities": []}, {"text": "1.drop prices!reduce", "entities": []}, {"text": "prices5378", "entities": []}, {"text": "catch flight   Verb : board   Direct Object : flight \udb82\udc01 Annotator De\ufb01nitions Verb : catch   Direct Object : flight   Entailments CommonCrawl Corpus Metaphoric   Constructions   They ran through the airport to   catch their flight .", "entities": []}, {"text": "They ran through the airport to   board their flight .Figurative sentences Figure 3 : Metaphor entailment generation .", "entities": []}, {"text": "Pairs are generated using annotator - de\ufb01ned literal translations substituted into metaphoric contexts .", "entities": []}, {"text": "2.hard truth!unpleasant truth 3.hairy problem!di\u000ecultproblem", "entities": []}, {"text": "These can then be replaced in a similar fashion : we start with the original \ufb01gurative sentence , replace the ME with the literal replacements , and the result is an entailing pair with the metaphoric sentence entailing the literal .", "entities": []}, {"text": "We apply this procedure to the dataset of Tsvetkov et al .", "entities": []}, {"text": "( 2014 ) , yielding 100 metaphoric /literal NLI entailment pairs .", "entities": []}, {"text": "We then take a portion of the Common Crawl dataset4 , and identify sentences that contain these original MEs .", "entities": [[7, 9, "DatasetName", "Common Crawl"]]}, {"text": "We identify sentences that contain the words from the metaphoric phrase , and replace the metaphoric word itself with its literal counterpart .", "entities": []}, {"text": "This yields 645 additional silver pairs .", "entities": []}, {"text": "3.2 Postprocessing For all silver methods , we also employ syntactic postprocessing to overcome a number of hurdles .", "entities": []}, {"text": "First , phrases used idiomatically often follow different syntactic patterns than when used literally .", "entities": []}, {"text": "Original :", "entities": []}, {"text": "These point out of this world , but where to is not made clear .", "entities": []}, {"text": "Replaced : * These point wonderful , but where to is not made clear .", "entities": []}, {"text": "This phrase in literal contexts functions syntactically as a prepositional phrase , while idiomatically it is used as an adjective .", "entities": []}, {"text": "When replaced with the de\ufb01nition \" wonderful \" in a literal context , we get a grammatically incoherent sentence .", "entities": []}, {"text": "Second , phrases in their literal usage often do not form full constituents , due to the string - matching approach of the original datasets .", "entities": []}, {"text": "Many literal usages of 4https://commoncrawl.org/these phrases are thus incompatible with the de\ufb01ned replacement .", "entities": []}, {"text": "\u0088I think", "entities": []}, {"text": "[ this one has to die ] forthe other one to live .", "entities": []}, {"text": "\u0088Turn in[the raw edges ] of both seam allowances towards each other and match the folded edges .", "entities": []}, {"text": "To avoid these issues , we ran syntactic parsing on the de\ufb01nition and the expression within each context , requiring that the expression in context begins with the same part of speech as the de\ufb01nition and that it does not end inside of another phrase .", "entities": []}, {"text": "Additionally , for each replacement , we ensured that the verb conjugation matched the context .", "entities": []}, {"text": "For this , we identi\ufb01ed the conjugation in the context , and used a de - lemmatization script to conjugate the replacement verb to match the original .", "entities": [[16, 17, "TaskName", "lemmatization"]]}, {"text": "3.2.1 Additional Issues In implementing and analyzing this procedure , we noted a number of practical issues .", "entities": []}, {"text": "First , a large number of the MEs provided are actually idiomatic or proverbial : the focus word does not actually contribute to the metaphor , but rather the entire expression is necessary .", "entities": []}, {"text": "Similarly , we found that replacing individual parts of MEs is often insu \u000ecient to fully remove the metaphoric meaning .", "entities": []}, {"text": "We iterated over possible solutions to circumvent these issues and found that it is best to simply skip instances for which a replacement does not yield a feasible literal interpretation .", "entities": []}, {"text": "3.3 Evaluating Pair Quality", "entities": []}, {"text": "In order for these automatically created pairs to be useful for NLI - based evaluation , they need to be of su\u000eciently high quality .", "entities": []}, {"text": "As the annotators were generating novel de\ufb01nitions and pairs , rather than inter - annotator agreement , we instead evaluate the quality of the resulting pairs by testing whether the automatically generated pairs contained the appropriate entailment relation .", "entities": []}, {"text": "For this task , each annotator was given 100 samples for each general category of silver generations ( idiomatic entailments , idiomatic non - entailments , and metaphoric entailments ) .", "entities": []}, {"text": "They were asked if the entailment relation between the two sentences was as expected .", "entities": []}, {"text": "An expert than adjudicated disagreements to determine the \ufb01nal percentage of valid pairs .", "entities": []}, {"text": "To evaluate the syntactic validity of the generated pairs , we additionally ran the Stanford PCFG dependency parser ( Klein and Manning , 2003 ) on5379", "entities": []}, {"text": "! Idioms 9Idioms!Met .", "entities": []}, {"text": "Correct Entailments % 88 % 90 % 97 Premise S root", "entities": []}, {"text": "% 89 % 90 % 82 Hypothesis S root % 90 % 90 % 82 Table 4 : Valid pairs .", "entities": []}, {"text": "Percentage of valid pairs , syntactically and with regard to the intended entailments , of automatic data generation .", "entities": []}, {"text": "the pairs .", "entities": []}, {"text": "Per previous work in NLI ( Williams et al . , 2018 ) , we evaluate the proportion of sentences for which the root node is S. Table 4 shows the results .", "entities": []}, {"text": "The semi - supervised examples evoked the correct entailment relation between % 88 and % 97 of the time : while there is still noise present , this indicates the e \u000b ectiveness of the proposed methods .", "entities": []}, {"text": "With regard to syntax , we see S node roots for between 82 % and % 90 of the sentences : within the range of the SNLI performance ( 74%-88 % ) , and slightly behind the MNLI ( 91%-98 % ) .", "entities": [[26, 27, "DatasetName", "SNLI"], [37, 38, "DatasetName", "MNLI"]]}, {"text": "We \ufb01nd that the generated hypotheses are not signi\ufb01cantly di \u000b erent in quality than the premises .", "entities": []}, {"text": "This indicates that the method for generation preserves the original syntax .", "entities": []}, {"text": "These methods allow us to quickly generate a substantial number of high - quality pairs to evaluate NLI systems on \ufb01gurative language .", "entities": []}, {"text": "However , they may introduce additional bias as we employ a number of restrictions in order to ensure syntactic and semantic compatibility , and we lack full nonentailment pairs for metaphoric data .", "entities": []}, {"text": "We therefore expand our dataset with manually generated pairs .", "entities": []}, {"text": "3.4 Manual Creation of Gold Pairs To create gold pairs , annotators were given a \ufb01gurative sentence along with the focus of the \ufb01gurative expression : for idioms , this is the IE ; for metaphors , the focus word of the metaphor .", "entities": []}, {"text": "For idioms , we used the MAGPIE dataset to collect contextually \ufb01gurative expressions .", "entities": []}, {"text": "For metaphors , we collected metaphoric sentences from the VUA Metaphor Corpus ( Steen et al . , 2010 ) , the metaphor dataset of ( Mohammad et al . , 2016 ) , and instances from the Gutenberg poetry corpus ( Jacobs , 2018 ) annotated for metaphoricity ( Chakrabarty et al . , 2021b ; Stowe et al . , 2021 ) .", "entities": []}, {"text": "Annotators were instructed to rewrite the sentence literally .", "entities": []}, {"text": "This was done by removing or rephrasing the \ufb01gurative component of the sentence .", "entities": []}, {"text": "This yields gold standard paraphrases for idiomatic and metaphoric contexts .", "entities": []}, {"text": "We then asked annotators to write non - entailed hypotheses for each premise .", "entities": []}, {"text": "They were encour - aged to keep as much of the original utterance as possible , ensuring high lexical overlap , while removing the main \ufb01gurative element of the sentence .", "entities": []}, {"text": "For idioms , this comes from adding or adjusting words to force a literal reading of the idiom : \u0088The old girl \ufb01nally kicked the bucket .", "entities": []}, {"text": "9The girl kicked the bucket on the right .", "entities": []}, {"text": "For metaphors , this typically involves keeping the same phrasing while adapting the sentence to have a di \u000b erent , non - metaphoric meaning .", "entities": []}, {"text": "\u0088You must adhere to the rules .", "entities": []}, {"text": "9You must adhere the rules to the wall .", "entities": []}, {"text": "3.5 Antonyms Previous work in NLI has employed the technique of replacing words in the literal sentences with their antonyms to yield non - entailing pairs ( Chakrabarty et al . , 2021a ) .", "entities": []}, {"text": "We replicate this process for idioms : for the manually elicited de\ufb01nitions , we replace key words as determined by annotators with their antonyms .", "entities": []}, {"text": "This yields sentences which negate the original \ufb01gurative meaning and are thus suitable non - entailment pairs .", "entities": []}, {"text": "Previous work found this antonym replacement for \ufb01gurative language remains relatively easy for NLI systems , which we can additionally explore with regard to idioms .", "entities": []}, {"text": "These manual annotations provide a number of concrete bene\ufb01ts .", "entities": []}, {"text": "First , they are not restricted to individual words or phrases ( excluding antonyms ): the \ufb01gurative components can be rewritten freely , allowing for diverse , interesting pairs .", "entities": []}, {"text": "Second , they are written by experts , ensuring higher quality than the automatic annotations , which may be noisy .", "entities": []}, {"text": "4 Experiments /Results", "entities": []}, {"text": "Using the IMPLI dataset , we aim to answer a series of questions via NLI pertaining to language models \u2019 ability to understand and represent \ufb01gurative language accurately .", "entities": []}, {"text": "These questions are : \u0088R1 : How well do pre - trained models perform on \ufb01gurative entailments and nonentailments ?", "entities": []}, {"text": "\u0088R2 : Does adding idiomatic pairs into the training data a \u000b ect model performance ?", "entities": []}, {"text": "\u0088R3 :", "entities": []}, {"text": "Does the \ufb02exibility of idiomatic expressions a \u000b ect model performance ?", "entities": []}, {"text": "Our dataset provides unique advantages in addressing these research questions that cover gaps5380", "entities": []}, {"text": "Idioms Metaphors Model MNLI MNLI - MM ! S9Sl9Sd!G9Ga9G!S!G9 G roberta - base .878 .876", "entities": [[3, 4, "DatasetName", "MNLI"], [4, 7, "DatasetName", "MNLI - MM"]]}, {"text": ".848 .539 .409 .890 .771 .311 .947 .818 .818 roberta - large .899 .899 .866 .536 .418 .889 .777 .348 .936 .871 .840 Table 5 :", "entities": []}, {"text": "R1 : Model accuracy .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "Accuracy on MNLI and IMPLI pairs , divided into silver ( S ) and gold ( G ) datasets .", "entities": [[0, 1, "MetricName", "Accuracy"], [2, 3, "DatasetName", "MNLI"]]}, {"text": "SlSilver non - entailment based on replacement in literal contexts , SdSilver non - entailment based on adversarial de\ufb01nitions , GaGold non - entailment based on antonyms .", "entities": []}, {"text": "in previous work : it contains a large number of both entailments and non - entailments and is large enough to be used for training the models .", "entities": []}, {"text": "R1 : pre - trained Model Performance We obtain baseline NLI models by \ufb01ne - tuning roberta - base androberta - large models on the MNLI dataset ( Williams et al . , 2018 ) , with entailments as the positive class and all others as the negative and evaluate them on their original test sets as well as IMPLI .5Due to variance in neural model performance ( Reimers and Gurevych , 2017 ) , we take the mean score over 5 runs using di \u000b erent seeds .", "entities": [[25, 26, "DatasetName", "MNLI"], [87, 88, "DatasetName", "seeds"]]}, {"text": "We report results in Table 5 .", "entities": []}, {"text": "We observe that idiomatic entailments are relatively easy to classify , with accuracy scores over .84 .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "Non - entailments were much more challenging .", "entities": []}, {"text": "Silver pairs generated through adversarial de\ufb01nitions were especially di\u000ecult : the pairs contain high lexical overlap , and in many cases the premise and hypotheses are semantically similar .", "entities": []}, {"text": "The replacement into literal samples were easier , as the idiomatic de\ufb01nition clashes more starkly with the original premise , making non - entailment predictions more likely .", "entities": []}, {"text": "Consistent with Chakrabarty et al .", "entities": []}, {"text": "( 2021a ) \u2019s work in metaphors , non - entailment through antonym replacement is easiest for idioms : the antonymic relationship can be a marker for non - entailment , despite the high word overlap .", "entities": []}, {"text": "With regard to metaphors , silver entailment pairs are relatively easy .", "entities": []}, {"text": "Manual pairs are more challenging but are still much easier than idioms .", "entities": []}, {"text": "This is supported by the fact that metaphors are common in everyday language : these models have likely seen the same ( or similar ) metaphors in training .", "entities": []}, {"text": "Our \ufb01ndings show that in fact metaphoricity may not be particularly challenging for deep pre - trained models , as they are able to e \u000b ectively capture the metaphoric entailment relations .", "entities": []}, {"text": "Theroberta - large model performs better for metaphoric expressions than roberta - base , but 5Model hyperparameters found in Appendix A.the di \u000b erence on other partitions is relatively small.6We also \ufb01nd that lexical overlap plays a signi\ufb01cant role here as noted by previous work ( McCoy et al . , 2019 ): sentences with high overlap tend to be classi\ufb01ed as entailments regardless of the true label ( for more , see Appendix B ) .", "entities": []}, {"text": "We note that the manual pairs tend to be more di\u000ecult for both idioms and metaphors : these pairs can be more \ufb02exible and creative , whereas the silver pairs are restricted to more regular patterns .", "entities": []}, {"text": "R2 :", "entities": []}, {"text": "Incorporating Idioms into Training To evaluate incorporating idioms into training , we then split the idiom data by idiomatic phrase types , keeping a set of IEs separate as test data to assess whether the model can learn to correctly handle novel , unseen phrases .", "entities": []}, {"text": "Our goal is to assess whether poor performance is due to models \u2019 not containing these expressions in training , or because their ability to represent \ufb01gurative language inherently limited .", "entities": []}, {"text": "We hypothesize that the noncompositional nature of these types of \ufb01guration should lead to poor performance on unseen phrases , even if the model is trained on other idiomatic data .", "entities": []}, {"text": "For each task , we split the data into 10 folds by IE and incrementally incorporate these folds into the original MNLI for training , leaving one fold out for testing .", "entities": [[21, 22, "DatasetName", "MNLI"]]}, {"text": "We experiment with incorporating all training data for both labels , as well as using only entailment or non - entailment samples .", "entities": []}, {"text": "We then evaluate our results on the entire test set , as well as the entailment and non - entailment partitions .", "entities": []}, {"text": "Figure 4 shows the results , highlighting that additional training data yields only small improvements .", "entities": []}, {"text": "Pairs with non - entailment relations remain exceedingly di \u000ecult , with performance capping out at only slightly better than chance .", "entities": []}, {"text": "As hypothesized , additional training data is only somewhat e \u000b ective in improving language models \u2019 idiomatic capabilities ; this is not su \u000ecient to overcome di \u000eculties from literal usages of idiomatic phrases and adversarial de\ufb01nitions , indicating that idiomatic 6We found minimal di \u000b erences between these models across R1 - R3.5381", "entities": []}, {"text": "Figure 4 : R2 : Training .", "entities": []}, {"text": "Performance of the roberta - base models as more idiom examples are added to the training data .", "entities": []}, {"text": "language remains di \u000ecult for pre - trained language models to learn to represent .", "entities": []}, {"text": "R3 :", "entities": []}, {"text": "Syntactic Flexibility Finally , we assess models \u2019 representation of idiomatic compositionality .", "entities": []}, {"text": "Nunberg et al .", "entities": []}, {"text": "( 1994 ) indicate that there are two general types of idioms : \" idiomatic phrases \" , which exhibit limited \ufb02exibility and generally occur only in a single surface form , and \" idiomatically combining expressions \" or ICEs , in which the constituent elements of the idiom carry semantic meaning which can in\ufb02uence their syntactic properties , allowing them to be more syntactically \ufb02exible .", "entities": []}, {"text": "For example , in the idiom spill the beans , we can map the spilling activity to divulging of information , and the beans to the information .", "entities": []}, {"text": "Because this expression has semantic mappings to \ufb01gurative meaning for its syntactic constituents , Nunberg et al .", "entities": []}, {"text": "( 1994 ) argue that it can be more syntactically \ufb02exible , allowing for expressions like the beans that were spilled by Martha to maintain idiomatic meaning .", "entities": []}, {"text": "For \ufb01xed expressions such as kick the bucket , no syntactic constituents map directly to the \ufb01gurative meaning ( \" die \" ) .", "entities": []}, {"text": "We then expect less syntactic \ufb02exibility , and thus the bucket that was kicked by John loses its idiomatic meaning .", "entities": []}, {"text": "We hypothesize that model performance will be correlated with the degree to which a given idiom type is \ufb02exible : more \ufb01xed expressions may be easier , as they are seen in regular , \ufb01xed patterns that the models can memorize , while more \ufb02exible ICEs will be more di \u000ecult , as they can appear in di \u000b erent patterns , cases , and word order , often even mixing in with other constituents .", "entities": []}, {"text": "To test this , we de\ufb01ne an ICE score as the percentage of times a phrase occurs in our test data in a form that does not match its original base form .", "entities": []}, {"text": "Higher percentages mean the phrase occurs more frequently ina non - standard form , acting as a measure for the syntactic \ufb02exibility of the expression .", "entities": []}, {"text": "We assessed the performance of the roberta - base model for each idiom type , evaluating Spearman correlations between performance and idioms \u2019 ICE scores .", "entities": []}, {"text": "We found no correlation between ICE scores and performance for entailments , nor for adversarial de\ufb01nition non - entailments ( r=:004 = : 45 , p=:921 = : 399 , see Appendix C ) .", "entities": []}, {"text": "However , we do see a weak but signi\ufb01cant correlation ( r=:188 , p=0:016 ) with non - entailments from literal contexts : the model performs better when the phrases are more \ufb02exible , contrary to our initial hypothesis .", "entities": []}, {"text": "One possible explanation is that the model memorizes a speci\ufb01c \ufb01gurative meanings for each \ufb01xed expression , disregarding the possibility of these words being used literally .", "entities": []}, {"text": "When the expression is used in a literal context , the model then still assumes the \ufb01gurative meaning , resulting in errors on non - entailment samples .", "entities": []}, {"text": "The ICEs are more \ufb02uid , and thus the model is less likely to have a concrete representation for the given phrase : it is better able to reason about the context and interacting words within the expression , making it easier to distinguish the entailing and non - entailing samples .", "entities": []}, {"text": "5 Conclusions and Future Work", "entities": []}, {"text": "In this work , we introduce the IMPLI dataset , which we then use to evaluate NLI models \u2019 capabilities on \ufb01gurative language .", "entities": []}, {"text": "We show that while widely used MNLI models handle entailment admirably and metaphoric expressions are relatively easy , nonentailment idiomatic relationships are more di \u000ecult .", "entities": [[6, 7, "DatasetName", "MNLI"]]}, {"text": "Additionally , adding idiom - speci\ufb01c training data fails to alleviate poor performance for nonentailing pairs .", "entities": []}, {"text": "This highlights how currently language models are inherently limited in representing some \ufb01gurative phenomena and can provide a target for future model improvements .", "entities": []}, {"text": "For future work , we aim to expand our data collection processes to new data sources .", "entities": []}, {"text": "Our dataset creation procedure relies on annotated samples and de\ufb01nitions : as more idiomatic and metaphoric resources become available , this process is broadly extendable to create new \ufb01gurative /literal pairs .", "entities": []}, {"text": "Additionally , we only explore this data for evaluating NLI systems : this data could also be used for other parallel data tasks such as \ufb01gurative language interpretation ( Shutova , 2013 ; Su et al . , 2017 ) and \ufb01gurative paraphrase generation .", "entities": [[42, 44, "TaskName", "paraphrase generation"]]}, {"text": "As natural language generation often relies on training or \ufb01ne - tuning5382", "entities": []}, {"text": "models with paired sentences , this data could be a valuable resource for \ufb01gurative language generation systems .", "entities": []}, {"text": "References Tosin P. Adewumi , Saleha Javed , Roshanak Vadoodi , Aparajita Tripathy , Konstantina Nikolaidou , Foteini Liwicki , and Marcus Liwicki . 2021 .", "entities": []}, {"text": "Potential idiomatic expression ( pie)-english : Corpus for classes of idioms .", "entities": []}, {"text": "arXiv preprint arXiv:2105.03280 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rodrigo Agerri . 2008 .", "entities": []}, {"text": "Metaphor in textual entailment .", "entities": []}, {"text": "InColing 2008 :", "entities": []}, {"text": "Companion volume : Posters , pages 3\u20136 , Manchester , UK .", "entities": []}, {"text": "Coling 2008 Organizing Committee .", "entities": []}, {"text": "Rodrigo Agerri , John Barnden , Mark Lee , and Alan Wallington . 2008 .", "entities": []}, {"text": "Textual entailment as an evaluation framework for metaphor resolution : A proposal .", "entities": []}, {"text": "InSemantics in Text Processing .", "entities": []}, {"text": "STEP 2008 Conference Proceedings , pages 357\u2013363 .", "entities": []}, {"text": "College Publications .", "entities": []}, {"text": "Yuri Bizzoni and Shalom Lappin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Predicting human metaphor paraphrase judgments with deep neural networks .", "entities": []}, {"text": "In Proceedings of the Workshop on Figurative Language Processing , pages 45\u201355 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Samuel R. Bowman , Gabor Angeli , Christopher Potts , and Christopher D. Manning .", "entities": []}, {"text": "2015a .", "entities": []}, {"text": "A large annotated corpus for learning natural language inference .", "entities": [[6, 9, "TaskName", "natural language inference"]]}, {"text": "InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 632\u2013642 , Lisbon , Portugal .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Samuel R. Bowman , Gabor Angeli , Christopher Potts , and Christopher D. Manning .", "entities": []}, {"text": "2015b .", "entities": []}, {"text": "A large annotated corpus for learning natural language inference .", "entities": [[6, 9, "TaskName", "natural language inference"]]}, {"text": "InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 632\u2013642 , Lisbon , Portugal .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Samuel R. Bowman , Jennimaria Palomaki , Livio Baldini Soares , and Emily Pitler .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "New protocols and negative results for textual entailment data collection .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 8203\u20138214 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tuhin Chakrabarty , Debanjan Ghosh , Adam Poliak , and Smaranda Muresan .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "2021a .", "entities": []}, {"text": "Figurative language in recognizing textual entailment .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : ACLIJCNLP 2021 , pages 3354\u20133361 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tuhin Chakrabarty , Xurui Zhang , Smaranda Muresan , and Nanyun Peng .", "entities": []}, {"text": "2021b .", "entities": []}, {"text": "MERMAID :", "entities": []}, {"text": "Metaphorgeneration with symbolism and discriminative decoding .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 4250\u20134261 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ido Dagan , Oren Glickman , and Bernardo Magnini . 2006 .", "entities": []}, {"text": "The pascal recognising textual entailment challenge .", "entities": []}, {"text": "In Machine Learning Challenges .", "entities": []}, {"text": "Evaluating Predictive Uncertainty , Visual Object Classi\ufb01cation , and Recognising Tectual Entailment , pages 177\u2013190 , Berlin , Heidelberg .", "entities": []}, {"text": "Springer Berlin Heidelberg .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Aniruddha Ghosh , Guofu Li , Tony Veale , Paolo Rosso , Ekaterina Shutova , John Barnden , and Antonio Reyes .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "SemEval-2015 task 11 : Sentiment analysis of \ufb01gurative language in Twitter .", "entities": [[4, 6, "TaskName", "Sentiment analysis"]]}, {"text": "In Proceedings of the 9th International Workshop on Semantic Evaluation ( SemEval 2015 ) , pages 470\u2013478 , Denver , Colorado .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hessel Haagsma , Johan Bos , and Malvina Nissim .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "MAGPIE :", "entities": []}, {"text": "A large corpus of potentially idiomatic expressions .", "entities": []}, {"text": "In Proceedings of the 12th Language Resources and Evaluation Conference , pages 279\u2013287 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Pere - Llu\u00eds Huguet Cabot , Verna Dankers , David Abadi , Agneta Fischer , and Ekaterina Shutova .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "The Pragmatics behind Politics : Modelling Metaphor , Framing and Emotion in Political Discourse .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 4479\u20134488 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Adith Iyer , Aditya Joshi , Sarvnaz Karimi , Ross Sparks , and Cecile Paris .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Figurative usage detection of symptom words to improve personal health mention detection .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1142\u20131147 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Arthur M Jacobs .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The Gutenberg English poetry corpus : exemplary quantitative narrative analyses .", "entities": []}, {"text": "Frontiers in Digital Humanities , 5:5 .", "entities": []}, {"text": "Charles Jochim , Francesca Bonin , Roy Bar - Haim , and Noam Slonim .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SLIDE - a sentiment lexicon of common idioms .", "entities": []}, {"text": "In Proceedings of the Eleventh International Conference on Language Resources and5383", "entities": []}, {"text": "Evaluation , Miyazaki , Japan .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Dan Klein and Christopher D. Manning .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Accurate unlexicalized parsing .", "entities": []}, {"text": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pages 423\u2013430 , Sapporo , Japan . Association for Computational Linguistics .", "entities": []}, {"text": "Ioannis Korkontzelos , Torsten Zesch , Fabio Massimo Zanzotto , and Chris Biemann .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "SemEval-2013 task 5 : Evaluating phrasal semantics .", "entities": []}, {"text": "In Second Joint Conference on Lexical and Computational Semantics ( * SEM ) , Volume 2 : Proceedings of the Seventh International Workshop on Semantic Evaluation ( SemEval 2013 ) , pages 39\u201347 , Atlanta , Georgia , USA .", "entities": [[27, 29, "DatasetName", "SemEval 2013"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "George Lako \u000b and Mark Johnson .", "entities": []}, {"text": "1980 .", "entities": []}, {"text": "Metaphors We Live By .", "entities": []}, {"text": "University of Chicago Press , Chicago and London .", "entities": []}, {"text": "Jens Lemmens , Ilia Markov , and Walter Daelemans .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Improving hate speech type and target detection with hateful metaphor features .", "entities": [[1, 3, "DatasetName", "hate speech"]]}, {"text": "In Proceedings of the Fourth Workshop on NLP for Internet Freedom : Censorship , Disinformation , and Propaganda , pages 7\u201316 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Vladimir Iosifovich Levenshtein .", "entities": []}, {"text": "1965 .", "entities": []}, {"text": "Binary codes capable of correcting deletions , insertions , and reversals .", "entities": []}, {"text": "In Doklady Akademii Nauk , volume 163 , pages 845\u2013848 .", "entities": []}, {"text": "Russian Academy of Sciences .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "RoBERTa : A robustly optimized bert pretraining approach .", "entities": [[0, 1, "MethodName", "RoBERTa"]]}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rui Mao , Chenghua Lin , and Frank Guerin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Word embedding and WordNet based metaphor identi\ufb01cation and interpretation .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1222\u20131231 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tom McCoy , Ellie Pavlick , and Tal Linzen .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Right for the wrong reasons : Diagnosing syntactic heuristics in natural language inference .", "entities": [[10, 13, "TaskName", "natural language inference"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3428\u20133448 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Saif Mohammad , Ekaterina Shutova , and Peter Turney . 2016 .", "entities": []}, {"text": "Metaphor as a medium for emotion : An empirical study .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "In Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics , pages 23\u201333 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yixin Nie , Yicheng Wang , and Mohit Bansal .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Analyzing compositionality - sensitivity of nli models .", "entities": []}, {"text": "Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 33(01):6867\u20136874 .", "entities": []}, {"text": "Geo \u000b rey Nunberg , Ivan A. Sag , and Thomas Wasow .", "entities": [[7, 8, "MethodName", "Sag"]]}, {"text": "1994 .", "entities": []}, {"text": "Idioms .", "entities": []}, {"text": "Language , ( 3):491\u2013538 .", "entities": []}, {"text": "Adam Poliak , Jason Naradowsky , Aparajita Haldar , Rachel Rudinger , and Benjamin Van Durme .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "2018 .", "entities": []}, {"text": "Hypothesis only baselines in natural language inference .", "entities": [[4, 7, "TaskName", "natural language inference"]]}, {"text": "In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics , pages 180\u2013191 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nils Reimers and Iryna Gurevych .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Reporting score distributions makes a di \u000b erence :", "entities": []}, {"text": "Performance study of LSTM - networks for sequence tagging .", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 338\u2013348 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Ivan A. Sag , Timothy Baldwin , Francis Bond , Ann Copestake , and Dan Flickinger .", "entities": [[2, 3, "MethodName", "Sag"]]}, {"text": "2002 .", "entities": []}, {"text": "Multiword expressions : A pain in the neck for nlp .", "entities": []}, {"text": "In Computational Linguistics and Intelligent Text Processing , pages 1\u201315 , Berlin , Heidelberg .", "entities": []}, {"text": "Springer Berlin Heidelberg .", "entities": []}, {"text": "Ekaterina Shutova .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Metaphor identi\ufb01cation as interpretation .", "entities": []}, {"text": "In Second Joint Conference on Lexical and Computational Semantics ( * SEM ) , Volume 1 : Proceedings of the Main Conference and the Shared Task :", "entities": []}, {"text": "Semantic Textual Similarity , pages 276\u2013285 , Atlanta , Georgia , USA . Association for Computational Linguistics .", "entities": [[0, 3, "TaskName", "Semantic Textual Similarity"]]}, {"text": "Ekaterina V Shutova .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Computational approaches to \ufb01gurative language .", "entities": []}, {"text": "Technical report , Cambridge University .", "entities": [[3, 4, "DatasetName", "Cambridge"]]}, {"text": "Gerard J. Steen , Aletta G. Dorst , J. Berenike Herrmann , Anna Kaal , Tina Krennmayr , and Trijntje Pasma . 2010 .", "entities": []}, {"text": "A Method for Linguistic Metaphor Identi\ufb01cation : From MIP to MIPVU .", "entities": []}, {"text": "John Benjamins .", "entities": []}, {"text": "Anatol Stefanowitsch and Stefan Th . Gries . 2008 .", "entities": []}, {"text": "Corpus - Based Approaches to Metaphor and Metonymy .", "entities": []}, {"text": "De Gruyter Mouton .", "entities": []}, {"text": "Kevin Stowe , Tuhin Chakrabarty , Nanyun Peng , Smaranda Muresan , and Iryna Gurevych . 2021 .", "entities": []}, {"text": "Metaphor generation with conceptual mappings .", "entities": []}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 6724\u20136736 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chang Su , Shuman Huang , and Yijiang Chen . 2017 .", "entities": []}, {"text": "Automatic detection and interpretation of nominal metaphor based on the theory of meaning .", "entities": []}, {"text": "Neurocomputing , 219:300\u2013311.5384", "entities": []}, {"text": "Yulia Tsvetkov , Leonid Boytsov , Anatole Gershman , Eric Nyberg , and Chris Dyer . 2014 .", "entities": []}, {"text": "Metaphor detection with cross - lingual model transfer .", "entities": []}, {"text": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 248\u2013258 , Baltimore , Maryland .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Tony Veale , Ekatarina Shutova , and Beata Beigman Klebanov . 2016 .", "entities": []}, {"text": "Metaphor : A computational perspective .", "entities": []}, {"text": "Morgan and Claypool .", "entities": []}, {"text": "Alex Wang , Yada Pruksachatkun , Nikita Nangia , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel R. Bowman . 2019 .", "entities": []}, {"text": "Superglue : A stickier benchmark for general - purpose language understanding systems .", "entities": [[0, 1, "DatasetName", "Superglue"]]}, {"text": "In Proceedings of the 33rd International Conference on Neural Information Processing Systems , pages 3266\u2013\u20133280 , Red Hook , NY , USA .", "entities": []}, {"text": "Curran Associates Inc.", "entities": []}, {"text": "Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "GLUE :", "entities": [[0, 1, "DatasetName", "GLUE"]]}, {"text": "A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[9, 12, "TaskName", "natural language understanding"]]}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 353\u2013355 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Adina Williams , Nikita Nangia , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A broad - coverage challenge corpus for sentence understanding through inference .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1112\u20131122 , New Orleans , Louisiana . Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 38\u201345 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shweta Yadav , Jainish Chauhan , Joy Prakash Sain , Krishnaprasad Thirunarayan , Amit Sheth , and Jeremiah Schumm .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Identifying depressive symptoms from tweets :", "entities": []}, {"text": "Figurative language enabled multitask learning framework .", "entities": []}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics , pages 696\u2013709 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Jianing Zhou , Hongyu Gong , and Suma Bhat . 2021 .", "entities": []}, {"text": "PIE : A parallel idiomatic expression corpus for idiomatic sentence generation and paraphrasing .", "entities": [[0, 1, "DatasetName", "PIE"]]}, {"text": "In Proceedings of the 17th Workshop on Multiword Expressions , pages 33\u201348 , Online .", "entities": []}, {"text": "Association for Computational Linguistics.5385", "entities": []}, {"text": "A Model Hyperparameters We use a \ufb01xed set of hyperparameters for all NLI \ufb01ne - tuning experiments : learning rate of 1e\u00005 , batch size 32 , and maximum input length of 128 tokens .", "entities": [[18, 20, "HyperparameterName", "learning rate"], [23, 25, "HyperparameterName", "batch size"]]}, {"text": "The models are trained for 3 epochs .", "entities": []}, {"text": "We used the HuggingFace implementation of the models ( Wolf et al . , 2020 ) .", "entities": []}, {"text": "B Lexical Overlap Previous research shows that NLI systems exploit cues based on lexical overlap , predicting entailment for overlapping sentences ( McCoy et al . , 2019 ; Nie et al . , 2019 ) .", "entities": []}, {"text": "Our dataset consists mostly of pairs with high overlap : this could explain why the nonentailment sections are more di \u000ecult .", "entities": []}, {"text": "We thus evaluate system predictions for our datasets as a function of lexical overlap .", "entities": []}, {"text": "Figure 5 shows densitybased histograms of the results , comparing overlap via Levenshtein distance ( Levenshtein , 1965 ) for correctly and incorrectly classi\ufb01ed pairs .", "entities": []}, {"text": "Our data contains higher overlap than the MNLI data , with the bulk of the density falling on minimally distant pairs .", "entities": [[7, 8, "DatasetName", "MNLI"]]}, {"text": "We also note a distinct di \u000b erence between our entailment and non - entailment pairs : non - entailments contain extremely high overlap and are frequently misclassi\ufb01ed in these cases where the distance is small , matching previous reports for NLI tasks : lexical overlap is a key artifact for entailment , and this reliance persists when classifying idiomatic pairs .", "entities": []}, {"text": "C Syntactic Flexibility Correlations Figure 6 shows correlations between ICE scores ( determined by frequency of occurences of a given IE outside of its normal form ) and roberta - base model performance on that IE .", "entities": []}, {"text": "D Dataset Examples Table 6 shows examples from each type of pair generation.5386", "entities": []}, {"text": "0 20 40 60 80 100 IMPLI Entailments0.000.010.020.030.040.05Density 0 20 40 60 80 100 IMPLI Non - entailments0.000.010.020.030.04 0 20 40 60 80 100 MNLI Entailments0.0000.0020.0040.0060.0080.0100.0120.014 0 20 40 60 80 100 MNLI Non - entailments0.0000.0020.0040.0060.0080.0100.0120.014 Correct IncorrectFigure 5 : R2 : Lexical Overlap .", "entities": [[0, 1, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [24, 25, "DatasetName", "MNLI"], [26, 27, "DatasetName", "0"], [32, 33, "DatasetName", "MNLI"]]}, {"text": "Classi\ufb01cation performance by lexical overlap .", "entities": []}, {"text": "The x - axis shows Levenshtein distance ; the y - axis shows stacked density of correctly and incorrectly tagged pairs .", "entities": []}, {"text": "The IMPLI non - entailments contain extremely high overlap , and are thus frequently misclassi\ufb01ed as entailment .", "entities": []}, {"text": "Figure 6 : R3 : Syntactic Flexibility .", "entities": []}, {"text": "Performance of idiom types compared to their syntactic \ufb02exibility ( based on ICE score de\ufb01ned in R3 ) , with Spearman coe \u000ecient correlations rand signi\ufb01cance values p.", "entities": []}, {"text": "The middle \ufb01gure is non - entailments based on replacement in literal context ; the right is those based on adversarial de\ufb01nitions .", "entities": []}, {"text": "Further right on the x - axis indicates greater \ufb02exibility.5387", "entities": []}, {"text": "Idioms ( ! S ) Replace idiom used in \ufb01gurative context with de\ufb01nition BITTER BLOW : Beer sales are feeling the pinch .!BITTER", "entities": []}, {"text": "BLOW : Beer sales are su \u000b ering a hardship .", "entities": []}, {"text": "I must have a word with them.!I must speak privately with them .", "entities": []}, {"text": "I \u2019ve been knocked out cold .!I\u2019ve been knocked unconscious .", "entities": []}, {"text": "( 9Sl ) Replace idiom used in literal context with de\ufb01nition It would be good to roll in hot water all over.9It would be good to roll in a di \u000ecult situation all over .", "entities": []}, {"text": "Pour in the soup .9Pour in trouble .", "entities": []}, {"text": "There \u2019s a marina down in the docks .9There \u2019s a marina down under scrutiny .", "entities": []}, {"text": "( 9Sd ) Replace idiom used in \ufb01gurative context with adversarial de\ufb01nition After taking a bow , the cast met Margaret backstage .", "entities": []}, {"text": "9After apologizing , the cast met Margaret backstage .", "entities": []}, {"text": "I \u2019ve been knocked out cold9I\u2019ve been knocked out into the cold air .", "entities": []}, {"text": "It worked like a charm !", "entities": []}, {"text": "9It worked poorly !", "entities": []}, {"text": "( ! G ) Hand written literal de\ufb01nition of idiom How have you weathered the storm ? !", "entities": []}, {"text": "How have you succeeded in getting through the di \u000ecult situation ?", "entities": []}, {"text": "Itbreaks my heart that his career has been ruined . !", "entities": []}, {"text": "Itoverwhelms me that his career has been ruined .", "entities": []}, {"text": "Jamie rushed out pissed o \u000b and upset this afternoon . !", "entities": []}, {"text": "Jamie rushed out irritated and upset this afternoon .", "entities": []}, {"text": "( 9Ga ) Manual replacement of key words in de\ufb01nition w /antonyms", "entities": []}, {"text": "Alison makes the grade for Scotland 9Alison fails for Scotland .", "entities": []}, {"text": "I\u2019llcatch a cold 9I\u2019ll become healthy It \u2019s very much swings and roundabouts", "entities": []}, {"text": "9It \u2019s very much one - sided .", "entities": []}, {"text": "( 9 G ) Hand written non - entailed sentence How have you weathered the storm ? 9How have you calmed the storm ?", "entities": []}, {"text": "Now Paul will think twice .9Now", "entities": []}, {"text": "Paul will score twice .", "entities": []}, {"text": "They went to ground somewhere in the area .", "entities": []}, {"text": "9They went to party somewhere in the area .", "entities": []}, {"text": "Metaphors ( ! S ) Replace metaphoric construction with literal construction Do not go and blow your paycheck .!Do", "entities": []}, {"text": "not go and waste your paycheck .", "entities": []}, {"text": "My computer battery died .!My computer battery lost all power .", "entities": []}, {"text": "Competition is dropping prices .!Competition is reducing prices .", "entities": []}, {"text": "( ! G ) Hand written literal paraphrase of metaphor Heabsorbed the knowledge or beliefs of his tribe . !", "entities": []}, {"text": "Hementally assimilated the knowledge or beliefs of his tribe .", "entities": []}, {"text": "Avon treads warily.!Avon proceeds warily .", "entities": []}, {"text": "All the hearts of men were softened .!All", "entities": []}, {"text": "the men were made kindler and gentler .", "entities": []}, {"text": "( 9 G ) Hand written non - entailed sentence The gun kicked back into my shoulder .", "entities": []}, {"text": "9The mule kicked back into my shoulder .", "entities": []}, {"text": "This was conveniently encapsulated on the \ufb01rst try .", "entities": []}, {"text": "9This was conveniently encapsulated in the \ufb01rst battle .", "entities": []}, {"text": "On their tracks his eyes were fastened .9On", "entities": []}, {"text": "their tracks his hands were fastened .", "entities": []}, {"text": "Table 6 : Dataset Summary : Overview of each entailment /non - entailment category in the IMPLI dataset.5388", "entities": []}]