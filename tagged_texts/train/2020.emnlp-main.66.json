[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 917\u2013929 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics917TOD - BERT : Pre - trained Natural Language Understanding for Task - Oriented Dialogue Chien - Sheng Wu , Steven Hoi , Richard Socher , and Caiming Xiong Salesforce", "entities": [[6, 7, "MethodName", "BERT"], [11, 14, "TaskName", "Natural Language Understanding"]]}, {"text": "Research", "entities": []}, {"text": "[ wu.jason , shoi , rsocher , cxiong]@salesforce.com Abstract", "entities": []}, {"text": "The underlying difference of linguistic patterns between general text and task - oriented dialogue makes existing pre - trained language models less useful in practice .", "entities": []}, {"text": "In this work , we unify nine human - human and multi - turn task - oriented dialogue datasets for language modeling .", "entities": []}, {"text": "To better model dialogue behavior during pre - training , we incorporate user and system tokens into the masked language modeling .", "entities": [[18, 21, "TaskName", "masked language modeling"]]}, {"text": "We propose a contrastive objective function to simulate the response selection task .", "entities": []}, {"text": "Our pre - trained task - oriented dialogue BERT ( TOD - BERT ) outperforms strong baselines like BERT on four downstream taskoriented dialogue applications , including intention recognition , dialogue state tracking , dialogue act prediction , and response selection .", "entities": [[8, 9, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"], [30, 33, "TaskName", "dialogue state tracking"]]}, {"text": "We also show that TOD - BERT has a stronger few - shot ability that can mitigate the data scarcity problem for task - oriented dialogue .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "1 Introduction Pre - trained models with self - attention encoder architectures ( Devlin et al . , 2018 ; Liu et al . , 2019 ) have been commonly used in many NLP applications .", "entities": []}, {"text": "Such models are self - supervised based on a massive scale of general text corpora , such as English Wikipedia or books ( Zhu et al . , 2015 ) .", "entities": []}, {"text": "By further \ufb01ne - tuning these representations , breakthroughs have been continuously reported for various downstream tasks , especially natural language understanding .", "entities": [[19, 22, "TaskName", "natural language understanding"]]}, {"text": "However , previous work ( Rashkin et al . , 2018 ; Wolf et al . , 2019 ) shows that there are some de\ufb01ciencies in the performance to apply \ufb01ne - tuning on conversational corpora directly .", "entities": []}, {"text": "One possible reason could be the intrinsic difference of linguistic patterns between human conversations and writing text , resulting in a large gap of data distributions ( Bao et al . , 2019 ) .", "entities": []}, {"text": "Therefore , pre - training dialoguelanguage models using chit - chat corpora from social media , such as Twitter or Reddit , has been recently investigated , especially for dialogue response generation ( Zhang et al . , 2019 ) and retrieval ( Henderson et al . , 2019b ) .", "entities": [[20, 21, "DatasetName", "Reddit"], [30, 32, "TaskName", "response generation"]]}, {"text": "Although these opendomain dialogues are diverse and easy - to - get , they are usually short , noisy , and without speci\ufb01c chatting goals .", "entities": []}, {"text": "On the other hand , a task - oriented dialogue has explicit goals ( e.g. restaurant reservation or ticket booking ) and many conversational interactions .", "entities": []}, {"text": "But each dataset is usually small and scattered because obtaining and labeling such data is time - consuming .", "entities": []}, {"text": "Moreover , a task - oriented dialogue has explicit user and system behaviors where a user has his / her goal , and a system has its belief and database information , which makes the language understanding component and dialogue policy learning more important than those chit - chat scenarios .", "entities": []}, {"text": "This paper aims to prove this hypothesis : selfsupervised language model pre - training using taskoriented corpora can learn better representations than existing pre - trained models for task - oriented downstream tasks .", "entities": []}, {"text": "We emphasize that what we care about the most is not whether our pre - trained model can achieve state - of - the - art results on each downstream task since most of the current best models are built on top of pre - trained models , and ours can easily replace them .", "entities": []}, {"text": "We avoid adding too many additional components on top of the pre - training architecture when \ufb01ne - tuning in our experiments .", "entities": []}, {"text": "We collect and combine nine human - human and multi - turn task - oriented dialogue corpora to train a task - oriented dialogue BERT ( TOD - BERT ) .", "entities": [[24, 25, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"]]}, {"text": "In total , there are around 100k dialogues with 1.4 M utterances across over 60 different domains .", "entities": []}, {"text": "Like BERT ( Devlin et al . , 2018 ) , TOD - BERT is formulated as a masked language model and uses a deep bidirectional Transformer ( Vaswani et al . , 2017 )", "entities": [[1, 2, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"], [26, 27, "MethodName", "Transformer"]]}, {"text": "918encoder as its model architecture .", "entities": []}, {"text": "Unlike BERT , TOD - BERT incorporates two special tokens for user and system to model the corresponding dialogue behavior .", "entities": [[1, 2, "MethodName", "BERT"], [5, 6, "MethodName", "BERT"]]}, {"text": "A contrastive objective function of response selection task is combined during pretraining stage to capture response similarity .", "entities": []}, {"text": "We select BERT because it is the most widely used model in NLP research recently , and our uni\ufb01ed datasets can be easily applied to pre - train any existing language models .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "We test TOD - BERT on task - oriented dialogue systems on four core downstream tasks , including intention recognition , dialogue state tracking , dialogue act prediction , and response selection .", "entities": [[4, 5, "MethodName", "BERT"], [6, 11, "TaskName", "task - oriented dialogue systems"], [21, 24, "TaskName", "dialogue state tracking"]]}, {"text": "What we observe is : TOD - BERT outperforms BERT and other strong baselines such as GPT-2 ( Radford et al . , 2019 ) and DialoGPT ( Zhang et", "entities": [[7, 8, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"], [16, 17, "MethodName", "GPT-2"]]}, {"text": "al . , 2019 ) on all the selected downstream tasks , which further con\ufb01rms its effectiveness for improving dialogue language understanding .", "entities": []}, {"text": "We \ufb01nd that response contrastive learning is bene\ufb01cial , but it is currently overlooked not well - investigated in dialogue pretraining research .", "entities": [[4, 6, "MethodName", "contrastive learning"]]}, {"text": "More importantly , TOD - BERT has a stronger few - shot ability than BERT on each task , suggesting that it can reduce the need for expensive human - annotated labels .", "entities": [[5, 6, "MethodName", "BERT"], [14, 15, "MethodName", "BERT"]]}, {"text": "TOD - BERT can be easily leveraged and adapted to a new taskoriented dialogue dataset .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "Our source code and data processing are released to facilitate future research on pre - training and \ufb01ne - tuning of task - oriented dialogue1 .", "entities": []}, {"text": "2 Related Work General Pre - trained Language Models , which are trained on massive general text such as Wikipedia and BookCorpus , can be roughly divided into two categories : uni - directional or bidirectional attention mechanisms .", "entities": [[3, 4, "DatasetName", "General"], [21, 22, "DatasetName", "BookCorpus"]]}, {"text": "GPT ( Radford et al . , 2018 ) and GPT-2 ( Radford et al . , 2019 ) are representatives of uni - directional language models using a Transformer decoder , where the objective is to maximize left - to - right generation likelihood .", "entities": [[0, 1, "MethodName", "GPT"], [10, 11, "MethodName", "GPT-2"], [29, 31, "MethodName", "Transformer decoder"]]}, {"text": "These models are commonly applied in natural language generation tasks .", "entities": []}, {"text": "On the other hand , BERT ( Devlin et al . , 2018 ) , RoBERTa ( Liu et al . , 2019 ) , and their variances are pre - trained using a Transformer encoder with bi - directional token prediction .", "entities": [[5, 6, "MethodName", "BERT"], [15, 16, "MethodName", "RoBERTa"], [34, 35, "MethodName", "Transformer"]]}, {"text": "These models are usually evaluated on classi\ufb01cation tasks such as GLUE benchmark ( Wang et al . , 2018 ) or span - based question answering tasks ( Ra1github.com/jasonwu0731/ToD-BERTjpurkar et al . , 2016 ) .", "entities": [[10, 11, "DatasetName", "GLUE"], [24, 26, "TaskName", "question answering"]]}, {"text": "Some language models can support both unidirectional and bi - directional attention , such as UniLM ( Dong et al . , 2019 ) .", "entities": []}, {"text": "Conditional language model pre - training is also proposed .", "entities": []}, {"text": "For example , CTRL ( Keskar et al . , 2019 ) is a conditional Transformer model , trained to condition on control codes that govern style , content , and task - speci\ufb01c behavior .", "entities": [[3, 4, "MethodName", "CTRL"], [15, 16, "MethodName", "Transformer"]]}, {"text": "Recently , multi - task language model pretraining with uni\ufb01ed sequence - to - sequence generation is proposed .", "entities": []}, {"text": "Text - to - text Transformer ( T5 ) ( Raffel et al . , 2019 ) uni\ufb01es multiple text modeling tasks and achieves the promising results in various NLP benchmarks .", "entities": [[5, 6, "MethodName", "Transformer"], [7, 8, "MethodName", "T5"]]}, {"text": "Dialogue Pre - trained Language Models are mostly trained on open - domain conversational data from Reddit or Twitter for dialogue response generation .", "entities": [[16, 17, "DatasetName", "Reddit"], [21, 23, "TaskName", "response generation"]]}, {"text": "Transfertransfo ( Wolf et al . , 2019 ) achieves good performance on ConvAI-2 dialogue competition using GPT-2 .", "entities": [[17, 18, "MethodName", "GPT-2"]]}, {"text": "DialoGPT ( Zhang et al . , 2019 ) is an extension of GPT-2 that is pre - trained on Reddit data for open - domain response generation .", "entities": [[13, 14, "MethodName", "GPT-2"], [20, 21, "DatasetName", "Reddit"], [26, 28, "TaskName", "response generation"]]}, {"text": "ConveRT", "entities": []}, {"text": "( Henderson et al . , 2019a ) pre - trained a dual transformer encoder for response selection task on large - scale Reddit ( input , response ) pairs .", "entities": [[23, 24, "DatasetName", "Reddit"]]}, {"text": "PLATO ( Bao et al . , 2019 ) uses both Twitter and Reddit data to pre - trained a dialogue generation model with discrete latent variables .", "entities": [[13, 14, "DatasetName", "Reddit"], [20, 22, "TaskName", "dialogue generation"]]}, {"text": "All of them are designed to cope with the response generation task for opendomain chatbots .", "entities": [[9, 11, "TaskName", "response generation"]]}, {"text": "Pretraining for task - oriented dialogues , on the other hand , has few related works .", "entities": []}, {"text": "Budzianowski and Vuli \u00b4 c ( 2019 ) \ufb01rst apply the GPT-2 model to train on response generation task , which takes system belief , database result , and last dialogue turn as input to predict next system responses .", "entities": [[11, 12, "MethodName", "GPT-2"], [16, 18, "TaskName", "response generation"]]}, {"text": "It only uses one dataset to train its model because few public datasets have database information available .", "entities": []}, {"text": "Henderson et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019b ) pre - trained a response selection model for task - oriented dialogues .", "entities": []}, {"text": "They \ufb01rst pre - train on Reddit corpora and then \ufb01ne - tune on target dialogue domains , but their training and \ufb01ne - tuning code is not released .", "entities": [[6, 7, "DatasetName", "Reddit"]]}, {"text": "Peng et al .", "entities": []}, {"text": "( 2020 ) focus on the natural language generation ( NLG ) task , which assumes dialogue acts and slot - tagging results are given to generate a natural language response .", "entities": []}, {"text": "Pre - training on a set of annotated NLG corpora can improve conditional generation quality using a GPT-2 model .", "entities": [[17, 18, "MethodName", "GPT-2"]]}, {"text": "919Name # Dialogue # Utterance Avg .", "entities": []}, {"text": "Turn # Domain MetaLWOZ ( Lee et al . , 2019 ) 37,884 432,036 11.4 47 Schema ( Rastogi et al . , 2019 ) 22,825 463,284 20.3 17 Taskmaster ( Byrne et al . , 2019 ) 13,215 303,066 22.9 6 MWOZ ( Budzianowski et al . , 2018 ) 10,420 71,410 6.9 7 MSR - E2E ( Li et al . , 2018 ) 10,087 74,686 7.4 3 SMD ( Eric and Manning , 2017 ) 3,031 15,928 5.3 3 Frames ( Asri et al . , 2017 ) 1,369 19,986 14.6 3 WOZ ( Mrk \u02c7si\u00b4c et", "entities": [[3, 4, "DatasetName", "MetaLWOZ"], [57, 58, "DatasetName", "E2E"], [70, 71, "DatasetName", "SMD"]]}, {"text": "al . , 2016 ) 1,200 5,012 4.2 1 CamRest676 ( Wen et al . , 2016 ) 676 2,744 4.1 1 Table 1 : Data statistics for task - oriented dialogue datasets .", "entities": []}, {"text": "3 Method This section discusses each dataset used in our taskoriented pre - training and how we process the data .", "entities": []}, {"text": "Then we introduce the selected pre - training base model and its objective functions .", "entities": []}, {"text": "3.1 Datasets We collect nine different task - oriented datasets which are English , human - human and multi - turn .", "entities": []}, {"text": "In total , there are 100,707 dialogues , which contain 1,388,152 utterances over 60 domains .", "entities": []}, {"text": "Dataset statistics is shown in Table 1 .", "entities": []}, {"text": "\u000fMetaLWOZ ( Lee et al . , 2019 ): Meta - Learning Wizard - of - Oz is a dataset designed to help develop models capable of predicting user responses in unseen domains .", "entities": [[9, 12, "TaskName", "Meta - Learning"], [12, 17, "DatasetName", "Wizard - of - Oz"]]}, {"text": "This large dataset was created by crowdsourcing 37,884 goaloriented dialogs , covering 227 tasks in 47 domains .", "entities": []}, {"text": "The MetaLWOZ dataset is used as the fast adaptation task for DSTC8 ( Kim et al . , 2019 ) dialogue competition .", "entities": [[1, 2, "DatasetName", "MetaLWOZ"]]}, {"text": "\u000fSchema ( Rastogi et al . , 2019 ): Schema - guided dialogue has 22,825 dialogues and provides a challenging testbed for several tasks , in particular , dialogue state tracking .", "entities": [[28, 31, "TaskName", "dialogue state tracking"]]}, {"text": "Each schema is a set of tracking slots , and each domain could have multiple possible schemas .", "entities": []}, {"text": "This allows a single dialogue system to support many services and facilitates the simple integration of new services without requiring much training data .", "entities": []}, {"text": "The Schema dataset is used as the dialogue state tracking task for DSTC8 ( Kim et al . , 2019 ) dialogue competition .", "entities": [[7, 10, "TaskName", "dialogue state tracking"]]}, {"text": "\u000fTaskmaster ( Byrne et al . , 2019 ):", "entities": []}, {"text": "This dataset includes 13,215 dialogues comprising six do - mains , including 5,507 spoken and 7,708 written dialogs created with two distinct procedures .", "entities": []}, {"text": "One is a two - person Wizard of Oz approach that one person acts like a robot , and the other is a self - dialogue approach in which crowdsourced workers wrote the entire dialog themselves .", "entities": []}, {"text": "It has 22.9 average conversational turns in a single dialogue , which is the longest among all taskoriented datasets listed .", "entities": []}, {"text": "\u000fMWOZ ( Budzianowski et al . , 2018 ): MultiDomain Wizard - of - Oz dataset contains 10,420 dialogues over seven domains , and it has multiple domains in a single dialogue .", "entities": [[10, 15, "DatasetName", "Wizard - of - Oz"]]}, {"text": "It has a detailed description of the data collection procedure , user goal , system act , and dialogue state labels .", "entities": []}, {"text": "Different from most of the existing corpora , it also provides full database information .", "entities": []}, {"text": "\u000fMSR - E2E ( Li et al . , 2018 ): Microsoft end - toend dialogue challenge has 10,087 dialogues in three domains , movie - ticket booking , restaurant reservation , and taxi booking .", "entities": [[2, 3, "DatasetName", "E2E"]]}, {"text": "It also includes an experiment platform with built - in simulators in each domain .", "entities": []}, {"text": "\u000fSMD ( Eric and Manning , 2017 ): Stanford multidomain dialogue is an in - car personal assistant dataset , comprising 3,301 dialogues and three domains : calendar scheduling , weather information retrieval , and point - of - interest navigation .", "entities": [[31, 33, "TaskName", "information retrieval"]]}, {"text": "It is designed to smoothly interface with knowledge bases , where a knowledge snippet is attached with each dialogue as a piece of simpli\ufb01ed database information .", "entities": []}, {"text": "\u000fFrames ( Asri et al . , 2017 ): This dataset comprises 1,369 human - human dialogues with an average of 14.6 turns per dialogue , where users", "entities": []}, {"text": "920are given some constraints to book a trip and assistants who search a database to \ufb01nd appropriate trips .", "entities": []}, {"text": "Unlike other datasets , it has labels to keep track of different semantic frames , which is the decision - making behavior of users throughout each dialogue .", "entities": []}, {"text": "\u000fWOZ ( Mrk \u02c7si\u00b4c et", "entities": []}, {"text": "al . , 2016 ) and CamRest676 ( Wen et al . , 2016 ):", "entities": []}, {"text": "These two corpora use the same data collection procedure and same ontology from DSTC2 ( Henderson et al . , 2014 ) .", "entities": [[11, 12, "MethodName", "ontology"]]}, {"text": "They are one of the \ufb01rst task - oriented dialogue datasets that use Wizard of Oz style with text input instead of speech input , which improves the model \u2019s capacity for the semantic understanding instead of its robustness to automatic speech recognition errors .", "entities": [[40, 43, "TaskName", "automatic speech recognition"]]}, {"text": "3.2 TOD - BERT We train our TOD - BERT based on BERT architecture using two loss functions : masked language modeling ( MLM ) loss and response contrastive loss ( RCL ) .", "entities": [[3, 4, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"], [16, 17, "MetricName", "loss"], [19, 22, "TaskName", "masked language modeling"], [23, 24, "DatasetName", "MLM"], [25, 26, "MetricName", "loss"], [29, 30, "MetricName", "loss"]]}, {"text": "Note that the datasets we used can be used to pre - train any existing language model architecture , and here we select BERT because it is the most widely used model in NLP research .", "entities": [[23, 24, "MethodName", "BERT"]]}, {"text": "We use the BERT - base uncased model , which is a transformer self - attention encoder ( Vaswani et al . , 2017 ) with 12 layers and 12 attention heads with its hidden size dB= 768 .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "To capture speaker information and the underlying interaction behavior in dialogue , we add two special tokens , [ USR ] and [ SYS ] , to the bytepair embeddings ( Mrk \u02c7si\u00b4c et", "entities": []}, {"text": "al . , 2016 ) .", "entities": []}, {"text": "We pre\ufb01x the special token to each user utterance and system response , and concatenate all the utterances in the same dialogue into one \ufb02at sequence , as shown in Figure 1 .", "entities": []}, {"text": "For example , for a dialogue D = fS1 ; U1 ; : : : ; S n ; Ung , where nis the number of dialogue turns and each SiorUicontains a sequence of words , the input of the pre - training model is processed as \u201c [ SYS ] S1[USR ] U1 : : : \u201d with standard positional embeddings and segmentation embeddings .", "entities": []}, {"text": "Masked language modeling is a common pretraining strategy for BERT - like architectures , in which a random sample of tokens in the input sequence is selected and replaced with the special token [ MASK ] .", "entities": [[0, 3, "TaskName", "Masked language modeling"], [9, 10, "MethodName", "BERT"]]}, {"text": "The MLM loss function is the crossentropy loss on predicting the masked tokens .", "entities": [[1, 2, "DatasetName", "MLM"], [2, 3, "MetricName", "loss"], [7, 8, "MetricName", "loss"]]}, {"text": "In the original implementation , random masking andreplacement are performed once in the beginning and saved for the training duration .", "entities": []}, {"text": "Here we conduct token masking dynamically during batch training .", "entities": []}, {"text": "TOD - BERT is initialized from BERT , a good starting parameter set , then is further pre - trained on those task - oriented corpora .", "entities": [[2, 3, "MethodName", "BERT"], [6, 7, "MethodName", "BERT"]]}, {"text": "The MLM loss function is Lmlm=\u0000PM m=1logP(xm ) ; ( 1 ) where Mis the total number of masked tokens and P(xm)is the predicted probability of the token xm over the vocabulary size .", "entities": [[1, 2, "DatasetName", "MLM"], [2, 3, "MetricName", "loss"]]}, {"text": "Response contrastive loss can also be used for dialogue language modeling since it does not require any additional human annotation .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "Pretraining with RCL can bring us several advantages : 1 ) we can learn a better representation for the [ CLS ] token , as it is essential for all the downstream tasks , and 2 ) we encourage the model to capture underlying dialogue sequential order , structure information , and response similarity .", "entities": []}, {"text": "Unlike the original next sentence prediction ( NSP ) objective in BERT pre - training , which concatenates two segments AandBto predict whether they are consecutive text with binary classi\ufb01cation , we apply a dual - encoder approach ( Henderson et al . , 2019a ) and simulate multiple negative samples .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "We \ufb01rst draw a batch of dialogues fD1 ; : : : ; D bgand split each dialogue at a randomly selected turn t.", "entities": []}, {"text": "For example , D1will be separated into two segments , one is the context fS1 1 ; U1 1 ; : : : ; S1 t ; U1 tgand the other is the response fS1 t+1 g.", "entities": []}, {"text": "We use TOD - BERT to encode all the contexts and their corresponding responses separately .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "Afterwards , we have a context matrix C2 Rb\u0002dBand a response matrix R2Rb\u0002dBby taking the output", "entities": []}, {"text": "[ CLS ] representations from the b dialogues .", "entities": []}, {"text": "We treat other responses in the same batch as randomly selected negative samples .", "entities": []}, {"text": "The RCL objective function is Lrcl=\u0000bP i=1logMi;i ; M = Softmax ( CRT)2Rb\u0002b:(2 ) Increasing batch size to a certain amount can obtain better performance on downstream tasks , especially for the response selection .", "entities": [[10, 11, "MethodName", "Softmax"], [15, 17, "HyperparameterName", "batch size"]]}, {"text": "The Softmax function normalizes the vector per row .", "entities": [[1, 2, "MethodName", "Softmax"]]}, {"text": "In our setting , increasing batch size also means changing the positive and negative ratio in the contrastive learning .", "entities": [[5, 7, "HyperparameterName", "batch size"], [17, 19, "MethodName", "contrastive learning"]]}, {"text": "Batch size is a hyper - parameter that may be", "entities": [[0, 2, "HyperparameterName", "Batch size"]]}, {"text": "921 Figure 1 : Dialogue pre - training based on Transformer encoder with user and system special tokens .", "entities": [[10, 11, "MethodName", "Transformer"]]}, {"text": "Two objective functions are used : masked language modeling and response contrastive learning .", "entities": [[6, 9, "TaskName", "masked language modeling"], [11, 13, "MethodName", "contrastive learning"]]}, {"text": "limited by hardware .", "entities": []}, {"text": "We also try different negative sampling strategies during pre - training such as local sampling ( Saeidi et al . , 2017 ) , but do not observe signi\ufb01cant change compared to random sampling .", "entities": []}, {"text": "Overall pre - training loss function is the weighted - sum of Lmlm andLrcl , and in our experiments , we simply sum them up .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "We gradually reduce the learning rate without a warm - up period .", "entities": [[4, 6, "HyperparameterName", "learning rate"]]}, {"text": "We train TOD - BERT with AdamW ( Loshchilov and Hutter , 2017 ) optimizer with a dropout ratio of 0.1 on all layers and attention weights .", "entities": [[4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "AdamW"], [14, 15, "HyperparameterName", "optimizer"]]}, {"text": "GELU activation functions ( Hendrycks and Gimpel , 2016 ) is used .", "entities": [[0, 1, "MethodName", "GELU"]]}, {"text": "Models are early - stopped using perplexity scores of a held - out development set , with mini - batches containing 32 sequences of maximum length 512 tokens .", "entities": [[6, 7, "MetricName", "perplexity"]]}, {"text": "Experiments are conducted on two NVIDIA Tesla V100 GPUs .", "entities": []}, {"text": "4 Downstream Tasks We care the most in this paper whether TOD - BERT , a pre - trained language model using aggregated taskoriented corpora , can show any advantage over BERT .", "entities": [[13, 14, "MethodName", "BERT"], [31, 32, "MethodName", "BERT"]]}, {"text": "Therefore , we avoid adding too many additional components on top of its architecture when \ufb01ne - tuning on each downstream task .", "entities": []}, {"text": "Also , we use the same architecture with a similar number of parameters for a fair comparison .", "entities": [[10, 13, "HyperparameterName", "number of parameters"]]}, {"text": "All the model parameters are updated with a gradient clipping to 1.0 using the same hyper - parameters during \ufb01netuning .", "entities": [[8, 10, "MethodName", "gradient clipping"]]}, {"text": "We select four crucial task - oriented downstream tasks to evaluate : intent recognition , dialogue state tracking , dialogue act prediction , and response selection .", "entities": [[12, 14, "TaskName", "intent recognition"], [15, 18, "TaskName", "dialogue state tracking"]]}, {"text": "All of them are core components in modularized task - oriented systems ( Wen et al . , 2016 ) .", "entities": []}, {"text": "Intent recognition task is a multi - class classi\ufb01cation problem , where we input a sentence Uandmodels predict one single intent class over Ipossible intents .", "entities": [[0, 2, "TaskName", "Intent recognition"]]}, {"text": "Pint = Softmax ( W1(F(U)))2RI ; ( 3 ) where Fis a pre - trained language model", "entities": [[2, 3, "MethodName", "Softmax"]]}, {"text": "and we use its [ CLS ] embeddings as the output representation .", "entities": []}, {"text": "W12RI\u0002dBis a trainable linear mapping .", "entities": []}, {"text": "The model is trained with cross - entropy loss between the predicted distributions Pintand the true intent labels .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "Dialogue state tracking can be treated as a multi - class classi\ufb01cation problem using a prede\ufb01ned ontology .", "entities": [[0, 3, "TaskName", "Dialogue state tracking"], [16, 17, "MethodName", "ontology"]]}, {"text": "Unlike intent , we use dialogue history X(a sequence of utterances ) as input and a model predicts slot values for each ( domain , slot ) pair at each dialogue turn .", "entities": []}, {"text": "Each corresponding value vj i , thei - th value for the j - th ( domain , slot ) pair , is passed into a pre - trained model and \ufb01xed its representation during training .", "entities": []}, {"text": "Sj i = Sim(Gj(F(X ) ) ; F(vj i))2R1 ; ( 4 ) where Sim is the cosine similarity function , and Sj2Rjvjjis the probability distribution of the j - th ( domain , slot ) pair over its possible values .", "entities": []}, {"text": "Gjis the slot projection layer of the jslot , and the number of layers jGjis equal to the number of ( domain , slot ) pairs .", "entities": [[11, 14, "HyperparameterName", "number of layers"]]}, {"text": "The model is trained with cross - entropy loss summed over all the pairs .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "Dialogue act prediction is a multi - label classi\ufb01cation problem because a system response may contain multiple dialogue acts , e.g. , request and inform at the same time .", "entities": []}, {"text": "Model take dialogue history as input and predict a binary result for each possible dialogue act : A = Sigmoid ( W2(F(X)))2RN ; ( 5 ) where W22RdB\u0002Nis a trainable linear mapping , Nis the number of possible dialogue acts , and each value in Ais between [ 0;1]after a Sigmoid layer .", "entities": []}, {"text": "The model is trained with binary cross - entropy loss and the i - th dialogue act is considered as a triggered dialogue act if Ai>0:5 .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "Response selection is a ranking problem , aiming to retrieve the most relative system response from a candidate pool .", "entities": []}, {"text": "We use a dual - encoder strategy ( Henderson et al . , 2019b ) and compute similarity scores between source Xand target Y , ri = Sim(F(X ) ; F(Yi))2R1 ; ( 6 )", "entities": []}, {"text": "922where Yiis the i - th response candidate and riis its cosine similarity score .", "entities": []}, {"text": "Source Xcan be truncated , and we limit the context lengths to the most recent 256 tokens in our experiments .", "entities": []}, {"text": "We randomly sample several system responses from the corpus as negative samples .", "entities": []}, {"text": "Although it may not be a true negative sample , it is common to train a ranker and evaluate its results ( Henderson et al . , 2019a ) .", "entities": []}, {"text": "5 Evaluation Datasets We pick up several datasets , OOS , DSTC2 , GSIM , and MWOZ , for downstream evaluation .", "entities": []}, {"text": "The \ufb01rst three corpora are not included in the pre - trained task - oriented datasets .", "entities": []}, {"text": "For MWOZ , to be fair , we do not include its test set dialogues during the pretraining stage .", "entities": []}, {"text": "Details of each evaluation dataset are discussed in the following : \u000fOOS ( Larson et al . , 2019 ): The out - of - scope intent dataset is one of the largest annotated intent datasets , including 15,100/3,100/5,500 samples for the train , validation , and test sets , respectively .", "entities": []}, {"text": "It covers 151 intent classes over ten domains , including 150 in - scope intent and one outof - scope intent .", "entities": []}, {"text": "The out - of - scope intent means that a user utterance that does not fall into any of the prede\ufb01ned intents .", "entities": []}, {"text": "Each of the intents has 100 training samples .", "entities": []}, {"text": "\u000fDSTC2 ( Henderson et al . , 2014 ): DSTC2 is a human - machine task - oriented dataset that may include a certain system response noise .", "entities": []}, {"text": "It has 1,612/506/1117 dialogues for train , validation , and test sets , respectively .", "entities": []}, {"text": "We follow Paul et al .", "entities": []}, {"text": "( 2019 ) to map the original dialogue act labels to universal dialogue acts , which results in 9 different system dialogue acts .", "entities": []}, {"text": "\u000fGSIM ( Shah et al . , 2018a ): GSIM is a humanrewrote machine - machine task - oriented corpus , including 1500/469/1039 dialogues for the train , validation , and test sets , respectively .", "entities": []}, {"text": "We combine its two domains , movie and restaurant domains , into one single corpus .", "entities": []}, {"text": "It is collected by Machines Talking To Machines ( M2 M )", "entities": []}, {"text": "( Shah et al . , 2018b ) approach , a functionality - driven process combining a dialogue self - play step and a crowdsourcing step .", "entities": []}, {"text": "We map its dialogue act labels to universal dialogue acts ( Paul et al . , 2019 ) , resulting in 6 different system dialogue acts .", "entities": []}, {"text": "\u000fMWOZ ( Budzianowski et al . , 2018 ): MWOZ is the most common benchmark for task - orienteddialogues , especially for dialogue state tracking .", "entities": [[22, 25, "TaskName", "dialogue state tracking"]]}, {"text": "It has 8420/1000/1000 dialogues for train , validation , and test sets , respectively .", "entities": []}, {"text": "Across seven different domains , in total , it has 30 ( domain , slot ) pairs that need to be tracked in the test set .", "entities": []}, {"text": "We use its revised version MWOZ 2.1 , which has the same dialogue transcripts but with cleaner state label annotation .", "entities": []}, {"text": "6 Results For each downstream task , we \ufb01rst conduct the experiments using the whole dataset , and then we simulate the few - shot setting to show the strength of our TOD - BERT .", "entities": [[34, 35, "MethodName", "BERT"]]}, {"text": "We run at least three times with different random seeds for each few - shot experiment to reduce data sampling variance , and we report its mean and standard deviation for these limited data scenarios .", "entities": [[9, 10, "DatasetName", "seeds"]]}, {"text": "We investigate two versions of TOD - BERT ; one is TOD - BERT - mlm that only uses MLM loss during pre - training , and the other is TOD - BERT - jnt , which is jointly trained with the MLM and RCL objectives .", "entities": [[7, 8, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"], [15, 16, "DatasetName", "mlm"], [19, 20, "DatasetName", "MLM"], [20, 21, "MetricName", "loss"], [32, 33, "MethodName", "BERT"], [42, 43, "DatasetName", "MLM"]]}, {"text": "We compare TOD - BERT with BERT and other baselines , including two other strong pre - training models GPT2 ( Radford et al . , 2019 ) and DialoGPT ( Zhang et al . , 2019 ) .", "entities": [[4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "BERT"]]}, {"text": "For a GPT - based model , we use mean pooling of its hidden states as its output representation , which we found it is better than using only the last token .", "entities": [[2, 3, "MethodName", "GPT"]]}, {"text": "6.1 Linear Probe Before \ufb01ne - tuning each pre - trained models , we \ufb01rst investigate their feature extraction ability by probing their output representations .", "entities": []}, {"text": "Probing methods are proposed to determine what information is carried intrinsically by the learned embeddings ( Tenney et al . , 2019 ) .", "entities": []}, {"text": "We probe the output representation using one single - layer perceptron on top of a \u201c \ufb01xed \u201d pre - trained language model and only \ufb01netune that layer for a downstream task with the same hyper - parameters .", "entities": []}, {"text": "Table 3 shows the probing results of domain classi\ufb01cation on MWOZ , intent identi\ufb01cation on OOS , and dialogue act prediction on MWOZ .", "entities": []}, {"text": "TOD - BERT - jnt achieves the highest performance in this setting , suggesting its representation contains the most useful information .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "6.2 Intent Recognition TOD - BERT outperforms BERT and other strong baselines in one of the largest intent recognition", "entities": [[1, 3, "TaskName", "Intent Recognition"], [5, 6, "MethodName", "BERT"], [7, 8, "MethodName", "BERT"], [17, 19, "TaskName", "intent recognition"]]}, {"text": "923ModelAcc ( all)Acc ( in)Acc ( out)Recall ( out ) 1 - ShotBERT 29.3%\u00063.4 % 35.7%\u00064.1 % 81.3%\u00060.4 % 0.4%\u00060.3 % TOD - BERT - mlm", "entities": [[23, 24, "MethodName", "BERT"], [25, 26, "DatasetName", "mlm"]]}, {"text": "38.9%\u00066.3 % 47.4%\u00067.6 % 81.6%\u00060.2 % 0.5%\u00060.2 % TOD - BERT - jnt 42.5%\u00060.1 % 52.0%\u00060.1 % 81.7%\u00060.1 % 0.1%\u00060.1 % 10 - ShotBERT 75.5%\u00061.1 % 88.6%\u00061.1 % 84.7%\u00060.3 % 16.5%\u00061.7 % TOD - BERT - mlm 76.6%\u00060.8 % 90.5%\u00061.2 % 84.3%\u00060.2 % 14.0%\u00061.3 % TOD - BERT - jnt", "entities": [[10, 11, "MethodName", "BERT"], [34, 35, "MethodName", "BERT"], [36, 37, "DatasetName", "mlm"], [47, 48, "MethodName", "BERT"]]}, {"text": "77.3%\u00060.5 % 91.0%\u00060.5 % 84.5%\u00060.4 % 15.3%\u00062.1 % Full ( 100 - Shot)FastText * - 89.0 % - 9.7 % SVM * - 91.0 % - 14.5 % CNN * - 91.2 % - 18.9 % GPT2 83.0 % 94.1 % 87.7 % 32.0 % DialoGPT 83.9 % 95.5 % 87.6 % 32.1 % BERT 84.9 % 95.8 % 88.1 % 35.6 % TOD - BERT - mlm 85.9 % 96.1 % 89.5 % 46.3 % TOD - BERT - jnt 86.6 % 96.2 % 89.9 % 43.6 % Table 2 : Intent recognition results on the OOS dataset , one of the largest intent corpus .", "entities": [[20, 21, "MethodName", "SVM"], [54, 55, "MethodName", "BERT"], [65, 66, "MethodName", "BERT"], [67, 68, "DatasetName", "mlm"], [78, 79, "MethodName", "BERT"], [92, 94, "TaskName", "Intent recognition"]]}, {"text": "Models with * are reported from Larson et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Domain ( acc)Intent ( acc)Dialogue Act ( F1 - micro ) GPT2 63.5 % 74.7 % 85.7 % DialoGPT 63.0 % 65.7 % 84.2 % BERT 60.5 % 71.1 % 85.3 % TOD - BERT - mlm 63.9 % 70.7 % 83.5 % TOD - BERT - jnt 68.7 % 77.8 % 86.2 % Table 3 : Probing results of different pre - trained language models using a single - layer perceptron .", "entities": [[7, 8, "MetricName", "F1"], [25, 26, "MethodName", "BERT"], [34, 35, "MethodName", "BERT"], [36, 37, "DatasetName", "mlm"], [45, 46, "MethodName", "BERT"]]}, {"text": "datasets , as shown in Table 2 .", "entities": []}, {"text": "We evaluate accuracy on all the data , the in - domain intents only , and the out - of - scope intent only .", "entities": [[2, 3, "MetricName", "accuracy"]]}, {"text": "Note that there are two ways to predict out - of - scope intent , one is to treat it as an additional class , and the other is to set a threshold for prediction con\ufb01dence .", "entities": []}, {"text": "Here we report the results of the \ufb01rst setting .", "entities": []}, {"text": "TOD - BERTjnt achieves the highest in - scope and out - of - scope accuracy .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "Besides , we conduct 1 - shot and 10 - shot experiments by randomly sampling one and ten utterances from each intent class in the training set .", "entities": []}, {"text": "TOD - BERT - jnt has 13.2 % all - intent accuracy improvement and 16.3 % in - domain accuracy improvement compared to BERT in the 1 - shot setting .", "entities": [[2, 3, "MethodName", "BERT"], [11, 12, "MetricName", "accuracy"], [19, 20, "MetricName", "accuracy"], [23, 24, "MethodName", "BERT"]]}, {"text": "6.3 Dialogue State Tracking Two evaluation metrics are commonly used in dialogue state tracking task : joint goal accuracy and slot accuracy .", "entities": [[1, 4, "TaskName", "Dialogue State Tracking"], [11, 14, "TaskName", "dialogue state tracking"], [18, 19, "MetricName", "accuracy"], [21, 22, "MetricName", "accuracy"]]}, {"text": "The joint goal accuracy compares the predicted dialogue states to the ground truth at each dialogue turn .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "The ground truth includes slot values for all the possible ( domain , slot ) pairs .", "entities": []}, {"text": "The output is considered as a correct prediction if and only if all the predicted values exactly match its ground truth values .", "entities": []}, {"text": "On the other hand , the slotaccuracy individually compares each ( domain , slot , value ) triplet to its ground truth label .", "entities": []}, {"text": "In Table 5 , we compare BERT to TOD - BERTjnt on the MWOZ 2.1 dataset and \ufb01nd the latter has 2.4 % joint goal accuracy improvement .", "entities": [[6, 7, "MethodName", "BERT"], [25, 26, "MetricName", "accuracy"]]}, {"text": "Since the original ontology provided by Budzianowski et al .", "entities": [[3, 4, "MethodName", "ontology"]]}, {"text": "( 2018 ) is not complete ( some labeled values are not included in the ontology ) , we create a new ontology of all the possible annotated values .", "entities": [[15, 16, "MethodName", "ontology"], [22, 23, "MethodName", "ontology"]]}, {"text": "We also list several well - known dialogue state trackers as reference , including DSTReader ( Gao et al . , 2019 ) , HyST ( Goel et al . , 2019 ) , TRADE ( Wu et al . , 2019 ) , and ZSDST ( Rastogi et al . , 2019 ) .", "entities": []}, {"text": "We also report the few - shot experiments using 1 % , 5 % , 10 % , and 25 % data .", "entities": []}, {"text": "Note that 1 % of data has around 84 dialogues .", "entities": []}, {"text": "TOD - BERT outperforms BERT in all the setting , which further show the strength of task - oriented dialogue pre - training .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "BERT"]]}, {"text": "6.4 Dialogue Act Prediction We conduct experiments on three different datasets and report micro - F1 and macro - F1 scores for the dialogue act prediction task , a multi - label classi\ufb01cation problem .", "entities": [[13, 16, "MetricName", "micro - F1"], [17, 20, "MetricName", "macro - F1"]]}, {"text": "For the MWOZ dataset , we remove the domain information from the original system dialogue act labels .", "entities": []}, {"text": "For example , the \u201c taxi - inform \u201d will be simpli\ufb01ed to \u201c inform \u201d .", "entities": []}, {"text": "This process reduces the number of possible dialogue acts from 31 to 13 .", "entities": []}, {"text": "For DSTC2 and GSIM corpora , we follow Paul et al .", "entities": []}, {"text": "( 2019 ) to apply universal dialogue act mapping that maps the original dialogue act labels to a general dialogue act format , resulting in 9 and 6 unique system dialogue acts in DSTC2 and GSIM , respectively .", "entities": []}, {"text": "We run two other baselines , MLP and RNN , to further show the strengths of BERT - based", "entities": [[6, 7, "DatasetName", "MLP"], [16, 17, "MethodName", "BERT"]]}, {"text": "924MWOZ ( 13 ) DSTC2 ( 9 ) GSIM ( 6 ) micro - F1 macro - F1 micro - F1 macro - F1 micro - F1 macro - F1 1 % Data BERT 84.0%\u00060.6 % 66.7%\u00061.7 % 77.1%\u00062.1 % 25.8%\u00060.8 % 67.3%\u00061.4 % 26.9%\u00061.0 % TOD - BERT - mlm 87.5%\u00060.6 % 73.3%\u00061.5 % 79.6%\u00061.0 % 26.4%\u00060.5 % 82.7%\u00060.7 % 35.7%\u00060.3 % TOD - BERT - jnt 86.9%\u00060.2 % 72.4%\u00060.8 % 82.9%\u00060.4 % 28.0%\u00060.1 % 78.4%\u00063.2 % 32.9%\u00062.1 % 10 % DataBERT 89.7%\u00060.2 % 78.4%\u00060.3 % 88.2%\u00060.7 % 34.8%\u00061.3 % 98.4%\u00060.3 % 45.1%\u00060.2 % TOD - BERT - mlm 90.1%\u00060.2 % 78.9%\u00060.1 % 91.8%\u00061.7 % 39.4%\u00061.7 % 99.2%\u00060.1 % 45.6%\u00060.1 % TOD - BERT - jnt", "entities": [[12, 15, "MetricName", "micro - F1"], [15, 18, "MetricName", "macro - F1"], [18, 21, "MetricName", "micro - F1"], [21, 24, "MetricName", "macro - F1"], [24, 27, "MetricName", "micro - F1"], [27, 30, "MetricName", "macro - F1"], [33, 34, "MethodName", "BERT"], [48, 49, "MethodName", "BERT"], [50, 51, "DatasetName", "mlm"], [65, 66, "MethodName", "BERT"], [97, 98, "MethodName", "BERT"], [99, 100, "DatasetName", "mlm"], [114, 115, "MethodName", "BERT"]]}, {"text": "90.2%\u00060.2 % 79.6%\u00060.7 % 90.6%\u00063.2 % 38.8%\u00062.2 % 99.3%\u00060.1 % 45.7%\u00060.0 % Full DataMLP 61.6 % 45.5 % 77.6 % 18.1 % 89.5 % 26.1 % RNN 90.4 % 77.3 % 90.8 % 29.4 % 98.4 % 45.2 % GPT2 90.8 % 79.8 % 92.5 % 39.4 % 99.1 % 45.6 %", "entities": []}, {"text": "DialoGPT 91.2 % 79.7 % 93.8 % 42.1 % 99.2 % 45.6 % BERT 91.4 % 79.7 % 92.3 % 40.1 % 98.7 % 45.2 % TOD - BERT - mlm 91.7 % 79.9 % 90.9 % 39.9 % 99.4 % 45.8 % TOD - BERT - jnt 91.7 % 80.6 % 93.8 % 41.3 % 99.5 % 45.8 % Table 4 : Dialogue act prediction results on three different datasets .", "entities": [[13, 14, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"], [30, 31, "DatasetName", "mlm"], [45, 46, "MethodName", "BERT"]]}, {"text": "The numbers reported are the micro and macro F1 scores , and each dataset has different numbers of dialogue acts .", "entities": [[7, 9, "MetricName", "macro F1"]]}, {"text": "ModelJoint AccSlot Acc 1 % DataBERT 6.4%\u00061.4 % 84.4%\u00061.0 % TOD - BERT - mlm 9.9%\u00060.6 % 86.6%\u00060.5 % TOD - BERT - jnt 8.0%\u00061.0 % 85.3%\u00060.4 % 5 % DataBERT 19.6%\u00060.1 % 92.0%\u00060.5 % TOD - BERT - mlm 28.1%\u00061.6 % 93.9%\u00060.1 % TOD - BERT - jnt 28.6%\u00061.4 % 93.8%\u00060.3 % 10 % DataBERT 32.9%\u00060.6 % 94.7%\u00060.1 % TOD - BERT - mlm 39.5%\u00060.7 % 95.6%\u00060.1 % TOD - BERT - jnt 37.0%\u00060.1 % 95.2%\u00060.1 % 25 % DataBERT 40.8%\u00061.0 % 95.8%\u00060.1 % TOD - BERT - mlm 44.0%\u00060.4 % 96.4%\u00060.1 % TOD - BERT - jnt 44.3%\u00060.3 % 96.3%\u00060.2 % Full DataDSTReader * 36.4 % HyST * 38.1 % ZSDST * 43.4 % TRADE *", "entities": [[2, 3, "MetricName", "Acc"], [12, 13, "MethodName", "BERT"], [14, 15, "DatasetName", "mlm"], [21, 22, "MethodName", "BERT"], [37, 38, "MethodName", "BERT"], [39, 40, "DatasetName", "mlm"], [46, 47, "MethodName", "BERT"], [62, 63, "MethodName", "BERT"], [64, 65, "DatasetName", "mlm"], [71, 72, "MethodName", "BERT"], [87, 88, "MethodName", "BERT"], [89, 90, "DatasetName", "mlm"], [96, 97, "MethodName", "BERT"]]}, {"text": "45.6 % GPT2 46.2 % 96.6 % DialoGPT 45.2 % 96.5 % BERT 45.6 % 96.6 % TOD - BERT - mlm 47.7 % 96.8 % TOD - BERT - jnt 48.0 % 96.9 % Table 5 : Dialogue state tracking results on MWOZ 2.1 .", "entities": [[12, 13, "MethodName", "BERT"], [19, 20, "MethodName", "BERT"], [21, 22, "DatasetName", "mlm"], [28, 29, "MethodName", "BERT"], [38, 41, "TaskName", "Dialogue state tracking"]]}, {"text": "We report joint goal accuracy and slot accuracy for the full data setting and the simulated few - shot settings .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 8, "MetricName", "accuracy"]]}, {"text": "models .", "entities": []}, {"text": "The MLP model simply takes bag - of - word embeddings to make dialogue act prediction , and the RNN model is a bi - directional GRU network .", "entities": [[1, 2, "DatasetName", "MLP"], [9, 11, "TaskName", "word embeddings"], [26, 27, "MethodName", "GRU"]]}, {"text": "In Table 4 , one can observe that in full data scenario , TOD - BERT consistently works better than BERT and other baselines , no matter which datasets or which evaluation metrics .", "entities": [[15, 16, "MethodName", "BERT"], [20, 21, "MethodName", "BERT"]]}, {"text": "In the fewshot experiments , TOD - BERT - mlm outperforms BERT by 3.5 % micro - F1 and 6.6 % macro - F1 on MWOZ corpus in the 1 % data scenario .", "entities": [[7, 8, "MethodName", "BERT"], [9, 10, "DatasetName", "mlm"], [11, 12, "MethodName", "BERT"], [15, 18, "MetricName", "micro - F1"], [21, 24, "MetricName", "macro - F1"]]}, {"text": "We also found that 10 % of training data can achieve good performance that is close to full data training .", "entities": []}, {"text": "( a ) BERT   ( b ) BERT ( c ) TOD - BERT - mlm   ( d ) TOD - BERT - mlm ( e ) TOD - BERT - jnt   ( f ) TOD - BERT - jnt Figure 2 : The tSNE visualization of BERT , TODBERT - mlm and TOD - BERT - jnt representations of system responses in the MWOZ test set .", "entities": [[3, 4, "MethodName", "BERT"], [8, 9, "MethodName", "BERT"], [14, 15, "MethodName", "BERT"], [16, 17, "DatasetName", "mlm"], [23, 24, "MethodName", "BERT"], [25, 26, "DatasetName", "mlm"], [31, 32, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"], [50, 51, "MethodName", "BERT"], [54, 55, "DatasetName", "mlm"], [58, 59, "MethodName", "BERT"]]}, {"text": "Different colors in the left - hand column mean different domains , and in the right - hand column represent different dialogue acts .", "entities": []}, {"text": "6.5 Response Selection To evaluate response selection in task - oriented dialogues , we follow the k - to-100 accuracy , which is becoming a research community standard ( Yang et al . , 2018 ; Henderson et al . , 2019a ) .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "The k - of-100", "entities": []}, {"text": "925MWOZ DSTC2 GSIM 1 - to-100 3 - to-100 1 - to-100 3 - to-100 1 - to-100 3 - to-100 1 % DataBERT 7.8%\u00062.0 % 20.5%\u00064.4 % 3.7%\u00060.6 % 9.6%\u00061.3 % 4.0%\u00060.4 % 10.3%\u00061.1 % TOD - BERT - mlm", "entities": [[38, 39, "MethodName", "BERT"], [40, 41, "DatasetName", "mlm"]]}, {"text": "13.0%\u00061.1 % 34.6%\u00060.4 % 12.5%\u00066.7 % 24.9%\u000610.7 % 7.2%\u00064.0 % 15.4%\u00068.0 % TOD - BERT - jnt - - 37.5%\u00060.6 % 55.9%\u00060.4 % 12.5%\u00060.9 % 26.8%\u00060.8 % 10 % DataBERT 20.9%\u00062.6 % 45.4%\u00063.8 % 8.9%\u00062.3 % 21.4%\u00063.1 % 9.8%\u00060.1 % 24.4%\u00061.2 % TOD - BERT - mlm 22.3%\u00063.2 % 48.7%\u00064.0 % 19.0%\u000616.3 % 33.8%\u000620.4 % 11.2%\u00062.5 % 26.0%\u00062.7 % TOD - BERT - jnt - - 49.7%\u00060.3 % 66.6%\u00060.1 % 23.0%\u00061.0 % 42.6%\u00061.0 % Full DataGPT2 47.5 % 75.4 % 53.7 % 69.2 % 39.1 % 60.5 % DialoGPT 35.7 % 64.1 % 39.8 % 57.1 % 16.5 % 39.5 % BERT 47.5 % 75.5 % 46.6 % 62.1 % 13.4 % 32.9 % TOD - BERT - mlm 48.1 % 74.3 % 50.0 % 65.1 % 36.5 % 60.1 % TOD - BERT - jnt", "entities": [[14, 15, "MethodName", "BERT"], [44, 45, "MethodName", "BERT"], [46, 47, "DatasetName", "mlm"], [61, 62, "MethodName", "BERT"], [101, 102, "MethodName", "BERT"], [116, 117, "MethodName", "BERT"], [118, 119, "DatasetName", "mlm"], [133, 134, "MethodName", "BERT"]]}, {"text": "65.8 % 87.0 % 56.8 % 70.6 % 41.0 % 65.4 % Table 6 : Response selection evaluation results on three corpora for 1 % , 10 % and full data setting .", "entities": []}, {"text": "We report 1 - to-100 and 3 - to-100 accuracy , which is similar to recall1 and recall@3 given 100 candidates .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "metric is computed using a random batch of 100 examples so that responses from other examples in the same batch can be used as random negative candidates .", "entities": []}, {"text": "This allows us to be compute the metric across many examples in batches ef\ufb01ciently .", "entities": []}, {"text": "While it is not guaranteed that the random negatives will indeed be \u201c true \u201d negatives , the 1 - of-100 metric still provides a useful evaluation signal .", "entities": []}, {"text": "During inference , we run \ufb01ve different random seeds to sample batches and report the average results .", "entities": [[8, 9, "DatasetName", "seeds"]]}, {"text": "In Table 6 , we conduct response selection experiments on three datasets , MWOZ , DSTC2 , and GSIM .", "entities": []}, {"text": "TOD - BERT - jnt achieves 65.8 % 1 - to-100 accuracy and 87.0 % 3 - to-100 accuracy on MWOZ , which surpasses BERT by 18.3 % and 11.5 % , respectively .", "entities": [[2, 3, "MethodName", "BERT"], [11, 12, "MetricName", "accuracy"], [18, 19, "MetricName", "accuracy"], [24, 25, "MethodName", "BERT"]]}, {"text": "The similar results are also consistently observed in DSTC2 and GSIM datasets , and the advantage of the TOD - BERT - jnt is more evident in the few - shot scenario .", "entities": [[20, 21, "MethodName", "BERT"]]}, {"text": "We do not report TODBERT - jnt for MWOZ few - shot setting because it is not fair to compare them with others as the full MWOZ training set is used for response contrastive learning during pre - training stage .", "entities": [[33, 35, "MethodName", "contrastive learning"]]}, {"text": "The response selection results are sensitive to the training batch size since the larger the batch size the harder the prediction .", "entities": [[9, 11, "HyperparameterName", "batch size"], [15, 17, "HyperparameterName", "batch size"]]}, {"text": "In our experiments , we set batch size equals to 25 for all the models .", "entities": [[6, 8, "HyperparameterName", "batch size"]]}, {"text": "7 Visualization In Figure 2 , we visualize the embeddings of BERT , TOD - BERT - mlm , and TOD - BERT - jnt given the same input from the MWOZ test set .", "entities": [[11, 12, "MethodName", "BERT"], [15, 16, "MethodName", "BERT"], [17, 18, "DatasetName", "mlm"], [22, 23, "MethodName", "BERT"]]}, {"text": "Each sample point is a system response representation , which is passed through a pre - trained model and reduced its high - dimension features to a two - dimension point using the t - distributed stochastic neighbor embedding ( tSNE ) for dimension reduction .", "entities": []}, {"text": "Since we know the true domain and dialogue act labels foreach utterance , we use different colors to represent different domains and dialogue acts .", "entities": []}, {"text": "As one can observe , TOD - BERT - jnt has more clear group boundaries than TOD - BERT - mlm , and two of them are better than BERT .", "entities": [[7, 8, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"], [20, 21, "DatasetName", "mlm"], [29, 30, "MethodName", "BERT"]]}, {"text": "To analyze the results quantitatively , we run Kmeans , a common unsupervised clustering algorithms , on top of the output embeddings of BERT and TOD - BERT .", "entities": [[23, 24, "MethodName", "BERT"], [27, 28, "MethodName", "BERT"]]}, {"text": "We set K for K - means equal to 10 and 20 .", "entities": []}, {"text": "After the clustering , we can assign each utterance in the MWOZ test set to a predicted class .", "entities": []}, {"text": "We then compute the normalized mutual information ( NMI ) between the clustering result and the actual domain label for each utterance .", "entities": [[8, 9, "MetricName", "NMI"]]}, {"text": "Here is what we observe : TOD - BERT consistently achieves higher NMI scores than BERT .", "entities": [[8, 9, "MethodName", "BERT"], [12, 13, "MetricName", "NMI"], [15, 16, "MethodName", "BERT"]]}, {"text": "For K=10 , TOD - BERT has a 0.143 NMI score , and BERT only has 0.094 .", "entities": [[5, 6, "MethodName", "BERT"], [9, 10, "MetricName", "NMI"], [13, 14, "MethodName", "BERT"]]}, {"text": "For K=20 , TOD - BERT achieves a 0.213 NMI score , while BERT has 0.109 .", "entities": [[5, 6, "MethodName", "BERT"], [9, 10, "MetricName", "NMI"], [13, 14, "MethodName", "BERT"]]}, {"text": "8 Conclusion We propose task - oriented dialogue BERT ( TODBERT ) trained on nine human - human and multiturn task - oriented datasets across over 60 domains .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "TOD - BERT outperforms BERT on four dialogue downstream tasks , including intention classi\ufb01cation , dialogue state tracking , dialogue act prediction , and response selection .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "BERT"], [15, 18, "TaskName", "dialogue state tracking"]]}, {"text": "It also has a clear advantage in the few - shot experiments when only limited labeled data is available .", "entities": []}, {"text": "TOD - BERT is easy - to - deploy and will be open - sourced , allowing the NLP research community to apply or \ufb01ne - tune any task - oriented conversational problem .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "References Layla El Asri , Hannes Schulz , Shikhar Sharma , Jeremie Zumer , Justin Harris , Emery Fine , Rahul", "entities": []}, {"text": "926Mehrotra , and Kaheer Suleman .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Frames : A corpus for adding memory to goal - oriented dialogue systems .", "entities": [[8, 13, "TaskName", "goal - oriented dialogue systems"]]}, {"text": "arXiv preprint arXiv:1704.00057 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Siqi Bao , Huang He , Fan Wang , and Hua Wu . 2019 .", "entities": []}, {"text": "Plato : Pre - trained dialogue generation model with discrete latent variable .", "entities": [[5, 7, "TaskName", "dialogue generation"]]}, {"text": "arXiv preprint arXiv:1910.07931 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Pawe\u0142 Budzianowski and Ivan Vuli \u00b4 c. 2019 .", "entities": []}, {"text": "Hello , it \u2019s gpt-2 \u2013 how can i help you ?", "entities": [[4, 5, "MethodName", "gpt-2"]]}, {"text": "towards the use of pretrained language models for task - oriented dialogue systems .", "entities": [[4, 7, "TaskName", "pretrained language models"], [8, 13, "TaskName", "task - oriented dialogue systems"]]}, {"text": "arXiv preprint arXiv:1907.05774 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Pawe\u0142 Budzianowski , Tsung - Hsien Wen , Bo - Hsiang Tseng , Inigo Casanueva , Stefan Ultes , Osman Ramadan , and Milica Ga \u02c7si\u00b4c .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Multiwoz - a large - scale multi - domain wizard - of - oz dataset for task - oriented dialogue modelling .", "entities": [[0, 1, "DatasetName", "Multiwoz"], [9, 14, "DatasetName", "wizard - of - oz"]]}, {"text": "arXiv preprint arXiv:1810.00278 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Bill Byrne , Karthik Krishnamoorthi , Chinnadhurai Sankar , Arvind Neelakantan , Daniel Duckworth , Semih Yavuz , Ben Goodrich , Amit Dubey , Andy Cedilnik , and Kyu - Young Kim . 2019 .", "entities": []}, {"text": "Taskmaster-1 : Toward a realistic and diverse dialog dataset .", "entities": [[0, 1, "DatasetName", "Taskmaster-1"]]}, {"text": "arXiv preprint arXiv:1909.05358 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805 .", "entities": []}, {"text": "Li Dong , Nan Yang , Wenhui Wang , Furu Wei , Xiaodong Liu , Yu Wang , Jianfeng Gao , Ming Zhou , and Hsiao - Wuen Hon . 2019 .", "entities": []}, {"text": "Uni\ufb01ed language model pre - training for natural language understanding and generation .", "entities": [[7, 10, "TaskName", "natural language understanding"]]}, {"text": "In Advances in Neural Information Processing Systems , pages 13042\u201313054 .", "entities": []}, {"text": "Mihail Eric and Christopher D Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Keyvalue retrieval networks for task - oriented dialogue .", "entities": []}, {"text": "arXiv preprint arXiv:1705.05414 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shuyang Gao , Abhishek Sethi , Sanchit Aggarwal , Tagyoung Chung , and Dilek Hakkani - Tur . 2019 .", "entities": []}, {"text": "Dialog state tracking : A neural reading comprehension approach .", "entities": [[6, 8, "TaskName", "reading comprehension"]]}, {"text": "arXiv preprint arXiv:1908.01946 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rahul Goel , Shachi Paul , and Dilek Hakkani - T \u00a8ur .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Hyst : A hybrid approach for \ufb02exible and accurate dialogue state tracking .", "entities": [[9, 12, "TaskName", "dialogue state tracking"]]}, {"text": "arXiv preprint arXiv:1907.00883 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matthew Henderson , I \u02dcnigo Casanueva , Nikola Mrk \u02c7si\u00b4c , Pei - Hao Su , Ivan Vuli \u00b4 c , et al . 2019a .", "entities": []}, {"text": "Convert : Ef\ufb01cient and accurate conversational representations from transformers .", "entities": []}, {"text": "arXiv preprint arXiv:1911.03688 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matthew Henderson , Blaise Thomson , and Jason D. Williams .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The second dialog state tracking challenge .", "entities": []}, {"text": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ( SIGDIAL ) , pages 263\u2013272 , Philadelphia , PA , U.S.A. Association for Computational Linguistics .", "entities": []}, {"text": "Matthew Henderson , Ivan Vuli \u00b4 c , Daniela Gerz , I \u02dcnigo Casanueva , Pawe\u0142 Budzianowski , Sam Coope , Georgios Spithourakis , Tsung - Hsien Wen , Nikola Mrk\u02c7si\u00b4c , and Pei - Hao Su . 2019b .", "entities": []}, {"text": "Training neural response selection for task - oriented dialogue systems .", "entities": [[5, 10, "TaskName", "task - oriented dialogue systems"]]}, {"text": "InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5392\u20135404 , Florence , Italy .", "entities": [[16, 17, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Dan Hendrycks and Kevin Gimpel .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Gaussian error linear units ( gelus ) .", "entities": [[0, 4, "MethodName", "Gaussian error linear units"]]}, {"text": "arXiv preprint arXiv:1606.08415 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Nitish Shirish Keskar , Bryan McCann , Lav R Varshney , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Ctrl :", "entities": [[0, 1, "MethodName", "Ctrl"]]}, {"text": "A conditional transformer language model for controllable generation .", "entities": []}, {"text": "arXiv preprint arXiv:1909.05858 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Seokhwan Kim , Michel Galley , Chulaka Gunasekara , Adam Atkinson Sungjin Lee , Baolin Peng , Hannes Schulz , Jianfeng Gao , Jinchao Li , Mahmoud Adada , Minlie Huang , Luis Lastras , Jonathan K. Kummerfeld , Walter S. Lasecki , Chiori Hori , Anoop Cherian , Tim K. Marks , Abhinav Rastogi , Xiaoxue Zang , Srinivas Sunkara , and Raghav Gupta . 2019 .", "entities": [[9, 10, "MethodName", "Adam"]]}, {"text": "The eighth dialog system technology challenge .", "entities": []}, {"text": "arXiv preprint .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Stefan Larson , Anish Mahendran , Joseph J Peper , Christopher Clarke , Andrew Lee , Parker Hill , Jonathan K Kummerfeld , Kevin Leach , Michael A Laurenzano , Lingjia Tang , et al . 2019 .", "entities": []}, {"text": "An evaluation dataset for intent classi\ufb01cation and out - of - scope prediction .", "entities": []}, {"text": "arXiv preprint arXiv:1909.02027 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sungjin Lee , Hannes Schulz , Adam Atkinson , Jianfeng Gao , Kaheer Suleman , Layla El Asri , Mahmoud Adada , Minlie Huang , Shikhar Sharma , Wendy Tay , and Xiujun Li . 2019 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Multi - domain task - completion dialog challenge .", "entities": []}, {"text": "In Dialog System Technology Challenges 8 .", "entities": []}, {"text": "Xiujun Li , Sarah Panda , JJ ( Jingjing ) Liu , and Jianfeng Gao .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Microsoft dialogue challenge : Building end - to - end task - completion dialogue systems .", "entities": []}, {"text": "In SLT 2018 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ilya Loshchilov and Frank Hutter . 2017 .", "entities": []}, {"text": "Decoupled weight decay regularization .", "entities": [[1, 3, "MethodName", "weight decay"]]}, {"text": "arXiv preprint arXiv:1711.05101 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Nikola Mrk \u02c7si\u00b4c , Diarmuid O S \u00b4 eaghdha , Tsung - Hsien Wen , Blaise Thomson , and Steve Young . 2016 .", "entities": []}, {"text": "Neural belief tracker : Data - driven dialogue state tracking.arXiv preprint arXiv:1606.03777 .", "entities": []}, {"text": "Shachi Paul , Rahul Goel , and Dilek Hakkani - T \u00a8ur .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Towards universal dialogue act tagging for task - oriented dialogues .", "entities": []}, {"text": "arXiv preprint arXiv:1907.03020 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "927Baolin Peng , Chenguang Zhu , Chunyuan Li , Xiujun Li , Jinchao Li , Michael Zeng , and Jianfeng Gao . 2020 .", "entities": []}, {"text": "Few - shot natural language generation for task - oriented dialog .", "entities": []}, {"text": "arXiv preprint arXiv:2002.12328 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training .", "entities": []}, {"text": "Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J Liu . 2019 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the limits of transfer learning with a uni\ufb01ed text - to - text transformer .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "arXiv preprint arXiv:1910.10683 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Squad : 100,000 + questions for machine comprehension of text .", "entities": []}, {"text": "arXiv preprint arXiv:1606.05250 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hannah Rashkin , Eric Michael Smith , Margaret Li , and Y - Lan Boureau .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Towards empathetic opendomain conversation models : A new benchmark and dataset .", "entities": [[9, 11, "DatasetName", "and dataset"]]}, {"text": "arXiv preprint arXiv:1811.00207 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Abhinav Rastogi , Xiaoxue Zang , Srinivas Sunkara , Raghav Gupta , and Pranav Khaitan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Towards scalable multi - domain conversational agents : The schema - guided dialogue dataset .", "entities": []}, {"text": "arXiv preprint arXiv:1909.05855 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Marzieh Saeidi , Ritwik Kulkarni , Theodosia Togia , and Michele Sama . 2017 .", "entities": []}, {"text": "The effect of negative sampling strategy on capturing semantic similarity in document embeddings .", "entities": [[8, 10, "TaskName", "semantic similarity"]]}, {"text": "In Proceedings of the 2nd Workshop on Semantic Deep Learning ( SemDeep-2 ) , pages 1\u20138 .", "entities": []}, {"text": "Pararth Shah , Dilek Hakkani - Tur , Bing Liu , and Gokhan Tur . 2018a .", "entities": []}, {"text": "Bootstrapping a neural conversational agent with dialogue self - play , crowdsourcing and on - line reinforcement learning .", "entities": [[4, 5, "DatasetName", "agent"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 3 ( Industry Papers ) , pages 41\u201351 .", "entities": []}, {"text": "Pararth Shah , Dilek Hakkani - T \u00a8ur , Gokhan T \u00a8ur , Abhinav Rastogi , Ankur Bapna , Neha Nayak , and Larry Heck . 2018b .", "entities": []}, {"text": "Building a conversational agent overnight with dialogue self - play .", "entities": [[3, 4, "DatasetName", "agent"]]}, {"text": "arXiv preprint arXiv:1801.04871 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ian Tenney , Patrick Xia , Berlin Chen , Alex Wang , Adam Poliak , R Thomas McCoy , Najoung Kim , Benjamin Van Durme , Samuel R Bowman , Dipanjan Das , et al . 2019 .", "entities": [[12, 13, "MethodName", "Adam"]]}, {"text": "What do you learn from context ?", "entities": []}, {"text": "probing for sentence structure in contextualized word representations .", "entities": []}, {"text": "arXiv preprint arXiv:1905.06316 .Ashish", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008 .", "entities": []}, {"text": "Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel R Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Glue : A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[11, 14, "TaskName", "natural language understanding"]]}, {"text": "arXiv preprint arXiv:1804.07461 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tsung - Hsien Wen , David Vandyke , Nikola Mrksic , Milica Gasic , Lina M Rojas - Barahona , Pei - Hao Su , Stefan Ultes , and Steve Young .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A networkbased end - to - end trainable task - oriented dialogue system .", "entities": []}, {"text": "arXiv preprint arXiv:1604.04562 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Thomas Wolf , Victor Sanh , Julien Chaumond , and Clement Delangue .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Transfertransfo :", "entities": []}, {"text": "A transfer learning approach for neural network based conversational agents .", "entities": [[1, 3, "TaskName", "transfer learning"]]}, {"text": "arXiv preprint arXiv:1901.08149 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Chien - Sheng Wu , Andrea Madotto , Ehsan HosseiniAsl , Caiming Xiong , Richard Socher , and Pascale Fung .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Transferable multi - domain state generator for task - oriented dialogue systems .", "entities": [[7, 12, "TaskName", "task - oriented dialogue systems"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 808\u2013819 , Florence , Italy .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yinfei Yang , Steve Yuan , Daniel Cer , Sheng - yi Kong , Noah Constant , Petr Pilar , Heming Ge , Yun - Hsuan Sung , Brian Strope , and Ray Kurzweil .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning semantic textual similarity from conversations .", "entities": [[1, 4, "TaskName", "semantic textual similarity"]]}, {"text": "InProceedings of The Third Workshop on Representation Learning for NLP , pages 164\u2013174 , Melbourne , Australia . Association for Computational Linguistics .", "entities": [[6, 8, "TaskName", "Representation Learning"]]}, {"text": "Yizhe Zhang , Siqi Sun , Michel Galley , Yen - Chun Chen , Chris Brockett , Xiang Gao , Jianfeng Gao , Jingjing Liu , and Bill Dolan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Dialogpt : Large - scale generative pre - training for conversational response generation .", "entities": [[10, 13, "TaskName", "conversational response generation"]]}, {"text": "arXiv preprint arXiv:1911.00536 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yukun Zhu , Ryan Kiros , Rich Zemel , Ruslan Salakhutdinov , Raquel Urtasun , Antonio Torralba , and Sanja Fidler .", "entities": [[9, 10, "DatasetName", "Ruslan"]]}, {"text": "2015 .", "entities": []}, {"text": "Aligning books and movies : Towards story - like visual explanations by watching movies and reading books .", "entities": []}, {"text": "In Proceedings of the IEEE international conference on computer vision , pages 19 \u2013 27 .", "entities": []}, {"text": "928A Appendices ( a ) BERT ( b ) TOD - BERT - mlm ( c ) TOD - BERT - jnt Figure 3 : The tSNE visualization of BERT and TODBERT representations of system responses in MWOZ test set .", "entities": [[5, 6, "MethodName", "BERT"], [11, 12, "MethodName", "BERT"], [13, 14, "DatasetName", "mlm"], [19, 20, "MethodName", "BERT"], [29, 30, "MethodName", "BERT"]]}, {"text": "Different colors mean different domains .", "entities": []}, {"text": "( a ) BERT ( b ) TOD - BERT - mlm ( c ) TOD - BERT - jnt Figure 4 : The tSNE visualization of BERT and TODBERT representations of system responses in MWOZ test set .", "entities": [[3, 4, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"], [11, 12, "DatasetName", "mlm"], [17, 18, "MethodName", "BERT"], [27, 28, "MethodName", "BERT"]]}, {"text": "Different colors mean different dialogue acts .", "entities": []}, {"text": "929 ( a ) BERT ( b ) TOD - BERT - mlm ( c ) TOD - BERT - jnt Figure 5 : The tSNE visualization of BERT and TODBERT representations of system responses in MWOZ test set .", "entities": [[4, 5, "MethodName", "BERT"], [10, 11, "MethodName", "BERT"], [12, 13, "DatasetName", "mlm"], [18, 19, "MethodName", "BERT"], [28, 29, "MethodName", "BERT"]]}, {"text": "Different colors mean different dialogue slots .", "entities": []}]