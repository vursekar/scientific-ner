[{"text": "Proceedings of the 1st Workshop on Document - grounded Dialogue and Conversational Question Answering , pages 103\u2013108 August 5\u20136 , 2021 .", "entities": [[11, 14, "TaskName", "Conversational Question Answering"]]}, {"text": "\u00a9 2021 Association for Computational Linguistics103Team JARS : DialDoc Subtask 1 - Improved Knowledge Identi\ufb01cation with Supervised Out - of - Domain Pretraining Sopan Khosla , Justin Lovelace , Ritam Dutt , Adithya Pratapa Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Pittsburgh , PA fsopank , jlovelac , rdutt , vpratapa g@andrew.cmu.edu", "entities": []}, {"text": "Abstract In this paper , we discuss our submission for DialDoc subtask 1 .", "entities": []}, {"text": "The subtask requires systems to extract knowledge from FAQ - type documents vital to reply to a user \u2019s query in a conversational setting .", "entities": []}, {"text": "We experiment with pretraining a BERT - based question - answering model on different QA datasets from MRQA , as well as conversational QA datasets like CoQA and QuAC .", "entities": [[5, 6, "MethodName", "BERT"], [17, 18, "DatasetName", "MRQA"], [26, 27, "DatasetName", "CoQA"], [28, 29, "DatasetName", "QuAC"]]}, {"text": "Our results show that models pretrained on CoQA and QuAC perform better than their counterparts that are pretrained on MRQA datasets .", "entities": [[7, 8, "DatasetName", "CoQA"], [9, 10, "DatasetName", "QuAC"], [19, 20, "DatasetName", "MRQA"]]}, {"text": "Our results also indicate that adding more pretraining data does not necessarily result in improved performance .", "entities": []}, {"text": "Our \ufb01nal model , which is an ensemble of AlBERT - XL pretrained on CoQA and QuAC independently , with the chosen answer having the highest average probability score , achieves an F1 - Score of 70:9%on the of\ufb01cial test - set .", "entities": [[14, 15, "DatasetName", "CoQA"], [16, 17, "DatasetName", "QuAC"], [32, 35, "MetricName", "F1 - Score"]]}, {"text": "1 Introduction Question Answering ( QA ) involves constructing an answer for a given question in either an extractive or an abstractive manner .", "entities": [[2, 4, "TaskName", "Question Answering"]]}, {"text": "QA systems are central to other Natural Language Processing ( NLP ) applications like search engines , and dialogue .", "entities": []}, {"text": "Recently , QA based solutions have also been proposed to evaluate factuality ( Wang et al . , 2020 ) and faithfulness ( Durmus et al . , 2020 ) of abstractive summarization systems .", "entities": [[32, 33, "TaskName", "summarization"]]}, {"text": "In addition to popular QA benchmarks like SQuAD ( Rajpurkar et al . , 2016 ) , and MRQA2019 ( Fisch et al . , 2019 ) , we have seen QA challenges that require reasoning over human dialogue .", "entities": [[7, 8, "DatasetName", "SQuAD"]]}, {"text": "Some notable examples being QuAC ( Choi et al . , 2018 ) and CoQA ( Reddy et al . , 2019 ) .", "entities": [[4, 5, "DatasetName", "QuAC"], [14, 15, "DatasetName", "CoQA"]]}, {"text": "These datasets require the model to attend to the entire dialogue context in the process of retrieving an answer .", "entities": []}, {"text": "In this work , we are interesting in building a QA system to help with human dialogue .", "entities": []}, {"text": "Feng et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) introduced a new dataset of goal - oriented dialogues ( Doc2Dial ) that are grounded in the associated documents .", "entities": [[13, 14, "DatasetName", "Doc2Dial"]]}, {"text": "Each sample in the dataset consists of an information - seeking conversation between a user and an agent where agent \u2019s responses are grounded in FAQ - like webpages .", "entities": [[17, 18, "DatasetName", "agent"], [19, 20, "DatasetName", "agent"]]}, {"text": "DialDoc shared task derives its training data from the Doc2Dial dataset and proposes two subtasks which require the participants to ( 1 ) identify the grounding knowledge in form of document span for the next agent turn ; and ( 2 ) generate the next agent response in natural language .", "entities": [[9, 10, "DatasetName", "Doc2Dial"], [35, 36, "DatasetName", "agent"], [45, 46, "DatasetName", "agent"]]}, {"text": "In this paper , we describe our solution to the subtask 1 .", "entities": []}, {"text": "This subtask is formulated as a span selection problem .", "entities": []}, {"text": "Therefore , we leverage a transformerbased extractive question - answering model ( Devlin et al . , 2019 ; Lan et al . , 2019 ) to extract the relevant spans from the document .", "entities": []}, {"text": "We pretrain our model on different QA datasets like SQuAD , different subsets of MRQA-2019 training set , and conversational QA datasets like CoQA and QuAC .", "entities": [[9, 10, "DatasetName", "SQuAD"], [23, 24, "DatasetName", "CoQA"], [25, 26, "DatasetName", "QuAC"]]}, {"text": "We \ufb01nd that models pretrained on out - of - domain QA datasets substantially outperform the baseline .", "entities": []}, {"text": "Our experiments suggest that conversational QA datasets are more useful than MRQA-2019 data or its subsets .", "entities": []}, {"text": "In the following sections , we \ufb01rst present an overview of the DialDoc shared task ( \u00a7 2 ) , followed by our system description ( \u00a7 3 ) and a detailed account of our experimental results , and ablation studies ( \u00a7 4 , \u00a7 5 ) .", "entities": []}, {"text": "2 DialDoc Shared Task Dataset Dataset used in the DialDoc shared - task is derived from Doc2Dial dataset ( Feng et al . , 2020 ) , a new dataset with goal - oriented document - grounded dialogue .", "entities": [[16, 17, "DatasetName", "Doc2Dial"]]}, {"text": "It includes a set of documents and conversations between a user and an agent grounded in the associated document .", "entities": [[13, 14, "DatasetName", "agent"]]}, {"text": "The authors provide annotations for dialogue acts for each utterance in the", "entities": []}, {"text": "104dialogue \ufb02ow , along with the span in the document that acts as the reference of it .", "entities": []}, {"text": "The dataset shared during the shared task was divided into train / validation / testdev / test splits .", "entities": []}, {"text": "Train and validation splits were provided to the participants to facilitate model development .", "entities": []}, {"text": "During phase 1 , the models were evaluated on testdev whereas , the \ufb01nal ranking was done on the performance on the test set .", "entities": []}, {"text": "Pre - processing Using the pre - processing scripts provided by the task organizers , we converted the Doc2Dial dataset into SQuAD v2.0 format with questions containing the latest user utterance as well as all previous turns in the conversation .", "entities": [[18, 19, "DatasetName", "Doc2Dial"], [21, 22, "DatasetName", "SQuAD"]]}, {"text": "This is in line with previous work from ( Feng et al . , 2020 ) which showed that including the entire conversational history performs better than just considering the current user utterance .", "entities": []}, {"text": "Dialogue context is concatenated with the latest user utterance in the reverse time order .", "entities": []}, {"text": "The output of this pre - processing step consisted of 20431 training , 3972 validation , 727 testdev , and 2824 test instances .", "entities": []}, {"text": "3 System Description As discussed earlier , subtask 1 of DialDoc shared task is formulated as a span selection problem .", "entities": []}, {"text": "Therefore , in order to learn to predict the correct span , we use an extractive question - answering setup .", "entities": []}, {"text": "3.1 Question - Answering Model", "entities": []}, {"text": "We pass the pre - processed training data through a QA model that leverages a transformer encoder to contextually represent the question ( dialogue history ) along with the context ( document ) .", "entities": []}, {"text": "Since the grounding document is often longer than the maximum input sequence length for transformers , we follow ( Feng et al . , 2020 ) and truncate the documents in sliding windows with a stride .", "entities": []}, {"text": "The document trunk and the dialogue history are passed through the transformer encoder to create contextual representations for each token in the input .", "entities": []}, {"text": "To extract the beginning and the ending positions of the answer span within the document , the encoded embeddings are sent to a linear layer to output two logits that correspond to the probability of the position being the start and end position of the answer span .", "entities": [[23, 25, "MethodName", "linear layer"]]}, {"text": "The training loss is computed using the Cross - Entropy loss function .", "entities": [[2, 3, "MetricName", "loss"], [10, 11, "MetricName", "loss"]]}, {"text": "We use the huggingface transformers toolkit in all of our experiments.3.2 Pretraining Recent work ( Gururangan et al . , 2020 ) has shown that multi - phase domain adaptive pretraining of transformer - based encoders on related datasets ( and tasks ) bene\ufb01ts the overall performance of the model on the downstream task .", "entities": []}, {"text": "Motivated by this , we experimented with further pretraining the QA model on different out - of - domain QA datasets to gauge its bene\ufb01ts on Doc2Dial ( Table 1 ) .", "entities": [[26, 27, "DatasetName", "Doc2Dial"]]}, {"text": "QA Dataset Domain # Samples SQuAD Wikipedia 86k", "entities": [[5, 6, "DatasetName", "SQuAD"]]}, {"text": "NewsQA News 74k NaturalQuestions Wikipedia 104k HotpotQA", "entities": [[0, 1, "DatasetName", "NewsQA"], [6, 7, "DatasetName", "HotpotQA"]]}, {"text": "Wikipedia 73k SearchQA Jeopardy 117k TriviaQA Trivia 62k MRQA-19 ( Train ) Mixed 516k QuAC Wikipedia 70k CoQA Kids \u2019 Stories , Literature , Exams , News , Wikipedia70k Table 1 : Statistics ( domain , # samples ) for different QA datasets used for continual pre - training .", "entities": [[2, 3, "DatasetName", "SearchQA"], [5, 6, "DatasetName", "TriviaQA"], [14, 15, "DatasetName", "QuAC"], [17, 18, "DatasetName", "CoQA"]]}, {"text": "4 Experimental Setup In this section , we discuss our experimental setup in detail .", "entities": []}, {"text": "4.1 Pretraining Datasets Firstly , we brie\ufb02y describe the different datasets used for the continual pretraining of our transformer - based QA models .", "entities": [[14, 16, "TaskName", "continual pretraining"]]}, {"text": "MRQA-19 Shared task ( Fisch et al . , 2019 ) focused on evaluating the generalizability of QA systems .", "entities": []}, {"text": "They developed a training set that combined examples from 6 different QA datasets and developed evaluation splits using 12 other QA datasets .", "entities": []}, {"text": "We explored the effectiveness of pretraining on the entire MRQA training set as well on each of the 6 training datasets : SQuAD ( Rajpurkar et al . , 2016 ) , NewsQA ( Trischler et al . , 2017 ) , NaturalQuestions ( Kwiatkowski et al . , 2019 ) , HotpotQA ( Yang et al . , 2018 ) , SearchQA ( Dunn et al . , 2017 ) , and TriviaQA ( Joshi et al . , 2017 ) .", "entities": [[9, 10, "DatasetName", "MRQA"], [22, 23, "DatasetName", "SQuAD"], [32, 33, "DatasetName", "NewsQA"], [52, 53, "DatasetName", "HotpotQA"], [62, 63, "DatasetName", "SearchQA"], [73, 74, "DatasetName", "TriviaQA"]]}, {"text": "Conversational QA datasets .", "entities": []}, {"text": "We also experiment with pretraining on two conversational QA datasets : QuAC ( Choi et al . , 2018)1and 1https://huggingface.co/datasets/quac", "entities": [[11, 12, "DatasetName", "QuAC"]]}, {"text": "105QA Dataset Validation EM F1 Doc2Dial 42.1 57.8 + SQuAD 45.0 60.3 + NewsQA 45.5 59.8 + NaturalQuestions ( NQ ) 44.2 59.9 + HotpotQA 43.0 58.0 + SearchQA 42.3 57.5 + TriviaQA 43.1 58.0 + MRQA-19 ( Train ) 43.4 58.9 + SQuAD + NewsQA + NQ 43.0 59.2 + SQuAD + NewsQA + NQ ( IS ) 43.8 59.4 + QuAC 46.4 60.3 + CoQA 47.7 66.0 Table 2 : Performance ( EM ( % ) , F1 ( % ) ) of bert - base - uncased on DialDoc validation set when further pretrained on different QA datasets .", "entities": [[3, 4, "MetricName", "EM"], [4, 5, "MetricName", "F1"], [5, 6, "DatasetName", "Doc2Dial"], [9, 10, "DatasetName", "SQuAD"], [13, 14, "DatasetName", "NewsQA"], [19, 20, "DatasetName", "NQ"], [24, 25, "DatasetName", "HotpotQA"], [28, 29, "DatasetName", "SearchQA"], [32, 33, "DatasetName", "TriviaQA"], [43, 44, "DatasetName", "SQuAD"], [45, 46, "DatasetName", "NewsQA"], [47, 48, "DatasetName", "NQ"], [51, 52, "DatasetName", "SQuAD"], [53, 54, "DatasetName", "NewsQA"], [55, 56, "DatasetName", "NQ"], [62, 63, "DatasetName", "QuAC"], [66, 67, "DatasetName", "CoQA"], [74, 75, "MetricName", "EM"], [79, 80, "MetricName", "F1"]]}, {"text": "CoQA ( Reddy et al . , 2019).2For both datasets , we \ufb01lter out samples which do not adhere to SQuADlike extractive QA setup ( e.g. yes / no questions ) or have a context length of more than 5000 characters .", "entities": [[0, 1, "DatasetName", "CoQA"], [34, 36, "HyperparameterName", "context length"]]}, {"text": "Table 1 presents the size of the different pretraining datasets after the removal of non - extractive QA samples .", "entities": []}, {"text": "4.2 Evaluation Metrics The shared - task relies on Exact Match ( EM ) and F1 metrics to evaluate the systems on subtask 1 .", "entities": [[9, 11, "MetricName", "Exact Match"], [12, 13, "MetricName", "EM"], [15, 16, "MetricName", "F1"]]}, {"text": "To compute these scores , we use the metrics for SQuAD from huggingface.3 4.3 Hyperparameters We use default parameters set by the subtask baseline provided by the authors.4However , we reduce the training per - device batch - size to 2 to accommodate the large models on an Nvidia Geforce GTX 1080", "entities": [[10, 11, "DatasetName", "SQuAD"]]}, {"text": "Ti 12 GB GPU .", "entities": []}, {"text": "We stop the continual out - ofdomain supervised pretraining after 2 epochs .", "entities": []}, {"text": "5 Results We now present the results for different experimental setups we tried for DialDoc subtask 1 . 5.1 Pretraining on Different QA Datasets Our \ufb01rst set of results portray the differential bene\ufb01ts of different out - of - domain QA datasets when used to pretrain the transformer encoder .", "entities": []}, {"text": "2https://huggingface.co/datasets/coqa 3https://huggingface.co/metrics/squad 4https://github.com/doc2dial/ sharedtask - dialdoc2021 / QA Dataset Validation Testdev Test EM F1 EM F1 EM F1 bert - large - uncased - whole - word - masking Doc2Dial 50.1 63.4 \u2013 \u2013 \u2013 \u2013 + SQuAD 52.4 63.9 \u2013 \u2013 \u2013 \u2013 + QuAC ( 1 ) 53.2 68.0 47.4 66.5 \u2013 \u2013 + CoQA ( 2 ) 54.3 70.3 49.4 68.7 45.5 65.5 + CoQA , QuAC ( 3 ) 54.2 70.1 51.0 68.1 \u2013 \u2013 albert - xl + QuAC ( 4 ) 59.1 72.6 47.6 67.1 52.6 67.4 + CoQA ( 5 ) 60.0 74.1 48.0 67.9 50.8 69.5 Ensembles E(4,5 ) 61.4 75.3 49.5 66.6 53.5 70.9 E(1,2,3,4,5 ) 61.5 76.1 49.5 68.7 52.0 69.9 Table 3 : Performance ( EM ( % ) , F1 ( % ) ) of large transformer - based QA models on DialDoc validation and testdev set when further pretrained on different QA datasets .", "entities": [[12, 13, "MetricName", "EM"], [13, 14, "MetricName", "F1"], [14, 15, "MetricName", "EM"], [15, 16, "MetricName", "F1"], [16, 17, "MetricName", "EM"], [17, 18, "MetricName", "F1"], [29, 30, "DatasetName", "Doc2Dial"], [37, 38, "DatasetName", "SQuAD"], [45, 46, "DatasetName", "QuAC"], [56, 57, "DatasetName", "CoQA"], [67, 68, "DatasetName", "CoQA"], [69, 70, "DatasetName", "QuAC"], [83, 84, "DatasetName", "QuAC"], [94, 95, "DatasetName", "CoQA"], [126, 127, "MetricName", "EM"], [131, 132, "MetricName", "F1"]]}, {"text": "Scores in bold are the best in their column ; underlined are best on the of\ufb01cial test - set .", "entities": []}, {"text": "Experiments with bert - base - uncased on the validation set ( Table 2 ) portray that pretraining on different QA datasets is indeed bene\ufb01cial .", "entities": []}, {"text": "Datasets like SQuAD , NewsQA , and NaturalQuestions are more useful than SearchQA , and TriviaQA .", "entities": [[2, 3, "DatasetName", "SQuAD"], [4, 5, "DatasetName", "NewsQA"], [12, 13, "DatasetName", "SearchQA"], [15, 16, "DatasetName", "TriviaQA"]]}, {"text": "However , pretraining on complete MRQA2019 training set does not outperform the individual datasets suggesting that merely introducing more pretraining data might not result in improved performance .", "entities": []}, {"text": "Furthermore , conversational QA datasets like CoQA and QuAC , which are more similar in their setup to DialDoc , perform substantially better than any of the other MRQA-2019 training datasets .", "entities": [[6, 7, "DatasetName", "CoQA"], [8, 9, "DatasetName", "QuAC"]]}, {"text": "We observe similar trends with larger transformers ( Table 3 ) .", "entities": []}, {"text": "Models pretrained on QuAC or CoQA outperform those pretrained on SQuAD .", "entities": [[3, 4, "DatasetName", "QuAC"], [5, 6, "DatasetName", "CoQA"], [10, 11, "DatasetName", "SQuAD"]]}, {"text": "However , combining CoQA and QuAC during pretraining does not seem to help with the performance on validation or testdev split .", "entities": [[3, 4, "DatasetName", "CoQA"], [5, 6, "DatasetName", "QuAC"]]}, {"text": "Analyzing Different Transformer Variants Table 3 also contains the results for experiments where albert - xl is used to encode the questioncontext pair .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "We \ufb01nd that albert - xl -based models outperform their bert counterparts on validation set .", "entities": []}, {"text": "However , they do not generalize well to the Testdev set , which contains about 30 % of the test instances but is much smaller than validation set in size ( 727 samples in testdev vs 3972 in validation set ) .", "entities": []}, {"text": "1065.2 Results on test set We only submitted our best performing models on the of\ufb01cial test set due to a constraint on the number of submissions .", "entities": []}, {"text": "Contrary to the trends for testdev phase , albert - xl models trained on conversational QA datasets perform the best .", "entities": []}, {"text": "albert - xl + QuAC is the best - performing single model according to the EM metric ( EM = 52:60 ) , whereas albert - xl + CoQA performs the best on F1 metric ( F1 = 69 : 48 ) on the test set .", "entities": [[4, 5, "DatasetName", "QuAC"], [15, 16, "MetricName", "EM"], [18, 19, "MetricName", "EM"], [28, 29, "DatasetName", "CoQA"], [33, 34, "MetricName", "F1"], [36, 37, "MetricName", "F1"]]}, {"text": "5.3", "entities": []}, {"text": "Ensembling We perform ensembling over the outputs of the model variants to obtain a single uni\ufb01ed ranked list .", "entities": []}, {"text": "For a given question Q , we produce 20candidate spans , along with a corresponding probability score ps .", "entities": []}, {"text": "We compute rank - scores rsfor the answer - spans at rank rasrs=1 log2(r+1 ) .", "entities": []}, {"text": "We then aggregate the information of the answer spans for the model variants using the following techniques .", "entities": []}, {"text": "Frequent : We chose the answer span which was the most frequent across the model variants .", "entities": []}, {"text": "Rank Score : We chose the answer span which was the highest average rank score .", "entities": [[1, 2, "MetricName", "Score"]]}, {"text": "Probability Score : We chose the answer span which was the highest average probability score .", "entities": [[1, 2, "MetricName", "Score"]]}, {"text": "We observe empirically that ensembling using the probability score performs the best and hence we report the results of ensembling using the probability score ( E ) in Table 3 .", "entities": []}, {"text": "We observe the highest gains after ensembling the outputs of all the 5 model variants on the validation test and test - dev set .", "entities": []}, {"text": "However , the best performance on the test set was achieved by ensembling over the albert - xl models pre - trained independently on CoQA and QuAC ( EM = 53.5 , F1= 70.9 ) .", "entities": [[24, 25, "DatasetName", "CoQA"], [26, 27, "DatasetName", "QuAC"], [28, 29, "MetricName", "EM"]]}, {"text": "This was the \ufb01nal submission for our team .", "entities": []}, {"text": "5.4 Informed Data Selection We investigate the disparate impact of pretraining on different MRQA-19 datasets on the Doc2Dial shared task .", "entities": [[17, 18, "DatasetName", "Doc2Dial"]]}, {"text": "Speci\ufb01cally , we explored factors such as answer length , relative position of the answer in the context , question length , and context length in Table 4 .", "entities": [[23, 25, "HyperparameterName", "context length"]]}, {"text": "We observe that the SQuAD , NewsQA , and NaturalQuestions ( NQ ) has compartaively longer answers than the other datasets .", "entities": [[4, 5, "DatasetName", "SQuAD"], [6, 7, "DatasetName", "NewsQA"], [11, 12, "DatasetName", "NQ"]]}, {"text": "However , we do not observe a noticeable difference in terms of question length , context length or relative position of the answer in the context , with respect to the other datasets .", "entities": [[15, 17, "HyperparameterName", "context length"]]}, {"text": "Dataset Question Answer Context Rel Pos SQuAD 59.6 20.2 754.7 0.462 NaturalQ 47.2 23.7 804.8 0.390 NewsQA 36.8 25.0 3022.7 0.261 TriviaQA 76.1 9.7 4069.3 0.380 SearchQA 80.4 10.9 3818.7 0.392 HotpotQA 114.0 14.3 945.0 0.457 Doc2Dial 61.4 129.3 4814.2 0.427 Table 4 : Statistics of Average Question Length , Average Answer Length , Average Context Length , and Average Relative Position of the Answer in the Context for Doc2Dial and different MRQA subsets .", "entities": [[6, 7, "DatasetName", "SQuAD"], [16, 17, "DatasetName", "NewsQA"], [21, 22, "DatasetName", "TriviaQA"], [26, 27, "DatasetName", "SearchQA"], [31, 32, "DatasetName", "HotpotQA"], [36, 37, "DatasetName", "Doc2Dial"], [55, 57, "HyperparameterName", "Context Length"], [69, 70, "DatasetName", "Doc2Dial"], [72, 73, "DatasetName", "MRQA"]]}, {"text": "Figure 1 : Distribution of Question Words for MRQA .", "entities": [[8, 9, "DatasetName", "MRQA"]]}, {"text": "We also use the dataset of Li and Roth ( 2002 ) to train a BERT classi\ufb01er to predict answer type of a question with 97 % accuracy .", "entities": [[15, 16, "MethodName", "BERT"], [27, 28, "MetricName", "accuracy"]]}, {"text": "The coarse - answer types are DESC ( Description ) , NUM ( Numerical ) , ENT ( Entity ) , HUM ( Person ) , LOC ( Location ) and ABBR ( Abbreviation ) .", "entities": []}, {"text": "We use the classi\ufb01er to gauge the distribution of answer types on MRQA datasets and Doc2Dial .", "entities": [[12, 13, "DatasetName", "MRQA"], [15, 16, "DatasetName", "Doc2Dial"]]}, {"text": "We observe from Figure 2 that a majority of questions in Doc2Dial require a descriptive answer .", "entities": [[11, 12, "DatasetName", "Doc2Dial"]]}, {"text": "These DESC type questions are more prevelant in SQuAD , NewsQA , and NQ , which might explain their ef\ufb01cacy .", "entities": [[8, 9, "DatasetName", "SQuAD"], [10, 11, "DatasetName", "NewsQA"], [13, 14, "DatasetName", "NQ"]]}, {"text": "To ascertain the bene\ufb01t of intelligent sampling , we pretrain on a much smaller subset of the SQuAD , NewsQA , and NaturalQuestions dataset , which we obtain via intelligent sampling .", "entities": [[17, 18, "DatasetName", "SQuAD"], [19, 20, "DatasetName", "NewsQA"]]}, {"text": "We select questions which satisfy one of the following criteria , ( i ) the answer length of the question is \u001550 , ( ii ) the question includes \u2018 how \u2019 or \u2018 why \u2019 question word or ( iii ) the answer type of the question is \u2018 DESC \u2019 .", "entities": []}, {"text": "Overall , the size of the selected sample is only 20 % of the original dataset , yet achieves a higher EM score than the combined dataset as seen in Table 2 .", "entities": [[21, 22, "MetricName", "EM"]]}, {"text": "Yet , surprisingly , the performance is lower than each of the individual dataset .", "entities": []}, {"text": "6 Conclusion Our submission to the DialDoc subtask 1 performs continual pretraining of a transformer - based encoder on out - of - domain QA datasets .", "entities": [[10, 12, "TaskName", "continual pretraining"]]}, {"text": "Experiments", "entities": []}, {"text": "107 Figure 2 : Distribution of Answer Types for MRQA .", "entities": [[9, 10, "DatasetName", "MRQA"]]}, {"text": "with different QA datasets suggest that conversational QA datasets like CoQA and QuAC are highly bene\ufb01cial as their setup is substantially similar to Doc2Dial , the downstream dataset of interest .", "entities": [[10, 11, "DatasetName", "CoQA"], [12, 13, "DatasetName", "QuAC"], [23, 24, "DatasetName", "Doc2Dial"]]}, {"text": "Our \ufb01nal submission ensembles two AlBERT - XL models independently pretrained on CoQA and QuAC and achieves an F1 - Score of 70:9%and", "entities": [[12, 13, "DatasetName", "CoQA"], [14, 15, "DatasetName", "QuAC"], [18, 21, "MetricName", "F1 - Score"]]}, {"text": "EM - Score of53:5%on the competition test - set", "entities": [[0, 1, "MetricName", "EM"], [2, 3, "MetricName", "Score"]]}, {"text": ".", "entities": []}, {"text": "Impact Statement In this work , we tackle the task of question answering ( QA ) for English language text .", "entities": [[11, 13, "TaskName", "question answering"]]}, {"text": "While we believe that the proposed methods can be effective in other languages , we leave this exploration for future work .", "entities": []}, {"text": "We also acknowledge that QA systems suffer from bias ( Li et al . , 2020 ) , which often lead to unintended real - world consequences .", "entities": []}, {"text": "For the purpose of the shared task , we focused solely on the modeling techniques , but a study of model bias in our systems is necessary .", "entities": []}, {"text": "References Eunsol Choi , He He , Mohit Iyyer , Mark Yatskar , Wentau Yih , Yejin Choi , Percy Liang , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "QuAC :", "entities": [[0, 1, "DatasetName", "QuAC"]]}, {"text": "Question answering in context .", "entities": [[0, 2, "TaskName", "Question answering"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2174\u20132184 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Matthew Dunn , Levent Sagun , Mike Higgins , V .", "entities": []}, {"text": "U. G\u00a8uney , V olkan Cirik , and Kyunghyun Cho . 2017 .", "entities": []}, {"text": "Searchqa :", "entities": [[0, 1, "DatasetName", "Searchqa"]]}, {"text": "A new qa dataset augmented with context from a search engine .", "entities": []}, {"text": "ArXiv , abs/1704.05179.Esin Durmus , He He , and Mona Diab .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "2020 .", "entities": []}, {"text": "FEQA :", "entities": []}, {"text": "A question answering evaluation framework for faithfulness assessment in abstractive summarization .", "entities": [[1, 3, "TaskName", "question answering"], [10, 11, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5055 \u2013 5070 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Song Feng , Hui Wan , Chulaka Gunasekara , Siva Patel , Sachindra Joshi , and Luis Lastras . 2020 .", "entities": []}, {"text": "doc2dial : A goal - oriented document - grounded dialogue dataset .", "entities": [[0, 1, "DatasetName", "doc2dial"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 8118\u20138128 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Adam Fisch , Alon Talmor , Robin Jia , Minjoon Seo , Eunsol Choi , and Danqi Chen .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "2019 .", "entities": []}, {"text": "MRQA 2019 shared task : Evaluating generalization in reading comprehension .", "entities": [[0, 1, "DatasetName", "MRQA"], [8, 10, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of the 2nd Workshop on Machine Reading for Question Answering , pages 1\u201313 , Hong Kong , China . Association for Computational Linguistics .", "entities": [[10, 12, "TaskName", "Question Answering"]]}, {"text": "Suchin Gururangan , Ana Marasovi \u00b4 c , Swabha Swayamdipta , Kyle Lo , Iz Beltagy , Doug Downey , and Noah A. Smith .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Do n\u2019t stop pretraining : Adapt language models to domains and tasks .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8342\u20138360 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mandar Joshi , Eunsol Choi , Daniel Weld , and Luke Zettlemoyer . 2017 .", "entities": []}, {"text": "TriviaQA : A large scale distantly supervised challenge dataset for reading comprehension .", "entities": [[0, 1, "DatasetName", "TriviaQA"], [10, 12, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1601\u20131611 , Vancouver , Canada . Association for Computational Linguistics .", "entities": []}, {"text": "Tom Kwiatkowski , Jennimaria Palomaki , Olivia Red\ufb01eld , Michael Collins , Ankur Parikh , Chris Alberti , Danielle Epstein , Illia Polosukhin , Matthew Kelcey , Jacob Devlin , Kenton Lee , Kristina N. Toutanova , Llion Jones , Ming - Wei Chang , Andrew Dai , Jakob Uszkoreit , Quoc Le , and Slav Petrov . 2019 .", "entities": []}, {"text": "Natural questions : a benchmark for question answering research .", "entities": [[0, 2, "DatasetName", "Natural questions"], [6, 8, "TaskName", "question answering"]]}, {"text": "Transactions of the Association of Computational Linguistics .", "entities": []}, {"text": "Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , and Radu Soricut .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Albert :", "entities": []}, {"text": "A lite bert for self - supervised learning of language representations .", "entities": [[4, 8, "TaskName", "self - supervised learning"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Tao Li , Daniel Khashabi , Tushar Khot , Ashish Sabharwal , and Vivek Srikumar . 2020 .", "entities": []}, {"text": "UNQOVERing stereotyping biases via underspeci\ufb01ed questions .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 3475\u20133489 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "108Xin Li and Dan Roth .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Learning question classi\ufb01ers .", "entities": []}, {"text": "In COLING 2002 : The 19th International Conference on Computational Linguistics .", "entities": []}, {"text": "Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "SQuAD : 100,000 + questions for machine comprehension of text .", "entities": [[0, 1, "DatasetName", "SQuAD"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383\u20132392 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Siva Reddy , Danqi Chen , and Christopher D. Manning .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "CoQA :", "entities": [[0, 1, "DatasetName", "CoQA"]]}, {"text": "A conversational question answering challenge .", "entities": [[1, 4, "TaskName", "conversational question answering"]]}, {"text": "Transactions of the Association for Computational Linguistics , 7:249\u2013266 .", "entities": []}, {"text": "Adam Trischler , Tong Wang , Xingdi Yuan , Justin Harris , Alessandro Sordoni , Philip Bachman , and Kaheer Suleman . 2017 .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "NewsQA : A machine comprehension dataset .", "entities": [[0, 1, "DatasetName", "NewsQA"]]}, {"text": "In Proceedings of the 2nd Workshop on Representation Learning for NLP , pages 191\u2013200 , Vancouver , Canada . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Representation Learning"]]}, {"text": "Alex Wang , Kyunghyun Cho , and Mike Lewis .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Asking and answering questions to evaluate the factual consistency of summaries .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5008\u20135020 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zhilin Yang , Peng Qi , Saizheng Zhang , Yoshua Bengio , William Cohen , Ruslan Salakhutdinov , and Christopher D. Manning .", "entities": [[15, 16, "DatasetName", "Ruslan"]]}, {"text": "2018 .", "entities": []}, {"text": "HotpotQA : A dataset for diverse , explainable multi - hop question answering .", "entities": [[0, 1, "DatasetName", "HotpotQA"], [8, 13, "TaskName", "multi - hop question answering"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2369\u20132380 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}]