[{"text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics , pages 1336\u20131350 April 19 - 23 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics1336An Expert Annotated Dataset for the Detection of Online Misogyny Ella Guest The Alan Turing Institute University of Manchester ella.guest@manchester.ac.ukBertie", "entities": []}, {"text": "Vidgen The Alan Turing Institute bvidgen@turing.ac.ukAlexandros Mittos Queen Mary University of London University College London", "entities": []}, {"text": "alexandros@mittos.net Nishanth Sastry University of Surrey King \u2019s College London The Alan Turing Institute nsastry@turing.ac.ukGareth Tyson Queen Mary University of London The Alan Turing Institute g.tyson@qmul.ac.ukHelen Margetts The Alan Turing Institute Oxford Internet Institute hmargetts@turing.ac.uk Abstract Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women .", "entities": []}, {"text": "We present a new hierarchical taxonomy for online misogyny , as well as an expert labelled dataset to enable automatic classi\ufb01cation of misogynistic content .", "entities": []}, {"text": "The dataset consists of 6,567 labels for Reddit posts and comments .", "entities": [[7, 8, "DatasetName", "Reddit"]]}, {"text": "As previous research has found untrained crowdsourced annotators struggle with identifying misogyny , we hired and trained annotators and provided them with robust annotation guidelines .", "entities": []}, {"text": "We report baseline classi\ufb01cation performance on the binary classi\ufb01cation task , achieving accuracy of 0.93 and F1 of 0.43 .", "entities": [[12, 13, "MetricName", "accuracy"], [16, 17, "MetricName", "F1"]]}, {"text": "The codebook and datasets are made freely available for future researchers .", "entities": []}, {"text": "1 Introduction Misogyny is a problem in many online spaces , making them less welcoming , safe , and accessible for women .", "entities": []}, {"text": "Women have been shown to be twice as likely as men to experience gender - based online harassment ( Duggan , 2017 ) .", "entities": []}, {"text": "This misogyny can in\ufb02ict serious psychological harm on women and produce a \u2018 silencing effect \u2019 , whereby women selfcensor or withdraw from online spaces entirely , thus limiting their freedom of expression ( Mantilla , 2013 ; International , 2017 ) .", "entities": []}, {"text": "Tackling such content is increasingly a priority for social media platforms and civil society organisations .", "entities": []}, {"text": "However , detecting online misogyny remains a dif\ufb01cult task ( Hewitt et al . , 2016 ; Nozza et al . , 2019 ) .", "entities": []}, {"text": "One problem is the lack of high - quality datasets to train machine learning models , which would enable the creation of ef\ufb01cient and scalable automated detection systems ( Anzovino et al . , 2018 ) .", "entities": []}, {"text": "Previous research has primarily used Twitter data and there is a pressing need for other platforms to be researched Lynn et al .", "entities": []}, {"text": "( 2019a )", "entities": []}, {"text": ".", "entities": []}, {"text": "Notably , de - spite social scienti\ufb01c studies that show online misogyny is pervasive on some Reddit communities , to date a training dataset for misogyny has not been created with Reddit data .", "entities": [[16, 17, "DatasetName", "Reddit"], [31, 32, "DatasetName", "Reddit"]]}, {"text": "In this paper we seek to address the limitations of previous research by presenting a dataset of Reddit content with expert labels for misogyny that can be used to develop more accurate and nuanced classi\ufb01cation models .", "entities": [[17, 18, "DatasetName", "Reddit"]]}, {"text": "Our contributions are four - fold .", "entities": []}, {"text": "First , we develop a detailed hierarchical taxonomy based on existing literature on online misogyny .", "entities": []}, {"text": "Second , we create and share a detailed codebook used to train annotators to identify different types of misogyny .", "entities": []}, {"text": "Third , we present a dataset of 6,383 entries from Reddit .", "entities": [[10, 11, "DatasetName", "Reddit"]]}, {"text": "Fourth , we create baseline classi\ufb01cation models based on these datasets .", "entities": []}, {"text": "All of the research artefacts are made freely available via a public repository for future researchers.1 The dataset itself has several innovations which differentiate it from previous training datasets for misogyny .", "entities": []}, {"text": "First , we use chronological and structured conversation threads , which mean annotators take into account the previous context of each entry before labelling .", "entities": []}, {"text": "Second , we distinguish between conceptually distinct types of misogynistic abuse , including gendered personal attacks , use of misogynistic pejoratives , and derogatory and threatening language .", "entities": []}, {"text": "Third , we highlight the speci\ufb01c section of text , also known as a \u2018 span \u2019 , on which each label is based .", "entities": []}, {"text": "This helps differentiate between multiple labels on one piece of text .", "entities": []}, {"text": "Fourth , we use trained annotators , rather than crowd - sourced workers .", "entities": []}, {"text": "We also use facilitated meetings to decide the \ufb01nal labels rather than just a majority decision .", "entities": []}, {"text": "Both of these factors lead to a high - quality dataset .", "entities": []}, {"text": "Additionally , we provide a second dataset with the original labels made by annotators before the \ufb01nal labels were decided .", "entities": []}, {"text": "1https://github.com/ellamguest/ online - misogyny - eacl2021", "entities": []}, {"text": "13372 Background Most previous classi\ufb01cation work on online misogyny has used data from Twitter ( Waseem and Hovy , 2016 ; Anzovino et al . , 2018 ; Jha and Mamidi , 2017 ) .", "entities": []}, {"text": "However , social scienti\ufb01c and ethnographic research shows that Reddit is increasingly home to numerous misogynistic communities .", "entities": [[9, 10, "DatasetName", "Reddit"]]}, {"text": "Reddit is a social news website organised in to topic - based communities .", "entities": [[0, 1, "DatasetName", "Reddit"]]}, {"text": "Each subreddit acts as a message board where users make posts and hold discussions in comment threads on those posts .", "entities": []}, {"text": "In recent years it has become a hub for anti - feminist activism online ( Massanari , 2017 ; Ging and Siapera , 2018 ) .", "entities": []}, {"text": "It is also home to many misogynistic communities , particularly those associated with the \u2018 manosphere \u2019 , a loosely connected set of communities which perpetuate traditional forms of misogyny and develop new types of misogynistic discourse which in turn spread to other online spaces ( Ging , 2017 ; Zuckerberg , 2018 ; Ging et", "entities": []}, {"text": "al . , 2019 ; Farrell et al . , 2019 ; Ribeiro et al . , 2020 ) .", "entities": []}, {"text": "Recent research suggests that the rate of misogynistic content in the Reddit manosphere is growing and such content is increasingly more violent ( Farrell et al . , 2019 ) .", "entities": [[11, 12, "DatasetName", "Reddit"]]}, {"text": "Waseem and Hovy ( 2016 ) provided a widelyused dataset for abusive language classi\ufb01cation .", "entities": [[11, 13, "TaskName", "abusive language"]]}, {"text": "They used expert annotators to identify sexist and racist tweets based on a set of criteria drawn from critical race theory .", "entities": []}, {"text": "The tweets were initially labelled by the authors then reviewed by a third annotator .", "entities": []}, {"text": "The resulting dataset consists of 17k tweets , of which 20 % are labelled as sexist .", "entities": []}, {"text": "However 85 % of the disagreements between annotators were over sexism labels , which shows that even experienced coders of abusive language can have dif\ufb01cultly identifying gendered abuse .", "entities": [[20, 22, "TaskName", "abusive language"]]}, {"text": "Jha and Mamidi ( 2017 ) extended on the Waseem and Hovy ( 2016 ) dataset to distinguish between between \u2018 benevolent \u2019 and \u2018 hostile \u2019 sexism ( Glick and Fiske , 1997 ) .", "entities": []}, {"text": "They classed all sexist labels in the previous dataset as \u2018 Hostile \u2019 and all non - sexist labels as \u2018 Other \u2019 .", "entities": []}, {"text": "They then augmented the dataset by collecting tweets using keyword sampling on benevolently sexist phrases ( e.g. \u2018 smart for a girl \u2019 ) and extracted those manually identi\ufb01ed as \u2018 benevolent sexism \u2019 .", "entities": []}, {"text": "In the combined dataset of 10,095 unique tweets 712 were labelled as \u2018 benevolent \u2019 , 2,254 as \u2018 hostile \u2019 , and 7,129 as \u2018 not sexist \u2019 .", "entities": []}, {"text": "They thus found that in the data hostile sexism was more than three times as common as the benevolent form .", "entities": []}, {"text": "Their work highlights the need for greater attentionto be placed on forms of \u2018 subtle abuse \u2019 , particularly for online misogyny ( Jurgens et al . , 2019 ) .", "entities": []}, {"text": "Anzovino et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) developed a taxonomy with \ufb01ve categories of misogyny , drawn from the work of Poland ( 2016 ): Stereotype & Objecti\ufb01cation , Dominance , Derailing , Sexual Harassment & Threats of Violence , Discredit .", "entities": []}, {"text": "They used a combination of expert and crowdsourced annotation to apply the taxonomy and present a dataset of 4,454 tweets with balanced levels of misogynistic and non - misogynistic content .", "entities": []}, {"text": "A shared task con\ufb01rmed that the dataset could be used to distinguish misogynistic and non - misogynistic content with high accuracy , but performance was lower in differentiating between types of misogyny ( Fersini et al . , 2018 ) .", "entities": [[20, 21, "MetricName", "accuracy"]]}, {"text": "Lynn et al .", "entities": []}, {"text": "( 2019b ) provide a dataset of 2k Urban Dictionary de\ufb01nitions of which half are labelled as misogynistic .", "entities": []}, {"text": "In Lynn et al .", "entities": []}, {"text": "( 2019a ) they show that deep learning techniques had greater accuracy in detecting misogyny than conventional machine learning techniques .", "entities": [[11, 12, "MetricName", "accuracy"]]}, {"text": "3 Data collection We collected conversation threads from Reddit .", "entities": [[8, 9, "DatasetName", "Reddit"]]}, {"text": "Given that a very small amount of content on social media is hateful , a key dif\ufb01culty when creating datasets for annotation is collecting enough instances of the \u2018 positive \u2019 class to be useful for machine learning ( Schmidt and Wiegand , 2017 ; Fortuna and Nunes , 2018 ) .", "entities": []}, {"text": "However , sampling strategies can introduce biases in the composition and focus of the datasets if overly simplistic methods are used , such as searching for explicitly misogynistic terms ( Wiegand et al . , 2019 ) .", "entities": []}, {"text": "To ensure that our dataset contains enough misogynistic abuse we began with targeted sampling , taking content from 12 subreddits that were identi\ufb01ed as misogynistic in previous research .", "entities": []}, {"text": "This includes subreddits such as r / MensRights , r / seduction , and r / TheRedPill .", "entities": []}, {"text": "The sources used to identify these subreddits are available in Table 9 in the Appendix .", "entities": []}, {"text": "We then identi\ufb01ed 22 additional subreddits which had been recommended by the moderators / owners of the original 12 subreddits in the \u2018 sidebar \u2019 .", "entities": []}, {"text": "Some of these are not misogynistic but discuss women ( e.g. r / AskFeminists ) and/or are otherwise related to misogyny .", "entities": []}, {"text": "For example , r / exredpill is a support group for former members of the misogynistic subreddit r / TheRedPill .", "entities": []}, {"text": "Table 9 in", "entities": []}, {"text": "1338the Appendix lists the 34 targeted subreddits and the number of entries and threads for each in the dataset .", "entities": []}, {"text": "Over 11 weeks , for each subreddit , we collected the entire threads of the 20 most popular posts that week .", "entities": []}, {"text": "Using subreddits to target the sampling rather than keywords should ensure that more linguistic variety is captured , minimising the amount of bias as keywords such as \u2018 slut \u2019 are associated with more explicit and less subtle forms of abuse .", "entities": []}, {"text": "Nonetheless , only sampling from suspected misogynistic communities could still lead to classi\ufb01ers which only identify the forms of misogyny found in those targeted contexts ( Davidson et al . , 2017 ; Wiegand et al . , 2019 ; Sap et al . , 2019 ) .", "entities": []}, {"text": "To account for this potential bias , and to enable greater generalisabilty , we sampled content from 71 randomly selected subreddits .", "entities": []}, {"text": "They accounted for 18 % of threads and 16 % of entries in our dataset .", "entities": []}, {"text": "For each randomly selected subreddit , we collected the thread of the most popular post .", "entities": []}, {"text": "All threads were in English with the exception of one thread from the subreddit r / Romania .", "entities": []}, {"text": "Posts and comments were collected from February to May 2020 using the python package PRAW , a wrapper for the Reddit API ( Boe , 2020 ) .", "entities": [[20, 21, "DatasetName", "Reddit"]]}, {"text": "Posts on Reddit have a text title and a body which can be text , an image , or a link .", "entities": [[2, 3, "DatasetName", "Reddit"]]}, {"text": "For posts with a text body we combined this with the post title to create a single unit of text .", "entities": []}, {"text": "For the 29 % of posts where the body was an image we also collected the image .", "entities": []}, {"text": "4 Taxonomy We developed a hierarchical taxonomy with three levels .", "entities": []}, {"text": "First , we make a binary distinction between Misogynistic content and Non - misogynistic content , which are mutually exclusive .", "entities": []}, {"text": "Second , we elaborated subtypes of Misogynistic and Nonmisogynistic content .", "entities": []}, {"text": "For Misogynistic content we de\ufb01ned four categories : ( i)Misogynistic Pejoratives,(ii)descriptions of Misogynistic Treatment , ( iii)acts of Misogynistic Derogation and ( iv)Gendered Personal attacks against women .", "entities": []}, {"text": "For Nonmisogynistic content we de\ufb01ned three categories : ( i)Counter speech against misogyny , ( ii)Nonmisogynistic personal attacks and ( iii)None of the categories .", "entities": []}, {"text": "Third , we included additional \ufb02ags for some of the second level categories .", "entities": []}, {"text": "Within both Misogynistic and Non - misogynistic content , the second level categories are not mutually exclusive , thereby allowing for multiple labels per entry .", "entities": []}, {"text": "Forinstance , a Misogynistic entry could be assigned labels for both a Pejorative andTreatment .", "entities": []}, {"text": "This taxonomy draws on the typologies of abuse presented by Waseem et al .", "entities": []}, {"text": "( 2017 ) and Vidgen et al .", "entities": []}, {"text": "( 2019 ) as well as theoretical work in online misogyny research ( Filipovic , 2007 ; Mantilla , 2013 ; Jane , 2016 ; Ging , 2017 ; Anzovino et al . , 2018 ; Ging and Siapera , 2019 ; Farrell et al . , 2019 ) .", "entities": []}, {"text": "It was developed by reviewing existing literature on online misogyny and then iterating over small samples of the dataset .", "entities": []}, {"text": "This deductive - inductive process allowed us to ensure that conceptually distinct varieties of abuse are separated and that different types of misogyny can be unpicked .", "entities": []}, {"text": "This is important given that they can have very different impacts on victims , different causes , and re\ufb02ect different outlooks and interests on the part of the speaker .", "entities": []}, {"text": "4.1 Misogynistic content Misogynistic content directs abuse at women or a closely related gendered group ( e.g. feminists ) .", "entities": []}, {"text": "This content can fall in to four non - mutually exclusive categories .", "entities": []}, {"text": "4.1.1", "entities": []}, {"text": "Misogynistic pejoratives Misogynistic pejoratives are terms which are used to disparage women .", "entities": []}, {"text": "It includes terms which are explicitly insulting and derogatory , such as \u2018 slut \u2019 or \u2018 whore \u2019 , as well as terms which implicitly express negativity or animosity against women , such as \u2018 Stacy \u2019 or \u2018 Becky \u2019 .", "entities": []}, {"text": "For example , \u2018 Stacy \u2019 is a term used in the incel community to describe women considered attractive and unattainable , in opposition to a more average and attainable \u2018 Becky \u2019 ( Jennings , 2018 ) .", "entities": []}, {"text": "4.1.2 Misogynistic treatment Misogynistic treatment is content that discusses , advocates , incites or plans negative or harmful treatment of women .", "entities": []}, {"text": "It includes expressing intent to take action against women , as well as expressing desires about how they should be treated .", "entities": []}, {"text": "Misogynistic treatment contains third - level subcategories : Threatening language andDisrespectful actions .", "entities": []}, {"text": "1.Threatening language : Content which expresses an intent / desire to in\ufb02ict / cause women to suffer harm , or expresses support for , encourages , advocates or incites such harm .", "entities": []}, {"text": "It is an \u2018 explicit \u2019 form of abuse .", "entities": []}, {"text": "It falls in to three thematic groups :", "entities": []}, {"text": "1339(a)Physical violence : non - sexual physical violence such as killing , maiming , beating , etc . e.g. \u2018 Feminists deserve to be shot \u2019 .", "entities": []}, {"text": "( b)Sexual violence : explicit sexual violence such as rape , penetration , molestation , etc . e.g. \u2018 Someone should rape her \u2013 that would put her in her place \u2019 .", "entities": []}, {"text": "( c)Privacy : an invasion of privacy such as the disclosure of personal information ( i.e. doxing ) or threats to visit them .", "entities": []}, {"text": "e.g. \u2018 I know where you live , bitch \u2019 .", "entities": []}, {"text": "2.Disrespectful actions : Content which treats / portrays women as either lacking or not deserving independence / autonomy .", "entities": []}, {"text": "This includes more subtly abusive statements about how women should be treated and what they should be allowed to do .", "entities": []}, {"text": "It is an \u2018 implicit \u2019 form of abuse .", "entities": []}, {"text": "It falls in to four thematic groups : ( a)Controlling : suggesting or stating that women should be controlled in some way , especially by a man or men .", "entities": []}, {"text": "E.g. \u2018 I would never let my girlfriend do that \u2019 .", "entities": []}, {"text": "( b)Manipulation : using or advocating the use of tactics such as lying and gaslighting to manipulate what women do or think .", "entities": []}, {"text": "E.g. \u2018 Told my last girlfriend she was hallucinating when she saw the texts from my side piece \u2019 .", "entities": []}, {"text": "( c)Seduction and conquest : discussing woman solely as sexual conquests or describing previous incidences of when they have been treated as such .", "entities": []}, {"text": "E.g. \u2018 Got her home and used her so hard \u2019 .", "entities": []}, {"text": "( d)Other : content that is not covered by the other subcategories .", "entities": []}, {"text": "4.1.3 Misogynistic derogation Misogynistic derogation is content that demeans or belittles women .", "entities": []}, {"text": "This content can be explicitly or implicitly abusive .", "entities": []}, {"text": "It is separated into third - level subcategories : 1.Intellectual inferiority : making negative judgements of women \u2019s intellectual abilities , such as a lack of critical thinking or emotional control .", "entities": []}, {"text": "This includes content which infantilizes women .", "entities": []}, {"text": "An implicit example would be \u2018 My gf cries at the stupidest shit \u2013 lol!\u2019for suggesting irrational emotional responses .", "entities": []}, {"text": "An explicit example is \u2018 Typical stupid bitch \u2013 talking about things she does n\u2019t understand \u2019 .", "entities": []}, {"text": "2.Moral inferiority : making negative judgements of women \u2019s moral worth , such as suggesting they are de\ufb01cient or lesser to men in some way .", "entities": []}, {"text": "This includes subjects such as super\ufb01ciality ( e.g. only liking men who are rich or attractive ) , promiscuity , and untrustworthiness .", "entities": []}, {"text": "An implicit example is \u2018 Girls love your money more than you \u2019 .", "entities": []}, {"text": "An explicit example is \u2018 My ex - girlfriend was a whore , she slept with every guy she saw \u2019 .", "entities": []}, {"text": "3.Sexual and/or physical limitations : making negative judgements of women \u2019s physical and/or sexual ability .", "entities": []}, {"text": "This includes perceived unattractiveness ( i.e. a lack of sexual desirability ) , ugliness ( i.e. a lack of beauty ) , frigidness ( i.e. a lack of sexual willingness ) , as well as belittling statements about feminine physical weakness .", "entities": []}, {"text": "An implicit example is \u2018 I gave it my A - game but she would not give in , so uptight ! \u2019", "entities": []}, {"text": "An explicit example is \u2018 Yikes , Dianne Abbott looks like a monkey ! \u2019", "entities": []}, {"text": "4.Other : content that is not covered by the other subcategories but is derogatory towards women .", "entities": []}, {"text": "4.1.4 Gendered personal attacks Gender personal attacks are highly gendered attacks and insults .", "entities": []}, {"text": "This category is used only when the nature of the abuse is misogynistic , e.g. \u2018 Hilary Clinton is such a stupid bitch , someone should give her a good fucking and put her in her place \u2019 .", "entities": []}, {"text": "The category has a level three \ufb02ag for the gender of the recipient of the abuse .", "entities": []}, {"text": "We include this \ufb02ag as research has shown that men can also be targeted by misogynistic attacks ( Jane , 2014 ) .", "entities": []}, {"text": "The gender can either be a woman ( e.g. \u2018 That chick is dumb \u2019 ) , a man ( e.g. \u2018 This dude is a piece of shit \u2019 ) or unknown ( e.g. \u2018 You \u2019re are an idiot , fuck off \u2019 ) .", "entities": []}, {"text": "If the content was replying to an entry which reveals the recipient \u2019s gender we can infer it from this context .", "entities": []}, {"text": "For example if \u2018 You \u2019re an idiot , fuck off \u2019 was a response to \u2018 I \u2019m a man and a feminist there \u2019s nothing contradictory about that \u2019 we know the abuse is targeted at a man .", "entities": []}, {"text": "13404.2 Non - misogynistic content Non - misogynistic content can fall in to three nonmutually exclusive categories , all of which are relevant for misogyny research .", "entities": []}, {"text": "4.2.1 Non - misogynistic personal attacks Interpersonal abuse which is notmisogynistic .", "entities": []}, {"text": "We include this category to allow for a comparison of the nature of abuse directed at women and men ( Duggan , 2017 ) .", "entities": []}, {"text": "It includes content which personally attacks a woman but is not misogynistic in nature , e.g. \u2018 Hilary Clinton has no clue what she \u2019s talking about , idiot ! \u2019 .", "entities": []}, {"text": "It uses the same level three \ufb02ag for the gender of the recipient as Misogynistic personal attack .", "entities": []}, {"text": "This allows us to compare the rates of personal attacks against women and men .", "entities": []}, {"text": "Note that although it is possible for an entry to contain both Misogyny and a Non - misogynistic personal attack , this was very rare .", "entities": []}, {"text": "In such cases , we chose to not annotate the Non - misogynistic personal attack in order to keep the \ufb01rst level as a binary distinction .", "entities": []}, {"text": "4.2.2 Counter speech Counter speech is content which challenges , refutes , and puts into question previous misogynistic abuse in a thread .", "entities": []}, {"text": "It could directly criticise previous abuse ( e.g. \u2018 What you said is unacceptable \u2019 ) , speci\ufb01cally accuse it of prejudice ( e.g. \u2018 That \u2019s incredibly sexist \u2019 ) , or offer a different perspective which challenges the misogyny ( e.g \u2018 That \u2019s not how women act , you \u2019re so wrong \u2019 ) .", "entities": []}, {"text": "4.2.3", "entities": []}, {"text": "None of the categories Content which does not contain misogynistic abuse , pejoratives , or related counter speech as de\ufb01ned in the previous categories .", "entities": []}, {"text": "This content is often not related to abuse or to women in general .", "entities": []}, {"text": "That said , it can include other forms of abusive language which are not misogynistic .", "entities": [[9, 11, "TaskName", "abusive language"]]}, {"text": "5 Annotation Methodology A key dif\ufb01culty in the formation of abusive language training datasets is producing high quality annotations .", "entities": [[10, 12, "TaskName", "abusive language"]]}, {"text": "Several factors affect this .", "entities": []}, {"text": "Deciding between similar categories , such as \u2018 hate speech \u2019 versus \u2018 offensive language \u2019 can be dif\ufb01cult ( Waseem et al . , 2017 ) .", "entities": [[8, 10, "DatasetName", "hate speech"]]}, {"text": "Determining the right category often requires close scrutiny and sustained critical thinking from annotators .", "entities": []}, {"text": "Annotators may face information overload if asked to work with too manycategories , both in terms of breadth ( e.g. annotating for different types of abuse ) and depth ( e.g. working with numerous subcategories ) .", "entities": []}, {"text": "Further , annotators may have different values and experiences and so make different assessments of the content they observe , especially when context plays a large role .", "entities": []}, {"text": "Annotators will also have unconscious social biases which may mean they interpret coding instructions differently to each other , and to how they were intended by the research authors .", "entities": []}, {"text": "For instance , Davidson et al .", "entities": []}, {"text": "( 2017 ) found that crowdsourced annotators were more likely to label sexist content as merely \u2018 offensive \u2019 while racist and homophobic content was considered \u2018 hate speech \u2019 .", "entities": [[27, 29, "DatasetName", "hate speech"]]}, {"text": "To mitigate such annotator biases , we used expert annotators speci\ufb01cally trained in identifying misogynistic content , as well as a group - based facilitation process to decide \ufb01nal labels .", "entities": []}, {"text": "Due to time and resource constraints , the \ufb01nal dataset is smaller than if we had used crowdsourced workers but captures more nuanced and detailed cases of misogyny .", "entities": []}, {"text": "Six annotators worked on the dataset .", "entities": []}, {"text": "Annotators were trained in the use of a codebook detailing the taxonomy and annotation guidelines .", "entities": []}, {"text": "The codebook was updated over time based on feedback from the annotators .", "entities": []}, {"text": "Demographic information on the annotators is available in Appendix A.2 5.1 Annotation process and disagreements Annotators independently marked up each entry for the three levels presented in Section 4 .", "entities": []}, {"text": "For all level two categories other than \u2018 None \u2019 , they also highlighted the speci\ufb01c part of the entry which was relevant to the labelled category ( the \u2018 span \u2019 ) .", "entities": []}, {"text": "This is particularly important information for long posts which can contain multiple forms of abuse .", "entities": []}, {"text": "Each entry was annotated by either two ( 43 % ) or three ( 57 % ) annotators .", "entities": []}, {"text": "If all annotators made the exact same annotation ( including all three levels and highlighting ) this was accepted as the \ufb01nal annotation .", "entities": []}, {"text": "All other entries were \ufb02agged as disagreements .", "entities": []}, {"text": "Annotators reviewed the disagreements in weekly meetings which were overseen by an expert facilitator , a PhD researcher who had developed the annotation taxonomy and was familiar with the literature on online misogyny and hate speech classi\ufb01cation .", "entities": [[34, 36, "DatasetName", "hate speech"]]}, {"text": "The role of the facilitator was to promote discussion between annotators and ensure the \ufb01nal labels re\ufb02ected the taxonomy .", "entities": []}, {"text": "Each disagreement was discussed until the annotators reached a consensus on the \ufb01nal agreed label or", "entities": []}, {"text": "1341labels .", "entities": []}, {"text": "5.2 Inter - annotator reliability For the level one binary task the Fleiss \u2019 Kappa is 0.484 and the Krippendorf \u2019s alpha is 0.487 .", "entities": [[21, 22, "HyperparameterName", "alpha"]]}, {"text": "By conventional NLP standards these results appear low .", "entities": []}, {"text": "However they are equivalent to , or above , those of existing abusive content datasets .", "entities": []}, {"text": "Sanguinetti et al .", "entities": []}, {"text": "( 2018 ) report category - wise Kappas from k=0.37 foroffence to k=0.54 for hate .", "entities": []}, {"text": "Gomez et al .", "entities": []}, {"text": "( 2020 ) have a Kappa of 0.15 in the \u201c MMH150 \u201d dataset of hateful memes .", "entities": [[15, 17, "DatasetName", "hateful memes"]]}, {"text": "Fortuna and Nunes ( 2018 ) report a Kappa of 0.17 for a text - only task .", "entities": []}, {"text": "Krippendorf \u2019s alpha is similar to the 0.45 reported by Wulczyn et al .", "entities": [[2, 3, "HyperparameterName", "alpha"]]}, {"text": "( 2017 ) .", "entities": []}, {"text": "We also calculated level two category - wise Fleiss \u2019 Kappas for each of the 17 sets of annotator groups , then took the mean across all groups ( Ravenscroft et al . , 2016 ) .", "entities": []}, {"text": "Table 1 shows the breakdown of Kappas per category .", "entities": []}, {"text": "There was greatest agreement for Misogynistic pejoratives ( k=0.559 ) down to the lowest agreement for Misogynistic personal attacks ( k=0.145 ) .", "entities": []}, {"text": "Category Fleiss \u2019 Kappa Mis .", "entities": []}, {"text": "Pejoratives 0.559", "entities": []}, {"text": "Mis .", "entities": []}, {"text": "Treatment 0.210 Mis .", "entities": []}, {"text": "Derogation 0.364", "entities": []}, {"text": "Mis .", "entities": []}, {"text": "Personal attack 0.145 Nonmis .", "entities": []}, {"text": "Personal attack 0.239 Counter speech 0.179 None of the categories 0.485 Table 1 : Category - wise Fleiss \u2019 Kappa Our taxonomy has seven partially overlapping categories , and as such annotation is considerably more dif\ufb01cult compared with most prior work , which tends to involve only binary labelling .", "entities": []}, {"text": "As such , whilst slightly low , we believe that our agreement scores show the robustness of our annotation approach .", "entities": []}, {"text": "Further , all disagreements were then discussed with an expert adjudicator , meaning that points of disagreement were addressed before the \ufb01nal labels were determined .", "entities": []}, {"text": "6 Prevalence of the categories Of the 6,567 agreed labels in the \ufb01nal dataset 10.6 % are Misogynistic ( n=699 ) and 89.4 % are Non - misogynistic ( n=5,868 ) .", "entities": []}, {"text": "Tables 2 and 3 show the number of labels in the \ufb01nal dataset for each ofthe Misogynistic and Non - misogynistic categories , broken down by the level two categories .", "entities": []}, {"text": "The vast majority of entries fall under None of the categories ( 88.6 % of all labels ) .", "entities": []}, {"text": "The next most common category is Misogynistic Pejoratives followed by Misogynistic Derogation pejoratives ( 4.2 % ) .", "entities": []}, {"text": "There are relatively few labels for Personal attacks with just 0.7 % in total for each of the Misogynistic and Nonmisogynistic categories , respectively .", "entities": []}, {"text": "The least common category is Counter speech against misogyny , with only ten cases ( 0.2 % ) .", "entities": []}, {"text": "Category Number Total % Pejorative 276 4.2 % Treatment 103 1.6 % Derogation 285 4.3 % Personal attack 35 0.7 % Total 696 10.6 % Table 2 : Breakdown of Misogynistic category counts Category Number Total % Personal attack 43 0.7 % Counter speech 10 0.2 % None 5815 88.6 % Total 5868 89.4 % Table 3 : Breakdown of Non - misogynistic category counts 6.1 Misogynistic pejoratives Annotators identi\ufb01ed at least one misogynistic pejorative in 4.2 % of all entries .", "entities": []}, {"text": "The most common misogynistic term in the labels is \u2018 bitch \u2019 ( n=43 ) followed by \u2018 stacy \u2019 ( 24 ) and \u2018 stacies \u2019 ( 21 ) .", "entities": []}, {"text": "6.2 Misogynistic treatment There are 103 labels of Treatment .", "entities": []}, {"text": "Figure 1 shows the number of labels for each level three subcategory .", "entities": []}, {"text": "There are almost \ufb01ve times as many labels forDisrespectful actions ( n=85 ) than Threatening language ( n=18 ) .", "entities": []}, {"text": "Both level three subcategories were broken down into more speci\ufb01c misogynistic themes .", "entities": []}, {"text": "Within Disrespectful actions , Seduction and conquest is the most common topic , with twice as many labels as the second most common , Controlling ( 43 vs 17 ) .", "entities": []}, {"text": "And , within Threatening language , Physical violence was the most common theme ( 13 ) while", "entities": []}, {"text": "1342Treatment ( 103 ) Threatening ( 18 ) Physical violence ( 13)Sexual violence ( 3)Privacy ( 2)Disrespectful actions ( 85 ) Controlling ( 17)Manipulation ( 16)Seduction and conquest ( 43)Other ( 9 ) Figure 1 : Prevalence of Misogynistic Treatment subcategories Sexual violence andInvasion of privacy only have a couple of labels each ( three and two , respectively ) .", "entities": []}, {"text": "6.3 Misogynistic derogation The are 286 Derogation labels .", "entities": []}, {"text": "Table 4 shows the number of labels for each subcategory within Derogation .", "entities": []}, {"text": "The counts are broken down by the strength of the abuse ( i.e. implicit / explicit ) .Implicit", "entities": []}, {"text": "derogation is almost twice as common as explicit ( 182 vs 103 ) .", "entities": []}, {"text": "The most common subcategory is Moral inferiority which accounts for 51 % of implicit and 54 % of explicit Derogation .Intellectual", "entities": []}, {"text": "inferioritythen has equal numbers of implicit andexplicit labels ( n=16 ) .", "entities": []}, {"text": "Subcategory Implicit Explicit Moral infer .", "entities": []}, {"text": "92 56 Intellectual infer .", "entities": []}, {"text": "16 16 Sexual & physical lim .", "entities": []}, {"text": "14 12 Other 60 19 Total 182 103 Table 4 : Breakdown of Misogynistic Derogation subcategories by implicit and explicit strength 6.4 Personal attacks Table 5 shows the breakdown of both Misogynistic andNonmisogynistic personal attacks .", "entities": []}, {"text": "Slightly more than half ( 55 % ) of interpersonal abuse was notmisogynistic .", "entities": []}, {"text": "Of these women were still the target of the abuse almost four times as often as men ( n=32 vs n=8 ) .", "entities": []}, {"text": "And women were as likely to receive misogynistic person attacks as non - misogynistic ones ( n=32).Gender Misog .", "entities": []}, {"text": "Nonmis .", "entities": []}, {"text": "Total Woman 32 32 64 ( 82 % ) Man 2 8 10 ( 13 % )", "entities": []}, {"text": "Unknown 1 3 4 ( 5 % )", "entities": []}, {"text": "Total 35 ( 45 % ) 43 ( 55 % ) 78 Table 5 : Breakdown of Misogynistic andNonmisogynistic personal attacks by Gender of the target The gender of the target was only unknown in 5 % of cases , one misogynistic and three not .", "entities": []}, {"text": "There were two cases of misogynistic abuse against men .", "entities": []}, {"text": "All other misogynistic personal attacks were towards women .", "entities": []}, {"text": "6.5 Counter speech There are only 10 cases of Counter speech in the \ufb01nal dataset of agreed labels .", "entities": []}, {"text": "Annotators originally identi\ufb01ed far more counter speech ( 188 labels for 149 unique entries were initially made ) but few were accepted during the adjudication meetings .", "entities": []}, {"text": "In Section 5.2 we showed that the category has one of the lowest Kappa values .", "entities": []}, {"text": "Notably , 39 % of original Counter speech labels were made by one annotator , showing that the annotators had different understandings of the threshold for Counter speech .", "entities": []}, {"text": "However , the number of original labels forCounter speech decreased over the \ufb01rst few weeks of the annotation process , as shown in Figure 2 .", "entities": []}, {"text": "This re\ufb02ects the complexity of the category ; it took annotators time to differentiate content that was pro - women from that which actually countered previous misogynistic speech .", "entities": []}, {"text": "1343 Figure 2 : Percentage of original counter speech labels by week 7 Experiments As reference points for further research using our dataset , we provide three experimental baselines on the binary task of distinguishing between misogynistic and non - misogynistic content , i.e. level one of our taxonomy .", "entities": []}, {"text": "As the simplest baseline , we evaluate a logistic unigram classi\ufb01er .", "entities": []}, {"text": "Further , we evaluate two uncased BERT - base models ( Devlin et al . , 2019 ) \u2013 one unweighted , the other using class weights emphasising the minority class , i.e. misogynistic content , to account for class imbalance .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "For all models , we use the same strati\ufb01ed 80/20 train / test split of the dataset .", "entities": []}, {"text": "Details on model training and parameters can be found in Appendix C. Performance of the three models is shown in Table 6 .", "entities": []}, {"text": "All models perform poorly on misogynistic content , with the logistic classi\ufb01er performing worst overall .", "entities": []}, {"text": "The logistic classi\ufb01er has the highest precision on misogynistic content ( 0.88 ) but very low recall ( 0.07 ) and a low F1 score ( 0.13 ) .", "entities": [[23, 25, "MetricName", "F1 score"]]}, {"text": "The weighted BERT model has the highest recall ( 0.50 ) and F1 score ( 0.43 ) .", "entities": [[2, 3, "MethodName", "BERT"], [12, 14, "MetricName", "F1 score"]]}, {"text": "Accuracy on all test cases , of which 91.9 % are non - misogynistic , is around 0.90 across models .", "entities": [[0, 1, "MetricName", "Accuracy"]]}, {"text": "The classi\ufb01cation task is complicated by the relatively small size of our dataset ( n=6,385 unique cases ) as well as the relatively small proportion of misogynistic cases in it ( 8.1 % ) .", "entities": []}, {"text": "These issues are common in abusive speech detection ( Fortuna and Nunes , 2018 ; Fortuna et al . , 2020 ) .", "entities": []}, {"text": "To address them , future research can leverage the typology and annotation process we introduced to collect additional cases , particularly misogynistic ones , thus growing and balancing the dataset.7.1 Error analysis We make use of the more granular secondary labels in our taxonomy to conduct an error analysis for the weighted BERT model .", "entities": [[30, 31, "MetricName", "Error"], [52, 53, "MethodName", "BERT"]]}, {"text": "Table 7 shows the confusion matrix for the 1,277 entries in the test set .", "entities": []}, {"text": "Overall , 137 entries ( 10.7 % ) were misclassi\ufb01ed .", "entities": []}, {"text": "86 ( 63 % ) of these errors were false positives and 51 ( 37 % ) were false negatives .", "entities": []}, {"text": "7.1.1 False positives Of the 86 entries misclassi\ufb01ed as Misogynistic , three are labelled as Nonmisogynistic personal attacks .", "entities": []}, {"text": "An example is \u201c Male feminist reacts to vile scumbag who murdered his kids by telling ALL men to , you know , stop killing women and kids ... \u201d .", "entities": []}, {"text": "The use of aggressive language combined with gendered phrases ( such as \u201c stop killing women \u201d ) likely led to its misclassi\ufb01cation .", "entities": []}, {"text": "The remaining 83 false positives fall under None of the categories and all contain some reference to women .", "entities": []}, {"text": "Some refer to topics often associated with misogyny but are not misogynistic in themselves .", "entities": []}, {"text": "For example , a comment in r / seduction stated , \u201c the most manly thing is to \ufb01nd your dream woman , marry her , and live happily ever after .", "entities": []}, {"text": "The constant sex with women is so overrated anyways \u201d .", "entities": []}, {"text": "This entry , which suggests that other things than high levels of sexual activity should be prioritised , is thematically similar to misogynistic content in the dataset .", "entities": []}, {"text": "Other false positives mention women indirectly .", "entities": []}, {"text": "\u201c Because they are n\u2019t men , they are SIMPS \u201d .", "entities": []}, {"text": "\u2018 Simp \u2019 is a pejorative term used in the manosphere for a man who cares too much about a woman .", "entities": []}, {"text": "Under our taxonomy it did not count as a misogynistic pejorative but it is likely that the term appears in misogynistic entries in the dataset .", "entities": []}, {"text": "Some false positives are critical of misogyny , though not actively enough to count as Counter speech .", "entities": []}, {"text": "For example \u201c Does this moid even know the meaning of the term \u2018 butterface \u2019 ?", "entities": []}, {"text": "If this woman is ugly , there is no hope for most of the female population . \u201d .", "entities": []}, {"text": "This discussion of unrealistic beauty standards of women references misogyny but is not itself misogynistic .", "entities": []}, {"text": "7.1.2 False negatives Of the 51 Misogynistic entries the model misses , almost half ( n=24 ) contain Derogation .", "entities": []}, {"text": "Implicit and explicit derogation are missed at roughly similar rates , as are each of the subcategories .", "entities": []}, {"text": "Importantly this shows that the different forms of derogation are no more or less likely to be missed .", "entities": []}, {"text": "1344Model Precision Recall F1 score Accuracy Logistic regression 0.88 0.07 0.13 0.92 BERT ( unweighted ) 0.67 0.30 0.42 0.93 BERT ( weighted ) 0.38 0.50 0.43 0.89 Table 6 : Model performance on misogynistic test cases ( n=103 ) and accuracy on all test cases ( n=1,277 ) .", "entities": [[1, 2, "MetricName", "Precision"], [2, 3, "MetricName", "Recall"], [3, 5, "MetricName", "F1 score"], [5, 6, "MetricName", "Accuracy"], [6, 8, "MethodName", "Logistic regression"], [12, 13, "MethodName", "BERT"], [20, 21, "MethodName", "BERT"], [41, 42, "MetricName", "accuracy"]]}, {"text": "Prediction Nonmis .", "entities": []}, {"text": "Mis . TOTAL LabelNonmis .", "entities": []}, {"text": "1,088 86 1,174 Mis . 51 52 103 TOTAL 1,139 138 1,277 Table 7 : Confusion matrix of the weighted BERT model In many cases , the derogation depends on the context of the earlier conversation thread , thus the BERT - model , which does not explicitly take into account prior entries in the thread , can not recognise the misogyny in isolation .", "entities": [[20, 21, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"]]}, {"text": "\u201c It \u2019s funny to see the hamster that starts to act up in their little widdle tiny brains after saying that too . \u201d is an explicit statement that women are intellectually inferior , but understanding that it refers to women depends on having seen previous entries in the conversation .", "entities": []}, {"text": "The next most common source of false negatives is Pejoratives ( n=19 ) .", "entities": []}, {"text": "The classi\ufb01er misses six counts each of \u2018 whore \u2019 and \u2018 stacy \u2019 and \ufb01ve of \u2018 bitch \u2019 .", "entities": []}, {"text": "There are seven missed labels for Treatment , \ufb01veDisrespectful actions and two Threatening language .", "entities": []}, {"text": "However , due to the low prevalence of the category in the training data we anticipate some errors .", "entities": []}, {"text": "For example , \u201c I am waiting for such incident to happen to me so that I can beat the shit out of her , and of course it will be all revenge \u201d details a speci\ufb01c form of violence ( i.e. \u2018 beat the shit out of her \u2019 ) which the model can not know to identify as misogyny without being trained on other uses of the term .", "entities": []}, {"text": "The \ufb01nal two errors are for Personal attacks .", "entities": []}, {"text": "For example , \u201c Yeah there s women that I as an Incel would nt even acknowledge and this is one of em", "entities": []}, {"text": "[ sic ] \u201d .", "entities": []}, {"text": "This is an implicit attack which requires understanding that considering a woman unworthy of the attention of an incel is a gendered insult .", "entities": []}, {"text": "As we can see from these examples the main classi\ufb01cation errors are due to context limitations .", "entities": []}, {"text": "For false negatives there is usually not enough infor - mation in the entry alone or in the training dataset to identify the misogyny .", "entities": []}, {"text": "Conversely , for false positives the classi\ufb01er appears to overly associate contentabout women with content that abuses women .", "entities": []}, {"text": "These limitations can be addressed by future work drawing on the taxonomy and annotation process presented here to develop larger datasets which can cover a greater range of forms of discourse , including both non - misogynistic discussions of women and a wider variety of misogynistic speech .", "entities": []}, {"text": "8 Conclusion In this paper we have presented a hierarchical granular taxonomy for misogyny and have described a dataset containing high quality , expert labels of misogynistic content from Reddit .", "entities": [[29, 30, "DatasetName", "Reddit"]]}, {"text": "We have also provided the detailed coding book we created and a dataset with all of the original labels .", "entities": []}, {"text": "The \ufb01nal dataset is small compared to other annotated datasets used for classi\ufb01cation .", "entities": []}, {"text": "However it bene\ufb01ts from a detailed taxonomy based on the existing literature focused on just one form of online abuse - misogyny .", "entities": []}, {"text": "The use of trained annotators and an adjudication process also ensures the quality of the labels .", "entities": []}, {"text": "The more granular subcategories in the taxonomy may be too small to classify separately , but they provide insights into the relative frequency of different forms of misogynistic content on Reddit and enable detailed error analysis .", "entities": [[30, 31, "DatasetName", "Reddit"]]}, {"text": "They are also useful for other researchers aiming to create larger datasets , who can build on the taxonomic work conducted here .", "entities": []}, {"text": "Acknowledgments This research was funded by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP / T001569/1 at The Alan Turing Institute , particularly the \u201c Tools , Practices and System \u201d and \u201c Criminal Justice \u201d themes .", "entities": []}, {"text": "1345References Maria Anzovino , Elisabetta Fersini , and Paolo Rosso .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Automatic Identi\ufb01cation and Classi\ufb01cation of Misogynistic Language on Twitter .", "entities": []}, {"text": "In Natural Language Processing and Information Systems , Lecture Notes in Computer Science , pages 57\u201364 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Emily M. Bender and Batya Friedman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Data Statements for Natural Language Processing : Toward Mitigating System Bias and Enabling Better Science .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 6:587\u2013604 .", "entities": []}, {"text": "Bryce Boe . 2020 .", "entities": []}, {"text": "PRAW .", "entities": []}, {"text": "Python Reddit API Wrapper Development .", "entities": [[1, 2, "DatasetName", "Reddit"]]}, {"text": "Thomas Davidson , Dana Warmsley , Michael Macy , and Ingmar Weber . 2017 .", "entities": []}, {"text": "Automated Hate Speech Detection and the Problem of Offensive Language .", "entities": [[1, 4, "TaskName", "Hate Speech Detection"]]}, {"text": "InProceedings of the Eleventh International AAAI Conference on Web and Social Media ( ICWSM 2017 ) , page 4 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Maeve Duggan . 2017 .", "entities": []}, {"text": "Online Harassment 2017 .", "entities": []}, {"text": "Technical report , Pew Research Center .", "entities": []}, {"text": "Tracie Farrell , Miriam Fernandez , Jakub Novotny , and Harith Alani .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Exploring Misogyny across the Manosphere in Reddit .", "entities": [[6, 7, "DatasetName", "Reddit"]]}, {"text": "In WebSci \u2019 19 Proceedings of the 10th ACM Conference on Web Science , pages 87\u201396 , Boston .", "entities": [[8, 9, "DatasetName", "ACM"]]}, {"text": "E Fersini , P Rosso , and M Anzovino .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Overview of the Task on Automatic Misogyny Identi\ufb01cation at IberEval 2018 .", "entities": []}, {"text": "In Proceedings of the Third Workshop on Evaluation of Human Language Technologies for Iberian Languages ( IberEval 2018 ) 215 , page 15 .", "entities": []}, {"text": "Jill Filipovic .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Blogging while female : How internet misogyny parallels real - world harassment .", "entities": []}, {"text": "Yale JL & Feminism , 19:295 .", "entities": []}, {"text": "Paula Fortuna and S \u00b4 ergio Nunes .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A Survey on Automatic Detection of Hate Speech in Text .", "entities": [[6, 8, "DatasetName", "Hate Speech"]]}, {"text": "ACM Computing Surveys , 51(4):85:1\u201385:30 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Paula Fortuna , Juan Soler , and Leo Wanner .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Toxic , hateful , offensive or abusive ?", "entities": []}, {"text": "what are we really classifying ?", "entities": []}, {"text": "an empirical analysis of hate speech datasets .", "entities": [[4, 6, "DatasetName", "hate speech"]]}, {"text": "In Proceedings of The 12th Language Resources and Evaluation Conference , pages 6786\u20136794.Jerome", "entities": []}, {"text": "Friedman , Trevor Hastie , and Robert Tibshirani .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Regularization paths for generalized linear models via coordinate descent .", "entities": []}, {"text": "Journal of Statistical Software , 33(1):1\u201322 .", "entities": []}, {"text": "Debbie Ging . 2017 .", "entities": []}, {"text": "Alphas , Betas , and Incels : Theorizing the Masculinities of the Manosphere .", "entities": []}, {"text": "Men and Masculinities , 1:20 .", "entities": []}, {"text": "Debbie Ging , Theodore Lynn , and Pierangelo Rosati .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Neologising misogyny : Urban Dictionary \u2019s folksonomies of sexual abuse : .", "entities": []}, {"text": "New Media & Society .", "entities": []}, {"text": "Debbie Ging and Eugenia Siapera .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Special issue on online misogyny .", "entities": []}, {"text": "Feminist Media Studies , 18(4):515\u2013524 .", "entities": []}, {"text": "Debbie Ging and Eugenia Siapera , editors .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Gender Hate Online :", "entities": []}, {"text": "Understanding the New AntiFeminism .", "entities": []}, {"text": "Springer International Publishing , Cham .", "entities": []}, {"text": "Peter Glick and Susan T. Fiske .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Hostile and Benevolent Sexism .", "entities": []}, {"text": "Psychology of Women Quarterly , 21(1):119\u2013135 .", "entities": []}, {"text": "Raul Gomez , Jaume Gibert , Lluis Gomez , and Dimosthenis Karatzas .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Exploring Hate Speech Detection in Multimodal Publications .", "entities": [[1, 4, "TaskName", "Hate Speech Detection"]]}, {"text": "In 2020 IEEE Winter Conference on Applications of Computer Vision ( WACV ) , pages 1459\u20131467 , Snowmass Village , CO , USA .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Sarah Hewitt , T. Tiropanis , and C. Bokhove .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "The Problem of Identifying Misogynist Language on Twitter ( and Other Online Social Spaces ) .", "entities": [[13, 14, "DatasetName", "Spaces"]]}, {"text": "In Proceedings of the 8th ACM Conference on Web Science , WebSci \u2019 16 , pages 333\u2013335 , New York , NY , USA . ACM .", "entities": [[5, 6, "DatasetName", "ACM"], [25, 26, "DatasetName", "ACM"]]}, {"text": "Amnesty International . 2017 .", "entities": []}, {"text": "Amnesty reveals alarming impact of online abuse against women .", "entities": []}, {"text": "https://www.amnesty.org/en/latest/news/2017/11/amnestyreveals-alarming-impact-of-online-abuse-againstwomen/. Emma A. Jane .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Misogyny Online : A Short ( and Brutish ) History .", "entities": []}, {"text": "SAGE .", "entities": []}, {"text": "Emma Alice Jane .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "\u2018 Back to the kitchen , cunt \u2019 : Speaking the unspeakable about online misogyny .", "entities": []}, {"text": "Continuum , 28(4):558\u2013570 .", "entities": []}, {"text": "Rebecca Jennings .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Incels Categorize Women by Personal Style and Attractiveness .", "entities": []}, {"text": "https://www.vox.com/2018/4/28/17290256/incelchad-stacy-becky .", "entities": []}, {"text": "Akshita Jha and Radhika Mamidi .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "When does a compliment become sexist ?", "entities": []}, {"text": "Analysis and classi\ufb01cation of ambivalent sexism using twitter data .", "entities": []}, {"text": "In Proceedings of the Second Workshop on NLP and Computational Social Science , pages 7\u201316 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "1346David Jurgens , Libby Hemphill , and Eshwar Chandrasekharan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A Just and Comprehensive Strategy for Using NLP to Address Online Abuse .", "entities": []}, {"text": "InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3658\u20133666 , Florence , Italy .", "entities": [[16, 17, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ilya Loshchilov and Frank Hutter .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Decoupled weight decay regularization .", "entities": [[1, 3, "MethodName", "weight decay"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Theo Lynn , Patricia Takako Endo , Pierangelo Rosati , Ivanovitch Silva , Guto Leoni Santos , and Debbie Ging . 2019a .", "entities": []}, {"text": "A Comparison of Machine Learning Approaches for Detecting Misogynistic Speech in Urban Dictionary .", "entities": []}, {"text": "In 2019 International Conference on Cyber Situational Awareness , Data Analytics And Assessment ( Cyber SA ) , pages 1\u20138 .", "entities": []}, {"text": "Theo Lynn , Patricia Takako Endo , Pierangelo Rosati , Ivanovitch Silva , Guto Leoni Santos , and Debbie Ging . 2019b .", "entities": []}, {"text": "Data set for automatic detection of online misogynistic speech .", "entities": []}, {"text": "Data in Brief , 26:104223 .", "entities": []}, {"text": "Karla Mantilla .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Gendertrolling : Misogyny Adapts to New Media .", "entities": []}, {"text": "Feminist Studies , 39(2):563 \u2013 570 .", "entities": []}, {"text": "Adrienne Massanari . 2017 .", "entities": []}, {"text": "# Gamergate and The Fappening : How Reddit \u2019s algorithm , governance , and culture support toxic technocultures .", "entities": [[7, 8, "DatasetName", "Reddit"]]}, {"text": "New Media & Society , 19(3):329\u2013346 .", "entities": []}, {"text": "Debora Nozza , Claudia V olpetti , and Elisabetta Fersini .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unintended Bias in Misogyny Detection .", "entities": []}, {"text": "In IEEE / WIC / ACM International Conference on Web Intelligence on - WI \u2019 19 , pages 149\u2013155 , Thessaloniki , Greece .", "entities": [[3, 4, "DatasetName", "WIC"], [5, 6, "DatasetName", "ACM"]]}, {"text": "ACM Press .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Bailey Poland .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Haters : Harassment , Abuse , and Violence Online .", "entities": []}, {"text": "Potomac Books .", "entities": []}, {"text": "Jing Qian , Anna Bethke , Yinyin Liu , Elizabeth Belding , and William Yang Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A Benchmark Dataset for Learning to Intervene in Online Hate Speech .", "entities": [[9, 11, "DatasetName", "Hate Speech"]]}, {"text": "arXiv:1909.04251", "entities": []}, {"text": "[ cs ] .", "entities": []}, {"text": "James Ravenscroft , Anika Oellrich , Shyamasree Saha , and Maria Liakata .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Multi - label Annotation in Scienti\ufb01c Articles - The Multi - label Cancer Risk Assessment Corpus .", "entities": []}, {"text": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC\u201916 ) , pages 4115\u20134123 , Portoro \u02c7z , Slovenia .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Manoel Horta Ribeiro , Jeremy Blackburn , Barry Bradlyn , Emiliano De Cristofaro , Gianluca Stringhini , Summer Long , Stephanie Greenberg , and Savvas Zannettou . 2020 .", "entities": []}, {"text": "From Pick - Up Artists to Incels : A Data - Driven Sketch of the Manosphere .", "entities": [[12, 13, "DatasetName", "Sketch"]]}, {"text": "arXiv:2001.07600", "entities": []}, {"text": "[ cs ] .Manuela", "entities": []}, {"text": "Sanguinetti , Fabio Poletto , Cristina Bosco , Viviana Patti , and Marco Stranisci .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "An Italian Twitter Corpus of Hate Speech against Immigrants .", "entities": [[5, 7, "DatasetName", "Hate Speech"]]}, {"text": "In Proceedings of the Eleventh International Conference on Language Resources and Evaluation ( LREC 2018 ) , Miyazaki , Japan .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Maarten Sap , Dallas Card , Saadia Gabriel , Yejin Choi , and Noah A. Smith .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The Risk of Racial Bias in Hate Speech Detection .", "entities": [[6, 9, "TaskName", "Hate Speech Detection"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1668\u20131678 , Florence , Italy .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Anna Schmidt and Michael Wiegand .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A Survey on Hate Speech Detection using Natural Language Processing .", "entities": [[3, 6, "TaskName", "Hate Speech Detection"]]}, {"text": "In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media , pages 1\u201310 , Valencia , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Bertie Vidgen , Alex Harris , Dong Nguyen , Rebekah Tromble , Scott Hale , and Helen Margetts .", "entities": [[16, 17, "DatasetName", "Helen"]]}, {"text": "2019 .", "entities": []}, {"text": "Challenges and frontiers in abusive content detection .", "entities": []}, {"text": "In Proceedings of the Third Workshop on Abusive Language Online , pages 80\u201393 , Florence , Italy .", "entities": [[7, 9, "TaskName", "Abusive Language"], [14, 15, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zeerak Waseem , Thomas Davidson , Dana Warmsley , and Ingmar Weber .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Understanding Abuse : A Typology of Abusive Language Detection Subtasks .", "entities": [[6, 8, "TaskName", "Abusive Language"]]}, {"text": "In Proceedings of the First Workshop on Abusive Language Online , pages 78\u201384 , Vancouver , BC , Canada .", "entities": [[7, 9, "TaskName", "Abusive Language"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zeerak Waseem and Dirk Hovy .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Hateful symbols or hateful people ?", "entities": []}, {"text": "predictive features for hate speech detection on twitter .", "entities": [[3, 6, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of the NAACL Student Research Workshop , pages 88\u201393 .", "entities": []}, {"text": "Michael Wiegand , Josef Ruppenhofer , and Thomas Kleinbauer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Detection of Abusive Language : The Problem of Biased Datasets .", "entities": [[2, 4, "TaskName", "Abusive Language"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 602\u2013608 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 38\u201345 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ellery Wulczyn , Nithum Thain , and Lucas Dixon . 2017 .", "entities": []}, {"text": "Ex Machina : Personal Attacks Seen at Scale .", "entities": []}, {"text": "In", "entities": []}, {"text": "1347Proceedings of the 26th International Conference on World Wide Web , pages 1391\u20131399 , Perth Australia .", "entities": []}, {"text": "International World Wide Web Conferences Steering Committee .", "entities": []}, {"text": "Donna Zuckerberg .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Not All Dead White Men : Classics and Misogyny in the Digital Age .", "entities": []}, {"text": "Harvard University Press , Cambridge , Massachusetts .", "entities": [[4, 5, "DatasetName", "Cambridge"]]}, {"text": "A Short form data statement Following the recommendation of Bender and Friedman ( 2018 ) we include the following short form data statement to summarise the main features of the datasets .", "entities": []}, {"text": "Further details on the creation of the datasets are in in Sections 3 and 5 in the main paper .", "entities": []}, {"text": "A.1 Data", "entities": []}, {"text": "The two datasets include labels for 6,383 unique Reddit entries ( i.e. posts or comments ) across 672 conversation threads collected .", "entities": [[8, 9, "DatasetName", "Reddit"]]}, {"text": "One dataset is of the 15,816 original labels selected by annotators and the second is of the 6,567 agreed labels .", "entities": []}, {"text": "Table 8 provides a description of each of the variables in the datasets .", "entities": []}, {"text": "We also include the accompanying set of images associated with some original post entries .", "entities": []}, {"text": "All threads except one are in English .", "entities": []}, {"text": "The majority of threads were sampled from a set of 34 subreddits selected for the expected prevalence of misogynistic content , or non - misogynistic discussions about women .", "entities": []}, {"text": "Paid annotators received extensive training to apply the taxonomy presented in this paper to label entries .", "entities": []}, {"text": "The majority of annotators were White - British , spoke English as a \ufb01rst language , and had or were pursuing a University degree .", "entities": []}, {"text": "Two - thirds of annotators were women .", "entities": []}, {"text": "A.2 Annotators", "entities": []}, {"text": "All annotators were based in the United Kingdom and worked remotely .", "entities": []}, {"text": "They were paid \u00a3 14 per hour for all work including training .", "entities": []}, {"text": "Five of the six annotators gave permission to share their basic demographic information .", "entities": []}, {"text": "All were between 18 and 29 years old .", "entities": []}, {"text": "Two had high school degrees , two had an undergraduate degree , and one had a postgraduate taught degree or equivalent .", "entities": []}, {"text": "Four identi\ufb01ed as women , one as a man .", "entities": []}, {"text": "All were British nationals , native English speakers , and identi\ufb01ed as ethnically white .", "entities": []}, {"text": "All annotators used social media at least once per day .", "entities": []}, {"text": "Two had never been personally targeted by online abuse , two had been targeted 2 - 3 times(in separate instances more than a year ago ) , and one had been personally targeted more than 3 times within the previous month .", "entities": []}, {"text": "B Frequency of targeted subreddits Table 9 lists the subreddits used for target sampling of data .", "entities": []}, {"text": "The columns Num entries andNum threads state how many individual entries and threads from each subreddit are in the datasets .", "entities": []}, {"text": "The column Selection shows whether the subreddit was identi\ufb01ed from existing literature , which is cited , or using snowball sampling .", "entities": []}, {"text": "Post ( 1 ) Comment ( 1.1 ) Comment ( 1.1.1)Comment ( 1.1.2)Comment ( 1.2 ) Comment ( 1.2.1 ) Comment ( 1.2.1.1)Comment ( 1.2.1.2 ) Figure 3 : Tree diagram for comment order in threads C Model", "entities": []}, {"text": "Details Pre - Processing We lowercase all text and remove newline and tab characters .", "entities": []}, {"text": "URLs and emojis are replaced with [ URL ] and [ EMOJI ] tokens .", "entities": []}, {"text": "C.1 Logistic regression Logistic regression with l1 - regularisation is implemented in R using the \u2018 glmnet \u2019 package ( Friedman et al . , 2010 ) on a unigram representation of the data .", "entities": [[1, 3, "MethodName", "Logistic regression"], [3, 5, "MethodName", "Logistic regression"]]}, {"text": "Lambda is selected using cross - validation and set to 0.015 .", "entities": []}, {"text": "C.2 BERT Models ( weighted / unweighted ) Model Architecture", "entities": [[1, 2, "MethodName", "BERT"]]}, {"text": "We implement uncased BERT - base models ( Devlin et al . , 2019 ) using thetransformers Python library ( Wolf et al . , 2020 ) .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "For sequence classi\ufb01cation , we add a linear layer with softmax output .", "entities": [[7, 9, "MethodName", "linear layer"], [10, 11, "MethodName", "softmax"]]}, {"text": "1348Training Parameters We apply a strati\ufb01ed 80/20 train / test split to our dataset .", "entities": []}, {"text": "Models are trained for three epochs each .", "entities": []}, {"text": "Training batch size is 16 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "We use cross - entropy loss .", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "For the weighted model , we add class weights emphasising the minority class , i.e. misogynistic content .", "entities": []}, {"text": "Weights are set to the relative proportion of the other class in the training data , meaning that for a 1:9 misogynistic : non - misogynistic case split , loss on misogynistic cases would be multiplied by 9 .", "entities": [[29, 30, "MetricName", "loss"]]}, {"text": "The optimiser is AdamW ( Loshchilov and Hutter , 2018 ) with a 5e-5 learning rate and a 0.01 weight decay .", "entities": [[3, 4, "MethodName", "AdamW"], [14, 16, "HyperparameterName", "learning rate"], [19, 21, "MethodName", "weight decay"]]}, {"text": "For regularisation , we set a 10 % dropout probability .", "entities": []}, {"text": "1349Variable Description entry i", "entities": []}, {"text": "d A unique string assigned to every comment and post by Reddit .", "entities": [[11, 12, "DatasetName", "Reddit"]]}, {"text": "link i", "entities": []}, {"text": "d", "entities": []}, {"text": "The i d number of the original post of a thread .", "entities": []}, {"text": "parent i", "entities": []}, {"text": "d", "entities": []}, {"text": "The i d number of parent entry ( i.e. the post or comment this entry responds to ) .", "entities": []}, {"text": "subreddit The subreddit community where the entry was made .", "entities": []}, {"text": "author The Reddit username of the entry author .", "entities": [[2, 3, "DatasetName", "Reddit"]]}, {"text": "body The text body of the entry .", "entities": []}, {"text": "For the original posts of threads the title and post body were combined .", "entities": []}, {"text": "image Whether the entry has an accompanying image .", "entities": []}, {"text": "Only applicable to posts .", "entities": []}, {"text": "Images are provided as jpg \ufb01les .", "entities": []}, {"text": "They are named as \u2018 X YZ \u2019 corresponding to the week ( X ) , group ( Y ) , and thread i", "entities": []}, {"text": "d", "entities": []}, {"text": "( Z ) .", "entities": []}, {"text": "label date The week commencing date of when the entry was labelled .", "entities": []}, {"text": "week The week in the annotation process when the entry as assigned ( 1 to 11 ) .", "entities": []}, {"text": "group", "entities": []}, {"text": "The weekly group the entry was assigned to .", "entities": []}, {"text": "All weeks had two groups except week 7 which only had 1 . sheet order The order of the entry in the weekly annotation sheet .", "entities": []}, {"text": "This is a list of numbers referring to the nested structure of comments in threads .", "entities": []}, {"text": "It shows the i d number of each level of the thread from the original post to the relevant entry .", "entities": []}, {"text": "For example , if an entry has the sheet order ( 1 , 2 , 3 ) it belongs to the \ufb01rst thread ( 1 ) , and replied to the second comment ( 2 ) , to which it is the third reply ( 3 ) .", "entities": []}, {"text": "See Fig .", "entities": []}, {"text": "3 for visual explanation .", "entities": []}, {"text": "annotator i", "entities": []}, {"text": "d", "entities": []}, {"text": "The i d number of the annotator who made the annotation ( 1 to 6 ) .", "entities": []}, {"text": "Only applicable to the original labels dataset .", "entities": []}, {"text": "level 1", "entities": []}, {"text": "Whether the entry is Misogynistic orNonmisogynistic .", "entities": []}, {"text": "level 2", "entities": []}, {"text": "The category of the label ( i.e. Pejoratives , Derogation , etc . level 3 EITHER the subcategory for Derogation orTreatment OR the gender of the target for either Personal attack category .", "entities": []}, {"text": "Empty for all other categories .", "entities": []}, {"text": "strength Whether the abuse is implicit or explicit .", "entities": []}, {"text": "Only applicable to identity directed abuse .", "entities": []}, {"text": "highlight The highlighted part of the entry \u2019s body which contains the abuse .", "entities": []}, {"text": "Mandatory for all primary categories except \u2018 None \u2019 .", "entities": []}, {"text": "split Whether the entry was included in the \u2018 train \u2019 or \u2018 test \u2019 dataset split for model building .", "entities": []}, {"text": "Only applicable to the \ufb01nal labels dataset .", "entities": []}, {"text": "Table 8 : Description of dataset variables", "entities": []}, {"text": "1350Subreddit Num entries Num threads Selection altTRP 2 1 Snowball AskFeminists 263 26 Snowball askseddit 142 16 Snowball badwomensanatomy 430 31 Farrell et al .", "entities": []}, {"text": "( 2019 ) becomeaman 2 1 Snowball Egalitarianism 115 15 Snowball exredpill 113 12 Snowball FeMRADebates 195 20 Snowball GEOTRP 11 1 Snowball IncelsInAction 110 14 Farrell et al .", "entities": []}, {"text": "( 2019 )", "entities": []}, {"text": "IncelsWithoutHate 325 28 Farrell et al .", "entities": []}, {"text": "( 2019 ) KotakuInAction 373 28 Qian et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) ; Zuckerberg ( 2018 ) marriedredpill 87 7 Snowball masculism 34 5", "entities": []}, {"text": "Snowball MensRants 4 1 Ging ( 2017 ) MensRights 364 29 Ging ( 2017 ) ;", "entities": []}, {"text": "Qian et al . ( 2019 ) ; Zuckerberg ( 2018 ) mensrightslaw 2 1 Snowball MensRightsMeta 4 1 Snowball MGTOW 601 41 Farrell et al .", "entities": []}, {"text": "( 2019 ) ; Ging ( 2017 ) ;", "entities": []}, {"text": "Qian et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) ; Zuckerberg ( 2018 ) mgtowbooks 2 1 Snowball MRActivism 8 2 Snowball NOMAAM 2 1 Snowball pua 10 1 Snowball PurplePillDebate 221 21 Snowball PussyPass 344 33 Qian et al .", "entities": []}, {"text": "( 2019 ) pussypassdenied 262 22 Qian et al .", "entities": []}, {"text": "( 2019 ) RedPillParenting 12 2 Snowball RedPillWives 61 8 Snowball RedPillWomen", "entities": []}, {"text": "217 23 Snowball seduction 392 33 Zuckerberg ( 2018 ) ThankTRP 8 1 Snowball TheRedPill 338 29 Ging ( 2017 ) ; Zuckerberg ( 2018 ) theredpillright 10 1 Snowball Trufemcels 434 37 Farrell et al .", "entities": []}, {"text": "( 2019 ) Table 9 : Number of entries and threads per targeted subreddit", "entities": []}]