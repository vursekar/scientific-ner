[{"text": "Proceedings of the 5th Conference on Machine Translation ( WMT ) , pages 1144\u20131149 Online , November 19\u201320 , 2020 .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics1144Adobe AMPS \u2019s Submission for Very Low Resource Supervised Translation Task at WMT20 Keshaw Singh AI / ML Platform & Solutions , Adobe Inc. Bengaluru , India kessingh@adobe.com", "entities": [[13, 14, "TaskName", "Translation"]]}, {"text": "Abstract", "entities": []}, {"text": "In this paper , we describe our systems submitted to the very low resource supervised translation task at WMT20 .", "entities": []}, {"text": "We participate in both translation directions for Upper SorbianGerman language pair .", "entities": []}, {"text": "Our primary submission is a subword - level Transformer - based neural machine translation model trained on original training bitext .", "entities": [[8, 9, "MethodName", "Transformer"], [12, 14, "TaskName", "machine translation"]]}, {"text": "We also conduct several experiments with backtranslation using limited monolingual data in our postsubmission work and include our results for the same .", "entities": []}, {"text": "In one such experiment , we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic , backtranslated corpus followed by \ufb01ne - tuning on the original parallel training data .", "entities": [[12, 13, "MetricName", "BLEU"]]}, {"text": "1 Introduction This paper describes our submissions to the shared task on Very Low Resource Supervised Machine Translation at WMT 2020 .", "entities": [[16, 18, "TaskName", "Machine Translation"], [19, 21, "DatasetName", "WMT 2020"]]}, {"text": "The task involved a single language pair : Upper Sorbian - German .", "entities": []}, {"text": "We submit supervised neural machine translation ( NMT ) systems for both translation directions , Upper Sorbian !", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "German and German !", "entities": []}, {"text": "Upper Sorbian .", "entities": []}, {"text": "NMT models ( Sutskever et al . , 2014 ; Bahdanau et al . , 2015 ; Cho et al . , 2014a ) have achieved stateof - the - art performance on benchmark datasets for multiple language pairs .", "entities": []}, {"text": "A big advantage of such systems over phrase - based statistical machine translation ( PBSMT ) ( Koehn et al . , 2003 ) models is that they can be trained end - to - end .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "The bulk of the development , however , has been limited to a handful of high - resource language pairs .", "entities": []}, {"text": "The primary reason is that training a well - performing NMT system requires a large amount of parallel training data , which means a lot of equivalent investment in terms of resources .", "entities": []}, {"text": "Koehn and Knowles ( 2017 ) show that when compared to PBSMT approaches , NMT models need more training data to achievethe same level of performance.1One of the most popular ways to increase the amount of parallel training data for supervised training is backtranslation ( Sennrich et", "entities": []}, {"text": "al . , 2016a )", "entities": []}, {"text": ".", "entities": []}, {"text": "We utilize this approach to improve upon the performance of our baseline models .", "entities": []}, {"text": "All of our systems follow the Transformer architecture ( Vaswani et al . , 2017 ) .", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "Our primary system is a supervised NMT model trained on the original training bitext .", "entities": []}, {"text": "We also report our results on experiments with backtranslation , which were completed post the shared task and hence not a part of our primary submissions .", "entities": []}, {"text": "We use the backtranslated data in two distinct ways - as a standalone parallel corpus , and to create a combined parallel corpus by mixing in a 1:1 ratio with the provided training data .", "entities": []}, {"text": "We also report the performance of \ufb01ne - tuned models originally trained only on the backtranslated data .", "entities": []}, {"text": "In the following sections , we begin by brie\ufb02y describing the Transformer architecture and backtranslation .", "entities": [[11, 12, "MethodName", "Transformer"]]}, {"text": "We then discuss our experimental setup as well as our experiments with backtranslation .", "entities": []}, {"text": "We conclude with a discussion of our results and possible future work .", "entities": []}, {"text": "2 Related Work The Transformer model is the dominant architecture within current NMT models due to its superior performance on several language pairs .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "While still a sequence - to - sequence ( Sutskever et al . , 2014 ) model composed of an encoder and a decoder , Transformer models are highly parallelizable thanks to being composed purely of feedforward and self - attention layers rather than recurrent layers ( Hochreiter and Schmidhuber , 1997 ; Cho et al . , 2014b ) .", "entities": [[25, 26, "MethodName", "Transformer"], [40, 42, "HyperparameterName", "attention layers"]]}, {"text": "The reader is encouraged to read the original paper ( Vaswani et al . , 2017 ) to gain a deeper understanding of the model .", "entities": []}, {"text": "We adopt the Transformer base architecture available under the 1As measured by BLEU score ( Papineni et al . , 2002 ) .", "entities": [[3, 4, "MethodName", "Transformer"], [12, 14, "MetricName", "BLEU score"]]}, {"text": "1145fairseq2(Ott et", "entities": []}, {"text": "al . , 2019 ) library for all our models .", "entities": []}, {"text": "However , NMT models are known to be datahungry ( Koehn and Knowles , 2017 ) ; their performance improves sharply with the availability of more parallel training data .", "entities": []}, {"text": "Except for a few language pairs ( e.g. English - German ) , most have little to no such data available .", "entities": []}, {"text": "On the other hand , a far greater number of languages have a decent amount of monolingual data available online ( e.g. Wikipedia ) .", "entities": []}, {"text": "To address this issue of lack of parallel data , Sennrich et al .", "entities": []}, {"text": "( 2016a ) introduced the concept of backtranslation .", "entities": []}, {"text": "It involves creating a synthetic parallel corpus by translating sentences from the target - side monolingual data to the source language and making corresponding pairs .", "entities": []}, {"text": "A baseline target ! source model ( PBSMT or NMT ) , trained with limited data , is generally used for this purpose .", "entities": []}, {"text": "It enables the use of large corpora of monolingual data for several languages , the size of which is typically orders of magnitude larger than any corresponding bitext available .", "entities": []}, {"text": "What is notable is that only the sourceside data is synthetic in such a scenario and the target - side still corresponds to original monolingual data .", "entities": []}, {"text": "Some studies ( Poncelas et al . , 2018 ; Popel , 2018 ) have investigated the effects of varying the amount of backtranslated data as a proportion of the total training corpus , including training only on the synthetic dataset as a standalone corpus .", "entities": []}, {"text": "We follow some of the related experiments conducted by Kocmi and Bojar ( 2019 ) on Gujarati - English ( another low - resource pair ) with a few exceptions .", "entities": []}, {"text": "Besides , we also report performance when pretraining solely on the synthetic corpus following by \ufb01netuning on either original or mixed data .", "entities": []}, {"text": "While not quite the same , one could think of this approach as having some similarities with transfer learning ( Zoph et al . , 2016 ) as well as domain adaptation ( Luong and Manning , 2015 ; Freitag and AlOnaizan , 2016 ) for machine translation .", "entities": [[17, 19, "TaskName", "transfer learning"], [30, 32, "TaskName", "domain adaptation"], [46, 48, "TaskName", "machine translation"]]}, {"text": "There has also been work on using sampling ( Edunov et al . , 2018 ) for generating backtranslations , but we stick to using beam search in this work .", "entities": []}, {"text": "3 Experimental Setup 3.1 Dataset We used the complete parallel training corpus for our primary systems .", "entities": []}, {"text": "In addition , we also made use of monolingual data from each language for 2https://github.com/pytorch/fairseqtwo purposes - learning Byte Pair Encodings ( BPE ) ( Sennrich et al . , 2016b ) and backtranslation .", "entities": [[22, 23, "MethodName", "BPE"]]}, {"text": "For Upper Sorbian ( hsb ) , we used the monolingual corpora provided by the Sorbian Institute and by the Witaj Sprachzentrum .", "entities": []}, {"text": "To control the quality of the backtranslated data , we chose not to use the data scraped from the web .", "entities": []}, {"text": "For the German ( de ) side , we made use of the News Crawl32009 dataset , as it is large enough to satisfy the requirements for our experiments .", "entities": []}, {"text": "3.2 Data Preprocessing Source No . of sentences hsb - de , bitext 58,389 hsb , monolingual 540,994 de , monolingual 2,000,000 Table 1 : Processed training data .", "entities": []}, {"text": "Moses toolkit ( Koehn et al . , 2007 ) was used for tokenization and punctuation normalization for all data .", "entities": []}, {"text": "Before doing any additional preprocessing , we learned separate truecaser models using the toolkit .", "entities": []}, {"text": "For this purpose , we took \ufb01rst 500 K sentences from each of the monolingual corpora and aggregated them with the corresponding portion from the training bitext .", "entities": []}, {"text": "After tokenizing and truecasing , we joined the parallel training corpus with the same monolingual data .", "entities": []}, {"text": "We learned joint BPE4 with 32 K merge operations over this corpus and applied them to the parallel training data to get vocabularies for each language .", "entities": []}, {"text": "Additionally , we used the clean-corpus-n.perl script within Moses to \ufb01lter out sentences from the parallel corpus with more than 250 subwords as well as sentence length ratio over 1.5 in either direction .", "entities": []}, {"text": "Final corpus statistics are presented in Table 1 .", "entities": []}, {"text": "3.3 Training Our primary system is a Transformer base model , trained on the parallel training corpus for both translation directions till 60 epochs .", "entities": [[7, 8, "MethodName", "Transformer"]]}, {"text": "We keep most of the hyperparameters to their default values in fairseq .", "entities": []}, {"text": "More precisely , we chose Adam ( Kingma and Ba , 2015 ) as the optimizer and Adam betas were set to 0.9 and 0.98 , respectively .", "entities": [[5, 6, "MethodName", "Adam"], [15, 16, "HyperparameterName", "optimizer"], [17, 18, "MethodName", "Adam"]]}, {"text": "The maximum number of tokens in each batch was set to 4096 .", "entities": []}, {"text": "Learning rate was set to 0.0005 , with an inverse squared 3http://data.statmt.org/news-crawl/de/ 4https://github.com/glample/fastBPE", "entities": [[0, 2, "HyperparameterName", "Learning rate"]]}, {"text": "1146root decay schedule and 4000 steps of warmup updates .", "entities": []}, {"text": "Label smoothing was set to 0.1 and dropout to 0.3 .", "entities": [[0, 2, "MethodName", "Label smoothing"]]}, {"text": "Label - smoothed cross", "entities": []}, {"text": "- entropy was used as the training criterion .", "entities": []}, {"text": "We trained all our models for a \ufb01xed number of epochs , determined separately for each system , and chose the last checkpoint for reporting BLEU ( Papineni et al . , 2002 ) scores on the test sets .", "entities": [[8, 11, "HyperparameterName", "number of epochs"], [25, 26, "MetricName", "BLEU"]]}, {"text": "All training was done using a single NVIDIA P100 GPU .", "entities": []}, {"text": "Due to the small amount of parallel training data , each epoch of training took about 90 seconds on average for the primary system .", "entities": []}, {"text": "4 Additional Backtranslation Experiments In this section , we report our post - submission work on using monolingual data for backtranslation .", "entities": []}, {"text": "We took the raw monolingual data that we describe in Section 3.1 and backtranslated with our primary submission models for the respective translation directions , i.e. , hsb ! de for Upper Sorbian data and de ! hsb for German data .", "entities": []}, {"text": "We used fairseq - generate function with a beam size of 5 for this purpose .", "entities": []}, {"text": "Once again , we limited the number of subwords in each sentence to 250 .", "entities": []}, {"text": "Finally , we took all sentence pairs for backtranslated Upper Sorbian corpus and the \ufb01rst two million sentence pairs for the German corpus .", "entities": []}, {"text": "Table 1 indicates the size of the backtranslated corpora by original language .", "entities": []}, {"text": "For further experiments , we name the datasets as follows : \u2022auth : Processed original training data .", "entities": []}, {"text": "\u2022synth : Backtranslated de ! hsb and hsb ! de corpora .", "entities": []}, {"text": "\u2022mixed :", "entities": []}, {"text": "Augmented training data obtained by mixing auth with a portion of synth in 1:1 ratio , providing a total of 116,778 sentence pairs .", "entities": []}, {"text": "We de\ufb01ne the following systems for making use of the backtranslated data .", "entities": []}, {"text": "Note that the \ufb01rst system only differs from the primary system in the number of training epochs completed .", "entities": []}, {"text": "\u2022auth - from - scratch : This system has the same settings as the primary system .", "entities": []}, {"text": "It was trained on the auth corpus till 80 epochs ( as opposed to 60 for primary).\u2022mixed - from - scratch : We trained models on mixed data from scratch for 40 epochs.5 \u2022synth - from - scratch : Models were trained only on the synth datasets .", "entities": []}, {"text": "To adjust for the difference in the size of the respective backtranslated corpora , we trained hsb ! de system for 10 epochs and de ! hsb system for 30 epochs .", "entities": []}, {"text": "\u2022synth - auth-\ufb01netune : We took the models trained via the previous system and \ufb01ne - tuned them on auth data for 20 epochs in each translation direction .", "entities": []}, {"text": "\u2022synth - mixed-\ufb01netune : Same as the last model , except that \ufb01ne - tuning was done on mixed data .", "entities": []}, {"text": "Fine - tuning was carried out by loading pretrained checkpoints and adding extra training \ufb02ags in reset - optimizer and reset - lr - scheduler .", "entities": [[18, 19, "HyperparameterName", "optimizer"]]}, {"text": "5 Results The systems were evaluated on the blind test set ( newstest2020 ) using automated metrics ; no human evaluation was done .", "entities": []}, {"text": "Table 2 shows cased BLEU scores for various systems .", "entities": [[4, 5, "MetricName", "BLEU"]]}, {"text": "Our primary systems achieved a BLEU score of 47.6 for Upper Sorbian!German and 45.2 for German !", "entities": [[5, 7, "MetricName", "BLEU score"]]}, {"text": "Upper Sorbian translation .", "entities": []}, {"text": "We achieved an improvement of 0.3 and 0.4 BLEU points , respectively , by training further till 80 epochs in each direction .", "entities": [[8, 9, "MetricName", "BLEU"]]}, {"text": "We also evaluated a third system , synth - auth-\ufb01netune , as described in Section 4 , which provided a jump of 2.6 points in BLEU score over the primary system for Upper Sorbian !", "entities": [[25, 27, "MetricName", "BLEU score"]]}, {"text": "German and 2.5 for German!Upper Sorbian .", "entities": []}, {"text": "In addition to evaluating on blind test sets , we also report BLEU scores on the development test set in the same table .", "entities": [[12, 13, "MetricName", "BLEU"]]}, {"text": "Two outcomes are worth highlighting : \u2022Model trained only on synth data for German!Upper Sorbian translation matched the performance of a similar model trained on the authentic bitext .", "entities": []}, {"text": "\u2022Best results were obtained by \ufb01ne - tuning a model trained on synth data with either auth ormixed .", "entities": []}, {"text": "5We trained further till 60 epochs , but observed no improvement in BLEU scores .", "entities": [[12, 13, "MetricName", "BLEU"]]}, {"text": "1147System Dataset Epochs newstest2020 devtest hsb!de Primary * auth 60 47.6 auth - from - scratch auth 80 47.9 45.6 mixed - from - scratch mixed 40 - 45.7 synth - from - scratch synth 10 - 38.0 synth - auth-\ufb01netune + auth 20 50.2 49.6 synth - mixed-\ufb01netune + mixed 20 - 48.3 de!hsb Primary auth 60 45.2 auth - from - scratch auth 80 45.6 46.4 mixed - from - scratch mixed 40 - 47.4 synth - from - scratch synth 30 - 46.5 synth - auth-\ufb01netune + auth 20 47.7 49.0 synth - mixed-\ufb01netune + mixed 20 - 49.6 Table 2 : BLEU scores for the blind test set ( newstest2020 ) and the development test set .", "entities": [[105, 106, "MetricName", "BLEU"]]}, {"text": "Bold values in a column indicate the best scores among the evaluated systems .", "entities": []}, {"text": "+ Additional \ufb01ne - tuning for models trained with backtranslated corpora .", "entities": []}, {"text": "*", "entities": []}, {"text": "Only the primary systems were evaluated before deadline .", "entities": []}, {"text": "The second result is notable since the regime of pretraining followed by \ufb01ne - tuning improves the BLEU scores by up to 4 points on this test set when compared to training only on the original bitext .", "entities": [[17, 18, "MetricName", "BLEU"]]}, {"text": "Moreover , while the model trained on synth was not able to match the performance of that trained on auth for Upper Sorbian !", "entities": []}, {"text": "German , it still provides the same bene\ufb01ts as German !", "entities": []}, {"text": "Upper Sorbian model when \ufb01ne - tuned further .", "entities": []}, {"text": "Looking at the small improvements achieved by using only the mixed corpus for training , increasing its size by combining upsampled auth data with more synth data might lead to even further jumps in the BLEU scores .", "entities": [[35, 36, "MetricName", "BLEU"]]}, {"text": "6 Conclusion In this paper , we described our Transformer model for supervised machine translation for Upper Sorbian - German language pair .", "entities": [[9, 10, "MethodName", "Transformer"], [13, 15, "TaskName", "machine translation"]]}, {"text": "We take note of relatively high BLEU scores achieved by our primary systems ( and those of other participants ) on this low - resource language pair , which could relate to the high quality of the training corpus .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "We also report results and takeaways from several experiments with backtranslated data completed post the shared task .", "entities": []}, {"text": "A key result is matching the performance of a system trained on the original bitext with one trained on a limited amount of synthetic , backtranslated data .", "entities": []}, {"text": "Domain mismatch and a difference in the quality of monolingual corpus might have prevented the system from achieving a similarresult in the other direction .", "entities": []}, {"text": "We notice big improvements in performance over the primary systems by following a \u201c pretraining then \ufb01ne - tuning \u201d regime .", "entities": []}, {"text": "An interesting future work would be to measure the applicability of this approach to other lowresource language pairs .", "entities": []}, {"text": "Additional systems could be added as well .", "entities": []}, {"text": "For instance , models trained on mixed data and \ufb01ne - tuned on auth data might provide a meaningful comparison .", "entities": []}, {"text": "Prior work ( Ding et al . , 2019 ) has shown that the number of BPE merge operations has a signi\ufb01cant effect on the performance of NMT systems .", "entities": [[16, 17, "MethodName", "BPE"]]}, {"text": "This work was pointed out during the review process and should be an avenue for further improvement of the model performance .", "entities": []}, {"text": "Acknowledgments The author would like to thank his manager for supporting this project , and the anonymous reviewers for their thoughtful comments which helped improve the presentation of this work .", "entities": []}, {"text": "References Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In 3rd International Conference on Learning Representations , ICLR 2015 , San Diego , California , USA . Kyunghyun Cho , Bart van Merri \u00a8enboer , Dzmitry Bahdanau , and Yoshua Bengio .", "entities": []}, {"text": "2014a .", "entities": []}, {"text": "On the properties of neural machine translation : Encoder \u2013 decoder", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "1148approaches .", "entities": []}, {"text": "In Proceedings of SSST-8 , Eighth Workshop on Syntax , Semantics and Structure in Statistical Translation , pages 103\u2013111 , Doha , Qatar .", "entities": [[15, 16, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kyunghyun Cho , Bart van Merri \u00a8enboer , Caglar Gulcehre , Dzmitry Bahdanau , Fethi Bougares , Holger Schwenk , and Yoshua Bengio .", "entities": []}, {"text": "2014b .", "entities": []}, {"text": "Learning phrase representations using RNN encoder \u2013 decoder for statistical machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1724 \u2013 1734 , Doha , Qatar .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shuoyang Ding , Adithya Renduchintala , and Kevin Duh . 2019 .", "entities": []}, {"text": "A call for prudent choice of subword merge operations in neural machine translation .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "In Proceedings of Machine Translation Summit XVII Volume 1 : Research Track , pages 204\u2013213 , Dublin , Ireland .", "entities": [[3, 5, "TaskName", "Machine Translation"]]}, {"text": "European Association for Machine Translation .", "entities": [[3, 5, "TaskName", "Machine Translation"]]}, {"text": "Sergey Edunov , Myle Ott , Michael Auli , and David Grangier .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Understanding back - translation at scale .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 489\u2013500 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Markus Freitag and Yaser Al - Onaizan .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Fast domain adaptation for neural machine translation .", "entities": [[1, 3, "TaskName", "domain adaptation"], [5, 7, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1612.06897 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sepp Hochreiter and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural computation , 9(8):1735\u20131780 .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In 3rd International Conference on Learning Representations , ICLR 2015 , San Diego , California , USA .", "entities": []}, {"text": "Tom Kocmi and Ond \u02c7rej Bojar .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "CUNI submission for low - resource languages in WMT news 2019 .", "entities": []}, {"text": "In Proceedings of the Fourth Conference on Machine Translation ( Volume 2 : Shared Task Papers , Day 1 ) , pages 234\u2013240 , Florence , Italy .", "entities": [[7, 9, "TaskName", "Machine Translation"], [24, 25, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Philipp Koehn , Hieu Hoang , Alexandra Birch , Chris Callison - Burch , Marcello Federico , Nicola Bertoldi , Brooke Cowan , Wade Shen , Christine Moran , Richard Zens , Chris Dyer , Ond \u02c7rej Bojar , Alexandra Constantin , and Evan Herbst .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Moses : Open source toolkit for statistical machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions , pages 177\u2013180 , Prague , Czech Republic . Association for Computational Linguistics .", "entities": []}, {"text": "Philipp Koehn and Rebecca Knowles .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Six challenges for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the First Workshop on Neural Machine Translation , pages 28\u201339 , Vancouver .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Philipp Koehn , Franz J. Och , and Daniel Marcu .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Statistical phrase - based translation .", "entities": []}, {"text": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics , pages 127\u2013133 .", "entities": []}, {"text": "Minh - Thang Luong and Christopher D Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Stanford neural machine translation systems for spoken language domains .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the International Workshop on Spoken Language Translation .", "entities": [[9, 10, "TaskName", "Translation"]]}, {"text": "Myle Ott , Sergey Edunov , Alexei Baevski , Angela Fan , Sam Gross , Nathan Ng , David Grangier , and Michael Auli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "fairseq :", "entities": []}, {"text": "A fast , extensible toolkit for sequence modeling .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics ( Demonstrations ) , pages 48\u201353 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311\u2013318 , Philadelphia , Pennsylvania , USA . Association for Computational Linguistics .", "entities": []}, {"text": "A Poncelas , D Shterionov , A Way , GM de Buy Wenniger , and P Passban .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Investigating backtranslation in neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1804.06189 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Martin Popel .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Machine translation using syntactic analysis .", "entities": [[0, 2, "TaskName", "Machine translation"]]}, {"text": "Univerzita Karlova .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016a .", "entities": []}, {"text": "Improving neural machine translation models with monolingual data .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 86\u201396 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016b .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Ilya Sutskever , Oriol Vinyals , and Quoc V Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "InAdvances in Neural Information Processing Systems , pages 3104\u20133112 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 5998\u20136008 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "1149Barret Zoph , Deniz Yuret , Jonathan May , and Kevin Knight .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Transfer learning for low - resource neural machine translation .", "entities": [[0, 2, "TaskName", "Transfer learning"], [3, 9, "TaskName", "low - resource neural machine translation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1568\u20131575 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}]