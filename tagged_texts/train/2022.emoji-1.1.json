[{"text": "Proceedings of the The Fifth International Workshop on Emoji Understanding and Applications in Social Media , pages 1 - 10 July 14 , 2022 \u00a9 2022 Association for Computational Linguistics Interpreting Emoji with Emoji : \u21d2 Jens Helge Reelfs1 , Timon Mohaupt\u22172 , Sandipan Sikdar3 , Markus Strohmaier4 , Oliver Hohlfeld1 1Brandenburg University of Technology,2Talentschmiede Unternehmensberatung AG 3RWTH Aachen University,4University of Mannheim , GESIS , CSH Vienna reelfs@b-tu.de , timon.mohaupt@me.com , sandipan.sikdar@cssh.rwth-aachen.de , markus.strohmaier@uni-mannheim.de , hohlfeld@b-tu.de", "entities": []}, {"text": "Abstract We study the extent to which emoji can be used to add interpretability to embeddings of text and emoji .", "entities": []}, {"text": "To do so , we extend the POLAR - framework that transforms word embeddings to interpretable counterparts and apply it to word - emoji embeddings trained on four years of messaging data from the Jodel social network .", "entities": [[12, 14, "TaskName", "word embeddings"]]}, {"text": "We devise a crowdsourced human judgement experiment to study six usecases , evaluating against words only , what role emoji can play in adding interpretability to word embeddings .", "entities": [[26, 28, "TaskName", "word embeddings"]]}, {"text": "That is , we use a revised POLAR approach interpreting words and emoji with words , emoji or both according to human judgement .", "entities": []}, {"text": "We \ufb01nd statistically signi\ufb01cant trends demonstrating that emoji can be used to interpret other emoji very well .", "entities": []}, {"text": "1 Introduction Word embeddings create a vector - space representation in which words with a similar meaning are in close proximity .", "entities": [[2, 4, "TaskName", "Word embeddings"]]}, {"text": "Existing approaches to make embeddings interpretable , e.g. , via contextual ( Subramanian et al . , 2018 ) , sparse embeddings ( Panigrahi et al . , 2019 ) , or learned ( Senel et al . , 2018 ) transformations ( Mathew et al . , 2020)\u2014all focus on text only .", "entities": []}, {"text": "Yet , emoji are widely used in casual communication , e.g. , Online Social Networks ( OSN ) , and are known to extend textual expressiveness , demonstrated to bene\ufb01t , e.g. , sentiment analysis ( Novak et al . , 2015 ;", "entities": [[33, 35, "TaskName", "sentiment analysis"]]}, {"text": "Hu et al . , 2017 ) .", "entities": []}, {"text": "Goal .", "entities": []}, {"text": "We raise the question if we can leverage the expressiveness of emoji to make word embeddings \u2014 and thus also emoji \u2014 interpretable .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "I.e. , can we adopt word embedding interpretability via leveraging semantic polar opposites ( e.g. , cold / hot ) to emoji ( e.g. ,   / , or   /   ) for interpreting words or emoji w.r.t .", "entities": []}, {"text": "human judgement .", "entities": []}, {"text": "* Timon Mohaupt performed this work during his master thesis at Brandenburg University of Technology and RWTH Aachen University .", "entities": []}, {"text": "Approach .", "entities": []}, {"text": "Motivated and based upon POLAR ( Mathew et al . , 2020 ) , we deploy a revised variant POLAR\u03c1that transforms arbitrary word embeddings into interpretable counterparts .", "entities": [[22, 24, "TaskName", "word embeddings"]]}, {"text": "The key idea is to leverage semantic differentials as a psychometric tool to align embedded terms on a scale between two polar opposites .", "entities": []}, {"text": "Employing a projection - based transformation in POLAR\u03c1 , we provide embedding dimensions with semantic information .", "entities": []}, {"text": "I.e. , the resulting interpretable embedding space values directly estimate a term \u2019s position on a - priori provided polar opposite scales , while approximately preserving in - embedding structures ( \u00a7 2 ) .", "entities": []}, {"text": "The main contribution of this work is the largescale application of this approach to a social media corpus and especially its evaluation in a crowdsourced human judgement experiment .", "entities": []}, {"text": "For studying the role of emoji in interpretability , we create a word - emoji input embedding from on a large social media corpus .", "entities": []}, {"text": "The dataset comprises four years of complete data in a single country from the online social network provider Jodel ( 48 M posts of which 11 M contain emoji ) .", "entities": []}, {"text": "For subsequent main evaluation , we make this embedding interpretable with word and emoji opposites by deploying our adopted tool POLAR\u03c1(\u00a7 3 ) .", "entities": []}, {"text": "Given different expressiveness of emoji , we ask RQ1 ) How does adding emoji to POLAR\u03c1impact interpretability w.r.t . to human judgement ?", "entities": []}, {"text": "I.e. , do humans agree on best interpretable dimensions for describing words or emoji with word or emoji opposites ?", "entities": []}, {"text": "And RQ2 ) How well do POLAR\u03c1 - semantic dimensions re\ufb02ect a term \u2019s position on a scale between word or emoji polar opposites ?", "entities": []}, {"text": "Human judgement .", "entities": []}, {"text": "We design a crowdsourced human judgement experiment ( \u00a7 4 ) to study if adding emoji to word embeddings and POLAR\u03c1 in particular increases the interpretability \u2014 while also answering how to describe emoji best .", "entities": [[17, 19, "TaskName", "word embeddings"]]}, {"text": "Our human judgement experiment involves six campaigns explaining Words ( W/ * ) or Emoji ( E/ * ) with Words , 1", "entities": []}, {"text": "( W / W ) Words with Words Beach 0 5 -5 10 -10 sand stone holiday home mountain sea swim sink land sea ( W / M ) Words with Words & Emoji , Mixed Beach 0 5 -5 10 -10 sand stone holiday home mountain sea ( W / E ) Words with Emoji Beach 0 5 -5 10 -10 ( E / W ) Emoji with Words 0 5 -5 10 -10 land sea night sun work recreation holiday home rains dryspell ( E / M ) Emoji with Words & Emoji , Mixed 0 5 -5 10 -10 land sea night sun ( E / E ) Emoji with Emoji ( cf . paper title ) 0", "entities": [[9, 10, "DatasetName", "0"], [37, 38, "DatasetName", "0"], [57, 58, "DatasetName", "0"], [70, 71, "DatasetName", "0"], [97, 98, "DatasetName", "0"], [120, 121, "DatasetName", "0"]]}, {"text": "5 -5 10 -10 Figure 1 : The POLAR - framework ( Mathew et al . , 2020 ) makes word embeddings interpretable leveraging polar opposites .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "It provides a new interpretable embedding subspace with systematic polar opposite scales : Along six use - cases , we evaluate which role emoji expressiveness plays in adding interpretability to word embeddings .", "entities": [[30, 32, "TaskName", "word embeddings"]]}, {"text": "I.e. , how well can our adopted POLAR\u03c1interpret ( W/ * ) words or ( E/ * ) emoji with words , emoji or both ( * /M ) , Mixed .", "entities": []}, {"text": "We test POLAR\u03c1alignment with human judgement as represented in shown semantic pro\ufb01les above .", "entities": []}, {"text": "Emoji , or both Mixed .", "entities": []}, {"text": "We evaluate two test conditions to answer both research questions : ( RQ1 ) aselection test studies if human subjects agree to the POLAR\u03c1identi\ufb01ed differentials ( e.g. , how do emoji affect POLAR\u03c1interpretability ? ) , and ( RQ2 ) apreference test that studies if the direction on a given differential scale is in line with human judgement ( e.g. , how well does POLAR\u03c1interpret scales ) .", "entities": []}, {"text": "Results .", "entities": []}, {"text": "POLAR\u03c1identi\ufb01es the best interpretable opposites for describing emoji with emoji , yet generally aligning well with human judgement .", "entities": []}, {"text": "Except interpreting words with emoji only probably due to lack of emoji expressiveness indicated by coder agreement .", "entities": []}, {"text": "Further , POLAR\u03c1estimates an embeddedterms \u2019 position on a scale between opposites successfully , especially for interpreting emoji .", "entities": []}, {"text": "Broader application .", "entities": []}, {"text": "Not all emoji have a universally agreed on meaning .", "entities": []}, {"text": "Prior work showed that differences in the meaning of emoji exist between cultures ( Guntuku et al . , 2019 ;", "entities": []}, {"text": "Gupta et al . , 2021 ) .", "entities": []}, {"text": "Even within the same culture , ambiguity and double meanings of emoji exist ( Reelfs et al . , 2020 ) .", "entities": []}, {"text": "Currently , no data - driven approach exists to infer the meaning of emoji \u2014 to make them interpretable .", "entities": []}, {"text": "Our proposed approach can be used to tackle this challenge since it makes emoji interpretable .", "entities": []}, {"text": "2 Creating Interpretable Embeddings We explain next our deployed tool for creating interpretable word - emoji embeddings :", "entities": []}, {"text": "PO - LAR ( Mathew et al . , 2020 ) ; and provide detail on a revised POLAR extension via projection .", "entities": []}, {"text": "2.1 POLAR Approach Semantic Differentials .", "entities": []}, {"text": "Based upon the idea of semantic differentials as a psychometric tool to align a word on a scale between two polar opposites ( Fig . 1 ) , POLAR ( Mathew et al . , 2020 ) takes a word embedding as input and creates a new interpretable embedding on a polar subspace .", "entities": []}, {"text": "This subspace , i.e. , the opposites used for the interpretable embedding are de\ufb01ned by an external source .", "entities": []}, {"text": "That is , starting with a corpus and its vocabularyV , a word embedding created by an algorithma(e.g . , Word2Vec or GloVe ) assigns vectors\u2212\u2192Wa v\u2208Rdonddimensions to all words v\u2208V according to an optimization function ( usually word co - occurrence ) .", "entities": [[22, 23, "MethodName", "GloVe"]]}, {"text": "This pretraining results in an embedding D=/bracketleftBig\u2212\u2192Wa v , v\u2208V / bracketrightBig \u2208R|V|\u00d7d .", "entities": []}, {"text": "Such embedding spaces carry a semantic structure between embedded words , whereas the dimensions do not have any speci\ufb01c meaning .", "entities": []}, {"text": "However , we can leverage the semantic structure between words to transform the embedding space to carrying over meaning into the dimensions : POLAR uses Nsemantic differentials / opposites that are itself items within the embedding , i.e. , P=/braceleftbig ( pi z , pi \u2212z),i\u2208[1 .. N],(pi z , pi \u2212z)\u2286V2 / bracerightbig .", "entities": []}, {"text": "As shown in Fig .", "entities": []}, {"text": "2a , given two anchor points for each polar opposite , a line between them represents a differential \u2014 which we name POLAR direction2", "entities": []}, {"text": "(      - ) (      - ) (      - ) ( a ) Polar Directions / Differentials .", "entities": []}, {"text": "(      - ) (      - )   ( b ) Excerpt : Per Direction Orthogonal Projection .", "entities": []}, {"text": "0 0 0   ( c ) Interpretable Scales for   .", "entities": [[0, 1, "DatasetName", "0"], [1, 2, "DatasetName", "0"], [2, 3, "DatasetName", "0"]]}, {"text": "Figure 2 : POLAR ( Mathew et al . , 2020 ) with Projection in a nutshell : We showcase POLAR\u03c1interpreting emoji with emoji ( E / E ) ( cf . paper title ) .", "entities": []}, {"text": "( a)We leverage polar opposites ( here :   / , / , / ) to provide embedding dimensions with semantic information .", "entities": []}, {"text": "By using opposite differential directions ( red dashed vectors ) , we create a new interpretable subspace .", "entities": []}, {"text": "( b)Orthogonal projection ( blue dotted vectors ) of an embedded term ( here : ) onto this subspace ( e.g. , left :   / , right :   / )", "entities": []}, {"text": "yields a direct scale measure between both opposites in the adjacent leg ( green vectors , directed alike the differential ) .", "entities": []}, {"text": "( c)The resulting interpretable embedding now contains a tangible position estimation along employed polar dimensions for each embedded term ( here :   ) .", "entities": []}, {"text": "( red dashed vectors ): \u2212\u2212\u2192diri=\u2212\u2212\u2192Wa piz\u2212\u2212\u2212\u2212\u2192Wa pi \u2212z\u2208Rd Base Change .", "entities": []}, {"text": "Naturally , we can use these differentials as a new basis for the interpretable embedding E. Gathering all directions in a matrix dir\u2208RN\u00d7d , we obtain for all embedded terms v\u2208V : dirT\u2212 \u2192Ev=\u2212\u2192Wa v , and ultimately apply a base change transformation\u2212 \u2192Ev= ( dirT)\u22121\u2212\u2192Wa vyielding an interpretable subspace along the differentials\u2212\u2212\u2192dirithat carries over speci\ufb01c geometric semantics from the input embedding .", "entities": []}, {"text": "I.e. , for each word v\u2208V within the resulting interpretable embedding E , its embedding vector\u2212 \u2192Evnow carries a measure along each polar dimension \u2019s semantics .", "entities": []}, {"text": "Limitations .", "entities": []}, {"text": "Polar opposites being very close in the original embedding space might tear apart .", "entities": []}, {"text": "From a technical perspective , the used pseudo inverse for the base change becomes numerically ill - conditioned if d\u2248N(Mathew et al . , 2020 ) .", "entities": []}, {"text": "2.2 POLAR\u03c1Extension : Projection While the base change approach seems natural , its given limitations lead us to propose a variant that comes with several bene\ufb01ts .", "entities": []}, {"text": "Instead of creating a new interpretable vector space , we take measurements on the differentials dirde\ufb01ned as before ( Fig .", "entities": []}, {"text": "2a , red dashed vectors ) .", "entities": []}, {"text": "However , we now project each embedding vector\u2212\u2192Wvfor vorthogonally onto the differentials as shown in Fig .", "entities": []}, {"text": "2b ( blue dotted vectors ) .", "entities": []}, {"text": "This leads to a smallest distance between both lines w.r.t .", "entities": []}, {"text": "the differential , yet simultaneously allows for a direct scale measure on the differential vector as shown inFig .", "entities": []}, {"text": "2b & Fig . 2c ( green vectors ) .", "entities": []}, {"text": "Thereby , we also decouple the transformation matrix , which eases later add - ins to the interpretable embedding .", "entities": []}, {"text": "Orthogonal projection ( blue dotted vectors ) of each input embedding vector\u2212\u2192Wa vonto a differential iprovides us the adjacent leg vector as follows : oprojdiri(\u2212\u2192Wa v )", "entities": []}, {"text": "= \u2212\u2192Wa v\u00b7\u2212\u2212\u2192diri |\u2212\u2212\u2192diri|/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright scalar\u00b7\u2212\u2212\u2192diri |\u2212\u2212\u2192diri|/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright direction As this adjacent leg ( green vectors ) \u2019s direction naturally equals the differential , we focus only on the scalar part representing a direct scale measure .", "entities": []}, {"text": "By normalizing the differential vector lengths \u02c6dir = dir\u00b7|dir|\u22121 , the projected scale value conveniently results in : oprojscalar diri(\u2212\u2192Wa v )", "entities": []}, {"text": "= \u2212\u2192Wa v\u00b7\u2212\u2212\u2192\u02c6diri .", "entities": []}, {"text": "This transformation allows to create a new interpretable embedding E\u2208R|V|\u00d7Nfor each embedding vector\u2212\u2192Wa v(exempli\ufb01ed in Fig .", "entities": []}, {"text": "1 ) as follows : \u2212 \u2192Ev = oprojscalar dir ( \u2212\u2192Wa v ) =", "entities": []}, {"text": "\u02c6dirT\u2212\u2192Wa v\u2208RN", "entities": []}, {"text": "Computationally it requires an inital matrix multiplication for each embedded term ; Dimension increments require a dot product on each term .", "entities": []}, {"text": "Downstream Tasks .", "entities": []}, {"text": "Other experiments indicate POLAR\u03c1downstream task performance being on par with the input embedding , and an edge over base change POLAR if d\u2248N(not shown ) .", "entities": []}, {"text": "2.3 Measuring Dimension Importance There can be many possible POLAR dimensions , which requires to select the most suitable ones.3", "entities": []}, {"text": "That is , we want to de\ufb01ne a limited set of opposites that best describes words or emoji w.r.t .", "entities": []}, {"text": "interpretability across the whole embedding .", "entities": []}, {"text": "Extremal Word Score ( EWSO ) .", "entities": [[2, 3, "MetricName", "Score"]]}, {"text": "We propose a new metric to measure the quality of polar dimensions complementing heuristics from ( Mathew et al . , 2020 ) .", "entities": []}, {"text": "It measures the embedding con\ufb01dence and consistency along available differentials .", "entities": []}, {"text": "The idea of POLAR\u03c1is that directions represent semantics within the input embedding .", "entities": []}, {"text": "We determine embedded terms shortest distance to these axes via orthogonal projection ; we use resulting intersections as the position w.r.t .", "entities": []}, {"text": "the directions .", "entities": []}, {"text": "That is , as a new heuristic , for each of our differentialsdiri , we look out for k= 10 embedded words at the extremes ( having the highest scores in each direction ) and take their average cosine distance within the original embedding Dto the differential as a measure .", "entities": []}, {"text": "This results in the average similarity of existing extremal words on our scale \u2014 a heuristic that represents the skew - whif\ufb01ness within the extremes on a differential scale .", "entities": []}, {"text": "3 Approach : Embedding & Polarization", "entities": []}, {"text": "We next propose an approach to improve the interpretability of word embeddings by adding emoji .", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "It uses our extended version POLAR\u03c1and adds emoji to the POLAR space by creating word embeddings that include emoji .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "3.1 Data Set We create a word embedding out of a social media text corpus , since emoji are prominent in communication within Online Social Networks .", "entities": []}, {"text": "We decided to use a corpus from the Jodel network , where about one out of four sentences contain emoji ( see ( Reelfs et al . , 2020 ) ) .", "entities": []}, {"text": "The Jodel Network .", "entities": []}, {"text": "We base our study on a country - wide complete dataset of posts in the online social network Jodel , a mobile - only messaging application .", "entities": []}, {"text": "It is location - based and establishes local communities relative to the users \u2019 location .", "entities": []}, {"text": "Within these communities , users can anonymously post photos from the camera app or content of up to 250 characters length , i.e. , microblogging , and reply to posts forming discussion threads .", "entities": []}, {"text": "Corpus .", "entities": []}, {"text": "The network operators provided us with data of content created in Germany from 2014 to 2017 .", "entities": []}, {"text": "It contains 48 M sentences , of which 11 M contain emoji ( 1.76 emoji per sentence on average ) .", "entities": []}, {"text": "Ethics .", "entities": [[0, 1, "DatasetName", "Ethics"]]}, {"text": "The dataset contains no personal informa - tion and can not be used to personally identify users except for data that they willingly have posted on the platform .", "entities": []}, {"text": "We synchronize with the Jodel operator on analyses we perform on their data .", "entities": []}, {"text": "3.2 Semantic Differential Sources POLAR\u03c1can create interpretable embeddings w.r.t .", "entities": []}, {"text": "a - priori provided opposites .", "entities": []}, {"text": "We next describe how we select these opposites to make POLAR\u03c1applicable to our data .", "entities": []}, {"text": "Most importantly , the approach requires being part of or locating desired opposites within the original embedding space .", "entities": []}, {"text": "Words .", "entities": []}, {"text": "As we extend the word embedding space with emoji , we still want to use words .", "entities": []}, {"text": "We \ufb01nd common sources of polar opposites in antonym wordlists ( Shwartz et al . , 2017 ) as used in the original POLAR work .", "entities": []}, {"text": "To \ufb01t our German dataset , we translated and manually checked all pairs keeping 1275 items .", "entities": []}, {"text": "From GermaNet ( Hamp and Feldweg , 1997 ) , we extracted 1732 word pairs via antonym relations leading to |Pwords|= 1832 word pairs .", "entities": []}, {"text": "Emoji .", "entities": []}, {"text": "Being not ideal , but due to lack of better alternatives , we ended up heuristically creating semantic opposites from emoji through qualitative surveys across friends and colleagues resulting in |Pemoji|= 44 emoji pairs , cf .", "entities": []}, {"text": "Tab .", "entities": []}, {"text": "3 .", "entities": []}, {"text": "While we could use far more opposites especially of facial emoji , due to emoji clustering in the input embedding , spanned expressive space would arguably become redundant at similar EWSO scores for many directions .", "entities": []}, {"text": "Effectively it may bias interpretability over proportionally towards facial emoji .", "entities": []}, {"text": "3.3 Polarization Preprocessing .", "entities": []}, {"text": "We tokenize sentences with spaCy and remove stopwords .", "entities": []}, {"text": "To increase amounts of available data , we remove all emoji modi\ufb01ers ( skin tone and gender ): {   , , } \u2192   .", "entities": []}, {"text": "Due to German language , we keep capitalization .", "entities": []}, {"text": "Original Embedding .", "entities": []}, {"text": "We use gensim implementation of Word2Vec ( W2V ) .", "entities": []}, {"text": "A qualitative investigation suggests that skip - gram works better than CBOW ( better word analogy ) .", "entities": []}, {"text": "We kept training parameters largely at defaults including negative sampling , opting for d= 300 dimensions .", "entities": []}, {"text": "Interpretable Embedding .", "entities": []}, {"text": "The actual application of embedding transformation is simple .", "entities": []}, {"text": "We create the matrix of differentials dir , the POLAR subspace , according to our antonym - set Pwords\u222a Pemoji ( \u00a7 3.2 ) .", "entities": []}, {"text": "After normalizing the subspace vectors , we create all embedding vectors via projec-4", "entities": []}, {"text": "with interpret W Mixed E Words ( W / W ) ( W / M ) ( W / E ) Emoji ( E / W ) ( E / M ) ( E / E ) ( a ) Campaigns Overview .", "entities": []}, {"text": "We interpret Words & Emoji with likewise Words , Emoji , and Mixed ( both).Please choose 5 Pairs that characterize   best !", "entities": []}, {"text": "2   2 black - white 2 female - male 2   2 slow - fast 2 fork - spoon ... ( b ) Selection Task for Emoji / Mixed ( E / M).Which term describes   better ?", "entities": []}, {"text": "\u2190", "entities": []}, {"text": "= \u2192 /circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt black / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt white female / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt male /circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt slow / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt fast fork / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt / circlecopyrt spoon ... ( c ) Preference Task for Emoji / Mixed ( E / M ) .", "entities": []}, {"text": "Figure 3 : ( a)We conduct six campaigns measuring human interpretability for including emoji to the POLAR\u03c1 embedding space .", "entities": []}, {"text": "Exempli\ufb01ed with the Emoji Mixed campaign ( E / M ): interpreting emoji with emoji and words .", "entities": []}, {"text": "( b)In the Selection test , coders choose suitable differentials for describing a given term .", "entities": []}, {"text": "( c)In the Preference test , coders provide their interpretation of a given term to a differential scale .", "entities": []}, {"text": "tion\u2212 \u2192Ev=\u02c6dirT\u2212\u2192Wv,\u2200v\u2208 V.", "entities": []}, {"text": "Though normalization requires careful later additions to the POLAR space , we opted for standard normalization , Estdnrm = [ E\u2212mean ( E)]\u00b7std(E)\u22121 , to ensure that the whole embedding space aligns properly around the center of gravity on each differential scale .", "entities": []}, {"text": "We select the best suited opposites for a given embedding space by using the Extremal Word Score ( \u00a7 2.3 ) ford=500 + 44 dimensions ( words + emoji ) .", "entities": [[16, 17, "MetricName", "Score"]]}, {"text": "4 Human Evaluation Approach While we have now created a supposedly interpretable embedding , it remains to be seen how well it isperceived by humans .", "entities": []}, {"text": "That is , we next evaluate our two key RQs , discuss signi\ufb01cance , and provide further details : RQ1 ) How well does POLAR\u03c1with EWSO perform in selecting most interpretable dimensions at varying expressiveness of words and emoji ?", "entities": []}, {"text": "RQ2 )", "entities": []}, {"text": "How well do POLAR\u03c1scalar values re\ufb02ect directions on the differential scales ?", "entities": []}, {"text": "i )", "entities": []}, {"text": "Do humans prefer emoji to words ?", "entities": []}, {"text": "ii)How", "entities": []}, {"text": "well do human raters align w.r.t .", "entities": []}, {"text": "interpretability ?", "entities": []}, {"text": "iii )", "entities": []}, {"text": "What impact do demographic factors play in interpretability with or without emoji ?", "entities": []}, {"text": "4.1 Evaluation design To gather human judgement , we employ crowdsourcing on the Microworkers platform .", "entities": []}, {"text": "4.1.1", "entities": []}, {"text": "Questions & Evaluation Metrics", "entities": []}, {"text": "Our evaluation of the POLAR\u03c1approach including emoji to the differentials bases on two main questions next to demographics .", "entities": []}, {"text": "Selection test .", "entities": []}, {"text": "Analogous to the original work , we want to \ufb01nd out whether humans agree on best interpretability of POLAR\u03c1selected differentials with a word intrusion task .", "entities": []}, {"text": "The question asks ourcoders to select \ufb01ve out of ten differentials that describe a given word best as shown in Fig .", "entities": []}, {"text": "3b .", "entities": []}, {"text": "We select half of these dimensions according to the highest absolute projection scale values ( most extreme ) .", "entities": []}, {"text": "The other half consists of a random selection from the bottom half of available differentials .", "entities": []}, {"text": "I.e. , if the projection approach determines interpretable dimensions well , humans would choose all \ufb01ve out of \ufb01ve POLAR\u03c1chosen differentials .", "entities": []}, {"text": "As any user might choose differently , we count how often coders choose certain differentials .", "entities": []}, {"text": "The resulting frequencies immediately translate in a ranking that we leverage for calculating the fraction of Top 1 .. 5 being POLAR\u03c1chosen differentials .", "entities": []}, {"text": "Preference test .", "entities": []}, {"text": "Additionally , we introduce the preference test evaluating whether the direction on a given differential scale is in line with human judgement .", "entities": []}, {"text": "That is , for the same words from the selection test , we display the same ten dimensions ( 5 top - POLAR\u03c1 , 5 random bottom ) where coders select their interpretation of the given word on scales as shown in Fig .", "entities": []}, {"text": "3c .", "entities": []}, {"text": "Typical for semantic differential scales ( Tullis and Albert , 2008 ; Osgood et al . , 1957 ) , we deliberately use a seven point scale representing -3 to 3 , allowing more freedom than 3 or 5 points ( Simms et al . , 2019 ) .", "entities": []}, {"text": "Further , we speci\ufb01cally allow a center point \u2014 being equal \u2014 as it might indicate both being equally well ornot good at all .", "entities": []}, {"text": "Due to scale usage heterogeneity ( Rossi et al . , 2001 ) , we normalize coder chosen directions ( shift+scale according to mean ) prohibiting disproportional in\ufb02uence of single coders .", "entities": []}, {"text": "We evaluate the coder agreement by counting direction ( sign ) non-/alignment with the POLAR\u03c1projection scale .", "entities": []}, {"text": "Demographics .", "entities": []}, {"text": "There is a multitude of other external factors that might have impact on coders \u2019 choices .", "entities": []}, {"text": "To better understand participant back-5", "entities": []}, {"text": "ground , we ask for their education , emoji usage ( familiarity ) , smartphone platform ( different emoji pictograms ) , and if they had used Jodel before .", "entities": []}, {"text": "4.1.2 Evaluation Setup Crowdworker Campaigns We run a campaign for each of the cross product between words only , emoji only , and mixed Tab . 3a and Fig .", "entities": []}, {"text": "2 . ( W / W ) word / word sets a baseline comparison to results from the original POLAR work , albeit now using the projection approach .", "entities": []}, {"text": "( W / M ) : word / mixed uses not only words , but includes emoji to the POLAR subspace .", "entities": []}, {"text": "( W / E ) : word / emoji uses only emoji to describe words .", "entities": []}, {"text": "( E / W ) : emoji / word provides another baseline as to how well emoji may be interpreted with words only .", "entities": []}, {"text": "( E / M ) : emoji / mixed uses both , emoji and words to interpret emoji .", "entities": []}, {"text": "( E / E ) : emoji / emoji may be the most interesting as we only use the expressiveness of emoji to describe emoji .", "entities": []}, {"text": "For mixed cases ( emoji and words within the POLAR subspace ) , we create rankings from absolute scale values on both types ( words / emoji ) separately and then select them equally often to achieve similar amounts of word and emoji differentials .", "entities": []}, {"text": "Used Words & Emoji .", "entities": []}, {"text": "We selected 50 words and emoji to be described in each campaign .", "entities": []}, {"text": "To ensure thati)we only use common words that are very likely known to our coders , and ii)these words are captured well within the underlying embedding , we pick them out of the upper 25 % quantile by occurrences in the corpus ( n\u22651.6k ) .", "entities": []}, {"text": "I.e. , we chose emoji and words that appear frequently and should therefore be well - known .", "entities": []}, {"text": "For words , we ensured that they are part of the German dictionary Duden .", "entities": []}, {"text": "Tasks Setup .", "entities": []}, {"text": "Within our six campagins , we now have each 50 emoji or 50 words to be interpreted .", "entities": []}, {"text": "We bundled this into 5 tasks each consisting of 10 emoji / words \u2014 resulting in 30 different tasks .", "entities": []}, {"text": "Each of these tasks contains the Selection test , Preference test , and demographics .", "entities": []}, {"text": "Subjects .", "entities": []}, {"text": "Human judgement and crowdsourced evaluations are noisy by nature .", "entities": []}, {"text": "While it is usually suf\ufb01cient to employ few trusted expert coders , it is suggested to use more in the non - expert case ( Snow et al . , 2008 ) .", "entities": []}, {"text": "Thus , we assign 5 different annotators to each of the 30 tasks .", "entities": []}, {"text": "At estimated 10 - 15min duration , we provide 3 $ compensation for answering a single task , above minimum wage in our country .", "entities": []}, {"text": "Quality Assurance .", "entities": []}, {"text": "Any crowdsourcing task offers an incentive to rush tasks for the money , which requires us to employ means of quality assurance ( QA ) .", "entities": []}, {"text": "As we have an uncontrolled environment andthus untrusted coders , we handcraft test questions for the selection and preference test .", "entities": []}, {"text": "This task is non - trivial as we require unambiguity in correct answers ( we ensured this with multiple qualitative tests among friends and colleagues ) , while simultaneously not being too obvious .", "entities": []}, {"text": "We place one test question for selection and one for preference randomly into each task ( ending up in 11 words or emoji per task ) .", "entities": []}, {"text": "This also means that each coder can only participate in up to 5 different tasks within a single campaign before re - seeing a test question .", "entities": []}, {"text": "We de\ufb01ne acceptance thresholds of four out of \ufb01ve correct answers for both , the selection test and the correct direction for the preference test .", "entities": []}, {"text": "4.2 Results Within the crowdsourcing process , we rejected about 10 % of all tasks according to our QA measures , which then had to be re - taken .", "entities": []}, {"text": "We ended up with 6 campaigns each having 50 words / emoji answered by 5 coders ; summing up to completed 150 tasks .", "entities": []}, {"text": "In total , 16 different coders accomplished this series of which 4 completed \u03a3\u2265100tasks .", "entities": []}, {"text": "4.2.1 Interpreting Emoji First we focus on the describing emoji campaigns ( E/ * ) .", "entities": []}, {"text": "We present our main evaluation results in Tab .", "entities": []}, {"text": "1 .", "entities": []}, {"text": "Within columns , we show results for random , original POLAR , and our six campaigns .", "entities": []}, {"text": "We split the rows into results from the selection test across Top1 .. 5 entries and the preference test .", "entities": []}, {"text": "Selection Test .", "entities": []}, {"text": "We \ufb01nd very good results along all emoji campaigns ( E/ * ) being consistently better than any campaign describing words ( W/ * ) .", "entities": []}, {"text": "The best performance was achieved for explaining emoji with emoji ( E / E ) ; others are on par .", "entities": []}, {"text": "We want to note however , that the small size of used emoji - differential set may ease selection .", "entities": []}, {"text": "E.g. , facial expression emoji regularly achieve higher embedding scores than others , which thus may bias the bottom control half ( \u00a7 4.1.1 ) .", "entities": []}, {"text": "However , interpreting emoji or words with words only , ( E / W ) and ( W / W ) , achieve comparable performance .", "entities": []}, {"text": "Preference Test .", "entities": []}, {"text": "Here , we make the same observation ; The projected scales on the differentials are mostly well in line with human judgement .", "entities": []}, {"text": "4.2.2 Interpreting Words", "entities": []}, {"text": "Again , we refer to Tab . 1 , but now change our focus to describing words , campaigns ( W/ * ) .", "entities": []}, {"text": "Selection Test .", "entities": []}, {"text": "Albeit not being directly comparable , using POLAR\u03c1in compaings : describing6", "entities": []}, {"text": "Task Random POLAR ( W / W ) ( W / M ) ( W / E ) ( E / W ) ( E / M ) ( E / E ) SelectionTop 1 0.500 0.876 0.79 0.83 0.60 0.81 0.79 0.88 Top 2 0.222 0.667 0.62 0.61 0.35 0.67 0.68 0.77 Top 3 0.083 0.420 0.45 0.42 0.15 0.54 0.57 0.67 Top 4 0.024 0.222 0.30 0.18 0.07 0.37 0.37 0.59 Top 5 0.004 0.086 0.14 0.08 0.01 0.22 0.19 0.38 Preference 0.500 - 0.740 0.672 0.576 0.800 0.848 0.832 Table 1 : Campaign results .", "entities": []}, {"text": "Random & original POLAR baseline .", "entities": []}, {"text": "Selection and Preference results across campaigns .", "entities": []}, {"text": "Words are better described by word dimensions , and emoji are better described by emoji dimensions .", "entities": []}, {"text": "words with words ( W / W ) , or describing words with words and emoji ( W / M ) achieved performance well on par with POLAR .", "entities": []}, {"text": "Noteworthy , describing words with emoji ( W / E ) yielded the worst results .", "entities": []}, {"text": "The projection scale values for the emoji dimensions were mostly lower compared to words .", "entities": []}, {"text": "I.e. , according to POLAR\u03c1 , for words only few emoji differentials would be among the top 5 opposites .", "entities": []}, {"text": "Preference Test .", "entities": []}, {"text": "As for the preference test , describing words yield the best results using word opposites only ( W / W ) .", "entities": []}, {"text": "Explaining words with emoji ( W / E ) performs particularly worse .", "entities": []}, {"text": "4.2.3 Result Con\ufb01dence Signi\ufb01cance .", "entities": []}, {"text": "To test for differences within the coder alignment with POLAR\u03c1 , we model both , the selection and preference test .", "entities": []}, {"text": "With our primary goal to understand the impact of including emoji to a POLAR\u03c1interpretable word embedding , we anchor to the ( W / W ) campaign as a baseline .", "entities": []}, {"text": "For the selection test , we count if coders aligned with POLAR\u03c1or chose any of the random alternatives across the Top 1 .. 5 selection .", "entities": []}, {"text": "For the preference test , we count whether coders aligned with POLAR\u03c1 \u2019s scale direction .", "entities": []}, {"text": "We apply double - sided chi - square tests \u03c72withp<0.05between the interpreting words with words ( W / W ) baseline and the remaining \ufb01ve campaigns .", "entities": []}, {"text": "We identify signi\ufb01cant differences in coderPOLAR\u03c1alignment to the ( W / W ) baseline when describing words with emoji ( W / E ) over Top1 .. 5 selection and preference .", "entities": []}, {"text": "Counts from explaining emoji with emoji ( E / E ) signal signi\ufb01cance for preference and selection Top3 ..", "entities": []}, {"text": "5 . Coder - POLAR\u03c1 alignment in preferences is also signi\ufb01cant for describing emoji with emoji and words ( E / M ) .", "entities": []}, {"text": "4.2.4 Observations Emoji .", "entities": []}, {"text": "As a byproduct , we also show if emoji opposites are preferred over words .", "entities": []}, {"text": "That is , we focus on the mixed campaigns describing words and emoji with words and emoji ( * /M).\u03b1 ( W / W ) ( W / M ) ( W / E ) ( E / W ) ( E / M ) ( E / E ) Selection 0.44 0.35 0.24 0.46 0.39 0.55 Preference 0.57 0.41 0.34 0.61 0.54 0.60 Preference 0.65 0.52 0.40 0.70 0.64 0.68 POLAR\u03c1only Preference 0.31 0.17 0.25 0.31 0.22 0.22 random only Table 2 : Inter - rater agreement Krippendorff \u2019s \u03b1across campaigns .", "entities": []}, {"text": "Coders achieve the best agreement in selection test of emoji - based campaigns ( E/ * ) and generally within the preference test measuring differential scales .", "entities": []}, {"text": "We establish a baseline by \ufb01ltering the counts for all non - POLAR\u03c1randomly chosen dimensions being word or emoji representing a Bernoulli experiment .", "entities": []}, {"text": "I.e. , along the random dimensions , our coders chose 228 vs. 221 and 167 vs. 187 words over emoji .", "entities": []}, {"text": "Applying chi - squared statistics indicates , that both types ( words and emoji ) are chosen equally often at least can not be rejected .", "entities": []}, {"text": "We next analyze the POLAR\u03c1chosen dimensions in the mixed campaigns .", "entities": []}, {"text": "Here , coders chose words over emoji as follows : 465 vs. 336 in the ( W / M ) , and 414 vs. 482 in the ( E / M ) campaign .", "entities": []}, {"text": "We \ufb01nd statistically signi\ufb01cant favors for words to interpret words and emoji to describe emoji .", "entities": []}, {"text": "Scale Usage .", "entities": []}, {"text": "We \ufb01nd no evidence for any directional biases within our preference test ( cf . 3c ) .", "entities": []}, {"text": "Coder Agreement .", "entities": []}, {"text": "While the aggregate results are compelling , we use the Krippendorff - alpha metric to measure coder agreement along all six campaigns as shown Tab . 2 ; higher scores depict better agreement .", "entities": [[12, 13, "HyperparameterName", "alpha"]]}, {"text": "We split the overall results by test \ufb01rst ( Selection & Preference ) , but also show additional agreement results for preferences along POLAR\u03c1 chosen dimensions and their random counterpart .", "entities": []}, {"text": "Most agreement is within the moderate regime .", "entities": []}, {"text": "This observation does not come unexpected from our \ufb01ve non - expert classi\ufb01ers per task .", "entities": []}, {"text": "Overall , we \ufb01nd that coders agree better for well - performing campaigns .", "entities": []}, {"text": "We identify the best agreement scores for interpreting emoji with emoji ( E / E ) ; coders agree least in the worst performing explaining words with emoji campaign ( W / E).7", "entities": []}, {"text": "For the preference test , we subdivide our results into POLAR\u03c1chosen differentials and compare them to the randomly chosen ones .", "entities": []}, {"text": "While the agreement on the random opposites is only fair , the agreement on POLAR\u03c1chosen opposites is consistently better : Estimating differential scale directions via POLAR\u03c1for words yields moderate agreement , whereas coders consistently align substantially in interpreting emoji .", "entities": []}, {"text": "We presume emoji may convey limited ideas , but are easier to grasp , have better readability ; the campaings interpreting emoji ( E/ * ) were generally accomplished faster .", "entities": []}, {"text": "4.2.5 Demographics Though we are con\ufb01dent in applied QA measures , none of the demographics can be con\ufb01rmed .", "entities": []}, {"text": "The annotator sample - size is small and thus most likely not representative .", "entities": []}, {"text": "Further , we \ufb01nd most workers providing contrasting answers across multiple tasks they participated in , rendering collected demographic information unusable .", "entities": []}, {"text": "5 Related Work No universal meaning of emoji .", "entities": []}, {"text": "Prior work showed that the interpretation of emoji varies ( Miller et al . , 2016 ; Kimura - Thollander and Kumar , 2019 ) , also between cultures ( Guntuku et al . , 2019 ; Gupta et al . , 2021 ) .", "entities": [[21, 22, "DatasetName", "Kumar"]]}, {"text": "Even within the same culture , ambiguity and double meanings of emoji exist ( Reelfs et al . , 2020 ) and differences exists on the basis of an individual usage ( Wiseman and Gould , 2018 ) .", "entities": []}, {"text": "These observations motivate the need to better understand the meaning of emoji .", "entities": []}, {"text": "Currently , no data - driven approach exists to make emoji interpretable \u2014 a gap that we aim to close .", "entities": []}, {"text": "Interpretable word embeddings .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "Word embeddings are a common approach to capture meaning ; they are a learned vector space representation of text that carries semantic relationships as distances between the embedded words .", "entities": [[0, 2, "TaskName", "Word embeddings"]]}, {"text": "A rich body of work aims at making word embeddings interpretable , e.g. , via contextual ( Subramanian et al . , 2018 ) , sparse embeddings ( Panigrahi et al . , 2019 ) , or learned ( Senel et al . , 2018 ) transformations ( Mathew et al . , 2020)\u2014all focus on text only .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "Recently , ( Mathew et al . , 2020 ) proposed the POLAR that takes a word embedding as input and creates a new interpretable embedding on a polar subspace .", "entities": []}, {"text": "The POLAR approach is similar to SEMCAT ( Senel et al . , 2018 ) , but is based on the concept of semantic differentials ( Osgood et al . , 1957 ) for creating a polar subspace .", "entities": [[6, 7, "DatasetName", "SEMCAT"]]}, {"text": "It measuresthe meaning of abstract concepts by relying on opposing dimensions associated ( good vs. bad , hot vs. cold , conservative vs. liberal ) .", "entities": []}, {"text": "In this work , we extend and use POLAR .", "entities": []}, {"text": "Emoji embeddings .", "entities": []}, {"text": "Few works focused on using word embeddings for creating emoji representations , e.g. , ( Eisner et al . , 2016 ) or ( Reelfs et al . , 2020 ) .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "( Barbieri et al . , 2016 ) used a vector space skip - gram model to infer the meaning of emoji in Twitter data ( Barbieri et al . , 2016 ) .", "entities": []}, {"text": "Yet , the general question if the interpretability of word embeddings can be improved by adding emoji and if different meaning of emoji can be captured remains still open .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "In this work , we adapt the POLAR interpretability approach to emoji and study in a human subject experiment if word embeddings can be made interpretable by adding emoji and how emoji can be interpretated by emoji .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "6 Conclusion We raise the question whether we can leverage the expressiveness of emoji to make word embeddings interpretable .", "entities": [[16, 18, "TaskName", "word embeddings"]]}, {"text": "Thus , we use the POLAR framework ( Mathew et al . , 2020 ) that creates interpretable word embeddings through semantic differentials , polar opposites .", "entities": [[18, 20, "TaskName", "word embeddings"]]}, {"text": "We employ a revised POLAR\u03c1method that transforms arbitrary word embeddings to interpretable counterparts to which we added emoji .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "We base our evaluation on an off the shelf word - emoji embedding from a large social media corpus , resulting in an interpretable embedding based on semantic differentials , i.e. , antonym lists and polar emoji opposites .", "entities": []}, {"text": "Via crowdsourced campaigns , we investigate the interpretable word - emoji embedding quality along six use - cases ( cf .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "1 ): Using word- & emoji - polar opposites ( or both Mixed ) , to interpret words ( W / W , W / E , W / M ) and emoji ( E / W , E / E , E / M ) , w.r.t .", "entities": []}, {"text": "human interpretability .", "entities": []}, {"text": "Overall , we \ufb01nd POLAR\u03c1 \u2019s interpretations w / wo emoji being well in line with human judgement .", "entities": []}, {"text": "We show that explaining emoji with emoji ( E / E ) works statistically signi\ufb01cantly best , whereas describing words with emoji ( W / E ) systematically yields the worst performance .", "entities": []}, {"text": "We also \ufb01nd good alignment to human judgement estimating a term \u2019s position on differential scales , using the POLAR\u03c1 - projection .", "entities": []}, {"text": "That is , emoji can improve POLAR\u03c1 \u2019s capability in identifying most interpretable semantic differentials .", "entities": []}, {"text": "We have demonstrated how emoji can be used to interpret other emoji using POLAR\u03c1.8", "entities": []}, {"text": "Acknowledgements We thank Felix Dommes , who was instrumental for this work by developing and implementing the POLAR\u03c1projection approach and the Extremal Word Score in his Master Thesis .", "entities": [[23, 24, "MetricName", "Score"]]}, {"text": "p\u2212zpz p\u2212zpz p\u2212zpz p\u2212zpz Table 3 : Used heuristically identi\ufb01ed polar emoji opposites(p\u2212z , pz)\u2208Pemoji .", "entities": []}, {"text": "We opted for a diverse set of opposites selecting only few facial emoji differentials .", "entities": []}, {"text": "References F. Barbieri , F. Ronzano , and H. Saggion .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "What does this Emoji Mean ?", "entities": []}, {"text": "A Vector Space Skip - Gram Model for Twitter Emojis .", "entities": []}, {"text": "In LREC .", "entities": []}, {"text": "Ben Eisner , Tim Rockt\u00e4schel , Isabelle Augenstein , Matko Bo\u0161njak , and Sebastian Riedel . 2016 .", "entities": []}, {"text": "emoji2vec : Learning emoji representations from their description .", "entities": []}, {"text": "In Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media , Austin , TX , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Sharath Chandra Guntuku , Mingyang Li , Louis Tay , and Lyle H Ungar .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Studying cultural differences in emoji usage across the east and the west .", "entities": []}, {"text": "In Proceedings of the International AAAI Conference on Web and Social Media , volume 13 , pages 226 \u2013 235 . Mitali Gupta , Damir D Torrico , Graham Hepworth , Sally L Gras , Lydia Ong , Jeremy J Cottrell , and Frank R Dunshea .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Differences in hedonic responses , facial expressions and self - reported emotions of consumers using commercial yogurts : A cross - cultural study .", "entities": []}, {"text": "Foods , 10(6):1237 .", "entities": []}, {"text": "Birgit Hamp and Helmut Feldweg .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "GermaNet - a lexical - semantic net for German .", "entities": []}, {"text": "In Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications .", "entities": []}, {"text": "T. Hu , H. Guo , H. Sun , T. T. Nguyen , and J. Luo . 2017 .", "entities": []}, {"text": "Spice up your chat : the intentions and sentiment effects of using emojis .", "entities": []}, {"text": "In ICWSM .Philippe", "entities": []}, {"text": "Kimura - Thollander and Neha Kumar .", "entities": [[5, 6, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Examining the \" global \" language of emojis : Designing for cultural representation .", "entities": []}, {"text": "In Proceedings of the 2019 CHI conference on human factors in computing systems , pages 1\u201314 .", "entities": []}, {"text": "Binny Mathew , Sandipan Sikdar , Florian Lemmerich , and Markus Strohmaier .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "The polar framework : Polar opposites enable interpretability of pre - trained word embeddings .", "entities": [[12, 14, "TaskName", "word embeddings"]]}, {"text": "In WWW , pages 1548\u20131558 .", "entities": []}, {"text": "Hannah Miller , Jacob Thebault - Spieker , Shuo Chang , Isaac Johnson , Loren Terveen , and Brent Hecht .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "\" blissfully happy \" or \" ready to \ufb01ght \" : Varying interpretations of emoji .", "entities": []}, {"text": "P. .", "entities": []}, {"text": "Novak , J. Smailovi \u00b4 c , B. Sluban , and I. Mozeti \u02c7c . 2015 .", "entities": []}, {"text": "Sentiment of emojis .", "entities": []}, {"text": "PloS one .", "entities": [[0, 1, "DatasetName", "PloS"]]}, {"text": "C.E. Osgood , G.J. Suci , and P.H. Tannenbaum .", "entities": []}, {"text": "1957 .", "entities": []}, {"text": "The Measurement of Meaning .", "entities": []}, {"text": "Illini Books , IB47 .", "entities": []}, {"text": "University of Illinois Press .", "entities": []}, {"text": "Abhishek Panigrahi , Harsha Vardhan Simhadri , and Chiranjib Bhattacharyya .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Word2sense : sparse interpretable word embeddings .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "In ACL .", "entities": []}, {"text": "Jens Helge Reelfs , Oliver Hohlfeld , Markus Strohmaier , and Niklas Henckell .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Wordemoji embeddings from large scale messaging data re\ufb02ect real - world semantic associations of expressive icons .", "entities": []}, {"text": "Workshop Proceedings of the 14th International AAAI Conference on Web and Social Media .", "entities": []}, {"text": "Peter E Rossi , Zvi Gilula , and Greg M Allenby .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Overcoming scale usage heterogeneity .", "entities": []}, {"text": "Journal of the American Statistical Association , 96(453 ) .", "entities": []}, {"text": "Lut\ufb01 Kerem Senel , Ihsan Utlu , Veysel Yucesoy , Aykut Koc , and Tolga Cukur .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Semantic structure and interpretability of word embeddings .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "In IEEE / ACM TASLP , 10 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Vered Shwartz , Enrico Santus , and Dominik Schlechtweg . 2017 .", "entities": []}, {"text": "Hypernyms under siege : Linguistically - motivated artillery for hypernymy detection .", "entities": []}, {"text": "In EACL , pages 65\u201375 .", "entities": []}, {"text": "Leonard Simms , Kerry Zelazny , Trevor Williams , and Lee Bernstein .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Does the number of response options matter ?", "entities": []}, {"text": "psychometric perspectives using personality questionnaire data .", "entities": []}, {"text": "Psychological Assessment , 31 .", "entities": []}, {"text": "Rion Snow , Brendan O\u2019connor , Dan Jurafsky , and Andrew Y Ng . 2008 .", "entities": []}, {"text": "Cheap and fast \u2013 but is it good ?", "entities": []}, {"text": "evaluating non - expert annotations for natural language tasks .", "entities": []}, {"text": "In EMNLP .", "entities": []}, {"text": "Anant Subramanian , Danish Pruthi , Harsh Jhamtani , Taylor Berg - Kirkpatrick , and Eduard Hovy .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Spine : Sparse interpretable neural embeddings .", "entities": []}, {"text": "In AAAI , volume 32.9", "entities": []}, {"text": "Thomas Tullis and William Albert . 2008 .", "entities": []}, {"text": "Measuring the User Experience : Collecting , Analyzing , and Presenting Usability Metrics .", "entities": []}, {"text": "Morgan Kaufmann Publishers Inc. , San Francisco , CA , USA .", "entities": []}, {"text": "Sarah Wiseman and Sandy JJ Gould .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Repurposing emoji for personalised communication : Why means \u201c i love you \u201d .", "entities": []}, {"text": "In Proceedings of the 2018 CHI conference on human factors in computing systems , pages 1\u201310.10", "entities": []}]