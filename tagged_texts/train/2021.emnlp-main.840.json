[{"text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 10758\u201310768 November 7\u201311 , 2021 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2021 Association for Computational Linguistics10758Multivalent Entailment Graphs for Question Answering Nick M cKennayLiane GuillouyMohammad Javad Hosseiniyz\u0003", "entities": [[8, 10, "TaskName", "Question Answering"]]}, {"text": "Sander Bijl de VroeyMark JohnsonxMark Steedmany yUniversity of EdinburghzAlan Turing Institute , UKxOracle Digital Assistant { nick.mckenna , javad.hosseini , sbdv}@ed.ac.uk { lguillou , steedman}@inf.ed.ac.uk mark.mj.johnson@oracle.com", "entities": []}, {"text": "Abstract Drawing inferences between open - domain natural language predicates is a necessity for true language understanding .", "entities": []}, {"text": "There has been much progress in unsupervised learning of entailment graphs for this purpose .", "entities": []}, {"text": "We make three contributions : ( 1 ) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies , like DEFEAT ( Biden , Trump ) \u000fWIN(Biden ) ; ( 2 ) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of open - domain predicates ; and ( 3 ) we demonstrate the capabilities of these graphs on a novel question answering task .", "entities": [[66, 68, "TaskName", "question answering"]]}, {"text": "We show that directional entailment is more helpful for inference than non - directional similarity on questions of \ufb01ne - grained semantics .", "entities": []}, {"text": "We also show that drawing on evidence across valencies answers more questions than by using only the same valency evidence .", "entities": []}, {"text": "1 Introduction We are reading a mystery about a dark and foreboding manor and have one question : \u201c is Mr. Boddy dead?\u201d1Our text might say \u201c Colonel Mustard killed Mr. Boddy , \u201d or \u201c Mr. Boddy was murdered in the kitchen with a candlestick , \u201d either of which answers the question , but only via natural language inference .", "entities": [[58, 61, "TaskName", "natural language inference"]]}, {"text": "An Entailment Graph ( EG ) is a structure of meaning postulates supporting these inferences such as \u201c if A kills B , then B is dead . \u201d", "entities": []}, {"text": "Entailment Graphs contain vertices of opendomain natural language predicates and entailments between them are represented as directed edges .", "entities": []}, {"text": "Previous models learn predicates of a single valency , the number and types of arguments controlled by the predicate .", "entities": []}, {"text": "Commonly these are binary graphs , which can not model single - argument predicates like the entity states \u201c is dead \u201d or \u201c is an \u0003Now at Google Research .", "entities": [[28, 29, "DatasetName", "Google"]]}, {"text": "1The murder mystery board game Clue ( also known as Cluedo ) lends inspiration to this project.author . \u201d", "entities": []}, {"text": "This means they miss a variety of entailments in text that could be used to answer questions such as our example .", "entities": []}, {"text": "The Distributional Inclusion Hypothesis ( DIH ) ( Dagan et al . , 1999 ; Kartsaklis and Sadrzadeh , 2016 ) is a theory which has been used effectively in unsupervised learning of these same - valency entailment graphs ( Geffet and Dagan , 2005 ; Berant et", "entities": []}, {"text": "al . , 2010 ; Hosseini , 2021 ) .", "entities": []}, {"text": "In this work the DIH is reinterpreted in a way which supports learning entailments between predicates of different valencies such as KILL ( Mustard , Boddy ) \u000fDIE(Boddy ) .", "entities": []}, {"text": "We extend the work of Hosseini et al .", "entities": []}, {"text": "( 2018 ) and develop a new Multivalent Entailment Graph ( MGraph ) where vertices may be predicates of different valencies .", "entities": []}, {"text": "This results in new kinds of entailments that answer a broader range of questions including entity state .", "entities": []}, {"text": "We further pose a true - false question answering task generated automatically from news text .", "entities": [[7, 9, "TaskName", "question answering"]]}, {"text": "Our model draws inferences across propositions of different valencies to answer more questions than using same - valence entailment graphs .", "entities": []}, {"text": "We also compare with several baselines , including unsupervised pretrained language models , and show that our directional entailment graphs succeed over non - directional similarity measures in answering questions of \ufb01ne - grained semantics .", "entities": [[9, 12, "TaskName", "pretrained language models"]]}, {"text": "Advantageously , EGs are structures designed to be queried , so they are inherently explainable .", "entities": []}, {"text": "This research is conducted in English , but as an unsupervised algorithm it may be applied to other languages given a parser and named entity linker .", "entities": []}, {"text": "2 Background The task of recognizing textual entailment ( Dagan et al . , 2006 ) requires models to predict a relation between a text T and hypothesis H ; \u201c T entails H if , typically , a human reading T would infer that H is most likely true . \u201d", "entities": []}, {"text": "From here , research has moved in several directions .", "entities": []}, {"text": "We study predicates , including verbs and phrases that apply to arguments .", "entities": []}, {"text": "10759Research in predicate entailment graphs has evolved from \u201c local \u201d learning of entailment rules ( Geffet and Dagan , 2005 ; Szpektor and Dagan , 2008 ) to later work on joint learning of \u201c globalized \u201d rules , overcoming sparsity in local graphs ( Berant et al . , 2010 ; Hosseini et al . , 2018 ) .", "entities": []}, {"text": "These graphs frequently rely on the DIH for the local learning step to learn initial predicate entailments .", "entities": []}, {"text": "The DIH states that for some predicates p and q , if the contextual features of p are included in those of q , then p entails q ( Geffet and Dagan , 2005 ) .", "entities": []}, {"text": "In previous work predicate arguments are successfully used as these contextual features , but only predicates of the same valency are considered ( e.g. binary predicates entail binary ; unary entail unary ) , and further research computes additional edges in these same - valency graphs such as with link prediction ( Hosseini et al . , 2019 ) .", "entities": [[49, 51, "TaskName", "link prediction"]]}, {"text": "However , this leaves out crucial inferences that cross valencies such as the kill / die example , which are easy for humans .", "entities": []}, {"text": "We generalize the DIH to learn entailments within and across valencies .", "entities": []}, {"text": "Typing is very helpful for entailment graph learning ( Berant et al . , 2010 ; Lewis and Steedman , 2013 ; Hosseini et al . , 2018 ) .", "entities": [[6, 8, "TaskName", "graph learning"]]}, {"text": "Inducing a type for each entity such as \u201c person , \u201d \u201c location , \u201d etc . enables generalized learning across instances and disambiguates word sense , e.g. \u201c running a company \u201d has different entailments than \u201c running code . \u201d", "entities": []}, {"text": "We compare our model to several baselines , including strong pretrained language models in an unsupervised setting using similarity .", "entities": [[10, 13, "TaskName", "pretrained language models"]]}, {"text": "BERT ( Devlin et al . , 2019 ) generates impressive word representations , even unsupervised ( Petroni et al . , 2019 ) , which we compare with on a task of predicate inference .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "We further test RoBERTa ( Liu et al . , 2019 ) to show the impact of robust in - domain pretraining on the same architecture .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}, {"text": "These non - directional similarity models provide a strong baseline for evaluating directional entailment graphs .", "entities": []}, {"text": "3 Multivalent Distributional Inclusion Hypothesis We pose a new , multivalent interpretation of the DIH ( the MDIH ) which models the entailment of predicates across valencies .", "entities": []}, {"text": "The intuition comes from observing eventualities ( Vendler , 1967 ) which occur in the world .", "entities": []}, {"text": "Neo - Davidsonian semantics ( Davidson , 1967 ; Maienborn , 2011 ) explains that a textual predicate , its arguments , and adjuncts , are all properties of an underlying event variable .", "entities": []}, {"text": "En - tailments about one or more of the arguments arise from their roles in this eventuality .", "entities": []}, {"text": "We may infer that \u201c Mr. Boddy died \u201d due to his role as a direct object in the killing / murdering event .", "entities": []}, {"text": "No other information is needed , including who murdered Mr. Boddy , where , or with what instrument .", "entities": []}, {"text": "Boddy is dead simply because he was murdered .", "entities": []}, {"text": "We build on this insight to develop the MDIH .", "entities": []}, {"text": "Here , a predicate is represented ( as in \u00a7 2 ) by features which are the argument tuples it appears with .", "entities": []}, {"text": "We recognize a tuple as a proxy for a world event , e.g. VISIT ( Obama , Hawaii ) identi\ufb01es one instance of a real VISIT event .", "entities": []}, {"text": "Our method learns by tracking entity tuples across events in the world .", "entities": []}, {"text": "The MDIH signals an entailment from a premise p to hypothesis hif , distributionally , subtuples of p are always found amongst tuples of h. Crucially , we allowhto drop in valency so that we learn entailments about subsets of p \u2019s arguments .", "entities": []}, {"text": "We now formalize the MDIH and then illustrate with an example .", "entities": []}, {"text": "We de\ufb01ne the argument tuple structures for a premise and hypothesis predicate : P = f(ak;1;:::;a k;I)jk2f1;:::;Mgg H = f(bk;1;:::;b k;J)jk2f1;:::;Ngg Pis a set ofMargument tuples ( each of size I ) which correspond to instances of a premise predicatep .", "entities": []}, {"text": "His a set ofNargument tuples ( each of sizeJ ) representing the same for hypothesis h.", "entities": []}, {"text": "We limitJ\u0014I , e.g. we learn about relations on realized entity subsets .", "entities": []}, {"text": "We do not learn entailments to higher valencies ( such as a unary entailing a binary ) because additional arguments must be existential , not real .", "entities": []}, {"text": "We leave this to future work .", "entities": []}, {"text": "To select argument subtuples from tuples in P , we de\ufb01ne a vector of indices jwith length J , which selects arguments by position .", "entities": []}, {"text": "For example , withj=", "entities": []}, {"text": "[ 2;3 ] , performP[:;j ] .", "entities": []}, {"text": "For each argument tuple inP , select just the 2nd and 3rd arguments , forming a new set of 2 - tuples .", "entities": []}, {"text": "We de\ufb01ne the Multivalent Distributional Inclusion Hypothesis : IfP[:;j]\u0012H[:;m(j)],thenp\u000fh Herem : NJ!NJis a simple bijective mapping from argument indices of ptoh .", "entities": []}, {"text": "An example wheremis needed for argument swapping is \u201c x boughty \u201d entails \u201c ysold tox . \u201d", "entities": []}, {"text": "We illustrate by working the kill / die example on a hypothetical corpus .", "entities": []}, {"text": "We might \ufb01nd that", "entities": []}, {"text": "10760KILL(x;y)\u000fDIE(y)by trying j= [ 2 ] and m([2 ] )", "entities": []}, {"text": "=", "entities": []}, {"text": "[ 1 ] .", "entities": []}, {"text": "We start with P , all 2 - tuples of killings , andH , all 1 - tuples of dyings and apply j andm .", "entities": []}, {"text": "We may \ufb01nd that selecting arg 2 from all tuples inPforms a subset of the selection of arg 1 from tuples in H.", "entities": []}, {"text": "Though dyings may happen in many ways , we observe that arg 2 of a killing often occurs elsewhere in the corpus with a dying , and thus we infer the entailment between predicates .", "entities": []}, {"text": "Intuitively this is true for arbitrarily large valencies : MURDER ( Mustard , Boddy , kitchen , candlestick ) entails KILL ( Mustard , Boddy ) and both entail DIE(Boddy ) .", "entities": []}, {"text": "Though arguments may be dropped from the premise , they still in\ufb02uence entailments .", "entities": []}, {"text": "This is because the MDIH tracks eventualities .", "entities": []}, {"text": "\u201c Person writing a book \u201d is a different kind of event than \u201c person writing software \u201d with a different distribution of argument tuples , so we learn that the former entails being an author , while the latter entails being a programmer .", "entities": []}, {"text": "4 Learning Multivalent Graphs We de\ufb01ne an Entailment Graph as a directed graph of predicates and their entailments , G= ( V;E ) .", "entities": []}, {"text": "The vertices Vare the set of predicates , where each argument has a type from the set of 49 FIGER base typesT , e.g. TRAVEL .TO(:person , : location ) 2V , and : person , : location 2 T .", "entities": [[19, 20, "DatasetName", "FIGER"]]}, {"text": "The directed edges areE = f(v1;v2)jv1;v22Vifv1\u000fv2 g , or all entailments between vertices in V. In Multivalent Entailment Graphs we expand V to contain predicates of both 1- and 2 - valency , and Eto edges between these vertices , described as follows .", "entities": []}, {"text": "Let bi;bj2Vbe distinct binary predicates andui;uj2Vbe unary predicates .", "entities": []}, {"text": "De\ufb01ne Eas the set of all entities in the world , and some particular entities x;y2E to illustrate argument slots .", "entities": []}, {"text": "Econtains these patterns of entailment : 1.bi(x;y)\u000fbj(x;y)orbi(x;y)\u000fbj(y;x ) Binary entails binary ( BBentailments ) 2.bi(x;y)\u000fui(x)orbi(x;y)\u000fui(y ) Binary entails unary of one argument ( BU ) 3.ui(x)\u000fuj(x )", "entities": []}, {"text": "Unary entails unary ( UU ) Predicates with valence > 2 are sparse in the text , but are also included in the MGraph by decomposing them into binary relations between pairs of entities .", "entities": []}, {"text": "This is another application of our Multivalent DIH .", "entities": []}, {"text": "We maintain argument roles , so eachbinary is a window into its higher - valency predicate , allowing higher - valency predicates to entail lower binaries and unaries .", "entities": []}, {"text": "To learn these new kinds of connections we develop a method of local entailment rule learning using the MDIH .", "entities": []}, {"text": "As in \u00a7 2 , the local step learns the initial directed edges of the entailment graph , which are further improved with global learning .", "entities": []}, {"text": "Our local step learns entailments by machine - reading the NewsSpike corpus ( 2.3 GB ) , which contains 550 K news articles , or over 20 M sentences ( Zhang and Weld , 2013 ) .", "entities": []}, {"text": "NewsSpike consists of multi - source news articles collected within a \ufb01xed timeframe , and due to these properties the articles frequently discuss the same events but phrased in different ways , providing appropriate training evidence .", "entities": []}, {"text": "\u201c This evening Mr. Boddy was killed . \u201d )", "entities": []}, {"text": "KILL .2(Mr.-Boddy )", "entities": []}, {"text": "Figure 1 : A sentence is CCG parsed , formed into a dependency graph ( shown ) using CCG dependencies , and traversed to extract a unary relation .", "entities": []}, {"text": "MoNTEE traverses from a predicate to all connected arguments .", "entities": []}, {"text": "4.1 Extraction of Predicate Relations Our pipeline processes raw article text into a list of propositions : predicates with associated typed arguments .", "entities": []}, {"text": "We use the MoNTEE system ( Bijl de Vroe et al . , 2021 ) to extract natural language relations between entities from raw text2 .", "entities": []}, {"text": "This system \ufb01rst parses sentences using the RotatingCCG parser ( Stanojevi \u00b4 c and Steedman , 2019 ) ( Combinatory Categorial Grammar ; Steedman , 2000 ) and then forms dependency graphs from the parses .", "entities": []}, {"text": "Fi2We disable modality tagging in our experiments .", "entities": []}, {"text": "10761nally , it traverses these graphs to extract the relations , each consisting of a predicate and its arguments .", "entities": []}, {"text": "Figure 1 shows an example dependency graph and the relation extracted from it .", "entities": []}, {"text": "Arguments may be either named entities3or general entities ( noun phrases ) .", "entities": []}, {"text": "These entities are mapped to types by linking to their Freebase IDs ( Bollacker et al . , 2008 ) using AIDA - Light ( Nguyen et al . , 2014 ) , and mapping the IDs to the 49 base FIGER types ( Ling and Weld , 2012 ) .", "entities": [[41, 42, "DatasetName", "FIGER"]]}, {"text": "Both binary and unary relations are extracted from the corpus if they contain at least one named entity , which helps anchor to a real - world event .", "entities": []}, {"text": "This poses a challenge as noted by Szpektor and Dagan ( 2008 ) .", "entities": []}, {"text": "While binary predicates may be extracted from dependency paths between two entities , unary predicates only have one endpoint , so we must carefully apply linguistic knowledge to extract meaningful unary relations .", "entities": []}, {"text": "We extract these neo - Davidsonian event cases : \u2022One - argument verbs including intransitives , e.g. \u201c Knowles sang\u201d)SING .1(Knowles ) and passivized transitives , e.g. \u201c Bill H.R. 1 was passed \u201d ) PASS .2(Bill - HR1 ) \u2022Copular constructions , where copular \u201c be \u201d acts as the main verb , e.g. \u201c Chiang is an author \u201d ) BE.AUTHOR .1(Chiang )", "entities": [[35, 36, "DatasetName", "PASS"]]}, {"text": "and where it does not , e.g. \u201c Phelps seems to be the winner \u201d )", "entities": []}, {"text": "SEEM .TO.BE.WINNER .1(Phelps )", "entities": []}, {"text": "As with binaries in earlier work , unary predicates are lemmatized , and tense , aspect , modality , and other auxiliaries are stripped .", "entities": []}, {"text": "The CCG argument position which corresponds to its case ( e.g. 1 for nominative , 2 for accusative ) , is appended to the predicate .", "entities": []}, {"text": "Passive predicates are mapped to active ones .", "entities": []}, {"text": "Modi\ufb01ers such as negation and predicates like \u201c planned to \u201d as in \u201c Professor Plum planned to attend \u201d are also extracted in the predicate .", "entities": []}, {"text": "We pay special attention to copular constructions , which always introduce stative predicates , rather than events ( Vendler , 1967 ) .", "entities": []}, {"text": "These are interesting for modeling the properties of entities .", "entities": []}, {"text": "4.2 Learning Local Graphs In previous entailment graph research ( Hosseini et al . , 2018 ) a representation vector is computed for each typed predicate in the graph .", "entities": []}, {"text": "These 3Identi\ufb01ed by the CoreNLP Named Entitiy Recogniser ( Manning et al . , 2014 ; Finkel et al . , 2005).are compared via the DIH to establish entailment edges between predicates .", "entities": []}, {"text": "The features of each vector are typically based on the argument pairs seen with that predicate .", "entities": []}, {"text": "Speci\ufb01cally , for a typed predicatepwith corresponding vector v , vconsists of features fiwhich are the pointwise mutual information ( PMI ) of pand the argument pair ai2 f(em;en)jem2 Et1;en2 Et2 g. Here t1;t22 T , andEtis the subset of entities of type t.", "entities": []}, {"text": "For example , the predicate BUILD (: company , : thing ) might have some feature f37 , the PMI of \u201c build \u201d with argument pair ( Apple , iPhone ) .", "entities": []}, {"text": "A Balanced Inclusion ( BInc ) score is calculated for the directed entailment from one predicate to another ( Szpektor and Dagan , 2008 ) .", "entities": []}, {"text": "BInc is the geometric mean of two subscores : a directional score , Weeds Precision ( Weeds and Weir , 2003 ) , measuring how much one vector \u2019s features \u201c cover \u201d the other \u2019s ; and a symmetrical score , Lin Similarity ( Lin , 1998 ) , which downweights infrequent predicates that cause spurious false positives .", "entities": [[14, 15, "MetricName", "Precision"]]}, {"text": "In this work we compute local binary graphs following Hosseini et al .", "entities": []}, {"text": "( 2018 ) and leverage the new MDIH to compute additional entailments for unaries and between valencies .", "entities": []}, {"text": "To do this we compute a vector for each argument slot respecting its position in the predicate .", "entities": []}, {"text": "For a predicate p , a slot vector v(s)fors2f1;2gconsists of features f(s ) i. We de\ufb01ne \u001c ( p;s ) = t , the type of slot sin predicate p. Eachf(s ) iis the PMI of pand the argument in slot s , a(s ) i2Et .", "entities": []}, {"text": "Slot vectors are computed for the slot in unary relations and both slots in binaries .", "entities": []}, {"text": "Each slot vector for phas sizejv(s)j = jEtj , the number of entities in the data with the same type t. Continuing the example , we calculate two vectors for BUILD (: company , : thing ): v(1)2RjE : companyj which contains a feature for Apple , and v(2)2 RjE : thingjwhich contains a feature for iPhone .", "entities": []}, {"text": "Slot vectors are comparable if they represent the same entity type .", "entities": []}, {"text": "Edges are learned by comparing corresponding slot vectors between predicates .", "entities": []}, {"text": "For instance , DEFEAT (: person1 , : person2 ) \u000fBE.WINNER (: person1)4is learned by comparing the slot 1 vector of DEFEAT with the slot 1 vector ofBE.WINNER .", "entities": []}, {"text": "If the entities who have defeated someone are usually found amongst the entities who are winners then we get a high BInc score , indicating defeat entails that its subject is a winner .", "entities": []}, {"text": "Figure 2 illustrates a Multivalent Graph .", "entities": []}, {"text": "This 4Here we number the typed arguments for demonstration to show which : person argument is in the entailment .", "entities": []}, {"text": "10762includes Bivalent Graphs which contain the entailments of binary predicates ( BB and BU edges ) , and separate Univalent Graphs which contain the entailments of unary predicates ( only UU edges , since we do not allow a unary to entail a binary ) .", "entities": []}, {"text": "We follow previous research and learn separate disjoint subgraphs for each typing , up to jTj2bivalent andjTjunivalent subgraphs given enough data .", "entities": []}, {"text": "For example , we learn a bivalent (: person , : location ) graph containing binary predicates such asFLY.INTO (: person , : location ) which may entail unaries like BE.AIRPORT (: location ) .", "entities": []}, {"text": "Because a unary has only one type tiit may be entailed by binaries in up to 2\u0003jTj\u0000 1subgraphs with typesf(ti;tj)jj2 Tg , i.e. all bivalent graphs containing type ti .", "entities": []}, {"text": "We learn entailments from unaries ( UU ) in separate 1 - type univalent graphs .", "entities": []}, {"text": "This ef\ufb01ciently learns one set of entailments for each unary , but allows them to be freely entailed by higher - valency predicates , e.g. binaries .", "entities": []}, {"text": "Bivalent graphs point transitively into univalent graphs .", "entities": []}, {"text": "In Figure 2 , DEFEAT (: person1 , : person2 ) \u000f BE.WINNER (: person1 ) in the person - person graph .", "entities": []}, {"text": "E.g. further entailments of BE.WINNER (: person ) are in the person univalent graph .", "entities": []}, {"text": "Person - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event GraphPerson - Event Graph BE.WINNER (: person)WIN.IN(:person , : event )", "entities": []}, {"text": "Person - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person GraphPerson - Person Graph BE.WINNER (: person1)DEFEAT (: person1 , : person2 ) OBLITERATE (: person1 , : person2 ) Bivalent Graphs Univalent Graphs Person GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson GraphPerson Graph BE.WINNER (: person ) BE.CHAMPION (: person ) Figure 2 : Bivalent graphs model entailments from binary predicates to equal- and lower - valency predicates ( binary and unary ) .", "entities": []}, {"text": "Univalent graphs model entailments from unaries to equal - valency unary predicates.4.3 Learning Global Graphs Local learning of entailments suffers from sparsity issues which can be improved by further learning of \u201c global \u201d graphs .", "entities": []}, {"text": "We use the soft constraint method of Hosseini et al .", "entities": []}, {"text": "( 2018 ) which has two optimizations .", "entities": []}, {"text": "The paraphrase resolution constraint encourages predicates within the same typed graphs that entail each other to have similar entailment patterns .", "entities": []}, {"text": "The cross - graph constraint additionally encourages compatible predicates across different typed graphs to share entailment patterns .", "entities": []}, {"text": "We apply global learning to bivalent graphs and separately to univalent graphs .", "entities": []}, {"text": "Globalization is valency - agnostic , using just the common structures between predicates , so bivalent graphs can use BB and BU edges to optimize binary predicate entailments .", "entities": []}, {"text": "Final graph size statistics are in Table 1 .", "entities": []}, {"text": "Valency Vertices Edges Bivalent 938 K Binary 94 M BB / 30 M BU Univalent 36 K Unary 3.6 M UU Table 1 : We learn 546 typed bivalent subgraphs which contain entailments of binary predicate antecedents ( BB and BU ) ; and 37 typed univalent subgraphs which contain entailments of unary predicates ( UU ) .", "entities": []}, {"text": "5 Evaluation :", "entities": []}, {"text": "Question Answering We pose an automatically generated QA task to evaluate our model explicitly for directional inference between binary and unary predicates , as we are not aware of any standard datasets for this problem .", "entities": [[0, 2, "TaskName", "Question Answering"]]}, {"text": "Our task is to answer true - false questions about real events that are discussed in the news , for example , \u201c Was Biden elected ? \u201d", "entities": []}, {"text": "These types of questions are surprisingly dif\ufb01cult and frequently require inference to answer ( Clark et al . , 2019 ) .", "entities": []}, {"text": "For this , entailment is especially useful : we must decide if the question ( hypothesis ) is true given a list of propositions from limited news text ( premises ) , which are all likely to be phrased differently .", "entities": []}, {"text": "This task is designed independently of the MGraph as a challenge in information retrieval .", "entities": [[12, 14, "TaskName", "information retrieval"]]}, {"text": "Positive questions made from binary and unary predicates are selected directly from the news text using special criteria , and are then removed .", "entities": []}, {"text": "From these positives we automatically generate false events to use as negatives , which are designed to mimic real , newsworthy events .", "entities": []}, {"text": "The remaining news text is used to answer the questions .", "entities": []}, {"text": "We at-", "entities": []}, {"text": "10763tempt to make every question answerable , but since they are generated automatically there is no guarantee .", "entities": []}, {"text": "However , the task is fair as all models are given the same information .", "entities": []}, {"text": "The additive effects of multivalent entailment should be demonstrated : with more kinds of entailment , the MGraph should \ufb01nd more textual support and answer more questions .", "entities": []}, {"text": "The task is presented on a text sample from NewsCrawl , a multi - source corpus of news articles , to be published separately .", "entities": []}, {"text": "A test set is extracted which contains 700 K sentences from articles over a period of several months , and also a development set from a further 500 K sentences .", "entities": []}, {"text": "We generate questions balanced to a ratio of 50 % binary questions / 50 % unary ; and within each 50 % positive / 50 % negative .", "entities": []}, {"text": "Table 2 shows a sample from the dev set .", "entities": []}, {"text": "We generate 34,394 questions on the test set : 17,256 unary questions and 17,138 binary .", "entities": []}, {"text": "5.1 Question Generation For realism , questions should be both interesting andanswerable using the corpus .", "entities": [[1, 3, "TaskName", "Question Generation"]]}, {"text": "A multi - step process extracts questions from the news text itself .", "entities": []}, {"text": "1 .", "entities": []}, {"text": "Partitioning .", "entities": []}, {"text": "First , the articles are grouped by publication date such that each partition covers a timespan of up to 3 consecutive days of news ( 49 partitions in the test set ) .", "entities": []}, {"text": "We ask yes - no questions about events drawn from the partition , and the news text within this 3 - day window is used as evidence to answer them .", "entities": []}, {"text": "We ask questions as if happening presently in this time window to control for the variable of time , so we can ask ambiguous questions like \u201c Did the Patriots win the Superbowl ? \u201d which may be \u201c true \u201d or not depending on the date and timespan .", "entities": []}, {"text": "The small 3 - day window size was chosen so multiple news stories about an event appear together , increasing the chances of \ufb01nding question answers .", "entities": []}, {"text": "Within each partition we do relation extraction in a process mirroring \u00a7 4.1 .", "entities": [[5, 7, "TaskName", "relation extraction"]]}, {"text": "2 . Selecting Positives .", "entities": []}, {"text": "We adapt a selection process from Poon and Domingos ( 2009 ) to choose good questions which are interesting to a human and answerable from the partition text .", "entities": []}, {"text": "First , we identify repeated entities that star in the events of the articles ; these will yield interesting questions as well as ample textual evidence for answering them .", "entities": []}, {"text": "In each partition we count the mentions of each entity pair ( from binary propositions ) and single entities ( from unary and binary ones ) .", "entities": []}, {"text": "The most frequent entities and pairs mentioned more than 5 times in the partition are selected .", "entities": []}, {"text": "Predicateswhich are mentioned across the entire news corpus 10 times or fewer are \ufb01ltered out ; we assume those remaining are popular to report in news and thus are interesting to a human questioner .", "entities": []}, {"text": "We randomly select propositions featuring both a star entity and predicate to use as questions , and remove them from the partition .", "entities": []}, {"text": "3 . Generating Negatives .", "entities": []}, {"text": "A simple strategy for producing negatives might seem to be substituting random predicates into the positive questions .", "entities": []}, {"text": "However , this is unsatisfactory because modern techniques in NLP excel at detecting unrelated words .", "entities": []}, {"text": "For example , a neural model will easily distinguish a random negative like DETONATE ( Google , YouTube ) from a news text discussing Google \u2019s acquisition of YouTube , classifying it as a false event on grounds of dissimilarity alone .", "entities": [[15, 16, "DatasetName", "Google"], [24, 25, "DatasetName", "Google"]]}, {"text": "To be a meaningful test of inference this task requires that negatives be dif\ufb01cult to discriminate from positives : they should be semantically related but should not logically follow from what is stated in the text .", "entities": []}, {"text": "To this end we derive negative questions from the selected positives using linguistic relations in WordNet ( Fellbaum , 1998 ) .", "entities": []}, {"text": "We assume that news text follows the Gricean cooperative principle of communication ( Davis , 2019 ) , such that it will report what facts are known and nothing more .", "entities": []}, {"text": "To this end , noun hyponyms and their verbal equivalent , troponyms , are mined from the \ufb01rst sense of each positive in WordNet .", "entities": []}, {"text": "For example , we extract \u201c burn \u201d as a troponym of \u201c hurt \u201d and the phrase \u201c inherit from \u201d as a troponym of \u201c receive from . \u201d", "entities": []}, {"text": "We therefore expect that these speci\ufb01c relations will be untrue of the argument tuple in question and may be used as negatives .", "entities": []}, {"text": "We also considered antonyms and other WordNet relations , but these are much sparser in English and have low coverage .", "entities": []}, {"text": "For fairness , generated negatives which actually occur in the current partition are screened out ( 0.1 % of proposed negatives ) , as well as negatives which never occur in the entire corpus ( 76.8 % of proposed negatives ) .", "entities": []}, {"text": "Only challenging negatives are left , which actually do occur in real news text .", "entities": []}, {"text": "See Table 2 for a sample of questions .", "entities": []}, {"text": "In the error analysis we \ufb01nd these negatives to be of good quality : they are uncommonly inferable from the text , accounting for a small percentage of false positives .", "entities": []}, {"text": "5.2 Question Answering Models", "entities": [[1, 3, "TaskName", "Question Answering"]]}, {"text": "In each partition , models receive factual propositions extracted from 3 days of news text to use", "entities": []}, {"text": "10764Positive Negative Did the Ohio State Buckeyes play?Did the Ohio State Buckeyes fumble ?", "entities": []}, {"text": "Was Mitt Romney a candidate ?", "entities": []}, {"text": "Was Mitt Romney a write - in ?", "entities": []}, {"text": "Did voters reject Mike Huckabee?Did voters discredit Mike Huckabee ?", "entities": []}, {"text": "Did Roger Clemens receive from Brian McNamee?Did Roger Clemens inherit from Brian McNamee ?", "entities": []}, {"text": "Table 2 : A sample of dev set questions .", "entities": []}, {"text": "as evidence for answering true - false questions .", "entities": []}, {"text": "A model scores how strongly it can infer the question proposition from each evidence proposition , and we take the maximum score as the model con\ufb01dence of a \u201c true \u201d answer .", "entities": []}, {"text": "Exact - Match .", "entities": []}, {"text": "Our text is multi - source news articles , so world events are often discussed multiple times in the data , even with the same phrasing .", "entities": []}, {"text": "We compute an \u201c exact - match \u201d baseline which shows how many questions can be answered from an exact string match in the text ; the rest require inference .", "entities": []}, {"text": "Binary Entailment Graph .", "entities": []}, {"text": "Our BB model is roughly equivalent to the state of the art binary - tobinary entailment graph ( Hosseini et al . , 2018 ) , so it serves as a baseline for the overall model.5 All graph models look for directed entailments from evidence propositions to the question proposition .", "entities": []}, {"text": "For example , \u201c Was YouTube sold to Google ? \u201d can be answered af\ufb01rmatively by reading \u201c Google bought YouTube \u201d using the graph edge BUY(x;y)\u000fSELL .TO(y;x ) .", "entities": [[8, 9, "DatasetName", "Google"], [18, 19, "DatasetName", "Google"]]}, {"text": "BInc scores range from 0 to 1 ; if no entailments are found we assume it is false ( score of 0 ) .", "entities": [[4, 5, "DatasetName", "0"], [21, 22, "DatasetName", "0"]]}, {"text": "Multivalent Entailment Graph .", "entities": []}, {"text": "The MGraph is made of 3 component models : ( 1 ) the BB model which uses binary evidence to answer binary questions ; ( 2 ) the UU model which uses unary evidence to answer unary questions ; and ( 3 ) the BU model which uses binary evidence to answer unary questions .", "entities": []}, {"text": "The MGraph is able to answer questions using evidence across valencies , e.g. \u201c Is J.K. Rowling an author ? \u201d is af\ufb01rmed by reading \u201c J.K. Rowling wrote The Sorcerer \u2019s Stone \u201d using the graph edge WRITE ( x;y)\u000fBE.AUTHOR ( x ) .", "entities": []}, {"text": "Individually , each model answers only binary or unary qustions , not both .", "entities": []}, {"text": "By combining them all kinds of 5We test the MGraph on the Levy / Holt dataset of 18,407 questions for BB entailment ( Levy and Dagan , 2016 ; Holt , 2018 ) , and achieve similar results to Hosseini et al .", "entities": []}, {"text": "( 2018).questions can be answered using all available evidence .", "entities": []}, {"text": "At each precision level if any component model predicts true , the overall model does too .", "entities": []}, {"text": "In some test instances the entity typer may make an error , and so we fail to \ufb01nd the question predicate in the typed subgraph .", "entities": []}, {"text": "Similarly to Hosseini et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) , in these cases we back off , querying all subgraphs for the untyped predicate and averaging the entailment scores found .", "entities": []}, {"text": "We \ufb01nd 5 % more unary questions and 18 % more binaries .", "entities": []}, {"text": "Similarity Models .", "entities": []}, {"text": "BERT and RoBERTa predicate embeddings ( Devlin et al . , 2019 ; Liu et al . , 2019 ) are used in an unsupervised manner to answer questions based on similarity to the evidence .", "entities": [[0, 1, "MethodName", "BERT"], [2, 3, "MethodName", "RoBERTa"]]}, {"text": "We encode the question into a representation vector , and each evidence proposition with the same arguments .", "entities": []}, {"text": "We compute the cosine similarity between the question and each evidence vector , adjusted to a scale of 0 to 1 : sim ( p;q ) = ( cos(p;q ) + 1)=2 .", "entities": [[18, 19, "DatasetName", "0"]]}, {"text": "To compute each vector encoding we construct a simple natural language sentence from the proposition using its predicate and arguments and encode it with the language model .", "entities": []}, {"text": "Our representation includes only the encoding for the predicate in the context of its arguments , but not the arguments themselves to make this a true test of predicate similarity .", "entities": []}, {"text": "We average all \ufb01nal hidden - state vectors from the model corresponding to the predicate , excluding those of the arguments .", "entities": []}, {"text": "We test the basic BERT model and RoBERTa model , which has robustly pretrained on 160 GB of text ( 76 GB news ) .", "entities": [[4, 5, "MethodName", "BERT"], [7, 8, "MethodName", "RoBERTa"]]}, {"text": "PPDB .", "entities": []}, {"text": "Though supervised , PPDB 2.0 ( we use XXXL ) ( Pavlick et al . , 2015 ) is a useful comparison as it is a large , well - understood resource for phrasal entailment .", "entities": []}, {"text": "PPDB relations come from bilingual pivoting and are categorized using textbased features , which is very different from our argument - tracking method .", "entities": []}, {"text": "We view PPDB as a kind of Entailment Graph with 9 M predicate phrases ( vertices ) and 33 M \u201c Equivalence \u201d and \u201c ForwardEntailment \u201d edges .", "entities": []}, {"text": "We convert evidence and question propositions into a natural text format and extract a PPDB relation score from each evidence phrase to the question .", "entities": []}, {"text": "6 Question Answering Results", "entities": [[1, 3, "TaskName", "Question Answering"]]}, {"text": "The models produce a gradation of judgement scores between 0 ( false ) and 1 ( true ) .", "entities": [[9, 10, "DatasetName", "0"]]}, {"text": "As in earlier work , we slide a classi\ufb01cation threshold over the score range to produce a precision - recall curve for each model .", "entities": []}, {"text": "Results are in Figure 3 ( left ) .", "entities": []}, {"text": "10765 Figure 3 : ( Left ) Overall performance on the QA task .", "entities": []}, {"text": "( Right ) performance on the \ufb01ltered task .", "entities": []}, {"text": "Note that BB , UU , and BU models may individually reach a max recall of 50 % because they answer only binary or unary questions .", "entities": []}, {"text": "Multivalent graph performance is shown incrementally .", "entities": []}, {"text": "The BB model can answer a portion of binary questions ; the UU model can answer more unary questions ; adding the BU model can answer still more unary questions using binary evidence .", "entities": []}, {"text": "We observe successful inference of our kill / die example and others .", "entities": []}, {"text": "\u201c Obama was elected to of\ufb01ce \u201d af\ufb01rms the question \u201c Was Obama a candidate ? \u201d", "entities": []}, {"text": "and \u201c Zach Randolph returned \u201d af\ufb01rms \u201c Did Zach Randolph arrive ? \u201d", "entities": []}, {"text": "Our test set is from multiple sources over the same time period .", "entities": []}, {"text": "The exact - match baseline shows the limitations of answering questions simply by collecting more data ; most questions require inference to answer .", "entities": []}, {"text": "The complete MGraph achieves ~3x this recall by drawing inferences .", "entities": []}, {"text": "Our model achieves higher precision than BERT and RoBERTa similarity models in the low recall range .", "entities": [[6, 7, "MethodName", "BERT"], [8, 9, "MethodName", "RoBERTa"]]}, {"text": "The similarity models perform well , achieving full recall by generalizing for rarer predicates .", "entities": []}, {"text": "We note that RoBERTa bests BERT due to extensive in - domain pretraining .", "entities": [[3, 4, "MethodName", "RoBERTa"], [5, 6, "MethodName", "BERT"]]}, {"text": "The BB model appears to struggle .", "entities": []}, {"text": "In fact 90.5 % of unary questions have a vertex in the graph , but only 64.1 % of binaries do .", "entities": []}, {"text": "The BB model frequently can not answer questions because the question predicate was n\u2019t seen in training .", "entities": []}, {"text": "This difference is because binary predicates are more diverse so suffer more from sparsity : they are often multiword expressions and have a second , typed argument .", "entities": []}, {"text": "Indeed , most binary predicate research ( in symbolic methods ) focuses on only the top 50 % of recall in several datasets ( Berant et al . , 2010 , 2015 ; Levy and Dagan , 2016 ; Hosseini et al . , 2018 ) .", "entities": []}, {"text": "For an even comparison we create a \ufb01ltered question set .", "entities": []}, {"text": "From all questions we remove those without a vertex in the MGraph , then balance them as in \u00a7 5 , resulting in 20,519 questions ( 10,273 unary and 10,246 binary ) .", "entities": []}, {"text": "This \ufb01ltered test directly comparesUnary Questions Binary Questions Model @1451 @2000 @802 @2000 BERT 91.4 % 76.9 % 92.0 % 82.9 % RoBERTa 92.5 % 78.6 % 91.5 % 85.1 % PPDB 92.3 % \u2014 81.8 % \u2014 MGraph UU 96.5 % 87.0 % \u2014 \u2014 BU 97.6 % 90.4 % \u2014 \u2014 BB \u2014 \u2014 100.0 % 88.8 % 1245 Exact - Match 597 Exact - Match Table 3 : The \ufb01ltered test .", "entities": [[13, 14, "MethodName", "BERT"], [22, 23, "MethodName", "RoBERTa"]]}, {"text": "Models rank question / answer pairs by con\ufb01dence .", "entities": []}, {"text": "We show accuracy on the Kmost con\ufb01dent predictions , at two points .", "entities": [[2, 3, "MetricName", "accuracy"]]}, {"text": "PPDB does n\u2019t answer enough questions to reach the @2000 cutoff , so we also compare at the smaller PPDB maximum .", "entities": []}, {"text": "the models , since both the entailment graphs and the similarity models have a chance to answer all the questions .", "entities": []}, {"text": "Results are shown in Figure 3 ( right ) , with a very different outcome .", "entities": []}, {"text": "Head - to - head , the MGraph offers substantially better precision across all recall levels .", "entities": []}, {"text": "At 50 % recall , the MGraph has 76 % precision with RoBERTa at 65 % .", "entities": [[12, 13, "MethodName", "RoBERTa"]]}, {"text": "Notably , on both tests , more unary questions are answered using both unary andbinary predicate evidence than just using unary evidence alone .", "entities": []}, {"text": "On the \ufb01ltered test , the BU model increases max recall from 54 % to 70 % .", "entities": []}, {"text": "Finally , we note PPDB \u2019s poor performance ( highest recall shown ) , only 1 % higher recall than the exact - match baseline despite having entries for 88 % of questions .", "entities": []}, {"text": "Though PPDB features many directional entailments , this sparsity of edges useful for the task is likely because bilingual pivoting excels at detecting near - paraphrases , not relations between distinct eventualities , e.g. it ca n\u2019t learn \u201c getting elected \u201d entails \u201c being a candidate . \u201d", "entities": []}, {"text": "Advantageously , our method learns this open - domain knowledge by tracking entities across all the events", "entities": []}, {"text": "10766they participate in .", "entities": []}, {"text": "We show a breakdown of the \ufb01ltered test results in Table 3 .", "entities": []}, {"text": "Models do n\u2019t answer all the questions , so following Lewis and Steedman ( 2013 ) who design a similar QA task , we evaluate models on the accuracy of their Kmost con\ufb01dent predictions .", "entities": [[28, 29, "MetricName", "accuracy"]]}, {"text": "7 Error Analysis We sample 300 false positives ( 100 for each model ) and report analyses in Table 4 .", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "In all models spurious entailments are the largest issue , and may occur due to normalization of predicates during learning , or incidental correlations in the data .", "entities": []}, {"text": "The UU and BU models also suffer during relation extraction ( parsing ) .", "entities": [[8, 10, "TaskName", "relation extraction"]]}, {"text": "When we fail to parse a second argument for a predicate we assume it only has one and extract a malformed unary , which can interfere with question answering ( e.g. reporting verbs \u201c explain , \u201d \u201c announce , \u201d etc . which fail to parse with their quote ) .", "entities": [[27, 29, "TaskName", "question answering"]]}, {"text": "We also \ufb01nd relatively few poorly generated negatives , which are actually true given the text .", "entities": []}, {"text": "In these cases the model \ufb01nds an entailment which the authors judge to be correct .", "entities": []}, {"text": "8 Conclusions The MDIH is shown as an effective theory of unsupervised , open - domain predicate entailment , which crosses valencies by respecting argument roles .", "entities": []}, {"text": "Our multivalent entailment graph \u2019s performance has been demonstrated on a question answering task requiring \ufb01ne - grained semantic understanding .", "entities": [[11, 13, "TaskName", "question answering"]]}, {"text": "Our method is able to answer a broader variety of questions than earlier entailment graphs , aided by drawing on evidence across valencies .", "entities": []}, {"text": "We outperform baseline models including a strong similarity measure using unsupervised BERT and RoBERTa , while using far less training data .", "entities": [[11, 12, "MethodName", "BERT"], [13, 14, "MethodName", "RoBERTa"]]}, {"text": "This shows that directional entailment is more helpful for inference on such a task than non - directional similarity , even with robust , in - domain pretraining .", "entities": []}, {"text": "We also noted a complementarity between unsupervised methods .", "entities": []}, {"text": "Our symbolic graph method achieves high precision for learned predicates , while sub - symbolic neural models achieve high recall by generalizing to unseen predicates .", "entities": []}, {"text": "Future work may leverage our MDIH signal to train a directional neural classi\ufb01er and combine bene\ufb01ts .", "entities": []}, {"text": "Acknowledgments This work was supported in part by ERC H2020 Advanced Fellowship GA 742137 SEMANTAX , Error Source False Positive Example Unary to Unary ( UU ) Judgements", "entities": [[12, 13, "MethodName", "GA"], [16, 17, "MetricName", "Error"]]}, {"text": "Spurious Entailment ( 57%)The United States advances \u000f", "entities": []}, {"text": "The United States falls Parsing ( 26 % )", "entities": []}, {"text": "Reuters reports \u000fReuters notes Poor Negative ( actually true ) ( 17%)Productivity increases \u000fProductivity grows Binary to", "entities": []}, {"text": "Unary ( BU ) Judgements", "entities": []}, {"text": "Spurious Entailment ( 65%)New York Mets create through camerawork \u000fNew York Mets bene\ufb01t Parsing ( 26 % )", "entities": []}, {"text": "John McCain spent part of 5 years \u000fJohn McCain drew Poor Negative ( actually true ) ( 9%)The Yankees overwhelm the Mariners \u000fthe Yankees prevail Binary to Binary ( BB )", "entities": []}, {"text": "Judgements Spurious Entailment ( 53%)A soldier was killed in Iraq \u000fA soldier was murdered in Iraq Poor Negative ( actually true ) ( 32%)Pro\ufb01ts fall in the \ufb01rst quarter \u000f", "entities": []}, {"text": "Pro\ufb01ts decline in the \ufb01rst quarter Parsing ( 17 % ) medal than United States \u000f United States take the medal Table 4 : False positive analysis .", "entities": []}, {"text": "Models predict entailments from the text ( left ) to generated negatives ( right ) .", "entities": []}, {"text": "and an Edinburgh and Huawei Technologies Research Centre award .", "entities": []}, {"text": "References Jonathan Berant , Noga Alon , Ido Dagan , and Jacob Goldberger . 2015 .", "entities": []}, {"text": "Ef\ufb01cient global learning of entailment graphs .", "entities": []}, {"text": "Computational Linguistics , 41(2):221 \u2013 263 .", "entities": []}, {"text": "Jonathan Berant , Ido Dagan , and Jacob Goldberger . 2010 .", "entities": []}, {"text": "Global learning of focused entailment graphs .", "entities": []}, {"text": "InProceedings of the 48th Annual Meeting of the Association for Computational Linguistics , pages 1220\u20131229 , Uppsala , Sweden .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sander Bijl de Vroe , Liane Guillou , Milo\u0161 Stanojevic , Nick McKenna , and Mark Steedman .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Modality and negation in event extraction .", "entities": [[4, 6, "TaskName", "event extraction"]]}, {"text": "In Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio - political Events from Text ( CASE 2021 ) , online .", "entities": []}, {"text": "Association for Computational Linguistics ( ACL ) .", "entities": []}, {"text": "Kurt Bollacker , Colin Evans , Praveen Paritosh , Tim Sturge , and Jamie Taylor . 2008 .", "entities": []}, {"text": "Freebase :", "entities": []}, {"text": "A collaboratively created graph database for structuring human knowledge .", "entities": []}, {"text": "In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data , SIGMOD \u2019 08 , page 1247\u20131250 , New York , NY , USA . Association for Computing Machinery .", "entities": [[5, 6, "DatasetName", "ACM"], [10, 11, "TaskName", "Management"]]}, {"text": "10767Christopher Clark , Kenton Lee , Ming - Wei Chang , Tom Kwiatkowski , Michael Collins , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BoolQ :", "entities": [[0, 1, "DatasetName", "BoolQ"]]}, {"text": "Exploring the surprising dif\ufb01culty of natural yes / no questions .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 2924\u20132936 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ido Dagan , Oren Glickman , and Bernardo Magnini . 2006 .", "entities": []}, {"text": "The pascal recognising textual entailment challenge .", "entities": []}, {"text": "In Machine Learning Challenges .", "entities": []}, {"text": "Evaluating Predictive Uncertainty , Visual Object Classi\ufb01cation , and Recognising Tectual Entailment , pages 177\u2013190 , Berlin , Heidelberg .", "entities": []}, {"text": "Springer Berlin Heidelberg .", "entities": []}, {"text": "Ido Dagan , Lillian Lee , and Fernando CN Pereira . 1999 .", "entities": []}, {"text": "Similarity - based models of word cooccurrence probabilities .", "entities": []}, {"text": "Machine learning , 34(1 - 3):43 \u2013 69 .", "entities": []}, {"text": "Donald Davidson .", "entities": []}, {"text": "1967 .", "entities": []}, {"text": "The logical form of action sentences .", "entities": []}, {"text": "In Nicholas Rescher , editor , The Logic of Decision and Action .", "entities": []}, {"text": "University of Pittsburgh Press .", "entities": []}, {"text": "Wayne Davis .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Implicature .", "entities": []}, {"text": "In Edward N. Zalta , editor , The Stanford Encyclopedia of Philosophy , fall 2019 edition .", "entities": [[11, 12, "TaskName", "Philosophy"]]}, {"text": "Metaphysics Research Lab , Stanford University .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Christiane Fellbaum .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "WordNet :", "entities": []}, {"text": "An Electronic Lexical Database .", "entities": []}, {"text": "Bradford Books .", "entities": []}, {"text": "Jenny Rose Finkel , Trond Grenager , and Christopher Manning .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Incorporating non - local information into information extraction systems by gibbs sampling .", "entities": []}, {"text": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics , ACL \u2019 05 , page 363\u2013370 , USA .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Maayan Geffet and Ido Dagan .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "The distributional inclusion hypotheses and lexical entailment .", "entities": [[5, 7, "TaskName", "lexical entailment"]]}, {"text": "InProceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ( ACL\u201905 ) , pages 107\u2013114 , Ann Arbor , Michigan .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xavier Holt .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Probabilistic models of relational implication .", "entities": []}, {"text": "Master \u2019s thesis , Macquarie University .", "entities": []}, {"text": "Mohammad Javad Hosseini .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Unsupervised Learning of Relational Entailment Graphs from Text .", "entities": []}, {"text": "Ph.D. thesis , University of Edinburgh .", "entities": []}, {"text": "Mohammad Javad Hosseini , Nathanael Chambers , Siva Reddy , Xavier R. Holt , Shay B. Cohen , Mark Johnson , and Mark Steedman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning typed entailment graphs with global soft constraints .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 6:703\u2013717 .", "entities": []}, {"text": "Mohammad Javad Hosseini , Shay B. Cohen , Mark Johnson , and Mark Steedman .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Duality of link prediction and entailment graph induction .", "entities": [[2, 4, "TaskName", "link prediction"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4736 \u2013 4746 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Dimitri Kartsaklis and Mehrnoosh Sadrzadeh .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Distributional inclusion hypothesis for tensor - based composition .", "entities": []}, {"text": "In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 2849\u20132860 , Osaka , Japan .", "entities": []}, {"text": "The COLING 2016 Organizing Committee .", "entities": []}, {"text": "Omer Levy and Ido Dagan .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Annotating relation inference in context via question answering .", "entities": [[6, 8, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 249\u2013255 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mike Lewis and Mark Steedman .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Combined distributional and logical semantics .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 1:179\u2013192 .", "entities": []}, {"text": "Dekang Lin .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "Automatic retrieval and clustering of similar words .", "entities": []}, {"text": "In COLING 1998 Volume 2 : The 17th International Conference on Computational Linguistics .", "entities": []}, {"text": "Xiao Ling and Daniel S. Weld .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Fine - grained entity recognition .", "entities": []}, {"text": "In Proceedings of the TwentySixth AAAI Conference on Arti\ufb01cial Intelligence , AAAI\u201912 , page 94\u2013100 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "Claudia Maienborn .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Event semantics , pages 802\u2013829 .", "entities": []}, {"text": "Christopher D. Manning , Mihai Surdeanu , John Bauer , Jenny Finkel , Steven J. Bethard , and David McClosky .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The Stanford CoreNLP natural language processing toolkit .", "entities": []}, {"text": "In Association for Computational Linguistics ( ACL ) System Demonstrations , pages 55\u201360 .", "entities": []}, {"text": "D.B. Nguyen , Johannes Hoffart , M. Theobald , and G. Weikum .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Aida - light : High - throughput named - entity disambiguation .", "entities": [[9, 11, "TaskName", "entity disambiguation"]]}, {"text": "volume 1184 .", "entities": []}, {"text": "10768Ellie Pavlick , Pushpendre Rastogi , Juri Ganitkevitch , Benjamin Van Durme , and Chris Callison - Burch . 2015 .", "entities": []}, {"text": "PPDB 2.0 : Better paraphrase ranking , \ufb01negrained entailment relations , word embeddings , and style classi\ufb01cation .", "entities": [[11, 13, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 425\u2013430 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "F. Petroni , T. Rockt\u00e4schel , A. H. Miller , P. Lewis , A. Bakhtin , Y .", "entities": []}, {"text": "Wu , and S. Riedel . 2019 .", "entities": []}, {"text": "Language models as knowledge bases ?", "entities": []}, {"text": "In In : Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , 2019 .", "entities": []}, {"text": "Hoifung Poon and Pedro Domingos .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Unsupervised semantic parsing .", "entities": [[0, 3, "TaskName", "Unsupervised semantic parsing"]]}, {"text": "In Proceedings of the 2009 conference on empirical methods in natural language processing , pages 1\u201310 .", "entities": []}, {"text": "Milo\u0161 Stanojevi \u00b4 c and Mark Steedman .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "CCG parsing algorithm with incremental tree rotation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 228\u2013239 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mark Steedman .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "The Syntactic Process .", "entities": []}, {"text": "MIT Press , Cambridge , MA , USA .", "entities": [[3, 4, "DatasetName", "Cambridge"]]}, {"text": "Idan Szpektor and Ido Dagan .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Learning entailment rules for unary templates .", "entities": []}, {"text": "In Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008 ) , pages 849\u2013856 , Manchester , UK .", "entities": []}, {"text": "Coling 2008 Organizing Committee .", "entities": []}, {"text": "Zeno Vendler .", "entities": []}, {"text": "1967 .", "entities": []}, {"text": "Facts and Events , pages 12\u2013146 .", "entities": []}, {"text": "Cornell University Press , Ithaca .", "entities": [[0, 1, "DatasetName", "Cornell"]]}, {"text": "Julie Weeds and David Weir .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "A general framework for distributional similarity .", "entities": []}, {"text": "In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing , EMNLP \u2019 03 , page 81\u201388 , USA .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Congle Zhang and Daniel S. Weld .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Harvesting parallel news streams to generate paraphrases of event relations .", "entities": []}, {"text": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1776\u20131786 , Seattle , Washington , USA . Association for Computational Linguistics .", "entities": []}]