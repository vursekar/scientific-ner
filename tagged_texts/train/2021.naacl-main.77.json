[{"text": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 982\u2013997 June 6\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics982LightningDOT : Pre - training Visual - Semantic Embeddings for Real - Time Image - Text Retrieval Siqi Sun\u0003 , Yen - Chun Chen\u0003 , Linjie Li , Shuohang Wang , Yuwei Fang , Jingjing Liu Microsoft Corporation { siqi.sun , yen-chun.chen , lindsey.li , shuohang.wang , yuwfan , jingjl}@microsoft.com Abstract Multimodal pre - training has propelled great advancement in vision - and - language research .", "entities": []}, {"text": "These large - scale pre - trained models , although successful , fatefully suffer from slow inference speed due to enormous computation cost mainly from cross - modal attention in Transformer architecture .", "entities": [[30, 31, "MethodName", "Transformer"]]}, {"text": "When applied to reallife applications , such latency and computation demand severely deter the practical use of pre - trained models .", "entities": []}, {"text": "In this paper , we study Image - text retrieval ( ITR ) , the most mature scenario of V+L application , which has been widely studied even prior to the emergence of recent pre - trained models .", "entities": []}, {"text": "We propose a simple yet highly effective approach , LightningDOT that accelerates the inference time of ITR by thousands of times , without sacri\ufb01cing accuracy .", "entities": [[24, 25, "MetricName", "accuracy"]]}, {"text": "LightningDOT removes the time - consuming cross - modal attention by pre - training on three novel learning objectives , extracting feature indexes of\ufb02ine , and employing instant dot - product matching with further re - ranking , which signi\ufb01cantly speeds up retrieval process .", "entities": []}, {"text": "In fact , LightningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k , COCO and Multi30 K , outperforming existing pre - trained models that consume 1000 \u0002magnitude of computational hours.1 1 Introduction Image - text retrieval ( ITR ) has been widely studied as a staple benchmark task in both NLP and computer vision communities .", "entities": [[16, 17, "DatasetName", "Flickr30k"], [18, 19, "DatasetName", "COCO"]]}, {"text": "Traditional ITR search engines typically deploy ranking - based models built upon visual - semantic embedding matching ( Faghri et al . , 2017 ; Huang et al . , 2018 ) or deep cross - modal fusion with attention mechanism ( Lee et al . , 2018 ; Li et al . , 2020a , b ) .", "entities": []}, {"text": "Earliest works ( Kiros et al . , 2014 ; Faghri et al . , 2017 ; \u0003Equal Contribution .", "entities": []}, {"text": "1Code and pre - training checkpoints are available at https://github.com/intersun/LightningDOT .", "entities": []}, {"text": "MLM /MRM / ITM/ \u2026 1 . CNN for image , LSTM for text , dot   product \u2022CNN 2 .", "entities": [[0, 1, "DatasetName", "MLM"], [11, 12, "MethodName", "LSTM"]]}, {"text": "Cross attention on top of Faster -RCNN ,   LSTM outputs ( SCAN ) 3 .", "entities": [[9, 10, "MethodName", "LSTM"], [12, 13, "DatasetName", "SCAN"]]}, {"text": "Pre - trained V+L Transformers ( UNITER ,   Unicoder -VL ) \u2022Self - attention as cross attention ( optional ) \u2022Deep transformer 4 .", "entities": [[6, 7, "MethodName", "UNITER"]]}, {"text": "Proposed method 1 .", "entities": []}, {"text": "Combine 1 + 3 for both speed and   accuracy \u2026 RNNFaster -RCNNCNN RNN Transformer \u2026", "entities": [[9, 10, "MetricName", "accuracy"], [14, 15, "MethodName", "Transformer"]]}, {"text": "Transformer Dot Product Word Features Region Features Global Text Features Global Image FeaturesCross Attention", "entities": [[0, 1, "MethodName", "Transformer"]]}, {"text": "[ CLS ] FeaturesTransformerSMRM VMLM CMR(a ) ( b )   ( c ) ( d ) Figure 1 : Evolution of Image - Text Retrieval ( ITR ) paradigm .", "entities": []}, {"text": "( a ) Early work ( Faghri et al . , 2017 ) using dot product to learn the similarity between global image features and global text features .", "entities": []}, {"text": "( b ) Later study ( Lee et al . , 2018 ) applying cross - attention between the features of each region and each word .", "entities": []}, {"text": "( c ) Pre - trained V+L models ( Chen et al . , 2020 ) with deep Transformer .", "entities": [[18, 19, "MethodName", "Transformer"]]}, {"text": "( d ) LightningDOT without cross - attention .", "entities": []}, {"text": "CMR , SMRM and VMLM refer to different pre - training tasks , which will be introduced later in method section .", "entities": []}, {"text": "Wang et", "entities": []}, {"text": "al . , 2018 ) employ separate image encoder ( e.g. , CNN ) and text encoder ( e.g. , RNN ) , the embeddings from which are then measured by doc product for similarity matching ( Figure 1(a ) ) .", "entities": []}, {"text": "Later studies ( Lee et al . , 2018 , 2019 ; Wang et al . , 2019 ; Zhang et al . , 2020 ) improve this paradigm by employing advanced region - level visual encoder ( e.g. , Faster - RCNN ) and applying cross - attention between word features and region features for multimodal fusion ( Figure 1(b ) ) .", "entities": []}, {"text": "With the advent of Transformer ( Vaswani et al . , 2017 ) and BERT ( Devlin et al . , 2019 ) , crossmodal retrieval tasks are more recently dominated by vision - and - language ( V+L ) pre - trained models , such as ViLBERT ( Lu et al . , 2019 ) , UNITER ( Chen et al . , 2020 ) , OSCAR ( Li et al . , 2020b ) , and VILLA ( Gan et al . , 2020 ) .", "entities": [[4, 5, "MethodName", "Transformer"], [14, 15, "MethodName", "BERT"], [47, 48, "MethodName", "ViLBERT"], [57, 58, "MethodName", "UNITER"], [67, 68, "MethodName", "OSCAR"]]}, {"text": "Large - scale pre - trained models learned from massive corpus of image - text pairs can power heterogeneous downstream tasks that take diverse modalities as inputs ( e.g. , text , image , video , audio ) .", "entities": []}, {"text": "These models bene\ufb01t from", "entities": []}, {"text": "983the self - attention mechanism in Transformer architecture , learning joint image+text embeddings through pre - training objectives such as masked language modeling ( MLM ) and masked region modeling ( MRM ) ( Figure 1(c ) ) .", "entities": [[6, 7, "MethodName", "Transformer"], [20, 23, "TaskName", "masked language modeling"], [24, 25, "DatasetName", "MLM"]]}, {"text": "However , the very ingredient that engenders the success of these pre - trained models , crossmodal attention between two modalities ( through self - attention ) , also destines the inevitable latency and huge computation cost in training and deploying such massive - scale models .", "entities": []}, {"text": "For example , UNITER ( Chen et al . , 2020 ) builds upon 12/24 Transformer layers , and trains over 10 million image+text pairs .", "entities": [[3, 4, "MethodName", "UNITER"], [15, 16, "MethodName", "Transformer"]]}, {"text": "The inference time of such large models with 110 million parameters is 48 seconds on average for text query from COCO dataset ( Chen et al . , 2015 ) , not scalable in real - life applications serving millions of queries per second .", "entities": [[20, 21, "DatasetName", "COCO"]]}, {"text": "To make real - time ITR possible with low latency , we ask a bold question : can we go back to the beginning , reverting to simple dot product for ef\ufb01cient cross - modal retrieval ?", "entities": [[32, 36, "TaskName", "cross - modal retrieval"]]}, {"text": "To make this retro experiment feasible , we rely on Transformer to pre - train high - quality image and text encoders , but use ef\ufb01cient dot product for multimodal fusion instead of computationally heavy self - attention .", "entities": [[10, 11, "MethodName", "Transformer"]]}, {"text": "To still facilitate effective cross - modal embedding learning , we use a special [ CLS ] token on both encoders , which transfers the learned embedding from the other modality ( Figure 1(d ) ) .", "entities": []}, {"text": "We name this new paradigm LightningDOT , for its lightening speed bene\ufb01ting from dot product computation .", "entities": []}, {"text": "By removing the time - consuming cross - attention between modalities , the model can learn visualsemantic embeddings without extensive matching between each image - text pair during inference , as used in existing pre - trained models ( Chen et al . , 2020 ; Li et", "entities": []}, {"text": "al . , 2020b ; Lu et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "Further , by eliminating the dependency on real - time computation over image - text pairs , we can compute all image and text embeddings independently of\ufb02ine just for once , and reuse these embeddings as cached indexes for new queries on the \ufb02y ( Figure 2 ) .", "entities": []}, {"text": "For model training , we propose three learning objectives to jointly train two Transformer blocks :", "entities": [[13, 14, "MethodName", "Transformer"]]}, {"text": "Image Encoder and Language Encoder .", "entities": []}, {"text": "Speci\ufb01cally , Visual - embedding fused MLM ( namely VMLM ) and Semantic - embedding fused MRM ( namely SMRM ) ensure cross - modal information is harnessed even without cross - modality self - attention .", "entities": [[6, 7, "DatasetName", "MLM"]]}, {"text": "A cross - modal retrieval objective ( namely CMR ) encourages the model to learn multimodal fusion through pre - training .", "entities": [[1, 5, "TaskName", "cross - modal retrieval"]]}, {"text": "To maintain competitive model performance , we further introduce a reranking mechanism to bring back the bene\ufb01t of cross - attention methods .", "entities": []}, {"text": "In summary , LightningDOT is designed with late fusion to learn visual - semantic embeddings .", "entities": []}, {"text": "Experiments on popular ITR benchmarks show that LightningDOT is 600/1900 times faster than existing pre - trained models on Flickr30k / COCO , while achieving new state - of - the - art results .", "entities": [[19, 20, "DatasetName", "Flickr30k"], [21, 22, "DatasetName", "COCO"]]}, {"text": "When retrieving from larger candidate pool ( > 120 K images ) , LightningDOT is 23,000 times faster .", "entities": []}, {"text": "To the best of our knowledge , this is the \ufb01rst known effort on improving V+L model ef\ufb01ciency .", "entities": []}, {"text": "2 Related Work V+L Pre - training Inspired by the success of Transformer - based ( Vaswani et al . , 2017 ) language model pre - training ( Devlin et al . , 2019 ; Liu et al . , 2019 ; Yang et al . , 2019 ;", "entities": [[7, 8, "DatasetName", "Inspired"], [12, 13, "MethodName", "Transformer"]]}, {"text": "Raffel et al . , 2020 ; Lan et al . , 2020 ; Clark et al . , 2020 ) , vision - andlanguage pre - training ( Huang et al . , 2020b ; Su et al . , 2020 ; Li et", "entities": []}, {"text": "al . , 2020b , 2019a ) has become the prevailing paradigm in learning multimodal representations , with strong results on tasks such as image - text retrieval ( Kiros et al . , 2014 ) , visual question answering ( Antol et al . , 2015 ) and referring expression comprehension ( Yu et al . , 2016 ) .", "entities": [[37, 40, "DatasetName", "visual question answering"], [49, 52, "TaskName", "referring expression comprehension"]]}, {"text": "Exemplary works include two - stream ( Tan and Bansal , 2019 ; Lu et al . , 2019 ) and single - stream models ( Chen et al . , 2020 ; Li et", "entities": []}, {"text": "al . , 2020a ; Zhou et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Multi - task learning ( Lu et al . , 2020 ) and adversarial training ( Gan et al . , 2020 ) are also explored .", "entities": [[0, 4, "TaskName", "Multi - task learning"]]}, {"text": "This family of pre - training methods aims for general - purpose V+L without computation cost consideration .", "entities": []}, {"text": "To the best of our knowledge , our work is the \ufb01rst known effort on pre - training visualsemantic embedding that enables low - latency realtime cross - modal retrieval .", "entities": [[26, 30, "TaskName", "cross - modal retrieval"]]}, {"text": "Ours is concurrent work with CLIP ( Radford et al . , 2021 ) .", "entities": [[5, 6, "MethodName", "CLIP"]]}, {"text": "Image - Text Retrieval Early cross - modal embedding works ( Kiros et al . , 2014 ; Wang et al . , 2018 ; Faghri et al . , 2017 ) focus on using a twostream model to learn a uni\ufb01ed visual - semantic embedding , with progressive improvement on two popular benchmarks : Flickr30 K ( Plummer et al . , 2015 ) and COCO ( Chen et al . , 2015 ) .", "entities": [[66, 67, "DatasetName", "COCO"]]}, {"text": "Later methods with cross - attention ( Lee et al . , 2018 , 2019 ; Wang et al . , 2019 ; Zhang et al . , 2020 ) become more popular , with signi\ufb01cant performance gain .", "entities": []}, {"text": "984 Image   Encoder Language   Encoder Top-\ud835\udc40 Re - Ranker Re - ranked   Top-\ud835\udc3eEncoded Offline A man riding a   surfboard on top   of a blue wave \u2026 Inference OnlyFaster R - CNN \u2026", "entities": [[33, 36, "MethodName", "R - CNN"]]}, {"text": "Image Encoder Language Encoder CMR SMRM VMLM", "entities": []}, {"text": "[ CLS ] A   man   \u2026   wave \u2026 \u2026 \u2026", "entities": []}, {"text": "[ CLS ]   ( b ) Image Retrieval Pipeline ( a ) Pre -training TasksSimilarity ScoreFigure 2 : An overview of our proposed framework .", "entities": [[7, 9, "TaskName", "Image Retrieval"]]}, {"text": "( a ) LightningDOT is pre", "entities": []}, {"text": "- trained with Sementic - embedding Fused Mask Region Modeling ( SMRM ) , Visual - embedding Fused Mask Language Modeling ( VMLM ) and Cross - modal Retrieval ( CMR ) .", "entities": [[25, 29, "TaskName", "Cross - modal Retrieval"]]}, {"text": "( b ) LightningDOT ITR pipeline ( image retrieval as an example ) .", "entities": [[7, 9, "TaskName", "image retrieval"]]}, {"text": "Similarities between input textual query and image candidates are computed via dot product .", "entities": []}, {"text": "During inference , image representations can be computed of\ufb02ine , and a re - ranker can be applied for better accuracy , still with signi\ufb01cant speedup .", "entities": [[20, 21, "MetricName", "accuracy"]]}, {"text": "Pre - trained V+L models also fall into this category .", "entities": []}, {"text": "By exploiting large - scale image - text datasets , pretrained V+L models further push the performance on Flickr30 K and COCO .", "entities": [[21, 22, "DatasetName", "COCO"]]}, {"text": "Although achieving high recall , cross - attention requires excessive computation cost during inference that can not be overlooked.2In this work , inspired by dense retrieval in text retrieval domain ( Guu et al . , 2020 ; Karpukhin et al . , 2020 ; Xiong et al . , 2020 ; Mao et al . , 2020 ; Lewis et al . , 2020 ) , we propose a more ef\ufb01cient attention - less framework .", "entities": []}, {"text": "With pre - training , our model achieves better performance while being signi\ufb01cantly faster than cross - modal attention methods .", "entities": []}, {"text": "Note that the proposed approach is orthogonal to model compression techniques that reduce the number of layers / parameters ( Sun et al . , 2019 ; Jiao et al . , 2020 ) , since we do not reduce the number of parameters from the UNITER baseline .", "entities": [[8, 10, "TaskName", "model compression"], [14, 17, "HyperparameterName", "number of layers"], [41, 44, "HyperparameterName", "number of parameters"], [46, 47, "MethodName", "UNITER"]]}, {"text": "These two approaches can be combined to further boost the speed , which is an interesting future work direction .", "entities": []}, {"text": "3 LightningDOT Framework In this section , we present the proposed LightningDOT framework , which consists of two deep Transformers as image and language encoders .", "entities": []}, {"text": "We \ufb01rst introduce three tasks designed to pre - train the model , then present our inference pipeline from of\ufb02ine feature extraction to online instant retrieval .", "entities": []}, {"text": "3.1 Model Pre - training We denote the Transformer - based ( Vaswani et al . , 2017 ) image encoder and language encoder by 2The total inference time is quadratic to the dataset size with cross - attention for image - text retrieval task.f\u0012Vandf\u0012L , respectively ( \u0012V,\u0012Lare learnable parameters ) .", "entities": [[8, 9, "MethodName", "Transformer"]]}, {"text": "Given a dataset of paired image and textf(i;t)g , we \ufb01rst extract region features v = fv0;v1 ; : : : ; vNg(vj2Rdv , Nis the number of regions ) for image i , along with bounding box positions of regions via a pre - trained Faster - RCNN ( Ren et al . , 2015 ; Anderson et", "entities": []}, {"text": "al . , 2018).3The image encoder f\u0012Vencodes this sequence of image regions into a d - dimensional space f\u0012V(v )", "entities": []}, {"text": "= h = fh0 ; : : : ; hNg(hj2Rd ) .", "entities": []}, {"text": "The corresponding text tis tokenized into sub - word units and projected into high - dimensional feature vectorsw = fw0;w1;:::;wTg(wj2Rdw , Tis the number of tokens ) following Devlin et al .", "entities": []}, {"text": "( 2019).4 Similarly , the text encoding process can be written asf\u0012L(w )", "entities": []}, {"text": "= z", "entities": []}, {"text": "= fz0 ; : : : ; zTg(zj2Rd ) .", "entities": []}, {"text": "We regard the output [ CLS ] embedding h0as global image representation , and z0as global text representation .", "entities": []}, {"text": "Following sections discuss how to jointly train these two encoders to learn strong visual - semantic embeddings , through three pre - training objectives .", "entities": []}, {"text": "Visual - embedding Fused Masked Language Modeling ( VMLM )", "entities": [[4, 7, "TaskName", "Masked Language Modeling"]]}, {"text": "Masked Language Modeling ( MLM ) pre - training is \ufb01rst proposed by Devlin et", "entities": [[0, 3, "TaskName", "Masked Language Modeling"], [4, 5, "DatasetName", "MLM"]]}, {"text": "al .", "entities": []}, {"text": "( 2019 ) , where 15 % of the words are masked5and the model is trained to reconstruct the masked words .", "entities": []}, {"text": "Formally , we denote wm= fwm1 ; : : : ; wmMgas masked tokens , where m2 NMis the set of masked indices of size M , randomly sampled from a natural number N.wnmare 3v0is a special [ CLS ] embedding .", "entities": []}, {"text": "4A 30k BPE ( Sennrich et al . , 2016 ) vocabulary ( bert - basecased ) is used to tokenize the text .", "entities": [[2, 3, "MethodName", "BPE"]]}, {"text": "A special [ CLS ] token is also prepended following the common practice ( w0 ) .", "entities": []}, {"text": "5In practice , this 15 % is further decomposed into 10 % random words , 10 % unchanged , and 80 % [ MASK ] .", "entities": []}, {"text": "985the unmasked words .", "entities": []}, {"text": "MLM can be optimized by minimizing the negative log - likelihood : LMLM(t )", "entities": [[0, 1, "DatasetName", "MLM"], [8, 11, "MetricName", "log - likelihood"]]}, {"text": "= \u0000logP\u0012L(wmjwnm )", "entities": []}, {"text": "= \u00001 MMX k=1logP\u0012mlm(wmkjzmk);(1 ) where\u0012mlmis the additional parameters introduced to map hidden states zto word probabilities .", "entities": []}, {"text": "Under the V+L setting , the textual input is usually highly correlated with the image .", "entities": []}, {"text": "To leverage this cross - modal relation , we propose visualembedding fused MLM ( VMLM ) , in which the paired image iis considered as additional input when training the model to reconstruct masked tokens in sentence t.", "entities": [[12, 13, "DatasetName", "MLM"]]}, {"text": "The loss function of VMLM can be formulated as : LVMLM ( t;i ) = \u0000logP\u0012(wmjwnm ; i )", "entities": [[1, 2, "MetricName", "loss"]]}, {"text": "= \u00001 MMX k=1logP\u0012mlm(wmkjzmk+h0);(2 ) where\u0012=f\u0012V;\u0012Lgand the word probabilities P\u0012are conditioned on the corresponding image", "entities": []}, {"text": "i via the global image representation h0 .", "entities": []}, {"text": "Although VMLM takes a similar mathematical form to the MLM task proposed in UNITER , they differ in two main aspects : 1 ) LightningDOT uses two separate encoders ( h0is computed by f\u0012V ) ; and 2 ) visual dependency is explicitly injected to text representations ( zmk+h0 ) , instead of implicitly learned through cross - modal attention .", "entities": [[9, 10, "DatasetName", "MLM"], [13, 14, "MethodName", "UNITER"]]}, {"text": "Semantic - embedding Fused Masked Region Modeling ( SMRM )", "entities": []}, {"text": "Recent works on V+L pretraining ( Lu et al . , 2019 ; Tan and Bansal , 2019 ) have shown that mask - then - reconstruct pre - training on image regions also helps image+text embedding learning .", "entities": []}, {"text": "Similar to MLM , Masked Region Modeling ( MRM ) is supervised by : LMRM(i ) = D\u0012mrm(vm ; f\u0012V(vnm ) )", "entities": [[2, 3, "DatasetName", "MLM"]]}, {"text": "= 1 MMX k=1D\u0012mrm(vmk;hmk);(3 ) whereDcan be any differentiable distance function .", "entities": []}, {"text": "Among the variants of MRM , we consider Masked Region Feature Regression ( MRFR ) with L2 distance and Masked Region Classi\ufb01cation with KL - Divergence ( MRC - kl ) , due to their proven success in learning V+L representations ( Chen et al . ,2020).6In", "entities": []}, {"text": "MRFR , the L2distance between two feature vectors xandyis de\ufb01ned as : D\u0012fr(x;y ) = X kkxk\u0000g\u0012fr(yk)k2 2 ; wherek\u0001k 2denotesL2 - norm , andg\u0012fr(\u0001)is a learnable Multi - layer Perceptron ( MLP ) with parameters \u0012fr .", "entities": [[32, 33, "DatasetName", "MLP"]]}, {"text": "The KL - divergence DKLin MRC - kl measures distance between two probability distributions : D\u0012mrc(x;y ) = X kDKL(c(xk)jjg\u0012mrc(yk ) ) ; where\u0012mrcis the parameters of a trainable MLP that maps feature vector xkto the object class distributionc(xk)predicted by Faster R - CNN .", "entities": [[29, 30, "DatasetName", "MLP"], [40, 44, "MethodName", "Faster R - CNN"]]}, {"text": "To incorporate language information encoded in the paired text , we extend MRM to Semanticembedding fused MRM ( SMRM ) , where the global text representation z0is exploited when reconstructing masked regions .", "entities": []}, {"text": "LSMRM ( i;t ) = D\u0012mrm(vm;f\u0012V(vnm);t ) = 1 MMX k=1D\u0012mrm(vmk;hmk+z0):(4 )", "entities": []}, {"text": "The speci\ufb01c variants SMRFR and SMRC - kl can be derived using the corresponding distance function , which is omitted for simplicity .", "entities": []}, {"text": "Note that both the cross - modal fusion introduced in Eqn .", "entities": []}, {"text": "( 2)and Eqn . ( 4)uses simple addition without introducing extra parameters from their uni - modal counterpart .", "entities": []}, {"text": "Moreover , the extra parameters \u0012mlmand\u0012mrmis not needed at downstream inference so will not slow down the retrieval .", "entities": []}, {"text": "Cross - modal Retrieval Objective ( CMR ) Beyond image or text focused reconstructive objectives , we also propose a new pre - training task , Cross - modal Retrieval ( CMR ) , to leverage the paired information between image and text .", "entities": [[0, 4, "TaskName", "Cross - modal Retrieval"], [26, 30, "TaskName", "Cross - modal Retrieval"]]}, {"text": "With this learning objective , the model is optimized to promote high similarity score for a matched imagesentence pair ( i;t)and vice versa .", "entities": []}, {"text": "The similarity score between query tand imageiis de\ufb01ned as : S(t;i )", "entities": []}, {"text": "= hz0;h0i ; ( 5 ) whereh\u0001;\u0001idenotes the inner product between two vectors , and h0andz0are the output", "entities": []}, {"text": "[ CLS ] embeddings from image encoder f\u0012Vand language encoderf\u0012L , respectively .", "entities": []}, {"text": "6In our implementation , no textual inputs are directly concatenated with image regions due to separate encoding of image and text .", "entities": []}, {"text": "986 i1 i2 int1t2 tn S(i1,\ud835\udc611)S(i1,\ud835\udc612 ) S(i1,\ud835\udc61\ud835\udc5b ) S(i2,\ud835\udc611 ) S(in,\ud835\udc611)S(i2,\ud835\udc612 ) S(i2,\ud835\udc61\ud835\udc5b ) S(in\ud835\udc612 ) S(in,\ud835\udc61\ud835\udc5b ) ........", "entities": []}, {"text": ".... ..... .", "entities": []}, {"text": ".. .", "entities": []}, {"text": ".. .", "entities": []}, {"text": ".. .", "entities": []}, {"text": ".. .", "entities": []}, {"text": ".\u2112\ud835\udc47\ud835\udc451 \u2112\ud835\udc3c\ud835\udc45\ud835\udc611\u2112\ud835\udc3c\ud835\udc45\ud835\udc612", "entities": []}, {"text": "\u2112\ud835\udc3c\ud835\udc45\ud835\udc5b\u2112\ud835\udc47\ud835\udc452 \u2112\ud835\udc47\ud835\udc45\ud835\udc5bFigure 3 : An illustration of the proposed CMR Loss .", "entities": []}, {"text": "Note that positive pairs lie in the diagonal of the matrix .", "entities": []}, {"text": "In order to capture both image - retrieval and textretrieval supervision signals in a single forwardbackward pass , we propose a bi - directional variant of contrastive loss .", "entities": [[27, 28, "MetricName", "loss"]]}, {"text": "Given any matched image - text pair(i;t ) , we treat text tas the query , sample n\u00001 negative imagesfi2;i3;:::;ing , and then compute the objective function as : L(t ) IR=\u0000logeS(t;i1 )", "entities": []}, {"text": "Pn k=1eS(t;ik ) ; wheret1:=t .", "entities": []}, {"text": "Similarly , we take image ias query ( i1:=i ) , samplen\u00001negative text , and compute : L(i ) TR=\u0000logeS(i;t1 )", "entities": []}, {"text": "Pn k=1eS(i;tk ) to optimize for text retrieval .", "entities": []}, {"text": "Following Henderson et al .", "entities": []}, {"text": "( 2017 ) ;", "entities": []}, {"text": "Gillick et al . ( 2019 ) ; Karpukhin et", "entities": []}, {"text": "al . ( 2020 ) , we use in - batch negatives to avoid the actual sampling of a negative image or text : given a batch of npositive image - text pairs B = f(i1;t1 ) ; : : : ; ( in;tn)g , we use all other images from within the batch as negatives ( fijg;wherej2f1;2;:::;ngandj6 = k ) for every positive pair ( ik;tk ) , and vice versa for negative text .", "entities": []}, {"text": "The \ufb01nal CMR loss for batch Bis : LCMR(B ) = 1 2nnX k=1L(ik ) TR+L(tk ) IR : ( 6 ) An illustration ofLCMR is presented in Figure 3.7 Through joint pre - training with CMR , VMLM and SMRM , the visual - semantic embeddings learned from image encoder and language encoder can be readily applied to downstream tasks .", "entities": [[3, 4, "MetricName", "loss"]]}, {"text": "During \ufb01netuning stage , we directly adopt CMR loss to supervise the training process .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "7The whole similarity matrix can be computed ef\ufb01ciently with one batched matrix multiplication call .", "entities": []}, {"text": "This operation can take advantage of GPU hardware with Tensor Cores for faster training.3.2 Real - time Inference For simplicity , we take text - to - image retrieval as an example to introduce the real - time inference pipeline ( Figure 2(b ) ): ( i)Of\ufb02ine image feature extraction and encoding ; ( ii)Online retrieval with text query ; and ( iii)Online re - ranking with topretrieved images .", "entities": [[23, 29, "TaskName", "text - to - image retrieval"]]}, {"text": "Text retrieval is conducted in a symmetric manner .", "entities": []}, {"text": "Of\ufb02ine Feature Extraction Image retrieval task requires the model to rank every image iin an image database Ibased on its similarity to a text query t.", "entities": [[3, 5, "TaskName", "Image retrieval"]]}, {"text": "In LightningDOT , we \ufb01rst apply the image encoderf\u0012Vto all images in I , and cache the resulting global image representations fh(i ) 02Rdji2Ig into an index ( Johnson et al . , 2019 ) in memory for later use .", "entities": []}, {"text": "Note that the entire image - to - index process , including Faster - RCNN feature extraction and Transformer encoding , can all be conducted of\ufb02ine .", "entities": [[18, 19, "MethodName", "Transformer"]]}, {"text": "Therefore , for every new query tat real time , the cached index can be reused for maximum inference time saving .", "entities": []}, {"text": "Online Retrieval During inference , given a text queryt , we encode it with the language encoder \u0012L , and then compute its similarity score to the embedding of every image in I(stored in memory index ) via Eqn ( 5 ) .", "entities": []}, {"text": "Finally , the images will be ranked by their similarity scores , from the highest to lowest .", "entities": []}, {"text": "In practice , people are more interested in top- K retrieval , with a list of KimagesItsatisfying : It:=fimkgK k=1;where S(t;im1)\u0015S(t;im2)\u0015\u0001\u0001\u0001\u0015S(t;imK ) and S(t;imK)\u0015S(t;i)8i2(InIt):(7 )", "entities": []}, {"text": "This optimization problem has been well studied , and we use FAISS ( Johnson et al . , 2019 ) to solve it in our implementation .", "entities": []}, {"text": "It is worth noting that in order to apply fast search , the similarity function has to be decomposable .", "entities": []}, {"text": "Therefore , we choose the simple dot product as Sinstead of a more complicated neural network function .", "entities": []}, {"text": "Similarly , for text retrieval , the same architecture can be applied by simply pre - computing the embedding for all sentences and using an image as query instead .", "entities": []}, {"text": "Re - ranking To further improve retrieval accuracy , we propose a two - stage approach by adopting an optional re - ranking model .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "In the \ufb01rst stage , we use LightningDOT to retrieve top- Mimages ( or texts ) , where Mis an integer much smaller", "entities": []}, {"text": "987ModelCOCO Test ( 5k images ) Flickr30 K Test ( 1k images )", "entities": []}, {"text": "Text Retrieval Image Retrieval Text Retrieval Image Retrieval R@1 R@5", "entities": [[2, 4, "TaskName", "Image Retrieval"], [6, 8, "TaskName", "Image Retrieval"], [8, 9, "MetricName", "R@1"], [9, 10, "MetricName", "R@5"]]}, {"text": "R@10 R@1 R@5", "entities": [[0, 1, "MetricName", "R@10"], [1, 2, "MetricName", "R@1"], [2, 3, "MetricName", "R@5"]]}, {"text": "R@10", "entities": [[0, 1, "MetricName", "R@10"]]}, {"text": "AR R@1", "entities": [[1, 2, "MetricName", "R@1"]]}, {"text": "R@5", "entities": [[0, 1, "MetricName", "R@5"]]}, {"text": "R@10 R@1 R@5", "entities": [[0, 1, "MetricName", "R@10"], [1, 2, "MetricName", "R@1"], [2, 3, "MetricName", "R@5"]]}, {"text": "R@10 AR VSE++\u000341.3 69.2 81.2 30.3 59.1 72.4 58.9 52.9 80.5 87.2 39.6 70.1 79.5 68.3 SCO\u000342.8 72.3 83.0 33.1 62.9 75.5 61.6 55.5 82.0 89.3 41.1 70.5 81.1 69.9 GXN 42.0 - 84.7 31.7 - 74.6 - 56.8 - 89.6 41.5 - 80.0 SCAN - single 46.4 77.4 87.2 34.4 63.7 75.7 64.1", "entities": [[0, 1, "MetricName", "R@10"], [44, 45, "DatasetName", "SCAN"]]}, {"text": "67.9 89.0 94.4 43.9 74.2 82.8 75.4 R - SCAN 45.4 77.9 87.9 36.2 65.6 76.7 65.0 66.3 90.6 96.0 51.4 77.8 84.9 77.8 CAMP 50.1 82.1 89.7 39.0 68.9 80.2 68.3 68.1 89.7 95.2 51.5 77.1 85.3 77.8 CAAN 52.5 83.3 90.9 41.2 70.3 82.9 70.2 70.1 91.6 97.2 52.8 79.0 87.9 79.8 ViLBERT - - - - - - - - - - 58.2 84.9 72.8 Unicoder - VL 62.3 87.1 92.8 46.7 76.0 85.3 75.0 86.2 86.3 99.0 71.5 90.9 94.9 88.1 UNITER - base 64.4 87.4 93.1 50.3 78.5 87.2 76.8 85.9 97.1 98.8 72.5 92.3 95.9 90.4 UNITER - large 65.7 88.6 93.8 52.9 79.9 88.0 78.1 86.9 98.1 99.2 75.5 94.0 96.6 91.7 OSCAR 73.5 92.2 96.0 57.5 82.8 89.8 82.0 - - - - - - LightningDOT\u000360.1 85.1 91.8 45.8 74.6 83.8 73.5 83.9 97.2 98.6 69.9 91.1 95.2 89.3 + UNITERbase Re - Ranker 64.6 87.6 93.5 50.3 78.7 87.5 77.0 86.5 97.5 98.9 72.6 93.1 96.1 90.8 + UNITERlarge Re - Ranker 65.7 89.0 93.7 53.0 80.1 88.0 78.2 87.2 98.3 99.0 75.6 94.0 96.5 91.8 + OSCAR Re - Ranker 74.2 92.4 96.0 57.4 82.7 89.9 82.1 - - - - - - Table 1 : Evaluation results on image - to - text and text - to - image retrieval over Flickr30k and COCO test sets .", "entities": [[9, 10, "DatasetName", "SCAN"], [54, 55, "MethodName", "ViLBERT"], [85, 86, "MethodName", "UNITER"], [102, 103, "MethodName", "UNITER"], [119, 120, "MethodName", "OSCAR"], [186, 187, "MethodName", "OSCAR"], [215, 221, "TaskName", "text - to - image retrieval"], [222, 223, "DatasetName", "Flickr30k"], [224, 225, "DatasetName", "COCO"]]}, {"text": "We compare the proposed method with both task - speci\ufb01c models : VSE++ ( Faghri et al . , 2017 ) , GXN ( Gu et al . , 2018 ) , SCO ( Huang et al . , 2018 ) , SCAN ( Lee et al . , 2018 ) , R - SCAN ( Lee et al . , 2019 ) , CAMP ( Wang et al . , 2019 ) and CAAN ( Zhang et al . , 2020 ) , and V+L pre - trained models : ViLBERT ( Lu et", "entities": [[42, 43, "DatasetName", "SCAN"], [54, 55, "DatasetName", "SCAN"], [91, 92, "MethodName", "ViLBERT"]]}, {"text": "al . , 2019 ) , Unicoder - VL ( Li et al . , 2020a ) , UNITER ( Chen et al . , 2020 ) and OSCAR ( Li et al . , 2020b ) .", "entities": [[18, 19, "MethodName", "UNITER"], [28, 29, "MethodName", "OSCAR"]]}, {"text": "Models in bold\u0003are embedding - based methods without cross - attention .", "entities": []}, {"text": "than the database ( index ) size .", "entities": []}, {"text": "Next , we apply a stronger retrieval model ( usually slower due to the use of cross - attention ) to re - rank the retrieved top - Mpairs from the \ufb01rst stage .", "entities": []}, {"text": "The \ufb01nal Msimilarity scores obtained from the second stage will be used to re - compute the desired top- Kretrieval ( K\u0014M ) in Eqn . ( 7 ) .", "entities": []}, {"text": "Please refer to \ufb01gure 2 for a more detailed visualization .", "entities": []}, {"text": "Our experiments show that this two - stage approach can bene\ufb01t from the best of both worlds : maintaining a constant fast speed per query8while achieving state - of - the - art accuracy .", "entities": [[33, 34, "MetricName", "accuracy"]]}, {"text": "Another advantage of this pipeline is that it can readily incorporate any advanced model as the re - ranker , thus future stronger image - text retrieval models can take advantage of LightningDOT for better ef\ufb01ciency .", "entities": []}, {"text": "4 Experiments This section discusses our experiments on pretraining and evaluating LightningDOT on downstream ITR benchmarks .", "entities": []}, {"text": "4.1 Datasets and Metrics For pre - training , we use pre - processed data provided by Chen et al .", "entities": []}, {"text": "( 2020 ) , including 4.2 million 8The computation time of LightningDOT is negligible compared to that of UNITER .", "entities": [[18, 19, "MethodName", "UNITER"]]}, {"text": "Therefore , the empirical speed is proportional to the number of pairs UNITER has to rank : constant Mfor LightningDOT + UNITER vs. the whole database ( index ) size for UNITER only.images with 9.5 million associated captions from COCO ( Chen et al . , 2015 ) , VG ( Krishna et al . , 2017 ) , Conceptual Captions ( Sharma et al . , 2018 ) , and SBU captions ( Ordonez et al . , 2011 ) .", "entities": [[12, 13, "MethodName", "UNITER"], [21, 22, "MethodName", "UNITER"], [31, 32, "MethodName", "UNITER"], [39, 40, "DatasetName", "COCO"], [59, 61, "DatasetName", "Conceptual Captions"]]}, {"text": "For evaluation , we use Flickr30k ( Plummer et al . , 2015 ) and COCO ( Lin et al . , 2014 ) datasets , which include 31K/123 K images , respectively , each associated with 5 human - written captions .", "entities": [[5, 6, "DatasetName", "Flickr30k"], [15, 16, "DatasetName", "COCO"]]}, {"text": "Following ( Faghri et al . , 2017 ) , we split COCO into 114K/5K/5 K and Flickr30 K into 29K/1k/1k images for train , validation and test .", "entities": [[12, 13, "DatasetName", "COCO"]]}, {"text": "Downstream performance is measured by recall atK(R@K ) for both image and text retrieval tasks .", "entities": []}, {"text": "We also use an additional metric \u201c AR \u201d , the average of R@K for all Kacross both image and sentence retrieval tasks .", "entities": []}, {"text": "4.2 Results on Flickr30 K and COCO", "entities": [[6, 7, "DatasetName", "COCO"]]}, {"text": "We compare the proposed approach with state - ofthe - art methods ( with and without pre - training ) and report the results in Table 1 .", "entities": []}, {"text": "Without crossattention , our method outperforms non - pre - training approaches by large margins on all metrics .", "entities": []}, {"text": "Specifically , our model improves over CAAN ( Zhang et al . , 2020 ) ( SOTA method with cross - attention ) by 3.3 % ( 73.5 vs. 70.2 ) on COCO and 9.5 % ( 89.3 vs. 79.8 ) on Flickr30 K in terms of AR .", "entities": [[32, 33, "DatasetName", "COCO"]]}, {"text": "When compared with methods without cross - attention ( VSE++ ( Faghri et al . , 2017 ) and SCO ( Huang et al . , 2018 ) ) , LightningDOT achieves nearly", "entities": []}, {"text": "988ModelCOCO Full ( 123 K Images )", "entities": []}, {"text": "Flickr30 K Full ( 31 K Images )", "entities": []}, {"text": "Text Retrieval Image Retrieval Text Retrieval Image Retrieval R@5", "entities": [[2, 4, "TaskName", "Image Retrieval"], [6, 8, "TaskName", "Image Retrieval"], [8, 9, "MetricName", "R@5"]]}, {"text": "R@10 R@20 R@5 R@10", "entities": [[0, 1, "MetricName", "R@10"], [2, 3, "MetricName", "R@5"], [3, 4, "MetricName", "R@10"]]}, {"text": "R@20", "entities": []}, {"text": "AR R@5", "entities": [[1, 2, "MetricName", "R@5"]]}, {"text": "R@10 R@20 R@5 R@10 R@20", "entities": [[0, 1, "MetricName", "R@10"], [2, 3, "MetricName", "R@5"], [3, 4, "MetricName", "R@10"]]}, {"text": "AR LightningDOT 40.1 51.0 62.0 28.2 37.4 47.8 44.4 69.6 78.9 86.1 51.8 62.3 72.3 70.2 + Re - Ranker - base 47.9 58.5 67.8 35.7 45.2 55.2 51.7 74.2 81.7 88.2 56.9 66.7 75.6 73.9 + Re - Ranker - large 48.0 59.0 68.9 37.3 46.8 56.4 52.7 75.1 83.9 90.5", "entities": []}, {"text": "60.1 69.5 78.3 76.2 Table 2 : Results on the extreme retrieval setting of full Flickr30k and full COCO datasets .", "entities": [[15, 16, "DatasetName", "Flickr30k"], [18, 19, "DatasetName", "COCO"]]}, {"text": "Method # images SCAN Ours + Re - ranker Flickr30K - test 1,000 1.8 \u0002 639\u0002 46\u0002 COCO - test 5,000 1.9 \u0002 1,927\u0002 95\u0002 Flickr30K - full 31,014 1.8 \u0002 6,591\u0002 1,255\u0002 COCO - full 123,287 1.9 \u000223,869\u0002 2,235\u0002 Table 3 : Speedup w.r.t .", "entities": [[3, 4, "DatasetName", "SCAN"], [18, 19, "DatasetName", "COCO"], [35, 36, "DatasetName", "COCO"]]}, {"text": "UNITER - base .", "entities": [[0, 1, "MethodName", "UNITER"]]}, {"text": "We compare LightningDOT ( Ours ) and + Re - Ranker , plus a lightweight crossattention method SCAN ( Lee et al . , 2018 ) .", "entities": [[17, 18, "DatasetName", "SCAN"]]}, {"text": "LightningDOT with / without UNITER - base re - ranker is signi\ufb01cantly faster .", "entities": [[4, 5, "MethodName", "UNITER"]]}, {"text": "20 - point gain on AR .", "entities": []}, {"text": "Although LightningDOT achieves slightly lower AR than UNITER ( pretraining method with cross - attention ) , with 3.5/1.1 points drop on Flickr30K / COCO , it is 600/1900 \u0002 faster than UNITER during inference time .", "entities": [[7, 8, "MethodName", "UNITER"], [25, 26, "DatasetName", "COCO"], [33, 34, "MethodName", "UNITER"]]}, {"text": "We further apply second - stage re - ranking , and use UNITER to score top- Mretrieved image - text pairs from LightningDOT to obtain the \ufb01nal topKranked lists .", "entities": [[12, 13, "MethodName", "UNITER"]]}, {"text": "With re - ranking , LightningDOT achieves an instant performance lift , surpassing UNITER on both benchmarks , while still 46 - 95 times faster than UNITER .", "entities": [[13, 14, "MethodName", "UNITER"], [26, 27, "MethodName", "UNITER"]]}, {"text": "With an even stronger re - ranker OSCAR , LightningDOT achieves similar results to the state - of - the - art performance on COCO .", "entities": [[7, 8, "MethodName", "OSCAR"], [24, 25, "DatasetName", "COCO"]]}, {"text": "4.3 Speed & Space Improvement To demonstrate the ef\ufb01ciency of LightningDOT , we use UNITER - base as baseline to compare inference speed .", "entities": [[14, 15, "MethodName", "UNITER"]]}, {"text": "We also compare with a more lightweight cross - attention method SCAN ( Lee et al . , 2018 ) , which uses GRU ( Chung et al . , 2014 ) instead of a 12 - layer Transformer .", "entities": [[11, 12, "DatasetName", "SCAN"], [23, 24, "MethodName", "GRU"], [38, 39, "MethodName", "Transformer"]]}, {"text": "All methods are tested on a single TITAN RTX GPU , with batch size of 400 .", "entities": [[7, 8, "DatasetName", "TITAN"], [12, 14, "HyperparameterName", "batch size"]]}, {"text": "As shown in Table 3 , SCAN is \u00181.9\u0002faster than UNITER - base across both benchmarks , as the computational cost of GRU is much cheaper than that of Transformer ( performance drop is signi\ufb01cant though ) .", "entities": [[6, 7, "DatasetName", "SCAN"], [10, 11, "MethodName", "UNITER"], [22, 23, "MethodName", "GRU"], [29, 30, "MethodName", "Transformer"]]}, {"text": "However , the speedup from SCAN is limited , as it computes cross - attention between each query and allimages .", "entities": [[5, 6, "DatasetName", "SCAN"]]}, {"text": "On the other hand , LightningDOT is 639 \u0002faster than UNITER on Flickr30K. When tested with 5 times more im - ages in COCO , the speedup from LightningDOT is 1927\u0002. Even with re - ranking , LightningDOT is still much more ef\ufb01cient than UNITER - base ( 46 \u0002 faster on Flickr30 K and 95 \u0002faster on COCO ) .", "entities": [[10, 11, "MethodName", "UNITER"], [24, 25, "DatasetName", "COCO"], [45, 46, "MethodName", "UNITER"], [59, 60, "DatasetName", "COCO"]]}, {"text": "To mimic a real - life scenario for image retrieval , where the candidate pool contains hundreds of thousands of images , we combine all images from training , validation and test set to form a larger candidate pool .", "entities": [[8, 10, "TaskName", "image retrieval"]]}, {"text": "Note that models are still trained on the training set .", "entities": []}, {"text": "Although the number of text queries remain the same , the number of candidate images scales up by > 20\u0002 , where cross - attention methods immediately become impractical .", "entities": []}, {"text": "We refer this setting on both benchmarks as Flickr30k - full ( 31k ) and COCO - full ( 123k ) .", "entities": [[8, 9, "DatasetName", "Flickr30k"], [15, 16, "DatasetName", "COCO"]]}, {"text": "Our algorithm is 6,591\u0002faster on Flickr30k - full and 23,869 \u0002faster on COCO - full , which clearly shows the advantage of LightningDOT and its potential in real - world applications .", "entities": [[5, 6, "DatasetName", "Flickr30k"], [12, 13, "DatasetName", "COCO"]]}, {"text": "With re - ranking , LightningDOT is still more than 1,000\u0002and 2,000\u0002faster on Flickr30kfull and COCO - full , respectively .", "entities": [[15, 16, "DatasetName", "COCO"]]}, {"text": "In general , for other re - rankers such as OSCAR , our algorithm can approximately speed up inference by Nimages = M times , where Nimages is the number of candidate images , andMis number of re - ranked images from top - Mretrieved results by LightningDOT .", "entities": [[10, 11, "MethodName", "OSCAR"]]}, {"text": "Similarly , we construct a full setting for text retrieval by combining all text queries from training , validation and test set .", "entities": []}, {"text": "Results are summarized in Table 2 .", "entities": []}, {"text": "Considering the size of candidate pool has become more than 20 \u0002larger , we adopt recall at top 5 , 10 , 50 as evaluation metrics .", "entities": []}, {"text": "Our method achieves reasonably good performance , with AR of 44.4 on COCO and 70.2 on Flickr30K. Re - ranking further lifts AR to 56.4 and 76.2 .", "entities": [[12, 13, "DatasetName", "COCO"]]}, {"text": "Results from UNITER or SCAN are not included as the computation of pairwise scores is extremely expensive , given the excessive amount of retrieval candidates .", "entities": [[2, 3, "MethodName", "UNITER"], [4, 5, "DatasetName", "SCAN"]]}, {"text": "While LightningDOT only takes minutes to evaluate , UNITER - base is estimated to take about 28 days9to evaluate under the full setting for both 9This estimation is based on the inference time taken by", "entities": [[8, 9, "MethodName", "UNITER"]]}, {"text": "989Text Retrieval Image Retrieval Method R@1 R@5", "entities": [[2, 4, "TaskName", "Image Retrieval"], [5, 6, "MetricName", "R@1"], [6, 7, "MetricName", "R@5"]]}, {"text": "R@10 R@1 R@5", "entities": [[0, 1, "MetricName", "R@10"], [1, 2, "MetricName", "R@1"], [2, 3, "MetricName", "R@5"]]}, {"text": "R@10 AR R - CNN only 62.2 85.9 91.1 42.0 70.9 80.3 72.1", "entities": [[0, 1, "MetricName", "R@10"], [2, 5, "MethodName", "R - CNN"]]}, {"text": "+ Image Encoder 73.4 92.5 95.6 59.5 84.5 90.3 82.6 + PTy83.5 96.4 98.7 68.6 90.5 94.8 88.8 LightningDOT 85.2 96.4 98.7 69.9 90.4 94.5 89.2 Table 4 : Ablation studies on model design over Flickr30 K validation set .", "entities": []}, {"text": "PTyindicates pre - training with MLM+MRM+CMR , while LightningDOT is pre - trained with VMLM+SMRM+CMR .", "entities": []}, {"text": "Text Retrieval Image Retrieval LightningDOT R@1 R@5", "entities": [[2, 4, "TaskName", "Image Retrieval"], [5, 6, "MetricName", "R@1"], [6, 7, "MetricName", "R@5"]]}, {"text": "R@10 R@1", "entities": [[0, 1, "MetricName", "R@10"], [1, 2, "MetricName", "R@1"]]}, {"text": "R@5", "entities": [[0, 1, "MetricName", "R@5"]]}, {"text": "R@10 AR", "entities": [[0, 1, "MetricName", "R@10"]]}, {"text": "No PT", "entities": []}, {"text": "73.4 92.5 95.6 59.5 84.5 90.3 82.6 PT(CMR ) 75.0 93.9 97.3 61.5 85.5 91.1 84.0 PT(All ) 78.1 94.0 96.9 62.6 85.7 91.8 84.8 Table 5 : Ablation studies on pre - training tasks over Flickr30 K validation set after \ufb01netuning on the corresponding training set .", "entities": []}, {"text": "All pre - training experiments are conducted on COCO dataset only .", "entities": [[8, 9, "DatasetName", "COCO"]]}, {"text": "PT is short for pre - training .", "entities": []}, {"text": "PT(CMR ) refers to pre - training using CMR task only , and PT(All ) refers to pre - training with all of the three tasks .", "entities": []}, {"text": "image retrieval and text retrieval .", "entities": [[0, 2, "TaskName", "image retrieval"]]}, {"text": "In addition , We compare all models with the same setting : cache as much as possible for fastest speed , where our model outperforms others in both speed and space on image retrieval .", "entities": [[32, 34, "TaskName", "image retrieval"]]}, {"text": "The proposed algorithm maps each image to a 768 - dimensional vector , which only consumes about 300Mb storage space for the whole COCO dataset .", "entities": [[23, 24, "DatasetName", "COCO"]]}, {"text": "For crossattention models such as SCAN , UNITER or OSCAR , they also need to cache image features , which typically requires to save a 36 x 2048 dimensional vector per image , and it consumes about 28 GB storage space for COCO dataset .", "entities": [[5, 6, "DatasetName", "SCAN"], [7, 8, "MethodName", "UNITER"], [9, 10, "MethodName", "OSCAR"], [27, 28, "DatasetName", "2048"], [42, 43, "DatasetName", "COCO"]]}, {"text": "4.4 Ablation Studies We conduct ablation studies on Flickr30 K ( Table 4 ) and compare LightningDOT ( L4 ) against 3 ablated instances : ( i)\u201cR - CNN only \u201d ( L1 ): image representations are extracted from Faster R - CNN directly , with no image encoder applied ; ( ii ) \u201c + Image Encoder \u201d ( L2 ): regional features are encoded with a 12 - layer Transformer as the image encoder ; ( iii ) \u201c + PTy \u201d ( L3 ): our model is pre - trained with MLM+MRM+CMR , then \ufb01netuned on Flickr30K. Note that the difference between MLM vs. VMLM and MRM vs. SMRM is whether the predictions of masked tokens ( regions ) rely on infused embeddings from the other modality .", "entities": [[39, 43, "MethodName", "Faster R - CNN"], [71, 72, "MethodName", "Transformer"], [105, 106, "DatasetName", "MLM"]]}, {"text": "UNITER - base on a smaller dataset .", "entities": [[0, 1, "MethodName", "UNITER"]]}, {"text": "Multi30 K COCO Method DE FR CS ZH JA Meta - Ave S - LIWE 72.1 63.4 59.4 73.6 70.0 67.7 MULE 64.1 62.3 57.7 75.9 75.6 67.1 SMALR 69.8 65.9 64.8 77.5 76.7 70.9 M3P 82.0 73.5 70.2 81.8 86.8 78.9 UNITER 85.9 87.1 85.7 88.4 85.9 86.6 LightningDOT 83.3 83.7 82.2 87.2 82.3 83.7 + Re - Ranker 86.1 87.1 86.2 88.4 86.1 86.8 Table 6 : Evaluation on multilingual image - text retrieval over Multi30 K and COCO datasets .", "entities": [[2, 3, "DatasetName", "COCO"], [6, 7, "DatasetName", "CS"], [42, 43, "MethodName", "UNITER"], [80, 81, "DatasetName", "COCO"]]}, {"text": "We compare with task - speci\ufb01c methods : S - LIWE ( Wehrmann et al . , 2019 ) , MULE ( Kim et al . , 2020 ) , SMALR ( Burns et al . , 2020 ) , pre - trained method M3P ( Huang et al . , 2020a ) and UNITER with translate - test .", "entities": [[54, 55, "MethodName", "UNITER"]]}, {"text": "Numbers in blue indicate the use of different dev / test splits of COCO compared to other methods .", "entities": [[13, 14, "DatasetName", "COCO"]]}, {"text": "UNITER and Re - ranker are large model size .", "entities": [[0, 1, "MethodName", "UNITER"]]}, {"text": "Results show that \u201c R - CNN only \u201d is not suf\ufb01cient in learning good image representations for ITR task , while image encoder with Transformer architecture can effectively learn contextualized image representations , hence achieving better performance .", "entities": [[4, 7, "MethodName", "R - CNN"], [25, 26, "MethodName", "Transformer"]]}, {"text": "Pre - trained models ( L3 - 4 ) generally achieve better performance , compared to nonpretrained models ( L1 - 2 ) .", "entities": []}, {"text": "Comparing \u201c + PTy \u201d to the full instance of LightningDOT , dependency on the other modality in VMLM and SMRM brings universal performance lift across all metrics .", "entities": []}, {"text": "This indicates that these cross - modal dependencies introduced by VMLM and SMRM are effective in learning the association between image and text inputs .", "entities": []}, {"text": "In addition , we investigate the effectiveness of each pre - training task in Table 5 .", "entities": []}, {"text": "Comparing to baseline without pre - training , pre - training with CMR alone lifts +1:4on AR .", "entities": []}, {"text": "Pre - training with all three tasks achieves the best performance , indicating that the learning of contextualized word and region representations promotes better global alignment between image and text , and these three pre - training tasks work collaboratively to yield better visual - semantic embeddings .", "entities": []}, {"text": "4.5 Multilingual Image - Text Retrieval We further report results on multilingual image - text retrieval tasks .", "entities": []}, {"text": "Specially , we evaluate LightningDOT under the translate - test setting , which is to translate the test captions in other languages to English by leveraging Machine Translation ( MT ) tool.10Note that our method is only trained on English captions , without exploiting the original or translated captions from multilingual benchmarks .", "entities": [[26, 28, "TaskName", "Machine Translation"]]}, {"text": "10We use Microsoft Azure Translation API Service .", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "990 Figure 4 : Retrieved top 10 images from the query \" Sky view of a blue and yellow biplane \ufb02ying near each other . \"", "entities": []}, {"text": "The ground truth is in the red rectangle .", "entities": []}, {"text": "We consider two benchmarks : Multi30 K ( Elliott et al . , 2016 , 2017 ; Barrault et al . , 2018 ) with captions in German , French and Czech ; and COCO Japanese ( Yoshikawa et al . , 2017 ) and Chinese ( Li et al . , 2019b ) .", "entities": [[34, 35, "DatasetName", "COCO"]]}, {"text": "Average Recall ( AR ) is used as the evaluation metric .", "entities": [[1, 2, "MetricName", "Recall"]]}, {"text": "Meta - Ave , the average of AR over different languages across two benchmarks , is used as a global metric .", "entities": []}, {"text": "More details on multilingual ITR benchmarks are included in Appendix .", "entities": []}, {"text": "We compare LightningDOT against 3 task - speci\ufb01c methods : S - LIWE ( Wehrmann et al . , 2019 ) , MULE ( Kim et al . , 2020 ) and SMALR", "entities": []}, {"text": "( Burns et al . , 2020 ) , which all exploit captions in different languages to learn multilingual or language - agnostic word embeddings .", "entities": [[23, 25, "TaskName", "word embeddings"]]}, {"text": "We also compare with a pre - trained model M3P ( Huang et al . , 2020a ) , which is alternatively pre - trained with image - caption pairs labeled in English and cross - lingual corpus in 100 different languages .", "entities": []}, {"text": "Note that all methods discussed above are trained/\ufb01netuned on captions in different languages .", "entities": []}, {"text": "For fair comparison , we report performance of UNITER under the same translate - test setting , which is \ufb01netuned with English captions only and tested on translated captions .", "entities": [[8, 9, "MethodName", "UNITER"]]}, {"text": "Table 6 shows similar trends of performance improvements as on English benchmarks .", "entities": []}, {"text": "Compared to both state - of - the - art task - speci\ufb01c methods and pre - trained models , LightningDOT under translatetestsetting achieves new state of the art on most languages and establishes a strong baseline for future study on these multilingual", "entities": []}, {"text": "benchmarks.4.6 Qualitative Examples We show an example of image retrieval results here at \ufb01gure 4 for query as \" Sky view of a blue and yellow biplane \ufb02ying near each other \" .", "entities": [[8, 10, "TaskName", "image retrieval"]]}, {"text": "In addition to the ground truth image in the red rectangle , all the 10 images retrieved by our model are valid retrieval since multiple keywords ( \" sky \" , \" blue \" , \" yellow \" , \" airplane \" , \" near \" ) are captured for each image .", "entities": []}, {"text": "Please see the appendix A.4 for more examples .", "entities": []}, {"text": "5 Conclusion In this paper , we propose a pre - training framework that learns joint visual - semantic embedding without any cross - attention between modalities .", "entities": []}, {"text": "LightningDOT outperforms previous state of the art , while signi\ufb01cantly speeding up inference time by 600 - 2000\u0002on Flickr30 K and COCO image - text retrieval benchmarks .", "entities": [[21, 22, "DatasetName", "COCO"]]}, {"text": "Future work includes extending the ef\ufb01cient training framework to other V+L tasks .", "entities": []}, {"text": "References Peter Anderson , Xiaodong He , Chris Buehler , Damien Teney , Mark Johnson , Stephen Gould , and Lei Zhang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Bottom - up and top - down attention for image captioning and visual question answering .", "entities": [[9, 11, "TaskName", "image captioning"], [12, 15, "DatasetName", "visual question answering"]]}, {"text": "In CVPR .", "entities": []}, {"text": "Stanislaw Antol , Aishwarya Agrawal , Jiasen Lu , Margaret Mitchell , Dhruv Batra , C Lawrence Zitnick , and Devi Parikh . 2015 .", "entities": []}, {"text": "Vqa : Visual question answering .", "entities": [[0, 1, "TaskName", "Vqa"], [2, 5, "DatasetName", "Visual question answering"]]}, {"text": "In ICCV .", "entities": []}, {"text": "Lo\u00efc Barrault , Fethi Bougares , Lucia Specia , Chiraag Lala , Desmond Elliott , and Stella Frank .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Find-", "entities": []}, {"text": "991ings of the third shared task on multimodal machine translation .", "entities": [[7, 10, "TaskName", "multimodal machine translation"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Shared Task Papers .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "ACL .", "entities": []}, {"text": "Andrea Burns , Donghyun Kim , Derry Wijaya , Kate Saenko , and Bryan A. Plummer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Learning to scale multilingual representations for visionlanguage tasks .", "entities": []}, {"text": "In ECCV .", "entities": []}, {"text": "Xinlei Chen , Hao Fang , Tsung - Yi Lin , Ramakrishna Vedantam , Saurabh Gupta , Piotr Doll\u00e1r , and C Lawrence Zitnick . 2015 .", "entities": []}, {"text": "Microsoft coco captions : Data collection and evaluation server .", "entities": [[1, 3, "DatasetName", "coco captions"]]}, {"text": "arXiv preprint arXiv:1504.00325 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yen - Chun Chen , Linjie Li , Licheng Yu , Ahmed El Kholy , Faisal Ahmed , Zhe Gan , Yu Cheng , and Jingjing Liu . 2020 .", "entities": []}, {"text": "Uniter : Universal image - text representation learning .", "entities": [[0, 1, "MethodName", "Uniter"], [2, 8, "MethodName", "Universal image - text representation learning"]]}, {"text": "In ECCV .", "entities": []}, {"text": "Junyoung Chung , Caglar Gulcehre , KyungHyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Empirical evaluation of gated recurrent neural networks on sequence modeling .", "entities": []}, {"text": "In Deep Learning and Representation Learning Workshop .", "entities": [[4, 6, "TaskName", "Representation Learning"]]}, {"text": "NeurIPS .", "entities": []}, {"text": "Kevin Clark , Minh - Thang Luong , Quoc V Le , and Christopher D Manning .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Electra : Pre - training text encoders as discriminators rather than generators .", "entities": [[0, 1, "MethodName", "Electra"]]}, {"text": "In ICLR .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In NAACL - HLT .", "entities": []}, {"text": "Desmond Elliott , Stella Frank , Lo\u00efc Barrault , Fethi Bougares , and Lucia Specia . 2017 .", "entities": []}, {"text": "Findings of the second shared task on multimodal machine translation and multilingual image description .", "entities": [[7, 10, "TaskName", "multimodal machine translation"]]}, {"text": "In Proceedings of the Second Conference on Machine Translation .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "ACL .", "entities": []}, {"text": "Desmond Elliott , Stella Frank , Khalil Sima\u2019an , and Lucia Specia .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Multi30 K : Multilingual EnglishGerman image descriptions .", "entities": []}, {"text": "In Proceedings of the 5th Workshop on Vision and Language .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Fartash Faghri , David J Fleet , Jamie Ryan Kiros , and Sanja Fidler .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Vse++ : Improving visualsemantic embeddings with hard negatives .", "entities": []}, {"text": "In BMVC .", "entities": []}, {"text": "Zhe Gan , Yen - Chun Chen , Linjie Li , Chen Zhu , Yu Cheng , and Jingjing Liu . 2020 .", "entities": []}, {"text": "Large - scale adversarial training for vision - and - language representation learning .", "entities": [[11, 13, "TaskName", "representation learning"]]}, {"text": "In NeurIPS .", "entities": []}, {"text": "Dan Gillick , Sayali Kulkarni , Larry Lansing , Alessandro Presta , Jason Baldridge , Eugene Ie , and Diego Garcia - Olano . 2019 .", "entities": []}, {"text": "Learning dense representations for entity retrieval .", "entities": [[4, 6, "TaskName", "entity retrieval"]]}, {"text": "In CoNLL .", "entities": []}, {"text": "Jiuxiang Gu , Jianfei Cai , Sha\ufb01q R Joty , Li Niu , and Gang Wang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Look , imagine and match : Improving textual - visual cross - modal retrieval with generative models .", "entities": [[10, 14, "TaskName", "cross - modal retrieval"]]}, {"text": "In CVPR .Kelvin", "entities": []}, {"text": "Guu , Kenton Lee , Zora Tung , Panupong Pasupat , and Ming - Wei Chang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Realm : Retrievalaugmented language model pre - training .", "entities": []}, {"text": "arXiv preprint arXiv:2002.08909 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matthew Henderson , Rami Al - Rfou , Brian Strope , YunHsuan Sung , L\u00e1szl\u00f3 Luk\u00e1cs , Ruiqi Guo , Sanjiv Kumar , Balint Miklos , and Ray Kurzweil . 2017 .", "entities": [[21, 22, "DatasetName", "Kumar"]]}, {"text": "Ef\ufb01cient natural language response suggestion for smart reply .", "entities": []}, {"text": "arXiv preprint arXiv:1705.00652 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Haoyang Huang , Lin Su , Di Qi , Nan Duan , Edward Cui , Taroon Bharti , Lei Zhang , Lijuan Wang , Jianfeng Gao , Bei Liu , Jianlong Fu , Dongdong Zhang , Xin Liu , and Ming Zhou .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "M3p : Learning universal representations via multitask multilingual multimodal pre - training .", "entities": []}, {"text": "arXiv preprint arXiv:2006.02635 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yan Huang , Qi Wu , Chunfeng Song , and Liang Wang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning semantic concepts and order for image and sentence matching .", "entities": []}, {"text": "In CVPR .", "entities": []}, {"text": "Zhicheng Huang , Zhaoyang Zeng , Bei Liu , Dongmei Fu , and Jianlong Fu . 2020b .", "entities": []}, {"text": "Pixel - bert : Aligning image pixels with text by deep multi - modal transformers .", "entities": [[0, 3, "MethodName", "Pixel - bert"]]}, {"text": "arXiv preprint arXiv:2004.00849 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xiaoqi Jiao , Yichun Yin , Lifeng Shang , Xin Jiang , Xiao Chen , Linlin Li , Fang Wang , and Qun Liu . 2020 .", "entities": []}, {"text": "Tinybert : Distilling bert for natural language understanding .", "entities": [[5, 8, "TaskName", "natural language understanding"]]}, {"text": "In Findings of EMNLP .", "entities": []}, {"text": "Jeff Johnson , Matthijs Douze , and Herv\u00e9 J\u00e9gou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Billion - scale similarity search with gpus .", "entities": []}, {"text": "IEEE Transactions on Big Data .", "entities": []}, {"text": "Andrej Karpathy and Li Fei - Fei . 2015 .", "entities": []}, {"text": "Deep visualsemantic alignments for generating image descriptions .", "entities": []}, {"text": "In CVPR .", "entities": []}, {"text": "Vladimir Karpukhin , Barlas O \u02d8guz , Sewon Min , Ledell Wu , Sergey Edunov , Danqi Chen , and Wentau Yih . 2020 .", "entities": []}, {"text": "Dense passage retrieval for open - domain question answering .", "entities": [[1, 3, "TaskName", "passage retrieval"], [4, 9, "TaskName", "open - domain question answering"]]}, {"text": "arXiv preprint arXiv:2004.04906 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Donghyun Kim , Kuniaki Saito , Kate Saenko , Stan Sclaroff , and Bryan A. Plummer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "MULE : Multimodal Universal Language Embedding .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "Ryan Kiros , Ruslan Salakhutdinov , and Richard S Zemel .", "entities": [[3, 4, "DatasetName", "Ruslan"]]}, {"text": "2014 .", "entities": []}, {"text": "Unifying visual - semantic embeddings with multimodal neural language models .", "entities": []}, {"text": "In Deep Learning and Representation Learning Workshop .", "entities": [[4, 6, "TaskName", "Representation Learning"]]}, {"text": "NeurIPS .", "entities": []}, {"text": "Ranjay Krishna , Yuke Zhu , Oliver Groth , Justin Johnson , Kenji Hata , Joshua Kravitz , Stephanie Chen , Yannis Kalantidis , Li - Jia Li , David A Shamma , et al . 2017 .", "entities": []}, {"text": "Visual genome : Connecting language and vision using crowdsourced dense image annotations .", "entities": [[0, 2, "DatasetName", "Visual genome"]]}, {"text": "IJCV .", "entities": []}, {"text": "992Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , and Radu Soricut . 2020 .", "entities": []}, {"text": "Albert :", "entities": []}, {"text": "A lite bert for self - supervised learning of language representations .", "entities": [[4, 8, "TaskName", "self - supervised learning"]]}, {"text": "In ICLR .", "entities": []}, {"text": "Kuang - Huei Lee , Xi Chen , Gang Hua , Houdong Hu , and Xiaodong He .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Stacked cross attention for image - text matching .", "entities": [[6, 8, "TaskName", "text matching"]]}, {"text": "In ECCV .", "entities": []}, {"text": "Kuang - Huei Lee , Hamid Palangi , Xi Chen , Houdong Hu , and Jianfeng Gao . 2019 .", "entities": []}, {"text": "Learning visual relation priors for image - text matching and image captioning with neural scene graph generators .", "entities": [[7, 9, "TaskName", "text matching"], [10, 12, "TaskName", "image captioning"]]}, {"text": "arXiv preprint arXiv:1909.09953 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Patrick Lewis , Ethan Perez , Aleksandara Piktus , Fabio Petroni , Vladimir Karpukhin , Naman Goyal , Heinrich K\u00fcttler , Mike Lewis , Wen - tau", "entities": []}, {"text": "Yih , Tim Rockt\u00e4schel , et al . 2020 .", "entities": []}, {"text": "Retrieval - augmented generation for knowledge - intensive nlp tasks .", "entities": []}, {"text": "arXiv preprint arXiv:2005.11401 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Gen Li , Nan Duan , Yuejian Fang , Daxin Jiang , and Ming Zhou . 2020a .", "entities": []}, {"text": "Unicoder - vl : A universal encoder for vision and language by cross - modal pretraining .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "Liunian Harold Li , Mark Yatskar , Da Yin , Cho - Jui Hsieh , and Kai - Wei Chang .", "entities": []}, {"text": "2019a .", "entities": []}, {"text": "Visualbert :", "entities": [[0, 1, "MethodName", "Visualbert"]]}, {"text": "A simple and performant baseline for vision and language .", "entities": []}, {"text": "arXiv preprint arXiv:1908.03557 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xirong Li , Chaoxi Xu , Xiaoxu Wang , Weiyu Lan , Zhengxiong Jia , Gang Yang , and Jieping Xu .", "entities": []}, {"text": "2019b .", "entities": []}, {"text": "Coco - cn for cross - lingual image tagging , captioning , and retrieval .", "entities": [[0, 3, "DatasetName", "Coco - cn"]]}, {"text": "IEEE Transactions on Multimedia .", "entities": []}, {"text": "Xiujun Li , Xi Yin , Chunyuan Li , Xiaowei Hu , Pengchuan Zhang , Lei Zhang , Lijuan Wang , Houdong Hu , Li Dong , Furu Wei , et al . 2020b .", "entities": []}, {"text": "Oscar : Object - semantics aligned pre - training for vision - language tasks .", "entities": []}, {"text": "In ECCV .", "entities": []}, {"text": "Tsung - Yi Lin , Michael Maire , Serge Belongie , James Hays , Pietro Perona , Deva Ramanan , Piotr Doll\u00e1r , and C Lawrence Zitnick .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Microsoft coco :", "entities": [[1, 2, "DatasetName", "coco"]]}, {"text": "Common objects in context .", "entities": []}, {"text": "In ECCV .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ilya Loshchilov and Frank Hutter .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Decoupled weight decay regularization .", "entities": [[1, 3, "MethodName", "weight decay"]]}, {"text": "In ICLR .", "entities": []}, {"text": "Jiasen Lu , Dhruv Batra , Devi Parikh , and Stefan Lee . 2019 .", "entities": []}, {"text": "Vilbert : Pretraining task - agnostic visiolinguistic representations for vision - and - language tasks .", "entities": [[0, 1, "MethodName", "Vilbert"]]}, {"text": "In NeurIPS .", "entities": []}, {"text": "Jiasen Lu , Vedanuj Goswami , Marcus Rohrbach , Devi Parikh , and Stefan Lee . 2020 .", "entities": []}, {"text": "12 - in-1 : Multi - task vision and language representation learning .", "entities": [[10, 12, "TaskName", "representation learning"]]}, {"text": "In CVPR .Yuning", "entities": []}, {"text": "Mao , Pengcheng He , Xiaodong Liu , Yelong Shen , Jianfeng Gao , Jiawei Han , and Weizhu Chen .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Generation - augmented retrieval for open - domain question answering .", "entities": [[5, 10, "TaskName", "open - domain question answering"]]}, {"text": "arXiv preprint arXiv:2009.08553 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Vicente Ordonez , Girish Kulkarni , and Tamara L Berg .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Im2text :", "entities": []}, {"text": "Describing images using 1 million captioned photographs .", "entities": []}, {"text": "In NeurIPS .", "entities": []}, {"text": "Bryan A Plummer , Liwei Wang , Chris M Cervantes , Juan C Caicedo , Julia Hockenmaier , and Svetlana Lazebnik . 2015 .", "entities": []}, {"text": "Flickr30k entities : Collecting region - to - phrase correspondences for richer imageto - sentence models .", "entities": [[0, 1, "DatasetName", "Flickr30k"]]}, {"text": "In ICCV .", "entities": []}, {"text": "Alec Radford , Jong Wook Kim , Chris Hallacy , Aditya Ramesh , Gabriel Goh , Sandhini Agarwal , Girish Sastry , Amanda Askell , Pamela Mishkin , Jack Clark , et al . 2021 .", "entities": []}, {"text": "Learning transferable visual models from natural language supervision .", "entities": []}, {"text": "arXiv preprint arXiv:2103.00020 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J Liu . 2020 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the limits of transfer learning with a uni\ufb01ed text - to - text transformer .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "Journal of Machine Learning Research .", "entities": []}, {"text": "Shaoqing Ren , Kaiming He , Ross Girshick , and Jian Sun . 2015 .", "entities": []}, {"text": "Faster r - cnn : Towards real - time object detection with region proposal networks .", "entities": [[0, 4, "MethodName", "Faster r - cnn"], [6, 11, "TaskName", "real - time object detection"], [12, 14, "TaskName", "region proposal"]]}, {"text": "In NeurIPs .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In ACL .", "entities": []}, {"text": "Piyush Sharma , Nan Ding , Sebastian Goodman , and Radu Soricut .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Conceptual captions : A cleaned , hypernymed , image alt - text dataset for automatic image captioning .", "entities": [[0, 2, "DatasetName", "Conceptual captions"], [15, 17, "TaskName", "image captioning"]]}, {"text": "In ACL .", "entities": []}, {"text": "Weijie Su , Xizhou Zhu , Yue Cao , Bin Li , Lewei Lu , Furu Wei , and Jifeng Dai . 2020 .", "entities": []}, {"text": "Vl - bert : Pretraining of generic visual - linguistic representations .", "entities": [[0, 3, "MethodName", "Vl - bert"]]}, {"text": "InICLR .", "entities": []}, {"text": "Siqi Sun , Yu Cheng , Zhe Gan , and Jingjing Liu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Patient knowledge distillation for bert model compression .", "entities": [[1, 3, "MethodName", "knowledge distillation"], [5, 7, "TaskName", "model compression"]]}, {"text": "In EMNLP .", "entities": []}, {"text": "Hao Tan and Mohit Bansal .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Lxmert : Learning cross - modality encoder representations from transformers .", "entities": [[0, 1, "MethodName", "Lxmert"], [2, 10, "MethodName", "Learning cross - modality encoder representations from transformers"]]}, {"text": "In EMNLP .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In NeurIPS .", "entities": []}, {"text": "Liwei Wang , Yin Li , Jing Huang , and Svetlana Lazebnik .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning two - branch neural networks for image - text matching tasks .", "entities": [[9, 11, "TaskName", "text matching"]]}, {"text": "IEEE Transactions on Pattern Analysis and Machine Intelligence .", "entities": []}, {"text": "993Zihao Wang , Xihui Liu , Hongsheng Li , Lu Sheng , Junjie Yan , Xiaogang Wang , and Jing Shao . 2019 .", "entities": []}, {"text": "Camp : Cross - modal adaptive message passing for text - image retrieval .", "entities": [[9, 13, "TaskName", "text - image retrieval"]]}, {"text": "In ICCV .", "entities": []}, {"text": "Jonatas Wehrmann , Douglas M. Souza , Mauricio A. Lopes , and Rodrigo C. Barros . 2019 .", "entities": []}, {"text": "Languageagnostic visual - semantic embeddings .", "entities": []}, {"text": "In ICCV .", "entities": []}, {"text": "Lee Xiong , Chenyan Xiong , Ye Li , Kwok - Fung Tang , Jialin Liu , Paul Bennett , Junaid Ahmed , and Arnold Overwijk .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Approximate nearest neighbor negative contrastive learning for dense text retrieval .", "entities": [[4, 6, "MethodName", "contrastive learning"]]}, {"text": "arXiv preprint arXiv:2007.00808 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Carbonell , Russ R Salakhutdinov , and Quoc V Le . 2019 .", "entities": []}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "In NeurIPS .", "entities": []}, {"text": "Yuya Yoshikawa , Yutaro Shigeto , and Akikazu Takeuchi . 2017 .", "entities": []}, {"text": "STAIR captions : Constructing a large - scale Japanese image caption dataset .", "entities": [[0, 2, "DatasetName", "STAIR captions"]]}, {"text": "In ACL .", "entities": []}, {"text": "Licheng Yu , Patrick Poirson , Shan Yang , Alexander C Berg , and Tamara L Berg .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Modeling context in referring expressions .", "entities": []}, {"text": "In ECCV .", "entities": []}, {"text": "Qi Zhang , Zhen Lei , Zhaoxiang Zhang , and Stan Z Li .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Context - aware attention network for imagetext retrieval .", "entities": []}, {"text": "In CVPR .", "entities": []}, {"text": "Luowei Zhou , Hamid Palangi , Lei Zhang , Houdong Hu , Jason J Corso , and Jianfeng Gao .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Uni\ufb01ed vision - language pre - training for image captioning and vqa .", "entities": [[8, 10, "TaskName", "image captioning"], [11, 12, "TaskName", "vqa"]]}, {"text": "In AAAI .", "entities": []}, {"text": "994A Appendix A.1 Implementation Details To further facilitate the reproductivity of our proposed method , we include more details about the choice of model size and hyper - parameters for both pre - training and \ufb01ne - tuning .", "entities": []}, {"text": "The model dimensions are set to ( L=12 , H=768 , A=12 ) for both image encoder and language encoder , where L is the number of stacked Transformer blocks ; H stands for hidden activation dimension , and A is the number of attention heads .", "entities": [[28, 29, "MethodName", "Transformer"]]}, {"text": "The total number of parameters in LightningDOT is 220M. Pre - training and \ufb01netuning learn the parameters of both encoders .", "entities": [[2, 5, "HyperparameterName", "number of parameters"]]}, {"text": "During inference , with of\ufb02ine representation caching , only the forwarding pass with one encoder from the query modality will be performed online .", "entities": []}, {"text": "For both pre - training and \ufb01netuning , AdamW ( Loshchilov and Hutter , 2019 ) is used to optimize the model training , with \f 1=0:9 , \f 2=0:98 .", "entities": [[8, 9, "MethodName", "AdamW"]]}, {"text": "We adopt a learning rate warmup strategy , where the learning rate is linearly increased during the \ufb01rst 10 % of training steps , followed by a linear decay to 0 .", "entities": [[3, 5, "HyperparameterName", "learning rate"], [10, 12, "HyperparameterName", "learning rate"], [30, 31, "DatasetName", "0"]]}, {"text": "We set the L2 weight decay to be 0.01 .", "entities": [[4, 6, "MethodName", "weight decay"]]}, {"text": "During pre - training , we follow UNITER ( Chen et al . , 2020 ) to randomly sample 1 task per minibatch update.11Our best model is pre - trained on VMLM+SMRM+CRM for 300,000 optimization steps .", "entities": [[7, 8, "MethodName", "UNITER"]]}, {"text": "We set the batch size to 10240 per GPU ( batch size is speci\ufb01ed by # tokens + # regions , as in UNITER ) .", "entities": [[3, 5, "HyperparameterName", "batch size"], [10, 12, "HyperparameterName", "batch size"], [23, 24, "MethodName", "UNITER"]]}, {"text": "Pre - training experiments are conducted on 8\u0002V100 GPUs with 6 - step gradient accumulation , and the learning rate is set to be 5e-5 .", "entities": [[18, 20, "HyperparameterName", "learning rate"]]}, {"text": "For ablation studies presented in Table 5 , the ablated instances of our model are pre - trained for 30k steps on COCO dataset ( Lin et al . , 2014 ) only , and the same choice of learning rate and batch size are applied as in the best pre - training setting .", "entities": [[22, 23, "DatasetName", "COCO"], [39, 41, "HyperparameterName", "learning rate"], [42, 44, "HyperparameterName", "batch size"]]}, {"text": "For \ufb01netuning , we set batch size nto 96 ( nis in examples , instead of the sequence length of tokens and regions ) , and search learning rate from { 1e-5 , 2e-5 , 5e-5 } .", "entities": [[5, 7, "HyperparameterName", "batch size"], [27, 29, "HyperparameterName", "learning rate"]]}, {"text": "We select models based on their AR on the validation set .", "entities": []}, {"text": "The best learning rate is 5e-5 for COCO and 1e-5 for Flickr30K.", "entities": [[2, 4, "HyperparameterName", "learning rate"], [7, 8, "DatasetName", "COCO"]]}, {"text": "Our models are trained for 15 epochs on Flickr30k , and 20 epochs on COCO .", "entities": [[8, 9, "DatasetName", "Flickr30k"], [14, 15, "DatasetName", "COCO"]]}, {"text": "For re - ranking , we choose kfrom { 20 , 50 } .", "entities": []}, {"text": "11Code obtained from https://github.com/ChenRocks/UNITER.A.2 Multilingual Image - Text Retrieval Benchmarks When evaluating on ITR under the multilingual setting , we consider two benchmarks :", "entities": []}, {"text": "Multi30 K ( Elliott et al . , 2016 , 2017 ; Barrault et al . , 2018 ) and COCO Japanese ( Yoshikawa et al . , 2017 ) and Chinese ( Li et al . , 2019b ) .", "entities": [[20, 21, "DatasetName", "COCO"]]}, {"text": "Multi30 K is constructed by manually translating English captions in Flickr30 K ( Plummer et al . , 2015 ) to German , French , and Czech .", "entities": []}, {"text": "Each image in Multi30 K is paired with 5 captions in German , 1 caption in French and Czech .", "entities": []}, {"text": "We adopt the same train / val / test split as in Flickr30K. COCO Japanese ( Yoshikawa et al . , 2017 ) collected 820 K Japanese captions for 165 K COCO images ( Lin et al . , 2014 ) .", "entities": [[13, 14, "DatasetName", "COCO"], [31, 32, "DatasetName", "COCO"]]}, {"text": "We use the same train / dev / test splits for COCO Japanese as in Karpathy and Fei - Fei ( 2015 ) , and present results on the 1 K test set .", "entities": [[11, 12, "DatasetName", "COCO"]]}, {"text": "Similarly , Li et al .", "entities": []}, {"text": "( 2019b ) collected 1 - 2 Chinese captions per image for 20 K COCO images to build COCO Chinese .", "entities": [[14, 15, "DatasetName", "COCO"], [18, 19, "DatasetName", "COCO"]]}, {"text": "We follow the original split de\ufb01ned in Li et", "entities": []}, {"text": "al . ( 2019b ) .", "entities": []}, {"text": "A.3 Inference Time We present the detailed inference time of UNITERbase , SCAN the proposed LightningDOT and LightningDOT with UNITER - base re - ranker in Table 7 , measured by seconds / query .", "entities": [[12, 13, "DatasetName", "SCAN"], [19, 20, "MethodName", "UNITER"]]}, {"text": "UNITER clearly is the slowest , as the 12 - layer Transformer model inference needs to be run between each query and allimages .", "entities": [[0, 1, "MethodName", "UNITER"], [11, 12, "MethodName", "Transformer"]]}, {"text": "Comparing between Flickr30k - test and COCO - test , its inference time scales up linearly with the number of images .", "entities": [[2, 3, "DatasetName", "Flickr30k"], [6, 7, "DatasetName", "COCO"]]}, {"text": "With the lightweight GRU ( Chung et al . , 2014 ) , SCAN is \u00181.9\u0002faster than UNITER .", "entities": [[3, 4, "MethodName", "GRU"], [13, 14, "DatasetName", "SCAN"], [17, 18, "MethodName", "UNITER"]]}, {"text": "Across all settings , LightningDOT is signi\ufb01cantly faster than both cross - attention methods ( UNITER - base and SCAN ) .", "entities": [[15, 16, "MethodName", "UNITER"], [19, 20, "DatasetName", "SCAN"]]}, {"text": "When adding UNITER - base as the re - ranker , our method slows down by\u001810 , but still achieves decent speedup .", "entities": [[2, 3, "MethodName", "UNITER"]]}, {"text": "A.4 More Qualitative Examples We show several qualitative results of image retrieval ( top-10 ) .", "entities": [[10, 12, "TaskName", "image retrieval"]]}, {"text": "All results are retrieved from COCO - Full dataset ( 123k images in total ) .", "entities": [[5, 6, "DatasetName", "COCO"]]}, {"text": "Our model can well understand the underlying semantic meaning .", "entities": []}, {"text": "For example , \u201c romantic \u201d only appears twice in the whole COCO dataset annotations , yet the top retrieved images are all topic - related ( Figure 5 ) .", "entities": [[12, 13, "DatasetName", "COCO"]]}, {"text": "With multiple keywords , our model attempts to retrieve the combinations of them ( if not all ) .", "entities": []}, {"text": "For example , for the query \u201c blue girl boy ball \u201d with four keywords , our model retrieves images", "entities": []}, {"text": "995Method # images UNITER - base SCAN LightningDOT LightningDOT+Re - ranker Flickr30K - test 1000 0.41 0.23 0.00064 0.0089 COCO - test 5000 1.95 1.04 0.00101 0.020 Flickr30K - full 31014 12.8 * 7.10 * 0.00193 0.010", "entities": [[3, 4, "MethodName", "UNITER"], [6, 7, "DatasetName", "SCAN"], [20, 21, "DatasetName", "COCO"]]}, {"text": "COCO - full 123287 48.0 * 25.7 * 0.00201 0.021 Table 7 : Image retrieval time cost measured by computation time ( in seconds ) for each query .", "entities": [[0, 1, "DatasetName", "COCO"], [13, 15, "TaskName", "Image retrieval"]]}, {"text": "The computation time for UNITER and SCAN is roughly linear to # images .", "entities": [[4, 5, "MethodName", "UNITER"], [6, 7, "DatasetName", "SCAN"]]}, {"text": "Numbers with * are estimated by running time on test set .", "entities": []}, {"text": "Figure 5 : Retrieved top-10 images for query \" romantic \" .", "entities": []}, {"text": "Figure 6 : Retrieved top-10 images for query \" blue girl boy ball \" that capture at least three keywords ( Figure 6 ) .", "entities": []}, {"text": "We also present image retrieval results where the text query is sampled from COCO dataset .", "entities": [[3, 5, "TaskName", "image retrieval"], [13, 14, "DatasetName", "COCO"]]}, {"text": "We randomly sample 3 queries and present the results as below ( ground truth on the top , retrieved top-10images at the bottom ) .", "entities": []}, {"text": "Clearly , our model retrieves related images from the full dataset .", "entities": []}, {"text": "996 Figure 7 : Retrieved top 10 images from the query \" A man and a little boy on skis on a ski hill . \"", "entities": []}, {"text": "( Top picture is the ground truth . )", "entities": []}, {"text": "Figure 8 : Retrieved top 10 images from the query \" A road is lined with buildings and has cars on it . \"", "entities": []}, {"text": "( Top picture is the ground truth . )", "entities": []}, {"text": "997 Figure 9 : Retrieved top 10 images from the query \" Two train employees stand near the open train car door . \"", "entities": []}, {"text": "( Top picture is the ground truth . )", "entities": []}, {"text": "Figure 10 : Retrieved top 10 images from the query \" The sun hits the \ufb02oor in a rustic bedroom . \"", "entities": []}, {"text": "( Top picture is the ground truth . )", "entities": []}]