[{"text": "Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages , pages 281\u2013286 April 20 , 2021 \u00a9 2021 Association for Computational Linguistics281Maoqin @ DravidianLangTech - EACL2021 : The Application of Transformer - Based Model Maoqin Yang School of Information Science and Engineering Yunnan University , Yunnan , P.R. China", "entities": [[34, 35, "MethodName", "Transformer"]]}, {"text": "1695157605@qq.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "This paper describes the result of teamMaoqin at DravidianLangTech - EACL2021 .", "entities": []}, {"text": "The provided task consists of three languages ( Tamil , Malayalam , and Kannada ) , I only participate in one of the language task - Malayalam .", "entities": []}, {"text": "The goal of this task is to identify offensive language content of the code - mixed dataset of comments / posts in Dravidian Languages ( Tamil - English , Malayalam - English , and Kannada - English ) collected from social media .", "entities": []}, {"text": "This is a classi\ufb01cation task at the comment / post level .", "entities": []}, {"text": "Given a Youtube comment , systems have to classify it into Notoffensive , Offensive - untargeted , Offensivetargeted - individual , Offensive - targeted - group , Offensive - targeted - other , or Not - in - indentedlanguage .", "entities": []}, {"text": "I use the transformer - based language model with BiGRU - Attention to complete this task .", "entities": [[9, 10, "MethodName", "BiGRU"]]}, {"text": "To prove the validity of the model , I also use some other neural network models for comparison .", "entities": []}, {"text": "And \ufb01nally , the team ranks 5th in this task with a weighted average F1 score of 0.93 on the private leader board .", "entities": [[13, 15, "MetricName", "average F1"]]}, {"text": "1 Introduction Offensive language refers to direct or indirect use of verbal abuse , slander , contempt , ridicule , and other means to infringe or damage the dignity , spiritual world , and mental health of others .", "entities": []}, {"text": "It will seriously affect the mental state of others , disrupt work , the life and learning order of others , and seriously pollute the public opinion environment of the entire network(Schmidt and Wiegand , 2017 ) .", "entities": []}, {"text": "Due to the development of the Internet and the popularity of anonymous comments , many offensive languages have spread on the Internet and caused trouble to relevant personnel ( Thavareesan and Mahesan , 2019 , 2020a , b ) .", "entities": []}, {"text": "Relevant organizations should take measures to prevent this from happening .", "entities": []}, {"text": "It is unrealistic to judge whether online sentences are completely offended by humans .", "entities": []}, {"text": "There - fore , mechanical methods must be used to distinguish whether the language is offensive .", "entities": []}, {"text": "The task is to directly test whether the system can distinguish offensive language in Dravidian languages .", "entities": []}, {"text": "Dravidian languages are a group of languages spoken by 220 million people , predominantly in southern India and northern Sri Lanka , but also in other areas of South Asia .", "entities": []}, {"text": "The Dravidian languages were \ufb01rst recorded in Tamili script inscribed on cave walls in Tamil Nadu \u2019s Madurai and Tirunelveli districts in the 6th century BCE .", "entities": []}, {"text": "The Dravidian languages are closely related languages the are under - resourced ( Chakravarthi , 2020 ) .", "entities": []}, {"text": "Existing deep learning and pre - training models have achieved good results on other tasks(Zampieri et al . , 2019 ) , so I use the deep learning method to deal with the related task .", "entities": []}, {"text": "According to the latest related research progress , the transformer - based language model has become my preferred model .", "entities": []}, {"text": "Because the pre - trained and \ufb01ne - tuned transformersbased models have shown excellent performance in many NLP problems , such as sentiment classi\ufb01cation and automatic extraction of text summaries .", "entities": []}, {"text": "So I choose ALBERT(Lan et", "entities": []}, {"text": "al . , 2019 ) as my basic model in this task .", "entities": []}, {"text": "To get a more effective and higher accuracy model , BiGRU combined with attention .", "entities": [[7, 8, "MetricName", "accuracy"], [10, 11, "MethodName", "BiGRU"]]}, {"text": "To prove the effectiveness of this model , I have also done comparative experiments with other neural networks .", "entities": []}, {"text": "In this task , my model is an effective way to perform well .", "entities": []}, {"text": "To obtain as much effective information as possible from the limited data , I also use the 5 - fold cross - validation method .", "entities": []}, {"text": "my model achieves the desired result .", "entities": []}, {"text": "The rest of this article is structured as follows .", "entities": []}, {"text": "Section 2 introduces related work .", "entities": []}, {"text": "Model and data preparation are described in Section 3 .", "entities": []}, {"text": "Experiments and evaluation are described in Section 4 .", "entities": []}, {"text": "Section 5 describes the results of my work .", "entities": []}, {"text": "The conclusions and future work are drawn in Section 6 .", "entities": []}, {"text": "2822 Related Work There are many competitions about offensive language detection(such as HASOC ( Chakravarthi et al . , 2020c ; Mandl et al . , 2020 ) and TRAC ( Kumar et al . , 2018 ) ) , and many corresponding methods have been produced .", "entities": [[31, 32, "DatasetName", "Kumar"]]}, {"text": "People often tend to abstract this task into a text classi\ufb01cation task ( Howard and Ruder , 2018 ) .", "entities": []}, {"text": "Text classi\ufb01cation is called extracting features from original text data and predicting the category of text data based on these features .", "entities": []}, {"text": "In the past few decades , many models for text classi\ufb01cation have been proposed ( Qian , 2020 ) .", "entities": []}, {"text": "From the 1960s to the 2010s , text classi\ufb01cation models based on shallow learning dominated .", "entities": []}, {"text": "Shallow learning means statistical - based models such as Naive Bayes ( NB ) , K Nearest Neighbors ( KNN)(Cover and Hart , 1967 ) and Support Vector Machines ( SVM ) .", "entities": [[30, 31, "MethodName", "SVM"]]}, {"text": "Compared with earlier rulebased methods , this method has obvious advantages in accuracy and stability .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "However , these methods still require functional design , which is time - consuming and expensive .", "entities": []}, {"text": "In addition , they usually ignore the natural order structure or context information in the text data , which makes learning the semantic information of words dif\ufb01cult .", "entities": []}, {"text": "Since the 2010s , text classi\ufb01cation has gradually changed from a shallow learning model to a deep learning model .", "entities": []}, {"text": "Compared with methods based on shallow learning , deep learning methods avoid the manual design of rules and functions and automatically provide semantically meaningful representations for text mining .", "entities": []}, {"text": "Therefore , most of the text classi\ufb01cation research work is based on DNN(Yu et al . , 2013 ) , which is a data - driven method with high computational complexity .", "entities": []}, {"text": "Few studies have focused on shallow learning models to solve the limitations of computation and data .", "entities": []}, {"text": "The shallow learning model speeds up the text classi\ufb01cation speed , improves the accuracy , and expands the application range of shallow learning .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "The shallow learning method is a type of machine learning .", "entities": []}, {"text": "It learns from data , which is a prede\ufb01ned function that is important to the performance of the predicted value .", "entities": []}, {"text": "However , element engineering is an arduous and giant job .", "entities": []}, {"text": "Before training the classi\ufb01er , we need to collect knowledge or experience to extract features from the original text .", "entities": []}, {"text": "The shallow learning method trains the initial classi\ufb01er based on various text features extracted from the original text .", "entities": []}, {"text": "For small data sets , under the limita - Set Total number train 16010 development 1999 test 2001 Table 1 : The number of sentences in each set .", "entities": []}, {"text": "tion of computational complexity , shallow learning models generally show better performance than deep learning models .", "entities": []}, {"text": "Therefore , some researchers have studied the design of shallow models in speci\ufb01c areas of data replacement .", "entities": []}, {"text": "Deep learning consists of multiple hidden layers in a neural network(Aroyehun and Gelbukh , 2018 ) , has higher complexity , and can be trained on unstructured data .", "entities": []}, {"text": "The deep learning architecture can directly learn feature representations from the input without excessive manual intervention and prior knowledge .", "entities": []}, {"text": "However , deep learning technology is a data - driven method that usually requires a lot of data to achieve high performance .", "entities": []}, {"text": "And the self - attention - based model can bring some interword interpretability to DNN , but the comparison with the shallow model does not explain why and how it works .", "entities": []}, {"text": "3 Methodology and Data An overall framework and processing pipeline of my solution are shown in Figure 1 .", "entities": []}, {"text": "In my job , I use the ALBERT model as my base model and take BiGRU - Attention behind it .", "entities": [[7, 8, "MethodName", "ALBERT"], [15, 16, "MethodName", "BiGRU"]]}, {"text": "My model is shown in Figure 2 .", "entities": []}, {"text": "3.1 Data Preparation", "entities": []}, {"text": "This is a comment / post level classi\ufb01cation task .", "entities": []}, {"text": "Given a Youtube comment ( Chakravarthi et al . , 2020b , a , 2021 ; Chakravarthi and Muralidaran , 2021 ) , the system has to classify it into one of the \ufb01ve categories mentioned in the Abstract section .", "entities": []}, {"text": "For this task , the available sentences including 16010 training sentences , 1999 development sentences , and 2001 testing sentences .", "entities": []}, {"text": "The label distribution is very uneven(Not - offensive label accounts 88.4 % .", "entities": []}, {"text": "The label with the second largest number is not - malayalam , which accounts for only 0.08 % of the total .", "entities": []}, {"text": "And there are relatively fewer labels in other categories.)The number of sentences for each domain is listed in Table 1 .", "entities": []}, {"text": "283 Figure 1 : An overall framework Figure 2 : The architecture of the model , where the E[CLS]andE[SEP ] are added at the beginning and end of each instance respectively , which can separate different sentences .", "entities": []}, {"text": "The format is as follows : [ CLS]+sentence+ [ SEP ] .", "entities": []}, {"text": "3.2 ALBERT The ALBERT model belongs to transformer - based language models .", "entities": [[1, 2, "MethodName", "ALBERT"], [3, 4, "MethodName", "ALBERT"]]}, {"text": "The ALBERT model is improved on the basis of Bidirectional Encoder Representations for Transformers(BERT)(Devlin et al . , 2018 ) model .", "entities": [[1, 2, "MethodName", "ALBERT"]]}, {"text": "It has designed a parameter reduction method to reduce memory consumption by changing the result of the original embedding parameter P(the product of the vocabulary size Vand the hidden layer size H ) .", "entities": [[28, 31, "HyperparameterName", "hidden layer size"]]}, {"text": "V\u0003H = P!V\u0003E+E\u0003H = P ( 1 ) Erepresents the size of the low - dimensional embedding space .", "entities": []}, {"text": "In BERT , E = H. While in ALBERT , H > >", "entities": [[1, 2, "MethodName", "BERT"], [8, 9, "MethodName", "ALBERT"]]}, {"text": "E , so the number of parameters will be greatly reduced .", "entities": [[4, 7, "HyperparameterName", "number of parameters"]]}, {"text": "At the same time , the self - supervised loss is used to focus on the internal coherence in the construction of sentences .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "TheALBERT model implements three embedding layers : word embedding , position embedding , and segment embedding .", "entities": []}, {"text": "The token embedding layer predicts each word as a \ufb01xed - size vector .", "entities": []}, {"text": "Position embedding is used to retain position information , use a vector to randomly initialize each position , add model training , and \ufb01nally obtain an embedding containing position information .", "entities": []}, {"text": "Segment embedding helps BERT distinguish between paired input sequences .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "3.3 BiGRU - Attention The BiGRU - Attention model(Cover and Hart , 1967 ) is divided into three parts : text vector input layer , hidden layer , and output layer .", "entities": [[1, 2, "MethodName", "BiGRU"], [5, 6, "MethodName", "BiGRU"]]}, {"text": "Among them , the hidden layer consists of three layers : the BiGRU layer , the attention layer , and the Dense layer ( fully connected layer ) .", "entities": [[12, 13, "MethodName", "BiGRU"]]}, {"text": "I set the output of the ALBERT model as the input .", "entities": [[6, 7, "MethodName", "ALBERT"]]}, {"text": "After receiving the input , it uses the BiGRU neural network layer to extract features of the deep - level information of the text \ufb01rstly .", "entities": [[8, 9, "MethodName", "BiGRU"]]}, {"text": "Secondly , it uses the attention layer to assign corresponding weights to the deep - level information of the extracted text .", "entities": []}, {"text": "Finally , the text feature information with different weights is put into the softmax function layer for classi\ufb01cation .", "entities": [[13, 14, "MethodName", "softmax"]]}, {"text": "The structure of the BiGRU - Attention model is shown in Figure 3 .", "entities": [[4, 5, "MethodName", "BiGRU"]]}, {"text": "284 Figure 3 : The structure of the BiGRU - Attention model .", "entities": [[8, 9, "MethodName", "BiGRU"]]}, {"text": "TheI1 ; I2:::Im represent the output of the ALBERT layer and the R1 ; R2:::Rm represent the output of the BiGRU layer and will be input to the Attention layer .", "entities": [[8, 9, "MethodName", "ALBERT"], [20, 21, "MethodName", "BiGRU"]]}, {"text": "Model ALBERT(Base ) train step 2501 learning rate 2e-5 batch size 32 epoch 5 Table 2 : The parameter con\ufb01guration of ALBERT .", "entities": [[6, 8, "HyperparameterName", "learning rate"], [9, 11, "HyperparameterName", "batch size"], [21, 22, "MethodName", "ALBERT"]]}, {"text": "4 Experiment In this task , I use the ALBERT model to pre - train the task .", "entities": [[9, 10, "MethodName", "ALBERT"]]}, {"text": "For the ALBERT model , the main hyperparameters I pay attention to are the training step size , batch size and learning rate .", "entities": [[2, 3, "MethodName", "ALBERT"], [15, 17, "HyperparameterName", "step size"], [18, 20, "HyperparameterName", "batch size"], [21, 23, "HyperparameterName", "learning rate"]]}, {"text": "The parameters of my model are shown in Table 2 .", "entities": []}, {"text": "I have obtained good performance using the ALBERT - BASE.1model .", "entities": [[7, 8, "MethodName", "ALBERT"]]}, {"text": "Considering that BiGRU - Attention can capture contextual information well and extract text information features more accurately(Radford et al . , 2018 ) , I add it after ALBERT .", "entities": [[2, 3, "MethodName", "BiGRU"], [28, 29, "MethodName", "ALBERT"]]}, {"text": "I use the development data set to verify the performance of the models .", "entities": []}, {"text": "The standard of judgment is a weighted F1 - score , and this standard is the judgment standard used for my task .", "entities": [[7, 10, "MetricName", "F1 - score"]]}, {"text": "Table3 lists the results of various models described previously .", "entities": []}, {"text": "The best performance is in bold .", "entities": []}, {"text": "My model gets the best performance of 0.93 .", "entities": []}, {"text": "As shown in the table my model can greatly improve the performance and my overall approach achieved 5th place on the \ufb01nal leader board .", "entities": []}, {"text": "5 Results The output of the classi\ufb01cation result is shown in Figure 4 .", "entities": []}, {"text": "We can see that the label of Offensive\u0000Targeted\u0000Insult\u0000Other , Offensive\u0000Targeted\u0000Insult\u0000Individual , andOffensive\u0000Targeted\u0000Insult\u0000Group 1https://huggingface.co/albert-base-v2Model F1 ALBERT(Base ) 0.919 BERT(Base ) 0.912 RoBERTa(Base ) 0.920 BERT(Base)+BiGRU - Attention 0.928 Mine(ALBERT+BiGRU - Attention ) 0.930 Table 3 : Results of comparative experiments .", "entities": [[13, 14, "MetricName", "F1"]]}, {"text": "Figure 4 : The classi\ufb01cation result is zero .", "entities": []}, {"text": "Not\u0000Offensive labels account for the majority , accounting for 91.15 % of the total number of labels .", "entities": []}, {"text": "The Not\u0000Malayalam labels account for the second most signi\ufb01cant 7.5 % of the total .", "entities": []}, {"text": "Offensive - Untargeted labels are the least , only about 1 % .", "entities": []}, {"text": "This may be due to data imbalance ( Not\u0000Offensive labels in the training set account for about 88 % of the total ) resulting in only three categories being identi\ufb01ed .", "entities": []}, {"text": "6 Conclusion and Future Work", "entities": []}, {"text": "In this paper , I present my result on Offensive Language Identi\ufb01cation in Dravidian LanguagesEACL 2021 which includes three tasks of different languages .", "entities": []}, {"text": "For this task , I regard it as a multiple classi\ufb01cation task , I use the BiGRU - Attention based on the ALBERT model to complete , and my model works very well .", "entities": [[16, 17, "MethodName", "BiGRU"], [22, 23, "MethodName", "ALBERT"]]}, {"text": "I also summarized the possible reasons for classifying only three types of labels .", "entities": []}, {"text": "At the same time , I also use some other neural networks for comparative experiments to prove that my model can obtain excellent performance .", "entities": []}, {"text": "The result shows that my model ranks 5th in the Malayalam task .", "entities": []}, {"text": "Due to the continuous development of the definition of offensive information on the Internet , it is dif\ufb01cult to accurately describe the nature of", "entities": []}, {"text": "285this information only from the perspective of data mining , which makes it impossible to model this information effectively .", "entities": []}, {"text": "In the future , I will use methods based on multidisciplinary discovery to guide model learning .", "entities": []}, {"text": "These models are more likely to use limited data to learn more effective models .", "entities": []}, {"text": "At the same time , I will also consider whether I can use other transfer learning models to perform better on multi - classi\ufb01cation tasks .", "entities": [[14, 16, "TaskName", "transfer learning"]]}, {"text": "References S. T. Aroyehun and A. Gelbukh .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Aggression detection in social media : Using deep neural networks , data augmentation , and pseudo labeling .", "entities": [[11, 13, "TaskName", "data augmentation"]]}, {"text": "Proceedings of the First Workshop on Trolling , Aggression and Cyberbullying ( TRAC-2018 ) , pages 90\u201397 .", "entities": []}, {"text": "Bharathi Raja Chakravarthi .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Leveraging orthographic information to improve machine translation of under - resourced languages .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "Ph.D. thesis , NUI Galway .", "entities": []}, {"text": "Bharathi Raja Chakravarthi , Navya Jose , Shardul Suryawanshi , Elizabeth Sherly , and John Philip McCrae . 2020a .", "entities": []}, {"text": "A sentiment analysis dataset for codemixed Malayalam - English .", "entities": [[1, 3, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under - resourced languages ( SLTU ) and Collaboration and Computing for Under - Resourced Languages ( CCURL ) , pages 177\u2013184 , Marseille , France .", "entities": []}, {"text": "European Language Resources association .", "entities": []}, {"text": "Bharathi Raja Chakravarthi and Vigneshwaran Muralidaran .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Findings of the shared task on Hope Speech Detection for Equality , Diversity , and Inclusion .", "entities": [[6, 9, "TaskName", "Hope Speech Detection"]]}, {"text": "In Proceedings of the First Workshop on Language Technology for Equality , Diversity and Inclusion .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Bharathi Raja Chakravarthi , Vigneshwaran Muralidaran , Ruba Priyadharshini , and John Philip McCrae .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Corpus creation for sentiment analysis in code - mixed Tamil - English text .", "entities": [[3, 5, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under - resourced languages ( SLTU ) and Collaboration and Computing for Under - Resourced Languages ( CCURL ) , pages 202\u2013210 , Marseille , France .", "entities": []}, {"text": "European Language Resources association .", "entities": []}, {"text": "Bharathi Raja Chakravarthi , Ruba Priyadharshini , Navya Jose , Anand Kumar M , Thomas Mandl , Prasanna Kumar Kumaresan , Rahul Ponnusamy , Hariharan V , Elizabeth Sherly , and John Philip McCrae . 2021 .", "entities": [[11, 12, "DatasetName", "Kumar"], [18, 19, "DatasetName", "Kumar"]]}, {"text": "Findings of the shared task on Offensive Language Identi\ufb01cation in Tamil , Malayalam , and Kannada .", "entities": []}, {"text": "In Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Bharathi Raja Chakravarthi , Ruba Priyadharshini , Vigneshwaran Muralidaran , Shardul Suryawanshi , Navya Jose , Elizabeth Sherly , and John P. McCrae .", "entities": []}, {"text": "2020c .", "entities": []}, {"text": "Overview of the Track on Sentiment Analysis for Dravidian Languages in Code - Mixed Text .", "entities": [[5, 7, "TaskName", "Sentiment Analysis"]]}, {"text": "In Forum for Information Retrieval Evaluation , FIRE 2020 , page 21\u201324 , New York , NY , USA . Association for Computing Machinery .", "entities": [[3, 5, "TaskName", "Information Retrieval"], [7, 8, "DatasetName", "FIRE"]]}, {"text": "Cover and Hart .", "entities": []}, {"text": "1967 .", "entities": []}, {"text": "Nearest neighbor pattern classi\ufb01cation .", "entities": []}, {"text": "J. Devlin , M.-W. Chang , K. Lee , and K. Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv:1810.04805 .", "entities": []}, {"text": "Jeremy Howard and Sebastian Ruder .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Universal language model \ufb01ne - tuning for text classi\ufb01cation .", "entities": []}, {"text": "arXiv:1801.06146 .", "entities": []}, {"text": "R. Kumar , A. K. Ojha , S. Malmasi , and M. Zampieri .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "2018 .", "entities": []}, {"text": "Benchmarking aggression identi\ufb01cation in social media .", "entities": []}, {"text": "Proceedings of the First Workshop on Trolling , Aggression and Cyberbullying ( TRAC2018 ) , pages 1\u201311 .", "entities": []}, {"text": "Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , and Piyush Sharma . 2019 .", "entities": []}, {"text": "Albert :", "entities": []}, {"text": "A lite bert for self - supervised learning of language representations .", "entities": [[4, 8, "TaskName", "self - supervised learning"]]}, {"text": "arXiv:1909.11942 .", "entities": []}, {"text": "Version 6 .", "entities": []}, {"text": "Thomas Mandl , Sandip Modha , Anand Kumar M , and Bharathi Raja Chakravarthi .", "entities": [[7, 8, "DatasetName", "Kumar"]]}, {"text": "2020 .", "entities": []}, {"text": "Overview of the HASOC Track at FIRE 2020 :", "entities": [[6, 7, "DatasetName", "FIRE"]]}, {"text": "Hate Speech and Offensive Language Identi\ufb01cation in Tamil , Malayalam , Hindi , English and German .", "entities": [[0, 5, "DatasetName", "Hate Speech and Offensive Language"]]}, {"text": "In Forum for Information Retrieval Evaluation , FIRE 2020 , page 29\u201332 , New York , NY , USA . Association for Computing Machinery .", "entities": [[3, 5, "TaskName", "Information Retrieval"], [7, 8, "DatasetName", "FIRE"]]}, {"text": "Qu Qian .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A review of the latest text classi\ufb01cation 2020 - the development of text classi\ufb01cation from shallow to deep from 1961 to 2020 .", "entities": []}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving language understanding with unsupervised learning .", "entities": []}, {"text": "Technical report , OpenAI .", "entities": []}, {"text": "A. Schmidt and M. Wiegand .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A survey on hate speech detection using natural language processing .", "entities": [[3, 6, "TaskName", "hate speech detection"]]}, {"text": "Proceedings of the Fifth International workshop on natural language processing for social media , ( 1):1 \u2013 10 .", "entities": []}, {"text": "Sajeetha Thavareesan and Sinnathamby Mahesan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Sentiment Analysis in Tamil Texts : A Study on Machine Learning Techniques and Feature Representation .", "entities": [[0, 2, "TaskName", "Sentiment Analysis"]]}, {"text": "In 2019 14th Conference on Industrial and Information Systems ( ICIIS ) , pages 320\u2013325 .", "entities": []}, {"text": "Sajeetha Thavareesan and Sinnathamby Mahesan .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "Sentiment Lexicon Expansion using Word2vec and fastText for Sentiment Prediction in Tamil texts .", "entities": [[6, 7, "MethodName", "fastText"]]}, {"text": "In 2020 Moratuwa Engineering Research Conference ( MERCon ) , pages 272\u2013276 .", "entities": []}, {"text": "286Sajeetha Thavareesan and Sinnathamby Mahesan .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Word embedding - based Part of Speech tagging in Tamil texts .", "entities": []}, {"text": "In 2020 IEEE 15th International Conference on Industrial and Information Systems ( ICIIS ) , pages 478\u2013482 .", "entities": []}, {"text": "Dong Yu , Michael L. Seltzer , Jinyu Li , Jui - Ting Huang , and Frank Seide .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Feature learning in deep neural networks - studies on speech recognition tasks .", "entities": [[9, 11, "TaskName", "speech recognition"]]}, {"text": "arXiv:1301.3605 .", "entities": []}, {"text": "M. Zampieri , S. Malmasi , P. Nakov , and S. Rosenthal .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Predicting the type and target of offensive posts in social media .", "entities": []}, {"text": "arXiv:1902.09666 .", "entities": []}]