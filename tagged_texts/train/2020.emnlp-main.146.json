[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 1860\u20131877 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics1860UniConv :", "entities": []}, {"text": "A Uni\ufb01ed Conversational Neural Architecture for Multi - domain Task - oriented Dialogues Hung Leyx , Doyen Sahooz , Chenghao Liuy , Nancy F. Chenx , Steven C.H. Hoiyz ySingapore Management University { hungle.2018,chliu}@smu.edu.sg zSalesforce Research Asia { dsahoo,shoi}@salesforce.com xInstitute for Infocomm Research , A*STAR nfychen@i2r.a-star.edu.sg Abstract Building an end - to - end conversational agent for multi - domain task - oriented dialogues has been an open challenge for two main reasons .", "entities": [[30, 31, "TaskName", "Management"], [55, 56, "DatasetName", "agent"]]}, {"text": "First , tracking dialogue states of multiple domains is non - trivial as the dialogue agent must obtain complete states from all relevant domains , some of which might have shared slots among domains as well as unique slots specifically for one domain only .", "entities": [[15, 16, "DatasetName", "agent"]]}, {"text": "Second , the dialogue agent must also process various types of information across domains , including dialogue context , dialogue states , and database , to generate natural responses to users .", "entities": [[4, 5, "DatasetName", "agent"]]}, {"text": "Unlike the existing approaches that are often designed to train each module separately , we propose \u201c UniConv \" \u2014 a novel uni\ufb01ed neural architecture for end - to - end conversational systems in multi - domain task - oriented dialogues , which is designed to jointly train ( i ) a Bi - level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently , and ( ii ) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously .", "entities": []}, {"text": "We conduct comprehensive experiments in dialogue state tracking , contextto - text , and end - to - end settings on the MultiWOZ2.1 benchmark , achieving superior performance over competitive baselines .", "entities": [[5, 8, "TaskName", "dialogue state tracking"]]}, {"text": "1 Introduction A conventional approach to task - oriented dialogues is to solve four distinct tasks : ( 1 ) natural language understanding ( NLU ) which parses user utterance into a semantic frame , ( 2 ) dialogue state tracking ( DST ) which updates the slots and values from semantic frames to the latest values for knowledge base retrieval , ( 3 ) dialogue policy which determines an appropriate dialogue act for the next system response , and ( 4 ) response generation which generates a natural language sequence conditioned onthe dialogue act .", "entities": [[20, 23, "TaskName", "natural language understanding"], [38, 41, "TaskName", "dialogue state tracking"], [83, 85, "TaskName", "response generation"]]}, {"text": "This traditional pipeline modular framework has achieved remarkable successes in task - oriented dialogues ( Wen et al . , 2017 ; Liu and Lane , 2017 ; Williams et al . , 2017 ; Zhao et al . , 2017 ) .", "entities": []}, {"text": "However , such kind of dialogue system is not fully optimized as the modules are loosely integrated and often not trained jointly in an end - to - end manner , and thus may suffer from increasing error propagation between the modules as the complexity of the dialogues evolves .", "entities": []}, {"text": "A typical case of a complex dialogue setting is when the dialogue extends over multiple domains .", "entities": []}, {"text": "A dialogue state in a multi - domain dialogue should include slots of all applicable domains up to the current turn ( See Table 1 ) .", "entities": []}, {"text": "Each domain can have shared slots that are common among domains or unique slots that are not shared with any .", "entities": []}, {"text": "Directly applying single - domain DST to multi - domain dialogues is not straightforward because the dialogue states extend to multiple domains .", "entities": []}, {"text": "A possible approach is to process a dialogue of NDdomains multiple times , each time obtaining a dialogue state of one domain .", "entities": []}, {"text": "However , this approach does not allow learning co - reference in dialogues in which users can switch from one domain to another .", "entities": []}, {"text": "As the number of dialogue domains increases , traditional pipeline approaches propagate errors from dialogue states to dialogue policy and subsequently , to natural language generator .", "entities": []}, {"text": "Recent efforts ( Eric et al . , 2017 ; Madotto et al . , 2018 ; Wu et al . , 2019b ) address this problem with an integrated sequence - to - sequence structure .", "entities": []}, {"text": "These approaches often consider knowledge bases as memory tuples rather than relational entity tables .", "entities": []}, {"text": "While achieving impressive performance , these approaches are not scalable to large - scale knowledgebases , e.g. thousands of entities , as the memory cost to query entity attributes increases substantially .", "entities": []}, {"text": "Another limitation of these approaches is the absence of dialogue act modelling .", "entities": []}, {"text": "Dialogue act", "entities": []}, {"text": "1861Human : hello Dialogue agent : hi !", "entities": [[4, 5, "DatasetName", "agent"]]}, {"text": "how can i help you ?", "entities": []}, {"text": "Dialogue state : { } Dialogue acts :", "entities": []}, {"text": "[ greeting ] Human : i would like a place to eat in the expensive price range Dialogue agent : sure , what type of food are you interested in ?", "entities": [[18, 19, "DatasetName", "agent"]]}, {"text": "Dialogue state : { restaurant : { pricerange : expensive } } Dialogue acts : [ request - food ] Human : could you make a suggestion ?", "entities": []}, {"text": "one in the centre ?", "entities": []}, {"text": "Dialogue agent : \ufb01tzbillies restaurant is an expensive british restaurant in the centre .", "entities": [[1, 2, "DatasetName", "agent"]]}, {"text": "can i book that for you ?", "entities": []}, {"text": "Dialogue state : { restaurant : { pricerange : expensive , area : centre } } Dialogue acts : [ inform - restaurant , request - booking ] ... ...", "entities": []}, {"text": "Human : also , i need the number for kings hedges learner pool .Dialogue agent : the phone number for the pool is 01234567 , is there something else i can help you ?", "entities": [[14, 15, "DatasetName", "agent"]]}, {"text": "Dialogue state : { restaurant : { pricerange : expensive , area : centre , name=\ufb01zbillies restaurant , request=[address ] } , attraction : { name : kings hedges learner pool , request=[phone]}}Dialogue acts : [ inform - phone ] Table 1 : Example of a multi - domain dialogue with two domains : restaurant andattraction .", "entities": []}, {"text": "is particularly important in task - oriented dialogues as it determines the general decision towards task completion before a dialogue agent can materialize it into natural language response ( See Table 1 ) .", "entities": [[20, 21, "DatasetName", "agent"]]}, {"text": "To tackle the challenges in multi - domain taskoriented dialogues while reducing error propagation among dialogue system modules and keeping the models scalable , we propose UniConv , a uni\ufb01ed neural network architecture for end - to - end dialogue systems .", "entities": []}, {"text": "UniConv consists of a Bi - level State Tracking ( BDST ) module which embeds natural language understanding as it can directly parse dialogue context into a structured dialogue state rather than relying on the semantic frame output from an NLU module in each dialogue turn .", "entities": [[15, 18, "TaskName", "natural language understanding"]]}, {"text": "BDST implicitly models and integrates slot representations from dialogue contextual cues to directly generate slot values in each turn and thus , remove the need for explicit slot tagging features from an NLU .", "entities": []}, {"text": "This approach is more practical than the traditional pipeline models as we do not need slot tagging annotation .", "entities": []}, {"text": "Furthermore , BDST tracks dialogue states in dialogue context in both slot and domain levels .", "entities": []}, {"text": "The output representations from two levels are combined in a late fusion approach to learn multi - domain dialogue states .", "entities": []}, {"text": "Our dialogue state tracker disentangles slot and domain representation learning while enabling deep learning of shared representations of slots common among domains .", "entities": [[8, 10, "TaskName", "representation learning"]]}, {"text": "UniConv integrates BDST with a Joint Dialogue Act and Response Generator ( DARG ) that simultaneously models dialogue acts and generates system responses by learning a latent variable representing dialogue acts and semantically conditioning output response tokens on this latent variable .", "entities": []}, {"text": "The multitask setting of DARG allows our models to model dialogue acts and utilize the distributed representations of dialogue acts , rather than hard discreteoutput values from a dialogue policy module , on output response tokens .", "entities": []}, {"text": "Our response generator incorporates information from dialogue input components and intermediate representations progressively over multiple attention steps .", "entities": []}, {"text": "The output representations are re\ufb01ned after each step to obtain high - resolution signals needed to generate appropriate dialogue acts and responses .", "entities": []}, {"text": "We combine both BDST and DARG for end - to - end neural dialogue systems , from input dialogues to output system responses .", "entities": []}, {"text": "We evaluate our models on the large - scale MultiWOZ benchmark ( Budzianowski et al . , 2018 ) , and compare with the existing methods in DST , context - to - text generation , and end - to - end settings .", "entities": [[9, 10, "DatasetName", "MultiWOZ"], [33, 35, "TaskName", "text generation"]]}, {"text": "The promising performance in all tasks validates the ef\ufb01cacy of our method .", "entities": []}, {"text": "2 Related Work Dialogue State Tracking .", "entities": [[3, 6, "TaskName", "Dialogue State Tracking"]]}, {"text": "Traditionally , DST models are designed to track states of singledomain dialogues such as WOZ ( Wen et al . , 2017 ) and DSTC2 ( Henderson et al . , 2014a ) benchmarks .", "entities": []}, {"text": "There have been recent efforts that aim to tackle multi - domain DST such as ( Ramadan et al . , 2018 ; Lee et al . , 2019 ; Wu et al . , 2019a ; Goel et al . , 2019 ) .", "entities": []}, {"text": "These models can be categorized into two main categories : Fixed vocabulary models ( Zhong et al . , 2018 ; Ramadan et al . , 2018 ; Lee et al . , 2019 ) , which assume known slot ontology with a \ufb01xed candidate set for each slot .", "entities": [[40, 41, "MethodName", "ontology"]]}, {"text": "On the other hand , open - vocabulary models ( Lei et al . , 2018 ; Wu et al . , 2019a ; Gao et al . , 2019 ;", "entities": []}, {"text": "Ren et al . , 2019 ; Le et al . , 2020 ) derive the candidate set based on the source sequence i.e. dialogue history , itself .", "entities": []}, {"text": "Our approach is more related to the open - vocabulary approach as we aim to generate unique dialogue states depending on the input dialogue .", "entities": []}, {"text": "Different from previous", "entities": []}, {"text": "1862 Ut : I am also looking for   a restaurant in the east   Slot    Encoder Domain   Encoder", "entities": []}, {"text": "Slot   Self - Attn Domain   Self - Attn Slot", "entities": []}, {"text": "\u2192Context   AttnDomain   \u2192Context   AttnSlot \u2192State   AttnDomain   \u2192Utterance   AttnSlot \u2192Utterance   AttnDialogue   Context   Encoder User Utt .", "entities": []}, {"text": "Encoder State   Encoder   DomainSlot   Self - Attn RNN RNN   Linear ...", "entities": []}, {"text": "< sos >   0 or 1 east east < eos >   Act+Res   Self - Attn Act+Res   \u2192State Attn Act+Res\u2192 DB AttnAct+Res   \u2192Context   AttnAct+Res   \u2192Utterance   Attn ZctxZD ZsZutt zstprev   Target   Res .", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "Encoder   Zres+act", "entities": []}, {"text": "Linear", "entities": []}, {"text": "Rt ZctxZuttSelect * from   restaurant where   location = east ...   0100 ..", "entities": []}, {"text": "10ZdbBt # entitiesSlot - level DST Domain - level DST   Joint Dialogue Act and Response Generator x NSdst x NgenZact Linear At(inform slot i )   ( request slot j )   ( dialogue acts )", "entities": []}, {"text": "( response ) ( U1 , R1 \u2026 ,Ut-1 , Rt-1 ) D : ( restaurant ,   hotel , taxi \u2026 )", "entities": []}, {"text": "Zuttx NDdst Bt-1 : abc < inf_name> ... <hotel >   cambridge < inf_depart> ...", "entities": []}, {"text": "<train > S : ( < inf_type > ,   < inf_name > \u2026", "entities": []}, {"text": "< req_address > , \u2026 )", "entities": []}, {"text": "Zctx Bt : abc < inf_name >   ...", "entities": []}, {"text": "< hotel >   east < inf_location >   < restaurant > State   Encoder   Zstcurr(shifted )", "entities": []}, {"text": "Rt : There are 8   restaurants in the   < res_location > .", "entities": []}, {"text": "ZdstFigure 1 : Our uni\ufb01ed architecture has three components : ( 1 ) Encoders encode all text input into continuous representations ; ( 2 ) Bi - level State Tracker ( BDST ) includes 2 modules for slot - level and domain - level representation learning ; and ( 3 ) Joint Dialogue Act and Response Generator ( DARG ) obtains dependencies between the target response representations and other dialogue components .", "entities": [[44, 46, "TaskName", "representation learning"]]}, {"text": "generation - based approaches", "entities": []}, {"text": ", our state tracker can incorporate contextual information into domain and slot representations independently .", "entities": []}, {"text": "Context - to - Text Generation .", "entities": [[4, 6, "TaskName", "Text Generation"]]}, {"text": "This task was traditionally solved by two separate dialogue modules :", "entities": []}, {"text": "Dialogue Policy ( Peng et al . , 2017 , 2018 ) and NLG ( Wen et al . , 2016 ; Su et al . , 2018 ) .", "entities": []}, {"text": "Recent work attempts to combine these two modules to directly generate system responses with or without modeling dialogue acts .", "entities": []}, {"text": "Zhao et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) models action space of dialogue agent as latent variables .", "entities": [[8, 9, "DatasetName", "agent"]]}, {"text": "Chen et al .", "entities": []}, {"text": "( 2019 ) predicts dialogue acts using a hierarchical graph structure with each path representing a unique act .", "entities": []}, {"text": "Pei et al .", "entities": []}, {"text": "( 2019 ) ;", "entities": []}, {"text": "Peng et al .", "entities": []}, {"text": "( 2019 ) use multiple dialogue agents , each trained for a speci\ufb01c dialogue domain , and combine them through a common dialogue agent .", "entities": [[23, 24, "DatasetName", "agent"]]}, {"text": "Mehri et al .", "entities": []}, {"text": "( 2019 ) models dialogue policy and NLG separately and fuses feature representations at different levels to generate responses .", "entities": []}, {"text": "Our models simultaneously learn dialogue acts as a latent variable while allowing semantic conditioning on distributed representations of dialogue acts rather than hard discrete features .", "entities": []}, {"text": "End - to - End Dialogue Systems .", "entities": []}, {"text": "In this task , conventional approaches combine Natural Language Understanding ( NLU ) , DST , Dialogue Policy , and NLG , into a pipeline architecture ( Wenet al . , 2017 ; Bordes et al . , 2016 ; Liu and Lane , 2017 ; Li et al . , 2017 ; Liu and Perez , 2017 ; Williams et al . , 2017 ;", "entities": [[7, 10, "TaskName", "Natural Language Understanding"]]}, {"text": "Zhao et", "entities": []}, {"text": "al . , 2017 ; Jhunjhunwala et al . , 2020 ) .", "entities": []}, {"text": "Another framework does not explicitly modularize these components but incorporate them through a sequence - to - sequence framework ( Serban et al . , 2016 ; Lei et al . , 2018 ; Yavuz et al . , 2019 ) and a memory - based entity dataset of triplets ( Eric and Manning , 2017 ; Eric et al . , 2017 ; Madotto et al . , 2018 ; Qin et al . , 2019 ; Gangi Reddy et al . , 2019 ; Wu et al . , 2019b ) .", "entities": []}, {"text": "These approaches bypass dialogue state and/or act modeling and aim to generate output responses directly .", "entities": []}, {"text": "They achieve impressive success in generating dialogue responses in open - domain dialogues with unstructured knowledge bases .", "entities": []}, {"text": "However , in a task - oriented setting with an entity dataset , they might suffer from an explosion of memory size when the number of entities from multiple dialogue domains increases .", "entities": []}, {"text": "Our work is more related to the traditional pipeline strategy but we integrate our dialogue models by unifying two major components rather than using the traditional four - module architecture , to alleviate error propagation from upstream to downstream components .", "entities": []}, {"text": "Different from prior work such as ( Shu et al . , 2019 ) , our model facilitates multi - domain state tracking and allows learning dialogue acts", "entities": []}, {"text": "1863during response generation .", "entities": [[1, 3, "TaskName", "response generation"]]}, {"text": "3 Method The input consists of dialogue context of t\u00001turns , each including a pair of user utterance Uand system response R,(U1 ; R1 ) ; : : : ; ( Ut\u00001 ; Rt\u00001 ) , and the user utterance at current turn Ut .", "entities": []}, {"text": "A taskoriented dialogue system aims to generate the next response Rt .", "entities": []}, {"text": "The information for responses is typically queried from a database based on the user \u2019s provided information i.e. inform slots tracked by a DST .", "entities": []}, {"text": "We assume access to a database of all domains with each column corresponding to a speci\ufb01c slot being tracked .", "entities": []}, {"text": "We denote the intermediate output , including the dialogue state of current turn Btand dialogue act as At .", "entities": []}, {"text": "We denote the list of all domains D= ( d1 ; d2 ; : : :) , all slots S= ( s1 ; s2 ; : : :) , and all acts A= ( a1 ; a2 ; : : :) .", "entities": []}, {"text": "We also denote the list of all ( domain , slot ) pairs asDS", "entities": []}, {"text": "= ( ds1 ; ds2 ; : : :) .", "entities": []}, {"text": "Note thatkDSk \u0014 kDk\u0002kSkas some slots might not be applicable in all domains .", "entities": []}, {"text": "Given the current dialogue turn t , we represent each text input as a sequence of tokens , each of which is a unique token index from a vocabulary set V : dialogue context Xctx , current user utterance Xutt , and target system response Xres .", "entities": []}, {"text": "Similarly , we also represent the list of domains as XDand the list of slots as XS .", "entities": []}, {"text": "In DST , we consider the raw text form of dialogue state of the previous turn Bt\u00001 , similarly as ( Lei et al . , 2018 ; Budzianowski and Vuli \u00b4 c , 2019 ) .", "entities": []}, {"text": "In the context - to - text setting , we assume access to the ground - truth dialogue states of current turn Bt .", "entities": []}, {"text": "The dialogue state of the previous and current turn can then be represented as a sequence of tokens Xprev st andXcurr strespectively .", "entities": []}, {"text": "For a fair comparison with current approaches , during inference , we use the model predicted dialogue states ^Xprev stand do not useXcurr stin DST and end - to - end tasks .", "entities": []}, {"text": "Following ( Wen et al . , 2015 ; Budzianowski et al . , 2018 ) , we consider the delexicalized target response Xdl res by replacing tokens of slot values by their corresponding generic tokens to allow learning valueindependent parameters .", "entities": []}, {"text": "Our model consists of 3 major components ( See Figure 1 ) .", "entities": []}, {"text": "First , Encoders encode all text input into continuous representations .", "entities": []}, {"text": "To make it consistent , we encode all input with the same embedding dimension .", "entities": [[12, 14, "HyperparameterName", "embedding dimension"]]}, {"text": "Secondly , our Bi - level State Tracker ( BDST ) is used to detect contextual dependencies to generate dialogue states .", "entities": []}, {"text": "The DST includes 2modules for slot - level and domain - level representation learning .", "entities": [[12, 14, "TaskName", "representation learning"]]}, {"text": "Each module comprises attention layers to project domain or slot representations and incorporate important information from dialogue context , dialogue state of the previous turn , and current user utterance .", "entities": [[3, 5, "HyperparameterName", "attention layers"]]}, {"text": "The outputs are combined as a context - aware vector to decode the correspondinginform orrequest slots in each domain .", "entities": []}, {"text": "Lastly , ourJoint Dialogue Act and Response Generator ( DARG ) projects the target system response representations and enhances them with information from various dialogue components .", "entities": []}, {"text": "Our response generator can also learn a latent representation to generate dialogue acts , which condition all target tokens during each generation step .", "entities": []}, {"text": "3.1 Encoders An encoder encodes a text sequence Xto a sequence of continuous representation Z2RLX\u0002d .", "entities": []}, {"text": "LXis the length of sequence Xanddis the embedding dimension .", "entities": [[7, 9, "HyperparameterName", "embedding dimension"]]}, {"text": "Each encoder includes a token - level embedding layer .", "entities": []}, {"text": "The embedding layer is a trainable embedding matrix E2RkVk\u0002d .", "entities": []}, {"text": "Each row represents a token in the vocabulary set Vas ad - dimensional vector .", "entities": []}, {"text": "We denote E(X ) as the embedding function that transform the sequence Xby looking up the respective token index : Zemb = E(X)2RLX\u0002d .", "entities": []}, {"text": "We inject the positional attribute of each token as similarly adopted in ( Vaswani et al . , 2017 ) .", "entities": []}, {"text": "The positional encoding is denoted as PE .", "entities": []}, {"text": "The \ufb01nal embedding is the element - wise summation between token - embedded representations and positional encoded representations with layer normalization ( Ba et al . , 2016 ): Z= LayerNorm ( Zemb+PE(X))2RLX\u0002d .", "entities": [[19, 21, "MethodName", "layer normalization"]]}, {"text": "The encoder outputs include representations of dialogue context Zctx , current user utterance Zutt , and target response Zdl res .", "entities": []}, {"text": "We also encode the dialogue states of the previous turn and current turn and obtain Zprev standZcurr strespectively .", "entities": []}, {"text": "We encode XSandXDusing only token - level embedding layer : ZS= LayerNorm ( E(XS))and ZD= LayerNorm ( E(XD ) ) .", "entities": []}, {"text": "During training , we shift the target response by one position to the left side to allow auto - regressive prediction in each generation step .", "entities": []}, {"text": "We share the embedding matrix Eto encode all text tokens except for tokens of target responses as the delexicalized outputs contain different semantics from natural language inputs .", "entities": []}, {"text": "18643.2 Bi - level Dialogue State Tracker ( BDST ) Slot - level DST .", "entities": []}, {"text": "We adopt the Transformer attention ( Vaswani et al . , 2017 ) , which consists of a dot - product attention with skip connection , to integrate dialogue contextual information into each slot representation .", "entities": [[3, 4, "MethodName", "Transformer"], [18, 22, "MethodName", "dot - product attention"]]}, {"text": "We denote Att(Z1 ; Z2)as the attention operation from Z2onZ1 .", "entities": []}, {"text": "We \ufb01rst enable models to process all slot representations together rather than separately as in previous DST models ( Ramadan et al . , 2018 ; Wu et al . , 2019a ) .", "entities": []}, {"text": "This strategy allows our models to explicitly learn dependencies between all pairs of slots .", "entities": []}, {"text": "Many pairs of slots could exhibit correlation such as time - wise relation ( \u201c departure_time \" and \u201c arrival_time \" ) .", "entities": []}, {"text": "We obtain Zdst SS= Att ( ZS ; ZS)2RkSk\u0002d .", "entities": []}, {"text": "We incorporate the dialogue information by learning dependencies between each slot representation and each token in the dialogue history .", "entities": []}, {"text": "Previous approaches such as ( Budzianowski and Vuli \u00b4 c , 2019 ) consider all dialogue history as a single sequence but we separate them into two inputs because the information in Xuttis usually more important to generate responses while Xctxincludes more background information .", "entities": []}, {"text": "We then obtain Zdst S;ctx= Att ( Zctx ; Zdst SS)2RkSk\u0002dand Zdst S;utt= Att ( Zutt ; Zdst S;ctx)2RkSk\u0002d .", "entities": []}, {"text": "Following ( Lei et al . , 2018 ) , we incorporate dialogue state of the previous turn Bt\u00001which is a more compact representation of dialogue context .", "entities": []}, {"text": "Hence , we can replace the full dialogue context to only Rt\u00001as the remaining part is represented inBt\u00001 .", "entities": []}, {"text": "This approach avoids taking in all dialogue history and is scalable as the conversation grows longer .", "entities": []}, {"text": "We add the attention layer to obtainZdst S;st= Att ( Zprev st ; Zdst S;ctx)2RkSk\u0002d(See Figure 1 ) .", "entities": []}, {"text": "We further improve the feature representations by repeating the attention sequence over Ndst Stimes .", "entities": []}, {"text": "We denote the \ufb01nal output Zdst S. Domain - level DST .", "entities": []}, {"text": "We adopt a similar architecture to learn domain - level representations .", "entities": []}, {"text": "The representations learned in this module exhibit global information while slot - level representations contain local dependencies to decode multi - domain dialogue states .", "entities": []}, {"text": "First , we enable the domain - level DST to capture dependencies between all pairs of domains .", "entities": []}, {"text": "For example , some domains such as \u201c taxi \u201d are typically paired with other domains such as \u201c attraction \u201d , but usually not with the \u201c train \u201d domain .", "entities": []}, {"text": "We then obtain Zdst DD= Att ( ZD ; ZD)2RkDk\u0002d .", "entities": []}, {"text": "We then allow models to capture dependencies between each domain representation and each tokenin dialogue context and current user utterance .", "entities": []}, {"text": "By segregating dialogue context and current utterance , our models can potentially detect changes of dialogue domains from past turns to the current turn .", "entities": []}, {"text": "Especially in multi - domain dialogues , users can switch from one domain to another and the next system response should address the latest domain .", "entities": []}, {"text": "We then obtain Zdst D;ctx= Att ( Zctx ; Zdst DD)2RkDk\u0002d andZdst D;utt= Att ( Zutt ; Zdst D;ctx)2RkDk\u0002dsequentially .", "entities": []}, {"text": "Similar to the slot - level module , we re\ufb01ne feature representations over Ndst Dtimes and denote the \ufb01nal output as Zdst D. Domain - Slot DST .", "entities": []}, {"text": "We combined domain and slot representations by expanding the tensors to identical dimensions i.e. kDk\u0002k Sk\u0002d .", "entities": []}, {"text": "We then apply Hadamard product , resulting in domain - slot joint features Zdst DS2RkDk\u0002kSk\u0002d .", "entities": []}, {"text": "We then apply a self - attention layer to allow learning of dependencies between joint domain - slot features : Zdst= Att ( Zdst DS ; Zdst DS)2RkDk\u0002kSk\u0002d .", "entities": []}, {"text": "In this attention , we mask the intermediate representations in positions of invalid domain - slot pairs .", "entities": []}, {"text": "Compared to previous work such as ( Wu et al . , 2019a ) , we adopt a late fusion method whereby domain and slot representations are integrated in deeper layers .", "entities": []}, {"text": "3.2.1 State Generator The representations Zdstare used as context - aware representations to decode individual dialogue states .", "entities": []}, {"text": "Given a domain index iand slot index j , the feature vector Zdst[i ; j;:]2Rdis used to generate value of the corresponding ( domain , slot ) pair .", "entities": []}, {"text": "The vector is used as an initial hidden state for an RNN decoder to decode an inform slot value .", "entities": []}, {"text": "Given thek - th ( domain , slot ) pair and decoding step l , the output hidden state in each recurrent step hklis passed through a linear transformation with softmax to obtain output distribution over vocabulary set V : Pinf kl= Softmax ( hklWinf)2RkVk where Winf dst2Rdrnn\u0002kVk .", "entities": [[30, 31, "MethodName", "softmax"], [42, 43, "MethodName", "Softmax"]]}, {"text": "For request slot of k - th ( domain , slot ) pair , we pass the corresponding vector Zdstvector through a linear layer with sigmoid activation to predict a value of 0 or 1 .", "entities": [[22, 24, "MethodName", "linear layer"], [25, 27, "MethodName", "sigmoid activation"], [32, 33, "DatasetName", "0"]]}, {"text": "Preq k= Sigmoid ( Zdst kWreq ) .", "entities": []}, {"text": "Optimization .", "entities": []}, {"text": "The DST is optimized by the crossentropy loss functions of inform andrequest slots : Ldst = Linf+Lreq = kDSkX k=1kYkkX l=1\u0000log(Pinf kl(ykl ) )", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "+ kDSkX k=1\u0000yklog(Preq k)\u0000(1\u0000yk)(1\u0000log(Preq k ) )", "entities": []}, {"text": "18653.3 Joint Dialogue Act and Response Generator ( DARG )", "entities": []}, {"text": "Database Representations .", "entities": []}, {"text": "Following ( Budzianowski et al . , 2018 ) , we create a one - hot vector for each domain d : xd db2f0;1g6 andP6 ixd db;i= 1 .", "entities": []}, {"text": "Each position of the vector indicates a number or a range of entities .", "entities": []}, {"text": "The vectors of all domains are concatenated to create a multi - domain vector Xdb2R6\u0002kDk .", "entities": []}, {"text": "We embed this vector as described in Section 3.1 .", "entities": []}, {"text": "Response Generation .", "entities": [[0, 2, "TaskName", "Response Generation"]]}, {"text": "We adopt a stackedattention architecture that sequentially learns dependencies between each token in target responses with each dialogue component representation .", "entities": []}, {"text": "First , we obtain Zgen res= Att ( Zres ; Zres)2RLres\u0002d .", "entities": []}, {"text": "This attention layer can learn semantics within the target response to construct a more semantically structured sequence .", "entities": []}, {"text": "We then use attention to capture dependencies in background information contained in dialogue context and user utterance .", "entities": []}, {"text": "The outputs are Zgen ctx= Att ( Zctx ; Zgen res)2RLres\u0002dand Zgen utt= Att ( Zutt ; Zgen ctx)2RLres\u0002dsequentially .", "entities": []}, {"text": "To incorporate information of dialogue states and DB results , we apply attention steps to capture dependencies between each response token representation and state or DB representation .", "entities": []}, {"text": "Specifically , we \ufb01rst obtain Zgen dst= Att ( Zdst ; Zgen utt)2 RLres\u0002d .", "entities": []}, {"text": "In the context - to - text setting , as we directly use the ground - truth dialogue states , we simply replace ZdstwithZcurr st .", "entities": []}, {"text": "Then we obtain Zgen db= Att ( Zdb ; Zgen dst)2RLres\u0002d .", "entities": []}, {"text": "These attention layers capture the information needed to generate tokens that are towards task completion and supplement the contextual cues obtained in previous attention layers .", "entities": [[1, 3, "HyperparameterName", "attention layers"], [23, 25, "HyperparameterName", "attention layers"]]}, {"text": "We let the models to progressively capture these dependencies for Ngentimes and denote the \ufb01nal output as Zgen .", "entities": []}, {"text": "The \ufb01nal output is passed to a linear layer with softmax activation to decode system responses auto - regressively : Pres= Softmax ( ZgenWgen)2RLres\u0002kVresk Dialogue Act Modeling .", "entities": [[7, 9, "MethodName", "linear layer"], [10, 11, "MethodName", "softmax"], [21, 22, "MethodName", "Softmax"]]}, {"text": "We couple response generation with dialogue act modeling by learning a latent variable Zact2Rd .", "entities": [[2, 4, "TaskName", "response generation"]]}, {"text": "We place the vector in the \ufb01rst position of Zres , resulting in Zres+act2R(Lres+1)\u0002d .", "entities": []}, {"text": "We then pass this tensor to the same stacked attention layers as above .", "entities": [[9, 11, "HyperparameterName", "attention layers"]]}, {"text": "By adding the latent variable in the \ufb01rst position , we allow our model to semantically condition all downstream tokens from second position , i.e. all tokens in the target response , on this latent variable .", "entities": []}, {"text": "The output representation of the latent vector i.e. Domain#dialogues train val test Restaurant 3,817 438 437 Hotel 3,387 416 394 Attraction 2,718 401 396 Train 3,117 484 495 Taxi 1,655 207 195 Police 245 0 0 Hospital 287 0 0 Table 2 : Summary of MultiWOZ dataset ( Budzianowski et al . , 2018 ) by domain \ufb01rst row in Zgen , incorporates contextual signals accumulated from all attention layers and is used to predict dialogue acts .", "entities": [[34, 35, "DatasetName", "0"], [35, 36, "DatasetName", "0"], [38, 39, "DatasetName", "0"], [39, 40, "DatasetName", "0"], [45, 46, "DatasetName", "MultiWOZ"], [68, 70, "HyperparameterName", "attention layers"]]}, {"text": "We denote this representation as Zgen actand pass it through a linear layer to obtain a multi - hot encoded tensor .", "entities": [[11, 13, "MethodName", "linear layer"]]}, {"text": "We apply Sigmoid on this tensor to classify each dialogue act as 0 or 1 : Pact= Sigmoid ( Zgen actWact)2RkAk .", "entities": [[12, 13, "DatasetName", "0"]]}, {"text": "Optimization .", "entities": []}, {"text": "The response generator is jointly trained by the cross - entropy loss functions of generated responses and dialogue acts :", "entities": [[11, 12, "MetricName", "loss"]]}, {"text": "Lgen = Lres+Lact = kYreskX l=1\u0000log(Pres l(yl ) )", "entities": []}, {"text": "+ kAkX a=1\u0000yalog(Pact a)\u0000(1\u0000ya)(1\u0000log(Pact a ) ) 4 Experiments 4.1 Dataset We evaluate our models with the multi - domain dialogue corpus MultiWOZ 2.0 ( Budzianowski et al . , 2018 ) and 2.1 ( Eric et al . , 2019 ) ( The latter includes corrected state labels for the DST task ) .", "entities": [[22, 24, "DatasetName", "MultiWOZ 2.0"]]}, {"text": "From the dialogue state annotation of the training data , we identi\ufb01ed all possible domains and slots .", "entities": []}, {"text": "We identi\ufb01edkDk= 7 domains andkSk= 30 slots , including 19 inform slots and 11 request slots .", "entities": []}, {"text": "We also identi\ufb01edkAk= 32 acts .", "entities": []}, {"text": "The corpus includes 8,438 dialogues in the training set and 1,000 in each validation and test set .", "entities": []}, {"text": "We present a summary of the dataset in Table 2 .", "entities": []}, {"text": "For additional information of data pre - processing procedures , domains , slots , and entity DBs , please refer to Appendix A. 4.2 Experiment Setup We select d= 256 , hatt= 8,Ndst S = Ndst D= Ngen= 3 .", "entities": []}, {"text": "We employed dropout ( Srivastava et al . , 2014 ) of 0:3and label smoothing ( Szegedy et al . , 2016 ) on target system responses during training .", "entities": [[13, 15, "MethodName", "label smoothing"]]}, {"text": "1866Model Joint Acc .", "entities": [[2, 3, "MetricName", "Acc"]]}, {"text": "HJST ( Eric et al . , 2019 ) 35.55 % DST Reader ( Gao et al . , 2019 ) 36.40 % TSCP ( Lei et al . , 2018 ) 37.12 % FJST ( Eric et al . , 2019 ) 38.00 % HyST ( Goel et al . , 2019 ) 38.10 % TRADE ( Wu et al . , 2019a ) 45.60 % NADST ( Le et al . , 2020 ) 49.04 % DSTQA ( Zhou and Small , 2019 ) 51.17 % SOM - DST ( Kim et al . , 2020 ) 53.01 % BDST ( Ours ) 49.55 % Table 3 : Evaluation of DST on MultiWOZ2.1Model Inform Success BLEU Baseline Budzianowski et al .", "entities": [[88, 89, "MethodName", "SOM"], [117, 118, "MetricName", "BLEU"]]}, {"text": "( 2018 ) 71.29 % 60.96 % 18.80 TokenMoE ( Pei et al . , 2019 ) 75.30 % 59.70 % 16.81 HDSA ( Chen et al . , 2019 ) 82.90 % 68.90 % 23.60 Structured Fusion ( Mehri et al . , 2019 ) 82.70 % 72.10 % 16.34 LaRL ( Zhao et al . , 2019 ) 82.78 % 79.20 % 12.80 GPT2 ( Budzianowski and Vuli \u00b4 c , 2019 ) 70.96 % 61.36 % 19.05 DAMD ( Zhang et al . , 2019 ) 89.50 % 75.80 % 18.30 DARG ( Ours ) 87.80 % 73.60 % 18.80 Table 4 : Evaluation of context - to - text task on MultiWOZ2.0 .", "entities": []}, {"text": "We adopt a teacher - forcing training strategy by simply using the ground - truth inputs of dialogue state of the previous turn and the gold DB representations .", "entities": []}, {"text": "During inference in DST and end - to - end tasks , we decode system responses sequentially turn by turn , using the previously decoded state as input in the current turn , and at each turn , using the new predicted state to query DBs .", "entities": []}, {"text": "We train all networks with Adam optimizer ( Kingma and Ba , 2015 ) and a decaying learning rate schedule .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [17, 19, "HyperparameterName", "learning rate"]]}, {"text": "All models are trained up to 30epochs and the best models are selected based on validation loss .", "entities": [[16, 17, "MetricName", "loss"]]}, {"text": "We used a greedy approach to decode all slots and a beam search with beam size 5 .", "entities": []}, {"text": "To evaluate the models , we use the following metrics : Joint Accuracy and Slot Accuracy ( Henderson et al . , 2014b ) , Inform and Success ( Wen et al . , 2017 ) , and BLEU score ( Papineni et al . , 2002 ) .", "entities": [[12, 13, "MetricName", "Accuracy"], [15, 16, "MetricName", "Accuracy"], [38, 40, "MetricName", "BLEU score"]]}, {"text": "As suggested by Liu et al . ( 2016 ) , human evaluation , even though popular in dialogue research , might not be necessary in tasks with domain constraints such as MultiWOZ .", "entities": [[32, 33, "DatasetName", "MultiWOZ"]]}, {"text": "We implemented all models using Pytorch and will release our code on github1 .", "entities": []}, {"text": "4.3 Results DST .", "entities": []}, {"text": "We test our state tracker ( i.e. using only Ldst ) and compare the performance with the baseline models in Table 3 ( Refer to Appendix B for description of DST baselines ) .", "entities": []}, {"text": "Our model can outperform \ufb01xed - vocabulary approaches such as HJST and FJST , showing the advantage of generating unique slot values rather than relying on a slot ontology with a \ufb01xed set of candidates .", "entities": [[28, 29, "MethodName", "ontology"]]}, {"text": "DST Reader model ( Gao et al . , 2019 ) does not perform well and we note that many slot values are not easily expressed as a text span in source text inputs .", "entities": []}, {"text": "DST approaches that separate domain and slot representations such as TRADE ( Wu et al . , 2019a ) reveal 1https://github.com/henryhungle/ UniConvcompetitive performance .", "entities": []}, {"text": "However , our approach has better performance as we adopt a late fusion strategy to explicitly obtain more \ufb01ne - grained contextual dependencies in each domain and slot representation .", "entities": []}, {"text": "In this aspect , our model is related to TSCP ( Lei et al . , 2018 ) which decodes output state sequence auto - regressively .", "entities": []}, {"text": "However , TSCP attempts to learn domain and slot dependencies implicitly and the model is limited by selecting the maximum output state length ( which can vary signi\ufb01cantly in multi - domain dialogues ) .", "entities": []}, {"text": "Context - to - Text Generation .", "entities": [[4, 6, "TaskName", "Text Generation"]]}, {"text": "We compare with existing baselines in Table 4 ( Refer to Appendix B for description of the baseline models ) .", "entities": []}, {"text": "Our model achieves very competitive Inform , Success , and BLEU scores .", "entities": [[10, 11, "MetricName", "BLEU"]]}, {"text": "Compared to TokenMOE ( Pei et al . , 2019 ) , our single model can outperform multiple domain - speci\ufb01c dialogue agents as each attention module can suf\ufb01ciently learn contextual features of multiple domains .", "entities": []}, {"text": "Compared to HDSA ( Chen et al . , 2019 ) which uses a graph structure to representacts , our approach is simpler yet able to outperform HDSA in Inform score .", "entities": []}, {"text": "Our work is related to Structured Fusion ( Mehri et al . , 2019 ) as we incorporate intermediate representations during decoding .", "entities": []}, {"text": "However , our approach does not rely on pretraining individual sub - modules but simultaneously learning both act representations and predicting output tokens .", "entities": []}, {"text": "Similarly , our stacked attention architecture can achieve good performance in BLEU score , competitively with a GPT-2 based model ( Budzianowski and Vuli \u00b4 c , 2019 ) , while consistently improve other metrics .", "entities": [[11, 13, "MetricName", "BLEU score"], [17, 18, "MethodName", "GPT-2"]]}, {"text": "For completion , we tested our models on MultiWOZ2.1 and achieved similar results : 87.90 % Inform , 72.70 % Success , and 18.52 BLEU score .", "entities": [[24, 26, "MetricName", "BLEU score"]]}, {"text": "Future work may further improve Success by optimizing the models towards a higher success rate using strategies such as LaRL ( Zhao et al . , 2019 ) .", "entities": []}, {"text": "Another direction is a data augmentation approach such as DAMD ( Zhang et al . ,", "entities": [[4, 6, "TaskName", "data augmentation"]]}, {"text": "1867Model Joint Acc Slot Acc Inform Success BLEU TSCP ( L=8 ) ( Lei et al . , 2018 ) 31.64 % 95.53 % 45.31 % 38.12 % 11.63 TSCP ( L=20 ) ( Lei et al . , 2018 ) 37.53 % 96.23 % 66.41 % 45.32 % 15.54 HRED - TS ( Peng et al . , 2019 ) - - 70.00 % 58.00 % 17.50 Structured Fusion ( Mehri et al . , 2019 ) - - 73.80 % 58.60 % 16.90 DAMD ( Zhang et al . , 2019 ) - - 76.30 % 60.40 % 16.60 UniConv ( Ours ) 50.14 % 97.30 % 72.60 % 62.90 % 19.80 Table 5 : Evaluation on MultiWOZ2.1 in the end - to - end setting . 2019 ) which achieves signi\ufb01cant performance gain in this task .", "entities": [[2, 3, "MetricName", "Acc"], [4, 5, "MetricName", "Acc"], [7, 8, "MetricName", "BLEU"], [52, 53, "MethodName", "TS"]]}, {"text": "End - to - End .", "entities": []}, {"text": "From Table 5 , our model outperforms existing baselines in all metrics except the Inform score ( See Appendix B for a description of baseline models ) .", "entities": []}, {"text": "In TSCP ( Lei et al . , 2018 ) , increasing the maximum dialogue state span Lfrom 8 to 20 tokens helps to improve the DST performance , but also increases the training time significantly .", "entities": []}, {"text": "Compared with HRED - TS ( Peng et al . , 2019 ) , our single model generates better responses in all domains without relying on multiple domainspeci\ufb01c teacher models .", "entities": [[4, 5, "MethodName", "TS"]]}, {"text": "We also noted that the performance of DST improves in contrast to the previous DST task .", "entities": []}, {"text": "This can be explained as additional supervision from system responses not only contributes to learn a natural response but also positively impact the DST component .", "entities": []}, {"text": "Other baseline models such as ( Eric and Manning , 2017 ; Wu et al . , 2019b ) present challenges in the MultiWOZ benchmark as the models could not fully optimize due to the large scale entity memory .", "entities": [[23, 24, "DatasetName", "MultiWOZ"]]}, {"text": "For example , following GLMP ( Wu et al . , 2019b ) , the restaurant domain alone has over 1,000 memory tuples of ( Subject , Relation , Object ) .", "entities": []}, {"text": "Ablation .", "entities": []}, {"text": "We conduct a comprehensive ablation analysis with several model variants in Table 6 and have the following observations : \u000fThe model variant with a single - level DST ( by considering S = DSandNdst D= 0 ) ( Row A2 ) performs worse than the Bi - level DST ( Row A1 ) .", "entities": [[35, 36, "DatasetName", "0"]]}, {"text": "In addition , using the dual architecture also improves the latency in each attention layers as typically kDk+kSk \u001c k DSk .", "entities": [[13, 15, "HyperparameterName", "attention layers"]]}, {"text": "The performance gap also indicates the potential of separating global and local dialogue state dependencies by domain and slot level .", "entities": []}, {"text": "\u000fUsing Bt\u00001and only the last user utterance as the dialogue context ( Row A1 and B1 ) performs as well as using Bt\u00001and a full - length dialogue history ( Row A5 and B3 ) .", "entities": []}, {"text": "This demonstrates that the information from thelast dialogue state is suf\ufb01cient to represent the dialogue history up to the last user utterance .", "entities": []}, {"text": "One bene\ufb01t from not using the full dialogue history is that it reduces the memory cost as the number of tokens in a full - length dialogue history is much larger than that of a dialogue state ( particularly as the conversation evolves over many turns ) .", "entities": []}, {"text": "\u000fWe note that removing the loss function to learn the dialogue act latent variable ( Row B2 ) can hurt the generation performance , especially by the task completion metrics Inform andSuccess .", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "This is interesting as we expect dialogue acts affect the general semantics of output sentences , indicated by BLEU score , rather than the model ability to retrieve correct entities .", "entities": [[18, 20, "MetricName", "BLEU score"]]}, {"text": "This reveals the bene\ufb01t of our approach .", "entities": []}, {"text": "By enforcing a semantic condition on each token of the target response , the model can facility the dialogue \ufb02ow towards successful task completion .", "entities": []}, {"text": "\u000fIn both state tracker and response generator modules , we note that learning feature representations through deeper attention networks can improve the quality of predicted states and system responses .", "entities": []}, {"text": "This is consistent with our DST performance as compared to baseline models of shallow networks .", "entities": []}, {"text": "\u000fLastly , in the end - to - end task , our model achieves better performance as the number of attention heads increases , by learning more high - resolution dependencies .", "entities": []}, {"text": "5 Domain - dependent Results DST .", "entities": []}, {"text": "For state tracking , the metrics are calculated for domain - speci\ufb01c slots of the corresponding domain at each dialogue turn .", "entities": []}, {"text": "We also report the DST separately for multi - domain and single - domain dialogues to evaluate the challenges in multi - domain dialogues and our DST performance gap as compared to single - domain dialogues .", "entities": []}, {"text": "From Table 7 ,", "entities": []}, {"text": "1868 # Xctx Bt\u00001Ndst", "entities": []}, {"text": "S", "entities": []}, {"text": "Ndst D NgenLact d h att Joint Acc .", "entities": [[7, 8, "MetricName", "Acc"]]}, {"text": "Slot Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "Inform Success BLEU", "entities": [[2, 3, "MetricName", "BLEU"]]}, {"text": "A1 Rt\u00001 X 3 3 0 256 8 49.55 % 97.32 % - - A2 Rt\u00001 X 3 0 0 256 8 47.91 % 97.25 % - - A3", "entities": [[5, 6, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [19, 20, "DatasetName", "0"]]}, {"text": "Rt\u00001 X 2 2 0 256 8 47.80 % 97.22 % - - A4", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "Rt\u00001 X 1 1 0 256 8 46.20 % 97.08 % - - A5", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "( U ; R ) 1 : t\u00001X 3 3 0 256 8 49.20 % 97.34 % - - B1 Rt\u00001 0 0 3 X 256 8 - - 87.90 % 72.70 % 18.52 B2 Rt\u00001 0 0 3 256 8 - - 82.70 % 70.60 % 18.51 B3 ( U ; R ) 1 : t\u00001 0 0 3 X 256 8 - - 87.14 % 71.52 % 18.90 B4 Rt\u00001 0 0 2 X 256 8 - - 81.60 % 66.40 % 18.48 B5 Rt\u00001 0 0 1 X 256 8 - - 77.70 % 62.80 % 18.50 C1 Rt\u00001 X 3 3 3 X 256 8 50.14 % 97.30 % 72.60 % 62.90 % 19.80 C2 Rt\u00001 X 3 3 3 X 128 8 45.70 % 97.00 % 67.40 % 58.30 % 19.90 C3 Rt\u00001 X 3 3 3 X 256 4 47.30 % 97.10 % 68.70 % 57.10 % 19.60 C4 Rt\u00001 X 3 3 3 X 256 2 45.90 % 97.00 % 66.10 % 55.60 % 19.80 C5 Rt\u00001 X 3 3 3 X 256 1 43.30 % 96.70 % 62.30 % 52.60 % 19.90 Table 6 : Ablation analysis on the MultiWOZ2.1 in DST ( top ) , context - to - text ( middle ) , and end - to - end ( bottom ) .", "entities": [[10, 11, "DatasetName", "0"], [21, 22, "DatasetName", "0"], [22, 23, "DatasetName", "0"], [36, 37, "DatasetName", "0"], [37, 38, "DatasetName", "0"], [57, 58, "DatasetName", "0"], [58, 59, "DatasetName", "0"], [70, 71, "MetricName", "B4"], [72, 73, "DatasetName", "0"], [73, 74, "DatasetName", "0"], [87, 88, "DatasetName", "0"], [88, 89, "DatasetName", "0"], [136, 137, "DatasetName", "C3"], [154, 155, "DatasetName", "C4"]]}, {"text": "our DST performs consistently well in the 3 domains attraction , restaurant , and train domains .", "entities": []}, {"text": "However , the performance drops in the taxiand hotel domain , signi\ufb01cantly in the taxidomain .", "entities": []}, {"text": "We note that dialogues with the taxidomain is usually not single - domain but typically entangled with other domains .", "entities": []}, {"text": "Secondly , we observe that there is a signi\ufb01cant performance gap of about 10 points absolute score between DST performances in singledomain and multi - domain dialogues .", "entities": []}, {"text": "State tracking in multi - domain dialogues is , hence , could be further improved to boost the overall performance .", "entities": []}, {"text": "Domain Joint Acc Slot Acc Multi - domain 48.40 % 97.14 % Single - domain 59.63 % 98.36 % Attraction 66.76 % 98.94 % Hotel 47.86 % 97.54 % Restaurant 65.11 % 98.68 % Taxi 30.84 % 96.86 % Train 63.77 % 98.53 % Table 7 : DST results on MultiWOZ2.1 by domains .", "entities": [[2, 3, "MetricName", "Acc"], [4, 5, "MetricName", "Acc"]]}, {"text": "Context - to - Text Generation For this task , we calculated the metrics for single - domain dialogues of the corresponding domain ( as Inform and Success are computed per dialogue rather than per turn ) .", "entities": [[4, 6, "TaskName", "Text Generation"]]}, {"text": "We do not report the Inform metric of the taxidomain because no DB was available for this domain .", "entities": []}, {"text": "From Table 8 , we observe some performance gap between Inform andSuccess scores on multi - domain dialogues and single - domain dialogues .", "entities": []}, {"text": "However , in terms of BLEU score , our model performs better with multi - domain dialogues .", "entities": [[5, 7, "MetricName", "BLEU score"]]}, {"text": "This could be caused by the data bias in MultiWOZ corpus as the majority of dialogues in this corpus is multidomain .", "entities": [[9, 10, "DatasetName", "MultiWOZ"]]}, {"text": "Hence , our models capture the semantics of multi - domain dialogue responses better than single - domain responses .", "entities": []}, {"text": "For domain - speci\ufb01c re - sults , we note that our models perform not as well as other domains in attraction andtaxidomains in terms of Success score .", "entities": []}, {"text": "Domain Inform Success", "entities": []}, {"text": "BLEU Multi - domain 85.01 % 68.86 % 18.68 Single - domain 97.79 % 85.84 % 17.62 Attraction 91.67 % 66.67 % 19.17 Hotel 97.01 % 91.04 % 16.55 Restaurant 96.77 % 88.71 % 19.88 Taxi - 78.85 % 13.85 Train 99.10 % 87.88 % 18.14 Table 8 : Context - to - text generation results on MultiWOZ2.1 .", "entities": [[0, 1, "MetricName", "BLEU"], [53, 55, "TaskName", "text generation"]]}, {"text": "by domains .", "entities": []}, {"text": "Additionally , we report qualitative analysis and the insights can be seen in Appendix C. 6 Conclusion We proposed UniConv , a novel uni\ufb01ed neural architecture of conversational agents for Multi - domain Task - oriented Dialogues , which jointly trains ( 1 ) a Bi - level State Tracker to capture dependencies in both domain and slot levels simultaneously , and ( 2 ) a Joint Dialogue Act and Response Generator to model dialogue act latent variable and semantically conditions output responses with contextual cues .", "entities": []}, {"text": "The promising performance of UniConv on the MultiWOZ benchmark ( including three tasks : DST , context - to - text generation , and end - to - end dialogues ) validates the ef\ufb01cacy of our method .", "entities": [[7, 8, "DatasetName", "MultiWOZ"], [20, 22, "TaskName", "text generation"]]}, {"text": "Acknowledgments We thank all reviewers for their insightful feedback to the manuscript of this paper .", "entities": []}, {"text": "The \ufb01rst author of this paper is supported by the Agency for Science , Technology and Research ( A*STAR ) Computing and Information Science scholarship .", "entities": []}, {"text": "1869References Jimmy Lei Ba , Jamie Ryan Kiros , and Geoffrey E Hinton .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Layer normalization .", "entities": [[0, 2, "MethodName", "Layer normalization"]]}, {"text": "arXiv preprint arXiv:1607.06450 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Antoine Bordes , Y - Lan Boureau , and Jason Weston .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Learning end - to - end goal - oriented dialog .", "entities": [[6, 10, "TaskName", "goal - oriented dialog"]]}, {"text": "arXiv preprint arXiv:1605.07683 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Pawe\u0142 Budzianowski and Ivan Vuli \u00b4 c. 2019 .", "entities": []}, {"text": "Hello , it \u2019s GPT-2 - how can I help you ?", "entities": [[4, 5, "MethodName", "GPT-2"]]}, {"text": "towards the use of pretrained language models for task - oriented dialogue systems .", "entities": [[4, 7, "TaskName", "pretrained language models"], [8, 13, "TaskName", "task - oriented dialogue systems"]]}, {"text": "In Proceedings of the 3rd Workshop on Neural Generation and Translation , pages 15\u201322 , Hong Kong .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Pawe\u0142 Budzianowski , Tsung - Hsien Wen , Bo - Hsiang Tseng , I\u00f1igo Casanueva , Ultes Stefan , Ramadan Osman , and Milica Ga\u0161i \u00b4 c. 2018 .", "entities": []}, {"text": "Multiwoz - a largescale multi - domain wizard - of - oz dataset for taskoriented dialogue modelling .", "entities": [[0, 1, "DatasetName", "Multiwoz"], [7, 12, "DatasetName", "wizard - of - oz"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) .", "entities": []}, {"text": "Wenhu Chen , Jianshu Chen , Pengda Qin , Xifeng Yan , and William Yang Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Semantically conditioned dialog response generation via hierarchical disentangled self - attention .", "entities": [[3, 5, "TaskName", "response generation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3696\u20133709 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Mihail Eric , Rahul Goel , Shachi Paul , Abhishek Sethi , Sanchit Agarwal , Shuyag Gao , and Dilek HakkaniTur .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Multiwoz 2.1 : Multi - domain dialogue state corrections and state tracking baselines .", "entities": [[0, 2, "DatasetName", "Multiwoz 2.1"]]}, {"text": "arXiv preprint arXiv:1907.01669 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mihail Eric , Lakshmi Krishnan , Francois Charette , and Christopher D. Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Key - value retrieval networks for task - oriented dialogue .", "entities": []}, {"text": "In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue , pages 37\u201349 , Saarbr\u00fccken , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Mihail Eric and Christopher Manning . 2017 .", "entities": []}, {"text": "A copyaugmented sequence - to - sequence architecture gives good performance on task - oriented dialogue .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers , pages 468\u2013473 , Valencia , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Revanth Gangi Reddy , Danish Contractor , Dinesh Raghu , and Sachindra Joshi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Multi - level memory for task oriented dialogs .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3744\u20133754 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shuyang Gao , Abhishek Sethi , Sanchit Agarwal , Tagyoung Chung , and Dilek Hakkani - Tur . 2019 .", "entities": []}, {"text": "Dialog state tracking : A neural reading comprehension approach .", "entities": [[6, 8, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue , pages 264\u2013273 , Stockholm , Sweden . Association for Computational Linguistics .", "entities": []}, {"text": "Rahul Goel , Shachi Paul , and Dilek Hakkani - T\u00fcr . 2019 .", "entities": []}, {"text": "HyST :", "entities": []}, {"text": "A Hybrid Approach for Flexible and Accurate Dialogue State Tracking .", "entities": [[7, 10, "TaskName", "Dialogue State Tracking"]]}, {"text": "In Proc .", "entities": []}, {"text": "Interspeech 2019 , pages 1458\u20131462 .", "entities": []}, {"text": "Matthew Henderson , Blaise Thomson , and Jason D Williams .", "entities": []}, {"text": "2014a .", "entities": []}, {"text": "The second dialog state tracking challenge .", "entities": []}, {"text": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ( SIGDIAL ) , pages 263\u2013272 .", "entities": []}, {"text": "Matthew Henderson , Blaise Thomson , and Steve Young .", "entities": []}, {"text": "2014b .", "entities": []}, {"text": "Word - based dialog state tracking with recurrent neural networks .", "entities": []}, {"text": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ( SIGDIAL ) , pages 292\u2013299 .", "entities": []}, {"text": "Megha Jhunjhunwala , Caleb Bryant , and Pararth Shah . 2020 .", "entities": []}, {"text": "Multi - action dialog policy learning with interactive human teaching .", "entities": []}, {"text": "In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue , pages 290\u2013296 , 1st virtual meeting .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sungdong Kim , Sohee Yang , Gyuwan Kim , and SangWoo Lee . 2020 .", "entities": []}, {"text": "Ef\ufb01cient dialogue state tracking by selectively overwriting memory .", "entities": [[1, 4, "TaskName", "dialogue state tracking"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 567\u2013582 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Diederick P Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Hung Le , Richard Socher , and Steven C.H. Hoi . 2020 .", "entities": []}, {"text": "Non - autoregressive dialog state tracking .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Hwaran Lee , Jinsik Lee , and Tae - Yoon Kim . 2019 .", "entities": []}, {"text": "SUMBT : Slot - utterance matching for universal and scalable belief tracking .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5478\u20135483 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Wenqiang Lei , Xisen Jin , Min - Yen Kan , Zhaochun Ren , Xiangnan He , and Dawei Yin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Sequicity : Simplifying task - oriented dialogue systems with single sequence - to - sequence architectures .", "entities": [[3, 8, "TaskName", "task - oriented dialogue systems"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1437\u20131447 .", "entities": []}, {"text": "Xiujun Li , Yun - Nung Chen , Lihong Li , Jianfeng Gao , and Asli Celikyilmaz . 2017 .", "entities": []}, {"text": "End - to - end taskcompletion neural dialogue systems .", "entities": []}, {"text": "In Proceedings", "entities": []}, {"text": "1870of the Eighth International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 733\u2013743 , Taipei , Taiwan .", "entities": []}, {"text": "Asian Federation of Natural Language Processing .", "entities": []}, {"text": "Bing Liu and Ian Lane . 2017 .", "entities": []}, {"text": "An end - to - end trainable neural network model with belief tracking for taskoriented dialog .", "entities": []}, {"text": "In Interspeech 2017 .", "entities": []}, {"text": "Chia - Wei Liu , Ryan Lowe , Iulian Serban , Mike Noseworthy , Laurent Charlin , and Joelle Pineau .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "How NOT to evaluate your dialogue system : An empirical study of unsupervised evaluation metrics for dialogue response generation .", "entities": [[17, 19, "TaskName", "response generation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2122\u20132132 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Fei Liu and Julien Perez . 2017 .", "entities": []}, {"text": "Gated end - to - end memory networks .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers , pages 1\u201310 , Valencia , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Andrea Madotto , Chien - Sheng Wu , and Pascale Fung .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Mem2seq : Effectively incorporating knowledge bases into end - to - end task - oriented dialog systems .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1468\u20131478 . Association for Computational Linguistics .", "entities": []}, {"text": "Shikib Mehri , Tejas Srinivasan , and Maxine Eskenazi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Structured fusion networks for dialog .", "entities": []}, {"text": "In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue , pages 165\u2013177 , Stockholm , Sweden .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th annual meeting on association for computational linguistics , pages 311\u2013318 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jiahuan Pei , Pengjie Ren , and Maarten de Rijke .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A modular task - oriented dialogue system using a neural mixture - of - experts .", "entities": []}, {"text": "arXiv preprint arXiv:1907.05346 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Baolin Peng , Xiujun Li , Jianfeng Gao , Jingjing Liu , and Kam - Fai Wong .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep Dyna - Q : Integrating planning for task - completion dialogue policy learning .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2182\u20132192 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Baolin Peng , Xiujun Li , Lihong Li , Jianfeng Gao , Asli Celikyilmaz , Sungjin Lee , and Kam - Fai Wong . 2017 .", "entities": []}, {"text": "Composite task - completion dialogue policy learning via hierarchical deep reinforcement learning .", "entities": []}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2231\u20132240 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Shuke Peng , Xinjing Huang , Zehao Lin , Feng Ji , Haiqing Chen , and Yin Zhang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Teacherstudent framework enhanced multi - domain dialogue generation .", "entities": [[6, 8, "TaskName", "dialogue generation"]]}, {"text": "arXiv preprint arXiv:1908.07137 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Libo Qin , Yijia Liu , Wanxiang Che , Haoyang Wen , Yangming Li , and Ting Liu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Entity - consistent end - to - end task - oriented dialogue system with KB retriever .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 133\u2013142 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Jeff Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Osman Ramadan , Pawe\u0142 Budzianowski , and Milica Gasic .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Large - scale multi - domain belief tracking with knowledge sharing .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , volume 2 , pages 432\u2013437 .", "entities": []}, {"text": "Liliang Ren , Jianmo Ni , and Julian McAuley .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Scalable and accurate dialogue state tracking via hierarchical sequence generation .", "entities": [[3, 6, "TaskName", "dialogue state tracking"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 1876\u20131885 , Hong Kong , China . Association for Computational Linguistics .", "entities": []}, {"text": "Iulian V Serban , Alessandro Sordoni , Yoshua Bengio , Aaron Courville , and Joelle Pineau .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Building end - to - end dialogue systems using generative hierarchical neural network models .", "entities": []}, {"text": "In Thirtieth AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Lei Shu , Piero Molino , Mahdi Namazifar , Hu Xu , Bing Liu , Huaixiu Zheng , and Gokhan Tur . 2019 .", "entities": []}, {"text": "Flexibly - structured model for task - oriented dialogues .", "entities": []}, {"text": "In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue , pages 178\u2013187 , Stockholm , Sweden .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nitish Srivastava , Geoffrey Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov .", "entities": [[13, 14, "DatasetName", "Ruslan"]]}, {"text": "2014 .", "entities": []}, {"text": "Dropout : a simple way to prevent neural networks from over\ufb01tting .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "The Journal of Machine Learning Research , 15(1):1929\u20131958 .", "entities": []}, {"text": "Shang - Yu Su , Kai - Ling Lo , Yi - Ting Yeh , and YunNung Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Natural language generation by hierarchical decoding with linguistic patterns .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 61\u201366 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "1871Ilya Sutskever , Oriol Vinyals , and Quoc V Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "In Z. Ghahramani , M. Welling , C. Cortes , N. D. Lawrence , and K. Q. Weinberger , editors , Advances in Neural Information Processing Systems 27 , pages 3104\u20133112 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Christian Szegedy , Vincent Vanhoucke , Sergey Ioffe , Jon Shlens , and Zbigniew Wojna .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Rethinking the inception architecture for computer vision .", "entities": []}, {"text": "In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2818\u20132826 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In I. Guyon , U. V .", "entities": []}, {"text": "Luxburg , S. Bengio , H. Wallach , R. Fergus , S. Vishwanathan , and R. Garnett , editors , Advances in Neural Information Processing Systems 30 , pages 5998\u20136008 .", "entities": []}, {"text": "Curran Associates , Inc. Tsung - Hsien Wen , Milica Gasic , Nikola Mrk\u0161i \u00b4 c , Lina M. Rojas Barahona , Pei - Hao Su , Stefan Ultes , David Vandyke , and Steve Young .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Conditional generation and snapshot learning in neural dialogue systems .", "entities": []}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2153\u20132162 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tsung - Hsien Wen , Milica Ga\u0161i \u00b4 c , Nikola Mrk\u0161i \u00b4 c , PeiHao Su , David Vandyke , and Steve Young .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Semantically conditioned LSTM - based natural language generation for spoken dialogue systems .", "entities": [[2, 3, "MethodName", "LSTM"], [9, 12, "TaskName", "spoken dialogue systems"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1711\u20131721 , Lisbon , Portugal . Association for Computational Linguistics .", "entities": []}, {"text": "Tsung - Hsien Wen , David Vandyke , Nikola Mrk\u0161i \u00b4 c , Milica Gasic , Lina M. Rojas Barahona , Pei - Hao Su , Stefan Ultes , and Steve Young .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A networkbased end - to - end trainable task - oriented dialogue system .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers , pages 438\u2013449 , Valencia , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jason D. Williams , Kavosh Asadi , and Geoffrey Zweig .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Hybrid code networks : practical and ef\ufb01cient end - to - end dialog control with supervised and reinforcement learning .", "entities": []}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 665 \u2013 677 , Vancouver , Canada . Association for Computational Linguistics .", "entities": []}, {"text": "Chien - Sheng Wu , Andrea Madotto , Ehsan HosseiniAsl , Caiming Xiong , Richard Socher , and Pascale Fung .", "entities": []}, {"text": "2019a .", "entities": []}, {"text": "Transferable multi - domain state generator for task - oriented dialogue systems .", "entities": [[7, 12, "TaskName", "task - oriented dialogue systems"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 808\u2013819,Florence , Italy .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chien - Sheng Wu , Richard Socher , and Caiming Xiong .", "entities": []}, {"text": "2019b .", "entities": []}, {"text": "Global - to - local memory pointer networks for task - oriented dialogue .", "entities": []}, {"text": "In Proceedings of the International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Semih Yavuz , Abhinav Rastogi , Guan - Lin Chao , and Dilek Hakkani - Tur . 2019 .", "entities": []}, {"text": "DeepCopy :", "entities": []}, {"text": "Grounded response generation with hierarchical pointer networks .", "entities": [[1, 3, "TaskName", "response generation"]]}, {"text": "In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue , pages 122\u2013132 , Stockholm , Sweden .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yichi Zhang , Zhijian Ou , and Zhou Yu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Taskoriented dialog systems that consider multiple appropriate responses under the same context .", "entities": []}, {"text": "arXiv preprint arXiv:1911.10484 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tiancheng Zhao , Allen Lu , Kyusong Lee , and Maxine Eskenazi . 2017 .", "entities": []}, {"text": "Generative encoder - decoder models for task - oriented spoken dialog systems with chatting capability .", "entities": []}, {"text": "In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue , pages 27\u201336 , Saarbr\u00fccken , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tiancheng Zhao , Kaige Xie , and Maxine Eskenazi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Rethinking action spaces for reinforcement learning in end - to - end dialog agents with latent variable models .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1208\u20131218 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Victor Zhong , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Global - locally self - attentive encoder for dialogue state tracking .", "entities": [[8, 11, "TaskName", "dialogue state tracking"]]}, {"text": "In ACL .", "entities": []}, {"text": "Li Zhou and Kevin Small .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Multi - domain dialogue state tracking as dynamic knowledge graph enhanced question answering .", "entities": [[0, 6, "TaskName", "Multi - domain dialogue state tracking"], [11, 13, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1911.06192 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "1872A Data Pre - processing First , we delexicalize each target system response sequence by replacing the matched entity attribute that appears in the sequence to the canonical tag hdomain _ sloti .", "entities": []}, {"text": "For example , the original target response \u2018 the train i d is tr8259 departing from cambridge \u2019 is delexicalized into \u2018 the train i d is train_id departing from train_departure \u2019 .", "entities": []}, {"text": "We use the provided entity databases ( DBs ) to match potential attributes in all target system responses .", "entities": []}, {"text": "To construct dialogue history , we keep the original version of all text , including system responses of previous turns , rather than the delexicalized form .", "entities": []}, {"text": "We split all sequences of dialogue history , user utterances of the current turn , dialogue states , and delexicalized target responses , into case - insensitive tokens .", "entities": []}, {"text": "We share the embedding weights of all source sequences , including dialogue history , user utterance , and dialogue states , but use a separate embedding matrix to encode the target system responses .", "entities": []}, {"text": "We summarize the number of dialogues in each domain in Table 2 .", "entities": []}, {"text": "For each domain , a dialogue is selected as long as the whole dialogue ( i.e. singledomain dialogue ) or parts of the dialogue ( i.e. in multi - domain dialogue ) is involved with the domain .", "entities": []}, {"text": "For each domain , we also build a set of possible inform andrequest slots using the dialogue state annotation in the training data .", "entities": []}, {"text": "The details of slots and database in each domain can be seen in Table 9 .", "entities": []}, {"text": "The DBs of 3 domains taxi , police , and hospital are not available as part of the benchmark .", "entities": [[17, 19, "DatasetName", "the benchmark"]]}, {"text": "On average , each dialogue has 1.8 domains and extends over 13 turns .", "entities": []}, {"text": "B Baselines We describe our baseline models in DST , contextto - text generation , and end - to - end dialogue tasks .", "entities": [[12, 14, "TaskName", "text generation"]]}, {"text": "B.1 DST FJST andHJST ( Eric et al . , 2019 ) .", "entities": []}, {"text": "These models adopt a \ufb01xed - vocabulary DST approach .", "entities": []}, {"text": "Both models include encoder modules ( either bidirectional LSTM or hierarchical LSTM ) to encode the dialogue history .", "entities": [[7, 9, "MethodName", "bidirectional LSTM"], [11, 12, "MethodName", "LSTM"]]}, {"text": "The models pass the context hidden states to separate linear transformation to obtain \ufb01nal vectors to predict individual slots separately .", "entities": []}, {"text": "The output vector is used to measure a score of each candidate from a prede\ufb01ned candidate set .", "entities": []}, {"text": "DST Reader ( Gao et al . , 2019 ) .", "entities": []}, {"text": "This model considers the DST task as a reading comprehension task and predicts each slot as a span over tokens withindialogue history .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}, {"text": "DST Reader utilizes attentionbased neural networks with additional modules to predict slot type and carryover probability .", "entities": []}, {"text": "TSCP ( Lei et al . , 2018 ) .", "entities": []}, {"text": "The model adopts a sequence - to - sequence framework with a pointer network to generate dialogue states .", "entities": [[12, 14, "MethodName", "pointer network"]]}, {"text": "The source sequence is a combination of the last user utterance , dialogue state of the previous turn , and user utterance .", "entities": []}, {"text": "To compare with TSCP in a multi - domain task - oriented dialogue setting , we adapt the model to multi - domain dialogues by formulating the dialogue state of the previous turn similarly as our models .", "entities": []}, {"text": "We reported the performance when the maximum length of the output dialogue state sequence Lis set to 20 tokens ( original default parameter is 8 tokens but we expect longer dialogue state in MultiWOZ benchmark and selected 20 tokens ) .", "entities": [[33, 34, "DatasetName", "MultiWOZ"]]}, {"text": "HyST ( Goel et al . , 2019 ) .", "entities": []}, {"text": "This model combines the advantage of \ufb01xed - vocabulary and openvocabulary approaches .", "entities": []}, {"text": "The model uses an openvocabulary approach in which the set of candidates of each slot is constructed based on all word ngrams in the dialogue history .", "entities": []}, {"text": "Both approaches are applied in all slots and depending on their performance in the validation set , the better approach is used to predict individual slots during test time .", "entities": []}, {"text": "TRADE ( Wu et al . , 2019a ) .", "entities": []}, {"text": "The model adopts a sequence - to - sequence framework with a pointer network to generate individual slot token - by - token .", "entities": [[12, 14, "MethodName", "pointer network"]]}, {"text": "The prediction is additionally supported by a slot gating component that decides whether the slot is \u201c none \" , \u201c dontcare \" , or \u201c generate \" .", "entities": []}, {"text": "When the gate of a slot is predicted as \u201c generate \" , the model will generate value as a natural output sequence for that slot .", "entities": []}, {"text": "NADST ( Le et al . , 2020 ) .", "entities": []}, {"text": "The model proposes a non - autoregressive approach for dialogue state tracking which enables learning dependencies between domain - level and slot - level representations as well as token - level representations of slot values .", "entities": [[9, 12, "TaskName", "dialogue state tracking"]]}, {"text": "DSTQA ( Zhou and Small , 2019 ) .", "entities": []}, {"text": "The model treats dialogue state tracking as a question answering problem in which state values can be predicted through lexical spans or unique generated values .", "entities": [[3, 6, "TaskName", "dialogue state tracking"], [8, 10, "TaskName", "question answering"]]}, {"text": "It is enhanced with a knowledge graph where each node represent a slot and edges are based on overlaps of their value sets .", "entities": []}, {"text": "SOM - DST ( Kim et al . , 2020 ) .", "entities": [[0, 1, "MethodName", "SOM"]]}, {"text": "This is the current state - of - the - art model on the MultiWOZ2.1 dataset .", "entities": []}, {"text": "The model exploits a selectively overwriting mechanism on a \ufb01xed - sized memory of dialogue states .", "entities": []}, {"text": "1873Domain Slots # entities DB attributes Restaurant inf_area , inf_food , inf_name , inf_pricerange , inf_bookday , inf_bookpeople , inf_booktime , req_address , req_area , req_food , req_phone , req_postcode110 i d , address , area , food , introduction , name , phone , postcode , pricerange , signature , type Hotel inf_area , inf_internet , inf_name , inf_parking , inf_pricerange , inf_stars , inf_type , inf_bookday , inf_bookpeople , inf_bookstay , req_address , req_area , req_internet , req_parking , req_phone , req_postcode , req_stars , req_type33 i d , address , area , internet , parking , single , double , family , name , phone , postcode , pricerange \u2019 , takesbookings , stars , type Attraction inf_area , inf_name , inf_type , req_address , req_area , req_phone , req_postcode , req_type79 i d , address , area , entrance , name , phone , postcode , pricerange , openhours , type Train inf_arriveBy , inform_day , inf_departure , inf_destination , inf_leaveAt , inf_bookpeople , req_duration , req_price2,828 trainID , arriveBy , day , departure , destination , duration , leaveAt , price Taxi inf_arriveBy , inf_departure , inf_destination , inf_leaveAt , req_phone- Police inf_department , req_address , req_phone , req_postcode- Hospital req_address , req_phone , req_postcode - Table 9 : Summary of slots and DB details by domain in the MultiWOZ dataset ( Budzianowski et al . , 2018 )", "entities": [[224, 225, "DatasetName", "MultiWOZ"]]}, {"text": "At each dialogue turn , the mechanism involve decision making on whether to update or carryover the state values from previous turns .", "entities": [[8, 10, "TaskName", "decision making"]]}, {"text": "B.2 Context - to - Text Generation Baseline .", "entities": [[5, 7, "TaskName", "Text Generation"]]}, {"text": "( Budzianowski et al . , 2018 ) provides a baseline for this setting by following the sequenceto - sequence model ( Sutskever et al . , 2014 ) .", "entities": []}, {"text": "The source sequence is all past dialogue turns and the target sequence is the system response .", "entities": []}, {"text": "The initial hidden state of the RNN decoder is incorporated with additional signals from the dialogue states and database representations .", "entities": []}, {"text": "TokenMoE ( Pei et al . , 2019 ) .", "entities": []}, {"text": "TokenMoE refers to Token - level Mixture - of - Expert model .", "entities": []}, {"text": "The model follows a modularized approach by separating different components known as expert bots for different dialogue scenarios .", "entities": []}, {"text": "A dialogue scenario can be dependent on a domain , a type of dialogue act , etc .", "entities": []}, {"text": "A chair bot is responsible for controlling expert bots to dynamically generate dialogue responses .", "entities": []}, {"text": "HDSA ( Chen et al . , 2019 ) .", "entities": []}, {"text": "This is the current stateof - the - art in terms of Inform and BLEU score in the context - to - text generation setting in MultiWOZ2.0 .", "entities": [[14, 16, "MetricName", "BLEU score"], [22, 24, "TaskName", "text generation"]]}, {"text": "HDSA leverages the structure of dialogue acts to build a multi - layer hierarchical graph .", "entities": []}, {"text": "The graph is incorporated as an inductive bias in a self - attention network to improve the semantic quality of generated dialogue responses .", "entities": [[10, 14, "MethodName", "self - attention network"]]}, {"text": "Structured Fusion ( Mehri et al . , 2019 ) .", "entities": []}, {"text": "This approach follows a traditional modularized dialogue system architecture , including separate components for NLU , DM , and NLG .", "entities": []}, {"text": "These compo - nents are pre", "entities": []}, {"text": "- trained and combined into an end - toend system .", "entities": []}, {"text": "Each component output is used as a structured input to other components .", "entities": []}, {"text": "LaRL ( Zhao et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "This model uses a latent dialogue action framework instead of handcrafted dialogue acts .", "entities": []}, {"text": "The latent variables are learned using unsupervised learning with stochastic variational inference .", "entities": [[10, 12, "MethodName", "variational inference"]]}, {"text": "The model is trained in a reinforcement learning framework whereby the parameters are trained to yield a better Success rate .", "entities": []}, {"text": "The model is the current state - of - the - art in terms of Success metric .", "entities": []}, {"text": "GPT2 ( Budzianowski and Vuli \u00b4 c , 2019 ) .", "entities": []}, {"text": "Unsupervised pre - training language models have signi\ufb01cantly improved machine learning performance in many NLP tasks .", "entities": [[0, 4, "TaskName", "Unsupervised pre - training"]]}, {"text": "This baseline model leverages the power of a pre - trained model ( Radford et al . , 2019 ) and adapts to the context - to - text generation setting in task - oriented dialogues .", "entities": [[28, 30, "TaskName", "text generation"]]}, {"text": "All input components , including dialogue state and database state , are transformed into raw text format and concatenated as a single sequence .", "entities": []}, {"text": "The sequence is used as input to a pre - trained GPT-2 model which is then \ufb01ne - tuned with MultiWOZ data .", "entities": [[11, 12, "MethodName", "GPT-2"], [20, 21, "DatasetName", "MultiWOZ"]]}, {"text": "DAMD ( Zhang et al . , 2019 ) .", "entities": []}, {"text": "This is the current state - of - the - art model for context - to - text generation task in MultiWOZ 2.1 .", "entities": [[17, 19, "TaskName", "text generation"], [21, 23, "DatasetName", "MultiWOZ 2.1"]]}, {"text": "This approach augments training data with multiple responses of similar context .", "entities": []}, {"text": "Each dialogue state is mapped to multiple valid dialogue acts to create additional state - act pairs .", "entities": []}, {"text": "1874B.3 End - to - End TSCP ( Lei et al . , 2018 ) .", "entities": []}, {"text": "In addition to the DST task , we evaluate TSCP as an end - to - end dialogue system that can do both DST and NLG .", "entities": []}, {"text": "We adapt the models to the multi - domain DST setting as described in Section B.1 and keep the original response decoder .", "entities": []}, {"text": "Similar to the DST component , the response generator of TSCP also adopts a pointer network to generate tokens of the target system responses by copying tokens from source sequences .", "entities": [[14, 16, "MethodName", "pointer network"]]}, {"text": "In this setting , we test TSCP with two settings of the maximum length of the output dialogue state sequence : L= 8andL= 20 .", "entities": []}, {"text": "HRED - TS ( Peng et", "entities": [[2, 3, "MethodName", "TS"]]}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "This model adopts a teacher - student framework to address multidomain task - oriented dialogues .", "entities": []}, {"text": "Multiple teacher networks are trained for different domains and intermediate representations of dialogue acts and output responses are used to guide a universal student network .", "entities": []}, {"text": "The student network uses these representations to directly generate responses from dialogue context without predicting dialogue states .", "entities": []}, {"text": "C Qualitative Analysis We examine an example of dialogue in the test data and compare our predicted outputs with the baseline TSCP ( L= 20 ) ( Lei et al . , 2018 ) and the ground truth .", "entities": []}, {"text": "From Figure 4 , we observe that both our predicted dialogue state and system response are more correct than the baseline .", "entities": []}, {"text": "Speci\ufb01cally , our dialogue state can detect the correct type slot in theattraction domain .", "entities": []}, {"text": "As our dialogue state is correctly predicted , the queried results from DB is also more correct , resulting in better response with the right information ( i.e. \u2018 no attraction available \u2019 ) .", "entities": []}, {"text": "In Figure 5 , we show the visualization of domain - level and slot - level attention on the user utterance .", "entities": []}, {"text": "We notice important tokens of the text sequences , i.e. \u2018 entertainment \u2019 and \u2018 close to \u2019 , are attended with higher attention scores .", "entities": []}, {"text": "Besides , at domain - level attention , we \ufb01nd a potential additional signal from the token \u2018 restaurant \u2019 , which is also the domain from the previous dialogue turn .", "entities": []}, {"text": "We also observe that attention is more re\ufb01ned throughout the neural network layers .", "entities": []}, {"text": "For example , in the domain - level processing , compared to the 2ndlayer , the 4thlayer attention is more clustered around speci\ufb01c tokens of the user utterance .", "entities": []}, {"text": "In Table 10 and 11 , we reported the complete output of this example dialogue .", "entities": []}, {"text": "Overall , our dialogue agent can carry a proper dialogue with theuser throughout the dialogue steps .", "entities": [[4, 5, "DatasetName", "agent"]]}, {"text": "Speci\ufb01cally , we observed that our model can detect new domains at dialogue steps where the domains are introduced e.g.attraction domain at the 5thturn and taxidomain at the 8thturn .", "entities": []}, {"text": "The dialogue agent can also detect some of the co - references among the domains .", "entities": [[2, 3, "DatasetName", "agent"]]}, {"text": "For example , at the 5thturn , the dialogue agent can infer the slot area for the new domain attraction as the user mentioned \u2018 close the restaurant \u2019 .", "entities": [[9, 10, "DatasetName", "agent"]]}, {"text": "We noticed that that at later dialogue steps such as the 6thturn , our decoded dialogue state is not correct possibly due to the incorrect decoded dialogue state in the previous turn , i.e. 5thturn .", "entities": []}, {"text": "In Figure 2 and 3 , we plotted the Joint Goal Accuracy and BLEU metrics of our model by dialogue turn .", "entities": [[11, 12, "MetricName", "Accuracy"], [13, 14, "MetricName", "BLEU"]]}, {"text": "As we expected , the Joint Accuracy metric tends to decrease as the dialogue history extends over time .", "entities": [[6, 7, "MetricName", "Accuracy"]]}, {"text": "The dialogue agent achieves the highest accuracy in state tracking at the 1stturn and gradually reduces to zero accuracy at later dialogue steps , i.e. 15thto18thturns .", "entities": [[2, 3, "DatasetName", "agent"], [6, 7, "MetricName", "accuracy"], [18, 19, "MetricName", "accuracy"]]}, {"text": "For response generation performance , the trend of BLEU score is less obvious .", "entities": [[1, 3, "TaskName", "response generation"], [8, 10, "MetricName", "BLEU score"]]}, {"text": "The dialogue agent obtains the highest BLEU scores at the 3rdturn and \ufb02uctuates between the2ndand13thturn .", "entities": [[2, 3, "DatasetName", "agent"], [6, 7, "MetricName", "BLEU"]]}, {"text": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Dialog turn00.20.40.60.81Goals Figure 2 : Joint Accuracy metric by dialogue turn in the test data .", "entities": [[24, 25, "MetricName", "Accuracy"]]}, {"text": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Dialog turn-0.0500.050.10.150.20.25BLEU Figure 3 : BLEU4 metric by dialogue turn in the test data .", "entities": []}, {"text": "1875 R4 : all set .", "entities": []}, {"text": "your reference number is k2bo09vq .", "entities": []}, {"text": "U5 : thanks .", "entities": []}, {"text": "i am also looking for some entertainment   close   to   the restaurant   .", "entities": []}, {"text": "any suggestions ?", "entities": []}, {"text": "B5 : { restaurant : { area : center , name : dojo noodle bar ,   pricerange : cheap } , attraction : { area : center , type :   entertainment } }   R5 : i am sorry i do not have   any attractions meeting the   criteria you listed .", "entities": []}, {"text": "is there another type of attraction or area   you would like me to search ?   B5tscp : { restaurant : { area : center , pricerange : cheap } ,   attraction : { area : center } }   R5tscp : there are many   attractions in the restaurant_area .", "entities": []}, {"text": "do   you have a preference ?", "entities": []}, {"text": "B5ours : { restaurant : { area : center , pricerange : cheap } ,   attraction : { area : center , type : entertainment } } R5ours : there are no attraction_type attractions in the   restaurant_area of town .", "entities": []}, {"text": "would you like to try another area ?", "entities": []}, {"text": "Figure 4 : Example dialogue with the input system response Rt\u00001and current user utterance Ut , and the output stateBS tand system response Rt .", "entities": []}, {"text": "Compared with TSCP , our dialogue state and response are more correct and closer to the ground truth .", "entities": []}, {"text": "2nd", "entities": []}, {"text": "Layer    4th", "entities": []}, {"text": "Layer 3rd Layer   Domain   -level   DST Slot -level   DST Figure 5 : Visualization of attention to the user utterance sequence at slot - level ( lower right ) and domain - level ( upper right ) is also included .", "entities": []}, {"text": "More red denotes higher attention score between domain or slot representation and token representation .", "entities": []}, {"text": "Best viewed in color .", "entities": []}, {"text": "1876Turn Input Dialogue Ground - truth State and Response Our Predicted State and Response R0 : < blank > B1 : { restaurant : { area : centre , pricerange : cheap}}B1 : { restaurant : { area : centre , pricerange : cheap } } 1 U1 :", "entities": []}, {"text": "i am looking for a cheap restaurant in the center of the cityR1 : do you have any speci\ufb01c type of food you would like ?", "entities": []}, {"text": "R1 : there are 15 restaurant_pricerange restaurant_type s in the restaurant_area of town .", "entities": []}, {"text": "do you have a preference ?", "entities": []}, {"text": "U1 : do you have any speci\ufb01c type of food you would like ? B2 : { restaurant : { area : centre , pricerange : cheap}}B2 : { restaurant : { area : centre , pricerange : cheap } } 2 U2 : no , i am not picky as long as the price -s are low .R2 : there is a restaurant_pricerange restaurant_food restaurant_type called the restaurant_name located in the restaurant_area of town .", "entities": []}, {"text": "would you like to book a table ?", "entities": []}, {"text": "R2 : how about restaurant_name ?", "entities": []}, {"text": "it s an restaurant_pricerange restaurant_food restaurant_type located in the restaurant_area of town .", "entities": []}, {"text": "R2 : there is a cheap chinese restaurant called the dojo noodle bar located in the centre of town .", "entities": []}, {"text": "would you like to book a table ? B3 : { restaurant : { area : centre , pricerange : cheap , name : dojo noodle bar}}B2 : { restaurant : { area : centre , pricerange : cheap } } 3 U3 : yes please , for 8 people at 1830 on thursday .R3 :", "entities": []}, {"text": "i am sorry but restaurant_name is solidly booked at that time .", "entities": []}, {"text": "i can try a different time or day for you .R3 : booking was successful .", "entities": []}, {"text": "the table will be reserved for 15 minutes .", "entities": []}, {"text": "reference number is restaurant_reference .", "entities": []}, {"text": "R3 : i am sorry but dojo noodle bar is solidly booked at that time .", "entities": []}, {"text": "i can try a different time or day for youB4 : { restaurant : { area : centre , pricerange : cheap , name : dojo noodle bar}}B4 : { restaurant : { area : centre , pricerange : cheap } } 4 U4 : can you try to book it at 1730 .R4 : all set .", "entities": []}, {"text": "your reference number is restaurant_reference .R4 : booking was successful .", "entities": []}, {"text": "the table will be reserved for 15 minutes .", "entities": []}, {"text": "reference number is restaurant_reference .", "entities": []}, {"text": "R4 : all set .", "entities": []}, {"text": "your reference number is k2bo09vq .B5 : { restaurant : { area : centre , pricerange : cheap , name : dojo noodle bar } , attraction : { area : centre , type : entertainment}}B5 : { restaurant : { area : centre , pricerange : cheap } , attraction : { area : centre , type : entertainment } } 5 U5 : thanks .", "entities": []}, {"text": "i am also looking for some entertainment close to the restaurant .", "entities": []}, {"text": "any suggestions ?", "entities": []}, {"text": "R5 : i am sorry i do not have any attractions meeting the criteria you listed .", "entities": []}, {"text": "is there another type of attraction or area you would like me to search ?", "entities": []}, {"text": "R5 : there are no attraction_type attractions in the restaurant_area of town .", "entities": []}, {"text": "would you like to try another area ?", "entities": []}, {"text": "R5 : i am sorry i do not have any attractions meeting the criteria you listed .", "entities": []}, {"text": "is there another type of attraction or area you would like me to search ? B6:{restaurant : { area : centre , pricerange : cheap , name : dojo noodle bar } , attraction : { area : centre , type : architecture}}B6:{restaurant : { area : centre , pricerange : cheap } , attraction : { area : centre , type : architecture } } 6 U6 : why do not you try an architectural attraction , instead .R6 : attraction_name looks good , would you like to head there ?", "entities": []}, {"text": "R6 : attraction_name is a great attraction_type in the center of town .", "entities": []}, {"text": "would you like more information ?", "entities": []}, {"text": "R6 : all saints church looks good , would you like to head there ? B7:{restaurant : { area : centre , pricerange : cheap , name : dojo noodle bar } , attraction : { area : centre , type : architecture}}B7:{restaurant : { area : centre , pricerange : cheap } , attraction : { area : centre , type : architecture } } 7 U7 : yes please .", "entities": []}, {"text": "can you let me know what the entrance fee is too ?", "entities": []}, {"text": "R7 : its entrance fee is attraction_pricerange .R7 : the entrance fee is attraction_pricerange .", "entities": []}, {"text": "is there anything else i can help you with ?", "entities": []}, {"text": "R7 : its entrance fee is free .", "entities": []}, {"text": "B8:{restaurant : { area : centre , pricerange : cheap , name : dojo noodle bar } , attraction : { area : centre , type : architecture } , taxi : { departure : dojo noodle bar , destination : all saints church}}B8:{restaurant : { area : centre , pricerange : cheap } , attraction : { area : centre , type : architecture } , taxi : { destination : dojo noodle bar } } 8 U8 : i also need to book a taxi between the restaurant and the church .R8 : what time would you like the taxi to pick you up from restaurant_name ?", "entities": []}, {"text": "R8 : what time would you like to leave or arrive by ?", "entities": []}, {"text": "Table 10 : Complete turns of an example multi - domain dialogue with each turn including the input of past system response Rt\u00001and current user utterance Ut , and the predicted dialogue state Btand system response Ut .", "entities": []}, {"text": "The dialogue consists of 11 turns in total and extends across 3 domains sequentially : restaurant , attraction , and taxi .", "entities": []}, {"text": "Dialogue continues in Table 11 .", "entities": []}, {"text": "1877Turn Input Dialogue Ground - truth State and Response Our Predicted State and Response R8 : what time would you like the taxi to pick you up from dojo noodle bar ?", "entities": []}, {"text": "B9 : { restaurant : { area : centre , pricerange : cheap , name : dojo noodle bar } , attraction : { area : centre , type : architecture } , taxi : { departure : dojo noodle bar , destination : all saints church , leaveAt : 1830}}B9:{restaurant : { area : centre , pricerange : cheap } , attraction : { area : centre , type : architecture } , taxi : { destination : dojo noodle bar } } 9 U9 : 1830 , please .", "entities": []}, {"text": "R9 : your taxi is booked .", "entities": []}, {"text": "the car is a taxi_type and the contact number is taxi_phone .", "entities": []}, {"text": "is there anything else i can help you with ? R9 : booking completed !", "entities": []}, {"text": "booked car type taxi_type contact number taxi_phone can i help you with anything else ?", "entities": []}, {"text": "R9 : your taxi is booked .", "entities": []}, {"text": "the car is a blue bmw and the contact number is 07239644669 .", "entities": []}, {"text": "is there anything else i can help you with ? B10 : { restaurant : { area : centre , pricerange : cheap , name : dojo noodle bar } , attraction : { area : centre , type : architecture } , taxi : { departure : dojo noodle bar , destination : all saints church , leaveAt : 1830}}B10 : { restaurant : { area : centre , pricerange : cheap } , attraction : { area : centre , type : architecture } , taxi : { destination : dojo noodle bar } } 10 U10 : that s it .", "entities": []}, {"text": "thanks .", "entities": []}, {"text": "R10 : awesome , enjoy your trip !", "entities": []}, {"text": "R10 : you are welcome .", "entities": []}, {"text": "have a great day !", "entities": []}, {"text": "R10 : awesome , enjoy your trip !", "entities": []}, {"text": "B11 : { restaurant : { area : centre , pricerange : cheap , name : dojo noodle bar } , attraction : { area : centre , type : architecture } , taxi : { departure : dojo noodle bar , destination : all saints church , leaveAt : 1830}}B11 : { restaurant : { area : centre , pricerange : cheap } , attraction : { area : centre , type : architecture } , taxi : { destination : dojo noodle bar } } 11 U11 : thanks !", "entities": []}, {"text": "i will de\ufb01nitely try to .R11 :", "entities": []}, {"text": "great !", "entities": []}, {"text": "goodbye and have a nice day .R11 : thank you for using our services .", "entities": []}, {"text": "have a great day .", "entities": []}, {"text": "Table 11 : Dialogue continues from Table 10 .", "entities": []}, {"text": "Each turn includes the input of past system response St\u00001and current user utterance Ut , and the predicted dialogue dialogue state BS tand system response St. The dialogue consists of 11 turns in total and extends across 3 domains sequentially : restaurant , attraction , and taxi .", "entities": []}]