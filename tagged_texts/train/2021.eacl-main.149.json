[{"text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics , pages 1740\u20131747 April 19 - 23 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics1740BERT Prescriptions to Avoid Unwanted Headaches : A Comparison of Transformer Architectures for Adverse Drug Event Detection Beatrice Portelli1Edoardo Lenzi1Emmanuele", "entities": [[15, 16, "MethodName", "Transformer"], [20, 22, "TaskName", "Event Detection"]]}, {"text": "Chersoni2 Giuseppe Serra1Enrico Santus3 1AILAB UniUd - University of Udine , Italy 2The Hong Kong Polytechnic University 3Decision Science and Advanced Analytics for MAPV & RA , Bayer fportelli.beatrice , lenzi.edoardo g@spes.uniud.it", "entities": []}, {"text": "emmanuele.chersoni@polyu.edu.hk giuseppe.serra@uniud.it , enrico.santus@bayer.com", "entities": []}, {"text": "Abstract Pretrained transformer - based models , such as BERT and its variants , have become a common choice to obtain state - of - the - art performances in NLP tasks .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "In the identi\ufb01cation of Adverse Drug Events ( ADE ) from social media texts , for example , BERT architectures rank \ufb01rst in the leaderboard .", "entities": [[18, 19, "MethodName", "BERT"]]}, {"text": "However , a systematic comparison between these models has not yet been done .", "entities": []}, {"text": "In this paper , we aim at shedding light on the differences between their performance analyzing the results of 12 models , tested on two standard benchmarks .", "entities": []}, {"text": "SpanBERT and PubMedBERT emerged as the best models in our evaluation : this result clearly shows that span - based pretraining gives a decisive advantage in the precise recognition of ADEs , and that in - domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch .", "entities": []}, {"text": "1 Introduction The identi\ufb01cation of Adverse Drug Events ( ADEs ) from text recently attracted a lot of attention in the NLP community .", "entities": []}, {"text": "On the one hand , it represents a challenge even for the most advanced NLP technologies , since mentions of ADEs can be found in different varieties of online text and present unconventional linguistic features ( they may involve specialized language , or consist of discontinuous spans of tokens etc . )", "entities": []}, {"text": "( Dai , 2018 ) .", "entities": []}, {"text": "On the other hand , the task has an industrial application of primary importance in the \ufb01eld of digital pharmacovigilance ( Sarker et al . , 2015 ; Karimi et al . , 2015b ) .", "entities": []}, {"text": "This raising interest is attested , for example , by the ACL workshop series on Social Media Health Mining ( SMM4H ) , in which shared tasks on ADE detection have been regularly organized since 2016 ( Paul et al . , 2016 ; Sarker and Gonzalez - Hernandez,2017 ;", "entities": [[20, 21, "DatasetName", "SMM4H"]]}, {"text": "Weissenbacher et al . , 2018 , 2019 ) .", "entities": []}, {"text": "With the recent introduction of Transformers architectures and their impressive achievements in NLP ( Vaswani et al . , 2017 ; Devlin et al . , 2019 ) , it is not surprising that these tools have become a common choice for the researchers working in the area .", "entities": []}, {"text": "The contribution of this paper is a comparison between different Transformers on ADE detection , in order to understand which one is the most appropriate for tackling the task .", "entities": []}, {"text": "Shared tasks are not the best scenario for addressing this question , since the wide range of differences in the architectures ( which could include , for example , ensembles of Transformers and other types of networks ) does not allow a comparison on the same grounds .", "entities": []}, {"text": "In our view , two key questions deserve a particular attention in this evaluation .", "entities": []}, {"text": "First , whether there is an advantage in using a model with some form ofin - domain language pretraining , given the wide availability of Transformers for the biomedical domain ( Lee et al . , 2020 ; Gu et al . , 2020 ) .", "entities": []}, {"text": "Second , whether a model trained to predict coherent spans of text instead of single words can achieve a better performance ( Joshi et al . , 2019 ) , since our goal is to identify the groups of tokens corresponding to ADEs as precisely as possible .", "entities": []}, {"text": "Two models that we introduce for the \ufb01rst time in this task , SpanBERT and PubMedBERT , achieved the top performance .", "entities": []}, {"text": "The former takes advantage of a span - based pretraining objective , while the latter shows that in - domain language data are better used for training the model from scratch , without any general - domain pretraining . 2 Related Work 2.1 ADE Detection Automatic extraction of ADE in social media started receiving more attention in the last few", "entities": []}, {"text": "1741years , given the increasing number of users that discuss their drug - related experiences on Twitter and similar platforms .", "entities": []}, {"text": "Studies like Sarker and Gonzalez ( 2015 ) ; Nikfarjam et al .", "entities": []}, {"text": "( 2015 ) ; Daniulaityte et al .", "entities": []}, {"text": "( 2016 ) were among the \ufb01rst to propose machine learning systems for the detection of ADE in social media texts , using traditional feature engineering and word embeddings - based approaches .", "entities": [[24, 26, "TaskName", "feature engineering"], [27, 29, "TaskName", "word embeddings"]]}, {"text": "With the introduction of the SMM4H shared task , methods based on neural networks became a more and more common choice for tackling the task ( Wu et al . , 2018 ; Nikhil and Mundra , 2018 ) , and \ufb01nally , it was the turn of Transformer - based models such as BERT ( Devlin et al . , 2019 ) and BioBERT ( Lee et al . , 2020 ) , which are the building blocks of most of the top performing systems in the recent competitions ( Chen et al . , 2019 ; Mahata et al . , 2019 ; Miftahutdinov et al . , 2019 ) .", "entities": [[5, 6, "DatasetName", "SMM4H"], [48, 49, "MethodName", "Transformer"], [54, 55, "MethodName", "BERT"]]}, {"text": "At the same time , the task has been independently tackled also by researchers in Named Entity Recognition , since ADE detection represents a classical case of a challenging task where the entities can be composed by discontinuous spans of text ( Stanovsky et al . , 2017 ;", "entities": [[15, 18, "TaskName", "Named Entity Recognition"]]}, {"text": "Dai et al . , 2020 ; Wunnava et al . , 2020 ) .", "entities": []}, {"text": "2.2 Transformers Architectures in NLP There is little doubt that Transformers ( Vaswani et al . , 2017 ) have been the dominant class of NLP systems in the last few years .", "entities": []}, {"text": "The \u201c golden child \u201d of this revolution is BERT ( Devlin et", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "al . , 2019 ) , which was the \ufb01rst system to apply the bidirectional training of a Transformer to a language modeling task", "entities": [[18, 19, "MethodName", "Transformer"]]}, {"text": ".", "entities": []}, {"text": "More speci\ufb01cally , BERT is trained with a Masked Language Modeling objective : random words in the input sentences are replaced by a [ MASK ] token and the model attempts to predict the masked token based on the surrounding context .", "entities": [[3, 4, "MethodName", "BERT"], [8, 11, "TaskName", "Masked Language Modeling"]]}, {"text": "Following BERT \u2019s success , several similar architectures have been introduced in biomedical NLP , proposing different forms of in - domain training or using different corpora ( Beltagy et al . , 2019 ; Alsentzer et al . , 2019 ; Lee et al . , 2020 ;", "entities": [[1, 2, "MethodName", "BERT"]]}, {"text": "Gu et al . , 2020 ) .", "entities": []}, {"text": "Some of them already proved to be ef\ufb01cient for ADE detection : for example , the top system of the SMM4H shared task 2019 is based on an ensemble of BioBERTs ( Weissenbacher et al . , 2019 ) .", "entities": [[20, 21, "DatasetName", "SMM4H"]]}, {"text": "Another potentially interesting addition to the library of BERTs for ADE detection is SpanBERT ( Joshi et al . , 2019 ) .", "entities": []}, {"text": "During the training of SpanBERT , random contiguous spans of tokens aremasked , rather than individual words , forcing the model to predict the full span from the tokens at its boundaries .", "entities": []}, {"text": "We decided to introduce SpanBERT in our experiments because longer spans and relations between multiple spans of text are a key factor in ADE detection , and thus encoding such information is potentially an advantage .", "entities": []}, {"text": "3 Experimental Settings 3.1 Datasets The datasets chosen for the experiments are two widely used benchmarks .", "entities": []}, {"text": "They are annotated for the presence of ADEs at character level : each document is accompanied by list of start and end indices for the ADEs contained in it .", "entities": []}, {"text": "We convert these annotations using the IOB annotation scheme for the tokens : B marks the start of a mention , I and O the tokens inside and outside a mention respectively .", "entities": []}, {"text": "CADEC ( Karimi et al . , 2015a ) contains 1250 posts from the health - related forum \u201c AskaPatient \u201d , annotated for the presence of ADEs .", "entities": []}, {"text": "We use the splits made publicly available by Dai et al . ( 2020 ) .", "entities": []}, {"text": "SMM4H is the training dataset for Task 2 of the SMM4H", "entities": [[0, 1, "DatasetName", "SMM4H"], [10, 11, "DatasetName", "SMM4H"]]}, {"text": "shared task 2019 ( Weissenbacher et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "It contains 2276 tweets which mention at least one drug name , 1300 of which are positive for the presence of ADEs while the other 976 are negative samples .", "entities": []}, {"text": "The competition includes a blind test set , but in order to perform a deeper analysis on the results , we use the training set only .", "entities": []}, {"text": "As far as we know there is no of\ufb01cial split for the training set alone , so we partitioned it into training , validation and test sets ( 60:20:20 ) , maintaining the proportions of positive and negative samples .", "entities": []}, {"text": "This split and the code for all the experiments are available athttps://github.com/AilabUdineGit/ADE .", "entities": []}, {"text": "The datasets correspond to different text genres : the tweets of SMM4H are mostly short messages , containing informal language , while the texts of CADEC are longer and structured descriptions .", "entities": [[11, 12, "DatasetName", "SMM4H"]]}, {"text": "To verify this point , we used the TEXTSTAT Python package to extract some statistics from the texts of the two datasets ( see Appendix A ) .", "entities": []}, {"text": "3.2 Metrics As evaluation metrics we use the Strict F1 score , which is commonly adopted for this task ( SeguraBedmar et al . , 2013 ) .", "entities": [[9, 11, "MetricName", "F1 score"]]}, {"text": "It is computed at the entity level , and assigns a hit only in case of perfect match between the labels assigned by the model and the labels in the gold annotation .", "entities": []}, {"text": "1742In CADEC around 10 % of mentions are discontinuous ( Dai et al . , 2020 ) and it is possible to have overlaps and intersections of discontinuous spans .", "entities": []}, {"text": "We performed data tidying by merging overlapping ADE mentions , keeping only the longer span ( as it is customary in the literature ) and splitting discontinuous spans in multiple continuous spans .", "entities": []}, {"text": "3.3 Overview of the Models 3.3.1 Pretrained BERT Variants Apart from the original BERT , we experimented with SpanBERT , for its peculiar pretraining procedure which focuses on predicting and encoding spans instead of single words , and with four BERT variants with in - domain knowledge , which differ from each other both for the corpus they were trained on and for the kind of pretraining .", "entities": [[7, 8, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"], [40, 41, "MethodName", "BERT"]]}, {"text": "BERT Standard model , pretrained on general purpose texts ( Wikipedia and BookCorpus ) .", "entities": [[0, 1, "MethodName", "BERT"], [12, 13, "DatasetName", "BookCorpus"]]}, {"text": "SpanBERT", "entities": []}, {"text": "This model is pretrained using the same corpus as the original BERT , so it comes with no in - domain knowledge .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "But the pretraining procedure makes its embeddings more appropriate for NER - like tasks .", "entities": [[10, 11, "TaskName", "NER"]]}, {"text": "as it introduces an additional loss called Span Boundary Objective ( SBO ) , alongside the traditional Masked Language Modelling ( MLM ) used for BERT .", "entities": [[5, 6, "MetricName", "loss"], [18, 20, "TaskName", "Language Modelling"], [21, 22, "DatasetName", "MLM"], [25, 26, "MethodName", "BERT"]]}, {"text": "Let us consider a sentence S= [ w1 ; w2 ; : : : ; w k ] and its substring Sm : n=", "entities": []}, {"text": "[ wm ; : : : ; w n].wm\u00001and wn+1are the boundaries of Sm : n(the words immediately preceding and following it ) .", "entities": []}, {"text": "We mask Sby replacing all the words in Sm : nwith the [ MASK ] token .", "entities": []}, {"text": "SpanBERT reads the masked version of Sand returns an embedding for each word .", "entities": []}, {"text": "The MLM loss measures if it is possible to reconstruct each original word wi2Sm : nfrom the corresponding embedding .", "entities": [[1, 2, "DatasetName", "MLM"], [2, 3, "MetricName", "loss"]]}, {"text": "The SBO loss measures if it is possible to reconstruct each wi2Sm : nusing the embeddings of the boundary words wm\u00001andwn+1 .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "BioBERT ( Lee et al . , 2020 ) , pretrained from a BERT checkpoint , on PubMed abstracts .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "The authors of BioBERT provide different versions of the model , pretrained on different corpora .", "entities": []}, {"text": "We selected the version which seemed to have the greatest advantage on this task , according to the results by Lee et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "We chose BioBERT v1.1 ( + PubMed ) , which outperformed other BioBERT v1.0 versions ( including the ones trained on full texts ) in NER tasks involving Diseases and Drugs .", "entities": [[25, 26, "TaskName", "NER"]]}, {"text": "Preliminary experiments against BioBERT v.1.0(+PubMed+PMC ) con\ufb01rmed this behaviour ( see Appendix D ) .", "entities": []}, {"text": "BioClinicalBERT ( Alsentzer et al . , 2019 ) , pretrained from a BioBERT checkpoint , on clinical texts from the MIMIC - III database .", "entities": [[21, 24, "DatasetName", "MIMIC - III"]]}, {"text": "SciBERT ( Beltagy et al . , 2019 ) , pretrained from scratch , on papers retrieved from Semantic Scholar ( 82 % of medical domain ) .", "entities": [[18, 20, "DatasetName", "Semantic Scholar"], [24, 26, "DatasetName", "medical domain"]]}, {"text": "PubMedBERT ( Gu et al . , 2020 ) , pretrained from scratch , on PubMed abstracts and full text articles from PubMed Central .", "entities": []}, {"text": "This model was created to prove that pretraining from scratch on a single domain produces substantial gains on in - domain downstream tasks .", "entities": []}, {"text": "Gu et al .", "entities": []}, {"text": "( 2020 ) compared it with various other models pretrained on either general texts , mixed - domain texts or in - domain texts starting from a general - purpose checkpoint ( e.g. BioBERT ) , showing that PubMedBERT outperforms them on several tasks based on medical language .", "entities": []}, {"text": "The vocabulary of PubMedBERT contains more in - domain medical words than any other model under consideration .", "entities": []}, {"text": "However , it should be kept in mind that ADE detection requires an understanding of both medical terms and colloquial language , as both can occur in social media text .", "entities": []}, {"text": "Notice that two in - domain architectures were pretrained from scratch ( SciBERT and PubMedBERT ) , meaning that they have a unique vocabulary tailored on their pretraining corpus , and include speci\ufb01c embeddings for in - domain words .", "entities": []}, {"text": "BioBERT and BioClinicalBERT were instead pretrained starting from a BERT and BioBERT checkpoint , respectively .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "This means that the vocabularies are built from general - domain texts ( similarly to BERT ) and the embeddings are initialized likewise .", "entities": [[15, 16, "MethodName", "BERT"]]}, {"text": "3.3.2 Simple and CRF Architecture", "entities": [[3, 4, "MethodName", "CRF"]]}, {"text": "For all of the BERT variants , we take into account two versions .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "The \ufb01rst one simply uses the model to generate a sequence of embeddings ( one for each sub - word token ) , which are then passed to a Linear Layer + Softmax to project them to the output space ( one value for each output label ) and turn them into a probability distribution over the labels .", "entities": [[29, 31, "MethodName", "Linear Layer"], [32, 33, "MethodName", "Softmax"]]}, {"text": "The second version combines the Transformerbased model with a Conditional Random Field ( CRF ) classi\ufb01er ( Lafferty et al . , 2001 ; Papay et al . , 2020 ) .", "entities": [[9, 12, "MethodName", "Conditional Random Field"], [13, 14, "MethodName", "CRF"]]}, {"text": "The outputs generated by the \ufb01rst version become the input of a CRF module , producing another sequence of subword - level IOB labels .", "entities": [[12, 13, "MethodName", "CRF"]]}, {"text": "This", "entities": []}, {"text": "1743step aims at denoising the output labels produced by the previous components .", "entities": [[3, 4, "TaskName", "denoising"]]}, {"text": "The output labels are calculated for sub - word tokens , then we aggregate each set of sub - word labelsf`iginto a word labelLusing the \ufb01rst rule that applies : ( i ) if ` i = Ofor all", "entities": []}, {"text": "i , thenL = O ; ( ii )", "entities": []}, {"text": "if ` i = Bfor any i , thenL = B ; ( iii ) if ` i = I for any i , thenL = I. The aggregated output is a sequence of word - level IOB labels .", "entities": []}, {"text": "3.3.3 Baseline As a strong baseline , we used the TMRLeiden architecture ( Dirkson and Verberne , 2019 ) , which achieved the 2nd best Strict F1 - Score in the latest SMM4H shared task ( Weissenbacher et al . , 2019 ) and is composed of a BiLSTM taking as input a concatenation of BERT and Flair embeddings ( Akbik et al . , 2019 ) .", "entities": [[26, 29, "MetricName", "F1 - Score"], [32, 33, "DatasetName", "SMM4H"], [48, 49, "MethodName", "BiLSTM"], [55, 56, "MethodName", "BERT"]]}, {"text": "We chose this baseline since the TMRLeiden code is publicly available .", "entities": []}, {"text": "3.4 Implementation details TMRLeiden was re - implemented starting from its the original code1and trained according to the details in the paper .", "entities": []}, {"text": "As for the Transformers , all experiments were performed using the TRANSFORMERS library ( Wolf et al . , 2019 ) ( see Appendix C ) .", "entities": []}, {"text": "Parameter - tuning was done via grid - search , using different learning rates ( [ 5e\u00004;5e\u00005;5e\u00006 ] ) and dropout rates ( from 0:15to0:30 , increments of 0:05 ) .", "entities": []}, {"text": "All the architectures were trained for 50 epochs on the training set .", "entities": []}, {"text": "Learning rate , dropout rate and maximum epoch were chosen evaluating the models on the validation set .", "entities": [[0, 2, "HyperparameterName", "Learning rate"]]}, {"text": "During evaluation all the models were then trained using the best hyperparameters on the concatenation of the training set and the validation set , and tested on the test set .", "entities": []}, {"text": "This procedure was repeated \ufb01ve times with different random seeds , and \ufb01nally we averaged the results over the \ufb01ve runs .", "entities": [[9, 10, "DatasetName", "seeds"]]}, {"text": "4 Evaluation The results for the two datasets are shown in Table 1 ( we focus on the F1 - score , but Precision and Recall are reported in Appendix D ) .", "entities": [[18, 21, "MetricName", "F1 - score"], [23, 24, "MetricName", "Precision"], [25, 26, "MetricName", "Recall"]]}, {"text": "For reference , we reported the scores of the best architecture by Dai et al .", "entities": []}, {"text": "( 2020 ) , which is the state - of - the - art system on CADEC .", "entities": []}, {"text": "At a glance , all systems perform better on CADEC , whose texts belong to a more standardized variety of language .", "entities": []}, {"text": "SpanBERT and 1https://github.com/AnneDirkson/ SharedTaskSMM4H2019SMM4H CADEC Architecture F1 std F1 std Dai et al .", "entities": [[6, 7, "MetricName", "F1"], [8, 9, "MetricName", "F1"]]}, {"text": "( 2020 ) \u2013 \u2013 68.90 \u2013 TMRLeiden 60.70 2.08 65.03 1.14 BERT 54.74 1.40 65.20 0.47 BERT+CRF 59.35 1.23 64.36 0.83 SpanBERT 62.15 2.17 67.18 0.78 SpanBERT+CRF 59.89 2.16 67.59 0.60 PubMedBERT 61.88 0.79 67.16 0.52 PubMedBERT+CRF 59.53 2.07 67.28 0.82 BioBERT 57.83 2.59 65.59 1.10 BioBERT+CRF 58.05 1.45 66.00 0.67 SciBERT 57.75 1.55 65.61 0.54 SciBERT+CRF 58.86 1.55 67.09 0.74 BioClinicalBert 58.03 0.89 64.64 0.53 BioClinicalBert+CRF 59.11 1.99 65.97 0.60 Table 1 : F1 scores with standard deviations for all models ( our best performing model is in bold ) .", "entities": [[12, 13, "MethodName", "BERT"], [75, 76, "MetricName", "F1"]]}, {"text": "PubMedBERT emerge as the top performing models , with close F1 - scores , and in particular , the SpanBERT models achieve the top score on both datasets , proving that modeling spans gives an important advantage for the identi\ufb01cation of ADEs .", "entities": [[10, 11, "MetricName", "F1"]]}, {"text": "For both models , the addition of CRF generally determines a slight improvement on CADEC , while it is detrimental on SMM4H. On SMM4H , the F1 - scores of BioBERT , SciBERT and BioClinicalBERT consistently improve over the standard BERT , but they are outperformed by its CRFaugmented version , while on CADEC they perform closely to the standard model .", "entities": [[7, 8, "MethodName", "CRF"], [23, 24, "DatasetName", "SMM4H"], [26, 27, "MetricName", "F1"], [40, 41, "MethodName", "BERT"]]}, {"text": "The results suggest that in - domain knowledge is consistently useful only when training is done on in - domain text from scratch , instead of using general domain text \ufb01rst .", "entities": []}, {"text": "SciBERT is also trained from scratch , but on a corpus that is less speci\ufb01c for the biomedical domain than the PubMedBERT one ( Gu et al . , 2020 ) .", "entities": []}, {"text": "The models are also being compared with TMRLeiden : we can notice that both versions of SpanBERT and PubMedBERT outperform it on CADEC ( the differences are also statistically signi\ufb01cant for the McNemar test at p < 0:001 ) , while only the basic versions of the same models retain an advantage on it on SMM4H ( also in this case , the difference is signi\ufb01cant at p < 0:001 ) .", "entities": [[55, 56, "DatasetName", "SMM4H"]]}, {"text": "4.1 Error Analysis We analyzed the differences between the ADE entities correctly identi\ufb01ed by the models and those that were missed , using the text statistics that we previously extracted with TEXTSTAT .", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "As it was", "entities": []}, {"text": "17441@hospitalpatient have been on humira 2years now n get on off chest infections that sometimes need 2diff pills 2sort out should i b worried ?", "entities": []}, {"text": "4i have had no side effects been taking arthrotec a little over a year , have not noticed any side effects .", "entities": []}, {"text": "it does help alot i noticed that when there are times when i forget to take it i ca n\u2019t stand or walk for any lengths of time .", "entities": []}, {"text": "2had a great few hours on my bike but exercise drives my olanzapine # munchies .", "entities": []}, {"text": "getting fed up with notbeing able to\ufb01tinto summer wardrobe5works just \ufb01ne .", "entities": []}, {"text": "if there are any side effects , they are de\ufb01nitely not noticeable .", "entities": []}, {"text": "what \u2019s with all these older people ( 70 \u2019s ) complaining about the lack of sex drive ?", "entities": []}, {"text": "how much of what you are complaining about is simply related to getting older ?", "entities": []}, {"text": "3this new baccy is just making my cough so much worse but ahh well need my nicotine6what a great store", "entities": []}, {"text": "@walmart is : i loss iq points , gained weight & got addicted to nicotine - all in under 15 min from going in ! !", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "Table 2 : Examples of ADEs extracted by PubMedBERT ( overlined in blue ) and SpanBERT ( underlined in red ) .", "entities": []}, {"text": "Actual ADEs in bold with gray background .", "entities": []}, {"text": "The Samples belong to SMM4H ( 1\u20133 , 6 ) and CADEC ( 4\u20135 ) .", "entities": [[4, 5, "DatasetName", "SMM4H"]]}, {"text": "predictable , it turns out that longer ADE spans are more dif\ufb01cult to identify : for all models , we extracted the average word length of correct and missed spans and we compared them with a twotailed Mann - Whitney U test , \ufb01nding that the latter are signi\ufb01cantly longer ( Z", "entities": []}, {"text": "= -6.176 , p < 0:001 ) .", "entities": []}, {"text": "We also extracted the average number of dif\ufb01cult words in the correct and in the missed spans , de\ufb01ned as words with more than two syllables that are not included in the TEXTSTAT list of words of common usage in standard English .", "entities": []}, {"text": "We took this as an approximation of the number of \u201d technical \u201d terms in the dataset instances .", "entities": []}, {"text": "However , the average values for correct and missed instances do not differ ( Z = 0.109 , p > 0:1 ) , suggesting that the presence of dif\ufb01cult or technical words in a given instance does not represent an inherent factor of dif\ufb01culty or facilitation .", "entities": []}, {"text": "Still , for some of the models \u2013 including SpanBERT , PubMedBERT and TMRLeiden \u2013 this difference reaches a marginal signi\ufb01cance ( p < 0:05 ) exclusively on the SMM4H dataset , where correctly identi\ufb01ed spans have more dif\ufb01cult words .", "entities": [[29, 30, "DatasetName", "SMM4H"]]}, {"text": "A possible interpretation is that , as the tweets \u2019 language is more informal , such words represent a stronger ADE cue , compared to the more technical language of the CADEC dataset .", "entities": []}, {"text": "Finally , we performed a qualitative analysis , comparing the predictions of SpanBERT and PubMedBERT .", "entities": []}, {"text": "We selected the samples on which one of the architectures performed signi\ufb01cantly better than the other one in terms of F1 - Score , and analyzed them manually .", "entities": [[20, 23, "MetricName", "F1 - Score"]]}, {"text": "Some signi\ufb01cant samples can be found in Table 2 .", "entities": []}, {"text": "We observed that most of the samples in which PubMedBERT performed better than SpanBERT contained medical terms , which SpanBERT had completely ignored ( e.g. Sample1 ) .", "entities": []}, {"text": "The samples in which SpanBERT outperformed the in - domain model contained instead long ADE mentions , often associated with informal descriptions ( e.g. Samples 2 , 3 ) .", "entities": []}, {"text": "As regards false positives , both models make similar errors , which \ufb01t into two broad categories : ( 1 ) extracting diseases or symptoms of a disease ( e.g. Samples 4 , 6 ) ; ( 2 ) not being able to handle general mentions , hypothetical language , negations and similar linguistic constructs ( e.g. Sample 5 ) .", "entities": []}, {"text": "While the second kind of error requires a deeper re\ufb02ection , the \ufb01rst one might be addressed by training the model to extract multiple kinds of entities ( e.g. both ADEs and Diseases ) .", "entities": []}, {"text": "5 Conclusions We presented a comparison between 12 transformers - based models , with the goal of \u201c prescribing \u201d the best option to the researchers working in the \ufb01eld .", "entities": []}, {"text": "We also wanted to test whether the span - based objective of SpanBERT and in - domain language pretraining were useful for the task .", "entities": []}, {"text": "We can positively answer to the \ufb01rst question , since SpanBERT turned out to be the best performing model on both datasets .", "entities": []}, {"text": "As for the in - domain models , PubMedBERT came as a close second after SpanBERT , suggesting that pretraining from scratch with no general domain data is the best strategy , at least for this task .", "entities": []}, {"text": "We have been the \ufb01rst , to our knowledge , to test these two models in a systematic comparison on ADE detection , and they delivered promising results for future research .", "entities": []}, {"text": "For the next step , a possible direction would be to combine the strengths of their respective representations : the accurate modeling of text spans on the one side , and deep biomedical knowledge on the other one .", "entities": []}, {"text": "1745References Alan Akbik , Tanja Bergmann , Duncan Blythe , Kashif Rasul , Stefan Schweter , and Roland V ollgraf .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Flair :", "entities": []}, {"text": "An Easy - to - use Framework for State - of - theart nlp .", "entities": []}, {"text": "In Proceedings of NAACL .", "entities": []}, {"text": "Emily Alsentzer , John R Murphy , Willie Boag , WeiHung Weng , Di Jin , Tristan Naumann , and Matthew McDermott . 2019 .", "entities": []}, {"text": "Publicly Available Clinical BERT Embeddings .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "In Proceedings of the NAACL Workshop on Clinical Natural Language Processing .", "entities": []}, {"text": "Iz Beltagy , Kyle Lo , and Arman Cohan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SciBERT : A Pretrained Language Model for Scienti\ufb01c Text .", "entities": []}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Shuai Chen , Yuanhang Huang , Xiaowei Huang , Haoming Qin , Jun Yan , and Buzhou Tang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "HITSZICRC :", "entities": []}, {"text": "A Report for SMM4H Shared Task 2019Automatic Classi\ufb01cation and Extraction of Adverse Effect Mentions in Tweets .", "entities": [[3, 4, "DatasetName", "SMM4H"]]}, {"text": "In Proceedings of the ACL Workshop on Social Media Mining for Health Applications .", "entities": []}, {"text": "Xiang Dai . 2018 .", "entities": []}, {"text": "Recognizing Complex Entity Mentions : A Review and Future Directions .", "entities": []}, {"text": "In Proceedings of ACL 2018 , Student Research Workshop .", "entities": []}, {"text": "Xiang Dai , Sarvnaz Karimi , Ben Hachey , and C \u00b4 ecile Paris .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "An Effective Transition - based Model for Discontinuous NER .", "entities": [[8, 9, "TaskName", "NER"]]}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "Raminta Daniulaityte , Lu Chen , Francois R Lamy , Robert G Carlson , Krishnaprasad Thirunarayan , and Amit Sheth . 2016 .", "entities": []}, {"text": "\u201c When \u2018 Bad \u2019 Is \u2018 Good \u2019 \u201d : Identifying Personal Communication and Sentiment in Drug - related Tweets .", "entities": []}, {"text": "JMIR Public Health and Surveillance , 2(2):e162 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of NAACL .", "entities": []}, {"text": "Anne Dirkson and Suzan Verberne . 2019 .", "entities": []}, {"text": "Transfer Learning for Health - related Twitter Data .", "entities": [[0, 2, "TaskName", "Transfer Learning"]]}, {"text": "In Proceedings of the ACL Workshop on Social Media Mining for Health Applications .", "entities": []}, {"text": "Yu Gu , Robert Tinn , Hao Cheng , Michael Lucas , Naoto Usuyama , Xiaodong Liu , Tristan Naumann , Jianfeng Gao , and Hoifung Poon .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "DomainSpeci\ufb01c Language Model Pretraining for Biomedical Natural Language Processing .", "entities": []}, {"text": "arXiv preprint arXiv:2007.15779 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mandar Joshi , Danqi Chen , Yinhan Liu , Daniel S. Weld , Luke Zettlemoyer , and Omer Levy . 2019 .", "entities": []}, {"text": "SpanBERT :", "entities": []}, {"text": "Improving Pre - training by Representing and Predicting Spans .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 8:64\u201377 .", "entities": []}, {"text": "Sarvnaz Karimi , Alejandro Metke - Jimenez , Madonna Kemp , and Chenchen Wang .", "entities": []}, {"text": "2015a .", "entities": []}, {"text": "Cadec : A Corpus of Adverse Drug Event Annotations .", "entities": []}, {"text": "Journal of Biomedical Informatics , 55:73\u201381.Sarvnaz", "entities": []}, {"text": "Karimi , Chen Wang , Alejandro MetkeJimenez , Raj Gaire , and Cecile Paris .", "entities": []}, {"text": "2015b .", "entities": []}, {"text": "Text and Data Mining Techniques in Adverse Drug Reaction Detection .", "entities": []}, {"text": "ACM Computing Surveys ( CSUR ) , 47(4):1\u201339 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "John Lafferty , Andrew Mccallum , and Fernando Pereira . 2001 .", "entities": []}, {"text": "Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data .", "entities": []}, {"text": "In Proceedings of ICML .", "entities": []}, {"text": "Jinhyuk Lee , Wonjin Yoon , Sungdong Kim , Donghyeon Kim , Sunkyu Kim , Chan Ho", "entities": []}, {"text": "So , and Jaewoo Kang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "BioBERT : A Pre - trained Biomedical Language Representation Model for Biomedical Text Mining .", "entities": []}, {"text": "Bioinformatics , 36(4):1234\u20131240 .", "entities": []}, {"text": "Debanjan Mahata , Sarthak Anand , Haimin Zhang , Simra Shahid , Laiba Mehnaz , Yaman Kumar , and Rajiv Shah . 2019 .", "entities": [[16, 17, "DatasetName", "Kumar"]]}, {"text": "MIDAS@ SMM4H-2019 : Identifying Adverse Drug Reactions and Personal Health Experience Mentions from Twitter .", "entities": []}, {"text": "In Proceedings of the ACL Workshop on Social Media Mining for Health Applications .", "entities": []}, {"text": "Zulfat Miftahutdinov , Ilseyar Alimova , and Elena Tutubalina .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "KFU NLP Team at SMM4H 2019 Tasks : Want to Extract Adverse Drugs Reactions from Tweets ?", "entities": [[4, 5, "DatasetName", "SMM4H"]]}, {"text": "BERT to the Rescue .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the ACL Workshop on Social Media Mining for Health Applications .", "entities": []}, {"text": "Azadeh Nikfarjam , Abeed Sarker , Karen O\u2019Connor , Rachel E. Ginn , and Graciela Gonzalez - Hernandez .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Pharmacovigilance from Social Media : Mining Adverse Drug Reaction Mentions Using Sequence Labeling with Word Embedding Cluster Features .", "entities": []}, {"text": "Journal of the American Medical Informatics Association : JAMIA , 22:671 \u2013 681 .", "entities": []}, {"text": "Nishant Nikhil and Shivansh Mundra .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural DrugNet .", "entities": []}, {"text": "In Proceedings of the EMNLP Workshop on Social Media Mining for Health Applications .", "entities": []}, {"text": "Sean Papay , Roman Klinger , and Sebastian Pad \u00b4 o. 2020 .", "entities": []}, {"text": "Dissecting Span Identi\ufb01cation Tasks with Performance Prediction .", "entities": []}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Michael Paul , Abeed Sarker , John Brownstein , Azadeh Nikfarjam , Matthew Scotch , Karen Smith , and Graciela Gonzalez . 2016 .", "entities": []}, {"text": "Social Media Mining for Public Health Monitoring and Surveillance .", "entities": []}, {"text": "In Biocomputing 2016 , pages 468\u2013479 .", "entities": []}, {"text": "Abeed Sarker , Rachel Ginn , Azadeh Nikfarjam , Karen O\u2019Connor , Karen Smith , Swetha Jayaraman , Tejaswi Upadhaya , and Graciela Gonzalez . 2015 .", "entities": []}, {"text": "Utilizing Social Media Data for Pharmacovigilance : A Review .", "entities": []}, {"text": "Journal of Biomedical Informatics , 54:202 \u2013 212 .", "entities": []}, {"text": "Abeed Sarker and Graciela Gonzalez . 2015 .", "entities": []}, {"text": "Portable Automatic Text Classi\ufb01cation for Adverse Drug Reaction Detection via Multi - corpus Training .", "entities": []}, {"text": "Journal of Biomedical Informatics , 53:196\u2013207 .", "entities": []}, {"text": "1746Abeed Sarker and Graciela Gonzalez - Hernandez .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Overview of the Social Media Mining for Health ( SMM4H ) Shared Tasks at AMIA 2017 .", "entities": [[9, 10, "DatasetName", "SMM4H"]]}, {"text": "Training , 1(10,822):1239 .", "entities": []}, {"text": "Isabel Segura - Bedmar , Paloma Mart \u00b4 \u0131nez , and Mar \u00b4 \u0131a Herrero - Zazo . 2013 .", "entities": []}, {"text": "SemEval-2013 Task 9 : Extraction of Drug - Drug Interactions from Biomedical Texts ( DDIExtraction 2013 ) .", "entities": []}, {"text": "In Proceedings of SemEval .", "entities": []}, {"text": "Gabriel Stanovsky , Daniel Gruhl , and Pablo Mendes . 2017 .", "entities": []}, {"text": "Recognizing Mentions of Adverse Drug Reaction in Social Media Using Knowledge - Infused Recurrent Models .", "entities": []}, {"text": "In Proceedings of EACL .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention Is All You Need .", "entities": []}, {"text": "In Proceedings of NIPS .", "entities": []}, {"text": "Davy Weissenbacher , Abeed Sarker , Arjun Magge , Ashlynn Daughton , Karen O\u2019Connor , Michael Paul , and Graciela Gonzalez . 2019 .", "entities": []}, {"text": "Overview of the Fourth Social Media Mining for Health ( SMM4H ) Shared Tasks at ACL 2019 .", "entities": [[10, 11, "DatasetName", "SMM4H"]]}, {"text": "In Proceedings of the ACL Social Media Mining for Health Applications ( # SMM4H ) Workshop & Shared Task .", "entities": [[13, 14, "DatasetName", "SMM4H"]]}, {"text": "Davy Weissenbacher , Abeed Sarker , Michael Paul , and Graciela Gonzalez .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Overview of the Social Media Mining for Health ( SMM4H ) Shared Tasks at EMNLP 2018 .", "entities": [[9, 10, "DatasetName", "SMM4H"]]}, {"text": "In Proceedings of the EMNLP Workshop on Social Media Mining for Health Applications .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , R \u00b4 emi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander M. Rush .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "HuggingFace \u2019s Transformers : State - of - the - art Natural Language Processing .", "entities": []}, {"text": "ArXiv , abs/1910.03771 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Chuhan Wu , Fangzhao Wu , Junxin Liu , Sixing Wu , Yongfeng Huang , and Xing Xie . 2018 .", "entities": []}, {"text": "Detecting Tweets Mentioning Drug Name and Adverse Drug Reaction with Hierarchical Tweet Representation and Multi - head Self - attention .", "entities": []}, {"text": "In Proceedings of the EMNLP Workshop on Social Media Mining for Health Applications .", "entities": []}, {"text": "Susmitha Wunnava , Xiao Qin , Tabassum Kakar , Xiangnan Kong , and Elke Rundensteiner . 2020 .", "entities": []}, {"text": "A DualAttention Network for Joint Named Entity Recognition and Sentence Classi\ufb01cation of Adverse Drug Events .", "entities": [[5, 8, "TaskName", "Named Entity Recognition"]]}, {"text": "In Findings of EMNLP .", "entities": []}, {"text": "A Text statistics for datasets Some statistics for the texts of the two datasets have been extracted with the TEXTSTAT Pythonpackage and reported reported in Table A : we extracted the counts of syllables , lexicon ( how many different word types are being used ) , sentences and characters .", "entities": []}, {"text": "Dif\ufb01cult words refers to the number of polysyllabic words with Syllable Count > 2that are not included in the list of words of common usage in English .", "entities": []}, {"text": "Metric CADEC SMM4H", "entities": [[2, 3, "DatasetName", "SMM4H"]]}, {"text": "Syllable Count 116\u00062.7 26\u00060.2 Lexicon Count 83\u00061.9 18\u00060.1 Sentence Count 5\u00060.1 1\u00060.0 Character Count 461\u000610.5 104\u00060.7 Dif\ufb01cult Words 14\u00060.3 4\u00060.1", "entities": []}, {"text": "Table 3 : Average metrics per dataset , computed by the TEXTSTAT Python library .", "entities": []}, {"text": "B Best hyperparameters on the two datasets Table 4 reports the best hyperparameters for all architectures on SMM4H and CADEC , respectively .", "entities": [[17, 18, "DatasetName", "SMM4H"]]}, {"text": "SMM4H CADEC Architecture lr dropout epoch", "entities": [[0, 1, "DatasetName", "SMM4H"]]}, {"text": "lr dropout epoch BERT 5e\u000050.20 4 5e\u000050.25 11 BERT+CRF 5e\u000050.15 6 5e\u000050.15 7 SpanBERT 5e\u000050.25", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "43 5e\u000050.25 19 SpanBERT+CRF 5e\u000050.15 14 5e\u000050.15 11 PubMedBERT 5e\u000050.25 21 5e\u000050.15 7 PubMedBERT+CRF 5e\u000050.25 13 5e\u000050.25 16 BioBERT 5e\u000050.20 8 5e\u000050.25 12 BioBERT+CRF 5e\u000050.15 6 5e\u000050.20 9 SciBERT 5e\u000050.15 7 5e\u000050.15 6 SciBERT+CRF 5e\u000050.25 13 5e\u000050.25 12 BioClinicalBERT 5e\u000050.25 10 5e\u000050.25 6 BioClinicalBERT+CRF 5e\u000050.25 12 5e\u000050.25 10 Table 4 : Best hyperparameters for all Transformerbased architectures on SMM4H and CADEC .", "entities": [[58, 59, "DatasetName", "SMM4H"]]}, {"text": "C General information on the models Table 5 is a summary of the information about the version of all Transformer - based models used and their pretraining methods .", "entities": [[1, 2, "DatasetName", "General"], [19, 20, "MethodName", "Transformer"]]}, {"text": "D Detailed metrics of all the models Table 6 and 7 report as Strict and Partial metrics the F1 - score , Precision and Recall calculated for all architectures on SMM4H and CADEC respectively .", "entities": [[18, 21, "MetricName", "F1 - score"], [22, 23, "MetricName", "Precision"], [24, 25, "MetricName", "Recall"], [30, 31, "DatasetName", "SMM4H"]]}, {"text": "Results are the average over \ufb01ve runs .", "entities": []}, {"text": "The Partial scores are standard metrics for this task ( Weissenbacher et", "entities": []}, {"text": "al . , 2019 ) and take into account \u201c partial\u201dmatches , in which it is suf\ufb01cient for a system prediction to partially overlap with the gold annotation to be considered as a true match .", "entities": []}, {"text": "1747Name Version Vocabulary Pretraining Pretraining Corpus BERT base uncased Wikipedia+BookCorpus from scratch Wikipedia+BookCorpus SpanBERT base cased Wikipedia+BookCorpus from scratch Wikipedia+BookCorpus PubMedBERT base uncased abstract+fulltext PubMed from scratch PubMed+PMC BioBERT base v1.1 ( + PubMed ) Wikipedia+BookCorpus from BERT PubMed BioBERT(v1.0 ) base v1.0 ( + PubMed+PMC ) Wikipedia+BookCorpus from BERT PubMed+PMC SciBERT scivocab cased Semantic Scholar from scratch Semantic Scholar BioClinicalBERT bio+clinical Wikipedia+BookCorpus from BioBERT MIMIC - III Table 5 : Information about the version of all the Transformer - based models used and their pretraining .", "entities": [[6, 7, "MethodName", "BERT"], [37, 38, "MethodName", "BERT"], [49, 50, "MethodName", "BERT"], [54, 56, "DatasetName", "Semantic Scholar"], [58, 60, "DatasetName", "Semantic Scholar"], [65, 68, "DatasetName", "MIMIC - III"], [78, 79, "MethodName", "Transformer"]]}, {"text": "Strict Partial F1 P R Architecture F1", "entities": [[2, 3, "MetricName", "F1"], [6, 7, "MetricName", "F1"]]}, {"text": "P R 60.70\u00062.08 68.36\u00062.41 54.59\u00061.97 TMRLeiden 66.08\u00061.79 74.42\u00062.11 59.43\u00061.76 54.74\u00061.40 48.50\u00061.67 62.84\u00061.12 BERT 64.53\u00061.09", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "57.17\u00061.52 74.08\u00060.78", "entities": []}, {"text": "59.35\u00061.23", "entities": []}, {"text": "54.12\u00061.19 65.69\u00061.34 BERT+CRF 68.35\u00060.64", "entities": []}, {"text": "62.33\u00060.74 75.66\u00060.68 62.15\u00062.17 54.54\u00063.06", "entities": []}, {"text": "72.31\u00061.30", "entities": []}, {"text": "SpanBERT 69.38\u00061.60 60.88\u00062.74 80.74\u00061.08 59.89\u00062.16", "entities": []}, {"text": "54.86\u00063.10 66.05\u00061.93 SpanBERT+CRF", "entities": []}, {"text": "68.09\u00061.51 62.35\u00062.79 75.10\u00061.72 61.88\u00060.79 58.70\u00060.83 65.45\u00061.39 PubMedBERT 69.82\u00060.60 66.23\u00060.86 73.84\u00061.26", "entities": []}, {"text": "59.53\u00062.07 55.29\u00062.19 64.49\u00062.27 PubMedBERT+CRF 67.94\u00061.48 63.10\u00061.69 73.61\u00061.84 55.22\u00061.71 49.85\u00061.76 61.89\u00061.78 BioBERT v1.0 64.25\u00061.09 58.00\u00061.22 72.02\u00061.30 57.83\u00062.59 53.68\u00063.20 62.72\u00062.30 BioBERT 66.58\u00061.34 61.79\u00062.25 72.23\u00061.42", "entities": []}, {"text": "58.05\u00061.45 54.44\u00062.18 62.22\u00061.22 BioBERT+CRF 66.30\u00060.85 62.17\u00061.83 71.07\u00061.15 57.75\u00061.55 53.49\u00060.97 62.75\u00062.54 SciBERT 66.49\u00060.83 61.61\u00060.61 72.25\u00061.89", "entities": []}, {"text": "58.86\u00061.55 52.94\u00062.27 66.35\u00061.86 SciBERT+CRF 67.12\u00060.97 60.36\u00061.93 75.67\u00061.99 58.03\u00060.89", "entities": []}, {"text": "51.63\u00061.51 66.26\u00060.46 BioClinicalBERT 66.90\u00060.57", "entities": []}, {"text": "59.52\u00061.29 76.39\u00060.99 59.11\u00061.99 52.35\u00062.55 67.92\u00061.55 BioClinicalBERT+CRF 67.41\u00061.19 59.69\u00061.92 77.48\u00061.40 Table 6 : Results on SMM4H , F1 - scores , Precision and Recall calculated as Strict and Partial metrics , with standard deviations for all models .", "entities": [[14, 15, "DatasetName", "SMM4H"], [16, 17, "MetricName", "F1"], [20, 21, "MetricName", "Precision"], [22, 23, "MetricName", "Recall"]]}, {"text": "Strict Partial F1 P R Architecture F1", "entities": [[2, 3, "MetricName", "F1"], [6, 7, "MetricName", "F1"]]}, {"text": "P R 65.03\u00061.14 67.50\u00061.01 62.75\u00061.26", "entities": []}, {"text": "TMRLeiden 77.08\u00060.78 79.99\u00060.60 74.36\u00060.97 65.20\u00060.47 62.86\u00060.52 67.72\u00060.70 BERT 77.73\u00060.28 74.95\u00060.57 80.74\u00060.47 64.36\u00060.83 62.47\u00060.97 66.36\u00060.79 BERT+CRF", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "77.23\u00060.45 74.97\u00060.72 79.63\u00060.41 67.18\u00060.78 65.84\u00060.94 68.57\u00060.78 SpanBERT 79.18\u00060.61 77.60\u00060.79 80.82\u00060.72 67.59\u00060.60 67.09\u00060.54 68.10\u00060.78 SpanBERT+CRF 79.43\u00060.27 78.84\u00060.24 80.02\u00060.60", "entities": []}, {"text": "67.16\u00060.52 66.60\u00060.67 67.73\u00060.57 PubMedBERT 79.13\u00060.23", "entities": []}, {"text": "78.47\u00060.51 79.81\u00060.42 67.28\u00060.82 66.69\u00060.99 67.88\u00060.91 PubMedBERT+CRF 79.12\u00060.43 78.43\u00060.72 79.83\u00060.71", "entities": []}, {"text": "65.54\u00060.47 64.24\u00060.48 66.90\u00060.46 BioBERT v1.0 77.86\u00060.34 76.32\u00060.36 79.47\u00060.33 65.59\u00061.10 64.86\u00061.39 66.34\u00060.85 BioBERT 78.17\u00060.75 77.30\u00061.13", "entities": []}, {"text": "79.06\u00060.48 66.00\u00060.67 65.52\u00060.97 66.48\u00060.63 BioBERT+CRF 78.24\u00060.43 77.68\u00060.81 78.82\u00060.58 65.61\u00060.54 64.46\u00060.70 66.80\u00060.50 SciBERT 78.05\u00060.19 76.69\u00060.36 79.47\u00060.46", "entities": []}, {"text": "67.09\u00060.74 65.99\u00060.74 68.23\u00060.80 SciBERT+CRF 79.01\u00060.35 77.72\u00060.36 80.35\u00060.50 64.64\u00060.53 61.99\u00060.51 67.53\u00060.56 BioClinicalBERT 76.95\u00060.35 73.80\u00060.36 80.39\u00060.37 65.97\u00060.60 64.23\u00061.16 67.82\u00060.60 BioClinicalBERT+CRF 77.98\u00060.49 75.92\u00061.26 80.17\u00060.53 Table 7 : Results on CADEC , F1 - scores , Precision and Recall calculated as Strict and Partial metrics , with standard deviations for all models .", "entities": [[28, 29, "MetricName", "F1"], [32, 33, "MetricName", "Precision"], [34, 35, "MetricName", "Recall"]]}]