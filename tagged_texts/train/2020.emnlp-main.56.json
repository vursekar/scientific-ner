[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 763\u2013780 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics763De - Biased Court \u2019s View Generation with Causality Yiquan Wu1 , Kun Kuang1\u0003 , Yating Zhang2\u0003 , Xiaozhong Liu3", "entities": []}, {"text": "Changlong Sun2 , Jun Xiao1 , Yueting Zhuang1 , Luo Si2 , Fei Wu1\u0003 1Zhejiang University , Hangzhou , China 2Alibaba Group , Hangzhou , China 3Indiana University Bloomington , USA fwuyiquan , kunkuang , yzhuang g@zju.edu.cn yatingz89@gmail.com , changlong.scl@taobao.com , liu237@indiana.edu fjunx , wufei g@cs.zju.edu.cn , luo.si@alibaba-inc.com", "entities": []}, {"text": "Abstract Court \u2019s view generation is a novel but essential task for legal AI , aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation .", "entities": []}, {"text": "While prior text - to - text natural language generation ( NLG ) approaches can be used to address this problem , neglecting the confounding bias from the data generation mechanism can limit the model performance , and the bias may pollute the learning outcomes .", "entities": []}, {"text": "In this paper , we propose a novel Attentional and Counterfactual based Natural Language Generation ( ACNLG ) method , consisting of an attentional encoder and a pair of innovative counterfactual decoders .", "entities": []}, {"text": "The attentional encoder leverages the plaintiff \u2019s claim and fact description as input to learn a claim - aware encoder from which the claim - related information in fact description can be emphasized .", "entities": []}, {"text": "The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgmentdiscriminative court \u2019s views ( both supportive and non - supportive views ) by incorporating with a synergistic judgment predictive model .", "entities": []}, {"text": "Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics .", "entities": []}, {"text": "1 Introduction Owing to the prosperity of machine learning , especially the natural language processing ( NLP ) techniques , many legal assistant systems have been proposed to improve the effectiveness and ef\ufb01ciency of a judge from different aspects , such as relevant case retrieval ( Chen et al . , 2013 ) , applicable law articles recommendation ( Chen et al . , 2019 ) , controversy focus mining ( Duan et al . , 2019 ) , and judgment prediction ( Lin et al . , 2012 ; Zhong et al . , 2018 ; Hu et al . , 2018 ; Jiang et al . , 2018 ; Chalkidis et al . , \u0003Corresponding Authors .", "entities": []}, {"text": "\ud835\udc6b(\ud835\udc71)\ud835\udc70\ud835\udc7d\ud835\udc96Figure 1 : Confounding bias from the data generation mechanism .", "entities": []}, {"text": "urefers to the unobserved mechanism ( i.e. , plaintiffs sue when they have a high probability to be supported ) that causes the judgment in dataset D(J ) to be imbalanced .", "entities": []}, {"text": "D(J)!Idenotes that the imbalanced dataD(J)has a causal effect on the representation of input I(i.e . , plaintiff \u2019s claim and fact description ) , andD(J)!Vdenotes that D(J)has a causal effect on the representation of court \u2019s view V. Such imbalance inD(J)leads to the confounding bias that the representations of IandVtend to be supportive , and blind the conventional training on P(VjI ) . 2019 ) .", "entities": []}, {"text": "Court \u2019s view can be regarded as the interpretation for the sentence of a case .", "entities": []}, {"text": "Being an important portion of verdict , court \u2019s view is dif\ufb01cult to generate due to its logic reasoning in the content .", "entities": []}, {"text": "Therefore court \u2019s view generation is regarded as one of the most critical functions in a legal assistant system .", "entities": []}, {"text": "Court \u2019s view consists of two main parts , including the judgment and the rationales , where the judgment is a response to the plaintiff \u2019s claims in civil cases or charges in criminal cases , and the rationales are summarized from the fact description to derive and explain the judgment .", "entities": []}, {"text": "Recently , Ye et al .", "entities": []}, {"text": "( 2018 ) investigated the problem of court \u2019s view generation for the criminal cases , but it focused on the generation of rationales in the court \u2019s view based on the given criminal charge and fact description of a case .", "entities": []}, {"text": "Such an experimental scenario is not applicable to the practical situation since the rationales should be concluded before reaching the \ufb01nal judgment .", "entities": []}, {"text": "Moreover , dif-", "entities": []}, {"text": "764 PLAINTIFF\u2019SCLAIMTheplaintiffAclaimedthatthedefendantBshouldreturntheloanof$29,500PrincipleClaimandthecorrespondinginterestInterestClaim .", "entities": []}, {"text": "FACTDESCRIPTIONAfterthehearing , thecourtheldthefactsasfollows : ThedefendantBborrowed$29,500fromtheplaintiffA , andagreedtoreturnafteronemonth .", "entities": []}, {"text": "Aftertheloanexpired , thedefendantfailedtoreturnFact . COURT\u2019SVIEWThecourtconcludedthattheloanrelationshipbetweentheplaintiffAandthedefendantBisvalid .", "entities": []}, {"text": "ThedefendantfailedtoreturnthemoneyontimeRa tiona le .", "entities": []}, {"text": "Therefore , theplaintiff\u2019sclaimonprinciplewassupportedAcceptanceaccordingtolaw .", "entities": []}, {"text": "Thecourtdidnotsupporttheplaintiff\u2019sclaimoninterestRejectionbecausetheevidencewasinsufficientRationale .", "entities": []}, {"text": "Figure 2 : An example of plaintiff \u2019s claim , fact description , and court \u2019s view from a legal document in a civil case .", "entities": []}, {"text": "The judgment is non - support since there exists a rejection on one of the plaintiff \u2019s claims in the court \u2019s view .", "entities": []}, {"text": "ferent from the criminal cases , in civil cases , the judgment depends not only on the facts recognized but also on the claims that the plaintiff declared .", "entities": []}, {"text": "In this paper , we focus on the problem of automatically generating the court \u2019s view in civil cases by injecting the plaintiff \u2019s claim and fact description , as shown in Fig .", "entities": []}, {"text": "2 . In such a context , the problem of the court \u2019s view generation can be formulated as a text - to - text natural language generation ( NLG ) problem , where the input is the plaintiff \u2019s claim and the fact description , and the output is the corresponding court \u2019s view that contains the judgment and the rationales1 .", "entities": []}, {"text": "Although classical text generative models ( e.g. , sequence - tosequence model Sutskever et al . , 2014 , attentionbased model , and pointer - generator networks See et al . , 2017 ) have been applied to many text generation tasks , yet , in the task of the court \u2019s view generation , such techniques can not be simply applied for the following reasons : ( 1 ) There exists \u201c no claim , no trial \u201d principle in civil legal systems : The judgment in the real court \u2019s view is the response to the claims declared by the plaintiff , where its rationales summarize the corresponding facts .", "entities": [[39, 41, "TaskName", "text generation"]]}, {"text": "In other words , there exists a correspondence relationship between the input ( claims and facts ) and the generated text ( court \u2019s view ) .", "entities": []}, {"text": "For example , the plaintiff \u2019s claims shown in Fig .", "entities": []}, {"text": "2 mentioned the principal and the interest , respectively .", "entities": []}, {"text": "Hence , the court \u2019s view of this case would and might only focus on the facts about the principal and the interest .", "entities": []}, {"text": "( 2 ) The imbalance of judgment in civil cases : The distribution of judgment results of civil cases is very imbalanced .", "entities": []}, {"text": "For example , over 76 % of cases were supported in private lending , which is the most frequent category in civil cases .", "entities": []}, {"text": "Such an imbalance of judgment would blind the training 1Since the claims are various , for simpli\ufb01cation , the judgment of a civil case is de\ufb01ned as supported if all its claims are accepted , otherwise , de\ufb01ned as non-supported.of the model by focusing on the supported cases while ignoring the non - supported cases , leading to incorrect judgment generation of court \u2019s view .", "entities": []}, {"text": "From the perspective of causality ( Pearl , 2009 ; Kuang et al . , 2020 ) , the imbalance of judgment reveals the confounding bias induced by the data generation mechanism that plaintiffs sue when they have a high probability to be supported .", "entities": []}, {"text": "Such imbalanced data would cause the learned representation of both inputs ( claims and recognized facts ) and output ( court \u2019s view ) to be supported , leading to confounding bias between inputs and output , and blinding the training process of conventional NLG models as we demonstrated in Fig .", "entities": []}, {"text": "1 . To address these challenges , we propose an Attentional and Counterfactual based Natural Language Generation ( AC - NLG ) method by jointly optimizing a claim - aware encoder , a pair of counterfactual decoders to generate judgmentdiscriminative court \u2019s views ( both supportive and non - supportive views ) and a synergistic judgment predictive model .", "entities": []}, {"text": "Speci\ufb01cally , the claim - aware encoder is designed to represent the fact description which emphasizes on the declared claims .", "entities": []}, {"text": "The counterfactual decoder is inspired by the backdoor adjustment in causal inference ( Pearl et al . , 2016 ; Kuang et al . , 2020 ) to address the confounding bias and the imbalance problem in judgment .", "entities": [[10, 12, "MethodName", "causal inference"]]}, {"text": "To determine the judgment result of each case , a judgment predictive model is jointly learned with the two decoders and decides which output to be selected as the \ufb01nal generated court \u2019s view .", "entities": []}, {"text": "We validate the effectiveness of our AC - NLG method with extensive experiments on real legal documents .", "entities": []}, {"text": "Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics .", "entities": []}, {"text": "Since legal AI is a sensitive \ufb01eld , we make ethical discussion in the penultimate section(Sec . 6 ) .", "entities": []}, {"text": "The main contributions of this paper can be sum-", "entities": []}, {"text": "765marized as follows : \u000fWe investigate the problem of de - biased court \u2019s view generation in civil cases from a causal perspective , considering the issue of confounding bias from judgment imbalance .", "entities": []}, {"text": "\u000fWe propose a novel AC - NLG model to jointly optimize a claim - aware encoder and a pair of counterfactual decoders for generating a judgment - discriminative court \u2019s view by incorporating with a judgment predictive model .", "entities": []}, {"text": "\u000fWe construct a dataset based on raw civil legal documents , where each case is objectively split into three parts : plaintiff \u2019s claim , fact description , and court \u2019s view with human annotation on the judgment .", "entities": []}, {"text": "To motivate other scholars to investigate this novel but important problem , we make the experiment dataset publicly available2 .", "entities": []}, {"text": "\u000fWe validate the superior performance of the proposed method with extensive experiments .", "entities": []}, {"text": "Our method can be applied to other natural language generation tasks with confounding bias or data imbalance .", "entities": []}, {"text": "2 Related Work 2.1 Legal Assistant", "entities": []}, {"text": "In recent years , many researchers from both law and computer science \ufb01elds have been exploring the potential and methods to perform judicial decisions and auxiliary tasks , aiming at helping lawyers and lower court judges .", "entities": []}, {"text": "In recent work , judicial intelligence is also applied to various tasks of natural language processing .", "entities": []}, {"text": "Since most of the legal documents appear in textual form , many NLP technologies have been brought into the legal \ufb01eld to improve the ef\ufb01ciency of legal work .", "entities": []}, {"text": "Charge prediction is a common task of judgment prediction , considered as a text classi\ufb01cation problem ( Lin et al . , 2012 ; Zhong et", "entities": []}, {"text": "al . , 2018 ; Hu et al . , 2018 ; Jiang et al . , 2018 ; Chalkidis et al . , 2019 ) .", "entities": []}, {"text": "Besides , there are also works on legal questions classi\ufb01cation ( Xiao et al . , 2017 ) , law articles recommendation ( Chen et al . , 2019 ) , controversy focus mining ( Duan et al . , 2019 ) and relevant case retrieval ( Chen et al . , 2013 ) .", "entities": []}, {"text": "Ye et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) explored the court \u2019s view generation in criminal cases , where the input is only fact description , and the court \u2019s view generation is conditioned on the known judgment results , which is not applicable in real cases .", "entities": []}, {"text": "2https://github.com/wuyiquan/AC-NLG2.2 Natural Language Generation Our task aims at generating the court \u2019s view based on the plaintiff \u2019s claim and the fact description , which can be regarded as a NLG task .", "entities": []}, {"text": "NLG has been widely studied and applied to many tasks , such as machine translation ( Wu et al . , 2016 ) , question answering ( McCann et al . , 2018 ; Bagchi and Wynter , 2013 ) and text summarization ( Rush et al . , 2015 ) .", "entities": [[13, 15, "TaskName", "machine translation"], [24, 26, "TaskName", "question answering"], [41, 43, "TaskName", "text summarization"]]}, {"text": "The recent success of sequence - to - sequence models ( Sutskever et al . , 2014 ) , in which recurrent neural networks ( RNNs ) reading and generating text simultaneously , has made the generation task feasible .", "entities": []}, {"text": "Bahdanau et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2014 ) \ufb01rstly applied the attention mechanism into the NLG task .", "entities": []}, {"text": "See et al .", "entities": []}, {"text": "( 2017 ) proposed a Pointer - Generator Networks ( PGN ) , which can effectively solve the OutOf - V ocabulary ( OOV ) problem .", "entities": []}, {"text": "Although the previous work on NLG can produce \ufb02uent sentences , they are struggling to be directly applied to our task since a good court \u2019s view considers not only the text \ufb02uency but also the logical correctness .", "entities": []}, {"text": "2.3 Causal Inference Causal Inference ( Pearl , 2009 ; Kuang et al . , 2020 ) is a powerful statistical modeling tool for explanatory analysis by removing confounding bias in data .", "entities": [[1, 3, "MethodName", "Causal Inference"], [3, 5, "MethodName", "Causal Inference"]]}, {"text": "That bias might bring a spurious correlation or confounding effect among variables .", "entities": []}, {"text": "Recently , many methods have been proposed to remove confounding bias in the literature of causal inference , including do - operation based on structure causal model ( Pearl , 2009 ) and counterfactual outcome prediction based on potential outcome framework ( Imbens and Rubin , 2015 ) .", "entities": [[15, 17, "MethodName", "causal inference"]]}, {"text": "With dooperation , the backdoor adjustment ( Pearl et al . , 2016 ) have been proposed for data de - bias .", "entities": []}, {"text": "In this paper , we sketch the causal structure model of our problem , as shown in Fig .", "entities": []}, {"text": "1 , and adopt backdoor for confounding bias reduction .", "entities": []}, {"text": "3 Problem Formulation In this work , we focus on the problem of the court \u2019s view generation in civil cases , where the input is the plaintiff \u2019s claim and the fact description , and the output is the corresponding court \u2019s view .", "entities": []}, {"text": "We formulate our problem with the de\ufb01nition of the plaintiff \u2019s claim , the fact description , and the court \u2019s view , as shown in Fig .", "entities": []}, {"text": "2 . Plaintiff \u2019s claim ( C ) is a descriptive sentence that depicts the claims from the plaintiff .", "entities": []}, {"text": "In a civil case , it often appears multiple claims from the", "entities": []}, {"text": "766 Claim - aware Encoder\ud835\udc57Attention LayerAttention LayerCounterfactual Decoder\ud835\udc2f\ud835\udc27(j=0)\ud835\udc2f\ud835\udc2c(j=1)JudgmentPredictor = addc = claimsf = facts\ud835\udc21\ud835\udc31=hidden states of x\ud835\udc2f\ud835\udc2c \t = supported view\ud835\udc2f\ud835\udc27=non - supported view\ud835\udc2c\ud835\udc2f\ud835\udc31=decode states of \ud835\udc2f\ud835\udc31 \t j = judgmentFC LayerPointer GeneratorPointer Generator\ud835\udc64#\ud835\udc97\ud835\udc8f\ud835\udc64%\ud835\udc97\ud835\udc8f\ud835\udc64&\ud835\udc97\ud835\udc8f\ud835\udc64#\ud835\udc97\ud835\udc94\ud835\udc64%\ud835\udc97\ud835\udc94\ud835\udc64&\ud835\udc97\ud835\udc94</s>\ud835\udc64%\ud835\udc97\ud835\udc8f Sigmoid</s>\ud835\udc64&\ud835\udc97\ud835\udc94 \u2026 \u2026 \ud835\udc2c\ud835\udc2f\ud835\udc27\ud835\udc2c\ud835\udc2f\ud835\udc2cs%\ud835\udc2f\ud835\udc2cs#\ud835\udc2f\ud835\udc27 \ud835\udc64#(\ud835\udc64%(\ud835\udc64&(\ud835\udc64)(\ud835\udc64*(Attention Layer\ud835\udc64#+\ud835\udc64%+\ud835\udc64&+\ud835\udc64,+ \ud835\udc1f\ud835\udc1c\ud835\udc21\ud835\udc1f\ud835\udc21\ud835\udc1c\u210e% ( \u2026 \u2026 Figure 3 : The architecture of AC - NLG , which consists of a claim - aware encoder , a pair of counterfactual decoders , and a judgment predictor .", "entities": []}, {"text": "plaintiff .", "entities": []}, {"text": "For example , the plaintiff \u2019s claim demonstrated in Fig .", "entities": []}, {"text": "2 contains the principal claim and the interest claim .", "entities": []}, {"text": "Here , we denote the plaintiff \u2019s claim in a case as a sentence form c = fwc tgm t=1 , wherewc trepresents one word , and mis the number of words in plaintiff \u2019s claim .", "entities": []}, {"text": "Fact description ( F ) is also a descriptive sentence , which describes the identi\ufb01ed facts ( relevant events that have happened ) in a case , as Fig .", "entities": []}, {"text": "2 shows .", "entities": []}, {"text": "Here , we denote the fact description in a case as f = fwf tgn t=1 , wherenis the length .", "entities": [[14, 15, "MethodName", "tgn"]]}, {"text": "Court \u2019s view ( V ) contains two main components , judgment and rationales , where the judgment is to respond the plaintiff \u2019s claims , and the rationales are the claim - related summarization on the fact description to determine and interpret the judgment .", "entities": [[34, 35, "TaskName", "summarization"]]}, {"text": "Here , we denote the court \u2019s view as v = fwv tgl t=1 , wherelis the length .", "entities": []}, {"text": "Moreover , we use a variable jto denote the judgment in the court \u2019s view .", "entities": []}, {"text": "For simplicity , we set j= 1to denote supported judgment ( all the claims are judged to be accepted ) , and j= 0to denote non - supported judgment .", "entities": []}, {"text": "Then , the problem of court \u2019s view generation can be denoted as follow : Problem 1 ( Court \u2019s View Generation ) .Given", "entities": []}, {"text": "the plaintiff \u2019s claim c =", "entities": []}, {"text": "fwc tgm t=1and the fact", "entities": []}, {"text": "description f = fwf tgn t=1 , our task is to generate the court \u2019s view v = fwv tgl t=1 .", "entities": [[4, 5, "MethodName", "tgn"]]}, {"text": "4 Method In this section , we \ufb01rst introduce the effect of mechanism confounding bias on the court \u2019s view generation and propose a backdoor - inspired method to eliminate that bias .", "entities": []}, {"text": "Then , we describe our Attentional and Counterfactual based Natural LanguageGeneration ( AC - NLG ) model in detail .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "3 shows the overall framework .", "entities": []}, {"text": "4.1 Backdoor Adjustment As shown in Fig .", "entities": []}, {"text": "1 , the confounding bias from the data generation mechanism would blind the conventional training on P(VjI ) , and current sequenceto - sequence models struggle to solve this problem .", "entities": []}, {"text": "Here , we see through why these models fail mathematically .", "entities": []}, {"text": "For a certain case , given the input I= ( c;f ) , using Bayes rule , we would train the model to generate the court \u2019s view Vas follow : P(VjI )", "entities": []}, {"text": "= X jP(VjI;j)P(jjI ) ( 1 ) If the supported cases dominate our training data , e.g. ,P(j= 1jI)\u00191 .", "entities": []}, {"text": "Thus , P(VjI)degrades toP(VjI;j= 1 ) , which would ignore the representation of non - supported cases , leading to the learned representations of inputs Iand outputV tend to be supported .", "entities": []}, {"text": "Thus , the model tends to build a strong connection between inputs and the supported court \u2019s view , even for the cases that are non - supported .", "entities": []}, {"text": "In this way , the representation of inputIis contaminated by the confounding bias fromI D(J)!V. Backdoor adjustment is a main de - confounding technique in causal inference ( Pearl et al . , 2016 ; Pearl , 2009 ) .", "entities": [[25, 27, "MethodName", "causal inference"]]}, {"text": "De - confounding seeks the exact causal effect of one variable on another , which appeals for our court \u2019s view generation task since the court \u2019s view should be faithful only to the content of the plaintiff \u2019s claims and fact descriptions .", "entities": []}, {"text": "The backdoor adjustment makes a do - operation onI , which promotes the posterior probability from passive observation to active intervention .", "entities": []}, {"text": "767The backdoor adjustment addresses the confounding bias by computing the interventional posterior P(Vjdo(I))and controlling the confounder as : P(Vjdo(I ) )", "entities": []}, {"text": "= X jP(VjI;j)P(j)(2 ) In our problem , the variable jis a binary variable ( support or non - support ) , hence , P(Vjdo(I ) )", "entities": []}, {"text": "= P(VjI;j= 0)P(j= 0 )", "entities": [[3, 4, "DatasetName", "0"]]}, {"text": "+ P(VjI;j= 1)P(j= 1)(3 )", "entities": []}, {"text": "The main difference between traditional posterior in Eq . 1 and interventional posterior in Eq . 2 is that P(jjI)is changed to P(j ) .", "entities": []}, {"text": "Since the backdooor adjustment help to cut the dependence between D(J)andI , we can eliminate the confounding bias from data generation mechanism and learn a interventional model for de - biased court \u2019s view generation .", "entities": []}, {"text": "4.2 Backdoor In Implementation As shown in Fig .", "entities": []}, {"text": "3 , to optimize Eq .", "entities": []}, {"text": "3 , we use a pair of counterfactual decoders to learn the likelihood P(VjI;j)for eachj .", "entities": []}, {"text": "At inference , we propose to use a predictor to approximate P(j ) .", "entities": []}, {"text": "Note that our implementation on backdoor - adjustment can be easily applied for multi - valued confounding with multiple counterfactual decoders .", "entities": []}, {"text": "4.3 Model Architecture Our model is conducted in a multi - task learning manner which consists of a shared encoder , a predictor , and a pair of counterfactual decoders .", "entities": [[9, 13, "TaskName", "multi - task learning"]]}, {"text": "The predictor and the decoders take the output of the encoder as input .", "entities": []}, {"text": "Our model looks like SHAPED(Zhang et al . , 2018 ) ( several decoders with a classi\ufb01er ) , but the motivations and mechanisms behind the model are different .", "entities": []}, {"text": "Claim - aware Encoder Intuitively , the plaintiff \u2019s claimcand the fact description fare sequences of words .", "entities": []}, {"text": "Therefore , the encoder \ufb01rstly transforms the words to embeddings .", "entities": []}, {"text": "Then the embedding sequences are fed to the Bi - LSTM , producing two sequences of hidden states hc , hfcorresponding to the plaintiff \u2019s claim and the fact description respectively .", "entities": [[10, 11, "MethodName", "LSTM"]]}, {"text": "After that , we use a claim - aware attention mechanism to fuse hcandhf .", "entities": []}, {"text": "For each hidden state hf iinhf , ei kis its attention weight on hc k , and the attention distribution qiis calculated as follow : ei k = vTtanh(Wchc k+Wfhf i+battn ) ( 4)qi = softmax ( ei ) ( 5 ) wherev , Wc , Wf , battnare learnable parameters .", "entities": [[26, 28, "HyperparameterName", "k ="], [35, 36, "MethodName", "softmax"]]}, {"text": "The attention distribution can be regarded as the importance of each word in the plaintiff \u2019s claim for a word in fact description .", "entities": []}, {"text": "Next , the new representation of fact description is produced as follows : hf\u0003 i = hf i+X kqi khc k ( 6 ) After feeding to another Bi - LSTM layer , we get the claim - aware representation of fact h. Judgment Predictor Given the claim - aware representation of fact h , the judgment predictor produces the probability of support Psupthrough a fully connected layer and a sigmoid operation .", "entities": [[30, 31, "MethodName", "LSTM"]]}, {"text": "The prediction result jis obtained as follow : j= ( 1Psup>0:5 0Psup<= 0:5(7 ) where 1 means support , and 0 means non - support .", "entities": [[20, 21, "DatasetName", "0"]]}, {"text": "Counterfactual Decoder To eliminate the effect of data bias , here we use a pair of counterfactual decoders , which contains two decoders , one is for supported cases , and the other is for non - supported cases .", "entities": []}, {"text": "The two decoders have the same structure but aim to generate the court \u2019s view with different judgments .", "entities": []}, {"text": "We name them as counterfactual decoders because every time there is only one of the two generated court \u2019s views correct .", "entities": []}, {"text": "Still , we apply the attention - mechanism .", "entities": []}, {"text": "At each step t , given the encoder \u2019s output h , and the decode state st , the attention distribution atis calculated the same way asqiin Eq . 5 , but with different parameters .", "entities": [[21, 22, "DatasetName", "atis"]]}, {"text": "The context vector h\u0003 tis then a weighted sum of h : h\u0003 t = X iat ihi ( 8) The context vector h\u0003 t , which can be regarded as a representation of the input for this step , is concatenated with the decode state stand fed to linear layers to produce the vocabulary distribution pvocab : pvocab = softmax ( V0(V[st;h\u0003 t ] )", "entities": [[60, 61, "MethodName", "softmax"]]}, {"text": "+ b ) + b0)(9 )", "entities": []}, {"text": "whereV , V0,b , b0are all learnable parameters .", "entities": []}, {"text": "Then we add a generation probability ( See et al . , 2017 ) to solve the OOV problem .", "entities": []}, {"text": "Given the context", "entities": []}, {"text": "768h\u0003 t , the decode state stand the decoder \u2019s input ( the word embedding of the previous word ) xt , the generation probability pgencan be calculated : Pgen=\u001b(wT", "entities": []}, {"text": "h\u0003h\u0003 t+wT", "entities": []}, {"text": "sst+wT xxt+bptr)(10 ) wherewh\u0003,ws , wxandbptrare learnable , and \u001bis the sigmoid function .", "entities": []}, {"text": "The \ufb01nal probability for a wordwin time step is obtained : P(w ) = Pgen\u0003pvocab(w ) + ( 1\u0000Pgen)X i : wi = wat i ( 11 ) We introduce how to alienate the two decoders in the training part .", "entities": []}, {"text": "Training For predictor , we use cross - entropy as the loss function : Lpred=\u0000^jlog(Psup)\u0000(1\u0000^j)log(1\u0000Psup ) ( 12 ) where ^jis the real judgment .", "entities": [[11, 12, "MetricName", "loss"]]}, {"text": "For decoders , the previous word in training is the word in real court \u2019s view , and the loss for timestep tis the negative log - likelihood of the target word w\u0003 t : Lt=\u0000logP(w\u0003 t ) ( 13 ) and the overall generation loss is : Lgen=1", "entities": [[19, 20, "MetricName", "loss"], [25, 28, "MetricName", "log - likelihood"], [45, 46, "MetricName", "loss"]]}, {"text": "TTX t=0Lt ( 14 ) where T is the length of real court \u2019s view .", "entities": []}, {"text": "Since we aim to make the two decoders generate two different court \u2019s views , we take a mask operation when calculating the loss of each decoder .", "entities": [[23, 24, "MetricName", "loss"]]}, {"text": "The exact loss for the support decoder is : Lsup= ( Lgen^j= 1 0 ^j= 0(15 ) the loss for the non - support decoder Lnsup is obtained by the opposite way .", "entities": [[2, 3, "MetricName", "loss"], [13, 14, "DatasetName", "0"], [18, 19, "MetricName", "loss"]]}, {"text": "Thus , the total loss is : Ltotal = Lsup+Lnsup+\u0015Lpred ( 16 ) where we set \u0015to 0.1 in our model .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "Inference In inference , the counterfactual decoders apply beam search to generate two court \u2019s views , and one of them will be selected as the \ufb01nal output , depending on the result of the predictor j. Table 1", "entities": []}, {"text": ": Statistics of private lending dataset Type Result # Supported case 51087(76 % ) #", "entities": []}, {"text": "Non - supported case 15817(24 % ) Avg . # tokens in claim 77.9 Avg . # tokens in fact 158.0 Avg . # tokens in court \u2019s view 194.4 5 Experiments 5.1 Data Construction Since there is no publicly available court \u2019s view generation dataset in civil cases , we build a dataset based on raw civil legal documents3 .", "entities": []}, {"text": "Speci\ufb01cally , we choose private lending , which is the most frequent category in civil cases , to construct the dataset .", "entities": []}, {"text": "We process the legal documents as following steps : 1 ) Split legal documents into three parts : plaintiff \u2019s claim , facts description , and court \u2019s view , which can be objectively split by keywords ( subtitles ) .", "entities": []}, {"text": "2 ) Human annotation .", "entities": []}, {"text": "We employ experts with legal backgrounds to annotate the judgment ( de\ufb01ned in Sec . 3 ) on the court \u2019s view .", "entities": []}, {"text": "3 ) Annotation veri\ufb01cation .", "entities": []}, {"text": "We use random sampling test to ensure that the annotation accuracy is over 95 % .", "entities": [[10, 11, "MetricName", "accuracy"]]}, {"text": "After that , we get the dataset as shown in Tab .", "entities": []}, {"text": "1 .", "entities": []}, {"text": "We randomly separate the dataset into a training set , a validation set , and a test set according to a ratio of 8 : 1 : 1 , the ratio of supported cases is about 76 % in each set .", "entities": []}, {"text": "5.2 Baselines We implement the following baselines for comparison : \u000fS2SSequence - to - sequence model ( Sutskever et al . , 2014 ) is a classic model for NLG task .", "entities": []}, {"text": "We concatenate the plaintiff claims and facts descriptions as input .", "entities": []}, {"text": "\u000fPGN Pointer Generator Networks ( See et al . , 2017 ) utilizes a pointer network to solve the outof - vocabulary ( OOV ) problem , which is essential for the court \u2019s view generation since many nouns occur there .", "entities": [[14, 16, "MethodName", "pointer network"]]}, {"text": "Oversampling is a common method to alleviate data imbalance .", "entities": []}, {"text": "We oversample the non - supported cases so that the ratio between supported cases and non - supported cases become 1 : 1 . \u000fS2SwS Apply oversampling to S2S. 3https://wenshu.court.gov.cn/", "entities": []}, {"text": "769Table 2 : Results on court \u2019s view generation .", "entities": []}, {"text": "MethodROUGE BLEU BERT SCORE R-1 R-2 R - L B-1 B-2 B - N p r f1 S2S 54.0 35.7 48.3 65.0 57.6 50.5 89.6 89.5 89.6 S2SwS 51.5 32.0 45.0 63.3 55.6 47.9 83.8 88.8 86.2 PGN 53.3 37.1 48.8 62.0 56.1 50.0 94.0 91.2 92.6 PGNwS 53.2 36.0 48.0 63.1 56.7 50.2 95.7 94.0 94.8 AC - NLGw / oBA", "entities": [[1, 2, "MetricName", "BLEU"], [2, 3, "MethodName", "BERT"]]}, {"text": "54.1 38.1 49.9 61.8 55.9 49.9 93.6 91.9 92.8 AC - NLGw / oCA 53.7 36.7 49.1 62.1 56.0 49.7 94.5 92.6 93.5 AC - NLGwS 53.7 36.4", "entities": []}, {"text": "48.5 62.8 56.5 50.0 94.0 92.1 93.0 AC - NLG 55.1 38.6 50.8 63.2 57.1 51.0 96.5 94.6 95.5 Table 3 : Results on judgment prediction .", "entities": []}, {"text": "MethodPrediction Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "Support Non - support p r f1 p r f1 w / oD", "entities": []}, {"text": "72.1 81.0 76.3 56.9 44.3 49.8 w / oCA 92.0 97.2 94.5 85.6 66.0 74.5 wS 86.0 94.3 90.0 62.8 38.6 47.8 AC - NLG 93.4 95.9 94.6 81.5 72.9 76.9 Table 4 : Results of human evaluation .", "entities": []}, {"text": "MethodJudgmentRational Flu . Support Non - support PGN 3.34 1.78 3.11 3.41 AC - NLG 3.52 3.24 3.25 3.50 \u000fPGNwS Apply oversampling to PGN .", "entities": []}, {"text": "\u000fAC - NLGwS Apply oversampling to ACNLG .", "entities": []}, {"text": "We do ablation experiments as follows : \u000fAC - NLGw / oD", "entities": []}, {"text": "We remove the decoder and train the remaining model ( encoder and predictor ) as a classi\ufb01cation task for judgment prediction .", "entities": []}, {"text": "\u000fAC - NLGw / oBA", "entities": []}, {"text": "We remove the backdoor adjustment by replacing the pair of counterfactual decoders and predictor with a single decoder , but keep the claim - aware attention mechanism .", "entities": []}, {"text": "\u000fAC - NLGw / oCA We remove the claim - aware attention , and concatenate the claims and the facts instead .", "entities": []}, {"text": "5.3 Metrics ROUGE4is a set of metrics used in the NLP task .", "entities": []}, {"text": "We keep the results of ROUGE-1 , ROUGE-2 , and ROUGE - L. ROUGE-1 and ROUGE-2 refer to the overlap of unigram and bigram between the generated and reference documents , respectively .", "entities": [[5, 6, "MetricName", "ROUGE-1"], [7, 8, "MetricName", "ROUGE-2"], [13, 14, "MetricName", "ROUGE-1"], [15, 16, "MetricName", "ROUGE-2"]]}, {"text": "ROUGE - L is a Longest Common Subsequence ( LCS ) based statistics .", "entities": [[0, 3, "MetricName", "ROUGE - L"]]}, {"text": "BLEU5(Papineni et", "entities": []}, {"text": "al . , 2002 ) is a method of au4https://pypi.org/project/rouge/ 5http://www.nltk.org/api/nltk.test .", "entities": []}, {"text": "unit.translate.htmltomatic text - generation evaluation that highly correlates with human evaluation .", "entities": []}, {"text": "We use BLEU-1 , BLEU-2 to evaluate from the perspectives of unigram , bigram .", "entities": [[2, 3, "MetricName", "BLEU-1"]]}, {"text": "BLEU - N is an average of BLEU-1 , BLEU2 , BLEU-3 , BLEU-4 .", "entities": [[0, 1, "MetricName", "BLEU"], [7, 8, "MetricName", "BLEU-1"], [13, 14, "MetricName", "BLEU-4"]]}, {"text": "BERT SCORE6(Zhang et", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "al . , 2019 ) computes a similarity score by using contextual embedding of the tokens .", "entities": []}, {"text": "We calculate the precision ( p),recall ( r ) andf1 - score to evaluate the information matching degree .", "entities": []}, {"text": "Accuracy of judgment prediction To evaluate the performance of the predictor , we calculate the precision ( p),recall ( r ) and , f1 - score of supported and non - supported cases , respectively .", "entities": [[0, 1, "MetricName", "Accuracy"], [23, 26, "MetricName", "f1 - score"]]}, {"text": "Human Evaluation We conduct a human evaluation to better analyze the quality of the generated court \u2019s view .", "entities": []}, {"text": "First , we randomly sample 500 test cases , where the ratio of the supported and nonsupported cases are 1:1 .", "entities": []}, {"text": "For each case , we present the generated court \u2019s views from each method7 with the ground truth to 5 human annotators with legal backgrounds .", "entities": []}, {"text": "The evaluation is conducted following three perspectives : ( 1 ) Judgment level .", "entities": []}, {"text": "Annotators are asked to give a score ( 1 - 5 ) on the judgment in the generated court \u2019s view .", "entities": []}, {"text": "1 for totally wrong and 5 for totally correct .", "entities": []}, {"text": "( 2 ) Rational level .", "entities": []}, {"text": "Annotators are asked to give a score ( 1 - 5 ) on the rationals in the generated court \u2019s view .", "entities": []}, {"text": "1 for the worst and 5 for the best .", "entities": []}, {"text": "( 3 ) Fluency level .", "entities": []}, {"text": "Annotators are asked to give a score ( 1 - 5 ) on the \ufb02uency of the generated court \u2019s view .", "entities": []}, {"text": "1 for the worst and 5 for the best .", "entities": []}, {"text": "5.4 Experimental Results Tab . 2 demonstrates the results of court \u2019s view generation with ROUGE , BLEU , and BERT SCORE .", "entities": [[17, 18, "MetricName", "BLEU"], [20, 21, "MethodName", "BERT"]]}, {"text": "Also , we report the results on the judgment prediction of our predictor component with precision 6https://github.com/Tiiiger/bert_score 7We shuf\ufb02e all the results to be fair for all the methods", "entities": []}, {"text": "770 PLAINTIFF\u2019SCLAIMThedefendantBreturntheloanof$495,000.ThedefendantCreturntheloantogether .", "entities": []}, {"text": "FACTDESCRIPTIONAfterthehearing , thecourtheldthefactsasfollows : FromNovember20,2010toMarch23,2011,thedefendantBsuccessivelyborrowedatotalof$495,000fromtheplaintiffAandissuedfourseparateborrowings . ThedefendantBhasnotrepaidtheaboveloan .", "entities": []}, {"text": "PGNThecourtconcludedthat : TheprivatelendingrelationshipbetweenPlaintiffAandDefendantB , wherethesubjectwasappropriate , thecontentwaslegitimate , andthemeaningwastrue , shouldbeconfirmedtobelegalandvalid .", "entities": []}, {"text": "Thetwosidesdidnotagreedinwritingonaloanperiod , sothedefendantshouldreturntheplaintiff'sloaninatimelymannerwithinareasonableperiodaftertheplaintiffurged . Thedefendant\u2019sfailuretoreturntheloantimelyconstitutedabreachofcontractandshouldbearcorrespondingcivilliabilities .", "entities": []}, {"text": "Therefore , the plaintiff 's claim was reasonable and legal , and the court supported it Acceptance .", "entities": []}, {"text": "AC - NLGThecourtconcludedthatthesubjectoftheprivatelendingrelationshipbetweenPlaintiffAandDefendantBwasqualified , thecontentwaslegal , andthemeaningwastrue .", "entities": []}, {"text": "Itshouldbedeemedvalid .", "entities": []}, {"text": "Thetwosidesdidnotagreedinwritingonaloanperiod , thedefendantshallreturntheloanwithinareasonableperiodaftertheplaintiffurged .", "entities": []}, {"text": "The plaintiff \u2019s claim requesting the defendant to return the loan of $ 495,000 was in compliance with the law and the court supported it Acceptance .", "entities": []}, {"text": "However , the court did not support the claim requesting the defendant C to bear the guarantee liability because   the evidence was insufficient Rejection .", "entities": []}, {"text": "REALThecourtconcludedthat : ThesubjectoftheprivatelendingrelationshipbetweenPlaintiffAandDefendantBwasqualified , thecontentwaslegal , andthemeaningwastrue .", "entities": []}, {"text": "Itshouldbedeemedvalid .", "entities": []}, {"text": "Defendantshouldrepaytheplaintiff'sloanwithinareasonableperiodaftertheplaintiffurged .", "entities": []}, {"text": "Therefore , Defendant B should bear the civil liability of returning the plaintiff 's loan of $ 495,000 and paying overdue interest Acceptance .", "entities": []}, {"text": "The court did not support the plaintiff \u2019s claim requesting the defendant C to return the loan together because the evidence was insufficientRejection .", "entities": []}, {"text": "DefendantBfailedtoappearincourtafterbeinglegallysummonedbythecourt .", "entities": []}, {"text": "Figure 4 : Case study .", "entities": []}, {"text": "( p),recall ( r ) , andf1 - score ( f1 ) in Tab .", "entities": []}, {"text": "3 .", "entities": []}, {"text": "To demonstrate that our method is de - biased on judgment generation , we report the result of human evaluation in Tab .", "entities": []}, {"text": "4 . Results of court \u2019s view generation : From Tab . 2 , we can conclude that : ( 1 ) S2S tends to repeat words , which makes it get high BLEU but low BERT SCORE .", "entities": [[33, 34, "MetricName", "BLEU"], [36, 37, "MethodName", "BERT"]]}, {"text": "( 2 ) Oversampling strategy does n\u2019t bene\ufb01t the models , hence , it can not address the confounding bias .", "entities": []}, {"text": "( 3 ) With claim - aware encoder and backdoor - inspired counterfactual decoders , our AC - NLG achieves better performance on court \u2019s view generation compared with baselines .", "entities": []}, {"text": "( 4 ) The performance gap between AC - NLGw / oCA and AC - NLG demonstrates the effectiveness of our proposed claim - aware encoder , and the gap between AC - NLGw / oBA andAC - NLG illustrates the superiority of our counterfactual decoders .", "entities": []}, {"text": "Results of judgment prediction : From Tab . 3 , we have the following observations : ( 1 ) The counterfactual decoders in our model can signi\ufb01cantly eliminate the confounding bias , hence , achieve remarkable improvement on the non - supported cases , for example boosting f1from 49.8 % to 76.9 % .", "entities": []}, {"text": "( 2 ) The proposed claim - aware encoder has a limited effect on judgment prediction since it \u2019s designed for improving the quality of generation as shown in Tab .", "entities": []}, {"text": "2 . ( 3 ) Still , oversampling brings no improvement to the model .", "entities": []}, {"text": "Results of human evaluation : From Tab . 4 , we have the following observations : ( 1 ) due to the confounding bias in data , the performance of judgment generation in PGN is poor for non - supported cases , and its performance gap between supported and non - supported cases is huge ( 1.56 ) .", "entities": []}, {"text": "( 2 ) Bydebiasing with backdoor - inspired counterfactual decoders , our AC - NLG signi\ufb01cantly improves the performance of judgment generation , especially for non - supported cases , and achieves a smaller performance gap ( only 0.28 ) between the supported and non - supported cases .", "entities": []}, {"text": "( 3 ) With a claim - aware encoder , our AC - NLG also achieves better performance on the generation of rational and generated court \u2019s view \ufb02uency .", "entities": []}, {"text": "( 4 ) Kappa coef\ufb01cient \u0014 is more than 0.8 between any two judges , which proves the validation of human evaluation .", "entities": []}, {"text": "Overall , thanks to the proposed claim - aware encoder , counterfactual decoders , and a synergistic judgment predictor , our model achieves better performance than single - task baselines on the task of judgment prediction , judgment generation in court \u2019s view and court \u2019s view generation .", "entities": []}, {"text": "5.5 Experiment Details We use Gensim ( \u02c7Reh\u02dau\u02c7rek and Sojka , 2010 ) with a large - scale generic corpus to train a language model as the pre - trained model , then use it to initialize the word embeddings , which is in the dimension of 300.8 5.6 Case Study Figure 4 shows three court \u2019s views for a certain case : the court \u2019s view generated by PGN , by the proposed AC - NLG method , and the real court \u2019s view .", "entities": [[38, 40, "TaskName", "word embeddings"]]}, {"text": "We \ufb01nd that the one generated by PGN accepts the claim for principal , but ignores other claims such as issue related to guarantee .", "entities": []}, {"text": "Compared with the real court \u2019s view , our model accu8Source code , data , more experiment details and results can be found in supplementary materials .", "entities": []}, {"text": "771rately responds to both claims and produces the correct judgment .", "entities": []}, {"text": "6 Ethical Discussion While AI is gaining adoption in legal justice(Lin et", "entities": []}, {"text": "al . , 2012 ; Zhong et al . , 2018 ; Hu et al . , 2018 ; Jiang et al . , 2018 ; Chalkidis et al . , 2019 ) , any subtle statistical miscalculation may trigger serious consequences .", "entities": []}, {"text": "From a fairness perspective , prior studies suggested that global ( statistical ) optimization6 = individual ( demographic ) fairness ( Zemel et al . , 2013 ) , and this ethical concern should be further investigated .", "entities": []}, {"text": "In this section , we explore the following ethical issues .", "entities": []}, {"text": "Target User :", "entities": []}, {"text": "According to the report of statistics , a typical active trial judge closed around 250 cases in a year .", "entities": []}, {"text": "Trial judges suffering from \u2018 daunting workload \u2019 is becoming an critical issue(Duan et al . , 2019 ) .", "entities": []}, {"text": "The proposed algorithm is designed for generating the court \u2019s view draft for assisting the trial judges for decision making .", "entities": [[18, 20, "TaskName", "decision making"]]}, {"text": "This work is an algorithmic investigation , but such algorithm should never \u2018 replace \u2019 human judges .", "entities": []}, {"text": "Human knowledge / judgment should be the \ufb01nal safeguard to protect social justice and individual fairness .", "entities": []}, {"text": "Potential Error : The potential error would be as follows : a ) generating a wrong judgment and b ) generating a wrong rationale .", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "The goal of our algorithm is to generate a draft of court \u2019s view for trail judge as a reference , and judges need to proofread the content generated from algorithm .", "entities": []}, {"text": "Demographic Bias : In this paper , we focus on addressing the bias problem from the data generation by treating the variable of data generation as confounder in back - door adjustment .", "entities": []}, {"text": "The model adoption can face potential demographic bias / unfairness challenges , such as gender and race bias in the training data .", "entities": []}, {"text": "To further ensure the model fairness , in the future , algorithm adoption should be empowered with de - biased legal content pretraining , which could avoid potential demographic bias .", "entities": []}, {"text": "For instance , in order to remove gender / race bias , system could use ( Bolukbasi et al . , 2016 ) algorithm to debias the sensitive gender / race information , e.g. , replace \u2018 he / she \u2019 and \u2018 asian / hispanic \u2019 with gender / race neutral words for pretraining , which can be vital for legal domain .", "entities": []}, {"text": "7 Conclusion and Future Work", "entities": []}, {"text": "In this paper , we propose a novel Attentional and Counterfactual based Natural Language Genera - tion ( AC - NLG ) method to solve the task of court \u2019s view generation in civil cases and ensure the fairness of the judgment .", "entities": []}, {"text": "We design a claim - aware encoder to represent the fact description which emphasizes on the plaintiff \u2019s claim , as well as a pair of backdoor - inspired counterfactual decoders to generate judgment - discriminative court \u2019s views ( both supportive and non - supportive views ) and to eliminate the bias that arose from the data generation mechanism by connecting with a synergistic judgment predictive model .", "entities": []}, {"text": "The experimental results show the effectiveness of our method .", "entities": []}, {"text": "Based on the AC - NLG method , in the future , we can explore the following directions : ( 1 ) Improve the accuracy of judgment on a claim - level .", "entities": [[24, 25, "MetricName", "accuracy"]]}, {"text": "( 2 ) Add external knowledge ( e.g. a logic graph ) to the predictor for the interpretability of the model .", "entities": []}, {"text": "Acknowledgments This work was supported by National Natural Science Foundation of China ( No . 62006207 , 61625107 ) , National Key R&D Program of China ( No . 2018AAA0101900 , 2018YFC0830200 , 2018YFC0830206 , 2020YFC0832500 ) , the Fundamental Research Funds for the Central Universities .", "entities": []}, {"text": "Finally , we would like to thank the anonymous reviewers for their helpful feedback and suggestions .", "entities": []}, {"text": "References Sugato Bagchi and Laura Wynter .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Method for a natural language question - answering system to complement decision - support in a real - time command center .", "entities": []}, {"text": "US Patent 8,601,030 .", "entities": []}, {"text": "Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1409.0473 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tolga Bolukbasi , Kai - Wei Chang , James Y Zou , Venkatesh Saligrama , and Adam T Kalai .", "entities": [[16, 17, "MethodName", "Adam"]]}, {"text": "2016 .", "entities": []}, {"text": "Man is to computer programmer as woman is to homemaker ?", "entities": []}, {"text": "debiasing word embeddings .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Advances in neural information processing systems , pages 4349\u20134357 .", "entities": []}, {"text": "Ilias Chalkidis , Ion Androutsopoulos , and Nikolaos Aletras .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Neural legal judgment prediction in english .", "entities": []}, {"text": "arXiv preprint arXiv:1906.02059 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yen - Liang Chen , Yi - Hung Liu , and Wu - Liang Ho .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "A text mining approach to assist the general public in the retrieval of legal documents .", "entities": []}, {"text": "Journal of the American Society for Information Science and Technology , 64(2):280\u2013290 .", "entities": []}, {"text": "772Yuh - Shyan Chen , Shin - Wei Chiang , and Tong - Ying Juang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A few - shot transfer learning approach using text - label embedding with legal attributes for law article prediction .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "Technical report , EasyChair .", "entities": []}, {"text": "Xinyu Duan , Yating Zhang , Lin Yuan , Xin Zhou , Xiaozhong Liu , Tianyi Wang , Ruocheng Wang , Qiong Zhang , Changlong Sun , and Fei Wu . 2019 .", "entities": []}, {"text": "Legal summarization for multi - role debate dialogue via controversy focus mining and multi - task learning .", "entities": [[1, 2, "TaskName", "summarization"], [13, 17, "TaskName", "multi - task learning"]]}, {"text": "In Proceedings of the 28th ACM International Conference on Information and Knowledge Management , pages 1361\u20131370 .", "entities": [[5, 6, "DatasetName", "ACM"], [12, 13, "TaskName", "Management"]]}, {"text": "Zikun Hu , Xiang Li , Cunchao Tu , Zhiyuan Liu , and Maosong Sun .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Few - shot charge prediction with discriminative legal attributes .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 487\u2013498 .", "entities": []}, {"text": "Guido W Imbens and Donald B Rubin .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Causal inference in statistics , social , and biomedical sciences .", "entities": [[0, 2, "MethodName", "Causal inference"]]}, {"text": "Cambridge University Press .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Xin Jiang , Hai Ye , Zhunchen Luo , Wenhan Chao , and Wenjia Ma . 2018 .", "entities": []}, {"text": "Interpretable rationale augmented charge prediction system .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics : System Demonstrations , pages 146\u2013151 .", "entities": []}, {"text": "Kun Kuang , Lian Li , Zhi Geng , Lei Xu , Kun Zhang , Beishui Liao , Huaxin Huang , Peng Ding , Wang Miao , and Zhichao Jiang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Causal inference .", "entities": [[0, 2, "MethodName", "Causal inference"]]}, {"text": "Engineering , 6(3):253\u2013263 .", "entities": []}, {"text": "Wan - Chen Lin , Tsung - Ting Kuo , Tung - Jia Chang , Chueh - An Yen , Chao - Ju Chen , and Shou - de Lin .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Exploiting machine learning models for chinese legal documents labeling , case classi\ufb01cation , and sentencing prediction .", "entities": []}, {"text": "Processdings of ROCLING , page 140 .", "entities": []}, {"text": "Bryan McCann , Nitish Shirish Keskar , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The natural language decathlon : Multitask learning as question answering .", "entities": [[8, 10, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1806.08730 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th annual meeting on association for computational linguistics , pages 311\u2013318 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Judea Pearl .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Causality .", "entities": []}, {"text": "Cambridge university press .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Judea Pearl , Madelyn Glymour , and Nicholas P Jewell . 2016 .", "entities": []}, {"text": "Causal inference in statistics : A primer .", "entities": [[0, 2, "MethodName", "Causal inference"]]}, {"text": "John Wiley & Sons .", "entities": []}, {"text": "Radim \u02c7Reh\u02dau\u02c7rek and Petr Sojka .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Software Framework for Topic Modelling with Large Corpora .", "entities": []}, {"text": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks , pages 45 \u2013 50 , Valletta , Malta .", "entities": []}, {"text": "ELRA .", "entities": []}, {"text": "http://is.muni.cz/ publication/884893 / en .Alexander", "entities": []}, {"text": "M Rush , Sumit Chopra , and Jason Weston .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A neural attention model for abstractive sentence summarization .", "entities": [[6, 8, "TaskName", "sentence summarization"]]}, {"text": "arXiv preprint arXiv:1509.00685 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Abigail See , Peter J Liu , and Christopher D Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Get to the point : Summarization with pointer - generator networks .", "entities": [[5, 6, "TaskName", "Summarization"]]}, {"text": "arXiv preprint arXiv:1704.04368 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "I Sutskever , O Vinyals , and QV Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "Advances in NIPS .", "entities": []}, {"text": "Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , et al . 2016 .", "entities": []}, {"text": "Google \u2019s neural machine translation system : Bridging the gap between human and machine translation .", "entities": [[0, 1, "DatasetName", "Google"], [3, 5, "TaskName", "machine translation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1609.08144 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Guangyi Xiao , Even Chow , Hao Chen , Jiqian Mo , Jingzhi Guo , and Zhiguo Gong .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Chinese questions classi\ufb01cation in the law domain .", "entities": []}, {"text": "In 2017 IEEE 14th International Conference on e - Business Engineering ( ICEBE ) , pages 214\u2013219 . IEEE .", "entities": []}, {"text": "Hai Ye , Xin Jiang , Zhunchen Luo , and Wenhan Chao .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Interpretable charge predictions for criminal cases : Learning to generate court views from fact descriptions .", "entities": []}, {"text": "arXiv preprint arXiv:1802.08504 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rich Zemel , Yu Wu , Kevin Swersky , Toni Pitassi , and Cynthia Dwork .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Learning fair representations .", "entities": []}, {"text": "InInternational Conference on Machine Learning , pages 325\u2013333 .", "entities": []}, {"text": "Tianyi Zhang , Varsha Kishore , Felix Wu , Kilian Q Weinberger , and Yoav Artzi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bertscore : Evaluating text generation with bert .", "entities": [[3, 5, "TaskName", "text generation"]]}, {"text": "arXiv preprint arXiv:1904.09675 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ye Zhang , Nan Ding , and Radu Soricut .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Shaped : Shared - private encoder - decoder for text style adaptation .", "entities": []}, {"text": "arXiv preprint arXiv:1804.04093 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Haoxi Zhong , Zhipeng Guo , Cunchao Tu , Chaojun Xiao , Zhiyuan Liu , and Maosong Sun .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Legal judgment prediction via topological learning .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3540\u20133549 .", "entities": []}, {"text": "A Appendices All models are trained on 2 V100 GPU(16 GB ) .", "entities": []}, {"text": "773Table 5 : Experiment details for each model .", "entities": []}, {"text": "Method Avg Runtime # of Paras . S2S(wS ) 22h", "entities": []}, {"text": "30,789,836 PGN(wS )", "entities": []}, {"text": "25h 30,791,161 AC - NLGw / oD 7h 19,972,418 AC - NLGw / oBA 28h 34,622,843 AC - NLGw / oCA 27h 45,244,852 AC - NLG(wS ) 29h 49,010,612 Table 6 : The hyperparameters of AC - NLG .", "entities": []}, {"text": "Name value Note hidden dim 256 dimension of RNN hidden states emb dim 300 dimension of word embeddings batch size 16 minibatch size max encsteps 300 max timesteps of encoder ( max source text tokens ) max decsteps 150 max timesteps of decoder ( max generated text tokens ) beam size 4 beam size for beam search decoding min decsteps 35 Minimum sequence length of generated text .", "entities": [[16, 18, "TaskName", "word embeddings"], [18, 20, "HyperparameterName", "batch size"], [21, 23, "HyperparameterName", "minibatch size"]]}, {"text": "vocab size 50000 Size of vocabulary lr 0.15 learning rate keep prob 0.5 keep prob adagrad initacc 0.1 initial accumulator value for Adagrad rand unif initmag 0.02 magnitude for lstm cells random uniform inititalization trunc norm initstd 0.1 std of trunc norm init , used for initializing everything else max grad norm 2.0 for gradient clipping", "entities": [[8, 10, "HyperparameterName", "learning rate"], [15, 16, "MethodName", "adagrad"], [22, 23, "MethodName", "Adagrad"], [29, 30, "MethodName", "lstm"], [54, 56, "MethodName", "gradient clipping"]]}, {"text": "774 PLAINTIFF\u2019SCLAIM1.ThedefendantBshallreturntheplaintiff'sloanof$30,000andpaytheoverdueinterestattheinterestrateof2.4%permonthfromthedateofprosecutiontothedateofrepayment.2.ThedefendantBshallpaythelitigationcostsofthiscase .", "entities": []}, {"text": "FACTDESCRIPTIONAfterthehearing , thecourtheldthefactsasfollows : OnDecember11,2013,thedefendantBborrowed$30,000fromtheplaintiffA.ThedefendantBreceivedtheloanandgaveareceiptofthisloan .", "entities": []}, {"text": "Note : Ihaveborrowed$30,000fromAtoday .", "entities": []}, {"text": "PGNThecourtconcludedthat : ThefactsthatdefendantBborrowed$30,000fromtheplaintiffAareclear . TheprivatelendingrelationshipbetweenPlaintiffAandDefendantBislegitimateandvalid , itshallbeprotectedbythelaw .", "entities": []}, {"text": "Theplaintiffnowdemandsthatthedefendantrepaytheloanof$30,000 .", "entities": []}, {"text": "This demand is justified and should be supported Acceptance .", "entities": []}, {"text": "DefendantBrefusedtoappearincourtwithoutjustificationafterbeinglegallysummonedbythecourt .", "entities": []}, {"text": "AC - NLGThecourtconcludedthat : Thelegitimateprivatelendingrelationshipsareprotectedbylaw . TheactofborrowingbetweentheplaintiffAandthedefendantBdidnotviolatetheprohibitiveprovisionsofstatelawsandregulations , soitshouldbevalid .", "entities": []}, {"text": "ThefactthatthedefendantBowedtheplaintiffAaloanof$30,000isclear , andtheevidenceissufficient .", "entities": []}, {"text": "The defendant was supposed to repay the loan in time , and his failure to repay in time constituted a breach of contract , and he shall assume corresponding civil liabilities according to law Acceptance .", "entities": []}, {"text": "The receipt of this loan provided by the plaintiff A does not have agreed interest , as not to pay interest .", "entities": []}, {"text": "So the court does not support the claim that the plaintiff A asked the defendant B to calculate the interest from the date of the loan Rejection .", "entities": []}, {"text": "Thedefendantwassummonedbythecourtandrefusedtoappearincourtwithoutjustification .", "entities": []}, {"text": "REALThecourtconcludedthat : thelendingagreementbetweentheplaintiffAandthedefendantBcontainsthetruemeaninganddoesnotviolatetheprohibitiveprovisionsofstatelawsandregulations , itislegalandvalid .", "entities": []}, {"text": "Althoughtheplaintiffandthedefendantdidnotspecificallyagreeonthetimeforrepayment , afterthedefendantreceivedtheloan , it shall be returned within a reasonable period after being appealed by the plaintiff .", "entities": []}, {"text": "If the defendant fails to return it within a reasonable period after being called , the defendant shall be responsible to pay the overdue interest from the date of prosecution Acceptance .", "entities": []}, {"text": "Forthecalculationstandardforoverdueinterest , theplaintiffclaimedthatthemonthlyinterestratewas2.4%,butitdidnotprovideacorrespondingevidence .", "entities": []}, {"text": "Therefore , the court does not support this claim of overdue interest Rejection .", "entities": []}, {"text": "WithreferencetotheloaninterestrateannouncedbythePeople'sBankofChinaforthesameperiod , thecourtdeterminedthatoverdueinterestiscalculatedatanannualinterestrateof5.6%.Thefactthatthedefendanthasnotreturnedtheloanof$30,000isclear . Sothecourtsupportsthereasonablepartoftheplaintiff\u2019sclaimrequestingthedefendanttoreturntheloanandpaytheoverdueinterest .", "entities": []}, {"text": "DefendantBrefusedtoappearincourtwithoutjustificationafterbeinglegallysummonedbythecourt .", "entities": []}, {"text": "Figure 5 : Show case 1 .", "entities": []}, {"text": "775 Figure 6 : Show case 2 .", "entities": []}, {"text": "776 Figure 7 : Show case 3 .", "entities": []}, {"text": "777 Figure 8 : Show case 4 .", "entities": []}, {"text": "778 Figure 9 : Show case 5 .", "entities": []}, {"text": "779 Figure 10 : Show case 6 .", "entities": []}, {"text": "780 PLAINTIFF\u2019SCLAIM1.ThedefendantBshallpay$28,000andinterest$2560.Paymentofinterestiscalculatedfromthedateofprosecutiontotheactualsettlementdate , basedonthebaserateofthePeople\u2018sBankofChinaofthesameperiodattheamountof$28,000.2.Thelitigationcostsinthiscaseshallbepaidbythedefendant .", "entities": []}, {"text": "FACTDESCRIPTIONAfterthehearing , thecourtheldthefactsasfollows : OnSeptember30,2013andAugust25,2014,thedefendantBborrowed$10,000eachtimefromtheplaintiffA.Thedefendantissuedaloanreceipttotheplaintiffforeachofthetwoloans .", "entities": []}, {"text": "Therewasnowrittenagreementontheinterestandloanperiod .", "entities": []}, {"text": "Later , thedefendantdidnotreturntheloan , thenitcausedadispute .", "entities": []}, {"text": "Theabovefactsareprovedbytworeceiptsoftheloanprovidedbytheplaintiffandtheplaintiff'sstatementinthecourt .", "entities": []}, {"text": "PGNThecourtconcludedthat : Theprivatelendingrelationshipbetweentheplaintiffandthedefendantisestablishedaccordingtolaw , andiseffectivefromthedatetheplaintiffprovidesthedefendantwiththeloan .", "entities": []}, {"text": "Aftertheplaintiffprovidedtheloantothedefendant , thedefendantfailedtoreturntheloanasagreed , itwasobviouslyabreachofcontract .", "entities": []}, {"text": "Therefore , the plaintiff\u2018s claim requesting the defendant to return the loan principal of $ 28,000 was justified , and the court supports it Acceptance .", "entities": []}, {"text": "Thedefendantwaslegallysummonedbythecourtandrefusedtoappearincourtwithoutjustifiablereasonstoparticipateintheproceedings .", "entities": []}, {"text": "AC - NLGThecourtconcludedthat : Theprivatelendingrelationshipbetweentheplaintiffanddefendantisestablishedaccordingtolawandshouldbeprotectedbylaw . Thedefendantborrowed$10,000fromtheplaintiff .", "entities": []}, {"text": "Thefactswereclearandtheevidencewassufficient .", "entities": []}, {"text": "The plaintiff now requires the defendant to repay the loan of $ 10,000 .", "entities": []}, {"text": "The reasons are justified , and the court supports it Acceptance .", "entities": []}, {"text": "But the court does not support the plaintiff \u2019s claim requesting the defendant to pay interest on the loan because the plaintiff failed to provide evidence to prove the fact that both of them agreed on the interest of the loan Rejection .", "entities": []}, {"text": "Thedefendantwaslegallysummonedbythiscourtandrefusedtoappearincourtwithoutjustifiablereasonstoparticipateintheproceedings .", "entities": []}, {"text": "REALThecourtconcludedthat : Theprivatelendingrelationshipbetweentheplaintiffandthedefendantisestablishedandeffective , andshallbeprotectedaccordingtolaw .", "entities": []}, {"text": "Thedefendantshouldrepaytheloanafterreceivingit , butnowhedidnotrepay , itisobviouslyabreachofcontract .", "entities": []}, {"text": "Therefore , this court supports the claim of the plaintiff that the defendant should return the loan of $ 20,000 and the corresponding loss of interest Acceptance .", "entities": [[23, 24, "MetricName", "loss"]]}, {"text": "The plaintiff claimed that the defendant should pay interest , but did not provide evidence to prove that both of them clearly agreed on the interest , so the court does not support the plaintiff\u2018s claim for interest Rejection .", "entities": []}, {"text": "Theplaintiffwithdrewsomeoftheclaimsinthecourthearing , andthiscourtpermittedit .", "entities": []}, {"text": "Figure 11 : Show case 7 .", "entities": []}]