[{"text": "Specializing Multilingual Language Models : An Empirical Study Ethan C. Chau\u2020Noah A. Smith\u2020\u22c6 \u2020Paul G. Allen School of Computer Science & Engineering , University of Washington \u22c6Allen Institute for Artificial Intelligence { echau18,nasmith}@cs.washington.edu", "entities": []}, {"text": "Abstract Pretrained multilingual language models have become a common tool in transferring NLP capabilities to low - resource languages , often with adaptations .", "entities": []}, {"text": "In this work , we study the performance , extensibility , and interaction of two such adaptations : vocabulary augmentation and script transliteration .", "entities": []}, {"text": "Our evaluations on part - of - speech tagging , universal dependency parsing , and named entity recognition in nine diverse low - resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low - resource settings .", "entities": [[3, 9, "TaskName", "part - of - speech tagging"], [11, 13, "TaskName", "dependency parsing"], [15, 18, "TaskName", "named entity recognition"]]}, {"text": "1 Introduction Research in natural language processing is increasingly carried out in languages beyond English .", "entities": []}, {"text": "This includes high - resource languages with abundant data , as well as low - resource languages , for which labeled ( and unlabeled ) data is scarce .", "entities": []}, {"text": "In fact , many of the world \u2019s languages fall into the latter category , even some with a high number of speakers .", "entities": []}, {"text": "This presents unique challenges compared to high - resource languages : effectively modeling low - resource languages involves both accurately tokenizing text in such languages and maximally leveraging the limited available data .", "entities": []}, {"text": "One common approach to low - resource NLP is themultilingual paradigm , in which methods that have shown success in English are applied to the union of many languages \u2019 data,1enabling transfer between languages .", "entities": []}, {"text": "For instance , multilingual contextual word representations ( CWRs ) from language models ( Devlin et al . , 2019 ; Huang et al . , 2019 ; Lample and Conneau , 2019 , inter alia ) are conventionally \u201c pretrained \u201d on large multilingual 1Within the multilingual paradigm , a distinction is sometimes made between massively multilingual methods , which consider tens or hundreds of languages ; and polyglot methods , which use only a handful .", "entities": []}, {"text": "In this paper , all mentions of \u201c multilingual \u201d refer to the former.corpora before being \u201c finetuned \u201d directly on supervised tasks ; this pretraining - finetuning approach is derived from analogous monolingual models ( Devlin et al . , 2019 ; Liu et al . , 2019 ; Peters et al . , 2018 ) .", "entities": []}, {"text": "However , considering the diversity of the world \u2019s languages and the great data imbalance among them , it is natural to question whether the current multilingual paradigm can be improved upon for low - resource languages .", "entities": []}, {"text": "Indeed , past work has demonstrated that it can .", "entities": []}, {"text": "For instance , Wu and Dredze ( 2020 ) find that multilingual models often lag behind non - contextualized baselines for the lowest - resource languages in their training data , drawing into question their utility in such settings .", "entities": []}, {"text": "Conneau et al .", "entities": []}, {"text": "( 2020a ) posit that this phenomenon is a result of limited model capacity , which proves to be a bottleneck for sufficient transfer to low - resource languages .", "entities": []}, {"text": "In fact , with multilingual models only being pretrained on a limited set of languages , most of the world \u2019s languages are unseen by the model .", "entities": []}, {"text": "For such languages , the performance of such models is even worse ( Chau et al . , 2020 ) , due in part to the diversity of scripts across the world \u2019s languages ( Muller et al . , 2021 ; Pfeiffer et al . , 2021b ; Rust et al . , 2021 ) as compared to the models \u2019 Latin - centricity ( \u00c1cs , 2019 ) .", "entities": []}, {"text": "Nonetheless , there have been multiple attempts to remedy this discrepancy by specializing2a multilingual model to a given target low - resource language , from which we take inspiration .", "entities": []}, {"text": "Among them , Chau et al . ( 2020 ) augment the model \u2019s vocabulary to more effectively tokenize text , then pretrain on a small amount of data in the target language ; they report significant performance improvements on a small set of low - resource languages .", "entities": []}, {"text": "In a similar vein , Muller et al . ( 2021 ) propose to transliterate text in the target language 2We use specialization to denote preparing a model for use on a specific target language , to the exclusion of others .", "entities": []}, {"text": "This is a subset of adaptation , which includes all techniques that adjust a model for use on target languages , regardless of their resulting universality .", "entities": []}, {"text": "to Latin script to be better tokenized by the existing model , followed by additional pretraining ; they observe mixed results and note that transliteration quality may be a confounding factor .", "entities": []}, {"text": "We hypothesize that these two methods can serve as the basis for improvements in modeling a broad set of low - resource languages .", "entities": []}, {"text": "In this work , we study the effectiveness , extensibility , and interaction of these two approaches to specialization : the vocabulary augmentation technique of Chau et al .", "entities": []}, {"text": "( 2020 ) and the script transliteration method of Muller et al .", "entities": []}, {"text": "( 2021 ) .", "entities": []}, {"text": "We verify the performance of vocabulary augmentation on three tasks in a diverse set of nine low - resource languages across three different scripts , especially on non - Latin scripts ( \u00a7 2 ) and find that these gains are associated with improved vocabulary coverage of the target language .", "entities": []}, {"text": "We further observe a negative interaction between vocabulary augmentation and transliteration in light of a broader framework for specializing multilingual models , while noting that vocabulary augmentation offers an appealing balance of performance and cost ( \u00a7 3 ) .", "entities": []}, {"text": "Overall , our results highlight several possible directions for future study in the low - resource setting .", "entities": []}, {"text": "Our code , data , and hyperparameters are publicly available.3 2", "entities": []}, {"text": "Revisiting Vocabulary Augmentation We begin by revisiting the V ocabulary Augmentation method of Chau et al .", "entities": []}, {"text": "( 2020 ) , which we recast more generally in light of recent work ( \u00a7 2.1 ) .", "entities": []}, {"text": "We evaluate their claims on three different tasks , using a diverse set of languages in multiple scripts ( \u00a7 2.2 ) , and find that the results hold to an even more pronounced degree in unseen low - resource languages with non - Latin scripts ( \u00a7 2.3 ) .", "entities": []}, {"text": "2.1 Method Overview Following Chau et al .", "entities": []}, {"text": "( 2020 ) , we consider how to apply the pretrained multilingual BERT model ( MBERT ; Devlin et al . , 2019 ) to a target lowresource language , for which both labeled and unlabeled data is scarce .", "entities": [[12, 13, "MethodName", "BERT"], [15, 16, "MethodName", "MBERT"]]}, {"text": "This model has produced strong CWRs for many languages ( Kondratyuk and Straka , 2019 , inter alia ) and has been the starting model for many studies on low - resource languages ( Muller et al . , 2021 ; Pfeiffer et al . , 2020 ; Wang et al . , 2020 ) .", "entities": []}, {"text": "MBERTcovers the languages with the 104 largest Wikipedias , and it uses this data to con3https://github.com/ethch18/ specializing - multilingualstruct a wordpiece vocabulary ( Wu et al . , 2016 ) and train its transformer - based architecture ( Vaswani et al . , 2017 ) .", "entities": [[20, 21, "MethodName", "wordpiece"]]}, {"text": "Although low - resource languages are slightly oversampled , high - resource languages still dominate both the final pretraining data and the vocabulary ( \u00c1cs , 2019 ; Devlin et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "Chau et al .", "entities": []}, {"text": "( 2020 ) note that target low - resource languages fall into three categories with respect toMBERT \u2019s pretraining data : the lowest - resource languages in the data ( Type 1 ) , completely unseen low - resource languages ( Type 2 ) , and low - resouce languages with more representation ( Type 0).4Due to their poor representation in the vocabulary , Type 1 and Type 2 languages achieve suboptimal tokenization and higher rates of the \u201c unknown \u201d wordpiece5when using MBERT out of the box .", "entities": [[83, 84, "MethodName", "MBERT"]]}, {"text": "This hinders the model \u2019s ability to capture meaningful patterns in the data , resulting in reduced data efficiency and degraded performance .", "entities": []}, {"text": "We note that this challenge is exacerbated when modeling languages written in non - Latin scripts .", "entities": []}, {"text": "MBERT \u2019s vocabulary is heavily Latin - centric ( \u00c1cs , 2019 ; Muller et al . , 2021 ) , resulting in a significantly larger portion of non - Latin scripts being represented with \u201c unknown \u201d tokens ( Pfeiffer et al . , 2021b ) and further limiting the model \u2019s ability to generalize .", "entities": [[0, 1, "MethodName", "MBERT"]]}, {"text": "In effect , MBERT \u2019s low initial performance on such languages can be attributed to its inability to represent the script itself .", "entities": [[3, 4, "MethodName", "MBERT"]]}, {"text": "To alleviate the problem of poor tokenization , Chau et al .", "entities": []}, {"text": "( 2020 ) propose to specialize MBERT using V ocabulary Augmentation ( VA ) .", "entities": [[6, 7, "MethodName", "MBERT"]]}, {"text": "Given unlabeled data in the target language , they train a new wordpiece vocabulary on the data , then select the 99 most common wordpieces in the new vocabulary that replace \u201c unknown \u201d tokens under the original vocabulary .", "entities": [[12, 13, "MethodName", "wordpiece"]]}, {"text": "They then add these 99 wordpieces to the original vocabulary and continue pretraining MBERTon the unlabeled data for additional steps .", "entities": []}, {"text": "They further describe a tiered variant ( TVA ) , in which a larger learning rate is used for the embeddings of these 99 new wordpieces .", "entities": [[14, 16, "HyperparameterName", "learning rate"]]}, {"text": "VA yields strong gains over unadapted multilingual language models on dependency parsing in four low - resource languages with Latin scripts .", "entities": [[10, 12, "TaskName", "dependency parsing"]]}, {"text": "How4Muller et al .", "entities": []}, {"text": "( 2021 ) further subdivide Type 2 into Easy , Medium , and Hard languages , based on the performance of MBERT after being exposed to these languages .", "entities": [[21, 22, "MethodName", "MBERT"]]}, {"text": "However , this categorization can not be determined a priori for a given language .", "entities": []}, {"text": "5The \u201c unknown \u201d wordpiece is inserted when the wordpiece algorithm is unable to segment a word - level token with the current vocabulary .", "entities": [[4, 5, "MethodName", "wordpiece"], [9, 10, "MethodName", "wordpiece"]]}, {"text": "ever , no evaluation has been performed on other tasks or on languages with non - Latin scripts , which raises our first research question : RQ1 : Do the conclusions of Chau et al .", "entities": []}, {"text": "( 2020 ) hold for other tasks and for languages with nonLatin scripts ?", "entities": []}, {"text": "We can view VAand TVA as an instantation of a more general framework of vocabulary augmentation , shared by other approaches to using MBERT in low - resource settings .", "entities": [[23, 24, "MethodName", "MBERT"]]}, {"text": "Given a new vocabulary V , number of wordpieces n , and learning rate multiplier a , thenmost common wordpieces in Vare added to the original vocabulary .", "entities": [[12, 14, "HyperparameterName", "learning rate"]]}, {"text": "Additional pretraining is then performed , with the embeddings of the nwordpieces taking on a learning rate a times greater than the overall learning rate .", "entities": [[15, 17, "HyperparameterName", "learning rate"], [23, 25, "HyperparameterName", "learning rate"]]}, {"text": "For VA , we set n= 99 anda= 1 , while we treat aas a hyperparameter for TVA .", "entities": []}, {"text": "The related E - MBERT method of Wang et al .", "entities": [[4, 5, "MethodName", "MBERT"]]}, {"text": "( 2020 ) sets n=|V|and a=", "entities": []}, {"text": "1 .", "entities": []}, {"text": "Investigating various other instantiations of this framework is an interesting research direction , though it is out of the scope of this work .", "entities": []}, {"text": "2.2 Experiments We expand on the dependency parsing evaluations of Chau et al . ( 2020 ) by additionally considering named entity recognition and part - of - speech tagging .", "entities": [[6, 8, "TaskName", "dependency parsing"], [20, 23, "TaskName", "named entity recognition"], [24, 30, "TaskName", "part - of - speech tagging"]]}, {"text": "We follow Kondratyuk and Straka ( 2019 ) and compute the CWR for each token as a weighted sum of the activations at each MBERT layer .", "entities": [[24, 25, "MethodName", "MBERT"]]}, {"text": "For dependency parsing , we follow the setup of Chau et al .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "( 2020 ) and Muller et al .", "entities": []}, {"text": "( 2021 ) and use the CWRs as input to the graph - based dependency parser of Dozat and Manning ( 2017 ) .", "entities": []}, {"text": "For named entity recognition , the CWRs are used as input to a CRF layer , while part - of - speech tagging uses a linear projection atop the representations .", "entities": [[1, 4, "TaskName", "named entity recognition"], [13, 14, "MethodName", "CRF"], [17, 23, "TaskName", "part - of - speech tagging"]]}, {"text": "In all cases , the underlying CWRs are finetuned during downstream task training , and we do not add an additional encoder layer above the transformer outputs .", "entities": []}, {"text": "We train models on five different random seeds and report average scores and standard errors .", "entities": [[7, 8, "DatasetName", "seeds"]]}, {"text": "2.2.1 Languages and Datasets We select a set of nine typologically diverse lowresource languages for evaluation , including three of the original four used by Chau et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "These languages use three different scripts and are chosen based on the availability of labeled datasets and their exemplification of the three language types identified by Chau et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "Of the lan - guages seen by MBERT , all selected Type 0 languages are within the 45 largest Wikipedias , while the remaining Type 1 languages are within the top 100 .", "entities": [[7, 8, "MethodName", "MBERT"], [12, 13, "DatasetName", "0"]]}, {"text": "The Type 2 languages , which are excluded from MBERT , are all outside of the top 150.6Additional information about the evaluation languages is given in Tab .", "entities": [[9, 10, "MethodName", "MBERT"]]}, {"text": "1 . Unlabeled Datasets Following Chau et al .", "entities": []}, {"text": "( 2020 ) , we use articles from Wikipedia as unlabeled data for additional pretraining in order to reflect the original pretraining data .", "entities": []}, {"text": "We downsample full articles from the largest Wikipedias to be on the order of millions of tokens in order to simulate a low - resource unlabeled setting , and we remove sentences that appear in the labeled validation or test sets .", "entities": []}, {"text": "Labeled Datasets For dependency parsing and part - of - speech tagging , we use datasets and train / test splits from Universal Dependencies ( Nivre et al . , 2020 ) , version 2.5 ( Zeman et al . , 2019 ) .", "entities": [[3, 5, "TaskName", "dependency parsing"], [6, 12, "TaskName", "part - of - speech tagging"], [22, 24, "DatasetName", "Universal Dependencies"]]}, {"text": "POS tagging uses language - specific partof - speech tags ( XPOS ) to evaluate understanding of language - specific syntactic phenomena .", "entities": []}, {"text": "The Belarusian treebank lacks XPOS tags for certain examples , so we use universal part - of - speech tags instead .", "entities": [[14, 17, "DatasetName", "part - of"]]}, {"text": "Dependency parsers are trained with gold word segmentation and no part - of - speech features .", "entities": [[10, 13, "DatasetName", "part - of"]]}, {"text": "Experiments with named entity recognition use the WikiAnn dataset ( Pan et al . , 2017 ) , following past work ( Muller et al . , 2021 ; Pfeiffer et al . , 2020 ; Wu and Dredze , 2020 ) .", "entities": [[2, 5, "TaskName", "named entity recognition"], [7, 8, "DatasetName", "WikiAnn"]]}, {"text": "Specifically , we use the balanced train / test splits of ( Rahimi et al . , 2019 ) .", "entities": []}, {"text": "We note that UD datasets were unavailable for Meadow Mari , and partitioned WikiAnn datasets were missing for Wolof .", "entities": [[3, 4, "DatasetName", "UD"], [13, 14, "DatasetName", "WikiAnn"]]}, {"text": "2.2.2", "entities": []}, {"text": "Baselines To measure the effectiveness of VA , we benchmark it against unadapted MBERT , as well as directly pretraining MBERT on the unlabeled data without modifying the vocabulary ( Chau et al . , 2020 ; Muller et al . , 2021 ; Pfeiffer et al . , 2020 ) .", "entities": [[13, 14, "MethodName", "MBERT"], [20, 21, "MethodName", "MBERT"]]}, {"text": "Following Chau et al .", "entities": []}, {"text": "( 2020 ) , we refer to the latter approach as language - adaptive pretraining ( LAPT ) .", "entities": []}, {"text": "We also evaluate two monolingual baselines that are trained on our unlabeled data : fastText embeddings ( FAST T ; Bojanowski et al . , 2017 ) , which represent a static word vector approach ; and a BERT model trained from scratch ( BERT ) .", "entities": [[14, 15, "MethodName", "fastText"], [38, 39, "MethodName", "BERT"], [44, 45, "MethodName", "BERT"]]}, {"text": "For 6Based on https://meta.wikimedia.org/ wiki / List_of_Wikipedias .", "entities": []}, {"text": "Language Type Script Family # Sentences # Tokens Downsample % # WP / Token Bulgarian ( BG ) 0", "entities": [[18, 19, "DatasetName", "0"]]}, {"text": "Cyrillic Slavic 357k 5.6 M 10 % 1.81 Belarusian ( BE ) 0", "entities": [[12, 13, "DatasetName", "0"]]}, {"text": "Cyrillic Slavic 187k 2.7 M 10 % 2.25 Meadow Mari ( MHR ) 2 Cyrillic Uralic 52k 512k \u2013 2.37 Vietnamese ( VI ) 0", "entities": [[24, 25, "DatasetName", "0"]]}, {"text": "Latin Viet - Muong 338k 6.9 M 5 % 1.17 Irish ( GA ) 1 Latin Celtic 274k 5.8 M \u2013 1.83 Maltese ( MT ) 2 Latin Semitic 75k", "entities": [[12, 13, "MethodName", "GA"]]}, {"text": "1.4 M \u2013 2.39 Wolof ( WO ) 2 Latin Niger - Congo 15k 396k \u2013 1.78 Urdu ( UR ) 0", "entities": [[17, 18, "DatasetName", "Urdu"], [21, 22, "DatasetName", "0"]]}, {"text": "Perso - Arabic Indic 201k 3.6 M 20 % 1.58 Uyghur ( UG ) 2 Perso - Arabic Turkic 136k 2.3 M \u2013 2.54 Table 1 : Language overview and unlabeled dataset statistics : number of sentences , number of tokens , and average wordpieces per token under the original MBERTvocabulary .", "entities": []}, {"text": "BERT , we follow Muller et al .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "( 2021 ) and train a six - layer RoBERTa model ( Liu et al . , 2019 ) with a language - specific SentencePiece tokenizer ( Kudo and Richardson , 2018 ) .", "entities": [[9, 10, "MethodName", "RoBERTa"], [24, 25, "MethodName", "SentencePiece"]]}, {"text": "For a fair comparison to VA , we use the same task - specific architectures and modify only the input representations .", "entities": []}, {"text": "2.2.3 Implementation Details To pretrain LAPT and VAmodels , we use the code of Chau et al .", "entities": []}, {"text": "( 2020 ) , who modify the pretraining code of Devlin", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) to only use the masked language modeling ( MLM ) loss .", "entities": [[7, 10, "TaskName", "masked language modeling"], [11, 12, "DatasetName", "MLM"], [13, 14, "MetricName", "loss"]]}, {"text": "To generate VA vocabularies , we train a new vocabulary of size 5000 and select the 99 wordpieces that replace the most unknown tokens .", "entities": []}, {"text": "We train with a fixed linear warmup of 1000 steps .", "entities": [[5, 7, "MethodName", "linear warmup"]]}, {"text": "To pretrain BERT models , we use the HuggingFace Transformers library ( Wolf et al . , 2020 ) .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "Following Muller et al .", "entities": []}, {"text": "( 2021 ) , we train a half - sized RoBERTa model with six layers and 12 attention heads .", "entities": [[10, 11, "MethodName", "RoBERTa"]]}, {"text": "We use a byte - pair vocabulary of size 52000 and a linear warmup of 1 epoch .", "entities": [[12, 14, "MethodName", "linear warmup"]]}, {"text": "For LAPT , VA , and BERT , we train for up to 20 epochs total , selecting the highest - performing epoch based on validation masked language modeling loss .", "entities": [[6, 7, "MethodName", "BERT"], [26, 29, "TaskName", "masked language modeling"], [29, 30, "MetricName", "loss"]]}, {"text": "FAST Tmodels are trained with the skipgram model for five epochs , with the default hyperparameters of Bojanowski et", "entities": []}, {"text": "al . ( 2017 ) .", "entities": []}, {"text": "Training of downstream parsers and taggers follows Chau et al .", "entities": []}, {"text": "( 2020 ) and Kondratyuk and Straka ( 2019 ) , with an inverse square - root learning rate decay and linear warmup , and layer - wise gradual unfreezing and discriminative finetuning .", "entities": [[17, 19, "HyperparameterName", "learning rate"], [21, 23, "MethodName", "linear warmup"]]}, {"text": "Models are trained with AllenNLP , version 2.1.0 ( Gardner et al . , 2018 ) , for up to 200 epochs with early stopping based on validation performance .", "entities": [[23, 25, "MethodName", "early stopping"]]}, {"text": "We choose batch sizes to be the maximum that allows for successful training on one GPU.2.3 Results Tab . 2 presents performance of the different input representations on POS tagging , dependency parsing , and named entity recognition .", "entities": [[31, 33, "TaskName", "dependency parsing"], [35, 38, "TaskName", "named entity recognition"]]}, {"text": "VAachieves strong results across all languages and tasks and is the top performer in the majority of them , suggesting that augmenting the vocabulary addresses MBERT \u2019s limited vocabulary coverage of the target language and is beneficial during continued pretraining .", "entities": [[25, 26, "MethodName", "MBERT"]]}, {"text": "The relative gains that VAprovides appear to correlate not only with language type , as in the findings of Chau et al . ( 2020 ) , but also with each language \u2019s script .", "entities": []}, {"text": "For instance , in Vietnamese , which is a Type 0 Latin script language , the improvements from VAare marginal at best , reflecting the Latindominated pretraining data of MBERT .", "entities": [[10, 11, "DatasetName", "0"], [29, 30, "MethodName", "MBERT"]]}, {"text": "Irish , the Type 1 Latin script language , is only slightly more receptive .", "entities": []}, {"text": "However , Type 0 languages in Cyrillic and Arabic scripts , which are less represented in MBERT \u2019s pretraining data , are more receptive to VA , with VAeven outperforming all other methods for Urdu .", "entities": [[3, 4, "DatasetName", "0"], [16, 17, "MethodName", "MBERT"], [34, 35, "DatasetName", "Urdu"]]}, {"text": "This trend is amplified in the Type 2 languages , as the improvements for Maltese and Wolof are small but significant .", "entities": []}, {"text": "However , they are dwarfed in magnitude by those of Uyghur , where VAachieves up to a 57 % relative error reduction over LAPT .", "entities": []}, {"text": "This result corroborates the findings of both Chau et al .", "entities": []}, {"text": "( 2020 ) and Muller et al .", "entities": []}, {"text": "( 2021 ) and answers RQ1 .", "entities": []}, {"text": "Prior to specialization , MBERT is especially poorly equipped to handle unseen lowresource languages and languages in non - Latin scripts due to its inability to model the script itself .", "entities": [[4, 5, "MethodName", "MBERT"]]}, {"text": "In such cases , specialization via VAis beneficial , providing MBERT with explicit signal about the target language and script while maintaining its language - agnostic insights .", "entities": [[10, 11, "MethodName", "MBERT"]]}, {"text": "On the other hand , this also motivates additional investigation into reme-", "entities": []}, {"text": "Rep. BE * ( 0 ) BG(0 ) GA(1 ) MT(2 ) UG(2 ) UR(0 ) VI(0 ) WO(2 ) Avg .", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "FAST T 68.84", "entities": []}, {"text": "\u00b17.16 88.86 \u00b10.37 86.87 \u00b12.55 89.68 \u00b12.15 89.45 \u00b11.37 90.81 \u00b10.31 81.84 \u00b11.15 87.48 \u00b10.55 85.48 BERT 91.00 \u00b10.30 94.48 \u00b10.10 90.36 \u00b10.20 92.61 \u00b10.10 90.87 \u00b10.13 89.88 \u00b10.13 84.73 \u00b10.13 87.71 \u00b10.31 90.20 MBERT 94.57 \u00b10.45 96.98 \u00b10.08 91.91 \u00b10.25 94.01 \u00b10.17 78.07 \u00b10.22 91.77 \u00b10.18 88.97 \u00b10.10 93.04 \u00b10.20 91.16 LAPT 95.74 \u00b10.44 97.15 \u00b10.04 93.28 \u00b10.19 95.76 \u00b10.09 79.88 \u00b10.27 92.18 \u00b10.16 89.64 \u00b10.20 94.58 \u00b10.13", "entities": [[16, 17, "MethodName", "BERT"], [34, 35, "MethodName", "MBERT"]]}, {"text": "92.28 VA 95.28 \u00b10.51 97.20 \u00b10.06 93.33 \u00b10.16 96.33 \u00b10.09 91.49 \u00b10.13 92.24 \u00b10.16 89.49 \u00b10.22 94.48 \u00b10.20 93.73 ( a ) POS tagging ( accuracy ) .", "entities": [[25, 26, "MetricName", "accuracy"]]}, {"text": "* Belarusian uses universal POS tags .", "entities": []}, {"text": "Rep. BE(0 ) BG(0 ) GA(1 ) MT(2 ) UG(2 ) UR(0 ) VI(0 ) WO(2 ) Avg .", "entities": []}, {"text": "FAST T 35.81 \u00b12.24 84.03 \u00b10.41 65.58 \u00b11.21 68.45 \u00b11.40 54.52 \u00b11.02 79.33 \u00b10.25 54.91 \u00b10.79 70.39 \u00b11.39 64.13 BERT 45.77 \u00b11.35 84.61 \u00b10.27 64.02 \u00b10.49 65.92 \u00b10.45 60.34 \u00b10.27 78.07 \u00b10.22 54.70 \u00b10.27 60.12 \u00b10.39 64.19 MBERT 71.83 \u00b10.90 91.62 \u00b10.23 71.68 \u00b10.62 76.63 \u00b10.35 47.70 \u00b10.44 81.45 \u00b10.26 64.58 \u00b10.42 76.24 \u00b10.83 72.72 LAPT 72.77 \u00b11.12 92.08 \u00b10.31 74.79 \u00b10.12 81.53 \u00b10.37 50.67 \u00b10.34 81.78 \u00b10.44 66.15 \u00b10.41 80.34 \u00b10.14 75.01 VA 73.22 \u00b11.23 91.90 \u00b10.20 74.35 \u00b10.22 82.00 \u00b10.31 67.55 \u00b10.17 81.88 \u00b10.25 65.64 \u00b10.12 80.22 \u00b10.41 77.09 ( b ) UD parsing ( LAS ) .", "entities": [[19, 20, "MethodName", "BERT"], [37, 38, "MethodName", "MBERT"], [94, 95, "DatasetName", "UD"]]}, {"text": "Rep. BE(0 ) BG(0 ) GA(1 ) MT(2 ) UG(2 ) UR(0 ) VI(0 ) MHR ( 2 ) Avg .", "entities": []}, {"text": "FAST T 84.26 \u00b10.86 87.98 \u00b10.76 67.21 \u00b14.30 33.53 \u00b117.89 \u2013 92.85 \u00b12.04 85.57 \u00b11.98 35.28 \u00b113.81 60.84 BERT 88.08 \u00b10.62 90.31 \u00b10.20 76.58 \u00b10.98 54.64 \u00b13.51 61.54 \u00b13.70 94.04 \u00b10.55 88.08 \u00b10.15 54.17 \u00b12.88 75.93 MBERT 91.13 \u00b10.07 92.56 \u00b10.09 82.82 \u00b10.57 61.86 \u00b12.60 50.76 \u00b11.86 94.60 \u00b10.34 92.13 \u00b10.27 61.85 \u00b13.25 78.46 LAPT 91.61 \u00b10.74 92.96 \u00b10.13 84.13 \u00b10.78 81.53 \u00b12.33 56.76 \u00b14.91", "entities": [[18, 19, "MethodName", "BERT"], [36, 37, "MethodName", "MBERT"]]}, {"text": "95.17 \u00b10.29 92.41 \u00b10.15 59.17 \u00b15.15 81.72 VA 91.38 \u00b10.56 92.70 \u00b10.11 84.82 \u00b11.00 80.00 \u00b12.77 68.93 \u00b13.30 95.43 \u00b10.22 92.43 \u00b10.16", "entities": []}, {"text": "64.23 \u00b13.07 83.74 ( c ) NER ( macro F1 ) .", "entities": [[6, 7, "TaskName", "NER"], [8, 10, "MetricName", "macro F1"]]}, {"text": "\u2013 indicates that a model did not converge .", "entities": []}, {"text": "Table 2 : Results on POS tagging , UD parsing , and NER , with standard deviations from five random initializations .", "entities": [[8, 9, "DatasetName", "UD"], [12, 13, "TaskName", "NER"]]}, {"text": "Bolded results are the maximum for each language , and scores in gray are not significantly worse than the best model ( 1 - sided paired t - test , p= 0.05with", "entities": []}, {"text": "Bonferonni correction ) .", "entities": []}, {"text": "dies for the script imbalance at a larger scale , e.g. , more diverse pretraining data .", "entities": []}, {"text": "2.4 Analysis We perform further analysis to investigate VA \u2019s patterns of success .", "entities": []}, {"text": "Concretely , we hypothesize that VAsignificantly improves the tokenizer \u2019s coverage of target languages where it is most successful .", "entities": []}, {"text": "Inspired by \u00c1cs ( 2019 ) , Chau et al .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "( 2020 ) , and Rust et", "entities": []}, {"text": "al . ( 2021 ) , we quantify tokenizer coverage using the percentage of tokens in the raw text that yield unknown wordpieces when tokenized with a given vocabulary ( \u201c UNK token percentage \u201d ) .", "entities": []}, {"text": "These are tokens whose representations contain at least partial ambiguity due to the inclusion of the unknown wordpiece .", "entities": [[17, 18, "MethodName", "wordpiece"]]}, {"text": "Tab .", "entities": []}, {"text": "3 presents the UNK token percentage for each dataset using the MBERT vocabulary , averaged over each script and language type .", "entities": [[11, 12, "MethodName", "MBERT"]]}, {"text": "This vocabulary is used in LAPT and represents the baseline level of vocabulary coverage .", "entities": []}, {"text": "We also include the change in the UNK token percentage between the MBERT and VAvocabularies , which quantifies the coverage improvement .", "entities": [[12, 13, "MethodName", "MBERT"]]}, {"text": "Both sets of values are juxtaposed against the average change in task - specific performance from LAPT toVA , representing the effect of augmenting the vocabulary on task - specific performance .", "entities": []}, {"text": "We observe that off - the - shelf MBERTalready at - tains relatively high vocabulary coverage for Type 0 and 1 languages , as well as languages written in Latin and Cyrillic scripts .", "entities": [[18, 19, "DatasetName", "0"]]}, {"text": "On the other hand , up to one - fifth of the tokens in Arabic languages and one - sixth of those in Type 2 languages yield an unknown wordpiece .", "entities": [[29, 30, "MethodName", "wordpiece"]]}, {"text": "For these languages , there is great room for increasing tokenizer coverage , and VAindeed addresses this more tangible need .", "entities": []}, {"text": "This aligns with the task - specific performance improvements for each group and helps to explain our results in \u00a7 2.3 .", "entities": []}, {"text": "It is notable that VAdoes not always eliminate the issue of unknown wordpieces , even in languages for which MBERTattains high vocabulary coverage .", "entities": []}, {"text": "This suggests that the remaining unknown wordpieces in these languages are more sparsely distributed ( i.e. , they represent low frequency sequences ) , while the unknown wordpieces in languages with lower vocabulary coverage represent sequences that occur more commonly .", "entities": []}, {"text": "As a result , augmenting the vocabulary in such languages quickly improves coverage while associating these commonly occurring sequences with each other , which benefits the overall tokenization quality .", "entities": []}, {"text": "We further explore the association between the improvements in vocabulary coverage and taskspecific performance in Fig .", "entities": []}, {"text": "1 .", "entities": []}, {"text": "Although we do not find that languages from the same types or scripts form clear clusters , we nonetheless observe a loose", "entities": []}, {"text": "Lang .", "entities": []}, {"text": "Group Avg . UNK Token % ( MBERT ) Avg . UNK Token % ( \u2206 ) Avg .", "entities": [[7, 8, "MethodName", "MBERT"]]}, {"text": "Task Performance ( \u2206 ) ( # of Langs . )", "entities": []}, {"text": "Unlabeled UD WikiAnn Unlabeled UD WikiAnn POS", "entities": [[1, 2, "DatasetName", "UD"], [2, 3, "DatasetName", "WikiAnn"], [4, 5, "DatasetName", "UD"], [5, 6, "DatasetName", "WikiAnn"]]}, {"text": "UD NER All ( 9 ) 5.9 % ( \u2013 ) 5.2 % ( \u2013 ) 6.2 % ( \u2013 ) \u2013 5.3 % ( \u2013 ) 4.7 % ( \u2013 ) \u2013 5.8 % ( \u2013 ) +1.45 ( \u2013 ) +2.08 ( \u2013 ) +2.02 ( \u2013 ) Type 0 ( 4 ) 1.0 % ( \u2193 ) 0.3 % ( \u2193 ) 1.2 % ( \u2193 ) \u2013 0.9 % ( \u2191 ) \u2013 0.3 % ( \u2191 ) \u2013 1.2 % ( \u2191 ) \u2013 0.13 ( \u2193 ) \u2013 0.04 ( \u2193 ) \u2013 0.05 ( \u2193 ) Type 1 ( 1 ) 0.3 % ( \u2193 ) 0.0 % ( \u2193 ) 0.4 % ( \u2193 ) \u2013 0.3 % ( \u2191 ) \u2013 0.00 % ( \u2191 ) \u2013 0.4 % ( \u2191 ) +0.05 ( \u2193 ) \u2013 0.44 ( \u2193 ) +0.69 ( \u2193 )", "entities": [[0, 1, "DatasetName", "UD"], [1, 2, "TaskName", "NER"], [51, 52, "DatasetName", "0"]]}, {"text": "Type 2 ( 4 ) 12.3 % ( \u2191 ) 13.5 % ( \u2191 ) 14.8 % ( \u2191 ) \u2013 10.8 % ( \u2193 ) \u2013 12.1 % ( \u2193 ) \u2013 13.7 % ( \u2193 ) +4.03 ( \u2191 ) +5.74 ( \u2191 ) +5.23 ( \u2191 ) Latin ( 4 ) 1.2 % ( \u2193 ) 0.6 % ( \u2193 ) 2.4 % ( \u2193 ) \u2013 1.2 % ( \u2191 ) \u2013 0.6 % ( \u2191 ) \u2013 2.3 % ( \u2191 ) +0.09 ( \u2193 ) \u2013 0.15 ( \u2193 ) \u2013 0.27 ( \u2193 ) Cyrillic ( 3 ) 3.6 % ( \u2193 ) 0.6 % ( \u2193 ) 2.8 % ( \u2193 ) \u2013 3.6 % ( \u2191 ) \u2013 0.6 % ( \u2191 ) \u2013 2.7 % ( \u2191 ) \u2013 0.21 ( \u2193 ) +0.14 ( \u2193 ) +1.52 ( \u2193 ) Arabic ( 2 ) 19.0 % ( \u2191 ) 19.2 % ( \u2191 ) 16.9 % ( \u2191 ) \u2013 16.1 % ( \u2193 ) \u2013 17.0 % ( \u2193 ) \u2013 15.5 % ( \u2193 ) +5.84 ( \u2191 ) +8.49 ( \u2191 ) +6.22 ( \u2191 ) Table 3 : Average UNK token percentage under the MBERTvocabulary ( left ) ; change in UNK token percentage from MBERTtoVAvocabularies ( center ) ; and average task performance change from LAPT toVA(right ) .", "entities": []}, {"text": "Averages are computed overall and within each script and language type , with comparisons to the overall average ; all UNK token percentages are computed on the respective training sets for illustration .", "entities": []}, {"text": "Note that Uyghur accounts for a large portion of the behavior of the Type 2 / Arabic rows .", "entities": []}, {"text": "correlation between the two factors in question and see that VAdelivers greater performance gains on Type 2 and Arabic - script languages compared to their Type 0/1 and Latin - script counterparts , respectively .", "entities": []}, {"text": "To quantify the strength of this association , we also compute the language - level Spearman correlation between the change in UNK token percentage on the unlabeled dataset7from the MBERTto VAvocabulary and the task - specific performance improvements from LAPT toVA .", "entities": [[15, 17, "MetricName", "Spearman correlation"]]}, {"text": "The resulting \u03c1 - values \u2013 0.29 for NER , 0.56 for POS tagging , and 0.81 for UD parsing \u2013 suggest that this set of factors is meaningful for some tasks , though additional and more fine - grained analysis in future work should give a more complete explanation .", "entities": [[8, 9, "TaskName", "NER"], [18, 19, "DatasetName", "UD"]]}, {"text": "3 Mix - in Specialization : VAand Transliteration We now expand on the observation made in \u00a7 2.3 regarding the difficulties that MBERT encounters when faced with unseen low - resource languages in non - Latin scripts because of its inability to model the script .", "entities": [[7, 8, "TaskName", "Transliteration"], [22, 23, "MethodName", "MBERT"]]}, {"text": "Having observed that VAis beneficial in such cases , we now investigate the interaction between this method and another specialization approach that targets this problem .", "entities": []}, {"text": "Specifically , we consider the transliteration methods of Muller et al .", "entities": []}, {"text": "( 2021 ) , in which unseen low - resource languages in non - Latin scripts are transliterated into the Latin script , often using transliteration schemes inspired by the Latin orthographies of languages related to the target language .", "entities": []}, {"text": "They hypothesize that the increased similarity in the languages \u2019 writing systems , combined with MBERT \u2019s overall Latin - centricity , provides increased opportunity for crosslingual transfer .", "entities": [[15, 16, "MethodName", "MBERT"]]}, {"text": "7We benchmark against the unlabeled dataset instead of task - specific ones for comparability .", "entities": []}, {"text": "We can view transliteration as a inverted form of vocabulary augmentation : instead of adapting the model to the needs of the data , the data is adjusted to meet the assumptions of the model .", "entities": []}, {"text": "Furthermore , the transliteration step is performed prior to pretraining MBERTon additional unlabeled data in the target language , the same stage at which VAis performed .", "entities": []}, {"text": "In both cases , the ultimate goal is identical : improving tokenization and more effectively using available data .", "entities": []}, {"text": "We can thus view transliteration and VAas two instantiations of a more general mix - in paradigm for model specialization , whereby various transformations ( mix - ins ) are applied to the data and/or model prior to performing additional pretraining .", "entities": []}, {"text": "These mix - ins target different components of the experimental pipeline , which naturally raises our second research question : RQ2 :", "entities": []}, {"text": "How do the VAand transliteration mix - ins forMBERTcompare and interact ?", "entities": []}, {"text": "3.1 Method and Experiments To test this research question , we apply transliteration and VAin succession and evaluate their compatibility .", "entities": []}, {"text": "Given unlabeled data in the target language , we first transliterate it into Latin script , which decreases but does not fully eliminate the issue of unseen wordpieces .", "entities": []}, {"text": "We then perform VA , generating the vocabulary for augmentation based on the transliterated data .", "entities": []}, {"text": "We evaluate on Meadow Mari and Uyghur , which are Type 2 languages where transliteration was successfully applied by Muller et al .", "entities": []}, {"text": "( 2021 )", "entities": []}, {"text": ".", "entities": []}, {"text": "To transliterate the data , we use the same methods as Muller et al .", "entities": []}, {"text": "( 2021 ): Meadow Mari uses the transliterate8package , while Uyghur uses 8https://pypi.org/project/ transliterate", "entities": []}, {"text": "( a ) POS tagging .", "entities": []}, {"text": "( b ) UD parsing .", "entities": [[3, 4, "DatasetName", "UD"]]}, {"text": "( c ) NER .", "entities": [[3, 4, "TaskName", "NER"]]}, {"text": "Figure 1 : Relationship between the change in UNK token percentage on task data and the change in task performance , from ( MBERT / LAPT toVA ) , with a 1 - degree line of best fit .", "entities": [[23, 24, "MethodName", "MBERT"]]}, {"text": "All vocabulary values are computed on the respective training sets .", "entities": []}, {"text": "a linguistically - motivated transliteration scheme9 aimed at associating Uyghur with Turkish .", "entities": []}, {"text": "We use the same training scheme , model architectures , and baselines as in \u00a7 2.2 , the only difference being the use of transliterated data .", "entities": []}, {"text": "This includes directly pretraining on the unlabeled data ( LAPT ) , which is comparable to the highest - performing transliteration models of Muller et al . ( 2021 ) .", "entities": []}, {"text": "Although our initial investigation of VAin \u00a7 2 also included non - Type 2 languages of other scripts , we omit them from our investigation based on the finding of Muller et al .", "entities": []}, {"text": "( 2021 ) that transliterating higherresource languages into Latin scripts is not beneficial .", "entities": []}, {"text": "3.2 Results Tab . 4 gives the results of our transliteration mix - in experiments .", "entities": []}, {"text": "For the MBERT - based models , both VAand transliteration provide strong improvements over their respective baselines .", "entities": [[2, 3, "MethodName", "MBERT"]]}, {"text": "Specifically , the improvements from LAPT toVAand LAPT toLAPT with transliteration are most pronounced .", "entities": []}, {"text": "This verifies the independent results of Chau et al . ( 2020 ) and Muller et al . ( 2021 ) and suggests that in the non - Latin low - resource setting , unadapted additional pretraining is insufficient , but that the mix - in stage between initial and additional pretraining is amenable to performance - improving modifications .", "entities": []}, {"text": "Unsurprisingly , transliteration provides no consistent improvement to the monolingual baselines , since the noisy transliteration process removes information without improving crosslingual alignment .", "entities": []}, {"text": "However , VAand transliteration appear to interact negatively .", "entities": []}, {"text": "Although VAwith transliteration im9https://github.com/benjamin-mlr/ mbert - unseen - languagesproves over plain VAfor Uyghur POS tagging and dependency parsing , it still slightly underperforms LAPT with transliteration for the latter .", "entities": [[4, 5, "MethodName", "mbert"], [16, 18, "TaskName", "dependency parsing"]]}, {"text": "For the two NER experiments , VAwith transliteration lags both methods independently .", "entities": [[3, 4, "TaskName", "NER"]]}, {"text": "One possible explanation is that transliteration into Latin script serves as implicit vocabulary augmentation , with embeddings that have already been updated during the initial pretraining stage ; as a result , the two sources of augmentation conflict .", "entities": []}, {"text": "Alternatively , since the transliteration process merges certain characters that are distinct in the original script , VAmay augment the vocabulary with misleading character clusters .", "entities": []}, {"text": "Either way , additional vocabulary augmentation is generally not as useful when combined with transliteration , answering RQ2 .", "entities": []}, {"text": "Nonetheless , additional investigation into the optimal amount of vocabulary augmentation might yield a configuration that is consistently complementary to transliteration and is an interesting direction for future work .", "entities": []}, {"text": "Furthermore , designing linguistically - informed transliteration schemes like those devised by Muller et al . ( 2021 ) for Uyghur requires large amounts of time and domain knowledge .", "entities": []}, {"text": "VA \u2019s fully data - driven nature and relatively comparable performance suggest that it achieves an appealing balance between performance gain and implementation difficulty .", "entities": []}, {"text": "4 Related Work Our work follows a long line of studies investigating the performance of multilingual language models like MBERTin various settings .", "entities": []}, {"text": "The exact source of such models \u2019 crosslingual ability is contested : early studies attributed MBERT \u2019s success to vocabulary overlap between languages ( Cao et al . , 2020 ;", "entities": [[15, 16, "MethodName", "MBERT"]]}, {"text": "Pires et al . , 2019 ; Wu and Dredze , 2019 ) ,", "entities": []}, {"text": "Rep. MHR ( NER ) UG(NER ) UG(POS ) UG(UD )", "entities": [[3, 4, "TaskName", "NER"]]}, {"text": "FAST T 35.28 \u219241.32 ( +6.04 ) \u2013 89.45 \u219289.03 ( \u2013 0.42 ) 54.52\u219254.45 ( \u2013 0.07 ) BERT 54.17\u219248.45 ( \u2013 5.72 ) 61.54\u219263.05 ( +1.51 ) 90.87\u219290.76 ( \u2013 0.09 ) 60.34\u219260.08 ( \u2013 0.26 ) MBERT 61.85\u219263.84 ( +1.99 ) 50.76\u219256.80 ( +6.04 ) 78.07\u219291.34 ( +13.27 ) 47.70\u219265.85 ( +18.15 ) LAPT 59.17\u219263.68 ( +4.51 ) 56.76\u219267.57 ( +10.81 ) 79.88\u219292.59 ( +12.71 ) 50.67\u219269.39 ( +18.72 ) VA 64.23\u219263.19 ( \u2013 1.04 ) 68.93\u219267.10 ( \u2013 1.83 ) 91.49\u219292.64 ( +1.15 ) 67.55\u219268.58 ( +1.03 ) Table 4 : Comparison of model performance before and after transliteration .", "entities": [[19, 20, "MethodName", "BERT"], [39, 40, "MethodName", "MBERT"]]}, {"text": "Bolded results are the maximum for each language - task pair . \u2013 indicates that a model did not converge .", "entities": []}, {"text": "but subsequent studies find typological similarity and parameter sharing to be better explanations ( Conneau et al . , 2020b ; K et al . , 2020 ) .", "entities": []}, {"text": "Nonetheless , past work has consistently highlighted the limitations of multilingual models in the context of low - resource languages .", "entities": []}, {"text": "Conneau et al .", "entities": []}, {"text": "( 2020a ) highlight the tension between crosslingual transfer and per - language model capacity , which poses a challenge for low - resource languages that require both .", "entities": []}, {"text": "Indeed , Wu and Dredze ( 2020 ) find that MBERT is unable to outperform baselines in the lowest - resource seen languages .", "entities": [[10, 11, "MethodName", "MBERT"]]}, {"text": "Our experiments build off these insights , which motivate the development of methods for adapting MBERTto target low - resource languages .", "entities": []}, {"text": "Adapting Language Models Several prior studies have proposed methods for adapting pretrained models to a downstream task .", "entities": []}, {"text": "The simplest of these is to perform additional pretraining on unlabeled data in the target language ( Chau et al . , 2020 ; Muller et al . , 2021 ;", "entities": []}, {"text": "Pfeiffer et al . , 2020 ) , which in turn builds off similar approaches for domain adaptation ( Gururangan et al . , 2020 ; Han and Eisenstein , 2019 ) .", "entities": [[16, 18, "TaskName", "domain adaptation"]]}, {"text": "Recent work uses one or more of these additional pretraining stages to specifically train modular adapter layers for specific tasks or languages , with the goal of maintaining a language - agnostic model while improving performance on individual languages ( Pfeiffer et al . , 2020 , 2021a ; Vidoni et al . , 2020 ) .", "entities": []}, {"text": "However , as Muller et al . ( 2021 ) note , the typological diversity of the world \u2019s languages ultimately limits the viability of this approach .", "entities": []}, {"text": "On the other hand , many adaptation techniques have focused on improving representation of the target language by modifying the model \u2019s vocabulary or tokenization schemes ( Chung et al . , 2020 ; Clark et al . , 2021 ; Wang et al . , 2021 ) .", "entities": []}, {"text": "This is wellmotivated : Artetxe et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) emphasize representation in the vocabulary as a key factor for effective crosslingual transfer , while Rust et al .", "entities": []}, {"text": "( 2021 ) find that MBERT \u2019s tokenization scheme for many languages is subpar .", "entities": [[5, 6, "MethodName", "MBERT"]]}, {"text": "Pfeiffer et al .", "entities": []}, {"text": "( 2021b ) furtherobserve that for languages with unseen scripts , a large proportion of the language is mapped to the generic \u201c unknown \u201d wordpiece , and they propose a matrix factorization - based approach to improve script representation .", "entities": [[25, 26, "MethodName", "wordpiece"]]}, {"text": "Wang et al .", "entities": []}, {"text": "( 2020 ) extend MBERT \u2019s vocabulary with an entire new vocabulary in the target language to facilitate zero - shot transfer to low - resource languages from English .", "entities": [[4, 5, "MethodName", "MBERT"]]}, {"text": "The present study most closely derives from Chau et al .", "entities": []}, {"text": "( 2020 ) , who select 99 wordpieces with the greatest amount of coverage to augment MBERT \u2019s vocabulary while preserving the remainder ; and Muller et al .", "entities": [[16, 17, "MethodName", "MBERT"]]}, {"text": "( 2021 ) , who transliterate target language data into Latin script to improve vocabulary coverage .", "entities": []}, {"text": "We deliver new insights on the effectiveness and applicability of these methods .", "entities": []}, {"text": "5 Conclusion We explore the interactions between vocabulary augmentation and script transliteration for specializing multilingual contextual word representations in low - resource settings .", "entities": []}, {"text": "We confirm vocabulary augmentation \u2019s effectiveness on multiple languages , scripts , and tasks ; identify the mix - in stage as amenable to specialization ; and observe a negative interaction between vocabulary augmentation and script transliteration .", "entities": []}, {"text": "Our findings highlight several open questions in model specialization and low - resource natural language processing at large , motivating further study in this area .", "entities": []}, {"text": "Future directions for investigation are manifold .", "entities": []}, {"text": "In particular , our results in this work unify the separate findings of past works , which use MBERTas a case study ; a natural continuation would extend these methods to a broader set of multilingual models , such as mT5 ( Xue et al . , 2021 ) and XLM - R ( Conneau et al . , 2020a ) , in order to obtain a clearer understanding of the factors behind specialization methods \u2019 patterns of success .", "entities": [[40, 41, "MethodName", "mT5"], [50, 51, "MethodName", "XLM"]]}, {"text": "While we intentionally choose a set of small unlabeled datasets to evaluate on a setting applicable to the vast majority of the world \u2019s low - resource languages , we acknowl-", "entities": []}, {"text": "edge great variation in the amount of unlabeled data available in different languages .", "entities": []}, {"text": "Continued study on the applicability of these methods to datasets of different sizes is an important future step .", "entities": []}, {"text": "An interesting direction of work is to train multilingual models on data where script respresentation is more balanced , which might also allow for different output scripts for transliteration .", "entities": []}, {"text": "Given that the mix - in stage is an effective opportunity to specialize models to target languages , constructing mix - ins at both the data and model level that are complementary by design has potential to be beneficial .", "entities": []}, {"text": "Finally , future work might shed light on the interaction between different configurations of the adaptations studied here ( e.g. , the number of wordpiece types used in vocabulary augmentation ) .", "entities": [[24, 25, "MethodName", "wordpiece"]]}, {"text": "Acknowledgments We thank Jungo Kasai , Phoebe Mulcaire , and members of UW NLP for their helpful comments on preliminary versions of this paper .", "entities": []}, {"text": "We also thank Benjamin Muller for insightful discussions and providing details about transliteration methods and baselines .", "entities": []}, {"text": "Finally , we thank the anonymous reviewers for their helpful remarks .", "entities": []}, {"text": "References Judit \u00c1cs . 2019 .", "entities": []}, {"text": "Exploring BERT \u2019s vocabulary .", "entities": [[1, 2, "MethodName", "BERT"]]}, {"text": "Mikel Artetxe , Sebastian Ruder , and Dani Yogatama .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "On the cross - lingual transferability of monolingual representations .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of ACL .", "entities": []}, {"text": "Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Enriching word vectors with subword information .", "entities": []}, {"text": "TACL , 5:135\u2013146 .", "entities": []}, {"text": "Steven Cao , Nikita Kitaev , and Dan Klein .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multilingual alignment of contextual word representations .", "entities": []}, {"text": "InProc . of ICLR .", "entities": []}, {"text": "Ethan C. Chau , Lucy H. Lin , and Noah A. Smith .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Parsing with multilingual BERT , a small corpus , and a small treebank .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "In Findings of ACL : EMNLP .", "entities": []}, {"text": "Hyung Won Chung , Dan Garrette , Kiat Chuan Tan , and Jason Riesa .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Improving multilingual models with language - clustered vocabularies .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLP .", "entities": []}, {"text": "Jonathan H. Clark , Dan Garrette , Iulia Turc , and John Wieting .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Canine : Pre - training an efficient tokenization - free encoder for language representation .", "entities": []}, {"text": "Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , FranciscoGuzm\u00e1n , Edouard Grave , Myle Ott , Luke Zettlemoyer , and Veselin Stoyanov . 2020a .", "entities": []}, {"text": "Unsupervised cross - lingual representation learning at scale .", "entities": [[4, 6, "TaskName", "representation learning"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ACL", "entities": []}, {"text": ".", "entities": []}, {"text": "Alexis Conneau , Shijie Wu , Haoran Li , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Emerging cross - lingual structure in pretrained language models .", "entities": [[6, 9, "TaskName", "pretrained language models"]]}, {"text": "InProc . of ACL .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proc .", "entities": []}, {"text": "of NAACL .", "entities": []}, {"text": "Timothy Dozat and Christopher D. Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Deep biaffine attention for neural dependency parsing .", "entities": [[5, 7, "TaskName", "dependency parsing"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ICLR .", "entities": []}, {"text": "Matt Gardner , Joel Grus , Mark Neumann , Oyvind Tafjord , Pradeep Dasigi , Nelson F. Liu , Matthew Peters , Michael Schmitz , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "AllenNLP : A deep semantic natural language processing platform .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of NLP - OSS .", "entities": []}, {"text": "Suchin Gururangan , Ana Marasovi \u00b4 c , Swabha Swayamdipta , Kyle Lo , Iz Beltagy , Doug Downey , and Noah A. Smith .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Do n\u2019t stop pretraining : Adapt language models to domains and tasks .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of ACL .", "entities": []}, {"text": "Xiaochuang Han and Jacob Eisenstein .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unsupervised domain adaptation of contextualized embeddings for sequence labeling .", "entities": [[0, 3, "TaskName", "Unsupervised domain adaptation"]]}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLPIJCNLP .", "entities": []}, {"text": "Haoyang Huang , Yaobo Liang , Nan Duan , Ming Gong , Linjun Shou , Daxin Jiang , and Ming Zhou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unicoder :", "entities": []}, {"text": "A universal language encoder by pretraining with multiple cross - lingual tasks .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLP - IJCNLP .", "entities": []}, {"text": "Karthikeyan K , Zihan Wang , Stephen Mayhew , and Dan Roth . 2020 .", "entities": []}, {"text": "Cross - lingual ability of multilingual BERT : An empirical study .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ICLR .", "entities": []}, {"text": "Dan Kondratyuk and Milan Straka . 2019 .", "entities": []}, {"text": "75 languages , 1 model : Parsing universal dependencies universally .", "entities": [[7, 9, "DatasetName", "universal dependencies"]]}, {"text": "InProc . of EMNLP - IJCNLP .", "entities": []}, {"text": "Taku Kudo and John Richardson .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SentencePiece : A simple and language independent subword tokenizer and detokenizer for neural text processing .", "entities": [[0, 1, "MethodName", "SentencePiece"]]}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLP", "entities": []}, {"text": ".", "entities": []}, {"text": "Guillaume Lample and Alexis Conneau .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crosslingual language model pretraining .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of NeurIPS .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "Benjamin Muller , Antonios Anastasopoulos , Beno\u00eet Sagot , and Djam\u00e9 Seddah . 2021 .", "entities": []}, {"text": "When being unseen from mBERT is just the beginning :", "entities": [[4, 5, "MethodName", "mBERT"]]}, {"text": "Handling new languages with multilingual language models .", "entities": []}, {"text": "InProc . of NAACL - HLT .", "entities": []}, {"text": "Joakim Nivre , Marie - Catherine de Marneffe , Filip Ginter , Jan Haji \u02c7c , Christopher D. Manning , Sampo Pyysalo , Sebastian Schuster , Francis Tyers , and Daniel Zeman .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Universal Dependencies v2 : An evergrowing multilingual treebank collection .", "entities": [[0, 2, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proc .", "entities": []}, {"text": "of LREC .", "entities": []}, {"text": "Xiaoman Pan , Boliang Zhang , Jonathan May , Joel Nothman , Kevin Knight , and Heng Ji . 2017 .", "entities": []}, {"text": "Cross - lingual name tagging and linking for 282 languages .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of ACL .", "entities": []}, {"text": "Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of NAACL - HLT", "entities": []}, {"text": ".", "entities": []}, {"text": "Jonas Pfeiffer , Aishwarya Kamath , Andreas R\u00fcckl\u00e9 , Kyunghyun Cho , and Iryna Gurevych .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "AdapterFusion : Non - destructive task composition for transfer learning .", "entities": [[8, 10, "TaskName", "transfer learning"]]}, {"text": "In Proc .", "entities": []}, {"text": "of EACL .", "entities": []}, {"text": "Jonas Pfeiffer , Ivan Vuli \u00b4 c , Iryna Gurevych , and Sebastian Ruder .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "MAD - X : An Adapter - Based Framework for Multi - Task Cross - Lingual Transfer .", "entities": [[0, 1, "DatasetName", "MAD"], [5, 6, "MethodName", "Adapter"], [13, 17, "TaskName", "Cross - Lingual Transfer"]]}, {"text": "InProc . of EMNLP .", "entities": []}, {"text": "Jonas Pfeiffer , Ivan Vuli \u00b4 c , Iryna Gurevych , and Sebastian Ruder .", "entities": []}, {"text": "2021b .", "entities": []}, {"text": "Unks everywhere : Adapting multilingual language models to new scripts .", "entities": []}, {"text": "Telmo Pires , Eva Schlinger , and Dan Garrette .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "How multilingual is multilingual BERT ?", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ACL .", "entities": []}, {"text": "Afshin Rahimi , Yuan Li , and Trevor Cohn . 2019 .", "entities": []}, {"text": "Massively multilingual transfer for NER .", "entities": [[4, 5, "TaskName", "NER"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ACL .", "entities": []}, {"text": "Phillip Rust , Jonas Pfeiffer , Ivan Vuli \u00b4 c , Sebastian Ruder , and Iryna Gurevych .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "How good is your tokenizer ?", "entities": []}, {"text": "on the monolingual performance of multilingual language models .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of ACL - IJCNLP .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of NeurIPS .", "entities": []}, {"text": "Marko Vidoni , Ivan Vuli \u00b4 c , and Goran Glava\u0161 .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Orthogonal language and task adapters in zero - shot cross - lingual transfer .", "entities": [[6, 13, "TaskName", "zero - shot cross - lingual transfer"]]}, {"text": "Xinyi Wang , Sebastian Ruder , and Graham Neubig .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Multi - view subword regularization .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of NAACL - HLT", "entities": []}, {"text": ".", "entities": []}, {"text": "Zihan Wang , Karthikeyan K , Stephen Mayhew , and Dan Roth . 2020 .", "entities": []}, {"text": "Extending multilingual BERT to lowresource languages .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "In Findings of ACL : EMNLP .Thomas", "entities": []}, {"text": "Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , R\u00e9mi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander M. Rush .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLP .", "entities": []}, {"text": "Shijie Wu and Mark Dredze .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Beto , bentz , becas : The surprising cross - lingual effectiveness of BERT .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "InProc . of EMNLP - IJCNLP .", "entities": []}, {"text": "Shijie Wu and Mark Dredze .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Are all languages created equal in multilingual BERT ?", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "In Proc .", "entities": []}, {"text": "of RepL4NLP .", "entities": []}, {"text": "Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V .", "entities": []}, {"text": "Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , Jeff Klingner , Apurva Shah , Melvin Johnson , Xiaobing Liu , \u0141ukasz Kaiser , Stephan Gouws , Yoshikiyo Kato , Taku Kudo , Hideto Kazawa , Keith Stevens , George Kurian , Nishant Patil , Wei Wang , Cliff Young , Jason Smith , Jason Riesa , Alex Rudnick , Oriol Vinyals , Greg Corrado , Macduff Hughes , and Jeffrey Dean .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Google \u2019s neural machine translation system : Bridging the gap between human and machine translation .", "entities": [[0, 1, "DatasetName", "Google"], [3, 5, "TaskName", "machine translation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "Linting Xue , Noah Constant , Adam Roberts , Mihir Kale , Rami Al - Rfou , Aditya Siddhant , Aditya Barua , and Colin Raffel . 2021 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "mT5 :", "entities": [[0, 1, "MethodName", "mT5"]]}, {"text": "A massively multilingual pre - trained text - to - text transformer .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of NAACL - HLT", "entities": []}, {"text": ".", "entities": []}, {"text": "Daniel Zeman , Joakim Nivre , Mitchell Abrams , No\u00ebmi Aepli , \u017deljko Agi \u00b4 c , Lars Ahrenberg , Gabriel \u02d9e Aleksandravi \u02c7ci\u00afut\u02d9e , Lene Antonsen , Katya Aplonova , Maria Jesus Aranzabe , Gashaw Arutie , Masayuki Asahara , Luma Ateyah , Mohammed Attia , Aitziber Atutxa , Liesbeth Augustinus , Elena Badmaeva , Miguel Ballesteros , Esha Banerjee , Sebastian Bank , Verginica Barbu Mititelu , Victoria Basmov , Colin Batchelor , John Bauer , Sandra Bellato , Kepa Bengoetxea , Yevgeni Berzak , Irshad Ahmad Bhat , Riyaz Ahmad Bhat , Erica Biagetti , Eckhard Bick , Agn\u02d9e Bielinskien \u02d9e , Rogier Blokland , Victoria Bobicev , Lo\u00efc Boizou , Emanuel Borges V\u00f6lker , Carl B\u00f6rstell , Cristina Bosco , Gosse Bouma , Sam Bowman , Adriane Boyd , Kristina Brokait \u02d9e , Aljoscha Burchardt , Marie Candito , Bernard Caron , Gauthier Caron , Tatiana Cavalcanti , G\u00fcl\u00b8 sen Cebiro \u02d8glu Eryi\u02d8git , Flavio Massimiliano Cecchini , Giuseppe G. A. Celano , Slavom\u00edr \u02c7C\u00e9pl\u00f6 , Savas Cetin , Fabricio Chalub , Jinho Choi , Yongseok Cho , Jayeol Chun , Alessandra T. Cignarella , Silvie Cinkov\u00e1 , Aur\u00e9lie Collomb , \u00c7a \u02d8gr\u0131 \u00c7\u00f6ltekin , Miriam Connor , Marine Courtin , Elizabeth Davidson , Marie - Catherine de Marneffe , Valeria de Paiva , Elvis de Souza , Arantza Diaz de Ilarraza , Carly Dickerson , Bamba Dione , Peter Dirix , Kaja Dobrovoljc , Timothy Dozat , Kira Droganova , Puneet Dwivedi , Hanne Eckhoff , Marhaba Eli , Ali Elkahky , Binyam Ephrem , Olga", "entities": [[32, 33, "TaskName", "Jesus"]]}, {"text": "Erina , Toma\u017e Erjavec , Aline Etienne , Wograine Evelyn , Rich\u00e1rd Farkas , Hector Fernandez Alcalde , Jennifer Foster , Cl\u00e1udia Freitas , Kazunori Fujita , Katar\u00edna Gajdo\u0161ov\u00e1 , Daniel Galbraith , Marcos Garcia , Moa G\u00e4rdenfors , Sebastian Garza , Kim Gerdes , Filip Ginter , Iakes Goenaga , Koldo Gojenola , Memduh G\u00f6k\u0131rmak , Yoav Goldberg , Xavier G\u00f3mez Guinovart , Berta Gonz\u00e1lez Saavedra , Bernadeta Grici \u00afut\u02d9e , Matias Grioni , Normunds Gr\u00afuz\u00af\u0131tis , Bruno Guillaume , C\u00e9line Guillot - Barbance , Nizar Habash , Jan Haji \u02c7c , Jan Haji \u02c7c jr . , Mika H\u00e4m\u00e4l\u00e4inen , Linh H\u00e0 M \u02dcy , Na - Rae Han , Kim Harris , Dag Haug , Johannes Heinecke , Felix Hennig , Barbora Hladk\u00e1 , Jaroslava Hlav\u00e1 \u02c7cov\u00e1 , Florinel Hociung , Petter Hohle , Jena Hwang , Takumi Ikeda , Radu Ion , Elena Irimia , O.l\u00e1j\u00edd\u00e9 Ishola , Tom\u00e1\u0161 Jel\u00ednek , Anders Johannsen , Fredrik J\u00f8rgensen , Markus Juutinen , H\u00fcner Ka\u00b8 s\u0131kara , Andre Kaasen , Nadezhda Kabaeva , Sylvain Kahane , Hiroshi Kanayama , Jenna Kanerva , Boris Katz , Tolga Kayadelen , Jessica Kenney , V\u00e1clava Kettnerov\u00e1 , Jesse Kirchner , Elena Klementieva , Arne K\u00f6hn , Kamil Kopacewicz , Natalia Kotsyba , Jolanta Kovalevskait \u02d9e , Simon Krek , Sookyoung Kwak , Veronika Laippala , Lorenzo Lambertino , Lucia Lam , Tatiana Lando , Septina Dian Larasati , Alexei Lavrentiev , John Lee , Phuong L\u00ea H ` \u00f4ng , Alessandro Lenci , Saran Lertpradit , Herman Leung , Cheuk Ying Li , Josie Li , Keying Li , KyungTae Lim , Maria Liovina , Yuan Li , Nikola Ljube\u0161i \u00b4 c , Olga Loginova , Olga Lyashevskaya , Teresa Lynn , Vivien Macketanz , Aibek Makazhanov , Michael Mandl , Christopher Manning , Ruli Manurung , C \u02d8at\u02d8alina M \u02d8ar\u02d8anduc , David Mare \u02c7cek , Katrin Marheinecke , H\u00e9ctor Mart\u00ednez Alonso , Andr\u00e9 Martins , Jan Ma\u0161ek , Yuji Matsumoto , Ryan McDonald , Sarah McGuinness , Gustavo Mendon\u00e7a , Niko Miekka , Margarita Misirpashayeva , Anna Missil\u00e4 , C \u02d8at\u02d8alin", "entities": []}, {"text": "Mititelu , Maria Mitrofan , Yusuke Miyao , Simonetta Montemagni , Amir More , Laura Moreno Romero , Keiko Sophie Mori , Tomohiko Morioka , Shinsuke Mori , Shigeki Moro , Bjartur Mortensen , Bohdan Moskalevskyi , Kadri Muischnek , Robert Munro , Yugo Murawaki , Kaili M\u00fc\u00fcrisep , Pinkey Nainwani , Juan Ignacio Navarro Hor\u00f1iacek , Anna Nedoluzhko , Gunta Ne\u0161pore - B \u00aferzkalne , Luong Nguy \u02dc\u00ean Th i. , Huy ` \u00ean Nguy \u02dc\u00ean Th i. Minh , Yoshihiro Nikaido , Vitaly Nikolaev , Rattima Nitisaroj , Hanna Nurmi , Stina Ojala , Atul Kr .", "entities": []}, {"text": "Ojha , Ad\u00e9day ` o. Ol\u00fa\u00f2kun , Mai Omura , Petya Osenova , Robert \u00d6stling , Lilja \u00d8vrelid , Niko Partanen , Elena Pascual , Marco Passarotti , Agnieszka Patejuk , Guilherme Paulino - Passos , Angelika Peljak\u0141api \u00b4 nska , Siyao Peng , Cenel - Augusto Perez , Guy Perrier , Daria Petrova , Slav Petrov , Jason Phelan , Jussi Piitulainen , Tommi A Pirinen , Emily Pitler , Barbara Plank , Thierry Poibeau , Larisa Ponomareva , Martin Popel , Lauma Pretkalni n , a , Sophie Pr\u00e9vost , Prokopis Prokopidis , Adam Przepi\u00f3rkowski , Tiina Puolakainen , Sampo Pyysalo , Peng Qi , Andriela R\u00e4\u00e4bis , Alexandre Rademaker , Loganathan Ramasamy , Taraka Rama , Carlos Ramisch , Vinit Ravishankar , Livy Real , Siva Reddy , Georg Rehm , Ivan Riabov , Michael Rie\u00dfler , Erika Rimkut \u02d9e , Larissa Rinaldi , Laura Rituma , Luisa Rocha , Mykhailo Ro - manenko , Rudolf Rosa , Davide Rovati , Valentin Ros , ca , Olga Rudina , Jack Rueter , Shoval Sadde , Beno\u00eet Sagot , Shadi Saleh , Alessio Salomoni , Tanja Samard\u017ei \u00b4 c , Stephanie Samson , Manuela Sanguinetti , Dage S\u00e4rg , Baiba Saul \u00af\u0131te , Yanin Sawanakunanon , Nathan Schneider , Sebastian Schuster , Djam\u00e9 Seddah , Wolfgang Seeker , Mojgan Seraji , Mo Shen , Atsuko Shimada , Hiroyuki Shirasu , Muh Shohibussirri , Dmitry Sichinava , Aline Silveira , Natalia Silveira , Maria Simi , Radu Simionescu , Katalin Simk\u00f3 , M\u00e1ria \u0160imkov\u00e1 , Kiril Simov , Aaron Smith , Isabela Soares - Bastos , Carolyn Spadine , Antonio Stella , Milan Straka , Jana Strnadov\u00e1 , Alane Suhr , Umut Sulubacak , Shingo Suzuki , Zsolt Sz\u00e1nt\u00f3 , Dima Taji , Yuta Takahashi , Fabio Tamburini , Takaaki Tanaka , Isabelle Tellier , Guillaume Thomas , Liisi Torga , Trond Trosterud , Anna Trukhina , Reut Tsarfaty , Francis Tyers , Sumire Uematsu , Zde \u02c7nka Ure\u0161ov\u00e1 , Larraitz Uria , Hans Uszkoreit , Andrius Utka , Sowmya Vajjala , Daniel van Niekerk , Gertjan van Noord , Viktor Varga , Eric Villemonte de la Clergerie , Veronika Vincze , Lars Wallin , Abigail Walsh , Jing Xian Wang , Jonathan North Washington , Maximilan Wendt , Seyi Williams , Mats Wir\u00e9n , Christian Wittern , Tsegay Woldemariam , Tak - sum Wong , Alina Wr\u00f3blewska , Mary Yako , Naoki Yamazaki , Chunxiao Yan , Koichi Yasuoka , Marat M. Yavrumyan , Zhuoran Yu , Zden \u02c7ek", "entities": [[95, 96, "MethodName", "Adam"]]}, {"text": "\u017dabokrtsk\u00fd , Amir Zeldes , Manying Zhang , and Hanzhi Zhu . 2019 .", "entities": []}, {"text": "Universal dependencies 2.5 .", "entities": [[0, 2, "DatasetName", "Universal dependencies"]]}, {"text": "LINDAT / CLARIAH - CZ digital library at the Institute of Formal and Applied Linguistics ( \u00daFAL ) , Faculty of Mathematics and Physics , Charles University .", "entities": []}]