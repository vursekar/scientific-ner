[{"text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , pages 665\u2013677 Vancouver , Canada , July 30 - August 4 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1062Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , pages 665\u2013677 Vancouver , Canada , July 30 - August 4 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1062", "entities": []}, {"text": "Hybrid Code Networks : practical and ef\ufb01cient end - to - end dialog control with supervised and reinforcement learning Jason D. Williams Microsoft Research jason.williams@microsoft.comKavosh Asadi Brown University kavosh@brown.eduGeoffrey Zweig\u2217 Microsoft Research g2zweig@gmail.com", "entities": []}, {"text": "Abstract End - to - end learning of recurrent neural networks ( RNNs ) is an attractive solution for dialog systems ; however , current techniques are data - intensive and require thousands of dialogs to learn simple behaviors .", "entities": []}, {"text": "We introduce Hybrid Code Networks ( HCNs ) , which combine an RNN with domain - speci\ufb01c knowledge encoded as software and system action templates .", "entities": []}, {"text": "Compared to existing end - toend approaches , HCNs considerably reduce the amount of training data required , while retaining the key bene\ufb01t of inferring a latent representation of dialog state .", "entities": []}, {"text": "In addition , HCNs can be optimized with supervised learning , reinforcement learning , or a mixture of both .", "entities": []}, {"text": "HCNs attain stateof - the - art performance on the bAbI dialog dataset ( Bordes and Weston , 2016 ) , and outperform two commercially deployed customer - facing dialog systems .", "entities": []}, {"text": "1 Introduction Task - oriented dialog systems help a user to accomplish some goal using natural language , such as making a restaurant reservation , getting technical support , or placing a phonecall .", "entities": []}, {"text": "Historically , these dialog systems have been built as a pipeline , with modules for language understanding , state tracking , action selection , and language generation .", "entities": []}, {"text": "However , dependencies between modules introduce considerable complexity \u2013 for example , it is often unclear how to de\ufb01ne the dialog state and what history to maintain , yet action selection relies exclusively on the state for input .", "entities": []}, {"text": "Moreover , training each module requires specialized labels .", "entities": []}, {"text": "\u2217Currently at JPMorgan ChaseRecently , end - to - end approaches have trained recurrent neural networks ( RNNs ) directly on text transcripts of dialogs .", "entities": []}, {"text": "A key bene\ufb01t is that the RNN infers a latent representation of state , obviating the need for state labels .", "entities": []}, {"text": "However , end - to - end methods lack a general mechanism for injecting domain knowledge and constraints .", "entities": []}, {"text": "For example , simple operations like sorting a list of database results or updating a dictionary of entities can expressed in a few lines of software , yet may take thousands of dialogs to learn .", "entities": []}, {"text": "Moreover , in some practical settings , programmed constraints are essential \u2013 for example , a banking dialog system would require that a user is logged in before they can retrieve account information .", "entities": []}, {"text": "This paper presents a model for end - to - end learning , called Hybrid Code Networks ( HCNs ) which addresses these problems .", "entities": []}, {"text": "In addition to learning an RNN , HCNs also allow a developer to express domain knowledge via software and action templates .", "entities": []}, {"text": "Experiments show that , compared to existing recurrent end - to - end techniques , HCNs achieve the same performance with considerably less training data , while retaining the key bene\ufb01t of end - to - end trainability .", "entities": []}, {"text": "Moreover , the neural network can be trained with supervised learning or reinforcement learning , by changing the gradient update applied .", "entities": []}, {"text": "This paper is organized as follows .", "entities": []}, {"text": "Section 2 describes the model , and Section 3 compares the model to related work .", "entities": []}, {"text": "Section 4 applies HCNs to the bAbI dialog dataset ( Bordes and Weston , 2016 ) .", "entities": []}, {"text": "Section 5 then applies the method to real customer support domains at our company .", "entities": []}, {"text": "Section 6 illustrates how HCNs can be optimized with reinforcement learning , and Section 7 concludes.665", "entities": []}, {"text": "What \u2019s the weather   this week in Seattle ?", "entities": []}, {"text": "Choose   action   templateEntity   outputAction   type?Anything else ?", "entities": []}, {"text": "textDense +   softmaxRNNEntity tracking Bag of words vectorForecast ( ) 0.93Normalization .", "entities": []}, {"text": "X Anything else ?", "entities": []}, {"text": "0.07 < city > , right ?", "entities": []}, {"text": "0.00API call API WeatherBot Utterance embedding   234 7 9 11 1415 186 12 1317Action mask Context   features Fully - formed   actionAPI   result15Entity   extraction t+1 t+1810 t+116Figure 1 : Operational loop .", "entities": []}, {"text": "Trapezoids refer to programmatic code provided by the software developer , and shaded boxes are trainable components .", "entities": []}, {"text": "Vertical bars under \u201c 6 \u201d represent concatenated vectors which form the input to the RNN .", "entities": []}, {"text": "2 Model description At a high level , the four components of a Hybrid Code Network are a recurrent neural network ; domain - speci\ufb01c software ; domain - speci\ufb01c action templates ; and a conventional entity extraction module for identifying entity mentions in text .", "entities": []}, {"text": "Both the RNN and the developer code maintain state .", "entities": []}, {"text": "Each action template can be a textual communicative action or an API call .", "entities": []}, {"text": "The HCN model is summarized in Figure 1 .", "entities": []}, {"text": "The cycle begins when the user provides an utterance , as text ( step 1 ) .", "entities": []}, {"text": "The utterance is featurized in several ways .", "entities": []}, {"text": "First , a bag of words vector is formed ( step 2 ) .", "entities": []}, {"text": "Second , an utterance embedding is formed , using a pre - built utterance embedding model ( step 3 ) .", "entities": []}, {"text": "Third , an entity extraction module identi\ufb01es entity mentions ( step 4 ) \u2013 for example , identifying \u201c Jennifer Jones \u201d as a < name > entity .", "entities": []}, {"text": "The text and entity mentions are then passed to \u201c Entity tracking \u201d code provided by the developer ( step 5 ) , which grounds and maintains entities \u2013 for example , mapping the text \u201c Jennifer Jones \u201d to a speci\ufb01c row in a database .", "entities": []}, {"text": "This code can optionally return an \u201c action mask \u201d , indicating actions which are permitted at the current timestep , as a bit vector .", "entities": []}, {"text": "For example , if a target phone number has not yet been identi\ufb01ed , the API action to place a phone call may be masked .", "entities": []}, {"text": "It can also optionally return \u201c context features \u201d which are features the developer thinks will be useful for distinguish - ing among actions , such as which entities are currently present and which are absent .", "entities": []}, {"text": "The feature components from steps 1 - 5 are concatenated to form a feature vector ( step 6 ) .", "entities": []}, {"text": "This vector is passed to an RNN , such as a long shortterm memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) or gated recurrent unit ( GRU ) ( Chung et al . , 2014 ) .", "entities": [[15, 16, "MethodName", "LSTM"], [25, 28, "MethodName", "gated recurrent unit"], [29, 30, "MethodName", "GRU"]]}, {"text": "The RNN computes a hidden state ( vector ) , which is retained for the next timestep ( step 8) , and passed to a dense layer with a softmax activation , with output dimension equal to the number of distinct system action templates ( step 9).1Thus the output of step 9 is a distribution over action templates .", "entities": [[29, 30, "MethodName", "softmax"]]}, {"text": "Next , the action mask is applied as an element - wise multiplication , and the result is normalized back to a probability distribution ( step 10 ) \u2013 this forces non - permitted actions to take on probability zero .", "entities": []}, {"text": "From the resulting distribution ( step 11 ) , an action is selected ( step 12 ) .", "entities": []}, {"text": "When RL is active , exploration is required , so in this case an action is sampled from the distribution ; when RL is not active , the best action should be chosen , and so the action with the highest probability is always selected .", "entities": []}, {"text": "The selected action is next passed to \u201c Entity output \u201d developer code that can substitute in entities ( step 13 ) and produce a fully - formed action \u2013 for example , mapping the template \u201c < city > , 1Implementation details for the RNN such as size , loss , etc . are given with each experiment in Sections 4 - 6.666", "entities": [[50, 51, "MetricName", "loss"]]}, {"text": "right ? \u201d to \u201c Seattle , right ? \u201d .", "entities": []}, {"text": "In step 14 , control branches depending on the type of the action : if it is an API action , the corresponding API call in the developer code is invoked ( step 15 ) \u2013 for example , to render rich content to the user .", "entities": []}, {"text": "APIs can act as sensors and return features relevant to the dialog , so these can be added to the feature vector in the next timestep ( step 16 ) .", "entities": []}, {"text": "If the action is text , it is rendered to the user ( step 17 ) , and cycle then repeats .", "entities": []}, {"text": "The action taken is provided as a feature to the RNN in the next timestep ( step 18 ) .", "entities": []}, {"text": "3 Related work Broadly there are two lines of work applying machine learning to dialog control .", "entities": []}, {"text": "The \ufb01rst decomposes a dialog system into a pipeline , typically including language understanding , dialog state tracking , action selection policy , and language generation ( Levin et al . , 2000 ; Singh et al . , 2002 ; Williams and Young , 2007 ; Williams , 2008 ;", "entities": []}, {"text": "Hori et al . , 2009 ; Lee et al . , 2009 ; Griol et al . , 2008 ; Young et al . , 2013 ; Li et al . , 2014 ) .", "entities": []}, {"text": "Speci\ufb01cally related to HCNs , past work has implemented the policy as feed - forward neural networks ( Wen et al . , 2016 ) , trained with supervised learning followed by reinforcement learning ( Su et al . , 2016 ) .", "entities": []}, {"text": "In these works , the policy has not been recurrent \u2013 i.e. , the policy depends on the state tracker to summarize observable dialog history into state features , which requires design and specialized labeling .", "entities": []}, {"text": "By contrast , HCNs use an RNN which automatically infers a representation of state .", "entities": []}, {"text": "For learning ef\ufb01ciency , HCNs use an external lightweight process for tracking entity values , but the policy is not strictly dependent on it : as an illustration , in Section 5 below , we demonstrate an HCNbased dialog system which has no external state tracker .", "entities": []}, {"text": "If there is context which is not apparent in the text in the dialog , such as database status , this can be encoded as a context feature to the RNN .", "entities": []}, {"text": "The second , more recent line of work applies recurrent neural networks ( RNNs ) to learn \u201c endto - end \u201d models , which map from an observable dialog history directly to a sequence of output words ( Sordoni et al . , 2015 ; Shang et al . , 2015 ; Vinyals and Le , 2015 ; Yao et", "entities": []}, {"text": "al . , 2015 ; Serban et al . , 2016 ; Li et al . , 2016a , c ; Luan et al . , 2016 ; Xu et", "entities": []}, {"text": "al . , 2016 ; Li et al . , 2016b ; Mei et al . , 2016 ; Lowe et al . , 2017 ; Serban et al . , 2017 ) .", "entities": []}, {"text": "These systems can be applied to task - oriented domains by adding special \u201c API call \u201d actions , enumeratingdatabase output as a sequence of tokens ( Bordes and Weston , 2016 ) , then learning an RNN using Memory Networks ( Sukhbaatar et al . , 2015 ) , gated memory networks ( Liu and Perez , 2016 ) , query reduction networks ( Seo et al . , 2016 ) , and copyaugmented networks ( Eric and Manning , 2017 ) .", "entities": []}, {"text": "In each of these architectures , the RNN learns to manipulate entity values , for example by saving them in a memory .", "entities": []}, {"text": "Output is produced by generating a sequence of tokens ( or ranking all possible surface forms ) , which can also draw from this memory .", "entities": []}, {"text": "HCNs also use an RNN to accumulate dialog state and choose actions .", "entities": []}, {"text": "However , HCNs differ in that they use developer - provided action templates , which can contain entity references , such as \u201c < city > , right ? \u201d .", "entities": []}, {"text": "This design reduce learning complexity , and also enable the software to limit which actions are available via an action mask , at the expense of developer effort .", "entities": []}, {"text": "To further reduce learning complexity in a practical system , entities are tracked separately , outside the the RNN , which also allows them to be substituted into action templates .", "entities": []}, {"text": "Also , past end - to - end recurrent models have been trained using supervised learning , whereas we show how HCNs can also be trained with reinforcement learning .", "entities": []}, {"text": "4 Supervised learning evaluation I", "entities": []}, {"text": "In this section we compare HCNs to existing approaches on the public \u201c bAbI dialog \u201d dataset ( Bordes and Weston , 2016 ) .", "entities": []}, {"text": "This dataset includes two end - to - end dialog learning tasks , in the restaurant domain , called task5 and task6.2Task5 consists of synthetic , simulated dialog data , with highly regular user behavior and constrained vocabulary .", "entities": [[9, 11, "TaskName", "dialog learning"]]}, {"text": "Dialogs include a database access action which retrieves relevant restaurants from a database , with results included in the dialog transcript .", "entities": []}, {"text": "We test on the \u201c OOV \u201d variant of Task5 , which includes entity values not observed in the training set .", "entities": []}, {"text": "Task6 draws on human - computer dialog data from the second dialog state tracking challenge ( DSTC2 ) , where usability subjects ( crowd - workers ) interacted with several variants of a spoken dialog system ( Henderson et al . , 2014a ) .", "entities": []}, {"text": "Since the database from DSTC2 was not provided , database calls have been inferred from the data and inserted into the dialog transcript .", "entities": []}, {"text": "Example dialogs are provided in the Appendix Sections A.2 and A.3 .", "entities": []}, {"text": "To apply HCNs , we wrote simple domain2Tasks 1 - 4 are sub - tasks of Task5.667", "entities": []}, {"text": "speci\ufb01c software , as follows .", "entities": []}, {"text": "First , for entity extraction ( step 4 in Figure 1 ) , we used a simple string match , with a pre - de\ufb01ned list of entity names \u2013 i.e. , the list of restaurants available in the database .", "entities": []}, {"text": "Second , in the context update ( step 5 ) , we wrote simple logic for tracking entities : when an entity is recognized in the user input , it is retained by the software , over - writing any previously stored value .", "entities": []}, {"text": "For example , if the price \u201c cheap \u201d is recognized in the \ufb01rst turn , it is retained asprice = cheap .", "entities": []}, {"text": "If \u201c expensive \u201d is then recognized in the third turn , it over - writes \u201c cheap \u201d so the code now holds price = expensive .", "entities": []}, {"text": "Third , system actions were templatized : for example , system actions of the form \u201c prezzo is a nice restaurant in the west of town in the moderate price range \u201d all map to the template \u201c < name > is a nice restaurant in the < location > of town in the < price > price range \u201d .", "entities": []}, {"text": "This results in 16 templates for Task5 and 58 for Task6.3Fourth , when database results are received into the entity state , they are sorted by rating .", "entities": []}, {"text": "Finally , an action mask was created which encoded common - sense dependencies .", "entities": []}, {"text": "These are implemented as simple if - then rules based on the presence of entity values : for example , only allow an API call if pre - conditions are met ; only offer a restaurant if database results have already been received ; do not ask for an entity if it is already known ; etc .", "entities": []}, {"text": "For Task6 , we noticed that the system can say that no restaurants match the current query withoutconsulting the database ( for an example dialog , see Section A.3 in the Appendix ) .", "entities": []}, {"text": "In a practical system this information would be retrieved from the database and not encoded in the RNN .", "entities": []}, {"text": "So , we mined the training data and built a table of search queries known to yield no results .", "entities": []}, {"text": "We also added context features that indicated the state of the database \u2013 for example , whether there were any restaurants matching the current query .", "entities": []}, {"text": "The complete set of context features is given in Appendix Section A.4 .", "entities": []}, {"text": "Altogether this code consisted of about 250 lines of Python .", "entities": []}, {"text": "We then trained an HCN on the training set , employing the domain - speci\ufb01c software described above .", "entities": []}, {"text": "We selected an LSTM for the recurrent layer ( Hochreiter and Schmidhuber , 1997 ) , with the AdaDelta optimizer ( Zeiler , 2012 ) .", "entities": [[3, 4, "MethodName", "LSTM"], [18, 19, "MethodName", "AdaDelta"], [19, 20, "HyperparameterName", "optimizer"]]}, {"text": "We used the development set to tune the number of hid3A handful of actions in Task6 seemed spurious ; for these , we replaced them with a special \u201c UNK \u201d action in the training set , and masked this action at test time.den units ( 128 ) , and the number of epochs ( 12 ) .", "entities": [[51, 54, "HyperparameterName", "number of epochs"]]}, {"text": "Utterance embeddings were formed by averaging word embeddings , using a publicly available 300dimensional word embedding model trained using word2vec on web data ( Mikolov et al . , 2013).4", "entities": [[6, 8, "TaskName", "word embeddings"]]}, {"text": "The word embeddings were static and not updated during LSTM training .", "entities": [[1, 3, "TaskName", "word embeddings"], [9, 10, "MethodName", "LSTM"]]}, {"text": "In training , each dialog formed one minibatch , and updates were done on full rollouts ( i.e. , non - truncated back propagation through time ) .", "entities": []}, {"text": "The training loss was categorical cross - entropy .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "Further low - level implementation details are in the Appendix Section A.1 .", "entities": []}, {"text": "We ran experiments with four variants of our model : with and without the utterance embeddings , and with and without the action mask ( Figure 1 , steps 3 and 6 respectively ) .", "entities": []}, {"text": "Following past work , we report average turn accuracy \u2013 i.e. , for each turn in each dialog , present the ( true ) history of user and system actions to the network and obtain the network \u2019s prediction as a string of characters .", "entities": [[8, 9, "MetricName", "accuracy"]]}, {"text": "The turn is correct if the string matches the reference exactly , and incorrect if not .", "entities": []}, {"text": "We also report dialog accuracy , which indicates if all turns in a dialog are correct .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "We compare to four past end - to - end approaches ( Bordes and Weston , 2016 ; Liu and Perez , 2016 ; Eric and Manning , 2017 ; Seo et al . , 2016 ) .", "entities": []}, {"text": "We emphasize that past approaches have applied purely sequence - to - sequence models , or ( as a baseline ) purely programmed rules ( Bordes and Weston , 2016 ) .", "entities": []}, {"text": "By contrast , Hybrid Code Networks are a hybrid of hand - coded rules and learned models .", "entities": []}, {"text": "Results are shown in Table 1 .", "entities": []}, {"text": "Since Task5 is synthetic data generated using rules , it is possible to obtain perfect accuracy using rules ( line 1 ) .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "The addition of domain knowledge greatly simpli\ufb01es the learning task and enables HCNs to also attain perfect accuracy .", "entities": [[17, 18, "MetricName", "accuracy"]]}, {"text": "On Task6 , rules alone fare poorly , whereas HCNs outperform past learned models .", "entities": []}, {"text": "We next examined learning curves , training with increasing numbers of dialogs .", "entities": []}, {"text": "To guard against bias in the ordering of the training set , we averaged over 5 runs , randomly permuting the order of the training dialogs in each run .", "entities": []}, {"text": "Results are in Figure 2 .", "entities": []}, {"text": "In Task5 , the action mask and utterance embeddings substantially reduce the number of training dialogs required ( note the horizontal axis scale is logarithmic ) .", "entities": []}, {"text": "For Task6 , the bene4Google News 100B model from https://github .", "entities": []}, {"text": "com/3Top / word2vec - api668", "entities": []}, {"text": "Task5 - OOV Task6 Model Turn Acc .", "entities": [[6, 7, "MetricName", "Acc"]]}, {"text": "Dialog Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "Turn Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "Dialog Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "Rules 100 % 100 % 33.3 % 0.0 % Bordes and Weston ( 2016 ) 77.7 % 0.0 % 41.1 % 0.0 % Liu and Perez ( 2016 ) 79.4 % 0.0 % 48.7 % 1.4 % Eric and Manning ( 2017 ) \u2014 \u2014 48.0 % 1.5 % Seo et al . ( 2016 ) 96.0 % \u2014 51.1 % \u2014 HCN 100 % 100 % 54.0 % 1.2 % HCN+embed 100 % 100 % 55.6 % 1.3 % HCN+mask 100 % 100 % 53.1 % 1.9 % HCN+embed+mask 100 % 100 % 52.7 % 1.5 % Table 1 : Results on bAbI dialog Task5 - OOV and Task6 ( Bordes and Weston , 2016 ) .", "entities": []}, {"text": "Results for \u201c Rules \u201d taken from Bordes and Weston ( 2016 ) .", "entities": []}, {"text": "Note that , unlike cited past work , HCNs make use of domainspeci\ufb01c procedural knowledge .", "entities": []}, {"text": "20%30%40%50%60%70%80%90%100 % 1 2 5 10 20 50 100 200 500 1000Turn accuracy Supervised learning training dialogsHCN+mask+embed HCN+mask HCN+embed HCN ( a ) bAbI dialog Task5 - OOV .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "0%10%20%30%40%50%60 % 1 2 5 10 20 50 100 200 500 1000 1618Turn accuracy Supervised learning training dialogsHCN+mask+embed HCN+mask HCN+embed HCN ( b ) bAbI dialog Task6 .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "Figure 2 : Training dialog count vs. turn accuracy for bAbI dialog Task5 - OOV and Task6 .", "entities": [[8, 9, "MetricName", "accuracy"]]}, {"text": "\u201c embed \u201d indicates whether utterance embeddings were included ; \u201c mask \u201d indicates whether the action masking code was active .", "entities": []}, {"text": "\ufb01ts of the utterance embeddings are less clear .", "entities": []}, {"text": "An error analysis showed that there are several systematic differences between the training and testing sets .", "entities": []}, {"text": "Indeed , DSTC2 intentionally used different dialog policies for the training and test sets , whereas our goal is to mimic the policy in the training set .", "entities": []}, {"text": "Nonetheless , these tasks are the best public benchmark we are aware of , and HCNs exceed performance of existing sequence - to - sequence models .", "entities": []}, {"text": "In addition , they match performance of past models using an order of magnitude less data ( 200 vs. 1618 dialogs ) , which is crucial in practical settings where collecting realistic dialogs for a new domain can be expensive.5 Supervised learning evaluation II", "entities": []}, {"text": "We now turn to comparing with purely handcrafted approaches .", "entities": []}, {"text": "To do this , we obtained logs from our company \u2019s text - based customer support dialog system , which uses a sophisticated rulebased dialog manager .", "entities": []}, {"text": "Data from this system is attractive for evaluation because it is used by real customers \u2013 not usability subjects \u2013 and because its rule - based dialog manager was developed by customer support professionals at our company , and not the authors .", "entities": []}, {"text": "This data is not publicly available , but we are unaware of suitable humancomputer dialog data in the public domain which uses rules .", "entities": []}, {"text": "Customers start using the dialog system by entering a brief description of their problem , such669", "entities": []}, {"text": "as \u201c I need to update my operating system \u201d .", "entities": []}, {"text": "They are then routed to one of several hundred domains , where each domain attempts to resolve a particular problem .", "entities": []}, {"text": "In this study , we collected humancomputer transcripts for the high - traf\ufb01c domains \u201c reset password \u201d and \u201c can not access account \u201d .", "entities": []}, {"text": "We labeled the dialog data as follows .", "entities": []}, {"text": "First , we enumerated unique system actions observed in the data .", "entities": []}, {"text": "Then , for each dialog , starting from the beginning , we examined each system action , and determined whether it was \u201c correct \u201d .", "entities": []}, {"text": "Here , correct means that it was the most appropriate action among the set of existing system actions , given the history of that dialog .", "entities": []}, {"text": "If multiple actions were arguably appropriate , we broke ties in favor of the existing rule - based dialog manager .", "entities": []}, {"text": "Example dialogs are provided in the Appendix Sections A.5 and A.6 .", "entities": []}, {"text": "If a system action was labeled as correct , we left it as - is and continued to the next system action .", "entities": []}, {"text": "If the system action was not correct , we replaced it with the correct system action , and discarded the rest of the dialog , since we do not know how the user would have replied to this new system action .", "entities": []}, {"text": "The resulting dataset contained a mixture of complete and partial dialogs , containing only correct system actions .", "entities": []}, {"text": "We partitioned this set into training and test dialogs .", "entities": []}, {"text": "Basic statistics of the data are shown in Table 2 .", "entities": []}, {"text": "In this domain , no entities were relevant to the control \ufb02ow , and there was no obvious mask logic since any question could follow any question .", "entities": []}, {"text": "Therefore , we wrote no domain - speci\ufb01c software for this instance of the HCN , and relied purely on the recurrent neural network to drive the conversation .", "entities": []}, {"text": "The architecture and training of the RNN was the same as in Section 4 , except that here we did not have enough data for a validation set , so we instead trained until we either achieved 100 % accuracy on the training set or reached 200 epochs .", "entities": [[39, 40, "MetricName", "accuracy"]]}, {"text": "To evaluate , we observe that conventional measures like average dialog accuracy unfairly penalize the system used to collect the dialogs \u2013 in our case , the rule - based system .", "entities": [[11, 12, "MetricName", "accuracy"]]}, {"text": "If the system used for collection makes an error at turn t , the labeled dialog only includes the sub - dialog up to turn t , and the system being evaluated off - line is only evaluated on that sub - dialog .", "entities": []}, {"text": "In other words , in our case , reporting dialog accuracy would favor the HCN because it would be evaluated on fewer turns than the rule - based system .", "entities": [[10, 11, "MetricName", "accuracy"]]}, {"text": "We thereforeForgot Account password Access Av .", "entities": []}, {"text": "sys .", "entities": []}, {"text": "turns / dialog 2.2 2.2 Max .", "entities": []}, {"text": "sys .", "entities": []}, {"text": "turns / dialog 5 9 Av .", "entities": []}, {"text": "words / user turn 7.7 5.4 Unique sys .", "entities": []}, {"text": "actions 7 16 Train dialogs 422 56 Test dialogs 148 60 Test acc .", "entities": [[12, 13, "MetricName", "acc"]]}, {"text": "( rules ) 64.9 % 42.1 % Table 2 : Basic statistics of labeled customer support dialogs .", "entities": []}, {"text": "Test accuracy refers to whole - dialog accuracy of the existing rule - based system .", "entities": [[1, 2, "MetricName", "accuracy"], [7, 8, "MetricName", "accuracy"]]}, {"text": "use a comparative measure that examines which method produces longer continuous sequences of correct system actions , starting from the beginning of the dialog .", "entities": []}, {"text": "Speci\ufb01cally , we report \u2206P= C(HCN - win ) \u2212C(rule - win ) C(all ) , whereC(HCN - win ) is the number of test dialogs where the rule - based approach output a wrong action before the HCN ; C(rule - win ) is the number of test dialogs where the HCN output a wrong action before the rulebased approach ; and C(all)is the number of dialogs in the test set .", "entities": []}, {"text": "When \u2206P > 0 , there are more dialogs in which HCNs produce longer continuous sequences of correct actions starting from the beginning of the dialog .", "entities": [[3, 4, "DatasetName", "0"]]}, {"text": "We run all experiments 5 times , each time shuf\ufb02ing the order of the training set .", "entities": []}, {"text": "Results are in Figure 3 .", "entities": []}, {"text": "HCNs exceed performance of the existing rule - based system after about 30 dialogs .", "entities": []}, {"text": "In these domains , we have a further source of knowledge : the rule - based dialog managers themselves can be used to generate example \u201c sunnyday \u201d dialogs , where the user provides purely expected inputs .", "entities": []}, {"text": "From each rule - based controller , synthetic dialogs were sampled to cover each expected user response at least once , and added to the set of labeled real dialogs .", "entities": []}, {"text": "This resulted in 75 dialogs for the \u201c Forgot password \u201d domain , and 325 for the \u201c Ca n\u2019t access account \u201d domain .", "entities": []}, {"text": "Training was repeated as described above .", "entities": []}, {"text": "Results are also included in Figure 3 , with the suf\ufb01x \u201c sampled \u201d .", "entities": []}, {"text": "In the \u201c Ca n\u2019t access account \u201d domain , the sampled dialogs yield a large improvement , probably because the \ufb02ow chart for this domain is large , so the sampled dialogs increase coverage .", "entities": []}, {"text": "The gain in the \u201c forgot password \u201d domain is present but smaller .", "entities": []}, {"text": "In summary , HCNs can out - perform670", "entities": []}, {"text": "-40%-30%-20%-10%0%10%20 % 0 20 40 60 80 100\u0394P Labeled supervised learning training dialogsHCN+embed+sampled HCN+sampled HCN+embed HCN(a ) \u201c Forgot password \u201d domain .", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "-40%-30%-20%-10%0%10%20%30 % 0", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "10 20 30 40 50\u0394P Labeled supervised learning training dialogsHCN+embed+sampled HCN+sampled HCN+embed HCN ( b ) \u201c Ca n\u2019t access account \u201d domain .", "entities": []}, {"text": "Figure 3 : Training dialogs vs. \u2206P , where \u2206Pis the fraction of test dialogs where HCNs produced longer initial correct sequences of system actions than the rules , minus the fraction where rules produced longer initial correct sequences than the HCNs .", "entities": []}, {"text": "\u201c embed \u201d indicates whether utterance embeddings were included ; \u201c sampled \u201d indicates whether dialogs sampled from the rule - based controller were included in the training set .", "entities": []}, {"text": "production - grade rule - based systems with a reasonable number of labeled dialogs , and adding synthetic \u201c sunny - day \u201d dialogs improves performance further .", "entities": []}, {"text": "Moreover , unlike existing pipelined approaches to dialog management that rely on an explicit state tracker , this HCN used no explicit state tracker , highlighting an advantage of the model .", "entities": []}, {"text": "6 Reinforcement learning illustration In the previous sections , supervised learning ( SL ) was applied to train the LSTM to mimic dialogs provided by the system developer .", "entities": [[19, 20, "MethodName", "LSTM"]]}, {"text": "Once a system operates at scale , interacting with a large number of users , it is desirable for the system to continue to learn autonomously using reinforcement learning ( RL ) .", "entities": []}, {"text": "With RL , each turn receives a measurement of goodness called a reward ; the agent explores different sequences of actions in different situations , and makes adjustments so as to maximize the expected discounted sum of rewards , which is called the return , denotedG. For optimization , we selected a policy gradient approach ( Williams , 1992 ) , which has been successfully applied to dialog systems ( Jur \u02c7c\u00b4\u0131\u02c7cek", "entities": [[15, 16, "DatasetName", "agent"]]}, {"text": "et", "entities": []}, {"text": "al . , 2011 ) , robotics ( Kohl and Stone , 2004 ) , and the board game Go ( Silver et al . , 2016 ) .", "entities": []}, {"text": "In policy gradient - based RL , a model \u03c0is parameterized by wand outputs a distribution from which actions are sampled at each timestep .", "entities": []}, {"text": "At the end of a trajectory \u2013 in our case , dialog \u2013 the return Gforthat trajectory is computed , and the gradients of the probabilities of the actions taken with respect to the model weights are computed .", "entities": []}, {"text": "The weights are then adjusted by taking a gradient step proportional to the return : w\u2190w+\u03b1(/summationdisplay t / triangleinvwlog\u03c0(at|ht;w))(G\u2212b)(1 ) where\u03b1is a learning rate ; atis the action taken at timestept;htis the dialog history at time t;Gis the return of the dialog ; /triangleinvxFdenotes the Jacobian ofFwith respect to x;bis a baseline described below ; and\u03c0(a|h;w)is the LSTM \u2013 i.e. , a stochastic policy which outputs a distribution over agiven a dialog history h , parameterized by weights", "entities": [[22, 24, "HyperparameterName", "learning rate"], [25, 26, "DatasetName", "atis"], [57, 58, "MethodName", "LSTM"]]}, {"text": "w.", "entities": []}, {"text": "The baseline bis an estimate of the average return of the current policy , estimated on the last 100 dialogs using weighted importance sampling.5Intuitively , \u201c better \u201d dialogs receive a positive gradient step , making the actions selected more likely ; and \u201c worse \u201d dialogs receive a negative gradient step , making the actions selected less likely .", "entities": [[7, 9, "MetricName", "average return"]]}, {"text": "SL and RL correspond to different methods of updating weights , so both can be applied to the same network .", "entities": []}, {"text": "However , there is no guarantee that the optimal RL policy will agree with the SL training set ; therefore , after each RL gradient step , we 5The choice of baseline does not affect the long - term convergence of the algorithm ( i.e. , the bias ) , but can dramatically affect the speed of convergence ( i.e. , the variance ) ( Williams , 1992).671", "entities": []}, {"text": "check whether the updated policy reconstructs the training set .", "entities": []}, {"text": "If not , we re - run SL gradient steps on the training set until the model reproduces the training set .", "entities": []}, {"text": "Note that this approach allows new training dialogs to be added at any time during RL optimization .", "entities": []}, {"text": "We illustrate RL optimization on a simulated dialog task in the name dialing domain .", "entities": []}, {"text": "In this system , a contact \u2019s name may have synonyms ( \u201c Michael \u201d may also be called \u201c Mike \u201d ) , and a contact may have more than one phone number , such as \u201c work \u201d or \u201c mobile \u201d , which may in turn have synonyms like \u201c cell \u201d for \u201c mobile \u201d .", "entities": []}, {"text": "This domain has a database of names and phone numbers taken from the Microsoft personnel directory , 5 entity types \u2013 firstname , nickname , lastname , phonenumber , and phonetype \u2013 and 14 actions , including 2 API call actions .", "entities": []}, {"text": "Simple entity logic was coded , which retains the most recent copy of recognized entities .", "entities": []}, {"text": "A simple action mask suppresses impossible actions , such as placing a phonecall before a phone number has been retrieved from the database .", "entities": []}, {"text": "Example dialogs are provided in Appendix Section A.7 .", "entities": []}, {"text": "To perform optimization , we created a simulated user .", "entities": []}, {"text": "At the start of a dialog , the simulated user randomly selected a name and phone type , including names and phone types not covered by the dialog system .", "entities": []}, {"text": "When speaking , the simulated user can use the canonical name or a nickname ; usually answers questions but can ignore the system ; can provide additional information not requested ; and can give up .", "entities": []}, {"text": "The simulated user was parameterized by around 10 probabilities , set by hand .", "entities": []}, {"text": "We de\ufb01ned the reward as being 1for successfully completing the task , and 0otherwise .", "entities": []}, {"text": "A discount of 0.95was used to incentivize the system to complete dialogs faster rather than slower , yielding return 0for failed dialogs , and G= 0.95T\u22121 for successful dialogs , where Tis the number of system turns in the dialog .", "entities": []}, {"text": "Finally , we created a set of 21 labeled dialogs , which will be used for supervised learning .", "entities": []}, {"text": "For the RNN in the HCN , we again used an LSTM with AdaDelta , this time with 32 hidden units .", "entities": [[11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "AdaDelta"]]}, {"text": "RL policy updates are made after each dialog .", "entities": []}, {"text": "Since a simulated user was employed , we did not have real user utterances , and instead relied on context features , omitting bag - of - words and utterance embedding features .", "entities": []}, {"text": "We \ufb01rst evaluate RL by randomly initializing an 0%10%20%30%40%50%60%70%Dialog success rate Reinforcement learning training dialogs10 interleaved 10 initial 5 initial 3 initial 1 initial 0Figure 4 : Dialog success rate vs. reinforcement learning training dialogs .", "entities": []}, {"text": "Curve marked \u201c 0 \u201d begins with a randomly initialized LSTM .", "entities": [[3, 4, "DatasetName", "0"], [10, 11, "MethodName", "LSTM"]]}, {"text": "Curves marked \u201c Ninitial \u201d are pre - trained with Nlabeled dialogs .", "entities": []}, {"text": "Curve marked \u201c 10 , interleaved \u201d adds one SL training dialog before RL dialog 0 , 100 , 200 , ...", "entities": [[15, 16, "DatasetName", "0"]]}, {"text": "900 .", "entities": []}, {"text": "LSTM , and begin RL optimization .", "entities": [[0, 1, "MethodName", "LSTM"]]}, {"text": "After 10 RL updates , we freeze the policy , and run 500 dialogs with the user simulation to measure task completion .", "entities": []}, {"text": "We repeat all of this for 100 runs , and report average performance .", "entities": []}, {"text": "In addition , we also report results by initializing the LSTM using supervised learning on the training set , consisting of 1 , 2 , 5 , or 10 dialogs sampled randomly from the training set , then running RL as described above .", "entities": [[10, 11, "MethodName", "LSTM"]]}, {"text": "Results are in Figure 4 .", "entities": []}, {"text": "Although RL alone can \ufb01nd a good policy , pre - training with just a handful of labeled dialogs improves learning speed dramatically .", "entities": []}, {"text": "Additional experiments , not shown for space , found that ablating the action mask slowed training , agreeing with Williams ( 2008 ) .", "entities": []}, {"text": "Finally , we conduct a further experiment where we sample 10 training dialogs , then add one to the training set just before RL dialog 0 , 100 , 200 , ... , 900 .", "entities": [[25, 26, "DatasetName", "0"]]}, {"text": "Results are shown in Figure 4 .", "entities": []}, {"text": "This shows that SL dialogs can be introduced as RL is in progress \u2013 i.e. , that it is possible to interleave RL and SL .", "entities": []}, {"text": "This is an attractive property for practical systems : if a dialog error is spotted by a developer while RL is in progress , it is natural to add a training dialog to the training set .", "entities": []}, {"text": "7 Conclusion This paper has introduced Hybrid Code Networks for end - to - end learning of task - oriented dialog672", "entities": []}, {"text": "systems .", "entities": []}, {"text": "HCNs support a separation of concerns where procedural knowledge and constraints can be expressed in software , and the control \ufb02ow is learned .", "entities": []}, {"text": "Compared to existing end - to - end approaches , HCNs afford more developer control and require less training data , at the expense of a small amount of developer effort .", "entities": []}, {"text": "Results in this paper have explored three different dialog domains .", "entities": []}, {"text": "On a public benchmark in the restaurants domain , HCNs exceeded performance of purely learned models .", "entities": []}, {"text": "Results in two troubleshooting domains exceeded performance of a commercially deployed rule - based system .", "entities": []}, {"text": "Finally , in a name - dialing domain , results from dialog simulation show that HCNs can also be optimized with a mixture of reinforcement and supervised learning .", "entities": []}, {"text": "In future work , we plan to extend HCNs by incorporating lines of existing work , such as integrating the entity extraction step into the neural network ( Dhingra et al . , 2017 ) , adding richer utterance embeddings ( Socher et al . , 2013 ) , and supporting text generation ( Sordoni et al . , 2015 ) .", "entities": [[51, 53, "TaskName", "text generation"]]}, {"text": "We will also explore using HCNs with automatic speech recognition ( ASR ) input , for example by forming features from n - grams of the ASR n - best results ( Henderson et al . , 2014b ) .", "entities": [[7, 10, "TaskName", "automatic speech recognition"]]}, {"text": "Of course , we also plan to deploy the model in a live dialog system .", "entities": []}, {"text": "More broadly , HCNs are a general model for stateful control , and we would be interested to explore applications beyond dialog systems \u2013 for example , in NLP medical settings or humanrobot NL interaction tasks , providing domain constraints are important for safety ; and in resourcepoor settings , providing domain knowledge can amplify limited data .", "entities": []}, {"text": "References Antoine Bordes and Jason Weston .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Learning end - to - end goal - oriented dialog .", "entities": [[6, 10, "TaskName", "goal - oriented dialog"]]}, {"text": "CoRR abs/1605.07683 .", "entities": []}, {"text": "http://arxiv.org/abs/1605.07683 .", "entities": []}, {"text": "Franois Chollet .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Keras .", "entities": []}, {"text": "https://github . com / fchollet / keras .", "entities": []}, {"text": "Junyoung Chung , Caglar Gulcehre , KyungHyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Empirical evaluation of gated recurrent neural networks on sequence modeling .", "entities": []}, {"text": "In Proc NIPS 2014 Deep Learning and Representation Learning Workshop .", "entities": [[7, 9, "TaskName", "Representation Learning"]]}, {"text": "Bhuwan Dhingra , Lihong Li , Xiujun Li , Jianfeng Gao , Yun - Nung Chen , Faisal Ahmed , and Li Deng .", "entities": []}, {"text": "2017.Towards end - to - end reinforcement learning of dialogue agents for information access .", "entities": []}, {"text": "In Proc Association for Computational Linguistics , Vancouver , Canada .", "entities": []}, {"text": "Mihail Eric and Christopher D Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A copy - augmented sequence - to - sequence architecture gives good performance on taskoriented dialogue .", "entities": []}, {"text": "CoRR abs/1701.04024 .", "entities": []}, {"text": "https://arxiv.org/abs/1701.04024 .", "entities": []}, {"text": "David Griol , Llus F. Hurtado , Encarna Segarra , and Emilio Sanchis . 2008 .", "entities": []}, {"text": "A statistical approach to spoken dialog systems design and evaluation .", "entities": []}, {"text": "Speech Communication 50(8\u20139 ) .", "entities": []}, {"text": "Matthew Henderson , Blaise Thomson , and Jason Williams .", "entities": []}, {"text": "2014a .", "entities": []}, {"text": "The second dialog state tracking challenge .", "entities": []}, {"text": "In Proc SIGdial Workshop on Discourse and Dialogue , Philadelphia , USA .", "entities": []}, {"text": "Matthew Henderson , Blaise Thomson , and Steve Young .", "entities": []}, {"text": "2014b .", "entities": []}, {"text": "Word - based Dialog State Tracking with Recurrent Neural Networks .", "entities": []}, {"text": "In Proc SIGdial Workshop on Discourse and Dialogue , Philadelphia , USA .", "entities": []}, {"text": "Sepp Hochreiter and Jurgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural Computation 9(8):1735\u20131780 .", "entities": []}, {"text": "Chiori Hori , Kiyonori Ohtake , Teruhisa Misu , Hideki Kashioka , and Satoshi Nakamura . 2009 .", "entities": []}, {"text": "Statistical dialog management applied to WFSTbased dialog systems .", "entities": []}, {"text": "In Acoustics , Speech and Signal Processing , 2009 .", "entities": []}, {"text": "ICASSP 2009 .", "entities": []}, {"text": "IEEE International Conference on .", "entities": []}, {"text": "pages 4793\u20134796 .", "entities": []}, {"text": "https://doi.org/10.1109/ICASSP.2009.4960703 .", "entities": []}, {"text": "Filip Jur \u02c7c\u00b4\u0131\u02c7cek , Blaise Thomson , and Steve Young .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Natural actor and belief critic : Reinforcement algorithm for learning parameters of dialogue systems modelled as pomdps .", "entities": []}, {"text": "ACM Transactions on Speech and Language Processing ( TSLP ) 7(3):6 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Nate Kohl and Peter Stone .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Policy gradient reinforcement learning for fast quadrupedal locomotion .", "entities": []}, {"text": "In Robotics and Automation , 2004 .", "entities": []}, {"text": "Proceedings .", "entities": []}, {"text": "ICRA\u201904 .", "entities": []}, {"text": "2004 IEEE International Conference on .", "entities": []}, {"text": "IEEE , volume 3 , pages 2619\u20132624 .", "entities": []}, {"text": "Cheongjae Lee , Sangkeun Jung , Seokhwan Kim , and Gary Geunbae Lee . 2009 .", "entities": []}, {"text": "Example - based dialog modeling for practical multi - domain dialog system .", "entities": []}, {"text": "Speech Communication 51(5):466\u2013484 .", "entities": []}, {"text": "Esther Levin , Roberto Pieraccini , and Wieland Eckert .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "A stochastic model of human - machine interaction for learning dialogue strategies .", "entities": []}, {"text": "IEEE Trans on Speech and Audio Processing 8(1):11\u201323 .", "entities": []}, {"text": "Jiwei Li , Michel Galley , Chris Brockett , Jianfeng Gao , and Bill Dolan .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "A diversity - promoting objective function for neural conversation models .", "entities": []}, {"text": "In Proc HLT - NAACL , San Diego , California , USA .673", "entities": []}, {"text": "Jiwei Li , Michel Galley , Chris Brockett , Georgios Spithourakis , Jianfeng Gao , and Bill Dolan .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "A persona - based neural conversation model .", "entities": []}, {"text": "In Proc Association for Computational Linguistics , Berlin , Germany .", "entities": []}, {"text": "Jiwei Li , Will Monroe , Alan Ritter , Michel Galley , Jianfeng Gao , and Dan Jurafsky .", "entities": []}, {"text": "2016c .", "entities": []}, {"text": "Deep reinforcement learning for dialogue generation .", "entities": [[4, 6, "TaskName", "dialogue generation"]]}, {"text": "In Proc Conference on Empirical Methods in Natural Language Processing , Austin , Texas , USA .", "entities": [[13, 14, "DatasetName", "Texas"]]}, {"text": "Lihong Li , He He , and Jason D. Williams .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Temporal supervised learning for inferring a dialog policy from example conversations .", "entities": []}, {"text": "In Proc IEEE Workshop on Spoken Language Technologies ( SLT ) , South Lake Tahoe , Nevada , USA .", "entities": []}, {"text": "Fei Liu and Julien Perez .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Gated end - toend memory networks .", "entities": []}, {"text": "CoRR abs/1610.04211 .", "entities": []}, {"text": "http://arxiv.org/abs/1610.04211 .", "entities": []}, {"text": "Ryan Thomas Lowe , Nissan Pow , Iulian Vlad Serban , Laurent Charlin , Chia - Wei Liu , and Joelle Pineau . 2017 .", "entities": []}, {"text": "Training end - to - end dialogue systems with the ubuntu dialogue corpus .", "entities": []}, {"text": "Dialogue and Discourse 8(1 ) .", "entities": []}, {"text": "Yi Luan , Yangfeng Ji , and Mari Ostendorf .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "LSTM based conversation models .", "entities": [[0, 1, "MethodName", "LSTM"]]}, {"text": "CoRR abs/1603.09457 .", "entities": []}, {"text": "http://arxiv.org/abs/1603.09457 .", "entities": []}, {"text": "Hongyuan Mei , Mohit Bansal , and Matthew R. Walter . 2016 .", "entities": []}, {"text": "Coherent dialogue with attentionbased language models .", "entities": []}, {"text": "CoRR abs/1611.06997 .", "entities": []}, {"text": "http://arxiv.org/abs/1611.06997 .", "entities": []}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , and Jeff Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In Proc Advances in Neural Information Processing Systems , Lake Tahoe , USA . pages 3111 \u2013 3119 .", "entities": []}, {"text": "Min Joon Seo , Hannaneh Hajishirzi , and Ali Farhadi .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Query - regression networks for machine comprehension .", "entities": []}, {"text": "CoRR abs/1606.04582 .", "entities": []}, {"text": "http://arxiv.org/abs/1606.04582 .", "entities": []}, {"text": "Iulian V .", "entities": []}, {"text": "Serban , Alessandro Sordoni , Yoshua Bengio , Aaron Courville , and Joelle Pineau .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Building end - to - end dialogue systems using generative hierarchical neural network models .", "entities": []}, {"text": "In Proceedings of the Thirtieth AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "AAAI Press , AAAI\u201916 , pages 3776\u20133783 . http://dl.acm.org/citation.cfm?id=3016387.3016435 .", "entities": []}, {"text": "Iulian Vlad Serban , Alessandro Sordoni , Ryan Lowe , Laurent Charlin , Joelle Pineau , Aaron Courville , and Yoshua Bengio .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A hierarchical latent variable encoder - decoder model for generating dialogues .", "entities": []}, {"text": "Lifeng Shang , Zhengdong Lu , , and Hang Li . 2015 .", "entities": []}, {"text": "Neural responding machine for short - text conversation .", "entities": [[4, 8, "TaskName", "short - text conversation"]]}, {"text": "In Proc Association for Computational Linguistics , Beijing , China .David", "entities": []}, {"text": "Silver , Aja Huang , Chris J. Maddison , Arthur Guez , Laurent Sifre , George Van Den Driessche , Julian Schrittwieser , Ioannis Antonoglou , Veda Panneershelvam , Marc Lanctot , et al . 2016 .", "entities": []}, {"text": "Mastering the game of Go with deep neural networks and tree search .", "entities": [[2, 5, "TaskName", "game of Go"]]}, {"text": "Nature 529(7587):484\u2013489 .", "entities": []}, {"text": "Satinder Singh , Diane J Litman , Michael Kearns , and Marilyn A Walker . 2002 .", "entities": []}, {"text": "Optimizing dialogue management with reinforcement leaning : experiments with the NJFun system .", "entities": [[1, 3, "TaskName", "dialogue management"]]}, {"text": "Journal of Arti\ufb01cial Intelligence 16:105\u2013133 .", "entities": []}, {"text": "Richard Socher , Alex Perelygin , Jean Wu , Jason Chuang , Chris Manning , Andrew Ng , and Chris Potts .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Recursive deep models for semantic compositionality over a sentiment treebank .", "entities": []}, {"text": "In Proc Conference on Empirical Methods in Natural Language Processing , Seattle , Washington , USA .", "entities": []}, {"text": "Alessandro Sordoni , Michel Galley , Michael Auli , Chris Brockett , Yangfeng Ji , Meg Mitchell , Jian - Yun Nie , Jianfeng Gao , and Bill Dolan .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A neural network approach to context - sensitive generation of conversational responses .", "entities": []}, {"text": "In Proc HLT - NAACL , Denver , Colorado , USA .", "entities": []}, {"text": "Pei - Hao Su , Milica Ga \u02c7si\u00b4c , Nikola Mrk \u02c7si\u00b4c , Lina RojasBarahona , Stefan Ultes , David Vandyke , TsungHsien Wen , and Steve Young .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Continuously learning neural dialogue management .", "entities": [[3, 5, "TaskName", "dialogue management"]]}, {"text": "In arXiv preprint : 1606.02689 .", "entities": [[1, 2, "DatasetName", "arXiv"]]}, {"text": "Sainbayar Sukhbaatar , Arthur Szlam , Jason Weston , and Rob Fergus . 2015 .", "entities": []}, {"text": "End - to - end memory networks .", "entities": []}, {"text": "In Proc Advances in Neural Information Processing Systems ( NIPS ) , Montreal , Canada .", "entities": []}, {"text": "Theano Development Team .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Theano :", "entities": []}, {"text": "A Python framework for fast computation of mathematical expressions .", "entities": []}, {"text": "arXiv e - prints abs/1605.02688 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "http://arxiv.org/abs/1605.02688 .", "entities": []}, {"text": "Oriol Vinyals and Quoc Le . 2015 .", "entities": []}, {"text": "A neural conversational model .", "entities": []}, {"text": "In Proc ICML", "entities": []}, {"text": "Deep Learning Workshop .", "entities": []}, {"text": "Tsung - Hsien Wen , Milica Gasic , Nikola Mrksic , Lina Maria Rojas - Barahona , Pei - Hao Su , Stefan Ultes , David Vandyke , and Steve J. Young .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A network - based end - to - end trainable taskoriented dialogue system .", "entities": []}, {"text": "CoRR abs/1604.04562 .", "entities": []}, {"text": "http://arxiv.org/abs/1604.04562 .", "entities": []}, {"text": "Jason D. Williams .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "The best of both worlds :", "entities": []}, {"text": "Unifying conventional dialog systems and POMDPs .", "entities": []}, {"text": "In Proc Intl Conf on Spoken Language Processing ( ICSLP ) , Brisbane , Australia .", "entities": []}, {"text": "Jason D. Williams and Steve Young .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Partially observable Markov decision processes for spoken dialog systems .", "entities": []}, {"text": "Computer Speech and Language 21(2):393\u2013422 .", "entities": []}, {"text": "Ronald J Williams .", "entities": []}, {"text": "1992 .", "entities": []}, {"text": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning .", "entities": []}, {"text": "Machine learning 8(3 - 4):229\u2013256.674", "entities": []}, {"text": "Zhen Xu , Bingquan Liu , Baoxun Wang , Chengjie Sun , and Xiaolong Wang . 2016 .", "entities": []}, {"text": "Incorporating loosestructured knowledge into LSTM with recall gate for conversation modeling .", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "CoRR abs/1605.05110 .", "entities": []}, {"text": "http://arxiv.org/abs/1605.05110 .", "entities": []}, {"text": "Kaisheng Yao , Geoffrey Zweig , and Baolin Peng . 2015 .", "entities": []}, {"text": "Attention with intention for a neural network conversation model .", "entities": []}, {"text": "In Proc NIPS workshop on Machine Learning for Spoken Language Understanding and Interaction .", "entities": [[8, 11, "TaskName", "Spoken Language Understanding"]]}, {"text": "Steve Young , Milica Gasic , Blaise Thomson , and Jason D. Williams .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "POMDP - based Statistical Spoken Dialogue Systems : a Review .", "entities": [[4, 7, "TaskName", "Spoken Dialogue Systems"]]}, {"text": "Proceedings of the IEEE PP(99):1\u201320 .", "entities": []}, {"text": "Matthew D. Zeiler .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "ADADELTA : an adaptive learning rate method .", "entities": [[0, 1, "MethodName", "ADADELTA"], [4, 6, "HyperparameterName", "learning rate"]]}, {"text": "CoRR abs/1212.5701 .", "entities": []}, {"text": "http://arxiv.org/abs/1212.5701 .", "entities": []}, {"text": "A Supplemental Material A.1 Model implementation details The RNN was speci\ufb01ed using Keras version 0.3.3 , with back - end computation in Theano version 0.8.0.dev0", "entities": []}, {"text": "( Theano Development Team , 2016 ; Chollet , 2015 ) .", "entities": []}, {"text": "The Keras model speci\ufb01cation is given below .", "entities": []}, {"text": "The input variable obs includes all features from Figure 1 step 6 except for the previous action ( step 18 ) and the action mask ( step 6 , top - most vector ) .", "entities": []}, {"text": "# Given : # obs_size , action_size , nb_hidden g = Graph ( ) g.add_input ( name=\u2019obs \u2019 , input_shape=(None , obs_size ) )", "entities": []}, {"text": "g.add_input ( name=\u2019prev_action \u2019 , input_shape=(None , action_size ) ) g.add_input ( name=\u2019avail_actions \u2019 , input_shape=(None , action_size ) )", "entities": []}, {"text": "g.add_node ( LSTM ( n_hidden , return_sequences = True , activation=\u2019tanh \u2019 , ) , name=\u2019h1 \u2019 , inputs= [ \u2019 obs \u2019 , \u2019 prev_action \u2019 , \u2019 avail_actions \u2019 ] )", "entities": [[2, 3, "MethodName", "LSTM"]]}, {"text": "g.add_node ( TimeDistributedDense ( action_size , activation=\u2019softmax \u2019 , ) , name=\u2019h2 \u2019 , input=\u2019h1 \u2019 )", "entities": []}, {"text": "g.add_node ( Activation ( activation = normalize , ) , name=\u2019action \u2019 , inputs=[\u2019h2\u2019,\u2019avail_actions \u2019 ] , merge_mode=\u2019mul \u2019 , create_output = True ) g.compile ( optimizer = Adadelta(clipnorm=1 . ) , sample_weight_modes= { \u2019 action \u2019 : \u2019 temporal \u2019 } , loss= { \u2019 action\u2019:\u2019categorical_crossentropy \u2019 } ) Model sizes are given in Table 3 .", "entities": [[26, 27, "HyperparameterName", "optimizer"]]}, {"text": "Example dialogs are given below for each of the 5 dialog systems .", "entities": []}, {"text": "For space and readability , the entity tags that appear in the user and system sides of the dialogs have been removed \u2013 for example , Call < name > Joan</name > is shown as Call Joan .", "entities": []}, {"text": "A.2 bAbI Task5 example dialog good morning", "entities": []}, {"text": "hello what can i help you with today i \u2019d like to book a table with italian food i \u2019m on it", "entities": []}, {"text": "[ silence ] where should it be in paris how many people would be in your party for six people please which price range are looking for in a cheap price range please ok let me look into some options for you", "entities": []}, {"text": "[ silence ] api_call italian paris", "entities": []}, {"text": "six cheap [ database results returned ]", "entities": []}, {"text": "[ silence ] what do you think of this option : resto madrid cheap spanish 8stars no this does not work for me sure let me \ufb01nd an other option for you", "entities": []}, {"text": "[ silence ] what do you think of this option : resto madrid cheap spanish 6stars do you have something else sure let me \ufb01nd an other option for you [ silence ] what do you think of this option : resto madrid cheap spanish 1stars it \u2019s perfect great let me do the reservation may i have the phone number of the restaurant here it is resto madrid cheap spanish 1stars phone675", "entities": []}, {"text": "bAbI Task5", "entities": []}, {"text": "bAbI Task6", "entities": []}, {"text": "Reset password Account access Dialer Utterance embed .", "entities": []}, {"text": "300 300 300 300 0 Bag of words 85 523 1124 278 0", "entities": [[4, 5, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}, {"text": "Context features 4 14 0 0 17 obs_size 389 837 1424 578 17 action_size 16 58 7 16 14 nb_hidden 128 128 128 128 32 Table 3 : Dimensions of the 5 HCNs in this paper .", "entities": [[4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"]]}, {"text": "thanks is there anything i can help you with no thank you you \u2019re welcome A.3 bAbI Task6 example dialog In the system turn marked", "entities": []}, {"text": "[ * ] , the system provided venue information without \ufb01rst making an API call .", "entities": []}, {"text": "Hello , welcome to the Cambridge restaurant system .", "entities": [[5, 6, "DatasetName", "Cambridge"]]}, {"text": "Y ou can ask for restaurants by area , price range or food type .", "entities": []}, {"text": "How may I help you ?", "entities": []}, {"text": "i m looking for canapes food in any price range I am sorry but there is no canapes restaurant that matches your request", "entities": []}, {"text": "[ * ] how about european api_call european R_location R_price", "entities": []}, {"text": "[ database results returned ]", "entities": []}, {"text": "[ silence ] michaelhouse cafe is a great restaurant serving european food and it is in the expensive price range food an What part of town do you have in mind ?", "entities": []}, {"text": "any any part of town michaelhouse cafe serves european food .", "entities": []}, {"text": "may i have the address and phone number please The phone number of michaelhouse cafe is michaelhouse cafe phone thank you", "entities": []}, {"text": "good", "entities": []}, {"text": "bye you are welcome", "entities": []}, {"text": "A.4 Task5", "entities": []}, {"text": "and Task6 context features Context features used in Section 4 are in Table 4 .", "entities": []}, {"text": "A.5 Forgot password example dialog Some system actions have been shortened for space .", "entities": []}, {"text": "How do I recover my password", "entities": []}, {"text": "Okay , you do n\u2019t need to remember your password , we can reset it .", "entities": []}, {"text": "Would you like to try that ?", "entities": []}, {"text": "Y es To reset your password , please see this URL ... Was that helpful?I do n\u2019t have a \ufb02ashdrive", "entities": []}, {"text": "Let \u2019s connect you to a person who can help you A.6 Account access example dialog Some system actions have been shortened for space .", "entities": []}, {"text": "can not sign in to my email Please check caps lock is off , and that your email is spelled correctly .", "entities": []}, {"text": "Are you able to access your account now ?", "entities": []}, {"text": "no", "entities": []}, {"text": "If you still ca n\u2019t sign in , please go the the following page and check that the email server is running ok ...", "entities": []}, {"text": "Is everything up and running ?", "entities": []}, {"text": "yes Y ou need to reset your password - here \u2019s how ...", "entities": []}, {"text": "Did this solve your problem ?", "entities": []}, {"text": "still can not get in Did you receive the security code ?", "entities": []}, {"text": "yes Check you entered the right numbers .", "entities": []}, {"text": "Did this solve your problem ?", "entities": []}, {"text": "no Have you seen a message that your pro\ufb01le is temporarily blocked ?", "entities": []}, {"text": "just your message of update on sept 15 Did you turn on two - step veri\ufb01cation recently ?", "entities": []}, {"text": "what s that ?", "entities": []}, {"text": "Sorry , I ca n\u2019t help .", "entities": []}, {"text": "Rephrase your question , or click below to talk to a person .", "entities": []}, {"text": "A.7 Name dialing example dialogs How can I help you ?", "entities": []}, {"text": "Call Joe", "entities": []}, {"text": "There \u2019s more than one person named Joe .", "entities": []}, {"text": "Can you say their full name ?", "entities": []}, {"text": "Call Joe Adamson SavePhonetypeavail ( )", "entities": []}, {"text": "Calling Joe Adamson , work PlaceCall ( ) How can I help you ?", "entities": []}, {"text": "Call Sally on her home676", "entities": []}, {"text": "Feature bAbI Task5", "entities": []}, {"text": "bAbI Task6", "entities": []}, {"text": "Presence of each entity in dialog state 4 3 Presence of each entity in current utterance \u2013 3 Whether DB has been queried yet \u2013 1 Whether DB results are empty \u2013 1 Whether DB results are non - empty \u2013 1 Whether any DB results have been presented \u2013 1 Whether all DB results have been presented \u2013 1 Whether any DB results are available to present \u2013 1 Whether curent query yields no results in training set \u2013 1 Whether cuisine in current query yields no results in training set \u2013 1 Total context features 4 14 Table 4 : Binary context features used to convey entity and database state in Section 4 .", "entities": []}, {"text": "Sorry , I do n\u2019t have a home number for Sally Smith .", "entities": []}, {"text": "I only have a work phone .", "entities": []}, {"text": "Do you want to call that number ?", "entities": []}, {"text": "no", "entities": []}, {"text": "Oh , sorry about that .", "entities": []}, {"text": "Goodbye.677", "entities": []}]