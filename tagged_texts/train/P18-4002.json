[{"text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics - System Demonstrations , pages 7\u201312 Melbourne , Australia , July 15 - 20 , 2018 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics7NovelPerspective : Identifying Point of View Characters Lyndon White , Roberto Togneri , Wei Liu , and Mohammed Bennamoun lyndon.white@research.uwa.edu.au , roberto.togneri@uwa.edu.au , wei.liu@uwa.edu.au , and mohammed.bennamoun@uwa.edu.au The University of Western Australia .", "entities": []}, {"text": "35 Stirling Highway , Crawley , Western Australia", "entities": []}, {"text": "Abstract We present NovelPerspective : a tool to allow consumers to subset their digital literature , based on point of view ( POV ) character .", "entities": []}, {"text": "Many novels have multiple main characters each with their own storyline running in parallel .", "entities": []}, {"text": "A well - known example is George R. R. Martin \u2019s novel : \u201c A Game of Thrones \u201d , and others from that series .", "entities": []}, {"text": "Our tool detects the main character that each section is from the POV of , and allows the user to generate a new ebook with only those sections .", "entities": []}, {"text": "This gives consumers new options in how they consume their media ; allowing them to pursue the storylines sequentially , or skip chapters about characters they \ufb01nd boring .", "entities": []}, {"text": "We present two heuristic - based baselines , and two machine learning based methods for the detection of the main character .", "entities": []}, {"text": "1 Introduction Often each section of a novel is written from the perspective of a different main character .", "entities": []}, {"text": "The characters each take turns in the spot - light , with their own parallel storylines being unfolded by the author .", "entities": []}, {"text": "As readers , we have often desired to read just one storyline at a time , particularly when reading the book a second - time .", "entities": []}, {"text": "In this paper , we present a tool , NovelPerspective , to give the consumer this choice .", "entities": []}, {"text": "Our tool allows the consumer to select which characters of the book they are interested in , and to generate a new ebook \ufb01le containing just the sections from that character \u2019s point of view ( POV ) .", "entities": []}, {"text": "The critical part of this system is the detection of the POV character .", "entities": []}, {"text": "This is not an insurmountable task , building upon the well established \ufb01eld of named entity recognition .", "entities": [[14, 17, "TaskName", "named entity recognition"]]}, {"text": "However to our knowl - edge there is no software to do this .", "entities": []}, {"text": "Such a tool would have been useless , in decades past when booked were distributed only on paper .", "entities": []}, {"text": "But today , the surge in popularity of ebooks has opened a new niche for consumer narrative processing .", "entities": []}, {"text": "Methods are being created to extract social relationships between characters ( Elson et al . , 2010 ; Wohlgenannt et al . , 2016 ) ; to align scenes in movies with those from books ( Zhu et al . , 2015 ) ; and to otherwise augment the literature consumption experience .", "entities": []}, {"text": "Tools such as the one presented here , give the reader new freedoms in controlling how they consume their media .", "entities": []}, {"text": "Having a large cast of characters , in particular POV characters , is a hallmark of the epic fantasy genre .", "entities": []}, {"text": "Well known examples include : George R.R. Martin \u2019s \u201c A Song of Ice and Fire \u201d , Robert Jordan \u2019s \u201c Wheel of Time \u201d , Brandon Sanderson \u2019s \u201c Cosmere \u201d universe , and Steven Erikson \u2019s \u201c Malazan Book of the Fallen \u201d , amongst thousands of others .", "entities": []}, {"text": "Generally , these books are written in limited third - person POV ; that is to say the reader has little or no more knowledge of the situation described than the main character does .", "entities": []}, {"text": "We focus here on novels written in the limited third - person POV .", "entities": []}, {"text": "In these stories , the main character is , for our purposes , the POV character .", "entities": []}, {"text": "Limited third - person POV is written in the thirdperson , that is to say the character is referred to by name , but with the observations limited to being from the perspective of that character .", "entities": []}, {"text": "This is in - contrast to the omniscient third - person POV , where events are described by an external narrator .", "entities": []}, {"text": "Limited third - person POV is extremely popular in modern \ufb01ction .", "entities": []}, {"text": "It preserves the advantages of \ufb01rst - person , in allowing the reader to observe inside the head of the character , while also allowing the \ufb02exibility to the perspective of another character ( Booth , 1961 ) .", "entities": []}, {"text": "This allows for multiple concurrent storylines around different characters .", "entities": []}, {"text": "8Our tool helps users un - entwine such storylines , giving the option to process them sequentially .", "entities": []}, {"text": "The utility of dividing a book in this way varies with the book in question .", "entities": []}, {"text": "Some books will cease to make sense when the core storyline crosses over different characters .", "entities": []}, {"text": "Other novels , particularly in epic fantasy genre , have parallel storylines which only rarely intersect .", "entities": []}, {"text": "While we are unable to \ufb01nd a formal study on this , anecdotally many readers speak of : \u000f\u201cSkipping the chapters about the boring characters . \u201d", "entities": []}, {"text": "\u000f\u201cOnly reading the real main character \u2019s sections . \u201d", "entities": []}, {"text": "\u000f\u201cReading ahead , past the side - stories , to get on with the main plot . \u201d", "entities": []}, {"text": "Particularly if they have read the story before , and thus do not risk confusion .", "entities": []}, {"text": "Such opinions are a matter of the consumer \u2019s personal taste .", "entities": []}, {"text": "The NovelPerspective tool gives the reader the option to customise the book in this way , according to their personal preference .", "entities": []}, {"text": "We note that sub - setting the novel once does not prevent the reader from going back and reading the intervening chapters if it ceases to make sense , or from sub - setting again to get the chapters for another character whose path intersects with the storyline they are currently reading .", "entities": []}, {"text": "We can personally attest for some books reading the chapters one character at a time is indeed possible , and pleasant : the \ufb01rst author of this paper read George R.R. Martin \u2019s \u201c A Song of Ice and Fire \u201d series in exactly this fashion .", "entities": []}, {"text": "The primary dif\ufb01culty in segmenting ebooks this way is attributing each section to its POV character .", "entities": []}, {"text": "That is to say detecting who is the point of view character .", "entities": []}, {"text": "Very few books indicate this clearly , and the reader is expected to infer it during reading .", "entities": []}, {"text": "This is easy for most humans , but automating it is a challenge .", "entities": []}, {"text": "To solve this , the core of our tool is its character classi\ufb01cation system .", "entities": []}, {"text": "We investigated several options which the main text of this paper will discuss .", "entities": []}, {"text": "2 Character Classi\ufb01cation Systems", "entities": []}, {"text": "The full NovelPerspective pipeline is shown in Figure 1 .", "entities": []}, {"text": "The core character classi\ufb01cation step ( step 3 ) , is detailed in Figure 2 .", "entities": []}, {"text": "In this step the raw text is \ufb01rst enriched with parts of speech , and named entity tags .", "entities": []}, {"text": "We do not perform coreference resolution , working only with direct entity mentions .", "entities": [[4, 6, "TaskName", "coreference resolution"]]}, {"text": "From this , features are extracted for each named entity .", "entities": []}, {"text": "These feature vectors are used to score the entities for the most - likely POV character .", "entities": []}, {"text": "The highest scoring character is returned by the system .", "entities": []}, {"text": "The different systems presented modify the Feature Extraction andCharacter Scoringsteps .", "entities": []}, {"text": "A broadly similar idea , for detecting the focus location of news articles , was presented by ( Imani et al . , 2017 ) .", "entities": []}, {"text": "2.1 Baseline systems To the best of our knowledge no systems have been developed for this task before .", "entities": []}, {"text": "As such , we have developed two deterministic baseline character classi\ufb01ers .", "entities": []}, {"text": "These are both potentially useful to the end - user in our deployed system ( Section 5 ) , and used to gauge the performance of the more complicated systems in the evaluations presented in Section 4 .", "entities": []}, {"text": "It should be noted that the baseline systems , while not using machine learning for the character classi\ufb01cation steps , do make extensive use of machine learning - based systems during the preprocessing stages .", "entities": []}, {"text": "2.1.1 \u201c First Mentioned \u201d Entity An obvious way to determine the main character of the section is to select the \ufb01rst named entity .", "entities": []}, {"text": "We use this to de\ufb01ne the \u201c First Mentioned \u201d baseline In this system , the Feature Extraction step is simply retrieving the position of the \ufb01rst use of each name ; and the Character Scoring step assigns each a score such that earlier is higher .", "entities": []}, {"text": "This works for many examples : \u201c One dark and stormy night , Bill heard a knock at the door . \u201d", "entities": []}, {"text": "; however it fails for many others : \u201c \u2018 Is that Tom ? \u2019 called out Bill , after hearing a knock . \u2019 \u2019 .", "entities": []}, {"text": "Sometimes a section may go several paragraphs describing events before it even mentions the character who is perceiving them .", "entities": []}, {"text": "This is a varying element of style .", "entities": []}, {"text": "2.1.2 \u201c Most Mentioned \u201d Entity A more robust method to determine the main character , is to use the occurrence counts .", "entities": []}, {"text": "We call this the \u201c Most Mentioned \u201d baseline .", "entities": []}, {"text": "The Feature Extraction step is to count how often the name is used .", "entities": []}, {"text": "The Character Scoring step assigns each a score based what proportional of all names used were for this entity .", "entities": []}, {"text": "This works well for many books .", "entities": []}, {"text": "The more important a character", "entities": []}, {"text": "91 .", "entities": []}, {"text": "User uploads ebook2 .", "entities": []}, {"text": "File is converted and preprocessed3 .", "entities": []}, {"text": "Sections are classifed by character See Figure 24 .", "entities": []}, {"text": "User selects sections to keep5 .", "entities": []}, {"text": "Subsetted ebook is created6 .", "entities": []}, {"text": "User downloads new ebookoriginal ebook + settingssection contentsection - character listsection selectionnew ebook Figure 1 : The full NovelPerspective pipeline .", "entities": []}, {"text": "Note that step 5 uses the original ebook to subset .", "entities": []}, {"text": "Tokenization POS Tagging Named Entity TaggingFeature ExtractionCharacter ScoringPOV Character Classi\ufb01cationraw textenriched textcharacter - name feature - vector pairscharacter - name score pairscharacter name Figure 2 : The general structure of the character classi\ufb01cation systems .", "entities": []}, {"text": "This repeated for each section of the book during step 3 of the full pipeline shown in Figure 1 . is , the more often their name occurs .", "entities": []}, {"text": "However , it is fooled , for example , by book chapters that are about the POV character \u2019s relationship with a secondary character .", "entities": []}, {"text": "In such cases the secondary character may be mentioned more often .", "entities": []}, {"text": "2.2 Machine learning systems One can see the determination of the main character as a multi - class classi\ufb01cation problem .", "entities": []}, {"text": "From the set of all named entities in the section , classify that section as to which one is the main character .", "entities": []}, {"text": "Unlike typical multi - class classi\ufb01cation problems the set of possible classes varies per section being classi\ufb01ed .", "entities": []}, {"text": "Further , even the total set of possible named characters , i.e. classes , varies from book to book .", "entities": []}, {"text": "An information extraction approach is required which can handle these varying classes .", "entities": []}, {"text": "As such , a machine learning model for this task can not incorporate direct knowledge of the classes ( i.e. character names ) .", "entities": []}, {"text": "We reconsider the problem as a series of binary predictions .", "entities": []}, {"text": "The task is to predict if a given named entity is the point of view character .", "entities": []}, {"text": "For each possible character ( i.e. each named - entity that occurs ) , a feature vector is extracted ( see Section 2.2.1 ) .", "entities": []}, {"text": "This feature vector is the input to a binary classi\ufb01er , which determines the probability that it represents the main character .", "entities": []}, {"text": "The Character Scoring step is thus the running of the binary classi\ufb01er : the score is the output probability normalised over all the named entities .", "entities": []}, {"text": "2.2.1 Feature Extraction for ML We investigated two feature sets as inputs for our machine learning - based solution .", "entities": []}, {"text": "They correspond to different Feature Extraction steps in Figure 2 .", "entities": []}, {"text": "A hand - engineered feature set , that we call the \u201c Classical \u201d feature set ; and a more modern \u201c Word Embedding \u201d feature set .", "entities": []}, {"text": "Both feature setsgive information about how the each named entity token was used in the text .", "entities": []}, {"text": "The \u201c Classical \u201d feature set uses features that are well established in NLP related tasks .", "entities": []}, {"text": "The features can be described as positional features , like in the First Mentioned baseline ; occurrence count features , like in the Most Mentioned baseline and adjacent POS counts , to give usage context .", "entities": []}, {"text": "The positional features are the index ( in the token counts ) of the \ufb01rst and last occurrence of the named entity .", "entities": []}, {"text": "The occurrence count features are simply the number of occurrences of the named entity , supplemented with its rank on that count compared to the others .", "entities": []}, {"text": "The adjacent POS counts are the occurrence counts of each of the 46 POS tags on the word prior to the named entity , and on the word after .", "entities": []}, {"text": "We theorised that this POS information would be informative , as it seemed reasonable that the POV character would be described as doing more things , so co - occurring with more verbs .", "entities": []}, {"text": "This gives 100 base features .", "entities": []}, {"text": "To allow for text length invariance we also provide each of the base features expressed as a portion of its maximum possible value ( e.g. for a given POS tag occurring before a named entity , the potion of times this tag occurred ) .", "entities": []}, {"text": "This gives a total of 200 features .", "entities": []}, {"text": "The \u201c Word Embedding \u201d feature set uses FastText word vectors ( Bojanowski et al . , 2017 ) .", "entities": [[8, 9, "MethodName", "FastText"]]}, {"text": "We use the pretrained 300 dimensional embeddings trained on English Wikipedia1 .", "entities": []}, {"text": "We concatenate the 300 dimensional word embedding for the word immediately prior to , and immediately after each occurrence of a named entity ; and take the element - wise mean of this concatenated vector over all occurrences of the entity .", "entities": []}, {"text": "Such averages of word embeddings have been shown to be a useful 1https://fasttext.cc/docs/en/ pretrained-vectors.html", "entities": [[3, 5, "TaskName", "word embeddings"]]}, {"text": "10feature in many tasks ( White et al . , 2015 ; Mikolov et al . , 2013 ) .", "entities": []}, {"text": "This has a total of 600 features .", "entities": []}, {"text": "2.2.2", "entities": []}, {"text": "Classi\ufb01er The binary classi\ufb01er , that predicts if a named entity is the main character , is the key part of the Character Scoring step for the machine learning systems .", "entities": []}, {"text": "From each text in the training dataset we generated a training example for every named entity that occurred .", "entities": []}, {"text": "All but one of these was a negative example .", "entities": []}, {"text": "We then trained it as per normal for a binary classi\ufb01er .", "entities": []}, {"text": "The score for a character is the classi\ufb01er \u2019s predicted probability of its feature vector being for the main character .", "entities": []}, {"text": "Our approach of using a binary classi\ufb01er to rate each possible class , may seem similar to the one - vs - rest approach for multi - class classi\ufb01cation .", "entities": []}, {"text": "However , there is an important difference .", "entities": []}, {"text": "Our system only uses a single binary classi\ufb01er ; not one classi\ufb01er per class , as the classes in our case vary with every item to be classi\ufb01ed .", "entities": []}, {"text": "The fundamental problem is information extraction , and the classi\ufb01er is a tool for the scoring which is the correct information to report .", "entities": []}, {"text": "With the classical feature set we use logistic regression , with the features being preprocessed with 0 - 1 scaling .", "entities": [[7, 9, "MethodName", "logistic regression"], [16, 17, "DatasetName", "0"]]}, {"text": "During preliminary testing we found that many classi\ufb01ers had similar high degree of success , and so chose the simplest .", "entities": []}, {"text": "With the word embedding feature set we used a radial bias support vector machine , with standardisation during preprocessing , as has been commonly used with word embeddings on other tasks .", "entities": [[11, 14, "MethodName", "support vector machine"], [26, 28, "TaskName", "word embeddings"]]}, {"text": "3 Experimental Setup 3.1 Datasets We make use of three series of books selected from our own personal collections .", "entities": []}, {"text": "The \ufb01rst four books of George R. R. Martin \u2019s \u201c A Song of Ice and Fire \u201d series ( hereafter referred to as ASOIAF ) ; The two books of Leigh Bardugo \u2019s \u201c Six of Crows \u201d duology ( hereafter referred to as SOC ) ; and the \ufb01rst 9 volumes of Robert Jordan \u2019s \u201c Wheel of Time \u201d series ( hereafter referred to as WOT ) .", "entities": [[45, 46, "DatasetName", "SOC"]]}, {"text": "In Section 4 we consider the use of each as a training and testing dataset .", "entities": []}, {"text": "In the online demonstration ( Section 5 ) , we deploy models trained on the combined total of all the datasets .", "entities": []}, {"text": "To use a book for the training and evaluation of our system , we require a ground truth for each section \u2019s POV character .", "entities": []}, {"text": "ASOIAF and SOC provideDataset Chapters POV Characters ASOIAF 256 15 SOC 91 9 WOT 432 52 combined 779 76 Table 1 : The number of chapters and point of view characters for each dataset .", "entities": [[2, 3, "DatasetName", "SOC"], [10, 11, "DatasetName", "SOC"]]}, {"text": "ground truth for the main character in the chapter names .", "entities": []}, {"text": "Every chapter only uses the POV of that named character .", "entities": []}, {"text": "WOT \u2019s ground truth comes from an index created by readers.2We do not have any datasets with labelled sub - chapter sections , though the tool does support such works .", "entities": []}, {"text": "The total counts of chapters and characters in the datasets , after preprocessing , is shown in Table 1 .", "entities": []}, {"text": "Preprocessing consisted of discarding chapters for which the POV character was not identi\ufb01ed ( e.g. prologues ) ; and of removing the character names from the chapter titles as required .", "entities": []}, {"text": "3.2 Evaluation Details In the evaluation , the systems are given the body text and asked to predict the character names .", "entities": []}, {"text": "During evaluation , we sum the scores of the characters alternative aliases / nick - names used in the books .", "entities": []}, {"text": "For example merging Ned intoEddard in ASOIAF .", "entities": []}, {"text": "This roughly corresponds to the case that a normal user can enter multiple aliases into our application when selecting sections to keep .", "entities": []}, {"text": "We do not use these aliases during training , though that is an option that could be investigated in a future work .", "entities": []}, {"text": "3.3 Implementation The full source code is available on GitHub.3 Scikit - Learn ( Pedregosa et al . , 2011 ) is used for the machine learning and evaluations , and NLTK ( Bird and Loper , 2004 ) is used for textual preprocessing .", "entities": []}, {"text": "The text is tokenised , and tagged with POS and named entities using NLTK \u2019s default methods .", "entities": []}, {"text": "Speci\ufb01cally , these are the Punkt sentence tokenizer , the regex - based improved TreeBank word tokenizer , greedy averaged perceptron POS tagger , and the max - entropy binary named entity chunker .", "entities": []}, {"text": "The use of a binary , rather than 2http://wot.wikia.com/wiki/List_of _ Point_of_View_Characters 3https://github.com/oxinabox/ NovelPerspective/", "entities": []}, {"text": "11Test Set Method Train Set Acc ASOIAF First Mentioned \u2014 0:250 ASOIAF Most Mentioned \u2014 0:914 ASOIAF ML Classical Features SOC 0:953 ASOIAF ML Classical Features WOT 0:984", "entities": [[5, 6, "MetricName", "Acc"], [20, 21, "DatasetName", "SOC"]]}, {"text": "ASOIAF ML Classical Features WOT+SOC 0:977", "entities": []}, {"text": "ASOIAF ML Word Emb .", "entities": []}, {"text": "Features SOC 0:863", "entities": [[1, 2, "DatasetName", "SOC"]]}, {"text": "ASOIAF ML Word Emb .", "entities": []}, {"text": "Features WOT 0:977", "entities": []}, {"text": "ASOIAF ML Word Emb .", "entities": []}, {"text": "Features WOT+SOC 0:973", "entities": []}, {"text": "SOC First Mentioned \u2014 0:429 SOC Most Mentioned \u2014 0:791 SOC ML Classical Features WOT 0:923", "entities": [[0, 1, "DatasetName", "SOC"], [5, 6, "DatasetName", "SOC"], [10, 11, "DatasetName", "SOC"]]}, {"text": "SOC ML Classical Features ASOIAF 0:923", "entities": [[0, 1, "DatasetName", "SOC"]]}, {"text": "SOC ML Classical Features WOT+ASOIAF 0:934 SOC ML Word Emb .", "entities": [[0, 1, "DatasetName", "SOC"], [6, 7, "DatasetName", "SOC"]]}, {"text": "Features WOT 0:934 SOC ML Word Emb .", "entities": [[3, 4, "DatasetName", "SOC"]]}, {"text": "Features ASOIAF 0:945 SOC ML Word Emb .", "entities": [[3, 4, "DatasetName", "SOC"]]}, {"text": "Features WOT+ASOIAF 0:945", "entities": []}, {"text": "WOT", "entities": []}, {"text": "First Mentioned \u2014 0:044 WOT Most Mentioned \u2014 0:660 WOT ML Classical Features SOC 0:701 WOT ML Classical Features ASOIAF 0:745", "entities": [[13, 14, "DatasetName", "SOC"]]}, {"text": "WOT ML Classical Features ASOIAF+SOC 0:736 WOT ML Word Emb .", "entities": []}, {"text": "Features SOC 0:551", "entities": [[1, 2, "DatasetName", "SOC"]]}, {"text": "WOT ML Word Emb .", "entities": []}, {"text": "Features ASOIAF 0:699 WOT ML Word Emb .", "entities": []}, {"text": "Features ASOIAF+SOC", "entities": []}, {"text": "0:681 Table 2 : The results of the character classi\ufb01er systems .", "entities": []}, {"text": "The best results are bolded .", "entities": []}, {"text": "a multi - class , named entity chunker is signi\ufb01cant .", "entities": []}, {"text": "Fantasy novels often use \u201c exotic \u201d names for characters , we found that this often resulted in character named entities being misclassi\ufb01ed as organisations or places .", "entities": []}, {"text": "Note that this is particularly disadvantageous to the First Mentioned baseline , as any kind of named entity will steal the place .", "entities": []}, {"text": "Nevertheless , it is required to ensure that all character names are a possibility to be selected .", "entities": []}, {"text": "4 Results and Discussion Our evaluation results are shown in Table 2 for all methods .", "entities": []}, {"text": "This includes the two baseline methods , and the machine learning methods with the different feature sets .", "entities": []}, {"text": "We evaluate the machine learning methods using each dataset as a test set , and using each of the other two and their combination as the training set .", "entities": []}, {"text": "The First Mentioned baseline is very weak .", "entities": []}, {"text": "The Most Mentioned baseline is much stronger .", "entities": []}, {"text": "In almost all cases machine learning methods outperform both baselines .", "entities": []}, {"text": "The results of the machine learning method on the ASOIAF and SOC are very strong .", "entities": [[11, 12, "DatasetName", "SOC"]]}, {"text": "The results for WOT are weaker , though they are still accurate enough to be useful when combined with manual checking .", "entities": []}, {"text": "It is surprising that using the combination ofTest Set Method Train Set Acc ASOIAF ML Classical Features ASOIAF 0:980 ASOIAF ML Word Emb .", "entities": [[12, 13, "MetricName", "Acc"]]}, {"text": "Features ASOIAF 0:988", "entities": []}, {"text": "SOC ML Classical Features SOC 0:945 SOC ML Word Emb .", "entities": [[0, 1, "DatasetName", "SOC"], [4, 5, "DatasetName", "SOC"], [6, 7, "DatasetName", "SOC"]]}, {"text": "Features SOC 0:956 WOT ML Classical Features WOT 0:785", "entities": [[1, 2, "DatasetName", "SOC"]]}, {"text": "WOT ML Word Emb .", "entities": []}, {"text": "Features WOT 0:794", "entities": []}, {"text": "Table 3 : The training set accuracy of the machine learning character classi\ufb01er systems .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "two training sets does not always out - perform each on their own .", "entities": []}, {"text": "For many methods training on just one dataset resulted in better results .", "entities": []}, {"text": "We believe that the difference between the top result for a method and the result using the combined training sets is too small to be meaningful .", "entities": []}, {"text": "It can , perhaps , be attributed to a coincidental small similarity in writing style of one of the training books to the testing book .", "entities": []}, {"text": "To maximise the generalisability of the NovelPerspective prototype ( see Section 5 ) , we deploy models trained on all three datasets combined .", "entities": []}, {"text": "Almost all the machine learning models resulted in similarly high accuracy .", "entities": [[10, 11, "MetricName", "accuracy"]]}, {"text": "The exception to this is word embedding features based model trained on SOC , which for both ASOIAF and WOT test sets performed much worse .", "entities": [[12, 13, "DatasetName", "SOC"]]}, {"text": "We attribute the poor performance of these models to the small amount of training data .", "entities": []}, {"text": "SOC has only 91 chapters to generate its training cases from , and the word embedding feature set has 600 dimensions .", "entities": [[0, 1, "DatasetName", "SOC"]]}, {"text": "It is thus very easily to over-\ufb01t which causes these poor results .", "entities": []}, {"text": "Table 3 shows the training set accuracy of each machine learning model .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "This is a rough upper bound for the possible performance of these models on each test set , as imposed by the classi\ufb01er and the feature set .", "entities": []}, {"text": "The WOT bound is much lower than the other two texts .", "entities": []}, {"text": "This likely relates to WOT being written in a style that closer to the line between third - person omniscient , than the more clear third - person limited POV of the other texts .", "entities": []}, {"text": "We believe longer range features are required to improve the results for WOT .", "entities": []}, {"text": "However , as this achieves such high accuracy for the other texts , further features would not improve accuracy signi\ufb01cantly , without additional more dif\ufb01cult training data ( and may cause over-\ufb01tting ) .", "entities": [[7, 8, "MetricName", "accuracy"], [18, 19, "MetricName", "accuracy"]]}, {"text": "The results do not show a clear advantage to either machine learning feature set .", "entities": []}, {"text": "Both the classical features and the word embeddings work well .", "entities": [[6, 8, "TaskName", "word embeddings"]]}, {"text": "12Though , it seems that the classical feature are more robust ; both with smaller training sets ( like SOC ) , and with more dif\ufb01cult test sets ( like WOT ) .", "entities": [[19, 20, "DatasetName", "SOC"]]}, {"text": "5 Demonstration System The demonstration system is deployed online at https://white.ucc.asn.au/tools/np .", "entities": []}, {"text": "A video demonstrating its use can be found at https://youtu.be/iu41pUF4wTY .", "entities": []}, {"text": "This web - app , made using the CherryPy framework,4 allows the user to apply any of the model discussed to their own novels .", "entities": []}, {"text": "The web - app functions as shown in Figure 1 .", "entities": []}, {"text": "The user uploads an ebook , and selects one of the character classi\ufb01cation systems that we have discussed above .", "entities": []}, {"text": "They are then presented with a page displaying a list of sections , with the predicted main character(/s ) paired with an excerpt from the beginning of the section .", "entities": []}, {"text": "The user can adjust to show the top - k most - likely characters on this screen , to allow for additional recall .", "entities": []}, {"text": "The user can select sections to retain .", "entities": []}, {"text": "They can use a regular expression to match the character names(/s ) they are interested in .", "entities": []}, {"text": "The sections with matching predicted character names will be selected .", "entities": []}, {"text": "As none of the models is perfect , some mistakes are likely .", "entities": []}, {"text": "The user can manually correct the selection before downloading the book .", "entities": []}, {"text": "6 Conclusion We have presented a tool to allow consumers to restructure their ebooks around the characters they \ufb01nd most interesting .", "entities": []}, {"text": "The system must discover the named entities that are present in each section of the book , and then classify each section as to which character \u2019s point of view the section is narrated from .", "entities": []}, {"text": "For named entity detection we make use of standard tools .", "entities": []}, {"text": "However , the classi\ufb01cation is non - trivial .", "entities": []}, {"text": "In this design we implemented several systems .", "entities": []}, {"text": "Simply selecting the most commonly named character proved successful as a baseline approach .", "entities": []}, {"text": "To improve upon this , we developed several machine learning based approaches which perform very well .", "entities": []}, {"text": "While none of the classi\ufb01ers are perfect , they achieve high enough accuracy to be useful .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "A future version of our application will allow the users to submit corrections , giving us more training data .", "entities": []}, {"text": "However , storing this information poses copyright issues that are yet to be resolved .", "entities": []}, {"text": "4http://cherrypy.org/Acknowledgements This research was partially funded by Australian Research Council grants DP150102405 and LP110100050 .", "entities": []}, {"text": "References Bird , S. and Loper , E. ( 2004 ) .", "entities": []}, {"text": "Nltk : the natural language toolkit .", "entities": []}, {"text": "In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions , page 31 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Bojanowski , P. , Grave , E. , Joulin , A. , and Mikolov , T. ( 2017 ) .", "entities": []}, {"text": "Enriching word vectors with subword information .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 5:135\u2013146 .", "entities": []}, {"text": "Booth , W. C. ( 1961 ) .", "entities": []}, {"text": "The rhetoric of \ufb01ction .", "entities": []}, {"text": "University of Chicago Press .", "entities": []}, {"text": "Elson , D. K. , Dames , N. , and McKeown , K. R. ( 2010 ) .", "entities": []}, {"text": "Extracting social networks from literary \ufb01ction .", "entities": []}, {"text": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , ACL \u2019 10 , pages 138\u2013147 , Stroudsburg , PA , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Imani , M. B. , Chandra , S. , Ma , S. , Khan , L. , and Thuraisingham , B. ( 2017 ) .", "entities": []}, {"text": "Focus location extraction from political news reports with bias correction .", "entities": []}, {"text": "In 2017 IEEE International Conference on Big Data ( Big Data ) , pages 1956\u20131964 .", "entities": []}, {"text": "Mikolov , T. , Sutskever , I. , Chen , K. , Corrado , G. S. , and Dean , J. ( 2013 ) .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "InAdvances in Neural Information Processing Systems , pages 3111\u20133119 .", "entities": []}, {"text": "Pedregosa , F. , Varoquaux , G. , Gramfort , A. , Michel , V . , Thirion , B. , Grisel , O. , Blondel , M. , Prettenhofer , P. , Weiss , R. , Dubourg , V . , Vanderplas , J. , Passos , A. , Cournapeau , D. , Brucher , M. , Perrot , M. , and Duchesnay , E. ( 2011 ) .", "entities": []}, {"text": "Scikit - learn : Machine learning in Python .", "entities": []}, {"text": "Journal of Machine Learning Research , 12:2825\u20132830 .", "entities": []}, {"text": "White , L. , Togneri , R. , Liu , W. , and Bennamoun , M. ( 2015 ) .", "entities": []}, {"text": "How well sentence embeddings capture meaning .", "entities": [[2, 4, "TaskName", "sentence embeddings"]]}, {"text": "In Proceedings of the 20th Australasian Document Computing Symposium , ADCS \u2019 15 , pages 9:1\u20139:8 .", "entities": []}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Wohlgenannt , G. , Chernyak , E. , and Ilvovsky , D. ( 2016 ) .", "entities": []}, {"text": "Extracting social networks from literary text with word embedding tools .", "entities": []}, {"text": "In Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities ( LT4DH ) , pages 18 \u2013 25 .", "entities": []}, {"text": "Zhu , Y . , Kiros , R. , Zemel , R. , Salakhutdinov , R. , Urtasun , R. , Torralba , A. , and Fidler , S. ( 2015 ) .", "entities": []}, {"text": "Aligning books and movies : Towards story - like visual explanations by watching movies and reading books .", "entities": []}, {"text": "InProceedings of the IEEE international conference on computer vision , pages 19\u201327 .", "entities": []}]