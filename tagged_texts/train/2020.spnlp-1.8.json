[{"text": "Proceedings of 4th Workshop on Structured Prediction for NLP , pages 62\u201373 November 20 , 2020 .", "entities": [[5, 7, "TaskName", "Structured Prediction"]]}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics62Improving Joint Training of Inference Networks and Structured", "entities": []}, {"text": "Prediction Energy Networks Lifu", "entities": []}, {"text": "Tu1Richard Yuanzhe Pang2\u0003Kevin Gimpel1 1Toyota Technological Institute at Chicago , Chicago , IL 60637 , USA 2New York University , New York , NY 10011 , USA lifu@ttic.edu , yzpang@nyu.edu , kgimpel@ttic.edu", "entities": []}, {"text": "Abstract Deep energy - based models are powerful , but pose challenges for learning and inference ( Belanger and McCallum , 2016 ) .", "entities": []}, {"text": "Tu and Gimpel ( 2018 ) developed an ef\ufb01cient framework for energy - based models by training \u201c inference networks \u201d to approximate structured inference instead of using gradient descent .", "entities": []}, {"text": "However , their alternating optimization approach suffers from instabilities during training , requiring additional loss terms and careful hyperparameter tuning .", "entities": [[14, 15, "MetricName", "loss"]]}, {"text": "In this paper , we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction .", "entities": [[22, 24, "TaskName", "structured prediction"]]}, {"text": "We design a compound objective to jointly train both costaugmented and test - time inference networks along with the energy function .", "entities": []}, {"text": "We propose joint parameterizations for the inference networks that encourage them to capture complementary functionality during learning .", "entities": []}, {"text": "We empirically validate our strategies on two sequence labeling tasks , showing easier paths to strong performance than prior work , as well as further improvements with global energy terms .", "entities": []}, {"text": "1 Introduction Energy - based modeling ( LeCun et al . , 2006 ) associates a scalar compatibility measure to each con\ufb01guration of input and output variables .", "entities": []}, {"text": "Belanger and McCallum ( 2016 ) formulated deep energy - based models for structured prediction , which they called structured prediction energy networks ( SPENs ) .", "entities": [[13, 15, "TaskName", "structured prediction"], [19, 21, "TaskName", "structured prediction"]]}, {"text": "SPENs use arbitrary neural networks to de\ufb01ne the scoring function over input / output pairs .", "entities": []}, {"text": "However , this \ufb02exibility leads to challenges for learning and inference .", "entities": []}, {"text": "The original work on SPENs used gradient descent for structured inference ( Belanger and McCallum , 2016 ; Belanger et al . , 2017 ) .", "entities": []}, {"text": "Tu \u0003Work done at the University of Chicago and Toyota Technological Institute at Chicago.and Gimpel ( 2018 , 2019 ) found improvements in both speed and accuracy by replacing the use of gradient descent with a method that trains a neural network ( called an \u201c inference network \u201d ) to do inference directly .", "entities": [[26, 27, "MetricName", "accuracy"]]}, {"text": "Their formulation , which jointly trains the inference network and energy function , is similar to training in generative adversarial networks ( Goodfellow et", "entities": []}, {"text": "al . , 2014 ) , which is known to suffer from practical dif\ufb01culties in training due to the use of alternating optimization ( Salimans et al . , 2016 ) .", "entities": []}, {"text": "To stabilize training , Tu and Gimpel ( 2018 ) experimented with several additional terms in the training objectives , \ufb01nding performance to be dependent on their inclusion .", "entities": []}, {"text": "Moreover , when using the approach of Tu and Gimpel ( 2018 ) , there is a mismatch between the training and test - time uses of the trained inference network .", "entities": []}, {"text": "During training with hinge loss , the inference network is actually trained to do \u201c costaugmented \u201d inference .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "However , at test time , the goal is to simply minimize the energy without any cost term .", "entities": []}, {"text": "Tu and Gimpel ( 2018 ) \ufb01ne - tuned the cost - augmented network to match the test - time criterion , but found only minimal change from this \ufb01ne - tuning .", "entities": []}, {"text": "This suggests that the cost - augmented network was mostly acting as a test - time inference network by convergence , which may be hindering the potential contributions of cost - augmented inference in max - margin structured learning ( Tsochantaridis et al . , 2004 ; Taskar et al . , 2004 ) .", "entities": []}, {"text": "In this paper , we contribute a new training objective for SPENs that addresses the above concern and also contribute several techniques for stabilizing and improving learning .", "entities": []}, {"text": "We empirically validate our strategies on two sequence labeling tasks from natural language processing ( NLP ) , namely part - of - speech tagging and named entity recognition .", "entities": [[19, 25, "TaskName", "part - of - speech tagging"], [26, 29, "TaskName", "named entity recognition"]]}, {"text": "We show easier paths to strong performance than prior work , as well as further improvements with global energy terms .", "entities": []}, {"text": "We summarize our list", "entities": []}, {"text": "63of contributions as follows .", "entities": []}, {"text": "\u2022We design a compound objective under the SPEN framework to jointly train the \u201c trainingtime \u201d cost - augmented inference network and test - time inference network ( Section 3 ) .", "entities": []}, {"text": "\u2022We propose shared parameterizations for the two inference networks so as to encourage them to capture complementary functionality while reducing the total number of trained parameters ( Section 3.1 ) .", "entities": []}, {"text": "Quantitative and qualitative analysis shows clear differences in the characteristics of the two networks ( Table 3 ) .", "entities": []}, {"text": "\u2022We present three methods to streamline and stabilize training that help with both the old and new objectives ( Section 4 ) .", "entities": []}, {"text": "\u2022We propose global energy terms to capture long - distance dependencies and obtain further improvements ( Section 5 ) .", "entities": []}, {"text": "While SPENs have been used for multiple NLP tasks , including multi - label classi\ufb01cation ( Belanger and McCallum , 2016 ) , part - of - speech tagging ( Tu and Gimpel , 2018 ) , and semantic role labeling ( Belanger et al . , 2017 ) , they are not widely used in NLP .", "entities": [[23, 29, "TaskName", "part - of - speech tagging"], [38, 41, "TaskName", "semantic role labeling"]]}, {"text": "Structured prediction is extremely common in NLP , but is typically approached using methods that are more limited than SPENs ( such as conditional random \ufb01elds ) or models that suffer from a train / test mismatch ( such as most auto - regressive models ) .", "entities": [[0, 2, "TaskName", "Structured prediction"]]}, {"text": "SPENs offer a maximally expressive framework for structured prediction while avoiding the train / test mismatch and therefore offer great potential for NLP .", "entities": [[7, 9, "TaskName", "structured prediction"]]}, {"text": "However , the training and inference have deterred NLP researchers .", "entities": []}, {"text": "While we have found bene\ufb01t from training inference networks for machine translation in recent work ( Tu et al . , 2020b ) , that work assumed a \ufb01xed , pretrained energy function .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "Our hope is that the methods in this paper will enable SPENs to be applied to a larger set of applications , including generation tasks in the future .", "entities": []}, {"text": "2 Background We denote the input space by X. For an input x2X , we denote the structured output space by Y(x ) .", "entities": []}, {"text": "The entire space of structured outputs is denotedY=[x2XY(x ) .", "entities": []}, {"text": "A SPEN ( Belanger and McCallum , 2016 ) de\ufb01nes an energy function E\u0002 : X\u0002Y !", "entities": []}, {"text": "Rparameterized by \u0002that computes a scalar energy for an input / output pair .", "entities": []}, {"text": "At test time , for a given input x , prediction is done by choosing the output with lowest energy : ^y= arg miny2Y(x)E\u0002(x;y ) ( 1 ) However , solving equation ( 1 ) requires combinatorial algorithms because Yis a structured , discrete space .", "entities": []}, {"text": "This becomes intractable when E\u0002does not decompose into a sum over small \u201c parts \u201d of y. Belanger and McCallum ( 2016 ) relaxed this problem by allowing the discrete vector yto be continuous;YRdenotes the relaxed output space .", "entities": []}, {"text": "They solved the relaxed problem by using gradient descent to iteratively minimize the energy with respect to y. The energy function parameters \u0002are trained using a structured hinge loss which requires repeated cost - augmented inference during training .", "entities": [[28, 29, "MetricName", "loss"]]}, {"text": "Using gradient descent for the repeated costaugmented inference steps is time - consuming and makes learning unstable ( Belanger et al . , 2017 ) .", "entities": []}, {"text": "Tu and Gimpel ( 2018 ) replaced gradient descent with a neural network trained to do ef\ufb01cient inference .", "entities": []}, {"text": "This \u201c inference network \u201d A \t : X!Y Ris parameterized by \t and trained with the goal that A \t ( x)\u0019arg min y2YR(x)E\u0002(x;y ) ( 2 ) When training the energy function parameters \u0002 , Tu and Gimpel ( 2018 ) replaced the cost - augmented inference step in the structured hinge loss from Belanger and McCallum ( 2016 ) with a costaugmented inference network F\b : F\b(x)\u0019arg min y2YR(x)(E\u0002(x;y)\u00004(y;y\u0003))(3 ) where4is a structured cost function that computes the distance between its two arguments .", "entities": [[54, 55, "MetricName", "loss"]]}, {"text": "We use L1 distance for4 .", "entities": []}, {"text": "This inference problem involves \ufb01nding an output with low energy but high cost relative to the gold standard .", "entities": []}, {"text": "Thus , it is not wellaligned with the test - time inference problem .", "entities": []}, {"text": "Here is the speci\ufb01c objective to jointly train \u0002 ( parameters of the energy function ) and \b(parameters of the cost - augmented inference network ): min \u0002max \bX hxi;yii2D[4(F\b(xi);yi ) \u0000E\u0002(xi;F\b(xi ) )", "entities": []}, {"text": "+ E\u0002(xi;yi)]+ ( 4 ) whereDis the set of training pairs , [ h]+= max(0;h ) , and4is a structured cost function that computes the distance between its two arguments .", "entities": []}, {"text": "64Tu and Gimpel ( 2018 ) alternatively optimized \u0002 and\b , which is similar to training in generative adversarial networks ( Goodfellow et", "entities": []}, {"text": "al . , 2014 ) .", "entities": []}, {"text": "The inference network is analogous to the generator and the energy function is analogous to the discriminator .", "entities": []}, {"text": "As alternating optimization can be dif\ufb01cult in practice ( Salimans et al . , 2016 ) , Tu & Gimpel experimented with including several additional terms in the above objective to stabilize training .", "entities": []}, {"text": "After the training of the energy function , an inference network A \t for test - time prediction is \ufb01netuned with the goal shown in Eq .", "entities": []}, {"text": "( 2 ) .", "entities": []}, {"text": "More speci\ufb01cally , for the \ufb01ne - tuning step , we \ufb01rst initialize \t  with\b ; next , we do gradient descent according to the following objective to learn \t : \t  arg min \t 0X x2XE\u0002(x;A \t 0(x ) )", "entities": []}, {"text": "whereXis a set of training or validation inputs .", "entities": []}, {"text": "It could also be the test inputs in a transductive setting .", "entities": []}, {"text": "3", "entities": []}, {"text": "An Objective for Joint Learning of Inference Networks One challenge with the above optimization problem is that it requires training a separate inference network A \t for test - time prediction after the energy function is trained .", "entities": []}, {"text": "In this paper , we propose an alternative that trains the energy function and both inference networks jointly .", "entities": []}, {"text": "In particular , we use a \u201c compound \u201d objective that combines two widely - used losses in structured prediction .", "entities": [[18, 20, "TaskName", "structured prediction"]]}, {"text": "We \ufb01rst present it without inference networks : min \u0002X hxi;yii2D\u0014 max y(4(y;yi)\u0000E\u0002(xi;y)+E\u0002(xi;yi))\u0015 + | { z } margin - rescaled hinge loss + \u0015\u0014 max y(\u0000E\u0002(xi;y )", "entities": [[22, 23, "MetricName", "loss"]]}, {"text": "+ E\u0002(xi;yi))\u0015 + | { z } perceptron loss(5 )", "entities": []}, {"text": "As indicated , this loss can be viewed as the sum of the margin - rescaled hinge and perceptron losses for SPENs .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "Two different inference problems are represented .", "entities": []}, {"text": "The margin - rescaled hinge loss contains cost - augmented inference , shown as part of Eq .", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "( 3 ) .", "entities": []}, {"text": "The perceptron loss contains the test - time inference problem , which is shown in Eq .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "( 1 ) .", "entities": []}, {"text": "Tuand Gimpel ( 2018 ) used a single inference network for solving both problems , so it was trained as a cost - augmented inference network during training and then \ufb01ne - tuned as a test - time inference network afterward .", "entities": []}, {"text": "We avoid this issue by training two inference networks , A \t for test - time inference and F\bfor cost - augmented inference : min \u0002max \b ; \t X hxi;yii2D", "entities": []}, {"text": "[ 4(F\b(xi);yi)\u0000E\u0002(xi;F\b(xi))+E\u0002(xi;yi)]+ + \u0015[\u0000E\u0002(xi;A \t ( xi))+E\u0002(xi;yi)]+ ( 6 ) We treat this optimization problem as a minimax game and \ufb01nd a saddle point for the game similar to Tu and Gimpel ( 2018 ) and Goodfellow et", "entities": []}, {"text": "al . ( 2014 ) .", "entities": []}, {"text": "We use minibatch stochastic gradient descent and alternately optimize \u0002,\b , and \t .", "entities": [[3, 6, "MethodName", "stochastic gradient descent"]]}, {"text": "The objective for the energy parameters \u0002in minibatchMis : ^\u0002 arg min \u0002X hxi;yii2M\u0002", "entities": []}, {"text": "4(F\b(xi);yi)\u0000E\u0002(xi;F\b(xi))+E\u0002(xi;yi)\u0003 + + \u0015\u0002 \u0000E\u0002(xi;A \t ( xi))+E\u0002(xi;yi)\u0003 +", "entities": []}, {"text": "When we remove 0 - truncation ( see Sec .", "entities": [[3, 4, "DatasetName", "0"]]}, {"text": "4.1 for the motivation ) , the objective for the inference network parameters in minibatch Mis : ^ \t ; ^\b arg max \t ; \bX hxi;yii2M4(F\b(xi);yi)\u0000 E\u0002(xi;F\b(xi))\u0000\u0015E\u0002(xi;A \t ( xi ) )", "entities": []}, {"text": "3.1 Joint Parameterizations If we were to train independent inference networks A \t andF\b , this new objective could be much slower than the approach of Tu and Gimpel ( 2018 ) .", "entities": []}, {"text": "However , the compound objective offers several natural options for de\ufb01ning joint parameterizations of the two inference networks .", "entities": []}, {"text": "We consider three options which are visualized in Figure 1 and described below : \u2022separated : F\bandA \t are two independent networks with their own architectures and parameters as shown in Figure 1(a ) .", "entities": []}, {"text": "\u2022shared : F\bandA \t share a \u201c feature \u201d network as shown in Figure 1(b ) .", "entities": []}, {"text": "We consider this option because both F\bandA \t are trained to produce output labels with low energy .", "entities": []}, {"text": "However F\balso needs to produce output labels with high cost 4 ( i.e. , far from the gold standard ) .", "entities": []}, {"text": "65 Figure 1 : Joint parameterizations for cost - augmented inference network F\band test - time inference network A \t . \u2022stacked : the cost - augmented network F\bis a function of the output of the test - time network A \t and the gold standard output y. That is , F\b(x;y ) = q(A \t ( x);y)whereqis a parameterized function .", "entities": []}, {"text": "This is depicted in Figure 1(c ) .", "entities": []}, {"text": "We block the gradient at A \t when updating \b. For theqfunction in the stacked option , we use an af\ufb01ne transformation on the concatenation of the inference network label distribution and the gold standard one - hot vector .", "entities": []}, {"text": "That is , denoting the vector at position tof the cost - augmented network output by F\b(x;y)t , we have : F\b(x;y)t= softmax ( Wq[A \t ( x)t;y(t ) ]", "entities": [[22, 23, "MethodName", "softmax"]]}, {"text": "+ bq ) where semicolon ( ;) is vertical concatenation , y(t ) ( positiontofy ) is anL - dimensional one - hot vector , A \t ( x)tis the vector at position tofA \t ( x),Wq is anL\u00022Lmatrix , andbqis a bias .", "entities": []}, {"text": "One motivation for these parameterizations is to reduce the total number of parameters in the procedure .", "entities": [[10, 13, "HyperparameterName", "number of parameters"]]}, {"text": "Generally , the number of parameters is expected to decrease when moving from separated to shared to stacked .", "entities": [[3, 6, "HyperparameterName", "number of parameters"]]}, {"text": "We will compare the three options empirically in our experiments , in terms of both accuracy and number of parameters .", "entities": [[15, 16, "MetricName", "accuracy"], [17, 20, "HyperparameterName", "number of parameters"]]}, {"text": "Another motivation , speci\ufb01cally for the third option , is to distinguish the two inference networks in terms of their learned functionality .", "entities": []}, {"text": "With all three parameterizations , the cost - augmented network will be trained to produce an output that differs from the gold standard , due to the presence of the4(\u0001)term in the combined objective .", "entities": []}, {"text": "However , Tu and Gimpel ( 2018 ) found that the trained cost - augmented network was barely affected by \ufb01ne - tuning for the test - time inference objective .", "entities": []}, {"text": "This suggests that the cost - augmented network wasmostly acting as a test - time inference network by the time of convergence .", "entities": []}, {"text": "With the stacked parameterization , however , we explicitly provide the gold standard yto the cost - augmented network , permitting it to learn to change the predictions of the test - time network in appropriate ways to improve the energy function .", "entities": []}, {"text": "4 Training Stability and Effectiveness We now discuss several methods that simplify and stabilize training SPENs with inference networks .", "entities": []}, {"text": "When describing them , we will illustrate their impact by showing training trajectories for the Twitter part - of - speech tagging task .", "entities": [[16, 22, "TaskName", "part - of - speech tagging"]]}, {"text": "4.1 Removing Zero Truncation Tu and Gimpel ( 2018 ) used the following objective for the cost - augmented inference network ( maximizing it with respect to \b):l0=", "entities": []}, {"text": "[ 4(F\b(x);y)\u0000E\u0002(x;F\b(x ) )", "entities": []}, {"text": "+ E\u0002(x;y)]+ where [ h]+= max(0;h ) .", "entities": []}, {"text": "However , there are two potential reasons why l0will equal zero and trigger no gradient update .", "entities": []}, {"text": "First , E\u0002(the energy function , corresponding to the discriminator in a GAN ) may already be well - trained , and it can easily separate the gold standard output from the costaugmented inference network output .", "entities": [[12, 13, "MethodName", "GAN"]]}, {"text": "Second , the cost - augmented inference network ( corresponding to the generator in a GAN ) could be so poorly trained that the energy of its output is very large , leading the margin constraints to be satis\ufb01ed and l0to be zero .", "entities": [[15, 16, "MethodName", "GAN"]]}, {"text": "In standard margin - rescaled max - margin learning in structured prediction ( Taskar et al . , 2004 ; Tsochantaridis et al . , 2004 ) , the cost - augmented inference step is performed exactly ( or approximately", "entities": [[10, 12, "TaskName", "structured prediction"]]}, {"text": "66 Epochs0 50 100Accuracy(% ) 0102030405060708090 with truncation with truncation with truncation without truncation without truncation without truncation(a )", "entities": []}, {"text": "Truncating at 0 ( without CE ) .", "entities": [[2, 3, "DatasetName", "0"]]}, {"text": "Epochs0 50 100Accuracy(% ) 0102030405060708090 without CE without CE without CE with CE with CE with CE ( b )", "entities": []}, {"text": "Adding CE loss ( without truncation ) .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "Figure 2 : Part - of - speech tagging training trajectories .", "entities": [[3, 9, "TaskName", "Part - of - speech tagging"]]}, {"text": "The three curves in each setting correspond to different random seeds .", "entities": [[10, 11, "DatasetName", "seeds"]]}, {"text": "( a ) Without the local CE loss , training fails when using zero truncation .", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "( b ) The CE loss reduces the number of epochs for training .", "entities": [[5, 6, "MetricName", "loss"], [8, 11, "HyperparameterName", "number of epochs"]]}, {"text": "Tu and Gimpel ( 2018 ) used zero truncation and CE during training .", "entities": []}, {"text": "with reasonable guarantee of effectiveness )", "entities": []}, {"text": ", ensuring that when l0is zero , the energy parameters are well trained .", "entities": []}, {"text": "However , in our case , l0may be zero simply because the cost - augmented inference network is undertrained , which will be the case early in training .", "entities": []}, {"text": "Then , when using zero truncation , the gradient of the inference network parameters will be 0 .", "entities": [[16, 17, "DatasetName", "0"]]}, {"text": "This is likely why Tu and Gimpel ( 2018 ) found it important to add several stabilization terms to thel0objective .", "entities": []}, {"text": "We \ufb01nd that by instead removing the truncation , learning stabilizes and becomes less dependent on these additional terms .", "entities": []}, {"text": "Note that we retain the truncation at zero when updating the energy parameters \u0002. As shown in Figure 2(a ) , without any stabilization terms and with truncation , the inference network will barely move from its starting point and learning fails overall .", "entities": []}, {"text": "However , without truncation , the inference network can work well even without any stabilization terms .", "entities": []}, {"text": "4.2 Local Cross Entropy ( CE ) Loss Tu and Gimpel ( 2018 ) proposed adding a local cross entropy ( CE ) loss , which is the sum of the label cross entropy losses over all positions in the sequence , to stabilize inference network training .", "entities": [[23, 24, "MetricName", "loss"]]}, {"text": "We similarly \ufb01nd this term to help speed up convergence and improve accuracy .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "Figure 2(b ) shows faster convergence to high accuracy when adding the local CE term .", "entities": [[8, 9, "MetricName", "accuracy"]]}, {"text": "See Section 7 for more details.4.3 Multiple Inference Network Update Steps When training SPENs with inference networks , the inference network parameters are nested within the energy function .", "entities": []}, {"text": "We found that the gradient components of the inference network parameters consequently have smaller absolute values than those of the energy function parameters .", "entities": []}, {"text": "So , we alternate betweenk\u00151steps of optimizing the inference network parameters ( \u201c I steps \u201d ) and one step of optimizing the energy function parameters ( \u201c E steps \u201d ) .", "entities": []}, {"text": "We \ufb01nd this strategy especially helpful when using complex inference network architectures .", "entities": []}, {"text": "To analyze , we compute the cost - augmented lossl1=4(F\b(x);y)\u0000E\u0002(x;F\b(x))and the margin - rescaled hinge loss l0=", "entities": [[15, 16, "MetricName", "loss"]]}, {"text": "[ 4(F\b(x);y)\u0000 E\u0002(x;F\b(x ) )", "entities": []}, {"text": "+ E\u0002(x;y)]+averaged over all training pairs ( x;y)after each set of I steps .", "entities": []}, {"text": "The I steps update \t and\bto maximize these losses .", "entities": []}, {"text": "Meanwhile the E steps update \u0002to minimize these losses .", "entities": []}, {"text": "Figs . 3(a ) and ( b ) show l1andl0during training for different numbers ( k ) of I steps for every one E step .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "3(c ) shows the norm of the energy parameters after the E steps , and Fig .", "entities": []}, {"text": "3(d ) shows the norm of@E\u0002(x;A \t ) @ \t after the I steps .", "entities": []}, {"text": "Withk= 1 , the setting used by Tu and Gimpel ( 2018 ) , the inference network lags behind the energy , making the energy parameter updates very small , as shown by the small norms in Fig . 3(c ) .", "entities": []}, {"text": "The inference network gradient norm ( Fig . 3(d ) ) remains high , indicating under\ufb01tting .", "entities": []}, {"text": "However , increasingktoo much also harms learning , as evi-", "entities": []}, {"text": "67 Epochs0 5 10 15 20cost - augmented loss after I steps 253035404550", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "1 1 1 5 5 5 10 10 10 50 50 50(a ) cost - augmented loss l1 Epochs0 5 10 15 20loss", "entities": [[16, 17, "MetricName", "loss"]]}, {"text": "after I steps 051015202530 1 1 1 5 5 5 10 10 10 50 50 50 ( b ) margin - rescaled loss l0 Epochs0 5 10 15 20gradient norm after E steps 0246810121416 1 1 1 5 5 5 10 10 10 50 50 50 ( c ) gradient norm of \u0002 Epochs0 5 10 15 20gradient norm after I steps 024681012 1 1 1 5 5 5 10 10 10 50 50 50 ( d ) gradient norm of \t  Figure 3 : POS training trajectories with different numbers of I steps .", "entities": [[22, 23, "MetricName", "loss"]]}, {"text": "The three curves in each setting correspond to different random seeds .", "entities": [[10, 11, "DatasetName", "seeds"]]}, {"text": "( a ) cost - augmented loss after I steps ; ( b ) margin - rescaled hinge loss after I steps ; ( c ) gradient norm of energy function parameters after E steps ; ( d ) gradient norm of test - time inference network parameters after I steps .", "entities": [[6, 7, "MetricName", "loss"], [18, 19, "MetricName", "loss"]]}, {"text": "denced by the \u201c plateau \u201d effect in the l1curves for k= 50 ; this indicates that the energy function is lagging behind the inference network .", "entities": []}, {"text": "Using k= 5 leads to more of a balance between l1andl0and gradient norms that are mostly decreasing during training .", "entities": []}, {"text": "We treat kas a hyperparameter that is tuned in our experiments .", "entities": []}, {"text": "There is a potential connection between our use of multiple I steps and a similar procedure used in GANs ( Goodfellow et", "entities": []}, {"text": "al . , 2014 ) .", "entities": []}, {"text": "In the GAN objective , the discriminator Dis updated in the inner loop , and they alternate between multiple update steps for Dand one update step for G.", "entities": [[2, 3, "MethodName", "GAN"]]}, {"text": "In this section , we similarly found bene\ufb01t from multiple steps of inner loop optimization for every step of the outer loop .", "entities": []}, {"text": "However , the analogy is limited , since GAN training involves sampling noise vectors and using them to generate data , while there are no noise vectors or explicitly - generated samples in our framework.5 Energies for Sequence Labeling For our sequence labeling experiments in this paper , the input xis a length - Tsequence of tokens , and the output yis a sequence of labels of length T. We use ytto denote the output label at position t , where ytis a vector of length L(the number of labels in the label set ) and where yt;jis thejth entry of the vector yt .", "entities": [[8, 9, "MethodName", "GAN"]]}, {"text": "In the original output space Y(x ) , yt;jis 1 for a single jand 0 for all others .", "entities": [[14, 15, "DatasetName", "0"]]}, {"text": "In the relaxed output space YR(x),yt;jcan be interpreted as the probability of the tth position being labeled with labelj .", "entities": []}, {"text": "We then use the following energy for sequence labeling ( Tu and Gimpel , 2018 ): E\u0002(x;y )", "entities": []}, {"text": "= \u0000 TX t=1LX j=1yt;j\u0010 U > jb(x;t)\u0011 + TX t=1y >", "entities": []}, {"text": "t\u00001Wyt !", "entities": []}, {"text": "( 7 )", "entities": []}, {"text": "68whereUj2Rdis a parameter vector for label jand the parameter matrix W2RL\u0002Lcontains label - pair parameters .", "entities": []}, {"text": "Also , b(x;t)2Rddenotes the \u201c input feature vector \u201d for position t. We de\ufb01ne bto be thed - dimensional BiLSTM ( Hochreiter and Schmidhuber , 1997 ) hidden vector at t.", "entities": [[19, 20, "MethodName", "BiLSTM"]]}, {"text": "The full set of energy parameters \u0002includes the Ujvectors , W , and the parameters of the BiLSTM .", "entities": [[17, 18, "MethodName", "BiLSTM"]]}, {"text": "Global Energies for Sequence Labeling .", "entities": []}, {"text": "In addition to new training strategies , we also experiment with several global energy terms for sequence labeling .", "entities": []}, {"text": "Eq .", "entities": []}, {"text": "( 7 ) shows the base energy , and to capture long - distance dependencies , we include global energy ( GE ) terms in the form of Eq .", "entities": []}, {"text": "( 8) .", "entities": []}, {"text": "We usehto denote an LSTM tag language model ( TLM ) that takes a sequence of labels as input and returns a distribution over next labels .", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "We de\ufb01ne yt = h(y0;:::;yt\u00001)to be the distribution given the preceding label vectors ( under a LSTM language model ) .", "entities": [[16, 17, "MethodName", "LSTM"]]}, {"text": "Then , the energy term is : ETLM(y ) = \u0000T+1X t=1log\u0010", "entities": []}, {"text": "y > tyt\u0011 ( 8) where y0is the start - of - sequence symbol", "entities": []}, {"text": "and yT+1 is the end - of - sequence symbol .", "entities": []}, {"text": "This energy returns the negative log - likelihood under the TLM of the candidate output y. Tu and Gimpel ( 2018 ) pretrained their hon a large , automatically - tagged corpus and \ufb01xed its parameters when optimizing \u0002.", "entities": [[5, 8, "MetricName", "log - likelihood"]]}, {"text": "Our approach has one critical difference .", "entities": []}, {"text": "We instead do not pretrainh , and its parameters are learned when optimizing \u0002.", "entities": []}, {"text": "We show that even without pretraining , our global energy terms are still able to capture useful additional information .", "entities": []}, {"text": "We also propose new global energy terms .", "entities": []}, {"text": "De\ufb01neyt = h(y0;:::;yt\u00001)wherehis an LSTM TLM that takes a sequence of labels as input and returns a distribution over next labels .", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "First , we add a TLM in the backward direction ( denoted y0 tanalogously to the forward TLM ) .", "entities": []}, {"text": "Second , we include words as additional inputs to forward and backward TLMs .", "entities": []}, {"text": "We de\ufb01ne eyt= g(x0;:::;xt\u00001;y0;:::;yt\u00001)wheregis a forward LSTM TLM .", "entities": [[6, 7, "MethodName", "LSTM"]]}, {"text": "We de\ufb01ne the backward version similarly ( denotedey0 t ) .", "entities": []}, {"text": "The global energy is therefore EGE(y )", "entities": []}, {"text": "= \u0000T+1X t=1log(y > tyt )", "entities": []}, {"text": "+ log ( y > ty0 t )", "entities": []}, {"text": "+", "entities": []}, {"text": "\u0000 log(y > teyt )", "entities": []}, {"text": "+ log ( y > tey0 t)\u0001 ( 9)Here", "entities": []}, {"text": "is a hyperparameter that is tuned .", "entities": []}, {"text": "We experiment with three settings for the global energy : GE(a ): forward TLM as in Tu and Gimpel ( 2018 ) ; GE(b ): forward and backward TLMs (", "entities": []}, {"text": "= 0 ) ; GE(c ): all four TLMs in Eq .", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "( 9 ) .", "entities": []}, {"text": "6 Experimental Setup We consider two sequence labeling tasks :", "entities": []}, {"text": "Twitter part - of - speech ( POS ) tagging ( Gimpel et al . , 2011 ) and named entity recognition ( NER ; Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[1, 4, "DatasetName", "part - of"], [19, 22, "TaskName", "named entity recognition"], [23, 24, "TaskName", "NER"]]}, {"text": "Twitter Part - of - Speech ( POS ) Tagging .", "entities": [[1, 4, "DatasetName", "Part - of"]]}, {"text": "We use the Twitter POS data from Gimpel et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2011 ) and Owoputi et al .", "entities": []}, {"text": "( 2013 ) which contain 25 tags .", "entities": []}, {"text": "We use 100 - dimensional skip - gram ( Mikolov et al . , 2013 ) embeddings from Tu et al . ( 2017 ) .", "entities": []}, {"text": "Like Tu and Gimpel ( 2018 ) , we use a BiLSTM to compute the input feature vector for each position , using hidden size 100 .", "entities": [[11, 12, "MethodName", "BiLSTM"]]}, {"text": "We also use BiLSTMs for the inference networks .", "entities": []}, {"text": "The output of the inference network is a softmax function , so the inference network will produce a distribution over labels at each position .", "entities": [[8, 9, "MethodName", "softmax"]]}, {"text": "The \u0001is L1 distance .", "entities": []}, {"text": "We train the inference network using stochastic gradient descent ( SGD ) with momentum and train the energy parameters using Adam ( Kingma and Ba , 2014 ) .", "entities": [[6, 9, "MethodName", "stochastic gradient descent"], [10, 11, "MethodName", "SGD"], [20, 21, "MethodName", "Adam"]]}, {"text": "We also explore training the inference network using Adam when not using the local CE loss.1In experiments with the local CE term , its weight is set to 1 .", "entities": [[8, 9, "MethodName", "Adam"]]}, {"text": "Named Entity Recognition ( NER ) .", "entities": [[0, 3, "TaskName", "Named Entity Recognition"], [4, 5, "TaskName", "NER"]]}, {"text": "We use the CoNLL 2003 English dataset ( Tjong Kim Sang and De Meulder , 2003 ) .", "entities": [[3, 5, "DatasetName", "CoNLL 2003"]]}, {"text": "We use the BIOES tagging scheme , following previous work ( Ratinov and Roth , 2009 ) , resulting in 17 NER labels .", "entities": [[21, 22, "TaskName", "NER"]]}, {"text": "We use 100 - dimensional pretrained GloVe embeddings ( Pennington et al . , 2014 ) .", "entities": [[6, 8, "MethodName", "GloVe embeddings"]]}, {"text": "The task is evaluated using F1 score computed with the conlleval script .", "entities": [[5, 7, "MetricName", "F1 score"]]}, {"text": "The architectures for the feature networks in the energy function and inference networks are all BiLSTMs .", "entities": []}, {"text": "The architectures for tag language models are LSTMs .", "entities": []}, {"text": "We use a dropout keep - prob of 0.7 for all LSTM cells .", "entities": [[11, 12, "MethodName", "LSTM"]]}, {"text": "The hidden size for all LSTMs is 128 .", "entities": []}, {"text": "We use Adam ( Kingma and Ba , 2014 ) and do early stopping on the development set .", "entities": [[2, 3, "MethodName", "Adam"], [12, 14, "MethodName", "early stopping"]]}, {"text": "We use a learning rate of 5\u000110\u00004 .", "entities": [[3, 5, "HyperparameterName", "learning rate"]]}, {"text": "Similar to above , the weight for the CE term is set to 1 .", "entities": []}, {"text": "We consider three NER modeling con\ufb01gurations .", "entities": [[3, 4, "TaskName", "NER"]]}, {"text": "NER uses only words as input and pretrained , \ufb01xed 1We \ufb01nd that Adam works better than SGD when training the inference network without the local cross entropy term .", "entities": [[0, 1, "TaskName", "NER"], [13, 14, "MethodName", "Adam"], [17, 18, "MethodName", "SGD"]]}, {"text": "69zero POS NER NER+ loss trunc .", "entities": [[2, 3, "TaskName", "NER"], [4, 5, "MetricName", "loss"]]}, {"text": "CE acc ( % ) F1 ( % ) F1 ( % )", "entities": [[1, 2, "MetricName", "acc"], [5, 6, "MetricName", "F1"], [9, 10, "MetricName", "F1"]]}, {"text": "yes no 13.9 3.91 3.91 margin-", "entities": []}, {"text": "no no 87.9 85.1 88.6 rescaled yes yes 89.4 * 85.2 * 89.5 *", "entities": []}, {"text": "no yes 89.4 85.2 89.5 perceptronno no 88.2 84.0 88.1", "entities": []}, {"text": "no", "entities": []}, {"text": "yes 88.6 84.7 89.0 Table 1 : Test results for POS and NER for several SPEN con\ufb01gurations .", "entities": [[12, 13, "TaskName", "NER"]]}, {"text": "Results with * correspond to the setting of Tu and Gimpel ( 2018 ) .", "entities": []}, {"text": "The inference network architecture is a one - layer BiLSTM .", "entities": [[9, 10, "MethodName", "BiLSTM"]]}, {"text": "GloVe embeddings .", "entities": [[0, 2, "MethodName", "GloVe embeddings"]]}, {"text": "NER+ uses words , the case of the \ufb01rst letter , POS tags , and chunk labels , as well as pretrained GloVe embeddings with \ufb01ne - tuning .", "entities": [[22, 24, "MethodName", "GloVe embeddings"]]}, {"text": "NER++ includes everything in NER+ as well as character - based word representations obtained using a convolutional network over the character sequence in each word .", "entities": []}, {"text": "Unless otherwise indicated , our SPENs use the energy in Eq .", "entities": []}, {"text": "( 7 ) .", "entities": []}, {"text": "7 Results and Analysis Effect of Zero Truncation and Local CE Loss .", "entities": []}, {"text": "Table 1 shows results for zero truncation and the local CE term .", "entities": []}, {"text": "Training fails for both tasks when using zero truncation without CE .", "entities": []}, {"text": "Removing truncation makes learning succeed and leads to effective models even without using CE .", "entities": []}, {"text": "However , when using the local CE term , truncation has little effect on performance .", "entities": []}, {"text": "The importance of CE in prior work ( Tu and Gimpel , 2018 ) is likely due to the fact that truncation was being used .", "entities": []}, {"text": "The local CE term is useful for both tasks , though it appears more helpful for tagging.2This may be because POS tagging is a more local task .", "entities": []}, {"text": "Regardless , for both tasks , as shown in Section 4.2 , the inclusion of the CE term speeds convergence and improves training stability .", "entities": []}, {"text": "For example , on NER , using the CE term reduces the number of epochs chosen by early stopping from \u0018100 to \u001825 .", "entities": [[4, 5, "TaskName", "NER"], [12, 15, "HyperparameterName", "number of epochs"], [17, 19, "MethodName", "early stopping"]]}, {"text": "For POS , using the CE term reduces the number of epochs from \u0018150 to\u001860 .", "entities": [[9, 12, "HyperparameterName", "number of epochs"]]}, {"text": "Effect of Compound Objective and Joint Parameterizations .", "entities": []}, {"text": "The compound objective is the sum of the margin - rescaled and perceptron losses , and outperforms them both ( see Table 2 ) .", "entities": []}, {"text": "Across all tasks , the shared and stacked parameterizations are more accurate than the previous objectives .", "entities": []}, {"text": "For the separated parameterization , the performance 2We found the local CE term to be useful for both the costaugmented and test - time inference networks during training.drops slightly for NER , likely due to the larger number of parameters .", "entities": [[30, 31, "TaskName", "NER"], [37, 40, "HyperparameterName", "number of parameters"]]}, {"text": "The shared and stacked options have fewer parameters to train than the separated option , and the stacked version processes examples at the fastest rate during training .", "entities": []}, {"text": "The top part of Table 3 shows how the performance of the test - time inference network A \t and the cost - augmented inference network F\bvary when using the new compound objective .", "entities": []}, {"text": "The differences between F\bandA \t are larger than in the baseline con\ufb01guration , showing that the two are learning complementary functionality .", "entities": []}, {"text": "With the stacked parameterization , the cost - augmented networkF\breceives as an additional input the gold standard label sequence , which leads to the largest differences as the cost - augmented network can explicitly favor incorrect labels.3 The bottom part of Table 3 shows qualitative differences between the two inference networks .", "entities": []}, {"text": "On the POS development set , we count the differences between the predictions of A \t andF\bwhen", "entities": []}, {"text": "A \t makes the correct prediction.4F\btends to output tags that are highly confusable with those output by A \t .", "entities": []}, {"text": "For example , it often outputs proper noun when the gold standard is common noun or vice versa .", "entities": []}, {"text": "It also captures the ambiguities among adverbs , adjectives , and prepositions .", "entities": []}, {"text": "Global Energies .", "entities": []}, {"text": "The results are shown in Table 4 .", "entities": []}, {"text": "Adding the backward ( b ) and word - augmented TLMs ( c ) improves over using only the forward TLM from Tu and Gimpel ( 2018 ) .", "entities": []}, {"text": "With the global energies , our performance is comparable to several strong results ( 90.94 of Lample et al . , 2016 and 91.37 of Ma and Hovy , 2016 ) .", "entities": []}, {"text": "However , it is still lower than the state of the art ( Akbik et al . , 2018 ; Devlin et", "entities": []}, {"text": "al . , 2019 ) , likely due to the lack of contextualized embeddings .", "entities": []}, {"text": "In other work , we proposed and evaluated several other high - order energy terms for sequence labeling using our framework ( Tu et al . , 2020a ) .", "entities": []}, {"text": "8 Related Work There are several efforts aimed at stabilizing and improving learning in generative adversarial networks ( GANs ) ( Goodfellow et", "entities": []}, {"text": "al . , 2014 ; Salimans et al . , 2016 ; Zhao et", "entities": []}, {"text": "al . , 2017 ; Arjovsky et al . , 2017 ) .", "entities": []}, {"text": "Progress in training GANs has come largely 3We also tried a BiLSTM in the \ufb01nal layer of the stacked parameterization but results were similar to the simpler af\ufb01ne architecture , so we only report results for the latter .", "entities": [[11, 12, "MethodName", "BiLSTM"]]}, {"text": "4We used the stacked parameterization .", "entities": []}, {"text": "70POS NER NER+ acc .", "entities": [[1, 2, "TaskName", "NER"], [3, 4, "MetricName", "acc"]]}, {"text": "( % ) jTj j Ijspeed F1 ( % ) jTj j Ijspeed F1 ( % ) BiLSTM 88.8 166 K 166 K \u2013 84.9 239 K 239 K \u2013 89.3 SPENs with inference networks ( Tu and Gimpel , 2018 ): margin - rescaled 89.4 333 K 166 K \u2013 85.2 479 K 239 K \u2013 89.5 perceptron 88.6 333 K 166 K \u2013 84.4 479 K 239 K \u2013 89.0 SPENs with inference networks , compound objective , CE , no zero truncation ( this paper ): separated 89.7 500 K 166 K 66 85.0 719 K 239 K 32 89.8 shared 89.8 339 K 166 K 78 85.6 485 K 239 K 38 90.1 stacked 89.8 335 K 166 K 92 85.6 481 K 239 K 46 90.1 Table 2 : Test results for POS and NER .", "entities": [[6, 7, "MetricName", "F1"], [13, 14, "MetricName", "F1"], [17, 18, "MethodName", "BiLSTM"], [139, 140, "TaskName", "NER"]]}, {"text": "jTjis the number of trained parameters ; jIjis the number of parameters needed during inference .", "entities": [[9, 12, "HyperparameterName", "number of parameters"]]}, {"text": "Training speeds ( examples / second ) are shown for joint parameterizations to compare them in terms of ef\ufb01ciency .", "entities": []}, {"text": "Best setting ( best performance with fewest parameters and fastest training ) is in bold .", "entities": []}, {"text": "POS NER A \t \u0000F\bA \t \u0000F\b margin - rescaled 0.2 0 separated 2.2 0.4 compound shared 1.9 0.5 stacked 2.6 1.7 test - time ( A \t ) cost - augmented ( F\b ) common noun proper noun proper noun common noun common noun adjective proper noun proper noun + possessive adverb adjective preposition adverb adverb preposition verb common noun adjective verb Table 3 : Top : differences in accuracy / F1 between test - time inference networks", "entities": [[1, 2, "TaskName", "NER"], [11, 12, "DatasetName", "0"], [70, 71, "MetricName", "accuracy"], [72, 73, "MetricName", "F1"]]}, {"text": "A \t and cost - augmented networks F\b(on development sets ) .", "entities": []}, {"text": "The \u201c marginrescaled \u201d row uses a SPEN with the local CE term and without zero truncation , where A \t is obtained by \ufb01netuningF\bas done by Tu and Gimpel ( 2018 ) .", "entities": []}, {"text": "Bottom : most frequent output differences between A \t andF\b on the development set .", "entities": []}, {"text": "NER NER+ NER++ margin - rescaled 85.2 89.5 90.2 compound , stacked , CE , no truncation85.6 90.1 90.8 + global energy GE(a )", "entities": [[0, 1, "TaskName", "NER"]]}, {"text": "85.8 90.2 90.7 + global energy GE(b ) 85.9 90.2 90.8 + global energy GE(c ) 86.3 90.4 91.0 Table 4 : NER test F1 scores with global energy terms .", "entities": [[22, 23, "TaskName", "NER"], [24, 25, "MetricName", "F1"]]}, {"text": "from overcoming learning dif\ufb01culties by modifying loss functions and optimization , and GANs have become more successful and popular as a result .", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "Notably , Wasserstein GANs ( Arjovsky et al . , 2017 ) provided the \ufb01rst convergence measure in GAN training using Wasserstein distance .", "entities": [[18, 19, "MethodName", "GAN"]]}, {"text": "To compute Wasserstein distance , the discriminator uses weight clipping , which limits network capacity .", "entities": []}, {"text": "Weightclipping was subsequently replaced with a gradient norm constraint ( Gulrajani et al . , 2017 ) .", "entities": []}, {"text": "Miyato et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) proposed a novel weight normalization technique called spectral normalization .", "entities": [[6, 8, "MethodName", "weight normalization"], [10, 12, "MethodName", "spectral normalization"]]}, {"text": "These methods may be applicable to the similar optimization problems solved in learning SPENs .", "entities": []}, {"text": "Another direction may be to explore alternative training objectives for SPENs , such as those that use weaker supervision than complete structures ( Rooshenas et al . , 2018 , 2019 ; Naskar et al . , 2020 ) .", "entities": []}, {"text": "9 Conclusions and Future Work", "entities": []}, {"text": "We contributed several strategies to stabilize and improve joint training of SPENs and inference networks .", "entities": []}, {"text": "Our use of joint parameterizations mitigates the need for inference network \ufb01ne - tuning , leads to complementarity in the learned inference networks , and yields improved performance overall .", "entities": []}, {"text": "These developments offer promise for SPENs to be more easily applied to a broad range of NLP tasks .", "entities": []}, {"text": "Future work will explore other structured prediction tasks , such as parsing and generation .", "entities": [[5, 7, "TaskName", "structured prediction"]]}, {"text": "We have taken initial steps in this direction , considering constituency parsing with the sequence - to - sequence model of Tran et al .", "entities": [[10, 12, "TaskName", "constituency parsing"]]}, {"text": "( 2018 ) .", "entities": []}, {"text": "Preliminary experiments are positive,5but signi\ufb01cant challenges remain , speci\ufb01cally in de\ufb01ning appropriate inference network architectures to enable ef\ufb01cient learning .", "entities": []}, {"text": "Acknowledgments We would like to thank the reviewers for insightful comments .", "entities": []}, {"text": "This research was supported in part by an Amazon Research Award to K. Gimpel .", "entities": []}, {"text": "5On NXT Switchboard ( Calhoun et al . , 2010 ) , the baseline achieves 82.80 F1 on the development set and the SPEN ( stacked parameterization ) achieves 83.22 .", "entities": [[16, 17, "MetricName", "F1"]]}, {"text": "More details are in the appendix .", "entities": []}, {"text": "71References Alan Akbik , Duncan Blythe , and Roland V ollgraf .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Contextual string embeddings for sequence labeling .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 1638\u20131649 , Santa Fe , New Mexico , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Mart \u00b4 \u0131n Arjovsky , Soumith Chintala , and L \u00b4 eon Bottou . 2017 .", "entities": []}, {"text": "Wasserstein generative adversarial networks .", "entities": []}, {"text": "InProceedings of the 34th International Conference on Machine Learning .", "entities": []}, {"text": "David Belanger and Andrew McCallum .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Structured prediction energy networks .", "entities": [[0, 2, "TaskName", "Structured prediction"]]}, {"text": "In Proceedings of the 33rd International Conference on Machine Learning .", "entities": []}, {"text": "David Belanger , Bishan Yang , and Andrew McCallum . 2017 .", "entities": []}, {"text": "End - to - end learning for structured prediction energy networks .", "entities": [[7, 9, "TaskName", "structured prediction"]]}, {"text": "In Proceedings of the 34th International Conference on Machine Learning .", "entities": []}, {"text": "Sasha Calhoun , Jean Carletta , Jason M Brenier , Neil Mayo , Dan Jurafsky , Mark Steedman , and David Beaver .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "The NXT - format Switchboard Corpus : a rich resource for investigating the syntax , semantics , pragmatics and prosody of dialogue .", "entities": [[4, 6, "DatasetName", "Switchboard Corpus"]]}, {"text": "Language resources and evaluation , 44(4):387\u2013419 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kevin Gimpel , Nathan Schneider , Brendan O\u2019Connor , Dipanjan Das , Daniel Mills , Jacob Eisenstein , Michael Heilman , Dani Yogatama , Jeffrey Flanigan , and Noah A. Smith .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Part - of - speech tagging for twitter : Annotation , features , and experiments .", "entities": [[0, 6, "TaskName", "Part - of - speech tagging"]]}, {"text": "InProceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , pages 42\u201347 , Portland , Oregon , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Generative adversarial nets .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 2672\u20132680 .", "entities": []}, {"text": "Ishaan Gulrajani , Faruk Ahmed , Martin Arjovsky , Vincent Dumoulin , and Aaron C Courville . 2017 .", "entities": []}, {"text": "Improved training of Wasserstein GANs .", "entities": []}, {"text": "In I. Guyon , U. V .", "entities": []}, {"text": "Luxburg , S. Bengio , H. Wallach , R. Fergus , S. Vishwanathan , and R. Garnett , editors , Advances in Neural Information Processing Systems 30 , pages 5767\u20135777.Sepp Hochreiter and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural Computation .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Guillaume Lample , Miguel Ballesteros , Sandeep Subramanian , Kazuya Kawakami , and Chris Dyer . 2016 .", "entities": []}, {"text": "Neural architectures for named entity recognition .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}, {"text": "InProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 260\u2013270 , San Diego , California .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yann LeCun , Sumit Chopra , Raia Hadsell , Marc\u2019Aurelio Ranzato , and Fu - Jie Huang .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "A tutorial on energy - based learning .", "entities": []}, {"text": "In Predicting Structured Data .", "entities": []}, {"text": "MIT Press .", "entities": []}, {"text": "Xuezhe Ma and Eduard Hovy . 2016 .", "entities": []}, {"text": "End - to - end sequence labeling via bi - directional LSTM - CNNsCRF .", "entities": [[11, 12, "MethodName", "LSTM"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1064\u20131074 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , and Jeff Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 3111\u20133119 .", "entities": []}, {"text": "Takeru Miyato , Toshiki Kataoka , Masanori Koyama , and Yuichi Yoshida .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Spectral normalization for generative adversarial networks .", "entities": [[0, 2, "MethodName", "Spectral normalization"]]}, {"text": "In Proceedings of International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Subhajit Naskar , Amirmohammad Rooshenas , Simeng Sun , Mohit Iyyer , and Andrew McCallum .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Energy - based reranking : Improving neural machine translation using energy - based models .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:2009.13267 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Olutobi Owoputi , Brendan O\u2019Connor , Chris Dyer , Kevin Gimpel , Nathan Schneider , and Noah A. Smith .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Improved part - of - speech tagging for online conversational text with word clusters .", "entities": [[1, 7, "TaskName", "part - of - speech tagging"]]}, {"text": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 380\u2013390 , Atlanta , Georgia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 , Doha , Qatar .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "72Lev Ratinov and Dan Roth . 2009 .", "entities": []}, {"text": "Design challenges and misconceptions in named entity recognition .", "entities": [[3, 4, "TaskName", "misconceptions"], [5, 8, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning ( CoNLL-2009 ) , pages 147\u2013155 .", "entities": [[12, 13, "DatasetName", "CoNLL-2009"]]}, {"text": "Amirmohammad Rooshenas , Aishwarya Kamath , and Andrew McCallum .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Training structured prediction energy networks with indirect supervision .", "entities": [[1, 3, "TaskName", "structured prediction"]]}, {"text": "InProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 130\u2013135 .", "entities": []}, {"text": "Amirmohammad Rooshenas , Dongxu Zhang , Gopal Sharma , and Andrew McCallum .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Searchguided , lightly - supervised training of structured prediction energy networks .", "entities": [[7, 9, "TaskName", "structured prediction"]]}, {"text": "In H. Wallach , H. Larochelle , A. Beygelzimer , F. d'Alch", "entities": []}, {"text": "\u00b4 e - Buc , E. Fox , and R. Garnett , editors , Advances in Neural Information Processing Systems 32 , pages 13522 \u2013 13532 .", "entities": []}, {"text": "Tim Salimans , Ian Goodfellow , Wojciech Zaremba , Vicki Cheung , Alec Radford , and Xi Chen . 2016 .", "entities": []}, {"text": "Improved techniques for training GANs .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 2234\u20132242 .", "entities": []}, {"text": "Ben Taskar , Carlos Guestrin , and Daphne Koller .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Max - margin Markov networks .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 25\u201332 .", "entities": []}, {"text": "Erik F. Tjong Kim Sang and Fien De Meulder .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Introduction to the CoNLL-2003 shared task : Language - independent named entity recognition .", "entities": [[3, 4, "DatasetName", "CoNLL-2003"], [10, 13, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the Seventh Conference on Natural Language Learning at HLT - NAACL 2003 , pages 142\u2013147 .", "entities": []}, {"text": "Trang Tran , Shubham Toshniwal , Mohit Bansal , Kevin Gimpel , Karen Livescu , and Mari Ostendorf .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Parsing speech : a neural approach to integrating lexical and acoustic - prosodic information .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 69\u201381 , New Orleans , Louisiana . Association for Computational Linguistics .", "entities": []}, {"text": "Ioannis Tsochantaridis , Thomas Hofmann , Thorsten Joachims , and Yasemin Altun .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Support vector machine learning for interdependent and structured output spaces .", "entities": [[0, 3, "MethodName", "Support vector machine"]]}, {"text": "In Proceedings of the Twenty-\ufb01rst International Conference on Machine Learning .", "entities": []}, {"text": "Lifu Tu and Kevin Gimpel .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning approximate inference networks for structured prediction .", "entities": [[5, 7, "TaskName", "structured prediction"]]}, {"text": "In Proceedings of International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "Lifu Tu and Kevin Gimpel .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Benchmarking approximate inference methods for neural structured prediction .", "entities": [[6, 8, "TaskName", "structured prediction"]]}, {"text": "In Proceedings of the 2019 Conferenceof the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3313\u20133324 , Minneapolis , Minnesota . Association for Computational Linguistics .", "entities": []}, {"text": "Lifu Tu , Kevin Gimpel , and Karen Livescu . 2017 .", "entities": []}, {"text": "Learning to embed words in context for syntactic tasks .", "entities": [[3, 6, "DatasetName", "words in context"]]}, {"text": "In Proceedings of the 2nd Workshop on Representation Learning for NLP , pages 265\u2013275 .", "entities": [[7, 9, "TaskName", "Representation Learning"]]}, {"text": "Lifu Tu , Tianyu Liu , and Kevin Gimpel . 2020a .", "entities": []}, {"text": "An exploration of arbitrary - order sequence labeling via energy - based inference networks .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "Lifu Tu , Richard Yuanzhe Pang , Sam Wiseman , and Kevin Gimpel .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "ENGINE :", "entities": []}, {"text": "Energy - based inference networks for non - autoregressive machine translation .", "entities": [[9, 11, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 2819\u20132826 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Junbo Jake Zhao , Micha \u00a8el Mathieu , and Yann LeCun . 2017 .", "entities": []}, {"text": "Energy - based generative adversarial network .", "entities": [[3, 6, "MethodName", "generative adversarial network"]]}, {"text": "In Proceedings of International Conference on Learning Representations ( ICLR ) .", "entities": []}, {"text": "A Appendices A.1 Constituency Parsing Experiments We linearize the constituency parsing outputs , similar to Tran et", "entities": [[3, 5, "TaskName", "Constituency Parsing"], [9, 11, "TaskName", "constituency parsing"]]}, {"text": "al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "We use the following equation plus global energy in the form of Eq .", "entities": []}, {"text": "( 8) as the energy function : E\u0002(x;y ) = \u0000 TX t=1LX j=1yt;j\u0010 U > jb(x;t)\u0011 + TX t=1y > t\u00001Wyt !", "entities": []}, {"text": "Here , bhas a seq2seq - with - attention architecture identical to Tran et al .", "entities": [[4, 5, "MethodName", "seq2seq"]]}, {"text": "( 2018 ) .", "entities": []}, {"text": "In particular , here is the list of implementation decisions .", "entities": []}, {"text": "\u2022We can write b = g\u000efwheref(which we call the \u201c feature network \u201d ) takes in an input sentence , passes it through the encoder , and passes the encoder output to the decoder feature layer to obtain hidden states ; gtakes in the hidden states and passes them into the rest of the layers in the decoder .", "entities": []}, {"text": "In our experiments , the cost - augmented inference network F\b , test - time inference networkA \t , andbof the energy function above share the same feature network ( de\ufb01ned as f above ) .", "entities": []}, {"text": "73\u2022The feature network ( f ) component of bis pretrained using the feed - forward local crossentropy objective .", "entities": []}, {"text": "The cost - augmented inference network F\band the test - time inference network A \t are both pretrained using the feed - forward local cross - entropy objective .", "entities": []}, {"text": "The seq2seq baseline achieves 82.80 F1 on the development set in our replication of Tran et al .", "entities": [[1, 2, "MethodName", "seq2seq"], [5, 6, "MetricName", "F1"]]}, {"text": "( 2018 ) .", "entities": []}, {"text": "Using a SPEN with our stacked parameterization , we obtain 83.22 F1 .", "entities": [[11, 12, "MetricName", "F1"]]}]