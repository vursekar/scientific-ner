[{"text": "Proceedings of The 13th International Conference on Natural Language Generation , pages 349\u2013359 , Dublin , Ireland , 15 - 18 December , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics349Transformer based Natural Language Generation for QuestionAnswering Imen Akermi Johannes", "entities": []}, {"text": "Heinecke", "entities": []}, {"text": "Fr \u00b4 ed\u00b4eric Herledan Orange / DATA", "entities": []}, {"text": "AI 22307 Lannion cedex , France imen.elakermi@orange.com johannes.heinecke@orange.com frederic.herledan@orange.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "This paper explores Natural Language Generation within the context of Question - Answering task .", "entities": []}, {"text": "The several works addressing this task only focused on generating a short answer or a long text span that contains the answer , while reasoning over a Web page or processing structured data .", "entities": []}, {"text": "Such answers \u2019 length are usually not appropriate as the answer tend to be perceived as too brief or too long to be read out loud by an intelligent assistant .", "entities": []}, {"text": "In this work , we aim at generating a concise answer for a given question using an unsupervised approach that does not require annotated data .", "entities": []}, {"text": "Tested over English and French datasets , the proposed approach shows very promising results .", "entities": []}, {"text": "1 Introduction Question - Answering systems ( QAS ) aim at analyzing and processing user questions in order to provide relevant answers ( Hirschman and Gaizauskas , 2001 ) .", "entities": []}, {"text": "The recent popularity of intelligent assistants has increased the interest in QAS which have become a key component of \u201c Human - Machine \u201d exchanges since they allow users to have instant answers to their questions in natural language using their own terminology without having to go through a long list of documents to \ufb01nd the appropriate answers .", "entities": []}, {"text": "Most of the existing research work focuses on the major complexity of these systems residing in the processing and interpretation of the question that expresses the user \u2019s need for information , without considering the representation of the answer itself .", "entities": []}, {"text": "Usually , the answer is either represented by a short set of terms answering exactly the question ( case of QAS which extract answers from structured data ) , or by a text span extracted from a document which , besides the exact answer , can integrate other unnecessary information that are not relevantto the context of the question asked .", "entities": []}, {"text": "The following presents two answers for Who is the thesis supervisor of Albert Einstein ?", "entities": []}, {"text": "possibly generated by two systems : Alfred Kleiner Albert Einstein is a German - born theoretical physicist who developed the theory of relativity , one of the two pillars of modern physics .", "entities": []}, {"text": "Given the speci\ufb01city of QAS which extract answers from structured data , users generally receive only a short and limited answer to their questions as illustrated by the example above .", "entities": []}, {"text": "This type of answer representation might not meet the user expectations .", "entities": []}, {"text": "Indeed , the type of answer given by the \ufb01rst system can be perceived as too brief not recalling the context of the question .", "entities": []}, {"text": "The second system returns a passage which contains information that are out of the question \u2019s scope and might be deemed by the user as irrelevant .", "entities": []}, {"text": "It is within this framework that we propose in this article an approach which allows to generate a concise answer in natural language ( e.g. The thesis superviser of Albert Einstein was Alfred Kleiner ) that shows very promising results tested over French and English questions .", "entities": []}, {"text": "This approach is a component of a QAS that we proposed in Rojas Barahona et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) and that we will brie\ufb02y present in this article .", "entities": []}, {"text": "In what follows , we detail in section 3 the approach we propose for answer generation in Natural Language and we brie\ufb02y discuss the QAS developed .", "entities": [[14, 16, "TaskName", "answer generation"]]}, {"text": "We present in section 4 the experiments that we have conducted to evaluate this approach .", "entities": []}, {"text": "2 Related Work The huge amount of information available nowadays makes the task of retrieving relevant informa-", "entities": []}, {"text": "350tion complex and time consuming .", "entities": []}, {"text": "This complexity has prompted the development of QAS which help spare the user the search and the information \ufb01ltering tasks , as it is often the case with search engines , and directly return the exact answer to a question asked in natural language .", "entities": []}, {"text": "The QAS cover mainly three tasks : question analysis , information retrieval and answer extraction ( Lopez et al . , 2011 ) .", "entities": [[10, 12, "TaskName", "information retrieval"]]}, {"text": "These tasks have been tackled in different ways , considering the knowledge bases used , the types of questions addressed ( Iida et al . , 2019 ; Zayaraz et al . , 2015 ; Dwivedi and Singh , 2013 ; Lopez et al . , 2011 ) and the way in which the answer is presented .", "entities": []}, {"text": "In this article , we particularly focus on the answer generation process .", "entities": [[9, 11, "TaskName", "answer generation"]]}, {"text": "We generally notice two forms of representation addressed in literature .", "entities": []}, {"text": "The answer can take the form of a paragraph selected from a set of text passages retrieved from the web ( Asai et al . , 2018 ;", "entities": []}, {"text": "Du and Cardie , 2018 ; Wang and Jiang , 2016 ; Wang et al . , 2017 ; Oh et al . , 2016 ) , as it can also be the exact answer to the question extracted from a knowledge base ( Wu et al . , 2003 ; Bhaskar et al . , 2013 ; Le et al . , 2016 ) .", "entities": []}, {"text": "Despite the abundance of work in the \ufb01eld of QAS , the answers generation issue has received little attention .", "entities": []}, {"text": "A \ufb01rst approach indirectly addressing this task has been proposed in Brill et al .", "entities": []}, {"text": "( 2001 , 2002 ) .", "entities": []}, {"text": "Indeed , the authors aimed at diversifying the possible answer patterns by permuting the question \u2019s words in order to maximise the number of retrieved documents that may contain the answer to the given question .", "entities": []}, {"text": "Another answer representation approach based on rephrasing rules has also been proposed in Agichtein and Gravano ( 2000 ) ; Lawrence and Giles ( 1998 ) within the context of query expansion task for document retrieval and not purposely for the question - answering task .", "entities": []}, {"text": "The few works that have considered this task within the QAS framework have approached it from a text summary generation perspective ( Ishida et al . , 2018 ; Iida et al . , 2019 ; Rush et al . , 2015 ; Chopra et al . , 2016 ;", "entities": []}, {"text": "Nallapati et al . , 2016 ; Miao and Blunsom,2016 ; See et al . , 2017 ; Oh et al . , 2016 ; Sharp et al . , 2016 ; Tan et al . , 2016 ; dos Santos et al . , 2016 ) .", "entities": []}, {"text": "These works consist in generating a summary of a single or various text spans that contain the answer to a question .", "entities": []}, {"text": "Most of these works have only considered causality questions like the ones starting with \u201c why \u201d and whose answers are para - graphs .", "entities": []}, {"text": "To make these answers more concise , the extracted paragraphs are summed up .", "entities": []}, {"text": "Other approaches ( Kruengkrai et al . , 2017 ; Girju , 2003 ; Verberne et al . , 2011 ;", "entities": []}, {"text": "Oh et al . , 2013 ) have explored this task as a classi\ufb01cation problem that consists in predicting whether a text passage can be considered as an answer to a given question .", "entities": []}, {"text": "It should be noted that these approaches only intend to diversify as much as possible the answer representation patterns to a given question in order to increase the probability of extracting the correct answer from the Web and do not focus on the answer \u2019s representation itself .", "entities": []}, {"text": "It should also be noted that these approaches are only applicable for QAS which extract answers as a text snippet and can not be applied to short answers usually extracted from knowledge bases .", "entities": []}, {"text": "The work presented in Pal et al . ( 2019 ) tried to tackle this issue by proposing a supervised approach that was trained on a small dataset whose questions / answers pairs were extracted from machine comprehension datasets and augmented manually which make generalization and capturing variation very limited .", "entities": []}, {"text": "Our answer generation approach differs from these works as it is unsupervised , can be adapted to any type of factual question ( except for why ) and is based only on easily accessible and unannotated data .", "entities": [[1, 3, "TaskName", "answer generation"]]}, {"text": "Indeed , we build upon the intuitive hypothesis that a concise answer and easily pronounced by an intelligent assistant can in fact consist of a reformulation of the question asked .", "entities": []}, {"text": "This approach is a part of a QAS that we have developed in Rojas Barahona et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) that extracts the answer to a question from structured data .", "entities": []}, {"text": "In what follows , we detail in section 3 the approach we propose for answer generation in Natural Language and we brie\ufb02y discuss the QAS developed .", "entities": [[14, 16, "TaskName", "answer generation"]]}, {"text": "We present in section 4 the experiments that we have conducted to evaluate this approach .", "entities": []}, {"text": "and we conclude in section 5 with the limitations noted and the perspectives considered .", "entities": []}, {"text": "3 NLG Approach for Answer Generation", "entities": [[4, 6, "TaskName", "Answer Generation"]]}, {"text": "The answer generation approach proposed is a component of a system which was developed in Rojas Barahona et", "entities": [[1, 3, "TaskName", "answer generation"]]}, {"text": "al . ( 2019 ) and which consists in a spoken conversational question - answering system which analyses and translates a question in natural language ( French or English ) in a formal representation that is transformed into a Sparql query1 .", "entities": []}, {"text": "1https://www.w3.org/TR/sparql11-overview/", "entities": []}, {"text": "351The Sparql query helps extracting the answer to the given question from an RDF knowledge base , in our case Wikidata2 .", "entities": []}, {"text": "The extracted answer takes the form of a list of URIs or values .", "entities": []}, {"text": "Although the QAS that we have developed ( Rojas Barahona et al . , 2019 ) is able to \ufb01nd the correct answer to a question , we have noticed that its short representation is not user - friendly .", "entities": []}, {"text": "Therefore , we propose an unsupervised approach which integrates the use of Transformer models such as BERT(Devlin et al . , 2019 ) and GPT ( Radford et al . , 2018 ) .", "entities": [[12, 13, "MethodName", "Transformer"], [24, 25, "MethodName", "GPT"]]}, {"text": "The choice of an unsupervised approach arises from the fact that there is no available training dataset associating a question with an exhaustive and concise answer at the same time .", "entities": []}, {"text": "such dataset could have helped use an End - to - End learning neural architecture that can generate an elaborated answer to a question .", "entities": []}, {"text": "This approach builds upon the fact that we have already extracted the short answer to a given question and assumes that a user - friendly answer can consist in rephrasing the question words along with the short answer .", "entities": []}, {"text": "This approach is composed of two fundamental phases : The dependency analysis of the input question and the answer generation using Transformer models .", "entities": [[18, 20, "TaskName", "answer generation"], [21, 22, "MethodName", "Transformer"]]}, {"text": "3.1 Dependency parsing For the dependency analysis , we use an extended version of UDPipeFuture ( Straka , 2018 ) which showed its state of the art performance by becoming \ufb01rst in terms of the Morphology - aware Labeled Attachment Score ( MLAS)3metric at the CoNLL Shared Task of dependency parsing in 2018 ( Zeman et al . , 2018 ) .", "entities": [[1, 3, "TaskName", "Dependency parsing"], [40, 41, "MetricName", "Score"], [49, 51, "TaskName", "dependency parsing"]]}, {"text": "UDPipeFuture is a POS tagger and graph parser based dependency parser using a BiLSTM , inspired by Dozat et", "entities": [[13, 14, "MethodName", "BiLSTM"]]}, {"text": "al . ( 2017 ) .", "entities": []}, {"text": "Our modi\ufb01cation consisted in adding several contextual word embeddings ( with respect to the language ) .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "In order to \ufb01nd the best con\ufb01guration we experimented with models like multilingual B ERT ( Devlin et al . , 2019 ) , XLM - R ( Conneau et al . , 2019 ) ( for both , English and French ) , RoB ERTA ( Liu et al . , 2019 ) ( for English ) , FlauB ERT ( Le et al . , 2020 ) and CamemB ERT(Martin et al . , 2019 ) ( for French ) during the training of the treebanks French2https://www.wikidata.org/ 3MLAS is a metric which takes into account POS tags and morphological features .", "entities": [[24, 25, "MethodName", "XLM"]]}, {"text": "It is inspired by the Content - Word Labeled Attachment Score ( CLAS , Nivre and Fang ( 2017 ) which differentiates between content word and function words .", "entities": [[10, 11, "MetricName", "Score"]]}, {"text": "Both are derived from the standard Labeled Attachment Score ( LAS ) metric .", "entities": [[8, 9, "MetricName", "Score"]]}, {"text": "GSD and English - EWT4 , of the Universal Dependencies project ( UD ) ( Nivre et al . , 2016 ) 5 .", "entities": [[8, 10, "DatasetName", "Universal Dependencies"], [12, 13, "DatasetName", "UD"]]}, {"text": "Adding contextual word embedding increases signi\ufb01cantly the results for all metrics , LAS , CLAS and MLAS ( cf . table 1 ) .", "entities": []}, {"text": "This is the case for all languages ( of the CoNLL shared task ) , where language speci\ufb01c contextual embeddings or multingual ones ( as B ERTor XLM - R ) improved parsing ( Heinecke , 2020 )", "entities": [[27, 28, "MethodName", "XLM"]]}, {"text": "French ( Fr - GSD ) embeddings MLAS CLAS LAS Straka ( 2018 ) 77.29 82.49 85.74 FlauB ERT 79.53 84.16 87.98 BERT 81.64 86.21 89.68 CamemB", "entities": [[22, 23, "MethodName", "BERT"]]}, {"text": "ERT 82.17 86.45 89.67 XLM - R 82.62 86.94 89.82 English ( En - EWT ) embeddings MLAS CLAS LAS Straka ( 2018 ) 74.71 79.14 82.51 BERT 81.16 85.89 88.63 RoB ERTA 82.38 86.89 89.40 XLM - R 82.91 87.24 89.54 Table 1 : Dependency Analysis for English and French ( UD v2.2 ) using different contextual word embeddings , best results in bold In order to parse simple , quiz - like questions , the training corpora of the two UD treebanks are not appropriate ( enough ) , since both treebanks do not contain many questions , if at all6 .", "entities": [[4, 5, "MethodName", "XLM"], [27, 28, "MethodName", "BERT"], [36, 37, "MethodName", "XLM"], [52, 53, "DatasetName", "UD"], [58, 60, "TaskName", "word embeddings"], [82, 83, "DatasetName", "UD"]]}, {"text": "An explanation for bad performance on questions of parser models trained on standard UD is the fact , that in both languages , the syntax of questions differs from the syntax of declarative sentences : apart from whquestion words , in English the to do periphrasis is nearly always used in questions .", "entities": [[13, 14, "DatasetName", "UD"]]}, {"text": "In French , subject and direct objects can be inversed and the est - ce que construction appears frequently .", "entities": []}, {"text": "Both , the English to do periphrasis and the French est - ce que construction are absent in declarative sentences .", "entities": []}, {"text": "Table 2shows the ( much lower ) results when parsing questions using models trained only on the standard UD treebanks .", "entities": [[18, 19, "DatasetName", "UD"]]}, {"text": "In order to get a better analysis , we decided to 4As for the Shared Task CoNLL 2018 , we use version 2.2 to be able to compare with the of\ufb01cial results 5https://universaldependencies.org/ 6At least for French a question treebank exists within the UD project ( French - FQB , Seddah and Candito ( 2016 ) ) .", "entities": [[43, 44, "DatasetName", "UD"]]}, {"text": "However its questions are rather long and literary , not like thoses used in quizzes .", "entities": []}, {"text": "352French ( Fr - GSD ) embeddings MLAS CLAS LAS BERT 60.52 73.04 79.27 CamemB ERT 61.32 75.26 80.49 FlauB ERT 58.09 70.96 78.40 Word2Vec 59.83 74.43 80.14 XLM - R 59.23 73.52 79.27 English ( En - EWT ) embeddings MLAS CLAS LAS BERT 80.45 88.02 90.58", "entities": [[10, 11, "MethodName", "BERT"], [28, 29, "MethodName", "XLM"], [44, 45, "MethodName", "BERT"]]}, {"text": "RoB ERTa 80.68 89.17 91.49 XLM - R 80.68 89.42 91.88 Table 2 : Dependency Analysis of questions using models trained on the standard UD treebanks annotate additional sentences ( quiz - like questions ) and add this data to the basic treebanks .", "entities": [[5, 6, "MethodName", "XLM"], [24, 25, "DatasetName", "UD"]]}, {"text": "For English we annotated 309 questions ( plus 91 questions for validation ) from the QALD7 ( Usbeck et al . , 2017 ) and QALD8 corpora7 .", "entities": []}, {"text": "For French we translated the QALD7 questions into French and formulated others ourselves ( 276 train , 66 validation ) .", "entities": []}, {"text": "For the annotations we followed the general UD guidelines8as well as the treebank speci\ufb01c guidelines of En - EWT and Fr - GSD .", "entities": [[7, 8, "DatasetName", "UD"]]}, {"text": "As table 3shows , the quality of the dependency analysis improves considerably .", "entities": []}, {"text": "The contextual word embeddings CamemB ERT(for French ) and BERT(English ) have the biggest impact .", "entities": [[2, 4, "TaskName", "word embeddings"]]}, {"text": "French ( Fr - GSD ) embeddings MLAS CLAS LAS BERT 91.20 96.10 97.55 CamemB", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "ERT 92.12 97.37 98.26 FlauB ERT 90.53 94.74 96.86 Word2Vec 90.88 95.79 97.21 XLM - R 91.23 96.14 97.56 English ( En - EWT ) embeddings MLAS CLAS LAS BERT 84.85 91.92 94.24 RoB ERTa 83.08 91.67 93.85 XLM - R 83.08 90.66 93.59 Table 3 : Dependency analysis of questions using models trained on enriched UD treebanks We rely on the UdpipeFuture version which we have improved with B ERT ( for English)/CamemB ERT ( for French ) and which 7https://github.com/ag-sc/QALD 8https://universaldependencies.org/guidelines.htmlgives the best results in terms of dependency analysis , in order to proceed with the partitioning of the question into textual fragments ( also called chunks ): Q={c1,c2 , ... , cn } .", "entities": [[13, 14, "MethodName", "XLM"], [29, 30, "MethodName", "BERT"], [38, 39, "MethodName", "XLM"], [56, 57, "DatasetName", "UD"]]}, {"text": "If we take the example of the question What is the political party of the mayor of Paris ? , the set of textual fragments would be Q={What , is , the political party of the mayor of Paris } .", "entities": []}, {"text": "3.2 Answer generation During this phase , we \ufb01rst carry out a \ufb01rst test of the set Qto check whether the text fragment which contains a question marker ( exp : what , when , who etc . ) represents the subject nsubj in the analysed question .", "entities": [[1, 3, "TaskName", "Answer generation"]]}, {"text": "If so , we simply replace that text fragment with the answer we identi\ufb01ed earlier .", "entities": []}, {"text": "Let us take the previous example What is the political party of the mayor of Paris ? , the system automatically detects that the text fragment containing the question marker What represents the subject and will therefore be replaced directly by the exact answer The Socialist Party .", "entities": []}, {"text": "Therefore , the concise answer generated will be The Socialist Party is the political party of the mayor of Paris .", "entities": []}, {"text": "Otherwise , we remove the text fragment containing the question marker that we detected and we add the short answer RtoQ : Q= { c1,c2 , ... , cn\u22121,R } Using the text fragments set Q , we proceed with a permutation based generation of all possible answer structures that can form the sentence answering the question asked : S={s1(R , c1,c2 , ... , cn\u22121 ) , s2(c1,R , c2 , ... , cn\u22121 ) , ... , sm(c1,c2 , ... , cn\u22121,R ) }", "entities": []}, {"text": "These structures will be evaluated by a Language Model ( LM ) based on Transformer models which will extract the most probable sequence of text fragments that can account for the answer to be sent to the user : structure\u2217=s\u2208S;p(s)=argmaxsi\u2208Sp(si )", "entities": [[14, 15, "MethodName", "Transformer"]]}, {"text": "Once the best structure is identi\ufb01ed , we initiate the generation process of possible missing words .", "entities": []}, {"text": "Indeed , we suppose that there could be some terms which do not necessarily appear in the question or in the short answer but which are , on the other hand , necessary to the generation of a correct grammatical structure of the \ufb01nal answer .", "entities": []}, {"text": "This process requires that we set two parameters , the number of possible missing words and", "entities": []}, {"text": "their", "entities": []}, {"text": "353positions within the selected structure .", "entities": []}, {"text": "In this paper , we experiment the assumption that one word could be missing and that it is located before the short answer within the identi\ufb01ed structure , as it could be the case for a missing article ( the , a , etc . ) or a preposition ( in , at , etc . ) for example .", "entities": []}, {"text": "Therefore , to predict this missing word , we use BERTas the generation model ( GM ) for its ability to capture bidirectionally the context of a given word within a sentence .", "entities": []}, {"text": "In case when B ERTreturns a non - alphabetic character sequence , we assume that the optimal structure , as predicted by the LM , does not need to be completed by an additional word .", "entities": []}, {"text": "The following example illustrates the different steps of the proposed approach : Question : When did princess Diana die ?", "entities": []}, {"text": "1.Question parsing and answer extraction using the system proposed in Rojas Barahona et al .", "entities": []}, {"text": "( 2019 ): short answer = { August 31 , 1997 } 2.Chunking the question into text fragments using the UDPipe based dependency analysis : Q={When , did die , princess Diana } 3.Removing question marker fragment ( when ) and updating the verb tense and form using a rule - based approach that we have de\ufb01ned : Q={died , princess Diana } 4 .", "entities": []}, {"text": "Adding the short answer : Q={died;princess Diana ; August 31 , 1997 } 5.Generating the set of possible answer structures S : S={died princess Diana August 31 , 1997 ; .August 31 , 1997 died princess Diana ; .princess Diana died August 31 , 1997 ; .. . . }", "entities": []}, {"text": "6.Evaluating the different answer structures using a LM : p(structure * )", "entities": []}, {"text": "= argmaxsi\u2208Sp(si ): structure\u2217=princess Diana died August 31 , 1997 7.Generating possible missing word for structure\u2217with B ERT : Princess Diana died [ missing word ] August 31 , 1997 ( missing word = on ) Answer : Princess Diana died on August 31 , 1997.4 Experiments and Evaluation The existing QAS test sets are more tailored to systems which generate the exact short answer to a question or more focused on the Machine Reading Comprehension task where the answer consists of a text passage from a document containing the short answer .", "entities": [[73, 76, "TaskName", "Machine Reading Comprehension"]]}, {"text": "Therefore , we have created a dataset which maps questions extracted from the QALD-7 challenge dataset ( Usbeck et al . , 2017 ) with natural language answers which were de\ufb01ned by a linguist and which we individually reviewed .", "entities": []}, {"text": "This dataset called Q UEREO consists of 150 questions with the short answers extracted by the QAS that we described above .", "entities": []}, {"text": "We denote an average of three possible gold sanswers in natural language for each question .", "entities": []}, {"text": "French and English versions were created for this dataset .", "entities": []}, {"text": "{ possible answer structures } LM structure*missing word generationanswer { structures}A1", "entities": []}, {"text": "LM Generation BERT en , fr CamemB", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "ERT fr FlauB ERT fr XLM en , fr", "entities": [[5, 6, "MethodName", "XLM"]]}, {"text": "XLM- RoBERTafr XLNet en GPT en GPT2 enBERT CamemB ERTArch .", "entities": [[2, 3, "MethodName", "XLNet"], [4, 5, "MethodName", "GPT"]]}, {"text": "A1 A2Metrics BLEU METEOR ROUGE BERTScore{possible answer structures } A2 missing word generationLMstructure *", "entities": [[2, 3, "MetricName", "BLEU"], [3, 4, "DatasetName", "METEOR"]]}, {"text": "=   answer Figure 1 : Experiment framework As illustrated in \ufb01gure 1 , two possible architectures of the approach proposed for answer generation have been evaluated .", "entities": [[22, 24, "TaskName", "answer generation"]]}, {"text": "The \ufb01rst architecture A1 consists in generating all possible answer structures in order to have them evaluated afterwards by a LM which will identify the optimal answer structure to which we generate possible missing words .", "entities": []}, {"text": "Architecture A2starts with generating missing words for each structure in Swhich will then be evaluated by the LM .", "entities": []}, {"text": "In this paper , we assume that there is only one missing word per structure .", "entities": []}, {"text": "To evaluate the proposed approach , we have referred to standard metrics de\ufb01ned for NLG tasks such as Automatic Translation and Summarization , as they allow to assess to what extent a generated sentence is similar to the gold sentence .", "entities": [[19, 20, "TaskName", "Translation"], [21, 22, "TaskName", "Summarization"]]}, {"text": "We con-", "entities": []}, {"text": "354sider three N - gram metrics ( BLEU , METEOR and ROUGE ) and the B ERTscore metric which exploits the pre - trained embeddings of B ERT to calculate the similarity between the answer generated and the gold answer .", "entities": [[7, 8, "MetricName", "BLEU"], [9, 10, "DatasetName", "METEOR"]]}, {"text": "To be able to compare the different con\ufb01gurations of the approach , we refer to Friedman \u2019s test ( Milton , 1939 ) which allows to detect the performance variation of different con\ufb01gurations of a model evaluated by several metrics based on the average ranks .", "entities": []}, {"text": "25   50 rank A1 / Bert / Cmbert - base A2 / Bert / flaubert - small A1 / Bert / xlm - roberta - base A1 / Bert / flaubert - base - unc A1 / Bert / xlm - mlm - enfr-1024 A1 / Bert / xlm - roberta - large A2 / Bert / gpt2 A1 / Bert / bert - base - mlg - unc A1 / Bert / bert - base - mlg A2 / Bert / xlm - clm - enfr-1024 A1 / Bert / xlm - clm - enfr-1024 A2 / Bert / xlm - mlm - enfr-1024 A2 / Bert / flaubert - large A2 / Bert / xlm - roberta - base A1 / Bert / flaubert - large A1 / Bert / flaubert - small A1 / Bert / openai - gpt A2 / Bert / Cmbert - base A2 / Bert / flaubert - base A2 / Bert / flaubert - base - unc A2 / Bert / xlm - roberta - large A1 / Bert / gpt2 A2 / Bert / bert - base - mlg - unc A2 / Bert / gpt2 - medium A1 / Bert / gpt2 - large A2 / Bert / bert - base - mlg A1 / Bert / gpt2 - medium A1 / Bert / flaubert - base A2 / Cmbert / xlm - roberta - base A1 / Cmbert / xlm - roberta - base A1 / Cmbert / Cmbert - base A2 / Cmbert / flaubert - small A2 / Bert / openai - gpt A1 / Cmbert / xlm - roberta - large A1 / Cmbert / xlm - mlm - enfr-1024 A1 / Cmbert / bert - base - mlg - unc A2 / Cmbert / bert - base - mlg - unc A1 / Cmbert / flaubert - base - unc A2 / Cmbert / Cmbert - base A2 / Cmbert / bert - base - mlg A2 / Cmbert / flaubert - large A2 / Cmbert / xlm - roberta - large A1 / Cmbert / bert - base - mlg A1 / Cmbert / gpt2 - large A1 / Cmbert / xlm - clm - enfr-1024 A2 / Cmbert / gpt2 A1 / Cmbert / flaubert - small A2 / Cmbert / gpt2 - medium A2 / Cmbert / xlm - mlm - enfr-1024 A2 / Cmbert / flaubert - base - unc A1 / Cmbert / flaubert - base A1 / Cmbert / flaubert - large A1 / Cmbert / gpt2 - medium A1 / Cmbert / gpt2 A1 / Cmbert / openai - gpt A2 / Cmbert / openai - gpt A2 / Cmbert / flaubert - base A2 / Cmbert / xlm - clm - enfr-1024configurations Bleu Rouge", "entities": [[22, 23, "MethodName", "xlm"], [40, 41, "MethodName", "xlm"], [42, 43, "DatasetName", "mlm"], [49, 50, "MethodName", "xlm"], [83, 84, "MethodName", "xlm"], [92, 93, "MethodName", "xlm"], [101, 102, "MethodName", "xlm"], [103, 104, "DatasetName", "mlm"], [117, 118, "MethodName", "xlm"], [142, 143, "MethodName", "gpt"], [170, 171, "MethodName", "xlm"], [232, 233, "MethodName", "xlm"], [241, 242, "MethodName", "xlm"], [266, 267, "MethodName", "gpt"], [271, 272, "MethodName", "xlm"], [280, 281, "MethodName", "xlm"], [282, 283, "DatasetName", "mlm"], [343, 344, "MethodName", "xlm"], [368, 369, "MethodName", "xlm"], [396, 397, "MethodName", "xlm"], [398, 399, "DatasetName", "mlm"], [442, 443, "MethodName", "gpt"], [449, 450, "MethodName", "gpt"], [461, 462, "MethodName", "xlm"], [466, 467, "MetricName", "Bleu"]]}, {"text": "Meteor BertSc .", "entities": [[0, 1, "DatasetName", "Meteor"]]}, {"text": "Human acc .", "entities": [[1, 2, "MetricName", "acc"]]}, {"text": "Figure 2 : Correlation assessment between human evaluation and the Bleu , Meteor , Rouge and Bert scores French Q / A ( \u201c CmBert \u201d stands for CamemB ERT ) Figure 3 : Screenshot of the evaluation tool We also conducted a human evaluation study for the French and the English versions of the dataset , in which we asked 20 native speakers participants to evaluate the relevance of a generated answer ( correct ornot correct ) regarding a given question while indicating the type of errors depicted ( grammar , wrong preposition , word order , extra word(s ) , etc ) .", "entities": [[10, 11, "MetricName", "Bleu"], [12, 13, "DatasetName", "Meteor"]]}, {"text": "Figure 3presents the evaluation framework that we have implemented and provided to the participants .", "entities": []}, {"text": "The results of each participant are saved in a json-\ufb01le ( \ufb01gure 4 ) .", "entities": []}, {"text": "The inter - agreement rate between participants reached 70 % which indicates a substantial agreement .", "entities": []}, {"text": "Through the human evaluation study , we wanted to explore to what extent the standard metrics are reliable to assess NLG approaches within the context of question - answering systems .", "entities": []}, {"text": "Table 4(French dataset ) represents the obtained results for the \ufb01rst three best models according to the human evaluation ranking and the Friedman test ranking .", "entities": []}, {"text": "We indicate between brackets each model \u2019s rank according to the metric used .", "entities": []}, {"text": "We note that the highest human accuracy score for French of about 85 % was scored with the \ufb01rst architecture coupled with B ERTas the generation model ( GM ) and CamemB ERT as the language model ( LM ) .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "We also notice that the architecture A1 , which considers the LM assessment of the structure before generating missing words , performs better .", "entities": []}, {"text": "Surprisingly , as a generative model , the multi-", "entities": []}, {"text": "355Hum .", "entities": []}, {"text": "Frm .", "entities": []}, {"text": "Arch .", "entities": []}, {"text": "GM LM H. Acc BLEU METEOR ROUGE BERTS rank rank score score rank score rank score rank score rank 1 1 A1 BT CmBt 84.85 86.28", "entities": [[3, 4, "MetricName", "Acc"], [4, 5, "MetricName", "BLEU"], [5, 6, "DatasetName", "METEOR"]]}, {"text": "[ 1 ] 96.76 [ 1 ] 93.69 [ 6 ] 97.89 [ 2 ] 2 2 A2 B T FBT - s - c 84.09 85.87 [ 7 ] 96.75 [ 2 ] 94.22 [ 1 ] 97.96", "entities": []}, {"text": "[ 1 ] 2 3 A1 B T XRob 84.09 85.93", "entities": []}, {"text": "[ 4 ] 96.63 [ 6 ] 93.79 [ 5 ] 97.88 [ 3 ] 2 9 A1 B T", "entities": []}, {"text": "BT - ml - c 84.09 85.01 [ 19 ] 96.52", "entities": []}, {"text": "[ 22 ] 93.81 [ 4 ] 97.79 [ 7 ] 5 4 A1 B T FBT - b - uc 83.33 86.17", "entities": []}, {"text": "[ 2 ] 96.72 [ 3 ] 93.56", "entities": []}, {"text": "[ 14 ] 97.81 [ 6 ] 5 5 A1 B T mlm-1024 83.33 85.39", "entities": []}, {"text": "[ 10 ] 96.60 [ 8 ] 93.61 [ 10 ] 97.83 [ 4 ] 5 6 A2 B T GPT 2 83.33 85.46 [ 9 ] 96.67", "entities": [[20, 21, "MethodName", "GPT"]]}, {"text": "[ 4 ] 93.48 [ 17 ] 97.76 [ 10 ] 5 10 A2 B T clm-1024 83.33 85.89", "entities": []}, {"text": "[ 6 ] 96.55 [ 18 ] 93.57", "entities": []}, {"text": "[ 13 ] 97.71 [ 19 ] 5 11 A1 B T clm-1024 83.33 84.99", "entities": []}, {"text": "[ 20 ] 96.52 [ 23 ] 93.87", "entities": []}, {"text": "[ 3 ] 97.76 [ 12 ] 5 12 A2 B T FBT - l - c 83.33 86.15", "entities": []}, {"text": "[ 3 ] 96.57 [ 13 ] 93.14", "entities": []}, {"text": "[ 37 ] 97.79 [ 8 ] 5 13 A2 B T mlm-1024 83.33 85.90", "entities": []}, {"text": "[ 5 ] 96.54 [ 20 ] 93.30", "entities": []}, {"text": "[ 27 ] 97.76 [ 11 ] 5 14 A2 B T XRob 83.33 85.32", "entities": []}, {"text": "[ 13 ] 96.46 [ 28 ] 93.63 [ 9 ] 97.71", "entities": []}, {"text": "[ 17 ] Table 4 : Model ranking for French dataset according to the human evaluation study ( best in bold ) and the Friedman test ( best in yellow ) .", "entities": []}, {"text": "\u201c B T \u201d in Column GM stands for B ERT - base - multilingual - cased .", "entities": []}, {"text": "In column LM we use \u201c CmB T \u201d for CamemB ERT - base , \u201c B T - ml - c \u201d for B ERT - base - multilingual - cased , \u201c XRob \u201d for XLM - RoB ERTa - base , \u201c FB T - s - c \u201d for FlauB ERT - small - cased , \u201c FB T - b - uc \u201d for FlauB ERT - base - uncased and \u201c clm-1024 \u201d for XLM - clmenfr-1024 lingual B ERTmodel predicts missing words better than CamemB ERT for French sentences .", "entities": [[37, 38, "MethodName", "XLM"], [80, 81, "MethodName", "XLM"]]}, {"text": "These \ufb01ndings are also con\ufb01rmed by the Friedman test where we can clearly see that the \ufb01rst ranked con\ufb01guration maps the best con\ufb01guration selected according to the human accuracy , with a very slight difference for the other four con\ufb01gurations .", "entities": [[28, 29, "MetricName", "accuracy"]]}, {"text": "Let us see if that means that the four metrics are correlated with the human accuracy .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "According to table 6which presents the Pearson correlation ( Benesty et al . , 2009 ) of the human accuracy with the four metrics and to \ufb01gure 2which illustrates the ranking given by each evaluation metric along with the human judgement for each con\ufb01guration ( i.e. con\ufb01guration = GM \u00d7architecture \u00d7LM ) tested , we can clearly see that the human evaluation results are positively and strongly correlated with the B LEU , the M ETEOR and the B ERTscores .", "entities": [[6, 8, "MetricName", "Pearson correlation"], [19, 20, "MetricName", "accuracy"]]}, {"text": "These metrics are practically matching the human ranking and thus are obviously able to identify which con\ufb01guration gives better results .", "entities": []}, {"text": "The rouge metric , used for French question / answer evaluation , is moderately correlated with the human evaluation which means that we should not only rely on this metric when assessing such task .", "entities": []}, {"text": "On the other hand , when the ROUGE metric is considered with the other metrics , it helps to get closer to the human judgement .", "entities": []}, {"text": "Table 5presents", "entities": []}, {"text": "the results for the English dataset and shows that the best accuracy scored is about 72 % with A1 , BERT as the generative model and the Generative Pretrained Transformer ( GPT ) as the language model .", "entities": [[11, 12, "MetricName", "accuracy"], [20, 21, "MethodName", "BERT"], [29, 30, "MethodName", "Transformer"], [31, 32, "MethodName", "GPT"]]}, {"text": "According to the \ufb01rst three con\ufb01gurations , architecture A2prevails and the GPT transformer takes over the other lan - guage models .", "entities": [[11, 12, "MethodName", "GPT"]]}, {"text": "These results are also con\ufb01rmed by the Friedman test with a very slight difference on the ranking and also upheld with the correlation scores between the human assessment and each of the four metrics as shown by \ufb01gure 5and table 6 .", "entities": []}, {"text": "These \ufb01ndings mean that we actually can rely on the use of these standard metrics to evaluate the answer generation task for question - answering .", "entities": [[18, 20, "TaskName", "answer generation"]]}, {"text": "We also tried to analyse the errors indicated by the participants .", "entities": []}, {"text": "As we can note from \ufb01gure 6 , the most common error reported for both English and French datasets is the word order which sheds the light on a problem related to the language model assessment phase .", "entities": []}, {"text": "The second most reported error addresses the generation process , whether to indicate that there are one or more missing words within the answer ( French ) or the presence of some odd words ( English ) .", "entities": []}, {"text": "When trying to get an insight on the answers generated by the current intelligent systems such as Google assistant and Alexa , we noted that these systems are very accurate when extracting the correct answer to a question and can sometimes generate user - friendly answers that help recall the question context , specially with Alexa .", "entities": [[17, 18, "DatasetName", "Google"]]}, {"text": "However , we noticed that most of the answers generated by these systems are more verbose than necessary , we also found out that when addressing yes / no questions , these systems generally settle for just a yesorno without elaborating , or , on the other hand , present a text span extracted from a Web page and let the user guess the answer .", "entities": []}, {"text": "Let us take for example the following question Was US president Jackson involved in a war ?", "entities": []}, {"text": "356Hum .", "entities": []}, {"text": "Frm .", "entities": []}, {"text": "Arch .", "entities": []}, {"text": "GM LM H. Acc BLEU METEOR ROUGE BERTS rank rank score score rank score rank score rank score rank 1 1 A2 BT - ml GPT 72.36 78.25", "entities": [[3, 4, "MetricName", "Acc"], [4, 5, "MetricName", "BLEU"], [5, 6, "DatasetName", "METEOR"], [25, 26, "MethodName", "GPT"]]}, {"text": "[ 2 ] 94.63 [ 1 ] 92.83 [ 2 ] 97.21", "entities": []}, {"text": "[ 3 ] 1 2 A1 B T - ml GPT 72.36 78.25", "entities": [[10, 11, "MethodName", "GPT"]]}, {"text": "[ 1 ] 94.51 [ 2 ] 92.53 [ 10 ] 97.23 [ 2 ] 3 2 A1 B T GPT 71.55 77.12", "entities": [[20, 21, "MethodName", "GPT"]]}, {"text": "[ 6 ] 94.45 [ 3 ] 92.80 [ 5 ] 97.32 [ 1 ] 3 4 A2 B T GPT 2 71.55 76.98 [ 7 ] 94.40", "entities": [[20, 21, "MethodName", "GPT"]]}, {"text": "[ 7 ] 92.86 [ 1 ] 97.17 [ 5 ] 3 5 A2 B T - ml GPT2 71.55 77.53 [ 4 ] 94.39", "entities": []}, {"text": "[ 8 ] 92.82", "entities": []}, {"text": "[ 4 ] 97.14 [ 6 ] 3 6 A2 B T - ml GPT2 - l 71.55 77.85", "entities": []}, {"text": "[ 3 ] 94.41 [ 5 ] 92.65 [ 7 ] 97.07", "entities": []}, {"text": "[ 10 ] 3 7 A2 B T - ml", "entities": []}, {"text": "GPT2 - m 71.55 77.42", "entities": []}, {"text": "[ 5 ] 94.40 [ 6 ] 92.58 [ 9 ] 97.10 [ 9 ] 3 7 A2 B T GPT 2 - m 71.55 75.96", "entities": [[20, 21, "MethodName", "GPT"]]}, {"text": "[ 13 ] 94.41 [ 4 ] 92.60 [ 8 ] 97.18 [ 4 ] 3 10 A2 B T GPT 71.55 76.28", "entities": [[20, 21, "MethodName", "GPT"]]}, {"text": "[ 11 ] 94.30 [ 9 ] 92.76 [ 6 ] 97.14", "entities": []}, {"text": "[ 7 ] 10 7 A2 B T GPT 2 - l 70.73 76.74 [ 8 ] 94.26 [ 10 ] 92.83 [ 3 ] 97.14", "entities": [[8, 9, "MethodName", "GPT"]]}, {"text": "[ 8 ] 10 30 A1 B T - ml B T - b - uc 70.73 74.85", "entities": []}, {"text": "[ 30 ] 93.94", "entities": []}, {"text": "[ 31 ] 90.86 [ 26 ] 96.61 [ 28 ] Table 5 : Model ranking for English dataset according to the human evaluation study ( best in bold ) and the Friedman ranking ( best in yellow ) .", "entities": []}, {"text": "In Column GM we use \u201c B T - ml \u201d for B ERT - base - multilingual - cased and \u201c B T \u201d for B ERTlarge - cased .", "entities": []}, {"text": "In column LM \u201c GPT \u201d stands for for OpenAI - GPT , \u201c GPT2 - l \u201d for GPT2 - large , \u201c GPT2 - m \u201d for GPT2medium , \u201c GPT2 \u201d for GPT2 , \u201c B T - b - uc \u201d for B ERT - base - uncased , \u201c mlm-2048 \u201d for XLM - mlm - en-2048 and \u201c B T - l - c \u201d for B ERT - large - cased .", "entities": [[4, 5, "MethodName", "GPT"], [11, 12, "MethodName", "GPT"], [57, 58, "MethodName", "XLM"], [59, 60, "DatasetName", "mlm"]]}, {"text": "[ { \" ID \" : \" quereo_5.4 \" , \" QUESTION \" : \" Quelles sont les companies d\u2019\u00b4electronique fond \u00b4 ees ` a Beijing ? \" , \" SHORT_ANSWER \" : [ \" Xiaomi \" , \" Lenovo \" ] , \" GENERATED_ANSWER \" : \" Les companies d \u2019 \u00b4 electronique fond\u00b4ees`a beijing sont xiao xiaomi et", "entities": []}, {"text": "lenovo \" , \" MISSING_WORD \" : \" Xiao \" , \" EVALUATION \" : \" correcte \" , \" ERROR \" : [ \" aucun \" ] , \" COMMENT \" : \" \" } , { \" ID \" : \" quereo_8.8 \" , \" QUESTION \" : \" Combien de films a r\u00b4ealis\u00b4e Park Chan - wook ? \" , \" SHORT_ANSWER \" : [ \" quatorze \" ] , \" GENERATED_ANSWER \" : \" Quatorze films a r \u00b4 ealis\u00b4e park chan - wook \" , \" MISSING_WORD \" : \" .", "entities": []}, {"text": "\" , \" EVALUATION \" : \" incorrecte \" , \" ERROR \" : [ \" ordre \" , \" accord \" ] , \" COMMENT \" : \" \" } , ... ]", "entities": []}, {"text": "Figure 4 : Extract of a human evaluation result 20   40 rank A2 / Bert - mlg / openai - gpt A1 / Bert - mlg / openai - gpt A1 / Bert / openai - gpt A2 / Bert - mlg / gpt2 A2 / Bert / gpt2 A2 / Bert - mlg / gpt2 - large A2 / Bert / gpt2 - large A2 / Bert - mlg / gpt2 - medium A2 / Bert / gpt2 - medium A2 / Bert / openai - gpt A2 / Bert / xlm - mlm - en-2048 A1 / Bert / gpt2 A1 / Bert / xlm - clm - enfr-1024 A1 / Bert / gpt2 - large A2 / Bert / xlm - mlm - enfr-1024 A1 / Bert / xlm - mlm - en-2048 A1 / Bert / gpt2 - medium A1 / Bert - mlg / bert - large A1 / Bert - mlg / xlm - mlm - en-2048 A2 / Bert - mlg / bert - base - unc A1 / Bert / xlm - mlm - enfr-1024 A2 / Bert / xlm - clm - enfr-1024 A1 / Bert - mlg / gpt2 - large A1 / Bert - mlg / gpt2 - medium A2 / Bert / xlnet - large A2 / Bert - mlg / bert - base A1 / Bert - mlg / gpt2 A1 / Bert - mlg / xlm - mlm - enfr-1024 A1 / Bert - mlg / xlm - clm - enfr-1024 A1 / Bert - mlg / bert - base - unc A1 / Bert / xlnet - large A2 / Bert - mlg / xlm - mlm - enfr-1024 A1 / Bert / bert - base - unc A1 / Bert - mlg / bert - base A1 / Bert - mlg / bert - base - mlg A2 / Bert - mlg / xlm - mlm - en-2048 A1 / Bert / bert - base - mlg A1 / Bert - mlg / xlnet - large A1 / Bert / bert - large A2 / Bert / bert - base A2 / Bert / bert - base - mlg A2 / Bert / bert - large A2 / Bert - mlg / bert - large A1 / Bert / bert - base A2 / Bert / bert - base - unc A2 / Bert - mlg / bert - base - mlg A2 / Bert - mlg / xlm - clm - enfr-1024 A2 / Bert - mlg / xlnet - largeconfigurations Bleu Rouge Meteor BertSc .", "entities": [[21, 22, "MethodName", "gpt"], [30, 31, "MethodName", "gpt"], [37, 38, "MethodName", "gpt"], [88, 89, "MethodName", "gpt"], [93, 94, "MethodName", "xlm"], [95, 96, "DatasetName", "mlm"], [107, 108, "MethodName", "xlm"], [123, 124, "MethodName", "xlm"], [125, 126, "DatasetName", "mlm"], [132, 133, "MethodName", "xlm"], [134, 135, "DatasetName", "mlm"], [159, 160, "MethodName", "xlm"], [161, 162, "DatasetName", "mlm"], [179, 180, "MethodName", "xlm"], [181, 182, "DatasetName", "mlm"], [188, 189, "MethodName", "xlm"], [215, 216, "MethodName", "xlnet"], [240, 241, "MethodName", "xlm"], [242, 243, "DatasetName", "mlm"], [251, 252, "MethodName", "xlm"], [271, 272, "MethodName", "xlnet"], [280, 281, "MethodName", "xlm"], [282, 283, "DatasetName", "mlm"], [320, 321, "MethodName", "xlm"], [322, 323, "DatasetName", "mlm"], [340, 341, "MethodName", "xlnet"], [415, 416, "MethodName", "xlm"], [426, 427, "MethodName", "xlnet"], [429, 430, "MetricName", "Bleu"], [431, 432, "DatasetName", "Meteor"]]}, {"text": "Human acc .", "entities": [[1, 2, "MetricName", "acc"]]}, {"text": "Figure 5 : Correlation assessment between human evaluation and the Bleu , Meteor , rouge and Bert scores English Q / A", "entities": [[10, 11, "MetricName", "Bleu"], [12, 13, "DatasetName", "Meteor"]]}, {"text": "357Metrics Pearson Correlation QUEREO -fr QUEREO -en", "entities": [[1, 3, "MetricName", "Pearson Correlation"]]}, {"text": "BLEU 98 % 85 % METEOR 99 % 80 % ROUGE 46 % 83 % BERT - score 97 % 88 % Table 6 : Pearson Correlation of the four metrics with the human evaluation / judgement extra words grammar   missing words prepositions word order error categories 0 20 40 60 % of all errors English French Figure 6 : Distribution of generation errors Andrew Jackson , who served as a major general in the War of 1812 , commanded U.S. forces in a \ufb01ve - month campaign against the Creek Indians , allies of the British .", "entities": [[0, 1, "MetricName", "BLEU"], [5, 6, "DatasetName", "METEOR"], [15, 16, "MethodName", "BERT"], [25, 27, "MetricName", "Pearson Correlation"], [48, 49, "DatasetName", "0"]]}, {"text": "Here \u2019s something I found on the Web .", "entities": []}, {"text": "According to constitutioncenter.org : After the War of 1812 , Jackson led military forces against the Indians and was involved in treaties that led to the relocation of Indians .", "entities": []}, {"text": "The user has to focus on the returned text fragment in order to guess that the answer to his question is actually yes .", "entities": []}, {"text": "This issue was particularly noted when addressing French questions .", "entities": []}, {"text": "If we also take the example How many grandchildren did Jacques Cousteau have ?", "entities": []}, {"text": "the two systems answer as follows : Fabien Cousteau , Alexandra Cousteau , Philippe Cousteau Jr. , C \u00b4 eline Cousteau .", "entities": []}, {"text": "Jacques Cousteau \u2019s grandchildren were Philippe Cousteau Jr. , Alexandra ousteau , C \u00b4 eline Cousteau , and", "entities": []}, {"text": "Fabien Cousteau However , the user is not asking about the names of Cousteau \u2019s grand - children and has to guess by himself that the answer for this question is four .", "entities": []}, {"text": "A more accurate answer should indicate the exact answer to the question and then elaborate Jacques Cousteau had four grand - children .", "entities": []}, {"text": "But these systems perform better in case when the terms employed in the question are not necessarily relevant to the answer .", "entities": []}, {"text": "If we take the example of the questionwho is the wife of Lance Bass , the approach that we propose will generate The wife of Lance Bass is Michael Turchin .", "entities": []}, {"text": "As we can note the answer generated was not adapted to the actual answer , while the other systems are able to detect such nuance :", "entities": []}, {"text": "Lance Bass is married to Michael Turchin .", "entities": []}, {"text": "They have been married since 2014 .", "entities": []}, {"text": "This issue has still to be addressed .", "entities": []}, {"text": "5 Conclusion and perspectives We have put forward , in this paper , an approach for Natural Language Generation within the framework of the question - answering task that considers dependency analysis and probability distribution of words sequences .", "entities": []}, {"text": "This approach takes part of a question / answering system in order to help generate a user - friendly answer rather than a short one .", "entities": []}, {"text": "The results obtained through a human evaluation and standard metrics tested over French and English questions are very promising and shows a good correlation with human judgement .", "entities": []}, {"text": "However , we intend to put more emphasis on the Language Model choice as reported by the human study and consider the generation of more than one missing word within the answer .", "entities": []}, {"text": "References Eugene Agichtein and Luis Gravano .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "Snowball :", "entities": []}, {"text": "Extracting relations from large plain - text collections .", "entities": []}, {"text": "InProceedings of the \ufb01fth ACM conference on Digital libraries , pages 85\u201394 .", "entities": [[4, 5, "DatasetName", "ACM"]]}, {"text": "Akari Asai , Akiko Eriguchi , Kazuma Hashimoto , and Yoshimasa Tsuruoka .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Multilingual extractive reading comprehension by runtime machine translation .", "entities": [[2, 4, "TaskName", "reading comprehension"], [6, 8, "TaskName", "machine translation"]]}, {"text": "https://arxiv.org/abs/1809.03275 .", "entities": []}, {"text": "Jacob Benesty , Jingdong Chen , Yiteng Huang , and Israel Cohen . 2009 .", "entities": []}, {"text": "Pearson Correlation Coef\ufb01cient , pages 1\u20134 .", "entities": [[0, 2, "MetricName", "Pearson Correlation"]]}, {"text": "Springer Berlin Heidelberg , Berlin , Heidelberg .", "entities": []}, {"text": "Pinaki Bhaskar , Somnath Banerjee , Partha Pakray , Samadrita Banerjee , Sivaji Bandyopadhyay , and Alexander Gelbukh .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "A hybrid question answering system for Multiple Choice Question", "entities": [[2, 4, "TaskName", "question answering"]]}, {"text": "358(MCQ )", "entities": []}, {"text": ".", "entities": []}, {"text": "In Question Answering for Machine Reading Evaluation ( QA4MRE ) at CLEF 2013 .", "entities": [[1, 3, "TaskName", "Question Answering"]]}, {"text": "Eric Brill , Susan Dumais , and Michele Banko .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "An analysis of the AskMSR question - answering system .", "entities": []}, {"text": "In EMNLP 2002 , pages 257\u2013264 .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Eric Brill , Jimmy Lin , Michele Banko , Susan Dumais , and Andrew Ng . 2001 .", "entities": []}, {"text": "Data - intensive question answering .", "entities": [[3, 5, "TaskName", "question answering"]]}, {"text": "In TREC 2001 , pages 393\u2013400 .", "entities": [[1, 2, "DatasetName", "TREC"]]}, {"text": "Sumit Chopra , Michael Auli , and Alexander M. Rush .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Abstractive sentence summarization with attentive recurrent neural networks .", "entities": [[1, 3, "TaskName", "sentence summarization"]]}, {"text": "In NAACL 2016 , pages 93\u201398 .", "entities": []}, {"text": "Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , Francisco Guzm \u00b4 an , Edouard Grace , Myle Ott , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unsupervised Cross - lingual Representation Learning at Scale .", "entities": [[4, 6, "TaskName", "Representation Learning"]]}, {"text": "https://arxiv.org/abs/1911.02116 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In NAACL , pages 4171\u20134186 , Minneapolis .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Timothy Dozat , Peng Qi , and Christopher D. Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Stanford \u2019s Graph - based Neural Dependency Parser at the CoNLL 2017 Shared Task .", "entities": []}, {"text": "In CoNLL 2017 Shared Task .", "entities": []}, {"text": "Multilingual Parsing from Raw Text to Universal Dependencies , pages 20\u201330 , Vancouver , Canada .", "entities": [[6, 8, "DatasetName", "Universal Dependencies"]]}, {"text": "ACL .", "entities": []}, {"text": "Xinya Du and Claire Cardie .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Harvesting paragraph - level question - answer pairs from Wikipedia .", "entities": []}, {"text": "In ACL 2018 , pages 1907\u20131917 , Melbourne , Australia .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Sanjay K. Dwivedi and Vaishali Singh .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Research and reviews in question answering system .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "Procedia Technology , 10:417\u2013424 .", "entities": []}, {"text": "Roxana Girju .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Automatic detection of causal relations for question answering .", "entities": [[6, 8, "TaskName", "question answering"]]}, {"text": "In ACL 2003 , pages 76\u201383 .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Johannes Heinecke .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Hybrid enhanced Universal Dependencies parsing .", "entities": [[2, 4, "DatasetName", "Universal Dependencies"]]}, {"text": "InInternational Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies , pages 174\u2013180 , Online .", "entities": [[15, 17, "DatasetName", "Universal Dependencies"]]}, {"text": "ACL .", "entities": []}, {"text": "Lynette Hirschman and Robert Gaizauskas .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Natural language question answering : the view from here .", "entities": [[2, 4, "TaskName", "question answering"]]}, {"text": "natural language engineering , 7(4):275\u2013300 .", "entities": []}, {"text": "Ryu Iida , Canasai Kruengkrai , Ryo Ishida , Kentaro Torisawa , Jong - Hoon Oh , and Julien Kloetzer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Exploiting background knowledge in compact answer generation for why - questions .", "entities": [[5, 7, "TaskName", "answer generation"]]}, {"text": "In AAI Conference on Arti\ufb01cial Intelligence , volume 33 , pages 142\u2013151.Ryo Ishida , Kentaro Torisawa , Jong - Hoon Oh , Ryu Iida , Canasai Kruengkrai , and Julien Kloetzer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Semi - distantly supervised neural model for generating compact answers to open - domain why questions .", "entities": []}, {"text": "In32nd", "entities": []}, {"text": "AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Canasai Kruengkrai , Kentaro Torisawa , Chikara Hashimoto , Julien Kloetzer , Jong - Hoon Oh , and Masahiro Tanaka . 2017 .", "entities": []}, {"text": "Improving event causality recognition with multiple background knowledge sources using multi - column convolutional neural networks .", "entities": []}, {"text": "In 31st AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Steve Lawrence and C. Lee Giles .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "Context and page analysis for improved web search .", "entities": []}, {"text": "IEEE Internet computing , 2(4):38\u201346 .", "entities": []}, {"text": "Hang Le , Lo \u00a8\u0131c Vial , Jibril Frej , Vincent Segonne , Maximin Coavoux , Benjamin Lecouteux , Alexandre Allauzen , Beno \u02c6\u0131t Crabb \u00b4 e , Laurent Besacier , and Didier Schwab .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "FlauBERT :", "entities": []}, {"text": "Unsupervised Language Model Pre - training for French .", "entities": []}, {"text": "In LREC 2020 .", "entities": []}, {"text": "Juan Le , Chunxia Zhang , and Zhendong Niu . 2016 .", "entities": []}, {"text": "Answer extraction based on merging score strategy of hot terms .", "entities": []}, {"text": "Chinese Journal of Electronics , 25(4):614\u2013620 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Mandar Du , Jingfei adn Joshi , Danqi Chen , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "RoBERTa : A Robustly Optimized BERT Pretraining Approach .", "entities": [[0, 1, "MethodName", "RoBERTa"], [5, 6, "MethodName", "BERT"]]}, {"text": "https://arxiv.org/abs/1907.11692 .", "entities": []}, {"text": "Vanessa Lopez , Victoria Uren , Marta Sabou , and Enrico Motta . 2011 .", "entities": []}, {"text": "Is question answering \ufb01t for the semantic web ? : a survey .", "entities": [[1, 3, "TaskName", "question answering"]]}, {"text": "Semantic Web , 2(2):125 \u2013 155 .", "entities": []}, {"text": "Louis Martin , Benjamin Muller , Pedro Javier Ortiz Su\u00b4arez , Yoann Dupont , Laurent Romary , \u00b4 Eric Villemonte de la Clergerie , Djam \u00b4 e Seddah , and Beno \u02c6\u0131t Sagot .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "CamemBERT : a Tasty French Language Model .", "entities": []}, {"text": "https://arxiv.org/abs/1911.03894 .", "entities": []}, {"text": "Yishu Miao and Phil Blunsom .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Language as a latent variable : Discrete generative models for sentence compression .", "entities": [[10, 12, "DatasetName", "sentence compression"]]}, {"text": "EMNLP 2016 .", "entities": []}, {"text": "Friedman Milton .", "entities": []}, {"text": "1939 .", "entities": []}, {"text": "A correction : The use of ranks to avoid the assumption of normality implicit in the analysis of variance .", "entities": []}, {"text": "Journal of the American Statistical Association , 34(205):109 .", "entities": []}, {"text": "Ramesh Nallapati , Bowen Zhou , Cicero dos Santos , C \u00b8 a\u02d8glar", "entities": []}, {"text": "G \u02d9ulc", "entities": []}, {"text": "\u00b8ehre , Bing Xiang , et al . 2016 .", "entities": []}, {"text": "Abstractive text summarization using sequence - to - sequence RNNs and beyond .", "entities": [[0, 3, "TaskName", "Abstractive text summarization"]]}, {"text": "In CoNLL 2016 , pages 280\u2013290 .", "entities": []}, {"text": "Joakim Nivre and Chiao - Ting Fang . 2017 .", "entities": []}, {"text": "Universal Dependency Evaluation .", "entities": []}, {"text": "In NoDaLiDa 2017 Workshop on Universal Dependencies , pages 86\u201395 , G\u00a8oteborg .", "entities": [[5, 7, "DatasetName", "Universal Dependencies"]]}, {"text": "359Joakim Nivre , Marie - Catherine de Marneffe , Filip Ginter , Yoav Goldberg , Yoav Goldberg , Jan Haji \u02c7c , Manning Christopher D. , Ryan McDonald , Slav Petrov , Sampo Pyysalo , Natalia Silveira , Reut Tsarfaty , and Daniel Zeman .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Universal Dependencies v1 : A Multilingual Treebank Collection .", "entities": [[0, 2, "DatasetName", "Universal Dependencies"]]}, {"text": "In 10th LREC , pages 23\u201338 , Portoro \u02c7z , Slovenia .", "entities": []}, {"text": "ELRA .", "entities": []}, {"text": "Jong - Hoon Oh , Kentaro Torisawa , Chikara Hashimoto , Ryu Iida , Masahiro Tanaka , and Julien Kloetzer .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A semi - supervised learning approach to whyquestion answering .", "entities": []}, {"text": "In Thirtieth AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Jong - Hoon Oh , Kentaro Torisawa , Chikara Hashimoto , Motoki Sano , Stijn De Saeger , and Kiyonori Ohtake .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Why - question answering using intra - and inter - sentential causal relations .", "entities": [[2, 4, "TaskName", "question answering"]]}, {"text": "In ACL 2013 , pages 1733\u20131743 .", "entities": []}, {"text": "Vaishali Pal , Manish Shrivastava , and Irshad Bhat . 2019 .", "entities": []}, {"text": "Answering naturally : Factoid to full length answer generation .", "entities": [[7, 9, "TaskName", "answer generation"]]}, {"text": "In 2nd Workshop on New Frontiers in Summarization , pages 1\u20139 , Hong Kong , China . Association for Computational Linguistics .", "entities": [[7, 8, "TaskName", "Summarization"]]}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training .", "entities": []}, {"text": "https://cdn.openai .", "entities": []}, {"text": "com / research - covers / language - unsupervised/ language understanding paper.pdf .", "entities": []}, {"text": "Lina M. Rojas Barahona , Pascal Bellec , Beno \u02c6\u0131t Besset , Martinho Dos Santos , Johannes Heinecke , Munshi Asadullah , Olivier Leblouch , Jean - Yves Lancien , G\u00b4eraldine Damnati , Emmanuel Mory , and Fr \u00b4 ed\u00b4eric Herl\u00b4edan . 2019 .", "entities": []}, {"text": "Spoken Conversational Search for General Knowledge .", "entities": [[1, 3, "TaskName", "Conversational Search"], [4, 6, "TaskName", "General Knowledge"]]}, {"text": "In SIGdial Meeting on Discourse and Dialogue , pages 110\u2013113 , Stockholm .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Alexander M Rush , Sumit Chopra , and Jason Weston .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A neural attention model for abstractive sentence summarization .", "entities": [[6, 8, "TaskName", "sentence summarization"]]}, {"text": "https://arxiv.org/abs/1509 . 00685 .", "entities": []}, {"text": "Cicero dos Santos , Ming Tan , Bing Xiang , and Bowen Zhou .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Attentive pooling networks .", "entities": []}, {"text": "https:// arxiv.org/abs/1602.03609 .", "entities": []}, {"text": "Djam \u00b4 e Seddah and Marie Candito .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Hard Time Parsing Questions : Building a QuestionBank for French .", "entities": []}, {"text": "In10th", "entities": []}, {"text": "LREC , Portoro \u02c7z , Slovenia .", "entities": []}, {"text": "ELRA .", "entities": []}, {"text": "Abigail See , Peter J. Liu , and Christopher D. Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Get to the point : Summarization withpointer - generator networks .", "entities": [[5, 6, "TaskName", "Summarization"]]}, {"text": "https://arxiv.org/abs/", "entities": []}, {"text": "1704.04368 .", "entities": []}, {"text": "Rebecca Sharp , Mihai Surdeanu , Peter Jansen , Peter Clark , and Michael Hammond .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Creating causal embeddings for question answering with minimal supervision .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "EMNLP 2016 .", "entities": []}, {"text": "Milan Straka .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task .", "entities": [[6, 7, "DatasetName", "UD"]]}, {"text": "In CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies , pages 197\u2013207 , Brussels .", "entities": [[12, 14, "DatasetName", "Universal Dependencies"]]}, {"text": "ACL .", "entities": []}, {"text": "Ming Tan , Cicero dos Santos , Bing Xiang , and Bowen Zhou .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Improved representation learning for question answer matching .", "entities": [[1, 3, "TaskName", "representation learning"]]}, {"text": "In ACL 2016 , pages 464 \u2013 473 .", "entities": []}, {"text": "Ricardo Usbeck , Axel - Cyrille Ngonga Ngomo , Bastian Haarmann , Anastasia Krithara , Michael R \u00a8oder , and Giulio Napolitano .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "7th Open Challenge on Question Answering over Linked Data ( QALD-7 ) .", "entities": [[4, 6, "TaskName", "Question Answering"]]}, {"text": "InSemantic Web Challenges , pages 59\u201369 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Suzan Verberne , Hans van Halteren , Daphne Theijssen , Stephan Raaijmakers , and Lou Boves .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Learning to rank for why - question answering .", "entities": [[6, 8, "TaskName", "question answering"]]}, {"text": "Information Retrieval , 14(2):107\u2013132 .", "entities": [[0, 2, "TaskName", "Information Retrieval"]]}, {"text": "Shuohang Wang and Jing Jiang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Machine comprehension using match - lstm and answer pointer .", "entities": [[5, 6, "MethodName", "lstm"]]}, {"text": "https://arxiv.org/abs/1608.07905 .", "entities": []}, {"text": "Tong Wang , Xingdi Yuan , and Adam Trischler . 2017 .", "entities": [[7, 8, "MethodName", "Adam"]]}, {"text": "A joint model for question answering and question generation .", "entities": [[4, 6, "TaskName", "question answering"], [7, 9, "TaskName", "question generation"]]}, {"text": "https://arxiv.org/abs/1706.01450 .", "entities": []}, {"text": "Min Wu , Xiaoyu Zheng , Michelle Duan , Ting Liu , Tomek Strzalkowski , and S Albany .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Question answering by pattern matching , web - proo\ufb01ng , semantic form proo\ufb01ng .", "entities": [[0, 2, "TaskName", "Question answering"]]}, {"text": "In TREC 2003 , pages 500 \u2013 255 .", "entities": [[1, 2, "DatasetName", "TREC"]]}, {"text": "Godandapani Zayaraz et al . 2015 .", "entities": []}, {"text": "Concept relation extraction using na \u00a8\u0131ve bayes classi\ufb01er for ontologybased question answering systems .", "entities": [[1, 3, "TaskName", "relation extraction"], [10, 12, "TaskName", "question answering"]]}, {"text": "Journal of King Saud University - Computer and Information Sciences , 27(1):13\u201324 .", "entities": []}, {"text": "Daniel Zeman , Jan Haji \u02c7c , Martin Popel , Martin Potthast , Milan Straka , Filip Ginter , Joakim Nivre , and Slav Petrov .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies .", "entities": [[11, 13, "DatasetName", "Universal Dependencies"]]}, {"text": "InCoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies , pages 1\u201321 , Brussels .", "entities": [[11, 13, "DatasetName", "Universal Dependencies"]]}, {"text": "ACL .", "entities": []}]