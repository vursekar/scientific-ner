[{"text": "Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP , pages 62\u201366 , Berlin , Germany , August 12 , 2016 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2016 Association for Computational Linguistics Evaluation of acoustic word embeddings Sahar Ghannay , Yannick Est ` eve , Nathalie Camelin , Paul del \u00b4 eglise LIUM - University of Le Mans , France firstname.lastname@univ-lemans.fr", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "Abstract", "entities": []}, {"text": "Recently , researchers in speech recognition have started to reconsider using whole words as the basic modeling unit , instead of phonetic units .", "entities": [[4, 6, "TaskName", "speech recognition"]]}, {"text": "These systems rely on a function that embeds an arbitrary or \ufb01xed dimensional speech segments to a vector in a \ufb01xed - dimensional space , named acoustic word embedding .", "entities": []}, {"text": "Thus , speech segments of words that sound similarly will be projected in a close area in a continuous space .", "entities": []}, {"text": "This paper focuses on the evaluation of acoustic word embeddings .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "We propose two approaches to evaluate the intrinsic performances of acoustic word embeddings in comparison to orthographic representations in order to evaluate whether they capture discriminative phonetic information .", "entities": [[11, 13, "TaskName", "word embeddings"]]}, {"text": "Since French language is targeted in experiments , a particular focus is made on homophone words .", "entities": []}, {"text": "1 Introduction Recent studies have started to reconsider the use of whole words as the basic modeling unit in speech recognition and query applications , instead of phonetic units .", "entities": [[19, 21, "TaskName", "speech recognition"]]}, {"text": "These systems are based on the use of acoustic word embedding , which are projection of arbitrary or \ufb01xed dimensional speech segments into a continuous space , in a manner that preserve acoustic similarity between words .", "entities": []}, {"text": "Thus , speech segments of words that sound similarly will have similar embeddings .", "entities": []}, {"text": "Acoustic word embedding were successfully used in a queryby - example search system ( Kamper et al . , 2015 ; Levin et al . , 2013 ) and in a ASR lattice re - scoring system ( Bengio and Heigold , 2014 ) .", "entities": []}, {"text": "The authors in ( Bengio and Heigold , 2014 ) proposed an approach to build acoustic word em - beddings from an orthographic representation of the word .", "entities": []}, {"text": "This paper focuses on the evaluation of these acoustic word embeddings .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "We propose two approaches to evaluate the intrinsic performances of acoustic word embeddings in comparison to orthographic representations .", "entities": [[11, 13, "TaskName", "word embeddings"]]}, {"text": "In particular we want to evaluate whether they capture discriminative information about their pronunciation , approximated by their phonetic representation .", "entities": []}, {"text": "In our experiments , we focus on French language whose particularity is to be rich of homophone words .", "entities": []}, {"text": "This aspect is also studied in this work .", "entities": []}, {"text": "2 Acoustic word embeddings 2.1 Building acoustic word embeddings The approach we used to build acoustic word embeddings is inspired from the one proposed in ( Bengio and Heigold , 2014 ) .", "entities": [[2, 4, "TaskName", "word embeddings"], [7, 9, "TaskName", "word embeddings"], [16, 18, "TaskName", "word embeddings"]]}, {"text": "The deep neural architecture depicted in \ufb01gure 1 is used to train the acoustic word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "It relies on a convolutional neural network ( CNN ) classi\ufb01er over words and on a deep neural network ( DNN ) trained by using a triplet ranking loss ( Bengio and Heigold , 2014 ; Wang et al . , 2014 ; Weston et al . , 2011 ) .", "entities": [[28, 29, "MetricName", "loss"]]}, {"text": "The two architectures are trained using different inputs : speech signal and orthographic representation of the word , which are detailed as follows .", "entities": []}, {"text": "The convolutional neural network classi\ufb01er is trained independently to predict a word given a speech signal as input .", "entities": []}, {"text": "It is composed of convolution and pooling layers , followed by fully connected layers which feed the \ufb01nal softmax layer .", "entities": [[4, 5, "MethodName", "convolution"], [18, 19, "MethodName", "softmax"]]}, {"text": "The embedding layer is the fully connected layer just below the softmax one , named sin the \ufb01gure 1 .", "entities": [[11, 12, "MethodName", "softmax"]]}, {"text": "This representation contains a compact representation of the acoustic signal .", "entities": []}, {"text": "It tends to preserve acoustic similarity between words , such that words are close in this space if they sound alike .", "entities": []}, {"text": "The feedforward neural network ( DNN ) is used62", "entities": []}, {"text": "convolution and max pooling layersfully connected layersTriplet Ranking LossDNNCNNEmbedding w+ O+Softmax   O - Embedding w - Embedding s Lookup tableWord Letter n - gramsWrong word Letter n - grams .................................... Figure 1 : Deep architecture used to train acoustic word embeddings .", "entities": [[0, 1, "MethodName", "convolution"], [2, 4, "MethodName", "max pooling"], [21, 22, "DatasetName", "Letter"], [26, 27, "DatasetName", "Letter"], [40, 42, "TaskName", "word embeddings"]]}, {"text": "with the purpose to build an acoustic word embedding for a word not observed in the audio training corpus , based on its orthographic representation .", "entities": []}, {"text": "It is trained using the triplet ranking loss function in order to project orthographic word representations to the same space as the acoustic embeddings s.", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "The orthographic word representation consists in a bag of n - grams ( n\u22643 ) of letters , with additional special symbols", "entities": []}, {"text": "[ and ] to specify the start and the end of a word .", "entities": []}, {"text": "The size of this bag of ngrams vector is reduced using an auto - encoder .", "entities": []}, {"text": "During the training process , this model takes as inputs acoustic embeddings sselected randomly from the training set and , for each signal acoustic embedding , the orthographic representation of the matching word o+ , and the orthographic representation of a randomly selected word different to the \ufb01rst word o\u2212. These two orthographic representations supply shared parameters in the DNN .", "entities": []}, {"text": "The resulting DNN model can then be used to build an acoustic word embedding ( w+ ) from any word , as long as one can extract an orthographic representation from it .", "entities": []}, {"text": "This acoustic word embedding can be perceived as a canonical acoustic representation for a word , since different pronunciations imply different signal embeddings s. 2.2 Evaluation In the literature ( Kamper et al . , 2015 ; Levin et al . , 2013 ; Carlin et al . , 2011 ) , a word discrimination task was used to evaluate acoustic embeddings s. Given a pair of acoustic segments , this task consists on deciding whether the segments correspond to the same words or not .", "entities": []}, {"text": "This evalua - tion task can be performed on many ways , for example through the use of a dynamic time warping ( DTW ) to quantify the similarity between two segments when using frame level embeddings ( Thiolliere et al . , 2015 ) , or by using the euclidean distance or the cosine similarity between embeddings representing the segments .", "entities": [[19, 22, "MethodName", "dynamic time warping"], [23, 24, "MethodName", "DTW"]]}, {"text": "In ( Kamper et al . , 2015 ) the evaluation was conducted on two collections of words ( train and test ) coming from the Switchboard English corpus .", "entities": []}, {"text": "After training the model on the training corpus , the cosine similarity is computed between the embeddings of each pair of words in the test set .", "entities": []}, {"text": "These pairs are classi\ufb01ed as similar or different by applying a threshold on their distance , and a precisionrecall curve is obtained by varying the threshold .", "entities": []}, {"text": "In this study , we propose two approaches to evaluate acoustic word embeddings w+ .", "entities": [[11, 13, "TaskName", "word embeddings"]]}, {"text": "We suggest to build different evaluation sets in order to assess the acoustic word embeddings ( w+ ) performances on orthographic andphonetic similarity andhomophones detection tasks .", "entities": [[13, 15, "TaskName", "word embeddings"]]}, {"text": "We remind that the acoustic word embedding w+is a projection of an orthographic word representation o+into the space of acoustic signal embeddings s.", "entities": []}, {"text": "In our evaluation , we would like to measure the loss of orthographic information carried by w+and the potential gain of acoustic information due to this projection , in comparison to the information carried byo+ .", "entities": [[10, 11, "MetricName", "loss"]]}, {"text": "The evaluation sets are built as follows : given a listLofnfrequent words ( candidate words ) in the vocabulary composed of mwords , a list of n\u00d7mword pairs was created .", "entities": []}, {"text": "Then , two alignments were performed between each word pair based on their orthographic ( letters ) and phonetic ( phonemes ) representations , using the sclite1tool .", "entities": []}, {"text": "From these alignment two edition distances are computed with respect to the alignment results of orthographic and phonetic representations .", "entities": []}, {"text": "The Edition distance is computed as follows : SER = # In+ # Sub + # Del # symbols in the reference word\u00d7100 ( 1 ) where SER stands for Symbol Error rate , symbols correspond to the letters for orthographic representations , and to the phonemes for phonetic ones , and In , Sub and Del correspond respectively to insertion , substitution and deletion .", "entities": [[31, 32, "MetricName", "Error"]]}, {"text": "1http://www.icsi.berkeley.edu/Speech/docs/sctk1.2/sclite.htm63", "entities": []}, {"text": "Next , we compute two similarity scores that correspond to the orthographic and phonetic similarity scores simscore attributed for each pair of words , which are de\ufb01ned as : simscore = 10\u2212min(10,SER/ 10 ) ( 2 ) where min ( ) is a function used to have an edition distance between 0 and 10 .", "entities": [[51, 52, "DatasetName", "0"]]}, {"text": "Then , for each candidate word in the list Lwe extract its orthographically and phonetically 10 nearest words .", "entities": []}, {"text": "This results in two lists for orthographic andphonetic similarity tasks .", "entities": []}, {"text": "For each candidate word in the listL , the Orthographic list contains its ten closest words in terms of orthographic similarity scores and the Phonetic list contains its ten closest words in terms of phonetic similarity scores .", "entities": []}, {"text": "Finally , the Homophones list , used for the homophone detection task , contains the homophone words ( i.e. sharing the same phonetic representation ) .", "entities": []}, {"text": "Table 1 shows an example of the content of the three lists .", "entities": []}, {"text": "List Exampls Orthographictr`es pr ` es 7.5 tr`es ors 5 Phononetictr`es frais 6.67 tr`es tra \u02c6\u0131nent", "entities": []}, {"text": "6.67 Homophonetr`es traie tr`es traient Table 1 : Example of the content of the three lists .", "entities": []}, {"text": "In the case of the orthographic and phonetic similarity tasks , the evaluation of the acoustic embeddings is performed by ranking the pairs according to their cosine similarities and measuring the Spearman \u2019s rank correlation coef\ufb01cient ( Spearman \u2019s \u03c1 ) .", "entities": []}, {"text": "This approach is used in ( Gao et al . , 2014 ;", "entities": []}, {"text": "Ji et al . , 2015 ; Levy et al . , 2015 ; Ghannay et al . , 2016 ) to evaluate the linguistic word embeddings on similarity tasks , in which the similarity scores are attributed by human annotators .", "entities": [[25, 27, "TaskName", "word embeddings"]]}, {"text": "For the homophone detection task , the evaluation is performed in terms of precision .", "entities": []}, {"text": "For each wordwin the Homophones list , let LH(w ) be the list of khomophones of the word w , and LHneighbour ( w)be the list of knearest neighbours extracted based on the cosine similarity and LHfound ( w)be the intersection between LH(w ) andLHneighbour ( w ) , that corresponds to the list of homophones found of the word", "entities": []}, {"text": "w.", "entities": []}, {"text": "The precision Pwof the wordwis de\ufb01ned as : Pw=|LHfound ( w)| |LH(w)|(3 ) where|.|refers to the size of a list .", "entities": []}, {"text": "We de\ufb01ne the overall homophone detection precision on the Homophones list as the average of the Pw : P=/summationtextN i=1Pwi N(4 )", "entities": []}, {"text": "whereNis", "entities": []}, {"text": "the number of candidate words which have a none - empty Homophones list .", "entities": []}, {"text": "3 Experiments on acoustic word embeddings 3.1 Experimental setup The training set for the CNN consists of 488hours of French Broadcast News with manual transcriptions .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "This dataset is composed of data coming from the ESTER1 ( Galliano et al . , 2005 ) , ESTER2 ( Galliano et al . , 2009 ) and EPAC ( Est ` eve et al . , 2010 ) corpora .", "entities": []}, {"text": "It contains 52kunique words that have been seen at least twice each in the corpus .", "entities": []}, {"text": "All of them corresponds to a total of 5.75millions occurrences .", "entities": []}, {"text": "In French language , many words have the same pronunciation without sharing the same spelling , and they can have different meanings ; e.g. the sound", "entities": []}, {"text": "[ so ] corresponds to four homophones : sot(fool ) , saut ( jump ) , sceau ( seal ) and seau ( bucket ) , and twice more by taking into account their plural forms that have the same pronunciation : sots , sauts , sceaux , and seaux .", "entities": []}, {"text": "When a CNN is trained to predict a word given an acoustic sequence , these frequent homophones can introduce a bias to evaluate the recognition error .", "entities": []}, {"text": "To avoid this , we merged all the homophones existing among the 52kunique words of the training corpus .", "entities": []}, {"text": "As a result , we obtained a new reduced dictionary containing 45kwords and classes of homophones .", "entities": []}, {"text": "Acoustic features provided to the CNN are log\ufb01lterbanks , computed every 10ms over a 25ms window yielding a 23 - dimension vector for each frame .", "entities": []}, {"text": "A forced alignment between manual transcriptions and speech signal was performed on the training set in order to detect word boundaries .", "entities": []}, {"text": "The statistics computed from this alignment reveal that 99 % of words are shorter than 1 second .", "entities": []}, {"text": "Hence we decided to represent each word by 100 frames , thus , by a vector of 2300 dimensions.64", "entities": []}, {"text": "When words are shorter they are padded with zero equally on both ends , while longer words are cut equally on both ends .", "entities": []}, {"text": "The CNN and DNN deep architectures are trained on 90 % of the training set and the remaining 10 % are used for validation .", "entities": []}, {"text": "3.2 Acoustic word embeddings evaluation The embeddings we evaluate are built from two different vocabularies : the one used to train the neural network models ( CNN and DNN ) , composed of 52kwords present in the manual transcriptions of the 488hours of audio ; and another one composed of 160kwords .", "entities": [[2, 4, "TaskName", "word embeddings"]]}, {"text": "The words present in the 52kvocabulary are nearly all present in the 160kvocabulary .", "entities": []}, {"text": "The evaluation sets described in section 2.2 are generated from these two vocabularies : in the 52k vocabulary , all the acoustic word embeddings w+ are related to words which have been observed during the training of the CNN .", "entities": [[22, 24, "TaskName", "word embeddings"]]}, {"text": "This means that at least two acoustic signal embeddings have been computed from the audio for each one of these words ; in the 160kvocabulary , about 110kacoustic word embeddings were computed for words never observed in the audio data .", "entities": [[28, 30, "TaskName", "word embeddings"]]}, {"text": "3.2.1 Quantitative Evaluation The quantitative evaluation of the acoustic word embeddings w+is performed on orthographic similarity , phonetic similarity , and homophones detection tasks .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "Results are summarized in table 2 . 52 K Vocab .", "entities": []}, {"text": "160 K Vocab .", "entities": []}, {"text": "Task o+w+o+w+ Orthographic 54.28 49.97 56.95 51.06 Phonetic 40.40 43.55 41.41 46.88 Homophone 64.65 72.28 52.87 59.33 Table 2 : Evaluation results of similarity ( \u03c1\u00d7100 ) and homophone detection tasks ( precision ) .", "entities": []}, {"text": "They show that the acoustic word embeddings w+are more relevant for the phonetic similarity task , while o+are obviously the best ones on the orthographic similarity task .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "These results show that the projection of the orthographic embeddings o+into the acoustic embeddings space schanges their properties , since they have captured more information about word pronunciation while they have lost informationabout spelling .", "entities": []}, {"text": "So , in addition to making possible a measure of similarity distance between the acoustic signal ( represented by s ) and a word ( represented by w+ ) , acoustic word embeddings are better than orthographic ones to measure the phonetic proximity between two words .", "entities": [[31, 33, "TaskName", "word embeddings"]]}, {"text": "For the homophone detection task , the Homophones list is computed from the 160kvocabulary : that results to 53869 homophone pairs in total .", "entities": []}, {"text": "The 52kvocabulary contains 13561 homophone pairs which are included in the pairs present in the 160kvocabulary .", "entities": []}, {"text": "As we can see , the w+ acoustic embeddings outperform the orthographic ones on this task on the two data sets .", "entities": []}, {"text": "This con\ufb01rms that acoustic word embeddings have captured additional information about word pronunciation than the one carried by orthographic word embeddings .", "entities": [[4, 6, "TaskName", "word embeddings"], [19, 21, "TaskName", "word embeddings"]]}, {"text": "For this task we can not compare the results between the two vocabularies , since the precision measure is dependent to the number of events .", "entities": []}, {"text": "For the Spearman \u2019s correlation , a comparison is roughly possible and results show that the way to compute w+is effective to generalize this computation to word not observed in the audio training data .", "entities": []}, {"text": "3.2.2 Qualitative Evaluation To give more insight into the difference of the quality of the orthographic word embeddings o+ and the acoustic ones w+ , we propose an empirical comparison by showing the nearest neighbours of a given set of words .", "entities": [[16, 18, "TaskName", "word embeddings"]]}, {"text": "Table 3 shows examples of such neighbour .", "entities": []}, {"text": "It can be seen that , as expected , neighbour of any given word share the same spelling with it when they are induced by the orthographic embeddings and arguably sound like it when they are induced by the acoustic word ones .", "entities": []}, {"text": "Candidate wordo+w+ grecs i - grec , rec , marecgrec , grecque , grecques ail aile , trail , fail aille , ailles , aile arts parts , charts , encartsarte , art , ars blocs bloch , blocher , blochebloc , bloque , bloquent Table 3 : Candidate words and their nearest neighbours65", "entities": []}, {"text": "4 Conclusion In this paper , we have investigated the intrinsic evaluation of acoustic word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "These latter offer the opportunity of an a priori acoustic representation of words that can be compared , in terms of similarity , to an embedded representation of the audio signal .", "entities": []}, {"text": "We have proposed two approaches to evaluate the performances of these acoustic word embeddings and compare them to their orthographic embeddings : orthographic and phonetic performance by ranking pairs and measuring the Spearman \u2019s rank correlation coef\ufb01cient ( Spearman \u2019s \u03c1 ) , and by measuring the precision in a homophone detection task .", "entities": [[12, 14, "TaskName", "word embeddings"]]}, {"text": "Experiments show that the acoustic word embeddings are better than orthographic ones to measure the phonetic proximity between two words .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "More , they are better too on homophone detection task .", "entities": []}, {"text": "This con\ufb01rms that acoustic word embeddings have captured additional information about word pronunciation .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "Acknowledgments This work was partially funded by the European Commission through the EUMSSI project , under the contract number 611057 , in the framework of the FP7 - ICT-2013 - 10 call , by the French National Research Agency ( ANR ) through the VERA project , under the contract number ANR12 - BS02 - 006 - 01 , and by the R \u00b4 egion Pays de la Loire .", "entities": []}, {"text": "References Samy Bengio and Georg Heigold .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Word embeddings for speech recognition .", "entities": [[0, 2, "TaskName", "Word embeddings"], [3, 5, "TaskName", "speech recognition"]]}, {"text": "In INTERSPEECH , pages 1053\u20131057 .", "entities": []}, {"text": "Michael A Carlin , Samuel Thomas , Aren Jansen , and Hynek Hermansky .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Rapid Evaluation of Speech Representations for Spoken Term Discovery .", "entities": []}, {"text": "InINTERSPEECH , pages 821\u2013824 .", "entities": []}, {"text": "Yannick Est ` eve , Thierry Bazillon , Jean - Yves Antoine , Fr\u00b4ed\u00b4eric B \u00b4 echet , and J \u00b4 er\u02c6ome Farinas .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "The EPAC Corpus : Manual and Automatic Annotations of Conversational Speech in French Broadcast News .", "entities": []}, {"text": "In LREC , Malta , 17 - 23 may 2010 .", "entities": []}, {"text": "Sylvain Galliano , Edouard Geoffrois , Djamel Mostefa , Khalid Choukri , Jean - Franc \u00b8ois Bonastre , and Guillaume Gravier .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "The ESTER phase II evaluation campaign for the rich transcription of French Broadcast News .", "entities": []}, {"text": "In Interspeech , pages 1149\u20131152.Sylvain Galliano , Guillaume Gravier , and Laura Chaubard .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "The ESTER 2 evaluation campaign for the rich transcription of French radio broadcasts .", "entities": []}, {"text": "In Interspeech , volume 9 , pages 2583 \u2013 2586 .", "entities": []}, {"text": "Bin Gao , Jiang Bian , and Tie - Yan Liu . 2014 .", "entities": []}, {"text": "Wordrep : A benchmark for research on learning word representations .", "entities": []}, {"text": "CoRR , abs/1407.1640 .", "entities": []}, {"text": "Sahar Ghannay , Benoit Favre , Yannick Est ` eve , and Nathalie Camelin . 2016 .", "entities": []}, {"text": "Word embedding evaluation and combination .", "entities": []}, {"text": "In 10th edition of the Language Resources and Evaluation Conference ( LREC 2016 ) , Portoro \u02c7z ( Slovenia ) , 23 - 28 May .", "entities": []}, {"text": "Shihao Ji , Hyokun Yun , Pinar Yanardag , Shin Matsushima , and S. V .", "entities": []}, {"text": "N. Vishwanathan .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Wordrank :", "entities": []}, {"text": "Learning word embeddings via robust ranking .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "CoRR , abs/1506.02761 .", "entities": []}, {"text": "Herman Kamper , Weiran Wang , and Karen Livescu . 2015 .", "entities": []}, {"text": "Deep convolutional acoustic word embeddings using word - pair side information .", "entities": [[3, 5, "TaskName", "word embeddings"]]}, {"text": "In arXiv preprint arXiv:1510.01032 .", "entities": [[1, 2, "DatasetName", "arXiv"]]}, {"text": "Keith Levin , Katharine Henry , Anton Jansen , and Karen Livescu .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Fixed - dimensional acoustic embeddings of variable - length segments in lowresource settings .", "entities": []}, {"text": "In Automatic Speech Recognition and Understanding ( ASRU ) , 2013 IEEE Workshop on , pages 410\u2013415 .", "entities": [[1, 4, "TaskName", "Automatic Speech Recognition"]]}, {"text": "IEEE .", "entities": []}, {"text": "Omer Levy , Yoav Goldberg , and Ido Dagan . 2015 .", "entities": []}, {"text": "Improving distributional similarity with lessons learned from word embeddings .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "Transactions of the Association for Computational Linguistics , 3:211\u2013225 .", "entities": []}, {"text": "Roland Thiolliere , Ewan Dunbar , Gabriel Synnaeve , Maarten Versteegh , and Emmanuel Dupoux . 2015 .", "entities": []}, {"text": "A hybrid dynamic time warping - deep neural network architecture for unsupervised acoustic modeling .", "entities": [[2, 5, "MethodName", "dynamic time warping"]]}, {"text": "In Proc .", "entities": []}, {"text": "Interspeech .", "entities": []}, {"text": "Jiang Wang , Yang Song , Thomas Leung , Chuck Rosenberg , Jingbin Wang , James Philbin , Bo Chen , and Ying Wu . 2014 .", "entities": []}, {"text": "Learning \ufb01ne - grained image similarity with deep ranking .", "entities": []}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 1386\u20131393 .", "entities": []}, {"text": "Jason Weston , Samy Bengio , and Nicolas Usunier . 2011 .", "entities": []}, {"text": "Wsabie : Scaling up to large vocabulary image annotation .", "entities": []}, {"text": "In IJCAI , volume 11 , pages 2764\u20132770.66", "entities": []}]