[{"text": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP , pages 300\u2013313 Online , November 20 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics300Evaluating Attribution Methods using White - Box LSTMs Yiding Hao Yale University New Haven , CT ,", "entities": []}, {"text": "USA yiding.hao@yale.edu", "entities": []}, {"text": "Abstract Interpretability methods for neural networks are dif\ufb01cult to evaluate because we do not understand the black - box models typically used to test them .", "entities": []}, {"text": "This paper proposes a framework in which interpretability methods are evaluated using manually constructed networks , which we call white - box networks , whose behavior is understood a priori .", "entities": []}, {"text": "We evaluate \ufb01ve methods for producing attribution heatmaps by applying them to white - box LSTM classi\ufb01ers for tasks based on formal languages .", "entities": [[15, 16, "MethodName", "LSTM"]]}, {"text": "Although our white - box classi\ufb01ers solve their tasks perfectly and transparently , we \ufb01nd that all \ufb01ve attribution methods fail to produce the expected model explanations .", "entities": []}, {"text": "1 Introduction Attribution methods are a family of interpretability techniques for individual neural network predictions that attempt to measure the importance of input features for determining the model \u2019s output .", "entities": []}, {"text": "Given an input , an attribution method produces a vector of attribution orrelevance scores , which is typically visualized as a heatmap that highlights portions of the input that contribute to model behavior .", "entities": [[21, 22, "MethodName", "heatmap"]]}, {"text": "In the context of NLP , attribution scores are usually computed at the token level , so that each score represents the importance of a token within an input sequence .", "entities": []}, {"text": "These heatmaps can be used to identify keywords upon which networks base their decisions ( Li et al . , 2016 ; Sundararajan et al . , 2017 ; Arras et al . , 2017a , b;Murdoch et al . , 2018 , inter alia ) .", "entities": []}, {"text": "One of the main challenges facing the evaluation of attribution methods is that it is dif\ufb01cult to assess the quality of a heatmap when the network in question is not understood in the \ufb01rst place .", "entities": [[22, 23, "MethodName", "heatmap"]]}, {"text": "If a word is deemed relevant by an attribution method , we do not know whether the model actually considers that word relevant , or whether the attribu - tion method has erroneously estimated its importance .", "entities": []}, {"text": "Indeed , previous studies have argued that attribution methods are sensitive to features unrelated to model behavior in some cases ( e.g. , Kindermans et al . , 2019 ) , and altogether insensitive to model behavior in others ( Adebayo et al . , 2018 ) .", "entities": []}, {"text": "To tease the evaluation of attribution methods apart from the interpretation of models , this paper proposes an evaluation framework for attribution methods in NLP that uses only models that are fully understood a priori .", "entities": []}, {"text": "Instead of testing attribution methods on black - box models obtained through training , we construct white - box models for testing by directly setting network parameters by hand .", "entities": []}, {"text": "Our focus is on white - box LSTMs that implement intuitive strategies for solving simple classi\ufb01cation tasks based on formal languages with deterministic solutions .", "entities": []}, {"text": "We apply our framework to \ufb01ve attribution methods : occlusion ( Zeiler and Fergus , 2014 ) , saliency ( Simonyan et", "entities": []}, {"text": "al . , 2014 ; Li et al . , 2016 ) , gradient \u0002input , ( G\u0002 I , Shrikumar et al . , 2017 ) , integrated gradients ( IG , Sundararajan et al . , 2017 ) , and layer - wise relevance propagation ( LRP , Bach et al . , 2015 ) .", "entities": []}, {"text": "In doing so , we make the following contributions .", "entities": []}, {"text": "\u000fWe construct four white - box LSTMs that can be used to test attribution methods .", "entities": []}, {"text": "We provide a complete description of our model weights in Appendix A .1Beyond", "entities": []}, {"text": "the \ufb01ve methods considered here , our white - box networks can be used to test any attribution method compatible with LSTMs .", "entities": []}, {"text": "\u000fEmpirically , we show that all \ufb01ve attribution methods produce erroneous heatmaps for our white - box networks , despite the models \u2019 transparent behavior .", "entities": []}, {"text": "As a preview of our re1We also provide code for our models at https:// github.com/yidinghao/whitebox-lstm .", "entities": []}, {"text": "301Task : Determine whether the input contains one of the following subsequences : ab , bc , cd , ordc .", "entities": []}, {"text": "Output : True , since the input aacb contains two ( noncontiguous ) instances of ab .", "entities": []}, {"text": "Occlusion Saliency G \u0002I IG LRP aacb aacb aacbaacbaacb aacb aacb aacbaacbaacb Table 1 : Sample heatmaps for two white - box networks : a \u201c counter - based \u201d network ( top ) and an \u201c FSA - based \u201d network ( bottom ) .", "entities": []}, {"text": "The features relevant to the output are the two as and the b. sults , Table 1 shows sample heatmaps computed for two models designed to identify the non - contiguous subsequence abin the input aacb .", "entities": []}, {"text": "Even though both models \u2019 outputs are determined by the presence of the two as and the b , all four methods either incorrectly highlight the cor fail to highlight at least one of the as in at least one case .", "entities": []}, {"text": "\u000fWe identify two general ways in which four of the \ufb01ve methods do not behave as intended .", "entities": []}, {"text": "Firstly , while saliency , G \u0002I and IG are theoretically invariant to differences in model implementation ( Sundararajan et al . , 2017 ) , in practice we \ufb01nd that these methods can still produce qualitatively different heatmaps for nearly identical models .", "entities": []}, {"text": "Secondly , we \ufb01nd that LRP is susceptible to numerical issues , which cause heatmaps to be zeroed out when values are rounded to zero .", "entities": []}, {"text": "2 Related Work Several approaches have been taken in the literature for understanding how to evaluate attribution methods .", "entities": []}, {"text": "On a theoretical level , axiomatic approaches propose formal desiderata that attribution methods should satisfy , such as implementation invariance ( Sundararajan et al . , 2017 ) , input translation invariance ( Kindermans et al . , 2019 ) , continuity with respect to inputs ( Montavon et al . , 2018 ; Ghorbani et al . , 2019 ) , or the existence of relationships between attribution scores and logit or softmax scores ( Sundararajan et al . , 2017 ; Ancona et al . , 2018 ; Montavon , 2019 ) .", "entities": [[73, 74, "MethodName", "softmax"]]}, {"text": "The degree to which attribution methods ful\ufb01ll these criteria can be determined either mathematically or empirically .", "entities": []}, {"text": "Other approaches , which are more experimental in nature , attempt to directly assess the relationship between attribution scores and model behav - ior .", "entities": []}, {"text": "A common test , due to Bach et al . ( 2015 ) and Samek et al . ( 2017 ) and applied to sequence modeling by Arras et al .", "entities": []}, {"text": "( 2017a ) , involves ablating or perturbing parts of the input , from those with the highest attribution scores to those with the lowest , and counting the number of features that need to be ablated in order to change the model \u2019s prediction .", "entities": []}, {"text": "Another test , proposed by Adebayo et al . ( 2018 ) , tracks how heatmaps change as layers of a network are incrementally randomized .", "entities": []}, {"text": "A third kind of approach evaluates the extent to which heatmaps identify salient input features .", "entities": []}, {"text": "For example , Zhang et al .", "entities": []}, {"text": "( 2018 ) propose the pointing game task , in which the highest - relevance pixel for an image classi\ufb01er input must belong to the object described by the target output class .", "entities": []}, {"text": "Within this framework , Kim et al . ( 2018 ) , Poerner et al . ( 2018 ) , Arras et al . ( 2019 ) , and Yang and Kim ( 2019 ) construct datasets in which input features exhibit experimentally controlled notions of importance , yielding \u201c ground truth \u201d attributions against which heatmaps can be evaluated .", "entities": []}, {"text": "Our paper incorporates elements of the groundtruth approaches , since it is straightforward to determine which input features are important for our formal language tasks .", "entities": []}, {"text": "We enhance these approaches by using white - box models that are guaranteed to be sensitive to those features .", "entities": []}, {"text": "3 Formal Language Tasks Formal languages are often used to evaluate the expressive power of RNNs .", "entities": []}, {"text": "Here , we focus on formal languages that have been recently used to probe LSTMs \u2019 ability to capture three kinds of dependencies : counting , long - distance , and hierarchical dependencies .", "entities": []}, {"text": "We de\ufb01ne a classi\ufb01cation task based on each of these formal languages .", "entities": []}, {"text": "3.1 Counting Dependencies Counter languages ( Fischer , 1966 ; Fischer et", "entities": []}, {"text": "al . , 1968 ) are languages recognized by automata equipped with counters .", "entities": []}, {"text": "Weiss et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) demonstrate using an acceptance task for the languages anbnandanbncnthat LSTMs naturally learn to use cell state units as counters .", "entities": []}, {"text": "Merrill \u2019s ( 2019 ) asymptotic analysis shows that LSTM acceptors accept only counter languages when their weights are fully saturated .", "entities": [[9, 10, "MethodName", "LSTM"]]}, {"text": "Thus , counter languages may be viewed as a characterization of the expressive power of LSTMs .", "entities": []}, {"text": "We de\ufb01ne the counting task based on a simple", "entities": []}, {"text": "302example of a counting language .", "entities": []}, {"text": "Task 1 ( Counting Task ) .Given", "entities": []}, {"text": "a string in x2 fa;bg\u0003 , determine whether or not xhas strictly more as than bs .", "entities": []}, {"text": "Example 2 .", "entities": []}, {"text": "The counting task classi\ufb01es aaab as True , abasFalse , andbbbba asFalse .", "entities": []}, {"text": "A counter automaton can solve the counting task by incrementing its counter whenever an a is encountered and decrementing it whenever a b is encountered .", "entities": []}, {"text": "It outputs True if and only if its counter is at least 1 .", "entities": []}, {"text": "We expect attribution scores for all input symbols to have roughly the same magnitude , but that scores assigned to awill have the opposite sign to those assigned to b. 3.2 Long - Distance Dependencies Strictly piecewise ( SP , Heinz , 2007 ) languages were used by Avcu et", "entities": []}, {"text": "al . ( 2017 ) and Mahalunkar and Kelleher ( 2018 , 2019a , b ) to test the propensity of LSTMs to learn long - distance dependencies , compared to Elman \u2019s ( 1990 ) simple recurrent networks .", "entities": []}, {"text": "SP languages are regular languages whose membership is de\ufb01ned by the presence or absence of certain subsequences , which may or may not be contiguous .", "entities": []}, {"text": "For example , adis a subsequence of abcde , since both letters of adoccur inabcde , in the same order .", "entities": []}, {"text": "Based on these ideas , we de\ufb01ne the SP task as follows .", "entities": []}, {"text": "Task 3 ( SP Task ) .Given", "entities": []}, {"text": "x2 fa;b;c;dg\u0003 , determine whether or not xcontains at least one of the following subsequences : ab , bc , cd , dc .", "entities": []}, {"text": "Example 4 .", "entities": []}, {"text": "In the SP task , aab is classi\ufb01ed as True , since it contains the subsequence ab .", "entities": []}, {"text": "Similarly , acb is classi\ufb01ed as True , since it contains abnon - contiguously .", "entities": []}, {"text": "The string aaa is classi\ufb01ed asFalse .", "entities": []}, {"text": "The choice of SP languages as a test for longdistance dependencies is motivated by the fact that symbols in a non - contiguous subsequence may occur arbitrarily far from one another .", "entities": []}, {"text": "The SP task yields a variant of the pointing game task in the sense that the input string may or may not contain an \u201c object \u201d ( one of the four subsequences ) that the network must identify .", "entities": []}, {"text": "Therefore , we expect an input symbol to receive a nonzero attribution score if and only if it comprises a subsequence .", "entities": []}, {"text": "3.3 Hierarchical Dependencies The Dyck language is the language Dgenerated by the following context - free grammar , where \" isthe empty string .", "entities": []}, {"text": "S!SSj(S)j[S]j \" Dcontains all balanced strings of parentheses and square brackets .", "entities": []}, {"text": "Since Dis often viewed as a canonical example of a context - free language ( Chomsky and Sch \u00a8utzenberger , 1959 ) , several recent studies , including Sennhauser and Berwick ( 2018 ) , Bernardy ( 2018 ) , Skachkova et al .", "entities": []}, {"text": "( 2018 ) , andYu et al . ( 2019 ) , have used Dto evaluate whether LSTMs can learn hierarchical dependencies implemented by pushdown automata .", "entities": []}, {"text": "Here , we consider the bracket prediction task proposed bySennhauser and Berwick ( 2018 ) .", "entities": []}, {"text": "Task 5 ( Bracket Prediction Task ) .Given", "entities": []}, {"text": "a pre\ufb01x p of some string in D , identify the next valid closing bracket for p. Example 6 .", "entities": []}, {"text": "The string [ ( [ ] requires a prediction of ) , since the ( is the last unclosed bracket .", "entities": []}, {"text": "Similarly , ( ( ) [ requires a prediction of ] .", "entities": []}, {"text": "Strings with no unclosed brackets , such as [ ( ) ] , require a prediction of None .", "entities": []}, {"text": "In heatmaps for the bracket prediction task , we expect the last unclosed bracket to receive the highest - magnitude relevance score .", "entities": []}, {"text": "4 White - Box Networks We use two approaches to construct white - box networks for our tasks .", "entities": []}, {"text": "In the counter - based approach , the cell state contains a set of counters , which are incremented or decremented throughout the computation .", "entities": []}, {"text": "The network \u2019s \ufb01nal output is based on the values of the counters .", "entities": []}, {"text": "In the automaton - based approach , we use the LSTM to simulate an automaton , with the cell state containing a representation of the automaton \u2019s state .", "entities": [[10, 11, "MethodName", "LSTM"]]}, {"text": "We use a counter - based network to solve the counter task and an automaton - based network to solve the bracket prediction task .", "entities": []}, {"text": "We use both kinds of networks to solve the SP task .", "entities": []}, {"text": "All networks perfectly solve the tasks they were designed for .", "entities": []}, {"text": "This section describes our white - box networks at a high level ; a detailed description is given in Appendix A .", "entities": []}, {"text": "In the rest of this paper , we identify the alphabet symbols a , b , c , and dwith the one - hot vectors for indices 1,2,3 , and 4 , respectively .", "entities": []}, {"text": "The vectors f(t),i(t ) , ando(t)represent the forget , input , and output gates , respectively .", "entities": []}, {"text": "g(t)is the value added to the cell state at each time step , and \u001brepresents", "entities": []}, {"text": "303the sigmoid function .", "entities": []}, {"text": "We assume that the hidden stateh(t)and cell state c(t)are updated as follows .", "entities": []}, {"text": "c(t)=f(t)\u2299c(t\u00001)+i(t)\u2299g(t ) h(t)=o(t)\u2299tanh ( c(t ) ) 4.1 Counter - Based Networks In the counter - based approach , each position of the cell state contains the value of a counter .", "entities": []}, {"text": "To adjust the counter in position jby some value v2(\u00001;1 ) , we set g(t )", "entities": []}, {"text": "j = v , and we saturate the gates by setting them to \u001b(m)\u00191 , where m\u226b0 is a large constant .", "entities": []}, {"text": "For example , our network for the counting task uses a single hidden unit , with the gates always saturated and with g(t)given by g(t)= tanh ( u [ 1\u00001 ] x(t ) ) , where u > 0is a hyperparameter that scales the counter by a factor of v= tanh ( u).2When x(t)= a , we have g(t)=v , so the counter is incremented byv .", "entities": []}, {"text": "When x(t)=b , we compute g(t)=\u0000v , so the counter is decremented by v. For the SP task , we use seven counters .", "entities": []}, {"text": "The \ufb01rst four counters record how many occurrences of each symbol have been observed at time step t.", "entities": []}, {"text": "The next three counters record the number of bs , cs , and ds that form one of the four distinguished subsequences with an earlier symbol .", "entities": []}, {"text": "For example , after seeing the input aaabbc , the counterbased network for the SP task satis\ufb01es c(6)=v [ 3 2 1 0 2 1 0]\u22a4.", "entities": [[22, 23, "DatasetName", "0"]]}, {"text": "The \ufb01rst four counters represent the fact that the input has 3 as , 2bs , 1c , and no ds .", "entities": []}, {"text": "Counter # 5 is2vbecause the two bs form a subsequence with theas , and counter # 6 is vbecause the cforms a subsequence with the bs .", "entities": []}, {"text": "The logit scores of our counter - based networks are computed by a linear decoder using the tanh of the counter values .", "entities": []}, {"text": "For the counting task , the score of the True class is h(t ) , while the score of theFalse class is \ufb01xed to tanh ( v)=2 .", "entities": []}, {"text": "This means that the network outputs True if and only if the \ufb01nal counter value is at least v. For the SP task , the score of the True class is h(t ) 5+h(t ) 6+h(t ) 7 , while the score of the False class is again tanh ( v)=2 . 2We use u= 0:5for the counting task , u= 0:7for the SP task , and m= 50 for both tasks.4.2 Automata - Based Networks We consider two types of automata - based networks : one that implements a \ufb01nite - state automaton ( FSA ) for the SP task , and one that implements a pushdown automaton ( PDA ) for the bracket prediction task .", "entities": []}, {"text": "Our FSA construction is similar to Korsky and Berwick \u2019s ( 2019 )", "entities": []}, {"text": "FSA construction for simple recurrent networks .", "entities": []}, {"text": "Consider a deterministic FSA Awith states Qand alphabet \u0006.", "entities": []}, {"text": "To simulate Ausing an LSTM , we use jQj \u0001 j\u0006jhidden units , with the following interpretation .", "entities": [[4, 5, "MethodName", "LSTM"]]}, {"text": "Suppose thatAtransitions to state qafter reading input x(1);x(2 ) ; : : : ; x(t ) .", "entities": []}, {"text": "The hidden state h(t)is a onehot representation of the pair\u27e8 q;x(t)\u27e9 , which encodes both the current state of Aand the most recent input symbol .", "entities": []}, {"text": "Since the FSA undergoes a state transition with each input symbol , the forget gate always clears c(t ) , so that information written to the cell state does not persist beyond a single time step .", "entities": []}, {"text": "The output layer simply detects whether or not the FSA is in an accepting state .", "entities": []}, {"text": "Details are provided in Appendix A.3 .", "entities": []}, {"text": "Next , we describe how to implement a PDA for the bracket prediction task .", "entities": []}, {"text": "We use a stack containing all unclosed brackets observed in the input string , and make predictions based on the top item of the stack .", "entities": []}, {"text": "We represent a bounded stack of size kusing 2k+ 1hidden units .", "entities": []}, {"text": "The \ufb01rst k\u00001positions contain all stack items except the top item , with(represented by the value 1,[represented by\u00001 , and empty positions represented by 0 .", "entities": [[24, 25, "DatasetName", "0"]]}, {"text": "The kth position contains the top item of the stack .", "entities": [[1, 2, "DatasetName", "kth"]]}, {"text": "The nextkpositions contain the height of the stack in unary notation , and the last position contains a bit indicating whether or not the stack is empty .", "entities": []}, {"text": "For example , after reading the input ( [ ( ( ) with a stack of size 4 , the stack contents ( [ ( are represented by c(5)= [ 1\u00001 0 1 1 1 1 0 0]\u22a4. The 1 in position 4 indicates that the top item of the stack is ( , and the 1,\u00001 , and 0 in positions 1\u20133 indicate that the remainder of the stack is ( [ .", "entities": [[31, 32, "DatasetName", "0"], [36, 37, "DatasetName", "0"], [59, 60, "DatasetName", "0"]]}, {"text": "The three 1s in positions 5\u20138 indicate that the stack height is 3 , and the 0 in position 9 indicates that the stack is not empty .", "entities": [[16, 17, "DatasetName", "0"]]}, {"text": "When x(t)is(or", "entities": []}, {"text": "[ , it is copied to c(t ) k , and c(t ) k is copied to the highest empty position in c(t ) :", "entities": []}, {"text": "k\u00001 , pushing the opening bracket to the stack .", "entities": []}, {"text": "The empty stack bit is then set to 0 , marking the stack", "entities": [[8, 9, "DatasetName", "0"]]}, {"text": "304Name Formula Saliency R(c ) t;i(X ) = @^yc @x(t ) i \f\f\f  x(t )", "entities": []}, {"text": "i = Xt;i G\u0002I R(c ) t;i(X ) = Xt;i@^yc @x(t ) i \f\f\f  x(t )", "entities": []}, {"text": "i = Xt;i IG R(c ) t;i(X ) = Xt;i\u222b1 0@^yc @x(t ) i \f\f\f  x(t ) i= \u000b X t;id \u000b  Table 2 : De\ufb01nitions of the gradient - based methods .", "entities": []}, {"text": "as non - empty .", "entities": []}, {"text": "When the current input symbol is a closing bracket , the highest item of positions 1 through k\u00001is deleted and copied to position k , popping the top item from the stack .", "entities": []}, {"text": "Because the PDA network is quite complex , we focus here on describing how the top stack item in position kis determined , and leave other details for Appendix A.4 .", "entities": []}, {"text": "Let \u000b ( t)be1ifx(t)=(,\u00001if x(t)= [ , and 0otherwise .", "entities": []}, {"text": "At each time step , g(t ) k= tanh ( m\u0001u(t ) ) ,", "entities": []}, {"text": "where m\u226b0and u(t)= 2k \u000b ( t)+k\u00001\u2211 j=12j\u00001h(t\u00001 ) j. ( 1 ) Observe that m\u0001u(t)\u226b0when \u000b ( t)= 1 , and m\u0001u(t)\u226a0when \u000b ( t)=\u00001 .", "entities": []}, {"text": "Thus , g(t ) kcontains the stack encoding of the current input symbol if it is an opening bracket .", "entities": []}, {"text": "If the current input symbol is a closing bracket , then \u000b ( t)= 0 , so the sign of u(t)is determined by the highest item of h(t\u00001 ) : k\u00001 . 5 Attribution Methods LetXbe a matrix of input vectors , such that the input at time tis the row vector Xt;:= ( x(t))\u22a4. Given X , an LSTM classi\ufb01er produces a vector ^yof logit scores .", "entities": [[14, 15, "DatasetName", "0"], [59, 60, "MethodName", "LSTM"]]}, {"text": "Based on X,^y , and possibly abaseline input X , an attribution method assigns an attribution score R(c ) t;i(X)to input feature Xt;i for each output class c.", "entities": []}, {"text": "These feature - level scores are then aggregated to produce token - level scores : R(c ) t(X )", "entities": []}, {"text": "= \u2211 iR(c ) t;i(X ) .", "entities": []}, {"text": "Broadly speaking , our \ufb01ve attribution methods are grouped into three types : one perturbation - based , three gradient - based , and one decompositionbased .", "entities": []}, {"text": "The following subsections describe how each method computes R(c ) t;i(X).5.1 Perturbation- and Gradient - Based Methods Perturbation - based methods are premised on the idea that if Xt;iis an important input feature , then changing the value of Xt;iwould cause ^y to change .", "entities": []}, {"text": "The one perturbation method we consider is occlusion .", "entities": []}, {"text": "In this method , R(c ) t;i(X)is the change in ^ycobserved when Xt;:is replaced by 0 .", "entities": [[15, 16, "DatasetName", "0"]]}, {"text": "Gradient - based methods rely on the same intuition as perturbation - based methods , but use automatic differentiation to simulate in\ufb01nitesimal perturbations .", "entities": []}, {"text": "The de\ufb01nitions of our three gradientbased methods are given in Table 2 .", "entities": []}, {"text": "The most basic of these is saliency , which simply measures relevance by the derivative of the logit score with respect to each input feature .", "entities": []}, {"text": "G \u0002I attempts to improve upon saliency by using the \ufb01rst - order terms in a Taylor - series approximation of the model instead of the gradients on their own .", "entities": []}, {"text": "IG is designed to address the issue of small gradients found in saturated units by integrating G \u0002I along the line connecting Xto a baseline input X , here taken to be the zero matrix .", "entities": []}, {"text": "5.2 Decomposition - Based Methods Decomposition - based methods are methods that satisfy the relation ^yc = R(c ) bias+\u2211 t;iR(c ) t;i(X ) , ( 2 ) where R(c ) biasis a relevance score assigned to the bias units of the network .", "entities": []}, {"text": "The interpretation of equation ( 2 ) is that the logit score ^ycis \u201c distributed \u201d among the input features and the bias units , so that the relevance scores form a \u201c decomposition \u201d of ^yc .", "entities": []}, {"text": "The one decomposition - based method we consider is LRP , which computes scores using a backpropagation algorithm that distributes scores layer by layer .", "entities": []}, {"text": "The scores of the output layer are initialized to r(c;output )", "entities": []}, {"text": "i = { ^yi ; i = c 0;otherwise .", "entities": []}, {"text": "For each layer lwith activation z(l ) , activation function f(l ) , and output a(l)=f(l ) ( z(l ) ) , the relevance r(c;l)ofa(l)is determined by the following propagation rule : r(c;l ) i=\u2211 l\u2032\u2211 jr(c;l\u2032 ) jW(l\u2032 l ) j;i a(l ) i z(l\u2032 ) j+ sign ( z(l\u2032 ) j ) \" ,", "entities": [[8, 10, "HyperparameterName", "activation function"]]}, {"text": "305where l\u2032ranges over all layers to which lhas a forward connection via W(l\u2032 l)and \" >", "entities": []}, {"text": "0is a stabilizing constant.3For the LSTM gate interactions , we follow Arras et al .", "entities": [[5, 6, "MethodName", "LSTM"]]}, {"text": "( 2017b ) in treating multiplicative connections of the form a(l1)\u2299a(l2 ) as activation functions of the form a(l1)\u2299f(l2)(\u0001 ) , where a(l1)isf(t),i(t ) , oro(t ) .", "entities": []}, {"text": "The \ufb01nal attribution scores are given by the values propagated to the input layer : R(c ) t;i(X ) = r(c;inputt ) i .", "entities": []}, {"text": "6 Qualitative Evaluation To evaluate attribution methods under our framework , we begin with a qualitative description of the heatmaps that are computed for our whitebox networks , based on the illustrative sample of heatmaps appearing in Table 3 .", "entities": []}, {"text": "6.1 Counting Task Occlusion , G \u0002I , and IG are well - behaved for the counting task .", "entities": []}, {"text": "As expected , these methods assign aa positive value and ba negative value when the output class for attribution is c = True .", "entities": []}, {"text": "When the number of as is different from the number of bs , occlusion assigns a lower - magnitude score to the symbol with fewer instances .", "entities": []}, {"text": "When c = False , all relevance scores are 0 .", "entities": [[9, 10, "DatasetName", "0"]]}, {"text": "This is because ^yFalseis \ufb01xed to a constant value supplied by a bias term , so input features can not affect its value .", "entities": []}, {"text": "Saliency and LRP both fail to produce nonzero scores , at least in some cases .", "entities": []}, {"text": "Saliency scores satisfy R(True ) t;1(X )", "entities": []}, {"text": "= \u0000R(True ) t;2(X ) , resulting in token - level scores of 0for all inputs .", "entities": []}, {"text": "Heatmaps # 3 and # 4 show that LRP assigns scores of 0to pre\ufb01xes containing equal numbers of as and bs .", "entities": []}, {"text": "We will see in Subsection 7.1 that this phenomenon appears to be related to the fact that the LSTM gates are saturated .", "entities": [[18, 19, "MethodName", "LSTM"]]}, {"text": "6.2 SP Task We obtain radically different heatmaps for the two SP task networks , despite the fact that they produce the same classi\ufb01cations for all inputs .", "entities": []}, {"text": "For the counter - based network , all methods except for saliency assign positive scores for c= True to symbols constituting one of the four subsequences , and scores of zero elsewhere .", "entities": []}, {"text": "The saliency heatmaps do not adhere to this pattern , and instead generally assign higher scores 3We use \" = 0:001.to tokens occurring near the end of the input .", "entities": []}, {"text": "Heatmaps # 7\u201310 show that LRP fails to assign positive scores to the \ufb01rst symbol of each subsequence , while the other methods generally do not.4 The LRP behavior re\ufb02ects the fact that the initial adoes not increment the subsequence counters , which determine the \ufb01nal logit score .", "entities": []}, {"text": "In contrast , the behavior of occlusion , G \u0002I , and IG is explained by the fact that removing either the aor thebdestroys the subsequence .", "entities": []}, {"text": "Note that the as in heatmap # 9 receive scores of 0from occlusion and G \u0002I , since removing only one of the two as does not destroy the subsequence .", "entities": [[5, 6, "MethodName", "heatmap"]]}, {"text": "For the FSA - based network , saliency , G \u0002I , and LRP assign only the last symbol a nonzero score when the relevance output class cmatches the network \u2019s predicted class .", "entities": []}, {"text": "IG appears to produce erratic heatmaps , exhibiting no immediately obvious pattern .", "entities": []}, {"text": "Although occlusion appears to be erratic at \ufb01rst glance , its behavior can be explained by the fact that changing x(t)to0causes h(t)to be0 , which the LSTM interprets as the initial state of the FSA ; thus , R(c ) t(X)\u0338= 0precisely when Xt+1:;:is classi\ufb01ed differently from X. In all cases , the heatmaps for the FSA - based network diverge signi\ufb01cantly from the expected heatmaps .", "entities": [[26, 27, "MethodName", "LSTM"]]}, {"text": "6.3 Bracket Prediction Task", "entities": []}, {"text": "The heatmaps for the PDA - based network also differ strikingly from those of the other networks , in that the gradient - based methods never assign nonzero scores .", "entities": []}, {"text": "This is because equation ( 1 ) causes g(t)to be highly saturated , resulting in zero gradients .", "entities": []}, {"text": "In the case of LRP , the matching bracket is highlighted when c\u0338=None .", "entities": []}, {"text": "When the matching bracket is not the last symbol of the input , the other unclosed brackets are also highlighted , with progressively smaller magnitudes , and with brackets of the opposite type from creceiving negative scores .", "entities": []}, {"text": "This pattern re\ufb02ects the mechanism of ( 1 ) , in which progressively larger powers of 2are used to determine the content copied to c(t ) k.", "entities": []}, {"text": "When the relevance output class is c = None , LRP assigns opening brackets a negative score , revealing the fact that those input symbols set the bit c(t ) 2k+1to indicate that the stack is not empty .", "entities": []}, {"text": "Although occlusion sometimes highlights the matching bracket , it does not appear to be consistent in doing so .", "entities": []}, {"text": "For example , it fails to highlight the matching bracket 4Although", "entities": []}, {"text": "it is dif\ufb01cult to see , IG assigns a small positive score to the bs in heatmaps # 7 and # 8 .", "entities": []}, {"text": "306Network # c Target Occlusion Saliency G \u0002I IG LRP Counting1", "entities": []}, {"text": "True True aaabb aaabb aaabb aaabb aaabb 2", "entities": []}, {"text": "True False bbbaa bbbaa bbbaa bbbaa bbbaa 3 True False aaabbbaaabbbaaabbbaaabbbaaabbb 4 True False aabbb aabbb aabbb aabbb aabbb 5 False True aaabb aaabb aaabb aaabb aaabb 6 False False aabbb aabbb aabbb aabbb aabbb SP ( Counter)7", "entities": []}, {"text": "True True", "entities": []}, {"text": "acb acb", "entities": []}, {"text": "acb acb", "entities": []}, {"text": "acb 8 True True acbb acbb acbb acbb acbb 9 True True aacb aacb aacb aacb aacb 10 True True abcab abcab abcab abcab abcab 11 True False aacc aacc aacc aacc aacc", "entities": []}, {"text": "12 False True", "entities": []}, {"text": "acb acb", "entities": []}, {"text": "acb acb", "entities": []}, {"text": "acb 13 False False aacc aacc aacc aacc aacc SP ( FSA)14", "entities": []}, {"text": "True True", "entities": []}, {"text": "acb acb", "entities": []}, {"text": "acb acb acb 15 True True acbb acbb acbb acbb acbb 16 True True aacb aacb aacb aacb aacb 17 True True abcab abcab abcab abcab abcab 18", "entities": []}, {"text": "True False aacc aacc aacc aacc aacc 19 False True", "entities": []}, {"text": "acb acb", "entities": []}, {"text": "acb acb", "entities": []}, {"text": "acb 20 False False aacc aacc aacc aacc aacc Bracket ( PDA)21 ] ]", "entities": []}, {"text": "( [ [ ( [ ( [ [ ( [ ( [ [ ( [ ( [ [ ( [ ( [ [ ( [ 22 ) )", "entities": []}, {"text": "( [ [ ( [ ] ( [ [ ( [ ] ( [ [ ( [ ] ( [ [ ( [ ] ( [ [ ( [ ] 23 None None ( [ [ ] ] )", "entities": []}, {"text": "( [ [ ] ] ) ( [ [ ] ] ) ( [ [ ] ] ) ( [ [ ] ] ) 24 ] ]", "entities": []}, {"text": "[ ( [ ] [ ( )", "entities": []}, {"text": "[ ( [ ] [ ( )", "entities": []}, {"text": "[ ( [ ] [ ( )", "entities": []}, {"text": "[ ( [ ] [ ( )", "entities": []}, {"text": "[ ( [ ] [ ( ) 25 ) ]", "entities": []}, {"text": "[ ( [ ] [ ( )", "entities": []}, {"text": "[ ( [ ] [ ( )", "entities": []}, {"text": "[ ( [ ] [ ( )", "entities": []}, {"text": "[ ( [ ] [ ( )", "entities": []}, {"text": "[ ( [ ] [ ( ) Table 3 : Selected heatmaps based on R(c ) t(X).Red represents positive values and blue represents negative values .", "entities": []}, {"text": "Heatmaps with all values within the range of \u00061\u000210\u00005are shown as all 0s .", "entities": []}, {"text": "u v ^yTrue Saliency G \u0002I IG 0.6 0.537 0.151 accb accbaccb 0.7 0.604 0.533 accb accbaccb 0.8 0.664 0.581 accb accbaccb 1 0.762 0.642 accb accbaccb 4 0.999 0.761 accb accbaccb 8 1.000 0.762 accb accbaccb 16 1.000 0.762 accb accbaccb 64 1.000 0.762 accb", "entities": []}, {"text": "accbaccb Table 4 : Gradient - based heatmaps of R(True ) t ( accb ) for the counter - based SP network , with 0:6\u0014u\u001464 .", "entities": []}, {"text": "in heatmap # 21 , and highlights one other bracket in heatmaps # 23\u201324 .", "entities": [[1, 2, "MethodName", "heatmap"]]}, {"text": "7 Detailed Evaluations We now turn to focused investigations of particular phenomena that attribution methods exhibit when applied to white - box networks .", "entities": []}, {"text": "Subsection 7.1 begins by discussing the effect of network saturation on the gradient - based methods and LRP .", "entities": []}, {"text": "In Subsection 7.2 we apply Bach et al . \u2019s ( 2015 ) ablation test to our attribution methods for the SP task .", "entities": []}, {"text": "7.1 Saturation As mentioned in the previous section , network saturation causes gradients to be approximately 0when using sigmoid or tanh activation functions .", "entities": [[20, 22, "MethodName", "tanh activation"]]}, {"text": "To test how attribution methods are affectedm \u001b ( m ) c(t)Accuracy", "entities": []}, {"text": "% Blank 4 0.982 \u00008.74\u000210\u0000390.1 0.2 5 0.993 \u00003.48\u000210\u0000396.1 2.2 6 0.998 \u00001.32\u000210\u0000399.8 6.5 7 0.999 \u00004.91\u000210\u00004100.0 22.0 8 1.000 \u00001.81\u000210\u00004100.0 42.1 9 1.000 \u00006.68\u000210\u00005100.0 69.9 10 1.000 \u00002.46\u000210\u00005100.0 92.3 11 1.000 \u00009.05\u000210\u00006100.0 98.7 12 1.000 \u00003.33\u000210\u00006100.0 99.8 Table 5 : The results of the LRP saturation test , including the value of m , the average value of c(t)when the counter reaches 0 , the network \u2019s testing accuracy , and the percentage of examples with blank heatmaps for pre\ufb01xes with equal numbers of as and bs .", "entities": [[63, 64, "DatasetName", "0"], [69, 70, "MetricName", "accuracy"]]}, {"text": "by saturation , Table 4 shows heatmaps for the inputaccb generated by gradient - based methods for different instantiations of the counter - based SP network with varying degrees of saturation .", "entities": []}, {"text": "Recall from Section 4 that counter values for this network are expressed in multiples of the scaling factorv .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "We control the saturation of the network via the parameter u= tanh\u00001(v ) .", "entities": []}, {"text": "For all three gradient - based methods , scores for adecrease and scores for bincrease as uincreases .", "entities": []}, {"text": "Additionally , saliency scores for the \ufb01rst cdecrease when uincreases .", "entities": []}, {"text": "When u= 8,vis almost completely saturated , causing G \u0002I to produce all - zero heatmaps .", "entities": []}, {"text": "307On the other hand , IG is still able to produce nonzero heatmaps even at u= 64 .", "entities": []}, {"text": "Thus , IG is much more resistant to the effects of saturation than G \u0002I.", "entities": []}, {"text": "According to Sundararajan et al . ( 2017 ) , gradient - based methods satisfy the axiom of implementation invariance : they produce the same heatmaps for any two networks that compute the same function .", "entities": []}, {"text": "This formal property is seemingly at odds with the diverse array of heatmaps appearing in Table 4 , which are produced for networks that all yield identical classi\ufb01ers .", "entities": []}, {"text": "In particular , the networks with u= 8,16 , and 64yield qualitatively different heatmaps , despite the fact that the three networks are distinguished only by differences in vof less than 0:001 .", "entities": []}, {"text": "Because the three functions are technically not equal , implementation invariance is not violated in theory ; but the fact that IG produces different heatmaps for three nearly identical networks shows that the intuition described by implementation invariance is not borne out in practice .", "entities": []}, {"text": "Besides the gradient - based methods , LRP is also susceptible to problems arising from saturation .", "entities": []}, {"text": "Recall from heatmaps # 3 and # 4 of Table 3 that for the counting task network , LRP assigns scores of 0to pre\ufb01xes with equal numbers of as andbs .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "We hypothesize that this phenomenon is related to the fact c(t)= 0after reading such pre\ufb01xes , since the counter has been incremented and decremented in equal amounts .", "entities": []}, {"text": "Accordingly , we test whether this phenomenon can be mitigated by desaturating the gates so that c(t)does not exactly reach 0 .", "entities": [[20, 21, "DatasetName", "0"]]}, {"text": "Recall that the white - box LSTM gates approximate 1\u0019\u001b(m)using a constant m\u226b0 .", "entities": [[0, 1, "MetricName", "Recall"], [6, 7, "MethodName", "LSTM"]]}, {"text": "We construct networks with varying values of mand compute LRP scores on a randomly generated testing set of 1000 strings , each of which contains at least one pre\ufb01x with equal numbers of as and bs .", "entities": []}, {"text": "InTable 5 we report the percentage of examples for which such pre\ufb01xes receive LRP scores of 0 , along with the network \u2019s accuracy on this testing set and the average value of c(t)when the counter reaches 0 .", "entities": [[16, 17, "DatasetName", "0"], [23, 24, "MetricName", "accuracy"], [37, 38, "DatasetName", "0"]]}, {"text": "Indeed , the percentage of pre\ufb01xes receiving scores of 0increases as the approximation c(t)\u00190becomes more exact .", "entities": []}, {"text": "7.2 Ablation Test So far , we have primarily compared attribution methods via visual inspection of individual examples .", "entities": []}, {"text": "To compare the \ufb01ve methods quantitatively , Method SP ( Counter ) SP ( FSA ) Occlusion 61.8 \u000612.2 52.6\u000611.7 Saliency 97.8\u00061.1 96.0\u00062.5 G\u0002I 65.7\u000614.4 96.0\u00062.5 IG 47.5\u00067.6 94.9\u00062.9 LRP 64.3\u000612.7 96.0\u00062.5 Random 96.1\u00062.5 Optimal 42.7\u00063.8 Table 6 : Mean and standard deviation results of the ablation test , normalized by string length and expressed as a percentage .", "entities": []}, {"text": "\u201c Optimal \u201d is the best possible score .", "entities": []}, {"text": "we apply the ablation test of Bach et al .", "entities": []}, {"text": "( 2015 ) to our two white - box networks for the SP task.5 Given an input string classi\ufb01ed as True , we iteratively remove the symbol with the highest relevance score , recomputing heatmaps at each iteration , until the string no longer contains any of the four subsequences .", "entities": []}, {"text": "We apply the ablation test to 100 randomly generated input strings , and report the average percentage of each string that is ablated in Table 6 .", "entities": []}, {"text": "A peculiar property of the SP task is that removing a symbol preserves the validity of input strings .", "entities": []}, {"text": "This means that , unlike in NLP settings , our ablation test does not suffer from the issue that ablation produces invalid inputs .", "entities": []}, {"text": "Saliency , G \u0002I , and LRP perform close to the random baseline on the FSA network ; this is unsurprising , since these methods only assign nonzero scores to the last input symbol .", "entities": []}, {"text": "While Table 3 shows some variation in the IG heatmaps , IG also performs close to the random baseline .", "entities": []}, {"text": "Only occlusion performs considerably better , since it is able to identify symbols whose ablation would destroy subsequences .", "entities": []}, {"text": "On the counter - based SP network , IG performs remarkably close to the optimal benchmark , which represents the best possible performance on this task .", "entities": []}, {"text": "Occlusion , G \u0002I , and LRP achieve a similar level of performance to one another , while saliency performs worse than the random baseline .", "entities": []}, {"text": "8 Conclusion Of all the heatmaps considered in this paper , only those computed by G \u0002I and IG for the counting task fully matched our expectations .", "entities": []}, {"text": "In other cases , all attribution methods fail to identify at least some of the input features that should be considered relevant , or assign relevance to input features that do 5We do not consider the counting task because its heatmaps are already easy to understand , and we do not consider the PDA network because the gradient - based methods fail to produce nonzero heatmaps for that network .", "entities": []}, {"text": "308not affect the model \u2019s behavior .", "entities": []}, {"text": "Among the \ufb01ve methods , saliency achieves the worst performance : it never assigns nonzero scores for the counting and bracket prediction tasks , and it does not identify the relevant symbols for either of the two SP networks .", "entities": []}, {"text": "Saliency also achieves the worst performance on the ablation test for both the counterbased and the FSA - based SP networks .", "entities": []}, {"text": "Among the four white - box networks , the two automatabased networks proved to be much more challenging for the attribution methods than the counterbased networks .", "entities": []}, {"text": "While the LRP heatmaps for the PDA network correctly identify the matching bracket when available , no other method produces reasonable heatmaps for the PDA network , and all \ufb01ve methods fail to interpret the FSA network .", "entities": []}, {"text": "Taken together , our results suggest that attribution heatmaps should be viewed with skepticism .", "entities": []}, {"text": "This paper has identi\ufb01ed cases in which heatmaps fail to highlight relevant features , as well as cases in which heatmaps incorrectly highlight irrelevant features .", "entities": []}, {"text": "Although most of the methods perform better for the counter - based networks than the automaton - based networks , in practical settings we do not know what kinds of computations are implemented by a trained network , making it impossible to determine whether the network under analysis is compatible with the attribution method being used .", "entities": []}, {"text": "In future work , we encourage the use of our four white - box models as qualitative benchmarks for evaluating interpretability methods .", "entities": []}, {"text": "For example , the style of evaluation we have developed can be replicated for attribution methods not covered in this paper , including DeepLIFT ( Shrikumar et al . , 2017 ) and contextual decomposition ( Murdoch et al . ,2018 ) .", "entities": []}, {"text": "We believe that insights gleaned from white - box analysis can help researchers choose between different attribution methods and identify areas of improvement in current techniques .", "entities": []}, {"text": "Acknowledgments I would like to thank Dana Angluin and Robert Frank for their advice and mentorship on this project .", "entities": []}, {"text": "I would also like to thank Yoav Goldberg , John Lafferty , Tal Linzen , R. Thomas McCoy , Aaron Mueller , Karl Mulligan , Shauli Ravfogel , Jason Shaw , and the reviewers for their helpful feedback and discussion .", "entities": []}, {"text": "References Julius Adebayo , Justin Gilmer , Michael Muelly , Ian Goodfellow , Moritz Hardt , and Been Kim . 2018 .", "entities": []}, {"text": "Sanity Checks for Saliency Maps .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 31 , volume 31 , pages 9505\u20139515 , Montreal , Canada .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Marco Ancona , Enea Ceolini , Cengiz \u00a8Oztireli , and Markus Gross . 2018 .", "entities": []}, {"text": "Towards better understanding of gradient - based attribution methods for Deep Neural Networks .", "entities": []}, {"text": "InICLR 2018", "entities": []}, {"text": "Conference Track , Vancouver , Canada .", "entities": []}, {"text": "OpenReview .", "entities": []}, {"text": "Leila Arras , Franziska Horn , Gr \u00b4 egoire Montavon , Klaus - Robert M \u00a8uller , and Wojciech Samek .", "entities": []}, {"text": "2017a .", "entities": []}, {"text": "\u201c What is relevant in a text document ? \u201d : An interpretable machine learning approach .PLOS", "entities": [[12, 15, "TaskName", "interpretable machine learning"]]}, {"text": "ONE , 12(8):e0181142 .", "entities": []}, {"text": "Leila Arras , Gr \u00b4 egoire Montavon , Klaus - Robert M \u00a8uller , and Wojciech Samek .", "entities": []}, {"text": "2017b .", "entities": []}, {"text": "Explaining Recurrent Neural Network Predictions in Sentiment Analysis .", "entities": [[6, 8, "TaskName", "Sentiment Analysis"]]}, {"text": "InProceedings of the 8th Workshop on Computational Approaches to Subjectivity , Sentiment and Social Media Analysis , pages 159\u2013168 , Copenhagen , Denmark .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Leila Arras , Ahmed Osman , Klaus - Robert M \u00a8uller , and Wojciech Samek .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Evaluating Recurrent Neural Network Explanations .", "entities": []}, {"text": "In Proceedings of the 2019 ACL Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 113 \u2013 126 , Florence , Italy .", "entities": [[22, 23, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Enes Avcu , Chihiro Shibata , and Jeffrey Heinz .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Subregular Complexity and Deep Learning .", "entities": []}, {"text": "In Proceedings of the Conference on Logic and Machine Learning in Natural Language ( LaML 2017 ) , Gothenburg , 12\u201313 June 2017 , volume 1 of CLASP Papers in Computational Linguistics , pages 20\u201333 , Gothenburg , Sweden .", "entities": []}, {"text": "Centre for Linguistic Theory and Studies in Probability ( CLASP ) , University of Gothenburg .", "entities": []}, {"text": "Sebastian Bach , Alexander Binder , Gr \u00b4 egoire Montavon , Frederick Klauschen , Klaus - Robert M \u00a8uller , and Wojciech Samek .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "On Pixel - Wise Explanations for Non - Linear Classi\ufb01er Decisions by Layer - Wise Relevance Propagation .PLOS", "entities": []}, {"text": "ONE , 10(7):e0130140 .", "entities": []}, {"text": "Jean - Philippe Bernardy .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Can Recurrent Neural Networks Learn Nested Recursion ?", "entities": []}, {"text": "Linguistic Issues in Language Technology , 16(1):1\u201320 .", "entities": []}, {"text": "N. Chomsky and M. P. Sch \u00a8utzenberger .", "entities": []}, {"text": "1959 .", "entities": []}, {"text": "The Algebraic Theory of Context - Free Languages .", "entities": []}, {"text": "In P. Braffort and D. Hirschberg , editors , Studies in Logic and the Foundations of Mathematics , volume 26 of Computer Programming and Formal Systems , pages 118\u2013161 .", "entities": []}, {"text": "North - Holland Publishing Company , Amsterdam , Netherlands .", "entities": []}, {"text": "309Jeffrey L. Elman .", "entities": []}, {"text": "1990 .", "entities": []}, {"text": "Finding Structure in Time .", "entities": []}, {"text": "Cognitive Science , 14(2):179\u2013211 .", "entities": []}, {"text": "Patrick C. Fischer .", "entities": []}, {"text": "1966 .", "entities": []}, {"text": "Turing Machines with Restricted Memory Access .Information and Control , 9(4):364\u2013379 .", "entities": [[3, 4, "DatasetName", "Restricted"]]}, {"text": "Patrick C. Fischer , Albert R. Meyer , and Arnold L. Rosenberg .", "entities": []}, {"text": "1968 .", "entities": []}, {"text": "Counter Machines and Counter Languages .Mathematical systems theory , 2(3):265 \u2013 283 .", "entities": []}, {"text": "Amirata Ghorbani , Abubakar Abid , and James Zou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Interpretation of Neural Networks Is Fragile .", "entities": []}, {"text": "Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 33(01):3681\u20133688 .", "entities": []}, {"text": "Jeffrey Nicholas Heinz .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Inductive Learning of Phonotactic Patterns .", "entities": []}, {"text": "PhD Dissertation , University of California , Los Angeles , Los Angeles , CA , USA .", "entities": []}, {"text": "Been Kim , Martin Wattenberg , Justin Gilmer , Carrie Cai , James Wexler , Fernanda Viegas , and Rory Sayres .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Interpretability Beyond Feature Attribution : Quantitative Testing with Concept Activation Vectors ( TCA V ) .", "entities": []}, {"text": "In International Conference on Machine Learning , 10\u201315 July 2018 , Stockholmsm \u00a8assan , Stockholm Sweden , volume 80 of Proceedings of Machine Learning Research , pages 2668\u20132677 , Stockholm , Sweden .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Pieter - Jan Kindermans , Sara Hooker , Julius Adebayo , Maximilian Alber , Kristof T. Sch \u00a8utt , Sven D\u00a8ahne , Dumitru Erhan , and Been Kim .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "The ( Un)reliability of Saliency Methods .", "entities": []}, {"text": "In Wojciech Samek , Gr \u00b4 egoire Montavon , Andrea Vedaldi , Lars Kai Hansen , and Klaus - Robert M \u00a8uller , editors , Explainable AI : Interpreting , Explaining and Visualizing Deep Learning , number 11700 in Lecture Notes in Computer Science , pages 267\u2013280 .", "entities": []}, {"text": "Springer International Publishing , Cham , Switzerland .", "entities": []}, {"text": "Samuel A. Korsky and Robert C. Berwick .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "On the Computational Power of RNNs .Computing Research Repository , arXiv:1906.06349 .", "entities": []}, {"text": "Jiwei Li , Xinlei Chen , Eduard Hovy , and Dan Jurafsky . 2016 .", "entities": []}, {"text": "Visualizing and Understanding Neural Models in NLP .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 681\u2013691 , San Diego , CA , USA .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Abhijit Mahalunkar and John Kelleher .", "entities": []}, {"text": "2019a .", "entities": []}, {"text": "MultiElement Long Distance Dependencies : Using SP k Languages to Explore the Characteristics of LongDistance Dependencies .", "entities": []}, {"text": "InProceedings of the Workshop on Deep Learning and Formal Languages : Building Bridges , pages 34\u201343 , Florence , Italy .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Abhijit Mahalunkar and John D. Kelleher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Using Regular Languages to Explore the Representational Capacity of Recurrent Neural Architectures .InArti\ufb01cial", "entities": []}, {"text": "Neural Networks and Machine Learning \u2013 ICANN 2018 , volume 11141 of Lecture Notes in Computer Science , pages 189\u2013198 , Rhodes , Greece .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Abhijit Mahalunkar and John D. Kelleher .", "entities": []}, {"text": "2019b .", "entities": []}, {"text": "Understanding Recurrent Neural Architectures by Analyzing and Synthesizing Long Distance Dependencies in Benchmark Sequential Datasets .Computing Research Repository , arXiv:1810.02966v3 .", "entities": []}, {"text": "William Merrill .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Sequential Neural Networks as Automata .", "entities": []}, {"text": "InProceedings of the Workshop on Deep Learning and Formal Languages : Building Bridges , pages 1\u201313 , Florence , Italy .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics . Gr\u00b4egoire Montavon .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Gradient - Based Vs .", "entities": []}, {"text": "Propagation - Based Explanations : An Axiomatic Comparison .", "entities": []}, {"text": "In Wojciech Samek , Gr \u00b4 egoire Montavon , Andrea Vedaldi , Lars Kai Hansen , and KlausRobert M \u00a8uller , editors , Explainable AI : Interpreting , Explaining and Visualizing Deep Learning , number 11700 in Lecture Notes in Computer Science , pages 253\u2013265 .", "entities": []}, {"text": "Springer International Publishing , Cham , Switzerland . Gr\u00b4egoire Montavon , Wojciech Samek , and KlausRobert M \u00a8uller .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Methods for interpreting and understanding deep neural networks .Digital", "entities": []}, {"text": "Signal Processing , 73:1\u201315 .", "entities": []}, {"text": "W. James Murdoch , Peter J. Liu , and Bin Yu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Beyond Word Importance : Contextual Decomposition to Extract Interactions from LSTMs .", "entities": []}, {"text": "In ICLR 2018 Conference Track , Vancouver , Canada . OpenReview .", "entities": []}, {"text": "Nina Poerner , Hinrich Sch \u00a8utze , and Benjamin Roth .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement .", "entities": []}, {"text": "InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics , volume 1 : Long Papers , pages 340\u2013350 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Wojciech Samek , Alexander Binder , Gr \u00b4 egoire Montavon , Sebastian Lapuschkin , and Klaus - Robert M\u00a8uller . 2017 .", "entities": []}, {"text": "Evaluating the Visualization of What a Deep Neural Network Has Learned .IEEE Transactions on Neural Networks and Learning Systems , 28(11):2660\u20132673 .", "entities": []}, {"text": "Luzi Sennhauser and Robert Berwick .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Evaluating the Ability of LSTMs to Learn Context - Free Grammars .", "entities": []}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 115\u2013124 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Avanti Shrikumar , Peyton Greenside , Anna Shcherbina , and Anshul Kundaje . 2017 .", "entities": []}, {"text": "Not Just a Black Box : Learning Important Features Through Propagating", "entities": []}, {"text": "310Activation Differences .Computing Research Repository , arXiv:1605.01713 .", "entities": []}, {"text": "Karen Simonyan , Andrea Vedaldi , and Andrew Zisserman .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Deep Inside Convolutional Networks : Visualising Image Classi\ufb01cation Models and Saliency Maps .", "entities": []}, {"text": "In ICLR 2014 Workshop Proceedings , Banff , Canada . arXiv .", "entities": [[10, 11, "DatasetName", "arXiv"]]}, {"text": "Natalia Skachkova , Thomas Trost , and Dietrich Klakow .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Closing Brackets with Recurrent Neural Networks .", "entities": []}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 232\u2013239 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mukund Sundararajan , Ankur Taly , and Qiqi Yan . 2017 .", "entities": []}, {"text": "Axiomatic Attribution for Deep Networks .", "entities": []}, {"text": "In Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 3319\u20133328 ,", "entities": []}, {"text": "Sydney , Australia . PMLR .", "entities": []}, {"text": "Gail Weiss , Yoav Goldberg , and Eran Yahav .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "On the Practical Computational Power of Finite Precision RNNs for Language Recognition .", "entities": [[7, 8, "MetricName", "Precision"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , volume 2 : Short Papers , pages 740\u2013745 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mengjiao Yang and Been Kim . 2019 .", "entities": []}, {"text": "Benchmarking Attribution Methods with Relative Feature Importance .Computing Research Repository , arXiv:1907.09701 .", "entities": [[5, 7, "TaskName", "Feature Importance"]]}, {"text": "Xiang Yu , Ngoc Thang Vu , and Jonas Kuhn . 2019 .", "entities": []}, {"text": "Learning the Dyck Language with Attention - based Seq2Seq Models .", "entities": [[8, 9, "MethodName", "Seq2Seq"]]}, {"text": "In Proceedings of the 2019 ACL Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 138\u2013146 , Florence , Italy .", "entities": [[20, 21, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Matthew D. Zeiler and Rob Fergus .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Visualizing and Understanding Convolutional Networks .", "entities": []}, {"text": "In Computer Vision \u2013 ECCV 2014 , volume 8689 of Lecture Notes in Computer Science , pages 818\u2013833 , Zurich , Switzerland .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Jianming Zhang , Sarah Adel Bargal , Zhe Lin , Jonathan Brandt , Xiaohui Shen , and Stan Sclaroff .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Top - Down Neural Attention by Excitation Backprop .", "entities": []}, {"text": "International Journal of Computer Vision , 126(10):1084\u20131102 .", "entities": []}, {"text": "A Detailed Descriptions of White - Box Networks This appendix provides detailed descriptions of our four white - box networks .", "entities": []}, {"text": "A.1 Counting Task Network As described in Subsection 4.1 , the network for the counting task simply sets g(t)tov= tanh ( u ) when x(t)=aand\u0000vwhen x(t)=b .", "entities": []}, {"text": "All gates are \ufb01xed to 1 .", "entities": []}, {"text": "The output layer uses h(t)= tanh ( c(t ) ) as the score for the True class and v=2 as the score for the False class .", "entities": []}, {"text": "g(t)= tanh ( u [ 1\u00001 ] x(t ) ) f(t)=\u001b(m ) i(t)=\u001b(m ) o(t)=\u001b(m ) ^y(t)=[1 0 ] h(t)+[0 v=2 ] A.2 SP Task Network ( Counter - Based )", "entities": [[17, 18, "DatasetName", "0"]]}, {"text": "The seven counters for the SP task are implemented as follows .", "entities": []}, {"text": "First , we compute g(t)under the assumption that one of the \ufb01rst four counters is always incremented , and one of the last three counters is always incremented as long as x(t)\u0338=a .", "entities": []}, {"text": "g(t)= tanh0 BB@u2 664I4 0 1 0 0 0 0 1 0 0 0 0 13 775x(t)1 CCA", "entities": [[4, 5, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [14, 15, "DatasetName", "0"]]}, {"text": "Then , we use the input gate to condition the last three counters on the value of the \ufb01rst four counters .", "entities": []}, {"text": "For example , if h(t\u00001 ) 1 = 0 , then no as have been encountered in the input string before time t.", "entities": [[8, 9, "DatasetName", "0"]]}, {"text": "In that case , the input gate for counter # 5 , which represents subsequences ending with b , is set to i(t ) 5=\u001b(\u0000m)\u00190 .", "entities": []}, {"text": "This is because a bencountered at time twould not form part of a subsequence if no as have been encountered so far , so counter # 5 should not be incremented .", "entities": []}, {"text": "i(t)=\u001b0", "entities": []}, {"text": "BB@2m2 6640 0 1 0 0 0 0 1 0 1 0 0 1 003 775h(t\u00001 ) + m [ 1 1 1 1 \u00001\u00001\u00001]\u22a4 )", "entities": [[2, 3, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}, {"text": "All other gates are \ufb01xed to 1 .", "entities": []}, {"text": "The output layer sets the score of the True class to h(t ) 5+h(t ) 6+h(t ) 7and the score of the False class to v=2 .", "entities": []}, {"text": "f(t)=\u001b(m1 ) o(t)=\u001b(m1 ) ^y(t)=[01 1 1 00 0 0 ] h(t)+[0 v=2 ]", "entities": [[8, 9, "DatasetName", "0"], [9, 10, "DatasetName", "0"]]}, {"text": "311A.3 FSA Network Here we describe a general construction of an LSTM simulating an FSA with states Q , accepting states QF\u0012Q , alphabet \u0006 , and transition function\u000e:Q\u0002\u0006!Q. Recall that h(t)contains a one - hot representation of pairs in Q\u0002\u0006encoding the current state of the FSA and the most recent input symbol .", "entities": [[11, 12, "MethodName", "LSTM"], [29, 30, "MetricName", "Recall"]]}, {"text": "The initial state h(0)=0represents the starting con\ufb01guration of the FSA .", "entities": []}, {"text": "At a high level , the state transition system works as follows .", "entities": []}, {"text": "First , g(t)\ufb01rst marks all the positions corresponding to the current input x(t).6 g(t ) \u27e8q;x\u27e9= { v ; x = x(t ) 0;otherwise", "entities": []}, {"text": "The input gate then \ufb01lters out any positions that do not represent valid transitions from the previous state q\u2032 , which is recovered from h(t\u00001 ) .", "entities": []}, {"text": "i(t ) \u27e8q;x\u27e9= { 1 ; \u000e(q\u2032 ; x ) = q 0;otherwise", "entities": []}, {"text": "Now , we describe how this behavior is implemented in our LSTM .", "entities": [[11, 12, "MethodName", "LSTM"]]}, {"text": "The cell state update is straightforwardly implemented as follows :", "entities": []}, {"text": "g(t)= tanh ( uW(c;x)x(t ) ) , where W(c;x ) \u27e8q;x\u27e9;j= { 1 ; j is the index for x 0;otherwise .", "entities": []}, {"text": "Observe that the matrix W(c;x)essentially contains a copy of I4for each state , such that each copy is distributed across the different cell state units designated for that state .", "entities": []}, {"text": "The input gate is more complex .", "entities": []}, {"text": "First , the bias term handles the case where the current case is the starting state q0 .", "entities": []}, {"text": "This is necessary because the initial con\ufb01guration of the network is represented by h(0)=0 .", "entities": []}, {"text": "b(i ) \u27e8q;x\u27e9= { m ; \u000e ( q0 ; x ) = q \u0000m ; otherwise The bias vector sets i(t ) \u27e8q;x\u27e9to be 1if the FSA transitions from q0toqafter reading x , and 0otherwise .", "entities": []}, {"text": "We replicate this behavior for other values 6We use v= tanh(1 ) \u00190:762.ofh(t\u00001)by using the weight matrix W(i;h ) , taking the bias vector into account : i(t)=\u001b ( W(i;h)h(t\u00001)+b(i ) ) , where W(i ) \u27e8q;x\u27e9;\u27e8q\u2032;x\u2032\u27e9= { m\u0000b(i ) \u27e8q;x\u27e9 ; \u000e ( q\u2032 ; x )", "entities": []}, {"text": "= q \u0000m\u0000b(i ) \u27e8q;x\u27e9;otherwise .", "entities": []}, {"text": "The forget gate is \ufb01xed to \u00001 , since the state needs to be updated at every time step .", "entities": []}, {"text": "The output gate is \ufb01xed to 1 . f(t)=\u001b(\u0000m1 ) o(t)=\u001b(m1 )", "entities": []}, {"text": "The output layer simply selects hidden units that represent accepting and rejecting states : ^y(t)=W h(t ) , where Wc;\u27e8q;x\u27e9=8 > < > :1 ; c = True andq2QF 1 ; c = False andq = 2QF 0;otherwise .", "entities": []}, {"text": "A.4 PDA Network", "entities": []}, {"text": "Finally , we describe how the PDA network for the bracket prediction task is implemented .", "entities": []}, {"text": "Of the four networks , this one is the most intricate .", "entities": []}, {"text": "Recall from Subsection 4.2 that we implement a bounded stack of size kusing 2k+ 1hidden units , with the following interpretation : \u000fc(t ) : k\u00001contains the stack , except for the top item \u000fc(t ) kcontains the top item of the stack \u000fc(t ) k+1:2 kcontains the height of the stack in unary notation \u000fc2k+1is a bit , which is set to be positive if the stack is empty and nonpositive otherwise .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "We represent the brackets ( , [ , ) , and ] in onehot encoding with the indices 1,2,3 , and 4 , respectively .", "entities": []}, {"text": "The opening brackets ( and[are represented on the stack by 1and\u00001 , respectively .", "entities": []}, {"text": "T", "entities": []}, {"text": "312We begin by describing g(t ) .", "entities": []}, {"text": "Due to the complexity of the network , we describe the weights and biases individually , which are combined as follows .", "entities": []}, {"text": "g(t)= tanh ( m ( z(g;t ) ) ) , where z(g;t)=W(c;x)x(t)+W(c;h)h(t\u00001)+b(c )", "entities": []}, {"text": "First , the bias vector sets c(t ) 2k+1to be 1 , indicating that the stack is empty .", "entities": []}, {"text": "This ensures that the initial hidden state h(t)=0is treated as an empty stack .", "entities": []}, {"text": "b(c)=[0", "entities": []}, {"text": "2 ] W(c;x)serves three functions when x(t)is an open bracket , and does nothing when x(t)is a closing bracket .", "entities": []}, {"text": "First , it pushes x(t)to the top of the stack , represented by c(t ) k.", "entities": []}, {"text": "The values \u00062kare determined by equation ( 1 ) in Subsection 4.2 .", "entities": []}, {"text": "Second , it sets g(t ) k+1:2 kto1 in order to increment the unary counter for the height of the stack .", "entities": []}, {"text": "Later , we will see that the input gate \ufb01lters out all positions except for the top of the stack .", "entities": []}, {"text": "Finally , W(c;x)sets the empty stack indicator to \u00001 , indicating that the stack is not empty .", "entities": []}, {"text": "W(c;x)=2 6640 0 0 0 2k\u00002k0 0 1 1 0 0 \u00002\u00002 0 03 775 W(c;h)performs two functions .", "entities": [[2, 3, "DatasetName", "0"], [3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [12, 13, "DatasetName", "0"]]}, {"text": "First , it completes equation ( 1 ) for c(t ) k , setting it to be the secondhighest stack item from the previous time step .", "entities": []}, {"text": "Second , it copies the top of the stack to the \ufb01rst k\u00001positions , with the input gate \ufb01ltering out all but the highest position .", "entities": []}, {"text": "W(c;h)=2 6640 1 0 0 2 4 \u0001\u0001\u0001 2k\u0000100 0 0 0 0 0 0 0\u0000103 775 Finally , the \u00001s serve to decrease the empty stack indicator by an amount proportional to the stack height at time t\u00001 .", "entities": [[3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [10, 11, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [14, 15, "DatasetName", "0"]]}, {"text": "Observe that if x(t)is a closing bracket and h(t\u00001)represents a stack with only one item , then W(c;x ) 2k+1;:x(t)+W(c;h ) 2k+1;:h(t\u00001)+b(c ) 2k+1 = \u00001 + 2 = 1 , so the empty stack indicator is set to 1 , indicating that the stack is empty .", "entities": []}, {"text": "Otherwise , W(c;x ) 2k+1;:x(t)+W(c;h ) 2k+1;:h(t\u00001)\u0014 \u00002 , so the empty stack indicator is nonpositive .", "entities": []}, {"text": "Now , we describe the input gate , given by the following .", "entities": []}, {"text": "i(t)=\u001b ( m ( z(i;t ) ) )", "entities": []}, {"text": "z(i;t)=W(i;x)x(t)+W(i;h)h(t\u00001)+b(i ) W(i;x)sets the input gate for the \ufb01rst k\u00001positions to 0when x(t)is a closing bracket .", "entities": []}, {"text": "In that case , an item needs to be popped from the stack , so nothing can be copied to these hidden units .", "entities": []}, {"text": "When x(t)is an opening bracket , W(i;x)setsi(t ) k= 1 , so that the bracket can be copied to the top of the stack .", "entities": []}, {"text": "W(i;x)= 22", "entities": []}, {"text": "40 0 \u00001\u00001 1 1 0 0 03 5 W(i;h)uses a matrix Tn2Rn\u0002n , de\ufb01ned below .", "entities": [[1, 2, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [6, 7, "DatasetName", "0"]]}, {"text": "Tn=2 6666641\u00001 0 : : : 0 0 0 1 \u00001 : : : 0 0 .................. 0 0 0 : : : 1\u00001 0 0 0 : : : 0 13 777775 Suppose vrepresents the number sin unary notation : vjis1ifj\u0014sand0otherwise .", "entities": [[2, 3, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [14, 15, "DatasetName", "0"], [15, 16, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [25, 26, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}, {"text": "Tnhas the special property that Tnvis a one - hot vector for s. Based on this , W(i;h)is de\ufb01ned as follows .", "entities": []}, {"text": "W(i;h)= 22 6640(Tk):k\u00001 ; : 0 ( Tk):k\u00001 ; : 003 775 W(i;h ) : k\u00001;k+1:2 kcontains Tk , with the last row truncated .", "entities": [[5, 6, "DatasetName", "0"]]}, {"text": "This portion of the matrix converts h(t\u00001 ) k+1:2 k , which contains a unary encoding of the stack height , to a one - hot vector marking the position of the top of the stack .", "entities": []}, {"text": "This ensures that , when pushing to the stack , the top stack item from time t\u00001is only copied to the appropriate position of h(t ) : k\u00001 .", "entities": []}, {"text": "The other copy of Tk , again with the last row omitted , occurs in W(i;h )", "entities": []}, {"text": "k+2:2 k;k+1:2 k.", "entities": []}, {"text": "This copy ofTkensures that when the unary counter for the", "entities": []}, {"text": "313stack height is incremented , only the appropriate position is updated .", "entities": []}, {"text": "Finally , the bias vector ensures that the top stack item and the empty stack indicator are always updated .", "entities": []}, {"text": "b(i)=2 664\u00001 1 \u00001 13 775", "entities": []}, {"text": "The forget gate is responsible for deleting portions of memory when stack items are popped .", "entities": []}, {"text": "f(t)=\u001b ( m ( z(f;t ) ) )", "entities": []}, {"text": "z(f;t)=W(f;x)x(t)+W(f;h)h(t\u00001)+b(f )", "entities": []}, {"text": "W(f;x)\ufb01rst ensures that no stack items are deleted when an item is pushed to the stack .", "entities": []}, {"text": "W(f;x)=", "entities": []}, {"text": "22 6641 1 0 0 0 0 0 0 1 1 0 0 0 0 0 03 775 Next , W(f;h)marks the second highest stack position and the top of the unary counter for deletion , in case an item needs to be popped .", "entities": [[3, 4, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [5, 6, "DatasetName", "0"], [6, 7, "DatasetName", "0"], [7, 8, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [14, 15, "DatasetName", "0"], [15, 16, "DatasetName", "0"]]}, {"text": "W(f;h)= 22 6640\u0000(Tk)2 : ; : 0 \u0000Tk 003 775", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "Finally , the bias term ensures that the top stack item and empty stack indicator are always cleared .", "entities": []}, {"text": "b(i)=2 6641 \u00001 1 \u000013 775 To complete the construction , we \ufb01x the output gate to 1 , and have the output layer read the top stack position : o(t)=\u001b(m1 ) ^y(t)=W h(t ) , where Wc;j=8 > > > > < > > > > :1 ; c =) andj = k \u00001 ; c =] andj = k 1 ; c", "entities": []}, {"text": "= None andj= 2k+ 1 0 ; otherwise .", "entities": [[5, 6, "DatasetName", "0"]]}]