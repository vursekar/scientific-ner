[{"text": "Proceedings of the Fifteenth Workshop on Graph - Based Methods for Natural Language Processing ( TextGraphs-15 ) , pages 67\u201382 June 11 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics67WikiGraphs : A Wikipedia Text - Knowledge Graph Paired Dataset Luyu Wang*and", "entities": []}, {"text": "Yujia Li*and Ozlem Aslan and Oriol Vinyals * Equal contribution DeepMind , London , UK { luyuwang,yujiali,ozlema,vinyals}@google.com Abstract We present a new dataset of Wikipedia articles each paired with a knowledge graph , to facilitate the research in conditional text generation , graph generation and graph representation learning .", "entities": [[38, 41, "TaskName", "conditional text generation"], [42, 44, "TaskName", "graph generation"], [45, 48, "TaskName", "graph representation learning"]]}, {"text": "Existing graph - text paired datasets typically contain small graphs and short text ( 1 or few sentences ) , thus limiting the capabilities of the models that can be learned on the data .", "entities": []}, {"text": "Our new dataset WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark ( Merity et al . , 2016 ) with a subgraph from the Freebase knowledge graph ( Bollacker et al . , 2008 ) .", "entities": [[3, 4, "DatasetName", "WikiGraphs"], [14, 15, "DatasetName", "WikiText-103"]]}, {"text": "This makes it easy to benchmark against other state - of - the - art text generative models that are capable of generating long paragraphs of coherent text .", "entities": []}, {"text": "Both the graphs and the text data are of signi\ufb01cantly larger scale compared to prior graph - text paired datasets .", "entities": []}, {"text": "We present baseline graph neural network and transformer model results on our dataset for 3 tasks : graph ! text generation , graph ! text retrieval and text!graph retrieval .", "entities": [[19, 21, "TaskName", "text generation"]]}, {"text": "We show that better conditioning on the graph provides gains in generation and retrieval quality but there is still large room for improvement.1 1 Introduction Parallel datasets that pair data from different sources and modalities have enabled large amounts of research on cross modality learning .", "entities": []}, {"text": "Paired image - caption datasets enable models to describe visual scenes in natural language ( Lin et al . , 2014 ; Vinyals et al . , 2016 ) , paired streams of speech and transcription data makes it possible to train speech recognition systems ( Garofolo et", "entities": [[42, 44, "TaskName", "speech recognition"]]}, {"text": "al . , 1993 ; Panayotov et al . , 2015 ) or text - to - speech synthesis models ( Oord et al . , 2016 ) , and parallel corpus of text in different languages enable learned machine translation models ( Barrault et al . , 2020 ) .", "entities": [[13, 19, "TaskName", "text - to - speech synthesis"], [39, 41, "TaskName", "machine translation"]]}, {"text": "1The data and the code to reproduce our baseline results are available at https://github.com/deepmind/ deepmind - research / tree / master / wikigraphs \u201c Where the Streets Have No Name \u201d is a song by Irish rock band U2 .", "entities": [[22, 23, "DatasetName", "wikigraphs"]]}, {"text": "It is the opening track from their 1987 album The Joshua Tree and was released as the album \u2019s third single in August 1987 .", "entities": []}, {"text": "The song \u2019s hook is a repeating guitar arpeggio using a delay effect , played during the song \u2019s introduction and again at the end .", "entities": []}, {"text": "Lead vocalist Bono wrote ... Freebase ns / m.06t2j0ns / m.01vswwxmusic.composer.compositions ns / m.01vswx5music.composer.compositionsns / m.074ftns / music.composition.formSongkey / wikipedia.enBonokey / wikipedia.enPaul", "entities": []}, {"text": "Hewsontype.object.nameWhere the streets have no namekey / wikipedia.enpeople.person.date of birth1960 - 05 - 10 The Edgekey / wikipedia.en1.77mpeople.person.date of birth1961 - 08 - 08 people.person.height metersWikiText-103 ns / music.composition.language David Howell Evans , better known by his stage name The Edge , is a British - born Irish musician , songwriter ... ns / common.topic.descriptionns / m.02h40lcFigure 1 : Illustration of a pair of Wikipedia article and the corresponding knowledge graph in our dataset .", "entities": []}, {"text": "We present a new dataset of Wikipedia text articles each paired with a relevant knowledge graph ( KG ) , which enables building models that can generate long text conditioned on a graph structured overview of relevant topics , and also models that extract or generate graphs from a text description .", "entities": []}, {"text": "There has been many prior efforts trying to build datasets for learning graph !", "entities": []}, {"text": "text generation models ( Jin et al . , 2020 ;", "entities": [[0, 2, "TaskName", "text generation"]]}, {"text": "Gardent et al . , 2017 ; Lebret et al . , 2016 ) .", "entities": []}, {"text": "However , existing graph - text paired datasets are mostly small scale , where the graphs tend to have 10 - 20 or even less nodes , and the text typically only contains one or a few sentences .", "entities": []}, {"text": "This represents a signi\ufb01cant contrast with the state - ofthe - art text generation models ( Dai et al . , 2019 ; Brown et", "entities": [[12, 14, "TaskName", "text generation"]]}, {"text": "al . , 2020 ) , which can already generate very \ufb02uent and long text that spans thousands of tokens over multiple paragraphs .", "entities": []}, {"text": "We attempt to bridge this gap , with the goal of advancing the state - of - the - art graph ! text generation models , graph representation learning models and also text - conditioned graph generative models .", "entities": [[22, 24, "TaskName", "text generation"], [26, 29, "TaskName", "graph representation learning"]]}, {"text": "Each text document in our dataset is a full - length Wikipedia article , and we pair each of them with a KG that are signi\ufb01cantly bigger than prior datasets of similar nature and includes much richer information .", "entities": []}, {"text": "Hand labelling text articles with KGs is expensive and not scalable ( Lebret et al . , 2016 ) ,", "entities": []}, {"text": "68therefore we utilize an existing and established knowledge base , Freebase ( Bollacker et al . , 2008 ) , and designed an automated process to extract a relevant subgraph from it for each Wikipedia article .", "entities": []}, {"text": "To make the text generation results on our dataset directly comparable to the state - of - the - art , we chose the set of Wikipedia articles from the established language modeling benchmark WikiText103 ( Merity et al . , 2016 ) , which contains a subset of high - quality Wikipedia articles .", "entities": [[3, 5, "TaskName", "text generation"]]}, {"text": "This gives us a dataset of 23,522 graph - text pairs in total , covering 82.3 % of Wikitext-103 articles .", "entities": [[18, 19, "DatasetName", "Wikitext-103"]]}, {"text": "On average each graph has 38.7 nodes and 48.3 edges , and each text article contains 3,533.8 tokens .", "entities": []}, {"text": "In addition to structural information , our graphs also contain rich text information with an average of 895.1 tokens in each graph .", "entities": []}, {"text": "Furthermore , the automatic process we used to create this dataset can be extended to pair any Wikipedia document with Freebase , and can be scaled up to create over 3 M graph - text pairs .", "entities": []}, {"text": "Out of many exciting new tasks that this dataset enables , we present 3 possibilities : graph ! text generation , graph !", "entities": [[18, 20, "TaskName", "text generation"]]}, {"text": "text retrieval , and text !", "entities": []}, {"text": "graph retrieval .", "entities": []}, {"text": "We benchmarked a few baseline models on these tasks .", "entities": []}, {"text": "The models we considered were based on the recent Transformer - XL ( Dai et al . , 2019 ) model , and we adapted it to condition the text generation on the KG in different ways .", "entities": [[9, 12, "MethodName", "Transformer - XL"], [29, 31, "TaskName", "text generation"]]}, {"text": "Our results show that better conditioning on the graph indeed improves the relevance of the generated text and the retrieval quality .", "entities": []}, {"text": "However , there is still signi\ufb01cant room for improvement on these tasks , which makes this an exciting dataset for research .", "entities": []}, {"text": "Our data and code for baseline models will be made publicly available .", "entities": []}, {"text": "2 Related work Graph - text paired data There has been a lot of prior work on creating graph - text paired datasets .", "entities": []}, {"text": "Example applications include generating text summaries conditioned on Abstract Meaning Representation graphs ( Liu et al . , 2018 ) , generating the abstract of a scienti\ufb01c article given a KG and title ( Koncel - Kedziorski et al . , 2019 ) and generating text from RDF triples ( Gardent et al . , 2017 ; Jin et al . , 2020 ) .", "entities": []}, {"text": "In the following we will mostly review related work on KG - text paired datasets .", "entities": []}, {"text": "Annotating KG or text to create paired datasets is expensive , as a good quality annotation requires annotators that understand the content and structure of the text and the corresponding KG ( Jin et al . ,Dataset", "entities": []}, {"text": "# examples # triples # tokens # vocab WebNLG 13,036 2.54 15.26 1,484 GenWiki 1.3 M 1.95 21.46 476,341 Ours 23,522 48.3 3,533.8 238,071 Table 1 : Our dataset contains signi\ufb01cantly larger graphs ( average # triples per graph ) and longer text ( average # tokens per text ) than previous KG - text datasets . 2020 ) .", "entities": [[8, 9, "DatasetName", "WebNLG"], [13, 14, "DatasetName", "GenWiki"]]}, {"text": "Therefore previous KG - text paired datasets that rely on human annotation have limited scale .", "entities": []}, {"text": "Among these , Gardent et al .", "entities": []}, {"text": "( 2017 ) crowdsourced human annotators to verbalize RDF triplets taken from DBpedia ( Auer et al . , 2007 ) to a few sentences ( WebNLG ) and this caused errors in annotation that were \ufb01xed with a few updates through years .", "entities": [[12, 13, "DatasetName", "DBpedia"], [26, 27, "DatasetName", "WebNLG"]]}, {"text": "Parikh et al .", "entities": []}, {"text": "( 2020 ) paired Wikipedia Table with one sentence text that is created by annotators that revise Wikipedia text .", "entities": []}, {"text": "Another line of research focuses on eliminating the need of human annotations by automatically matching KG - text pairs or generating KGs from text using existing tools .", "entities": []}, {"text": "Lebret et al .", "entities": []}, {"text": "( 2016 ) automatically matched Wikipedia infobox of biographies with their \ufb01rst sentence .", "entities": []}, {"text": "Koncel - Kedziorski et al .", "entities": []}, {"text": "( 2019 ) utilized an earlier information extraction system that extracts entities , co - reference and relations from given text to build KG \u2019s .", "entities": []}, {"text": "The GenWiki dataset ( Jin et al . , 2020 ) is automatically constructed by querying KGs in DBpedia with the title of articles in Wikipedia followed by \ufb01ltering and entity annotation .", "entities": [[1, 2, "DatasetName", "GenWiki"], [18, 19, "DatasetName", "DBpedia"]]}, {"text": "We construct our WikiGraphs dataset by extracting a subgraph from Freebase ( Bollacker et al . , 2008 ) for each Wikipedia article following a scalable automatic process .", "entities": [[3, 4, "DatasetName", "WikiGraphs"]]}, {"text": "Compared to previous work , our WikiGraphs dataset contains signi\ufb01cantly larger graphs and longer text ( Table 1 ) .", "entities": [[6, 7, "DatasetName", "WikiGraphs"]]}, {"text": "Models for graph - text paired data Recent state of art language models are based on the Transformer architecture ( Vaswani et al . , 2017 ) that uses the self attention mechanism .", "entities": [[17, 18, "MethodName", "Transformer"]]}, {"text": "The TransformerXL ( Dai et al . , 2019 ) model further introduces a segment level recurrence with a novel positional encoding resulting in impressive performance in long sequences by capturing dependencies beyond a \ufb01xed length window .", "entities": []}, {"text": "Graph neural networks ( GNNs ) ( Battaglia et al . , 2018 ; Gilmer et al . , 2017 ) learn representations for graph structured data through a message passing process .", "entities": []}, {"text": "This class of models naturally exploit", "entities": []}, {"text": "69Train Valid Test All Num . pairs 23,431 48 43 23,522 % of WikiText-103 82.3 % 80.0 % 71.7 % 82.3 % Nodes per graph 38.7 35.4 40.6 38.7 Edges per graph 48.3 42.8 49.5 48.3 Avg .", "entities": [[13, 14, "DatasetName", "WikiText-103"]]}, {"text": "Node degree 2.5 2.4 2.4 2.5 Tokens per graph 895.1 807.7 1,010.1 895.1 Total graph tokens 21.0 M 38,771 43,435 21.1 M Graph vocab size - - - 31,090 Tokens per article 3,531.7 3,644.2 4,564.7 3,533.8 Total text tokens 82.8 M 174,923 196,280", "entities": []}, {"text": "83.1 M Text vocab size - - - 238,071 Table 2 : Basic statistics about our WikiGraphs dataset .", "entities": [[16, 17, "DatasetName", "WikiGraphs"]]}, {"text": "the graph structures , making them a good \ufb01t for graph data .", "entities": []}, {"text": "GNNs have been used in many applications on KG \u2019s ( Kipf and Welling , 2016 ;", "entities": []}, {"text": "Wang et al . , 2019 ; Xu et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "Fundamentally , transformers can also be understood as a special type of GNNs with a fully - connected graph structure .", "entities": []}, {"text": "The most recent prior work on graph - to - text generation follows an encoder - decoder architecture ( Koncel - Kedziorski et al . , 2019 ; Jin et al . , 2020 ) , where the graph part is encoded with a GNN model , e.g. Graph Attention Network ( GAT ) ( Veli \u02c7ckovi \u00b4 c et al . , 2018 ) .", "entities": [[10, 12, "TaskName", "text generation"], [48, 51, "MethodName", "Graph Attention Network"], [52, 53, "MethodName", "GAT"]]}, {"text": "The text part is typically modeled using an attention based decoder with a copy mechanism ( e.g. BiLSTMs as in ( Jin et al . , 2020 ) ) to process input from both the KG and text .", "entities": []}, {"text": "The models we benchmarked for graph - to - text generation were based on the Transformer - XL architecture and conditioned on the graph through a GNN , making full use of the graph structure and capable of generating very long text comparable to the state - of - the - art .", "entities": [[9, 11, "TaskName", "text generation"], [15, 18, "MethodName", "Transformer - XL"]]}, {"text": "3 Dataset", "entities": []}, {"text": "In this section we \ufb01rst present some properties of our dataset , and then describe the process that we used to create it .", "entities": []}, {"text": "3.1 Properties of the data 3.1.1 Scale of the data Basic statistics about our WikiGraphs dataset are listed in Table 2 .", "entities": [[14, 15, "DatasetName", "WikiGraphs"]]}, {"text": "An illustration of a graph - text pair is shown in Figure 1 .", "entities": []}, {"text": "A few actual examples from our dataset are included in the Appendix ( Figure 7 , 8) .", "entities": []}, {"text": "All of the articles come from the WikiText-103 dataset ( Merity et al . , 2016 ) , which contains highquality articles that \ufb01t the Good orFeatured criteria speci\ufb01ed by the Wikipedia editors when the data was collected .", "entities": [[7, 8, "DatasetName", "WikiText-103"]]}, {"text": "Merity et al .", "entities": []}, {"text": "( 2016 ) have already cleaned up and tokenized the articles , thereforethey appear as plain text without any markup tags .", "entities": []}, {"text": "As will be described in Section 3.2 , we try to pair each article with a subgraph from Freebase , centered at the entity node that has a Wikipedia link to the title of the article .", "entities": []}, {"text": "We are not able to match every article to an entity in Freebase , but through this process we retained a signi\ufb01cant portion of 82.3 % of the WikiText-103 articles .", "entities": [[28, 29, "DatasetName", "WikiText-103"]]}, {"text": "We kept the original train / valid / test split .", "entities": []}, {"text": "As we will see in Section 4.2 , training models on this set gives us results that are very close to training on the full WikiText-103 dataset when evaluated on our test set .", "entities": [[25, 26, "DatasetName", "WikiText-103"]]}, {"text": "Therefore the text part of WikiGraphs appears to be suf\ufb01cient to reproduce and benchmark against the state - of - the - art text generative models .", "entities": [[5, 6, "DatasetName", "WikiGraphs"]]}, {"text": "Figure 2 shows the distribution of graph sizes and article lengths across our dataset .", "entities": []}, {"text": "All the distributions are skewed with a long tail .", "entities": []}, {"text": "Notably , average graph size in our dataset is 38.7 nodes and 48.3 edges , considerably larger than the graphs in previous datasets ( Jin et al . , 2020 ;", "entities": []}, {"text": "Gardent et al . , 2017 ) .", "entities": []}, {"text": "Also the length of the text articles averages to 3,533.8 tokens and can go up to 26,994 tokens , which is orders of magnitudes longer than the text data in previous graph - text paired datasets that typically only contains a single or few sentences ( Jin et al . , 2020 ; Gardent et", "entities": []}, {"text": "al . , 2017 ; Lebret et al . , 2016 ) .", "entities": []}, {"text": "3.1.2 Nodes and edges The graphs in our dataset contains two types of nodes : entities and string literals .", "entities": []}, {"text": "Each entity is labeled by a unique Freebase entity ID , e.g. ns / m.0f9q9z , and each string literal contains some natural language text , that could be for example a name , date , or description of an entity .", "entities": []}, {"text": "Each edge in the graphs also has an associated edge label , e.g. ns / common.topic.description , indicating which type of edge it is .", "entities": []}, {"text": "There are a total of 522 different edge types in our dataset .", "entities": []}, {"text": "Figure 3 shows the frequency of all the different edge types in our dataset .", "entities": []}, {"text": "Every graph always has one entity node ( we call it \u201c center node \u201d ) that has a link to the paired Wikipedia article , through a special edge key / wikipedia.en , and the whole graph is a 1 - hop neighborhood of entities around the center node within the bigger Freebase KG , plus the string literals associated with all the entities included .", "entities": []}, {"text": "Note that it is possible to have edges between the 1 - hop neighbors of the center node , therefore the graphs typically are not star structured .", "entities": []}, {"text": "Section 3.2", "entities": []}, {"text": "70 0 50 100 150 200 250 Nodes per graph010002000300040005000600070008000CountMin 3 , Mean 38.7 , Max 255 0 100 200 300 400 500 Edges per graph0200040006000800010000CountMin 2 , Mean 48.3 , Max 504 0 2000 4000 6000 8000 Tokens per graph02000400060008000CountMin 7 , Mean 895.1 , Max 9092 0 5000 10000 15000 20000 25000 Tokens per article010002000300040005000600070008000CountMin 69 , Mean 3533.8 , Max 26994Figure 2 : Distribution of graph and article sizes across our WikiGraphs dataset .", "entities": [[1, 2, "DatasetName", "0"], [17, 18, "DatasetName", "0"], [33, 34, "DatasetName", "0"], [48, 49, "DatasetName", "0"], [74, 75, "DatasetName", "WikiGraphs"]]}, {"text": "0 100 200 300 400 500 Rank of edge type100101102103104105Edge type frequency Min 1 , Mean 2,177.0 , Max 163,722 Figure 3 : Edge type distribution roughly follows an inverse exponential law .", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "0 50 100 150 200 Number of nodes in a graph02500500075001000012500150001750020000Counttype Entity String literal Figure 4 : Distribution of the per - graph number of entity nodes and string literal nodes in our dataset .", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "provides more details about how these graphs are constructed and any additional \ufb01ltering we did .", "entities": []}, {"text": "One special characteristic about our graph data is that the natural language text contained in the string literal nodes can sometimes be quite long ( see e.g. Figure 7,8 ) , and therefore provide much richer information not included in the graph structure itself .", "entities": []}, {"text": "On average , each graph contains 895.1 tokens across all the string literal nodes in one graph ( Table 2 , Figure 2 , \u201c Tokens per graph \u201d ) .", "entities": []}, {"text": "Figure 4 shows the distribution of per - graph number of entity nodes and string literal nodes in our dataset .", "entities": []}, {"text": "We can see that our graphs tend to have more string literal nodes than entity nodes , indicating that the entities are supplemented with the rich information in the string literals .", "entities": []}, {"text": "The distribution of information is not uniform across the nodes in a graph .", "entities": []}, {"text": "Figure 5 shows that most entity nodes in our graph has a small degree , while few nodes have much larger degrees .", "entities": []}, {"text": "Also most string literal nodes contain short text , while fewer nodes contain longer text .", "entities": []}, {"text": "The skewed distribution of nodes and edges in our dataset re\ufb02ect the nature of KG \u2019s like Freebase , and presents new challenges to graph representation learning models .", "entities": [[24, 27, "TaskName", "graph representation learning"]]}, {"text": "3.2 The dataset construction process We follow three principles when designing the dataset construction process : 1.The text part of the data should be directly comparable in complexity to the capability of state - of - the - art text generative models .", "entities": []}, {"text": "2.The graph part of the data should be constructed in an automatic and scalable way .", "entities": []}, {"text": "3.The graph part of the data should be relevant for the paired text data .", "entities": []}, {"text": "Note that our process is general , and can be applied to any set of Wikipedia articles .", "entities": []}, {"text": "We have tried to pair a full dump of English Wikipedia with Freebase and managed to get over 3 million graphtext pairs .", "entities": []}, {"text": "Here we restrict the process to the set of articles from the WikiText-103 dataset .", "entities": [[12, 13, "DatasetName", "WikiText-103"]]}, {"text": "We try to map each Wikipedia article to a relevant subgraph of the existing large scale KG Freebase ( Bollacker et al . , 2008 ) .", "entities": []}, {"text": "We used the last public dump of Freebase2 , which contains 1.9B triples and a total of 250 GB of data .", "entities": []}, {"text": "We \ufb01ltered the data by keeping only the entities with at least 4 string attributes ( otherwise the entities are less interpretable ) , and keeping only the top 1024 most frequent relation types and restricting the relations to 2https://developers.google.com/ freebase", "entities": []}, {"text": "71 0 20 40 60 80 100 120 140 Node degree0200040006000800010000CountMin 2 , Mean 12.6 , Max 140 0 10 20 30 40 50 60 70 Node degree020000400006000080000CountMin 2 , Mean 7.6 , Max 72 0 200 400 600 800 Number of tokens per string literal0100000200000300000400000500000600000CountMin 1 , Mean 28.6 , Max 872(a ) Center node degree dist .", "entities": [[1, 2, "DatasetName", "0"], [18, 19, "DatasetName", "0"], [35, 36, "DatasetName", "0"]]}, {"text": "( b ) Non - center node degree dist .", "entities": []}, {"text": "( c ) String literal node length dist .", "entities": []}, {"text": "Figure 5 : Node degree distribution for entity nodes and token count distribution for string literal nodes .", "entities": []}, {"text": "only those among the retained entities and between the entities and string attributes .", "entities": []}, {"text": "We also simpli\ufb01ed the entity and relation names by stripping off the irrelevant \u201c http://rdf.freebase.com/ \u201d and further removed duplicates .", "entities": []}, {"text": "This gives us a signi\ufb01cantly cleaner and smaller backbone graph for Freebase , with about 20 M nodes .", "entities": []}, {"text": "Finding the relevant subgraph for an article in such a cleaned up but still large KG remains nontrivial .", "entities": []}, {"text": "Our process for this contains 3 stages : mapping , expansion , and \ufb01ltering .", "entities": []}, {"text": "Mapping In the \ufb01rst stage of the process , we map each article into an entity in our processed Freebase KG .", "entities": []}, {"text": "This is made possible through triples from Freebase like the following : ns / g.11b6jbqpt4 key / wikipedia.en \" Madunnella \" where ns / g.11b6jbqpt4 refers to an entity in the KG , key / wikipedia.en is the type of the edge , which indicates that this entity is linked to a Wikipedia article and \u201c Madunnella \u201d is the title of that article .", "entities": []}, {"text": "We normalize the title string ( and in general any string literals ) from Freebase by replacing \u201c _ \u201d with white space and handle unicode characters properly .", "entities": []}, {"text": "We extract the titles from the Wikipedia article through string matching , where titles are enclosed in a \u201c = [ title ] = \" pattern .", "entities": []}, {"text": "In this step we managed to map 24,345 out of 28,475 ( 85.5 % ) article titles from WikiText-103 to an entity in our KG .", "entities": [[18, 19, "DatasetName", "WikiText-103"]]}, {"text": "Expansion We treat each of the mapped entities as the center node of a subgraph , and expand 1 hop out in the entire \ufb01ltered Freebase graph to include all the neighboring entities that are the most relevant to the center entity .", "entities": []}, {"text": "We then expand further from this 1 - hop graph out to include all the relations that connect the selected entities to string attributes as well as between these entities themselves .", "entities": []}, {"text": "Note that because of these edges between the 1 - hop neighbor entities the graphs are typicallynot star structured .", "entities": []}, {"text": "This gives us a relevant but compact graph for each article .", "entities": []}, {"text": "We have also investigated the possibility of a 2 - hop neighborhood from the center node , and found that 2 - hop neighborhoods are signi\ufb01cantly larger than 1 - hop and through some \u201c hub \u201d nodes like \u201c Male \u201d or \u201c Female \u201d a 2 - hop neighborhood from an entity can easily include many other irrelevant entities .", "entities": []}, {"text": "Based on such observations we decided to use the 1 - hop neighborhood to keep the relevance of the subgraph high .", "entities": []}, {"text": "Filtering The last stage of the process involves more \ufb01ltering and cleaning up of the data .", "entities": []}, {"text": "We noticed that in Freebase it is common for one entity to have multiple relations of the same type pointing to different string attributes , like the following : ns / m.07c72 key / wikipedia.en \" The SImpsons \" ns / m.07c72 key / wikipedia.en \" The Simpson \" ns / m.07c72 key / wikipedia.en \" The simsons \" ns / m.07c72 key / wikipedia.en \" Thr Simpsons \" ns / m.07c72 key / wikipedia.en \" The Simpson \u2019s \" It is clear that there is a lot of redundancy in this data .", "entities": []}, {"text": "We reduced all such edges ( from the same entity with the same edge type to string attributes ) to a single edge by picking the most \u201c canonical \u201d one .", "entities": []}, {"text": "This was done by \ufb01tting a unigram model to the characters in the collection of strings and using that model to pick the most likely string .", "entities": []}, {"text": "We also \ufb01ltered the graphs based on size and created three versions of the data with maximum graph size capped at 256 , 512 , and 1024 nodes , respectively .", "entities": []}, {"text": "All the statistics and results in the rest of the paper are based on graphs with a maximum size of 256 , but all versions of the data are made available online .", "entities": []}, {"text": "4 Experiments We perform a set of experiments to showcase how the text and graph information can be combined in a language model .", "entities": []}, {"text": "Speci\ufb01cally , we consider three", "entities": []}, {"text": "72tasks : text generation conditioned on the graph , graph retrieval given the text , and text retrieval given the graph .", "entities": [[2, 4, "TaskName", "text generation"]]}, {"text": "4.1 Graph - conditioned Transformer - XL", "entities": [[4, 7, "MethodName", "Transformer - XL"]]}, {"text": "In order to incorporate graph information into an advanced language model , we adapt the recent Transformer - XL model ( Dai et al . , 2019 ) to also attend to the graph features .", "entities": [[16, 19, "MethodName", "Transformer - XL"]]}, {"text": "At a high - level our model embeds the graph into a set of embedding vectors , and then exposes these embeddings to the Transformer - XL model as extra \u201c token \u201d embeddings to condition on .", "entities": [[24, 27, "MethodName", "Transformer - XL"]]}, {"text": "The size of this set depends on the graph model we choose .", "entities": []}, {"text": "Given the features for Ttext tokens Ht2RT\u0002d and features for T0graph \u201c tokens \u201d Hg2RT0\u0002d0 , we illustrate the graph - conditioned attention procedure with a single head as follows : Qt;Kt;Vt = HtWt q;HtWt k;HtWt v Kg;Vg = HgWg k;HgWg v At;Ag = QtK > t;QtK > g A;V=", "entities": []}, {"text": "[ At\u000eAg];[Vt\u000eVg ]", "entities": []}, {"text": "O = Masked - Softmax ( A)V", "entities": [[4, 5, "MethodName", "Softmax"]]}, {"text": "where", "entities": []}, {"text": "[ a\u000eb]stands for concatenation on the sequence dimension and thus A2RT\u0002(T+T0)and V2R(T+T0)\u0002dh , where dhis the head dimension .", "entities": []}, {"text": "In other words , comparing to the original Transformer - XL , our model also computes the attention scores between the text queries Qtand both the text keys Ktand the graph keys Kg .", "entities": [[8, 11, "MethodName", "Transformer - XL"]]}, {"text": "As a result , the attention outputs contain information from both the graph and the text context .", "entities": []}, {"text": "Note that this formulation is compatible with an additional memory ( Dai et al . , 2019 ) with minimal changes , as it simply adds in an extra set of \u201c tokens \u201d for the model to attend to .", "entities": []}, {"text": "We do n\u2019t use position encodings for the graph \u201c tokens \u201d as there is no sequential ordering for them .", "entities": []}, {"text": "In this work we consider three different approaches for encoding the graph structure : \u000fBag - of - words ( BoW ): we construct a single bag - of - words representation of all the tokens from both the nodes and edges in the graph .", "entities": []}, {"text": "Entity IDs and numeric values in the graph are replaced with special tokens < entity > and < number > .", "entities": []}, {"text": "The BoW vector is further projected using a linear layer to a latent space .", "entities": [[8, 10, "MethodName", "linear layer"]]}, {"text": "In this case T0= 1.\u000fNodes only ( Nodes ) : we construct separate BoW representations for each node and project each to an embedding and ignore the edges .", "entities": []}, {"text": "In this case T0is equal to the number of nodes in the graph .", "entities": []}, {"text": "\u000fGraph neural network ( GNN ): we embed BoW representations for both nodes and edges and then use a graph neural network ( Battaglia et al . , 2018 ) on top of those embeddings to compute a new set of node embeddings .", "entities": []}, {"text": "T0is equal to the number of nodes .", "entities": []}, {"text": "TheT0graph embeddings from this process are shared across all the time steps for text tokens .", "entities": []}, {"text": "This model can be further improved , e.g. by using word embeddings and text summarization techniques , but we leave these for future work .", "entities": [[10, 12, "TaskName", "word embeddings"], [13, 15, "TaskName", "text summarization"]]}, {"text": "4.1.1", "entities": []}, {"text": "Implementation details We reimplement the Transformer - XL model in Jax ( Bradbury et al . , 2018 ) .", "entities": [[5, 8, "MethodName", "Transformer - XL"]]}, {"text": "In our experiments , we employ the base model in ( Dai et al . , 2019 ) , except that we increase the tail shrinkage factor used for the adaptive softmax and input representations from 1 to 4 , which saves 63 % of the parameters without compromising the performance .", "entities": [[30, 32, "MethodName", "adaptive softmax"]]}, {"text": "On the full Wikitext-103 dataset , our implementation has a test perplexity of 24.2 ( published result for this base model was 24.0 ) .", "entities": [[3, 4, "DatasetName", "Wikitext-103"], [11, 12, "MetricName", "perplexity"]]}, {"text": "We train our models using the standard likelihood objective for language models with a total batch size of 64 on 8 V100 GPUs .", "entities": [[15, 17, "HyperparameterName", "batch size"]]}, {"text": "Adam optimizer is used with an initial learning rate of2:5\u000210\u00004 , which decays up to 200k steps following a cosine curve .", "entities": [[0, 1, "MethodName", "Adam"], [1, 2, "HyperparameterName", "optimizer"], [7, 9, "HyperparameterName", "learning rate"]]}, {"text": "During training , we use text segments of 150 steps and a memory of equal size .", "entities": []}, {"text": "When evaluating the model , we use a sequence length of 64 and memory size 640 .", "entities": []}, {"text": "Unless further noted , in our experiments we use an embedding size of 256 for BoW - conditioned models .", "entities": []}, {"text": "For other models , we project each node or edge represented by BoW to an embedding space of size 128 .", "entities": []}, {"text": "The default GNN we use has a single linear message passing layer of 256 hidden units .", "entities": []}, {"text": "4.2 Graph!text generation Our \ufb01rst task is text generation conditioned on the graph .", "entities": [[7, 9, "TaskName", "text generation"]]}, {"text": "We evaluate model performance by ( 1 ) computing model perplexity on held - out text and ( 2 ) drawing samples from the model and comparing that to the ground truth text article .", "entities": [[10, 11, "MetricName", "perplexity"]]}, {"text": "We use BLEU score ( Papineni et al . , 2002 ) to measure the similarity of our generated samples to the ground truth .", "entities": [[2, 4, "MetricName", "BLEU score"]]}, {"text": "73Cond .", "entities": []}, {"text": "Test Ppl.rBLEU", "entities": []}, {"text": "rBLEU(w / title ) Valid Test Valid Test None 25.85 10.97 9.98 27.98 24.07 BoW 26.65 29.53 24.41 32.41 27.39 Nodes 27.40 30.51 25.31 32.60 27.43 GNN 26.93 31.39 26.22 32.65 28.35 Table 3 : The perplexity and the generated text reverseBLEU score of different types of graph - conditioned models .", "entities": [[36, 37, "MetricName", "perplexity"]]}, {"text": "We show the reverse - BLEU score with or without prompting the original title at the start of the text generation .", "entities": [[5, 7, "MetricName", "BLEU score"], [19, 21, "TaskName", "text generation"]]}, {"text": "Unlike previous use cases for BLEU score where there are many references for one generated sample , here we have only one ground truth reference but we can generate multiple samples .", "entities": [[5, 7, "MetricName", "BLEU score"]]}, {"text": "We therefore simply swapped the reference with the samples when computing the score , which we term as the reverse - BLEU ( rBLEU ) .", "entities": [[21, 22, "MetricName", "BLEU"]]}, {"text": "We have also tried other ways of computing the BLEU score and \ufb01nd that they do n\u2019t change how models compare against each other .", "entities": [[9, 11, "MetricName", "BLEU score"]]}, {"text": "Unless explicitly stated , we let the model sample with a memory size of 640 , and condition on the graphs in the test set to generate text for up to 512 tokens per sample for a total of 20 samples per graph .", "entities": []}, {"text": "The rBLEU score is computed based on these samples and corresponding ground - truth texts are truncated to the same length .", "entities": []}, {"text": "We sample the texts from the distribution with a temperature of 0.8 .", "entities": []}, {"text": "For each case , we report the average rBLEU score of 3 sampling runs .", "entities": []}, {"text": "We \ufb01nd the variances are insigni\ufb01cant which do not affect the comparison results .", "entities": []}, {"text": "In Appendix A.3 we also report results for generating longer samples for up to 4096 tokens .", "entities": []}, {"text": "4.2.1 Main result In Table 3 , we show the perplexity and the rBLEU score of the unconditional , BoW , nodes - only , and GNN conditioned models .", "entities": [[10, 11, "MetricName", "perplexity"]]}, {"text": "As a reference , a standard Transformer - XL model trained on the full Wikitext-103 training set reaches 25.08 perplexity on our test set , which contains 71.7 % of the original test articles .", "entities": [[6, 9, "MethodName", "Transformer - XL"], [14, 15, "DatasetName", "Wikitext-103"], [19, 20, "MetricName", "perplexity"]]}, {"text": "We can see that the unconditional , i.e. text only , model trained on our dataset gets a very similar performance as trained on the full set .", "entities": []}, {"text": "This is strong evidence that our dataset can be a good benchmark for state - of - the - art text generative models .", "entities": []}, {"text": "We also see that conditioned on the graphs , model perplexity did n\u2019t improve , but the relevance # MP layers Test Ppl .", "entities": [[10, 11, "MetricName", "perplexity"]]}, {"text": "Test rBLEU 0 26.65 25.31 1 27.40 26.22 3 27.20 26.16 5 26.85 25.91 Table 4 : The test perplexity and the generated text reverse - BLEU score ( without title prompt ) of GNNbased models with different numbers of message passing layers .", "entities": [[2, 3, "DatasetName", "0"], [19, 20, "MetricName", "perplexity"], [26, 28, "MetricName", "BLEU score"]]}, {"text": "0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Proportion of graph nodes to keep2223242526Test rBLEU", "entities": []}, {"text": "BoW Nodes GNN Figure 6 : Performance vs size of graph to condition on .", "entities": []}, {"text": "The model is trained with a smaller version of the data by subsampling the number of nodes .", "entities": []}, {"text": "of the samples measured by the BLEU scores did improve signi\ufb01cantly .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "This indicates that the graph conditioned models can indeed steer the language model towards more relevant topics , but this so far can not yet improve likelihood metrics .", "entities": []}, {"text": "To make the evaluation more fair to the text - only model , we also tried to prompt the generation with the title of the article , such that the text - only model also has some context .", "entities": []}, {"text": "In this setting the graph models are still better , showing the importance of modeling the structure .", "entities": []}, {"text": "Lastly , among all the 3 graph model variants , we observe that using a set of embeddings from the nodes model is better than using a single embedding from the BoW model , and fully utilizing the graph structure through the GNN model is consistently better than ignoring the edges as in the nodes model .", "entities": []}, {"text": "However the differences among the methods are relatively small .", "entities": []}, {"text": "For visualizations of a few graphs in our dataset and the corresponding samples generated based on them please refer to Appendix A. 4.2.2 Ablation studies We show a few ablations on the graph model and sampling parameters , to provide some insights into the models .", "entities": []}, {"text": "74Cond .", "entities": []}, {"text": "Recall@1 Recall@5 mAP", "entities": [[0, 1, "MetricName", "Recall@1"], [1, 2, "MetricName", "Recall@5"], [2, 3, "MetricName", "mAP"]]}, {"text": "None 0.02 0.12 0.10 BoW 16.28 30.23 25.98 Nodes 16.28 34.88 26.62 GNN 18.60 34.88 27.79 Table 5 : Text retrieval given the graph .", "entities": []}, {"text": "Table 4 shows the effect of varying the number of message passing layers in the GNN .", "entities": []}, {"text": "We can observe that there is a big difference between using message passing ( \u00151 layers ) or not ( 0 layers ) in terms of rBLEU score , but increasing the number of message passing layers does not change the results signi\ufb01cantly .", "entities": [[20, 21, "DatasetName", "0"]]}, {"text": "We believe however , that these results can be improved by employing bigger and more powerful graph representation learning models , and potentially use initial node and edge representations better than bag - of - words .", "entities": [[16, 19, "TaskName", "graph representation learning"]]}, {"text": "In Figure 6 we show the effect of the graph size on model performance .", "entities": []}, {"text": "In this experiment we subsample the nodes in each graph to control for the amount of context the model has access to .", "entities": []}, {"text": "It is clear from the results that when we heavily subsample and keep only a small portion of the graphs , the GNN model performs similarly as the simpler BoW model , but GNNs bene\ufb01t more as we keep more of the graph structure .", "entities": []}, {"text": "4.3 Graph!text retrieval In this task , we evaluate the possibility of retrieving relevant text for a given query graph .", "entities": []}, {"text": "We pair all articles with all graphs in the test set , resulting in 43\u000243=1849 pairs .", "entities": []}, {"text": "Then the trained graphconditioned language models are used to produce the per - token likelihood of each pair , and we use these likelihood scores to rank the text articles for each graph .", "entities": []}, {"text": "We expect the learned models can rank the correct pairs higher than wrong ones .", "entities": []}, {"text": "To measure the results we use standard ranking metrics including recall@K , which computes the fraction of times the correct pair is included in the top K predictions , as well as mean average precision ( mAP ) .", "entities": [[33, 35, "MetricName", "average precision"], [36, 37, "MetricName", "mAP"]]}, {"text": "In Table 5 , it is observed that graph - conditioned models can indeed retrieve more relevant texts from the graph than the unconditional model , among which the GNN - based model performs the best , and the unconditional model performs close to a random guess .", "entities": []}, {"text": "Cond .", "entities": []}, {"text": "Recall@1 Recall@5 mAP", "entities": [[0, 1, "MetricName", "Recall@1"], [1, 2, "MetricName", "Recall@5"], [2, 3, "MetricName", "mAP"]]}, {"text": "None 0.02 0.07 0.02 BoW 95.35 100.00 97.67 Nodes 93.02 100.00 96.51 GNN 100.00 100.00 100.00 Table 6 : Graph Retrieval given the text .", "entities": []}, {"text": "4.4 Text!graph retrieval In this last task , we evaluate the performance of graph retrieval given a text query .", "entities": []}, {"text": "We use exactly the same setting and scores as Section 4.3 , but instead rank the graphs for each text article using the likelihood scores .", "entities": []}, {"text": "The results are shown in Table 6 .", "entities": []}, {"text": "Note that this task is quite easy with our data and setup , potentially because the graphs are much more distinguishable than the text articles .", "entities": []}, {"text": "All the graph - conditioned models perform almost perfectly , with the GNN model again outperforming the others .", "entities": []}, {"text": "5 Conclusion In this paper , we present WikiGraphs , a new graphtext paired dataset with signi\ufb01cantly larger graphs and longer text compared to previous datasets of similar nature .", "entities": [[8, 9, "DatasetName", "WikiGraphs"]]}, {"text": "We show that the text part of this data is a good benchmark for state - of - the - art text generation models , and the paired dataset can help us benchmark models that are capable of generating long and coherent text conditioned on a graph structure .", "entities": [[21, 23, "TaskName", "text generation"]]}, {"text": "In the \ufb01rst set of experiments on this dataset we showcase 3 different tasks using our dataset , and demonstrate the bene\ufb01t of better models that make more use of the graph structure .", "entities": []}, {"text": "There is still signi\ufb01cant room for improvement for these tasks on our dataset , and we hope the release of the data and baseline code can help spur more interest in developing models that can generate long text conditioned on graphs , and generate graphs given text , which is another exciting direction our dataset enables but we did not explore , and eventually bridging the graph and text modalities .", "entities": []}, {"text": "References S\u00f6ren Auer , Christian Bizer , Georgi Kobilarov , Jens Lehmann , Richard Cyganiak , and Zachary Ives .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Dbpedia : A nucleus for a web of open data .", "entities": [[0, 1, "DatasetName", "Dbpedia"]]}, {"text": "InThe Semantic Web , pages 722\u2013735 , Berlin , Heidelberg .", "entities": []}, {"text": "Springer Berlin Heidelberg .", "entities": []}, {"text": "75Lo\u00efc Barrault , Magdalena Biesialska , Ond \u02c7rej Bojar , Marta R Costa - juss\u00e0 , Christian Federmann , Yvette Graham , Roman Grundkiewicz , Barry Haddow , Matthias Huck , Eric Joanis , et al . 2020 .", "entities": []}, {"text": "Findings of the 2020 conference on machine translation ( wmt20 ) .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Fifth Conference on Machine Translation , pages 1\u201355 .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Peter W Battaglia , Jessica B Hamrick , Victor Bapst , Alvaro Sanchez - Gonzalez , Vinicius Zambaldi , Mateusz Malinowski , Andrea Tacchetti , David Raposo , Adam Santoro , Ryan Faulkner , et al . 2018 .", "entities": [[28, 29, "MethodName", "Adam"]]}, {"text": "Relational inductive biases , deep learning , and graph networks .", "entities": []}, {"text": "arXiv preprint arXiv:1806.01261 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kurt Bollacker , Colin Evans , Praveen Paritosh , Tim Sturge , and Jamie Taylor . 2008 .", "entities": []}, {"text": "Freebase : a collaboratively created graph database for structuring human knowledge .", "entities": []}, {"text": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data , pages 1247\u20131250 .", "entities": [[5, 6, "DatasetName", "ACM"], [10, 11, "TaskName", "Management"]]}, {"text": "James Bradbury , Roy Frostig , Peter Hawkins , Matthew James Johnson , Chris Leary , Dougal Maclaurin , George Necula , Adam Paszke , Jake VanderPlas , Skye Wanderman - Milne , and Qiao Zhang .", "entities": [[22, 23, "MethodName", "Adam"]]}, {"text": "2018 .", "entities": []}, {"text": "JAX : composable transformations of Python+NumPy programs .", "entities": []}, {"text": "Tom B Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . 2020 .", "entities": []}, {"text": "Language models are few - shot learners .", "entities": []}, {"text": "arXiv preprint arXiv:2005.14165 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zihang Dai , Zhilin Yang , Yiming Yang , Jaime Carbonell , Quoc V Le , and Ruslan Salakhutdinov .", "entities": [[17, 18, "DatasetName", "Ruslan"]]}, {"text": "2019 .", "entities": []}, {"text": "Transformer - xl : Attentive language models beyond a \ufb01xed - length context .", "entities": [[0, 3, "MethodName", "Transformer - xl"]]}, {"text": "arXiv preprint arXiv:1901.02860 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Claire Gardent , Anastasia Shimorina , Shashi Narayan , and Laura Perez - Beltrachini . 2017 .", "entities": []}, {"text": "The WebNLG challenge : Generating text from RDF data .", "entities": [[1, 2, "DatasetName", "WebNLG"]]}, {"text": "In Proceedings of the 10th International Conference on Natural Language Generation , pages 124\u2013133 , Santiago de Compostela , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "John S Garofolo , Lori F Lamel , William M Fisher , Jonathan G Fiscus , and David S Pallett .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "Darpa timit acoustic - phonetic continous speech corpus cdrom .", "entities": [[0, 1, "DatasetName", "Darpa"], [1, 2, "DatasetName", "timit"]]}, {"text": "nist speech disc 1 - 1.1 .", "entities": []}, {"text": "NASA STI / Recon technical report n , 93:27403 .", "entities": [[3, 4, "DatasetName", "Recon"]]}, {"text": "Justin Gilmer , Samuel S Schoenholz , Patrick F Riley , Oriol Vinyals , and George E Dahl . 2017 .", "entities": []}, {"text": "Neural message passing for quantum chemistry .", "entities": []}, {"text": "In International Conference on Machine Learning , pages 1263\u20131272 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Zhijing Jin , Qipeng Guo , Xipeng Qiu , and Zheng Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "GenWiki : A dataset of 1.3 million content - sharing text and graphs for unsupervised graph - to - text generation .", "entities": [[0, 1, "DatasetName", "GenWiki"], [19, 21, "TaskName", "text generation"]]}, {"text": "In Proceedings of the 28thInternational Conference on Computational Linguistics , pages 2398\u20132409 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Thomas N Kipf and Max Welling .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semisupervised classi\ufb01cation with graph convolutional networks .", "entities": []}, {"text": "arXiv preprint arXiv:1609.02907 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rik Koncel - Kedziorski , Dhanush Bekal , Yi Luan , Mirella Lapata , and Hannaneh Hajishirzi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Text Generation from Knowledge Graphs with Graph Transformers .", "entities": [[0, 2, "TaskName", "Text Generation"], [3, 5, "TaskName", "Knowledge Graphs"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 2284\u20132293 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "R\u00e9mi Lebret , David Grangier , and Michael Auli .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Neural text generation from structured data with application to the biography domain .", "entities": [[1, 3, "TaskName", "text generation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1203\u20131213 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tsung - Yi Lin , Michael Maire , Serge Belongie , James Hays , Pietro Perona , Deva Ramanan , Piotr Doll\u00e1r , and C Lawrence Zitnick .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Microsoft coco :", "entities": [[1, 2, "DatasetName", "coco"]]}, {"text": "Common objects in context .", "entities": []}, {"text": "In European conference on computer vision , pages 740\u2013755 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Fei Liu , Jeffrey Flanigan , Sam Thomson , Norman Sadeh , and Noah A Smith .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Toward abstractive summarization using semantic representations .", "entities": [[2, 3, "TaskName", "summarization"]]}, {"text": "arXiv preprint arXiv:1805.10399 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Stephen Merity , Caiming Xiong , James Bradbury , and Richard Socher .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Pointer sentinel mixture models.arXiv preprint arXiv:1609.07843 .", "entities": []}, {"text": "Aaron van den Oord , Sander Dieleman , Heiga Zen , Karen Simonyan , Oriol Vinyals , Alex Graves , Nal Kalchbrenner , Andrew Senior , and Koray Kavukcuoglu .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Wavenet :", "entities": [[0, 1, "MethodName", "Wavenet"]]}, {"text": "A generative model for raw audio .", "entities": []}, {"text": "arXiv preprint arXiv:1609.03499 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Vassil Panayotov , Guoguo Chen , Daniel Povey , and Sanjeev Khudanpur . 2015 .", "entities": []}, {"text": "Librispeech : an asr corpus based on public domain audio books .", "entities": [[0, 1, "DatasetName", "Librispeech"]]}, {"text": "In 2015 IEEE international conference on acoustics , speech and signal processing ( ICASSP ) , pages 5206\u20135210 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics , pages 311\u2013318 .", "entities": []}, {"text": "Ankur P. Parikh , Xuezhi Wang , Sebastian Gehrmann , Manaal Faruqui , Bhuwan Dhingra , Diyi Yang , and Dipanjan Das . 2020 .", "entities": []}, {"text": "Totto : A controlled table - totext generation dataset .", "entities": [[0, 1, "DatasetName", "Totto"]]}, {"text": "76Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Petar Veli \u02c7ckovi \u00b4 c , Guillem Cucurull , Arantxa Casanova , Adriana Romero , Pietro Li\u00f2 , and Yoshua Bengio .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graph attention networks .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Oriol Vinyals , Alexander Toshev , Samy Bengio , and Dumitru Erhan .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Show and tell : Lessons learned from the 2015 mscoco image captioning challenge .", "entities": [[9, 10, "DatasetName", "mscoco"], [10, 12, "TaskName", "image captioning"]]}, {"text": "IEEE transactions on pattern analysis and machine intelligence , 39(4):652\u2013663 .", "entities": []}, {"text": "Hongwei Wang , Fuzheng Zhang , Mengdi Zhang , Jure Leskovec , Miao Zhao , Wenjie Li , and Zhongyuan Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Knowledge - aware graph neural networks with label smoothness regularization for recommender systems .", "entities": []}, {"text": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 968\u2013977 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Kun Xu , Liwei Wang , Mo Yu , Yansong Feng , Yan Song , Zhiguo Wang , and Dong Yu . 2019 .", "entities": []}, {"text": "Cross - lingual knowledge graph alignment via graph matching neural network .", "entities": [[7, 9, "TaskName", "graph matching"]]}, {"text": "arXiv preprint arXiv:1905.11605 .A", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Appendix A.1 Graph visualization Some example visualizations of the KG structures are shown in Figure 7 and Figure 8 .", "entities": []}, {"text": "The corresponding graph truth texts are shown in Table 7 .", "entities": []}, {"text": "A.2 Generated examples The generated texts based on the graph shown in Figure 7 and Figure 8 are listed in Table 8 and Table 9 , respectively .", "entities": []}, {"text": "A.3 Ablations on sampling con\ufb01gurations We show additional ablation results on the sample length ( Table 10 ) and the temperature ( Table 11 ) for greedy sampling .", "entities": []}, {"text": "Note that for each case we show the rBLEU score based on the validation set computed with a single sampling run ( 20 samples per graph ) .", "entities": []}, {"text": "Note that the GNN model has overall the best performance .", "entities": []}, {"text": "However as the sample length increases the advantage of the GNN model also decreases .", "entities": []}, {"text": "This indicates that it is still very challenging to generate long text that stays on - topic , and potentially the noise overwhelms the signal when number of tokens increases to 4096 .", "entities": []}, {"text": "77 ns / m.02h40lc\"Ingl\u00e9s \" key / wikipedia.en \" /en / english\"ns / type.object.id\"English\"ns / common.topic.alias\"English is a West Germanic language that was first spoken in early medieval England and is now a global lingua franca .", "entities": []}, {"text": "It is an official language of almost 60 sovereign states , the most commonly spoken language in the United Kingdom , the United States , Canada , Australia , Ireland , and New Zealand , and a widely spoken language in countries in the Caribbean , Africa , and South Asia .", "entities": []}, {"text": "It is the third most common native language in the world , after Mandarin and Spanish .", "entities": []}, {"text": "It is widely learned as a secon ... ns / common.topic.description \" English Language\"ns / type.object.name ns / m.01vs14jns / m.06t2j0 ns / music.composer.compositions", "entities": []}, {"text": "\" Larry Mullen\"key / wikipedia.en \" Larry Mullen\"ns / common.topic.alias \" Larry Mullen , Jr. \"ns / type.object.name \" 1.702\"ns / people.person.height meters \" Laurence Joseph \\\"Larry\\ \" Mullen , Jr. is an Irish musician and actor , best known as the drummer of the Irish rock band U2 .", "entities": []}, {"text": "A member of the group since its inception , he has recorded 13 studio albums with the group .", "entities": []}, {"text": "Mullen was born and raised in Dublin , and attended Mount Temple Comprehensive School , where , in 1976 , he co - founded U2 after posting a message on the school 's notice board .", "entities": []}, {"text": "His drumming style developed from his playing martial beats in a childhood marching band , the Artane Boys Band .... ns / common.topic.description \" 1961 - 10 - 31\"ns / people.person.date of birth \" 099P1wW8Ad4It\"ns / common.identity.daylife topicns / music.composition.language ns / music.composition.composer ns / m.074ftns / music.composition.formns / m.0vfz0 ns / music.composition.recordings\"Where the Streets Have No Name \" ns / type.object.name \" Where the Streets Have", "entities": []}, {"text": "No Name\"key / wikipedia.en ns / m.02nmm lns / common.topic.notable types \" \\\"Where the Streets Have No Name\\ \" is a song by Irish rock band U2 .", "entities": []}, {"text": "It is the opening track from their 1987 album The Joshua Tree and was released as the album 's third single in August 1987 .", "entities": []}, {"text": "The song 's hook is a repeating guitar arpeggio using a delay effect , played during the song 's introduction and again at the end .", "entities": []}, {"text": "Lead vocalist Bono wrote the lyrics in response to the notion that it is possible to identify a person 's religion and income based on the street on which they lived , particularly ... ns / common.topic.description\"T-011.335.525 - 1 \" ns / media common.cataloged instance.iswc ns / music.compositional form.compositions \" A song is an artistic form of expression based on sound , generally considered a single work of music with distinct and fixed pitches , pattern , and form .", "entities": []}, {"text": "It can be wordless or with words , but must include some form of vocalization .", "entities": []}, {"text": "Written words created specifically for music or for which music is specifically created , are called lyrics .", "entities": []}, {"text": "If poetry , a pre - existing poem , is set to composed music , that is an art song .", "entities": []}, {"text": "Songs that are sung on repeated pitches without distinct contours and patterns th ... ns / common.topic.description \" Song\"ns / type.object.name \" \u6b4c\"key / wikipedia.enns / music.recording.songns / common.topic.aliasns / type.object.name \" 289.106\"ns / music.recording.lengthns / m.025352 ns / music.composer.compositions\"Lyrist \" key / wikipedia.en\"Lyricist \" ns / type.object.name\"A lyricist is a musician who specializes in writing lyrics .", "entities": []}, {"text": "A singer who writes the lyrics to songs is a singer - lyricist .", "entities": []}, {"text": "This differentiates from a singer - composer , who composes the song 's melody . \" ns / common.topic.description ns / m.01vswwxns / people.profession.people with this profession ns / music.composer.compositionsns / people.person.profession\"Bon \" ns / common.topic.alias \" 1960 - 05 - 10 \" ns / people.person.date of birth \" 02tG71n0SIgTC \" ns / common.identity.daylife topic\"Bono \" ns / type.object.name\"Paul Hewson \" key / wikipedia.en\"Paul David Hewson , known by his stage name Bono , is an Irish singersongwriter , musician , venture capitalist , businessman , and philanthropist .", "entities": []}, {"text": "He is best recognized as the frontman of the Dublin - based rock band U2 .", "entities": []}, {"text": "Bono was born and raised in Dublin , Ireland , and attended Mount Temple Comprehensive School where he met his future wife , Alison Stewart , and the future members of U2 .", "entities": []}, {"text": "Bono writes almost all U2 lyrics , frequently using religious , social , and political themes .", "entities": []}, {"text": "During U2 's early years , ... ns / common.topic.description \" 1.75 \" ns / people.person.height metersns / m.0y hml ns / music.recording.songns / type.object.name\"Where the Streets Have No Name ( Ca n't Take My Eyes Off Y ou ) \" ns / common.topic.alias\"270.373 \" ns / music.recording.length\"GBCEW0300047 \" ns / media common.cataloged instance.isrc ns / common.topic.alias \" 67297\"ns / freebase.type profile.instance count \" A composition is a written musical work .", "entities": []}, {"text": "It includes musical works ranging from classical to modern .", "entities": []}, {"text": "It may or may not have words .", "entities": []}, {"text": "A composition usually has composer and a lyricist if there are words .", "entities": []}, {"text": "A composition may include smaller compositions such as arias , movements , etc . ( e.g. The composition Der Ring des Nibelungen includes the smaller compositions Das Rheingold , Die Walkure , Siegfried and G\u00f6tterd\u00e4mmerung ) .", "entities": []}, {"text": "For more information , please see the Freebase wiki page on composition .", "entities": []}, {"text": "\"ns / common.topic.description\"Composition\"ns / type.object.name \" /music / composition\"ns / freebase.object hints.best hrid ns / type.object.idns / m.0nm1l3k ns / music.recording.songns / type.object.name\"Where The Streeets Have No Name \" ns / common.topic.alias\"CAG600957002 \" ns / media common.cataloged instance.isrc\"272.96 \" ns / music.recording.length ns / m.01vswx5ns / music.composer.compositions \" Edge\"ns / common.topic.alias", "entities": []}, {"text": "\" The Edge\"key / wikipedia.en\"The Edge \" ns / type.object.name \" David Howell Evans , better known by his stage name The Edge , is a British - born Irish musician , songwriter and singer best known as the lead guitarist , keyboardist and backing vocalist of the rock band U2 .", "entities": []}, {"text": "A member of the group since its inception , he has recorded 13 studio albums with the band as well as one solo record .", "entities": []}, {"text": "As a guitarist , the Edge has crafted a minimalistic and textural style of playing .", "entities": []}, {"text": "His use of a rhythmic delay effect yields a distinctive ambient , chiming sound that has becom ... ns / common.topic.description\"1961 - 08 - 08 \" ns / people.person.date of birth \" 1.77\"ns / people.person.height metersns / m.0x7ml ns / music.composer.compositions\"Adam", "entities": []}, {"text": "Charles Clayton is an English - born Irish musician best known as the bass guitarist of the Irish rock band U2 .", "entities": []}, {"text": "He has resided in County Dublin since the time his family moved to Malahide when he was five years old in 1965 .", "entities": []}, {"text": "Clayton is well known for his bass playing on songs such as \\\"Gloria\\ \" , \\\"New Year 's Day\\ \" , \\\"Bullet the Blue Sky\\ \" , \\\"With or Without Y ou\\ \" , \\\"Mysterious Ways\\ \" , \\\"Get on Y our Boots\\ \" , and \\\"Magnificent\\ \" .", "entities": []}, {"text": "His work on No Line on the Horizon has been cited as his best bas ... ns / common.topic.description\"06tHcdA5m21fG \" ns / common.identity.daylife topic\"1.78 \" ns / people.person.height meters\"Sparky \" ns / common.topic.alias\"Adam Clayton \" key / wikipedia.en \" 1960 - 03 - 13 \" ns / people.person.date of birth \" Adam Clayton\"ns / type.object.nameFigure 7 : Visualization of the \u201c Where the Streets Have No Name \u201d KG in our dataset .", "entities": [[51, 52, "MethodName", "Adam"]]}, {"text": "78 ns / m.02h8512 \" Park\"ns / common.topic.alias \" 7510\"ns / freebase.type profile.instance count \" /protected sites / protected site\"ns / type.object.id ns / freebase.object hints.best hrid \" A \\\"protected site\\ \" is any location that is protected under law , usually by being designated as a park , preserve , monument , etc . , and which are usually under the control ( at least in part ) by some form of government agency .", "entities": []}, {"text": "This most often will apply to areas of land or water , but may also apply to human - made structures .", "entities": []}, {"text": "This type is distinct from \\\"listed sites\\ \" , which have been designated as significant , but may not have any legal protection thereby .", "entities": []}, {"text": "However , many such places will be both , ... ns / common.topic.description \" Protected Site\"ns / type.object.namens / m.0t592 ns / location.hud county place.place\"8263 \" ns / topic server.population number \" Fort Scott , Kansas\"ns / common.topic.alias \" Fort Scott , KS\"key / wikipedia.enns / m.0nq49 ns / location.location.containedbyns / location.hud county place.county\"Fort Scott is a city in and the county seat of Bourbon County , Kansas , 88 miles south of Kansas City , on the Marmaton River .", "entities": []}, {"text": "As of the 2010 census , the city population was 8,087 .", "entities": []}, {"text": "It is the home of the Fort Scott National Historic Site and the Fort Scott National Cemetery .", "entities": []}, {"text": "Fort Scott is named for Gen. Winfield Scott . \"", "entities": []}, {"text": "ns / common.topic.description\"Fort Scott \" ns / type.object.name \" 14.48\"ns / location.location.area ns / m.0gn2mns / symbols.namesake.named after ns / m.03 fsmns / location.location.containsns / location.location.containsns / location.us county.hud county place\"15153\"ns / topic server.population number\"1855 - 08 - 25 \" ns / location.dated location.date founded \" Bourbon County\"ns / type.object.name \" Bourbon County , KS\"key / wikipedia.en\"1655.0024025 \" ns / location.location.area \" Bourbon County is a county located in Southeast Kansas .", "entities": []}, {"text": "As of the 2010 census , the county population was 15,173 .", "entities": []}, {"text": "Its county seat and most populous city is Fort Scott .", "entities": []}, {"text": "\"ns / common.topic.descriptionns / m.0488 g ns / location.location.containedby ns / symbols.name source.namesakes\"1866 - 05 - 29 \" ns / people.deceased person.date of death \" Old Fuss \" key / wikipedia.en\"0gZa9e9c38gT7 \" ns / common.identity.daylife topic \" Winfield Scott \" ns / type.object.name\"Winfield Scott was a United States Army general , and unsuccessful presidential candidate of the Whig Party in 1852 .", "entities": []}, {"text": "Known as \\\"Old Fuss and Feathers\\ \" and the \\\"Grand Old Man of the Army,\\ \" he served on active duty as a general longer than any other man in American history , and many historians rate him the best American commander of his time .", "entities": []}, {"text": "Over the course of his 53 - year career , he commanded forces in the War of 1812 , the Black Hawk War , the Mexican - American War , the Second Seminole War , and ... ns / common.topic.description \" 1786 - 06 - 13\"ns / people.person.date of birth ns / common.topic.notable typesns / location.location.containedby ns / symbols.namesake.named after \" Fort Scott National Historic Site is a historical area under the control of the United States National Park Service in Bourbon County , Kansas , United States .", "entities": []}, {"text": "Named after General Winfield Scott , who achieved renown during the Mexican - American War , during the middle of the 19th century the fort served as a military base for US Army action in what was the edge of settlement in 1850 .", "entities": [[2, 3, "DatasetName", "General"]]}, {"text": "For the next quarter century , it was used as a supply base and to provide security in turbulent areas during the ope ... ns / common.topic.description \" 66000106\"ns / base.usnris.nris listing.item number \" Fort Scott National Historic Site\"ns / type.object.name \" 0.0271149\"ns / location.location.area\"Fort Scott Historic Area \" ns / common.topic.alias \" 1842\"ns / base.usnris.nris", "entities": []}, {"text": "listing.significant year \" Fort Scott National Historic Site\"key / wikipedia.enns / location.location.contains\"2200000 \" ns / topic server.population number\"Kan . \" key / wikipedia.en\"KA \" ns / common.topic.alias\"1861 - 01 - 29 \" ns / location.dated location.date founded\"Kansas \" ns / type.object.name\"0anV49y925fIj \" ns / common.identity.daylife topic\"Kansas /\u02c8k\u00e6nz\u0259s/ is a U.S. state located in the Midwestern United States .", "entities": []}, {"text": "It is named after the Kansa Native American tribe which inhabited the area .", "entities": []}, {"text": "The tribe 's name is often said to mean \\\"people of the wind\\ \" or \\\"people of the south wind,\\ \" although this was probably not the term 's original meaning .", "entities": []}, {"text": "Residents of Kansas are called \\\"Kansans\\ \" .", "entities": []}, {"text": "For thousands of years , what is now Kansas was home to numerous and diverse Native American tribes .", "entities": []}, {"text": "Tribes in the Eastern part of the state generally ... ns / common.topic.description \" 213096.0 \" ns / location.location.areaFigure", "entities": []}, {"text": "8 : Visualization of the \u201c Fort Scott National Historic Site \u201d KG in our dataset .", "entities": []}, {"text": "79Visualization Ground Truth Text Figure 7 = Where the Streets Have No Name = \" Where the Streets Have No Name \" is a song by Irish rock band U2 .", "entities": []}, {"text": "It is the opening track from their 1987 album The Joshua Tree and was released as the album \u2019s third single in August 1987 .", "entities": []}, {"text": "The song \u2019s hook is a repeating guitar arpeggio using a delay effect , played during the song \u2019s introduction and again at the end .", "entities": []}, {"text": "Lead vocalist Bono wrote the lyrics in response to the notion that it is possible to identify a person \u2019s religion and income based on the street on which they lived , particularly in Belfast .", "entities": []}, {"text": "During the band \u2019s dif\ufb01culties recording the song , producer Brian Eno considered erasing the song \u2019s tapes to have them start from scratch . \"", "entities": []}, {"text": "Where the Streets Have No Name \" was praised by critics and became a commercial success , peaking at number thirteen in the US , number fourteen in Canada , number ten in the Netherlands , and number four in the United Kingdom .", "entities": []}, {"text": "The song has remained a staple of their live act since the song debuted in 1987 on The Joshua Tree Tour .", "entities": []}, {"text": "The song was performed on a Los Angeles rooftop for the \ufb01lming of its music video , which won a Grammy Award for Best Performance Music Video .", "entities": []}, {"text": "= = Writing and recording = =", "entities": []}, {"text": "The music for \" Where the Streets Have No Name \" originated from a demo that guitarist The Edge composed the night before the group resumed The Joshua Tree sessions .", "entities": []}, {"text": "In an upstairs room at Melbeach House \u2014 his newly purchased home \u2014 The Edge used a four @-@ track tape machine to record an arrangement of keyboards , bass , guitar , and a drum machine .", "entities": []}, {"text": "Realising that the album sessions were approaching the end and that the band were short on exceptional live songs , The Edge wanted to \" conjure up the ultimate U2 live @-@ song \" , so he imagined what he would like to hear at a future U2 show if he were a fan .", "entities": []}, {"text": "After \ufb01nishing the rough mix , he felt he had come up with \" the most amazing guitar part and song of [ his ] life \" .", "entities": []}, {"text": "With no one in the house to share the demo with , The Edge recalls dancing around and punching the air in celebration .", "entities": []}, {"text": "Although the band liked the demo , it was dif\ufb01cult for them to record the song .", "entities": []}, {"text": "Bassist Adam Clayton said , \" At the time it sounded like a foreign language , whereas now we understand how it works \" .", "entities": [[1, 2, "MethodName", "Adam"]]}, {"text": "The arrangement , with two time signature shifts and frequent chord changes , was rehearsed many times , but the group struggled to get a performance they liked .", "entities": []}, {"text": "According to co @-@ producer Daniel Lanois , \" that was the science project song .", "entities": []}, {"text": "Figure 8", "entities": []}, {"text": "=", "entities": []}, {"text": "Fort Scott National Historic Site", "entities": []}, {"text": "=", "entities": []}, {"text": "Fort Scott National Historic Site is a historical area under the control of the United States National Park Service in Bourbon County , Kansas , United States .", "entities": []}, {"text": "Named after General Win\ufb01eld Scott , who achieved renown during the Mexican @-@ American War , during the middle of the 19th century the fort served as a military base for US Army action in what was the edge of settlement in 1850 .", "entities": [[2, 3, "DatasetName", "General"]]}, {"text": "For the next quarter century , it was used as a supply base and to provide security in turbulent areas during the opening of the West to settlement , a period which included Bleeding Kansas and the American Civil War .", "entities": []}, {"text": "The current national historic site protects 20 historic structures , a parade ground , and \ufb01ve acres ( 20 @,@ 000 m2 ) of restored < unk > prairie , inside the city of Fort Scott .", "entities": []}, {"text": "It is open to visitors most days of the year .", "entities": []}, {"text": "= = History =", "entities": []}, {"text": "= In 1842 , Fort Scott was named after Win\ufb01eld Scott , was established on the American frontier on the military road in eastern Kansas between Fort Leavenworth and Fort Gibson .", "entities": []}, {"text": "It was established to provide protection to the rapidly increasing number of settlers , who were migrating from the Eastern United States .", "entities": []}, {"text": "Fort Scott became one of a chain of forts intended to protect the new settlers from the Plains Indians , as well as to protect the Indians from the settlers \u2019 encroachment .", "entities": []}, {"text": "The United States government intention to reserve permanent Indian lands west of the Missouri River gave way to the competition of settlers continuing to encroach on the Indian settlements .", "entities": []}, {"text": "Fort Scott \u2019s most active days were between 1842 and 1853 , although it was also used during the Civil War .", "entities": []}, {"text": "= = = Army days =", "entities": []}, {"text": "= = The Cherokee of Indian Territory ( now Oklahoma ) were upset to have Fort Wayne in their proximity .", "entities": []}, {"text": "After some delay , the US Army decided to abandon Fort Wayne and move its soldiers to a new fort to be built between Fort Leavenworth and the site .", "entities": []}, {"text": "The Army both wanted to placate the Cherokee ( who were supervised by the Department of Defense ) and provide more defense for white settlers and other Indians against the Osage , who had been conducting frequent raids in the area .", "entities": []}, {"text": "On April 1 , 1842 , some soldiers of Fort Wayne left their fort and on April 22 arrived where Fort Scott would be built , in the Osage < unk > section of modern @-@ day Kansas .", "entities": []}, {"text": "After haggling with the Cherokees to acquire the land , the rest of Fort Wayne \u2019s garrison left the fort on May 26 and arrived at the Fort Scott site on May 30 .", "entities": []}, {"text": "Unlike most forts for military use , the fort did not have defensive walls or structures Table 7 : Ground truth samples .", "entities": []}, {"text": "80Cond .", "entities": []}, {"text": "Generated Text - = Final Fantasy Adventure = Final Fantasy Adventure is a platform video game developed by Square ( now Square Enix ) and published by Square ( then Square Enix ) for the Super Nintendo Entertainment System , and published by Nintendo for the Nintendo 64 and the Nintendo 64 in North America and Europe .", "entities": []}, {"text": "It was released in North America on August 10 , 1995 for the Nintendo 64 and in Europe in March 1995 for the Wii .", "entities": []}, {"text": "It was also re @-@ released for the Wii in Japan on March 25 , 2015 for the Wii U \u2019s Virtual Console service .", "entities": []}, {"text": "Final Fantasy Adventure is set in the fantasy world of the \ufb01ctional land of Ooo , between 1000 and 1000 years before the events of the story of the kingdom of Ooo .", "entities": []}, {"text": "The story follows the protagonists , Cid , a boy who is sent to Ooo by the legendary J\u00f6rmungandr to \ufb01nd the Wizard of Ooo .", "entities": []}, {"text": "Throughout the game , players train for a series of random battles , which are played out over a single screen .", "entities": []}, {"text": "In the end , players control a \" Scarlet Witch \" who is summoned to the realm of Ooo .", "entities": []}, {"text": "The game was well @-@ received , with praise particularly directed at its graphics and its gameplay , despite criticism for its repetitive gameplay .", "entities": []}, {"text": "Critics generally praised its graphics , plot , and simplistic gameplay .", "entities": []}, {"text": "The game was commercially successful , selling over 20 @,@ 000 copies , and in 2004 it was re @-@ released in Japan as part of the Final Fantasy Collections series .", "entities": []}, {"text": "= = Gameplay = =", "entities": []}, {"text": "The game is a third @-@ person shooter , with gameplay elements related to worlds , time travel , and exploration .", "entities": []}, {"text": "Characters are made up of two polygons ; the main character , Aya , is a sprite @-@ based character and inhabits a world of trees and caves , where she is able to perform offensive actions along the way .", "entities": []}, {"text": "The protagonist , Aya , is a heavily armored , humanoid creature with red eyes and black eyes .", "entities": []}, {"text": "In contrast to other Final Fantasy games , her prominent appearances in this game are not as straightforward as in previous Final Fantasy installments .", "entities": []}, {"text": "The game \u2019s world is divided into two sections : the world of Ooo and the world of the dinosaurs .", "entities": []}, {"text": "The player \u2019s adventure begins with Aya , a girl who initially leads the plot of Twilight Town , but later returns to Ooo to save her sister .", "entities": []}, {"text": "Aya explores the area and is able to save her family .", "entities": []}, {"text": "After interacting with the player , Aya is able to resume the story in the future .", "entities": []}, {"text": "The game \u2019s main family member , Aya , is a woman named Alice .", "entities": []}, {"text": "She is a member of the religious order , the Jehovah \u2019s Witnesses , who is a member of BoW = Zoo Station ( song )", "entities": []}, {"text": "= \" Zoo Station \" is a song by Australian rock band U2 from their second studio album , PopMart , released in 1992 as the album \u2019s lead single .", "entities": []}, {"text": "Rise to popularity after its release prompted U2 to record two songs with bassist Adam Clayton and guitarist Joe Bono , as they felt that it was their breakthrough album .", "entities": [[14, 15, "MethodName", "Adam"]]}, {"text": "The song comprises lyrics set in an acoustic , with lyrics that have always focused on a woman being stuck in an abusive relationship .", "entities": []}, {"text": "It was developed due to the band \u2019s involvement in the construction of Zoo Station and its accompanying soundtrack , and it was inspired by U2 \u2019s admiration of the groups they both represented .", "entities": []}, {"text": "The song was inspired by a series of incidents that occurred in El Salvador when a man repeatedly attacked an apartment building .", "entities": []}, {"text": "The song received positive reviews from music critics , and it was labelled a \" masterpiece \" by several publications .", "entities": []}, {"text": "Thirty @-@ \ufb01ve tracks were released as singles , and the song has been covered by numerous artists , including such \ufb01gures as Sam & Max , The < unk > , and Mickey Rourke .", "entities": []}, {"text": "The band released their version of \" Zoo Station \" on non @-@ consecutive studio albums , with \" Where the Streets Have No Name \" and \" Numb \" released in 1999 and 2000 respectively . \"", "entities": []}, {"text": "Zoo Station \" is on the band \u2019s greatest hits album , 10 Mile Wild , and on their greatest hits album Across the Universe .", "entities": []}, {"text": "It was performed live at the Vertigo Tour in 2007 .", "entities": []}, {"text": "The song was included on the band \u2019s \ufb01fth studio album , New Order ( 2008 ) , and was included on the \ufb01lm The Last Years ( 2012 ) .", "entities": []}, {"text": "= = Background", "entities": []}, {"text": "= = \" Zoo Station \" was written by U2 frontman Bono and produced by U2 and was the \ufb01rst song on the album .", "entities": []}, {"text": "Clayton still had a strong fan base and was a regular singer .", "entities": []}, {"text": "The two met while performing in the amphitheaters of Los Angeles , and in a live performance he was the lead singer on the last leg of the Vertigo Tour .", "entities": []}, {"text": "The Edge and Clayton were both fans of the band , and the pair decided to collaborate on the album .", "entities": []}, {"text": "Both performed on their 2004 tour , and made a solo appearance on the 2004 The Zoo TV Tour .", "entities": []}, {"text": "Clayton and Clayton had been close friends , and the pair became friends again in 2008 .", "entities": []}, {"text": "In late April 2004 , U2 announced that the song had been released as the \ufb01rst single for the album , and would be released on 31 May , \ufb01ve weeks after the album \u2019s release . \"", "entities": []}, {"text": "Zoo Station \" was released as the fourth single from PopMart and premiered on 13 June 2005 .", "entities": []}, {"text": "The song is a Nodes = You Can \u2019 t Take This Away ( U2 song )", "entities": []}, {"text": "= \" You Can \u2019 t Take This Away ( U2 song ) \" is a song by the Irish rock band U2 .", "entities": []}, {"text": "It was written by vocalist Bono and produced by The Smiths for their third solo album , The Joshua Tree .", "entities": []}, {"text": "Inspired by Romania roots rock and roll , \" You Can \u2019 t Take This Away \" is a song about a man who suffers from severe nightmares .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "The song was \ufb01rst released on the album and premiered on radio on 19 August 1996 , where it reached number 40 on the Billboard Hot 100 .", "entities": []}, {"text": "\"", "entities": []}, {"text": "You Can \u2019 t Take This Away ( U2 song ) \" received mixed reviews from critics ; some tracks , such as the opening chorus and bridge , were praised as highlights by some reviewers .", "entities": []}, {"text": "The song was well received by critics , as the record company \u2019s highest @-@ selling single at that time , where \" The Edge of Forever \" peaked at number 64 .", "entities": []}, {"text": "It was later re @-@ released on the band \u2019s 2006 compilation album No Line on the Horizon , but has since been re @-@ released on live performances in 2006 and 2009 .", "entities": []}, {"text": "\" You Can \u2019 t Take This Away ( U2 song ) \" was performed on the Late Show with David Letterman on 31 December 2005 .", "entities": []}, {"text": "\"", "entities": []}, {"text": "You Can \u2019 t Take This Away ( U2 song ) \" has since been covered by many groups and has achieved enormous commercial success .", "entities": []}, {"text": "A video for the song was \ufb01lmed by then @-@ frontman Bono , for which it was nominated for a Grammy Award .", "entities": []}, {"text": "= = Background and writing = = \" You Can \u2019 t Take This Away ( Kingdom of Ireland song ) \" is a track that features Bono and The Smiths discussing their relationship and how they changed their lives .", "entities": []}, {"text": "His father , Jack Clayton , was assassinated in 1981 at the age of 23 .", "entities": []}, {"text": "Bono was born in Philadelphia and worked for a business , \ufb01rst working as a secretary in Los Angeles , then as a photographer for a commercial for Primus .", "entities": []}, {"text": "He later worked for the Coca @-@ Cola Company as a drummer in the music industry .", "entities": []}, {"text": "The musician picked up the song again after Nikolai < unk > , an engineer who worked with The Smiths , heard it and decided to play it for him after U2 agreed to record it for a solo album .", "entities": []}, {"text": "The band originally intended to release \" You Can \u2019 t Take This Away \" as a single ; however , with a critical failure , the song eventually became a single .", "entities": []}, {"text": "In 2004 , \" You Can \u2019 t Take This Away ( U2 song ) \" is one of two songs on the album that U2 released as a single with approval of the record label .", "entities": []}, {"text": "The [ ] GNN = The Edge of Glory = \" The Edge of Glory \" is a song by Irish rock band U2 , released as a single .", "entities": []}, {"text": "It was written by guitarist Larry Mullen , Jr .", "entities": []}, {"text": ", who plays lead guitar on the song , and was produced by Alex < unk > , who described the song as \" a track with a lot of meaning , but no connection . \"", "entities": []}, {"text": "The song contains several pop rock elements and is set in that time period , and is among the most prominent in the album .", "entities": []}, {"text": "In addition to its lyrics , the song \u2019s lyrics detail hypocrisy , and also deals with the effects of adultery .", "entities": []}, {"text": "The song \u2019s lyrics have been described by music critics as being autobiographical .", "entities": []}, {"text": "The lyrics have been described as \" a bold exploration of the \ufb01gure of a New York City man \" , and \" an expression of the inability of freedom to live in a world that is also a place in the world of space . \"", "entities": []}, {"text": "The song \u2019s lyrics describe a \" Manhattan @-@ like place \" , with Bono calling the arrival a \" pleasant little optimism from before it came to life . \"", "entities": []}, {"text": "\" The Edge of Glory \" was a success in the United Kingdom , reaching number two in the charts in the United States , and topping the charts in Australia and New Zealand .", "entities": []}, {"text": "The song has been certi\ufb01ed platinum by the Recording Industry Association of America , and has sold over four million copies worldwide .", "entities": []}, {"text": "The song has been covered by several artists , including German band U2 .", "entities": []}, {"text": "The music video for \" The Edge of Glory \" won Best Video at the 2004 MTV Video Music Awards .", "entities": []}, {"text": "The video also served as an inspiration for the \ufb01lm U2 360\u000e ( 1998 ) .", "entities": []}, {"text": "= = Background", "entities": []}, {"text": "= =", "entities": []}, {"text": "The song has been described as a \" relaxed representation \" of globalization , with Bono proclaiming himself the \" lost king of rock \u2019 n \u2019 roll \" , and Chris McGuinness as \" the only one who has ever achieved the sound of a rock \u2019 n \u2019 roll . \"", "entities": []}, {"text": "Bono \u2019s lyrics have been described as a parody of Lord Byron \u2019s \"", "entities": []}, {"text": "My Own Time \" , and as an \" attack on social and political issues \" .", "entities": []}, {"text": "= = Recording and production = = Bono and U2 made the \ufb01nal stages of recording the song at the Abbey Road Studios in Dublin , Dublin .", "entities": []}, {"text": "The sessions were divided into two sessions : Sessions at Damien the \ufb02autist and Context at the Cave of Christ .", "entities": []}, {"text": "The results of the sessions were mixed by Brian Eno .", "entities": []}, {"text": "U2 was very excited to record the result , with Eno referring to the recording as a \" special event \" , and they decided to change the track Table 8 : Generated samples based on the \u201c Where the Streets Have No Name \u201d graph .", "entities": []}, {"text": "81Cond .", "entities": []}, {"text": "Generated Text - = Carbuncle = Bloodlines is a 2011 action role @-@ playing video game developed by < unk > and published by Activision for the Wii .", "entities": []}, {"text": "The game is part of a sequel of the 2013 game Eclipse : The Series .", "entities": []}, {"text": "It was published by Activision on November 19 , 2011 , for Microsoft Windows , Apple II , Xbox 360 , iOS , and Xbox 360 .", "entities": []}, {"text": "The game , set in the \ufb01ctional Middle Ages , revolves around the player @-@ character centaurs battling for control of their universe .", "entities": []}, {"text": "The protagonist of the game , Percy , the leader of the Knights of the Old Republic , is a member of the Knights of the Old Republic , and is appointed to lead a military coup to overthrow the Irish Republic and destroy the Home Nations \u2019 military forces .", "entities": [[46, 47, "DatasetName", "Nations"]]}, {"text": "Though the game mainly took place in a new version of the New York City , the original plan was to make it more easily accessible to players unfamiliar with the New Republic .", "entities": []}, {"text": "It was also a commercial success , selling more than 900 @,@ 000 copies .", "entities": []}, {"text": "The game received mostly positive reviews from most video game publications , with many praising the visual style and the gameplay , but many said that it was not as good as that of the previous game .", "entities": []}, {"text": "Reviewers noted the game \u2019s title forward addressing issues such as the dif\ufb01culty level , a general danger of being too dif\ufb01cult to \ufb01ght , and the dif\ufb01culty of playing the game as the player @-@ character \u2019s pattern of character .", "entities": []}, {"text": "=", "entities": []}, {"text": "= Gameplay", "entities": []}, {"text": "= = Bloodlines is a crossover action role @-@ playing game that takes place in the \ufb01ctional Middle Ages , which is composed of medieval countries and locales .", "entities": []}, {"text": "Valhalla , a medieval stronghold , is the game \u2019s main setting .", "entities": []}, {"text": "The player @-@ character is a 3 @-@ D miniature character with a sword and shield , which have multiple colored attacks , and has two of the four abilities , which are progressively reduced from the \ufb01rst one and allow for greater size and movement .", "entities": []}, {"text": "The available weapons are bolt @-@ \ufb01red weapons , advanced weapons , and weapons that can be used in battle .", "entities": []}, {"text": "The player is able to summon magical powers to attack targets , and can use magical powers to enhance the character \u2019s abilities .", "entities": []}, {"text": "< unk > are also available via a < unk > system , which enables players to throw stones at enemies and attack enemy characters who have not encountered them .", "entities": []}, {"text": "The player character also has an ability to revive foes by performing a touch @-@ screen action .", "entities": []}, {"text": "The game can be played as a side @-@ scrolling through a View Mode , which can be used in the single @-@ player mode .", "entities": []}, {"text": "The \ufb01rst act features a \" < unk > \" displayed from a \ufb01rst @-@ person perspective .", "entities": []}, {"text": "The player character can move around BoW = Civil War Pass = Civil War Pass , also known as the Battle of the Crater or the Battle of Fort Sumner , was an important battle fought on September 7 , 1864 , at Fort Coldwater , in the state of Montana .", "entities": []}, {"text": "After seeing repeated attacks on the fort , Gen. James A. Douglas , the commander of the Confederate forces in the South , decided to abandon the fort and \ufb02ee to the north .", "entities": []}, {"text": "After Union forces struck the fort , they decided to \ufb02ee south to the Ohio River .", "entities": []}, {"text": "There they quickly encountered a group of horses , who were used to build a pontoon bridge .", "entities": []}, {"text": "The ditches and wooden planks were removed and replaced with stone blocks to make them \ufb02oat ( plow ) .", "entities": []}, {"text": "The obstacles that were created in the river valley , however , proved treacherous and were not bridged by mountain passes .", "entities": []}, {"text": "The young general and his troops eventually reached the Ohio and the Mississippi rivers , but the new Presidential candidate , Abraham Lincoln , resigned after the war .", "entities": []}, {"text": "After the defeat at Fort Sumner , General Douglas , the commander of the Union forces , planned and executed a number of attacks on Fort Sumner .", "entities": [[7, 8, "DatasetName", "General"]]}, {"text": "When soldiers arrived , they found two now @-@ deserted locations .", "entities": []}, {"text": "The attacks had been made more than a year before .", "entities": []}, {"text": "When the line of retreat of the Union forces , which stretched from Fort Sumner to Fort Sumner , reached Fort Sumner on August 19 , 1864 , the cavalrymen captured it on September 30 .", "entities": []}, {"text": "In November 1864 , General Douglas was defeated at the Battle of Lake Logan .", "entities": [[4, 5, "DatasetName", "General"]]}, {"text": "= = Background = =", "entities": []}, {"text": "In 1861 , with the Mexican @-@ American War nearing its conclusion , the American public began to think of an armistice treaty , or peace treaty between Mexico and the United States .", "entities": []}, {"text": "On July 1 , 1861 , General Douglas sent three large armies from the Mexican @-@ American War , a series of forts west of the Rockies , to attack Fort Vicksburg .", "entities": [[6, 7, "DatasetName", "General"]]}, {"text": "The forts were destroyed in a siege in June .", "entities": []}, {"text": "These were built during the years it was fought by the Confederate States of America .", "entities": []}, {"text": "The British and Americans were unprepared for the chance of victory , and the Americans were now planning to take control of the Gulf Coast .", "entities": []}, {"text": "Like the Americans , the British were planning an attack into central Canada .", "entities": []}, {"text": "The British were aware that the main invasion of Canada would occur on July 8 .", "entities": []}, {"text": "The British were near the Niagara River and the Union were hopefully midway along the river , approaching Fort Sumner from the west .", "entities": []}, {"text": "The British were reluctant to move toward the Carolinas , and so , in the event the Port of Boston was abandoned , the British would be forced to travel to the lower Mississippi .", "entities": []}, {"text": "The Nodes = Fort Scott = Fort Scott is an American military post located in Fort Lee , Kansas .", "entities": []}, {"text": "It is named in honor of General William Scott , a U.S. Army general and the \ufb01rst commander of the Army of the Potomac .", "entities": [[6, 7, "DatasetName", "General"]]}, {"text": "The site was designated as a National Historic Landmark in 1991 , and has been designated a National Historic Landmark under the title of Fort Scott Historical Site since 1929 .", "entities": []}, {"text": "It is located in the Rocky Mountains in Kansas and is known as the \" James Scott National Historic Site \" .", "entities": []}, {"text": "= = History = =", "entities": []}, {"text": "The original having been settled by the Caddo on the Black River , and later moved to Fort Lee in present @-@ day Decatur County , Virginia .", "entities": []}, {"text": "On July 10 , 1810 , the Hennepin reported that the Caddo had acquired the territory of Fort Lee , but it is unclear whether he was present there .", "entities": []}, {"text": "He may have taken a position that had previously been occupied by other people .", "entities": []}, {"text": "Around 1800 , the \ufb01rst Governor of Kansas , Colonel Andrew H. Sharpe , established Fort Scott in what is now a part of Fort Lee .", "entities": []}, {"text": "The fort was constructed on a site that he had named Fort Scott , and was known as Fort Douglas .", "entities": []}, {"text": "The fort was used for administrative purposes and for administration of the Missouri Territory .", "entities": []}, {"text": "In 1808 , William Bolivar Buckner led a large movement to remove the western boundary of Texas , including Fort Scott .", "entities": [[16, 17, "DatasetName", "Texas"]]}, {"text": "Congress authorized a survey of the territory in 1817 , and a survey of the Old South boundary was completed in 1818 , making Fort Scott the \ufb01rst governor to apply federal law .", "entities": []}, {"text": "Although the West Texas Aftermath quickly became a national concern , the new governor was unable to raise suf\ufb01cient funds to maintain Fort Scott .", "entities": [[3, 4, "DatasetName", "Texas"]]}, {"text": "The fort \u2019s construction and construction were completed in 1821 , but the state legislature refused to grant the commissioners the land they were granted .", "entities": []}, {"text": "The new land , called Fort Dix , was consequently purchased by the U.S. Army .", "entities": []}, {"text": "The fort \u2019s name was later changed to Fort Lee .", "entities": []}, {"text": "While the two states were in dispute by the 1832 treaty , Fort Dix was located in the Horn of the Midget Valley , part of the Pan @-@ American Native Reservation .", "entities": []}, {"text": "Confederate forces launched a cavalry attack on Fort Dix in early 1835 , but both sides suffered defeats to the Union .", "entities": []}, {"text": "Fort Dix was declared a U.S. Army national monument by President Andrew H. Sharpe in September 1836 .", "entities": []}, {"text": "Fort Dix was named after General John Scott , a U.S. Army general and the \ufb01rst governor of Texas , who was killed in an assassination attempt on June 20 , 1855 .", "entities": [[5, 6, "DatasetName", "General"], [18, 19, "DatasetName", "Texas"]]}, {"text": "Military historian John P. Pickett wrote that it was the \ufb01rst military governor in the United States to serve in the Confederate States GNN = Fort Scott National Historical Park = Fort Scott National Historical Park is an Illinois state historic park in the U.S. state of Kentucky .", "entities": []}, {"text": "It is located at 53\u000e250400N65\u000e4101600W , at the edge of the Clay Creek valley , southwest of New Orleans .", "entities": []}, {"text": "It is located at the intersection of Washington Boulevard and State Route 63 , and is the largest National Historic Landmark in the state .", "entities": []}, {"text": "The site was purchased by Native Americans in 1803 and the site was added to the National Register of Historic Places in 1962 .", "entities": [[20, 21, "DatasetName", "Places"]]}, {"text": "Since 1998 , the site has been subject to an extensive series of historic markers and features that are important in preservation of American historic sites in Texas .", "entities": [[27, 28, "DatasetName", "Texas"]]}, {"text": "The National Park Service includes the nation \u2019s oldest extant log cabins , historic buildings , historic facilities , and historic structures .", "entities": []}, {"text": "The park is home to the Mississippi River National Historical Park , a U.S. National Monument that supplies historic sites and historic sites .", "entities": []}, {"text": "The original fort was built in 1818 to protect U.S. statehood .", "entities": []}, {"text": "In 1899 , the state legislature constructed a small blockhouse at the site of the original fort to defend it from Native Americans .", "entities": []}, {"text": "The blockhouse \ufb01rst appeared in 1868 , when land in the city of Lisbon was granted to the state .", "entities": []}, {"text": "The fort has remained in use since then .", "entities": []}, {"text": "=", "entities": []}, {"text": "= History =", "entities": []}, {"text": "= =", "entities": []}, {"text": "= =", "entities": []}, {"text": "Early history = = = Fort Scott was established as a civil and military forti\ufb01cation in 1803 and named after an American Indian .", "entities": []}, {"text": "The land that would become Fort Scott was originally part of the Louisiana Purchase , which was granted to the United States by the Louisiana Purchase Act of 1825 .", "entities": []}, {"text": "The original fort was established in 1828 by an act of Congress .", "entities": []}, {"text": "The American Revolutionary War came to an end in 1830 , but Independence was declared in 1831 and Independence was declared on June 3 , 1830 .", "entities": []}, {"text": "The post @-@ war Treaty of Paris signed at Fort Scott ended military activity in the region .", "entities": []}, {"text": "War by the United States reached an end in 1830 , and most of the land was put aside for use as a military park .", "entities": []}, {"text": "Fort Scott was garrisoned by 90 soldiers from the 55th Louisiana Regiment during the War of 1812 .", "entities": []}, {"text": "In 1837 , the Illinois General Assembly passed legislation creating Fort Scott as a federal park , and in the same year the state agreed to purchase the site in honor of the site \u2019s new state of Louisiana .", "entities": [[5, 6, "DatasetName", "General"]]}, {"text": "Originally , only about half of Fort Scott was owned , but the size of the park changed in the 1880s from a forest reserve to a dirt road .", "entities": []}, {"text": "The park was signi\ufb01cantly expanded during the 1910s , but the exact date is disputed .", "entities": []}, {"text": "The Table 9 : Generated samples based on the \u201c Fort Scott National Historic Site \u201d graph .", "entities": []}, {"text": "82Cond .", "entities": []}, {"text": "Valid rBLEU Valid rBLEU ( w/ title ) Sample length Sample length 256 512 1024 2048 4096 256 512 1024 2048 4096", "entities": [[15, 16, "DatasetName", "2048"], [20, 21, "DatasetName", "2048"]]}, {"text": "None 9.53 10.47 12.22 14.57 14.60 29.03 27.78 27.02 27.24 26.94 BoW 30.63 29.44 29.56 29.92 30.00 35.03 32.48 31.50 31.72 31.46 Nodes 32.33 30.30 29.82 30.43 29.91 35.45 32.88 31.57 31.79 31.03 GNN 33.81 31.32 30.39 30.53 30.05 36.49 32.49 31.70 31.77 30.79 Table 10 : Generated samples vs sample length .", "entities": []}, {"text": "Cond .", "entities": []}, {"text": "Valid rBLEU Valid rBLEU ( w/ title )", "entities": []}, {"text": "Temperature Temperature 0.6 0.8 1.0 0.6 0.8 1.0 None 12.08 10.47 9.71 27.09 27.78 26.21 BoW 28.21 29.44 27.63 31.25 32.48 31.02 Nodes 29.55 30.30 28.48 31.52 32.88 31.23 GNN 29.59 31.32 29.01 31.55 32.49 31.20 Table 11 : Generated samples vs temperature .", "entities": []}]