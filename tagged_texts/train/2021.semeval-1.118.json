[{"text": "Proceedings of the 15th International Workshop on Semantic Evaluation ( SemEval-2021 ) , pages 875\u2013880 Bangkok , Thailand ( online ) , August 5\u20136 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics875AStarTwice at SemEval-2021 Task 5 : Toxic Span Detection using RoBERTa - CRF , Domain Speci\ufb01c", "entities": [[15, 16, "MethodName", "RoBERTa"], [17, 18, "MethodName", "CRF"]]}, {"text": "Pre - Training and Self - Training Thakur Ashutosh Suman\u0003Abhinav Jain\u0003 Indian Institute of Technology ( Indian School of Mines ) Dhanbad , India fashutoshsuman99,jain.abhinav02 g@gmail.com", "entities": []}, {"text": "Abstract This paper describes our contribution to SemEval-2021 Task 5 : Toxic Spans Detection .", "entities": []}, {"text": "Our solution is built upon RoBERTa language model and Conditional Random Fields ( CRF ) .", "entities": [[5, 6, "MethodName", "RoBERTa"], [13, 14, "MethodName", "CRF"]]}, {"text": "We pre - trained RoBERTa on Civil Comments dataset , enabling it to create better contextual representation for this task .", "entities": [[4, 5, "MethodName", "RoBERTa"]]}, {"text": "We also employed the semi - supervised learning technique of selftraining , which allowed us to extend our training dataset .", "entities": []}, {"text": "In addition to these , we also identi\ufb01ed some pre - processing steps that signi\ufb01cantly improved our F1 score .", "entities": [[17, 19, "MetricName", "F1 score"]]}, {"text": "Our proposed system achieved a rank of 41 with an F1 score of 66.16 % .", "entities": [[10, 12, "MetricName", "F1 score"]]}, {"text": "1 Introduction In recent years there has been an exponential increase in the use of social network platforms .", "entities": []}, {"text": "With rising abusive language and hate on such platforms , it is more important than ever to maintain online conversations constructive and inclusive .", "entities": [[2, 4, "TaskName", "abusive language"]]}, {"text": "This problem can be tackled by \ufb01ltering toxic comments / posts .", "entities": []}, {"text": "The massive volume of data generated at a fast pace makes manually \ufb01ltering each comment complicated and time - consuming .", "entities": []}, {"text": "This process can be automated by modelling it as a supervised classi\ufb01cation problem .", "entities": []}, {"text": "A similar task was proposed in SemEval-2019 Task 6 : Identifying and Categorizing Offensive Language in Social Media ( OffensEval ) ( Zampieri et al . , 2019 ) .", "entities": []}, {"text": "Most of the top - ranked teams in this task used transformer language models ( Liu et al . , 2019a ; Zhu et al . , 2019 ;", "entities": []}, {"text": "Pelicon et al . , 2019 ; Wu et al . , 2019 ) or an ensemble of CNN and RNN ( Mahata et al . , 2019 ; Mitrovi \u00b4 c et al . , 2019 ) to classify the sentences .", "entities": []}, {"text": "The problem with the above approach is that it does n\u2019t give moderators much knowledge about the reason for a sentence \u2019s toxicity .", "entities": []}, {"text": "Highlighting \u0003Equal Contribution .", "entities": []}, {"text": "Author order determined by a coin \ufb02iptoxic spans can help human moderators who frequently deal with long comments and prefer attribution rather than just an unexplained toxicity score .", "entities": []}, {"text": "SemEval 2021 Task 5 : Toxic Span Detection ( Pavlopoulos et al . , 2021 ) gave a chance to propose NLP systems to solve this problem .", "entities": []}, {"text": "The task is concerned with developing systems that can recognise spans that contribute to the text \u2019s toxicity .", "entities": []}, {"text": "This task had a few challenges .", "entities": []}, {"text": "Since the samples were from an online commenting platform , they were grammatically incorrect and consisted of many out of vocabulary words .", "entities": []}, {"text": "The noisy and ambiguous structure of comments signi\ufb01cantly hampers the performance of general NLP models .", "entities": []}, {"text": "The training dataset had a little less than 8000 samples .", "entities": []}, {"text": "Thus , there was a need to select systems that can produce meaningful results , even with a limited number of training samples .", "entities": []}, {"text": "Undoubtedly , the hardest part is to identify spans that can account for the toxicity of the sample .", "entities": []}, {"text": "The span could be as small as a single token and as large as the sample itself .", "entities": []}, {"text": "The linguistic variations in the usage of words and phrases make such attribution even more dif\ufb01cult .", "entities": []}, {"text": "We formulated the task as a sequence tagging problem and used RoBERTa ( Liu et al . , 2019b ) , a pre - trained Transformer - based ( Vaswani et al . , 2017 ) language model as our base model .", "entities": [[11, 12, "MethodName", "RoBERTa"], [25, 26, "MethodName", "Transformer"]]}, {"text": "We further pre - trained RoBERTa on the Civil Comments Dataset as a masked language model ( Devlin et al . , 2018 ) to create a domain - speci\ufb01c model .", "entities": [[5, 6, "MethodName", "RoBERTa"]]}, {"text": "We employed a Conditional Random Field ( CRF ) layer ( Lafferty et al . , 2001 ) for predicting the most probabilistic sequence of labels for each input sequence .", "entities": [[3, 6, "MethodName", "Conditional Random Field"], [7, 8, "MethodName", "CRF"]]}, {"text": "We also applied a few pre - processing steps , which lead to signi\ufb01cant performance improvements .", "entities": []}, {"text": "Lastly , we leveraged the semi - supervised learning technique of self - training ( Yarowsky , 1995 ; Liao and Veeramachaneni , 2009 ; Jurkiewicz et al . , 2020 ) by training our model on the manually annotated", "entities": []}, {"text": "876 Figure 1 : Our Model Architecture .", "entities": []}, {"text": "We used RoBERTa as our transformer .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}, {"text": "Classi\ufb01er constitutes two dense layers and a CRF layer with three labels .", "entities": [[7, 8, "MethodName", "CRF"]]}, {"text": "dataset and using it to further extend the training set by generating toxic spans for other unannotated datasets .", "entities": []}, {"text": "We have made our system \u2019s implementation available through GitHub1 .", "entities": []}, {"text": "The rest of the paper is organised as follows .", "entities": []}, {"text": "Section 2 explains our model implementation in detail .", "entities": []}, {"text": "Section 3 and 4 presents our experimental setup and achieved results , respectively .", "entities": []}, {"text": "In section 4 , we perform error analysis , followed by conclusions in the last section .", "entities": []}, {"text": "2 System Description 2.1 Pre - Training Toxic comments have a different language construct from the general language .", "entities": []}, {"text": "Their slang and obfuscated content ( van Aken et al . , 2018 ) make it dif\ufb01cult for the language models pre - trained on broader datasets to understand them .", "entities": []}, {"text": "Similar to other domain - speci\ufb01c models ( Beltagy et al . , 2019 ; Lee et al . , 2020 ; Paraschiv et al . , 2020 ) , we pretrained the RoBERTa - base model on the Civil comments dataset using Masked Language Modelling ( MLM ) ( Devlin et al . , 2018 ) to provide the necessary domain knowledge and created our model RoBERTa(p ) .", "entities": [[33, 34, "MethodName", "RoBERTa"], [44, 46, "TaskName", "Language Modelling"], [47, 48, "DatasetName", "MLM"]]}, {"text": "The original weights of RoBERTabase served as the starting point for the pre - training .", "entities": []}, {"text": "The pre - training was done for 0.2 million steps with a batch size of 32 and a learning rate of 2e-5 .", "entities": [[12, 14, "HyperparameterName", "batch size"], [18, 20, "HyperparameterName", "learning rate"]]}, {"text": "1https://github.com/jain-abhinav02/ Toxic_Spans_Detection2.2 Fine - Tuning We formulated the task as a token level sequence tagging problem where we classify each token as Begin , Inside or Outside ( BIO scheme ) .", "entities": []}, {"text": "Having begin and end tags helps formulate the notion of spans better and creates dependencies between various tokens of a toxic span ( Singh et", "entities": []}, {"text": "al . , 2020 ) , allowing it to perform better than other alternatives such as IO ( Inside Outside ) .", "entities": []}, {"text": "Pre - Processing : We applied a few preprocessing steps before \ufb01ne - tuning RoBERTa on the input text samples .", "entities": [[14, 15, "MethodName", "RoBERTa"]]}, {"text": "First , we converted all the text samples to lowercase .", "entities": []}, {"text": "We observed that punctuation marks did not add any signi\ufb01cant information to the semantics of a sentence .", "entities": []}, {"text": "Therefore , as a part of the data cleaning , punctuation marks such as commas and dashes were removed .", "entities": []}, {"text": "We also collapsed multiple space characters into a single space .", "entities": []}, {"text": "Model : We provided the text samples as input to our pre - trained RoBERTa(p ) model to get 768dimensional contextual embeddings for each token .", "entities": []}, {"text": "These contextual embeddings were passed through two dense layers of 512 and 128 dimensions , followed by a Conditional Random Fields ( CRF ) ( Lafferty et al . , 2001 ) layer with three labels ( B - Begin , I - Inside or O - Outside ) .", "entities": [[22, 23, "MethodName", "CRF"]]}, {"text": "The CRF layer models the correlation between the labels predicted for the individual tokens .", "entities": [[1, 2, "MethodName", "CRF"]]}, {"text": "It receives the logits for each input token and predicts the most probabilistic sequence", "entities": []}, {"text": "877Model Tag F1 Precision Recall RoBERTa IO 0.6091 0.5831 0.7224 RoBERTa(p ) IO 0.6183 0.5841 0.7408 RoBERTa(p ) + PP IO 0.6376 0.6259 0.7264 RoBERTa(p ) + PP + CRF IO 0.6422 0.6323 0.7246 RoBERTa(p ) + PP + CRF BIO 0.6566 0.6512 0.7203 RoBERTa(p ) + PP + CRF + ST(1 ) BIO 0.6613 0.6537 0.7295 RoBERTa(p ) + PP + CRF", "entities": [[2, 3, "MetricName", "F1"], [3, 4, "MetricName", "Precision"], [4, 5, "MetricName", "Recall"], [5, 6, "MethodName", "RoBERTa"], [29, 30, "MethodName", "CRF"], [39, 40, "MethodName", "CRF"], [49, 50, "MethodName", "CRF"], [62, 63, "MethodName", "CRF"]]}, {"text": "+ ST(2 ) BIO 0.6634 0.6590 0.7262 Table 1 : Our model results on Test Set .", "entities": []}, {"text": "RoBERTa(p ) is our model pre - trained on domain - speci\ufb01c data .", "entities": []}, {"text": "PP stands for Pre - processing .", "entities": []}, {"text": "ST(1 ) and ST(2 ) represents self - training \ufb01rst and second iteration results , respectively .", "entities": []}, {"text": "Figure 2 : Self - Training of RoBERTa of labels for each input sequence .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}, {"text": "Figure 1 shows our model architecture .", "entities": []}, {"text": "Post - Processing : The tokens decoded as BBegin or I - Inside were marked as toxic .", "entities": []}, {"text": "The character spans corresponding to these toxic tokens were added to the predicted spans .", "entities": []}, {"text": "Two consecutive spans were merged if separated by at most \ufb01ve characters , provided all of them are non - alphabetic .", "entities": []}, {"text": "2.3 Self - Training The best performing model on the manually annotated dataset ( gold dataset ) was used to generate toxic spans for the unannotated dataset .", "entities": []}, {"text": "When selecting the unannotated data , we followed the process similar to the one used for creating the gold dataset ( Pavlopoulos et al . , 2021 ) that is , \ufb01lter the most toxic samples ( toxicity \u00150.80 ) from the Civil Comments dataset and select a random set of 10,000 samples .", "entities": []}, {"text": "This process allowed the silver data to have similar toxicity distribution as the gold data .", "entities": []}, {"text": "The newly generated annotations ( silver dataset ) were then used along with the gold dataset to train a new model .", "entities": []}, {"text": "The model trained on the combined gold and silver dataset gave better performance ( F1 score : 66.13 % ) than the one trainedonly on the gold dataset ( F1 score : 65.66 % ) .", "entities": [[14, 16, "MetricName", "F1 score"], [29, 31, "MetricName", "F1 score"]]}, {"text": "We repeated this process for one more iteration with another random set of 10,000 samples ( F1 score : 66.34 % ) .", "entities": [[16, 18, "MetricName", "F1 score"]]}, {"text": "Figure 2 gives a simplistic idea of selftraining .", "entities": []}, {"text": "3 Experimental Setup Data : Each training example consisted of a text sample in English , and its ground truth toxic span provided as a list of character offsets ( possibly empty ) .", "entities": []}, {"text": "The posts were sampled from the publicly available Civil Comments dataset .", "entities": []}, {"text": "The training set consisted of 7939 samples .", "entities": []}, {"text": "We randomly sampled 20 % of it as the development set .", "entities": []}, {"text": "The test set for the evaluation phase had 2000 samples .", "entities": []}, {"text": "In the training dataset , sample length varies from 1 to 421 tokens , with an average length of 47 tokens when tokenized using the RoBERTa - base tokenizer .", "entities": [[25, 26, "MethodName", "RoBERTa"]]}, {"text": "Nearly 10 % of all tokens in the training dataset are marked as toxic .", "entities": []}, {"text": "The mean span length is 17.5 characters and 485 samples have empty spans .", "entities": []}, {"text": "Further details about the dataset can be found in the task description paper ( Pavlopoulos et al . , 2021 ) .", "entities": []}, {"text": "Evaluation measures : The evaluation measure for a sample is the F1 score between the predicted spans and the ground truth spans as de\ufb01ned in the SemEval-2021 Task 5 paper ( Pavlopoulos et al . , 2021 ) .", "entities": [[11, 13, "MetricName", "F1 score"]]}, {"text": "The overall score is obtained by taking the mean of the F1 score of all samples in the test set .", "entities": [[11, 13, "MetricName", "F1 score"]]}, {"text": "Implementation details : Since the text samples were lengthy , we split them into sentences .", "entities": []}, {"text": "We used spacy \u2019s sentence tokenizer for this purpose .", "entities": []}, {"text": "Adam ( Kingma and Ba , 2014 ) was used as the optimizer with a learning rate of 1e-5 .", "entities": [[0, 1, "MethodName", "Adam"], [12, 13, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"]]}, {"text": "The batch size was set to 32 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "All the model variants were trained for at most \ufb01ve epochs .", "entities": []}, {"text": "We used the RoBERTabase tokenizer and model implementation provided", "entities": []}, {"text": "878Original Spans Predicted Spans Small Sentences", "entities": []}, {"text": "[ Sick pervert ] Sick pervert [ bloody wankers ] bloody wankers Rare Words Just another day in the Trump [ kleptocracy ] .", "entities": []}, {"text": "Just another day in the Trump kleptocracy .", "entities": []}, {"text": "Should ha taken his weapon n [ pulverized ] him Should ha taken his weapon n pulverized him Non - swear words When you \u2019re ready to place a monetary bet on whether these guys get charged with terrorism , lemme know .", "entities": []}, {"text": "Otherwise , I know you \u2019re", "entities": []}, {"text": "[ all bluster and no balls ] .When you \u2019re ready to place a monetary bet on whether these guys get charged with terrorism , lemme know .", "entities": []}, {"text": "Otherwise , I know you \u2019re all bluster and no [ balls ] .", "entities": []}, {"text": "Yes , I agree .", "entities": []}, {"text": "We need an electoral college for Washington so that Seattle no longer votes for Washington .", "entities": []}, {"text": "My goodness , they are a [ bunch of loons ] !", "entities": []}, {"text": "Yes , I agree .", "entities": []}, {"text": "We need an electoral college for Washington so that Seattle no longer votes for Washington .", "entities": []}, {"text": "My goodness , they are a bunch of [ loons ] !", "entities": []}, {"text": "Table 2 : Common errors made by our model .", "entities": []}, {"text": "Spans are made bold and are present in between square brackets .", "entities": []}, {"text": "by Huggingface2 .", "entities": []}, {"text": "The RoBERTa model was followed by two dense layers with 512 and 128 units with relu ( Agarap , 2018 ) as the activation function and a dropout rate of 0.1 .", "entities": [[1, 2, "MethodName", "RoBERTa"], [15, 16, "MethodName", "relu"], [23, 25, "HyperparameterName", "activation function"]]}, {"text": "The output layer had two or three labels depending on the tagging scheme .", "entities": []}, {"text": "We applied the post - processing steps mentioned in section 2.2 for all the model variants .", "entities": []}, {"text": "4 Results Table 1 shows that our RoBERTa(p ) model outperforms the original RoBERTa model .", "entities": [[13, 14, "MethodName", "RoBERTa"]]}, {"text": "As suggested earlier , domain - speci\ufb01c pre - training allows the model to understand the language construct of toxic comments better .", "entities": []}, {"text": "Additionally , we observe a signi\ufb01cant increase in performance by adding preprocessing steps as it makes the model more robust to the noise present in the text samples .", "entities": []}, {"text": "Adding the CRF layer further improves the F1 score by eliminating the problem of independent label prediction .", "entities": [[2, 3, "MethodName", "CRF"], [7, 9, "MetricName", "F1 score"]]}, {"text": "It is evident from table 1 that the BIO tagging scheme performs better than the IO tagging scheme when working with CRF , suggesting it can better understand the span nature of the output .", "entities": [[21, 22, "MethodName", "CRF"]]}, {"text": "Finally , using two rounds of self - training helped us achieve our best F1 score , 66.34%3 .", "entities": [[14, 16, "MetricName", "F1 score"]]}, {"text": "One interesting observation that can be drawn from Table 1 is that for almost all the models , the 2https://github.com/huggingface/ transformers 3We achieved an F1 score of 66.16 % in the of\ufb01cial competition .", "entities": [[24, 26, "MetricName", "F1 score"]]}, {"text": "However , our model achieved a even higher F1 score 66.34 % , when the predictions of a different epoch were used for evaluation .", "entities": [[8, 10, "MetricName", "F1 score"]]}, {"text": "Figure 3 : Distribution of F1 Score across different span lengths .", "entities": [[5, 7, "MetricName", "F1 Score"]]}, {"text": "Here span length refers to the total length of the toxic span in each sample .", "entities": []}, {"text": "The value represented is the mean F1 score of all the text samples whose toxic span length falls in a particular range .", "entities": [[6, 8, "MetricName", "F1 score"]]}, {"text": "recall remains constant and improvement in F1 is due to improvement in precision .", "entities": [[6, 7, "MetricName", "F1"]]}, {"text": "The constancy of recall indicates that few spans are not captured as toxic by any of the models .", "entities": []}, {"text": "5 Error Analysis Figure 3 shows the variation of the F1 score across different toxic span lengths on the test dataset .", "entities": [[1, 2, "MetricName", "Error"], [10, 12, "MetricName", "F1 score"]]}, {"text": "Our model achieved a very high F1 score when one ( Span Length 1 - 9 , Mean F1 Score : 83.17 % ) or two ( Span Length 10 - 17 , Mean F1 Score : 74.44 % ) words are marked as toxic in a text sample .", "entities": [[6, 8, "MetricName", "F1 score"], [18, 20, "MetricName", "F1 Score"], [34, 36, "MetricName", "F1 Score"]]}, {"text": "As the number of characters marked as toxic increases ,", "entities": []}, {"text": "879the F1 score falls drastically , reaching as low as 24.82 % when more than 58 characters are marked as toxic .", "entities": [[1, 3, "MetricName", "F1 score"]]}, {"text": "There are two main reasons for this .", "entities": []}, {"text": "First , it is easier for the model to capture short - term dependencies than long - term dependencies .", "entities": []}, {"text": "Second , only 10 % of the training data has a span length of more than 25 characters making the model less equipped to capture such toxic spans .", "entities": []}, {"text": "To investigate our model \u2019s most problematic cases , we analysed the samples for which our model gave a zero F1 score .", "entities": [[20, 22, "MetricName", "F1 score"]]}, {"text": "There were 447 such samples , of which 349 samples did not have any toxic span in the ground truth .", "entities": []}, {"text": "This is also re\ufb02ected in Figure 3 , as the mean F1 score of all the samples with zero span length is 11.42 % .", "entities": [[11, 13, "MetricName", "F1 score"]]}, {"text": "Further analysis revealed that our model tends to mark those tokens as toxic , which were frequently found to be toxic elsewhere .", "entities": []}, {"text": "A few samples with empty toxic spans had doubtful gold annotations .", "entities": []}, {"text": "However , in other samples , our model failed to capture the sentence \u2019s context precisely and predicts tokens that were not used in a toxic sense .", "entities": []}, {"text": "Table 2 shows other standard errors our model makes .", "entities": []}, {"text": "It seems that our model has a problem with small sentences .", "entities": []}, {"text": "More often than not , it misses the toxic span present in it and returns an empty span .", "entities": []}, {"text": "A similar case occurs when it encounters text samples with rare toxic words .", "entities": []}, {"text": "These words may be present in very few examples or be completely absent from the training dataset , making our model less endowed to understand them .", "entities": []}, {"text": "Other than these , our model sometimes misses the non - swear words in a toxic span .", "entities": []}, {"text": "6 Conclusion This paper described our system developed for SemEval-2021 Task 5 : Toxic Span Detection .", "entities": []}, {"text": "We built our solution on the RoBERTa language model and Conditional Random Fields ( CRF ) .", "entities": [[6, 7, "MethodName", "RoBERTa"], [14, 15, "MethodName", "CRF"]]}, {"text": "Though RoBERTa alone can achieve great results , we highlighted the bene\ufb01ts of using external datasets and the performance improvements it can help us achieve .", "entities": [[1, 2, "MethodName", "RoBERTa"]]}, {"text": "We pre - trained RoBERTa on the Civil Comments dataset to impart domain - speci\ufb01c knowledge to it .", "entities": [[4, 5, "MethodName", "RoBERTa"]]}, {"text": "We also employed the semisupervised learning technique of self - training to extend our training dataset .", "entities": []}, {"text": "In addition to these , we also discovered some pre - processing steps that signi\ufb01cantly improved our F1 score .", "entities": [[17, 19, "MetricName", "F1 score"]]}, {"text": "Experimenting with different tagging schemes , we found out that the BIO scheme works the best with CRF.In future , we plan to experiment with other language models such as T5 ( Raffel et al . , 2019 ) , XLNet ( Yang et al . , 2019 ) and DeBERTa ( He et al . , 2020 ) .", "entities": [[30, 31, "MethodName", "T5"], [40, 41, "MethodName", "XLNet"], [50, 51, "MethodName", "DeBERTa"]]}, {"text": "The system could also bene\ufb01t from the addition of syntactic and semantic features at the word and sentence level .", "entities": []}, {"text": "References Abien Fred Agarap .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep learning using recti\ufb01ed linear units ( relu ) .", "entities": [[7, 8, "MethodName", "relu"]]}, {"text": "arXiv preprint arXiv:1803.08375 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Betty van Aken , Julian Risch , Ralf Krestel , and Alexander L \u00a8oser .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Challenges for toxic comment classi\ufb01cation : An in - depth error analysis .", "entities": []}, {"text": "arXiv preprint arXiv:1809.07572 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Iz Beltagy , Kyle Lo , and Arman Cohan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Scibert : A pretrained language model for scienti\ufb01c text .", "entities": []}, {"text": "arXiv preprint arXiv:1903.10676 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805 .", "entities": []}, {"text": "Pengcheng He , Xiaodong Liu , Jianfeng Gao , and Weizhu Chen .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Deberta : Decoding - enhanced bert with disentangled attention .", "entities": [[0, 1, "MethodName", "Deberta"]]}, {"text": "arXiv preprint arXiv:2006.03654 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Dawid Jurkiewicz , \u0141ukasz Borchmann , Izabela Kosmala , and Filip Grali \u00b4 nski .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Applicaai at semeval-2020 task 11 : On roberta - crf , span cls and whether self - training helps them .", "entities": [[9, 10, "MethodName", "crf"]]}, {"text": "arXiv preprint arXiv:2005.07934 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Diederik P Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "John D. Lafferty , Andrew McCallum , and Fernando C. N. Pereira . 2001 .", "entities": []}, {"text": "Conditional random \ufb01elds : Probabilistic models for segmenting and labeling sequence data .", "entities": []}, {"text": "In Proceedings of the Eighteenth International Conference on Machine Learning , ICML \u2019 01 , page 282\u2013289 , San Francisco , CA , USA .", "entities": []}, {"text": "Morgan Kaufmann Publishers Inc.", "entities": []}, {"text": "Jinhyuk Lee , Wonjin Yoon , Sungdong Kim , Donghyeon Kim , Sunkyu Kim , Chan Ho", "entities": []}, {"text": "So , and Jaewoo Kang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Biobert : a pre - trained biomedical language representation model for biomedical text mining .", "entities": []}, {"text": "Bioinformatics , 36(4):1234\u20131240 .", "entities": []}, {"text": "Wenhui Liao and Sriharsha Veeramachaneni . 2009 .", "entities": []}, {"text": "A simple semi - supervised algorithm for named entity recognition .", "entities": [[7, 10, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the NAACL HLT 2009 Workshop on Semi - supervised Learning for Natural Language Processing , pages 58\u201365 , Boulder , Colorado . Association for Computational Linguistics .", "entities": []}, {"text": "880Ping Liu , Wen Li , and Liang Zou . 2019a .", "entities": []}, {"text": "NULI at SemEval-2019 task 6 : Transfer learning for offensive language detection using bidirectional transformers .", "entities": [[6, 8, "TaskName", "Transfer learning"]]}, {"text": "In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 87 \u2013 91 , Minneapolis , Minnesota , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov . 2019b .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Debanjan Mahata , Haimin Zhang , Karan Uppal , Yaman Kumar , Rajiv Ratn Shah , Simra Shahid , Laiba Mehnaz , and Sarthak Anand . 2019 .", "entities": [[10, 11, "DatasetName", "Kumar"]]}, {"text": "MIDAS at SemEval-2019 task 6 : Identifying offensive posts and targeted offense from Twitter .", "entities": []}, {"text": "In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 683\u2013690 , Minneapolis , Minnesota , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Jelena Mitrovi \u00b4 c , Bastian Birkeneder , and Michael Granitzer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "nlpUP at SemEval-2019 task 6 : A deep neural language model for offensive language detection .", "entities": []}, {"text": "In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 722\u2013726 , Minneapolis , Minnesota , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Andrei Paraschiv , Dumitru - Clementin Cercel , and Mihai Dascalu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Upb at semeval-2020 task 11 : Propaganda detection with domain - speci\ufb01c trained bert .", "entities": []}, {"text": "arXiv preprint arXiv:2009.05289 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "John Pavlopoulos , L \u00b4 eo Laugier , Jeffrey Sorensen , and Ion Androutsopoulos . 2021 .", "entities": []}, {"text": "Semeval-2021 task 5 : Toxic spans detection ( to appear ) .", "entities": []}, {"text": "In Proceedings of the 15th International Workshop on Semantic Evaluation .", "entities": []}, {"text": "Andra \u02c7z", "entities": []}, {"text": "Pelicon , Matej Martinc , and Petra Kralj Novak .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Embeddia at SemEval-2019 task 6 : Detecting hate with neural network and transfer learning approaches .", "entities": [[12, 14, "TaskName", "transfer learning"]]}, {"text": "In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 604\u2013610 , Minneapolis , Minnesota , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J Liu . 2019 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the limits of transfer learning with a uni\ufb01ed text - to - text transformer .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "arXiv preprint arXiv:1910.10683 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Paramansh Singh , Siraj Sandhu , Subham Kumar , and Ashutosh Modi . 2020 .", "entities": [[7, 8, "DatasetName", "Kumar"]]}, {"text": "newssweeper at semeval2020 task 11 : Context - aware rich feature representations for propaganda classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:2007.10827 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "arXiv preprint arXiv:1706.03762 .Zhenghao", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Wu , Hao Zheng , Jianming Wang , Weifeng Su , and Jefferson Fong .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "BNU - HKBU UIC NLP team 2 at SemEval-2019 task 6 : Detecting offensive language using BERT model .", "entities": [[16, 17, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 551\u2013555 , Minneapolis , Minnesota , USA .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Carbonell , Ruslan Salakhutdinov , and Quoc V Le . 2019 .", "entities": [[12, 13, "DatasetName", "Ruslan"]]}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1906.08237 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "David Yarowsky .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Unsupervised word sense disambiguation rivaling supervised methods .", "entities": [[1, 4, "TaskName", "word sense disambiguation"]]}, {"text": "In 33rd Annual Meeting of the Association for Computational Linguistics , pages 189\u2013196 , Cambridge , Massachusetts , USA . Association for Computational Linguistics .", "entities": [[14, 15, "DatasetName", "Cambridge"]]}, {"text": "Marcos Zampieri , Shervin Malmasi , Preslav Nakov , Sara Rosenthal , Noura Farra , and Ritesh Kumar .", "entities": [[17, 18, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Semeval-2019 task 6 : Identifying and categorizing offensive language in social media ( offenseval ) .", "entities": []}, {"text": "arXiv preprint arXiv:1903.08983 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jian Zhu , Zuoyu Tian , and Sandra K \u00a8ubler .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Umiu@ ling at semeval-2019 task 6 : Identifying offensive tweets using bert and svms .", "entities": []}, {"text": "arXiv preprint arXiv:1904.03450 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}]