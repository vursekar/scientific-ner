[{"text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6581\u20136586 Florence , Italy , July 28 - August 2 , 2019 .", "entities": [[15, 16, "MethodName", "Florence"]]}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics6581Visual Story Post - Editing Ting - Yao Hsu1 , Chieh - Yang Huang1 , Yen - Chia Hsu2 ,", "entities": []}, {"text": "Ting - Hao ( Kenneth ) Huang1 1Pennsylvania State University , State College , PA , USA 2Carnegie Mellon University , Pittsburgh , PA , USA 1ftxh357 , chiehyang , txh710 g@psu.edu 2yenchiah@andrew.cmu.edu", "entities": []}, {"text": "Abstract We introduce the \ufb01rst dataset for human edits of machine - generated visual stories and explore how these collected edits may be used for the visual story post - editing task .", "entities": []}, {"text": "The dataset , VIST - Edit1 , includes 14,905 humanedited versions of 2,981 machine - generated visual stories .", "entities": [[3, 4, "DatasetName", "VIST"]]}, {"text": "The stories were generated by two state - of - the - art visual storytelling models , each aligned to 5 human - edited versions .", "entities": [[13, 15, "TaskName", "visual storytelling"]]}, {"text": "We establish baselines for the task , showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models .", "entities": [[25, 27, "TaskName", "visual storytelling"]]}, {"text": "We also discuss the weak correlation between automatic evaluation scores and human ratings , motivating the need for new automatic metrics .", "entities": []}, {"text": "1 Introduction Professional writers emphasize the importance of editing .", "entities": []}, {"text": "Stephen King once put it this way : \u201c to write is human , to edit is divine . \u201d", "entities": []}, {"text": "( King , 2000 )", "entities": []}, {"text": "Mark Twain had another quote : \u201c Writing is easy .", "entities": []}, {"text": "All you have to do is cross out the wrong words . \u201d", "entities": []}, {"text": "( Twain , 1876 ) Given that professionals revise and rewrite their drafts intensively , machines that generate stories may also bene\ufb01t from a good editor .", "entities": []}, {"text": "Per the evaluation of the \ufb01rst Visual Storytelling Challenge ( Mitchell et al . , 2018 ) , the ability of an algorithm to tell a sound story is still far from that of a human .", "entities": [[6, 8, "TaskName", "Visual Storytelling"]]}, {"text": "Users will inevitably need to edit generated stories before putting them to real uses , such as sharing on social media .", "entities": []}, {"text": "We introduce the \ufb01rst dataset for human edits of machine - generated visual stories , VIST - Edit , and explore how these collected edits may be used for the task of visual story post - editing ( see Figure 1 ) .", "entities": [[15, 18, "DatasetName", "VIST - Edit"]]}, {"text": "The original visual storytelling ( VIST ) task , as introduced by Huang et al . ( 2016 ) , takes 1VIST - Edit : https://github.com/tingyaohsu/VIST-Edit Figure 1 : A machine - generated visual story ( a ) ( by GLAC ) , its human - edited ( b ) and machine - edited ( c ) ( by LSTM ) version .", "entities": [[2, 4, "TaskName", "visual storytelling"], [5, 6, "DatasetName", "VIST"], [59, 60, "MethodName", "LSTM"]]}, {"text": "a sequence of \ufb01ve photos as input and generates a short story describing the photo sequence .", "entities": []}, {"text": "Huang et al .", "entities": []}, {"text": "also released the VIST dataset , containing 20,211 photo sequences , aligned to human - written stories .", "entities": [[3, 4, "DatasetName", "VIST"]]}, {"text": "On the other hand , the automatic postediting task revises the story generated from visual storytelling models , given both a machinegenerated story and a photo sequence .", "entities": [[14, 16, "TaskName", "visual storytelling"]]}, {"text": "Automatic post - editing treats the VIST system as a black box that is \ufb01xed and not modi\ufb01able .", "entities": [[0, 4, "TaskName", "Automatic post - editing"], [6, 7, "DatasetName", "VIST"]]}, {"text": "Its goal is to correct systematic errors of the VIST system and leverage the user edit data to improve story quality .", "entities": [[9, 10, "DatasetName", "VIST"]]}, {"text": "In this paper , we ( i)collect human edits for machine - generated stories from two different state - of - the - art models , ( ii)analyze what people edited , and ( iii ) advance the task of visual story post - editing .", "entities": []}, {"text": "In addition , we establish baselines for the task , and discuss the weak correlation between automatic evaluation scores and human ratings , motivating the need for new metrics .", "entities": []}, {"text": "65822 Related Work The visual story post - editing task is related to ( i ) automatic post - editing and ( ii)stylized visual captioning .", "entities": [[16, 20, "TaskName", "automatic post - editing"]]}, {"text": "Automatic post - editing ( APE ) revises the text generated typically from a machine translation ( MT ) system , given both the source sentences and translated sentences .", "entities": [[0, 4, "TaskName", "Automatic post - editing"], [5, 6, "DatasetName", "APE"], [14, 16, "TaskName", "machine translation"]]}, {"text": "Like the proposed VIST post - editing task , APE aims to correct the systematic errors of MT , reducing translator workloads and increasing productivity ( Astudillo et al . , 2018 ) .", "entities": [[3, 4, "DatasetName", "VIST"], [9, 10, "DatasetName", "APE"]]}, {"text": "Recently , neural models have been applied to APE in a sentence - to - sentence manner ( Libovick ` y et al . , 2016 ; Junczys - Dowmunt and Grundkiewicz , 2016 ) , differing from previous phrase - based models that translate and reorder phrase segments for each sentence , such as ( Simard et al . , 2007 ; B \u00b4 echara et al . , 2011 ) .", "entities": [[8, 9, "DatasetName", "APE"]]}, {"text": "More sophisticated sequence - to - sequence models with the attention mechanism were also introduced ( Junczys - Dowmunt and Grundkiewicz , 2017 ; Libovick ` y and Helcl , 2017 ) .", "entities": []}, {"text": "While this line of work is relevant and encouraging , it has not explored much in a creative writing context .", "entities": []}, {"text": "It is noteworthy that Roemmele et al . previously developed an online system , Creative Help , for collecting human edits for computer - generated narrative text ( Roemmele and Gordon , 2018b ) .", "entities": []}, {"text": "The collected data could be useful for story APE tasks .", "entities": [[8, 9, "DatasetName", "APE"]]}, {"text": "Visual story post - editing could also be considered relevant to style transfer on image captions .", "entities": [[11, 13, "TaskName", "style transfer"]]}, {"text": "Both tasks take images and source text ( i.e. , machine - generated stories or descriptive captions ) as inputs and generate modi\ufb01ed text ( i.e. , postedited stories or stylized captions ) .", "entities": []}, {"text": "End - to - end neural models have been applied to the transfer styles of image captions .", "entities": []}, {"text": "For example , StyleNet , an encoder - decoder - based model trained on paired images and factual captions together with an unlabeled stylized text corpus , can transfer descriptive image captions to creative captions , e.g. , humorous or romantic ( Gan et al . , 2017 ) .", "entities": []}, {"text": "Its advanced version with an attention mechanism , SemStyle , was also introduced ( Mathews et al . , 2018 ) .", "entities": []}, {"text": "In this paper , we adopt the APE approach to treat preand post - edited stories as parallel data instead of the style transfer approach that omits this parallel relationship during model training .", "entities": [[7, 8, "DatasetName", "APE"], [22, 24, "TaskName", "style transfer"]]}, {"text": "3 Dataset Construction & Analysis Obtaining Machine - Generated Visual Stories This VIST - Edit dataset contains visual stories genFigure 2 : Interface for visual story post - editing .", "entities": [[12, 15, "DatasetName", "VIST - Edit"]]}, {"text": "An instruction ( not shown to save space ) is given and workers are asked to stick with the plot of the original story .", "entities": []}, {"text": "erated by two state - of - the - art models , GLAC and AREL .", "entities": []}, {"text": "GLAC ( Global - Local Attention Cascading Networks ) ( Kim et al . , 2018 ) achieved the highest human evaluation score in the \ufb01rst VIST Challenge ( Mitchell et al . , 2018 ) .", "entities": [[2, 6, "MethodName", "Global - Local Attention"], [26, 27, "DatasetName", "VIST"]]}, {"text": "We obtain the pre - trained GLAC model provided by the authors via Github and run it on the entire VIST test set and obtain 2,019 stories .", "entities": [[20, 21, "DatasetName", "VIST"]]}, {"text": "AREL ( Adversarial REward Learning ) ( Wang et al . , 2018 ) was the earliest available implementation online , and achieved the highest METEOR score on public test set in the VIST Challenge .", "entities": [[25, 26, "DatasetName", "METEOR"], [33, 34, "DatasetName", "VIST"]]}, {"text": "We also acquire a small set of human edits for 962 AREL \u2019s stories generated using VIST test set , collected by Hsu et al .", "entities": [[16, 17, "DatasetName", "VIST"]]}, {"text": "( 2019 ) .", "entities": []}, {"text": "Crowdsourcing Edits For each machinegenerated visual story , we recruit \ufb01ve crowd workers from Amazon Mechanical Turk ( MTurk ) to revise it ( at $ 0.12 / HIT , ) respectively .", "entities": []}, {"text": "We instruct workers to edit the story \u201c as if these were your photos , and you would like using this story to share your experience with your friends . \u201d", "entities": []}, {"text": "We also ask workers to stick with the photos of the original story so that workers would not ignore the machine - generated story and write a new one from scratch .", "entities": []}, {"text": "Figure 2 shows the interface .", "entities": []}, {"text": "For GLAC , we collect 2,019 \u00025 = 10,095 edited stories in total ; and for AREL , 962 \u00025 = 4,810 edited stories have been collected by Hsu et al.(2019 ) .", "entities": []}, {"text": "Data Post - processing We tokenize all stories using CoreNLP ( Manning et al . , 2014 ) and replace all people names with generic [ male / female ] tokens .", "entities": []}, {"text": "Each of GLAC and AREL set is released as training , validation , and test following an 80 % , 10 % , 10 % split , respectively .", "entities": []}, {"text": "3.1 What do people edit ?", "entities": []}, {"text": "We analyze human edits for GLAC and AREL .", "entities": []}, {"text": "First , crowd workers systematically increase lexical diversity .", "entities": []}, {"text": "We use type - token ratio ( TTR ) , the", "entities": []}, {"text": "6583ratio between the number of word types and the number of tokens , to estimate the lexical diversity of a story ( Hardie and McEnery , 2006 ) .", "entities": []}, {"text": "Figure 3 shows signi\ufb01cant ( p < .001 , paired t - test ) positive shifts of TTR for both AREL and GLAC , which con\ufb01rms the \ufb01ndings in Hsu et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Figure 3 also indicates that GLAC generates stories with higher lexical diversity than that of AREL .", "entities": []}, {"text": "Figure 3 : KDE plot of type - token ratio ( TTR ) for pre / post - edited stories .", "entities": []}, {"text": "People increase lexical diversity in machine - generated stories for both AREL and GLAC .", "entities": []}, {"text": "Second , people shorten AREL \u2019s stories but lengthen GLAC \u2019s stories .", "entities": []}, {"text": "We calculate the average number of Part - Of - Speech ( POS ) tags for tokens in each story using the python NLTK ( Bird et al . , 2009 ) package , as shown in Table 1 .", "entities": [[6, 9, "DatasetName", "Part - Of"]]}, {"text": "We also \ufb01nd that the average number of tokens in an AREL story ( 43.0 , SD=5.0 ) decreases ( 41.9 , SD=5.6 ) after human editing , while that of GLAC ( 35.0 , SD=4.5 ) increases ( 36.7 , SD=5.9 ) .", "entities": []}, {"text": "Hsu has observed that people often replace \u201c determiner / article + noun \u201d phrases ( e.g. , \u201c a boy \u201d ) with pronouns ( e.g. , \u201c he \u201d ) in AREL stories ( 2019 ) .", "entities": []}, {"text": "However , this observation can not explain the story lengthening in GLAC , where each story on average has an increased 0.9 nouns after editing .", "entities": []}, {"text": "Given the average per - story edit distances ( Levenshtein , 1966 ; Damerau , 1964 ) for AREL ( 16.84 , SD=5.64 ) and GLAC ( 17.99 , SD=5.56 ) are similar , this difference is unlikely to be caused by deviation in editing amount .", "entities": []}, {"text": "AREL .", "entities": []}, {"text": "ADJ ADP ADV CONJ DET NOUN PRON PRT", "entities": [[4, 5, "DatasetName", "DET"]]}, {"text": "VERB Total Pre 5.2 3.1 3.5 1.9 0.5 8.1 10.1 2.1 1.6 6.9 43.0 Post 4.7 3.1 3.4 1.9 0.8 7.1 9.9 2.3 1.6 7.0 41.9 \u0001 -0.5 0.0 -0.1 -0.1 0.4 -1.0 -0.2 0.2 0.0 0.1 -1.2 GLAC .", "entities": []}, {"text": "ADJ ADP ADV CONJ DET NOUN", "entities": [[4, 5, "DatasetName", "DET"]]}, {"text": "PRON PRT VERB Total Pre 5.0 3.3 1.7 1.9 0.2 6.5 7.4 1.2 0.8 6.9 35.0 Post 4.5 3.2 2.4 1.8 0.8 6.1 8.3 1.5 1.0 7.0 36.7 \u0001 -0.5", "entities": []}, {"text": "-0.1", "entities": []}, {"text": "0.7 -0.1 0.6 -0.3 0.9 0.3 0.2 0.1 1.7 Table 1 : Average number of tokens with each POS tag per story .", "entities": []}, {"text": "( \u0001 : the differences between post- and preedit stories .", "entities": []}, {"text": "NUM is omitted because it is nearly 0 .", "entities": [[7, 8, "DatasetName", "0"]]}, {"text": "Numbers are rounded to one decimal place . )", "entities": []}, {"text": "Deleting extra words requires much less time than other editing operations ( Popovic et al . , 2014 ) .", "entities": []}, {"text": "Per Figure 3 , AREL \u2019s stories are muchmore repetitive .", "entities": []}, {"text": "We further analyze the type - token ratio for nouns ( TTR noun ) and \ufb01nd AREL generates duplicate nouns .", "entities": []}, {"text": "The average TTR noun of an AREL \u2019s story is 0.76 while that of GLAC is 0.90 .", "entities": []}, {"text": "For reference , the average TTR noun of a human - written story ( the entire VIST dataset ) is 0.86 .", "entities": [[16, 17, "DatasetName", "VIST"]]}, {"text": "Thus , we hypothesize workers prioritized their efforts in deleting repetitive words for AREL , resulting in the reduction of story length .", "entities": []}, {"text": "4 Baseline Experiments We report baseline experiments on the visual story post - editing task in Table 2 .", "entities": []}, {"text": "AREL \u2019s post - editing models are trained on the augmented AREL training set and evaluated on the AREL test set of VISTEdit , and GLAC \u2019s models are tested using GLAC sets , too .", "entities": []}, {"text": "Figure 4 shows examples of the output .", "entities": []}, {"text": "Human evaluations ( Table 2 ) indicate that the post - editing model improves visual story quality .", "entities": []}, {"text": "4.1 Methods Two neural approaches , Long short - term memory ( LSTM ) and Transformer , are used as baselines , where we experiment using ( i)text only ( T ) and ( ii)both text and images ( T+I ) as inputs .", "entities": [[6, 11, "MethodName", "Long short - term memory"], [12, 13, "MethodName", "LSTM"], [15, 16, "MethodName", "Transformer"]]}, {"text": "LSTM", "entities": [[0, 1, "MethodName", "LSTM"]]}, {"text": "An LSTM seq2seq model is used ( Sutskever et al . , 2014 ) .", "entities": [[1, 2, "MethodName", "LSTM"], [2, 3, "MethodName", "seq2seq"]]}, {"text": "For the text - only setting , the original stories and the human - edited stories are treated as source - target pairs .", "entities": []}, {"text": "For the text - image setting , we \ufb01rst extract the image features using the pre - trained ResNet-152 model ( He et al . , 2016 ) and represent each image as a 2048 - dimensional vector .", "entities": [[34, 35, "DatasetName", "2048"]]}, {"text": "We then apply a dense layer on image features in order to both \ufb01t its dimension to the word embedding and learn the adjusting transformation .", "entities": []}, {"text": "By placing the image features in front of the sequence of text embedding , the input sequence becomes a matrix2R(5+len)\u0002dim , wherelen is the text sequence length , 5means 5 photos , and dim is the dimension of the word embedding .", "entities": []}, {"text": "The input sequence with both image information and text information is then encoded by LSTM , identical as in the text - only setting .", "entities": [[14, 15, "MethodName", "LSTM"]]}, {"text": "Transformer ( TF ) We also use the Transformer architecture ( Vaswani et al . , 2017 ) as baseline .", "entities": [[0, 1, "MethodName", "Transformer"], [8, 9, "MethodName", "Transformer"]]}, {"text": "The text - only setup and image feature extraction are identical to that of LSTM .", "entities": [[14, 15, "MethodName", "LSTM"]]}, {"text": "For Transformer , the image features are attached at the end of the sequence of text embedding to form an image-", "entities": [[1, 2, "MethodName", "Transformer"]]}, {"text": "6584AREL GLAC Edited By Focus Coherence Share Human Grounded Detailed Focus Coherence Share Human Grounded Detailed N / A 3.487 3.751 3.763 3.746 3.602 3.761 3.878 3.908 3.930 3.817 3.864 3.938 TF ( T ) 3.433 3.705 3.641 3.656 3.619 3.631 3.717 3.773 3.863 3.672 3.765 3.795 TF ( T+I ) 3.542 3.693 3.676 3.643 3.548 3.672 3.734 3.759 3.786 3.622 3.758 3.744 LSTM ( T ) 3.551 3.800 3.771 3.751 3.631 3.810 3.894 3.896 3.864 3.848 3.751 3.897 LSTM ( T+I ) 3.497 3.734 3.746 3.742 3.573 3.755 3.815 3.872 3.847 3.813 3.750 3.869 Human 3.592 3.870 3.856 3.885 3.779 3.878 4.003 4.057 4.072 3.976 3.994 4.068 Table 2 : Human evaluation results .", "entities": [[63, 64, "MethodName", "LSTM"], [79, 80, "MethodName", "LSTM"]]}, {"text": "Five human judges on MTurk rate each story on the following six aspects , using a 5 - point Likert scale ( from Strongly Disagree to Strongly Agree ): Focus , Structure and Coherence , Willingto - Share ( \u201c I Would Share \u201d ) , Written - by - a - Human ( \u201c This story sounds like it was written by a human . \u201d ) , VisuallyGrounded , and Detailed .", "entities": []}, {"text": "We take the average of the \ufb01ve judgments as the \ufb01nal score for each story .", "entities": []}, {"text": "LSTM(T ) improves all aspects for stories by AREL , and improves \u201c Focus \u201d and \u201c Human - like \u201d aspects for stories by GLAC .", "entities": []}, {"text": "enriched embedding .", "entities": []}, {"text": "It is noteworthy that the position encoding is only applied on text embedding .", "entities": []}, {"text": "The input matrix2R(len+5)\u0002dimis then passed into the Transformer as in the text - only setting .", "entities": [[7, 8, "MethodName", "Transformer"]]}, {"text": "4.2 Experimental Setup and Evaluation Data Augmentation In order to obtain suf\ufb01cient training samples for neural models , we pair lessedited stories with more -edited stories of the same photo sequence to augment the data .", "entities": [[5, 7, "TaskName", "Data Augmentation"]]}, {"text": "In VIST - Edit , \ufb01ve human - edited stories are collected for each photo sequence .", "entities": [[1, 4, "DatasetName", "VIST - Edit"]]}, {"text": "We use the human - edited stories that are less edited \u2013 measured by its Normalized Damerau - Levenshtein distance ( Levenshtein , 1966 ; Damerau , 1964 ) to the original story \u2013 as the source and pair them with the stories that are more edited ( as the target . )", "entities": []}, {"text": "This data augmentation strategy gives us in total \ufb01fteen ( \u00005 2\u0001 +5 = 15 ) training samples given \ufb01ve human - edited stories .", "entities": [[1, 3, "TaskName", "data augmentation"]]}, {"text": "Human Evaluation Following the evaluation procedure of the \ufb01rst VIST Challenge ( Mitchell et al . , 2018 ) , for each visual story , we recruit \ufb01ve human judges on MTurk to rate it on six aspects ( at $ 0.1 / HIT . )", "entities": [[9, 10, "DatasetName", "VIST"]]}, {"text": "We take the average of the \ufb01ve judgments as the \ufb01nal scores for the story .", "entities": []}, {"text": "Table 2 shows the results .", "entities": []}, {"text": "The LSTM using text - only input outperforms all other baselines .", "entities": [[1, 2, "MethodName", "LSTM"]]}, {"text": "It improves all six aspects for stories by AREL , and improves \u201c Focus \u201d and \u201c Human - like \u201d aspects for stories by GLAC .", "entities": []}, {"text": "These results demonstrate that a relatively small set of human edits can be used to boost the story quality of an existing large VIST model .", "entities": [[23, 24, "DatasetName", "VIST"]]}, {"text": "Table 2 also suggests that the quality of a post - edited story is heavily decided by its pre - edited version .", "entities": []}, {"text": "Even after editing by human editors , AREL \u2019s stories still do not achieve the quality of pre - edited stories by GLAC .", "entities": []}, {"text": "The inef\ufb01cacy of image features and Transformer model might be caused by the small size of VIST - Edit .", "entities": [[6, 7, "MethodName", "Transformer"], [16, 19, "DatasetName", "VIST - Edit"]]}, {"text": "It also requires furtherresearch to develop a post - editing model in a multimodal context .", "entities": []}, {"text": "5 Discussion Automatic evaluation scores do not re\ufb02ect the quality improvements .", "entities": []}, {"text": "APE for MT has been using automatic metrics , such as BLEU , to benchmark progress ( Libovick ` y et", "entities": [[0, 1, "DatasetName", "APE"], [11, 12, "MetricName", "BLEU"]]}, {"text": "al . , 2016 ) .", "entities": []}, {"text": "However , classic automatic evaluation metrics fail to capture the signal in human judgments for the proposed visual story post - editing task .", "entities": []}, {"text": "We \ufb01rst use the human - edited stories as references , but all the automatic evaluation metrics generate lower scores when human judges give a higher rating ( Table 3 . )", "entities": []}, {"text": "Reference : AREL Stories Edited by Human BLEU4 METEOR ROUGE Skip - ThoughtsHuman Rating AREL 0.93 0.91 0.92 0.97 3.69 AREL Edited By LSTM(T)0.21 0.46 0.40 0.76 3.81 Table 3 : Average evaluation scores for AREL stories , using the human - edited stories as references .", "entities": [[8, 9, "DatasetName", "METEOR"]]}, {"text": "All the automatic evaluation metrics generate lower scores when human judges give a higher rating .", "entities": []}, {"text": "We then switch to use the human - written stories ( VIST test set ) as references , but again , all the automatic evaluation metrics generate lower scores even when the editing was done by human ( Table 4 . )", "entities": [[11, 12, "DatasetName", "VIST"]]}, {"text": "Reference : Human- Written Stories BLEU4 METEOR ROUGE Skip - Thoughts GLAC 0.03 0.30 0.26 0.66 GLAC Edited By Human0.02 0.28 0.24 0.65 Table 4 : Average evaluation scores on GLAC stories , using human - written stories as references .", "entities": [[6, 7, "DatasetName", "METEOR"]]}, {"text": "All the automatic evaluation metrics generate lower scores even when the editing was done by human .", "entities": []}, {"text": "6585 Figure 4 : Example stories generated by baselines .", "entities": []}, {"text": "Spearman rank - order correlation \u001a Data Includes BLEU4 METEOR ROUGE Skip - Thoughts \u00acAREL .110 .099 .063 .062 \u00adLSTM - Edited AREL .106 .109 .067 .205 \u00ae \u00ac", "entities": [[9, 10, "DatasetName", "METEOR"]]}, {"text": "+ \u00ad .095 .092 .059 .116 \u00afGLAC .222 .203 .140 .151 \u00b0 LSTM - Edited GLAC .163 .176 .138 .087 \u00b1 \u00af", "entities": [[12, 13, "MethodName", "LSTM"]]}, {"text": "+ \u00b0 .196 .194 .148 .116 \u00b2 \u00ac", "entities": []}, {"text": "+ \u00af .091 .086 .059 .088 \u00b3 \u00ad + \u00b0 .089 .103 .067 .101 \u00b4 \u00ac", "entities": []}, {"text": "+ \u00ad+\u00af+ \u00b0 .090 .096 .069 .094 Table 5 : Spearman rank - order correlation \u001abetween the automatic evaluation scores ( sum of all six aspects ) and human judgment .", "entities": []}, {"text": "When comparing among machine - edited stories ( \u00adand \u00b0 ) , among pre- and post - edited stories ( \u00ae and\u00b1 ) , or among any combinations of them ( \u00b2,\u00b3and \u00b4 ) , all metrics result in weak correlations with human judgments .", "entities": []}, {"text": "Table 5 further shows the Spearman rank - order correlation\u001abetween the automatic evaluation scores ( sum of all six aspects ) and human judgment calculated using different data combination .", "entities": []}, {"text": "In row\u00afof Table 5 , the reported correlation \u001aof METEOR is consistent with the \ufb01ndings in Huang et al .", "entities": [[9, 10, "DatasetName", "METEOR"]]}, {"text": "( 2016 ) , which suggests that METEOR could be useful when comparing among stories generated by the same visual storytelling model .", "entities": [[7, 8, "DatasetName", "METEOR"], [19, 21, "TaskName", "visual storytelling"]]}, {"text": "However , when comparing among machine - edited stories ( row \u00adand \u00b0 ) , among pre- and post - edited stories ( row \u00ae and\u00b1 ) , or among any combinations of them ( row \u00b2,\u00b3and \u00b4 ) , all metrics result in weak correlations with human judgments .", "entities": []}, {"text": "These results strongly suggest the need of a new automatic evaluation metric for visual story postediting task .", "entities": []}, {"text": "Some new metrics have recently been introduced using linguistic ( Roemmele and Gor - don , 2018a ) or story features ( Purdy et al . , 2018 ) to evaluate story automatically .", "entities": []}, {"text": "More research is needed to examine whether these metrics are useful for story post - editing tasks too .", "entities": []}, {"text": "6 Conclusion VIST - Edit , the \ufb01rst dataset for human edits of machine - generated visual stories , is introduced .", "entities": [[2, 5, "DatasetName", "VIST - Edit"]]}, {"text": "We argue that human editing on machinegenerated stories is unavoidable , and such edited data can be leveraged to enable automatic postediting .", "entities": []}, {"text": "We have established baselines for the task of visual story post - editing , and have motivated the need for a new automatic evaluation metric .", "entities": []}, {"text": "References Ram \u00b4 on Astudillo ,", "entities": []}, {"text": "Jo \u02dcao", "entities": []}, {"text": "Grac \u00b8a , and Andr \u00b4 e Martins .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Proceedings of the amta 2018 workshop on translation quality estimation and automatic postediting .", "entities": []}, {"text": "In Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post - Editing .", "entities": [[8, 9, "TaskName", "Translation"], [12, 16, "TaskName", "Automatic Post - Editing"]]}, {"text": "Hanna B \u00b4 echara , Yanjun Ma , and Josef van Genabith .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Statistical post - editing for a statistical mt system .", "entities": []}, {"text": "In MT Summit , volume 13 .", "entities": []}, {"text": "Steven Bird , Ewan Klein , and Edward Loper .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Natural language processing with Python : analyzing text with the natural language toolkit .", "entities": []}, {"text": "O\u2019Reilly Media , Inc.", "entities": []}, {"text": "Fred J Damerau .", "entities": []}, {"text": "1964 .", "entities": []}, {"text": "A technique for computer detection and correction of spelling errors .", "entities": []}, {"text": "Communications of the ACM , 7(3):171\u2013176 . C. Gan , Z. Gan , X. He , J. Gao , and L. Deng . 2017 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Stylenet : Generating attractive visual captions with", "entities": []}, {"text": "6586styles .", "entities": []}, {"text": "In 2017 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 955 \u2013 964 .", "entities": []}, {"text": "Andrew Hardie and Tony McEnery .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Statistics .", "entities": []}, {"text": ", volume 12 , pages 138\u2013146 .", "entities": []}, {"text": "Elsevier .", "entities": []}, {"text": "Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Deep residual learning for image recognition .", "entities": [[4, 6, "TaskName", "image recognition"]]}, {"text": "In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770 \u2013 778 .", "entities": []}, {"text": "Ting - Yao Hsu , Yen - Chia Hsu , and Ting - Hao K. Huang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "On how users edit computer - generated visual stories .", "entities": []}, {"text": "In Proceedings of the 2019 CHI Conference Extended Abstracts ( Late - Breaking - Work ) on Human Factors in Computing Systems . ACM .", "entities": [[23, 24, "DatasetName", "ACM"]]}, {"text": "Ting - Hao Kenneth Huang , Francis Ferraro , Nasrin Mostafazadeh , Ishan Misra , Aishwarya Agrawal , Jacob Devlin , Ross Girshick , Xiaodong He , Pushmeet Kohli , Dhruv Batra , et al . 2016 .", "entities": []}, {"text": "Visual storytelling .", "entities": [[0, 2, "TaskName", "Visual storytelling"]]}, {"text": "InProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1233\u20131239 .", "entities": []}, {"text": "Marcin Junczys - Dowmunt and Roman Grundkiewicz . 2016 .", "entities": []}, {"text": "Log - linear combinations of monolingual and bilingual neural machine translation models for automatic post - editing .", "entities": [[9, 11, "TaskName", "machine translation"], [13, 17, "TaskName", "automatic post - editing"]]}, {"text": "arXiv preprint arXiv:1605.04800 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Marcin Junczys - Dowmunt and Roman Grundkiewicz . 2017 .", "entities": []}, {"text": "An exploration of neural sequence - tosequence architectures for automatic post - editing .", "entities": [[9, 13, "TaskName", "automatic post - editing"]]}, {"text": "arXiv preprint arXiv:1706.04138 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Taehyeong Kim , Min - Oh Heo , Seonil Son , KyoungWha Park , and Byoung - Tak Zhang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Glac net : Glocal attention cascading networks for multiimage cued story generation .", "entities": [[10, 12, "TaskName", "story generation"]]}, {"text": "arXiv preprint arXiv:1805.10973 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Stephen King .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "On writing : A memoir ofthe craft .", "entities": []}, {"text": "New Yorle : Scrihner .", "entities": []}, {"text": "Vladimir I Levenshtein .", "entities": []}, {"text": "1966 .", "entities": []}, {"text": "Binary codes capable of correcting deletions , insertions , and reversals .", "entities": []}, {"text": "In Soviet physics doklady , volume 10 , pages 707\u2013710 .", "entities": []}, {"text": "Jind\u02c7rich Libovick ` y and Jind \u02c7rich Helcl . 2017 .", "entities": []}, {"text": "Attention strategies for multi - source sequence - to - sequence learning .", "entities": []}, {"text": "arXiv preprint arXiv:1704.06567 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jind\u02c7rich Libovick ` y , Jind \u02c7rich Helcl , Marek Tlust ` y , Ond\u02c7rej Bojar , and Pavel Pecina .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Cuni system for wmt16 automatic post - editing and multimodal translation tasks .", "entities": [[3, 4, "DatasetName", "wmt16"], [4, 8, "TaskName", "automatic post - editing"]]}, {"text": "In Proceedings of the First Conference on Machine Translation : Volume 2 , Shared Task Papers , volume 2 , pages 646\u2013654.Christopher D. Manning , Mihai Surdeanu , John Bauer , Jenny Finkel , Steven J. Bethard , and David McClosky .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "2014 .", "entities": []}, {"text": "The Stanford CoreNLP natural language processing toolkit .", "entities": []}, {"text": "In Association for Computational Linguistics ( ACL ) System Demonstrations , pages 55\u201360 .", "entities": []}, {"text": "Alexander Mathews , Lexing Xie , and Xuming He .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Semstyle : Learning to generate stylised image captions using unaligned text .", "entities": []}, {"text": "In The IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) .", "entities": []}, {"text": "Margaret Mitchell , Francis Ferraro , Ishan Misra , et al . 2018 .", "entities": []}, {"text": "Proceedings of the \ufb01rst workshop on storytelling .", "entities": []}, {"text": "In Proceedings of the First Workshop on Storytelling .", "entities": []}, {"text": "Maja Popovic , Arle Lommel , Aljoscha Burchardt , Eleftherios Avramidis , and Hans Uszkoreit .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Relations between different types of post - editing operations , cognitive effort and temporal effort .", "entities": []}, {"text": "InProceedings of the 17th Annual Conference of the European Association for Machine Translation ( EAMT 14 ) , pages 191\u2013198 .", "entities": [[11, 13, "TaskName", "Machine Translation"]]}, {"text": "Christopher Purdy , Xinyu Wang , Larry He , and Mark Riedl .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Predicting generated story quality with quantitative measures .", "entities": []}, {"text": "In Fourteenth Arti\ufb01cial Intelligence and Interactive Digital Entertainment Conference .", "entities": []}, {"text": "Melissa Roemmele and Andrew Gordon .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Linguistic features of helpfulness in automated support for creative writing .", "entities": []}, {"text": "In Proceedings of the First Workshop on Storytelling , pages 14\u201319 .", "entities": []}, {"text": "Melissa Roemmele and Andrew S Gordon .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "Automated assistance for creative writing with an rnn language model .", "entities": []}, {"text": "In Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion , page 21 .", "entities": []}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Michel Simard , Cyril Goutte , and Pierre Isabelle .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Statistical phrase - based post - editing .", "entities": []}, {"text": "In Proceedings of NAACL HLT , pages 508\u2013515 .", "entities": []}, {"text": "Ilya Sutskever , Oriol Vinyals , and Quoc V Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "In Advances in neural information processing systems , pages 3104\u20133112 .", "entities": []}, {"text": "Mark Twain .", "entities": []}, {"text": "1876 .", "entities": []}, {"text": "The Adventures of Tom Sawyer .", "entities": []}, {"text": "American Publishing Company .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 5998\u20136008 .", "entities": []}, {"text": "Xin Wang , Wenhu Chen , Yuan - Fang Wang , and William Yang Wang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "No metrics are perfect : Adversarial reward learning for visual storytelling .", "entities": [[9, 11, "TaskName", "visual storytelling"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , Melbourne , Victoria , Australia . ACL .", "entities": []}]