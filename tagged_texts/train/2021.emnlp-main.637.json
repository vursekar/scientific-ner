[{"text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 8078\u20138088 November 7\u201311 , 2021 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2021 Association for Computational Linguistics8078Honey or Poison ?", "entities": []}, {"text": "Solving the Trigger Curse in Few - shot Event Detection via Causal Intervention Jiawei Chen1;3 , Hongyu Lin1;\u0003 , Xianpei Han1;2;\u0003\u0003 , Le Sun1;2 , 1Chinese Information Processing Laboratory2State Key Laboratory of Computer Science Institute of Software , Chinese Academy of Sciences , Beijing , China 3University of Chinese Academy of Sciences , Beijing , China { jiawei2020}@iscas.ac.cn { hongyu,xianpei,sunle}@iscas.ac.cn Abstract Event detection has long been troubled by the trigger curse : over\ufb01tting the trigger will harm the generalization ability while under\ufb01tting it will hurt the detection performance .", "entities": [[8, 10, "TaskName", "Event Detection"], [61, 63, "TaskName", "Event detection"]]}, {"text": "This problem is even more severe in few - shot scenario .", "entities": []}, {"text": "In this paper , we identify and solve the trigger curse problem in few - shot event detection ( FSED ) from a causal view .", "entities": [[16, 18, "TaskName", "event detection"]]}, {"text": "By formulating FSED with a structural causal model ( SCM ) , we found that the trigger is a confounder of the context and the result , which makes previous FSED methods much easier to over\ufb01t triggers .", "entities": []}, {"text": "To resolve this problem , we propose to intervene on the context via backdoor adjustment during training .", "entities": []}, {"text": "Experiments show that our method signi\ufb01cantly improves the FSED on ACE05 , MA VEN and KBP17 datasets .", "entities": []}, {"text": "1 Introduction Event detection ( ED ) aims to identify and classify event triggers in a sentence , e.g. , detecting an Attack event triggered by \ufb01rein \u201c They killed by hostile \ufb01re in Iraqi \u201d .", "entities": [[2, 4, "TaskName", "Event detection"]]}, {"text": "Recently , supervised ED approaches have achieved promising performance ( Chen et al . , 2015 ; Nguyen and Grishman , 2015 ;", "entities": []}, {"text": "Nguyen et al . , 2016 ; Lin et al . , 2018 , 2019b , a ; Du and Cardie , 2020 ; Liu et al . , 2020a ; Lu et", "entities": []}, {"text": "al . , 2021 ) , but when adapting to new event types and domains , a large number of manually annotated event data is required which is expensive .", "entities": []}, {"text": "By contrast , fewshot event detection ( FSED ) aims to build effective event detectors that are able to detect new events from instances ( query ) with a few labeled instances ( support set ) .", "entities": [[4, 6, "TaskName", "event detection"]]}, {"text": "Due to their ability to classify novel types , many few - shot algorithms have been used in FSED , e.g. , metric - based methods like Prototypical Network ( Lai et al . , 2020 ; Deng et al . , 2020 ;", "entities": [[37, 40, "DatasetName", "Deng et al"]]}, {"text": "Cong et al . , 2021 ) .", "entities": []}, {"text": "Unfortunately , there has long been a \u201c trigger curse \u201d which troubles the learning of event detec\u0003Corresponding authors .", "entities": []}, {"text": "AnAttack event in Iraqi.\ufb01re They were killed by hostile [ MASK ] in Iraqi .", "entities": []}, {"text": "They were killed by hostile \ufb01rein Iraqi.1 or 0QueryET C S YE CT SYQ ( a ) Structural Causal Model for the data distribution of FSED AnAttack event in Iraqi.forces , \ufb01re , attack , ...", "entities": []}, {"text": "They were killed by hostile [ MASK ] in Iraqi .", "entities": []}, {"text": "They were killed by hostile [ T]in Iraqi.1 or 0Query E T C S YE CT SYQ ( b ) The data distribution of FSED after causal intervention Figure 1 : Illustration of the causal intervention strategy proposed in this paper .", "entities": []}, {"text": "The graph includes the event E , the trigger set T , the context set C , the support instance S , the prediction Yand the query instance Q. tion models , especially in few - shot scenario ( Bronstein et al . , 2015 ;", "entities": []}, {"text": "Liu et al . , 2017 ; Chen et al . , 2018 ; Liu et al . , 2019 ; Ji et al . , 2019 ) .", "entities": []}, {"text": "For many event types , their triggers are dominated by several popular words , e.g. , the Attack event type is dominated by war , attack , \ufb01ght , \ufb01re , bomb in ACE05 .", "entities": []}, {"text": "And we found the top 5 triggers of each event type cover 78 % of event occurrences in ACE05 .", "entities": []}, {"text": "Due to the trigger curse , event detection models nearly degenerate to a trigger matcher , ignore the majority of contextual information and mainly rely on whether the candidate word matches the dominant triggers .", "entities": [[6, 8, "TaskName", "event detection"]]}, {"text": "This problem is more severe in FSED : since the given support instances are very sparse and lack diversity , it is much easier to over\ufb01t the trigger of the support instances .", "entities": []}, {"text": "An intuitive solution for the trigger curse is to erase the trigger information in instances and forces the model to focus more on the context .", "entities": []}, {"text": "Unfortunately , due to the decisive role of triggers , directly wiping out the trigger information commonly hurts the performance ( Lu et al . , 2019 ; Liu et al . , 2020b ) .", "entities": []}, {"text": "Some previous approaches try to tackle this problem by introducing more di-", "entities": []}, {"text": "8079versi\ufb01ed context information like event argument information ( Liu et al . , 2017 , 2019 ; Ji et al . , 2019 ) and document - level information ( Ji and Grishman , 2008 ; Liao and Grishman , 2010 ; Duan et al . , 2017 ; Chen et al . , 2018 ) .", "entities": []}, {"text": "However , rich context information is commonly not available for FSED , and therefore these methods can not be directly applied .", "entities": []}, {"text": "In this paper , we revisit the trigger curse in FSED from a causal view .", "entities": []}, {"text": "Speci\ufb01cally , we formulate the data distribution of FSED using a trigger - centric structural causal model ( SCM ) ( Pearl et al . , 2016 ) shown in Figure 1(a ) .", "entities": []}, {"text": "Such trigger - centric formulation is based on the fact that , given the event type , contexts have a much lower impact on triggers , compared with the impact of triggers on contexts .", "entities": []}, {"text": "This results in the decisive role of triggers in event extraction , and therefore conventional event extraction approaches commonly follow the triggercentric procedure ( i.e. , identifying triggers \ufb01rst and then using triggers as an indicator to \ufb01nd arguments in contexts ) .", "entities": [[9, 11, "TaskName", "event extraction"], [15, 17, "TaskName", "event extraction"]]}, {"text": "Furthermore , the case grammar theory in linguistics ( Fillmore , 1967 ) also formulate the language using such trigger / predicate - centric assumption , and have been widely exploited in many NLP tasks like semantic role labeling ( Gildea and Jurafsky , 2002 ) and abstract meaning representation ( Banarescu et al . , 2013 ) .", "entities": [[36, 39, "TaskName", "semantic role labeling"]]}, {"text": "From the SCM , we found that T(trigger set ) is a confounder of the C(context set ) and the Y(result ) , and therefore there exists a backdoor pathC T!Y.", "entities": []}, {"text": "The backdoor path explains why previous FSED models disregard contextual information : it misleads the conventional learning procedure to mistakenly regard effects of triggers as the effects of contexts .", "entities": []}, {"text": "Consequently , the learning criteria of conventional FSED methods are optimized towards spurious correlation , rather than capturing causality between CandY. To address this issue , we propose to intervene on context to block the information from trigger to context .", "entities": []}, {"text": "Speci\ufb01cally , we apply backdoor adjustment to estimate the interventional distribution that is used for optimizing causality .", "entities": []}, {"text": "Furthermore , because backdoor adjustment relies on the unknown prior confounder ( trigger ) distribution , we also propose to estimate it based on contextualized word prediction .", "entities": []}, {"text": "We conducted experiments on ACE051 , MA VEN2and KBP173datasets .", "entities": []}, {"text": "Experiments 1https://catalog.ldc.upenn.edu/LDC2006T06 2https://github.com/THU-KEG/MA VEN - dataset 3https://tac.nist.gov/2017/KBP/data.htmlshow that causal intervention can signi\ufb01cantly alleviate trigger curse , and therefore the proposed method signi\ufb01cantly outperforms previous FSED methods .", "entities": []}, {"text": "2 Structural Causal Model for FSED", "entities": []}, {"text": "This section describes the structural causal model ( SCM ) for FSED , illustrated in Figure 1(a ) .", "entities": []}, {"text": "Note that , we omit the causal structure of the query for simplicity since it is the same as the support set .", "entities": []}, {"text": "Concretely , the SCM formulates the data distribution of FSED : 1 ) Starting from an event Ewe want to describe ( in Figure 1(a ) is an Attack in Iraqi ) .", "entities": []}, {"text": "2 ) The path E!Tindicates the trigger decision process , i.e. , selecting words or phrases ( in Figure 1(a ) is \ufb01re ) which can almost clearly express the event occurrence ( Doddington et al . , 2004 ) .", "entities": []}, {"text": "3 ) The pathE!C Tindicates that a set of contexts are generated depending on both the event and the trigger , which provides background information and organizes this information depending on the trigger .", "entities": []}, {"text": "For instance , the context \u201c They killed by hostile [ \ufb01re ] in Iraqi \u201d provides the place , the role and the consequences of the event , and this information is organized following the structure determined by \ufb01re .", "entities": []}, {"text": "4 ) an event instance is generated by combining one of the contexts in Cand one of the triggers in Tvia the path C!S", "entities": []}, {"text": "T. 5 ) Finally , a matching between query and support set is generated through S!Y Q. Conventional learning criteria for FSED directly optimize towards the conditional distribution P(YjS;Q ) .", "entities": []}, {"text": "However , from the SCM , we found that the backdoor path C T!Ypass on associations ( Pearl et al . , 2016 ) and mislead the learning with spurious correlation .", "entities": []}, {"text": "Consequently , the learning procedure towards P(YjS;Q)will mistakenly regard the effects of triggers as the effects of contexts , and therefore over\ufb01t the trigger information .", "entities": []}, {"text": "3 Causal Intervention for Trigger Curse Based on the SCM , this section describes how to resolve the trigger curse via causal intervention .", "entities": []}, {"text": "Context Intervention .", "entities": []}, {"text": "To block the backdoor path , we intervene on the context Cand the new context - intervened SCM is shown in Figure 1(b ) .", "entities": []}, {"text": "Given support set s , event seteofs , context setC ofsand query instance q , we optimize the interventional distribution P(Yjdo(C = C);E = e;Q= q)rather thanP(YjS = s;Q = q ) , wheredo(\u0001 ) denotes causal intervention operation .", "entities": []}, {"text": "By interven-", "entities": []}, {"text": "8080ing , the learning objective of models changes from optimizing correlation to optimizing causality .", "entities": []}, {"text": "Backdoor Adjustment .", "entities": []}, {"text": "Backdoor adjustment is used to estimate the interventional distribution4 : P(Yjdo(C = C);E = e;Q = q ) = X t2TX s2SP(Yjs;q)P(sjC;t)P(tje);(1 ) whereP(sjC;t)denotes the generation of sfrom the trigger and contexts .", "entities": []}, {"text": "P(sjC;t )", "entities": []}, {"text": "= 1 = jCjif and only if the context of sinCand the trigger of sis t. P(Yjs;q)/ \u001e ( s;q;\u0012)is the matching model betweenqandsparametrized by \u0012.", "entities": []}, {"text": "Estimating P(tje)via Contextualized Prediction .", "entities": []}, {"text": "The confounder distribution P(tje)is unknown because Eis a hidden variable .", "entities": []}, {"text": "Since the event argument information is contained in C , we argue thatP(tje)/M(tjC)whereM(\u0001jC)indicates a masked token prediction task ( Taylor , 1953 ) which is constructed by masking triggers in the support set .", "entities": []}, {"text": "In this paper , we use masked language model to calculate P(tje)by \ufb01rst generating a set of candidate triggers through the context : Tc = ftiji= 1;2;:::g[ft0 g , wheretiis the i - th predicted token and t0is the original trigger of the support set instance , then P(tje)is estimated by averaging logit obtained from the MLM : P(tije )", "entities": [[56, 57, "DatasetName", "MLM"]]}, {"text": "= 8 > < > : \u0015 i = 0 ( 1\u0000\u0015)exp(li)P jexp(lj)i6= 0(2 ) whereliis the logit for the ithtoken .", "entities": [[9, 10, "DatasetName", "0"]]}, {"text": "To reduce the noise introduced by MLM , we assign an additional hyperparameter \u00152(0;1)tot0 .", "entities": [[6, 7, "DatasetName", "MLM"]]}, {"text": "Optimizing via Representation Learning .", "entities": [[2, 4, "TaskName", "Representation Learning"]]}, {"text": "Given the interventional distribution , FSED model can be learned by minimizing the loss function on it : L(\u0012 ) = \u0000X q2Qf(P(Yjdo(C);e;q;\u0012 ) )", "entities": [[13, 14, "MetricName", "loss"]]}, {"text": "= \u0000X q2Qf(X t2TX s2SP(Yjs;q;\u0012)P(sjC;t)P(tje ) )", "entities": []}, {"text": "( 3 ) whereQis training queries and fis a strict monotonically increasing function .", "entities": []}, {"text": "However , the optimization ofL(\u0012)needs to calculate every P(Yjs;q;\u0012 ) , which is quite time - consuming .", "entities": []}, {"text": "To this end , we propose a surrogate learning criteria LSG(\u0012)to optimize the causal relation based on representation learning : 4The proof is shown in AppendixLSG(\u0012 )", "entities": [[17, 19, "TaskName", "representation learning"]]}, {"text": "= \u0000X q2Qg(R(q;\u0012 ) ; X t2TX s2SP(sjC;t)P(tje)R(s;\u0012))(4 )", "entities": []}, {"text": "HereRis a representation model which inputs s orqand outputs a dense representation .", "entities": []}, {"text": "g(\u0001;\u0001)is a distance metric measuring the similarity between two representations .", "entities": [[2, 4, "HyperparameterName", "distance metric"]]}, {"text": "Such loss function is widely used in many metric - based methods ( e.g. , Prototypical Networks and Relation Networks ) .", "entities": [[1, 2, "MetricName", "loss"]]}, {"text": "In the Appendix , we prove LSG(\u0012)is equivalent toL(\u0012 ) .", "entities": []}, {"text": "4 Experiments 4.1 Experimental Settings Datasets.5We conducted experiments on ACE05 , MA VEN ( Wang et al . , 2020c ) and KBP17 datasets .", "entities": []}, {"text": "We split train / dev / test sets according to event types and we use event types with more instances for training , the other for dev / test .", "entities": []}, {"text": "To conduct 5 - shot experiments , we \ufb01lter event types less than 6 instances .", "entities": []}, {"text": "Finally , for ACE05 , its train / dev / test set contains 3598/140/149 instances and 20/10/10 types respectively , for MA VEN , those are 34651/1494/1505 instances and 120/45/45 types , for KBP17 , those are 15785/768/792 instances and 25/13/13 types .", "entities": []}, {"text": "Task Settings .", "entities": []}, {"text": "Different from episode evaluation in Lai et al .", "entities": []}, {"text": "( 2020 ) and Cong et", "entities": []}, {"text": "al . ( 2021 ) , we employ a more practical event detection setting inspired by Yang and Katiyar ( 2020 ) in Few - shot NER .", "entities": [[11, 13, "TaskName", "event detection"], [23, 27, "TaskName", "Few - shot NER"]]}, {"text": "We randomly sample few instances as support set and all other instances in the test set are used as queries .", "entities": []}, {"text": "A support set corresponds to an event type and all types will be evaluated by traversing each event type .", "entities": []}, {"text": "Models need to detection the span and type of triggers in a sentence .", "entities": []}, {"text": "We also compared the results across settings in Section 4.3 .", "entities": []}, {"text": "We evaluate all methods using macro - F1 and microF1 scores , and micro - F1 is taken as the primary measure .", "entities": [[5, 8, "MetricName", "macro - F1"], [13, 16, "MetricName", "micro - F1"]]}, {"text": "Baselines .", "entities": []}, {"text": "We conduct experiments on two metric - based methods : Prototypical Network ( Snell et al . , 2017 ) and Relation Network ( Sung et al . , 2018 ) , which are referred as FS - Base .", "entities": []}, {"text": "Based on these models , we compare our causal intervention method ( FS - Casual ) with 1 ) FS - LexFree ( Lu et al . , 2019 ) , which address over\ufb01t triggers via adversarial learning , we use their lexical - free encoder ; 5Our source codes are openly available at https://github.com/chen700564/causalFSED", "entities": []}, {"text": "8081ModelACE05 MA VEN KBP17 Macro Micro Macro Micro Macro Micro Finetuing - basedFinetune 51.0\u00061.4 58.2\u00061.6 30.7\u00061.5 31.6\u00062.3 59.4\u00061.9 62.7\u00061.8 Finetune * 39.9\u00061.1 45.5\u00060.7 20.8\u00061.0 20.6\u00060.8 45.0\u00060.7 47.3\u00060.6 Pretrain+Finetune 22.9\u00066.0 20.3\u00064.3 20.9\u00064.6 16.9\u00065.2 35.1\u00065.9 30.1\u00065.5 Pretrain+Finetune * 14.6\u00063.3 15.6\u00063.4 12.5\u00063.8 14.9\u00064.0 23.4\u00066.8 25.8\u00066.3", "entities": []}, {"text": "Prototypical NetFS - Base 63.8\u00062.8 67.3\u00062.7 44.7\u00061.4 44.5\u00062.0 65.5\u00062.7 67.3\u00063.1 FS - LexFree 52.7\u00062.9 53.9\u00063.2 25.6\u00061.0 21.8\u00061.4 60.7\u00062.5 61.4\u00062.8 FS - ClusterLoss 64.9\u00061.5 69.4\u00062.0 44.2\u00061.2 44.0\u00061.2 65.5\u00062.3 67.1\u00062.4 FS - Causal ( Ours ) 73.0\u00062.2 76.9\u00061.4 52.1\u00060.2 55.0\u00060.4 70.9\u00060.6 73.2\u00060.9 Relation NetFS - Base 65.7\u00063.7 68.7\u00064.5 52.4\u00061.4 56.0\u00061.4 67.2\u00061.5 71.2\u00061.4 FS - LexFree 59.3\u00063.5 60.1\u00063.9", "entities": []}, {"text": "43.8\u00061.9 45.9\u00062.4 61.9\u00062.4 65.4\u00062.8 FS - ClusterLoss 57.6\u00062.3 60.2\u00063.2 46.3\u00061.1 51.8\u00061.4 56.8\u00063.0 62.1\u00062.5 FS - Causal ( Ours ) 67.2\u00061.4 71.8\u00061.9 53.0\u00060.5 57.0\u00060.9 66.4\u00060.4 72.0\u00060.6 Table 1 : F1 score of 5 - shot FSED on test set .", "entities": [[28, 30, "MetricName", "F1 score"]]}, {"text": "* means \ufb01xing the parameters of encoder when \ufb01netuning .", "entities": []}, {"text": "\u0006is the standard deviation of 5 random training rounds .", "entities": []}, {"text": "Episode Ambiguity Ours556065707580FS - Base FS - Cluster FS - Causal Figure 2 : Micro F1 of prototypical network with different settings on ACE05 test set .", "entities": [[14, 16, "MetricName", "Micro F1"]]}, {"text": "2)FS - ClusterLoss ( Lai et al . , 2020 ) , which add two auxiliary loss functions when training .", "entities": [[16, 17, "MetricName", "loss"]]}, {"text": "Furthermore , we compare our method with models \ufb01netuned with support set ( Finetune ) and pretrained using the training set ( Pretrain ) .BERT", "entities": []}, {"text": "base(uncased ) is used as the encoder for all models and MLM for trigger collection .", "entities": [[11, 12, "DatasetName", "MLM"]]}, {"text": "4.2 Experimental Results The performance of our method and all baselines is shown in Table 1 .", "entities": []}, {"text": "We can see that : 1)By intervening on the context in SCM and using backdoor adjustment during training , our method can effectively learn FSED models .", "entities": []}, {"text": "Compared with the original metric - based models , our method achieves 8.7 % and 1.6 % microF1 ( average ) improvement in prototypical network and relation network respectively .", "entities": []}, {"text": "2)The causal theory is a promising technique for resolving the trigger cruse problem .", "entities": []}, {"text": "Notice that FS - LexFree can not achieve the competitive performance with the original FS models , which indicates that trigger information is import and under\ufb01tting triggers will hurt the detection performance .", "entities": []}, {"text": "This veri\ufb01es that trigger curse is very challenging and causal intervention can effectively resolve it.3)Our method can achieve state - of - the - art FSED performance .", "entities": []}, {"text": "Compared with best score in baselines , our method gains 7.5 % , 1.0 % , and 2.0 % micro - F1 improvements on ACE05 , MA VEN and KBP17 datasets respectively .", "entities": [[19, 22, "MetricName", "micro - F1"]]}, {"text": "4.3 Effect on Different Settings To further demonstrate the effectiveness of the proposed method , we also conduct experiments under different FSED settings : 1 ) The primal episodebased settings ( Episode ) , which is the 5 + 1 - way 5 - shot settings in Lai et al . ( 2020 ) .", "entities": []}, {"text": "2 ) Episode + ambiguous instances ( Ambiguity ) , which samples some additional negative query instances that include words same as triggers in support set to verify whether models over\ufb01t the triggers .", "entities": []}, {"text": "The performance of different models with different settings is shown in Figure 2 .", "entities": []}, {"text": "We can see that : 1 ) Generally speaking , all models can achieve better performance on Episode because correctly recognize high - frequent triggers can achieve good performance in this setting .", "entities": []}, {"text": "Consequently , the performance under this setting can not well represent how FSED is in\ufb02uenced by trigger over\ufb01tting .", "entities": []}, {"text": "2 ) The performance of all models dropped on Ambiguity setting , which suggests that trigger over\ufb01tting has a signi\ufb01cant impact on FSED .", "entities": []}, {"text": "3 ) Our method still maintains good performance on Ambiguity , which indicates that our method can alleviate the trigger curse problem by optimizing towards the underlying causality .", "entities": []}, {"text": "4.4 Case Study We select ambiguous cases ( in Table 2 ) to better illustrate the effectiveness of our method .", "entities": []}, {"text": "For Query 1 , FS - Base wrongly detects the word runto be a trigger word .", "entities": []}, {"text": "In Support set 1 , runmeans nomi-", "entities": []}, {"text": "8082Support set 1", "entities": []}, {"text": "I mean , I \u2019d like to - \u2013 I \u2019d like to see the Greens", "entities": []}, {"text": "[ Nominate]run[/Nominate ]", "entities": []}, {"text": "David Cobb again .", "entities": []}, {"text": "Query 1 Release a known terrorist to run the PLO and that will bring about peace FS - Base Release a known terrorist to [ Nominate]run[/Nominate ] the PLO and that will bring about peace FS - Causal Release a known terrorist to run the PLO and that will bring about peace Support set 2", "entities": []}, {"text": "They were [ Suspicion]suspected[/Suspicion ] of having facilitated the suicide bomber .", "entities": []}, {"text": "Query 2", "entities": []}, {"text": "A fourth suspect , Osman Hussein , was arrested in Rome , Italy , and later extradited to the UK .", "entities": []}, {"text": "FS - Base A fourth [ Suspicion]suspect[/Suspicion ] , Osman Hussein , was arrested in Rome , Italy , and later extradited to the UK .", "entities": []}, {"text": "FS - Causal A fourth suspect , Osman Hussein , was arrested in Rome , Italy , and later extradited to the UK .", "entities": []}, {"text": "Table 2 : Ambiguous cases from ACE05 and MA VEN test set .", "entities": []}, {"text": "The results are based on prototypical network and Support set means one instance in the support set .", "entities": []}, {"text": "nating while runmeans managing in Query 1 .", "entities": []}, {"text": "FSBase fails to recognize such different sense of word under context .", "entities": []}, {"text": "For Query 2 , FS - Base makes mistake again on the ambiguous word suspect .", "entities": []}, {"text": "Even though suspect is the noun form of suspected in Support set 2 , it does not trigger a Suspicion event in Query 2 .", "entities": []}, {"text": "In contract to FS - Base , our approach is able to handle both cases correctly , illustrating its effectiveness .", "entities": []}, {"text": "5 Related Work Causal Inference .", "entities": [[3, 5, "MethodName", "Causal Inference"]]}, {"text": "Causal inference aims to make reliable predictions using the causal effect between variables ( Pearl , 2009 ) .", "entities": [[0, 2, "MethodName", "Causal inference"]]}, {"text": "Many studies have used causal theory to improve model robustness ( Wang et al . , 2020a , b ; Qi et al . , 2020 ; Tang et al . , 2020b ; Zeng et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Recently , backdoor adjustment has been used to remove the spurious association brought by the confounder ( Tang et al . , 2020a ; Zhang et al . , 2020 ;", "entities": []}, {"text": "Yue et al . , 2020 ; Liu et al . , 2021 ; Zhang et al . , 2021 ) .", "entities": []}, {"text": "Few - shot Event Detection .", "entities": [[3, 5, "TaskName", "Event Detection"]]}, {"text": "Few - shot event detection has been studied in many different settings .", "entities": [[3, 5, "TaskName", "event detection"]]}, {"text": "Bronstein et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2015 ) collect some seed triggers , then detect unseen event with feature - based method .", "entities": []}, {"text": "Deng et al .", "entities": [[0, 3, "DatasetName", "Deng et al"]]}, {"text": "( 2020 ) decompose FSED into two subtasks : trigger identi\ufb01cation and few - shot classi\ufb01cation .", "entities": []}, {"text": "Feng et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) adopt a sentence - level few - shot classi\ufb01cation without triggers .", "entities": []}, {"text": "Lai et al . ( 2020 ) and Cong et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2021 ) adopt N+1 - way fewshot setting that is closest to our setting .", "entities": []}, {"text": "6 Conclusions This paper proposes to revisit the trigger curse in FSED from a causal view .", "entities": []}, {"text": "Speci\ufb01cally , we identify the cause of the trigger curse problem from a structural causal model , and then solve the problem through casual intervention via backdoor adjustment .", "entities": []}, {"text": "Experimental results demonstrate the effectiveness and robustness of our methods .", "entities": []}, {"text": "Acknowledgments We thank the reviewers for their insightful comments and helpful suggestions .", "entities": []}, {"text": "This research work is supported by National Key R&D Program of China under Grant 2018YFB1005100 , the National Natural Science Foundation of China under Grants no . 62106251 and 62076233 , and in part by the Youth Innovation Promotion Association CAS(2018141 ) .", "entities": []}, {"text": "References Laura Banarescu , Claire Bonial , Shu Cai , Madalina Georgescu , Kira Grif\ufb01tt , Ulf Hermjakob , Kevin Knight , Philipp Koehn , Martha Palmer , and Nathan Schneider .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Abstract Meaning Representation for sembanking .", "entities": []}, {"text": "In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse , pages 178\u2013186 , So\ufb01a , Bulgaria .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ofer Bronstein , Ido Dagan , Qi Li , Heng Ji , and Anette Frank . 2015 .", "entities": []}, {"text": "Seed - based event trigger labeling : How far can event descriptions get us ?", "entities": []}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 372\u2013376 , Beijing , China . Association for Computational Linguistics .", "entities": []}, {"text": "Yubo Chen , Liheng Xu , Kang Liu , Daojian Zeng , and Jun Zhao .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Event extraction via dynamic multipooling convolutional neural networks .", "entities": [[0, 2, "TaskName", "Event extraction"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 167\u2013176 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yubo Chen , Hang Yang , Kang Liu , Jun Zhao , and Yantao Jia .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Collective event detection via a hierarchical and bias tagging networks with gated multi - level attention mechanisms .", "entities": [[1, 3, "TaskName", "event detection"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1267\u20131276 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "8083Xin Cong , Shiyao Cui , Bowen Yu , Tingwen Liu , Wang Yubin , and Bin Wang . 2021 .", "entities": []}, {"text": "Few - Shot Event Detection with Prototypical Amortized Conditional Random Field .", "entities": [[3, 5, "TaskName", "Event Detection"], [8, 11, "MethodName", "Conditional Random Field"]]}, {"text": "In Findings of the Association for Computational Linguistics : ACL - IJCNLP 2021 , pages 28\u201340 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shumin Deng , Ningyu Zhang , Jiaojian Kang , Yichi Zhang , Wei Zhang , and Huajun Chen .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Metalearning with dynamic - memory - based prototypical network for few - shot event detection .", "entities": [[13, 15, "TaskName", "event detection"]]}, {"text": "In Proceedings of the 13th International Conference on Web Search and Data Mining , pages 151\u2013159 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "George R Doddington , Alexis Mitchell , Mark A Przybocki , Lance A Ramshaw , Stephanie M Strassel , and Ralph M Weischedel .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "The automatic content extraction ( ace ) program - tasks , data , and evaluation .", "entities": []}, {"text": "InLrec , volume 2 , pages 837\u2013840 .", "entities": []}, {"text": "Lisbon .", "entities": []}, {"text": "Xinya Du and Claire Cardie .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Event extraction by answering ( almost ) natural questions .", "entities": [[0, 2, "TaskName", "Event extraction"], [7, 9, "DatasetName", "natural questions"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 671\u2013683 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shaoyang Duan , Ruifang He , and Wenli Zhao . 2017 .", "entities": []}, {"text": "Exploiting document level information to improve event detection via recurrent neural networks .", "entities": [[6, 8, "TaskName", "event detection"]]}, {"text": "In Proceedings of the Eighth International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 352\u2013361 .", "entities": []}, {"text": "Rui Feng , Jie Yuan , and Chao Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Probing and \ufb01ne - tuning reading comprehension models for few - shot event extraction .", "entities": [[5, 7, "TaskName", "reading comprehension"], [12, 14, "TaskName", "event extraction"]]}, {"text": "CoRR , abs/2010.11325 .", "entities": []}, {"text": "Charles Fillmore .", "entities": []}, {"text": "1967 .", "entities": []}, {"text": "The case for case .", "entities": []}, {"text": "Matt Gardner , Joel Grus , Mark Neumann , Oyvind Tafjord , Pradeep Dasigi , Nelson F. Liu , Matthew E. Peters , Michael Schmitz , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Allennlp :", "entities": []}, {"text": "A deep semantic natural language processing platform .", "entities": []}, {"text": "CoRR , abs/1803.07640 .", "entities": []}, {"text": "Daniel Gildea and Daniel Jurafsky .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Automatic labeling of semantic roles .", "entities": []}, {"text": "Computational linguistics , 28(3):245\u2013288 .", "entities": []}, {"text": "Heng Ji and Ralph Grishman . 2008 .", "entities": []}, {"text": "Re\ufb01ning event extraction through cross - document inference .", "entities": [[1, 3, "TaskName", "event extraction"]]}, {"text": "In Proceedings of ACL-08 : HLT , pages 254\u2013262 , Columbus , Ohio .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yuze Ji , Youfang Lin , Jianwei Gao , and Huaiyu Wan . 2019 .", "entities": []}, {"text": "Exploiting the entity type sequence to bene\ufb01t event detection .", "entities": [[7, 9, "TaskName", "event detection"]]}, {"text": "In Proceedings of the 23rd Conference on Computational Natural Language Learning ( CoNLL ) , pages 613\u2013623 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Viet Dac Lai , Thien Huu Nguyen , and Franck Dernoncourt .", "entities": [[1, 2, "MethodName", "Dac"]]}, {"text": "2020 .", "entities": []}, {"text": "Extensively matching for few - shot learning event detection .", "entities": [[3, 7, "TaskName", "few - shot learning"], [7, 9, "TaskName", "event detection"]]}, {"text": "In Proceedings of the First Joint Workshop on Narrative Understanding , Storylines , and Events , pages 38\u201345 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shasha Liao and Ralph Grishman .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Using document level cross - event inference to improve event extraction .", "entities": [[9, 11, "TaskName", "event extraction"]]}, {"text": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , pages 789\u2013797 , Uppsala , Sweden .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hongyu Lin , Yaojie Lu , Xianpei Han , and Le Sun .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Nugget proposal networks for Chinese event detection .", "entities": [[5, 7, "TaskName", "event detection"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1565\u20131574 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hongyu Lin , Yaojie Lu , Xianpei Han , and Le Sun . 2019a .", "entities": []}, {"text": "Cost - sensitive regularization for label confusion - aware event detection .", "entities": [[9, 11, "TaskName", "event detection"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5278\u20135283 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Hongyu Lin , Yaojie Lu , Xianpei Han , and Le Sun . 2019b .", "entities": []}, {"text": "Sequence - to - nuggets : Nested entity mention detection via anchor - region networks .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5182\u20135192 , Florence , Italy .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Fangchao Liu , Lingyong Yan , Hongyu Lin , Xianpei Han , and Le Sun . 2021 .", "entities": []}, {"text": "Element intervention for open relation extraction .", "entities": [[4, 6, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 4683\u20134693 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jian Liu , Yubo Chen , and Kang Liu . 2019 .", "entities": []}, {"text": "Exploiting the ground - truth : An adversarial imitation based knowledge distillation approach for event detection .", "entities": [[10, 12, "MethodName", "knowledge distillation"], [14, 16, "TaskName", "event detection"]]}, {"text": "InProceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 33 , pages 6754\u20136761 .", "entities": []}, {"text": "Jian Liu , Yubo Chen , Kang Liu , Wei Bi , and Xiaojiang Liu . 2020a .", "entities": []}, {"text": "Event extraction as machine reading comprehension .", "entities": [[0, 2, "TaskName", "Event extraction"], [3, 6, "TaskName", "machine reading comprehension"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1641\u20131651 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "8084Jian Liu , Yubo Chen , Kang Liu , Yantao Jia , and Zhicheng Sheng . 2020b .", "entities": []}, {"text": "How does context matter ?", "entities": []}, {"text": "on the robustness of event detection with contextselective mask generalization .", "entities": [[4, 6, "TaskName", "event detection"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 2523\u20132532 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shulin Liu , Yubo Chen , Kang Liu , and Jun Zhao . 2017 .", "entities": []}, {"text": "Exploiting argument information to improve event detection via supervised attention mechanisms .", "entities": [[5, 7, "TaskName", "event detection"]]}, {"text": "InProceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1789\u20131798 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yaojie Lu , Hongyu Lin , Xianpei Han , and Le Sun .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Distilling discrimination and generalization knowledge for event detection via deltarepresentation learning .", "entities": [[6, 8, "TaskName", "event detection"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4366\u20134376 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Yaojie Lu , Hongyu Lin , Jin Xu , Xianpei Han , Jialong Tang , Annan Li , Le Sun , Meng Liao , and Shaoyi Chen .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Text2Event :", "entities": []}, {"text": "Controllable sequence - tostructure generation for end - to - end event extraction .", "entities": [[11, 13, "TaskName", "event extraction"]]}, {"text": "InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 2795\u20132806 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thien Huu Nguyen , Kyunghyun Cho , and Ralph Grishman .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Joint event extraction via recurrent neural networks .", "entities": [[1, 3, "TaskName", "event extraction"]]}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 300\u2013309 , San Diego , California . Association for Computational Linguistics .", "entities": []}, {"text": "Thien Huu Nguyen and Ralph Grishman .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Event detection and domain adaptation with convolutional neural networks .", "entities": [[0, 2, "TaskName", "Event detection"], [3, 5, "TaskName", "domain adaptation"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 365\u2013371 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Judea Pearl .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Causal diagrams for empirical research .", "entities": []}, {"text": "Biometrika , 82(4):669\u2013688 .", "entities": []}, {"text": "Judea Pearl .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Causality .", "entities": []}, {"text": "Cambridge university press .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Judea Pearl .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Probabilistic reasoning in intelligent systems : networks of plausible inference .", "entities": []}, {"text": "Elsevier .", "entities": []}, {"text": "Judea Pearl , Madelyn Glymour , and Nicholas P Jewell . 2016 .", "entities": []}, {"text": "Causal inference in statistics : A primer .", "entities": [[0, 2, "MethodName", "Causal inference"]]}, {"text": "John Wiley & Sons .", "entities": []}, {"text": "Jiaxin Qi , Yulei Niu , Jianqiang Huang , and Hanwang Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Two causal principles for improving visual dialog .", "entities": [[5, 7, "TaskName", "visual dialog"]]}, {"text": "In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 10860\u201310869 .", "entities": []}, {"text": "Jake Snell , Kevin Swersky , and Richard Zemel .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Prototypical networks for few - shot learning .", "entities": [[3, 7, "TaskName", "few - shot learning"]]}, {"text": "In Advances in neural information processing systems , pages 4077\u20134087 .", "entities": []}, {"text": "Flood Sung , Yongxin Yang , Li Zhang , Tao Xiang , Philip HS Torr , and Timothy M Hospedales .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning to compare : Relation network for few - shot learning .", "entities": [[7, 11, "TaskName", "few - shot learning"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 1199\u20131208 .", "entities": []}, {"text": "Kaihua Tang , Jianqiang Huang , and Hanwang Zhang .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "Long - tailed classi\ufb01cation by keeping the good and removing the bad momentum causal effect .", "entities": []}, {"text": "InAdvances in Neural Information Processing Systems 33 : Annual Conference on Neural Information Processing Systems 2020 , NeurIPS 2020 , December 6 - 12 , 2020 , virtual .", "entities": []}, {"text": "Kaihua Tang , Yulei Niu , Jianqiang Huang , Jiaxin Shi , and Hanwang Zhang .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Unbiased scene graph generation from biased training .", "entities": [[0, 4, "TaskName", "Unbiased scene graph generation"]]}, {"text": "In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 3716\u20133725 .", "entities": []}, {"text": "Wilson L Taylor .", "entities": []}, {"text": "1953 .", "entities": []}, {"text": "\u201c cloze procedure \u201d : A new tool for measuring readability .", "entities": []}, {"text": "Journalism quarterly , 30(4):415\u2013433 .", "entities": []}, {"text": "Tan Wang , Jianqiang Huang , Hanwang Zhang , and Qianru Sun . 2020a .", "entities": []}, {"text": "Visual commonsense r - cnn .", "entities": [[2, 5, "MethodName", "r - cnn"]]}, {"text": "In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 10760 \u2013 10770 .", "entities": []}, {"text": "Tan Wang , Jianqiang Huang , Hanwang Zhang , and Qianru Sun .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Visual commonsense representation learning via causal inference .", "entities": [[2, 4, "TaskName", "representation learning"], [5, 7, "MethodName", "causal inference"]]}, {"text": "In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition Workshops , pages 378 \u2013 379 .", "entities": []}, {"text": "Xiaozhi Wang , Ziqi Wang , Xu Han , Wangyi Jiang , Rong Han , Zhiyuan Liu , Juanzi Li , Peng Li , Yankai Lin , and Jie Zhou .", "entities": []}, {"text": "2020c .", "entities": []}, {"text": "MA VEN : A Massive General Domain Event Detection Dataset .", "entities": [[5, 6, "DatasetName", "General"], [7, 9, "TaskName", "Event Detection"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1652\u20131671 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , R\u00e9mi Louf , Morgan Funtowicz , and Jamie Brew .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Huggingface \u2019s transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "CoRR , abs/1910.03771 .", "entities": []}, {"text": "8085Yi", "entities": []}, {"text": "Yang and Arzoo Katiyar .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Simple and effective few - shot named entity recognition with structured nearest neighbor learning .", "entities": [[6, 9, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 6365\u20136375 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zhongqi Yue , Hanwang Zhang , Qianru Sun , and XianSheng Hua . 2020 .", "entities": []}, {"text": "Interventional few - shot learning .", "entities": [[1, 5, "TaskName", "few - shot learning"]]}, {"text": "InAdvances in Neural Information Processing Systems 33 : Annual Conference on Neural Information Processing Systems 2020 , NeurIPS 2020 , December 6 - 12 , 2020 , virtual .", "entities": []}, {"text": "Xiangji Zeng , Yunliang Li , Yuchen Zhai , and Yin Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Counterfactual generator : A weaklysupervised method for named entity recognition .", "entities": [[7, 10, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 7270\u20137280 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Dong Zhang , Hanwang Zhang , Jinhui Tang , XianSheng Hua , and Qianru Sun . 2020 .", "entities": []}, {"text": "Causal intervention for weakly - supervised semantic segmentation .", "entities": [[3, 8, "TaskName", "weakly - supervised semantic segmentation"]]}, {"text": "InAdvances in Neural Information Processing Systems 33 : Annual Conference on Neural Information Processing Systems 2020 , NeurIPS 2020 , December 6 - 12 , 2020 , virtual .", "entities": []}, {"text": "Wenkai Zhang , Hongyu Lin , Xianpei Han , and Le Sun . 2021 .", "entities": []}, {"text": "De - biasing distantly supervised named entity recognition via causal intervention .", "entities": [[5, 8, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 4803\u20134813 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "A Proof of Backdoor Adjustment We prove the backdoor adjustment for our SCM using the rules of do - calculus ( Pearl , 1995 ) .", "entities": []}, {"text": "For a causal graph G , letGXdenote the graph where all of the incoming edges to Node Xare removed .", "entities": []}, {"text": "let GXdenote the graph where all of the outgoing edges from Node Xare removed . ? ?", "entities": []}, {"text": "G denotes d - separation in G. D - separation ( Pearl , 2014 ): Two ( sets of ) nodes XandYare d - separation by a set of nodes Z(i.e .", "entities": []}, {"text": "X ? ?", "entities": []}, {"text": "GYjZ ) if all of the paths between ( any node in)Xand ( any node in )", "entities": []}, {"text": "Yare blocked by Z.", "entities": []}, {"text": "The rules of do - calculus are : Rule 1 P(yjdo(t);z;w )", "entities": []}, {"text": "= P(yjdo(t);w ) ifY ? ? GTZjT;W", "entities": []}, {"text": "Rule 2 P(yjdo(t);do(z);w )", "entities": []}, {"text": "= P(yjdo(t);z;w ) ifY ? ?", "entities": []}, {"text": "GTZZjT;W Rule 3 P(yjdo(t);do(z);w ) = P(yjdo(t);w ) ifY ? ?", "entities": []}, {"text": "GTZ(W)ZjT;W ( 5 ) whereZ(W)denotes the set of nodes of Z that are n\u2019t ancestors of any node of WinGT .", "entities": []}, {"text": "We can prove our interventional distribution P(Yjdo(C = C);E = e ): Step 1 Using the law of total probability : P(Yjdo(C = C);E = e;Q = q ) = X t2TX s2S[P(Yjdo(C);e;t;s;q ) \u0002 P(s;tjdo(C);e;q ) ]", "entities": []}, {"text": "Step 2 Using the law of conditional probability : P(Yjdo(C = C);E = e;Q = q ) = X t2TX s2S[P(Yjdo(C);e;t;s;q ) \u0002 P(sjdo(C);e;t;q ) P(tjdo(C);e;q ) ]", "entities": []}, {"text": "Step 3 Using the Rule 3 : P(Yjdo(C = C);E = e;Q = q ) = X t2TX s2S[P(Yje;t;s;q ) \u0002 P(sjdo(C);e;t;q ) P(tje;q ) ]", "entities": []}, {"text": "8086Step 4 Using the Rule 1 : P(Yjdo(C = C);E = e;Q = q ) = X t2TX s2SP(Yjs;q)P(sjdo(C);t)P(tje )", "entities": []}, {"text": "Step 5", "entities": []}, {"text": "Using the Rule 2 : P(Yjdo(C = C);E = e;Q = q ) = X t2TX s2SP(Yjs;q)P(sjC;t)P(tje ) B Detailed Task Settings One - way K - Shot Settings .", "entities": []}, {"text": "We adopt One - way K - shot setting in our experiments , in which the support set in an episode contains one event type ( called concerned event ) and the query can contain any event type .", "entities": []}, {"text": "The model aims to detect triggers of the concerned event in query and all types will be evaluated by traversing each event type .", "entities": []}, {"text": "The support set and query in an episode can be formulated as follows : S = f(S1;E;Y1 ) ; : : : ; ( SK;E;YK)g whereSis the support set , Eis the concerned event , Si = fsi 1;si 2;:::;si", "entities": []}, {"text": "nigis the i - th sentence in support , si jis the j - th token in Si , Yi= fyi 1;yi 2;:::;yi ngis the labels of tokens in Siand yi j= 1only iftiis the trigger ( or part of trigger ) of concerned event , otherwise yi j= 0 .", "entities": [[49, 50, "DatasetName", "0"]]}, {"text": "Q = fQ1;Q2;:::;QMg whereQis the set of query and Qi= fqi 1;qi 2;:::;qi migis the i - th query sentence and qi j is the j - th token in Qi", "entities": []}, {"text": "The model is expected to output the concerned event inQ : OQ = f(Q1;E;T1 1 ) ; : : : ; ( Q1;E;T1 n1 ) ; ( Q2;E;T2 1 ) ; : : : ; ( Q2;E;T2 n2 ) ; : : : ; ( QM;E;TM 1 ) ; : : : ; ( QM;E;TM nM)g whereOQis the set of triggers of concerned event detected inQ , Ti kis the k - th trigger of concerned event in sentence Qiandni\u00150means the number of triggers of concerned event in Qi .", "entities": []}, {"text": "Evaluation We improve the traditional episode evaluation setting by evaluating the full test set .", "entities": []}, {"text": "For each event type in test set , we randomly sample K instances as support set and all other instances are used as query .", "entities": []}, {"text": "Following previous event detection works ( Chen et al . , 2015 ) , the predicted trigger is correct if its event type and offsets match those of a gold trigger .", "entities": [[2, 4, "TaskName", "event detection"]]}, {"text": "We evaluate all methods using macroF1 and micro - F1 scores , and micro - F1 is taken as the primary measure .", "entities": [[7, 10, "MetricName", "micro - F1"], [13, 16, "MetricName", "micro - F1"]]}, {"text": "C Few - shot Event Detection Baselines We use two metric - base methods in our experiments : Prototypical network ( Snell et al . , 2017 ) and Relation network ( Sung et al . , 2018 ) , which contain an encoder component and a classi\ufb01er component .", "entities": [[4, 6, "TaskName", "Event Detection"]]}, {"text": "Encoder We use BERT ( Devlin et al . , 2019 ) to encoder the support set and the query .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "Given a sentece X = fx1;x2;:::;x ng , BERT encodes the sequence and output the represent of each token in X : R = fr1;r2;:::;rng .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "After obtaining the feature representation of the support set , we calculate the prototype of the categories ( concerned event and other ): pi=1 jRijX r2Rir ; i= 0;1 where piis the prototype of category i , Riis the set of feature representation of tokens that labeled withy = iin support set .", "entities": []}, {"text": "Classi\ufb01er The models classify each token in query based on its similarity to the prototype .", "entities": []}, {"text": "We \ufb01rst calculate the similarity between prototype and token in query .", "entities": []}, {"text": "si;j;k = g(pk;qi j ) ;", "entities": []}, {"text": "k = 0;1 ( 6 ) whereg(x;y)measures the similarity between x andy , qi jis the represent of j - th token in i - th query sentence .", "entities": [[0, 2, "HyperparameterName", "k ="]]}, {"text": "Then we calculate the probability distribution of tokenqi j : P(Yjqi j;S )", "entities": []}, {"text": "= Softmax ( si;j;0;si;j;1 ) ( 7 ) During training , we use the Cross - Entropy loss on each token of query .", "entities": [[1, 2, "MethodName", "Softmax"], [17, 18, "MetricName", "loss"]]}, {"text": "And the support set and the query are randomly sampled from the training set .", "entities": []}, {"text": "8087When evaluating , we treat the labels as IO tagging schemes , and adjacent I are considered to be the same trigger so that we can handle a trigger with multiple tokens .", "entities": []}, {"text": "Similarity Functions For prototypical network , the similarity in Equation 6 is Euclidean distance .", "entities": []}, {"text": "For relation network , we calculate similarity using neural networks .", "entities": []}, {"text": "Unlike the original paper , we \ufb01nd the following calculation to be more ef\ufb01cient : g(pk;qj i )", "entities": []}, {"text": "= F(pk\bqj i\bjpk\u0000qj ij)where\b means concatenation vectors and Fis two - layer feed - forward neural networks with a ReLU function on the \ufb01rst layer .", "entities": [[19, 20, "MethodName", "ReLU"]]}, {"text": "D Proof of Loss Function We proveLSG(\u0012)is equivalent toL(\u0012 ) , which indicates that minimizing LSG(\u0012)is equivalent to minimizingL(\u0012 ) .", "entities": []}, {"text": "At \ufb01rst , we de\ufb01ne a function \u001e ( s;q)/P(Yjs;q;\u0012)and then we need to prove thatg(P t2TP s2SP(tje)p(sjC;t)rs;q ) = f(P t2TP s2SP(tje)P(sjC;t ) \u001e ( s;q ) ) .", "entities": []}, {"text": "From Appendix - A , we can obtain : X t2TX s2SP(tje)p(sjC;t )", "entities": []}, {"text": "= X t2TX s2SP(s;tjdo(C);e;q )", "entities": []}, {"text": "= 1 D.1 Prototypical Network For prototypical network , g(r;q )", "entities": []}, {"text": "= ( r\u0000q)2 .", "entities": []}, {"text": "Let \u001e ( s;q ) = jrs\u0000qjandf(x )", "entities": []}, {"text": "= x2;x > 0 .", "entities": [[3, 4, "DatasetName", "0"]]}, {"text": "g(X t2TX s2SP(tje)p(sjC;t)rs;q ) =[ X t2TX s2SP(tje)p(sjC;t)rs\u0000q]2 =[ X t2TX", "entities": []}, {"text": "s2SP(tje)p(sjC;t)rs \u0000X t2TX s2SP(tje)p(sjC;t)q]2", "entities": []}, {"text": "=[ X t2TX s2SP(tje)P(sjC;t)jrs\u0000qj]2", "entities": []}, {"text": "= f[X t2TX s2SP(tje)P(sjC;t ) \u001e ( s;q ) ]", "entities": []}, {"text": "/f(P(Yjdo(C = C);E = e;Q = q ) ) D.2 Relation Network Letg(r;q )", "entities": []}, {"text": "= F[r\bq\bjr\u0000qj ] .", "entities": []}, {"text": "Let \u001e ( s;q ) = g(rs;q)andf(x )", "entities": []}, {"text": "= xg(X t2TX s2SP(tje)P(sjC;t)rs;q )", "entities": []}, {"text": "= F[X t2TX s2SP(tje)P(sjC;t)rs\bq \bjX t2TX s2SP(tje)P(sjC;t)rs\u0000qj ]", "entities": []}, {"text": "= F[X t2TX s2SP(tje)P(sjC;t)rs \bX t2TX s2SP(tje)P(sjC;t)q \bjX t2TX s2SP(tje)P(sjC;t)rs \u0000X t2TX s2SP(tje)P(sjC;t)qj ] \u0019F[X t2TX s2SP(tje)P(sjC;t)rs \bX t2TX s2SP(tje)P(sjC;t)q \bX t2TX s2SP(tje)P(sjC;t)jrs\u0000qj ]", "entities": []}, {"text": "= X t2TX s2SP(tje)P(sjC;t)g(r;q ) = f[X t2TX s2SP(tje)P(sjC;t ) \u001e ( s;q ) ]", "entities": []}, {"text": "/f(P(Yjdo(C = C);E = e;Q = q ) )", "entities": []}, {"text": "Here , we assume that the feature representations of the same event type in support are close to each other so thatjP spsrs\u0000P spsqj\u0019P spsjrs\u0000qj .", "entities": []}, {"text": "E Implementation Details All of our experiments are implemented on one Nvidia TITAN RTX .", "entities": [[12, 13, "DatasetName", "TITAN"]]}, {"text": "Our implementation is based on HuggingFace \u2019s Transformers ( Wolf et al . , 2019 ) and Allennlp ( Gardner et al . , 2018 ) .", "entities": []}, {"text": "We tune the hyperparameters based on the dev performance .", "entities": []}, {"text": "We train each model 5 times with different random seed , and when evaluating , we sample 4 different support sets .", "entities": []}, {"text": "Metric - based Methods", "entities": []}, {"text": "The hyperparameter is shown in Table 5 .", "entities": []}, {"text": "During training , the support set and the query is sampled in training set , the query contains 2 positive instances and 10 negative instances ( 5 times of positive instances ) .", "entities": []}, {"text": "During validating , the support set and the query is sampled in dev set , the query contains 10 positive instances and 100 negative instances ( 10 times of positive", "entities": []}, {"text": "8088ModelACE05 MA VEN KBP17 Macro Micro Macro Micro Macro Micro Prototypical Network FS - Base(Snell et al . , 2017 ) 66.2\u00063.8", "entities": []}, {"text": "63.8\u00064.1 44.1\u00061.6 44.0\u00062.3 67.1\u00061.7 68.0\u00061.6 FS - Lexfree ( Lu et al . , 2019 ) 50.4\u00062.8 50.7\u00063.6 24.9\u00061.1 20.5\u00061.4 60.8\u00062.7 60.4\u00063.7 FS - Cluster ( Lai et al . , 2020 ) 69.9\u00061.9 67.3\u00062.2 43.8\u00061.4 43.6\u00061.5 67.6\u00062.4 68.7\u00062.6 FS - Causal ( Ours ) 76.8\u00060.6 76.3\u00060.7 51.8\u00060.5 55.1\u00060.4 72.6\u00060.9 74.9\u00060.9 Relation Network FS - Base(Sung et al . , 2018 ) 65.7\u00063.9 66.9\u00062.1 51.0\u00061.1 55.6\u00061.6 66.8\u00062.2 71.1\u00062.0 FS - Lexfree ( Lu et al . , 2019 ) 59.1\u00066.4 59.6\u00063.6 42.9\u00061.0 45.4\u00062.1 63.5\u00061.9 65.7\u00062.0 FS - Cluster ( Lai et al . , 2020 ) 54.4\u00062.9 57.2\u00063.2 45.8\u00062.3", "entities": []}, {"text": "51.4\u00061.4 57.5\u00064.2 62.0\u00063.2 FS - Causal ( Ours ) 65.0\u00062.1 69.3\u00061.5 53.6\u00060.7 57.9\u00061.0 66.9\u00060.1 72.7\u00060.9 Table 3 : F1 score of 5 - shot FSED on dev set .", "entities": [[18, 20, "MetricName", "F1 score"]]}, {"text": "\u0006is the standard deviation of 5 random training rounds .", "entities": []}, {"text": "ACE MA VEN KBP Proto ( support ) 76.25 55.11 74.80 Proto ( query ) 65.75 37.89 69.25 Proto ( support + query ) 74.03 51.19 74.93 Relation ( support ) 68.40 54.34 69.09 Relation ( query ) 66.31 57.85 71.59 Relation ( support + query ) 69.31 56.93 72.65 Table 4 : Micro F1 score of 5 - shot FSED on dev set .", "entities": [[53, 55, "MetricName", "Micro F1"]]}, {"text": "( support ) means the backdoor adjustment is used in the support set , ( query ) means the backdoor adjustment is used in the query .", "entities": []}, {"text": "ACE MA VEN KBP Optimizer AdamW AdamW AdamW", "entities": [[4, 5, "HyperparameterName", "Optimizer"], [5, 6, "MethodName", "AdamW"], [6, 7, "MethodName", "AdamW"], [7, 8, "MethodName", "AdamW"]]}, {"text": "Learning rate 2e-5 2e-5 2e-5 Warmup step 40 240 50 Batch size 1 1 1 patience 15 15 15 max epoch num 80 80 80 batches per epoch 40 240 50 \u0015forP(TjC ) 0.5 0.5 0.5 FS - Causal ( Prototypical ) S S", "entities": [[0, 2, "HyperparameterName", "Learning rate"], [10, 12, "HyperparameterName", "Batch size"]]}, {"text": "S+Q FS - Causal ( Relation ) S+Q Q S+Q Table 5 : Hyperparameters of metric - based methods .", "entities": []}, {"text": "For FS - Causal , S means the backdoor adjustment is used in the support set , Q means the backdoor adjustment is used in the query .", "entities": []}, {"text": "ACE MA VEN KBP Optimizer AdamW AdamW AdamW", "entities": [[4, 5, "HyperparameterName", "Optimizer"], [5, 6, "MethodName", "AdamW"], [6, 7, "MethodName", "AdamW"], [7, 8, "MethodName", "AdamW"]]}, {"text": "Learning rate ( Pretrain ) 1e-5", "entities": [[0, 2, "HyperparameterName", "Learning rate"]]}, {"text": "1e-5 1e-5 batches per epoch ( Pretrain ) 50 200 200 Warmup step ( Pretrain ) 50 200 200 Batch size ( Pretrain ) 128 128 128 patience ( Pretrain ) 10 10 10 max epoch num ( Pretrain ) 50 50 50 Learning rate ( Finetune )", "entities": [[19, 21, "HyperparameterName", "Batch size"], [43, 45, "HyperparameterName", "Learning rate"]]}, {"text": "2e-5 2e-5 2e-5 Learning rate ( Finetune * )", "entities": [[3, 5, "HyperparameterName", "Learning rate"]]}, {"text": "1e-3 1e-3 1e-3", "entities": []}, {"text": "Finetuning Step ( Finetune ) 20 20 20 Finetuning Step ( Pretrain + Finetune ) 10 10 10 Table 6 : Hyperparameters of \ufb01netuning - based methods.instances ) .", "entities": []}, {"text": "The results of dev set are shown in Table 3 .", "entities": []}, {"text": "For FS - Causal , we found that there is an impact on whether backdoor adjustment is applied separately to the support set and query , as shown in Table 4 .", "entities": []}, {"text": "Based on the best results of the dev set , we evaluate it on the test set .", "entities": []}, {"text": "Finetuning - based Methods", "entities": []}, {"text": "The hyperparameter is shown in Table 6 .", "entities": []}, {"text": "For pretraining , we train a supervised event detection model using the training set .", "entities": [[7, 9, "TaskName", "event detection"]]}, {"text": "For \ufb01netuning , we use the support set to \ufb01netune the parameters of the event detection model and then detect the event in query .", "entities": [[14, 16, "TaskName", "event detection"]]}]