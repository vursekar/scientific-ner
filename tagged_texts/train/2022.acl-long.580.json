[{"text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 8479 - 8492 May 22 - 27 , 2022 c", "entities": []}, {"text": "2022 Association for Computational Linguistics SUPERB - SG : Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities Hsiang - Sheng Tsai1\u0006 , Heng - Jui Chang1\u0006 , Wen - Chin Huang2\u0006 , Zili Huang3\u0006 , Kushal Lakhotia4\u0006 , Shu - wen Yang1 , Shuyan Dong4 , Andy T. Liu1 , Cheng - I Jeff Lai5 , Jiatong Shi6 , Xuankai Chang6 , Phil Hall7 , Hsuan - Jui Chen1 , Shang - Wen Li4 , Shinji Watanabe6 , Abdelrahman Mohamed4 , Hung - yi Lee1 1National Taiwan University , Taiwan 2Nagoya University , Japan 3Johns Hopkins University , USA 4Meta AI , USA 5Massachusetts Institute of Technology , USA 6Carnegie Mellon University , USA 7LXT { r09922024 , b06901020}@ntu.edu.tw wen.chinhuang@g.sp.m.is.nagoya-u.ac.jp hzili1@jhu.edu , kushall@fb.com shangwel@fb.com , shinjiw@ieee.org abdo@fb.com , hungyilee@ntu.edu.tw Abstract Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years .", "entities": [[133, 135, "TaskName", "Transfer learning"]]}, {"text": "In speech , a model pre - trained by self - supervised learning transfers remarkably well on multiple tasks .", "entities": [[9, 13, "TaskName", "self - supervised learning"]]}, {"text": "However , the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the ef\ufb01cacy of such models .", "entities": []}, {"text": "SUPERB was a step towards introducing a common benchmark to evaluate pretrained models across various speech tasks .", "entities": []}, {"text": "In this paper , we introduce SUPERB - SG , a new benchmark focused on evaluating the semantic and generative capabilities of pre - trained models by increasing task diversity and dif\ufb01culty over SUPERB .", "entities": []}, {"text": "We use a lightweight methodology to test the robustness of representations learned by pre - trained models under shifts in data domain and quality across different types of tasks .", "entities": []}, {"text": "It entails freezing pretrained model parameters , only using simple task - speci\ufb01c trainable heads .", "entities": []}, {"text": "The goal is to be inclusive of all researchers , and encourage ef\ufb01cient use of computational resources .", "entities": []}, {"text": "We also show that the task diversity of SUPERB - SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation .", "entities": []}, {"text": "\u0006 \u0006Equal contribution.1 Introduction Transfer learning is a paradigm in machine learning that has been very effective for natural language processing ( NLP ) ( Peters et al . , 2018 ; Devlin et", "entities": [[4, 6, "TaskName", "Transfer learning"]]}, {"text": "al . , 2019 ; Liu et al . , 2019 ; Lan et al . , 2019 ; Dong et", "entities": []}, {"text": "al . , 2019 ; Yang et al . , 2019 ; Raffel et al . , 2020 ; Lewis et al . , 2019 ; Conneau et al . , 2020 ) , and speech processing ( van den Oord", "entities": []}, {"text": "et al . , 2018 ; Rivi\u00e8re et", "entities": []}, {"text": "al . , 2020 ; Chung et al . , 2019 ; Schneider et al . , 2019 ; Baevski et al . , 2020b ; Hsu et al . , 2021 ; Liu et al . , 2020c , b ; Ravanelli et al . , 2020 ; Ling et", "entities": []}, {"text": "al . , 2020 ; Ling and Liu , 2020 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Self - supervised learning ( SSL ) is the main driver of this paradigm , an effective and scalable way to learn high - level representation of language that transfers to a variety of tasks .", "entities": [[0, 4, "TaskName", "Self - supervised learning"], [5, 6, "DatasetName", "SSL"]]}, {"text": "SSL entails learning from the input or some perturbation of it without the need for labelled data .", "entities": [[0, 1, "DatasetName", "SSL"]]}, {"text": "This has unlocked the usage of large amounts of cheaply available unlabelled data .", "entities": []}, {"text": "It lends naturally to neural network models that have been shown to possess impressive scaling characteristics such that it is often enough to increase the model and data sizes to improve downstream performance ( Hestness et al . , 2017 ; Shazeer et", "entities": []}, {"text": "al . , 2017 ; Jozefowicz et al . , 2016 ; Mahajan et al . , 2018 ; Radford et", "entities": []}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "Speech signal consists of acoustic , linguistic , prosodic , and speaker characteristics .", "entities": []}, {"text": "SSL algo-8479", "entities": [[0, 1, "DatasetName", "SSL"]]}, {"text": "rithms in speech must be evaluated in their ability to produce representations that are useful for tasks that demand understanding of linguistic , speaker , and prosodic elements of spoken language as well as high - level semantics .", "entities": []}, {"text": "Researchers have used auto - regressive , contrastive , discriminative and multi - task learning objectives to pre - train models , and have investigated their capabilities across tasks like phoneme recognition ( van den Oord et al . , 2018 ; Chung et", "entities": [[11, 15, "TaskName", "multi - task learning"]]}, {"text": "al . , 2019 ) , automatic speech recognition ( ASR ) ( Liu et al . , 2020b ; Schneider et al . , 2019 ; Ling and Liu , 2020 ; Ravanelli et al . , 2020 ; Hsu et al . , 2021 ; Chang et al . , 2021 ) , speaker veri\ufb01cation ( Fan et al . , 2020 ) , speaker identi\ufb01cation ( Chung et al . , 2019 ; Liu et al . , 2020c ) , emotion recognition ( Macary et al . , 2021 ) , speech translation ( Chung et al . , 2019 ) , voice conversion ( Lin et al . , 2020 ; Huang et al . , 2021a ) , spoken language understanding ( Lai et al . , 2021 ) , and text - tospeech ( \u00c1lvarez et al . , 2019 ) .", "entities": [[6, 9, "TaskName", "automatic speech recognition"], [84, 86, "TaskName", "emotion recognition"], [106, 108, "TaskName", "voice conversion"], [124, 127, "TaskName", "spoken language understanding"]]}, {"text": "However , the methodologies in such studies vary in the use of datasets , \ufb01ne - tuning strategies and task - speci\ufb01c model architectures .", "entities": []}, {"text": "To bridge this gap , SUPERB ( Yang et al . , 2021 ) introduced a standardized benchmark of 10 speech tasks to compare 13 pre - trained models and a Log Mel - Filterbank baseline .", "entities": []}, {"text": "It studied the models \u2019 performance in tasks focusing on linguistic ( phoneme recognition and automatic speech recognition , keyword spotting and query by example ) , shallow semantic ( intent classi\ufb01cation and slot \ufb01lling ) , speaker ( speaker identi\ufb01cation , speaker veri\ufb01cation and speaker diarization ) , and prosodic ( emotion recognition ) characteristics .", "entities": [[15, 18, "TaskName", "automatic speech recognition"], [19, 21, "TaskName", "keyword spotting"], [45, 47, "TaskName", "speaker diarization"], [52, 54, "TaskName", "emotion recognition"]]}, {"text": "In this paper , we introduce SUPERB - SG , a benchmark with 5 new tasks , which are speech translation , out - of - domain ASR , voice conversion , speech separation , and speech enhancement , with an emphasis on evaluating the semantic and generative capabilities of pre - trained models that require high - level representations to capture linguistic , semantic , and speaker characteristics .", "entities": [[29, 31, "TaskName", "voice conversion"], [32, 34, "TaskName", "speech separation"], [36, 38, "TaskName", "speech enhancement"]]}, {"text": "These tasks go beyond speech recognition by focusing on various other aspects that are essential to building intelligent speech interfaces .", "entities": [[4, 6, "TaskName", "speech recognition"]]}, {"text": "Further , we show that while SSL models achieve close to state - of - the - art performance on many tasks , there is n\u2019t one model that outperforms all others , and that a simple Log Mel - Filterbank can perform competitively on some tasks .", "entities": [[6, 7, "DatasetName", "SSL"]]}, {"text": "We also demonstrate the robustness of our methodology with an ablation study over different task - speci\ufb01c model architectures and data sizes .", "entities": []}, {"text": "Downstream   Model Downstream Features", "entities": []}, {"text": "1 Upstream   Model ( eg .   FBANK ,   TERA ,   etc . )", "entities": []}, {"text": "Upstream   ( frozen )", "entities": []}, {"text": "Features   L Weighted - sum   Mechanism ?", "entities": []}, {"text": "Prediction Features Loss TargetFigure 1 : Illustration of the detailed training procedure .", "entities": []}, {"text": "A trainable weighted - sum mechanism is used to summarize all layers \u2019 representations into a sequence of vectors and then taken by downstream model as input .", "entities": []}, {"text": "Upstream is frozen through the whole process .", "entities": []}, {"text": "Dashed arrow ( 99 K ) is used to indicate the \ufb02ow of gradient when back propagating .", "entities": []}, {"text": "The introduction of these new tasks of varying dif\ufb01culty takes us closer to a more comprehensive uni\ufb01ed standard speech benchmark .", "entities": []}, {"text": "We hope that this will motivate the development of more powerful , generalizable , and reusable pre - trained models to democratize the advancement of speech research .", "entities": []}, {"text": "To facilitate this , we released the codes1and integrated the tasks with the SUPERB benchmark .", "entities": []}, {"text": "2 Related Work As more powerful SSL models are proposed with promising performance on various tasks , researchers continually try to \ufb01nd extensive evaluation methods to assess model performance , and wish to holistically understand the capability of the learned representations in these models .", "entities": [[6, 7, "DatasetName", "SSL"]]}, {"text": "SUPERB ( Yang et al . , 2021 ) is a framework to benchmark the SSL models on 10 speech tasks by learning task - speci\ufb01c prediction heads on top of the frozen shared SSL models .", "entities": [[15, 16, "DatasetName", "SSL"], [34, 35, "DatasetName", "SSL"]]}, {"text": "Although the tasks inSUPERB span across different domains , most of them are simple classi\ufb01cation problems , or only require utilization of shallow semantics .", "entities": []}, {"text": "In contrast , we focus on harder semantic and generative tasks .", "entities": []}, {"text": "Another recently proposed benchmark is the LeBenchmark ( Evain et al . , 2021 ) , investigating the performance of SSL models trained on French data with three semantic tasks .", "entities": [[20, 21, "DatasetName", "SSL"]]}, {"text": "However , they only consider wav2vec 2.0 ( Baevski et al . , 2020b ) with 1https://github.com/s3prl/s3prl : Tasks in SUPERB - SG are open - sourced and reproducible in the S3PRL toolkit which supports benchmarking the most existing and customized pretrained models.8480", "entities": []}, {"text": "different architectures as their upstream models ( i.e. , networks pre - trained with SSL ) .", "entities": [[14, 15, "DatasetName", "SSL"]]}, {"text": "Here , we evaluate a diverse set of SSL models , and offer a more comprehensive analysis .", "entities": [[8, 9, "DatasetName", "SSL"]]}, {"text": "The Zero Resource Speech Benchmark 2021 ( Nguyen et al . , 2020 ) introduces unsupervised speech processing tasks , particularly the spoken language modeling problem .", "entities": []}, {"text": "They evaluate the SSL models via zero - shot probings at four linguistic levels .", "entities": [[3, 4, "DatasetName", "SSL"]]}, {"text": "While their benchmark task is speci\ufb01c for certain domain , we use various tasks to evaluate different aspects of SSL models .", "entities": [[19, 20, "DatasetName", "SSL"]]}, {"text": "The HEAR 2021 Challenge2aims to develop general - purpose audio representation by focusing on audio tasks beyond speech that include sound event detection , speech commands and pitch & chroma classi\ufb01cation .", "entities": [[20, 23, "TaskName", "sound event detection"], [24, 26, "DatasetName", "speech commands"]]}, {"text": "We speci\ufb01cally focus on various aspects of speech processing , thus providing a wide variety of spoken language tasks .", "entities": []}, {"text": "3 SUPERB - SG 3.1 Tasks and Datasets This section introduces the tasks in SUPERB - SG , including why we choose these tasks and how we design the task - speci\ufb01c heads for \ufb01ne - tuning .", "entities": []}, {"text": "Following SUPERB \u2019s methodology , we use a lightweight \ufb01ne - tuning approach wherein we freeze the pre - trained model parameters and only keep the task - speci\ufb01c head \u2019s parameters trainable .", "entities": []}, {"text": "This setting serves the dual purpose of evaluating the robustness as well as the generalizability of the speech representations , and provides a resourceef\ufb01cient way of \ufb01ne - tuning the models that is inclusive of participants with constrained compute resources .", "entities": []}, {"text": "We call the pre - trained model as upstream model and the task - speci\ufb01c heads as downstream model .", "entities": []}, {"text": "We now discuss the newly added tasks in SUPERB - SG in the following sub - sections .", "entities": []}, {"text": "3.1.1 Speech Translation Speech translation ( ST ) involves translating the acoustic speech signals in the source language into the words in the target language .", "entities": [[2, 3, "TaskName", "Translation"]]}, {"text": "We use it to evaluate the semantic capability of SSL models , and how they bene\ufb01t the translation task .", "entities": [[9, 10, "DatasetName", "SSL"]]}, {"text": "We use the CoV oST2", "entities": []}, {"text": "En \u00d1De ( Wang et al . , 2020 ) dataset ( CC0 Licensed ) with their of\ufb01cial train , validation , and test splits while removing all the samples containing \" REMOVE \" , resulting in 425.8 , 25.9 2https://neuralaudio.ai/hear2021-holistic-evaluation-ofaudio-representations.htmland 24.5 hours respectively .", "entities": []}, {"text": "For text , we keep original case , normalize punctuation , and build character vocabulary with 100 % train - set coverage .", "entities": []}, {"text": "We report case - sensitive de - tokenized BLEU using sacreBLEU ( Post , 2018 ) .", "entities": [[8, 9, "MetricName", "BLEU"], [10, 11, "MetricName", "sacreBLEU"]]}, {"text": "Our downstream model has an encoder - decoder architecture with 3 layers of Transformers ( Vaswani et", "entities": []}, {"text": "al . , 2017 ) each with hidden dimension of 512 .", "entities": []}, {"text": "A convolutional subsampler is used to reduce the sequence length of the input before feeding it to the encoder .", "entities": []}, {"text": "We train our model with label - smoothing using a probability of 0.1 .", "entities": []}, {"text": "A beam size of 20 is used for inference .", "entities": []}, {"text": "3.1.2 Out - of - domain ASR Although an ASR is included in SUPERB , it only examines SSL models on read English corpus LibriSpeech ( Panayotov et al . , 2015 ) .", "entities": [[18, 19, "DatasetName", "SSL"], [24, 25, "DatasetName", "LibriSpeech"]]}, {"text": "Therefore , we introduce out - of - domain ASR ( OOD - ASR ) , which aims to evaluate the models \u2019 capabilities across languages , and out - of - domain scenarios .", "entities": []}, {"text": "The OODASR tasks are categorized into cross - lingual and spontaneous speech tasks .", "entities": []}, {"text": "For the cross - lingual tasks , we choose the Mexican Spanish ( es ) , Mandarin ( zh ) , and Arabic ( ar ) subsets from Common V oice 7.0 ( Ardila et", "entities": []}, {"text": "al . , 2020 ) ( CC0 Licensed ) containing 21.5 , 31.2 , and 30.7 hours of training data respectively .", "entities": []}, {"text": "The validation set sizes are 1.2 hours , 14.4 hours and 12.24 hours , and the test set sizes are 0.6 hour , 15.3 hours and 12.5 hours for es , zh and ar respectively .", "entities": []}, {"text": "For the spontaneous speech task ( spon ) , we use the Santa Barbara Corpus of Spoken American English ( SBCSAE ) ( Du Bois et al . , 2000 \u2013 2005 ) ( CC BY - ND 3.0 Licensed ) , consisting of 60 conversations over different topics spanning 16.7 hours of data .", "entities": []}, {"text": "The validation and test set sizes are 1.6 hours and 2.2 hours respectively .", "entities": []}, {"text": "For evaluation , we use word error rate ( WER ) as the metric except for Mandarin which character error rate ( CER ) is used .", "entities": [[5, 11, "MetricName", "word error rate ( WER )"]]}, {"text": "The error rates are averaged across all sub - tasks to offer an overall score .", "entities": []}, {"text": "The ASR model is a 2 - layer BLSTM ( Hochreiter and Schmidhuber , 1997 ) with hidden states of 1024 dimension .", "entities": []}, {"text": "The training objective is to minimize the Connectionist Temporal Classi\ufb01cation ( CTC ) loss ( Graves et al . , 2006 ) .", "entities": [[11, 12, "DatasetName", "CTC"], [13, 14, "MetricName", "loss"]]}, {"text": "During inference , we use CTC greedy decoding without language model re - scoring to simplify the process and to highlight the impact of the learned acoustic representations .", "entities": [[5, 6, "DatasetName", "CTC"]]}, {"text": "3.1.3 Voice Conversion For voice conversion ( VC ) , we consider the intralingual VC task in VCC2020 ( Zhao et", "entities": [[1, 3, "TaskName", "Voice Conversion"], [4, 6, "TaskName", "voice conversion"]]}, {"text": "al . , 2020)8481", "entities": []}, {"text": "Upstream Network # Params Stride Input Corpus Pretraining Of\ufb01cial Github FBANK - 0 10ms waveform - - PASE+ SincNet , 7 - Conv , 1 - QRNN 7.83 M 10ms waveform LS 50", "entities": [[3, 4, "MetricName", "Params"], [12, 13, "DatasetName", "0"], [17, 18, "MethodName", "PASE+"], [26, 27, "MethodName", "QRNN"]]}, {"text": "hr multi - task santi - pdp / pase APC 3 - GRU 4.11 M 10ms FBANK", "entities": [[12, 13, "MethodName", "GRU"]]}, {"text": "LS 360 hr F - G iamyuanchung / APC VQ - APC 3 - GRU 4.63 M 10ms FBANK", "entities": [[14, 15, "MethodName", "GRU"]]}, {"text": "LS 360 hr F - G + VQ iamyuanchung / VQ - APC NPC 4 - Conv , 4 - Masked Conv 19.38 M 10ms FBANK", "entities": []}, {"text": "LS 360 hr M - G + VQ Alexander - H - Liu / NPC Mockingjay", "entities": []}, {"text": "12 - Trans 85.12 M 10ms FBANK", "entities": []}, {"text": "LS 360 hr time M - G s3prl / s3prl", "entities": []}, {"text": "TERA 3 - Trans 21.33 M 10ms FBANK", "entities": []}, {"text": "LS 960 hr time / freq M - G s3prl / s3prl DeCoAR 2.0 12 - Trans 89.84 M 10ms FBANK", "entities": []}, {"text": "LS 960 hr time M - G + VQ awslabs / speech - representations Modi\ufb01ed CPC 5 - Conv , 1 - LSTM 1.84 M 10ms waveform LL 60k hr F - C facebookresearch /", "entities": [[22, 23, "MethodName", "LSTM"]]}, {"text": "CPC_audio wav2vec 19 - Conv 32.54 M 10ms waveform LS 960", "entities": []}, {"text": "hr F - C pytorch / fairseq vq - wav2vec 20 - Conv 34.15 M 10ms waveform LS 960", "entities": []}, {"text": "hr F - C + VQ pytorch / fairseq wav2vec 2.0 Base 7 - Conv 12 - Trans 95.04 M 20ms waveform LS 960 hr M - C + VQ pytorch / fairseq wav2vec 2.0 Large 7 - Conv 24 - Trans 317.38 M 20ms waveform LL 60k hr M - C + VQ pytorch / fairseq HuBERT Base 7 - Conv 12 - Trans 94.68 M 20ms waveform LS 960", "entities": []}, {"text": "hr M - P + VQ pytorch / fairseq HuBERT Large 7 - Conv 24 - Trans 316.61 M 20ms waveform LL 60k hr M - P + VQ pytorch / fairseq Table 1 : Details of the investigated SSL representations .", "entities": [[39, 40, "DatasetName", "SSL"]]}, {"text": "LibriSpeech and LibriLight are denoted as LS and LL , respectively .", "entities": [[0, 1, "DatasetName", "LibriSpeech"]]}, {"text": "For the pretraining methods , we abbreviate \" vector quantization \" as VQ , \" future \" as F , \" masked \" as M , \" generation \" as G , \" contrastive discrimination \" as C , and \" token prediction / classi\ufb01cation \" as P. Parameters for both pretraining and inference are counted .", "entities": [[9, 10, "TaskName", "quantization"]]}, {"text": "( ODbL Licensed ) under the any - to - one ( A2O ) setting .", "entities": []}, {"text": "A2O VC aims to convert speech from any arbitrary speaker into that of a prede\ufb01ned target speaker .", "entities": []}, {"text": "We use the task to evaluate the speaker transferability as well as the generalizability of the SSL models .", "entities": [[16, 17, "DatasetName", "SSL"]]}, {"text": "We use 60 utterances from the target speaker that spans 5 minutes for training , and 25 utterances for testing that span 2 minutes .", "entities": []}, {"text": "No validation set was used .", "entities": []}, {"text": "We use the commonly used mel - cepstrum distortion ( MCD ) , word error rate ( WER ) and automatic speaker veri\ufb01cation ( ASV ) accept rate from off - the - shelf ASR and ASV models as evaluation metrics .", "entities": [[13, 19, "MetricName", "word error rate ( WER )"]]}, {"text": "The downstream model is trained to reconstruct the acoustic feature from the upstream representations in a target - speaker - dependent manner .", "entities": []}, {"text": "In the conversion phase , given the representations extracted by the upstream , the model generates the converted acoustic features , which are then sent to a neural vocoder to synthesize the converted waveform .", "entities": []}, {"text": "We adopted Tacotron2 ( Shen et al . , 2018 ) as the downstream model , which is an autoregressive network consisting of convolutional and LSTM layers .", "entities": [[2, 3, "MethodName", "Tacotron2"], [25, 26, "MethodName", "LSTM"]]}, {"text": "For the neural vocoder , we used the Hi\ufb01-GAN ( Kong et al . , 2020 ) .", "entities": []}, {"text": "We follow an implementation described in ( Huang et al . , 2021b ) .", "entities": []}, {"text": "3.1.4 Speech Separation Speech separation ( SS ) is the task of separating target speech from background interference ( Wang and Chen , 2018 ) .", "entities": [[1, 3, "TaskName", "Speech Separation"], [3, 5, "TaskName", "Speech separation"]]}, {"text": "It is an important step in speech processing , especially for noisy and multispeaker scenarios .", "entities": []}, {"text": "We investigate the speech sep - aration problem on a dataset simulated from LibriSpeech ( Cosentino et al . , 2020 ) ( CC BY 4.0 Licensed ) and WHAM !", "entities": [[13, 14, "DatasetName", "LibriSpeech"], [29, 31, "DatasetName", "WHAM !"]]}, {"text": "( Wichern et al . , 2019 ) ( CC BY - NC 4.0 Licensed ) noise .", "entities": []}, {"text": "We use 16kHz version of the dataset containing 2 speakers , and focus on themix_clean condition .", "entities": []}, {"text": "The train and evaluation sets contain 43.3 and 4.2 hours of speech simulated from LibriSpeech \u2019s train - clean-100 andtest - clean .", "entities": [[14, 15, "DatasetName", "LibriSpeech"]]}, {"text": "This task is used to evaluate the generative capability of SSL models when input is a mixture of acoustic signals .", "entities": [[10, 11, "DatasetName", "SSL"]]}, {"text": "We use the scale - invariant signalto - distortion ratio improvement ( SI - SDRi ) as the evaluation metric .", "entities": [[12, 15, "MetricName", "SI - SDRi"]]}, {"text": "For the downstream model , we use a 3 - layer BLSTM model with dimension of 896 for each direction to predict the short - time Fourier transform ( STFT ) masks for each speaker , and the predictions are transformed back to the time domain using inverse short - time Fourier transform ( iSTFT ) .", "entities": []}, {"text": "Permutation invariant training ( PIT ) ( Yu et al . , 2017 ) is performed to optimize the mean square error between the predicted mask and Ideal Non - negative Phase Sensitive Mask ( INPSM ) ( Erdogan et al . , 2015 ; Kolb\u00e6k et al . , 2017 ) .", "entities": [[4, 5, "DatasetName", "PIT"]]}, {"text": "We choose frequency domain method instead of a time domain based method because of the stride size constraint and computational cost .", "entities": []}, {"text": "3.1.5 Speech Enhancement Speech enhancement ( SE ) is the task of removing background noise from a degraded speech signal , and it aims to improve the perceived quality and intelligibility of the signal .", "entities": [[1, 3, "TaskName", "Speech Enhancement"], [3, 5, "TaskName", "Speech enhancement"]]}, {"text": "We include this8482", "entities": []}, {"text": "UpstreamST OOD - ASR VC SS SE BLEU\u00d2 WER\u00d3 MCD\u00d3WER\u00d3 ASV\u00d2 SI - SDRi\u00d2 PESQ\u00d2STOI\u00d2 FBANK 2.32 63.58 8.47 38.3 77.25 9.23 2.55 93.6 PASE+ 3.16 61.56 8.66 30.6 63.20 9.87 2.56 93.9 APC 5.95 63.12 8.05 27.2 87.25 8.92 2.56 93.4 VQ - APC 4.23 63.56 7.84 22.4 94.25 8.44 2.56 93.4 NPC 4.32 61.66 7.86 30.4 94.75 8.04 2.52 93.1 Mockingjay 4.45 65.27 8.29 35.1 79.75 9.29 2.53 93.4 TERA 5.66 58.49 8.21 25.1 83.75 10.19 2.54 93.6 DeCoAR 2.0 9.94 53.62 7.83 17.1 90.75 8.54 2.47 93.2", "entities": [[24, 25, "MethodName", "PASE+"]]}, {"text": "Modi\ufb01ed CPC 4.82 62.54 8.41 26.2 71.00 10.40 2.57 93.7", "entities": []}, {"text": "wav2vec 6.61 55.86 7.45 10.1 98.25 9.30 2.53 93.8 vq - wav2vec 5.66 60.66 7.08 13.4 100.00 8.16 2.48 93.6 wav2vec 2.0 Base 14.81 46.95 7.50 10.5 98.00 9.77 2.55 93.9 wav2vec 2.0", "entities": []}, {"text": "Large 12.48 44.69 7.63 15.8 97.25 10.02 2.52 94.0 HuBERT Base 15.53 46.69 7.47 8.0 98.50 9.36 2.58 93.9 HuBERT Large 20.01 44.08 7.22 9.0 99.25 10.45 2.64 94.2 Table 2 : Evaluating various SSL representations on new semantic and generative downstream tasks .", "entities": [[34, 35, "DatasetName", "SSL"]]}, {"text": "\u00d2indicates the higher the better and \u00d3indicates the lower the better .", "entities": []}, {"text": "The complete results of OOD - ASR are in Appendix A. task to evaluate the generative capability under noisy conditions .", "entities": []}, {"text": "In SUPERB - SG , we discuss the speech enhancement problem on the V oicebankDEMAND ( Veaux et al . , 2013 ) ( CC BY 4.0 Licensed ) corpus .", "entities": [[8, 10, "TaskName", "speech enhancement"]]}, {"text": "The train , validation , and test sets contain 8.8 , 0.6 and 0.6 hours of speech respectively .", "entities": []}, {"text": "Our evaluation metrics are Perceptual Evaluation of Speech Quality ( PESQ ) and ShortTime Objective Intelligibility ( STOI ) .", "entities": []}, {"text": "For the downstream model , we follow the mask - based speech enhancement pipeline in ( Kolb\u00e6k et al . , 2017 ) .", "entities": [[11, 13, "TaskName", "speech enhancement"]]}, {"text": "A 3 - layer BLSTM model similar to the speech separation task is trained to predict the spectral mask for the clean signal .", "entities": [[9, 11, "TaskName", "speech separation"]]}, {"text": "The mean square error between the predicted mask and INPSM is used as the objective .", "entities": []}, {"text": "3.2 Self - supervised Models We evaluate the tasks on 15 upstream models , which are PASE+ ( Ravanelli et al . , 2020 ) , APC ( Chung et al . , 2019 ) , VQ - APC ( Chung et al . , 2020 ) , NPC ( Liu et al . , 2020a ) , Mockingjay ( Liu et al . , 2020c ) , TERA ( Liu et al . , 2020b ) , DeCoAR 2.0 ( Ling and Liu , 2020 ) , Modi\ufb01le CPC ( Rivi\u00e8re et al . , 2020 ) , wav2vec family ( Schneider et al . , 2019 ) ( Baevski et al . , 2020a ) ( Baevski et al . , 2020b ) and HuBERT ( Hsu et al . , 2021 ) .", "entities": [[16, 17, "MethodName", "PASE+"]]}, {"text": "They span across different architectures , sizes and learning objectives .", "entities": []}, {"text": "Some models also use vector quantization which has an added bene\ufb01t of signal compression .", "entities": [[5, 6, "TaskName", "quantization"]]}, {"text": "For grounding , we use Log Mel Filterbank as our baseline .", "entities": []}, {"text": "The detailed properties of upstream mod - els are shown in Table 1 . 4 Experimental Setup Following SUPERB , we \ufb01x upstream models parameters for all downstream tasks \u2019 training .", "entities": []}, {"text": "We extract the frame - level representations for each hidden layer of the upstream models from raw waveform , and use a trainable task - speci\ufb01c weightedsum mechanism to summarize all layers \u2019 representations into a sequence of vectors .", "entities": []}, {"text": "The summarized representations are then used as the downstream model \u2019s input .", "entities": []}, {"text": "An overview of the training procedure is demonstrated in Figure 1 .", "entities": []}, {"text": "Each experiment is done by one single run with the same seed .", "entities": []}, {"text": "This procedure is consistent for all experiments , offering a fair and simple evaluation strategy for all upstream models .", "entities": []}, {"text": "5 Results and Discussion 5.1 Main result The results of the upstream models evaluated on SUPERB - SG are shown in Table 2 .", "entities": []}, {"text": "We only report the averaged WER for OOD - ASR .", "entities": []}, {"text": "Full results can be found in Appendix A.", "entities": []}, {"text": "For speech - to - text tasks ( ST and OOD - ASR ) , wav2vec 2.0 and HuBERT offer competitive results , while DeCoAR 2.0 shows some improvements .", "entities": []}, {"text": "In speech generation tasks ( VC , SS , and SE ) , FBANK yields comparable or superior performance than some SSL models , especially for those metrics that take the quality of the output signal into account .", "entities": [[21, 22, "DatasetName", "SSL"]]}, {"text": "For VC , the 3 reported metrics have the same trend for respective8483", "entities": []}, {"text": "ST OOD - ASR VC ( MCD ) VC ( WER ) VC ( ASV ) SS SE ( PESQ ) SE ( STOI )", "entities": []}, {"text": "PR ASR SID ASV", "entities": [[2, 3, "DatasetName", "SID"]]}, {"text": "IC ER ST OOD - ASR VC ( MCD ) VC ( WER ) VC ( ASV ) SS SE ( PESQ ) SE ( STOI )", "entities": []}, {"text": "PR ASR SID ASV", "entities": [[2, 3, "DatasetName", "SID"]]}, {"text": "IC ER1.0.86.74.85.69.38.10.51.89.92.86.75.92.83 .861.0.69.83.65.46.02.66.86.92.79.70.83.79", "entities": []}, {"text": ".74.691.0.90.98-.08-.15 .31.84.75.61.66.87.63", "entities": []}, {"text": ".85.83.901.0.85.24.13.54.91.85.75.70.90.79", "entities": []}, {"text": ".69.65.98.851.0-.13-.11", "entities": []}, {"text": ".30.79.71.58.66.84.57 .38.46-.08", "entities": []}, {"text": ".24-.13", "entities": []}, {"text": "1.0.52.78.23.40.31.04.15.37 .10.02-.15 .13-.11 .521.0.46.08-.01", "entities": []}, {"text": ".21.10.05.25 .51.66.31.54.30.78.461.0.52.54.42.41.44.53", "entities": []}, {"text": ".89.86.84.91.79.23.08.521.0.89.89.85.98.94 .92.92.75.85.71.40-.01 .54.891.0.83.71.88.83", "entities": []}, {"text": ".86.79.61.75.58.31.21.42.89.831.0.81.88.88", "entities": []}, {"text": ".75.70.66.70.66.04.10.41.85.71.811.0.85.81 .92.83.87.90.84.15.05.44.98.88.88.851.0.89 .83.79.63.79.57.37.25.53.94.83.88.81.891.0", "entities": []}, {"text": "0.00.20.40.60.81.0Figure 2 : Spearman \u2019s \u001abetween tasks .", "entities": []}, {"text": "models .", "entities": []}, {"text": "Here , vq - wav2vec achieves the best performance on MCD and ASV , while HuBERT performs the best on WER .", "entities": []}, {"text": "For SS , Hubert - Large achieves the best performance , followed by Modi\ufb01ed CPC .", "entities": []}, {"text": "PASE+ , which is pre - trained with denoising tasks , performs better than half the SSL models , but this observation does n\u2019t transfer to the other tasks .", "entities": [[0, 1, "MethodName", "PASE+"], [8, 9, "TaskName", "denoising"], [16, 17, "DatasetName", "SSL"]]}, {"text": "For SE , all upstream models perform comparably .", "entities": []}, {"text": "The largest gap is only 0.17 in PESQ and 1.1 in STOI .", "entities": []}, {"text": "Overall , no model outperforms all others on all tasks .", "entities": []}, {"text": "However , HuBERT - Large performs most competitively on all downstream tasks , especially those requiring linguistic and semantic signals .", "entities": []}, {"text": "5.2 Correlation between tasks We analyze the correlations between tasks in SUPERB - SG to understand the similarity between tasks , and verify if the experimental results agree with the common understanding of related tasks based on shared representation they require .", "entities": []}, {"text": "To compute the correlation , we \ufb01rst change all metrics into a higher - better manner .", "entities": []}, {"text": "Then , we compute the Spearman \u2019s rank correlation coef\ufb01cients ( Spearman \u2019s \u001a ) between all pairs of tasks .", "entities": []}, {"text": "For multiple metrics contained in a single task , such as MCD / WER / ASV in VC as well as PESQ / STOI in SE , we compute each of them separately .", "entities": []}, {"text": "To make our analysis more representative and generalized to all speech domains , we bring back the six tasks from SUPERB ( Yang et al . , 2021 ) that are considered representative of the following four domains : ( i ) Content recognition tasks contain(A ) ST ( A ) OOD - ASR ( A ) PR ( A ) VC ( WER ) ( A ) ASR ( A ) IC ( B ) SID ( B ) ASV ( B ) ER ( C ) VC ( MCD ) ( C ) VC ( ASV ) ( D ) SS ( E ) SE ( PESQ ) ( F ) SE (", "entities": [[76, 77, "DatasetName", "SID"]]}, {"text": "STOI ) ST ( A ) OOD - ASR ( A ) PR ( A ) VC ( WER ) ( A ) ASR ( A ) IC ( A ) SID ( B ) ASV ( B ) ER ( B ) VC ( MCD ) ( C ) VC ( ASV ) ( C ) SS ( D ) SE ( PESQ ) ( E ) SE ( STOI ) ( F)1.0.86.89.85.92.92.86.75.83.74.69.38.10.51 .861.0.86.83.92.83.79.70.79.69.65.46.02.66 .89.861.0.91.89.98.89.85.94.84.79.23.08.52 .85.83.911.0.85.90.75.70.79.90.85.24.13.54", "entities": [[31, 32, "DatasetName", "SID"]]}, {"text": ".92.92.89.851.0.88.83.71.83.75.71.40-.01.54 .92.83.98.90.881.0.88.85.89.87.84.15.05.44", "entities": []}, {"text": ".86.79.89.75.83.881.0.81.88.61.58.31.21.42 .75.70.85.70.71.85.811.0.81.66.66.04.10.41 .83.79.94.79.83.89.88.811.0.63.57.37.25.53", "entities": []}, {"text": ".74.69.84.90.75.87.61.66.631.0.98-.08-.15.31 .69.65.79.85.71.84.58.66.57.981.0-.13-.11.30", "entities": []}, {"text": ".38.46.23.24.40.15.31.04.37-.08-.131.0.52.78 .10.02.08.13-.01.05.21.10.25-.15-.11.521.0.46", "entities": []}, {"text": ".51.66.52.54.54.44.42.41.53.31.30.78.461.0", "entities": []}, {"text": "0.00.20.40.60.81.0Figure", "entities": []}, {"text": "3 : Spearman \u2019s \u001abetween tasks rearranged by clustering result .", "entities": []}, {"text": "ing Phoneme Recognition ( PR ) , Automatic Speech Recognition ( ASR ) ( ii ) Speaker identity tasks including Identi\ufb01cation ( SID ) , Automatic Speaker Veri\ufb01cation ( ASV ) ( iii ) Semantics task which is Intent Classi\ufb01cation ( IC ) and ( iv ) Prosodic task which is Emotion Recognition ( ER ) .", "entities": [[7, 10, "TaskName", "Automatic Speech Recognition"], [22, 23, "DatasetName", "SID"], [51, 53, "TaskName", "Emotion Recognition"]]}, {"text": "Together with the 5 tasks introduced in this paper , we show the results of total 11 downstream tasks with the 14 corresponding metrics in Figure 2 .", "entities": []}, {"text": "Overall , results show that all tasks except SS and SE have strong positive correlation among them .", "entities": []}, {"text": "One possible explanation for SS and SE not showing strong correlation is that the low - level information closely related to audio signals is more critical as they need to reconstruct clean speech from interfering speakers and background noise by estimating the STFT masks .", "entities": []}, {"text": "As a result , high - level information extracted from SSL models has little bene\ufb01t for these tasks but is helpful for other tasks .", "entities": [[10, 11, "DatasetName", "SSL"]]}, {"text": "As noted earlier , there is only a small gap in performance between FBANK and SSL models .", "entities": [[15, 16, "DatasetName", "SSL"]]}, {"text": "If we leave SS and SE out , all correlation coef\ufb01cients are greater than 0.58 , showing that the SSL model representations are useful for multiple domains .", "entities": [[19, 20, "DatasetName", "SSL"]]}, {"text": "Although the Spearman \u2019s \u001aare large in general in Figure 2 , differences between tasks are observable .", "entities": []}, {"text": "Here , we focus on the relation between correlation and similarity of tasks .", "entities": []}, {"text": "We list the most and the least two correlated tasks comparing with ST , OOD - ASR , VC , SS , and SE .", "entities": []}, {"text": "SS and SE are skipped as candidates for for the least correlated8484", "entities": []}, {"text": "Tasks Top 2 Last 2 STASR ( 0.92)IC ( 0.92)ASV ( 0.75)VC ( 0.76 ) OOD - ASRASR ( 0.92)PR ( 0.86)ASV ( 0.70)VC ( 0.72 ) VCPR ( 0.84)ASR ( 0.77)SID ( 0.64)ER ( 0.66 )", "entities": []}, {"text": "SSSE ( 0.65)OOD - ASR ( 0.46)VC ( 0.01)ASV ( 0.04 ) SESS ( 0.65)ER ( 0.39)VC ( 0.17)IC ( 0.25 ) Table 3 : Top 2 and last 2 tasks correlated with the \ufb01ve SUPERB - SG tasks ranked by Spearman \u2019s \u001a. Cluster Metrics AST , OOD - ASR , PR VC ( WER ) , ASR , IC B SID , ASV , ER C VC ( MCD ) , VC ( ASV ) D SS E SE ( PESQ ) F SE ( STOI ) Table 4 : K - means clustering result based on the correlation between each downstream tasks .", "entities": [[62, 63, "DatasetName", "SID"], [92, 96, "MethodName", "K - means clustering"]]}, {"text": "tasks since they dominate the results .", "entities": []}, {"text": "For VC , we average the correlation coef\ufb01cients across the three metrics .", "entities": []}, {"text": "The results are shown in Table 3 . ST and OOD - ASR are highly correlated with ASR since they both transform speech signals into discrete text tokens .", "entities": []}, {"text": "IC is also correlated with ST since semantic information is required to perform both tasks .", "entities": []}, {"text": "Moreover , ASV and VC are the least correlated tasks since they primarily focus on the speaker information with lesser regard to the semantic content .", "entities": []}, {"text": "However , the absolute correlation values are still larger than 0.7 .", "entities": []}, {"text": "For VC , the speaker information needs to be removed while the content has to be kept , similar to PR and ASR but different from SID .", "entities": [[26, 27, "DatasetName", "SID"]]}, {"text": "SS and SE are correlated with each other and have a much lower correlation with speaker identity and semantics tasks , supporting our assumption .", "entities": []}, {"text": "Overall , we \ufb01nd that empirically highly - correlated tasks require similar knowledge or understanding ability .", "entities": []}, {"text": "To give a broader view of our correlation results , we further cluster the downstream tasks by their correlation with each other using K - means .", "entities": []}, {"text": "In this way , all the tasks are considered simultaneously , and the grouping is driven automatically by the empirical correlation results .", "entities": []}, {"text": "If more than one metric are used in a downstream task , we cluster them independently .", "entities": []}, {"text": "The clustering results are shown in Table 4 and a rearranged correlation map is shown in Figure 3 .", "entities": []}, {"text": "The result shows that the clusters of the tasks align with our empirical knowledge .", "entities": []}, {"text": "Cluster A includes tasks that require content information , while tasks in cluster B are more sensitive to speaker and prosodic features .", "entities": []}, {"text": "Cluster C contains metrics MCD and ASV of VC , which are used to evaluate the signal quality and the rates of speaker transfer .", "entities": []}, {"text": "It is worth noting that WER in VC belongs to cluster A , showing that it is more similar to content - related tasks .", "entities": []}, {"text": "Furthermore , clusters D , E , and F each contain one of the metrics in SS and SE , aligning with our assumption that these tasks utilize different types of information compared to other tasks .", "entities": []}, {"text": "With the analysis of the correlation between tasks , we empirically con\ufb01rm the reliability of the results , and show that we increase the heterogeneity among speech tasks over SUPERB .", "entities": []}, {"text": "We further discover shared properties between tasks with clustering , and the result is aligned with our common understanding of related tasks .", "entities": []}, {"text": "5.3 Robustness of SUPERB - SG To study the impact of downstream model architecture and the data sizes used in SUPERB - SG we evaluate the robustness of SUPERB - SG with variations in downstream model as well as training data size , and show that our conclusions still hold true .", "entities": []}, {"text": "We choose ST , OOD - ASR and SS as the downstream tasks for evaluation with an aim to cover semantic , content recognition , and generative task types .", "entities": []}, {"text": "For the upstream models , FBANK , TERA , CPC , wav2vec 2.0 Base and HuBERT Base are used to cover different SSL algorithms .", "entities": [[22, 23, "DatasetName", "SSL"]]}, {"text": "5.3.1 Downstream model For each task , 2 additional downstream architectures are created by modifying the number of layers and the hidden dimensions compared to our default setting .", "entities": [[16, 19, "HyperparameterName", "number of layers"]]}, {"text": "We create small andlarge models that are roughly the half and twice of default in terms of the number of trainable parameters .", "entities": []}, {"text": "A detailed comparison of the downstream architectures is shown in Table 5 .", "entities": []}, {"text": "The results are shown in Table 6 .", "entities": []}, {"text": "We show that the ranking of the upstream models is almost \ufb01xed when the model sizes are varied .", "entities": []}, {"text": "As expected , the small architecture has worse perfor-8485", "entities": []}, {"text": "ArchitectureST OOD - ASR SS architecture # params architecture # params architecture # params default3 - layer encoder 3 - layer decoder Transformer ( dim 512)28.8M2 - layer BLSTM ( dim 1024)53.4M3 - layer BLSTM ( dim 896)51.4 M smallno encoder 1 - layer decoder Transformer ( dim 512)10.9 M ( \u00020.38)1 - layer BLSTM ( dim 1024)24.1 M ( \u00020.45)2 - layer BLSTM ( dim 768)24.4 M ( \u00020.47 ) large12 - layer encoder 6 - layer decoder Transformer ( dim 512)69.8 M ( \u00022.42)4 - layer BLSTM ( dim 1024)112.2 M ( \u00022.10)4 - layer BLSTM ( dim 1152)114.50 M ( \u00022.23 ) Table 5 : A detailed comparison of downstream model architectures .", "entities": [[7, 8, "MetricName", "params"], [10, 11, "MetricName", "params"], [13, 14, "MetricName", "params"], [22, 23, "MethodName", "Transformer"], [45, 46, "MethodName", "Transformer"], [79, 80, "MethodName", "Transformer"]]}, {"text": "We report the number of trainable parameters when using TERA as upstream model while minor difference ( < 10 % ) exists due to different upstream dimensions .", "entities": []}, {"text": "For OOD - ASR , we average values across all sub - tasks since sub - tasks have different vocabulary sizes .", "entities": []}, {"text": "UpstreamST OOD - ASR SS BLEU\u00d2 WER\u00d3 SI - SDRi\u00d2 default FBANK 2.32 63.58 9.23 TERA 5.66 58.49 10.19 Modi\ufb01ed CPC 4.82 62.54 10.40 wav2vec 2.0 Base 14.81 46.95 9.77 HuBERT Base 15.53 46.69 9.36 small FBANK 0.58 70.86 8.19 TERA 1.84 64.80 9.20 Modi\ufb01ed CPC 1.44 67.83 9.56 wav2vec 2.0 Base 8.55 50.75 8.83 HuBERT Base 9.24 50.32 8.73 large FBANK 3.02 60.49 9.77 TERA 6.64 57.95 ( \u009c)10.87 Modi\ufb01ed CPC 4.56 59.73 ( \u009d ) 10.61 wav2vec 2.0 Base 16.81 ( \u009c)45.61 9.86 HuBERT Base 17.59 ( \u009d ) 45.78 9.83 Table 6 : Results on SS , ST , OOD - ASR when using different architectures .", "entities": []}, {"text": "\u009cand\u009dare used to denote the rank changing .", "entities": []}, {"text": "The complete results of OOD - ASR are in Appendix A. mance than default , while large has better .", "entities": []}, {"text": "Moreover , the scores causing the change in ranking are negligible , e.g. , TERA / CPC in SS and wav2vec 2.0 Base / HuBERT Base in OOD - ASR with large .", "entities": []}, {"text": "The results show that the relative performance achieved by different upstream models is agnostic to the downstream architecture , con\ufb01rming the robustness of the framework used in SUPERB - SG .", "entities": []}, {"text": "5.3.2 Training data size To study the effect of data size , we create 3 pseudo datasets per task by sub - sampling 10 % , 5 % andPartition STOOD - ASRSS es zh ar spon Train 100 % 425.80 21.44 31.05 30.39 11.43 43.27 10 % 42.58 2.15 3.11 3.04 1.14 4.34 5 % 25.91 1.07 1.56 1.52 0.57 2.17 1 % 4.26 0.22 0.31 0.31 0.12 0.43 Dev 25.91 1.19 14.41 12.24 1.59 1.52 Test 24.51 0.62 15.32 12.46 2.15 4.19 Table 7 : Hours of data in pseudo datasets .", "entities": []}, {"text": "1 % from the original training set while \ufb01xing the validation and test sets .", "entities": []}, {"text": "The statistics of the datasets are shown in Table 7 , and the results are in Table 8 .", "entities": []}, {"text": "The ranking of the upstream models remains almost the same for 10 % of training data .", "entities": []}, {"text": "When that is further reduced to 5 % , there is a change in ranking in SS due to a performance drop in Modi\ufb01ed CPC .", "entities": []}, {"text": "Excluding Modi\ufb01ed CPC , the ranking is still \ufb01xed showing that the relative performance of the upstream models is agnostic to data size .", "entities": []}, {"text": "Furthermore , when using only 1 % of training data , most of the SSL models fail on the 3 downstream tasks .", "entities": [[14, 15, "DatasetName", "SSL"]]}, {"text": "This phenomenon is caused by insuf\ufb01cient task - speci\ufb01c knowledge due to limited training data size .", "entities": []}, {"text": "Although SSL models learn highlevel representations from the unlabeled speech signal , acquisition of task - speci\ufb01c knowledge such as translingual ability in ST , text - level token mapping in OOD - ASR , and mask prediction in SS , requires non - trivial supervision .", "entities": [[1, 2, "DatasetName", "SSL"]]}, {"text": "We note that fewer training examples speeds training up but sacri\ufb01ces the evaluation quality.8486", "entities": []}, {"text": "UpstreamST OOD - ASR SS BLEU \u00d2 WER\u00d3 SI - SDRi \u00d2 100 % FBANK 2.32 63.58 9.23 TERA 5.66 58.49 10.19 Modi\ufb01ed CPC 4.82 62.54 10.40 wav2vec 2.0 Base 14.81 46.95 9.77 HuBERT Base 15.53 46.69 9.36 10 % FBANK 0.46 85.39 5.65 TERA ( \u009d ) 0.88 80.32 ( \u009c)6.72 Modi\ufb01ed CPC ( \u009c ) 1.30 85.32 ( \u009d ) 6.59 wav2vec 2.0 Base 5.04 63.85 6.45 HuBERT Base 5.57 63.43 6.13 5 % FBANK 0.27 89.70 4.52 TERA 0.44 86.95 ( \u009c1)5.59 Modi\ufb01ed CPC 0.37 87.97 ( \u009d3 ) 4.95 wav2vec 2.0 Base 2.91 69.88 ( \u009c1 ) 5.36 HuBERT Base 3.35 69.33 ( \u009c1 ) 5.03 1 % FBANK 0.03 99.53 2.29 TERA 0.04 98.31 3.24 Modi\ufb01ed CPC 0.03 98.37 ( \u009d3 ) 2.87 wav2vec 2.0 Base 0.33 92.46 ( \u009c2)3.34 HuBERT Base 0.38 92.17 ( \u009c1 ) 3.01 Table 8 : Results on ST , OOD - ASR and SS when using different amount of training data .", "entities": [[5, 6, "MetricName", "BLEU"], [8, 11, "MetricName", "SI - SDRi"]]}, {"text": "\u009cand\u009dare used to denote the rank changing .", "entities": []}, {"text": "The complete results of OOD - ASR are in Appendix A. Overall , we show the robustness of SUPERB - SG to variations in data size even when the training data is reduced to 5 % , showing the reliability of the benchmark .", "entities": [[41, 43, "DatasetName", "the benchmark"]]}, {"text": "6 Conclusion We introduce SUPERB - SG , a set of 5 new tasks that include speech translation , out - of - domain ASR , voice conversion , speech separation , and speech enhancement to evaluate the deep semantic and generative capabilities of SSL models .", "entities": [[26, 28, "TaskName", "voice conversion"], [29, 31, "TaskName", "speech separation"], [33, 35, "TaskName", "speech enhancement"], [44, 45, "DatasetName", "SSL"]]}, {"text": "We evaluate 15 SSL models , and do a comprehensive analysis of the task correlations to demonstrate the reliability of our methodology .", "entities": [[3, 4, "DatasetName", "SSL"]]}, {"text": "We test and con\ufb01rm the robustness of SUPERB - SG in terms of the downstream model architecture as well as the training data size .", "entities": []}, {"text": "The latest introduction of the semantic and generative tasks increases the diversity and dif\ufb01culty of SUPERB , which can boost a more comprehensive understanding of the capability of various SSL models \u2019 representations , and help researchers discover the hidden properties of SSLtechniques in development .", "entities": [[29, 30, "DatasetName", "SSL"]]}, {"text": "We have open - sourced all the codes1and released a challenge3to encourage further research of SSL in speech .", "entities": [[15, 16, "DatasetName", "SSL"]]}, {"text": "We welcome the community to participate and advance the research frontier together .", "entities": []}, {"text": "Ethics This work fully adheres to the ACL code of ethics .", "entities": [[0, 1, "DatasetName", "Ethics"]]}, {"text": "For more details , we provide a checklist in Appendix B. References David \u00c1lvarez et al . 2019 .", "entities": []}, {"text": "Problem - agnostic speech embeddings for multi - speaker text - to - speech with samplernn .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "10th ISCA Speech Synthesis Workshop , pages 35\u201339 .", "entities": [[2, 4, "TaskName", "Speech Synthesis"]]}, {"text": "Rosana Ardila , Megan Branson , Kelly Davis , Michael Kohler , Josh Meyer , Michael Henretty , Reuben Morais , Lindsay Saunders , Francis Tyers , and Gregor Weber .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Common voice :", "entities": [[0, 2, "DatasetName", "Common voice"]]}, {"text": "A massivelymultilingual speech corpus .", "entities": []}, {"text": "In Proceedings of the 12th Language Resources and Evaluation Conference , pages 4218\u20134222 .", "entities": []}, {"text": "Alexei Baevski , Steffen Schneider , and Michael Auli . 2020a .", "entities": []}, {"text": "vq - wav2vec : Self - supervised learning of discrete speech representations .", "entities": [[4, 8, "TaskName", "Self - supervised learning"]]}, {"text": "In ICLR .", "entities": []}, {"text": "Alexei Baevski , Yuhao Zhou , Abdelrahman Mohamed , and Michael Auli .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "wav2vec 2.0 : A framework for self - supervised learning of speech representations .", "entities": [[6, 10, "TaskName", "self - supervised learning"]]}, {"text": "In NeurIPS .", "entities": []}, {"text": "Heng - Jui Chang , Shu - wen Yang , and Hung - yi Lee . 2021 .", "entities": []}, {"text": "DistilHuBERT : Speech representation learning by layer - wise distillation of hidden - unit BERT .", "entities": [[3, 5, "TaskName", "representation learning"], [14, 15, "MethodName", "BERT"]]}, {"text": "arXiv preprint arXiv:2110.01900 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yu - An Chung , Wei - Ning Hsu , Hao Tang , and James Glass .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "An Unsupervised Autoregressive Model for Speech Representation Learning .", "entities": [[6, 8, "TaskName", "Representation Learning"]]}, {"text": "In Interspeech , pages 146\u2013150 .", "entities": []}, {"text": "Yu - An Chung , Hao Tang , and James Glass . 2020 .", "entities": []}, {"text": "Vector - quantized autoregressive predictive coding .", "entities": []}, {"text": "InInterspeech , pages 3760\u20133764 .", "entities": []}, {"text": "Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , Francisco Guzm\u00e1n , Edouard Grave , Myle Ott , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unsupervised cross - lingual representation learning at scale .", "entities": [[4, 6, "TaskName", "representation learning"]]}, {"text": "Joris Cosentino , Manuel Pariente , Samuele Cornell , Antoine Deleforge , and Emmanuel Vincent .", "entities": [[7, 8, "DatasetName", "Cornell"]]}, {"text": "2020 .", "entities": []}, {"text": "Librimix : An open - source dataset for generalizable speech separation .", "entities": [[0, 1, "DatasetName", "Librimix"], [9, 11, "TaskName", "speech separation"]]}, {"text": "arXiv preprint arXiv:2005.11262 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "3https://superbbenchmark.org8487", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In NAACL , pages 4171\u20134186 .", "entities": []}, {"text": "Li Dong , Nan Yang , Wenhui Wang , Furu Wei , Xiaodong Liu , Yu Wang , Jianfeng Gao , Ming Zhou , and Hsiao - Wuen Hon . 2019 .", "entities": []}, {"text": "Uni\ufb01ed language model pre - training for natural language understanding and generation .", "entities": [[7, 10, "TaskName", "natural language understanding"]]}, {"text": "arXiv preprint arXiv:1905.03197 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "John W Du Bois , Wallace L Chafe , Charles Meyer , Sandra A Thompson , and Nii Martey . 2000 \u2013 2005 .", "entities": []}, {"text": "Santa Barbara corpus of spoken American English .", "entities": []}, {"text": "CD - ROM .", "entities": []}, {"text": "Philadelphia : Linguistic Data Consortium .", "entities": []}, {"text": "Hakan Erdogan , John R Hershey , Shinji Watanabe , and Jonathan Le Roux .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Phase - sensitive and recognition - boosted speech separation using deep recurrent neural networks .", "entities": [[7, 9, "TaskName", "speech separation"]]}, {"text": "In 2015 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pages 708\u2013712 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Solene Evain , Ha Nguyen , Hang Le , Marcely Zanon Boito , Salima Mdhaffar , Sina Alisamir , Ziyi Tong , Natalia Tomashenko , Marco Dinarelli , Titouan Parcollet , Alexandre Allauzen , Yannick Esteve , Benjamin Lecouteux , Francois Portet , Solange Rossato , Fabien Ringeval , Didier Schwab , and Laurent Besacier . 2021 .", "entities": []}, {"text": "Lebenchmark :", "entities": []}, {"text": "A reproducible framework for assessing self - supervised representation learning from speech .", "entities": [[8, 10, "TaskName", "representation learning"]]}, {"text": "Zhiyun Fan , Meng Li , Shiyu Zhou , and Bo Xu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Exploring wav2vec 2.0 on speaker veri\ufb01cation and language identi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:2012.06185 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Alex Graves , Santiago Fern\u00e1ndez , Faustino Gomez , and J\u00fcrgen Schmidhuber .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Connectionist temporal classi\ufb01cation : labelling unsegmented sequence data with recurrent neural networks .", "entities": []}, {"text": "In Proceedings of the 23rd international conference on Machine learning , pages 369\u2013376 .", "entities": []}, {"text": "Joel Hestness , Sharan Narang , Newsha Ardalani , Gregory Diamos , Heewoo Jun , Hassan Kianinejad , Md. Mostofa Ali Patwary , Yang Yang , and Yanqi Zhou . 2017 .", "entities": []}, {"text": "Deep learning scaling is predictable , empirically .", "entities": []}, {"text": "arXiv preprint arXiv:1712.00409 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural computation , 9(8):1735\u20131780 .", "entities": []}, {"text": "Wei - Ning Hsu , Benjamin Bolte , Yao - Hung Hubert Tsai , Kushal Lakhotia , Ruslan Salakhutdinov , and Abdelrahman Mohamed .", "entities": [[17, 18, "DatasetName", "Ruslan"]]}, {"text": "2021 .", "entities": []}, {"text": "Hubert : Self - supervised speech representation learning by masked prediction of hidden units .", "entities": [[6, 8, "TaskName", "representation learning"]]}, {"text": "arXiv preprint arXiv:2106.07447 .W.-C.", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Huang , Y .-C.", "entities": []}, {"text": "Wu , T. Hayashi , and T. Toda .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "Any - to - One Sequence - to - Sequence V oice Conversion using Self - Supervised Discrete Speech Representations .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "ICASSP , pages 5944 \u2013 5948 .", "entities": []}, {"text": "Wen - Chin Huang , Shu - Wen Yang , Tomoki Hayashi , Hung - Yi Lee , Shinji Watanabe , and Tomoki Toda . 2021b .", "entities": []}, {"text": "S3prl - vc : Open - source voice conversion framework with self - supervised speech representations .", "entities": [[7, 9, "TaskName", "voice conversion"]]}, {"text": "arXiv preprint arXiv:2110.06280 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rafal Jozefowicz , Oriol Vinyals , Mike Schuster , Noam Shazeer , and Yonghui Wu . 2016 .", "entities": []}, {"text": "Exploring the limits of language modeling .", "entities": []}, {"text": "arXiv preprint arXiv:1602.02410 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Morten Kolb\u00e6k , Dong Yu , Zheng - Hua Tan , and Jesper Jensen . 2017 .", "entities": []}, {"text": "Multitalker speech separation with utterance - level permutation invariant training of deep recurrent neural networks .", "entities": [[1, 3, "TaskName", "speech separation"]]}, {"text": "IEEE / ACM Transactions on Audio , Speech , and Language Processing , 25(10):1901\u20131913 .", "entities": [[2, 3, "DatasetName", "ACM"]]}, {"text": "J. Kong , J. Kim , and J. Bae . 2020 .", "entities": []}, {"text": "HiFi - GAN : Generative Adversarial Networks for Ef\ufb01cient and High Fidelity Speech Synthesis .", "entities": [[0, 3, "MethodName", "HiFi - GAN"], [12, 14, "TaskName", "Speech Synthesis"]]}, {"text": "In Proc .", "entities": []}, {"text": "NeurIPS , volume 33 , pages 17022\u201317033 .", "entities": []}, {"text": "Cheng - I Lai , Yung - Sung Chuang , Hung - Yi Lee , ShangWen Li , and James Glass . 2021 .", "entities": []}, {"text": "Semi - supervised spoken language understanding via self - supervised speech and language model pretraining .", "entities": [[3, 6, "TaskName", "spoken language understanding"]]}, {"text": "In ICASSP .", "entities": []}, {"text": "Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , and Radu Soricut .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Albert :", "entities": []}, {"text": "A lite bert for self - supervised learning of language representations .", "entities": [[4, 8, "TaskName", "self - supervised learning"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Ves Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bart : Denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension .", "entities": [[2, 3, "TaskName", "Denoising"]]}, {"text": "ACL .", "entities": []}, {"text": "Yist Y Lin , Chung - Ming Chien , Jheng - Hao Lin , Hungyi Lee , and Lin - shan Lee . 2020 .", "entities": []}, {"text": "Fragmentvc : Anyto - any voice conversion by end - to - end extracting and fusing \ufb01ne - grained voice fragments with attention .", "entities": [[5, 7, "TaskName", "voice conversion"]]}, {"text": "arXiv preprint arXiv:2010.14150 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shaoshi Ling and Yuzong Liu . 2020 . DeCoAR 2.0 : Deep contextualized acoustic representations with vector quantization .", "entities": [[17, 18, "TaskName", "quantization"]]}, {"text": "arXiv preprint arXiv:2012.06659 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shaoshi Ling , Yuzong Liu , Julian Salazar , and Katrin Kirchhoff .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Deep contextualized acoustic representations for semi - supervised speech recognition .", "entities": [[8, 10, "TaskName", "speech recognition"]]}, {"text": "InICASSP 2020 - 2020 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pages 6429\u20136433 .", "entities": []}, {"text": "IEEE.8488", "entities": []}, {"text": "Alexander H Liu , Yu - An Chung , and James Glass . 2020a .", "entities": []}, {"text": "Non - autoregressive predictive coding for learning speech representations from local dependencies .", "entities": []}, {"text": "arXiv preprint arXiv:2011.00406 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Andy T Liu , Shang - Wen Li , and Hung - yi Lee . 2020b .", "entities": []}, {"text": "Tera : Self - supervised learning of transformer encoder representation for speech .", "entities": [[2, 6, "TaskName", "Self - supervised learning"]]}, {"text": "arXiv preprint arXiv:2007.06028 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Andy T. Liu , Shu - wen Yang , Po - Han Chi , Po - chun Hsu , and Hung - yi Lee . 2020c .", "entities": []}, {"text": "Mockingjay :", "entities": []}, {"text": "Unsupervised speech representation learning with deep bidirectional transformer encoders .", "entities": [[2, 4, "TaskName", "representation learning"]]}, {"text": "ICASSP .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "RoBERTa : A robustly optimized BERT pretraining approach .", "entities": [[0, 1, "MethodName", "RoBERTa"], [5, 6, "MethodName", "BERT"]]}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Manon Macary , Marie Tahon , Yannick Est\u00e8ve , and Anthony Rousseau . 2021 .", "entities": []}, {"text": "On the use of selfsupervised pre - trained acoustic and linguistic features for continuous speech emotion recognition .", "entities": [[14, 17, "TaskName", "speech emotion recognition"]]}, {"text": "In 2021 IEEE Spoken Language Technology Workshop ( SLT ) , pages 373\u2013380 . IEEE .", "entities": []}, {"text": "Dhruv Mahajan , Ross Girshick , Vignesh Ramanathan , Kaiming He , Manohar Paluri , Yixuan Li , Ashwin Bharambe , and Laurens van der Maaten .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Exploring the limits of weakly supervised pretraining .", "entities": []}, {"text": "InProceedings of the European Conference on Computer Vision ( ECCV ) .", "entities": []}, {"text": "Tu Anh Nguyen et al . 2020 .", "entities": []}, {"text": "The zero resource speech benchmark 2021 : Metrics and baselines for unsupervised spoken language modeling .", "entities": []}, {"text": "In NeurIPS Workshop on Self - Supervised Learning for Speech and Audio Processing .", "entities": [[4, 8, "TaskName", "Self - Supervised Learning"]]}, {"text": "Vassil Panayotov , Guoguo Chen , Daniel Povey , and Sanjeev Khudanpur .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Librispeech : an asr corpus based on public domain audio books .", "entities": [[0, 1, "DatasetName", "Librispeech"]]}, {"text": "In 2015 IEEE international conference on acoustics , speech and signal processing ( ICASSP ) , pages 5206\u20135210 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In NAACL , pages 2227\u20132237 .", "entities": []}, {"text": "Matt Post . 2018 .", "entities": []}, {"text": "A call for clarity in reporting BLEU scores .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 186 \u2013 191 , Belgium , Brussels .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J. Liu . 2020 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the limitsof transfer learning with a uni\ufb01ed text - to - text transformer .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mirco Ravanelli , Jianyuan Zhong , Santiago Pascual , Pawel Swietojanski , Joao Monteiro , Jan Trmal , and Yoshua Bengio .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multi - task self - supervised learning for robust speech recognition .", "entities": [[3, 7, "TaskName", "self - supervised learning"], [8, 11, "TaskName", "robust speech recognition"]]}, {"text": "In ICASSP , pages 6989\u20136993 .", "entities": []}, {"text": "Morgane Rivi\u00e8re , Armand Joulin , Pierre - Emmanuel Mazar\u00e9 , and Emmanuel Dupoux .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Unsupervised pretraining transfers well across languages .", "entities": []}, {"text": "In ICASSP , pages 7414\u20137418 .", "entities": []}, {"text": "Steffen Schneider , Alexei Baevski , Ronan Collobert , and Michael Auli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "wav2vec :", "entities": []}, {"text": "Unsupervised pre - training for speech recognition .", "entities": [[0, 4, "TaskName", "Unsupervised pre - training"], [5, 7, "TaskName", "speech recognition"]]}, {"text": "In Interspeech .", "entities": []}, {"text": "Noam Shazeer , Azalia Mirhoseini , Krzysztof Maziarz , Andy Davis , Quoc Le , Geoffrey Hinton , and Jeff Dean . 2017 .", "entities": []}, {"text": "Outrageously large neural networks : The sparsely - gated mixture - of - experts layer .", "entities": []}, {"text": "arXiv preprint arXiv:1701.06538 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "J. Shen , R. Pang , R. J. Weiss , M. Schuster , N. Jaitly , Z. Yang , Z. Chen , Y .", "entities": []}, {"text": "Zhang , Y .", "entities": []}, {"text": "Wang , R. SkerryRyan , R. A. Saurous , Y .", "entities": []}, {"text": "Agiomyrgiannakis , and Y .", "entities": []}, {"text": "Wu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Natural TTS Synthesis by Conditioning WaveNet on MEL Spectrogram Predictions .", "entities": [[5, 6, "MethodName", "WaveNet"]]}, {"text": "In Proc .", "entities": []}, {"text": "ICASSP , pages 4779\u20134783 .", "entities": []}, {"text": "A\u00e4ron van den Oord , Yazhe Li , and Oriol Vinyals .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Representation learning with contrastive predictive coding .", "entities": [[0, 2, "TaskName", "Representation learning"], [3, 6, "MethodName", "contrastive predictive coding"]]}, {"text": "CoRR , abs/1807.03748 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008 .", "entities": []}, {"text": "Christophe Veaux , Junichi Yamagishi , and Simon King .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "The voice bank corpus : Design , collection and data analysis of a large regional accent speech database .", "entities": []}, {"text": "In 2013 international conference oriental COCOSDA held jointly with 2013 conference on Asian spoken language research and evaluation ( OCOCOSDA / CASLRE ) , pages 1\u20134 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Changhan Wang , Anne Wu , and Juan Pino .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "CoVoST 2 : A massively multilingual speech - to - text translation corpus .", "entities": [[0, 1, "DatasetName", "CoVoST"], [6, 12, "TaskName", "speech - to - text translation"]]}, {"text": "DeLiang Wang and Jitong Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Supervised speech separation based on deep learning : An overview .", "entities": [[1, 3, "TaskName", "speech separation"]]}, {"text": "IEEE / ACM Transactions on Audio , Speech , and Language Processing , 26(10):1702 \u2013 1726 .", "entities": [[2, 3, "DatasetName", "ACM"]]}, {"text": "Gordon Wichern , Joe Antognini , Michael Flynn , Licheng Richard Zhu , Emmett McQuinn , Dwight Crow , Ethan Manilow , and Jonathan Le Roux .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Wham ! :", "entities": [[0, 2, "DatasetName", "Wham !"]]}, {"text": "Extending speech separation to noisy environments .", "entities": [[1, 3, "TaskName", "speech separation"]]}, {"text": "arXiv preprint arXiv:1907.01160 .8489", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shu - wen Yang , Po - Han Chi , Yung - Sung Chuang , Cheng - I Jeff Lai , Kushal Lakhotia , Yist Y .", "entities": []}, {"text": "Lin , Andy T. Liu , Jiatong Shi , Xuankai Chang , GuanTing Lin , Tzu - Hsien Huang , Wei - Cheng Tseng , Ko tik Lee , Da - Rong Liu , Zili Huang , Shuyan Dong , Shang - Wen Li , Shinji Watanabe , Abdelrahman Mohamed , and Hung yi Lee . 2021 .", "entities": []}, {"text": "SUPERB :", "entities": []}, {"text": "Speech processing universal performance benchmark .", "entities": []}, {"text": "Interspeech .", "entities": []}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Carbonell , Ruslan Salakhutdinov , and Quoc V .", "entities": [[12, 13, "DatasetName", "Ruslan"]]}, {"text": "Le . 2019 .", "entities": []}, {"text": "XLNet :", "entities": [[0, 1, "MethodName", "XLNet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1906.08237 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Dong Yu , Morten Kolb\u00e6k , Zheng - Hua Tan , and Jesper Jensen . 2017 .", "entities": []}, {"text": "Permutation invariant training of deep models for speaker - independent multi - talker speech separation .", "entities": [[13, 15, "TaskName", "speech separation"]]}, {"text": "In 2017 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pages 241\u2013245 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Y .", "entities": []}, {"text": "Zhao , W.-C. Huang , X. Tian , J. Yamagishi , R. K. Das , T. Kinnunen , Z. Ling , and T. Toda .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "V oice Conversion Challenge 2020 - Intra - lingual semi - parallel and cross - lingual voice conversion - .", "entities": [[16, 18, "TaskName", "voice conversion"]]}, {"text": "InProc .", "entities": []}, {"text": "Joint Workshop for the BC and VCC 2020 , pages 80\u201398.8490", "entities": []}, {"text": "A Complete Out - of - domain ASR Results Here , we provide complete results of OOD - ASR tasks , as shown in Tables 9 , 10 , 11 .", "entities": []}, {"text": "All upstream models used in this paper are trained with English speech data , but we are also interested in multilingual pre - trained models in OOD - ASR .", "entities": []}, {"text": "Therefore , we evaluate the wav2vec 2.0 XLSR model on the OOD - ASR tasks , as shown in the last row of Table 9 .", "entities": [[7, 8, "MethodName", "XLSR"]]}, {"text": "XLSR has identical architecture as wav2vec 2.0 Large , but is trained with 56k hours of speech including 53 different languages .", "entities": [[0, 1, "MethodName", "XLSR"]]}, {"text": "The pre - training data of XLSR cover our cross - lingual tasks \u2019 training data .", "entities": [[6, 7, "MethodName", "XLSR"]]}, {"text": "As expected , using multilingual data improves OOD - ASR tasks and achieves the best performance among all upstream models .", "entities": []}, {"text": "Upstreames zh ar spon WER\u00d3 CER\u00d3 WER\u00d3WER\u00d3", "entities": []}, {"text": "A VG FBANK 54.03 35.44 72.07 92.78 63.58 PASE+ 52.11 35.52 70.47 88.15 61.56 APC 55.23 36.38 70.79 90.07 63.12 VQ - APC 55.32 37.06 71.56 90.29 63.56 NPC 51.07 35.85 69.87 89.86 61.66 Mockingjay 58.11 38.13 73.57 91.27 65.27 TERA 48.67 32.21 66.18 86.89 58.49 Modi\ufb01ed CPC 54.37 36.22 68.94 90.61 62.54 DeCoAR 2.0 43.18 28.77 61.00 81.53 53.62 wav2vec 46.16 31.69 60.85 84.72 55.86 vq - wav2vec 52.02 36.55 66.19 87.89 60.66 wav2vec 2.0 Base 37.85 26.44 55.95 67.55 46.95 wav2vec 2.0 Large 35.75:25.07:54.29:63.64:44.69 HuBERT Base 37.15 26.23 54.94 68.41 46.69 HuBERT Large 30.90 23.73:50.60;71.09;44.08 wav2vec 2.0 XLSR 26.90:22.97:49.63:63.05:40.64 : Table 9 : Results of OOD - ASR tasks , where spon denotes spontaneous speech.:Normalized across dimensionality of representation to stabilize training and ensure convergence.;Uses linear warmup of learning rates in the \ufb01rst 8k steps to stabilize training and ensure convergence .", "entities": [[8, 9, "MethodName", "PASE+"], [99, 100, "MethodName", "XLSR"], [127, 129, "MethodName", "linear warmup"]]}, {"text": "B Responsible NLP Research Checklist Here we answer the ethics questions to show our ethics statement .", "entities": []}, {"text": "B.1 Did you discuss the limitations of your work ?", "entities": []}, {"text": "Yes , we discussed the constrains on the frozen upstreams and simple task speci\ufb01c heads in abstract and Section 3 .", "entities": []}, {"text": "B.2 Did you discuss any potential risks of your work ?", "entities": []}, {"text": "Yes , in Section 5.3 , we discussed about the risks of the unstable benchmark results , and we showed theUpstreames zh ar spon WER\u00d3CER\u00d3WER\u00d3WER\u00d3 A VG default FBANK 54.03 35.44 72.07 92.78 63.58 TERA 48.67 32.21 66.18 86.89 58.49 Modi\ufb01ed CPC 54.37 36.22 68.94 90.61 62.54 wav2vec 2.0 Base 37.85 26.44 55.95 67.55 46.95 HuBERT Base 37.15 26.23 54.94 68.41 46.69 small FBANK 63.86 41.97 80.30 97.30 70.86 TERA 57.13 37.66 73.92 90.49 64.80 Modi\ufb01ed CPC 60.81 41.47 76.45 92.59 67.83 wav2vec 2.0 Base 41.84 30.22 61.72 69.23 50.75 HuBERT Base 41.45 29.68 59.93 70.21 50.32 large FBANK 46.39 37.71 65.35 92.52 60.49 TERA 45.41 37.40 64.48 84.53 57.95 Modi\ufb01ed CPC 48.70 35.16 69.15 85.93 59.73 wav2vec 2.0 Base 34.02 27.60 54.10 66.73 45.61 HuBERT Base 33.91 27.22 53.43 68.57 45.78 Table 10 : Complete results of OOD - ASR tasks with different model sizes .", "entities": []}, {"text": "Upstreames zh ar spon WER\u00d3CER\u00d3WER\u00d3WER\u00d3 A VG 100 % FBANK 54.03 35.44 72.07 92.78 63.58 TERA 48.67 32.21 66.18 86.89 58.49 Modi\ufb01ed CPC 54.37 36.22 68.94 90.61 62.54 wav2vec 2.0 Base 37.85 26.44 55.95 67.55 46.95 HuBERT Base 37.15 26.23 54.94 68.41 46.69 10 % FBANK 84.82 62.97 93.27 100.49 85.39 TERA 76.44 58.54 88.49 97.79 80.32 Modi\ufb01ed CPC 83.84 64.78 91.20 101.44 85.32 wav2vec 2.0 Base 61.26 43.50 72.98 77.65 63.85 HuBERT Base 58.08 42.94 72.78 79.94 63.43 5 % FBANK 89.48 71.99 96.69 100.65 89.70 TERA 83.98 71.04 93.15 99.62 86.95 Modi\ufb01ed CPC 88.61 67.61 95.71 99.93 87.97 wav2vec 2.0 Base 67.09 50.58 78.53 83.33 69.88 HuBERT Base 66.29 50.72 76.59 83.74 69.33 1 % FBANK 96.79 96.73 99.85 104.73 99.53 TERA 94.73 98.82 99.77 99.93 98.31 Modi\ufb01ed CPC 95.93 97.94 99.80 99.84 98.37 wav2vec 2.0 Base 82.00 94.38 92.41 101.06 92.46 HuBERT Base 82.36 94.34 90.37 101.60 92.17 Table 11 : Complete results of OOD - ASR tasks with different data sizes .", "entities": []}, {"text": "robustness of SUPERB - SG .", "entities": []}, {"text": "B.3 Do the abstract and introduction summarize the paper \u2019s main claims ?", "entities": []}, {"text": "Yes , the paper \u2019s main claims are summarized in abstract and Section 1.8491", "entities": []}, {"text": "B.4 Did you use or create scienti\ufb01c artifacts ?", "entities": []}, {"text": "Yes , we used public datasets and pre - trained models mentioned in Section 3 .", "entities": []}, {"text": "B.4.1 Did you cite the creators of artifacts you used ?", "entities": []}, {"text": "Yes , we cited those artifacts properly in Section 3 .", "entities": []}, {"text": "B.4.2 Did you discuss the license or terms for use and/or distribution of any artifacts ?", "entities": []}, {"text": "Yes , the licenses of the artifacts are clearly indicated in Section 3 .", "entities": []}, {"text": "B.4.3 Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided that it was speci\ufb01ed ?", "entities": []}, {"text": "For the artifacts you create , do you specify intended use and whether that is compatible with the original access conditions ( in particular , derivatives of data accessed for research purposes should not be used outside of research contexts ) ?", "entities": []}, {"text": "Yes , we use the of\ufb01cial implementations of the upstream models in Table 1 and followed their public API to access the models .", "entities": []}, {"text": "For the datasets , we also follow their licenses .", "entities": []}, {"text": "B.4.4 Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identi\ufb01es individual people oroffensive content , and the steps taken to protect / anonymize it ?", "entities": []}, {"text": "No , there were no data collection involved in this work .", "entities": []}, {"text": "We used the widely - used public datasets and followed the common data preprocessing steps .", "entities": []}, {"text": "B.4.5 Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ?", "entities": []}, {"text": "Yes , the properties of the artifacts were indicated in Section 3 .", "entities": []}, {"text": "B.4.6 Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ?", "entities": []}, {"text": "Yes , the relevant statistics were reported in Section 3.ST", "entities": []}, {"text": "OOD - ASR VC SS SE Steps 32k 500k 10k 150k 150k Time 25hr", "entities": []}, {"text": "36hr 4hr 48hr 72hr GPU 3090 V100 3090 1080", "entities": []}, {"text": "Ti 1080 Ti Table 12 : Training steps , time and GPU devices used by each task when using HuBERT Base as upstream .", "entities": []}, {"text": "NVIDIA ReForce RTX 3090 , NVIDIA Tesla V100 and NVIDIA GeForce GTX", "entities": []}, {"text": "1080", "entities": []}, {"text": "Ti are denoted as 3090 , V100 and 1080", "entities": []}, {"text": "Ti respectively .", "entities": []}, {"text": "B.5 Did you run computational experiments ?", "entities": []}, {"text": "Yes .", "entities": []}, {"text": "B.5.1 Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , andcomputing infrastructure used ?", "entities": [[5, 8, "HyperparameterName", "number of parameters"]]}, {"text": "We reported the number of the parameters in Section 5.3.1 .", "entities": []}, {"text": "The computational budget and computing infrastructures are reported in Table 12 .", "entities": []}, {"text": "B.5.2 Did you discuss the experimental setup , including hyperparameter search and best - found hyperparameter values ?", "entities": []}, {"text": "No , we did n\u2019t do the hyperparameter searching in a uni\ufb01ed way .", "entities": []}, {"text": "Some hyperparameters came from the of\ufb01cial implementation or related works and some were searched by ourselves .", "entities": []}, {"text": "However , the hyperparameters we used are public available1 .", "entities": []}, {"text": "B.5.3 Did you report descriptive statistics about your results ( e.g. , error bars around results , summary statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean , etc . or just a single run ?", "entities": []}, {"text": "Yes , we indicated that in Section 4 .", "entities": []}, {"text": "B.5.4 If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE , etc . ) ?", "entities": []}, {"text": "Yes , we reported them in Section 3 .", "entities": []}, {"text": "B.6 Did you use human annotators ( e.g. , crowdworkers ) or research with human subjects ?", "entities": []}, {"text": "No.8492", "entities": []}]