[{"text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 2890 - 2903 May 22 - 27 , 2022 c", "entities": []}, {"text": "2022 Association for Computational Linguistics BRIO : Bringing Order to Abstractive Summarization Yixin Liu1 , Pengfei Liu2 , Dragomir Radev1 , Graham Neubig2 1Yale University,2Carnegie Mellon University { yixin.liu,dragomir.radev}@yale.edu,{pliu3,gneubig}@cs.cmu.edu", "entities": [[11, 12, "TaskName", "Summarization"]]}, {"text": "Abstract Abstractive summarization models are commonly trained using maximum likelihood estimation , which assumes a deterministic ( onepoint ) target distribution in which an ideal model will assign all the probability mass to the reference summary .", "entities": [[2, 3, "TaskName", "summarization"]]}, {"text": "This assumption may lead to performance degradation during inference , where the model needs to compare several system - generated ( candidate ) summaries that have deviated from the reference summary .", "entities": []}, {"text": "To address this problem , we propose a novel training paradigm which assumes a non - deterministic distribution so that different candidate summaries are assigned probability mass according to their quality .", "entities": []}, {"text": "Our method achieves a new state - of - the - art result on the CNN / DailyMail ( 47.78 ROUGE-1 ) and XSum ( 49.07 ROUGE-1 ) datasets .", "entities": [[20, 21, "MetricName", "ROUGE-1"], [23, 24, "DatasetName", "XSum"], [26, 27, "MetricName", "ROUGE-1"]]}, {"text": "Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.1 1 Introduction Neural methods for abstractive summarization ( Rush et al . , 2015 ;", "entities": [[28, 29, "TaskName", "summarization"]]}, {"text": "Nallapati et al . , 2016 ; Chopra et al . , 2016 ; Lewis et al . , 2020 ; Zhang et al . , 2020 ) formulate summarization as a sequenceto - sequence ( Seq2Seq ) problem ( Sutskever et al . , 2014 ) , learning to generate the summary in an autoregressive manner .", "entities": [[29, 30, "TaskName", "summarization"], [36, 37, "MethodName", "Seq2Seq"]]}, {"text": "Such models are commonly trained with maximum likelihood estimation ( MLE ) , maximizing predictive probability of the reference output given the gold sub - sequence before it .", "entities": []}, {"text": "However , during inference the model must also generate the output based on possibly erroneous previous steps .", "entities": []}, {"text": "This can hurt model performance , a phenomenon often called exposure bias ( Bengio et al . , 2015 ;", "entities": []}, {"text": "Ranzato et al . , 2016 ) .", "entities": []}, {"text": "To maintain reasonable performance even in the case of a sub - sequence with errors , we argue that the 1We have made our code , results , and trained models publicly available at https://github.com/yixinL7/BRIO.System R-1 R-2 R - L Acc.(% )", "entities": []}, {"text": "High 53.99 29.85 51.12 100.00 Low 33.48 10.85 30.45 0.00 BART 44.88 21.68 41.92 54.80 Ours 50.10 26.29 47.19 79.63 Table 1 : Accuracy of different abstractive summarization systems w.r.t ranking the quality of candidate summaries on CNNDM dataset .", "entities": [[10, 11, "MethodName", "BART"], [23, 24, "MetricName", "Accuracy"], [27, 28, "TaskName", "summarization"]]}, {"text": "Acc . stands for the frequency of the model assigning higher probabilities to better candidate summaries .", "entities": [[0, 1, "MetricName", "Acc"]]}, {"text": "The candidate summaries are generated by a pre - trained model ( BART ) , and we select the best and the worst candidates ( w.r.t .", "entities": [[12, 13, "MethodName", "BART"]]}, {"text": "ROUGE scores ) for each of the samples .", "entities": []}, {"text": "High andLow represent the average performance of the best and worst candidates respectively .", "entities": []}, {"text": "R-1/2 / L are the ROUGE-1/2 / L scores .", "entities": []}, {"text": "The original BART only achieves 54.80 % accuracy .", "entities": [[2, 3, "MethodName", "BART"], [7, 8, "MetricName", "accuracy"]]}, {"text": "model must accurately estimate relative quality of different generated outputs , since effective inference requires comparison among these candidates .", "entities": []}, {"text": "To understand whether existing models can accurately perform such relative comparisons , we conducted a preliminary study on pre - trained BART ( Lewis et al . , 2020 ) , \ufb01rst generating two candidate summaries from the model and observing whether a higher probability is assigned to the candidate with a higher ROUGE ( Lin , 2004 ) score .", "entities": [[21, 22, "MethodName", "BART"]]}, {"text": "As Tab . 1 shows , the accuracy is far from ideal .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "This is likely due to the fact that MLE training only encourages the model to assign high probability to the reference summary , and is agnostic about any relative comparison between non - reference summaries .", "entities": []}, {"text": "However , we argue that it is also important for the order of model scores to be coordinated with the actual quality metrics by which the summaries will be evaluated \u2013 higher model scores should indicate better quality summaries .", "entities": []}, {"text": "In the following we will refer to models that have such scores as \u201c coordinated \u201d for conciseness .", "entities": []}, {"text": "We introduce a training paradigm which requires the abstractive model to be able to be accurate with respect to predicting the tokens in the reference summaries and coordinated with respect to2890", "entities": []}, {"text": "Encoder Decoder \ud835\udc91\ud835\udfcf\ud835\udc91\ud835\udfd0\ud835\udc91\ud835\udfd1\ud835\udc91\ud835\udfd2 Source Input Reference Output \u2112\ud835\udc40\ud835\udc3f\ud835\udc38 EncoderDecoder \ud835\udc91\ud835\udfcf(\ud835\udc83)\ud835\udc91\ud835\udfd0(\ud835\udc83)\ud835\udc91\ud835\udfd1(\ud835\udc83)\ud835\udc91\ud835\udfd2(\ud835\udc83 )", "entities": []}, {"text": "Source InputCandidate Output \ud835\udc35 Decoder\ud835\udc91\ud835\udfcf(\ud835\udc82)\ud835\udc91\ud835\udfd0(\ud835\udc82)\ud835\udc91\ud835\udfd1(\ud835\udc82)\ud835\udc91\ud835\udfd2(\ud835\udc82 )", "entities": []}, {"text": "Candidate Output \ud835\udc34\u2112\ud835\udc36\ud835\udc61\ud835\udc5f\ud835\udc40(\ud835\udc34 ) \ud835\udc40(\ud835\udc35)\u2212>Seq2Seq Generation Model Reference -free Evaluation Model\ud835\udf03 \ud835\udf03Figure 1 : Comparison of MLE loss ( LMLE ) and the contrastive loss ( LCtr ) in our method .", "entities": [[17, 18, "MetricName", "loss"], [24, 25, "MetricName", "loss"]]}, {"text": "MLE assumes a deterministic ( one - point ) distribution , in which the reference summary receives all the probability mass .", "entities": []}, {"text": "Our method assumes a nondeterministic distribution in which system - generated summaries also receive probability mass according to their quality .", "entities": []}, {"text": "The contrastive loss encourages the order of model - predicted probabilities of candidate summaries to be coordinated with the actual quality metric Mby which the summaries will be evaluated .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "We assign the abstractive model a dual role \u2013 a single model could be used both as a generation model and a reference - free evaluation model .", "entities": []}, {"text": "thecandidate summaries .", "entities": []}, {"text": "In other words , we give the abstractive model a dual role : as a generation model , it generates the output summaries in an autoregressive way ; as an evaluation model , it can be used to score the quality of candidate summaries by estimating a probability distribution over candidate outputs .", "entities": []}, {"text": "The generation model is trained using the standard MLE loss , but to train the evaluation model we introduce a contrastive loss ( Hadsell et al . , 2006 ) de\ufb01ned over different candidate summaries generated by pre - trained abstractive models ( Fig . 1 ) , following previous work on ranking - based or contrastive learning ( Hopkins and May , 2011 ; Zhong et al . , 2020 ; Liu et al . , 2021b ) .", "entities": [[9, 10, "MetricName", "loss"], [21, 22, "MetricName", "loss"], [56, 58, "MethodName", "contrastive learning"]]}, {"text": "Our main contribution is to change the target distribution of abstractive models from a one - point deterministic distribution assumed by MLE training to a non - deterministic distribution in which candidate summaries are also assigned probability mass according to their quality .", "entities": []}, {"text": "The new SOTA performance on CNN / DailyMail ( Hermann et al . , 2015 ) and XSum ( Narayan et al . , 2018 ) datasets demonstrated the effectiveness of our method .", "entities": [[17, 18, "DatasetName", "XSum"]]}, {"text": "Our in - depth analysis also found that the abstractive models trained using our method can estimate the candidate summary quality more accurately , in concert with the the objective of our training paradigm.2 Neural Abstractive Summarization The goal of abstractive summarization is to create a functiongthat takes a source document Dand generates an appropriate summary S S g(D ) ( 1 ) Training Objective Neural abstractive summarization models aim to learn a neural model gthat results in good summaries .", "entities": [[36, 37, "TaskName", "Summarization"], [41, 42, "TaskName", "summarization"], [67, 68, "TaskName", "summarization"]]}, {"text": "Maximum likelihood estimation ( MLE ) is the standard training algorithm .", "entities": []}, {"text": "It aims to maximize the likelihood of the reference summary S\u0003 , i.e. , \u0012\u0003= argmax \u0012X ilogpg\u0012(S\u0003(i)jD(i);\u0012 ) ( 2 ) where\u0012denotes the parameters of gandpg\u0012denotes the probability distribution entailed by these parameters .", "entities": []}, {"text": "The summation is over the training set andfD(i);S\u0003(i)gis thei - th training sample .", "entities": []}, {"text": "For a speci\ufb01c sample fD(i);S\u0003(i)g , Eq . 2 is equivalent to minimizing the sum of negative loglikelihoods of the tokens fs\u0003", "entities": []}, {"text": "1;\u0001\u0001\u0001;s\u0003 j;\u0001\u0001\u0001;s\u0003 lgin the reference summary S\u0003whose length is l , which is the cross - entropy loss : Lxent= \u0000lX j=1X sptrue(sjD;S\u0003", "entities": [[17, 18, "MetricName", "loss"]]}, {"text": "< j ) logpg\u0012(sjD;S\u0003 < j;\u0012)(3 ) whereS\u0003 < jdenotes the partial reference sequence fs\u0003 0;\u0001\u0001\u0001;s\u0003", "entities": []}, {"text": "j\u00001gands\u0003 0is a pre - de\ufb01ned start token .", "entities": []}, {"text": "ptrueis a one - hot distribution under the standard MLE framework : ptrue(sjD;S\u0003 < j ) =(", "entities": []}, {"text": "1s = s\u0003 j 0s6 = s\u0003 j(4 )", "entities": []}, {"text": "In practice , label smoothing ( Szegedy et al . , 2016 ) is a widely used and effective technique that modi\ufb01es the target distribution in Eq . 4 to a \" soft \" label by assigning probability mass \f to other tokens : ptrue(sjD;S\u0003 < j ) =( 1\u0000 \f  s = s\u0003 j \f  N\u00001s6", "entities": [[3, 5, "MethodName", "label smoothing"]]}, {"text": "=", "entities": []}, {"text": "s\u0003 j(5 ) whereNis the size of the dictionary .", "entities": []}, {"text": "Inference and Exposure Bias During inference , the abstractive model gis used to generate the candidate summary in an autoregressive manner .", "entities": []}, {"text": "It is intractable to enumerate all the possible candidate outputs , so in practice methods such as beam search are used to reduce the search space.2891", "entities": []}, {"text": "One important step in search is estimating the probability of the next word stgiven the previous predicted sequence S < t : pg\u0012(stjD;S < t;\u0012 ) ( 6 ) Comparing Eq . 6 with Eq .", "entities": []}, {"text": "3 , the major difference is that during inference the model makes new predictions based on its own previous predictions S < t instead of the referenceS\u0003 < t.", "entities": []}, {"text": "As a result , even if the generation model gachieves very high accuracy w.r.t .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "Eq .", "entities": []}, {"text": "3 , once S < tstarts to deviate from S\u0003 , there is the risk that the performance of gwill signi\ufb01cantly degrade .", "entities": []}, {"text": "This problem has been identi\ufb01ed as the exposure bias ( Bengio et al . , 2015 ) .", "entities": []}, {"text": "3 Coordinating Abstractive Models Eq . 6 implies that the abstractive model gshould be able to assign higher estimated probability to the better candidate summary during inference .", "entities": []}, {"text": "However , this intuition is not directly captured in the standard MLE objective used in training \u2013 a model obtaining zero MLE loss would assign zero probability to any candidate summary different from the reference .", "entities": [[22, 23, "MetricName", "loss"]]}, {"text": "This is obviously improper for any task where multiple reasonable generations may exist ( Khayrallah et al . , 2020 ) , and also does not say anything about the ordering of two imperfect references .", "entities": []}, {"text": "We therefore advocate for making the alternative assumption that the probability of one candidate should be well - correlated with its quality as evaluated by an automatic metric M. Since it is intractable to enumerate all the possible candidate outputs , we only require our model to be able to accurately predict the ranking order of a set of the most probable candidate summaries ^S , which are its own beam search results .", "entities": []}, {"text": "In order to achieve this objective , we slightly modify the conditions of Eq . 5 , maintaining the general functional form , but instead specifying the marginal probability of the non - reference candidates Sto be \f , and encouraging coordination of probabilities and qualities among non - reference candidates as follows : 8 > > > < > > > : ptruey(SjD ) = 1\u0000 \f  S = S\u0003 P S2Sptruey(SjD )", "entities": []}, {"text": "= \f  S6 = S\u0003 ptruey(SijD)>ptruey(SjjD)8Si;Sj2^S ; M(Si)>M ( Sj)(7 )", "entities": []}, {"text": "We next describe precisely how we encourage coordination through contrastive learning .", "entities": [[9, 11, "MethodName", "contrastive learning"]]}, {"text": "Contrastive Learning for Coordination The candidate quality measure Mcan be de\ufb01ned inmany ways .", "entities": [[0, 2, "MethodName", "Contrastive Learning"]]}, {"text": "In this work we de\ufb01ne it as the ROUGE ( Lin , 2004 ) score of a candidate summary Sigiven the reference summary S\u0003. To coordinate a pre - trained abstractive model , we 1 ) use it to generate different candidate summaries with various levels of quality,2then 2 ) encourage the model to assign higher estimated probabilities to better candidates by \ufb01ne - tuning the model with a contrastive loss , following the previous work ( Hopkins and May , 2011 ; Zhong et al . , 2020 ):", "entities": [[70, 71, "MetricName", "loss"]]}, {"text": "Lctr = X iX j > imax(0;f(Sj)\u0000f(Si )", "entities": []}, {"text": "+ \u0015ij )", "entities": []}, {"text": "( 8) whereSiandSjare two different candidate summaries and ROUGE ( Si;S\u0003)>ROUGE ( Sj;S\u0003 ) , 8i;j;i < j .\u0015ijis the margin multiplied by the difference in rank between the candidates , i.e. , \u0015ij= ( j\u0000i)\u0003\u0015.f(Si)is the length - normalized estimated log - probability3 f(S )", "entities": []}, {"text": "= Pl t=1logpg\u0012(stjD;S < t;\u0012 ) jSj \u000b ( 9 ) where \u000b is the length penalty hyperparameter .", "entities": []}, {"text": "This loss gives the abstractive model a dual purpose , \ufb01rst as a reference - free evaluation model , which can be used in a two - stage summarization pipeline , where it is used to score the candidates generated by a pre - trained generation model and select the \ufb01nal output from them .", "entities": [[1, 2, "MetricName", "loss"], [28, 29, "TaskName", "summarization"]]}, {"text": "However , since the autoregressive generation depends on both the token - level prediction accuracy and sequencelevel coordination , the model \ufb01ne - tuned with the contrastive loss alone can no longer be used as a generation model .", "entities": [[14, 15, "MetricName", "accuracy"], [27, 28, "MetricName", "loss"]]}, {"text": "Multi - task Fine - tuning Following Edunov et al .", "entities": []}, {"text": "( 2018 ) , we combine the contrastive ( Eq . 8) and cross - entropy ( Eq . 3 ) losses to preserve the generation ability of the pre - trained abstractive model : Lmul = Lxent+", "entities": []}, {"text": "Lctr ( 10 ) where", "entities": []}, {"text": "is the weight of the contrastive loss .", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "We note that the contrastive and the cross - entropy loss can effectively complement each other \u2013 since the contrastive loss is de\ufb01ned on the sequence level , the token - level cross - entropy loss serves as a normalization to ensure that the model could assign balanced probability mass across the whole sequence .", "entities": [[10, 11, "MetricName", "loss"], [20, 21, "MetricName", "loss"], [35, 36, "MetricName", "loss"]]}, {"text": "2This is achieved by using diverse beam search ( Vijayakumar et al . , 2018 ) .", "entities": []}, {"text": "3We length - normalize as it is standard in comparing hypotheses in neural sequence generation ( Cho et al . , 2014).2892", "entities": []}, {"text": "4 Related Work Training Methods of Seq2Seq Models In order to align the training objective and evaluation metric , structured losses have been used for the Seq2Seq model training .", "entities": [[6, 7, "MethodName", "Seq2Seq"], [26, 27, "MethodName", "Seq2Seq"]]}, {"text": "Among them , marginbased losses ( Herbrich et al . , 1999 ; Taskar et al . , 2004 ; Gimpel and Smith , 2010 ) , which require the model to assign higher probability to the better output , are a major category .", "entities": []}, {"text": "Many margin - based losses used in modern seq2seq models ( Wiseman and Rush , 2016 ; Edunov et al . , 2018 ) assume a deterministic ( one - point ) distribution : a model can achieve zero loss if it can assign a much higher probability to the ( pseudo)-reference , regardless of relative comparisons of other candidate summaries .", "entities": [[8, 9, "MethodName", "seq2seq"], [39, 40, "MetricName", "loss"]]}, {"text": "By contrast , our method has a non - deterministic assumption ( Eq . 7 ) , which focuses on the pair - wise ranking of a set of candidate summaries .", "entities": []}, {"text": "One main challenge of directly optimizing a Seq2Seq model with quality scores of the output is that the discrete sampling process makes the loss non - differentiable .", "entities": [[7, 8, "MethodName", "Seq2Seq"], [23, 24, "MetricName", "loss"]]}, {"text": "To circumvent this problem , reinforcement learning has been used to reformulate the conditional text generation tasks ( Ranzato et al . , 2016 ;", "entities": [[13, 16, "TaskName", "conditional text generation"]]}, {"text": "Bahdanau et al . , 2016 ; Li et al . , 2016 ; Paulus et al . , 2018 ; Li et al . , 2019 ) .", "entities": []}, {"text": "Compared to this school of methods , our method is based on supervised learning , and it is more stable and less sensitive to the design choices ( e.g. reward shaping ) , which are well - known challenges of reinforcement learning methods .", "entities": []}, {"text": "Minimum risk training ( Shen et al . , 2016 ;", "entities": []}, {"text": "Wieting et al . , 2019 ) and other online sampling based methods ( Bengio et al . , 2015 ; Norouzi et al . , 2016 ; Zhang et al . , 2019 ) belong to another school of methods used to circumvent the problem of non - differentiability .", "entities": []}, {"text": "However , they also exhibit similar problems of stability as reinforcement learning .", "entities": []}, {"text": "Contrastive Learning Recently , contrastive learning ( Hadsell et al . , 2006 ) has been introduced into several conditional text generation tasks , such as machine translation ( Yang et al . , 2019 ; Pan et al . , 2021 ) , text summarization ( Cao and Wang , 2021 ; Xu et", "entities": [[0, 2, "MethodName", "Contrastive Learning"], [4, 6, "MethodName", "contrastive learning"], [19, 22, "TaskName", "conditional text generation"], [26, 28, "TaskName", "machine translation"], [44, 46, "TaskName", "text summarization"]]}, {"text": "al . , 2021 ; Sun and Li , 2021 ) , and other tasks ( Uehara et al . , 2020 ; Cho et al . , 2021 ; Lee et al . , 2021b ) .", "entities": []}, {"text": "Among these application scenarios , most work deployed contrastive learning in the latent representation space , following the framework proposed in Chen et al .", "entities": [[8, 10, "MethodName", "contrastive learning"]]}, {"text": "( 2020 ) .", "entities": []}, {"text": "However , in this work we adopt contrastive learning over thediscrete space of the generated texts .", "entities": [[7, 9, "MethodName", "contrastive learning"]]}, {"text": "Besides , instead of constructing the contrastive learning examples by rule - based methods ( e.g. perturbing the reference output ) , we use the generation models to construct the examples , which makes the contrastive learning task closer to the generation task .", "entities": [[6, 8, "MethodName", "contrastive learning"], [35, 37, "MethodName", "contrastive learning"]]}, {"text": "Sun and Li ( 2021 ) also adopted contrastive learning on the generated texts .", "entities": [[8, 10, "MethodName", "contrastive learning"]]}, {"text": "However , their formulation belongs to the margin - based losses .", "entities": []}, {"text": "We have discussed the difference between our method and the margin - based losses in the previous paragraphs .", "entities": []}, {"text": "Discriminative Reranking Discriminative reranking has been widely studied for conditional generation tasks ( Shen et al . , 2004 ;", "entities": []}, {"text": "Och et al . , 2004 ; Wan et al . , 2015 ; Mizumoto and Matsumoto , 2016 ) .", "entities": []}, {"text": "Some recent works ( Liu and Liu , 2021 ; Lee et al . , 2021a ) have also explored discriminative reranking of candidates from neural natural language generation models , which adopt large pre - trained language models ( e.g. BERT ( Devlin et al . , 2019 ) ) as the reranker .", "entities": [[41, 42, "MethodName", "BERT"]]}, {"text": "In this work , we factorize the Seq2Seq model ( e.g. , BART ) trained on the same dataset as the reranking model , which maximizes the parameter sharing across two stages .", "entities": [[7, 8, "MethodName", "Seq2Seq"], [12, 13, "MethodName", "BART"]]}, {"text": "Besides , our approach contributes an instance of leveraging large pre - trained Seq2Seq models as a quality estimation model ( Yuan et al . , 2021 ) .", "entities": [[13, 14, "MethodName", "Seq2Seq"]]}, {"text": "5 Experiments 5.1 Experimental Settings Datasets", "entities": []}, {"text": "We mainly use three datasets in our experiments ( statistics in Appendix A ) .", "entities": []}, {"text": "CNNDM4(Hermann et al . , 2015 ) is a large scale news dataset .", "entities": []}, {"text": "Following Nallapati et al .", "entities": []}, {"text": "( 2016 ) , we treat the news articles as the source documents and the associated highlights as the summaries .", "entities": []}, {"text": "XSum5(Narayan et", "entities": []}, {"text": "al . , 2018 ) is a highly abstractive dataset of articles from the British Broadcasting Corporation ( BBC ) .", "entities": []}, {"text": "NYT6(Sandhaus , 2008 ) contains articles from the New York Times and the associated summaries .", "entities": []}, {"text": "We follow Kedzie et al .", "entities": []}, {"text": "( 2018 ) for data preprocessing and splitting , and use the associated archival abstracts as the summaries .", "entities": []}, {"text": "Baselines We choose a variety of related models with strong performance as baselines .", "entities": []}, {"text": "BART ( Lewis et al . , 2020 ) and PEGASUS ( Zhang et al . , 2020 ) are both large pre - trained Seq2Seq LMs standard in the literature .", "entities": [[0, 1, "MethodName", "BART"], [25, 26, "MethodName", "Seq2Seq"]]}, {"text": "GSum ( Dou et al . , 4https://cs.nyu.edu/~kcho/DMQA/ 5https://github.com/EdinburghNLP/XSum 6https://catalog.ldc.upenn.edu/LDC2008T192893", "entities": []}, {"text": "2021 ) is built on BART , and improves performance by using additional guidance from an extractive summarizer .", "entities": [[5, 6, "MethodName", "BART"]]}, {"text": "SimCLS ( Liu and Liu , 2021 ) introduces a two - stage framework where the pre - trained BART model is used to generate candidates and a pre - trained RoBERTa ( Liu et al . , 2019 ) model is \ufb01ne - tuned as an evaluation model to score the candidate summaries and select from them .", "entities": [[19, 20, "MethodName", "BART"], [31, 32, "MethodName", "RoBERTa"]]}, {"text": "It achieves state - of - the - art performance on both CNNDM and XSum .GOLD", "entities": [[14, 15, "DatasetName", "XSum"]]}, {"text": "( Pang and He , 2021 ) uses of\ufb02ine reinforcement learning to train the BART model by treating the reference summaries as the demonstrations , a different formulation that can also improve the performance of the original BART .", "entities": [[14, 15, "MethodName", "BART"], [37, 38, "MethodName", "BART"]]}, {"text": "SeqCo ( Xu et al . , 2021 ) and ConSum ( Sun and Li , 2021 ) are two recent methods that aim to leverage contrastive learning to improve the performance of the abstractive summarization model ( BART ) .", "entities": [[26, 28, "MethodName", "contrastive learning"], [35, 36, "TaskName", "summarization"], [38, 39, "MethodName", "BART"]]}, {"text": "Implementation Details", "entities": []}, {"text": "In the following experiments , we use either BART or PEGASUS as a backbone .", "entities": [[8, 9, "MethodName", "BART"]]}, {"text": "We label our proposed methods BRIO , with two variants : ( 1 ) BRIO - Ctr is \ufb01ne - tuned with the contrastive loss ( Eq . 8) only ; ( 2 ) BRIOMul is \ufb01ne - tuned with the multi - task loss ( Eq . 10 ) .", "entities": [[24, 25, "MetricName", "loss"], [44, 45, "MetricName", "loss"]]}, {"text": "We use BRIO - Ctr as an evaluation model that scores different candidate summaries generated by a Seq2Seq abstractive model and selects the \ufb01nal output from them , and BRIO - Mul as a standard Seq2Seq model that takes the source documents as input and generates the output in an autoregressive manner .", "entities": [[17, 18, "MethodName", "Seq2Seq"], [35, 36, "MethodName", "Seq2Seq"]]}, {"text": "Further details are in", "entities": []}, {"text": "Appendix B. 5.2 Results The results are shown in Tab 2 .", "entities": []}, {"text": "For CNNDM and NYT we use BART as the backbone model while for XSum we use the pre - trained PEGASUS model as our base model since it achieves better performance than BART .", "entities": [[6, 7, "MethodName", "BART"], [13, 14, "DatasetName", "XSum"], [32, 33, "MethodName", "BART"]]}, {"text": "We have the following observations : ( 1 ) BRIO - Ctr outperforms SimCLS , its counterpart as an evaluation model in a two - stage summarization framework .", "entities": [[26, 27, "TaskName", "summarization"]]}, {"text": "Speci\ufb01cally , both BRIO - Ctr and SimCLS are used to score the candidate summaries generated by a Seq2Seq abstractive model ( BART ) .", "entities": [[18, 19, "MethodName", "Seq2Seq"], [22, 23, "MethodName", "BART"]]}, {"text": "The \ufb01nal outputs are selected based on those scores .", "entities": []}, {"text": "We attribute BRIO - Ctr \u2019s superior performance to its use of the same model architecture ( BART ) for both candidate generation and scoring , while SimCLS uses RoBERTa as the evaluation model .", "entities": [[17, 18, "MethodName", "BART"], [29, 30, "MethodName", "RoBERTa"]]}, {"text": "As a result , BRIO - Ctr maximizes the parameter sharing between the two stages , and preserves the power of the Seq2Seq model pre - trained on the same dataset .", "entities": [[22, 23, "MethodName", "Seq2Seq"]]}, {"text": "System R-1 R-2 R - L CNNDM BART * 44.16 21.28 40.90 PEGASUS * 44.17 21.47 41.11 GSum * 45.94 22.32 42.48 ConSum * 44.53 21.54 41.57 SeqCo * 45.02 21.80 41.75 GOLD-", "entities": [[7, 8, "MethodName", "BART"]]}, {"text": "p * 45.40", "entities": []}, {"text": "22.01 42.25 GOLD- s * 44.82 22.09 41.81 SimCLS * 46.67 22.15 43.54 BARTz44.29 21.17 41.09 BRIO - Ctr 47:28y22:93y44:15y", "entities": []}, {"text": "BRIO - Mul 47.78y23.55y44.57y XSum BART * 45.14 22.27 37.25 PEGASUS * 47.21 24.56 39.25 GSum * 45.40 21.89 36.67 ConSum * 47.34 24.67 39.40 SeqCo * 45.65 22.41 37.04 GOLD- p * 45.75 22.26 37.30 GOLD- s * 45.85 22.58 37.65 SimCLS * 47.61 24.57 39.44 PEGASUSz47.46 24.69 39.53 BRIO - Ctr 48:13y25:13y39:84y BRIO - Mul 49.07y25.59y40.40y NYT BARTz55.78 36.61 52.60 BRIO - Ctr 55.98 36.54 52.51 BRIO - Mul 57.75y38.64y54.54y Table 2 : Results on CNNDM , XSum andNYT .", "entities": [[4, 5, "DatasetName", "XSum"], [5, 6, "MethodName", "BART"], [79, 80, "DatasetName", "XSum"]]}, {"text": "OnNYT we only reported our own results due to different data pre - processing .", "entities": []}, {"text": "\u2020 : signi\ufb01cantly better than the baseline model ( p<0:01 ) .", "entities": []}, {"text": "* : results reported in the original papers .", "entities": []}, {"text": "\u2021 : results from our own evaluation script .", "entities": []}, {"text": "R-1/2 / L are the ROUGE-1/2 / L F 1scores .", "entities": []}, {"text": "( 2 ) BRIO - Mul is able to establish the new stare - of - the - art performance on CNNDM .", "entities": []}, {"text": "Notably , the previous state - of - the - art model , GSum , takes additional guidance as input and needs a separate encoder to encode the guidance information , while BRIO - Mul uses the same parameterization of BART .", "entities": [[40, 41, "MethodName", "BART"]]}, {"text": "Compared to other methods ( ConSum , SeqCo , GOLD ) that aim to improve upon BART , BRIO - Mul performs much better , showing the effectiveness of our training method .", "entities": [[16, 17, "MethodName", "BART"]]}, {"text": "( 3 ) Since on XSum we use PEGASUS instead of BART as the base model , the result shows that our method is not restricted to the speci\ufb01c choice of the base model .", "entities": [[5, 6, "DatasetName", "XSum"], [11, 12, "MethodName", "BART"]]}, {"text": "5.3 Analysis We further perform some in - depth analyses from diverse perspectives on the CNNDM dataset to gain more insights into our proposed method.2894", "entities": []}, {"text": "Coef\ufb01cient (", "entities": []}, {"text": ") R-1 R-2 R - L 0 ( BART ) 44.29 21.17 41.09 0.1 45.08 21.63 41.71 1 46.01 22.22 42.68 2 46.36 22.79 43.07 5 46.91 23.03 43.63 10 47.22 23.31 43.94 100 47.78 23.55 44.57 1000 46.83 22.17 43.68 +1(BRIO - Ctr ) 47.28 22.93 44.15 Table 3 : Model performance with different", "entities": [[6, 7, "DatasetName", "0"], [8, 9, "MethodName", "BART"]]}, {"text": "coef\ufb01cients weighting the contrastive loss ( Eq . 10 ) on CNNDM .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "BRIOCtr is trained with the contrastive loss only , which no longer preserves its generation ability .", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "We report its performance when it is used as an evaluation model to select from candidate summaries .", "entities": []}, {"text": "R-1/2 / L are the ROUGE-1/2 / L F 1scores .", "entities": []}, {"text": "Figure 2 : Loop of candidate generation and model \ufb01netuning .", "entities": []}, {"text": "System R-1 R-2 R - L BART 44.29 21.17 41.09 BRIO - Mul 47.78 23.55 44.57 BRIO - Loop 48.01y23.80y44.67y Table 4 : Results on CNNDM when the pre - trained model are \ufb01ne - tuned twice .", "entities": [[6, 7, "MethodName", "BART"]]}, {"text": "BRIO - Loop is trained on the candidates generated by BRIO - Mul .\u2020 : signi\ufb01cantly better than the baseline ( BART ) ( p<0:01 ) .", "entities": [[21, 22, "MethodName", "BART"]]}, {"text": "R-1/2 / L are ROUGE-1/2 / L F 1scores .", "entities": []}, {"text": "Coef\ufb01cients of the Multi - Task Loss The multitask loss ( Eq . 10 ) used to train our model contains two parts : the cross - entropy loss and the contastive loss .", "entities": [[9, 10, "MetricName", "loss"], [28, 29, "MetricName", "loss"], [32, 33, "MetricName", "loss"]]}, {"text": "As shown in Tab . 3 , as the weight of the contrastive loss (", "entities": [[13, 14, "MetricName", "loss"]]}, {"text": ") increases , the model \u2019s performance improves .", "entities": []}, {"text": "However , the cross - entropy loss is still necessary to preserve the model \u2019s ability as a generation model .", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "We argue that this is because the token level accuracy is still important during the autoregressive generation process , where the individual tokens are predicted sequentially .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "In addition , we also found that the model tends to achieve the best performance ( w.r.t the ROUGE scores on the development set ) faster with a higher", "entities": []}, {"text": ".", "entities": []}, {"text": "Speci\ufb01cally , it requires less than one entire epoch to achieve the best performance on CNNDM , making our approach an ef\ufb01cient \ufb01ne - tuning method .", "entities": []}, {"text": "Generation - Finetuning as a Loop Since the \ufb01ne - tuned model ( BRIO - Mul ) is still able to gen - Beams BART BRIO - Mul R-1 R-2 R-1 R-2 4 44.29 21.17 47.78 23.55 10 43.83 20.76 47.98 23.81 20 43.53 20.49 48.07 23.92 50 43.06 20.05 48.18 24.01 100 42.79 19.76 48.23 24.09 Table 5 : Results on CNNDM with different beam widths ( the number of beams ) used in beam search .", "entities": [[24, 25, "MethodName", "BART"]]}, {"text": "The default beam width is 4 . R-1/2 are the ROUGE-1/2 F 1scores .", "entities": []}, {"text": "erate , we can use it to generate a new set of candidates in the same way as we used the pre - trained BART model , and continue \ufb01ne - tuning it on this newly created set of candidates ( Och , 2003 ) .", "entities": [[24, 25, "MethodName", "BART"]]}, {"text": "Fig .", "entities": []}, {"text": "2 illustrates this iterative process .", "entities": []}, {"text": "The results shown in Tab . 4 illustrate that this new model ( BRIOLoop ) outperforms BRIO - Mul .", "entities": []}, {"text": "Besides , the model reached the best performance very quickly , showing the potential of adopting our method in an online framework where the new candidates are dynamically generated from the current model .", "entities": []}, {"text": "We leave this direction for future work .", "entities": []}, {"text": "Increasing the Beam Width While theoretically a larger beam width ( i.e. the number of candidates maintained during beam search ) would allow more candidates to be considered and therefore increase the upper bound of the performance , in practice model performance may be lower if the beam width is too large .", "entities": []}, {"text": "The reason for this phenomenon is closely related to the low sequence - level coordination of the generator .", "entities": []}, {"text": "Speci\ufb01cally , increasing the beam width may introduce candidates with lower quality ( Stahlberg and Byrne , 2019 ) , and the generator may not be able to differentiate them from high - quality candidates .", "entities": []}, {"text": "In Tab . 5 , we compare the performance of the pre - trained BART and our model ( BRIO - Mul ) with different beam widths used during inference .", "entities": [[14, 15, "MethodName", "BART"]]}, {"text": "We observe that the performance of BART goes down as the beam width increases .", "entities": [[6, 7, "MethodName", "BART"]]}, {"text": "On the other hand , our model is able to achieve better performance with a larger number of beams , demonstrating that our training method can improve the coordination of the model by encouraging the model to assign estimated probabilities to candidate summaries wellcorrelated with their quality .", "entities": []}, {"text": "Training with Different Evaluation Metrics In the previous experiments , we used ROUGE as the evaluation metric to de\ufb01ne the target ordering of the candidate summaries ( Eq.7 ) .", "entities": []}, {"text": "To evaluate our method \u2019s performance beyond ROUGE,2895", "entities": []}, {"text": "System R-1 R-2 R - L BS BART 44.29 21.17 41.09 27.38 BRIO - Mul ( R ) 47.78 23.55 44.57 32.11 BRIO - Mul ( B ) 47.53 23.22 44.37 32.59 Table 6 : Results on CNNDM using different evaluation metrics asMin Eq.7 .", "entities": [[7, 8, "MethodName", "BART"]]}, {"text": "BRIO - Mul ( R ) is trained with candidate summaries ordered by ROUGE scores , while BRIO - Mul ( B ) is trained with candidate summaries ordered by BERTScore .", "entities": []}, {"text": "R1/2 / L are ROUGE-1/2 / L F 1scores .", "entities": []}, {"text": "BS denotes BERTScore .", "entities": []}, {"text": "System Unigram Bigram Reference .1110 .4865", "entities": []}, {"text": "BART .0101 .0924", "entities": [[0, 1, "MethodName", "BART"]]}, {"text": "BRIO - Mul .0262 .2381", "entities": []}, {"text": "Table 7 : Ratio of novel n - grams of different models on CNNDM .", "entities": []}, {"text": "Noveln - grams are those that appear in the summaries but not in the source documents .", "entities": []}, {"text": "we use a model - based semantic similarity metric , BERTScore ( Zhang * et al . , 2020),7as the evaluation metricMin Eq.7 to compare the performance of different candidate summaries .", "entities": [[6, 8, "TaskName", "semantic similarity"]]}, {"text": "Then , we trained another version of BRIO - Mul based on the order of candidate summaries calculated by BERTScore .", "entities": []}, {"text": "The results in Tab . 6 show that ( 1 ) Our model can signi\ufb01cantly improve the model performance when either ROUGE or BERTScore is used as the target evaluation metric for ordering candidate summaries .", "entities": []}, {"text": "This suggests that it is possible to use our method to optimize any speci\ufb01c target metric , making our method an alternative to reinforcement learning or minimum risk training .", "entities": []}, {"text": "( 2 ) Our model that is trained on one evaluation metric ( e.g. BERTScore ) also achieves improvement on another metric ( e.g. ROUGE ) compared with the baseline model , which indicates that the improvement made by our model is not from exploiting the potential weaknesses of individual metrics .", "entities": []}, {"text": "Besides , this result also demonstrates a non - trivial degree of agreement between ROUGE and BERTScore .", "entities": []}, {"text": "Noveln - grams We compare the ratio of novel n - grams in reference , BRIO - Mul \u2019s , and BART \u2019s summaries .", "entities": [[21, 22, "MethodName", "BART"]]}, {"text": "As Tab . 7 shows , our model is more \u201c abstractive \u201d compared to BART , although reference summaries still contain more novel n - grams .", "entities": [[15, 16, "MethodName", "BART"]]}, {"text": "This is likely due to the fact that our model is optimized at the sequence - level , allowing more freedom for paraphrasing and compression .", "entities": []}, {"text": "We further investigate the relation of the \u201c abstractiveness \" and model performance by com7https://github.com/Tiiiger/bert_score .", "entities": []}, {"text": "We use its default version for English texts .", "entities": []}, {"text": "0 2 4 6 8 Novelty1.52.02.53.03.54.0ROUGE-1Performance Comparison 0123456789 Novelty0102030405060ROUGE-1Performance", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "BART BRIO - MulFigure 3 : Performance comparison ( BART v.s. BRIO - Mul ) w.r.t . reference summary novelty .", "entities": [[0, 1, "MethodName", "BART"], [9, 10, "MethodName", "BART"]]}, {"text": "The x - axis represents different buckets of test examples grouped by reference summary novelty ( Eq . 11 ) .", "entities": []}, {"text": "Larger x - coordinates correspond to examples of which the reference summaries have higher novelty .", "entities": []}, {"text": "The left \ufb01gure shows the performance improvement of our model compared with the baseline model , while the right one shows model performance .", "entities": []}, {"text": "Own PEGASUS BART .0470 .1205", "entities": [[2, 3, "MethodName", "BART"]]}, {"text": "BRIO - Mul : 1839y:2768y Table 8 : Rank Correlation between the model \u2019s estimated probabilities of the candidate summaries and the quality scores ( ROUGE ) of the candidate summaries on CNNDM .Own stands for the candidates generated by the models themselves , while PEGASUS stands for the candidates generated by the pretrained PEGASUS model . \u2020 : signi\ufb01cantly better than the baseline model ( BART ) ( p<0:01 ) .", "entities": [[66, 67, "MethodName", "BART"]]}, {"text": "paring our model ( BRIO - Mul ) with the baseline model ( BART ) on different buckets of test examples grouped by the \u201c novelty \" of the reference summaries,8i.e . , Novelty(D;S\u0003 )", "entities": [[13, 14, "MethodName", "BART"]]}, {"text": "= P g2GS\u00031(g = 2GD ) jGS\u0003j(11 ) whereDandS\u0003are the source document and reference summary respectively , GDandGS\u0003are the sets of bigrams in DandS\u0003 , 1is the indicator function .", "entities": []}, {"text": "The results in Fig .", "entities": []}, {"text": "3 show that when novelty is higher , ( 1 ) all models \u2019 performance decreases ; ( 2 ) our model achieves larger improvement over the baseline model .", "entities": []}, {"text": "Rank Correlation We computed the rank correlation between the estimated probabilities of the candidate summaries calculated by the generators and the quality scores of the candidate summaries .", "entities": []}, {"text": "We use Eq . 9 to calculate the estimated probabilities9and we use ROUGE-1 as the quality score metric of the candidate summaries .", "entities": [[12, 13, "MetricName", "ROUGE-1"]]}, {"text": "We calculate 8The calculation is performed using ExplainaBoard ( Liu et al . , 2021a ) .", "entities": []}, {"text": "https://github.com/neulab/ExplainaBoard .", "entities": []}, {"text": "9We found the value of the length penalty factor \u000b in Eq . 9 by maximizing the rank correlation on the validation set.2896", "entities": []}, {"text": "Dataset System ECE Acc Conf CNNDMBART .4097 .3711 .7365", "entities": [[3, 4, "MetricName", "Acc"]]}, {"text": "BRIO - Mul .2719", "entities": []}, {"text": ".4271 .6652", "entities": []}, {"text": "XSumPEGASUS .2369 .4688 .6990", "entities": []}, {"text": "BRIO - Mul .1423 .4744", "entities": []}, {"text": ".5881", "entities": []}, {"text": "Table 9 : Expected Calibration Error ( ECE ) , accuracy ( Acc ) and con\ufb01dence ( Conf ) on the test set of CNNDM andXSum .", "entities": [[5, 6, "MetricName", "Error"], [10, 11, "MetricName", "accuracy"], [12, 13, "MetricName", "Acc"]]}, {"text": "0.0 0.2 0.4 0.6 0.8 1.0 confidence0.20.40.60.81.0accuracyCNNDM BART CoordSum - Mul 0.0", "entities": [[7, 8, "MethodName", "BART"]]}, {"text": "0.2 0.4 0.6 0.8 1.0 confidence0.20.40.60.81.0accuracyXSum PEGASUS CoordSum - Mul Figure 4 : Reliability graphs on the CNNDM andXSum datasets .", "entities": []}, {"text": "The accuracy of model \u2019s predictions is plotted against the model \u2019s con\ufb01dence on these predictions .", "entities": [[1, 2, "MetricName", "accuracy"]]}, {"text": "Spearman \u2019s rank correlation for each sample , and use the average score as the overall correlation , We investigated two speci\ufb01c settings : 1 ) ranking candidate summaries generated by a different model ( PEGASUS ) ; 2 ) ranking candidate summaries generated by themselves ( BART & BRIOMul ) .", "entities": [[47, 48, "MethodName", "BART"]]}, {"text": "We use 16 candidates in total for calculation .", "entities": []}, {"text": "As Tab . 8 shows , our model achieves better rank correlation on the candidate summaries generated by both itself and the independent model .", "entities": []}, {"text": "This suggests that our model can better estimate the quality of candidate summaries .", "entities": []}, {"text": "5.4 Token - level Calibration Calibration requires that a model \u2019s con\ufb01dence on its predictions is equal to the accuracy of these predictions ( Guo et al . , 2017 ) .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "Previous work ( M\u00fcller et al . , 2019 ; Kumar and Sarawagi , 2019 ; Wang et al . , 2020 ) has found that a more calibrated text generation model tends to have better performance , and techniques like label smoothing can improve both the token - level calibration and sequence - level accuracy ( i.e. the ability of generating better results ) .", "entities": [[10, 11, "DatasetName", "Kumar"], [29, 31, "TaskName", "text generation"], [41, 43, "MethodName", "label smoothing"], [55, 56, "MetricName", "accuracy"]]}, {"text": "One intuitive explanation of this phenomenon is to interpret the model \u2019s estimated probability of a generated summary as the product of the model \u2019s con\ufb01dences on a series of tokenlevel predictions .", "entities": []}, {"text": "Then , since a more calibrated model \u2019s con\ufb01dence estimates better the accuracy of its predictions , the model \u2019s estimated probabilityof one sequence should be more indicative ofthequality of this sequence , which is essential for the beam search during inference .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "However , the relation of token - level calibration and sequencelevel performance remains inconclusive ( M\u00fcller et al . , 2019).10For example , a generator that always predicts a uniform distribution over all tokens would be perfectly calibrated , however , such a model would not generate high - quality outputs .", "entities": []}, {"text": "We investigate this relation from the opposite direction by evaluating whether our model ( BRIOMul ) , which is trained to have better sequencelevel performance , would also be more calibrated at thetoken - level compared with the baseline models that are trained using MLE and label smoothing .", "entities": [[46, 48, "MethodName", "label smoothing"]]}, {"text": "We follow previous work by using the Expected Calibration Error ( Naeini et al . , 2015 ) ( ECE ) as the evaluation metric of calibration : ECE = MX m=1jBmj njacc(Bm)\u0000conf(Bm)j(12 ) where the samples are grouped into Mequal - width buckets by con\ufb01dence ( conf ) , Bmdenotes them - th bucket , and nis the total number of samples .", "entities": [[9, 10, "MetricName", "Error"], [60, 63, "HyperparameterName", "number of samples"]]}, {"text": "Following Wang et al .", "entities": []}, {"text": "( 2020 ) , we evaluate model calibration on the system - generated summaries during inference and use the tercom toolkit11to assign labels ( correct / incorrect ) to the system - generated summaries based on the reference summaries .", "entities": []}, {"text": "The results in Tab . 9 show that BRIO - Mul is better calibrated compared to BART , suggesting that our method helps to improve the token - level calibration by explicitly encouraging the model to have more accurate sequence - level probability estimations .", "entities": [[16, 17, "MethodName", "BART"]]}, {"text": "The reliability graph is shown in Fig .", "entities": []}, {"text": "4 .", "entities": []}, {"text": "We found that ( 1 ) abstractive models are generally over - con\ufb01dent on their own predictions , ( 2 ) models are generally more calibrated on XSum than CNNDM .", "entities": [[27, 28, "DatasetName", "XSum"]]}, {"text": "This is likely due to the fact that XSum has shorter summaries therefore it is less likely to be affected by the exposure bias .", "entities": [[8, 9, "DatasetName", "XSum"]]}, {"text": "5.5 Few - shot Fine - tuning The training paradigm proposed in this paper may be extended to any Seq2Seq model .", "entities": [[19, 20, "MethodName", "Seq2Seq"]]}, {"text": "However , it can be a non - trivial overhead to generate the candidate summaries using large neural models on the entire training set .", "entities": []}, {"text": "On the other hand , recent work ( Raffel et al . , 2020 ; Zhang et al . , 2020 ; Schick and Sch\u00fctze , 10In general , better token - level calibration does n\u2019t guarantee better sequence - level performance .", "entities": []}, {"text": "11http://cs.umd.edu/~snover/tercom/2897", "entities": []}, {"text": "System Summary Reference chelsea forward tammy abraham nets \ufb01rst - half double for chelsea .", "entities": []}, {"text": "dominic solanke adds a third late on as chelsea look set to win trophy .", "entities": []}, {"text": "manchester city struggle without injured star thierry ambrose .", "entities": []}, {"text": "read : mourinho warns his young chelsea players he can not play them all .", "entities": []}, {"text": "click here to read our match report from man city \u2019s academy stadium .", "entities": []}, {"text": "BART tammy abraham scored twice in the \ufb01rst half to give chelsea the lead .", "entities": [[0, 1, "MethodName", "BART"]]}, {"text": "isaac buckley - ricketts levelled the game for manchester city .", "entities": []}, {"text": "dominic solanke scored late on to put a gloss on the scoreline .", "entities": []}, {"text": "click here to read sportsmail \u2019s player ratings from the youth cup \ufb01nal .", "entities": []}, {"text": "BRIO - Mul chelsea beat manchester city 3 - 1 in the youth cup \ufb01nal at the etihad stadium .", "entities": []}, {"text": "tammy abraham scored twice in the \ufb01rst half to give chelsea the lead .", "entities": []}, {"text": "dominic solanke scored late on to seal the win for the home side .", "entities": []}, {"text": "Reference alejandro valverde won ahead of julian alaphilippe and michael albasini .", "entities": []}, {"text": "chris froome \ufb01nished 123rd after a crash during the \ufb01nal 12 kilometres .", "entities": []}, {"text": "team sky \u2019s sports director gabriel rasch praised froome for \ufb01nishing .", "entities": []}, {"text": "rasch said froome was \u2018 banged up \u2019 but expects to ride tour de romandie .", "entities": []}, {"text": "BART movistar rider alejandro valverde won \ufb02eche wallonne on wednesday .", "entities": [[0, 1, "MethodName", "BART"]]}, {"text": "team sky \u2019s chris froome fell in the \ufb01nal 12 km but \ufb01nished the race .", "entities": []}, {"text": "philippe gilbert pulled out of the race after a bad crash 50 km from the end .", "entities": []}, {"text": "click here for more cycling news .", "entities": []}, {"text": "BRIO - Mul alejandro valverde defended his \ufb02eche wallonne title in belgium on wednesday .", "entities": []}, {"text": "movistar rider \ufb01nished ahead of julian alaphilippe and michael albasini .", "entities": []}, {"text": "team sky \u2019s chris froome fell in the \ufb01nal 12 km of the race but \ufb01nished in 123rd .", "entities": []}, {"text": "froome was involved in a crash but \ufb01nished the race despite being \u2018 banged up \u2019 Reference manuel pellegrini won the premier league and capital one cup last season .", "entities": []}, {"text": "city currently sit fourth in the league table - 12 points behind chelsea .", "entities": []}, {"text": "pellegrini \u2019s contract expires at the end of the 2015 - 16 season .", "entities": []}, {"text": "city players have been impressed with vieira \u2019s work with the youth team .", "entities": []}, {"text": "pep guardiola is city \u2019s \ufb01rst - choice to succeed pellegrini at the etihad .", "entities": []}, {"text": "BART manuel pellegrini \u2019s future at manchester city is under scrutiny .", "entities": [[0, 1, "MethodName", "BART"]]}, {"text": "patrick vieira is highly - respected among the city players .", "entities": []}, {"text": "city \u2019s \ufb01rst - choice managerial option is bayern munich boss pep guardiola .", "entities": []}, {"text": "click here for all the latest manchester city news .", "entities": []}, {"text": "click here for more premier league news .", "entities": []}, {"text": "BRIO - Mul manchester city players have backed patrick vieira to replace manuel pellegrini as manager of the club .", "entities": []}, {"text": "the frenchman is highly - respected among the players at the etihad stadium .", "entities": []}, {"text": "pellegrini \u2019s future at the club is under scrutiny after a disappointing season .", "entities": []}, {"text": "city \u2019s \ufb01rst - choice manager is current bayern munich boss pep guardiola .", "entities": []}, {"text": "Table 10 : Case Study on CNNDM .", "entities": []}, {"text": "BRIO - Mul learns to ignore the noise pattern ( \u201c click here \" ) while BART can not .", "entities": [[16, 17, "MethodName", "BART"]]}, {"text": "Dataset System R-1 R-2 R - L CNNDMBART 44.29 21.17 41.09 BRIO - Few 45.81 21.91 42.61 XSumPEGASUS 47.46 24.69 39.53 BRIO - Few 47.95 24.89 39.71 Table 11 : Few - shot Fine - tuning .", "entities": []}, {"text": "BRIO - Few is trained on only 100/1000 training examples on CNNDM andXSum respectively .", "entities": []}, {"text": "R-1/2 / L are ROUGE-1/2 / L F 1scores .", "entities": []}, {"text": "2021 ;", "entities": []}, {"text": "Fabbri et al . , 2021 ) has shown that few - shot learning can be an effective \ufb01ne - tuning method of pre - trained models for text generation tasks .", "entities": [[10, 14, "TaskName", "few - shot learning"], [28, 30, "TaskName", "text generation"]]}, {"text": "Therefore , we investigate our model \u2019s performance in a few - shot setting .", "entities": []}, {"text": "Speci\ufb01cally , we randomly sample 100/1000 examples from the training set of CNNDM /XSum , and \ufb01ne - tune the models that are pre - trained using MLE loss on those examples .", "entities": [[28, 29, "MetricName", "loss"]]}, {"text": "More training details can be found in Appendix C.", "entities": []}, {"text": "The results are shown in Tab . 11 .", "entities": []}, {"text": "All experiments are repeated three times , and the reported results are the average performance .", "entities": []}, {"text": "The results indicate that our model can achieve improvement over the baseline model under the few - shot learning setting with a small computational overhead .", "entities": [[15, 19, "TaskName", "few - shot learning"]]}, {"text": "5.6 Case Study on CNNDM Tab .", "entities": []}, {"text": "10 presents an interesting pattern we observed when comparing the results of BRIO - Mul and BART , which demonstrates that our method helps the abstractive model to \ufb01lter out noise patterns in the original data .", "entities": [[16, 17, "MethodName", "BART"]]}, {"text": "Speci\ufb01cally , some of the reference summaries ( 331/11490 ) in CNNDM containsthe phrase \u201c click here \u201d , pointing to a hyperlink , and 103 source documents also contain this phrase .", "entities": []}, {"text": "BART picked up this pattern , and generates this phrase in 96 output summaries .", "entities": [[0, 1, "MethodName", "BART"]]}, {"text": "On the contrary , our model learns to ignore this noise pattern and never generated it across the whole test set , likely because it identi\ufb01ed that generated candidates with this pattern rarely achieve a high ROUGE score , and downweighted the probability accordingly .", "entities": []}, {"text": "6 Conclusion and Future Work", "entities": []}, {"text": "In this work , we presented a new training paradigm that assigns candidate outputs probability mass according to their quality using contrastive learning .", "entities": [[21, 23, "MethodName", "contrastive learning"]]}, {"text": "While our method has achieved signi\ufb01cant improvement on abstractive summarization , we note several directions for the future work to explore .", "entities": [[9, 10, "TaskName", "summarization"]]}, {"text": "First , since our method makes no assumptions speci\ufb01cally about the summarization task , it can be extended to other conditional text generation tasks such as machine translation .", "entities": [[11, 12, "TaskName", "summarization"], [20, 23, "TaskName", "conditional text generation"], [26, 28, "TaskName", "machine translation"]]}, {"text": "Second , it is possible to apply our method in a reinforcement learning setting , where the candidate summaries are dynamically generated .", "entities": []}, {"text": "Finally , in experiments we only used diverse beam search to generate the candidate summaries , but it is likely that other candidate generation methods could yield further improvements .", "entities": []}, {"text": "Acknowledgements We thank the anonymous reviewers for valuable feedback and helpful suggestions.2898", "entities": []}, {"text": "References Dzmitry Bahdanau , Philemon Brakel , Kelvin Xu , Anirudh Goyal , Ryan Lowe , Joelle Pineau , Aaron C. Courville , and Yoshua Bengio .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "An actorcritic algorithm for sequence prediction .", "entities": []}, {"text": "CoRR , abs/1607.07086 .", "entities": []}, {"text": "Samy Bengio , Oriol Vinyals , Navdeep Jaitly , and Noam Shazeer .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Scheduled sampling for sequence prediction with recurrent neural networks .", "entities": []}, {"text": "InProceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 , NIPS\u201915 , page 1171\u20131179 , Cambridge , MA , USA . MIT Press .", "entities": [[20, 21, "DatasetName", "Cambridge"]]}, {"text": "Shuyang Cao and Lu Wang .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "CLIFF : Contrastive learning for improving faithfulness and factuality in abstractive summarization .", "entities": [[2, 4, "MethodName", "Contrastive learning"], [11, 12, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6633\u20136649 , Online and Punta Cana , Dominican Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ting Chen , Simon Kornblith , Mohammad Norouzi , and Geoffrey Hinton .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A simple framework for contrastive learning of visual representations .", "entities": [[4, 6, "MethodName", "contrastive learning"]]}, {"text": "In Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 1597\u20131607 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Kyunghyun Cho , Bart van Merri\u00ebnboer , Dzmitry Bahdanau , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "On the properties of neural machine translation : Encoder \u2013 decoder approaches .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of SSST-8 , Eighth Workshop on Syntax , Semantics and Structure in Statistical Translation , pages 103\u2013111 , Doha , Qatar .", "entities": [[15, 16, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Woon Sang Cho , Yizhe Zhang , Sudha Rao , Asli Celikyilmaz , Chenyan Xiong , Jianfeng Gao , Mengdi Wang , and Bill Dolan .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Contrastive multi - document question generation .", "entities": [[4, 6, "TaskName", "question generation"]]}, {"text": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics : Main Volume , pages 12\u201330 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sumit Chopra , Michael Auli , and Alexander M. Rush .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Abstractive sentence summarization with attentive recurrent neural networks .", "entities": [[1, 3, "TaskName", "sentence summarization"]]}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 93\u201398 , San Diego , California .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zi - Yi Dou , Pengfei Liu , Hiroaki Hayashi , Zhengbao Jiang , and Graham Neubig .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "GSum : A general framework for guided neural abstractive summarization .", "entities": [[9, 10, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 4830\u20134842 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sergey Edunov , Myle Ott , Michael Auli , David Grangier , and Marc\u2019Aurelio Ranzato .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Classical structured prediction losses for sequence to sequence learning .", "entities": [[1, 3, "TaskName", "structured prediction"], [5, 8, "MethodName", "sequence to sequence"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 355\u2013364 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alexander Fabbri , Simeng Han , Haoyuan Li , Haoran Li , Marjan Ghazvininejad , Sha\ufb01q Joty , Dragomir Radev , and Yashar Mehdad . 2021 .", "entities": []}, {"text": "Improving zero and few - shot abstractive summarization with intermediate \ufb01ne - tuning and data augmentation .", "entities": [[7, 8, "TaskName", "summarization"], [14, 16, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 704\u2013717 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kevin Gimpel and Noah A. Smith .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Softmaxmargin CRFs : Training log - linear models with cost functions .", "entities": []}, {"text": "In Human Language Technologies : The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics , pages 733\u2013736 , Los Angeles , California . Association for Computational Linguistics .", "entities": []}, {"text": "Chuan Guo , Geoff Pleiss , Yu Sun , and Kilian Q. Weinberger . 2017 .", "entities": []}, {"text": "On calibration of modern neural networks .", "entities": []}, {"text": "In Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 1321\u20131330 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Raia Hadsell , Sumit Chopra , and Yann LeCun .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Dimensionality reduction by learning an invariant mapping .", "entities": [[0, 2, "TaskName", "Dimensionality reduction"]]}, {"text": "In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 , CVPR \u2019 06 , page 1735\u20131742 , USA .", "entities": []}, {"text": "IEEE Computer Society .", "entities": []}, {"text": "Ralf Herbrich , Thore Graepel , and Klaus Obermayer .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Support vector learning for ordinal regression .", "entities": []}, {"text": "InIn International Conference on Arti\ufb01cial Neural Networks , pages 97\u2013102 .", "entities": []}, {"text": "Karl Moritz Hermann , Tom\u00e1\u0161 Ko \u02c7cisk\u00fd , Edward Grefenstette , Lasse Espeholt , Will Kay , Mustafa Suleyman , and Phil Blunsom .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Teaching machines to read2899", "entities": []}, {"text": "and comprehend .", "entities": []}, {"text": "In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 , NIPS\u201915 , page 1693\u20131701 , Cambridge , MA , USA . MIT Press .", "entities": [[21, 22, "DatasetName", "Cambridge"]]}, {"text": "Mark Hopkins and Jonathan May .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Tuning as ranking .", "entities": []}, {"text": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , pages 1352\u20131362 , Edinburgh , Scotland , UK .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chris Kedzie , Kathleen McKeown , and Hal Daum\u00e9 III .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Content selection in deep learning models of summarization .", "entities": [[7, 8, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1818\u20131828 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Huda Khayrallah , Brian Thompson , Matt Post , and Philipp Koehn .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Simulated multiple reference training improves low - resource machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 82\u201389 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In 3rd International Conference on Learning Representations , ICLR 2015 , San Diego , CA , USA , May 7 - 9 , 2015 , Conference Track Proceedings .", "entities": []}, {"text": "Aviral Kumar and Sunita Sarawagi .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "2019 .", "entities": []}, {"text": "Calibration of encoder decoder models for neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "CoRR , abs/1903.00802 .", "entities": []}, {"text": "Ann Lee , Michael Auli , and Marc\u2019Aurelio Ranzato .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "Discriminative reranking for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 7250\u20137264 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Seanie Lee , Dong Bok Lee , and Sung Ju Hwang .", "entities": []}, {"text": "2021b .", "entities": []}, {"text": "Contrastive learning with adversarial perturbations for conditional text generation .", "entities": [[0, 2, "MethodName", "Contrastive learning"], [6, 9, "TaskName", "conditional text generation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "BART :", "entities": [[0, 1, "MethodName", "BART"]]}, {"text": "Denoising sequence - to - sequence pretraining for natural language generation , translation , and comprehension .", "entities": [[0, 1, "TaskName", "Denoising"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871\u20137880 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jiwei Li , Will Monroe , Alan Ritter , Dan Jurafsky , Michel Galley , and Jianfeng Gao . 2016 .", "entities": []}, {"text": "Deep reinforcement learning for dialogue generation .", "entities": [[4, 6, "TaskName", "dialogue generation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1192 \u2013 1202 , Austin , Texas .", "entities": [[21, 22, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Siyao Li , Deren Lei , Pengda Qin , and William Yang Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Deep reinforcement learning with distributional semantic rewards for abstractive summarization .", "entities": [[9, 10, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 6038\u20136044 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chin - Yew Lin .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "ROUGE :", "entities": []}, {"text": "A package for automatic evaluation of summaries .", "entities": []}, {"text": "In Text Summarization Branches Out , pages 74\u201381 , Barcelona , Spain . Association for Computational Linguistics .", "entities": [[1, 3, "TaskName", "Text Summarization"]]}, {"text": "Pengfei Liu , Jinlan Fu , Yang Xiao , Weizhe Yuan , Shuaichen Chang , Junqi Dai , Yixin Liu , Zihuiwen Ye , and Graham Neubig .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "ExplainaBoard : An explainable leaderboard for NLP .", "entities": []}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing : System Demonstrations , pages 280\u2013289 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized BERT pretraining approach .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "CoRR , abs/1907.11692 .", "entities": []}, {"text": "Yixin Liu , Zi - Yi Dou , and Pengfei Liu . 2021b .", "entities": []}, {"text": "RefSum : Refactoring neural summarization .", "entities": [[4, 5, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1437\u20131448 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yixin Liu and Pengfei Liu . 2021 .", "entities": []}, {"text": "SimCLS :", "entities": []}, {"text": "A simple framework for contrastive learning of abstractive summarization .", "entities": [[4, 6, "MethodName", "contrastive learning"], [8, 9, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 1065\u20131072 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tomoya Mizumoto and Yuji Matsumoto .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Discriminative reranking for grammatical error correction with statistical machine translation .", "entities": [[3, 6, "TaskName", "grammatical error correction"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1133\u20131138 , San Diego , California . Association for Computational Linguistics .", "entities": []}, {"text": "Rafael M\u00fcller , Simon Kornblith , and Geoffrey E Hinton .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "When does label smoothing help ?", "entities": [[2, 4, "MethodName", "label smoothing"]]}, {"text": "In Advances in Neural Information Processing Systems , volume 32 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Mahdi Pakdaman Naeini , Gregory F. Cooper , and Milos Hauskrecht .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Obtaining well calibrated2900", "entities": []}, {"text": "probabilities using bayesian binning .", "entities": []}, {"text": "In Proceedings of the Twenty - Ninth AAAI Conference on Arti\ufb01cial Intelligence , AAAI\u201915 , page 2901\u20132907 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Ramesh Nallapati , Bowen Zhou , Cicero dos Santos , \u00c7a\u02d8glar Gul\u00e7ehre , and Bing Xiang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Abstractive text summarization using sequence - to - sequence RNNs and beyond .", "entities": [[0, 3, "TaskName", "Abstractive text summarization"]]}, {"text": "In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning , pages 280\u2013290 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Shashi Narayan , Shay B. Cohen , and Mirella Lapata .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Do n\u2019t give me the details , just the summary !", "entities": []}, {"text": "topic - aware convolutional neural networks for extreme summarization .", "entities": [[7, 9, "TaskName", "extreme summarization"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1797\u20131807 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Mohammad Norouzi , Samy Bengio , zhifeng Chen , Navdeep Jaitly , Mike Schuster , Yonghui Wu , and Dale Schuurmans .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Reward augmented maximum likelihood for neural structured prediction .", "entities": [[6, 8, "TaskName", "structured prediction"]]}, {"text": "In Advances in Neural Information Processing Systems , volume 29 , pages 1723\u20131731 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Franz Josef Och . 2003 .", "entities": []}, {"text": "Minimum error rate training in statistical machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pages 160\u2013167 , Sapporo , Japan . Association for Computational Linguistics .", "entities": []}, {"text": "Franz Josef Och , Daniel Gildea , Sanjeev Khudanpur , Anoop Sarkar , Kenji Yamada , Alex Fraser , Shankar Kumar , Libin Shen , David Smith , Katherine Eng , Viren Jain , Zhen Jin , and Dragomir Radev .", "entities": [[20, 21, "DatasetName", "Kumar"]]}, {"text": "2004 .", "entities": []}, {"text": "A smorgasbord of features for statistical machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics : HLT - NAACL 2004 , pages 161\u2013168 , Boston , Massachusetts , USA .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xiao Pan , Mingxuan Wang , Liwei Wu , and Lei Li . 2021 .", "entities": []}, {"text": "Contrastive learning for many - to - many multilingual neural machine translation .", "entities": [[0, 2, "MethodName", "Contrastive learning"], [10, 12, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 244\u2013258 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Richard Yuanzhe Pang and He He .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Text generation by learning from demonstrations .", "entities": [[0, 2, "TaskName", "Text generation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Romain Paulus , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A deep reinforced model for abstractive summarization .", "entities": [[6, 7, "TaskName", "summarization"]]}, {"text": "In International Conference on Learning Representations .Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J. Liu . 2020 .", "entities": [[12, 13, "MethodName", "Adam"]]}, {"text": "Exploring the limits of transfer learning with a uni\ufb01ed text - totext transformer .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "Journal of Machine Learning Research , 21(140):1\u201367 .", "entities": []}, {"text": "Marc\u2019Aurelio Ranzato , Sumit Chopra , Michael Auli , and Wojciech Zaremba .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Sequence level training with recurrent neural networks .", "entities": []}, {"text": "In 4th International Conference on Learning Representations , ICLR 2016 , San Juan , Puerto Rico , May 2 - 4 , 2016 , Conference Track Proceedings .", "entities": []}, {"text": "Alexander M. Rush , Sumit Chopra , and Jason Weston .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A neural attention model for abstractive sentence summarization .", "entities": [[6, 8, "TaskName", "sentence summarization"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 379\u2013389 , Lisbon , Portugal . Association for Computational Linguistics .", "entities": []}, {"text": "Evan Sandhaus .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "The New York Times Annotated Corpus .", "entities": [[1, 6, "DatasetName", "New York Times Annotated Corpus"]]}, {"text": "LDC corpora .", "entities": []}, {"text": "Linguistic Data Consortium .", "entities": []}, {"text": "Timo Schick and Hinrich Sch\u00fctze . 2021 .", "entities": [[0, 1, "DatasetName", "Timo"]]}, {"text": "Few - shot text generation with natural language instructions .", "entities": [[3, 5, "TaskName", "text generation"]]}, {"text": "InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 390\u2013402 , Online and Punta Cana , Dominican Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Libin Shen , Anoop Sarkar , and Franz Josef Och . 2004 .", "entities": []}, {"text": "Discriminative reranking for machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "InProceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics : HLT - NAACL 2004 , pages 177\u2013184 , Boston , Massachusetts , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Shiqi Shen , Yong Cheng , Zhongjun He , Wei He , Hua Wu , Maosong Sun , and Yang Liu . 2016 .", "entities": []}, {"text": "Minimum risk training for neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1683\u20131692 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Felix Stahlberg and Bill Byrne .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "On NMT search errors and model errors : Cat got your tongue ?", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3356 \u2013 3362 , Hong Kong , China . Association for Computational Linguistics .", "entities": []}, {"text": "Shichao Sun and Wenjie Li .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Alleviating exposure bias via contrastive learning for abstractive text summarization .", "entities": [[4, 6, "MethodName", "contrastive learning"], [7, 10, "TaskName", "abstractive text summarization"]]}, {"text": "CoRR , abs/2108.11846 .", "entities": []}, {"text": "Ilya Sutskever , Oriol Vinyals , and Quoc V .", "entities": []}, {"text": "Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "InProceedings of the 27th International Conference2901", "entities": []}, {"text": "on Neural Information Processing Systems - Volume 2 , NIPS\u201914 , page 3104\u20133112 , Cambridge , MA , USA . MIT Press .", "entities": [[14, 15, "DatasetName", "Cambridge"]]}, {"text": "C. Szegedy , V .", "entities": []}, {"text": "Vanhoucke , S. Ioffe , J. Shlens , and Z. Wojna .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Rethinking the inception architecture for computer vision .", "entities": []}, {"text": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 2818\u20132826 , Los Alamitos , CA , USA .", "entities": []}, {"text": "IEEE Computer Society .", "entities": []}, {"text": "Ben Taskar , Carlos Guestrin , and Daphne Koller .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Max - margin markov networks .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 16 .", "entities": []}, {"text": "MIT Press .", "entities": []}, {"text": "Yui Uehara , Tatsuya Ishigaki , Kasumi Aoki , Hiroshi Noji , Keiichi Goshima , Ichiro Kobayashi , Hiroya Takamura , and Yusuke Miyao .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Learning with contrastive examples for data - to - text generation .", "entities": [[5, 11, "TaskName", "data - to - text generation"]]}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics , pages 2352 \u2013 2362 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Ashwin Vijayakumar , Michael Cogswell , Ramprasaath Selvaraju , Qing Sun , Stefan Lee , David Crandall , and Dhruv Batra .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Diverse beam search for improved description of complex scenes .", "entities": []}, {"text": "Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 32(1 ) .", "entities": []}, {"text": "Xiaojun Wan , Ziqiang Cao , Furu Wei , Sujian Li , and M. Zhou . 2015 .", "entities": []}, {"text": "Multi - document summarization via discriminative summary reranking .", "entities": [[0, 4, "TaskName", "Multi - document summarization"]]}, {"text": "ArXiv , abs/1507.02062 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Shuo Wang , Zhaopeng Tu , Shuming Shi , and Yang Liu . 2020 .", "entities": []}, {"text": "On the inference calibration of neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 3070\u20133079 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "John Wieting , Taylor Berg - Kirkpatrick , Kevin Gimpel , and Graham Neubig .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Beyond BLEU : training neural machine translation with semantic similarity .", "entities": [[1, 2, "MetricName", "BLEU"], [5, 7, "TaskName", "machine translation"], [8, 10, "TaskName", "semantic similarity"]]}, {"text": "InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4344\u20134355 , Florence , Italy .", "entities": [[16, 17, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sam Wiseman and Alexander M. Rush . 2016 .", "entities": []}, {"text": "Sequence - to - sequence learning as beam - search optimization .", "entities": []}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1296\u20131306 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 38\u201345 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shusheng Xu , Xingxing Zhang , Yi Wu , and Furu Wei . 2021 .", "entities": []}, {"text": "Sequence level contrastive learning for text summarization .", "entities": [[2, 4, "MethodName", "contrastive learning"], [5, 7, "TaskName", "text summarization"]]}, {"text": "CoRR , abs/2109.03481 .", "entities": []}, {"text": "Zonghan Yang , Yong Cheng , Yang Liu , and Maosong Sun . 2019 .", "entities": []}, {"text": "Reducing word omission errors in neural machine translation : A contrastive learning approach .", "entities": [[6, 8, "TaskName", "machine translation"], [10, 12, "MethodName", "contrastive learning"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6191\u20136196 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Weizhe Yuan , Graham Neubig , and Pengfei Liu . 2021 .", "entities": []}, {"text": "BARTScore : Evaluating generated text as text generation .", "entities": [[6, 8, "TaskName", "text generation"]]}, {"text": "In Thirty - Fifth Conference on Neural Information Processing Systems .", "entities": []}, {"text": "Jingqing Zhang , Yao Zhao , Mohammad Saleh , and Peter Liu . 2020 .", "entities": []}, {"text": "Pegasus : Pre - training with extracted gap - sentences for abstractive summarization .", "entities": [[12, 13, "TaskName", "summarization"]]}, {"text": "In International Conference on Machine Learning , pages 11328\u201311339 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Tianyi Zhang * , Varsha Kishore * , Felix Wu * , Kilian Q. Weinberger , and Yoav Artzi .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Bertscore : Evaluating text generation with bert .", "entities": [[3, 5, "TaskName", "text generation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Wen Zhang , Yang Feng , Fandong Meng , Di You , and Qun Liu . 2019 .", "entities": []}, {"text": "Bridging the gap between training and inference for neural machine translation .", "entities": [[9, 11, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4334 \u2013 4343 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "Ming Zhong , Pengfei Liu , Yiran Chen , Danqing Wang , Xipeng Qiu , and Xuanjing Huang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Extractive summarization as text matching .", "entities": [[0, 2, "TaskName", "Extractive summarization"], [3, 5, "TaskName", "text matching"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 6197\u20136208 , Online .", "entities": []}, {"text": "Association for Computational Linguistics.2902", "entities": []}, {"text": "A Datasets Statistics Datasets # Examples Avg .", "entities": []}, {"text": "Words Train Valid Test Doc .", "entities": []}, {"text": "Sum .", "entities": []}, {"text": "CNNDM 287 K 13 K 11 K 791.6 55.6 XSum 203 K 11 K 11 K 429.2 23.3 NYT 44 K 5.5 K 6.4 K 1320.2 123.4 Table 12 : Datasets Statistics .", "entities": [[9, 10, "DatasetName", "XSum"]]}, {"text": "B Implementation Details We use diverse beam search ( Vijayakumar et al . , 2018 ) to generate 16 candidates for each data sample .", "entities": []}, {"text": "On CNNDM andXSum , we use the pre - trained BART12and PEGASUS13models from the Transformers ( Wolf et al . , 2020 ) library as the base abstractive models for candidate summary generation and model \ufb01netuning respectively .", "entities": []}, {"text": "On NYT , we \ufb01rst \ufb01ne - tuned a BART model14with MLE training as the base abstractive model , since our data pre - processing is sightly different from the previous work and there are no available pre - trained checkpoints .", "entities": [[9, 10, "MethodName", "BART"]]}, {"text": "We use 4 NVIDIA RTX 3090 GPUs for the model training , and the average running time for one epoch is around 20 hours .", "entities": []}, {"text": "We use the Adam optimizer ( Kingma and Ba , 2015 ) with learning rate scheduling for the model training : lr=2\u000210\u00003min ( step\u00000:5;step\u0001warmup\u00001:5 ) where warmup denotes the warmup steps , which is set to 10000 , step is the number of updating steps , lris the learning rate .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [13, 15, "HyperparameterName", "learning rate"], [48, 50, "HyperparameterName", "learning rate"]]}, {"text": "We set the length penalty factor \u000b in the scoring function ( Eq . 9 ) to the same value as used in the original beam search .", "entities": []}, {"text": "We search the value of the margin\u0015in the contrastive loss ( Eq . 8) within the range [ 1\u000210\u00005;1 ] , and decide the value based on the model performance on the validation set .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "We also performed extensive search for the coef\ufb01cient", "entities": []}, {"text": "in Eq .", "entities": []}, {"text": "10 .", "entities": []}, {"text": "The speci\ufb01c hyper - parameter setting is reported in Tab . 13 .", "entities": []}, {"text": "We use the standard ROUGE ( Lin , 2004 ) Perl package15for evaluation .", "entities": []}, {"text": "The command line parameters are \u2018 -c 95 -r 1000 -n 2 -m \u2019 .", "entities": []}, {"text": "Before the 12The checkpoint is \u201c facebook / bart - large - cnn \u201d , containing around 400 M parameters .", "entities": []}, {"text": "13The checkpoint is \u201c google / pegasus - xsum \" \" containing around 568 M parameters .", "entities": [[8, 9, "DatasetName", "xsum"]]}, {"text": "14The checkpoint is \u201c facebook / bart - large \u201d .", "entities": []}, {"text": "15https://github.com/summanlp/evaluation/tree/master/ ROUGE - RELEASE-1.5.5Datasets\u0015(Eq .", "entities": []}, {"text": "8) \u000b ( Eq . 9 )", "entities": []}, {"text": "( Eq . 10 ) CNNDM 0.001 2.0 100 XSum 0.1 0.6 100 NYT 0.001 2.0 100 Table 13 : Hyper - parameter Setting .", "entities": [[9, 10, "DatasetName", "XSum"]]}, {"text": "ROUGE evaluation , the reference summaries and system outputs are lower - cased and tokenized.16", "entities": []}, {"text": "C Details of Few - shot Fine - tuning OnCNNDM", "entities": []}, {"text": ", we randomly select 100 examples from the training set for \ufb01ne - tuning .", "entities": []}, {"text": "On XSum , we found that at least 1000 examples are needed for the model to achieve better performance compared to the baseline model .", "entities": [[1, 2, "DatasetName", "XSum"]]}, {"text": "All experiments are repeated three times .", "entities": []}, {"text": "We randomly select 1000 examples from the original validation set for hyper - parameter selection .", "entities": []}, {"text": "We use the Adam optimizer with the learning rate set to 1\u000210\u00006 .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [7, 9, "HyperparameterName", "learning rate"]]}, {"text": "The model is trained for 15 epochs on CNNDM and 10 epochs on XSum .", "entities": [[13, 14, "DatasetName", "XSum"]]}, {"text": "16PTB tokenizer is used for tokenization .", "entities": []}, {"text": "https : //nlp.stanford.edu / nlp / javadoc / javanlp / edu / stanford / nlp/ process / PTBTokenizer.html2903", "entities": []}]