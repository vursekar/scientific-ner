[{"text": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1380\u20131391 June 6\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics1380Structure - Aware Abstractive Conversation Summarization via Discourse and Action Graphs Jiaao Chen School of Interactive Computing Georgia Institute of Technology jiaaochen@gatech.eduDiyi Yang School of Interactive Computing Georgia Institute of Technology dyang888@gatech.edu", "entities": [[10, 11, "TaskName", "Summarization"]]}, {"text": "Abstract Abstractive conversation summarization has received much attention recently .", "entities": [[3, 4, "TaskName", "summarization"]]}, {"text": "However , these generated summaries often suffer from insuf\ufb01cient , redundant , or incorrect content , largely due to the unstructured and complex characteristics of human - human interactions .", "entities": []}, {"text": "To this end , we propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization , by \ufb01rst incorporating discourse relations between utterances and action triples ( \u201c WHO DOING -WHAT \u201d ) in utterances through structured graphs to better encode conversations , and then designing a multi - granularity decoder to generate summaries by combining all levels of information .", "entities": [[20, 21, "TaskName", "summarization"]]}, {"text": "Experiments show that our proposed models outperform state - of - theart methods and generalize well in other domains in terms of both automatic evaluations and human judgments .", "entities": []}, {"text": "We have publicly released our code at https://github.com/ GT - SALT / Structure - Aware - BART .", "entities": [[16, 17, "MethodName", "BART"]]}, {"text": "1 Introduction Online interaction has become an indispensable component of everyday life and people are increasingly using textual conversations to exchange ideas , make plans , and share information .", "entities": []}, {"text": "However , it is time - consuming to recap and grasp all the core content within every complex conversation ( Gao et al . , 2020 ; Feng et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "As a result , how to organize massive everyday interactions into natural , concise , and informative text , i.e. , abstractive conversation summarization , starts to gain importance .", "entities": [[23, 24, "TaskName", "summarization"]]}, {"text": "Signi\ufb01cant progress has been made on abstractive summarization for structured document via pointer generator ( See et al . , 2017 ) , reinforcement methods ( Paulus et al . , 2018 ;", "entities": [[7, 8, "TaskName", "summarization"]]}, {"text": "Huang et al . , 2020a ) and pre - trained models ( Liu and Lapata , 2019 ; Lewis et al . , 2020 ; Zhang et al . , 2019 ) .", "entities": []}, {"text": "Despite the huge success , it is challenging to directly apply document models to summarize conversations , due to Figure 1 : An example of discourse relation graph ( a ) and action graph ( b ) from one conversation in SAMSum ( Gliwa et al . , 2019 ) .", "entities": [[41, 42, "DatasetName", "SAMSum"]]}, {"text": "The annotated summary is Simon was on the phone before , so he did n\u2019t here Helen calling .", "entities": [[16, 17, "DatasetName", "Helen"]]}, {"text": "Simon will fetch Helen some tissues .", "entities": [[3, 4, "DatasetName", "Helen"]]}, {"text": "a set of inherent differences between conversations and documents ( Gliwa et al . , 2019 ) .", "entities": []}, {"text": "First , speaker interruptions like repetitions , false - starts , and hesitations are frequent in conversations ( Sacks et al . , 1978 ) , and key information resides in different portions of a conversation .", "entities": []}, {"text": "These unstructured properties pose challenges for models to focus on salient contents that are necessary for generating both abstractive and informative summaries .", "entities": []}, {"text": "Second , there is more than one speaker in conversations and people interact with each other in different language styles ( Zhu et al . , 2020b ) .", "entities": []}, {"text": "The complex interactions among multiple speakers make it harder for mod-", "entities": []}, {"text": "1381els to identify and associate speakers with correct actions so as to generate factual summaries .", "entities": []}, {"text": "In order to summarize the unstructured and complex conversations , a growing body of research has been conducted , such as transferring document summarization methods to conversation settings ( Shang et al . , 2018 ; Gliwa et al . , 2019 ) , adopting hierarchical models ( Zhao et al . , 2019 ; Zhu et al . , 2020b ) , or incorporating conversation structures like topic segmentation ( Liu et al . , 2019b ; Li et al . , 2019 ; Chen and Yang , 2020 ) , dialogue acts ( Goo and Chen , 2018 ) , and conversation stages ( Chen and Yang , 2020 ) .", "entities": [[22, 24, "TaskName", "document summarization"]]}, {"text": "However , current approaches still face challenges in terms of succinctness and faithfulness , as most prior studies ( i ) fail to explicitly model dependencies between utterances which can help identify salient portions of conversations ( Bui et al . , 2009 ) , and ( ii ) lack structured representations ( Huang et al . , 2020a ) to learn the associations between speakers , actions and events .", "entities": []}, {"text": "We argue that these rich linguistic structures associated with conversations are key components towards generating abstractive and factual conversation summaries .", "entities": []}, {"text": "To this end , we present a structure - aware sequence - to - sequence model , in which we equip abstractive conversation summarization models with rich conversation structures through two types of graphs : discourse relation graph andaction graph .", "entities": [[23, 24, "TaskName", "summarization"]]}, {"text": "Discourse relation graphs are constructed based on dependency - based discourse relations ( Kirschner et al . , 2012 ; Stone et al . , 2013 ; Asher et al . , 2016 ;", "entities": []}, {"text": "Qin et al . , 2017 ) between intertwined utterances , where each Elementary Discourse Unit ( EDU ) is one single utterance and they are linked through 16 different types of relations ( Asher et al . , 2016 ) .", "entities": []}, {"text": "As shown in Figure 1(a ) , highly related utterances are linked based on discourse relations likeQuestion Answer Pairs , Comment andExplanation .", "entities": []}, {"text": "Explicitly modeling these utterances relations in conversations can aid models in recognizing key content for succinct and informative summarization .", "entities": [[18, 19, "TaskName", "summarization"]]}, {"text": "Action graphs are constructed as the \u201c WHO -DOING -WHAT \u201d triplets in conversations which express socially situated identities and activities ( Gee , 2014 ) .", "entities": []}, {"text": "For instance , in Figure 1(b ) , the action graph provides explicit information between Simon , fetch , and tissues for the utterance it is Simon who will fetch the tissues , making models less likely to generate summaries with wrong references ( e.g. , Helen will fetch the tissues ) .", "entities": [[46, 47, "DatasetName", "Helen"]]}, {"text": "To sum up , our contributions are : ( 1 ) We pro - pose to utilize discourse relation graphs and action graphs to better encode conversations for conversation summarization .", "entities": [[29, 30, "TaskName", "summarization"]]}, {"text": "( 2 ) We design structureaware sequence - to - sequence models to combine these structured graphs and generate summaries with the help of a novel multi - granularity decoder .", "entities": []}, {"text": "( 3 ) We demonstrate the effectiveness of our proposed methods through experiments on a largescale conversation summarization dataset , SAMSum ( Gliwa et al . , 2019 ) .", "entities": [[17, 18, "TaskName", "summarization"], [20, 21, "DatasetName", "SAMSum"]]}, {"text": "( 4 ) We further show that our structure - aware models can generalize well in new domains such as debate summarization .", "entities": [[21, 22, "TaskName", "summarization"]]}, {"text": "2 Related Work Document Summarization Compared to extractive document summarization ( Gupta and Lehal , 2010 ; Narayan et al . , 2018 ; Liu and Lapata , 2019 ) , abstractive document summarization is generally considered more challenging and has received more attention .", "entities": [[3, 5, "TaskName", "Document Summarization"], [7, 10, "TaskName", "extractive document summarization"], [32, 34, "TaskName", "document summarization"]]}, {"text": "Various methods have been designed to tackle abstractive document summarization like sequence - to - sequence models ( Rush et al . , 2015 ) ,", "entities": [[8, 10, "TaskName", "document summarization"]]}, {"text": "pointer generators ( See et al . , 2017 ) , reinforcement learning methods ( Paulus et al . , 2018 ; Huang et al . , 2020a ) and pre - trained models ( Lewis et al . , 2020 ; Zhang et al . , 2019 ) .", "entities": []}, {"text": "To generate faithful abstractive document summaries ( Maynez et al . , 2020 ) , graphbased models were introduced recently such as extracting entity types ( Fernandes et al . , 2018 ; Fan et al . , 2019 ) , leveraging knowledge graphs ( Huang et al . , 2020a ; Zhu et al . , 2020a ) or designing extra fact correction modules ( Dong et al . , 2020 ) .", "entities": [[42, 44, "TaskName", "knowledge graphs"]]}, {"text": "Inspired by these graph - based methods , we also construct action graphs for generating more factual conversation summaries .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "Conversation Summarization Extractive dialogue summarization ( Murray et al . , 2005 ) has been studied extensively via statistical machine learning methods such as skip - chain CRFs ( Galley , 2006 ) , SVM with LDA models ( Wang and Cardie , 2013 ) , and multi - sentence compression algorithms ( Shang et al . , 2018 ) .", "entities": [[1, 2, "TaskName", "Summarization"], [4, 5, "TaskName", "summarization"], [34, 35, "MethodName", "SVM"], [36, 37, "MethodName", "LDA"], [49, 51, "DatasetName", "sentence compression"]]}, {"text": "Such methods struggled with generating succinct , \ufb02uent , and natural summaries , especially when the key information needs to be aggregated from multiple \ufb01rst - person point - of - view utterances ( Song et al . , 2020 ) .", "entities": []}, {"text": "Abstractive conversation summarization overcomes these issues by designing hierarchical models ( Zhao et al . , 2019 ; Zhu et al . , 2020b ) , incorporating commonsense knowledge ( Feng et al . , 2020 ) , or leveraging conversational structures like dialogue acts ( Goo and Chen ,", "entities": [[2, 3, "TaskName", "summarization"]]}, {"text": "1382 Figure 2 : Model architecture .", "entities": []}, {"text": "Each utterance is encoded via transformer encoder ; discourse relation graphs and action graphs are encoded through Graph Attention Networks ( a ) .", "entities": []}, {"text": "The multi - granularity decoder ( b ) then generates summaries based on all levels of encoded information including utterances , action graphs , and discourse graphs .", "entities": []}, {"text": "2018 ) , key point sequences ( Liu et al . , 2019a ) , topic segments ( Liu et al . , 2019b ; Li et", "entities": []}, {"text": "al . , 2019 ) and stage developments ( Chen and Yang , 2020 ) .", "entities": []}, {"text": "Some recent research has also utilized discourse relations as input features in classi\ufb01ers to detect important content in conversations ( Murray et al . , 2006 ; Bui et al . , 2009 ; Qin et al . , 2017 ) .", "entities": []}, {"text": "However , current models still have not explicitly utilized the dependencies between different utterances , making models hard to leverage long - range dependencies and utilize these salient utterances .", "entities": []}, {"text": "Moreover , less attention has been paid to identify the actions of different speakers and how they interact with or refer to each other , leading to unfaithful summarization with incorrect references or wrong reasoning ( Gliwa et al . , 2019 ) .", "entities": [[28, 29, "TaskName", "summarization"]]}, {"text": "To \ufb01ll these gaps , we propose to explicitly model actions within utterances , and relations between utterances in conversations in a structured way , by using discourse relation graphs and action graphs and further combining these through relational graph encoders and multigranularity decoders for abstractive conversation summarization .", "entities": [[47, 48, "TaskName", "summarization"]]}, {"text": "3 Methods To generate abstractive and factual summaries from unstructured conversations , we propose to model structural signals in conversations by \ufb01rst constructing discourse relation graphs and action graphs ( Section 3.1 ) , and then encoding the graphs together with conversations ( Section 3.2 ) as well as incorporating these different levels of information in the decoding stage through a multi - granularitydecoder ( Section 3.3 ) to summarize given conversations .", "entities": []}, {"text": "The overall architecture is shown in Figure 2 .", "entities": []}, {"text": "3.1 Structured Graph Construction", "entities": [[2, 4, "TaskName", "Graph Construction"]]}, {"text": "This section describes how to construct the discourse relation graphs and action graphs .", "entities": []}, {"text": "Formally , for a given conversation C = fu0;:::;umg withmutterances , we construct discourse relation graphGD= ( VD;ED ) , where VDis the set of nodes representing Elementary Discourse Units ( EDUs ) , and EDis the adjacent matrix that describes the relations between EDUs , and action graphGA= ( VA;EA ) , where VAis the set of nodes representing \u201c WHO \u201d , \u201c DOING \u201d and \u201c WHAT \u201d arguments , and EAis the adjacent matrix to link \u201c WHO -DOING -WHAT \u201d triples .", "entities": []}, {"text": "Discourse Relation Graph Utterances from different speakers do not occur in isolation ; instead , they are related within the context of discourse ( Murray et al . , 2006 ;", "entities": []}, {"text": "Qin et al . , 2017 ) , which has been shown effective for dialogue understanding like identifying the decisions in multi - party dialogues ( Bui et al . , 2009 ) and detecting salient content in email conversations ( McKeown et al . , 2007 ) .", "entities": [[14, 16, "TaskName", "dialogue understanding"]]}, {"text": "Although current attention - based neural models are supposed to , or might implicitly , learn certain relations between utterances , they often struggle to focus on many informative utterances ( Chen and Yang , 2020 ;", "entities": []}, {"text": "Song et al . , 2020 ) and fail to address long - range dependencies ( Xu et al . , 2020 ) , especially when there are frequent interruptions .", "entities": []}, {"text": "As a result , explicitly incorporating the discourse relations will help neural summarization models better", "entities": [[12, 13, "TaskName", "summarization"]]}, {"text": "1383encode the unstructured conversations and concentrate on the most salient utterances to generate more informative and less redundant summaries .", "entities": []}, {"text": "To do so , we view each utterance as an EDU and use the discourse relation types de\ufb01ned in Asher et al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": ".", "entities": []}, {"text": "We \ufb01rst pre - train a discourse parsing model ( Shi and Huang , 2019 ) on a humanannotated multiparty dialogue corpus ( Asher et al . , 2016 ) , with 0.775 F1 score on link predictions and 0.557 F1 score on relation classi\ufb01cations , which are comparable to the state - of - the - art results ( Shi and Huang , 2019 ) .", "entities": [[6, 8, "TaskName", "discourse parsing"], [33, 35, "MetricName", "F1 score"], [40, 42, "MetricName", "F1 score"]]}, {"text": "We then utilize this pre - trained parser to predict the discourse relations within conversations in our SAMSum corpus ( Gliwa et al . , 2019 ) .", "entities": [[17, 19, "DatasetName", "SAMSum corpus"]]}, {"text": "After predictions , there are 138,554 edges identi\ufb01ed in total and 8.48 edges per conversation .", "entities": []}, {"text": "The distribution of these predicted discourse relation types is : Comment ( 19.3 % ) , Clari\ufb01cation Question ( 15.2 % ) , Elaboration ( 2.3 % ) , Acknowledgement(8.4 % ) , Continuation ( 10.1 % ) , Explanation ( 2.8 % ) , Conditional ( 0.2 % ) , Question Answer Pair ( 21.5 % ) , Alternation ( 0.3 % ) , Q - Elab ( 2.5 % ) , Result ( 5.5 % ) , Background ( 0.4 % ) , Narration ( 0.4 % ) , Correction ( 0.4 % ) , Parallel ( 0.9 % ) , and Contrast ( 1.0 % ) .", "entities": []}, {"text": "Then for each conversation , we construct a discourse relation graph GD= ( VD;ED ) , where VD[k]represents the k - th utterance .", "entities": []}, {"text": "ED[i][j ]", "entities": []}, {"text": "= r if there is a link from the i - th utterance to the j - th one with discourse relation r. Action Graph The \u201c who - doing - what \u201d triples from utterances can provide explicit visualizations of speakers and their actions , the key to understanding concrete details happened in conversations ( Moser , 2001 ; Gee , 2014 ; Sacks et al . , 1978 ) .", "entities": []}, {"text": "Simply relying on neural models to identify this information from conversations often fail to produce factual characterizations of concrete details happened ( Cao et al . , 2018 ; Huang et al . , 2020a ) .", "entities": []}, {"text": "To this end , we extract \u201c WHO -DOING -WHAT \u201d triples from utterances and construct action graphs for conversation summarization ( Chen et al . , 2019 ; Huang et al . , 2020b , a ) .", "entities": [[20, 21, "TaskName", "summarization"]]}, {"text": "Speci\ufb01cally , we \ufb01rst transform the \ufb01rst - person point - of - view utterances to its thirdperson point - of - view forms based on simple rules : ( i ) substituting \ufb01rst / second - person pronouns with the names of current speaker or surrounding speakers and ( ii ) replacing third - person pronouns based on coreference clusters in conversations detected by the Stanford CoreNLP ( Manning et al . , 2014 ) .", "entities": []}, {"text": "For example , an utterance \u201c I \u2019ll bring it to you to - morrow \u201d from Amanda to Jerry will be transformed into \u201c Amanda\u2019ll bring cakes to Jerry tomorrow \u201d .", "entities": []}, {"text": "Then we extract \u201c WHO -DOING -WHAT \u201d ( subjectpredicate - object ) triples from transformed conversations using the open information extraction ( OpenIE ) systems1(Angeli et al . , 2015 ) .", "entities": [[19, 22, "TaskName", "open information extraction"]]}, {"text": "We then construct the Action Graph GA= ( VA;EA)from the extracted triples by taking arguments ( \u201c WHO \u201d , \u201c DOING \u201d , or \u201c WHAT \u201d ) as nodes in VA , and connect them with edge EA[i][j ] = 1 if they are adjacent in one \u201c WHO -DOING -WHAT \u201d triple .", "entities": []}, {"text": "3.2 Encoder Given a conversation and its corresponding discourse relation graph and action graph , we utilize an utterance encoder and two graph encoders , to obtain its hidden representations shown in Figure 2(a ) .", "entities": []}, {"text": "3.2.1 Utterance Encoder We initialize our utterance encoder FU(:)with a pre - trained encoder , i.e. , BART - base ( Lewis et al . , 2020 ) , and encode tokens fxi;0;:::;x i;lgin an utterance uiinto its hidden representation : fhU i;0;:::;hU i;lg = FU(fxi;0;:::;x i;lg ) ( 1 ) Here we add a special token xi;0=<S > at the beginning of each utterance to represent it .", "entities": [[17, 18, "MethodName", "BART"]]}, {"text": "3.2.2 Graph Encoder Node Initialization For discourse relation graph , we employ the output embeddings of the special tokens xi;0from", "entities": []}, {"text": "the utterance encoder , i.e. , hU i;0 , to initialize the i - th nodevD iinGD .", "entities": []}, {"text": "We use a one - hot embedding layer to encode the relations ED[i][j ] = eD i;jbetween utterance iandj .", "entities": []}, {"text": "For action graph , we \ufb01rst utilize FU(:)to encode each token in nodes vA iand then average their output embeddings as their initial representations .", "entities": []}, {"text": "Structured Graph Attention Network Based on Graph Attention Network (", "entities": [[1, 4, "MethodName", "Graph Attention Network"], [6, 9, "MethodName", "Graph Attention Network"]]}, {"text": "Veli \u02c7ckovi \u00b4 c et al . , 2018 ) , we utilize these relations between nodes to encode each node vD iinGDorvA iinGAthrough : \u000b ij = exp\u0000 \u001b\u0000 aT[WvikWvjkWeei;j]\u0001\u0001 P k2N iexp ( \u001b(aT[WvikWvkkWeei;k ] ) )", "entities": []}, {"text": "hi=\u001b(X", "entities": []}, {"text": "j2N i \u000b ijWvj ) 1https://github.com/philipperemy/ Stanford - OpenIE - Python", "entities": []}, {"text": "1384Dataset Split # Conv #", "entities": []}, {"text": "Participants # Turns #", "entities": []}, {"text": "Discourse Edges # Action Triples SAMSumTrain 14732 2.40 11.17 8.47 6.72 Val 818 2.39 10.83 8.34 6.48 Test 819 2.36 11.25 8.63 6.81 ADSC Full 45 2.00 7.51 6.51 37.20 Table 1 : Statistics of the used datasets , including the total number of conversations ( # Conv ) , the average number of participants , turns , discourse edges and action triples per conversation .", "entities": []}, {"text": "W , Weandaare trainable parameters .", "entities": []}, {"text": "[ . k. ] denotes the concatenation of two vectors .", "entities": []}, {"text": "\u001bis the activation function , Niis the set containing nodei \u2019s neighbours inG.", "entities": [[2, 4, "HyperparameterName", "activation function"]]}, {"text": "Through two graph encoders FD(:;:)and FA (: ; :) , we then obtain the hidden representations of these nodes as : fhD 0;:::;hD mg = FD(fvD 0;:::;vD mg;ED)(2 ) fhA 0;:::;hA ng = FA(fxA 0;:::;xA ng;EA ) ( 3 ) 3.3 Multi - Granularity Decoder Different levels of encoded representations are then aggregated via our multi - granularity decoder to generate summaries as shown in Figure 2(b ) .", "entities": [[5, 6, "MethodName", "FA"]]}, {"text": "With s\u00001previously generated tokens y1;:::;y s\u00001 , our decoderG(:)predicts the l - th token via : ^y = G(y1 : s\u00001;FU(C);FD(GD);FA(GA))(4 ) P(~ysjy < s;C;GD;GA )", "entities": []}, {"text": "= Softmax ( Wp^y)(5 ) To better incorporate the information in constructed graphs , different from the traditional pretrained BART model ( Lewis et al . , 2020 ) , we improve the BART transformer decoder with two extra cross attentions ( Discourse Attention and Action Attention ) added to each decoder layer , which attends to the encoded node representations in discourse relation graphs and action graphs .", "entities": [[1, 2, "MethodName", "Softmax"], [19, 20, "MethodName", "BART"], [33, 34, "MethodName", "BART"], [34, 36, "MethodName", "transformer decoder"]]}, {"text": "In each decoder layer , after performing the original cross attentions over every token in utterances fhU i;0 : lgand getting the utterance - attended representationxU , multi - granularity decoder then conducts cross attentions over nodes fhD 0 : mgandfhA 0 : ngthat are encoded from graph encoders in parallel , to obtain the discourse - attended representation xD and action - attended representation xA. These two attended vectors are then combined into a structureaware representation xS , through a feed - forward network for further forward passing in the decoder .", "entities": [[38, 39, "DatasetName", "0"], [41, 42, "DatasetName", "0"]]}, {"text": "To alleviate the negative impact of randomly initialized graph encoders and cross attentions over graphs on pre - trained BART decoders atearly stages and accelerate the learning of newlyintroduced modules during training , we apply ReZero ( Bachlechner et al . , 2020 ) to the residual connection after attending to graphs in each decoder layer : ~xS = xU+ \u000b xS(6 ) where \u000b is one trainable parameter instead of a \ufb01xed value 1 , which modulates updates from cross attentions over graphs .", "entities": [[19, 20, "MethodName", "BART"], [35, 36, "MethodName", "ReZero"], [46, 48, "MethodName", "residual connection"]]}, {"text": "Training During training , we seek to minimize the cross entropy and use the teacher - forcing strategy ( Bengio et al . , 2015 ): L=\u0000X logP(~yljy < l;C;GD;GA)(7 ) 4 Experiments 4.1 Datasets We trained and evaluated our models on a conversation summarization dataset SAMSum ( Gliwa et al . , 2019 ) covering messenger - like conversations about daily topics , such as arranging meetings and discussing events .", "entities": [[44, 45, "TaskName", "summarization"], [46, 47, "DatasetName", "SAMSum"]]}, {"text": "We also showed the generalizability of our models on the Argumentative Dialogue Summary Corpus ( ADSC ) ( Misra et al . , 2015 ) , a debate summarization corpus .", "entities": [[28, 29, "TaskName", "summarization"]]}, {"text": "The data statistics of two datasets were shown in Table 1 , with the discourse relation types distributions in the Appendix .", "entities": []}, {"text": "4.2 Baselines We compare our methods with several baselines : \u000fPointer Generator ( See et al . , 2017 ): We followed the settings in Gliwa et al .", "entities": []}, {"text": "( 2019 ) and used special tokens to separate each utterance .", "entities": []}, {"text": "\u000fTransformer ( Vaswani et al . , 2017 ): We trained transformer seq2seq models following the OpenNMT ( Klein et al . , 2017 ) .", "entities": [[12, 13, "MethodName", "seq2seq"]]}, {"text": "\u000fD - HGN ( Feng et al . , 2020 ) incorporated commonsense knowledge from ConceptNet ( Liu and Singh , 2004 ) for dialogue summarization .", "entities": [[15, 16, "DatasetName", "ConceptNet"], [25, 26, "TaskName", "summarization"]]}, {"text": "1385ModelROUGE-1 ROUGE-2 ROUGE - L F P R F P R F P R Pointer Generator ( See et al . , 2017 ) 40.08 - - 15.28 - - 36.63 - Transformer ( Vaswani et al . , 2017 ) 37.27 - - 10.76 - - 32.73 - D - HGN ( Feng et al . , 2020 ) 42.03 - - 18.07 - - 39.56 - Multi - view Seq2Seq ( Chen and Yang , 2020 ) 45.56 52.13 44.68 22.30 25.58 22.03 44.70 50.82 43.29 BART ( Lewis et al . , 2020 ) 45.15 49.58 45.97 21.66 23.95 22.16 44.46 48.92 44.26 S - BART w. Discourse y 45.89 51.34 45.87 22.50 25.26 22.33 44.83 49.93 44.17 S - BART w. Action y 45.67 50.25 46.44 22.39 24.70 22.96 44.86 49.29 44.75 S - BART w. Discourse&Action", "entities": [[1, 2, "MetricName", "ROUGE-2"], [2, 5, "MetricName", "ROUGE - L"], [32, 33, "MethodName", "Transformer"], [71, 72, "MethodName", "Seq2Seq"], [88, 89, "MethodName", "BART"], [108, 109, "MethodName", "BART"], [123, 124, "MethodName", "BART"], [138, 139, "MethodName", "BART"]]}, {"text": "y 46.07 51.13 46.24 22.60 25.11 22.81 45.00 49.82 44.47 Table 2 : ROUGE-1 , ROUGE-2 and ROUGE - L scores for different models on the SAMSum Corpus test set .", "entities": [[13, 14, "MetricName", "ROUGE-1"], [15, 16, "MetricName", "ROUGE-2"], [17, 20, "MetricName", "ROUGE - L"], [26, 28, "DatasetName", "SAMSum Corpus"]]}, {"text": "Results are averaged over three random runs .", "entities": []}, {"text": "ymeans our methods .", "entities": []}, {"text": "We performed Pitman \u2019s permutation test ( Dror et al . , 2018 ) and found that S - BART w. Discourse & Action signi\ufb01cantly outperformed the base BART ( p < 0.05 ) .", "entities": [[19, 20, "MethodName", "BART"], [28, 29, "MethodName", "BART"]]}, {"text": "ModelROUGE-1 ROUGE-2 ROUGE - L F P R F P R F P R BART ( Lewis et al . , 2020 ) 20.90 51.71 13.53 5.04 12.46 3.24 21.23 56.29 13.54 S - BART w. Discourse y 22.42 54.13 14.60 5.58 13.83 3.61 22.16 51.88 14.45 S - BART w. Action y 30.91 85.42 19.12 20.64 56.31 12.78 35.30 85.51 22.58 S - BART w. Discourse&Action y34.74 84.99 22.20 23.86 58.08 15.24 38.69 83.81 25.51 Table 3 : ROUGE-1 , ROUGE-2 and ROUGE - L scores on the out - of - domain ADSC corpus using different models trained on SAMSum Corpus .", "entities": [[1, 2, "MetricName", "ROUGE-2"], [2, 5, "MetricName", "ROUGE - L"], [14, 15, "MethodName", "BART"], [34, 35, "MethodName", "BART"], [49, 50, "MethodName", "BART"], [64, 65, "MethodName", "BART"], [79, 80, "MetricName", "ROUGE-1"], [81, 82, "MetricName", "ROUGE-2"], [83, 86, "MetricName", "ROUGE - L"], [101, 103, "DatasetName", "SAMSum Corpus"]]}, {"text": "ymeans our methods .", "entities": []}, {"text": "\u000fBART ( Lewis et al . , 2020 ): We utilized BART 2 , and separated utterances by a special token .", "entities": [[11, 12, "MethodName", "BART"]]}, {"text": "\u000fMulti - View Seq2Seq ( Chen and Yang , 2020 ) utilized topic and stage views on top of BART for summarizing conversations .", "entities": [[3, 4, "MethodName", "Seq2Seq"], [19, 20, "MethodName", "BART"]]}, {"text": "Here we implemented it based on BART - base models .", "entities": [[6, 7, "MethodName", "BART"]]}, {"text": "4.3 Implementation Details We used the BART - base model to initialize our sequence - to - sequence model for training in all experiments .", "entities": [[6, 7, "MethodName", "BART"]]}, {"text": "For parameters in the original BART encoder / decoder , we followed the default settings and set the learning rate 3e-5 with 120 warm - up steps .", "entities": [[5, 6, "MethodName", "BART"], [18, 20, "HyperparameterName", "learning rate"]]}, {"text": "For graph encoders , we set the number of hidden dimensions as 768 , the number of attention heads as 2 , the number of layers as 2 , and the dropout rate as 0.2 .", "entities": [[23, 26, "HyperparameterName", "number of layers"]]}, {"text": "For graph cross attentions added to BART decoder layers , we set the number of attention heads as 2 .", "entities": [[6, 7, "MethodName", "BART"]]}, {"text": "The weights \u000b in ReZero residual connections were initialized with 1 .", "entities": [[4, 5, "MethodName", "ReZero"]]}, {"text": "The learning rate for parameters in newly added modules was 3e-4 with 60 warm - up steps .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "All experiments were performed on GeForce RTX 2080Ti ( 11 GB memory ) .", "entities": []}, {"text": "4.4 Results on In - Domain Corpus Automatic Evaluation We evaluated all the models with the widely used automatic metric , 2The version on 10/7 in https://huggingface.co/ transformers / model_doc / bart.htmlModels Fac .", "entities": []}, {"text": "Suc .", "entities": []}, {"text": "Inf .", "entities": []}, {"text": "Ground Truth 4.29 4.40 4.06 BART 3.90 4.13 3.74 S - BART w. Discourse 4.11 4.42 3.98 S - BART w. Action 4.17 4.29 3.95 S - BART w. Discourse&Action 4.19 4.41 3.91 Table 4 : Human evaluation on Factualness , Succinctness , Informativeness .", "entities": [[5, 6, "MethodName", "BART"], [11, 12, "MethodName", "BART"], [19, 20, "MethodName", "BART"], [27, 28, "MethodName", "BART"]]}, {"text": "All model variants of S - BART received signi\ufb01cantly higher ratings than BART ( student t - test , p < 0.05 ) .", "entities": [[6, 7, "MethodName", "BART"], [12, 13, "MethodName", "BART"]]}, {"text": "ROUGE scores ( Lin and Och , 2004)3 , and reported ROUGE-1 , ROUGE-2 , and ROUGE - L in Table 2 .", "entities": [[11, 12, "MetricName", "ROUGE-1"], [13, 14, "MetricName", "ROUGE-2"], [16, 19, "MetricName", "ROUGE - L"]]}, {"text": "We found that , compared to simple sequence - to - sequence models ( Pointer Generator andTransformer ) , incorporating extra information such as commonsense knowledge from ConceptNet ( D - HGN ) increased the ROUGE metrics .", "entities": [[27, 28, "DatasetName", "ConceptNet"]]}, {"text": "When equipped with pre - trained models and simple conversation structures such as topics and conversation stages , Multi - View Seq2Seq boosted ROUGE scores .", "entities": [[21, 22, "MethodName", "Seq2Seq"]]}, {"text": "Incorporating discourse relation graphs or action graphs helped the performances of summarization , suggesting the effectiveness of explicitly modeling relations between utterances and the associations between speakers and actions within utterances .", "entities": [[11, 12, "TaskName", "summarization"]]}, {"text": "Combining two different structured graphs produced better ROUGE scores compared to previous state - of - the - art methods and our base models , 3We followed fairseq and used https://github . com / pltrdy / rouge to calculate ROUGE scores .", "entities": []}, {"text": "Note that different tools may result in different ROUGE scores .", "entities": []}, {"text": "1386with an increase of 2.0 % on ROUGE-1 , 4.3 % on ROUGE-2 , and 1.2 % on ROUGE - L compared to our base model , BART .", "entities": [[7, 8, "MetricName", "ROUGE-1"], [12, 13, "MetricName", "ROUGE-2"], [18, 21, "MetricName", "ROUGE - L"], [27, 28, "MethodName", "BART"]]}, {"text": "This indicates that , our structure - aware models with discourse and action graphs could help abstractive conversation summarization , and these two graphs complemented each other in generating better summaries .", "entities": [[18, 19, "TaskName", "summarization"]]}, {"text": "Human Evaluation We conducted human evaluation to qualitatively evaluate the generated summaries .", "entities": []}, {"text": "Speci\ufb01cally , we asked annotators from Amazon Mechanical Turk to score a set of randomly sampled 100 generated summaries from ground - truth , BART and our structured models , using a Likert scale from 1 ( worst ) to 5 ( best ) in terms of factualness ( e.g. , associates actions with the right actors ) , succinctness ( e.g. , does not contain redundant information ) , and informativeness ( e.g. , covers the most important content ) ( Feng et al . , 2020 ;", "entities": [[24, 25, "MethodName", "BART"]]}, {"text": "Huang et al . , 2020a ) .", "entities": []}, {"text": "To increase annotation quality , we required turkers to have a 98 % approval rate and at least 10,000 approved tasks for their previous work .", "entities": []}, {"text": "Each message was rated by three workers .", "entities": []}, {"text": "The scores for each summary were averaged .", "entities": []}, {"text": "The Intra - Class Correlation was 0.543 , showing moderate agreement ( Koo and Li , 2016 ) .", "entities": []}, {"text": "As shown in Table 4 , S - BART that utilized structured information from discourse relation graphs and action graphs generated signi\ufb01cantly better summaries with respect to factualness , succinctness , and informativeness .", "entities": [[8, 9, "MethodName", "BART"]]}, {"text": "This might because that the incorporation of structured information such as discourse relations helped S - BART to recognize the salient parts in conversations , and thus improve the succinctness and informativeness over BART .", "entities": [[16, 17, "MethodName", "BART"], [33, 34, "MethodName", "BART"]]}, {"text": "Modeling the connections between speakers and actions greatly helped generate more factual summaries than the baselines , e.g. , with an increase of 0.27 from BART toS - BART w. Action .", "entities": [[25, 26, "MethodName", "BART"], [28, 29, "MethodName", "BART"]]}, {"text": "4.5 Results on Out - Of - Domain Corpus To investigate the generalizability of our structureaware models , we then tested the S - BART model trained on SAMSum corpus directly on the debate summarization domain ( ADSC Corpus ( Misra et al . , 2015 ) ) in a zero - shot setting .", "entities": [[24, 25, "MethodName", "BART"], [28, 30, "DatasetName", "SAMSum corpus"], [34, 35, "TaskName", "summarization"]]}, {"text": "Besides the differences in topics , utterances in debate conversations were generally longer and include more action triples ( 37.20 vs 6.81 as shown in Table 1 ) and fewer participants .", "entities": []}, {"text": "The distribution of discourse relation types also differed a lot across differentGraph Types R-1 R-2 R - L S - BART w. Discourse Graph 45.89 22.50 44.83 S - BART w. Random Graph 45.28 21.80 44.30", "entities": [[20, 21, "MethodName", "BART"], [29, 30, "MethodName", "BART"]]}, {"text": "Table 5 : ROUGE-1 , ROUGE-2 and ROUGE - L scores of S - BART with either the constructed discourse relation graphs or random graphs .", "entities": [[3, 4, "MetricName", "ROUGE-1"], [5, 6, "MetricName", "ROUGE-2"], [7, 10, "MetricName", "ROUGE - L"], [14, 15, "MethodName", "BART"]]}, {"text": "Results are averaged over three random runs .", "entities": []}, {"text": "Combination Strategy R-1 R-2 R - L Parallel 46.07 22.60 45.00 Sequential ( discourse , action ) 45.40 22.14 44.67 Sequential ( action , discourse ) 45.62 22.41 44.62 Table 6 : ROUGE-1 , ROUGE-2 and ROUGE - L scores of S - BART models using different ways to combine discourse relation graphs and action graphs .", "entities": [[32, 33, "MetricName", "ROUGE-1"], [34, 35, "MetricName", "ROUGE-2"], [36, 39, "MetricName", "ROUGE - L"], [43, 44, "MethodName", "BART"]]}, {"text": "Results are averaged over three random runs .", "entities": []}, {"text": "domains4(e.g . , more Contrast in debates ( 19.5 % ) than in daily conversations ( 1.0 % ) ) .", "entities": []}, {"text": "As shown in Table 3 , our single graph modelsS - BART w. Discourse andS - BART w. Action boosted ROUGE scores compared to BART , suggesting that utilizing structures can also increase the generalizability of conversation summarization methods .", "entities": [[11, 12, "MethodName", "BART"], [16, 17, "MethodName", "BART"], [24, 25, "MethodName", "BART"], [37, 38, "TaskName", "summarization"]]}, {"text": "However , contrary to in - domain results in Table 2 , action graphs led to much more gains than discourse graphs .", "entities": []}, {"text": "This indicated that when domain shifts , action triples were most robust in terms of zero - shot setups ; differences in discourse relation distributions could limit such generalization .", "entities": []}, {"text": "Consistent with in - domain scenarios , our S - BART w. Discourse&Action achieved better results , with an increase of 66.2 % on ROUGE-1 , 373.4 % on ROUGE-2 , and 82.2 % on ROUGE - L over BART .", "entities": [[10, 11, "MethodName", "BART"], [24, 25, "MetricName", "ROUGE-1"], [29, 30, "MetricName", "ROUGE-2"], [35, 38, "MetricName", "ROUGE - L"], [39, 40, "MethodName", "BART"]]}, {"text": "4.6 Ablation Studies This part conducted ablation studies to show the effectiveness of structured graphs in our S - BART .", "entities": [[19, 20, "MethodName", "BART"]]}, {"text": "The Quality of Discourse Relation Graphs We showed how the quality of discourse relation graphs affected the performances of conversation summarization in Table 5 .", "entities": [[20, 21, "TaskName", "summarization"]]}, {"text": "Speci\ufb01cally , we compared the ROUGE scores of S - BART using our constructed discourse relation graphs ( S - BART w. Discourse Graph ) and S - BART using randomly generated discourse relation graphs S - BART w. Random Graph where both connections between nodes and relation types were randomized .", "entities": [[10, 11, "MethodName", "BART"], [20, 21, "MethodName", "BART"], [28, 29, "MethodName", "BART"], [37, 38, "MethodName", "BART"]]}, {"text": "The number of edges in two graphs was kept the same .", "entities": []}, {"text": "We found that S - BART with our discourse graphs outperformed 4The detailed distributions were shown in the Appendix .", "entities": [[5, 6, "MethodName", "BART"]]}, {"text": "1387 Figure 3 : Averaged \u000b over decoder layers in the trained S - BART models using different graphs models with random graphs , indicating the effectiveness of the constructed discourse relation graphs and the importance of their qualities .", "entities": [[14, 15, "MethodName", "BART"]]}, {"text": "Different Ways to Combine Graphs We experimented with different ways to combine discourse relation graphs and action graphs in our S - BART w. Discourse & Action , and presented the results in Table 6 .", "entities": [[22, 23, "MethodName", "BART"]]}, {"text": "Here , parallel strategy performed cross attentions on different graphs separately and then combined the attended results with feed - forward networks as discussed in Section 3.3 ; sequential strategy performed cross attentions on two graphs in a speci\ufb01c order ( from discourse relation graphs to actions graphs , or vice versa ) .", "entities": []}, {"text": "We found that the parallel strategy showed better performances and the sequential ones did not introduce gains compared to S - BART with single graphs .", "entities": [[21, 22, "MethodName", "BART"]]}, {"text": "This demonstrates that discourse relation graphs and action graphs were both important and provided different signals for abstractive conversation summarization .", "entities": [[19, 20, "TaskName", "summarization"]]}, {"text": "Visualizing ReZero Weights We further tested our structure - aware BART with two ReZero settings : ( i ) initializing \u000b from 0 , ( ii ) initializing \u000b  from 1 , and found initializing \u000b from 1 would bring in more performance gains ( see Appendix ) .", "entities": [[1, 2, "MethodName", "ReZero"], [10, 11, "MethodName", "BART"], [13, 14, "MethodName", "ReZero"], [22, 23, "DatasetName", "0"]]}, {"text": "We then visualized the average \u000b over different decoder layers after training in Figure 3 , and observed that ( i ) when \u000b was initialized with 1 , the \ufb01nal \u000b was much larger than the setting where \u000b was initialized with 0 , which might because randomly initialized modules barely received supervisions at early stages and therefore contributes less to BART .", "entities": [[43, 44, "DatasetName", "0"], [62, 63, "MethodName", "BART"]]}, {"text": "( ii ) Compared to discourse graphs , action graphs received higher \u000b weights after training in both initializing settings , suggesting that the information from structured action graphs might be harder for the end - to - end BART models to capture .", "entities": [[39, 40, "MethodName", "BART"]]}, {"text": "( iii ) Utilizing both graphs spontaneously led to higherConversations # Num # Dis .", "entities": []}, {"text": "# Act .", "entities": []}, {"text": "Test Set 819 8.63 6.81 Similar 373 8.31 6.36 Increase 208 9.13 7.40", "entities": []}, {"text": "Challenging 160 9.58 7.85 Table 7 : The total number of examples , average number of Discourse edges and Action triples in different set of conversations in the SAMSUM test set .", "entities": [[28, 29, "DatasetName", "SAMSUM"]]}, {"text": "ReZero weights , further validating the effectiveness of combining discourse relation graphs and action graphs and their complementary properties .", "entities": [[0, 1, "MethodName", "ReZero"]]}, {"text": "4.7 Error Analyses To inspect when our summarization models could help the conversations summarization , we visualized the average number of discourse edges and the average number of action triples in three sets of conversations in Table 7 : ( i ) Similar : examples where S - BART generated similar ROUGE scores ( the differences were less than 0.1 ) compared to BART ; ( ii)Increase : examples where S - BART resulted in higher ROUGE scores ( the differences were larger than 1.0 ) compared to BART ; ( iii ) Challenging : examples where both S - BART andBART showed low ROUGE scores ( ROUGE-1 < 20.0 , ROUGE-2 < 10.0 , ROUGE - L < 10.0 ) .", "entities": [[1, 2, "MetricName", "Error"], [7, 8, "TaskName", "summarization"], [13, 14, "TaskName", "summarization"], [48, 49, "MethodName", "BART"], [63, 64, "MethodName", "BART"], [72, 73, "MethodName", "BART"], [88, 89, "MethodName", "BART"], [100, 101, "MethodName", "BART"], [107, 108, "MetricName", "ROUGE-1"], [111, 112, "MetricName", "ROUGE-2"], [115, 118, "MetricName", "ROUGE - L"]]}, {"text": "When the structures in conversations were simpler ( fewer discourse edges and fewer action triples than the average ) , BART showed similar performance as S - BART .", "entities": [[20, 21, "MethodName", "BART"], [27, 28, "MethodName", "BART"]]}, {"text": "As the structures of conversations become more complex with more discourse relations and more action mentions , S - BART outperformed BART as it explicitly incorporated these structured graphs .", "entities": [[19, 20, "MethodName", "BART"], [21, 22, "MethodName", "BART"]]}, {"text": "However , both BART andSBART struggled when there were much more interactions beyond certain thresholds , calling for better mechanisms to model structures in conversations for generating better summaries .", "entities": [[3, 4, "MethodName", "BART"]]}, {"text": "5 Conclusion In this work , we introduced a structure - aware sequence - to - sequence model for abstractive conversation summarization by incorporating discourse relations between utterances , and the connections between speakers and actions within utterances .", "entities": [[21, 22, "TaskName", "summarization"]]}, {"text": "Experiments and ablation studies on SAMSum corpus showed the effectiveness of these structured graphs in aiding the task of conversation summarization via both quantitative and qualitative eval-", "entities": [[5, 7, "DatasetName", "SAMSum corpus"], [20, 21, "TaskName", "summarization"]]}, {"text": "1388uation metrics .", "entities": []}, {"text": "Results in zero - shot settings on ADCS Corpus further demonstrated the generalizability of our structure - aware models .", "entities": []}, {"text": "In the future , we plan to extend our current conversation summarization models for various application domains such as emails , debates , and podcasts , and in conversations that might involve longer utterances and more participants in an unsynchronized way .", "entities": [[11, 12, "TaskName", "summarization"]]}, {"text": "Acknowledgement We would like to thank the anonymous reviewers for their helpful comments , and the members of Georgia Tech SALT group for their feedback .", "entities": []}, {"text": "This work is supported in part by grants from Google , Amazon and Salesforce .", "entities": [[9, 10, "DatasetName", "Google"]]}, {"text": "References Gabor Angeli , Melvin Jose Johnson Premkumar , and Christopher D. Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Leveraging linguistic structure for open domain information extraction .", "entities": []}, {"text": "InProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 344\u2013354 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nicholas Asher , Julie Hunter , Mathieu Morey , Benamara Farah , and Stergos Afantenos .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Discourse structure and dialogue acts in multiparty dialogue : the stac corpus .", "entities": [[10, 11, "MethodName", "stac"]]}, {"text": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC\u201916 ) , pages 2721\u20132727 .", "entities": []}, {"text": "Thomas Bachlechner , Bodhisattwa Prasad Majumder , Huanru Henry Mao , Garrison W Cottrell , and Julian McAuley .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Rezero is all you need : Fast convergence at large depth .", "entities": [[0, 1, "MethodName", "Rezero"]]}, {"text": "arXiv preprint arXiv:2003.04887 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Samy Bengio , Oriol Vinyals , Navdeep Jaitly , and Noam Shazeer .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Scheduled sampling for sequence prediction with recurrent neural networks .", "entities": []}, {"text": "InAdvances in Neural Information Processing Systems , pages 1171\u20131179 .", "entities": []}, {"text": "Trung Bui , Matthew Frampton , John Dowding , and Stanley Peters .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Extracting decisions from multi - party dialogue using directed graphical models and semantic similarity .", "entities": [[12, 14, "TaskName", "semantic similarity"]]}, {"text": "In Proceedings of the SIGDIAL 2009 Conference , pages 235\u2013243 , London , UK . Association for Computational Linguistics .", "entities": []}, {"text": "Ziqiang Cao , Furu Wei , W. Li , and Sujian Li .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Faithful to the original : Fact aware neural abstractive summarization .", "entities": [[9, 10, "TaskName", "summarization"]]}, {"text": "In AAAI .", "entities": []}, {"text": "Jiaao Chen , Jianshu Chen , and Zhou Yu . 2019 .", "entities": []}, {"text": "Incorporating structured commonsense knowledge instory completion .", "entities": []}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 33 , pages 6244\u20136251 .", "entities": []}, {"text": "Jiaao Chen and Diyi Yang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multi - view sequenceto - sequence models with conversational structure for abstractive dialogue summarization .", "entities": [[13, 14, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 4106\u20134118 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yue Dong , Shuohang Wang , Zhe Gan , Yu Cheng , Jackie Chi Kit Cheung , and Jingjing Liu . 2020 .", "entities": []}, {"text": "Multi - fact correction in abstractive text summarization .", "entities": [[5, 8, "TaskName", "abstractive text summarization"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 9320\u20139331 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rotem Dror , Gili Baumer , Segev Shlomov , and Roi Reichart .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The hitchhiker \u2019s guide to testing statistical signi\ufb01cance in natural language processing .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1383\u20131392 .", "entities": []}, {"text": "Angela Fan , Claire Gardent , Chlo\u00e9 Braud , and Antoine Bordes .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Using local knowledge graph construction to scale seq2seq models to multi - document inputs .", "entities": [[3, 5, "TaskName", "graph construction"], [7, 8, "MethodName", "seq2seq"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 4177\u20134187 .", "entities": []}, {"text": "Xiachong Feng , Xiaocheng Feng , Bing Qin , and Ting Liu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Incorporating commonsense knowledge into abstractive dialogue summarization via heterogeneous graph networks .", "entities": [[6, 7, "TaskName", "summarization"]]}, {"text": "arXiv preprint arXiv:2010.10044 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Patrick Fernandes , Miltiadis Allamanis , and Marc Brockschmidt .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Structured neural summarization .", "entities": [[2, 3, "TaskName", "summarization"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Michel Galley .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "A skip - chain conditional random \ufb01eld for ranking meeting utterances by importance .", "entities": []}, {"text": "InProceedings of the 2006 Conference on Empirical Methods in Natural Language Processing , pages 364\u2013372 .", "entities": []}, {"text": "Shen Gao , Xiuying Chen , Zhaochun Ren , Dongyan Zhao , and Rui Yan . 2020 .", "entities": []}, {"text": "From standard summarization to new tasks and beyond : Summarization with manifold information .", "entities": [[2, 3, "TaskName", "summarization"], [9, 10, "TaskName", "Summarization"]]}, {"text": "arXiv preprint arXiv:2005.04684 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "James Paul Gee . 2014 .", "entities": []}, {"text": "An introduction to discourse analysis : Theory and method .", "entities": []}, {"text": "Routledge .", "entities": []}, {"text": "Bogdan Gliwa , Iwona Mochol , Maciej Biesek , and Aleksander Wawer .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SAMSum corpus : A human - annotated dialogue dataset for abstractive summarization .", "entities": [[0, 12, "DatasetName", "SAMSum corpus : A human - annotated dialogue dataset for abstractive summarization"]]}, {"text": "In Proceedings of the 2nd Workshop", "entities": []}, {"text": "1389on New Frontiers in Summarization , pages 70\u201379 , Hong Kong , China .", "entities": [[4, 5, "TaskName", "Summarization"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chih - Wen Goo and Yun - Nung Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Abstractive dialogue summarization with sentence - gated modeling optimized by dialogue acts .", "entities": [[2, 3, "TaskName", "summarization"]]}, {"text": "2018 IEEE Spoken Language Technology Workshop ( SLT ) .", "entities": []}, {"text": "Vishal Gupta and Gurpreet Singh Lehal .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "A survey of text summarization extractive techniques .", "entities": [[3, 5, "TaskName", "text summarization"]]}, {"text": "Journal of emerging technologies in web intelligence , 2(3):258\u2013268 .", "entities": []}, {"text": "Luyang Huang , Lingfei Wu , and Lu Wang .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "Knowledge graph - augmented abstractive summarization with semantic - driven cloze reward .", "entities": [[5, 6, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5094 \u2013 5107 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shanshan Huang , Kenny Q. Zhu , Qianzi Liao , Libin Shen , and Yinggong Zhao .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Enhanced story representation by conceptnet for predicting story endings .", "entities": [[4, 5, "DatasetName", "conceptnet"]]}, {"text": "In Proceedings of the 29th ACM International Conference on Information Knowledge Management , CIKM \u2019 20 , page 3277\u20133280 , New York , NY , USA .", "entities": [[5, 6, "DatasetName", "ACM"], [11, 12, "TaskName", "Management"]]}, {"text": "Association for Computing Machinery .", "entities": []}, {"text": "Paul A Kirschner , Simon J Buckingham - Shum , and Chad S Carr . 2012 .", "entities": []}, {"text": "Visualizing argumentation : Software tools for collaborative and educational sensemaking .", "entities": []}, {"text": "Springer Science & Business Media .", "entities": []}, {"text": "Guillaume Klein , Yoon Kim , Yuntian Deng , Jean Senellart , and Alexander M. Rush .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Opennmt : Open - source toolkit for neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "Terry K Koo and Mae Y Li .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A guideline of selecting and reporting intraclass correlation coef\ufb01cients for reliability research .", "entities": []}, {"text": "Journal of chiropractic medicine , 15(2):155\u2013163 .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "BART :", "entities": [[0, 1, "MethodName", "BART"]]}, {"text": "Denoising sequence - to - sequence pretraining for natural language generation , translation , and comprehension .", "entities": [[0, 1, "TaskName", "Denoising"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871\u20137880 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Manling Li , Lingyu Zhang , Heng Ji , and Richard J. Radke .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Keep meeting summaries on topic : Abstractive multi - modal meeting summarization .", "entities": [[10, 12, "TaskName", "meeting summarization"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2190\u20132196 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Chin - Yew Lin and Franz Josef Och . 2004 .", "entities": []}, {"text": "Automatic evaluation of machine translation quality using longest common subsequence and skip - bigramstatistics .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics , page 605 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chunyi Liu , Peng Wang , Jiang Xu , Zang Li , and Jieping Ye . 2019a .", "entities": []}, {"text": "Automatic dialogue summary generation for customer service .", "entities": []}, {"text": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , KDD19 , page 1957\u20131965 , New York , NY , USA . Association for Computing Machinery .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "H. Liu and P. Singh .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Conceptnet \u2014 a practical commonsense reasoning tool - kit .", "entities": [[0, 1, "DatasetName", "Conceptnet"]]}, {"text": "BT Technology Journal , 22(4):211\u2013226 .", "entities": []}, {"text": "Yang Liu and Mirella Lapata .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Text summarization with pretrained encoders .", "entities": [[0, 2, "TaskName", "Text summarization"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3721\u20133731 .", "entities": []}, {"text": "Zhengyuan Liu , Angela Ng , Sheldon Lee , Ai Ti", "entities": []}, {"text": "Aw , and Nancy F. Chen . 2019b .", "entities": []}, {"text": "Topic - aware pointergenerator networks for summarizing spoken conversations .", "entities": []}, {"text": "2019 IEEE Automatic Speech Recognition and Understanding Workshop ( ASRU ) .", "entities": [[2, 5, "TaskName", "Automatic Speech Recognition"]]}, {"text": "Christopher Manning , Mihai Surdeanu , John Bauer , Jenny Finkel , Steven Bethard , and David McClosky .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The Stanford CoreNLP natural language processing toolkit .", "entities": []}, {"text": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 55\u201360 , Baltimore , Maryland .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Joshua Maynez , Shashi Narayan , Bernd Bohnet , and Ryan McDonald . 2020 .", "entities": []}, {"text": "On faithfulness and factuality in abstractive summarization .", "entities": [[6, 7, "TaskName", "summarization"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1906\u20131919 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "K. McKeown , Lokesh Shrestha , and Owen Rambow .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Using question - answer pairs in extractive summarization of email conversations .", "entities": [[6, 8, "TaskName", "extractive summarization"]]}, {"text": "In CICLing .", "entities": []}, {"text": "Amita Misra , Pranav Anand , Jean E. Fox Tree , and Marilyn Walker . 2015 .", "entities": []}, {"text": "Using summarization to discover argument facets in online idealogical dialog .", "entities": [[1, 2, "TaskName", "summarization"]]}, {"text": "InProceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 430\u2013440 , Denver , Colorado .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Beverly Moser .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "An introduction to discourse analysis .", "entities": []}, {"text": "theory and method .", "entities": []}, {"text": "Gabriel Murray , S. Renals , and J. Carletta .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Extractive summarization of meeting recordings .", "entities": [[0, 2, "TaskName", "Extractive summarization"]]}, {"text": "In INTERSPEECH .", "entities": []}, {"text": "1390Gabriel Murray , Steve Renals , Jean Carletta , and Johanna Moore .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Incorporating speaker and discourse features into speech summarization .", "entities": [[7, 8, "TaskName", "summarization"]]}, {"text": "In Proceedings of the Human Language Technology Conference of the NAACL , Main Conference , pages 367 \u2013 374 , New York City , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Shashi Narayan , Shay B Cohen , and Mirella Lapata .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Ranking sentences for extractive summarization with reinforcement learning .", "entities": [[3, 5, "TaskName", "extractive summarization"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1747\u20131759 .", "entities": []}, {"text": "Romain Paulus , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A deep reinforced model for abstractive summarization .", "entities": [[6, 7, "TaskName", "summarization"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Kechen Qin , Lu Wang , and Joseph Kim . 2017 .", "entities": []}, {"text": "Joint modeling of content and discourse relations in dialogues .", "entities": []}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 974\u2013984 .", "entities": []}, {"text": "Alexander M. Rush , Sumit Chopra , and Jason Weston .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A neural attention model for abstractive sentence summarization .", "entities": [[6, 8, "TaskName", "sentence summarization"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 379\u2013389 , Lisbon , Portugal . Association for Computational Linguistics .", "entities": []}, {"text": "Harvey Sacks , Emanuel A Schegloff , and Gail Jefferson .", "entities": []}, {"text": "1978 .", "entities": []}, {"text": "A simplest systematics for the organization of turn taking for conversation .", "entities": []}, {"text": "In Studies in the organization of conversational interaction , pages 7 \u2013 55 .", "entities": []}, {"text": "Elsevier .", "entities": []}, {"text": "Abigail See , Peter J. Liu , and Christopher D. Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Get to the point : Summarization with pointergenerator networks .", "entities": [[5, 6, "TaskName", "Summarization"]]}, {"text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) .", "entities": []}, {"text": "Guokan Shang , Wensi Ding , Zekun Zhang , Antoine Tixier , Polykarpos Meladianos , Michalis Vazirgiannis , and Jean - Pierre Lorr\u00e9 .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Unsupervised abstractive meeting summarization with multisentence compression and budgeted submodular maximization .", "entities": [[2, 4, "TaskName", "meeting summarization"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 664 \u2013 674 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Zhouxing Shi and Minlie Huang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A deep sequential model for discourse parsing on multi - party dialogues .", "entities": [[5, 7, "TaskName", "discourse parsing"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 33 , pages 7007\u20137014 .", "entities": []}, {"text": "Kaiqiang Song , Chen Li , Xiaoyang Wang , Dong Yu , and Fei Liu . 2020 .", "entities": []}, {"text": "The ucf podcast summarization system at trec 2020 .", "entities": [[3, 4, "TaskName", "summarization"], [6, 7, "DatasetName", "trec"]]}, {"text": "arXiv preprint arXiv:2011.04132 .Matthew", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Stone , Una Stojnic , and Ernest Lepore .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Situated utterances and discourse relations .", "entities": []}, {"text": "In Proceedings of the 10th International Conference on Computational Semantics ( IWCS 2013)\u2013Short Papers , pages 390\u2013396 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008 .", "entities": []}, {"text": "Petar Veli \u02c7ckovi \u00b4 c , Guillem Cucurull , Arantxa Casanova , Adriana Romero , Pietro Li\u00f2 , and Yoshua Bengio .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graph attention networks .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Lu Wang and Claire Cardie .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Domainindependent abstract generation for focused meeting summarization .", "entities": [[5, 7, "TaskName", "meeting summarization"]]}, {"text": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1395 \u2013 1405 .", "entities": []}, {"text": "Jiacheng Xu , Zhe Gan , Yu Cheng , and Jingjing Liu . 2020 .", "entities": []}, {"text": "Discourse - aware neural extractive text summarization .", "entities": [[4, 7, "TaskName", "extractive text summarization"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5021\u20135031 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jingqing Zhang , Yao Zhao , Mohammad Saleh , and Peter J Liu . 2019 .", "entities": []}, {"text": "Pegasus : Pre - training with extracted gap - sentences for abstractive summarization .", "entities": [[12, 13, "TaskName", "summarization"]]}, {"text": "arXiv preprint arXiv:1912.08777 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhou Zhao , Haojie Pan , Changjie Fan , Yan Liu , Linlin Li , Min Yang , and Deng Cai . 2019 .", "entities": []}, {"text": "Abstractive meeting summarization via hierarchical adaptive segmental network learning .", "entities": [[1, 3, "TaskName", "meeting summarization"]]}, {"text": "In The World Wide Web Conference , WWW \u2019 19 , page 3455\u20133461 , New York , NY , USA .", "entities": []}, {"text": "Association for Computing Machinery .", "entities": []}, {"text": "Chenguang Zhu , William Hinthorn , Ruochen Xu , Qing kai Zeng , Michael Zeng , Xuedong Huang , and Meng Jiang . 2020a .", "entities": []}, {"text": "Boosting factual correctness of abstractive summarization with knowledge graph .", "entities": [[5, 6, "TaskName", "summarization"]]}, {"text": "ArXiv , abs/2003.08612 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Chenguang Zhu , Ruochen Xu , Michael Zeng , and Xuedong Huang .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "A hierarchical network for abstractive meeting summarization with cross - domain pretraining .", "entities": [[5, 7, "TaskName", "meeting summarization"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "A Discourse Relation Distributions We pre - trained a deep sequential model ( Shi and Huang , 2019 ) on STAC Corpus ( 1,062 dialogues )", "entities": [[20, 21, "MethodName", "STAC"]]}, {"text": "1391ModelROUGE-1 ROUGE-2 ROUGE - L F P R F P R F P R BART ( Lewis et al . , 2020 ) 45.15 49.58 45.97 21.66 23.95 22.16 44.46 48.92 44.26 S - BART w. Discourse \u000b = 0 y 45.40 50.22 45.86 21.96 24.49 22.25 44.56 49.32 44.13 S - BART w. Action \u000b = 0 y 45.47 50.82 45.42 22.23 24.96 22.34 44.55 49.69 43.75 S - BART w. Discourse&Action \u000b = 0 y45.59 51.47 45.09 22.42 25.51 22.27 44.67 50.24 43.52 S - BART w. Discourse \u000b = 1 y 45.89 51.34 45.87 22.50 25.26 22.33 44.83 49.93 44.17 S - BART w. Action \u000b = 1 y 45.67 50.25 46.44 22.39 24.70 22.96 44.86 49.29 44.75 S - BART w. Discourse&Action", "entities": [[1, 2, "MetricName", "ROUGE-2"], [2, 5, "MetricName", "ROUGE - L"], [14, 15, "MethodName", "BART"], [34, 35, "MethodName", "BART"], [39, 40, "DatasetName", "0"], [52, 53, "MethodName", "BART"], [57, 58, "DatasetName", "0"], [70, 71, "MethodName", "BART"], [75, 76, "DatasetName", "0"], [87, 88, "MethodName", "BART"], [105, 106, "MethodName", "BART"], [123, 124, "MethodName", "BART"]]}, {"text": "= 1 y46.07 51.13 46.24 22.60 25.11 22.81 45.00 49.82 44.47 Table 8 : Results on SAMSum Corpus .", "entities": [[16, 18, "DatasetName", "SAMSum Corpus"]]}, {"text": "ROUGE-1 , ROUGE-2 and ROUGE - L scores for different models on the test set .", "entities": [[0, 1, "MetricName", "ROUGE-1"], [2, 3, "MetricName", "ROUGE-2"], [4, 7, "MetricName", "ROUGE - L"]]}, {"text": "Results are averaged over three random runs .", "entities": []}, {"text": "ymeans our methods .", "entities": []}, {"text": "Discourse Type SAMSum ADSC Comment 19.3 % 42.7 % Clari\ufb01cation 15.2 % 13.3 % Elaboration 2.3 % 0.1 % Acknowlegement 8.4 % 0.9 % Explanation 2.8 % 0.3 % Conditional 0.2 % 0 % QA pair 21.5 % 12.3 % Alternation 0.3 % 0.6 % Result 5.5 % 0.2 % Backgraound 0.4 % 0 % Narration 0.4 % 0 % Correction 0.4 % 1.1 % Continuation 0.9 % 7.5 % Q - Elab 2.5 % 0 % Parallel 0.9 % 0 % Contrast 1.0 % 19.5 % Table 9 : The distribution of predicted discourse relation types on SAMSum Corpus and ADSC Corpus .", "entities": [[2, 3, "DatasetName", "SAMSum"], [32, 33, "DatasetName", "0"], [53, 54, "DatasetName", "0"], [58, 59, "DatasetName", "0"], [75, 76, "DatasetName", "0"], [80, 81, "DatasetName", "0"], [98, 100, "DatasetName", "SAMSum Corpus"]]}, {"text": "( Asher et al . , 2016 ) with default settings5to get the link prediction and relation classi\ufb01cation models to label discourse relations in SAMSum and ADSC corpus .", "entities": [[13, 15, "TaskName", "link prediction"], [24, 25, "DatasetName", "SAMSum"]]}, {"text": "The distribution of the relation types in two datasets were shown in Table 9 .", "entities": []}, {"text": "The major discourse relations in daily conversations are Comment , Clari\ufb01cation and QA pairs , while the main discourse relations in debate are Comment , Contrast , Clari\ufb01cation and QA pairs .", "entities": []}, {"text": "B Impact of Different ReZero Weight Initializations We tested our structure - aware BART ( S - BART w. Discourse / Action ) within two ReZero settings : ( i ) initializing \u000b from 0 , ( ii ) initializing \u000b from 1 .", "entities": [[4, 5, "MethodName", "ReZero"], [13, 14, "MethodName", "BART"], [17, 18, "MethodName", "BART"], [25, 26, "MethodName", "ReZero"], [34, 35, "DatasetName", "0"]]}, {"text": "And the results were shown in Table 8 .", "entities": []}, {"text": "S - BART with 1 as the initialized ReZero weight outperformed 5https://github.com/shizhouxing/ DialogueDiscourseParsingthat with 0 under under all graph settings , suggesting utilizing more information from graphs would bring in more performance boosts .", "entities": [[2, 3, "MethodName", "BART"], [8, 9, "MethodName", "ReZero"], [14, 15, "DatasetName", "0"]]}]