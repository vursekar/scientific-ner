[{"text": "Proceedings of the Second Workshop on Economics and Natural Language Processing , pages 41\u201350 Hong Kong , November 4 . c", "entities": []}, {"text": "2019 Association for Computational Linguistics41Group , Extract and Aggregate : Summarizing a Large Amount of Finance News for Forex Movement Prediction Deli Chen1\u0003 , Shuming Ma1 , Keiko Harimoto2 , Ruihan Bao2 , Qi Su1 , Xu Sun1 1MOE Key Lab of Computational Linguistics , School of EECS , Peking University 2Mizuho Securities Co. , Ltd. fchendeli , shumingma , sukia , xusun g@pku.edu.cn , fkeiko.harimoto , ruihan.bao g@mizuho-sc.com Abstract Incorporating related text information has proven successful in stock market prediction .", "entities": [[78, 81, "TaskName", "stock market prediction"]]}, {"text": "However , it is a huge challenge to utilize texts in the enormous forex ( foreign currency exchange ) market because the associated texts are too redundant .", "entities": []}, {"text": "In this work , we propose a BERT - based Hierarchical Aggregation Model to summarize a large amount of \ufb01nance news to predict forex movement .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "We \ufb01rstly group news from different aspects : time , topic and category .", "entities": []}, {"text": "Then we extract the most crucial news in each group by the SOTA extractive summarization method .", "entities": [[13, 15, "TaskName", "extractive summarization"]]}, {"text": "Finally , we conduct interaction between the news and the trade data with attention to predict the forex movement .", "entities": []}, {"text": "The experimental results show that the category based method performs best among three grouping methods and outperforms all the baselines .", "entities": []}, {"text": "Besides , we study the in\ufb02uence of essential news attributes ( category and region ) by statistical analysis and summarize the in\ufb02uence patterns for different currency pairs .", "entities": []}, {"text": "1 Introduction Deep learning and Natural Language Processing technologies have been widely applied in market prediction tasks ( Strau\u00df et al . , 2018 ; Alostad and Davulcu , 2017 ; Li et al . , 2015 ; Ni et al . , 2019 ) , and the market related \ufb01nance news has proven very useful for the prediction ( Ding et al . , 2016 ; Xu and Cohen , 2018 ) .", "entities": []}, {"text": "However , the studies of prediction in forex market , which is the largest market in the world with the highest daily trading volume , is much less than that in the stock market .", "entities": []}, {"text": "Figure 1 shows the average numbers per hour of forex related news .", "entities": []}, {"text": "There is a large amount of \ufb01nance news related to forex trading with different in\ufb02uence , so it is a huge challenge to extract the useful semantic information from news .", "entities": []}, {"text": "Most of previous works ( Bakhach et al . , 2016 ; Shen and Liang , \u0003This work is done when Deli Chen is a intern at Mizuho Securities .", "entities": []}, {"text": "329 234331 282 050100150200250300350 US_EU US_JP US_CN US_GBFigure 1 : Average numbers per hour of forex related news from Reuters in 2013 - 2017 .", "entities": []}, {"text": "USEUrepresents news related to US , Europe or both of them .", "entities": []}, {"text": "2016 ; Pradeepkumar and Ravi , 2016 ; Contreras et al . , 2018 ; Weeraddana et al . , 2018 ) on forex prediction ignore related text totally and focus on the forex trade data only , which loses the important semantic information .", "entities": []}, {"text": "Yet existing works ( Seifollahi and Shajari , 2019 ; Nassirtoussi et al . , 2015 ) applying \ufb01nance news in forex prediction mainly rely on manual rules to build feature vectors , which can hardly access the semantic information effectively .", "entities": []}, {"text": "To make better use of \ufb01nance news , we propose a novel neural model : Bert - based Hierarchical Aggregation Model ( BHAM ) to summarize a large amount of \ufb01nance news for forex movement prediction .", "entities": []}, {"text": "We suppose that the \ufb01nance news is redundant and only a small amount of news plays a crucial role in forex trading .", "entities": []}, {"text": "So the key point is how to extract the most important news .", "entities": []}, {"text": "In BHAM , we design a hierarchical structure to extract essential news at the group level \ufb01rst and then aggregate the semantic information across all groups .", "entities": []}, {"text": "We expect the news is more related intragroup and less related inter - groups to make the extraction more effective .", "entities": []}, {"text": "We design three grouping methods from different aspects : time , topic or category .", "entities": []}, {"text": "At the group level , we concatenate news headlines in the same group and regard news extraction in each group as an extractive summarization task .", "entities": [[22, 24, "TaskName", "extractive summarization"]]}, {"text": "We modify the SOTA extractive summarization model proposed in ( Liu , 2019 ) to select the most important news .", "entities": [[4, 6, "TaskName", "extractive summarization"]]}, {"text": "The connection process", "entities": []}, {"text": "42can let the selected news both content aware and context aware .", "entities": []}, {"text": "Followingly , we conduct multimodal interaction between news data and trade data through attention mechanism to predict the forex prediction .", "entities": []}, {"text": "The trade data represents the history movement of the forex , and the news data represents the environment variable .", "entities": []}, {"text": "These two types of information are highly related .", "entities": []}, {"text": "We conduct experiments on four major currency pairs ( USD - EUR , USD - JPY , USD - RMB , USDGBP ) , and the experimental results show that the category - based BHAM performs best among all the baselines and proposed methods in all currency pairs .", "entities": []}, {"text": "Based on this method , we analyze the in\ufb02uence of input time and prediction time on forex trading .", "entities": []}, {"text": "We also analyze the in\ufb02uence of news category and news region and \ufb01nd various in\ufb02uence patterns for different currency pairs , which may be enlightening to the forex investors .", "entities": []}, {"text": "The main contributions of this works are summarized as follows : \u000fWe design a novel neural model to incorporate \ufb01nance news in forex movement prediction .", "entities": []}, {"text": "To the best of our knowledge , this is the \ufb01rst work to use the neural model to summarize a large amount of news for forex movement prediction .", "entities": []}, {"text": "\u000fWe propose three news grouping methods from different aspects : time , topic and category .", "entities": []}, {"text": "Experiments show that the category based method performs best and outperforms all the baselines .", "entities": []}, {"text": "\u000fBased on our experiments , we study the effect of time parameters on forex trading .", "entities": []}, {"text": "We also analyze and summarize different in\ufb02uence patterns of \ufb01nance news ( both category and region ) on different currency pairs .", "entities": []}, {"text": "2 Related Work BERT ( Devlin et al . , 2018 ) is a potent pretrained contextualized sentence representation and has proven obvious improvement for many NLP tasks ( Sun et al . , 2019 ; Xu et", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "al . , 2019 ) .", "entities": []}, {"text": "Liu ( 2019 ) proposes a modi\ufb01ed BERT for extractive summarization and achieve the state - of - the - art result in extractive document summarization task .", "entities": [[7, 8, "MethodName", "BERT"], [9, 11, "TaskName", "extractive summarization"], [23, 26, "TaskName", "extractive document summarization"]]}, {"text": "There have been many studies applying the related text in market prediction tasks .", "entities": []}, {"text": "Moreover , the text assisted stock movement prediction has attracted many researchers \u2019 interest .", "entities": []}, {"text": "Most of theseworks predict stock movement based on single news :", "entities": []}, {"text": "Si et al .", "entities": []}, {"text": "( 2014 ) utilize the sentiment analysis to help the prediction .", "entities": [[5, 7, "TaskName", "sentiment analysis"]]}, {"text": "Duan et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) adopt the summarization of news body instead of headline to predict .", "entities": [[5, 6, "TaskName", "summarization"]]}, {"text": "Ding et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2016 ) propose the knowledgedriven event embedding method to make the forecast .", "entities": []}, {"text": "Yet some others choose multi - news :", "entities": [[4, 7, "DatasetName", "multi - news"]]}, {"text": "Hu et al .", "entities": []}, {"text": "( 2018 ) propose a hybrid attention network to combine news in different days .", "entities": []}, {"text": "However , the number of combined news is still limited and much smaller than that of forex news .", "entities": []}, {"text": "Compared to stock prediction , works about forex prediction is much scarce , and most of these works ( Carapuc \u00b8o et al . , 2018 ; Bakhach et al . , 2016 ; Yong et al . , 2018 ; Roledene et al . , 2016 ; Contreras et al . , 2018 ; Weeraddana et al . , 2018 ) do not consider the text information .", "entities": [[2, 4, "TaskName", "stock prediction"]]}, {"text": "Shen and Liang ( 2016 ) employ stacked autoencoder to get the trade data representation and adopt support vector regression to predict .", "entities": [[8, 9, "MethodName", "autoencoder"]]}, {"text": "de Almeida et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) combine SVM with genetic algorithms to optimize investments in Forex markets based on history price .", "entities": [[4, 5, "MethodName", "SVM"], [6, 8, "MethodName", "genetic algorithms"]]}, {"text": "Tsai et al .", "entities": []}, {"text": "( 2018 ) choose the convolutional neural network to process the trading data .", "entities": []}, {"text": "Besides , only limited works utilize the forex related text in the prediction process .", "entities": []}, {"text": "Nassirtoussi et al .", "entities": []}, {"text": "( 2015 ) adopt the WordNet ( Miller , 1995 ) and SentiWordNet ( Baccianella et al . , 2010 ) to extract the text semantic and sentiment information and build the text feature vector to forecast forex movement .", "entities": []}, {"text": "Following this work , Seifollahi and Shajari ( 2019 ) add word sense disambiguation in the sentiment analysis of news headlines .", "entities": [[11, 14, "TaskName", "word sense disambiguation"], [16, 18, "TaskName", "sentiment analysis"]]}, {"text": "Vijayan and Potey ( 2016 ) apply the J48 algorithm in analyzing text .", "entities": []}, {"text": "This kind of method pays more attention to access a \ufb01xed feature vector from news and can only represent news on a shallow level .", "entities": []}, {"text": "In this work , we propose a selection and aggregation neural framework to process the larger amount of \ufb01nance news and employ the powerful pre - trained BERT as text encoder , which can learn the deep semantic information effectively .", "entities": [[27, 28, "MethodName", "BERT"]]}, {"text": "3 Approach 3.1 Problem Formulation Each sample in the dataset ( x;y;f ) contains the set of news text x , the forex trade data y , and the forex movement label f.xandyhappen in the same input time window .", "entities": []}, {"text": "To be more speci\ufb01c , x is a list of news groups x=\b", "entities": []}, {"text": "C1;C2;\u0001\u0001\u0001;CL \t  .", "entities": []}, {"text": "Lis the number of groups .", "entities": []}, {"text": "The methods for", "entities": []}, {"text": "43 Multi -layer   Perception\ud835\udc361 \u2026 \ud835\udc50\ud835\udc59\ud835\udc601 BertBert Bert\ud835\udc50\ud835\udc59\ud835\udc602 \ud835\udc50\ud835\udc59\ud835\udc60\ud835\udc57 \u2026 Score    LayerSummarization G2G1 G\ud835\udc3f\ud835\udc362 \ud835\udc36\ud835\udc3f+", "entities": [[11, 12, "MetricName", "Score"]]}, {"text": "\ud835\udc45\ud835\udc61\ud835\udc45\ud835\udc60 SoftmaxNews Input Trade Data   InputMovement Prediction FX - News   AttentionFigure 2 : The overview of the proposed model .", "entities": []}, {"text": "Group News Token Embeddings Position EmbeddingsSegment Embeddings[CLS ] news one", "entities": []}, {"text": "[ SEP ]", "entities": []}, {"text": "[ CLS ] news two [ SEP ]", "entities": []}, {"text": "[ CLS ] news three", "entities": []}, {"text": "[ SEP ] E[CLS]EnewsEoneE[SEP]E[CLS]EnewsEtwoE[SEP]E[CLS]EnewsEthreeE[SEP ] EAEAEAEAEBEBEBEBEAEAEAEA E0E1E2E3E4E5E6E7E8E9E10E11 + + + + + + + + + + + + + + + + + + + + + + + + Figure 3 : The BERT input in each news group .", "entities": [[35, 36, "MethodName", "BERT"]]}, {"text": "dividing groups are introduced in Section 3.5 .", "entities": []}, {"text": "Each news group is a sequence of \ufb01nance news [ news 1;news 2;\u0001\u0001\u0001;news K]in chronological order.yis the trade data embedding accessed by the method introduced in Section 3.6 .", "entities": []}, {"text": "And f2f1;0 g is the forex movement label telling whether the forex trade price is up or down after a certain time ( we call it prediction delay ) .", "entities": []}, {"text": "The forex movement prediction task can be de\ufb01ned as assigning movement label for the news input and trade data input .", "entities": []}, {"text": "3.2 Model Overview The overview of the Bert - based Hierarchical Aggregation Model ( BHAM ) is displayed in Figure 2 .", "entities": []}, {"text": "The model can be generally divided into two steps : ( 1)Intra - group extraction and ( 2)Intergroups aggregation .", "entities": []}, {"text": "In the Intra - group extraction step , news in the same group is connected as a continuous paragraph , and we conduct extractive summarization on this paragraph to select the most important news .", "entities": [[23, 25, "TaskName", "extractive summarization"]]}, {"text": "Speci\ufb01cally , we employ BERT as the encoder to get the contextualized paragraph representation and compute the importance score for each news .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "Then we select and aggregate the top - k ( k is a hyper - parameters ) news to get the \ufb01nal group representation .", "entities": []}, {"text": "In the Inter - groups aggregation step , we \ufb01rst access the trade data representation by a 3 - layer perceptron and then employ the trade data representation as a query to calculate the attention scores of all the news group and obtain the \ufb01nal news representation .", "entities": []}, {"text": "Finally , we fuse the \ufb01nal news representation and the tradedata representation to predict the forex movement .", "entities": []}, {"text": "3.3 Intra - group Extraction There will be lots of news in the same group , and we suppose that only a small amount of news has the greatest in\ufb02uence on the forex movement .", "entities": []}, {"text": "The purpose of this step is to select the essential news from all news in group , which is redundant and full of noise .", "entities": []}, {"text": "Inspired by the BERT - based extractive summarization model proposed in ( Liu , 2019 ) , we modify this method to select the most crucial news in each group .", "entities": [[0, 1, "DatasetName", "Inspired"], [3, 4, "MethodName", "BERT"], [6, 8, "TaskName", "extractive summarization"]]}, {"text": "All the news in the same group is related to the subject of this group , and the connection of them in chronological order can be regarded as the continuous description of the group subject .", "entities": []}, {"text": "The connection can make the news representations realize the context information of this group by passing information among different news .", "entities": []}, {"text": "We suppose the context information can help select better news in group .", "entities": []}, {"text": "The form of group news input for BERT encoder is illustrated in Figure 3 .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "We insert a [ CLS ] token before each news and a [ SEP]token after each news .", "entities": []}, {"text": "For the segment embedding , we use the loop of [ EA;EB]to extend the raw segment embedding of BERT to multi - sentences .", "entities": [[18, 19, "MethodName", "BERT"]]}, {"text": "After the BERT encoding , all the [ CLS]tokens clsare regarded as the semantic representations of the corresponding news .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "The importance score for each", "entities": []}, {"text": "44news is calculated base on these [ CLS]tokens : scorei= sigmoid ( W0\u0003clsi+b0)(1 )", "entities": []}, {"text": "ti= TOP k(scorei ) ( 2 ) si= softmax ( ti ) ( 3 ) Wherei2 f1;2;\u0001\u0001\u0001;Lg , Lis the number of groups .", "entities": [[8, 9, "MethodName", "softmax"]]}, {"text": "clsiis the list of [ CLS]tokens in the ith group .", "entities": []}, {"text": "W0andb0are the trainable parameters .", "entities": []}, {"text": "scoreiis a list of values indicating the important scores of news .", "entities": []}, {"text": "TOP kis an operation to select the top - k pieces of news with the highest scores .", "entities": []}, {"text": "Then the group representation is calculated by the weighted sum of the top - k", "entities": []}, {"text": "[ CLS]tokens : Gi = kX j=1clsi j\u0003si j ( 4 ) TheGiis the \ufb01nal representation of the i - th news group which contains the semantic information from the most important news in this group .", "entities": []}, {"text": "3.4 Inter - groups Aggregation The purpose of this step is to aggregate semantic information at the inter - groups level .", "entities": []}, {"text": "The forex trade data and the \ufb01nance news are highly relevant : the trade data represents the history movement of forex , and the \ufb01nance news represents the environmental variable .", "entities": []}, {"text": "So the combination of them can help us model the forex movement better .", "entities": []}, {"text": "In a certain input time , news groups have different impacts on forex movement .", "entities": []}, {"text": "So we employ the trade data as a query to calculate the attention weights of news groups .", "entities": []}, {"text": "Then the weighted sum of news groups and the trade data representation are \ufb01nally fused to predict the forex movement .", "entities": []}, {"text": "For forex trade data y , we apply a 3 - layer perceptron to access the trade data representation Rt , and each layer is a non - linear transform with Relu activation function .", "entities": [[31, 32, "MethodName", "Relu"], [32, 34, "HyperparameterName", "activation function"]]}, {"text": "Then we calculate the attention weight between RtandGi : g(i ) = Relu ( Rt\u0003Wa\u0003Gi > ) ( 5 ) atti = eg(i ) PL i=1eg(i)(6 ) Whereatt(i)is the i - th news group \u2019s attention weight to trade data .", "entities": [[12, 13, "MethodName", "Relu"]]}, {"text": "Then we sum the news groups representations up to get the \ufb01nal news semantic representation Rs : Rs = LX i=1Gi\u0003atti ( 7)To fuse the news semantic and trade data representations effectively , we choose the fusion function used in ( Wang et al . , 2018 ; Mou et al . , 2016 ) to fuse RsandRtand predict the movement : R= [ Rt;Rs;Rt\u0000Rs;Rt\u000eRs ] ( 8) bp(fjx;y ) = softmax ( Wp\u0003R+bp)(9 ) \u000emeans element - wise multiplication .", "entities": [[71, 72, "MethodName", "softmax"]]}, {"text": "3.5 Methods of Grouping News", "entities": []}, {"text": "In this part , we introduce the three news grouping methods .", "entities": []}, {"text": "The ideal division enables news groups to be high cohesion and low coupling , which means the semantic information of \ufb01nance news should be highly related intra - group and less related inter - groups .", "entities": []}, {"text": "We suppose that extracting news by groups can reduce the extraction dif\ufb01culty compared to extracting from all news directly because news in the same group is close to each other and has less noise .", "entities": []}, {"text": "Moreover , this method can help us analyze the contributions of different groups .", "entities": []}, {"text": "3.5.1 Grouping by Time In this method , \ufb01nance news is divided into groups according to the time when news happens .", "entities": []}, {"text": "We set the time unit to 5 minutes and news released in the same time unit will be divided into the same group .", "entities": []}, {"text": "This method supposes that news happened closely is highly correlated .", "entities": []}, {"text": "3.5.2 Grouping by Topic In this method , \ufb01nance news is divided into groups by news topic .", "entities": []}, {"text": "The news topics are generated by unsupervised news clustering .", "entities": []}, {"text": "In this work , we choose the af\ufb01nity propagation algorithm ( Frey and Dueck , 2007 ) to generate news clusters without setting the number of clusters subjectively .", "entities": []}, {"text": "Moreover , we choose the tf - idf of 2 - gram features from news headlines .", "entities": []}, {"text": "This method supposes that \ufb01nance news focuses on several \ufb01nance event topics at a particular time .", "entities": []}, {"text": "News in the same topic describes this topic from different aspects and has a high correlation .", "entities": []}, {"text": "3.5.3 Grouping by Category In this method , news is divided into groups by category .", "entities": []}, {"text": "The news categories1are fBusiness Sectors , Business General , Business Assets , Business Commodities , Business Organizations , Politics&International Affairs , 1Use the Reuters professional \ufb01nancial news category(https://liaison.reuters.com / tools/ topic - codes ) and merge some similar categories .", "entities": [[7, 8, "DatasetName", "General"]]}, {"text": "45Arts&Culture&Entertainment&Sports , Science & Technology ,", "entities": []}, {"text": "Other g.", "entities": []}, {"text": "This method supposes that news in the same category is close to each other .", "entities": []}, {"text": "3.6 Trade Data Embedding The raw record of forex data includes the open/ close / high / low trade prices for each minute .", "entities": []}, {"text": "In order to extract all the possible features , we build the trade data embedding ycontaining multi aspects : \u000fRaw Number : open / close / high / low trade price for each trade minute .", "entities": []}, {"text": "\u000fChange Rate : change rate of open / close/ high / low price compared to last trade minute .", "entities": []}, {"text": "\u000fTrade Statistics : mean value , max value , min value , median , variance of all the trade prices in input minutes .", "entities": []}, {"text": "The min - max scale is applied for each currency pair \u2019s samples to scale the raw numbers in yto", "entities": []}, {"text": "[ 0;1]according to the maximum and minimum value of each feature .", "entities": []}, {"text": "3.7 Training Objective The loss function of the proposed model includes two parts : the negative log - likelihood training loss and theL2regularization item : Loss = \u0000f\u0003logp(fjx;y;\u0012 )", "entities": [[4, 5, "MetricName", "loss"], [16, 19, "MetricName", "log - likelihood"], [20, 21, "MetricName", "loss"]]}, {"text": "+ \u0015 2k\u0012k2 2(10 ) \u0012is the model parameters .", "entities": []}, {"text": "Experiments show that the performance improves after adding L2regularization .", "entities": []}, {"text": "We train three models with different news grouping methods : time , topic and category , and we call them BHAM - Time , BHAM - Topic , BHAM - Category , respectively .", "entities": []}, {"text": "4 Experiment 4.1 Dataset", "entities": []}, {"text": "The experiment dataset is accessed from the professional \ufb01nance news providers Reuters2 .", "entities": []}, {"text": "We collect forex trade data of four major currency pairs ( USD - EUR , USD - JPY , USD - RMB , USDGBP ) from 2013 to 2017 .", "entities": []}, {"text": "We collect the open / close / high / low trade price for each trade minute .", "entities": []}, {"text": "As for the \ufb01nance news data , we collect all the English news happened in trade time released by Reuters and match the news with target currency pairs according to news region .", "entities": []}, {"text": "For example , we match USD - EUR with news related to 2Source Reuters News cThomson Reuters cREFINITIV , https://www.thomsonreuters.com/en.htmlUS , Europe or both of them .", "entities": []}, {"text": "The raw data contains both news headline and body , and we utilize the headline only since the headline contains the most valuable information and has less noise .", "entities": []}, {"text": "The forex movement label fis decided by the comparison of prediction time price and the input window ending price .", "entities": []}, {"text": "We design the symbol USD - EUR(2010 ) to represent the prediction for the USD - EUR exchange rate with 20 minutes input time and 10 minutes prediction delay .", "entities": []}, {"text": "To access more data for training , we overlap the input time of samples .", "entities": []}, {"text": "For example , when overlap - rate is 50 % , two consecutive samples \u2019 input time will be 8:00 - 8:20 am and 8:10 - 8:30 am .", "entities": []}, {"text": "Then the data samples will be twice as large as no overlap condition ( In the USDEUR(20 - 10 ) dataset , the number of samples will increase from 31k to 62k ) .", "entities": [[23, 26, "HyperparameterName", "number of samples"]]}, {"text": "We reserve 5k samples for developing and 5k samples for testing .", "entities": []}, {"text": "All the rest of samples are applied for training .", "entities": []}, {"text": "4.2 Experiment Setting We choose the pytorch - pretrained - BERT3as BERT implement and choose the bert - baseuncased version in which there are 12 layers , 768 hidden states and 12 attention heads in the transformer .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "We truncate the BERT input to 256 tokens and \ufb01ne - tune the BERT parameters during training .", "entities": [[3, 4, "MethodName", "BERT"], [13, 14, "MethodName", "BERT"]]}, {"text": "We adopt the Adam ( Kingma and Ba , 2014 ) optimizer with the initial learning rate of 0.001 .", "entities": [[3, 4, "MethodName", "Adam"], [11, 12, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"]]}, {"text": "We apply the dropout ( Srivastava et al . , 2014 ) regularization with the dropout probability of 0:2to reduce over-\ufb01tting .", "entities": []}, {"text": "The batch size is 32 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "The training epoch is 60 with early stop .", "entities": []}, {"text": "The weight ofL2regularization is 0.015 .", "entities": []}, {"text": "The learning rate begins to decay after 10 epoch .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "The overlap rate of data samples is 50 % , and the number of selected news in each group is 3 .", "entities": []}, {"text": "When splitting the dataset , we guarantee that the samples in train set are previous to samples in valid set and test set to avoid the possible information leakage .", "entities": []}, {"text": "We tune the hyper - parameters on the development set and test model on the test set .", "entities": []}, {"text": "The forex prediction is conducted as a binary classi\ufb01cation task ( up or down ) .", "entities": []}, {"text": "The evaluation metrics are macro - F1 and Matthews Correlation Coef\ufb01cient ( MCC ) .", "entities": [[4, 7, "MetricName", "macro - F1"]]}, {"text": "MCC is often reported in stock movement forecast ( Xu and Cohen , 2018 ;", "entities": []}, {"text": "Ding et al . , 2016 ) because it can overcome the data imbalance issue .", "entities": []}, {"text": "3https://github.com/huggingface/ pytorch - pretrained - BERT", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "465 Results and Analysis 5.1 Comparison with Baselines Here , we introduce the baselines in this work .", "entities": []}, {"text": "Since there are few existing works , we modify two advanced models from stock prediction \ufb01eld which adopt multi - news as input for this task .", "entities": [[13, 15, "TaskName", "stock prediction"], [18, 21, "DatasetName", "multi - news"]]}, {"text": "Besides , we design some ablation variations of the proposed model to check the effects of different modules .", "entities": []}, {"text": "The baselines are shown below : \u000fNoNews : This method considers the forex trade data only and use a 3 - layer perceptron ( the setting is same as full model ) to encode the trade data and make prediction .", "entities": []}, {"text": "This is a baseline to check the improvement by adding text information .", "entities": []}, {"text": "\u000fSVM :", "entities": []}, {"text": "This method chooses the support vector machine to predict the result based on the feature vectors extracted by the method introduced in ( Seifollahi and Shajari , 2019 ) .", "entities": [[4, 7, "MethodName", "support vector machine"]]}, {"text": "\u000fHAN : This method is proposed in ( Hu et al . , 2018 ) for stock movement prediction .", "entities": []}, {"text": "It includes a hybrid attention mechanism and Gated Recurrent Unit to combine multi - day \u2019s stock news to predict movement .", "entities": [[7, 10, "MethodName", "Gated Recurrent Unit"]]}, {"text": "We use every 5 minutes instead of each day as time unit for this method and the StockNet method because there is too much news for forex trading and the experiments show that the latest news has the most in\ufb02uence .", "entities": [[17, 18, "DatasetName", "StockNet"]]}, {"text": "\u000fStockNet :", "entities": []}, {"text": "This method is proposed in ( Xu and Cohen , 2018 ) .", "entities": []}, {"text": "It treats the prediction task as a generation task and designs a modi\ufb01ed variational auto encoder to process multidays \u2019 tweets to predict stock movement .", "entities": []}, {"text": "\u000fNoGroup :", "entities": []}, {"text": "This method does not group news and select key news directly from all news .", "entities": []}, {"text": "\u000fNoConnect :", "entities": []}, {"text": "This method does not connect news in the same group .", "entities": []}, {"text": "Instead , it gets the representation for each news independently using BERT .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "This method groups news by category .", "entities": []}, {"text": "\u000fLSTM+Attention : This method uses the bidirectional LSTM and self - attention to replace the BERT as text encoder .", "entities": [[6, 8, "MethodName", "bidirectional LSTM"], [15, 16, "MethodName", "BERT"]]}, {"text": "The number of LSTM hidden states is 256 , and the hidden - layer is 3 .", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "This method groups news by category .", "entities": []}, {"text": "As shown in Table 1 , all the three proposed /uni00000013 / uni00000018 / uni00000050 / uni0000004c / uni00000051 /uni00000014 / uni00000013 / uni00000050 / uni0000004c / uni00000051 /uni00000014 / uni00000018 / uni00000050 / uni0000004c / uni00000051 /uni00000015 / uni00000013 / uni00000050 / uni0000004c / uni00000051 /uni00000015 / uni00000018 / uni00000050 / uni0000004c / uni00000051 /uni00000016 / uni00000013 / uni00000050 / uni0000004c / uni00000051 /uni00000033 / uni00000055 / uni00000048 / uni00000047 / uni00000003 / uni00000027 / uni00000048 / uni0000004f / uni00000044 / uni0000005c / uni00000014 / uni00000013 / uni00000050 / uni0000004c / uni00000051 /uni00000015 / uni00000013 / uni00000050 / uni0000004c / uni00000051 /uni00000016 / uni00000013 / uni00000050 / uni0000004c / uni00000051 /uni00000017 / uni00000013 / uni00000050 / uni0000004c / uni00000051 /uni00000018 / uni00000013 / uni00000050 / uni0000004c / uni00000051 /uni00000019 / uni00000013 / uni00000050 / uni0000004c / uni00000051 / uni0000002c / uni00000051 / uni00000053 / uni00000058 / uni00000057 / uni00000003 / uni00000037 / uni0000004c / uni00000050 / uni00000048 / uni00000019 / uni0000001b / uni00000011 / uni00000016 /uni00000019 / uni0000001a / uni00000011 / uni00000017 /uni00000019 / uni00000019 / uni00000011 / uni00000013 /uni00000019 / uni00000017 / uni00000011 / uni0000001b /uni00000019 / uni00000016 / uni00000011 / uni0000001a /uni00000019 / uni00000015 / uni00000011 / uni00000017 /uni0000001a / uni00000013 / uni00000011 / uni0000001a /uni00000019 / uni0000001c / uni00000011 / uni00000015", "entities": []}, {"text": "/uni00000019 / uni0000001b / uni00000011 / uni00000017 /uni00000019 / uni0000001a / uni00000011 / uni00000018 /uni00000019 / uni00000018 / uni00000011 / uni0000001b /uni00000019 / uni00000016 / uni00000011 / uni00000019 /uni0000001a / uni00000015 / uni00000011 / uni00000016 /uni0000001a / uni00000013 / uni00000011 / uni00000017 /uni00000019 / uni0000001c / uni00000011 / uni00000017 /uni00000019 / uni0000001a / uni00000011 / uni0000001b /uni00000019 / uni00000019 / uni00000011 / uni00000016 /uni00000019 / uni00000017 / uni00000011 / uni00000016 /uni0000001a / uni00000016 / uni00000011 / uni00000017 /uni0000001a / uni00000015 / uni00000011 / uni00000018 /uni0000001a / uni00000014 / uni00000011 / uni00000019 /uni00000019 / uni0000001c / uni00000011 / uni00000017 /uni00000019 / uni00000019 / uni00000011 / uni0000001a /uni00000019 / uni00000018 / uni00000011 / uni00000015 /uni0000001a / uni00000014 / uni00000011 / uni00000019 /uni0000001a / uni00000014 / uni00000011 / uni00000013 /uni00000019 / uni0000001c / uni00000011 / uni00000019 /uni00000019 / uni0000001a / uni00000011 / uni00000017 /uni00000019 / uni00000018 / uni00000011 / uni00000016 /uni00000019 / uni00000017 / uni00000011 / uni00000018 /uni00000019 / uni0000001c / uni00000011 / uni00000017 /uni00000019 / uni0000001b / uni00000011 / uni00000014 /uni00000019 / uni00000019 / uni00000011 / uni00000016", "entities": []}, {"text": "/uni00000019 / uni00000017 / uni00000011 / uni0000001a /uni00000019 / uni00000017 / uni00000011 / uni00000013 /uni00000019 / uni00000016 / uni00000011 / uni00000015 /uni00000019 / uni00000013 / uni00000011 / uni00000013 / uni00000019 / uni00000015 / uni00000011 / uni00000018 / uni00000019 / uni00000018 / uni00000011 / uni00000013 / uni00000019 / uni0000001a / uni00000011 / uni00000018 / uni0000001a / uni00000013 / uni00000011 / uni00000013 / uni0000001a / uni00000015 / uni00000011 / uni00000018 / uni0000001a / uni00000018 / uni00000011 / uni00000013", "entities": []}, {"text": "Figure 4 : The BHAM - Category model \u2019s performances ( macro - F1 % ) on USD - JPY pair under different conditions of input time and prediction delay .", "entities": [[11, 14, "MetricName", "macro - F1"]]}, {"text": "The dark colour means low performance and light colour means high performance .", "entities": []}, {"text": "methods perform well , and both BHAM - Topic and BHAM - Category methods outperform all the baselines .", "entities": []}, {"text": "The BHAM - Category performs best among these methods , which shows that the semantic information of \ufb01nance news is mostly aggregated by category .", "entities": []}, {"text": "All the methods get improved after introducing the text information , which proves the related \ufb01nance news is helpful for the prediction .", "entities": []}, {"text": "The performance of NoGroup method decreases by a large margin compared to BHAM - Category , which demonstrates that the hierarchical structure works well .", "entities": []}, {"text": "Without hierarchical structure , selecting essential news directly from all news has more noise and requires the model to have a stronger \ufb01tting ability for a longer paragraph .", "entities": []}, {"text": "After removing the news connection , the performance of NoConnect method drops sharply compared to BHAM - Category .", "entities": []}, {"text": "Accessing the news representation from the connected paragraph helps the news representation realize the context information in the group .", "entities": []}, {"text": "The LSTM+Attention method performs worse than the BERT - based method , which proves that BERT has stronger power of sentence encoding .", "entities": [[7, 8, "MethodName", "BERT"], [15, 16, "MethodName", "BERT"]]}, {"text": "The two methods borrowed from stock movement prediction are designed to consider all news \u2019s information , but the forex related news is redundant , which can explain the poor performance of these two methods .", "entities": []}, {"text": "5.2 Effect of Time Parameters In this section , we analyze the in\ufb02uence of two crucial time parameters on model performance , which are input time and predic-", "entities": []}, {"text": "47USD - EUR USD - JPY USD - RMB USD - GBP Method F1 MCC F1 MCC F1 MCC F1 MCC NoNews 63.0 0.266 64.8 0.295 65.4 0.304 64.7 0.301 SVM 64.8 0.297", "entities": [[13, 14, "MetricName", "F1"], [15, 16, "MetricName", "F1"], [17, 18, "MetricName", "F1"], [19, 20, "MetricName", "F1"], [30, 31, "MethodName", "SVM"]]}, {"text": "65.7 0.314 66.2 0.324 65.1 0.310 HAN 65.2 0.305 67.0 0.341 66.7 0.334 66.9 0.346 StockNet 65.4 0.309 66.8 0.336 67.2 0.343 66.5 0.339 NoGroup 66.7 0.335 67.5 0.350 68.0 0.361 68.3 0.375 NoConnect 68.8 0.377 70.9 0.418 69.6 0.392 68.7 0.383 LSTM+Attention 69.8 0.397 71.2 0.422 71.8 0.434 69.7 0.403 BHAM - Time 70.7 0.414 70.5 0.409 71.4 0.426 69.2 0.392 BHAM - Topic 71.8 0.436 72.6 0.451 72.3 0.445 71.3 0.435 BHAM - Category 72.5 0.450 73.4 0.466 73.5 0.468 71.6 0.441 Table 1 : Results of baselines and proposed methods on the test set ( input time window is 40 minutes , and prediction delay is 5 minutes , we observe similar result in other time settings ) .", "entities": [[15, 16, "DatasetName", "StockNet"]]}, {"text": "All the experiment results have proven signi\ufb01cant with p<0:05by student t - test .", "entities": []}, {"text": "tion delay .", "entities": []}, {"text": "We choose the input time 2 f10;20;30;40;50;60g(minutes ) , the prediction delay2f5;10;15;20;25;30g(minutes ) and experiment all combinations .", "entities": []}, {"text": "We take the USDJPY for example to analyze the time effect of forex trading , and we observe similar results in other currency pairs .", "entities": []}, {"text": "The Figure 4 shows BHAMCategory model \u2019s performances ( macro - F1 % ) on USD - JPY pair under different combinations of input time and prediction delay .", "entities": [[9, 12, "MetricName", "macro - F1"]]}, {"text": "We can observe that with the increase of input time from 10 minutes to 40 minutes , the model performance improves too .", "entities": []}, {"text": "However , when we increase the input time continuously , the model performance begins to decrease .", "entities": []}, {"text": "Too less text is not enough to support the prediction , but too many texts may bring much noise .", "entities": []}, {"text": "The ideal input time is around 40 minutes .", "entities": []}, {"text": "Besides , at all input time conditions , the model \u2019s performances decline with the increase of prediction delay because events happened in the prediction delay time may also in\ufb02uence the forex movement .", "entities": []}, {"text": "We can also conclude that forex movement pays more attention to the latest news because when masking the latest news input ( such as USD - JPY(40 - 05 ) and USD - JPY(30 - 15 ) , the latter one can be seen as the former one masking the lastest 10 minutes input ) , the model performance declines obviously at almost all conditions .", "entities": []}, {"text": "5.3 In\ufb02uence of News Attributes In this section , we analyze the in\ufb02uence of \ufb01nance news \u2019s attributes ( category and region ) on prediction results and summarize the in\ufb02uence patterns for different currency pairs .", "entities": []}, {"text": "We conduct the experiments based on BHAM - Category.5.3.1 Effect of News Category The forex trading data \u2019s attention weights over news categories are calculated by Equation 6 .", "entities": []}, {"text": "We sum up all the attention weights of test samples and calculate the proportions each category contributes .", "entities": []}, {"text": "As shown in Figure 5 , we display the in\ufb02uence patterns of news category for different currency pairs .", "entities": []}, {"text": "We observe that there are obvious differences among currency pairs .", "entities": []}, {"text": "USDEUR trading pays more attention to the Business Sectors and Politics / International Affairs news .", "entities": []}, {"text": "USD - JPY trading is mostly in\ufb02uenced by Business Sectors and Science / Technology news .", "entities": []}, {"text": "Politics / International Affairs news has the most signi\ufb01cant impact on USD - RMB trading and Business Commodities news effects USD - GBP trading most .", "entities": []}, {"text": "The summarized in\ufb02uence patterns can serve as decision - making reference for forex traders when facing news from various categories .", "entities": []}, {"text": "5.3.2 Effect of News Region", "entities": []}, {"text": "The trading data \u2019s attention weight for selected newsattijis calculated by the following formula : attij = atti\u0003si j ( 11 ) Whereattiis the trade data \u2019s attention on the i - th category in Equation 6 and si jin Equation 4 is the weight of selected news in group .", "entities": []}, {"text": "We sum up all the selected news \u2019s attention according to their regions and access the region in\ufb02uence weight .", "entities": []}, {"text": "The results are shown in Figure 6 .", "entities": []}, {"text": "For each currency pair , the news are divided into three classes : news related to region A only , news related to region B only and news related to both region A and B.", "entities": []}, {"text": "And we observe that the news related to both region A and B has the least in\ufb02uence on all currency", "entities": []}, {"text": "48 BUSINESS_SECTORS BUSINESS_GENERAL NEWS BUSINESS_ASSETS BUSINESS_COMMODITIES BUSINESS_ORGANIZATIONS POLITICS / INTERNATIONAL_AFFAIRS", "entities": []}, {"text": "ARTS / CULTURE / ENTERTAINMENT / SPOTRS SCIENCE / TECHNOLOGY OTHER 15 % 17 % 5 % 6 % 3 % 31 % 6 % 15 % 2%USD - RMB 30 % 11 % 5 % 8 % 5 % 21 % 6 % 10 % 4%USD - EUR 21 % 10 % 5 % 16 %   8 % 11 % 8 % 18 % 3%USD - JPY 12 % 10 % 8 % 21 %   16 % 6 % 15 % 7 % 5%USD - GBPFigure 5 : The attention distributions over categories for different currency pairs .", "entities": []}, {"text": "US 36 % CN 44%US&CN 20%USD - RMB US 49 % GB 33%US&G B 18%USD - GBP US 43 % JP 35%US&JP 22%USD - JPY US 32 % EU 42%US&EU 26%USD - EUR Figure 6 : The attention distributions over regions for different currency pairs .", "entities": []}, {"text": "USD - EUR USD - JPY USD - RMB USD - GBP 1 67.6 68.8 69.3 67.3 2 71.4 73.1 72.2 70.8 3 72:5 73 : 4 73 : 5 71 : 6 4 72.2 72.8 73.1 70.7 5 70.8 70.3 71.9 68.4 1 64.1 64.5 65.7 63.6 Table 2 : Impact of selection number in each group in BHAM - Category .", "entities": []}, {"text": "1means keeping all news .", "entities": []}, {"text": "The results have proven statistic signi\ufb01cant .", "entities": []}, {"text": "pairs .", "entities": []}, {"text": "News related to the US has the largest in\ufb02uence weight on USD - JPY and USD - GBP trading .", "entities": []}, {"text": "Yet news related to China / Europe has a larger in\ufb02uence weight than news related to US in USDRMB / USD - EUP trading .", "entities": []}, {"text": "We can intuitively observe the in\ufb02uence weights of different regions for forex trading , which is helpful for the analysis and forecast of forex movement .", "entities": []}, {"text": "5.4 Impact of Selection Number The selection number in each group is an essential hyper - parameter to control the amount of extracted information .", "entities": []}, {"text": "As shown in Table 2 , the BHAM - Category performs best when the selection number is 3 in all currency pairs .", "entities": []}, {"text": "When the selection number is small ( 1,2 ) , the model is too strict so that some crucial information will be missed .", "entities": []}, {"text": "When the selection number is large ( 4,5 ) , some less in\ufb02uential news will be selected and interfere model \u2019s decision .", "entities": []}, {"text": "When we keep all newsin the group , the model \u2019s performance declines by a large margin .", "entities": []}, {"text": "This experiment demonstrates that the selection mechanism plays an important role in the proposed model .", "entities": []}, {"text": "6 Conclusion In this work , we propose a BERT - based Hierarchical Aggregation Model to summarize a large amount of \ufb01nance news for forex movement prediction .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "Experiments show that our model outperforms all the baselines by a large margin , which proves the effectiveness of the proposed framework .", "entities": []}, {"text": "We design three grouping news methods : time , topic and category and experiments show that the category - based method performs best , which shows that the semantic information of forex related news is mostly aggregated by category .", "entities": []}, {"text": "Experiments about time effect prove that the proper input time is about 40 minutes and the prediction accuracy declines with the increase of prediction delay .", "entities": [[17, 18, "MetricName", "accuracy"]]}, {"text": "Besides , we analyze the in\ufb02uence of news attributes on forex trading and observe some interesting conclusions : Business Sectors news has the most in\ufb02uence on USD - EUR trading and Politics / International Affairs news effects USD - RMB trading most .", "entities": []}, {"text": "Besides , both USDJPY trading and USD - GBP trading pay most attention to news from US .", "entities": []}, {"text": "All these in\ufb02uence patterns can help forex traders handle different news", "entities": []}, {"text": "49more wisely and make better decisions .", "entities": []}, {"text": "To our knowledge , this is the \ufb01rst work to utilize the advanced NLP pre - train technology in the enormous forex market and the results show the potential of this research area .", "entities": []}, {"text": "Promising future studies may include designing more suitable grouping methods or combining news grouping and market predicting in an end2end model .", "entities": []}, {"text": "7 Acknowledgement This work is supported by a Research Grant from Mizuho Securities Co. , Ltd. Mizuho Securities also provide experiment data and valuable domain experts suggestions .", "entities": []}, {"text": "References Bernardo Jubert de Almeida , Rui Ferreira Neves , and Nuno Horta .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Combining support vector machine with genetic algorithms to optimize investments in forex markets with high leverage .", "entities": [[1, 4, "MethodName", "support vector machine"], [5, 7, "MethodName", "genetic algorithms"]]}, {"text": "Applied Soft Computing , 64:596\u2013613 .", "entities": []}, {"text": "Hana Alostad and Hasan Davulcu .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Directional prediction of stock prices using breaking news on twitter .", "entities": []}, {"text": "Web Intelligence , 15(1):1\u201317 .", "entities": []}, {"text": "Stefano Baccianella , Andrea Esuli , and Fabrizio Sebastiani .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Sentiwordnet 3.0 : an enhanced lexical resource for sentiment analysis and opinion mining .", "entities": [[8, 10, "TaskName", "sentiment analysis"], [11, 13, "TaskName", "opinion mining"]]}, {"text": "InLrec , volume 10 , pages 2200\u20132204 .", "entities": []}, {"text": "Amer Bakhach , Edward PK Tsang , and Hamid Jalalian .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Forecasting directional changes in the fx markets .", "entities": []}, {"text": "In 2016 IEEE Symposium Series on Computational Intelligence ( SSCI ) , pages 1\u20138 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Jo\u02dcao", "entities": []}, {"text": "Carapuc \u00b8o , Rui Neves , and Nuno Horta .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Reinforcement learning applied to forex trading .", "entities": []}, {"text": "Applied Soft Computing , 73:783\u2013794 .", "entities": []}, {"text": "Antonio V Contreras , Antonio Llanes , Alberto P \u00b4 erezBernabeu , Sergio Navarro , Horacio P \u00b4 erez - S \u00b4 anchez , Jose J L \u00b4 opez - Esp \u00b4 \u0131n , and Jos \u00b4 e M Cecilia .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Enmx : An elastic network model to predict the forex market evolution .", "entities": []}, {"text": "Simulation Modelling Practice and Theory , 86:1\u201310 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xiao Ding , Yue Zhang , Ting Liu , and Junwen Duan .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Knowledge - driven event embedding for stock prediction .", "entities": [[6, 8, "TaskName", "stock prediction"]]}, {"text": "In COLING 2016 , 26th International Conference on Computational Linguistics , Proceedings of the Conference : Technical Papers , December 11 - 16 , 2016 , Osaka , Japan , pages 2133\u20132142.Junwen Duan , Yue Zhang , Xiao Ding , Ching - Yun Chang , and Ting Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning target - speci\ufb01c representations of \ufb01nancial news documents for cumulative abnormal return prediction .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , COLING 2018 , Santa Fe , New Mexico , USA , August 20 - 26 , 2018 , pages 2823 \u2013 2833 .", "entities": []}, {"text": "Brendan J Frey and Delbert Dueck .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Clustering by passing messages between data points .", "entities": []}, {"text": "science , 315(5814):972\u2013976 .", "entities": []}, {"text": "Ziniu Hu , Weiqing Liu , Jiang Bian , Xuanzhe Liu , and Tie - Yan Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Listening to chaotic whispers : A deep learning framework for news - oriented stock trend prediction .", "entities": [[13, 16, "TaskName", "stock trend prediction"]]}, {"text": "In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining , WSDM 2018 , Marina Del Rey , CA , USA , February 5 - 9 , 2018 , pages 261\u2013269 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Diederik P. Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "CoRR , abs/1412.6980 .", "entities": []}, {"text": "Qing Li , LiLing Jiang , Ping Li , and Hsinchun Chen . 2015 .", "entities": []}, {"text": "Tensor - based learning for predicting stock movements .", "entities": []}, {"text": "In Proceedings of the Twenty - Ninth AAAI Conference on Arti\ufb01cial Intelligence , January 25 - 30 , 2015 , Austin , Texas , USA . , pages 1784\u20131790 .", "entities": [[22, 23, "DatasetName", "Texas"]]}, {"text": "Yang Liu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Fine - tune bert for extractive summarization .", "entities": [[5, 7, "TaskName", "extractive summarization"]]}, {"text": "George A Miller .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Wordnet : a lexical database for english .", "entities": []}, {"text": "Communications of the ACM , 38(11):39 \u2013 41 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Lili Mou , Rui Men , Ge Li , Yan Xu , Lu Zhang , Rui Yan , and Zhi Jin .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Natural language inference by tree - based convolution and heuristic matching .", "entities": [[0, 3, "TaskName", "Natural language inference"], [7, 8, "MethodName", "convolution"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , ACL 2016 , August 7 - 12 , 2016 , Berlin , Germany , Volume 2 : Short Papers .", "entities": []}, {"text": "Arman Khadjeh Nassirtoussi , Saeed Aghabozorgi , Teh Ying Wah , and David Chek Ling Ngo . 2015 .", "entities": []}, {"text": "Text mining of news - headlines for forex market prediction : A multi - layer dimension reduction algorithm with semantics and sentiment .", "entities": []}, {"text": "Expert Systems with Applications , 42(1):306\u2013324 .", "entities": []}, {"text": "Lina Ni , Yujie Li , Xiao Wang , Jinquan Zhang , Jiguo Yu , and Chengming Qi . 2019 .", "entities": []}, {"text": "Forecasting of forex time series data based on deep learning .", "entities": [[3, 5, "TaskName", "time series"]]}, {"text": "Procedia computer science , 147:647\u2013652 .", "entities": []}, {"text": "Dadabada Pradeepkumar and Vadlamani Ravi . 2016 .", "entities": []}, {"text": "Forex rate prediction using chaos and quantile regression random forest .", "entities": []}, {"text": "In 2016 3rd International Conference on Recent Advances in Information Technology ( RAIT ) , pages 517\u2013522 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "50Sasika Roledene , Lakna Ariyathilaka , Nadun Liyanage , Prasad Lakmal , and Jeewanee Bamunusinghe . 2016 .", "entities": []}, {"text": "Genibux - event based intelligent forex trading strategy enhancer .", "entities": []}, {"text": "In 2016 IEEE International Conference on Information and Automation for Sustainability ( ICIAfS ) , pages 1\u20136 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Saeed Seifollahi and Mehdi Shajari .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Word sense disambiguation application in sentiment analysis of news headlines : an applied approach to forex market prediction .", "entities": [[0, 3, "TaskName", "Word sense disambiguation"], [5, 7, "TaskName", "sentiment analysis"]]}, {"text": "Journal of Intelligent Information Systems , 52(1):57\u201383 .", "entities": []}, {"text": "Hua Shen and Xun Liang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A time series forecasting model based on deep learning integrated algorithm with stacked autoencoders and svr for fx prediction .", "entities": [[1, 4, "TaskName", "time series forecasting"], [13, 14, "MethodName", "autoencoders"]]}, {"text": "In International Conference on Arti\ufb01cial Neural Networks , pages 326\u2013335 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Jianfeng Si , Arjun Mukherjee , Bing Liu , Sinno Jialin Pan , Qing Li , and Huayi Li . 2014 .", "entities": []}, {"text": "Exploiting social relations and sentiment for stock prediction .", "entities": [[6, 8, "TaskName", "stock prediction"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing , EMNLP 2014 , October 25 - 29 , 2014 , Doha , Qatar , A meeting of SIGDAT , a Special Interest Group of the ACL , pages 1139\u20131145 .", "entities": []}, {"text": "Nitish Srivastava , Geoffrey E. Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov .", "entities": [[14, 15, "DatasetName", "Ruslan"]]}, {"text": "2014 .", "entities": []}, {"text": "Dropout : a simple way to prevent neural networks from over\ufb01tting .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "Journal of Machine Learning Research , 15(1):1929\u20131958 .", "entities": []}, {"text": "Nadine Strau\u00df , Rens Vliegenthart , and Piet Verhoeven .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Intraday news trading : The reciprocal relationships between the stock market and economic news .", "entities": []}, {"text": "Communication Research , 45(7):1054\u20131077 .", "entities": []}, {"text": "Chi Sun , Luyao Huang , and Xipeng Qiu . 2019 .", "entities": []}, {"text": "Utilizing bert for aspect - based sentiment analysis via constructing auxiliary sentence .", "entities": [[3, 8, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "arXiv preprint arXiv:1903.09588 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yun - Cheng Tsai , Jun - Hao Chen , and Jun - Jie Wang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Predict forex trend via convolutional neural networks .", "entities": []}, {"text": "Journal of Intelligent Systems .", "entities": []}, {"text": "Mrs Remya Vijayan and Mrs MA Potey . 2016 .", "entities": []}, {"text": "Improved accuracy of forex intraday trend prediction through text mining of news headlines using j48 .", "entities": [[1, 2, "MetricName", "accuracy"]]}, {"text": "International Journal of Advanced Research in Computer Engineering & Technology ( IJARCET ) , 5(6 ) .", "entities": []}, {"text": "Wei Wang , Chen Wu , and Ming Yan .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Multigranularity hierarchical attention fusion networks for reading comprehension and question answering .", "entities": [[6, 8, "TaskName", "reading comprehension"], [9, 11, "TaskName", "question answering"]]}, {"text": "InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics , ACL 2018 , Melbourne , Australia , July 15 - 20 , 2018 , Volume 1 : Long Papers , pages 1705\u20131714 .", "entities": []}, {"text": "NR Weeraddana , ATP Silva , and PWDC Jayathilake .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Detection of black regions in the forex market by analyzing high - frequency intraday data .", "entities": []}, {"text": "In 2018 18th International Conference on Advances inICT for Emerging Regions ( ICTer ) , pages 384\u2013391 . IEEE .", "entities": []}, {"text": "Hu Xu , Bing Liu , Lei Shu , and Philip S Yu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Bert post - training for review reading comprehension and aspect - based sentiment analysis .", "entities": [[6, 8, "TaskName", "reading comprehension"], [9, 14, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "arXiv preprint arXiv:1904.02232 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yumo Xu and Shay B. Cohen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Stock movement prediction from tweets and historical prices .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , ACL 2018 , Melbourne , Australia , July 15 - 20 , 2018 , Volume 1 : Long Papers , pages 1970\u20131979 .", "entities": []}, {"text": "Yoke Leng Yong , Yunli Lee , Xiaowei Gu , Plamen P Angelov , David Chek Ling Ngo , and Elnaz Sha\ufb01pour .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Foreign currency exchange rate prediction using neuro - fuzzy systems .", "entities": []}, {"text": "Procedia computer science , 144:232\u2013238 .", "entities": []}]