[{"text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , pages 4438\u20134450 , November 16\u201320 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics4438On Negative Interference in Multilingual Models : Findings and A Meta - Learning Treatment Zirui Wang Zachary C. Lipton Yulia", "entities": [[14, 17, "TaskName", "Meta - Learning"]]}, {"text": "Tsvetkov Carnegie Mellon University , Pittsburgh , USA fziruiw , zlipton , ytsvetko g@cs.cmu.edu Abstract Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring bene\ufb01ts to each ( positive transfer ) , with the most pronounced bene\ufb01ts accruing to low - resource languages .", "entities": []}, {"text": "However , recent work has shown that this approach can degrade performance on high - resource languages , a phenomenon known as negative interference .", "entities": []}, {"text": "In this paper , we present the \ufb01rst systematic study of negative interference .", "entities": []}, {"text": "We show that , contrary to previous belief , negative interference also impacts low - resource languages .", "entities": []}, {"text": "While parameters are maximally shared to learn language - universal structures , we demonstrate that language - speci\ufb01c parameters do exist in multilingual models and they are a potential cause of negative interference .", "entities": []}, {"text": "Motivated by these observations , we also present a meta - learning algorithm that obtains better cross - lingual transferability and alleviates negative interference , by adding languagespeci\ufb01c layers as meta - parameters and training them in a manner that explicitly improves shared layers \u2019 generalization on all languages .", "entities": [[9, 12, "TaskName", "meta - learning"]]}, {"text": "Overall , our results show that negative interference is more common than previously known , suggesting new directions for improving multilingual representations.1 1 Introduction Advances in pretraining language models ( Devlin et al . , 2018 ; Liu et al . , 2019 ; Yang et al . , 2019 ) as general - purpose representations have pushed the state of the art on a variety of natural language tasks .", "entities": []}, {"text": "However , not all languages enjoy large public datasets for pretraining and/or downstream tasks .", "entities": []}, {"text": "Multilingual language models such as mBERT ( Devlin et al . , 2018 ) and XLM ( Lample and Conneau , 2019 ) have been proven effective for cross - lingual 1Source code is available at https://github.com/ iedwardwangi / MetaAdapter .transfer learning by pretraining a single shared Transformer model ( Vaswani et al . , 2017 ) jointly on multiple languages .", "entities": [[5, 6, "MethodName", "mBERT"], [15, 16, "MethodName", "XLM"], [47, 48, "MethodName", "Transformer"]]}, {"text": "The goals of multilingual modeling are not limited to improving language modeling in low - resource languages ( Lample and Conneau , 2019 ) , but also include zero - shot crosslingual transfer on downstream tasks \u2014 it has been shown that multilingual models can generalize to target languages even when labeled training data is only available in the source language ( typically English ) on a wide range of tasks ( Pires et al . , 2019 ; Wu and Dredze , 2019 ;", "entities": []}, {"text": "Hu et al . , 2020 ) .", "entities": []}, {"text": "However , multilingual models are not equally bene\ufb01cial for all languages .", "entities": []}, {"text": "Conneau et al .", "entities": []}, {"text": "( 2019 ) demonstrated that including more languages in a single model can improve performance for lowresource languages but hurt performance for highresource languages .", "entities": []}, {"text": "Similarly , recent work ( Johnson et al . , 2017 ; Tan et al . , 2019 ; Aharoni et al . , 2019 ; Arivazhagan et al . , 2019 ) in multilingual neural machine translation ( NMT ) also observed performance degradation on high - resource language pairs .", "entities": [[36, 38, "TaskName", "machine translation"]]}, {"text": "In multi - task learning ( Ruder , 2017 ) , this phenomenon is known as negative interference or negative transfer ( Wang et al . , 2019 ) , where training multiple tasks jointly hinders the performance on individual tasks .", "entities": [[1, 5, "TaskName", "multi - task learning"]]}, {"text": "Despite these empirical observations , little prior work analyzed or showed how to mitigate negative interference in multilingual language models .", "entities": []}, {"text": "Particularly , it is natural to ask : ( 1 ) Can negative interference occur for low - resource languages also ?", "entities": []}, {"text": "( 2 ) What factors play an important role in causing it ?", "entities": []}, {"text": "( 3 ) Can we mitigate negative interference to improve the model \u2019s cross - lingual transferability ?", "entities": []}, {"text": "In this paper , we take a step towards addressing these questions .", "entities": []}, {"text": "We pretrain a set of monolingual and bilingual models and evaluate them on a range of downstream tasks to analyze negative interference .", "entities": []}, {"text": "We seek to individually characterize the un-", "entities": []}, {"text": "4439derlying factors of negative interference through a set of ablation studies and glean insights on its causes .", "entities": []}, {"text": "Speci\ufb01cally , we examine if training corpus size and language similarity affect negative interference , and also measure gradient and parameter similarities between languages .", "entities": []}, {"text": "Our results show that negative interference can occur in both high - resource and low - resource languages .", "entities": []}, {"text": "In particular , we observe that neither subsampling the training corpus nor adding typologically similar languages substantially impacts negative interference .", "entities": []}, {"text": "On the other hand , we show that gradient con\ufb02icts and language - speci\ufb01c parameters do exist in multilingual models , suggesting that languages are \ufb01ghting for model capacity , which potentially causes negative interference .", "entities": []}, {"text": "We further test whether explicitly assigning language - speci\ufb01c modules to each language can alleviate negative interference , and \ufb01nd that the resulting model performs better within each individual language but worse on zero - shot cross - lingual tasks .", "entities": []}, {"text": "Motivated by these observations , we further propose to meta - learn these language - speci\ufb01c parameters to explicitly improve generalization of shared parameters on all languages .", "entities": []}, {"text": "Empirically , our method improves not only within - language performance on monolingual tasks but also cross - lingual transferability on zero - shot transfer benchmarks .", "entities": []}, {"text": "To the best of our knowledge , this is the \ufb01rst work to systematically study and remedy negative interference in multilingual language models .", "entities": []}, {"text": "2 Motivation Multilingual transfer learning aims at utilizing knowledge transfer across languages to boost performance on low - resource languages .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "State - of - theart multilingual language models are trained on multiple languages jointly to enable cross - lingual transfer through parameter sharing .", "entities": [[16, 20, "TaskName", "cross - lingual transfer"]]}, {"text": "However , languages are heterogeneous , with different vocabularies , morphosyntactic rules , and different pragmatics across cultures .", "entities": []}, {"text": "It is therefore natural to ask , is knowledge transfer bene\ufb01cial for all languages in a multilingual model ?", "entities": []}, {"text": "To analyze the effect of knowledge transfer from other languages on a speci\ufb01c language lg , we can compare multilingual models with the monolingual model trained onlg .", "entities": []}, {"text": "For example , in Figure 1 , we compare the performance on a named entity recognition ( NER ) task of monolingually - trained models vs. bilingual models ( trained on lgand English ) vs. state - ofFigure 1 : Comparing monolingual vs multilingual models on NER .", "entities": [[13, 16, "TaskName", "named entity recognition"], [17, 18, "TaskName", "NER"], [46, 47, "TaskName", "NER"]]}, {"text": "Lower performance of multilingual models is likely an indicator of negative interference .", "entities": []}, {"text": "the - art XLM ( Conneau et al . , 2019 ) .", "entities": [[3, 4, "MethodName", "XLM"]]}, {"text": "We can see that monolingual models outperform multilingual models on four out of six languages ( See \u00a7 3.3 for details ) .", "entities": []}, {"text": "This shows that language con\ufb02icts may induce negative impacts on certain languages , which we refer to as negative interference .", "entities": []}, {"text": "Here , we investigate the causes of negative interference ( \u00a7 3.3 ) and methods to overcome it ( \u00a7 4 ) .", "entities": []}, {"text": "3 Investigating the Sources of Negative Interference in Multilingual Models 3.1 Methodology To study negative interference , we compare multilingual models with monolingual baselines .", "entities": []}, {"text": "Without loss of generality , we focus on analyzing bilingual models to minimize confounding factors .", "entities": [[1, 2, "MetricName", "loss"]]}, {"text": "For two languages lg1and lg2 , we pretrain a single bilingual model and two monolingual models .", "entities": []}, {"text": "We then assess their performance on downstream tasks using two different settings .", "entities": []}, {"text": "To examine negative interference , we evaluate both monolingual and multilingual models using the withinlanguage monolingual setting , such that the pretrained model is \ufb01netuned and tested on the same language .", "entities": []}, {"text": "For instance , if the monolingual model oflg1outperforms the bilingual model on lg1 , we know that lg2induces negative impact on lg1 in the bilingual model .", "entities": []}, {"text": "Besides , since multilingual models are trained to enable cross - lingual transfer , we also report their performance on the zero - shot cross - lingual transfer setting , where the model is only \ufb01netuned on the source language , say lg1 , and tested on the target language lg2 .", "entities": [[9, 13, "TaskName", "cross - lingual transfer"], [21, 28, "TaskName", "zero - shot cross - lingual transfer"]]}, {"text": "We hypothesize that the following factors play important roles in causing negative interference and study each individually : Training Corpus Size While prior work mostly report negative interference for high - resource lan-", "entities": []}, {"text": "4440en ar fr ru hi sw te corpus size 44.6 8.7 16.2 13.1 0.5 0.2 0.3 NER X X X X X X X POS X X X X X X QA X X X X X XNLI X X X X X X Table 1 : Language training corpra statitstics and downstream tasks availability .", "entities": [[16, 17, "TaskName", "NER"], [37, 38, "DatasetName", "XNLI"]]}, {"text": "Corpus size measured in millions of sentences .", "entities": []}, {"text": "guages ( Conneau et al . , 2019 ; Arivazhagan et al . , 2019 ) , we hypothesize that it can also occur for languages with less resources .", "entities": []}, {"text": "We study the impact of training data size per language on negative interference .", "entities": []}, {"text": "We subsample a high - resource language , say lg1 , to create a \u201c low - resource version \u201d .", "entities": []}, {"text": "We then retrain the monolingual and bilingual models and compare with results of their high - source counterparts .", "entities": []}, {"text": "Particularly , we test if reducing lg1 \u2019s training size also reduces negative interference on lg2 .", "entities": []}, {"text": "Language Similarity Language similarity has been shown important for effective transfer in multilingual models .", "entities": []}, {"text": "Wu et al .", "entities": []}, {"text": "( 2019 ) shows that bilingual models trained on more similar language pairs result in better zero - shot transfer performance .", "entities": []}, {"text": "We thus expect it to play a critical role in negative interference as well .", "entities": []}, {"text": "For a speci\ufb01c language lg1 , we pair it with languages that are closely and distantly related .", "entities": []}, {"text": "We then compare these bilingual models \u2019 performance on lg1to investigate if more similar languages cause less severe interference .", "entities": []}, {"text": "In addition , we further add a third language lg3 that is similar to lg1and train a trilingual model on lg1 - lg2 - lg3 .", "entities": []}, {"text": "We compare the trilingual model with the bilingual model to examine if adding lg3can mitigate negative interference on lg1 .", "entities": []}, {"text": "Gradient Con\ufb02ict Recent work ( Yu et al . , 2020 ) shows that gradient con\ufb02ict between dissimilar tasks , de\ufb01ned as a negative cosine similarity between gradients , is predictive of negative interference in multi - task learning .", "entities": [[35, 39, "TaskName", "multi - task learning"]]}, {"text": "Therefore , we study whether gradient con\ufb02icts exist between languages in multilingual models .", "entities": []}, {"text": "In particular , we sample one batch for each language in the model and compute the corresponding gradients \u2019 cosine similarity for every 10 steps during pretraining .", "entities": []}, {"text": "Parameter Sharing State - of - the - art multilingual models aim to share as many parameters as possible in the hope of learning a languageuniversal model for all languages ( Wu et al . , 2019 ) .", "entities": []}, {"text": "While prior studies measure the latent embeddingsimilarity between languages , we instead examine model parameters directly .", "entities": []}, {"text": "The idea is to test whether model parameters are language - universal orlanguage - speci\ufb01c .", "entities": []}, {"text": "To achieve this , we prune multilingual models for each language using relaxedL0norm regularization ( Louizos et al . , 2017 ) , and compare parameter similarities between languages .", "entities": []}, {"text": "Formally , for a model f(\u0001;\u0012)parameterized by \u0012=f\u0012ign i=1where each \u0012irepresents an individual parameter or a group of parameters , the method introduces a set of binary masks z , drawn from some distribution q(zj\u0019)parametrized by \u0019 , and learns a sparse model f(\u0001;\u0012 \f z)by optimizing : min \u0019Eq(zj\u0019 ) \" 1 NNX i=1L(f(xi;~\u0012);yi ) + \u0015k~\u0012k0 # s.t . ~\u0012=\u0012 \f z ; ( 1 ) where \f is the Hadamard ( elementwise ) product , L(\u0001)is some task loss and \u0015is a hyper - parameter .", "entities": [[81, 82, "MetricName", "loss"]]}, {"text": "We follow the work of ( Louizos et al . , 2017 ) and use the Hard Concrete distribution for the binary mask z , such that the above objective is fully differentiable .", "entities": []}, {"text": "Then , for each bilingual model , we freeze its pretrained parameter weights and learn binary masks zforeach language independently .", "entities": []}, {"text": "As a result , we obtain two independent sets of mask parameters \u0019which can be used to determine parameter importance .", "entities": []}, {"text": "Intuitively , for each parameter group , it is language - universal if both languages consider it important ( positive \u0019values ) .", "entities": []}, {"text": "On the other hand , if one language assigns positive value while the other assigns negative , it shows that the parameter group is language - speci\ufb01c .", "entities": []}, {"text": "We compare them across languages and layers to analyze parameter similarity in multilingual models .", "entities": []}, {"text": "3.2 Experimental Setup We focus on standard multilingual masked language modeling ( MLM ) used in mBERT and XLM .", "entities": [[8, 11, "TaskName", "masked language modeling"], [12, 13, "DatasetName", "MLM"], [16, 17, "MethodName", "mBERT"], [18, 19, "MethodName", "XLM"]]}, {"text": "We \ufb01rst pretrain models and then evaluate their performance on four NLP benchmarks .", "entities": []}, {"text": "For pretraining , we mainly follow the setup and implementation of XLM ( Lample and Conneau , 2019 ) .", "entities": [[11, 12, "MethodName", "XLM"]]}, {"text": "We focus on monolingual and bilingual models for a more controllable comparison , which we refer to as Mono andJointPair respectively .", "entities": []}, {"text": "In particular , we always include English ( En ) in bilingual models to compare on zero - shot transfer settings with prior work .", "entities": []}, {"text": "Besides , we consider three", "entities": []}, {"text": "4441ModelNER ( F1 ) POS ( F1 ) ar fr ru hi", "entities": [[2, 3, "MetricName", "F1"], [6, 7, "MetricName", "F1"]]}, {"text": "sw te avg ar fr ru hi te avg", "entities": []}, {"text": "Within - language Monolingual Mono 89.2 88.0 87.8 89.1 85.1 82.1 86.9 92.7 76.2 96.7 97.0 94.5 91.4 JointPair 86.9", "entities": []}, {"text": "86.5 84.2 88.3 86.1 76.2 84.7 89.2 75.8 93.2 95.2 88.7 88.4 + ffn 88.2 88.4 86.6 88.9 85.4 81.2 86.5 92.4 76.1 95.6 96.1 92.4 90.5 + attn 87.3 86.8 84.1 88.5 84.9 77.4 84.8 91.8 75.4 94.4 95.3 90.9 89.6 + adpt 87.8 86.8 84.5 87.7 86.3 77.0 85.0 91.7 75.6 94.0 95.2 91.5 89.6 + share adpt 86.8 86.7 84.3 88.6 86.1 76.0 84.8 89.3 76.4 93.5 95.2 88.2 88.5 + meta adpt 88.9 88.3 85.1 88.4 86.5 79.5 86.1 92.4 75.9 95.1 95.8 92.2 90.3 XLM 89.4 87.5 85.5 88.5 86.3 80.5 86.3 94.5 72.9 96.6 97.1 92.2 90.7 Zero - shot Cross - lingual JointPair 38.1 77.5 57.5 61.4 64.8 45.2 57.4 58.5 44.2 80.1 58.9 72.8 62.9 + ffn 8.9 35.2 5.8 10.5 9.7 12.5 13.8 5.4 8.1 4.5 3.3 7.7 5.8 + attn 15.4 39.4 10.2 9.9 13.4 11.6 16.7 6.2 4.5 7.5 4.8 6.9 6.0 + adpt 37.2 75.5 59.2 61.0 64.4 44.7 57.0 57.0 43.5 81.6 58.2 73.5 62.8 + share adpt 38.5 77.8 58.4 62.0 65.4 44.5 57.8 58.7 43.8 82.5 59.7 71.8 63.3 + meta adpt 44.4 78.5 62.4 66.0 67.3 50.1 61.5 63.5 44.6 84.9 62.7 78.5 66.8 XLM 44.8", "entities": [[89, 90, "MethodName", "XLM"], [200, 201, "MethodName", "XLM"]]}, {"text": "78.3 63.6 65.8 68.4 49.3 61.7 62.8 42.4 86.3 65.7 76.9 66.8 Table 2 : NER and POS results .", "entities": [[15, 16, "TaskName", "NER"]]}, {"text": "We observe negative interference when monolingual models outperform multilingual models .", "entities": []}, {"text": "Besides , adding language - speci\ufb01c layers ( e.g. ffn ) mitigates interference but sacri\ufb01ces transferability .", "entities": []}, {"text": "high - resource languages fArabic ( Ar ) , French ( Fr ) , Russian ( Ru)gand three low - resource languages fHindi ( Hi ) , Swahili ( Sw ) , Telugu ( Te ) g(see Table 1 for their statistics ) .", "entities": []}, {"text": "We choose these six languages based their data availability in downstream tasks .", "entities": []}, {"text": "We use Wikipedia as training data with statistics shown in Table 1 .", "entities": []}, {"text": "For each model , we use BPE ( Sennrich et al . , 2016 ) to learn 32k subword vocabulary shared between languages .", "entities": [[6, 7, "MethodName", "BPE"]]}, {"text": "For multilingual models , we sample language proportionally toPi= ( LiP jLj)1 T , whereLiis the size of the training corpus for i - th language pair and T is the temperature .", "entities": []}, {"text": "Each model is a standard Transformer ( Vaswani et al . , 2017 ) with 8 layers , 12 heads , 512 embedding size and 2048 hidden dimension for the feedforward layer .", "entities": [[5, 6, "MethodName", "Transformer"], [25, 26, "DatasetName", "2048"]]}, {"text": "Notice that we speci\ufb01cally consider a smaller model capacity to be comparable with existing models with larger capacity but also include much more ( over 100 ) languages .", "entities": []}, {"text": "We use the Adam optimizer ( Kingma and Ba , 2014 ) and exploit the same learning rate schedule as Lample and Conneau ( 2019 ) .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [16, 18, "HyperparameterName", "learning rate"]]}, {"text": "We train each model with 4 NVIDIA V100 GPUs with 32 GB of memory .", "entities": []}, {"text": "Using mixed precision , we \ufb01t a batch of 128 for each GPU and the total batch size is 512 .", "entities": [[16, 18, "HyperparameterName", "batch size"]]}, {"text": "Each epoch contains 10k steps and we train for 50 epochs .", "entities": []}, {"text": "For evaluation , we consider four downstream tasks : named entity recognition ( NER ) , part - ofspeech tagging ( POS ) , question answering ( QA ) , and natural language inference ( NLI ) .", "entities": [[9, 12, "TaskName", "named entity recognition"], [13, 14, "TaskName", "NER"], [24, 26, "TaskName", "question answering"], [31, 34, "TaskName", "natural language inference"]]}, {"text": "( See Appendix A for \ufb01netuning details.)NER We use the WikiAnn ( Pan et al . , 2017 ) dataset , which is a sequence labelling task built automatically from Wikipedia .", "entities": [[10, 11, "DatasetName", "WikiAnn"]]}, {"text": "A linear layer with softmax classi\ufb01er is added on top of pretrained models to predict the label for each word based on its \ufb01rst subword .", "entities": [[1, 3, "MethodName", "linear layer"], [4, 5, "MethodName", "softmax"]]}, {"text": "We report the F1 score .", "entities": [[3, 5, "MetricName", "F1 score"]]}, {"text": "POS Similar to NER , POS is also a sequence labelling task but with a focus on synthetic knowledge .", "entities": [[3, 4, "TaskName", "NER"]]}, {"text": "In particular , we use the Universal Dependencies treebanks ( Nivre et al . , 2018 ) .", "entities": [[6, 8, "DatasetName", "Universal Dependencies"]]}, {"text": "Task - speci\ufb01c layers are the same and we report F1 , as in NER .", "entities": [[10, 11, "MetricName", "F1"], [14, 15, "TaskName", "NER"]]}, {"text": "QA We choose to use the TyDiQA - GoldP dataset ( Clark et al . , 2020 ) that covers typologically diverse languages .", "entities": [[6, 9, "DatasetName", "TyDiQA - GoldP"]]}, {"text": "Similar to popular QA dataset such as SQuAD ( Rajpurkar et al . , 2018 ) , this is a span prediction task where task - speci\ufb01c linear classi\ufb01ers are used to predict start / end positions of the answer .", "entities": [[7, 8, "DatasetName", "SQuAD"]]}, {"text": "Standard metrics of F1 and Exact Match ( EM ) are reported .", "entities": [[3, 4, "MetricName", "F1"], [5, 7, "MetricName", "Exact Match"], [8, 9, "MetricName", "EM"]]}, {"text": "NLI XNLI ( Conneau et al . , 2018 ) is probably the most popular cross - lingual benchmark .", "entities": [[1, 2, "DatasetName", "XNLI"]]}, {"text": "Notice that the original dataset only contains training data for English .", "entities": []}, {"text": "Consequently , we only evaluate this task on the zero - shot transfer setting while we consider both settings for the rest of other tasks .", "entities": []}, {"text": "3.3 Results and Analysis In Table 2 and 3 , we report our results on NER , POS and QA together with XLM-100 , which is trained on 100 languages and contains 827 M parameters .", "entities": [[15, 16, "TaskName", "NER"]]}, {"text": "In particular , we observe that monolin-", "entities": []}, {"text": "4442Model ar ru sw te avg", "entities": []}, {"text": "Within - language Monolingual Mono 74.2 63.1 52.5 58.2 62.0 JointPair 71.3 58.2 52.8 52.2 58.6 + ffn 73.4 61.2 51.4 57.5 60.9 + attn 72.8 60.8 51.2 52.8 59.4 + adpt 71.5 59.4 52.1 55.5 59.6 + share adpt 71.0 58.5 52.8 53.9 59.1 + meta adpt 73.0 61.8 54.5 56.2 61.4 XLM 74.3 62.5 58.7 55.4 62.7 Zero - shot Cross - lingual JointPair 54.1 43.2 41.5 21.5 40.1 + ffn 2.2 0.0 4.4 0.0 1.7 + attn 3.7 2.1 0.7 0.0 1.6 + adpt 53.4 44.7 41.2 20.4 39.9 + share adpt 54.3 44.8 42.2 22.7 41.0 + meta adpt 57.5 45.8 43.0 23.1 42.4 XLM 59.4 47.3 42.3 16.3 41.3 Table 3 : TyDiQA - GoldP results ( F1 ) .", "entities": [[53, 54, "MethodName", "XLM"], [108, 109, "MethodName", "XLM"], [117, 120, "DatasetName", "TyDiQA - GoldP"], [122, 123, "MetricName", "F1"]]}, {"text": "See Appendix C for full results .", "entities": []}, {"text": "gual models outperform bilingual models for all languages except Swahili on all three tasks .", "entities": []}, {"text": "In fact , monolingual models even perform better than XLM on four out of six languages including hiandte , despite that XLM is much larger in model sizes and trained with much more resources .", "entities": [[9, 10, "MethodName", "XLM"], [21, 22, "MethodName", "XLM"]]}, {"text": "This shows that negative interference canoccur on low - resource languages as well .", "entities": []}, {"text": "While the negative impact is expected to be more prominent on high - resource languages , we demonstrate that it may occur for languages with resources fewer than commonly believed .", "entities": []}, {"text": "The existence of negative interference con\ufb01rms that state - of - the - art multilingual models can not generalize equally well on all languages , and there is still a gap compared to monolingual models on certain languages .", "entities": []}, {"text": "We next turn to dissect negative interference by studying the four factors described in Section 3.1 .", "entities": []}, {"text": "Training Corpus Size By comparing the validation perplexity on Swahili and Telugu in Figure 2 , we \ufb01nd that while both monolingual models outperform bilingual models in the \ufb01rst few epochs , the Swahili model \u2019s perplexity starts to increase and is eventually surpassed by the bilingual model in later epochs .", "entities": [[7, 8, "MetricName", "perplexity"], [36, 37, "MetricName", "perplexity"]]}, {"text": "This matches the intuition that monolingual models may over\ufb01t when training data size is small .", "entities": []}, {"text": "To verify this , we subsample French and Russian to 100k sentences to create a \u201c low - resource version \u201d of them ( denoted as frl / rul ) .", "entities": []}, {"text": "As shown in Table 5 , while the performance for both models drop compared to their \u201c high - resource \u201d counterparts , bilingual models indeed outperform mono - ModelNER ( F1 ) POS ( F1 )", "entities": [[31, 32, "MetricName", "F1"], [35, 36, "MetricName", "F1"]]}, {"text": "hi", "entities": []}, {"text": "te", "entities": []}, {"text": "hi te", "entities": []}, {"text": "Within - language Monolingual JointPair 88.3 76.2 95.2 88.7 JointTri 87.8 76.4 95.3 88.7 Zero - shot Cross - lingual JointPair 61.4 45.2 58.9 72.8 JointTri 63.5 47.6 59.5 74.4 Table 4 : Comparing trilingual models with bilingual models .", "entities": []}, {"text": "This shows the effect of adding a third similar language to bilingual models .", "entities": []}, {"text": "( a ) hi   ( b ) sw Figure 2 : Validation perplexity during pretraining .", "entities": [[13, 14, "MetricName", "perplexity"]]}, {"text": "lingual models for frl / rul , in contrast for fr / ru .", "entities": []}, {"text": "This suggests that multilingual models can stimulate positive transfer for low - resource languages when monolingual models over\ufb01t .", "entities": []}, {"text": "On the other hand , when we compare bilingual models on English , models trained using different sizes of fr / ru data obtain similar performance , indicating that the training size of the source language has little impact on negative interference on the target language ( English in this case ) .", "entities": []}, {"text": "While more training data usually implies larger vocabulary and more diverse linguistic phenomena , negative interference seems to arise from more fundamental con\ufb02icts contained in even small training corpus .", "entities": []}, {"text": "Language Similarity As illustrated by Table 5 , the in - language performance on English drops as the paired language becomes more distantly related ( French vs Russian ) .", "entities": []}, {"text": "This veri\ufb01es that transferring from more distant languages results in more severe negative interference .", "entities": []}, {"text": "It is therefore natural to ask if adding more similar languages can mitigate negative interference , especially for low - resource languages .", "entities": []}, {"text": "We then train two trilingual models , adding Marathi to English - Hindi , and Kannada to English - Telugu .", "entities": []}, {"text": "Compared to their bilingual counterparts ( Table 4 ) , trilingual models obtain similar within - language performance , which indicates that adding similar languages can not mitigate negative interference .", "entities": []}, {"text": "4443ModelNER ( F1 ) POS ( F1 ) QA ( F1 / EM ) fr frl ru rul fr frl ru rul ru ru l Within - language Performance on fr / ru Mono 88.0 81.7 87.8 82.4 76.2 68.5 96.7 88.7 63.1/49.2 47.2/29.5 JointPair 86.5 83.2 84.2 82.7 75.8 71.4 93.2 89.5 58.2/43.1 49.5/30.4", "entities": [[2, 3, "MetricName", "F1"], [6, 7, "MetricName", "F1"], [10, 11, "MetricName", "F1"], [12, 13, "MetricName", "EM"]]}, {"text": "Within - language Performance on en JointPair 78.6 78.4 75.8 75.9 94.5 94.5 92.7 92.3 61.7/49.8 62.1/50.2 Table 5 : Evaluating effects of training corpus sizes on negative interference .", "entities": []}, {"text": "Figure 3 : Gradients similarity throughout training .", "entities": []}, {"text": "\u201c En - En \u201d refers to gradients of two English batches within the Ar - En model , while \u201c Ar - En \u201d and \u201c Fr - En \u201d refer to gradients of two batches , one from each language , within Ar - En and Fr - En models respectively .", "entities": []}, {"text": "However , they do improve zero - shot cross - lingual performance .", "entities": []}, {"text": "One possible explanation is that even similar languages can \ufb01ght for language - speci\ufb01c capacity but they may nevertheless bene\ufb01t the generalization of the shared knowledge .", "entities": []}, {"text": "Gradient Con\ufb02ict In Figure 3 , we plot the gradient cosine similarity between Arabic - English and French - English in their corresponding bilingual models over the \ufb01rst 25 epochs .", "entities": []}, {"text": "We also plot the similarity within English , measured using two independently sampled batches2 .", "entities": []}, {"text": "Speci\ufb01cally , gradients between two different languages are indeed less similar than those within the same language .", "entities": []}, {"text": "The gap is more evident in the early few epochs , where we observe negative gradient similarities for Ar - En and Fr - En while those for En - En are positive .", "entities": []}, {"text": "In addition , gradients in Ar - En are less similar than those in Fr - En , indicating that distant language pair can cause more severe gradient con\ufb02icts .", "entities": []}, {"text": "These results con\ufb01rm that gradient con\ufb02ict exists in multilingual models and is correlated to per language performance , suggesting it may introduce optimization challenge that results in negative interference .", "entities": []}, {"text": "Parameter Sharing The existence of gradient 2Notice that we use gradient accumulation to sample an effectively larger batch of 4096 sentences to calculate the gradient similarity.con\ufb02icts may imply that languages are \ufb01ghting for capacity .", "entities": []}, {"text": "Thus , we next study how languageuniversal these multilingual parameters are .", "entities": []}, {"text": "Figure 4a shows the cosine similarity of mask parameters\u0019across different layers .", "entities": []}, {"text": "We observe that within - language similarity ( En - En ) is near perfect , which validates the pruning method \u2019s robustness .", "entities": []}, {"text": "The trend shows that model parameters are better shared in the bottom layers than the upper ones .", "entities": []}, {"text": "Besides , it also demonstrates that parameters in multi - head attention layers obtain higher similarities than those in feedforward layers , suggesting that attention mechanism might be more languageuniversal .", "entities": [[8, 12, "MethodName", "multi - head attention"]]}, {"text": "We additionally inspect \u0019parameters with the highest absolute values and plot those values for Ar ( Figure 4b ) , together with their En counterparts .", "entities": []}, {"text": "A more negative value indicates that the parameter is more likely to be pruned for that language and vice versa .", "entities": []}, {"text": "Interestingly , while many parameters with positive values ( on the right ) are language - universal as both languages assign very positive values , parameters with negative values ( on the left ) are mostly language - speci\ufb01c for Ar as En assigns positive values .", "entities": []}, {"text": "We observe similar patterns for other languages as well .", "entities": []}, {"text": "These results demonstrate that language - speci\ufb01c parameters do exist in multilingual models .", "entities": []}, {"text": "Having language - speci\ufb01c capacity in shared parameters is sub - optimal .", "entities": []}, {"text": "It is less transferable and thus can hinder cross - lingual performance .", "entities": []}, {"text": "Moreover , it may also take over capacity budgets for other languages and degrade their within - language performance , i.e. , causing negative interference .", "entities": []}, {"text": "A natural next question is whether explicitly adding language - speci\ufb01c capacity into multilingual models can alleviate negative interference .", "entities": []}, {"text": "We thus train variants of bilingual models that contain language - speci\ufb01c components for each language .", "entities": []}, {"text": "Particularly , we consider adding language - speci\ufb01c feedforward layers , attention layers , and residual adapter layers ( Rebuf\ufb01 et al . , 2017 ; Houlsby et al . , 2019 ) , denoted as ffn , attn and adpt respectively .", "entities": [[11, 13, "HyperparameterName", "attention layers"]]}, {"text": "4444 ( a )   ( b )   ( c ) Figure 4 : Left : Parameter similarity across layers .", "entities": []}, {"text": "Middle : Normalized pruning variables of highest absolute values for Ar in Ar - En model .", "entities": []}, {"text": "10 parameter groups with most negative values are shown on the left and 10 with most positive values are shown on the right .", "entities": []}, {"text": "Right : Average MLM training loss after the warm - up stage .", "entities": [[3, 4, "DatasetName", "MLM"], [5, 6, "MetricName", "loss"]]}, {"text": "For each type of component , we create two separate copies in each Transformer layer , one designated for each language , while the rest of the network remains unchanged .", "entities": [[13, 14, "MethodName", "Transformer"]]}, {"text": "As shown in Table 2 and 3 , adding language - speci\ufb01c capacity does mitigate negative interference and improve monolingual performance .", "entities": []}, {"text": "We also \ufb01nd that language - speci\ufb01c feedforward layers obtain larger performance gains compared to attention layers , consistent with our prior analysis .", "entities": [[15, 17, "HyperparameterName", "attention layers"]]}, {"text": "However , these gains come at a cost of cross - lingual transferability , such that their zeroshot performance drops tremendously .", "entities": []}, {"text": "Our results suggest a tension between addressing interference versus improving transferability .", "entities": []}, {"text": "In the next section , we investigate how to address negative interference in a manner that can improve performance onboth within - language tasks and cross - lingual benchmarks .", "entities": []}, {"text": "4 Mitigating Negative Interference via Meta Learning 4.1 Proposed Method", "entities": []}, {"text": "In the previous section , we demonstrated that while explicitly adding language - speci\ufb01c components can alleviate negative interference , it can also hinder cross - lingual transferability .", "entities": []}, {"text": "We notice that a critical shortcoming of language - speci\ufb01c capacity is that they are agnostic of the rest of other languages , since by design they are trained on the designated language only .", "entities": []}, {"text": "They are thus more likely to over\ufb01t and can induce optimization challenges for shared capacity as well .", "entities": []}, {"text": "Inspired by recent work in meta learning ( Flennerhag et al . , 2019 ) that utilizes meta parameters to improve gradient geometry of the base network , we propose a novel meta - learning formulation of multilingual models that exploits language - speci\ufb01c parameters to im - prove generalization of shared parameters .", "entities": [[0, 1, "DatasetName", "Inspired"], [32, 35, "TaskName", "meta - learning"]]}, {"text": "For a model with some prede\ufb01ned languagespeci\ufb01c parameters \u001e = f \u001e igL i=1 , where \u001e iis designated for the i - th language , and shared parameters \u0012 , our solution is to treat \u001e as meta parameters and \u0012as base parameters .", "entities": []}, {"text": "Ideally , we want \u001e to store non - transferable language - speci\ufb01c knowledge to resolve con\ufb02icts and improve generalization of \u0012in all languages ( a.k.a . mitigate negative interference and improve cross - lingual transferability ) .", "entities": []}, {"text": "Therefore , we train \u001e based on the following principle : if\u0012follows the gradients on training data for a given \u001e , the resulting \u0012should obtain a good validation performance on all languages .", "entities": []}, {"text": "This implies a bilevel optimization problem ( Colson et al . , 2007 ) formally written as : min \u001e 1 LLX i=1Li val(\u0012\u0003 ; \u001e i ) s.t.\u0012\u0003= arg min \u00121 LLX i=1Li train(\u0012 ; \u001e i);(2 ) whereLi valandLi traindenote the training and the validation MLM loss for the i - th language .", "entities": [[47, 48, "DatasetName", "MLM"], [48, 49, "MetricName", "loss"]]}, {"text": "Since directly solving this problem can be prohibitive due to the expensive inner optimization , we approximate\u0012\u0003by adapting the current \u0012(t)using a single gradient step , similar to techniques used in prior meta - learning methods ( Finn et al . , 2017 ) .", "entities": [[32, 35, "TaskName", "meta - learning"]]}, {"text": "This results in a two - phase iterative training process shown in Algorithm 1 ( See Appendix B ) .", "entities": []}, {"text": "To be speci\ufb01c , at each training step ton the ith language during pretraining , we \ufb01rst adapt a gradient step on \u0012to obtain a new \u00120and update \u001e i", "entities": []}, {"text": "4445Algorithm 1 Training XLM with Meta Language - speci\ufb01c Layers 1 : Input : Training data 2 : Output : The converged model f\u0012\u0003 ; \u001e \u0003g 3 : Initialize model parameters f\u0012(0 ) ; \u001e ( 0)g 4 : while not converged do 5 : Sample language i 6 : Update language - speci\ufb01c parameters as : \u001e ( t+1 ) i GradientUpdate ( \u001e ( t ) i;r \u001e ( t ) i1 LPL j=1Lj val(\u0012(t ) i\u0000 \f r\u0012(t)Li train(\u0012(t ) ; \u001e ( t ) i ) ; \u001e ( t ) j ) )", "entities": [[3, 4, "MethodName", "XLM"]]}, {"text": "7 : Update shared parameters as : \u0012(t+1 ) GradientUpdate ( \u0012(t);r\u0012(t)Ltrain(\u0012(t ) ; \u001e ( t+1 ) ) ) 8 : end while based on the \u00120 \u2019s validation MLM loss : \u001e ( t+1 ) i", "entities": [[30, 31, "DatasetName", "MLM"], [31, 32, "MetricName", "loss"]]}, {"text": "= \u001e ( t ) i\u0000 \u000b r \u001e ( t ) i1 LLX j=1Lj val(\u00120 ; \u001e ( t ) j ) \u00120=\u0012(t)\u0000 \f r\u0012(t)Li train(\u0012(t ) ; \u001e ( t ) i);(3 ) where \u000b and \f are learning rates .", "entities": []}, {"text": "Notice that \u00120is a function of \u001e ( t ) iand thus this optimization requires computing the gradient of gradient .", "entities": []}, {"text": "Particularly , by applying chain rule to the gradient of \u001e ( t ) i , we can observe that it contains a higher - order term : h r2 \u001e ( t ) i;\u0012(t)Li train(\u0012(t ) ; \u001e ( t ) i)i \u0001 \"", "entities": []}, {"text": "r\u001201 LLX j=1Lj val(\u00120 ; \u001e ( t ) j ) # ( 4 ) This is important , since it shows that \u001e ican obtain information from other languages through higherorder gradients .", "entities": []}, {"text": "In other words , language - speci\ufb01c parameters are notagnostic of other languages anymore without violating the language - speci\ufb01c requirement .", "entities": []}, {"text": "This is because , in Eq .", "entities": []}, {"text": "3 , while r\u0012(t)is based on the i - th language only , the validation loss is computed for all languages .", "entities": [[15, 16, "MetricName", "loss"]]}, {"text": "Finally , in the second phase , we update \u0012based on the new \u001e ( t+1 ): \u0012(t+1)=\u0012(t)\u0000 \f r\u0012(t)Ltrain(\u0012(t ) ; \u001e ( t+1))(5 ) 4.2 Evaluation While our method is generic , we evaluate it applied on bilingual models with adapter networks .", "entities": []}, {"text": "Adapters have been effectively utilized in multilingual models ( Bapna et al . , 2019 ) , and we choose them for practical consideration of limiting perlanguage capacity .", "entities": []}, {"text": "Unlike prior works that \ufb01netune adapters for adaptation , we train them jointly with shared parameters during pretraining .", "entities": []}, {"text": "We follow Houlsby et al .", "entities": []}, {"text": "( 2019 ) and insert language - speci\ufb01c adapters after attention and feedforward layers .", "entities": []}, {"text": "Weleave a more thorough investigation of how to better pick language - speci\ufb01c structures for future work .", "entities": []}, {"text": "For downstream task evaluation , we \ufb01netune all layers .", "entities": []}, {"text": "Notice that computing the gradient of gradient in Eq .", "entities": []}, {"text": "3 doubles the memory requirement .", "entities": []}, {"text": "In practice , we utilize the \ufb01nite difference approximation ( Appendix B ) .", "entities": []}, {"text": "By evaluating their performance on the zeroshot transfer settings ( Table 2 , 3 and 6 ) , we observe that our method , denoted as meta adpt , consistently improves the performance over JointPair baselines , while ordinary adapters ( adpt ) perform worse than JointPair .", "entities": []}, {"text": "This shows that , the proposed method can effectively utilize the added language - speci\ufb01c adapters to improve generalization of shared parameters across languages .", "entities": []}, {"text": "At the same time , our method also mitigates negative interference and outperforms JointPair on withinlanguage performance , closing the gap with monolingual models .", "entities": []}, {"text": "In particular , it performs better than ordinary adapters in both settings .", "entities": []}, {"text": "We hypothesize that this is because it alleviates language con\ufb02icts during training and thus converges more robustly .", "entities": []}, {"text": "For example , we plot training loss in the early stage in Figure 4c , which shows that ordinary adapters converge slower than JointPair due to over\ufb01tting of language - speci\ufb01c adapters while meta adapters converge much faster .", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "For ablation studies , we also report results for JointPair trained with adapters shared between two languages , denoted as share adpt .", "entities": []}, {"text": "Unlike language - speci\ufb01c adapters that can hinder transferability , shared adapters improve both within - language and cross - lingual performance with the extra capacity .", "entities": []}, {"text": "However , meta adapters still obtain better performance .", "entities": []}, {"text": "These results show that mitigating negative interference can improve multilingual representations .", "entities": []}, {"text": "4446Model ar fr ru hi sw avg JointPair 67.1 73.5 69.2 61.5 62.3 66.7 + ffn 42.5 51.4 40.7 36.2 34.8 41.1 + attn 48.5 50.7 41.2 33.3 35.1 41.8 + adpt 67.8 73.7 69.5 62.2 59.7 66.6 + share adpt 67.9 73.4 70.0 61.8 62.2 67.1 + meta adpt 68.5 74.8 70.2 64.5 61.5 67.9 XLM 68.2 75.2 72.3 65.4 58.1 67.8 Table 6 : XNLI results ( Accuracy ) .", "entities": [[56, 57, "MethodName", "XLM"], [66, 67, "DatasetName", "XNLI"], [69, 70, "MetricName", "Accuracy"]]}, {"text": "5 Related Work Unsupervised multilingual language models such as mBERT ( Devlin et al . , 2018 ) and XLM ( Lample and Conneau , 2019 ; Conneau et al . , 2019 ) work surprisingly well on many NLP tasks without parallel training signals ( Pires et al . , 2019 ; Wu and Dredze , 2019 ) .", "entities": [[9, 10, "MethodName", "mBERT"], [19, 20, "MethodName", "XLM"]]}, {"text": "A line of follow - up work ( Wu et al . , 2019 ; Artetxe et", "entities": []}, {"text": "al . , 2019 ; Karthikeyan et al . , 2020 ) study what contributes to the cross - lingual ability of these models .", "entities": []}, {"text": "They show that vocabulary overlap is not required for multilingual models , and suggest that abstractions shared across languages emerge automatically during pretraining .", "entities": []}, {"text": "Another line of research investigate how to further improve these shared knowledge , such as applying post - hoc alignment ( Wang et al . , 2020b ; Cao et al . , 2020 ) and utilizing better calibrated training signal ( Mulcaire et al . , 2019 ; Huang et al . , 2019 ) .", "entities": []}, {"text": "While prior work emphasize how to share to improve transferability , we study multilingual models from a different perspective of how to unshare to resolve language con\ufb02icts .", "entities": []}, {"text": "Our work is also related to transfer learning ( Pan and Yang , 2010 ) and multi - task learning ( Ruder , 2017 ) .", "entities": [[6, 8, "TaskName", "transfer learning"], [16, 20, "TaskName", "multi - task learning"]]}, {"text": "In particular , prior work have observed ( Rosenstein et al . , 2005 ) and studied ( Wang et al . , 2019 ) negative transfer , such that transferring knowledge from source tasks can degrade the performance in the target task .", "entities": []}, {"text": "Others show it is important to remedy negative transfer in multi - source settings ( Ge et al . , 2014 ; Wang and Carbonell , 2018 ) .", "entities": []}, {"text": "In this work , we study negative transfer in multilingual models , where languages contain heavily unbalanced training data and exhibit complex intertask relatedness .", "entities": []}, {"text": "In addition , our work is related to methods that measure similarity between cross - lingual representations .", "entities": []}, {"text": "For example , existing methods utilize statistical metrics to examine cross - lingual embeddings such as singular vector canonical correlation analysis ( Raghu et al . , 2017 ; Kudugunta et al . , 2019 ) , eigenvector similarity ( S\u00f8gaard et al . , 2018),and centered kernel alignment ( Kornblith et al . , 2019 ; Wu et al . , 2019 ) .", "entities": []}, {"text": "While these methods focus on testing latent representations , we directly compare similarity of neural network structures through network pruning .", "entities": [[18, 20, "TaskName", "network pruning"]]}, {"text": "Finally , our work is related to meta learning , which sets a meta task to learn model initialization for fast adaptation ( Finn et al . , 2017 ; Gu et al . , 2018 ; Flennerhag et al . , 2019 ) , data selection ( Wang et al . , 2020a ) , and hyperparameters ( Baydin et", "entities": []}, {"text": "al . , 2018 )", "entities": []}, {"text": ".", "entities": []}, {"text": "In our case , the meta task is to mitigate negative interference .", "entities": []}, {"text": "6 Conclusion We present the \ufb01rst systematic study of negative interference in multilingual models and shed light on its causes .", "entities": []}, {"text": "We further propose a method and show it can improve cross - lingual transferability by mitigating negative interference .", "entities": []}, {"text": "While prior efforts focus on improving sharing and cross - lingual alignment , we provide new insights and a different perspective on unsharing and resolving language con\ufb02icts .", "entities": []}, {"text": "Acknowledgments We want to thank Jaime Carbonell for his support on the early stage of this project .", "entities": []}, {"text": "We also would like to thank Zihang Dai , Graham Neubig , Orhan Firat , Yuan Cao , Jiateng Xie , Xinyi Wang , Ruochen Xu and Yiheng Zhou for insightful discussions .", "entities": []}, {"text": "Lastly , we thank anonymous reviewers for their valueable feedbacks .", "entities": []}, {"text": "References Roee Aharoni , Melvin Johnson , and Orhan Firat .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Massively multilingual neural machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "CoRR , abs/1903.00089 .", "entities": []}, {"text": "Naveen Arivazhagan , Ankur Bapna , Orhan Firat , Dmitry Lepikhin , Melvin Johnson , Maxim Krikun , Mia Xu Chen , Yuan Cao , George Foster , Colin Cherry , et al . 2019 .", "entities": []}, {"text": "Massively multilingual neural machine translation in the wild : Findings and challenges .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1907.05019 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mikel Artetxe , Sebastian Ruder , and Dani Yogatama .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "On the cross - lingual transferability of monolingual representations .", "entities": []}, {"text": "arXiv preprint arXiv:1910.11856 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ankur Bapna , Naveen Arivazhagan , and Orhan Firat .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Simple , scalable adaptation for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1909.08478 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "4447Atilim Gunes Baydin , Robert Cornish , David Martinez Rubio , Mark Schmidt , and Frank Wood .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Online learning rate adaptation with hypergradient descent .", "entities": [[0, 2, "TaskName", "Online learning"]]}, {"text": "Steven Cao , Nikita Kitaev , and Dan Klein .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multilingual alignment of contextual word representations .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Jonathan H Clark , Eunsol Choi , Michael Collins , Dan Garrette , Tom Kwiatkowski , Vitaly Nikolaev , and Jennimaria Palomaki .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Tydi qa : A benchmark for information - seeking question answering in typologically diverse languages .", "entities": [[0, 2, "DatasetName", "Tydi qa"], [9, 11, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:2003.05002 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Beno \u02c6\u0131t Colson , Patrice Marcotte , and Gilles Savard .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "An overview of bilevel optimization .", "entities": []}, {"text": "Annals of operations research , 153(1):235\u2013256 .", "entities": []}, {"text": "Alexis Conneau , Kartikay Khandelwal , Naman Goyal , Vishrav Chaudhary , Guillaume Wenzek , Francisco Guzm \u00b4 an , Edouard Grave , Myle Ott , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unsupervised cross - lingual representation learning at scale .", "entities": [[4, 6, "TaskName", "representation learning"]]}, {"text": "arXiv preprint arXiv:1911.02116 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Alexis Conneau , Guillaume Lample , Ruty Rinott , Adina Williams , Samuel R Bowman , Holger Schwenk , and Veselin Stoyanov .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Xnli : Evaluating crosslingual sentence representations .", "entities": [[0, 1, "DatasetName", "Xnli"]]}, {"text": "arXiv preprint arXiv:1809.05053 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Chelsea Finn , Pieter Abbeel , and Sergey Levine .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Model - agnostic meta - learning for fast adaptation of deep networks .", "entities": [[0, 6, "MethodName", "Model - agnostic meta - learning"]]}, {"text": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , pages 1126\u20131135 . JMLR .", "entities": []}, {"text": "org .", "entities": []}, {"text": "Sebastian Flennerhag , Andrei A Rusu , Razvan Pascanu , Hujun Yin , and Raia Hadsell . 2019 .", "entities": []}, {"text": "Metalearning with warped gradient descent .", "entities": []}, {"text": "arXiv preprint arXiv:1909.00025 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Liang Ge , Jing Gao , Hung Ngo , Kang Li , and Aidong Zhang .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "On handling negative transfer and imbalanced distributions in multiple source transfer learning .", "entities": [[10, 12, "TaskName", "transfer learning"]]}, {"text": "Statistical Analysis and Data Mining : The ASA Data Science Journal , 7(4):254\u2013271 .", "entities": []}, {"text": "Jiatao Gu , Yong Wang , Yun Chen , Victor OK Li , and Kyunghyun Cho . 2018 .", "entities": []}, {"text": "Meta - learning for lowresource neural machine translation .", "entities": [[0, 3, "TaskName", "Meta - learning"], [6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3622\u20133631 .", "entities": []}, {"text": "Neil Houlsby , Andrei Giurgiu , Stanislaw Jastrzebski , Bruna Morrone , Quentin De Laroussilhe , Andrea Gesmundo , Mona Attariyan , and Sylvain Gelly .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Parameter - ef\ufb01cient transfer learning for nlp .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "arXiv preprint arXiv:1902.00751 .Junjie", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hu , Sebastian Ruder , Aditya Siddhant , Graham Neubig , Orhan Firat , and Melvin Johnson .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Xtreme : A massively multilingual multi - task benchmark for evaluating cross - lingual generalization .", "entities": []}, {"text": "arXiv preprint arXiv:2003.11080 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Haoyang Huang , Yaobo Liang , Nan Duan , Ming Gong , Linjun Shou , Daxin Jiang , and Ming Zhou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Unicoder :", "entities": []}, {"text": "A universal language encoder by pretraining with multiple cross - lingual tasks .", "entities": []}, {"text": "arXiv preprint arXiv:1909.00964 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Melvin Johnson , Mike Schuster , Quoc V Le , Maxim Krikun , Yonghui Wu , Zhifeng Chen , Nikhil Thorat , Fernanda Vi \u00b4 egas , Martin Wattenberg , Greg Corrado , et al . 2017 .", "entities": []}, {"text": "Google \u2019s multilingual neural machine translation system : Enabling zero - shot translation .", "entities": [[0, 1, "DatasetName", "Google"], [4, 6, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 5:339\u2013351 .", "entities": []}, {"text": "K Karthikeyan , Zihan Wang , Stephen Mayhew , and Dan Roth . 2020 .", "entities": []}, {"text": "Cross - lingual ability of multilingual bert : An empirical study .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Diederik P Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv preprint arXiv:1412.6980 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Simon Kornblith , Mohammad Norouzi , Honglak Lee , and Geoffrey Hinton .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Similarity of neural network representations revisited .", "entities": []}, {"text": "In International Conference on Machine Learning .", "entities": []}, {"text": "Sneha Reddy Kudugunta , Ankur Bapna , Isaac Caswell , Naveen Arivazhagan , and Orhan Firat .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Investigating multilingual nmt representations at scale .", "entities": []}, {"text": "arXiv preprint arXiv:1909.02197 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Guillaume Lample and Alexis Conneau .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crosslingual language model pretraining .", "entities": []}, {"text": "arXiv preprint arXiv:1901.07291 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Christos Louizos , Max Welling , and Diederik P Kingma . 2017 .", "entities": []}, {"text": "Learning sparse neural networks through l0regularization .", "entities": []}, {"text": "arXiv preprint arXiv:1712.01312 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Phoebe Mulcaire , Jungo Kasai , and Noah A Smith .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Polyglot contextual representations improve crosslingual transfer .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3912\u20133918 .", "entities": []}, {"text": "Joakim Nivre , Mitchell Abrams , \u02c7Zeljko Agi \u00b4 c , Lars Ahrenberg , Lene Antonsen , Maria Jesus Aranzabe , Gashaw Arutie , Masayuki Asahara , Luma Ateyah , Mohammed Attia , et al . 2018 .", "entities": [[18, 19, "TaskName", "Jesus"]]}, {"text": "Universal dependencies 2.2 .", "entities": [[0, 2, "DatasetName", "Universal dependencies"]]}, {"text": "4448Sinno Jialin Pan and Qiang Yang .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "A survey on transfer learning .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "IEEE Transactions on knowledge and data engineering , 22(10):1345\u20131359 .", "entities": []}, {"text": "Xiaoman Pan , Boliang Zhang , Jonathan May , Joel Nothman , Kevin Knight , and Heng Ji . 2017 .", "entities": []}, {"text": "Crosslingual name tagging and linking for 282 languages .", "entities": []}, {"text": "InProceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1946\u20131958 .", "entities": []}, {"text": "Telmo Pires , Eva Schlinger , and Dan Garrette .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "How multilingual is multilingual bert ?", "entities": []}, {"text": "arXiv preprint arXiv:1906.01502 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Maithra Raghu , Justin Gilmer , Jason Yosinski , and Jascha Sohl - Dickstein . 2017 .", "entities": []}, {"text": "Svcca : Singular vector canonical correlation analysis for deep learning dynamics and interpretability .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 6076 \u2013 6085 .", "entities": []}, {"text": "Pranav Rajpurkar , Robin Jia , and Percy Liang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Know what you do n\u2019t know : Unanswerable questions for squad .", "entities": []}, {"text": "arXiv preprint arXiv:1806.03822 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sylvestre - Alvise Rebuf\ufb01 , Hakan Bilen , and Andrea Vedaldi .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Learning multiple visual domains with residual adapters .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 506\u2013516 .", "entities": []}, {"text": "Michael T Rosenstein , Zvika Marx , Leslie Pack Kaelbling , and Thomas G Dietterich .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "To transfer or not to transfer .", "entities": []}, {"text": "In NIPS 2005 workshop on transfer learning , volume 898 , pages 1\u20134 .", "entities": [[5, 7, "TaskName", "transfer learning"]]}, {"text": "Sebastian Ruder .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "An overview of multi - task learning in deep neural networks .", "entities": [[3, 7, "TaskName", "multi - task learning"]]}, {"text": "arXiv preprint arXiv:1706.05098 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 .", "entities": []}, {"text": "Anders S\u00f8gaard , Sebastian Ruder , and Ivan Vuli \u00b4 c. 2018 .", "entities": []}, {"text": "On the limitations of unsupervised bilingual dictionary induction .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 778 \u2013 788 .", "entities": []}, {"text": "Xu Tan , Yi Ren , Di He , Tao Qin , Zhou Zhao , and TieYan Liu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Multilingual neural machine translation with knowledge distillation .", "entities": [[2, 4, "TaskName", "machine translation"], [5, 7, "MethodName", "knowledge distillation"]]}, {"text": "arXiv preprint arXiv:1902.10461 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5998\u20136008.Xinyi Wang , Yulia Tsvetkov , and Graham Neubig .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "Balancing training for multilingual neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:2004.06748 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zirui Wang and Jaime Carbonell . 2018 .", "entities": []}, {"text": "Towards more reliable transfer learning .", "entities": [[3, 5, "TaskName", "transfer learning"]]}, {"text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases , pages 794\u2013810 .", "entities": []}, {"text": "Zirui Wang , Zihang Dai , Barnab \u00b4 as P\u00b4oczos , and Jaime Carbonell . 2019 .", "entities": []}, {"text": "Characterizing and avoiding negative transfer .", "entities": []}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 11293\u201311302 .", "entities": []}, {"text": "Zirui Wang , Jiateng Xie , Ruochen Xu , Yiming Yang , Graham Neubig , and Jaime Carbonell .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Cross - lingual alignment vs joint training : A comparative study and a simple uni\ufb01ed framework .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Shijie Wu , Alexis Conneau , Haoran Li , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Emerging cross - lingual structure in pretrained language models.arXiv preprint arXiv:1911.01464 .", "entities": []}, {"text": "Shijie Wu and Mark Dredze .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Beto , bentz , becas : The surprising cross - lingual effectiveness of bert .", "entities": []}, {"text": "arXiv preprint arXiv:1904.09077 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Carbonell , Russ R Salakhutdinov , and Quoc V Le . 2019 .", "entities": []}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5754\u20135764 .", "entities": []}, {"text": "Tianhe Yu , Saurabh Kumar , Abhishek Gupta , Sergey Levine , Karol Hausman , and Chelsea Finn .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "2020 .", "entities": []}, {"text": "Gradient surgery for multi - task learning .", "entities": [[3, 7, "TaskName", "multi - task learning"]]}, {"text": "arXiv preprint arXiv:2001.06782 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "4449A Fine - tuning Details Notice that XNLI only has training data in available in English so we only evaluate zero - shot crosslingual performance on it .", "entities": [[7, 8, "DatasetName", "XNLI"]]}, {"text": "Following ( Hu et al . , 2020 ) , we \ufb01netune the model for 10 epochs for NER and POS , 2 epochs for QA and 200 epochs for XNLI .", "entities": [[18, 19, "TaskName", "NER"], [30, 31, "DatasetName", "XNLI"]]}, {"text": "For NER , POS and QA , we search the following hyperparameters : batch size f16 , 32 g ; learning ratef2e-5 , 3e-5 , 5e-5 g. We use English dev set for zero - shot cross - lingual setting and the target language dev set for within - language monolingual setting .", "entities": [[1, 2, "TaskName", "NER"], [13, 15, "HyperparameterName", "batch size"]]}, {"text": "For XNLI , we search for : batch sizef4 , 8 g ; encoder learning rate f1e-6 , 5e-6 , 2e5 g ; classi\ufb01er learning rate f5e-6 , 2e-5 , 5e-5 g. For models with language - speci\ufb01c components , we test freezing these components or \ufb01netuning them together .", "entities": [[1, 2, "DatasetName", "XNLI"], [14, 16, "HyperparameterName", "learning rate"], [24, 26, "HyperparameterName", "learning rate"]]}, {"text": "We discover that \ufb01netuning the whole network always yields better results .", "entities": []}, {"text": "For all experiments , we save checkpoint after each epoch .", "entities": []}, {"text": "B Method Details Letzibe the output of the i - th layer of dimension d.", "entities": []}, {"text": "The residual adapter network ( Rebuf\ufb01 et al . , 2017 ; Houlsby et al . , 2019 ; Bapna et al . , 2019 ) is a bottleneck layer that \ufb01rst projects zito an inner layer with dimension b : hi = g(Wz izi ) ( 6 ) whereWz", "entities": []}, {"text": "i2Rd\u0002bandgis some activation function such as relu .", "entities": [[2, 4, "HyperparameterName", "activation function"], [6, 7, "MethodName", "relu"]]}, {"text": "It is then projected back to the original input dimension dwith a residual connection : oi = Wh ihi+zi ( 7 ) whereWh i2Rb\u0002d .", "entities": [[12, 14, "MethodName", "residual connection"]]}, {"text": "In our experiments , we \ufb01x b= 1 4d .", "entities": []}, {"text": "For a bilingual model of lg1andlg2 , we inject two langauge - speci\ufb01c adapters after each attention and feedforward layer , one for each language .", "entities": []}, {"text": "For example , if the input text is in lg1 , the network will be routed to adapters designated for lg1 .", "entities": []}, {"text": "The rest of the network and training protocol remain unchanged .", "entities": []}, {"text": "The injected adapter layers mimic the warp layers interleaved between base network layers in Flennerhag", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Warp layers are meta parameters that aim to improve the performance of the base network .", "entities": []}, {"text": "They precondition base network gradients to obtain better gradient geometry .", "entities": []}, {"text": "In our experiments , we treat language - speci\ufb01c adapters as meta parameters to improve generalization of the shared network .", "entities": []}, {"text": "The algorithm is outlined inAlgorithm 1 .", "entities": []}, {"text": "The adapters are updated according to Eq 3 , which doubles the memory requirement .", "entities": []}, {"text": "In particular , the high - order term in Eq 4 requires computing the gradient of gradient .", "entities": []}, {"text": "In practice , we approximate this term using the \ufb01nite difference approximation as : r \u001e ( t ) iLi train(\u0012+ ; \u001e ( t ) i)\u0000r \u001e ( t ) iLi train(\u0012\u0000 ; \u001e ( t ) i ) 2\u000f(8 ) where \u0012\u0006=\u0012(t)\u0006\u000fr\u001201 LPL j=1Lj val(\u00120 ; \u001e ( t ) j)and \u000fis a small scalar .", "entities": []}, {"text": "We use the same value for learning rates \u000b and \f in Eq 3 , to be consistent with standard learning rate schedule used in XLM ( Lample and Conneau , 2019 ) .", "entities": [[20, 22, "HyperparameterName", "learning rate"], [25, 26, "MethodName", "XLM"]]}, {"text": "C Extra Results We show the full results on the TyDiQA - GoldP dataset in Table 7 .", "entities": [[10, 13, "DatasetName", "TyDiQA - GoldP"]]}, {"text": "4450Model ar ru sw te avg", "entities": []}, {"text": "Within - language Monolingual Mono 74.2/62.5 63.1/49.2 52.5/37.4 58.2/41.0 62.0/47.5 JointPair 71.3/58.1 58.2/43.1 52.8/39.0 52.2/36.4 58.6/44.2 + ffn 73.4/61.2 61.2/45.8 51.4/34.3 57.5/40.5 60.9/45.5 + attn 72.8/61.0 60.8/45.4 51.2/34.0 52.8/36.8 59.4/44.3 +", "entities": []}, {"text": "adpt 71.5/58.7 59.4/44.8 52.1/38.7 55.5/38.9 59.6/45.3 + share adpt 71.0/57.8 58.5/43.2 52.8/39.0 53.9/37.2 59.1/44.3 + meta adpt 73.0/61.4 61.8/46.7 54.5/40.0 56.2/39.5 61.4/36.4 XLM 74.3/63.2 62.5/48.7 58.7/40.4 55.4/38.3 62.7/47.7 Zero - shot Cross - lingual JointPair 54.1/39.5 43.2/27.5 41.5/22.2 21.5/14.7 40.1/26.0 + ffn 2.2/1.5 0.0/0.0 4.4/3.7 0.0/0.0 1.7/1.3 + attn 3.7/2.0 2.1/1.2 0.7/1.0 0.0/0.0 1.6/1.1 + adpt 53.4/39.1 44.7/27.9 41.2/21.8 20.4/13.8 39.9/25.7 + share adpt 54.3/39.6 44.8/27.8 42.2/22.9 22.7/15.6 41.0/26.5 + meta adpt 57.5/40.8 45.8/28.8 43.0/24.2 23.1/17.7 42.4/27.9 XLM 59.4/41.2 47.3/29.8 42.3/22.0 16.3/7.2 41.3/25.1 Table 7 : Full results on TyDiQA - GoldP ( F1 / EM ) .", "entities": [[22, 23, "MethodName", "XLM"], [77, 78, "MethodName", "XLM"], [89, 92, "DatasetName", "TyDiQA - GoldP"], [93, 94, "MetricName", "F1"], [95, 96, "MetricName", "EM"]]}]