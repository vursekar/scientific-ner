[{"text": "Learning and Reasoning for Robot Dialog and Navigation Tasks Keting Lu1Shiqi Zhang2Peter Stone3Xiaoping Chen1 1USTC2SUNY Binghamton3UT Austin ktlu@mail.ustc.edu.cn ; zhangs@binghamton.edu ; pstone@cs.utexas.edu ; xpchen@ustc.edu.cn Abstract Reinforcement learning and probabilistic reasoning algorithms aim at learning from interaction experiences and reasoning with probabilistic contextual knowledge respectively .", "entities": []}, {"text": "In this research , we develop algorithms for robot task completions , while looking into the complementary strengths of reinforcement learning and probabilistic reasoning techniques .", "entities": []}, {"text": "The robots learn from trial - and - error experiences to augment their declarative knowledge base , and the augmented knowledge can be used for speeding up the learning process in potentially different tasks .", "entities": []}, {"text": "We have implemented and evaluated the developed algorithms using mobile robots conducting dialog and navigation tasks .", "entities": []}, {"text": "From the results , we see that our robot \u2019s performance can be improved by both reasoning with human knowledge and learning from task - completion experience .", "entities": []}, {"text": "More interestingly , the robot was able to learn from navigation tasks to improve its dialog strategies .", "entities": []}, {"text": "1 Introduction Knowledge representation and reasoning ( KRR ) and reinforcement learning ( RL ) are two important research areas in arti\ufb01cial intelligence ( AI ) and have been applied to a variety of problems in robotics .", "entities": []}, {"text": "On the one hand , KRR research aims to concisely represent knowledge , and robustly draw conclusions with the knowledge ( or generate new knowledge ) .", "entities": []}, {"text": "Knowledge in KRR is typically provided by human experts in the form of declarative rules .", "entities": []}, {"text": "Although KRR paradigms are strong in representing and reasoning with knowledge in a variety of forms , they are not designed for ( and hence not good at ) learning from experiences of accomplishing the tasks .", "entities": []}, {"text": "On the other hand , RL algorithms enable agents to learn by interacting with an environment , and RL agents are good at learning action policies from trial - and - error experiences toward maximizing long - term rewards un - der uncertainty , but they are ill - equipped to utilize declarative knowledge from human experts .", "entities": []}, {"text": "Motivated by the complementary features of KRR and RL , we aim at a framework that integrates both paradigms to enable agents ( robots in our case ) to simultaneously reason with declarative knowledge and learn by interacting with an environment .", "entities": []}, {"text": "Most KRR paradigms support the representation and reasoning of knowledge in logical form , e.g. , Prolog - style .", "entities": []}, {"text": "More recently , researchers have developed hybrid KRR paradigms that support both logical and probabilistic knowledge ( Richardson and Domingos , 2006 ; Bach et al . , 2017 ; Wang et al . , 2019 ) .", "entities": []}, {"text": "Such logical - probabilistic KRR paradigms can be used for a variety of reasoning tasks .", "entities": []}, {"text": "We use P - log ( Baral et al . , 2009 ) in this work to represent and reason with both human knowledge and the knowledge from RL .", "entities": []}, {"text": "The reasoning results are then used by our robot to compute action policies at runtime .", "entities": []}, {"text": "Reinforcement learning ( RL ) algorithms can be used to help robots learn action policies from the experience of interacting with the real world ( Sutton and Barto , 2018 ) .", "entities": []}, {"text": "We use model - based RL in this work , because the learned world model can be used to update the robot \u2019s declarative knowledge base and combined with human knowledge .", "entities": []}, {"text": "Theoretical Contribution :", "entities": []}, {"text": "In this paper , we develop a learning and reasoning framework ( called KRR - RL ) that integrates logical - probabilistic KRR and model - based RL .", "entities": []}, {"text": "The KRR component reasons with the qualitative knowledge from humans ( e.g. , it is dif\ufb01cult for a robot to navigate through a busy area ) and the quantitative knowledge from modelbased RL ( e.g. , a navigation action \u2019s success rate in the form of a probability ) .", "entities": []}, {"text": "The hybrid knowledge is then used for computing action policies at runtime by planning with task - oriented partial world models .", "entities": []}, {"text": "KRR - RL enables a robot to : i ) represent", "entities": []}, {"text": "the probabilistic knowledge ( i.e. , world dynamics ) learned from RL in declarative form ; ii ) unify and reason with both human knowledge and the knowledge from RL ; and iii ) compute policies at runtime by dynamically constructing task - oriented partial world models .", "entities": []}, {"text": "Application Domain : We use a robot delivery domain for demonstration and evaluation purposes , where the robot needs to dialog with people to \ufb01gure out the delivery task \u2019s goal location , and then physically take navigation actions to complete the delivery task ( Thomason et al . , 2020 ; Veloso , 2018 ) .", "entities": []}, {"text": "A delivery is deemed successful only if both the dialog and navigation subtasks are successfully conducted .", "entities": []}, {"text": "We have conducted experiments using a simulated mobile robot , as well as demonstrated the system using a real mobile robot .", "entities": []}, {"text": "Results show that the robot is able to learn world dynamics from navigation tasks through model - based RL , and apply the learned knowledge to both navigation tasks ( with different goals ) and delivery tasks ( that require subtasks of navigation and dialog ) through logical - probabilistic reasoning .", "entities": []}, {"text": "In particular , we observed that the robot is able to adjust its dialog strategy through learning from navigation behaviors .", "entities": []}, {"text": "2 Related Work Research areas related to this work include integrated logical KRR and RL , relational RL , and integrated KRR and probabilistic planning .", "entities": []}, {"text": "Logical KRR has previously been integrated with RL .", "entities": []}, {"text": "Action knowledge ( McDermott et al . , 1998 ; Jiang et al . , 2019 ) has been used to reason about action sequences and help an RL agent explore only the states that can potentially contribute to achieving the ultimate goal ( Leonetti et al . , 2016 ) .", "entities": [[29, 30, "DatasetName", "agent"]]}, {"text": "As a result , their agents learn faster by avoiding choosing \u201c unreasonable \u201d actions .", "entities": []}, {"text": "A similar idea has been applied to domains with nonstationary dynamics ( Ferreira et al . , 2017 ) .", "entities": []}, {"text": "More recently , task planning was used to interact with the high level of a hierarchical RL framework ( Yang et al . , 2018 ) .", "entities": []}, {"text": "The goal shared by these works is to enable RL agents to use knowledge to improve the performance in learning ( e.g. , to learn faster and/or avoid risky exploration ) .", "entities": []}, {"text": "However , the KRR capabilities of these methods are limited to logical action knowledge .", "entities": []}, {"text": "By contrast , we use a logicalprobabilistic KRR paradigm that can directly reason with probabilities learned from RL.Relational RL ( RRL ) combines RL with relational reasoning ( D \u02c7zeroski et", "entities": [[25, 27, "TaskName", "relational reasoning"]]}, {"text": "al . , 2001 ) .", "entities": []}, {"text": "Action models have been incorporated into RRL , resulting in a relational temporal difference learning method ( Asgharbeygi et al . , 2006 ) .", "entities": []}, {"text": "Recently , RRL has been deployed for learning affordance relations that forbid the execution of speci\ufb01c actions ( Sridharan et al . , 2017 ) .", "entities": []}, {"text": "These RRL methods , including deep RRL ( Zambaldi et al . , 2018 ) , exploit structural representations over states and actions in ( only ) current tasks .", "entities": []}, {"text": "In this research , KRR - RL supports the KRR of world factors beyond those in state and action representations , e.g. , time in navigation tasks , as detailed in Section 4.2 .", "entities": []}, {"text": "The research area of integrated KRR and probabilistic planning is related to this research .", "entities": []}, {"text": "Logicalprobabilistic reasoning has been used to compute informative priors and world dynamics ( Zhang et al . , 2017 ; Amiri et al . , 2020 ) for probabilistic planning .", "entities": []}, {"text": "An action language was used to compute a deterministic sequence of actions for robots , where individual actions are then implemented using probabilistic controllers ( Sridharan et al . , 2019 ) .", "entities": []}, {"text": "Recently , human - provided information has been incorporated into belief state representations to guide robot action selection ( Chitnis et al . , 2018 ) .", "entities": []}, {"text": "In comparison to our approach , learning ( from reinforcement or not ) was not discussed in the abovementioned algorithms .", "entities": []}, {"text": "Finally , there are a number of robot reasoning and learning architectures ( Tenorth and Beetz , 2013 ; Oh et al . , 2015 ; Hanheide et", "entities": []}, {"text": "al . , 2017 ; Khandelwal et al . , 2017 ) , which are relatively complex , and support a variety of functionalities .", "entities": []}, {"text": "In comparison , we aim at a concise representation for robot KRR and RL capabilities .", "entities": []}, {"text": "To the best of our knowledge , this is the \ufb01rst work on a tightly coupled integration of logical - probabilistic KRR with model - based RL .", "entities": []}, {"text": "3 Background We brie\ufb02y describe the two most important building blocks of this research , namely model - based RL and hybrid KRR .", "entities": []}, {"text": "3.1 Model - based Reinforcement Learning Following the Markov assumption , a Markov decision process ( MDP ) can be described as a fourtuplehS ; A;T;Ri(Puterman , 1994 ) .", "entities": []}, {"text": "Sde\ufb01nes the state set , where we assume a factored space in this work .", "entities": []}, {"text": "Ais the action set .", "entities": []}, {"text": "T : S\u0002A\u0002S !", "entities": []}, {"text": "[ 0;1]speci\ufb01es the state transition probabilities .", "entities": []}, {"text": "R : S\u0002A !", "entities": []}, {"text": "Rspeci\ufb01es the rewards .", "entities": []}, {"text": "Solving an MDP produces an action policy p : s7!athat maps a state to an action to maximize long - term rewards .", "entities": []}, {"text": "RL methods fall into classes including modelbased and model - free .", "entities": []}, {"text": "Model - based RL methods learn a model of the domain by approximating R(s;a)andP(s0js;a)for state - action pairs , where Prepresents the probabilistic transition system .", "entities": []}, {"text": "An agent can then use planning methods to calculate an action policy ( Sutton , 1990 ; Kocsis and Szepesv \u00b4 ari , 2006 ) .", "entities": [[1, 2, "DatasetName", "agent"]]}, {"text": "Model - based methods are particularly attractive in this work , because they output partial world models that can better accommodate the diversity of tasks we are concerned with , c.f . , modelfree RL that is typically goal - directed .", "entities": []}, {"text": "One of the best known examples of model - based RL is R - Max ( Brafman and Tennenholtz , 2002 ) , which is guaranteed to learn a near - optimal policy with a polynomial number of suboptimal ( exploratory ) actions .", "entities": []}, {"text": "The algorithm classi\ufb01es each state - action pair as known or unknown , according to the number of times it was visited .", "entities": []}, {"text": "When planning on the model , known state - actions are modeled with the learned reward , while unknown stateactions are given the maximum one - step reward , Rmax .", "entities": []}, {"text": "This \u201c maximum - reward \u201d strategy automatically enables the agent to balance the exploration of unknown states and exploitation .", "entities": [[10, 11, "DatasetName", "agent"]]}, {"text": "We use R - Max in this work , though KRR - RL practitioners can use supervised machine learning methods , e.g. , imitation learning ( Osa et al . , 2018 ) , to build the model learning component .", "entities": [[23, 25, "TaskName", "imitation learning"]]}, {"text": "3.2 Logical Probabilistic KRR KRR paradigms are concerned with concisely representing and robustly reasoning with declarative knowledge .", "entities": []}, {"text": "Answer set programming ( ASP ) is a non - monotonic logical KRR paradigm ( Baral , 2010 ; Gelfond and Kahl , 2014 ) building on the stable model semantics ( Gelfond and Lifschitz , 1988 ) .", "entities": []}, {"text": "An ASP program consists of a set of logical rules , in the form of \u201c head : - body \u201d , that read \u201c head is true if body is true \u201d .", "entities": []}, {"text": "Each ASP rule is of the form : a or ... or b : - c , ... , d , not e , ... , not f. where a ... f are literals that correspond to true or false statements .", "entities": []}, {"text": "Symbol notis a logical connective called default negation ; not l is read as \u201c it is not believed that lis true \u201d , which does not imply thatlis false .", "entities": []}, {"text": "ASP has a variety of applications ( Erdem et al . , 2016 ) .", "entities": []}, {"text": "Model - based RL Human   KRR Declarative   knowledge World   dynamics World   Controller   New task Learning   Task   completions Figure 1 : An overview of KRR - RL for robot learning and reasoning to complete complex tasks .", "entities": []}, {"text": "Traditionally , ASP does not explicitly quantify degrees of uncertainty : a literal is either true , false or unknown .", "entities": []}, {"text": "P - log extends ASP to allow probability atoms ( orpr - atoms ) ( Baral et al . , 2009 ; Balai and Gelfond , 2017 ) .", "entities": []}, {"text": "The following pr - atom states that , if Bholds , the probability of a(t)=y isv : pr(a(t)=y|B)=v .", "entities": []}, {"text": "where Bis a collection of literals or their default negations ; ais a random variable ; tis a vector of terms ( a term is a constant or a variable ) ; yis a term ; andv2[0;1 ] .", "entities": []}, {"text": "Reasoning with an ASP program generates a set of possible worlds : fW0;W1;\u0001\u0001\u0001g .", "entities": []}, {"text": "The pr - atoms in P - log enable calculating a probability for each possible world .", "entities": []}, {"text": "Therefore , P - log is a KRR paradigm that supports both logical and probabilistic inferences .", "entities": []}, {"text": "We use P - log in this work for KRR purposes .", "entities": []}, {"text": "4 KRR - RL Framework KRR - RL integrates logical - probabilistic KRR and model - based RL , and is illustrated in Figure 1 .", "entities": []}, {"text": "The KRR component includes both declarative qualitative knowledge from humans and the probabilistic knowledge from model - based RL .", "entities": []}, {"text": "When the robot is free , the robot arbitrarily selects goals ( different navigation goals in our case ) to work on , and learns the world dynamics , e.g. , success rates and costs of navigation actions .", "entities": []}, {"text": "When a task becomes available , the KRR component dynamically constructs a partial world model ( excluding unrelated factors ) , on which a task - oriented controller is computed using planning algorithms .", "entities": []}, {"text": "Human knowledge concerns environment variables and their dependencies , i.e. , what variables are related to each action .", "entities": []}, {"text": "For instance , the human provides knowledge that navigation actions \u2019 success rates depend on current time and area ( say elevator areas are busy in the mornings ) , while the robot must learn speci\ufb01c probabilities by interacting with the environment .", "entities": []}, {"text": "Why is KRR - RL needed ?", "entities": []}, {"text": "Consider an indoor robot navigation domain , where a robot wants to", "entities": [[3, 5, "TaskName", "robot navigation"]]}, {"text": "maximize the success rate of moving to goal positions through navigation actions .", "entities": []}, {"text": "Shall we include factors , such as time , weather , positions of human walkers , etc , into the state space ?", "entities": []}, {"text": "On the one hand , to ensure model completeness , the answer should be \u201c yes \u201d .", "entities": []}, {"text": "Human walkers and sunlight ( that blinds robot \u2019s LiDAR sensors ) reduce the success rates of the robot \u2019s navigation actions , and both can cause the robot irrecoverably lost .", "entities": []}, {"text": "On the other hand , to ensure computational feasibility , the answer is \u201c no \u201d .", "entities": []}, {"text": "Modeling whether one speci\ufb01c grid cell being occupied by humans or not introduces one extra dimension in the state space , and doubles the state space size .", "entities": []}, {"text": "If we consider ( only ) ten such grid cells , the state space becomes 210\u00191000 times bigger .", "entities": []}, {"text": "As a result , RL practitioners frequently have to make a trade - off between model completeness and computational feasibility .", "entities": []}, {"text": "In this work , we aim at a framework that retains both model scalability and computational feasibility , i.e. , the agent is able to learn within relatively little memory while computing action policies accounting for a large number of domain variables .", "entities": [[21, 22, "DatasetName", "agent"]]}, {"text": "4.1 A General Procedure In factored spaces , state variables V= fV0;V1 ; : : : ; Vn\u00001gcan be split into two categories , namely endogenous variables Ven and exogenous variables Vex(Chermack , 2004 ) , whereVen = fVen 0;Ven 1 ; : : : ; Ven p\u00001gand Vex = fVex 0;Vex 1 ; : : : ; Vex q\u00001 g.", "entities": [[2, 3, "DatasetName", "General"]]}, {"text": "In our integrated KRRRL context , Venis goal - oriented and includes the variables whose values the robot wants to actively change so as to achieve the goal ; and Vexcorresponds to the variables whose values affect the robot \u2019s action outcomes , but the robot can not ( or does not want to ) change their values .", "entities": []}, {"text": "Therefore , VenandVexboth depend on task t.", "entities": []}, {"text": "Continuing the navigation example , robot position is an endogenous variable , and current time is an exogenous variable .", "entities": []}, {"text": "For each task , V = Ven[Vex and n = p+q , and RL agents learn in spaces speci\ufb01ed byVen .", "entities": []}, {"text": "The KRR component models V , their dependencies from human knowledge , and conditional probabilities on how actions change their values , as learned through model - based RL .", "entities": []}, {"text": "When a task arrives , the KRR component uses probabilistic rules to generate a task - oriented Markov decision process ( MDP ) ( Puterman , 1994 ) , which only contains a subset ofVthat are relevant to the current task , Procedure 1 Learning in KRR - RL Framework Require : Logical rules PL ; probabilistic rules PP ; random variables V = fV0;V1 ; : : : ; Vn\u00001 g ; task selector D ; and guidance functions ( from human knowledge ) of fV(V;t)and fA(t ) 1 : while Robot has no task do 2 : t D ( ): a task is heuristically selected 3 : Ven fV(V;t ) , andVex VnVen 4 : A fA(t ) 5 : M Procedure- 2(PL;PP;Ven;Vex;A ) 6 : Initialize agent : agent R - Max ( M ) 7 : RLagent repeatedly works on task t , and keeps maintaining task model M0 , until policy convergence 8 : end while 9 : Use M0to update PP i.e. ,Ven , and their transition probabilities .", "entities": [[130, 131, "DatasetName", "agent"], [132, 133, "DatasetName", "agent"]]}, {"text": "Given this task - oriented MDP , a corresponding action policy is computed using value iteration or policy iteration .", "entities": []}, {"text": "Procedures 1 and 2 focus on how our KRR - RL agent learns by interacting with an environment when there is no task assigned.1Next , we present the details of these two interleaved processes .", "entities": [[11, 12, "DatasetName", "agent"]]}, {"text": "Procedure 1 includes the steps of the learning process .", "entities": []}, {"text": "When the robot is free , it interacts with the environment by heuristically selecting a task2 , and repeatedly using a model - based RL approach , RMax ( Brafman and Tennenholtz , 2002 ) in our case , to complete the task .", "entities": []}, {"text": "The two guidance functions come from human knowledge .", "entities": []}, {"text": "For instance , given a navigation task , it comes from human knowledge that the robot should model its own position ( speci\ufb01ed by fV ) and actions that help the robot move between positions ( speci\ufb01ed by fA ) .", "entities": []}, {"text": "After the policy converges or this learning process is interrupted ( e.g. , by task arrivals ) , the robot uses the learned probabilities to update the corresponding world dynamics in KRR .", "entities": []}, {"text": "For instance , the robot may have learned the probability and cost of navigating through a particular area in early morning .", "entities": []}, {"text": "In case this learning process is interrupted , the sofar-\u201cknown \u201d probabilities are used for knowledge base update .", "entities": []}, {"text": "Procedure 2 includes the steps for building the probabilistic transition system of MDPs .", "entities": []}, {"text": "The key point is that we consider only endogenous variables in the task - speci\ufb01c state space .", "entities": []}, {"text": "However , when 1As soon as the robot \u2019s learning process is interrupted by the arrival of a real service task ( identi\ufb01ed via dialog ) , it will call Procedure 2 to generate a controller to complete the task .", "entities": []}, {"text": "This process is not included in the procedures .", "entities": []}, {"text": "2Here curriculum learning in RL ( Narvekar et al . , 2017 ) can play a role to task selection and we leave this aspect of the problem for future work .", "entities": []}, {"text": "Dialog manager Navigation   \u201c What item ? \u201d", "entities": []}, {"text": "\u201c Apple , please \u201d Once become confident about the request   Human Action : Move to X Current state   Environment Restart if wrong delivery If successful ,   return Figure 2 : Transition system speci\ufb01ed for delivery tasks , where question - asking actions are used for estimating the service request in dialog .", "entities": []}, {"text": "Once the robot becomes con\ufb01dent about the service request , it starts to work on the navigation subtask .", "entities": []}, {"text": "After the robot arrives , the robot might have to come back to the dialog subtask and redeliver , depending on whether the service request was correctly identi\ufb01ed .", "entities": []}, {"text": "reasoning to compute the transition probabilities ( Line 5 ) , the KRR component uses both PPand Vex .", "entities": []}, {"text": "The computed probabilistic transition systems are used for building task - oriented controllers , i.e. , p , for task completions .", "entities": []}, {"text": "In this way , the dynamically constructed controllers do not directly include exogenous variables , but their parameters already account for the values of all variables .", "entities": []}, {"text": "Next , we demonstrate how our KRR - RL framework is instantiated on a real robot .", "entities": []}, {"text": "4.2 An Instantiation on a Mobile Robot", "entities": []}, {"text": "We consider a mobile service robot domain where a robot can do navigation , dialog , and delivery tasks .", "entities": []}, {"text": "Anavigation task requires the robot to use a sequence of ( unreliable ) navigation actions to move from one point to another .", "entities": []}, {"text": "In a dialog task , the robot uses spoken dialog actions to specify service requests from people under imperfect language understanding .", "entities": []}, {"text": "There is the trend of integrating language and navigation in the NLP and CV communities ( Chen et al . , 2019 ; Shridhar et al . , 2020 ) .", "entities": []}, {"text": "In this paper , they are integrated into delivery tasks that require the robot to use dialog to \ufb01gure out the delivery request and conduct navigation tasks to physically ful\ufb01ll the request .", "entities": []}, {"text": "Speci\ufb01cally , a delivery task requires the robot to deliver item Ito room Rfor person P , resulting in services in the form of < I , R , P > .", "entities": []}, {"text": "The challenges come from unreliable human language understanding ( e.g. , speech recognition ) and unforeseen obstacles that probabilistically block the robot in navigation .", "entities": [[11, 13, "TaskName", "speech recognition"]]}, {"text": "Human - Robot Dialog The robot needs spoken dialog to identify the request under unreliable language understanding , and navigation controllers for physically making the delivery .", "entities": []}, {"text": "The service request is not directly observable to the robot , and has to be estimated by asking questions , such as \u201c What item do you want ? \u201d", "entities": []}, {"text": "and \u201c IsProcedure 2 Model Construction for Task Completion Require : PL;PP;Ven;Vex ; Action set A 1 : forVi2Ven , iin[0;\u0001\u0001\u0001 ; jVenj\u00001]do 2 : foreach possible value vinrange ( Vi)do 3 : foreach a2Ado 4 : foreach possible value v0inrange ( Vi)do 5 : M(v0ja;v ) Reason with PLandPPw.r.t Vex 6 : end for 7 : end for 8 : end for 9 : end for 10 : return M this delivery for Alice ? \u201d", "entities": []}, {"text": "Once the robot is con\ufb01dent about the request , it takes a delivery action ( i.e. , serve(I , R , P ) ) .", "entities": []}, {"text": "We follow a standard way to use partially observable MDPs ( POMDPs ) ( Kaelbling et al . , 1998 ) to build our dialog manager , as reviewed in ( Young et al . , 2013 ) .", "entities": []}, {"text": "The state set Sis speci\ufb01ed using curr s.", "entities": []}, {"text": "The action setAis speci\ufb01ed using serve and question - asking actions .", "entities": []}, {"text": "Question - asking actions do not change the current state , and delivery actions lead to one of the terminal states ( success or failure).3 After the robot becomes con\ufb01dent about the request via dialog , it will take a delivery action serve fI , R , P g.", "entities": []}, {"text": "This delivery action is then implemented with a sequence of actmove actions .", "entities": []}, {"text": "When the request identi\ufb01cation is incorrect , the robot needs to come back to the shop , \ufb01gure out the correct request , and redeliver , where we assume the robot will correctly identify the request in the second dialog .", "entities": []}, {"text": "We use an MDP to model this robot navigation task , where the states and actions are speci\ufb01ed using sorts cell andmove .", "entities": [[7, 9, "TaskName", "robot navigation"]]}, {"text": "We use pr - atoms to represent the success rates of the unreliable movements , which are learned through model - based RL .", "entities": []}, {"text": "The dialog system builds on our 3More details in the supplementary document .", "entities": []}, {"text": "previous work ( Lu et al . , 2017 ) .", "entities": []}, {"text": "Figure 2 shows the probabilistic transitions in delivery tasks .", "entities": []}, {"text": "Learning from Navigation We use RMax ( Brafman and Tennenholtz , 2002 ) , a model - based RL algorithm , to help our robot learn the success rate of navigation actions in different positions .", "entities": []}, {"text": "The agent \ufb01rst initializes an MDP , from which it uses R - Max to learn the partial world model ( of navigation tasks ) .", "entities": [[1, 2, "DatasetName", "agent"]]}, {"text": "Speci\ufb01cally , it initializes the transition function with TN(s;a;sv ) = 1:0 , where s2S and a2A , meaning that starting from any state , after any action , the next state is always sv .", "entities": []}, {"text": "The reward function is initialized with R(s;a )", "entities": []}, {"text": "= Rmax , where Rmaxis an upper bound of reward .", "entities": []}, {"text": "The initialization of TNandRenables the learner to automatically balance exploration and exploitation .", "entities": []}, {"text": "There is a \ufb01xed small cost for each navigation action .", "entities": []}, {"text": "The robot receives a big bonus if it successfully achieves the goal ( Rmax ) , whereas it receives a big penalty otherwise ( \u0000Rmax ) .", "entities": []}, {"text": "A transition probability in navigation , TN(s;a;s0 ) , is not computed until there are a minimum number ( M ) of transition samples visiting s0 .", "entities": []}, {"text": "We recompute the action policy after Eaction steps .", "entities": []}, {"text": "Dialog - Navigation Connection The update of knowledge base is achieved through updating the success rate of delivery actions serve(I , R , P ) ( in dialog task ) using the success rate of navigation actions actmove = M in different positions .", "entities": []}, {"text": "TD(sr;ad;st )", "entities": []}, {"text": "=(", "entities": []}, {"text": "PN(ssp;sgl);ifsr \f ad PN(ssp;smi)\u0002PN(smi;ssp)\u0002PN(ssp;sgl);ifsr ad where TD(sr;ad;st)is the probability of ful\ufb01lling request srusing delivery action ad;stis the \u201c success \u201d terminal state ; ssp , smiandsglare states of the robot being in the shop , a misidenti\ufb01ed goal position , and real goal position respectively ; and PN(s;s0)is the probability of the robot successfully navigating from stos0positions .", "entities": []}, {"text": "When srandad are aligned in all three dimensions ( i.e. , sr \f ad ) , the robot needs to navigate once from the shop ( ssp ) to the requested navigation goal ( sgl).PN(ssp;sgl ) is the probability of the corresponding navigation task .", "entities": []}, {"text": "When the request and delivery action are not aligned in at least one dimension ( i.e. , sr ad ) , the robot has to navigate back to the shop to \ufb01gure out the correct request , and then redeliver , resulting in three navigation tasks .", "entities": []}, {"text": "Figure 3 : Occupancy - grid map used in our experiments ( Left ) , including \ufb01ve rooms , one shop , and four blocking areas ( indicated by \u2018 BA \u2019 ) , where all deliveries are from the shop and to one of the rooms ; and ( Right ) mobile robot platform used in this research .", "entities": [[30, 31, "DatasetName", "BA"]]}, {"text": "Intuitively , the penalty of failures in a dialog subtask depends on the dif\ufb01culty of the wrongly identi\ufb01ed navigation subtask .", "entities": []}, {"text": "For instance , a robot supposed to deliver to a near ( distant ) location being wrongly directed to a distant ( near ) location , due to a failure in the dialog subtask , will produce a higher ( lower ) penalty to the dialog agent .", "entities": [[46, 47, "DatasetName", "agent"]]}, {"text": "5 Experiments In this section , the goal is to evaluate our hypothesis that our KRR - RL framework enables a robot to learn from model - based RL , reason with both the learned knowledge and human knowledge , and dynamically construct task - oriented controllers .", "entities": []}, {"text": "Speci\ufb01cally , our robot learns from navigation tasks , and applied the learned knowledge ( through KRR ) to navigation , dialog , and delivery tasks .", "entities": []}, {"text": "We also evaluated whether the learned knowledge can be represented and applied to tasks under different world settings .", "entities": []}, {"text": "In addition to simulation experiments , we have used a real robot to demonstrate how our robot learns from navigation to perform better in dialog .", "entities": []}, {"text": "Figure 3 shows the map of the working environment ( generated using a real robot ) used in both simulation and real - robot experiments .", "entities": []}, {"text": "Human walkers in the blocking areas ( \u201c BA \u201d ) can probabilistically impede the robot , resulting in different success rates in navigation tasks .", "entities": [[8, 9, "DatasetName", "BA"]]}, {"text": "We have implemented our KRR - RL framework on a mobile robot in an of\ufb01ce environment .", "entities": []}, {"text": "As shown in Figure 3 , the robot is equipped with two Lidar sensors for localization and obstacle avoidance in navigation , and a Kinect RGB - D camera for human - robot interaction .", "entities": []}, {"text": "We use the Speech Application Programming Interface ( SAPI ) package ( http://www.iflytek.com/en ) for speech recognition .", "entities": [[15, 17, "TaskName", "speech recognition"]]}, {"text": "The robot software runs in the Robot Operating System ( ROS ) ( Quigley et al . , 2009 ) .", "entities": []}, {"text": "( a ) ( b ) ( c ) ( d ) ( e ) ( f ) Figure 4 :", "entities": []}, {"text": "Screenshots of a demonstration trial on a real robot .", "entities": []}, {"text": "( a ) User gives the service request ; ( b ) The robot decided to con\ufb01rm about the item , considering its unreliable language understanding capability ; ( c ) After hearing \u201c coke \u201d , the robot became more con\ufb01dent about the item , and decided to ask again about the goal room \u2019 ; ( d ) After hearing \u201c of\ufb01ce2 \u201d , the robot became con\ufb01dent about the whole request , and started to work on the task ; ( e ) Robot was on the way to the kitchen to pick up the object ; and ( f ) Robot arrived at the kitchen , and was going to pick up the object for delivery .", "entities": []}, {"text": "An Illustrative Trial on a Robot : Figure 4 shows the screenshots of milestones of a demo video , which will be made available given its acceptance .", "entities": []}, {"text": "After hearing \u201c a coke for Bob to of\ufb01ce2 \u201d , the three sub - beliefs are updated ( turn1 ) .", "entities": []}, {"text": "Since the robot is aware of its unreliable speech recognition , it asked about the item , \u201c Which item is it ? \u201d", "entities": [[8, 10, "TaskName", "speech recognition"]]}, {"text": "After hearing \u201c a coke \u201d , the belief is updated ( turn2 ) , and the robot further con\ufb01rmed on the item by asking \u201c Should I deliver a coke ? \u201d", "entities": []}, {"text": "It received a positive response ( turn3 ) , and decided to move on to ask about the delivery room : \u201c Should I deliver to of\ufb01ce 2 ? \u201d", "entities": []}, {"text": "After this question , the robot did not further con\ufb01rm the delivery room , because it learned through model - based RL that navigating tooffice2 is relatively easy and it decided that it is more worth risking an error and having to replan than it is to ask the person another question .", "entities": []}, {"text": "The robot became con\ufb01dent in three dimensions of the service request ( < coke , Bob , office2 > inturn4 ) without asking about person , because of the prior knowledge ( encoded in P - log ) about Bob \u2019s of\ufb01ce .", "entities": []}, {"text": "Figure 5 shows the belief changes ( in the di - mensions of item , person , and room ) as the robot interacts with a human user .", "entities": []}, {"text": "The robot started with a uniform distribution in all three categories .", "entities": []}, {"text": "It should be noted that , although the marginal distributions are uniform , the joint belief distribution is not , as the robot has prior knowledge such as Bob \u2019s of\ufb01ce is office2 and people prefer deliveries to their own of\ufb01ces .", "entities": []}, {"text": "Demo video is not included to respect the anonymous review process .", "entities": []}, {"text": "Learning to Navigate from Navigation Tasks In this experiment , the robot learns in the shop - room1 navigation task , and extracts the learned partial world model to the shop - room2 task .", "entities": [[2, 3, "TaskName", "Navigate"]]}, {"text": "It should be noted that navigation from shop to room2 requires traveling in areas that are unnecessary in the shop - room1 task .", "entities": []}, {"text": "Figure 6 presents the results , where each data points corresponds to an average of 1000 trials .", "entities": []}, {"text": "Each episode allows at most 200 ( 300 ) steps in small ( large ) domain .", "entities": []}, {"text": "The curves are smoothed using a window of 10 episodes .", "entities": []}, {"text": "The results suggest that with knowledge extraction ( the dashed line ) the robot learns faster than without extraction , and this performance improvement is more signi\ufb01cant in a larger domain ( the Right sub\ufb01gure ) .", "entities": []}, {"text": "Learning to Dialog and Navigate from Navigation Tasks Robot delivering objects requires both tasks : dialog management for specifying service request ( under unreliable speech recognition ) and navigation for physically delivering objects ( under unforeseen obstacles ) .", "entities": [[4, 5, "TaskName", "Navigate"], [24, 26, "TaskName", "speech recognition"]]}, {"text": "Our of\ufb01ce domain includes \ufb01ve rooms , two persons , and three items , resulting in 30 possible service requests .", "entities": []}, {"text": "In the dialog manager , the reward function gives delivery actions a big bonus ( 80 ) if a request is ful\ufb01lled , and a big penalty ( -80 ) otherwise .", "entities": []}, {"text": "General questions and con\ufb01rming questions cost 2.0 and 1.5 respectively .", "entities": [[0, 1, "DatasetName", "General"]]}, {"text": "In case a dialog does not end after 20 turns , the robot is forced to work on the most likely delivery .", "entities": []}, {"text": "The cost / bonus / penalty values are heuristically set in this work , following guidelines based on studies from the literature on dialog agent behaviors ( Zhang and Stone , 2015 ) .", "entities": [[24, 25, "DatasetName", "agent"]]}, {"text": "Table 1 : Overall performance in delivery tasks ( requiring both dialog and navigation ) .", "entities": []}, {"text": "Static policy KRR - RL Reward Ful\ufb01lled QA Cost Reward Ful\ufb01lled QA Cost br=0:1 182.07 0.851 20.86 206.21 0.932 18.73 br=0:5 30.54 0.853 20.84 58.44 0.927 18.98 br=0:7 -40.33 0.847 20.94 -14.50 0.905 20.56", "entities": []}, {"text": "Figure 5 : Belief change in three dimensions ( In order from the left : Items , Persons and Of\ufb01ces ) over \ufb01ve turns in a human - robot dialog .", "entities": []}, {"text": "The distributions are grouped by turns ( Including the initial distribution ) .", "entities": []}, {"text": "In each turn , there are three distribution bars which means three different dimensions ( In order from the left : Item , Person and Of\ufb01ce ) .", "entities": []}, {"text": "In order from the bottom , the values in each dimension are 1 ) coke , coffee and soda in Item ; 2 ) John and Bob in Person ; and 3 ) of\ufb01ce1 , of\ufb01ce2 , of\ufb01ce3 , of\ufb01ce4 and of\ufb01ce5 in Of\ufb01ce .", "entities": []}, {"text": "01234567", "entities": []}, {"text": "Episodes ( x10)-200 - 150 - 100 - 500Reward 30x30 grid w/o extraction w/ extraction 0123456789101112 Episodes ( x10)-350 - 300 - 250 - 200 - 150 - 100 - 50050Reward 50x50 grid w/o extraction w/ extraction Figure 6 : Navigation tasks in small ( Left : 30 \u000230 grid ) and large ( Right : 50 \u000250 grid ) domains .", "entities": []}, {"text": "With extraction ( KRR - RL in dashed line ) , the robot learns faster in the target navigation task .", "entities": []}, {"text": "Table 1 reports the robot \u2019s overall performance in delivery tasks , which requires accurate dialog for identifying delivery tasks and safe navigation for object delivery .", "entities": []}, {"text": "We conduct 10,000 simulation trials under each blocking rate .", "entities": []}, {"text": "Without learning from RL , the robot uses a world model ( outdated ) that was learned under br=0:3 .", "entities": []}, {"text": "With learning , the robot updates its world model in domains with different blocking rates .", "entities": []}, {"text": "We can see , when learning is enabled , our KRR - RL framework produces higher overall reward , higher request ful\ufb01llment rate , and lower question - asking cost .", "entities": []}, {"text": "The improvement is statistically signi\ufb01cant , i.e. , the p\u0000values are 0.028 , 0.035 , and 0.049 for overall reward , when bris 0.1 , 0.5 , and 0.7 respectively ( 100 randomly selected trials with / without extraction ) .", "entities": []}, {"text": "Learning to Adjust Dialog Strategies from Navigation In the last experiment , we quantify the information collected in dialog in terms of entropy reduction .", "entities": []}, {"text": "The hypothesis is that , using our KRRRL framework , the dialog manager wants to collect more information before physically working on more challenging tasks .", "entities": []}, {"text": "In each trial , we randomly generate a belief distribution over all possible service requests , evaluate the entropy of this belief , and record the suggested action given this belief .", "entities": []}, {"text": "We then statistically analyze the entropy values of beliefs , under which delivery actions are suggested .", "entities": []}, {"text": "Table 2 : The amount of information ( in terms of entropy ) needed by a robot before taking delivery actions .", "entities": []}, {"text": "Entropy ( room1 ) Entropy ( room2 ) Entropy ( room5 ) Mean ( std ) Max Mean ( std ) Max Mean ( std ) Max br=0:1 .274 ( .090 ) .419 .221 ( .075 ) .334 .177", "entities": []}, {"text": "( .063 ) .269 br=0:5 .154 ( .056 ) .233 .111 ( .044 ) .176 .100 ( .041 ) .156 br=0:7 .132 ( .050 ) .207 .104", "entities": []}, {"text": "( .042 ) .166 .100 ( .041 ) .156 Table 2 shows that , when brgrows from 0.1 to 0.7 , the means of belief entropy decreases ( i.e. , belief is more converged ) .", "entities": []}, {"text": "This suggests that the robot collected more information in dialog in environments that are more challenging for navigation , which is consistent with Table 1 in the main paper .", "entities": []}, {"text": "Comparing the three columns of results , we \ufb01nd the robot collects the most information before it delivers to room5 .", "entities": []}, {"text": "This is because such delivery tasks are the most dif\ufb01cult due to the location of room5 .", "entities": []}, {"text": "The results support our hypothesis that learning from navigation tasks enables the robot to adjust its information gathering strategy in dialog given tasks of different dif\ufb01culties .", "entities": []}, {"text": "Adaptive Control in New Circumstances The knowledge learned through model - based RL is contributed to a knowledge base that can be used for many tasks .", "entities": []}, {"text": "So our KRR - RL framework enables a robot to dynamically generate partial world models for tasks under settings that were never experienced .", "entities": []}, {"text": "For example , an agent does not know the current time is morning or noon , there are two possible values for variable \u201c time \u201d .", "entities": [[4, 5, "DatasetName", "agent"]]}, {"text": "Consider that our agent has learned world dynamics under the times of morning and noon .", "entities": [[3, 4, "DatasetName", "agent"]]}, {"text": "Our KRR - RL framework enables the robot to reason about the two transition systems under the two settings and generate a new transition system for this \u201c morning - or - noon \u201d setting .", "entities": []}, {"text": "baseline KRR - RL204060CostFigure 7 : Adaptive behaviors under new circumstances .", "entities": []}, {"text": "Without our framework , an agent would have to randomly select one between the \u201c morning \u201d and \u201c noon \u201d policies .", "entities": [[5, 6, "DatasetName", "agent"]]}, {"text": "To evaluate our policies dynamically constructed via KRR , we let an agent learn three controllers under three different environment settings \u2013 the navigation actions have decreasing success rates under the settings .", "entities": [[12, 13, "DatasetName", "agent"]]}, {"text": "In this experiment , the robot does not know which setting it is in ( out of two that are randomly selected ) .", "entities": []}, {"text": "The baseline does not have the KRR capability of merging knowledge learned from different settings , and can only randomly select a policy from the two ( each corresponding to a setting ) .", "entities": []}, {"text": "Experimental results show that the baseline agent achieved an average of 26:8%success rate in navigation tasks , whereas our KRRRL agent achieved 83:8%success rate on average .", "entities": [[6, 7, "DatasetName", "agent"], [20, 21, "DatasetName", "agent"]]}, {"text": "Figure 7 shows the costs in a box plot ( including min - max , 25 % , and 75 % values ) .", "entities": []}, {"text": "Thus , KRR - RL enables a robot to effectively apply the learned knowledge to tasks under new settings .", "entities": []}, {"text": "Let us take a closer look at the \u201c time \u201d variable T. IfTis the domain of T , the RL - only baseline has to compute a total of 2jTjworld models to account for all possible information about the value of T , where 2jTjis the number of subsets of T. If there areNsuch variables , the number of world models grows exponentially to 2jTj\u0001N.", "entities": []}, {"text": "In comparison , the KRR - RL agent needs to compute only jTjNworld models , which dramatically reduces the number of parameters that must be learned through RL while retaining policy quality .", "entities": [[7, 8, "DatasetName", "agent"], [19, 22, "HyperparameterName", "number of parameters"]]}, {"text": "6 Conclusions and Future Work We develop a KRR - RL framework that integrates computational paradigms of logical - probabilistic knowledge representation and reasoning ( KRR ) , and model - based reinforcement learning ( RL ) .", "entities": []}, {"text": "Our KRR - RL agent learns world dynamics via modelbased RL , and then incorporates the learned dynamics into the logical - probabilistic reasoning module , which is used for dynamic construction of ef\ufb01cientrun - time task - speci\ufb01c planning models .", "entities": [[4, 5, "DatasetName", "agent"]]}, {"text": "Experiments were conducted using a mobile robot ( simulated and physical ) working on delivery tasks that involve both navigation and dialog .", "entities": []}, {"text": "Results suggested that the learned knowledge from RL can be represented and used for reasoning by the KRR component , enabling the robot to dynamically generate task - oriented action policies .", "entities": []}, {"text": "The integration of a KRR paradigm and modelbased RL paves the way for at least the following research directions .", "entities": []}, {"text": "We plan to study how to sequence source tasks to help the robot perform the best in the target task ( i.e. , a curriculum learning problem within the RL context ( Narvekar et al . , 2017 ) ) .", "entities": []}, {"text": "Balancing the ef\ufb01ciencies between service task completion and RL is another topic for further study \u2013 currently the robot optimizes for task completions ( without considering the potential knowledge learned in this process ) once a task becomes available .", "entities": []}, {"text": "Fundamentally , all domain variables are endogenous , because one can hardly \ufb01nd variables whose values are completely independent from robot actions .", "entities": []}, {"text": "However , for practical reasons ( such as limited computational resources ) , people have to limit the number of endogenous .", "entities": []}, {"text": "It remains an open question of how to decide what variables should be considered as being endogenous .", "entities": []}, {"text": "Acknowledgments This work is supported in part by the National Natural Science Foundation of China under grant number U1613216 .", "entities": []}, {"text": "This work has taken place partly in the Autonomous Intelligent Robotics ( AIR ) Group at SUNY Binghamton .", "entities": []}, {"text": "AIR research is supported in part by grants from the National Science Foundation ( IIS-1925044 ) , Ford Motor Company ( URP Award ) , OPPO ( Faculty Research Award ) , and SUNY Research Foundation .", "entities": []}, {"text": "This work has taken place partly in the Learning Agents Research Group ( LARG ) at the Arti\ufb01cial Intelligence Laboratory , The University of Texas at Austin .", "entities": [[24, 25, "DatasetName", "Texas"]]}, {"text": "LARG research is supported in part by grants from the National Science Foundation ( CPS-1739964 , IIS-1724157 , NRI-1925082 ) , the Of\ufb01ce of Naval Research ( N00014 - 18 - 2243 ) , Future of Life Institute ( RFP2 - 000 ) , DARPA , Lockheed Martin , General Motors , and Bosch .", "entities": [[44, 45, "DatasetName", "DARPA"], [49, 50, "DatasetName", "General"]]}, {"text": "The views and conclusions contained in this document are those of the authors alone .", "entities": []}, {"text": "Peter Stone serves as the Executive Director of Sony AI America and receives \ufb01nancial compensation for this work .", "entities": []}, {"text": "The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research .", "entities": [[14, 15, "DatasetName", "Texas"]]}, {"text": "References Saeid Amiri , Mohammad Shokrolah Shirazi , and Shiqi Zhang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Learning and reasoning for robot sequential decision making under uncertainty .", "entities": [[6, 10, "TaskName", "decision making under uncertainty"]]}, {"text": "In AAAI .", "entities": []}, {"text": "Nima Asgharbeygi , David Stracuzzi , and Pat Langley .", "entities": [[0, 1, "MethodName", "Nima"]]}, {"text": "2006 .", "entities": []}, {"text": "Relational temporal difference learning .", "entities": []}, {"text": "In Proceedings of the 23rd international Conference on Machine learning .", "entities": []}, {"text": "Stephen H. Bach , Matthias Broecheler , Bert Huang , and Lise Getoor .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Hinge - loss markov random \ufb01elds and probabilistic soft logic .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "JMLR , 18(1):3846\u20133912 .", "entities": []}, {"text": "Evgenii Balai and Michael Gelfond . 2017 .", "entities": []}, {"text": "Re\ufb01ning and generalizing p - log : Preliminary report .", "entities": []}, {"text": "In Proceedings of the 10th Workshop on ASPOCP .", "entities": []}, {"text": "Chitta Baral .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Knowledge Representation , Reasoning and Declarative Problem Solving , 1st edition .", "entities": []}, {"text": "Cambridge University Press , New York , NY , USA .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Chitta Baral , Michael Gelfond , and Nelson Rushton . 2009 .", "entities": []}, {"text": "Probabilistic reasoning with answer sets .", "entities": []}, {"text": "Theory and Practice of Logic Programming , 9(1):57 \u2013 144 .", "entities": []}, {"text": "Ronen I Brafman and Moshe Tennenholtz .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Rmax - a general polynomial time algorithm for nearoptimal reinforcement learning .", "entities": []}, {"text": "JMLR .", "entities": []}, {"text": "Howard Chen , Alane Suhr , Dipendra Misra , Noah Snavely , and Yoav Artzi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Touchdown : Natural language navigation and spatial reasoning in visual street environments .", "entities": []}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 12538\u201312547 .", "entities": []}, {"text": "Thomas J Chermack .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Improving decisionmaking with scenario planning .", "entities": []}, {"text": "Futures , 36(3):295 \u2013 309 .", "entities": []}, {"text": "Rohan Chitnis , Leslie Pack Kaelbling , and Tom \u00b4 as Lozano - P \u00b4 erez .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Integrating human - provided information into belief state representation using dynamic factorization .", "entities": []}, {"text": "In IEEE / RSJ International Conference on Intelligent Robots and Systems .", "entities": []}, {"text": "Sa\u02c7so D \u02c7zeroski , Luc De Raedt , and Kurt Driessens .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Relational reinforcement learning .", "entities": []}, {"text": "Machine learning .", "entities": []}, {"text": "Esra Erdem , Michael Gelfond , and Nicola Leone .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Applications of answer set programming .", "entities": []}, {"text": "AI Magazine , 37(3):53\u201368 .", "entities": []}, {"text": "Leonardo A Ferreira , Reinaldo AC Bianchi , Paulo E Santos , and Ramon Lopez de Mantaras .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Answer set programming for non - stationary markov decision processes .", "entities": []}, {"text": "Applied Intelligence , 47(4):993 \u2013 1007.Michael Gelfond and Yulia Kahl .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Knowledge Representation , Reasoning , and the Design of Intelligent Agents : The Answer - Set Programming Approach .", "entities": []}, {"text": "Cambridge University Press .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Michael Gelfond and Vladimir Lifschitz .", "entities": []}, {"text": "1988 .", "entities": []}, {"text": "The Stable Model Semantics for Logic Programming .", "entities": []}, {"text": "InInternational Conference on Logic Programming , pages 1070\u20131080 .", "entities": []}, {"text": "Marc Hanheide , Moritz G \u00a8obelbecker , Graham S Horn , et al . 2017 .", "entities": []}, {"text": "Robot task planning and explanation in open and uncertain worlds .", "entities": [[0, 3, "TaskName", "Robot task planning"]]}, {"text": "Arti\ufb01cial Intelligence , 247:119\u2013150 .", "entities": []}, {"text": "Yuqian Jiang , Shiqi Zhang , Piyush Khandelwal , and Peter Stone .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Task planning in robotics : an empirical comparison of pddl- and asp - based systems .", "entities": []}, {"text": "Frontiers of Information Technology & Electronic Engineering , 20(3):363\u2013373 .", "entities": []}, {"text": "Leslie Pack Kaelbling , Michael L Littman , and Anthony R Cassandra .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "Planning and acting in partially observable stochastic domains .", "entities": []}, {"text": "Arti\ufb01cial Intelligence , 101(1):99\u2013134 .", "entities": []}, {"text": "Piyush Khandelwal , Shiqi Zhang , Jivko Sinapov , et al . 2017 .", "entities": []}, {"text": "Bwibots : A platform for bridging the gap between ai and humanrobot interaction research .", "entities": []}, {"text": "The International Journal of Robotics Research , 36(57):635\u2013659 .", "entities": []}, {"text": "Levente Kocsis and Csaba Szepesv \u00b4 ari . 2006 .", "entities": []}, {"text": "Bandit based monte - carlo planning .", "entities": []}, {"text": "In Machine Learning : ECML 2006 , pages 282\u2013293 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Matteo Leonetti , Luca Iocchi , and Peter Stone .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A synthesis of automated planning and reinforcement learning for ef\ufb01cient , robust decision - making .", "entities": []}, {"text": "Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Dongcai Lu , Shiqi Zhang , Peter Stone , and Xiaoping Chen . 2017 .", "entities": []}, {"text": "Leveraging commonsense reasoning and multimodal perception for robot spoken dialog systems .", "entities": []}, {"text": "In 2017 IEEE / RSJ International Conference on Intelligent Robots and Systems ( IROS ) , pages 6582\u20136588 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Drew McDermott , Malik Ghallab , Adele Howe , Craig Knoblock , Ashwin Ram , Manuela Veloso , Daniel Weld , and David Wilkins .", "entities": [[6, 7, "MethodName", "Adele"]]}, {"text": "1998 .", "entities": []}, {"text": "Pddl - the planning domain de\ufb01nition language .", "entities": []}, {"text": "Sanmit Narvekar , Jivko Sinapov , and Peter Stone .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Autonomous task sequencing for customized curriculum design in reinforcement learning .", "entities": []}, {"text": "In Proceedings of IJCAI .", "entities": []}, {"text": "Jean Oh , Arne Supp \u00b4 e , Felix Duvallet , Abdeslam Boularias , Luis Navarro - Serment , Martial Hebert , Anthony Stentz , Jerry Vinokurov , Oscar Romero , Christian Lebiere , et al . 2015 .", "entities": []}, {"text": "Toward mobile robots reasoning like humans .", "entities": []}, {"text": "In AAAI .", "entities": []}, {"text": "Takayuki Osa , Joni Pajarinen , Gerhard Neumann , J Andrew Bagnell , Pieter Abbeel , Jan Peters , et al . 2018 .", "entities": []}, {"text": "An algorithmic perspective on imitation learning .", "entities": [[4, 6, "TaskName", "imitation learning"]]}, {"text": "Foundations and Trends in Robotics , 7(1 - 2):1\u2013179 .", "entities": []}, {"text": "Martin L Puterman .", "entities": []}, {"text": "1994 .", "entities": []}, {"text": "Markov decision processes : discrete stochastic dynamic programming .", "entities": []}, {"text": "John Wiley & Sons .", "entities": []}, {"text": "Morgan Quigley , Ken Conley , Brian Gerkey , Josh Faust , Tully Foote , Jeremy Leibs , Rob Wheeler , and Andrew Y Ng . 2009 .", "entities": []}, {"text": "Ros : an open - source robot operating system .", "entities": []}, {"text": "In ICRA workshop on open source software .", "entities": []}, {"text": "Matthew Richardson and Pedro Domingos .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Markov logic networks .", "entities": []}, {"text": "Machine learning , 62(1):107\u2013136 .", "entities": []}, {"text": "Mohit Shridhar , Jesse Thomason , Daniel Gordon , Yonatan Bisk , Winson Han , Roozbeh Mottaghi , Luke Zettlemoyer , and Dieter Fox . 2020 .", "entities": []}, {"text": "Alfred : A benchmark for interpreting grounded instructions for everyday tasks .", "entities": []}, {"text": "In Computer Vision and Pattern Recognition ( CVPR ) .", "entities": []}, {"text": "Mohan Sridharan , Michael Gelfond , Shiqi Zhang , and Jeremy Wyatt .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Reba :", "entities": []}, {"text": "A re\ufb01nement - based architecture for knowledge representation and reasoning in robotics .", "entities": []}, {"text": "Journal of Arti\ufb01cial Intelligence Research , 65:87\u2013180 .", "entities": []}, {"text": "Mohan Sridharan , Ben Meadows , and Rocio Gomez . 2017 .", "entities": []}, {"text": "What can i not do ? towards an architecture for reasoning about and learning affordances .", "entities": []}, {"text": "In ICAPS .", "entities": []}, {"text": "Richard S Sutton .", "entities": []}, {"text": "1990 .", "entities": []}, {"text": "Integrated architectures for learning , planning , and reacting based on approximating dynamic programming .", "entities": []}, {"text": "In ICML .", "entities": []}, {"text": "Richard S Sutton and Andrew G Barto .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Reinforcement learning : An introduction .", "entities": []}, {"text": "MIT press .", "entities": []}, {"text": "Moritz Tenorth and Michael Beetz .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Knowrob : A knowledge processing infrastructure for cognitionenabled robots .", "entities": []}, {"text": "The International Journal of Robotics Research , 32(5):566\u2013590.Jesse Thomason , Aishwarya Padmakumar , Jivko Sinapov , Nick Walker , Yuqian Jiang , Harel Yedidsion , Justin Hart , Peter Stone , and Raymond Mooney . 2020 .", "entities": []}, {"text": "Jointly improving parsing and perception for natural language commands through human - robot dialog .", "entities": []}, {"text": "Journal of Arti\ufb01cial Intelligence Research , 67:327\u2013374 .", "entities": []}, {"text": "Manuela M Veloso .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The increasingly fascinating opportunity for human - robot - ai interaction : The cobot mobile service robots .", "entities": []}, {"text": "Yi Wang , Shiqi Zhang , and Joohyung Lee . 2019 .", "entities": []}, {"text": "Bridging commonsense reasoning and probabilistic planning via a probabilistic action language .", "entities": []}, {"text": "In the 35th International Conference on Logic Programming ( ICLP ) .", "entities": []}, {"text": "Fangkai Yang , Daoming Lyu , Bo Liu , and Steven Gustafson .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "PEORL : integrating symbolic planning and hierarchical reinforcement learning for robust decision - making .", "entities": [[6, 9, "TaskName", "hierarchical reinforcement learning"]]}, {"text": "In IJCAI .", "entities": []}, {"text": "Steve Young , Milica Gai , Blaise Thomson , and Jason D. Williams .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Pomdp - based statistical spoken dialog systems : A review .", "entities": []}, {"text": "Proceedings of the IEEE , 101(5):1160\u20131179 . Vinicius Zambaldi , David Raposo , Adam Santoro , et al . 2018 .", "entities": [[13, 14, "MethodName", "Adam"]]}, {"text": "Relational deep reinforcement learning .", "entities": []}, {"text": "arXiv preprint arXiv:1806.01830 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shiqi Zhang , Piyush Khandelwal , and Peter Stone . 2017 .", "entities": []}, {"text": "Dynamically constructed ( po ) mdps for adaptive robot planning .", "entities": []}, {"text": "In Proceedings of the ThirtyFirst AAAI Conference on Arti\ufb01cial Intelligence , pages 3855\u20133862 .", "entities": []}, {"text": "Shiqi Zhang and Peter Stone .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "CORPP : Commonsense reasoning and probabilistic planning , as applied to dialog with a mobile robot .", "entities": []}, {"text": "In AAAI .", "entities": []}]