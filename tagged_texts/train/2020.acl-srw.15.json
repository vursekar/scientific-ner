[{"text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics : Student Research Workshop , pages 103\u2013110 July 5 - July 10 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics103     1      Abstract    Neural machine translation ( NMT ) has   achieved impressive performance recently   by using large -scale parallel corpora .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "However , it s truggles in the low -resource   and morphologically -rich scenarios   of   agglutinative langu age translation task .", "entities": []}, {"text": "Inspired by the finding that monolingual   data can grea tly improve   the NMT   performance , we propose a multi -task   neural model   that jointly learns to perform   bi - directional translation and aggl utinative   language stemming .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "Our approach employs   the shared encoder and decoder to train a   single model without changing the standard   NMT architecture   but instead   adding   a   token before each source -side sentence to   specif y the desired target outputs of the two   different tasks .", "entities": []}, {"text": "Experiment al results on   Turkish -English and Uyghur -Chinese    show that our proposed approach can   significantly improve the translation    performance   on agglutinative languages by   using a small amount of monolingual data .", "entities": []}, {"text": "1 Introduction    Neural machine translation ( NMT ) has achieved   impressive performance on many high -resource   machine translation tasks ( Bahdanau et al . , 2015 ;   Luong et al . , 2015a ;", "entities": [[4, 6, "TaskName", "machine translation"], [19, 21, "TaskName", "machine translation"]]}, {"text": "Vaswani et", "entities": []}, {"text": "al . , 2017 ) .", "entities": []}, {"text": "The   standard NMT model uses the encoder to map the   source sentence to a continuous represent ation   vector , and then it feeds the resulting vector to the   decoder to produce the target sentence .", "entities": []}, {"text": "However , the N MT model still suffers from the   low - resource and   morphologically -rich scenarios   of agg lutinative language translation   tasks , such as   Turkish -English and Uyghur -Chinese .", "entities": []}, {"text": "Both   Turkish and Uy ghur are agglutinative language s   with complex morphology .", "entities": []}, {"text": "The morpheme   structure of the word can be denoted as : prefix1", "entities": []}, {"text": "+ \u2026 + prefixN + stem + suffix1 + \u2026 + suffixN   ( Ablimit   et al . , 2010 ) .", "entities": []}, {"text": "Since t he suffixes have   many infle cted and morphological variants , the   vocabulary size of an agglutinative language   is   considerable even in small -scale traini ng data .", "entities": []}, {"text": "Moreover , many word s have different   morpheme s   and meanings   in different context , which leads to   inaccurate translation results .", "entities": []}, {"text": "Recently , researchers show their great interest   in utilizing monolingual data   to further improve   the NMT model performa nce ( Cheng et al . , 2016 ;", "entities": []}, {"text": "Ramachandran et", "entities": []}, {"text": "al . , 2017 ; Currey et al . , 2017 ) .", "entities": []}, {"text": "Sennrich et al .", "entities": []}, {"text": "( 2016 ) pair the target -side   monolingual data with automatic back -translation   as additional training data to train the NMT model .", "entities": []}, {"text": "Zhang and Zong ( 2016 ) use the source -side   monolingual data and employ   the multi -task   learning framework for translation and source   sentence reordering .", "entities": []}, {"text": "Domhan and Hieber   ( 2017 )   modify the decoder to enable multi -task learning   for translation and language modeling .", "entities": []}, {"text": "However ,   the above   works   mainly focus on   boost ing the   translation fluency , and lack the consideration of   morphological and linguistic knowledge .", "entities": []}, {"text": "Stemming is a morphological analysis   method ,   which is widely used for information retrieval tasks   ( Kishida , 2005 ) .", "entities": [[3, 5, "TaskName", "morphological analysis"], [14, 16, "TaskName", "information retrieval"]]}, {"text": "By removing the suffixes   in the   word , stemming allows the variants of the same   word to share representation s and reduce s data   sparseness .", "entities": []}, {"text": "We consider that stemmin g can lead to   better generalization on agglutinative languages ,   which helps NMT to capture the in -depth semantic   infor mation .", "entities": []}, {"text": "Thus we use   stemming as an auxiliary   task for agglutinative language translation .", "entities": []}, {"text": "In this paper , we investigate a method to exploit   the monolingual data of the agglutinative language   to enhance the representation ability of the encoder .", "entities": []}, {"text": "This is ach ieved by   training a multi -task neural   model to jointly perform bi - directional translation   and agglutinative language stemming , which   utilizes the shared encoder and decoder .", "entities": []}, {"text": "We treat   stemming a s a sequence generation task .", "entities": []}, {"text": "Multi -Task Neural Model for Agglutinative Language Translation      Yirong Pan1,2,3 , Xiao Li1,2,3 , Yating Yang1,2,3 , and Rui Dong1,2,3   1 Xinjiang Technical Institute of Physics & Chemistry , Chinese Academy of Sciences , China", "entities": [[7, 8, "TaskName", "Translation"]]}, {"text": "2 University of Chinese Academy o f Sciences , China    3 Xinjiang Laboratory of Minority Speech and Language Information Processing , China    panyirong15@mails.ucas.ac.cn    { xiaoli , yangyt , dongrui}@ms.xjb.ac.cn", "entities": []}, {"text": "104     2       Encoder - Decoder   FrameworkBilingual Data Monolingual Data Training data < MT > + English sentence < MT > + Turkish sentence < ST > + Turkish sentenceTurkish sentence English sentence stem sequence   Figure 1 : The architecture of th e multi -task neural model   that jointly learns to perform bi - directional translation   between Turkish and English , and stemming   for Turkish   sentence .", "entities": []}, {"text": "2 Related Work    Multi - task learning ( MTL ) aims to improve the   generalization   perfor mance of a main task by using   the other related tasks , which has been successfully   applied to various research fields   ranging from   language ( Liu et al . , 2015 ;", "entities": [[4, 8, "TaskName", "Multi - task learning"]]}, {"text": "Luong et al . , 2015 a ) ,   vision ( Yim et al . , 2015 ; Misra et al . , 2016 ) , and   speech ( Chen   and Mak , 20 15 ; Kim et al . , 2016 ) .", "entities": []}, {"text": "Many natural language processing ( NLP ) tasks   have been   chosen as auxiliary task   to deal with   the   increasingly   complex tasks .", "entities": []}, {"text": "Luong et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2015b )   employ   a small amount   of data of syntactic parsing   and i mage caption for English -German   translation .", "entities": []}, {"text": "Hashimoto et al .", "entities": []}, {"text": "( 2017 ) present   a joint MTL model   to handle the tasks   of part -of - speech ( POS ) tagging ,    dependency parsing , semantic relatedness , and   textual entailment   for English .", "entities": [[26, 28, "TaskName", "dependency parsing"]]}, {"text": "Kiperwasser   and   Ballesteros ( 2018 ) utilize   the POS tagging and   dependency parsin g for English -German machine   translation .", "entities": []}, {"text": "To the best of our   knowledge , we are   the first to incorporate   stemming task into MTL   framework   to further improve the translation   performance on   agglutinative language s.", "entities": []}, {"text": "Recently , several works h ave combined   the   MTL   method with sequence -to - sequence NMT   model for", "entities": []}, {"text": "machine translation task s. Dong et", "entities": [[0, 2, "TaskName", "machine translation"]]}, {"text": "al .    ( 2015 ) follow   a one - to - many   setting that utilize s a   shared encoder for al l the source   language s with   respective attention mechanisms and multiple   decoders for the different target   language s. Luong   et al .   ( 2015 b ) follow a many -to - many   setting that   uses multiple   encoders and decoders with two   separate unsupervised objective functions .", "entities": []}, {"text": "Zoph   and Knight ( 2016 ) follow   a many -to - one setting   that employs   multi ple encoders for all the source   languages and one decoder for the desired target   language .", "entities": []}, {"text": "Johnson et", "entities": []}, {"text": "al .   ( 2017 ) propose   a more   simple method in one - to - one setting , which trains   a single NMT model with the shared encoder and   decoder in order to enable multilingual translation .", "entities": []}, {"text": "The method requires no changes to the standard   NMT architecture but instead requires adding   a   token at the beginning of eac h source sentence to   specify the desired target sentence .", "entities": []}, {"text": "Inspired by   their work , we employ the standard NMT model   with one encoder and one decoder for parameter   sharing and model generalization .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "In addition , we   build   a joint vocabulary on the concatenation of the   source -side and target -side words .", "entities": []}, {"text": "", "entities": []}, {"text": "Several works   on morphologically -rich NMT   have focused on using morphological analysis   to   pre - process the training data ( Luong et al . , 2016 ;   Huck et al . , 2017 ;", "entities": [[12, 14, "TaskName", "morphological analysis"]]}, {"text": "Tawfik   et al . , 2019 ) .", "entities": []}, {"text": "Gulcehre   et   al .", "entities": []}, {"text": "( 2015 ) segment each Turkish sentence   into a   sequence of morpheme   units   and remove   any non surface morphemes   for Turkish -Engli sh translation .", "entities": []}, {"text": "Ataman   et al .", "entities": []}, {"text": "( 2017 ) propose a vocabulary   reduction method   that considers   the morphological   properties of the agglutinative language , which is   based   on the unsupervised morphology learning .", "entities": []}, {"text": "This work takes inspiration from   our previously   proposed segmentation method ( Pan et al . , 2020 )    that segments the   word into a   sequence of   subword unit s with morpheme structure , which can   effectively reduce language complexity .", "entities": []}, {"text": "3 Multi -Task Neural Model    3.1 Overview    We propose a multi -task neural model for mach ine   translation from and into a l ow - resource and   morphologically -rich agglutinative language .", "entities": []}, {"text": "We   train the model to jointly learn to perform both the   bi - directional translation task and the stemming   task on an agglutinative language by using the   standard N MT framework .", "entities": []}, {"text": "Moreover , we add   an   artificial token before each source   sentence   to   specify the desired target   outputs for different tasks .", "entities": []}, {"text": "The architecture   of the propose d model is shown in   Figure 1 .", "entities": []}, {"text": "We take the Turkish -English translation   task as example .", "entities": []}, {"text": "The \u201c < MT > \u201d token denotes the   bilingual translation task and the \u201c < ST > \u201d token   denotes the stemmi", "entities": []}, {"text": "ng task on   Turkish sentence .", "entities": []}, {"text": "3.2 Neural Machine Translation   ( NMT )", "entities": [[2, 4, "TaskName", "Machine Translation"]]}, {"text": "Our proposed multi -task neural model on using the   source -side monolingual data for agglutin ative   language translation task can be applied in any   NMT structures   with encoder -decoder   framework .", "entities": []}, {"text": "In this work , we follow the NMT model proposed   by Vaswani et", "entities": []}, {"text": "al . ( 2017 ) , which is implemented as   Transformer .", "entities": [[11, 12, "MethodName", "Transformer"]]}, {"text": "We will briefly summarize it here .", "entities": []}, {"text": "105     3      Task   Data   # Sent # Src # Trg   Tr - En train 355,251   6,356,767 8,021,161   valid 2,455   37,153 52,125   test 4,962 69,006 96,291   UyCh train 333,097   6,026,953 5,748,298   valid   700 17,821 17,085   test 1,000 20,580 18,179    Table 1 :   The statistics of the training , validation , and   test datasets on Turkish -English and Uyghur -Chinese   machine translation tasks .", "entities": [[76, 78, "TaskName", "machine translation"]]}, {"text": "The \u201c # Src \u201d denotes the   number of the source tokens , and the \u201c # Trg \u201d denotes   the number s of the   target tokens .", "entities": []}, {"text": "bir dilin son hecelerini kendisiyle birlikte mezara Morpheme Segmentation hece+ler+i+ni hece+lerini he@@+ce@@+leriniApply BPE on StemStem+Combined Suffix    Figure 2 : The example of morpholo gical segmentation   method for the word in Turkish .", "entities": [[12, 13, "MethodName", "BPE"]]}, {"text": "Firstly , the Transformer model maps the source   sequence \ud835\udc99=(\ud835\udc651, \u2026 ,\ud835\udc65\ud835\udc5a ) and the target sentence   \ud835\udc9a=(\ud835\udc661, \u2026 ,\ud835\udc66\ud835\udc5b )   into a word embedding matrix ,   respectively .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "Secondly , in order to m ake use of the   word order in   the sequence , the above word   embedding   matrices   sum with   their positional   encoding   matrices   to generate the source -side and   target -side positional embedding matrices .", "entities": []}, {"text": "The   encoder is composed of a stack of N identical   layers .", "entities": []}, {"text": "Each layer has two sub -layers   consisting   of   the multi -head self -attention   and the fully   connected feed - forward network , whi ch map s the   source -side positional embedding mat rix into   a   representation   vector .", "entities": []}, {"text": "The decoder   is also composed of a stack of N   identical layers .", "entities": []}, {"text": "Each layer has three sub -layers :   the multi -head self -attention , the multi -head   attention , and the fully conne cted feed - forward   network .", "entities": []}, {"text": "The multi -head attention attends to the   outputs of the encode r and decoder to generate a   context vector .", "entities": []}, {"text": "The feed - forward network followed   by a linear layer maps the context vector into a   vector with the original space dimension .", "entities": [[9, 11, "MethodName", "linear layer"]]}, {"text": "F inally ,   the softmax   function   is applied on the vector to   predict   the target word sequence .", "entities": [[5, 6, "MethodName", "softmax"]]}, {"text": "", "entities": []}, {"text": "1 https://wit3.fbk.eu/archive/2018 -01 / additional_TED_xml/    2 http://data.statmt.org/wmt18/translation -task/    3 http://uy.ts.cn/   4 Experiment    4.1 Dataset    The statistics of the training , validation , and test   datasets on Turkish -English and Uyghur -Chinese   machine translation tasks   are shown in Table 1 .", "entities": [[38, 40, "TaskName", "machine translation"]]}, {"text": "For the Turkish -English   machine translation ,   following ( Sennrich et al . , 2015 a ) , we use the WIT   corpus   ( Cettolo et al . , 2012 ) and the SETimes   corpus ( Tyers and Alperen , 2010 ) as the training   dataset , merge the dev2010 and tst2010 as the   validation data set , and use tst2011 , tst2012 , tst2013 ,   tst2014   from the IWSLT as the test datasets .", "entities": [[5, 7, "TaskName", "machine translation"], [23, 24, "DatasetName", "WIT"]]}, {"text": "We   also use the talks data from the IWSLT   evaluation   campaign1 in 2018 and the news data from News   Crawl corpora2   in 2017 as external monolingual   data for the stemming   task on Turkish sentences .", "entities": []}, {"text": "For the Uyghur -Chinese   machine translation , we   use the news data from the China Worksho p on   Machine Translation in 2017 ( CWMT2017 ) as the   training dataset and validation dataset , use the   news data from CWMT 2015 as the test dataset .", "entities": [[5, 7, "TaskName", "machine translation"], [21, 23, "TaskName", "Machine Translation"], [34, 36, "DatasetName", "validation dataset"]]}, {"text": "Each Uyghur sentence has four Chinese reference   sentences .", "entities": []}, {"text": "Moreover , we use the news data from   the Tianshan website3 as external monolingu al data   for the stemming   task on Uyg hur sentence s.   4.2 Data Preprocessing    We normalize and tokenize the experimental data .", "entities": []}, {"text": "We utilize   the jieba   toolkit4 to segmen t the Chinese   sentences , we utilize the Zemberek   toolkit5   with   morphological disambiguation ( Sak et al . , 2007 )    and the morphological analysis to ol ( Tursun et al . ,   2016 ) to annotate the morpheme structure of the   words i n Turkish and Uyghur , respectively .", "entities": [[24, 26, "TaskName", "morphological disambiguation"], [37, 39, "TaskName", "morphological analysis"]]}, {"text": "We use our previously proposed morphological   segmentation method ( Pan et al . , 2020 ) , which   segment s the word into smaller   subword units with   morpheme structure .", "entities": []}, {"text": "Since Turkish and Uyghur   only have a few prefixes , we combi ne the prefixes   with stem into the stem unit .", "entities": []}, {"text": "As shown in Figure 2 ,   the morpheme structure   of the Turkish word   \u201c hecelerini \u201d ( syllables ) is : hece + lerini .", "entities": []}, {"text": "Then t he   byte pair encoding ( BPE ) technique ( Sennrich et   al . , 2015b ) is applied on the stem unit \u201c hece \u201d to   segment it into \u201c he@@ \u201d and \u201c ce@@ \u201d .", "entities": [[4, 7, "MethodName", "byte pair encoding"], [8, 9, "MethodName", "BPE"]]}, {"text": "Thus the   Turkish word is segmented into a   sequence of sub word units : he@@   + ce@@   + lerini .", "entities": []}, {"text": "4 https://github.com/fxsjy/jieba    5 https://github.com/ahmetaa/zemberek -nlp", "entities": []}, {"text": "106     4      Task   Training Sentence Samples    En - Tr   Translation   < MT > We go through initiation   rit@@ es .", "entities": [[14, 15, "TaskName", "Translation"]]}, {"text": "Ba\u015fla@@ ma rit\u00fcel@@ lerini   ya\u015f@@ \u0131yoruz .", "entities": []}, {"text": "Tr - En   Translation   < MT >   Ba\u015fla@@ ma rit\u00fcel@@   lerini ya\u015f@@ \u0131yoruz .", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "We go through initiation rit@@ es .", "entities": []}, {"text": "Turkish   Stemming   < ST >   Ba\u015fla@@ ma rit\u00fcel@@ lerini   ya\u015f@@ \u0131yoruz .", "entities": []}, {"text": "Ba\u015fla@@ rit\u00fcel@@ ya\u015f@@    Table 2 :   The training sentence samples for multi -task   neural model on Turkish -English machine translation   task .", "entities": [[21, 23, "TaskName", "machine translation"]]}, {"text": "We add \u201c < MT > \u201d and \u201c < ST > \u201d before each source   sentence   to specify the desired target outputs for   different tasks .", "entities": []}, {"text": "Lang   Method   # Merge Vocab   Avg . Len", "entities": []}, {"text": "Tr Morph   15 K 36,468   28   Tr BPE 36 K 36,040   22", "entities": [[10, 11, "MethodName", "BPE"]]}, {"text": "En BPE 32 K 31,306   25   Uy Morph   10 K", "entities": [[1, 2, "MethodName", "BPE"]]}, {"text": "38,164   28   Uy BPE 38 K 38,292   21   Ch BPE 32 K 40,835   19   Table 3 :   The detailed statistics of using different word   segmentation methods on Turkis h , English , Uyghur ,   and Chinese .", "entities": [[5, 6, "MethodName", "BPE"], [13, 14, "MethodName", "BPE"]]}, {"text": "In this paper , we utilize the above morphological   segmentation   method for our experiments   by   applying BPE on the stem units with 15 K merge   operations for the Turkish words and 10 K merge   operations for the Uyghur words .", "entities": [[20, 21, "MethodName", "BPE"]]}, {"text": "The standard   NMT model trained on this experimental dat a is   denoted as \u201c baseline NMT model \u201d .", "entities": []}, {"text": "Moreover , we   employ BPE to segment the words in English and   Chinese by learning separate vocabulary wit h 32 K   merge operations .", "entities": [[5, 6, "MethodName", "BPE"]]}, {"text": "Table 2 shows the training   sentence samples for multi -task neural model on   Turkish -English machine translation task .", "entities": [[17, 19, "TaskName", "machine translation"]]}, {"text": "In addition , to certify the effectiveness of the   morphological segmentation me thod , we   employ   the pure BPE to segment the words in Turkish and   Uyghur by learning a separate vocabulary with   36 K and 38 K merge operations , respectivel y.", "entities": [[21, 22, "MethodName", "BPE"]]}, {"text": "The   standard NMT model   trained on this experimental   data is denoted as \u201c general   NMT model \u201d .", "entities": []}, {"text": "Table 3   shows t he detailed st atistics of using different word   segmentation methods on Turkish , English ,   Uyghur , and Chinese .", "entities": []}, {"text": "The \u201c Vocab \u201d token denotes   the vocabu lary size after data preprocessing .", "entities": []}, {"text": "The   \u201c Avg . Len \u201d token deno tes the average sentence   length .", "entities": []}, {"text": "4.3 Training and Evaluation Details    We employ the Transformer model implemented in   the Sockeye   toolkit ( Hieber et al . , 2017 ) .", "entities": [[9, 10, "MethodName", "Transformer"]]}, {"text": "The   number of layer in both the encoder a nd decoder is   set to   N=6 , the number of head is set to 8 , and the   number of hidden unit in the feed - forward network   is set to 1024 .", "entities": []}, {"text": "We use an   embedding size of both   the source and target words of 512 dimension , and   use a batch size of 128 sentences .", "entities": [[22, 24, "HyperparameterName", "batch size"]]}, {"text": "The maximum   sentence length is set to 100 tokens with 0.1 label   smoothing .", "entities": []}, {"text": "We apply layer normalization and add   dropout to the embedding and transformer layers   with 0.1 probability .", "entities": [[2, 4, "MethodName", "layer normalization"]]}, {"text": "Moreover , we   use the Adam   optimizer ( Kingma and Ba , 2015 ) with an initial   learni ng rate of 0.0002 , and save the checkpoint   every 1500   updates .", "entities": [[6, 7, "MethodName", "Adam"], [8, 9, "HyperparameterName", "optimizer"]]}, {"text": "Model t raining process stops after 8 checkpoints   without improvement s on the validation perplexity .", "entities": [[15, 16, "MetricName", "perplexity"]]}, {"text": "Following Niu et al .", "entities": []}, {"text": "( 2018a ) , we select the 4 best   checkpoint   based on the validation perplex ity   values   and combine them in a linear ensemble for   decoding .", "entities": []}, {"text": "Decoding is performed by using beam   search with a beam size of 5 .", "entities": []}, {"text": "We evaluate the   machine translation performance by using the   case - sensitive BLEU score ( Papineni et al . , 2002 )   with standard to kenization .", "entities": [[4, 6, "TaskName", "machine translation"], [14, 16, "MetricName", "BLEU score"]]}, {"text": "4.4 Neural Translation Models", "entities": [[2, 3, "TaskName", "Translation"]]}, {"text": "In this paper , we select 4 neural translation models   for comparison .", "entities": []}, {"text": "More details about the models are   shown below :    General   NMT Model : The standard NMT model   trained on the experimental data segmented by   BPE .", "entities": [[11, 12, "DatasetName", "General"], [29, 30, "MethodName", "BPE"]]}, {"text": "Baseline NMT Model : The standard NMT model   trained on the experimental   data segmented by    morphological   segmentation .", "entities": []}, {"text": "The f ollowing   models   also use this   word segmentation method .", "entities": []}, {"text": "Bi - Directional NMT Model :", "entities": []}, {"text": "Following Niu et al .   ( 2018 b ) , we train a single NMT model to perform    bi - directional machine translation .", "entities": [[23, 25, "TaskName", "machine translation"]]}, {"text": "We concatenate   the bilingual parallel sentences in   both directions .", "entities": []}, {"text": "Since the source   and target   sentences come from   the same language pairs , we share the source and   target vocabular y , and tie their word embedding    during model training .", "entities": []}, {"text": "Multi -Task Neural Model :", "entities": []}, {"text": "We simply use the    monolingual data of the agglutinative language   from the bilingual parallel sentences .", "entities": []}, {"text": "We use a joint   vocabulary , tie the word embedding   as well as t he   output layer \u2019s weight matrix .", "entities": []}, {"text": "107     5      Task   Mod el tst11 tst12   tst13   tst14    TrEn general   25.92   26.55   27.34   26.35    baseline", "entities": []}, {"text": "26.48   27.02   27.91   26.33    EnTr general   13.73   14.68   13.84   14.65    baseline   14.85   15.93   15.45   15.93    Table 4 :   The BLEU scores of the general NMT model   and baseline NMT model on the machine translation   task between Turkish and English .", "entities": [[34, 35, "MetricName", "BLEU"], [48, 50, "TaskName", "machine translation"]]}, {"text": "Task   Model   tst11 tst12 tst13 tst14   TrEn baseline   26.48   27.02   27.91   26.33    bidirectional   26.21   27.17   28.68   26.90    multi -task 26.82   27.96   29.16", "entities": []}, {"text": "27.98    EnTr baseline   14.85   15.93   15.45   15.93    bidirectional   15.08   16.20   16.25   16.56    multi -task 15.65   17.10   16.35   16.41    Table 5 :   The BLEU scores of the baseline NMT model ,   bi - directional NMT model , and multi -task neural   model on the machine translation task   between Turkish   and English .", "entities": [[38, 39, "MetricName", "BLEU"], [61, 63, "TaskName", "machine translation"]]}, {"text": "5 Results and Discu ssion    Table 4 shows the BLEU scores of the general    NMT model and baseline NMT model on machine   translation task .", "entities": [[10, 11, "MetricName", "BLEU"]]}, {"text": "We can observe   that the baseline   NMT model is comparable to the general NMT   model , and it achieves the highest BLEU scores on   almost all the test datasets in both directions , which   indicates that the NMT baseline based on our   proposed segmentation method   is competitive .", "entities": [[24, 25, "MetricName", "BLEU"]]}, {"text": "5.1 Using Original Monolingual Data    Table 5 shows the BLEU scores of the baseline   NMT model , bi -directional NMT model , and   multi -task neural model on the machine translation   task between Turkish and English .", "entities": [[10, 11, "MetricName", "BLEU"], [32, 34, "TaskName", "machine translation"]]}, {"text": "The table shows   that the multi -task neural model outperforms both   the baseline NMT model and bi - directional NMT   model , and it achieves the highest BLEU scores on   almost all the test datasets in both   directions , which   suggests   that the multi -task neural model is capable   of improving the bi -directional translation   quality   on agglutinative language s.", "entities": [[30, 31, "MetricName", "BLEU"]]}, {"text": "The main reason is that   compared with the bi -directional NMT model , our   proposed multi -task neural model   additionally   employs the stemming task for the agglutinative   langu age , which is effective for the NMT model to   learn both the source -side semantic information   and the target -side language modeling .", "entities": []}, {"text": "", "entities": []}, {"text": "Figure 3 : The function of epoch s ( x - axis ) and perplexity   ( y - axis ) values on the validation   dataset in different   neural translation models for the translation task .", "entities": [[14, 15, "MetricName", "perplexity"]]}, {"text": "Translation   Examples    source   \u00fcniversite hayat\u0131 taklit   ediyordu .", "entities": [[0, 1, "TaskName", "Translation"]]}, {"text": "reference   College was imitating   life .", "entities": []}, {"text": "baseline   It was emulating   a university life .", "entities": []}, {"text": "bidirectional   The university was emulating   its   lives .", "entities": []}, {"text": "multi -task", "entities": []}, {"text": "The university was imitating   life .", "entities": []}, {"text": "Table 6 :   A translation example   for the different   NMT    models   on Turkish -English .", "entities": []}, {"text": "The function of epoch s and perplexity values on   the validation dataset in different neural translation   models   are shown in Figure 3 .", "entities": [[6, 7, "MetricName", "perplexity"], [11, 13, "DatasetName", "validation dataset"]]}, {"text": "We can see that the   perplexity value s are consistently lower on the   multi -task neural model , and it converges   rapidly .", "entities": [[6, 7, "MetricName", "perplexity"]]}, {"text": "Table 6 shows a translation example for the   different models on Turkish -English .", "entities": []}, {"text": "We can see   that the translation result of the multi -task neural   model is more accurate .", "entities": []}, {"text": "The Turkish word \u201c taklit \u201d   means \u201c imitate \u201d in English , both the baseline NMT   and bi -directional NMT translate it in to a synonym    \u201c emulate \u201d .", "entities": []}, {"text": "However ,   they are not able to express   the meaning   of the   sentence   correctly .", "entities": []}, {"text": "The main   reason is that   the auxiliary task of stemming forces   the proposed model to focus more strongly on the   core m eaning of each word ( or stem ) , therefore   helping the model m ake the correct lexical choices   and capture the in -depth semantic information .", "entities": []}, {"text": "5.2 Using External Monolingual Data    Moreover , we evaluate the multi -task neural model   on using external   monolingual   data for Turkish   stemming task .", "entities": []}, {"text": "We employ the parallel sen tences   and the monolingual data in a 1 - 1 ratio , and shuffle   them randomly before each training epoch .", "entities": []}, {"text": "More   details about the data are shown below :", "entities": []}, {"text": "108     6      Task   Data   tst11 tst12 tst13 tst14   Tr - En original   26.82", "entities": []}, {"text": "27.96   29.16   27.98    talks 26.55   27.94   29.13   28.02   news   26.47   28.18   28.89   27.40    mixed   26.60   27.93   29.58   27.32    En - Tr original   15.65   17.10   16.35   16.41    talks 15.57   16.97   16.22   16.91    news   15.67   17.19   16.26   16.69    mixed   15.96   17.35   16.55   16.89    Table 7 :   The BLEU scores of the multi -task neural   model on using e xternal monolingual data of talks data ,   news data , and mixed data .", "entities": [[82, 83, "MetricName", "BLEU"]]}, {"text": "Task   Model   BLEU    Uy - Ch general NMT model   35.12    baseline NMT model   35.46    multi -task neural model with   external monolingual data   36.47    Ch - Uy general NMT model   21.00    baseline NMT model   21.57    multi -task neural model with   external monolingual data   23.02    Table 8 :   The BLEU scores of the general NMT model ,   baseline NMT model , and the multi -task neural model   with external monolingual data on Uyghur -Chinese   and Chinese -Uyghur   machine translation tasks .", "entities": [[4, 5, "MetricName", "BLEU"], [65, 66, "MetricName", "BLEU"], [97, 99, "TaskName", "machine translation"]]}, {"text": "Original Data : The monolingual data co mes from   the original bilingual parallel sentences .", "entities": []}, {"text": "Talks Data : The monolingual data contains talks .", "entities": []}, {"text": "News Data :", "entities": []}, {"text": "The monolingual data contains new s.   Talks and News Mixed Data : The monolingual    data contains talks and news in a 3:4 ratio as the   same with the original bilingual parallel sentences .", "entities": []}, {"text": "Table 7 shows the BLEU scores of the proposed   multi -task neural model on using different external   monolingual data .", "entities": [[4, 5, "MetricName", "BLEU"]]}, {"text": "We can see that there is no   obvious   difference on Turkish -English translation   performance by using different monoli ngual data ,   whether the data is in - domain or out -of - domain to   the test dataset .", "entities": []}, {"text": "However , for the English -Turkish    machine translation task , which can be seen as   agglutinative la nguage generation task , using the   mixed   data of talks and news achieve s further    improvements of the BLEU scores on almost all   the test datasets .", "entities": [[7, 9, "TaskName", "machine translation"], [40, 41, "MetricName", "BLEU"]]}, {"text": "The main reason is that the   proposed multi -task neural model incorporates   many morphological and linguistic inf ormation of   Turkish rather than that of English , which mainly   pays attention to   the source -side representation   ability on agglutinative language s rather than the   target -side language modeling .", "entities": []}, {"text": "We also evaluate the translation performance of    the general NMT   model , baseline NMT model , and   multi -task neural model with external   news data on   the machine translation task between Uyghur and   Chinese .", "entities": [[32, 34, "TaskName", "machine translation"]]}, {"text": "The experimental results are shown in   Table 8 .", "entities": []}, {"text": "The results indicate that the multi -task   neural model   achieve s the hi ghest BLEU scores   on   the test dataset by utilizing external monolingual   data for the stemming task on Uyghur sentences .", "entities": [[16, 17, "MetricName", "BLEU"]]}, {"text": "6 Conclusion s   In this paper , we propose a multi -task neural model   for translation task from and into a low -resource   and morphologica lly - rich agglutinative language .", "entities": []}, {"text": "The model jointly learns to perform bi -directional   translation and agglutinative language stemming    by utilizing the shared encoder and decoder under    standard NMT framework .", "entities": []}, {"text": "Extensive experimental   results show that the proposed model is beneficial    for the agglutinative language machine translation ,   and only a small amount of the agg lutinative data   can improve the translation performance in both   directions .", "entities": [[16, 18, "TaskName", "machine translation"]]}, {"text": "Moreover , the proposed   approach with   external monolingual data is more usefu l for    translating into the agglutinative   language , which   achieves an improvement of +1.42   BLEU points   for translation from English into Turkish and +1.45    BLEU points from Chinese into Uyghur .", "entities": [[32, 33, "MetricName", "BLEU"], [44, 45, "MetricName", "BLEU"]]}, {"text": "In future   work , we pla n to utilize other word   segmentation method s for model training .", "entities": []}, {"text": "We also   plan to combine t he proposed multi -task neural   model with back - translat ion method to enhance the   ability of the NMT model on   target -side language   modeling .", "entities": []}, {"text": "Acknowledge ments    We are very gratefu l to the mentor of this paper for   her meaningful feedback .", "entities": []}, {"text": "Thanks three anon ymous   reviewers for their insightful comments and    practical   suggestions .", "entities": []}, {"text": "This work is supported by   the High -Level Talents Introduction Project of   Xinjiang   under Grant No . Y839031201 , the   National Natural Science Foundation of China    under Grant No .", "entities": []}, {"text": "U1703133 , the National Natural   Science Foundation of Xinjiang under Grant   No.2019BL -0006 , the Open Project of Xinjiang   Key Laboratory   under Grant No.2018D04018 , and   the Youth Innovation Prom otion Ass ociation of the   Chinese Academy of Sciences   under Grant   No.2017472 .", "entities": []}, {"text": "109     7      References    Mijit   Ablimit , Graham Neubig , Masato Mimura ,   Shinsuke Mori , Tatsuya Kawahara , and Askar   Hamdulla .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Uyghur morpheme -based   language models and ASR .", "entities": []}, {"text": "In IEEE Internati onal   Conference on Signal Processing .", "entities": []}, {"text": "Duygu Ataman , Matteo Negri , Marco Turchi , and   Marcello Federico .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Linguistically motivated   vocabulary reduction for neural machine translation   from Turkish to English .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "Journal of the Prague   Bulletin of Mathematica l Linguistics , 108 : 331 - 342 .", "entities": []}, {"text": "Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua   Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Neural machine translation by jointly   learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "arXiv preprint   arXiv:1409.0473 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mauro Cettolo , Christian Girardi and Marcello   Federico . 2012 .", "entities": []}, {"text": "WIT3 :   Web inventory of   transcribed and translated talks .", "entities": []}, {"text": "In Proceedings of   the 16th Conference of the European Association   for Machine Translation .", "entities": [[13, 15, "TaskName", "Machine Translation"]]}, {"text": "Dongpeng Chen and Brian Kan -Wing Mak . 2015 .", "entities": []}, {"text": "Multitask learning of deep neural networks for low resource speech re cognition .", "entities": []}, {"text": "In IEEE / ACM   Transactions on Audio , Speech , and Language   Processing .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Yong Cheng , Wei Xu , Zhongjun He , Wei He , Hua Wu ,   Maosong Sun , and Yang Liu .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semi Supervised learning for neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meet ing of ACL .", "entities": []}, {"text": "Kyunghyun Cho , Bart Van Merrienboer , Caglar   Gulcehre , Dzmitry Bahdanau , Fethi Bougares ,   Holger Schwenk , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Learning phrase representations using RNN   encoder -decoder for statistical machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv : 1406.1078 .", "entities": [[0, 1, "DatasetName", "arXiv"], [2, 3, "DatasetName", "arXiv"]]}, {"text": "Anna Currey , Antonio Valerio Miceli Barone , and   Kenneth Heafield . 2017 .", "entities": []}, {"text": "Copied monolingual data   improves low -resource neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Second Conference on    Machine Translation .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Tobias Domhan and Felix Hieber .", "entities": []}, {"text": "20 17 .", "entities": []}, {"text": "Using target side monolingual data for neural machine   translation through multi -task learning .", "entities": []}, {"text": "In   Proceedings of EMNLP .", "entities": []}, {"text": "Daxiang Dong , Hua Wu , Wei He , Dianhai Yu , and   Haifeng Wang .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Multi -task learning for   multiple language translation .", "entities": []}, {"text": "In Proceedi ngs of   ACL .", "entities": []}, {"text": "Caglar Gulcehre , Orhan Firat , Kelvin Xu , Kyunghyun   Cho , et al . 2015 .", "entities": []}, {"text": "On using monolingual corpora   in   neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "arXiv preprint   arXiv:1503.03535v2 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kazuma Hashimoto , Caiming Xiong , Yoshimasa   Tsuruoka , and Richard Socher . 2017 .", "entities": []}, {"text": "A   joint many task model : Growing a   neural network for multiple   NLP tasks .", "entities": []}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Felix Hieber , Tobias Domhan , Michael Denkowski ,   David Vilar , Artem Sokolov , Ann Clifton , and Matt   Post . 2017 .", "entities": []}, {"text": "Sockeye : A toolkit for neural machine   translatio n. arXiv preprint arXiv:1712.05690 .", "entities": [[10, 11, "DatasetName", "arXiv"]]}, {"text": "Matthias Huck , Simon Riess , and Alexander Fraser .   2017 .", "entities": []}, {"text": "Target -side word segmentation strategies for   neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the   Second Conference on Machine Translation .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "S\u00e9bastien Jean , Kyunghyun C ho , Roland Memisevic ,   and Yoshua Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "On using very large   target vocabulary for neural machine translation .", "entities": [[9, 11, "TaskName", "machine translation"]]}, {"text": "In   Proceedings of ACL .", "entities": []}, {"text": "Melvin Johnson , Mike Schuster , Quoc V .", "entities": []}, {"text": "Le , Maxim   Krikun , Yonghui Wu , Zhifeng C hen , Nikhil Thorat ,   Fernanda Vi\u00e9gas , M artin Wattenberg , Greg Corrado ,   Macduff Hughes , and Jeffrey Dean . 2017 .", "entities": []}, {"text": "Google \u2019s   multilingual neural machine translation system :   Enabling zero -shot translation .", "entities": [[0, 1, "DatasetName", "Google"], [5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of   ACL .", "entities": []}, {"text": "Suyoun Kim , Takaaki Hori , and Shinji Watanabe . 2016 .", "entities": []}, {"text": "Joint CTC -Attention   based end -to - end speech   recognition using multi -task learning .", "entities": [[1, 2, "DatasetName", "CTC"]]}, {"text": "arXiv   preprint arXiv : 1609.06773 .", "entities": [[0, 1, "DatasetName", "arXiv"], [3, 4, "DatasetName", "arXiv"]]}, {"text": "Diederik Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A   method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [6, 8, "TaskName", "stochastic optimization"]]}, {"text": "In Proceedings   of ICLR .", "entities": []}, {"text": "Eliyahu Kiperwasser , Miguel Ballesteros . 2018 .", "entities": []}, {"text": "Schedule d multi - task learning :", "entities": [[2, 6, "TaskName", "multi - task learning"]]}, {"text": "From syntax to   translation .", "entities": []}, {"text": "In Transactions of the Association for   Computational Linguistics .", "entities": []}, {"text": "Kazuaki Kishida .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Technical issues of cross language information retrieval : A review .", "entities": [[5, 7, "TaskName", "information retrieval"]]}, {"text": "Journal   of   the Information Processing and Manage ment ,   41(3):433 - 455 .   https://doi.org/10.1016/j.ipm.2004.06.007 .", "entities": []}, {"text": "Xiaodong Liu , Jianfeng Gao , Xiaodong He , Li Deng ,   Kevin Duh , and Ye -Yi Wang .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Representation   learning using multi -task deep neural networks for   semantic classification and information ret rieval .", "entities": []}, {"text": "In   Proceedings of NAACL .", "entities": []}, {"text": "Minh -Thang Luong , Hieu Pham , and Christopher D   Manning .", "entities": []}, {"text": "2015a .", "entities": []}, {"text": "Effective approaches to Attention based neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of   EMNLP .", "entities": []}, {"text": "Minh -Thang Luong , Quoc V Le , Ilya Sutskever , Oriol   Vinyals , and Luk asz Kaiser .", "entities": []}, {"text": "2015 b. Multi -task   sequence to sequence learning .", "entities": [[5, 8, "MethodName", "sequence to sequence"]]}, {"text": "arXiv preprint   arXiv:1511.06114 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "110     8      Minh -Thang Luong , Christopher D. Manning .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Achieving open vocabulary neural machine   translation with hybrid word -character models .", "entities": []}, {"text": "In   Proceedings of ACL .", "entities": []}, {"text": "Ishan Misra , Abhinav Shrivastava , Abhinav Gupta , and   Martial Hebert .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Cross -Stitch networks for   multi -task learning .", "entities": []}, {"text": "In Proceedings of CVPR .", "entities": []}, {"text": "Xing Niu , Sudha Rao , and Marine Carpuat .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Multi -task neural models for translating between   styles within an d across languages .", "entities": []}, {"text": "In Proceedings   of COLING .", "entities": []}, {"text": "Xing Niu , Michael Denkowski , and Marine Carpuat .", "entities": []}, {"text": "2018 b. Bi - Directional neural machine translation   with synthetic parallel data .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv :   1805.11213 .", "entities": [[0, 1, "DatasetName", "arXiv"], [2, 3, "DatasetName", "arXiv"]]}, {"text": "Prajit Ramachandran , Peter Liu , and Quoc Le .", "entities": []}, {"text": "201 7 .", "entities": []}, {"text": "Unsupervised pretraining for sequence to sequence    learning .", "entities": [[3, 6, "MethodName", "sequence to sequence"]]}, {"text": "In Proceedings of the 2017 Conference on   EMNLP .", "entities": []}, {"text": "Yirong Pan , Xiao Li , Yating Yang , and Rui Dong .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Morphological word segmentation on agglutinative   languages for neural machine translation .", "entities": [[9, 11, "TaskName", "machine translation"]]}, {"text": "arXiv   preprint arXiv : 2001.01589 .", "entities": [[0, 1, "DatasetName", "arXiv"], [3, 4, "DatasetName", "arXiv"]]}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and   WeiJing Zhu . 2002 .", "entities": []}, {"text": "BLEU :", "entities": [[0, 1, "MetricName", "BLEU"]]}, {"text": "A method for automatic   evaluation of machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of   ACL .", "entities": []}, {"text": "Ha\u00b8Sim Sak , Tunga G\u00fcng\u00f6r , and Murat Sara\u00e7lar .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Morphological dis ambiguation of Turkish text with   perceptron algorithm .", "entities": []}, {"text": "In International Conference   on Intelligent Text Processing and Computational   Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch .", "entities": []}, {"text": "2015 a. Improving neural machine translation   models with monol ingual data .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "arXiv preprint   arXiv:1511.06709 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch .", "entities": []}, {"text": "Neural machine translation of rare words with   subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "2015 b. arXiv preprint arXiv :   1508.07909 .", "entities": [[2, 3, "DatasetName", "arXiv"], [4, 5, "DatasetName", "arXiv"]]}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch .   2016 .", "entities": []}, {"text": "Co ntrolling politeness in neural machine   translation via side constraints .", "entities": []}, {"text": "In Proceedings of   NAACL .", "entities": []}, {"text": "Ilya Sutskever , Oriol Vinyals , and Quoc VV Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural   networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "In Proceedings of NIPS .", "entities": []}, {"text": "Ahmed Tawfik , Mahitab Emam , K haled Essam ,   Robert Nabil , and Hany Hassan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Morphology -aware word -segmentation in dialectal   Arabic adaptation of neural machine translation .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "In   Proceedings of the Fourth Arabic Natural   Language Processing Workshop .", "entities": []}, {"text": "Eziz Tursun , Debasis Ganguly , Turgh un Osman ,   Yating Yang , Ghalip Abdukerim , Junlin Zhou , and   Qun Liu . 2016 .", "entities": []}, {"text": "A semi -supervised tag -transition based Markovian model for Uyghur morphology   analysis .", "entities": []}, {"text": "In ACM Transactions on Asian and Low Resource Language Information Processing .", "entities": [[1, 2, "DatasetName", "ACM"]]}, {"text": "Francis M. Tyers and Murat Alperen .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "South -East   European Times : A parallel corpus of the Balkan   languages , In Proceedings of the LREC Workshop   on Exploitation of Multilingual Resources and Tools   for Central and ( South - ) Eastern European   Languages .", "entities": []}, {"text": "Ashish Vaswani , Noam   Shazeer , Niki Parmar , Jakob   Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz   Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all   you need .", "entities": []}, {"text": "In Advances in Neural In formation   Processing Systems .", "entities": []}, {"text": "Junho Yim , Heechul Jung , ByungIn Yoo , Changkyu   Choi , Dusik Park , and Junmo Kim . 2015 .", "entities": []}, {"text": "Rotating   your face using multi -task de ep neural network .", "entities": []}, {"text": "In   Proceedings of CVPR .", "entities": []}, {"text": "Jiajun Zhang and Chengqing Zong .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Exploiting   source -side monolingual data in neural machine   translation .", "entities": []}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Barret Zoph and Kevin Knight .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Multi -source   neural translation .", "entities": []}, {"text": "In Procee dings of NAACL .", "entities": []}]