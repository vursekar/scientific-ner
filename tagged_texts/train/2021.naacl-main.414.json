[{"text": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 5259\u20135274 June 6\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics5259Text Editing by Command Felix Faltings};\u0003Michel Galley\u007fGerold Hintz\u007fChris Brockett\u007f", "entities": []}, {"text": "Chris Quirk\u007fJianfeng Gao\u007fBill", "entities": []}, {"text": "Dolan\u007f } Department of Computer Science , ETH Z \u00a8urich\u007fMicrosoft Research fafelix@student.ethz.ch fmgalley , gehint , chrisbkt , chrisq , jfgao , billdol g@microsoft.com", "entities": [[7, 8, "DatasetName", "ETH"]]}, {"text": "Abstract A prevailing paradigm in neural text generation is one - shot generation , where text is produced in a single step .", "entities": [[6, 8, "TaskName", "text generation"]]}, {"text": "The one - shot setting is inadequate , however , when the constraints the user wishes to impose on the generated text are dynamic , especially when authoring longer documents .", "entities": []}, {"text": "We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text .", "entities": [[7, 9, "TaskName", "text generation"]]}, {"text": "To this end , we propose a novel text editing task , and introduce WikiDocEdits , a dataset of singlesentence edits extracted from Wikipedia revision histories .", "entities": [[14, 15, "DatasetName", "WikiDocEdits"]]}, {"text": "We show that our Interactive Editor , a transformer - based model trained on this dataset , outperforms baselines and obtains positive results in both automatic and human evaluations .", "entities": []}, {"text": "We present empirical and qualitative analyses of this model \u2019s performance.1 1 Introduction A long - standing goal of natural language processing research has been to generate long - form text ( Lebowitz , 1985 ;", "entities": []}, {"text": "Fan et al . , 2018 ; Rashkin et al . , 2020 ) .", "entities": []}, {"text": "Recent large generative language models such as GPT-2 ( Radford et al . , 2019 ) , and GPT3 ( Brown et al . , 2020 ) , demonstrate an impressive ability to generate \ufb02uent text , but their outputs are dif\ufb01cult to control beyond a prompt , and they manifest a tendency to hallucinate facts ( Wiseman et al . , 2017 ) .", "entities": [[7, 8, "MethodName", "GPT-2"]]}, {"text": "Much recent work has thus focused on making such models more controllable ( Keskar et al . , 2019 ;", "entities": []}, {"text": "Hu et al . , 2017 ; Zhang et al . , 2020 ; Dathathri et al . , 2019 ) , and factually grounded ( Guu et al . , 2020 ; Liu et al . , 2018b ) .", "entities": []}, {"text": "*", "entities": []}, {"text": "Work done at Microsoft Research .", "entities": []}, {"text": "1All", "entities": []}, {"text": "our code ( including code to recreate our data ) and pre - trained models will be made available at : http://microsoft.com/research/project/ interactive - document - generation Barack Obama was the 44thPresident of the United States .", "entities": []}, {"text": "Barack Obama was the 44thPresident of the United States and the first African - American to hold the office .", "entities": []}, {"text": "Barack Obama was the 44thPresident of the United States from 2009 to 2017 and the first African - American to hold the office.expandadd years in officeFigure 1 : An illustration of our interactive text generation setting .", "entities": [[33, 35, "TaskName", "text generation"]]}, {"text": "This is an example generated by our model .", "entities": []}, {"text": "The blue panels represent the text being edited , taken from the document shown on the right .", "entities": []}, {"text": "The orange panels represent user edit commands .", "entities": []}, {"text": "The model grounds edits in query results from a commercial search engine .", "entities": []}, {"text": "Most such work only considers a one - shot generation setting .", "entities": []}, {"text": "Given a set of inputs , which may be a prompt , a control code ( Keskar et al . , 2019 ) , or a table of data ( Liu et al . , 2018b ) for example , the system generates text in a single step .", "entities": []}, {"text": "Humans , though , often produce text through an evolutionary process involving multiple draft - edit cycles .", "entities": []}, {"text": "This is not simply because they make mistakes when writing , but because they may require multiple iterations to help them shape and even make sense of what they want to express ( Pirolli and Card , 2005 ) .", "entities": []}, {"text": "For example , consider a user writing an article about Barack Obama .", "entities": []}, {"text": "They might start with a simple sentence such as \u201c Barack Obama was the 44th President of the United States \u201d .", "entities": []}, {"text": "Next , they may wish to expand on that sentence , adding information , or rephrasing it to integrate it better with", "entities": []}, {"text": "5260the text .", "entities": []}, {"text": "Replicating this process in software will mean allowing users to adjust their requirements in response to model outputs .", "entities": []}, {"text": "Even an error - free system that meets all of a user \u2019s initial requirements does not obviate the need for iteration , since those constraints are themselves dynamic .", "entities": []}, {"text": "While this work focuses on text , we also note that these arguments extend to other settings where a system must generate a complex , structured object for a user , such as image or code generation .", "entities": [[35, 37, "TaskName", "code generation"]]}, {"text": "The purpose of this paper is to bring into view the task of controllable text editing , as a step beyond one - shot generation towards interactive document generation .", "entities": []}, {"text": "A full interactive document generation system will likely comprise multiple components , possibly including one - shot generation to create a \ufb01rst draft .", "entities": []}, {"text": "Editing is crucial to interactivity because it allows users to change previously generated text to \ufb01t their dynamic constraints .", "entities": []}, {"text": "This is a stateful operation , where the state is the current version of the document , as opposed to stateless recasting of text from scratch using a one - shot model .", "entities": []}, {"text": "While services like Grammarly or MS Word already offer rewriting suggestions , they mainly focus on syntactic or stylistic edits such as paraphrases ( Gupta et al . , 2018 ) .", "entities": []}, {"text": "In this work , we are interested in a broader range of edits , particularly those that add or remove content , or change the meaning of text .", "entities": []}, {"text": "Figure 1 illustrates this editing setting with an example from our trained model , where a user produces a sentence about Barack Obama over multiple edits .", "entities": []}, {"text": "In sum , we make the following contributions : We introduce a challenging new text editing task , wherein a model must learn to edit text in response to a user command , while drawing on grounding to avoid problems of hallucination ( Wiseman et al . , 2017 ) .", "entities": []}, {"text": "To accompany this task , we release an open - source dataset of sentence - level edits extracted from Wikipedia , including editor comments , which we leverage as natural language commands , together with pre - retrieved grounding documents .", "entities": []}, {"text": "We show that a transformer - based editing model trained on our data outperforms \u201c parrot \u201d and GPT-2 baselines , and obtains competitive results compared to gold - standard edits in human evaluations .", "entities": [[15, 16, "MethodName", "parrot"], [18, 19, "MethodName", "GPT-2"]]}, {"text": "We then perform an empirical analysis of our model \u2019s performance , showing the importance of the command and grounding , and the varying dif\ufb01culty of edits in our dataset.2 Text Editing Task", "entities": []}, {"text": "We now formalize our text editing task .", "entities": []}, {"text": "Let Dbe a document , qa user command2 , andG some appropriate form of grounding .", "entities": []}, {"text": "Moreover , letD0be an edited version of D. Then our task is , given a dataset of edits D= f(D0;q0;G0;D0 0);:::;(DN;qN;GN;D0 N)g , learn to produce document D0 , givenD , q , andG. Note that while previous work on text editing usually only considers Das input , we include both a form of control", "entities": []}, {"text": "qand groundingG.", "entities": []}, {"text": "The command is needed because otherwise the type of edit to be made is unde\ufb01ned , while the grounding provides external knowledge needed to make an edit .", "entities": []}, {"text": "In our speci\ufb01c instance of this task , we will only consider sentence - level edits .", "entities": []}, {"text": "More formally , we consider edits D\u0000 !", "entities": []}, {"text": "D0 , whereDandD0differ only on a single sentence s2D , respectively s02 D0 .", "entities": []}, {"text": "While , in general , edits can vary in complexity from document - level to character - level changes , sentences are a natural way to break down text into relatively independent units of meaning , so it makes sense to edit text one sentence at a time .", "entities": []}, {"text": "More complex , document - level edits can be seen as a composition of multiple sentence - level edits .", "entities": []}, {"text": "Additionally , we will consider user commands q written in natural language , e.g. , \u201c add years in of\ufb01ce \u201d .", "entities": []}, {"text": "The command could also take other forms , such as a categorical variable , but natural language allows for the greatest \ufb02exibility in specifying what the edit should accomplish .", "entities": []}, {"text": "Moreover , natural language commands are a good \ufb01t for our model , which we will initialize with pretrained language model weights .", "entities": []}, {"text": "For similar reasons , we will also consider corpora of text snippets as our groundingG. Alternatively , the grounding could also consist of structured data such as tables or graphs .", "entities": []}, {"text": "In a real user scenario , this grounding might be supplied by the user , or retrieved on the \ufb02y .", "entities": []}, {"text": "For our dataset , we pre - retrieve groundings by querying a commercial search engine .", "entities": []}, {"text": "3 Data To accompany our text editing task we present a novel dataset of nearly 12 million sentence - level edits , WikiDocEdits .", "entities": [[22, 23, "DatasetName", "WikiDocEdits"]]}, {"text": "These edits were extracted from the revision histories in the February 1 , 2020 2This notation re\ufb02ects that the edit command is analogous to a query in a retrieval or QA setting in that it expresses a form of user intent .", "entities": []}, {"text": "5261dump of English Wikipedia.3 For a given Wikipedia page , a revision consists of a source and target text , corresponding to the old and new versions of the page .", "entities": []}, {"text": "Each revision is also accompanied by an editor comment , which we will use as a proxy for the user command .", "entities": []}, {"text": "For a given revision , we split the source and target texts into sentences and then attempt to match the sentences between source and target .", "entities": []}, {"text": "For ef\ufb01ciency , we only look at a k - sentence neighborhood .", "entities": []}, {"text": "Unmatched sentences are candidates for edits .", "entities": []}, {"text": "A source sentence sand target sentence tform an edit pairs\u0000 ! tiff(s;t)>\u000f , wherefis sentencelevel BLEU4without smoothing and \u000f= 0:1 in our case .", "entities": []}, {"text": "If an unmatched source sentence does not form an edit pair with any target sentence , we consider it to be a sentence deletion .", "entities": []}, {"text": "This can also be thought of as matching to an empty sentence .", "entities": []}, {"text": "We identify sentence insertions in an analogous manner .", "entities": []}, {"text": "Importantly , we only consider revisions that contain a single sentence - level edit .", "entities": []}, {"text": "Otherwise , the editor comment that accompanies each revision may only describe one of the possibly many sentence - level edits .", "entities": []}, {"text": "See appendix A for a detailed description of our processing pipeline .", "entities": []}, {"text": "3.1 Grounding We retrieve grounding snippets for the edits in our dataset by querying a commercial search engine .", "entities": []}, {"text": "In order to formulate a query for a given edit , we combine the relevant page and section titles with keywords5from the target sentence .", "entities": []}, {"text": "While the target sentence is not available at test time , we make the assumption that in a real user scenario the relevant grounding would be provided by the user .", "entities": []}, {"text": "We retrieve the top 200 returned web page results and only keep the preview snippets returned by the search engine as the grounding corpus.6 Because Wikipedia , as well as several clones , often appear in search engine results , we check for4 - gram overlap between the target sentence and each grounding snippet , removing any snippet with more than 50 % overlap .", "entities": []}, {"text": "Finally , we rerank7 the retrieved snippets using an information extraction score , and merge the ranked snippets to take the \ufb01rstN= 512 tokens .", "entities": []}, {"text": "3Downloadable from https://dumps.wikimedia.org/. 4We use BLEU-4 in all experiments of this paper .", "entities": [[5, 6, "MetricName", "BLEU-4"]]}, {"text": "5See appendix B for how we identify keywords .", "entities": []}, {"text": "6We also experimented with retrieving and parsing the HTML pages from the search but this did not lead to better end - to - end performance than just using the snippets .", "entities": []}, {"text": "7See appendix C for details on reranking .", "entities": []}, {"text": "Statistic Percentiles Mean 25 % 50 % 75 % Sentence length 16 23 31 25:25 Diff length 2 3 9 7:27 Comment length 2 3 7 5:20 Table 1 : Summary statistics of WikiDocEdits .", "entities": [[33, 34, "DatasetName", "WikiDocEdits"]]}, {"text": "All statistics were computed on a 1%subsample of the data .", "entities": []}, {"text": "Lengths reported in number of words .", "entities": []}, {"text": "The diff length corresponds to the number of words , inserted or deleted , affected by a given edit .", "entities": []}, {"text": "3.2 Data Analysis We now provide an overview of our dataset .", "entities": []}, {"text": "From 667dump \ufb01les in the February 1st2020 dump of Wikipedia , we extract 11,850,786 edits , and take a1%sample of 118,818 edits to run our analyses .", "entities": []}, {"text": "Table 1 presents summary statistics for our data , and in the following , we break down the edits by edit type , and present some examples .", "entities": []}, {"text": "See also appendix D for an analysis of the quality of the retrieved grounding .", "entities": []}, {"text": "Fluency and Content Edits We are interested in the distribution of different edit types within our dataset .", "entities": []}, {"text": "In particular , we want to distinguish between \ufb02uency edits , which only affect the grammar or structure of a sentence , and content edits , which change the meaning of a sentence .", "entities": []}, {"text": "We can lean on previous work to categorize edits on Wikipedia .", "entities": []}, {"text": "Yang et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2017 ) create 13 edit intention categories , and train a classi\ufb01er to label revisions according to the categories .", "entities": []}, {"text": "We apply their classi\ufb01er to our data , and group their 13 categories into \u201c \ufb02uency \u201d , \u201c content \u201d , or \u201c other \u201d edits , as reported in table 2 .", "entities": []}, {"text": "With the caveat that the edits were labelled automatically using a trained classi\ufb01er , we see that , while \ufb02uency edits make up the majority of the edits in our data , a large proportion are content edits .", "entities": []}, {"text": "Examples Table 3 presents some examples from our data .", "entities": []}, {"text": "These were chosen to illustrate a variety of edits .", "entities": []}, {"text": "The \ufb01rst example shows an elaboration edit , appending new information to the end of a sentence .", "entities": []}, {"text": "The second example is a simple typo \ufb01x , while the third is changing a fact .", "entities": []}, {"text": "Finally , the last example is a more complex edit to reword a sentence .", "entities": []}, {"text": "We can see that there is a large variety of edits in our dataset .", "entities": []}, {"text": "See table 11 in the appendix for more examples .", "entities": []}, {"text": "5262Group Labels % Fluency Refactoring , Copy - editing , Wiki\ufb01cation , Point - of - view57:00 Content Fact - update , Simpli\ufb01cation , Elaboration , Veri\ufb01ability , Clari\ufb01cation24:77 Other Unlabeled , Counter - vandalism , Vandalism , Process ,", "entities": []}, {"text": "Disambiguation26:65 Table 2 : Breakdown of edits by grouped intention labels .", "entities": []}, {"text": "See Table 10 in the appendix for a breakdown by intention label instead of group .", "entities": []}, {"text": "The percentages do not total 100because edits can have multiple labels .", "entities": []}, {"text": "\ud835\udc5e\ud835\udca2\ud835\udc37\ud835\udc60T5Encoder < bos>\ud835\udc600\u2032\ud835\udc601\u2032T5 Decoder\ud835\udc600\u2032\ud835\udc601\u2032\ud835\udc602\u2032 \u2026\u2026 Figure 2 : An illustration of our model .", "entities": []}, {"text": "The inputs to the encoder are sequences of tokens separated by hsepi tokens , represented by the vertical bars in the \ufb01gure .", "entities": []}, {"text": "4 Model We formalize our model , which we refer to as Interactive Editor , as a standard auto - regressive sequence to sequence model .", "entities": [[21, 24, "MethodName", "sequence to sequence"]]}, {"text": "Because our data only contains single - sentence edits , we assume that the sentence to be edited in the source document is given as an input to the model .", "entities": []}, {"text": "Given a source sentence s2D , the context arounds , which we will refer to as Dby abuse of notation , a user command q , a grounding corpus G , and a candidate target sentence s0 , the model , f , computes f(s;s0;D;q;G )", "entities": []}, {"text": "= P(s0js;D;q;G )", "entities": []}, {"text": "= Y iP(s0 ijs0 < i;s;D;q;G ) ; wheres0 < i", "entities": []}, {"text": "= fs0 0;:::;s0", "entities": []}, {"text": "i\u00001gare the tokens precedings0 iins0 .", "entities": []}, {"text": "We use the same encoder - decoder architecture as T5 ( Raffel et al . , 2020 ) and initialize our model with pretrained language model weights .", "entities": [[9, 10, "MethodName", "T5"]]}, {"text": "Theencoder - decoder architecture allows us to perform full attention over the inputs s;D;q , andG , while the decoder allows us to auto - regressively generates0 .", "entities": []}, {"text": "Meanwhile , initializing with pretrained weights has been shown to achieve state - of - the - art results on many NLP tasks ( Raffel et al . , 2020 ) .", "entities": []}, {"text": "In order to adapt T5 for our task , we represent all our inputs as sequences of tokens .", "entities": [[4, 5, "MethodName", "T5"]]}, {"text": "We then concatenate these sequences together using separator tokens , truncating and padding them to \ufb01xed lengths .", "entities": []}, {"text": "This is straightforward since all our inputs are text .", "entities": []}, {"text": "See \ufb01g . 2 for reference .", "entities": []}, {"text": "We also use the standard cross - entropy loss to train .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "5 Experiments We train our model on a subset of \u00181,020 K edits from WikiDocEdits .", "entities": [[14, 15, "DatasetName", "WikiDocEdits"]]}, {"text": "We use a training / validation / test split of 1,000K/10K/10 K edits , and train for 3 epochs with a \ufb01xed learning rate of 0.0001 , and a batch size of 128 .", "entities": [[22, 24, "HyperparameterName", "learning rate"], [29, 31, "HyperparameterName", "batch size"]]}, {"text": "We use the T5 - base implementation from Huggingface ( Wolf et al . , 2020 ) , and \ufb01netune all weights in the model .", "entities": [[3, 4, "MethodName", "T5"]]}, {"text": "We validate every 200 steps and select the model with the lowest validation loss .", "entities": [[13, 14, "MetricName", "loss"]]}, {"text": "5.1 Evaluation For inference we use beam search with a beam width of 5 , and keep the 5 highest ranked candidates , excluding any generation that parrots the source as this corresponds to making no edits .", "entities": []}, {"text": "Metrics We consider several metrics to evaluate our model .", "entities": []}, {"text": "One natural metric to consider is BLEU ( ( Papineni et al . , 2002 ) ) .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "BLEU shows high correlation with human judgement on machine translation ( Papineni et al . , 2002 ; Doddington , 2002 ) .", "entities": [[0, 1, "MetricName", "BLEU"], [8, 10, "TaskName", "machine translation"]]}, {"text": "While this should not a priori transfer to evaluating different tasks , our task in fact bears a high similarity to machine translation because of how the output is constrained by the inputs .", "entities": [[21, 23, "TaskName", "machine translation"]]}, {"text": "If , for example , the source sentence in an English to German translation task is \u201c Sally met Lucy \u201d , the German translation must in some way mention Sally and Lucy .", "entities": []}, {"text": "Similarly , in our task , if the source sentence is \u201c Barack Obama was the 44th President of the United States \u201d , and the command is \u201c add birth date \u201d , the edit must somehow mention a birth date somewhere .", "entities": []}, {"text": "Thus , in our setting , BLEU makes sense as a metric since in principle a good model output should not deviate too far from the reference .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "We use macro - averaged", "entities": []}, {"text": "5263Comment added class of \u2019 13 Source Krishna attended Dartmouth College where she was a double major in government and French .", "entities": []}, {"text": "Target Krishna attended Dartmouth College where she was a double major in government and French and graduated in the class of \u2019 13 .", "entities": []}, {"text": "Comment sp Source Mountain State is currently seeing alternative accreditation by the Commission on Collegiate Nursing Education .", "entities": []}, {"text": "Target Mountain State is currently seeking alternative accreditation by the Commission on Collegiate Nursing Education .", "entities": []}, {"text": "Comment correct year of marriage ( did not fit NSW records )", "entities": []}, {"text": "Source He married Margaret Frances Prowse Shaw in Sydney in 1874 .", "entities": []}, {"text": "Target He married Margaret Frances Prowse Shaw in Sydney in 1871 .", "entities": []}, {"text": "Comment Rephrasing Source Entitled \" It Feels Like Home ( Re Invented ) Tour 2011 \" , it contained his songs and remakes of Alliage hits .", "entities": []}, {"text": "Target Entitled \" It Feels Like Home ( Re Invented ) Tour 2011 \" , it included many remakes of Alliage hits as well as some of his newer songs .", "entities": []}, {"text": "Table 3 : Example edits from WikiDocEdits .", "entities": [[6, 7, "DatasetName", "WikiDocEdits"]]}, {"text": "The edited portions are highlighted in bold .", "entities": []}, {"text": "sentence - level BLEU with epsilon smoothing and equally weighted n - grams , with nup to 4 .", "entities": [[3, 4, "MetricName", "BLEU"], [5, 6, "HyperparameterName", "epsilon"]]}, {"text": "One issue with BLEU is that the source and target sentences in our task are already very similar , so a model that simply parrots back the source sentence could achieve an unduly high score .", "entities": [[3, 4, "MetricName", "BLEU"]]}, {"text": "Therefore , we also evaluate model outputs by comparing the word - level edits made by the model against the reference , where a word - level edit is a tuple of an operation , either insertion or deletion , a position , and a word .", "entities": []}, {"text": "For example , in the edit \u201c Barack Obama was the 44thPresident of the United States\u201d\u0000 !", "entities": []}, {"text": "\u201c Barack Obama , born August 4th1961 , was the 44thPresident of the United States \u201d , the set of word edits would look like f(insert;2;\\;\");(insert;3;\\born\");:::g .", "entities": []}, {"text": "Now , denote the set of word edits between two sentences aandbas WE ( a;b ) .", "entities": []}, {"text": "Then , with sthe source sentence , s0the reference target sentence and hthe target sentence generated by the model , we compute the precision PWE(s0;h;s ) = jWE(s0;s)\\WE(h;s)j jWE(h;s)j ; recall , RWE(s0;h;s ) = jWE(s0;s)\\WE(h;s)j jWE(s0;s)j;and F1 score , F1;WE(s0;h;s ) = 2\u0001PWE\u0001RWE PWE+RWE :", "entities": [[37, 39, "MetricName", "F1 score"]]}, {"text": "Finally , we compute sentence - level accuracy , which reports the proportion of edits for which the model output exactly matched the reference .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "Baselines We use two baselines to compare our model to .", "entities": []}, {"text": "First , we consider the parrot baseline that simply outputs the source sentence as is .", "entities": [[5, 6, "MethodName", "parrot"]]}, {"text": "The second baseline attempts to delete the source sentence and replace it with a new sentence .", "entities": []}, {"text": "We use a pretrained GPT-2 model ( Radford et al . , 2019 ) that generates a sentence given the left context .", "entities": [[4, 5, "MethodName", "GPT-2"]]}, {"text": "5.2 Results Table 5 presents our main results .", "entities": []}, {"text": "Notice that the parrot baseline is able to achieve a considerably high BLEU score , as expected , while the GPT-2 baseline surprisingly achieves a high word edit recall score .", "entities": [[3, 4, "MethodName", "parrot"], [12, 14, "MetricName", "BLEU score"], [20, 21, "MethodName", "GPT-2"]]}, {"text": "Our interactive neural editor model is able to beat both baselines across all metrics , as would be expected .", "entities": []}, {"text": "Even on a harsh metric like accuracy our model achieves a nontrivial score , although we suspect most of the edits that the model gets exactly right are \ufb02uency edits .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "See table 6 for", "entities": []}, {"text": "5264Comment Added more marriage info .", "entities": []}, {"text": "Reference editJohnson married Group 1 Crew member Manwell Reyes in 2011 .", "entities": []}, {"text": "Johnson married Group 1 Crew member Manwell Reyes on June 11 , 2011 in Half Moon Bay , California .", "entities": []}, {"text": "Model editJohnson married Group 1 Crew member Manwell Reyes in 2011 .", "entities": []}, {"text": "Johnson married Group 1 Crew member Manwell Reyes in 2011 in a ceremony at Half Moon Bay , California .", "entities": []}, {"text": "Comment another minor addition Reference editThey are more frequent than primary brain tumors .", "entities": []}, {"text": "They are more frequent than primary brain tumors , and are mainly a problem in adults , though children may also have secondary tumors .", "entities": []}, {"text": "Model editThey are more frequent than primary brain tumors .", "entities": []}, {"text": "Secondary brain tumors are more frequent than primary brain tumors .", "entities": []}, {"text": "Table 4 : Example outputs from Interactive Editor for two edits from the test data .", "entities": []}, {"text": "The edit shown is the top - ranked generation from beam search , excluding the parrot generation if it occurs .", "entities": [[15, 16, "MethodName", "parrot"]]}, {"text": "The grounding and context are omitted here for brevity .", "entities": []}, {"text": "Model Acc .", "entities": [[1, 2, "MetricName", "Acc"]]}, {"text": "Word Edit BLEU R P F1 Baselines : Parrot baseline 0 0 0 0 0.67 GPT-2 0 0.38 0.05 0.07 0.00 Ablations : Only source 0.17 0.24 0.26 0.23 0.62", "entities": [[2, 3, "MetricName", "BLEU"], [5, 6, "MetricName", "F1"], [8, 9, "MethodName", "Parrot"], [10, 11, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [12, 13, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [15, 16, "MethodName", "GPT-2"], [16, 17, "DatasetName", "0"]]}, {"text": "No command 0.20 0.28 0.31 0.28 0.65", "entities": []}, {"text": "No grounding 0.18 0.24 0.28 0.24 0.64 Our system : Interactive 0.30 0.41 0.44 0.41 0.70 Table 5 : Evaluation of our model ( Interactive Editor ) against baselines and ablations .", "entities": []}, {"text": "a breakdown by edit type , and table 4 for example model outputs .", "entities": []}, {"text": "Ablations The middle rows of Table 5 show the results for three ablations of our model .", "entities": []}, {"text": "The \ufb01rst ablation removes everything but the source sentences .", "entities": []}, {"text": "This is similar to the paraphrase setting ( Gupta et al . , 2018 ) , and the editing setting in Faruqui et al .", "entities": []}, {"text": "( 2018 ) and Yin", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "We can see that including the context , grounding , and command as additional inputs yields signi\ufb01cant improvements over only using the source sentence .", "entities": []}, {"text": "We can also see from the second ablation that the commands are a crucial element in the model \u2019s performance .", "entities": []}, {"text": "This is not surprising sincewithout a command the model must guess what type of edit to make .", "entities": []}, {"text": "Similarly , the model without grounding performs considerably worse than the full model , showing that the grounding is equally important as the command .", "entities": []}, {"text": "Surprisingly , the last two ablations perform only marginally better than the \ufb01rst , meaning that removing the grounding in addition to the commands , or vice - versa , does not lead to a large drop in performance .", "entities": []}, {"text": "This seems to suggest a synergistic effect between the command and the grounding , which makes sense since the model would not know what to do with the grounding without a command , and likewise , the model would not have access to the right information without the grounding , even if it knew what to edit from the command .", "entities": []}, {"text": "Breakdown by edit type The results of our full model are broken down by edit intention labels in Table 6 .", "entities": []}, {"text": "The columns report the same metrics as in our main table of results , with the exception of S - BLEU , which reports the BLEU score between the source sentence and target , and the last column , which reports the number of test edits that were classi\ufb01ed into each category .", "entities": [[20, 21, "MetricName", "BLEU"], [25, 27, "MetricName", "BLEU score"]]}, {"text": "With the caveat that intention labels come from an automatic classi\ufb01er and not human annotation , we can observe that our model has varying performance across different types of edits .", "entities": []}, {"text": "The model performs very well on \ufb02uency edits , but worse on content edits .", "entities": []}, {"text": "This comes at no surprise given that \ufb02uency ed-", "entities": []}, {"text": "5265Intention Category Acc .", "entities": [[2, 3, "MetricName", "Acc"]]}, {"text": "Word Edit BLEU S - BLEU # Edits P R F1 Fluency 0.36 0.49 0.47 0.46 0.76 0.73 6244 Content 0.10 0.24 0.19 0.20 0.42 0.38 2792", "entities": [[2, 3, "MetricName", "BLEU"], [5, 6, "MetricName", "BLEU"], [10, 11, "MetricName", "F1"]]}, {"text": "Other 0.29 0.45 0.41 0.41 0.74 0.72 3027 Table 6 : Breakdown of results by intention category for our full model .", "entities": []}, {"text": "The categories are the same as in table 2 .", "entities": []}, {"text": "Task Preference ( % ) Reference Neutral Interactive Command 41.00 31.71 27.29 Grounding 29.14 34.86 36.00 Table 7 : Human Evaluation : judging preferences for our system ( Interactive Editor ) vs. human references .", "entities": []}, {"text": "its should be easier as they usually correct minor mistakes , which a language model should be able to detect from pretraining .", "entities": []}, {"text": "Content edits , on the other hand , require pulling the correct information from the grounding and incorporating it in the correct manner into the sentence .", "entities": []}, {"text": "The S - BLEU scores con\ufb01rm this since the source sentences in the \ufb02uency examples are much more similar to the target sentences than for the content edits .", "entities": [[3, 4, "MetricName", "BLEU"]]}, {"text": "In fact , when looking at the absolute improvement of the BLEU over the S - BLEU scores , the model performs equally well on both types of edits .", "entities": [[11, 12, "MetricName", "BLEU"], [16, 17, "MetricName", "BLEU"]]}, {"text": "5.3 Human Evaluations We conducted two rounds of human evaluations , each time across 200 examples from our test set .", "entities": []}, {"text": "Annotators were crowd sourced , and each example was rated by seven judges for a total of 1400 judgements.8 Command and Grounding In our \ufb01rst round of human evaluations we compared our model \u2019s top output from beam search to the reference edit .", "entities": []}, {"text": "There were two tasks .", "entities": []}, {"text": "In the \ufb01rst task , we asked judges to choose which system better accomplished the command q.", "entities": []}, {"text": "In the second , we asked which system was more faithful to the grounding G. Table 7 presents the results .", "entities": []}, {"text": "Although there is a clear preference for the Reference edits in the command - related task , 59 % of judgments suggest that Interactive Editor may be equal to or better 8The annotators were remunerated at a rate above the prevailing Seattle minimum wage at the time .", "entities": []}, {"text": "System A System B Full + 3:45 2:55 Ablated + Full - 3:33 3:12 Ablated Full + 3:45 3:33 Full Ablated - 3:12 2:55 Ablated + Table 8 : Human Evaluation : comparisons between absolute evaluations of different settings .", "entities": []}, {"text": "Raters were asked whether edits were satisfactory .", "entities": []}, {"text": "0 corresponds to strong disagreement , and 5 to strong agreement .", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "Systems are given by model ( full or with the comment ablated ) , and whether the command was shown to the raters ( + or - ) .", "entities": []}, {"text": "Bolded numbers indicate signi\ufb01cant difference with p<0:0125 .", "entities": []}, {"text": "than the reference.9In the grounding task , Interactive Editor demonstrates good correspondence with the background material.10Judges were further asked whether the retrieved grounding was relevant to the context D : 92.86 % of judgments recorded the grounding as either \u201d Somewhat relevant \u201d or \u201d Very relevant \u201d .", "entities": []}, {"text": "Absolute Scoring We also evaluated the overall quality of model outputs .", "entities": []}, {"text": "We considered our full model , and our ablated model that only takes the source sentence as input .", "entities": []}, {"text": "We also considered showing and hiding the edit commands , for a total of 4 settings .", "entities": []}, {"text": "For a given setting , raters were asked whether they found each of the top 3 model outputs satisfactory .", "entities": []}, {"text": "Table 8 presents the results for the top model outputs , with bootstrapped pvalues for pairwise comparisons .", "entities": []}, {"text": "We use a Bonferroni corrected \u000b = 0:0125 to determine significance .", "entities": []}, {"text": "Note that our full model outperforms our ablated model in the \ufb01rst two comparisons .", "entities": []}, {"text": "Inter9The high percentage of Neutral judgments here may be partially attributable to other factors .", "entities": []}, {"text": "Majority Neutral judgments are observed for approximately 65 % of those examples that received at least one Neutral judgment .", "entities": []}, {"text": "This suggests many commands may not be readily interpretable to judges .", "entities": []}, {"text": "10Appendix E presents some additional automatic metrics to measure the faithfulness of the model to the grounding .", "entities": []}, {"text": "5266estingly , the difference is smaller when the raters are not shown the commands .", "entities": []}, {"text": "Additionally , only the ablated model is rated differently depending on whether the commands are shown .", "entities": []}, {"text": "This is to be expected since the ablated model is not likely to be faithful to the commands .", "entities": []}, {"text": "In addition to reporting the mean scores from the raters , we can also look at the number of examples where at least one of the top model outputs was found satisfactory by human judges ( i.e. scored higher than 3 ) .", "entities": []}, {"text": "We \ufb01nd that , when showing the edit commands , at least one of the outputs from our full model was satisfactory in 85:83 % of cases versus 60:17 % for the ablated model .", "entities": []}, {"text": "6 Discussion Text Geoff Hinton is an English tennis player .", "entities": []}, {"text": "Command fix profession Text Geoffrey Hinton is a computer science professor at the University of Toronto .", "entities": []}, {"text": "Command add nationality Text Geoffrey Hinton is an EnglishCanadian computer science professor at the University of Toronto .", "entities": []}, {"text": "Command add birthdate Text Geoffrey Hinton ( born 1946 ) is an English - Canadian computer science professor at the University of Toronto .", "entities": []}, {"text": "Command add most famous work Text Geoffrey Hinton ( born 1946 ) is an English - Canadian computer science professor at the University of Toronto .", "entities": []}, {"text": "Geoffrey Hinton is most famous for his work on artificial neural networks .", "entities": []}, {"text": "Table 9 : An example of a multi - turn interaction with our model .", "entities": []}, {"text": "At each turn , the edit was chosen among the top 3 outputs returned by beam - search .", "entities": []}, {"text": "See table 12 in the appendix for the grounding used in this example .", "entities": []}, {"text": "This paper focuses on the task of editing individual sentences , which we believe to be a challenging task for NLP , as it involves making nuanced changes to text according to natural language commands .", "entities": []}, {"text": "We also believe this task hasuseful applications , particularly in speech - to - text scenarios , where it may be more convenient to speak out a command rather than edit the text directly .", "entities": []}, {"text": "However , we also wish to emphasize that this task is a step towards a larger goal of interactive document generation , and that there are many interesting future directions to explore in this space .", "entities": []}, {"text": "While this paper has focused on single interactions ( i.e. making isolated edits to text ) , it would be worth modeling multiple interactions between the user and model .", "entities": []}, {"text": "One can imagine that there may be a natural order in which to make edits , such as adding information at the start , and \ufb01ne - tuning the language at the end .", "entities": []}, {"text": "It is an open question whether or not a model could learn this .", "entities": []}, {"text": "For illustration , table 9 gives an example of using our model to make several edits in order to create a sentence .", "entities": []}, {"text": "Ultimately , this may look more like a dialogue than a sequence of commands coming from the user .", "entities": []}, {"text": "Additionally , it would also be interesting to look at other settings where a model must generate a complex , structured object for a user , such as code , or images .", "entities": []}, {"text": "We hope that our text editing task , as a \ufb01rst step , can demonstrate the potential for interactive generation systems , and that it will encourage the community to pursue more ideas in this space .", "entities": []}, {"text": "7 Related Work Grounded Generation Large language models can generate \ufb02uent text ( Radford et al . , 2019 ; Brown et", "entities": []}, {"text": "al . , 2020 ; Raffel et al . , 2020 ) , but they have a tendency to hallucinate facts ( Wiseman et al . , 2017 ) .", "entities": []}, {"text": "Thus , several works have explored using various forms of grounding to enable models to generate factually consistent texts ( KoncelKedziorski et al . , 2019 ;", "entities": []}, {"text": "Liu et al . , 2018b ; Prabhumoye et al . , 2019 ; Liu et al . , 2018a ; Guu et al . , 2020 ) .", "entities": []}, {"text": "Our work uses grounding to ensure that edits are factually correct , although our task differs from previous work because of the user command , which requires speci\ufb01c information to be retrieved from the grounding during generation .", "entities": []}, {"text": "Controllable Generation", "entities": []}, {"text": "While grounding can be seen as a way to implicitly control the contents of generated text , other works have explored more explicit forms of control .", "entities": []}, {"text": "Hokamp and Liu ( 2017 ) and Zhang et al .", "entities": []}, {"text": "( 2020 ) use lexical constraints , while Keskar et al .", "entities": []}, {"text": "( 2019 ) and Dathathri et al .", "entities": []}, {"text": "( 2019 ) control higher level attributes of text , such as style , tone , or topic .", "entities": []}, {"text": "Our task instead", "entities": []}, {"text": "5267uses natural language commands , which can \ufb02exibly express different types of constraints , ranging from low - level lexical ones , to high - level topical ones .", "entities": []}, {"text": "In this sense , we can also draw the parallel to dialog response generation ( Ghazvininejad et al . , 2018 ; Dinan et al . , 2018 ) , task - oriented dialog ( Gao et", "entities": [[12, 14, "TaskName", "response generation"]]}, {"text": "al . , 2018 ) , or open domain question answering ( Min et al . , 2019 ; Chen et al . , 2017 ) , that also involve user responses or queries , although these tasks are not concerned with text generation in the context of document creation .", "entities": [[9, 11, "TaskName", "question answering"], [42, 44, "TaskName", "text generation"]]}, {"text": "Story Generation The task of Document Generation considered in our work bears similarity with work on generating long - form narratives ( Jain et al . , 2017 ) .", "entities": [[0, 2, "TaskName", "Story Generation"]]}, {"text": "While earlier work in Story Generation focused more on plan - based architectures ( Lebowitz , 1985 ) , more recent work moved towards end - to - end approaches ( Fan et al . , 2018 ) allowing generation to be unconstrained and creative .", "entities": [[4, 6, "TaskName", "Story Generation"]]}, {"text": "As narratives are often aimed at particular goals expressed in terms of outlines and plans , much of the literature in Story Generation is framed as a form of controllable generation , using storylines ( Peng et al . , 2018 ) , events ( Martin et al . , 2017 ; Harrison et al . , 2017 ) , plot words or word skeletons ( Xu et al . , 2018 ; Ippolito et", "entities": [[21, 23, "TaskName", "Story Generation"]]}, {"text": "al . , 2019 ) , plans ( Yao et al . , 2019 ) , story ending ( Tambwekar et al . , 2019 ) , and outlines ( Rashkin et al . , 2020 ) as various forms of constraints .", "entities": []}, {"text": "Our work takes a signi\ufb01cantly different approach , as we treat document or story generation as an iterative process that allows a human to generate a full document from scratch , but also allows constraints to be more dynamic ( e.g. , add nationality in Table 9 only if the system missed that the \ufb01rst time ) .", "entities": [[13, 15, "TaskName", "story generation"]]}, {"text": "Text Editing Several previous works have focused on text editing .", "entities": []}, {"text": "Guu et al .", "entities": []}, {"text": "( 2018 ) generate sentences by editing prototypes taken from their training corpus , although they use editing only as a means for language modeling .", "entities": []}, {"text": "Wu et al .", "entities": []}, {"text": "( 2019 ) expand upon Guu et al .", "entities": []}, {"text": "( 2018 ) \u2019s setting , but for dialog .", "entities": []}, {"text": "More related to our own setting , Faruqui et al .", "entities": []}, {"text": "( 2018 ) propose WikiAtomicEdits , a dataset of edits crawled from Wikipedia .", "entities": [[4, 5, "DatasetName", "WikiAtomicEdits"]]}, {"text": "However , they consider a much narrower de\ufb01nition of edits than our data does .", "entities": []}, {"text": "Yin et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) use WikiAtomicEdits and propose the task of learning to represent edits , which Marrese - Taylor et al .", "entities": [[4, 5, "DatasetName", "WikiAtomicEdits"]]}, {"text": "( 2020 ) expand using a variational approach .", "entities": []}, {"text": "In contrast , we are more interested in generating edits rather than repre - senting them .", "entities": []}, {"text": "Related to Wikipedia data , Pryzant et al .", "entities": []}, {"text": "( 2020 ) also used Wikipedia revision histories to learn to debias text , whereas we considered general edits .", "entities": []}, {"text": "Iso et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) propose a factbased text editing task , but they do not consider control or other types of edits .", "entities": []}, {"text": "Another related task to text editing is text paraphrasing ( Gupta et al . , 2018 ) , however paraphrasing usually conserves the meaning of a sentence .", "entities": []}, {"text": "While the edits we consider include meaning - preserving edits , we are mostly interested in edits that affect meaning .", "entities": []}, {"text": "8 Conclusion In this work we argued that text generation should be interactive , and , as a means towards that end , we proposed a general text editing task , where a system must edit a document in response to a user command .", "entities": [[8, 10, "TaskName", "text generation"]]}, {"text": "In our speci\ufb01c instance of the task we considered single - sentence edits , and we crawled a dataset of several million edits from Wikipedia that included commands , in the form of editor comments , as well as grounding documents .", "entities": []}, {"text": "We then showed that training a transformer - based model on our data , while initializing with pretrained language model weights , yields encouraging results on both automatic and human evaluations .", "entities": []}, {"text": "Additionally , our ablation studies showed the crucial role played by the user command and grounding .", "entities": []}, {"text": "Breaking down our results by types of edits , we saw that our model not only performs well on easier \ufb02uency edits , but also on much harder content edits .", "entities": []}, {"text": "Finally , we discussed future research directions for interactive document generation , as well as possible extensions to other domains such as images or code .", "entities": []}, {"text": "Acknowledgments The authors would like to thank Thomas Hofmann , as well as Sudha Rao , Matt Richardson , Zhang Li , Kosh Narayanan , and Chandra Chikkareddy for their helpful suggestions .", "entities": []}, {"text": "References Giusepppe Attardi .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "WikiExtractor .", "entities": []}, {"text": "https:// github.com/attardi/wikiextractor .", "entities": []}, {"text": "Steven Bird , Ewan Klein , and Edward Loper .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Natural language processing with Python : analyzing text with the natural language toolkit .", "entities": []}, {"text": "O\u2019Reilly Media , Inc.", "entities": []}, {"text": "5268T. Brown , B. Mann , Nick Ryder , Melanie Subbiah , J. Kaplan , P. Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - V oss , G. Kr \u00a8uger , Tom Henighan , R. Child , Aditya Ramesh , D. Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , E. Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , J. Clark , Christopher Berner , Sam McCandlish , A. Radford , Ilya Sutskever , and Dario Amodei .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Language models are few - shot learners .", "entities": []}, {"text": "ArXiv , abs/2005.14165 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Danqi Chen , Adam Fisch , Jason Weston , and Antoine Bordes . 2017 .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "Reading wikipedia to answer opendomain questions .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of ACL .", "entities": []}, {"text": "Sumanth Dathathri , Andrea Madotto , Janice Lan , Jane Hung , Eric Frank , Piero Molino , Jason Yosinski , and Rosanne Liu . 2019 .", "entities": []}, {"text": "Plug and play language models : A simple approach to controlled text generation .", "entities": [[11, 13, "TaskName", "text generation"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ICLR .", "entities": []}, {"text": "Emily Dinan , Stephen Roller , Kurt Shuster , Angela Fan , Michael Auli , and Jason Weston .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Wizard of wikipedia : Knowledge - powered conversational agents .", "entities": [[0, 3, "DatasetName", "Wizard of wikipedia"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ICLR .", "entities": []}, {"text": "George Doddington .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Automatic evaluation of machine translation quality using n - gram cooccurrence statistics .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the second international conference on Human Language Technology Research , pages 138\u2013145 .", "entities": []}, {"text": "Angela Fan , Mike Lewis , and Yann Dauphin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Hierarchical neural story generation .", "entities": [[2, 4, "TaskName", "story generation"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ACL , pages 889\u2013898 .", "entities": []}, {"text": "Manaal Faruqui , Ellie Pavlick , Ian Tenney , and Dipanjan Das . 2018 .", "entities": []}, {"text": "Wikiatomicedits : A multilingual corpus of wikipedia edits for modeling language and discourse .", "entities": [[0, 1, "DatasetName", "Wikiatomicedits"]]}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLP , pages 305\u2013315 .", "entities": []}, {"text": "Jianfeng Gao , Michel Galley , and Lihong Li .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural approaches to conversational ai .", "entities": []}, {"text": "In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval , pages 1371\u20131374 .", "entities": [[4, 5, "DatasetName", "ACM"], [12, 14, "TaskName", "Information Retrieval"]]}, {"text": "Marjan Ghazvininejad , Chris Brockett , Ming - Wei Chang , W. Dolan , Jianfeng Gao , Wen tau Yih , and Michel Galley .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A knowledge - grounded neural conversation model .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of AAAI .", "entities": []}, {"text": "A. Gupta , A. Agarwal , Prawaan Singh , and P. Rai . 2018 .", "entities": []}, {"text": "A deep generative framework for paraphrase generation .", "entities": [[5, 7, "TaskName", "paraphrase generation"]]}, {"text": "In Proc .", "entities": []}, {"text": "of AAAI .", "entities": []}, {"text": "Kelvin Guu , Tatsunori B. Hashimoto , Yonatan Oren , and Percy Liang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Generating sentences by editing prototypes .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 6:437\u2013450 .", "entities": []}, {"text": "Kelvin Guu , Kenton Lee , Zora Tung , Panupong Pasupat , and Ming - Wei Chang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "REALM : Retrieval - augmented language model pre - training .", "entities": []}, {"text": "ArXiv , abs/2002.08909.B. Harrison , Christopher Purdy , and Mark O. Riedl .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "2017 .", "entities": []}, {"text": "Toward automated story generation with markov chain monte carlo methods and deep neural networks .", "entities": [[2, 4, "TaskName", "story generation"]]}, {"text": "In AIIDE Workshops .", "entities": []}, {"text": "Chris Hokamp and Qun Liu .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Lexically constrained decoding for sequence generation using grid beam search .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of ACL , pages 1535\u20131546 .", "entities": []}, {"text": "Zhiting Hu , Zichao Yang , Xiaodan Liang , Ruslan Salakhutdinov , and Eric P Xing .", "entities": [[9, 10, "DatasetName", "Ruslan"]]}, {"text": "2017 .", "entities": []}, {"text": "Toward controlled generation of text .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of ICML , pages 1587\u20131596 .", "entities": []}, {"text": "Daphne Ippolito , David Grangier , Chris CallisonBurch , and Douglas Eck . 2019 .", "entities": []}, {"text": "Unsupervised hierarchical story in\ufb01lling .", "entities": []}, {"text": "In Proceedings of the First Workshop on Narrative Understanding .", "entities": []}, {"text": "Hayate Iso , Chao Qiao , and Hang Li .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Fact - based text editing .", "entities": [[0, 5, "TaskName", "Fact - based text editing"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ACL , pages 171\u2013182 .", "entities": []}, {"text": "Parag Jain , Priyanka Agrawal , Abhijit Mishra , Mohak Sukhwani , Anirban Laha , and Karthik Sankaranarayanan . 2017 .", "entities": []}, {"text": "Story generation from sequence of independent short descriptions .", "entities": [[0, 2, "TaskName", "Story generation"]]}, {"text": "arXiv preprint arXiv:1707.05501 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Nitish Shirish Keskar , Bryan McCann , Lav R. Varshney , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "CTRL :", "entities": [[0, 1, "MethodName", "CTRL"]]}, {"text": "A conditional transformer language model for controllable generation .", "entities": []}, {"text": "ArXiv , abs/1909.05858 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "T. Kiss and J. Strunk .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Unsupervised multilingual sentence boundary detection .", "entities": [[3, 5, "TaskName", "boundary detection"]]}, {"text": "Computational Linguistics , 32:485\u2013525 .", "entities": []}, {"text": "Rik Koncel - Kedziorski , Dhanush Bekal , Yi Luan , Mirella Lapata , and Hannaneh Hajishirzi .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Text generation from knowledge graphs with graph transformers .", "entities": [[0, 2, "TaskName", "Text generation"], [3, 5, "TaskName", "knowledge graphs"]]}, {"text": "In Proc .", "entities": []}, {"text": "of NAACL - HLT", "entities": []}, {"text": ".", "entities": []}, {"text": "M. Lebowitz .", "entities": []}, {"text": "1985 .", "entities": []}, {"text": "Story - telling as planning and learning .", "entities": []}, {"text": "Poetics , 14:483\u2013502 .", "entities": []}, {"text": "Peter J Liu , Mohammad Saleh , Etienne Pot , Ben Goodrich , Ryan Sepassi , Lukasz Kaiser , and Noam Shazeer .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Generating wikipedia by summarizing long sequences .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of ICLR .", "entities": []}, {"text": "T. Liu , Kexiang Wang , Lei Sha , Baobao Chang , and Z. Sui .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "Table - to - text generation by structureaware seq2seq learning .", "entities": [[0, 6, "TaskName", "Table - to - text generation"], [8, 9, "MethodName", "seq2seq"]]}, {"text": "In Proc .", "entities": []}, {"text": "of AAAI .", "entities": []}, {"text": "Edison Marrese - Taylor , Machel Reid , and Y .", "entities": []}, {"text": "Matsuo . 2020 .", "entities": []}, {"text": "Variational inference for learning representations of natural language edits .", "entities": [[0, 2, "MethodName", "Variational inference"]]}, {"text": "ArXiv , abs/2004.09143 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Lara J Martin , Prithviraj Ammanabrolu , Xinyu Wang , William Hancock , Shruti Singh , Brent Harrison , and Mark O Riedl .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Event representations for automated story generation with deep neural nets .", "entities": [[4, 6, "TaskName", "story generation"]]}, {"text": "arXiv preprint arXiv:1706.01331 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "5269Sewon Min , Danqi Chen , Luke Zettlemoyer , and Hannaneh Hajishirzi . 2019 .", "entities": []}, {"text": "Knowledge guided text retrieval and reading for open domain question answering .", "entities": [[9, 11, "TaskName", "question answering"]]}, {"text": "ArXiv , abs/1911.03868 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "BLEU : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "BLEU"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ACL .", "entities": []}, {"text": "Nanyun Peng , Marjan Ghazvininejad , Jonathan May , and Kevin Knight .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Towards controllable story generation .", "entities": [[2, 4, "TaskName", "story generation"]]}, {"text": "In Proceedings of the First Workshop on Storytelling , pages 43\u201349 .", "entities": []}, {"text": "Peter Pirolli and Stuart Card .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "The sensemaking process and leverage points for analyst technology as identi\ufb01ed through cognitive task analysis .", "entities": []}, {"text": "In Proceedings of international conference on intelligence analysis .", "entities": []}, {"text": "Shrimai Prabhumoye , Chris Quirk , and Michel Galley . 2019 .", "entities": []}, {"text": "Towards content transfer through grounded text generation .", "entities": [[5, 7, "TaskName", "text generation"]]}, {"text": "In Proc .", "entities": []}, {"text": "of NAACL - HLT , pages 2622\u20132632 .", "entities": []}, {"text": "Reid Pryzant , Richard Diehl Martinez , Nathan Dass , Sadao Kurohashi , Dan Jurafsky , and Diyi Yang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Automatically neutralizing subjective bias in text .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of AAAI , volume 34 , pages 480\u2013489 .", "entities": []}, {"text": "Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Technical report , Open AI .", "entities": []}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J Liu . 2020 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the limits of transfer learning with a uni\ufb01ed text - to - text transformer .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "Journal of Machine Learning Research , 21:1\u201367 .", "entities": []}, {"text": "Hannah Rashkin , Asli Celikyilmaz , Yejin Choi , and Jianfeng Gao . 2020 .", "entities": []}, {"text": "PlotMachines : Outlineconditioned generation with dynamic plot state tracking .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLP .", "entities": []}, {"text": "Pradyumna Tambwekar , Murtaza Dhuliawala , Lara J Martin , Animesh Mehta , Brent Harrison , and Mark O Riedl .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Controllable neural story plot generation via reward shaping .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of IJCAI , pages 5982\u20135988 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Sam Joshua Wiseman , Stuart Merrill Shieber , and Alexander Sasha Matthew Rush .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Challenges in data - to - document generation .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLP .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Wolf , Julien Chaumond , Lysandre Debut , Victor Sanh , Clement Delangue , Anthony Moi , Pierric Cistac , Morgan Funtowicz , Joe Davison , Sam Shleifer , et al . 2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLP , pages 38\u201345.Yu Wu , Furu Wei , Shaohan Huang , Yunli Wang , Zhoujun Li , and Ming Zhou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Response generation by context - aware prototype editing .", "entities": [[0, 2, "TaskName", "Response generation"]]}, {"text": "In Proc .", "entities": []}, {"text": "of AAAI , volume 33 , pages 7281\u20137288 .", "entities": []}, {"text": "Jingjing Xu , Xuancheng Ren , Yi Zhang , Qi Zeng , Xiaoyan Cai , and Xu Sun .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A skeleton - based model for promoting coherence among sentences in narrative story generation .", "entities": [[12, 14, "TaskName", "story generation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 4306\u20134315 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Diyi Yang , Aaron Halfaker , Robert Kraut , and Eduard Hovy . 2017 .", "entities": []}, {"text": "Identifying semantic edit intentions from revisions in wikipedia .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLP , pages 2000\u20132010 .", "entities": []}, {"text": "Lili Yao , Nanyun Peng , Ralph Weischedel , Kevin Knight , Dongyan Zhao , and Rui Yan . 2019 .", "entities": []}, {"text": "Planand - write : Towards better automatic storytelling .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of AAAI , volume 33 , pages 7378\u20137385 .", "entities": []}, {"text": "Pengcheng Yin , Graham Neubig , Miltiadis Allamanis , Marc Brockschmidt , and Alexander L Gaunt .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning to represent edits .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of ICLR .", "entities": []}, {"text": "Tianyi Zhang , Varsha Kishore , Felix Wu , Kilian Q Weinberger , and Yoav Artzi . 2019a .", "entities": []}, {"text": "Bertscore : Evaluating text generation with bert .", "entities": [[3, 5, "TaskName", "text generation"]]}, {"text": "In Proc .", "entities": []}, {"text": "of ICLR .", "entities": []}, {"text": "Xizhe Zhang , Dheeraj Rajagopal , Michael Gamon , Sujay Kumar Jauhar , and Chang - Tien Lu . 2019b .", "entities": [[10, 11, "DatasetName", "Kumar"]]}, {"text": "Modeling the relationship between user comments and edits in document revision .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of EMNLPIJCNLP .", "entities": []}, {"text": "Yizhe Zhang , G. Wang , C. Li , Zhe Gan , Chris Brockett , and B. Dolan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Pointer : Constrained text generation via insertion - based generative pre - training .", "entities": [[3, 5, "TaskName", "text generation"]]}, {"text": "ArXiv , abs/2005.00558 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "5270A Data Processing pipeline This section describes our pipeline to obtain atomic edits from Wikipedia revisions in more detail .", "entities": []}, {"text": "We start by \ufb01ltering the revisions in the data .", "entities": []}, {"text": "In particular , following ( Zhang et al . , 2019b ) , we only keep revisions that affect a single section , and we exclude revisions that do not contain an editor comment .", "entities": []}, {"text": "We also exclude certain page types like talk or user pages .", "entities": []}, {"text": "We then strip the Wikipedia markup in the retrieved text , using the WikiExtractor script ( Attardi , 2015 ) .", "entities": []}, {"text": "This removes most markup and Wikimedia templates from the text .", "entities": []}, {"text": "Because the markup language used on Wikipedia is not completely formalized11 , and because malformed markup often appears in intermediate versions of Wikipedia pages , there is no guarantee that we can remove all the markup from the text .", "entities": []}, {"text": "We then split each section into sentences using the Punkt sentence tokenizer ( Kiss and Strunk , 2006 ) provided in the NLTK python package ( Bird et al . , 2009 ) .", "entities": []}, {"text": "After splitting into sentences , we attempt to match the sentences from the pre - edit ( source ) document to the sentences in the post - edit ( target ) document .", "entities": []}, {"text": "Unmatched sentences will be candidates for edits .", "entities": []}, {"text": "Similarly to ( Faruqui et al . , 2018 ) , for each sentence siin the source document , we only look at the target sentences fti\u0000k;:::;t i;:::;t i+kg , withk= 20 .", "entities": []}, {"text": "This avoids the quadratic complexity of looking at all matches .", "entities": []}, {"text": "We then \ufb01lter out revisions that contain more than one sentence - level edit to ensure that the comment is relevant .", "entities": []}, {"text": "If there is a single unmatched source , respectively target , sentence , we consider it a sentence deletion , respectively insertion .", "entities": []}, {"text": "Because we do not look at all matches between source and target sentences , a sentence may remain unmatched if , in the target document , it was moved more than ksentences away compared to the source document .", "entities": []}, {"text": "Thus we only keep a sentence insertion or deletion if the total number of source and target sentences differ by one .", "entities": []}, {"text": "If there are both an unmatched source sentence sand target sentence t , we consider them to form an edit s\u0000 !", "entities": []}, {"text": "tiff(s;t ) > \u000f , wherefis the BLEU score and\u000f= 0:1 .", "entities": [[7, 9, "MetricName", "BLEU score"]]}, {"text": "As a \ufb01nal step , we \ufb01lter out edits that involve sentences with markup punctuation .", "entities": []}, {"text": "We have 11See https://www.mediawiki.org/wiki/ Markup_spec for a discussion.found that this helps remedy the shortfalls of the markup removal step , since it often leaves behind markup symbols .", "entities": []}, {"text": "While there may be valid sentences that use markup punctuation , we do not expect them to make up a signi\ufb01cant part of the data , nor do we expect them to be signi\ufb01cantly different from regular sentences , except for their use of unusual punctuation .", "entities": []}, {"text": "B Grounding Search Query Construction For a given edit , we combine the relevant page and section titles with keywords from the target sentence to construct a query that we use to retrieve grounding from a commercial search engine .", "entities": []}, {"text": "In order to identify keywords we look at document frequency df(w )", "entities": []}, {"text": "= jfD2Djw2Dgj jDj ; whereDis a sample of 500;000Wikipedia articles taken from the Tensor\ufb02ow Wikipedia dataset.12 We consider words wwith df ( w)<0:01to be keywords .", "entities": []}, {"text": "C Grounding Document Reranking Because the combined length of the grounding snippets we retrieve far exceeds the capacity of our model , we rerank the retrieved snippets using an information extraction score .", "entities": []}, {"text": "We then merge the ranked snippets and take only the \ufb01rst N= 512 tokens .", "entities": []}, {"text": "Following ( Liu et al . , 2018a ) we use tf - idf scores to rerank .", "entities": []}, {"text": "For a given edit s\u0000 ! s0 , with retrieved grounding documents G , the information extraction score of snippet G2Gis score(G )", "entities": []}, {"text": "= X w2s0tf - idf(w;G ) ; where the tf - idf score of word wis tf - idf(w;G )", "entities": []}, {"text": "= Nw(G)\u0001log\u0012Ng Ngw\u0013 ; whereNw(G)is the number of occurrences of w inG , Ngwis the number of documents in Gthat containw , andNgis the number of documents inG. 12https://www.tensorflow.org/datasets/ catalog / wikipedia", "entities": []}, {"text": "5271Label Description % Edits % Orig .", "entities": []}, {"text": "Counter - Vandalism Revert or otherwise ; remove vandalism 0:05 1:90 Fact - update Update numbers , dates , scores , episodes , status , etc . based on newly available information1:57 5:50 Copy - editing Rephrase ; improve grammar , spelling , tone , or punctuation29:22 11:80 Wiki\ufb01cation Format text to meet style guidelines , e.g. add links or remove them where necessary21:12 33:10 Vandalism Deliberately attempt to damage the article 1:01 2:50 Simpli\ufb01cation Reduce the complexity or breadth of discussion ; may remove information3:13 1:60 Elaboration Extend / add substantive new content ; insert a fact or new meaningful assertion9:50 12 Veri\ufb01ability Add / modify references / citations ; remove unveri\ufb01ed text7:63 5:40 Process Start / continue a wiki process work\ufb02ow such as tagging an article with cleanup , merge or deletion notices0:62 4:40 Clari\ufb01cation Specify or explain an existing fact or meaning by example or discussion without adding new information3:54 0:70 Disambiguation Relink from a disambiguation page to a speci\ufb01c page0:70 0:30 Point - of - view Rewrite using encyclopedic , neutral tone ; remove bias ; apply due weight0 0:30 Unlabeled No label 21:39 1:20 Table 10 : Breakdown of the edits in our data by intention label .", "entities": []}, {"text": "The descriptions are taken from Yang et al .", "entities": []}, {"text": "( 2017 ) .", "entities": []}, {"text": "% Edits gives the prevalence of each label in our data , while % Orig .", "entities": []}, {"text": "gives the prevalence in the hand - labelled dataset presented in Yang et al .", "entities": []}, {"text": "( 2017 ) .", "entities": []}, {"text": "The percentages do not total 100because edits can have multiple labels .", "entities": []}, {"text": "5272Comment Reword Source ByteDance responded by adding a kids - only mode to TikTok which allows music videos to be recorded , but not posted and by removing some accounts and content from those determined to be underage .", "entities": []}, {"text": "Target ByteDance responded by adding a kids - only mode to TikTok which blocks the upload of videos , the building of user profiles , direct messaging , and commenting on other \u2019s videos , while still allowing the viewing and recording of content .", "entities": []}, {"text": "Comment corrected tense for decedent Source While Bob Steward has not been an active producer since 1992 , he serves as a Creative Consultant in his son \u2019s new production company , Steward Television , and islisted on the official website as Steward Television \u2019s founder .", "entities": []}, {"text": "Target While Bob Steward was not an active producer since 1992 , he served as a Creative Consultant in his son \u2019s new production company , Steward Television , and was listed on the official website as Steward Television \u2019s founder .", "entities": []}, {"text": "Comment fixed spelling for Walter Yetnikoff Source Mottola was hired by Sony Music ( then known as CBS Records ) by its controversial President Walter Yentlkoff to run its U.S. operations .", "entities": []}, {"text": "Target Mottola was hired by Sony Music ( then known as CBS Records ) by its controversial President Walter Yetnikoff to run its U.S. operations .", "entities": []}, {"text": "Table 11 : More example edits from WikiDocEdits .", "entities": [[7, 8, "DatasetName", "WikiDocEdits"]]}, {"text": "The edited portions are highlighted in bold .", "entities": []}, {"text": "5273Geoffrey Everest Hinton CC FRS FRSC ( born 6 December 1947 ) is an English Canadian cognitive psychologist and computer scientist , most noted for his work on artificial neural networks .", "entities": []}, {"text": "Since 2013 he divides his time working for Google ( Google Brain ) and the University of Toronto .", "entities": [[8, 9, "DatasetName", "Google"], [10, 11, "DatasetName", "Google"]]}, {"text": "In 2017 , he cofounded and became the Chief Scientific Advisor of the Vector Institute in Toronto .", "entities": []}, {"text": "Geoffrey Hinton : index .", "entities": []}, {"text": "Department of Computer Science : email :", "entities": []}, {"text": "[ REDACTED ] : University of Toronto : voice : send email : 6 King \u2019s College Rd .", "entities": []}, {"text": "We would like to show you a description here but the site wo n\u2019t allow us .", "entities": []}, {"text": "Geoffrey \u2019s great grandfather , the mathematician", "entities": []}, {"text": "[ REDACTED ] Charles Hinton , coined the word \\tesseract \" and popularized the idea of higher dimensions , while his father , Howard Everest Hinton , was a distinguished entomologist .", "entities": []}, {"text": "Geoffrey Hinton is a fellow of the Royal Society , the Royal Society of Canada , and the Association for the Advancement of Artificial Intelligence .", "entities": []}, {"text": "He is an honorary foreign member of the American Academy of Arts and Sciences and the National Academy of Engineering , and a former president of the Cognitive Science Society .", "entities": []}, {"text": "Geoffrey Hinton .", "entities": []}, {"text": "Emeritus Prof. Comp Sci , U.Toronto & Engineering Fellow , Google .", "entities": [[10, 11, "DatasetName", "Google"]]}, {"text": "Verified email at cs.toronto.edu - Homepage . machine learning psychology artificial intelligence cognitive science computer science .", "entities": []}, {"text": "Articles Cited by Co - authors .", "entities": []}, {"text": "Title .", "entities": []}, {"text": "Sort .", "entities": []}, {"text": "Sort by citations Sort by year Sort by title .", "entities": []}, {"text": "Geoff Hinton was born in Wimbledon in 1947 to Howard Hinton , an entomologist , and a schoolteacher mother , Margaret Clark .", "entities": []}, {"text": "The childhood Hinton describes is a mash - up of Lemony Snicket , ...", "entities": []}, {"text": "As the first of this interview series , I am delighted to present to you an interview with Geoffrey Hinton .", "entities": []}, {"text": "Welcome Geoff , and thank you for doing this interview with deeplearning.ai .", "entities": []}, {"text": "iiThank you for inviting me .", "entities": []}, {"text": "iiI think that at this point you more than anyone else on this planet has invented so many of the ideas behind deep learning .", "entities": []}, {"text": "Talks by Geoffrey Hinton .", "entities": []}, {"text": "The next generation of neural networks A 45min version of this talk which was given at the 10 year celebration of the Microsoft Cambridge Research Laboratory .", "entities": [[23, 24, "DatasetName", "Cambridge"]]}, {"text": "the original powerpoint file version for most browsers.ps version with 4 slides per page .", "entities": []}, {"text": "Very gentle after - dinner version of IJCAI-2005 Research Excellence ...", "entities": []}, {"text": "Table 12 : Grounding used for the example in table 9 .", "entities": []}, {"text": "Parts indicated by [ REDACTED ] were removed for containing sensitive material .", "entities": []}, {"text": "Coverage corpus", "entities": []}, {"text": "Percentiles Mean 25 % 50 % 75 % All Inputs 0:66 0:75 0:83 0:74", "entities": []}, {"text": "Grounding 0:50 0:62 0:73 0:61 Comment 0:22 0:31 0:46 0:36 Table 13 : RBERT statistics of inserted words for edits in WikiDocEdits .", "entities": [[21, 22, "DatasetName", "WikiDocEdits"]]}, {"text": "All statistics were computed on a 1 % subsample of the data .", "entities": []}, {"text": "The BERT embeddings used to compute RBERT were produced using a pretrained BERT base model .", "entities": [[1, 2, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"]]}, {"text": "The idf weights were computed from a sample of 500,000 Wikipedia pages .", "entities": []}, {"text": "Each row represents a different recall when considering a different coverage corpus C.D Grounding Coverage Analysis We are also interested in knowing how well edits in the data are covered by the inputs ( i.e. D;s;q , orG ) , where an edit is well covered if the information necessary to produce the edit appears somewhere in the inputs .", "entities": []}, {"text": "To measure coverage we use word recall : how many words that were inserted in an edit also appear in the grounding ?", "entities": []}, {"text": "However , because simple recall fails to account for synonyms , or the context in which words appear , we use the BERTScore ( Zhang et al . , 2019a ) recall .", "entities": []}, {"text": "This allows for fuzzy matching between BERT embeddings instead of requiring exact word matches .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "We also use idf scores to weigh words , since we are mostly interested in covering rare words , which are more likely to be meaning - carrying .", "entities": []}, {"text": "We can de\ufb01ne the BERT recall , RBERT , for a sentence edit", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "5274Coverage corpus Percentiles Mean 25 % 50 % 75 % Full Model 0:54 0:66 0:75 0:64 Ablated Model 0:46 0:57 0:67 0:57 Table 14 : RBERT statistics of inserted words across test edits in WikiDocEdits .", "entities": [[34, 35, "DatasetName", "WikiDocEdits"]]}, {"text": "The BERT embeddings used to compute RBERT were produced using a pretrained BERT base model .", "entities": [[1, 2, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"]]}, {"text": "The idf weights were computed from a sample of 500,000 Wikipedia pages .", "entities": []}, {"text": "In all rows , the considered corpus Ccorresponds to the grounding .", "entities": []}, {"text": "s\u0000 ! s0 , with respect to some text corpus Cas P w2s0nsidf(w)\u0001maxw02CBERT ( w)TBERT ( w0 ) P w2s0nsidf(w ) ; wheres0ns = fw2s0jw = 2sg , and idf(w ) are the inverse document frequency scores computed on a random sample of 500KWikipedia pages .", "entities": []}, {"text": "Table 13 reports the coverage statistics for our subsample of the data .", "entities": []}, {"text": "We used an uncased BERT base model to compute the embeddings .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "The \ufb01rst row reports the coverage of the target by all of the inputs , namely the command , grounding , context , and source sentence .", "entities": []}, {"text": "The second row shows the coverage by the grounding alone .", "entities": []}, {"text": "Note that , even with just the grounding , coverage is already fairly high .", "entities": []}, {"text": "Finally , the last row presents the coverage by the command alone , which shows that it also provides grounding .", "entities": []}, {"text": "E Additional Factuality Results", "entities": []}, {"text": "In addition to human evaluations , we also used automatic metrics to evaluate how faithful our model is to the grounding .", "entities": []}, {"text": "BERT Recall Similarly to the coverage analysis in appendix D , we can use RBERT , with the grounding asC , to assess how well each word inserted by the model is supported by the grounding .", "entities": [[0, 1, "MethodName", "BERT"], [1, 2, "MetricName", "Recall"]]}, {"text": "The only difference is that the model output now replaces the reference target s0 in the formula for RBERT .", "entities": []}, {"text": "Table 14 gives the summary statistics for RBERT across our test set , computed on the outputs of our full model , and the ablated model without grounding .", "entities": []}, {"text": "Note that we only consider edits where the model makes at least one insertion .", "entities": []}, {"text": "The ablated model serves as a baseline to compare the grounded model to .", "entities": []}, {"text": "This baseline achieves a high RBERT score , likely because of spurious matcheswith the grounding .", "entities": []}, {"text": "Nevertheless , our grounded model is still more faithful to the grounding , as expected .", "entities": []}, {"text": "Grounding Usage WhileRBERT attempts to measure how faithful the model is to the grounding ( i.e. is the information inserted by the model found in the grounding ? ) , we can also attempt to measure how much the grounding is used ( i.e. how much of the information inserted by the model is only found in the grounding ? ) .", "entities": []}, {"text": "One simple approach is to look at how many words inserted by the model are found in the grounding but not in the rest of the inputs .", "entities": []}, {"text": "While this is n\u2019t obvious to compute similarities between BERT embeddings , we can use exact word matches instead .", "entities": [[9, 10, "MethodName", "BERT"]]}, {"text": "For the model without grounding we \ufb01nd that in 30:48 % of edits in the test set ( with at least one insertion ) , at least one of the words inserted by the model is found in the grounding but not in the rest of the inputs .", "entities": []}, {"text": "For the full model , this number increases to 48:66 % as expected .", "entities": []}, {"text": "The ablated model appears to insert words exclusive to the grounding in a high proportion of edits .", "entities": []}, {"text": "However , this could be due to \ufb02uency edits , where the model might insert a functional word that happens to only appear in the grounding .", "entities": []}, {"text": "If we restrict our attention to content edits , as de\ufb01ned in section 3.2 , the ablated model inserts grounding - exclusive words in only 36:85 % of edits , and 65:40 % for the full model .", "entities": []}]