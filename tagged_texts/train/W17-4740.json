[{"text": "Proceedings of the Conference on Machine Translation ( WMT ) , Volume 2 : Shared Task Papers , pages 400\u2013404 Copenhagen , Denmark , September 711 , 2017 .", "entities": [[5, 7, "TaskName", "Machine Translation"]]}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics XMU Neural Machine Translation Systems for WMT 17 Zhixing Tan , Boli Wang , Jinming Hu , Yidong Chen", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "andXiaodong Shi School of Information Science and Engineering , Xiamen University , Fujian , China { playinf , boliwang , todtom } @stu.xmu.edu.cn { ydchen , mandel }", "entities": []}, {"text": "@xmu.edu.cn Abstract This paper describes the Neural Machine Translation systems of Xiamen University for the translation tasks of WMT 17 .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Our systems are based on the Encoder - Decoder framework with attention .", "entities": []}, {"text": "We participated in three directions of shared news translation tasks : English \u2192German and Chinese\u2194English .", "entities": []}, {"text": "We experimented with deep architectures , different segmentation models , synthetic training data and targetbidirectional translation models .", "entities": []}, {"text": "Experiments show that all methods can give substantial improvements .", "entities": []}, {"text": "1 Introduction Neural Machine Translation ( NMT ) ( Cho et al . , 2014 ; Sutskever et al . , 2014 ; Bahdanau et al . , 2015 ) has achieved great success in recent years and obtained state - of - the - art results on various language pairs ( Zhou et al . , 2016 ; Sennrich et al . , 2016a ; Wu et al . , 2016 ) .", "entities": [[3, 5, "TaskName", "Machine Translation"]]}, {"text": "This paper describes the NMT systems of Xiamen University ( XMU ) for the WMT 17 .", "entities": []}, {"text": "We participated in three directions of shared news translation tasks : English\u2192German and Chinese \u2194English .", "entities": []}, {"text": "We use two different NMTs for shared news translation tasks : \u2022MININMT : A deep NMT system ( Zhou et al . , 2016 ; Wu et al . , 2016 ; Wang et al . , 2017 ) with a simple architecture .", "entities": []}, {"text": "The decoder is a stacked Long Short - Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) with 8 layers .", "entities": [[5, 10, "MethodName", "Long Short - Term Memory"], [11, 12, "MethodName", "LSTM"]]}, {"text": "The encoder has two variants .", "entities": []}, {"text": "For English - German translation , we use an interleaved bidirectional encoder with 2 columns .", "entities": []}, {"text": "Each column consists of 4 LSTMs .", "entities": []}, {"text": "For Chinese - English translation , we use a stacked bidirectional encoder with 8 layers .", "entities": []}, {"text": "\u2022DL4MT : Our reimplementation of dl4mttutorial1with minor changes .", "entities": []}, {"text": "We also use a modi\ufb01ed version of AmuNMT C++ decoder2 for decoding .", "entities": []}, {"text": "This system is used in the English - Chinese translation task .", "entities": []}, {"text": "We use both Byte Pair Encoding ( BPE ) ( Sennrich et al . , 2016c ) and mixed word / character segmentation ( Wu et al . , 2016 ) to achieve open - vocabulary translation .", "entities": [[3, 6, "MethodName", "Byte Pair Encoding"], [7, 8, "MethodName", "BPE"]]}, {"text": "Back - translation method ( Sennrich et al . , 2016b ) is applied to make use of monolingual data .", "entities": []}, {"text": "We also use target - bidiretional translation models to alleviate the label bias problem ( Lafferty et al . , 2001 ) .", "entities": []}, {"text": "The remainder of this paper is organized as follows : Section 2 describes the architecture of M ININMT .", "entities": []}, {"text": "Section 3 describes all experimental features used in WMT 17 shared translation tasks .", "entities": []}, {"text": "Section 4 shows the results of our experiments .", "entities": []}, {"text": "Section 5 shows the results of shared translation task .", "entities": []}, {"text": "Finally , we conclude in section 6 . 2", "entities": []}, {"text": "Model Description Deep architectures have recently shown promising results on various language pairs ( Zhou et al . , 2016 ;", "entities": []}, {"text": "Wu et al . , 2016 ; Wang et al . , 2017 ) .", "entities": []}, {"text": "We also experimented with a deep architecture as depicted in Figure 1 .", "entities": []}, {"text": "We use LSTM as the main recurrent unit and residual connections ( He et al . , 2016 ) to help training .", "entities": [[2, 3, "MethodName", "LSTM"]]}, {"text": "Given a source sentence x={x1, ... ,x S}and a target sentence y={y1, ... ,y T } , the encoder maps the source sentence xinto a sequence of annotation vectors { xi } .", "entities": []}, {"text": "The decoder produces 1https://github.com/nyu-dl/ dl4mt - tutorial 2https://github.com/emjotde/amunmt400", "entities": []}, {"text": "Attention ... ... ...", "entities": []}, {"text": "...", "entities": []}, {"text": "Annotation x1 x2 x3", "entities": []}, {"text": "< /s > ...", "entities": []}, {"text": "... ... ...", "entities": []}, {"text": "< /s >", "entities": []}, {"text": "y1 y2 y3Softmaxy1 y2 y3 < /s", "entities": []}, {"text": "> Figure 1 : The architecture of our deep NMT system , which is inspired by Deep - Att ( Zhou et", "entities": []}, {"text": "al . , 2016 ) and GNMT ( Wu et al . , 2016 ) .", "entities": []}, {"text": "Both the encoder and decoder adopt LSTM as its main recurrent unit .", "entities": [[6, 7, "MethodName", "LSTM"]]}, {"text": "We also use residual connections ( He et al . , 2016 ) to help training , but here we omit it for clarity .", "entities": []}, {"text": "We use black lines to denote input connections while use blue lines to denote recurrent connections .", "entities": []}, {"text": "translationytgiven the source annotation vectors { xi}and target history y < t. 2.1 Encoder 2.1.1 Interleaved Bidirectional Encoder The interleaved bidirectional encoder was introduced by ( Zhou et", "entities": []}, {"text": "al . , 2016 ) , which is also used in ( Wang et al . , 2017 ) .", "entities": []}, {"text": "Like ( Zhou et al . , 2016 ) , our interleaved bidirectional encoder consists of two columns .", "entities": []}, {"text": "In interleaved bidirectional encoder , the LSTMs in adjacent layers run in opposite directions : \u2212 \u2192xi t = LSTMf i(\u2212 \u2192xi\u22121 t,\u2212 \u2192si t+(\u22121)i ) ( 1 ) \u2190 \u2212xi t = LSTMb i(\u2190 \u2212xi\u22121", "entities": []}, {"text": "t,\u2190 \u2212si t+(\u22121)i+1 ) ( 2 ) Herex0 t\u2208Reis the word embedding of word xt ,", "entities": []}, {"text": "xi t\u2208Rhis", "entities": []}, {"text": "the output of LSTM unit and si t= ( ci t , mi t)denotes the memory and hidden state of LSTM .", "entities": [[3, 4, "MethodName", "LSTM"], [20, 21, "MethodName", "LSTM"]]}, {"text": "We set both eandhto 512 in all our experiments .", "entities": []}, {"text": "The annotation vectors xi\u2208R2hare obtained by concatenating the \ufb01nal output\u2212 \u2192xLenc and\u2190 \u2212xLencof two encoder columns .", "entities": []}, {"text": "In our experiments , we set Lenc= 4 . 2.1.2 Stacked Bidirectional Encoder To better exploit source representation , we adopt a stacked bidirectional encoder .", "entities": []}, {"text": "As shown in Figure 1 , all layers in the encoder are bidirectional .", "entities": []}, {"text": "Thecalculation is described as follows :", "entities": []}, {"text": "\u2212 \u2192xi = LSTMf i(xi\u22121", "entities": []}, {"text": "t,\u2212 \u2192si t\u22121 ) ( 3 ) \u2190 \u2212xi = LSTMb i(xi\u22121 t,\u2190 \u2212si t+1 ) ( 4 ) xi=", "entities": []}, {"text": "[ \u2212 \u2192xiT;\u2190 \u2212xiT]T(5 ) To reduce parameters , we reduce the dimension of hidden units from htoh/2so that xi\u2208Rh .", "entities": []}, {"text": "The annotation vectors are taken from the output xLenc of top LSTM layer .", "entities": [[11, 12, "MethodName", "LSTM"]]}, {"text": "In our experiments , Lencis set to 8 .", "entities": []}, {"text": "2.2 Decoder The decoder network is similar to GNMT ( Wu et al . , 2016 ) .", "entities": []}, {"text": "At each time - step t , lety0 t\u22121\u2208Re denotes the word embedding of yt\u22121andy1 t\u22121\u2208", "entities": []}, {"text": "Rhdenotes the output of bottom LSTM from previous time - step .", "entities": [[5, 6, "MethodName", "LSTM"]]}, {"text": "The attention network calculates the context vector atas the weighted sum of source annotation vectors : at = S / summationdisplay i=1\u03b1t , i\u00b7xi ( 6 ) Different from GNMT ( Wu et al . , 2016 ) , we use the concatenation of y0 t\u22121andy1 t\u22121as the query vector for attention network , as described follows :", "entities": []}, {"text": "ht= [ y0 t\u22121T;y1 t\u22121T]T(7 ) et , i = vT atanh ( Waht+Uaxi ) ( 8) \u03b1t , i = exp(et , i)/summationtextS j=1exp(et , j)(9)401", "entities": []}, {"text": "This approach is also used in ( Wang et al . , 2017 ) .", "entities": []}, {"text": "The context vector atis then fed to all decoder LSTMs .", "entities": [[3, 4, "DatasetName", "atis"]]}, {"text": "The probability of the next word ytis simply modeled using a softmax layer on the output of top LSTM : p(yt|x , y < t ) = softmax ( yt , yLdec t ) ( 10 ) We setLdecto 8 in all our experiments .", "entities": [[11, 12, "MethodName", "softmax"], [18, 19, "MethodName", "LSTM"], [27, 28, "MethodName", "softmax"]]}, {"text": "3 Experimental Features 3.1 Segmentation Approaches To enable open - vocabulary , we use two approaches : BPE and mixed word / character segmentation .", "entities": [[17, 18, "MethodName", "BPE"]]}, {"text": "In most of our experiments , we use BPE3(Sennrich et al . , 2016c ) with 50 K operations .", "entities": []}, {"text": "In our preliminary experiments , we found that BPE works better than UNK replacement techniques .", "entities": [[8, 9, "MethodName", "BPE"]]}, {"text": "For English - Chinese translation task , we apply mixed word / character model ( Wu et al . , 2016 ) to Chinese sentences .", "entities": []}, {"text": "We keep the most frequent 50 K words and split other words into characters .", "entities": []}, {"text": "Unlike ( Wu et al . , 2016 ) , we do not add any pre\ufb01xes or suf\ufb01xes to the segmented Chinese characters .", "entities": []}, {"text": "In post - processing step , we simply remove all the spaces .", "entities": []}, {"text": "3.2 Synthetic Training Data We apply back - translation ( Sennrich et al . , 2016b ) method to use monolingual data .", "entities": []}, {"text": "For EnglishGerman and Chinese - English translation , we sample monolingual data from the NewsCrawl2016 corpora .", "entities": []}, {"text": "For English - Chinese translation , we sample monolingual data from the XinhuaNet2011 corpus .", "entities": []}, {"text": "3.3 Target - bidirectional Translation For Chinese - English translation , we also use a target - bidirectional model ( Liu et al . , 2016 ;", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "Sennrich et al . , 2016a ) to rescore the hypotheses .", "entities": []}, {"text": "To train a target - bidirectional model , we reverse the target side of bilingual pairs from left - to - right ( L2R ) to right - to - left ( R2L ) .", "entities": []}, {"text": "We \ufb01rst output 50 candidates from the ensemble of 4 L2R models .", "entities": []}, {"text": "Then we rescore candidates by interpolating L2R score and R2L score with uniform weights .", "entities": []}, {"text": "3https://github.com/rsennrich/ subword - nmt3.4 Training For all our models , we adopt Adam ( Kingma and Ba , 2015 ) ( \u03b21= 0.9,\u03b22= 0.999 and /epsilon1= 1\u00d7 10\u22128 ) as the optimizer .", "entities": [[12, 13, "MethodName", "Adam"], [31, 32, "HyperparameterName", "optimizer"]]}, {"text": "The learning rate is set to5\u00d710\u22124 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "We gradually halve the learning rate during the training process .", "entities": [[4, 6, "HyperparameterName", "learning rate"]]}, {"text": "As a common way to train RNNs , we clip the norm of gradient to a prede\ufb01ned value 5.0 .", "entities": []}, {"text": "The batch size is 128 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "We use dropout ( Srivastava et al . , 2014 ) to avoid over\ufb01tting with a keep probability of 0.8 . 4 Results 4.1 Results on English - German Translation System Test ( BLEU )", "entities": [[29, 30, "TaskName", "Translation"], [33, 34, "MetricName", "BLEU"]]}, {"text": "Baseline 25.7", "entities": []}, {"text": "+ Synthetic 26.1 + Ensemble 26.7 Table 1 : English - German translation results on newstest2017 .", "entities": []}, {"text": "Table 1 show the results of English - German Translation .", "entities": [[9, 10, "TaskName", "Translation"]]}, {"text": "The baseline system is trained on preprocessed parallel data4 .", "entities": []}, {"text": "For synthetic data , we randomly sample 10 M German sentences from NewsCrawl2016 and translate them back to English using an German - English model .", "entities": []}, {"text": "However , we found random sampling do not work well .", "entities": []}, {"text": "As a result , for Chinese - English translation , we select monolingual data according to development set .", "entities": []}, {"text": "We \ufb01rst train one baseline model and continue to train 4 models on synthetic data with different shuf\ufb02es .", "entities": []}, {"text": "Next we ensemble 4 models and get the \ufb01nal results .", "entities": []}, {"text": "We found this approach do not lead to substantial improvements .", "entities": []}, {"text": "4.2 Results on Chinese - English Translation System Test ( BLEU ) Baseline 23.1", "entities": [[6, 7, "TaskName", "Translation"], [10, 11, "MetricName", "BLEU"]]}, {"text": "+ Synthetic 23.7 + Ensemble 25.3 + R2L reranking 26.0 Table 2 : Chinese - English translation results on newstest2017 .", "entities": []}, {"text": "We use all training data ( CWMT Corpus , UN Parallel Corpus and News Commentary ) to train a 4http://data.statmt.org/wmt17/ translation - task / preprocessed / de - en/402", "entities": []}, {"text": "baseline system .", "entities": []}, {"text": "The Chinese sentences are segmented using Stanford Segmenter5 .", "entities": []}, {"text": "For English sentences , we use the moses tokenizer6 .", "entities": []}, {"text": "We \ufb01lter bad sentences according to the alignment score obtained by fast - align toolkit7and remove duplications in the training data .", "entities": []}, {"text": "The preprocessed training data consists of 19 M bilingual pairs .", "entities": []}, {"text": "As noted earlier , the monolingual data is selected using newsdev2017 .", "entities": []}, {"text": "We \ufb01rst train 4 L2R models and one R2L model on training data , then we \ufb01netune our model on a mixture of 2.5 M synthetic bilingual pairs and 2.5 M bilingual pairs sampled from CWMT corpus .", "entities": []}, {"text": "As shown in Table 2 , we obtained +1.6 BLEU score when ensembling 4 models .", "entities": [[9, 11, "MetricName", "BLEU score"]]}, {"text": "When rescoring with one R2L model , we further gain +0.7 BLEU score .", "entities": [[11, 13, "MetricName", "BLEU score"]]}, {"text": "4.3 Results on English - Chinese Translation System Test ( BLEU ) Baseline 30.4", "entities": [[6, 7, "TaskName", "Translation"], [10, 11, "MetricName", "BLEU"]]}, {"text": "+ Synthetic 34.3 + Ensemble 35.8 Table 3 : English - Chinese translation results on newstest2017 .", "entities": []}, {"text": "Table 3 show the results of English - Chinese Translation .", "entities": [[9, 10, "TaskName", "Translation"]]}, {"text": "We use our reimplementation of DL4MT to train English - Chinese models on CWMT and UN parallel corpus .", "entities": []}, {"text": "The preprocessing steps , including word segmentation , tokenization , and sentence \ufb01ltering , are almost the same as Section 4.2 , except that we limited the vocabulary size to 50 K and split all target side OOVs into characters .", "entities": []}, {"text": "For synthetic parallel data , we use SRILM8to train a 5 - gram KN language model on XinhuaNet2011 and select 2.5 M sentences from XinhuaNet2011 according to their perplexities .", "entities": []}, {"text": "We obtained +3.9 BLEU score when tuning the single best model on a mixture of 2.5 M synthetic bilingual pairs and 2.5 M bilingual pairs selected from CWMT parallel data randomly .", "entities": [[3, 5, "MetricName", "BLEU score"]]}, {"text": "We further gain +1.5 BLEU score when ensembling 4 models .", "entities": [[4, 6, "MetricName", "BLEU score"]]}, {"text": "5https://nlp.stanford.edu/software/ segmenter.shtml 6http://statmt.org/moses/ 7https://github.com/clab/fast_align 8http://www.speech.sri.com/projects/ srilm/5 Shared Task Results Table 4 shows the ranking of our submitted systems at the WMT17 shared news translation task .", "entities": []}, {"text": "Our submissions are ranked ( tied ) \ufb01rst for 2 out of 3 translation directions in which we participated : EN\u2194ZH .", "entities": []}, {"text": "Direction BLEU Rank Human Rank EN\u2192DE 4 2 - 9 of 16 ZH\u2192EN 2 1 - 3 of 16 EN\u2192ZH 2 1 - 3 of 11 Table 4 : Automatic ( BLEU ) and human ranking of our submitted systems at WMT17 shared news translation task .", "entities": [[1, 2, "MetricName", "BLEU"], [31, 32, "MetricName", "BLEU"]]}, {"text": "6 Conclusion We describe XMU \u2019s neural machine translation systems for the WMT 17 shared news translation tasks .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "All our models perform quite well on all tasks we participated .", "entities": []}, {"text": "Experiments also show the effectiveness of all features we used .", "entities": []}, {"text": "Acknowledgments This work was supported by the Natural Science Foundation of China ( Grant No . 61573294 , 61303082 , 61672440 ) , the Ph.D. Programs Foundation of Ministry of Education of China ( Grant No . 20130121110040 ) , the Foundation of the State Language Commission of China ( Grant No . WT135 - 10 ) and the Natural Science Foundation of Fujian Province ( Grant No . 2016J05161 ) .", "entities": []}, {"text": "References Dzmitry Bahdanau , KyungHyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of ICLR .", "entities": []}, {"text": "Kyunghyun Cho , Bart van Merrienboer , Caglar Gulcehre , Dzmitry Bahdanau , Fethi Bougares , Holger Schwenk , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Learning phrase representations using rnn encoder \u2013 decoder for statistical machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) .", "entities": []}, {"text": "Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . 2016 .", "entities": []}, {"text": "Deep residual learning for image recognition .", "entities": [[4, 6, "TaskName", "image recognition"]]}, {"text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770\u2013778.403", "entities": []}, {"text": "Sepp Hochreiter and Jurgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural Computation , pages 1735\u20131780 .", "entities": []}, {"text": "Diederik Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In Proceedings of ICLR .", "entities": []}, {"text": "John Lafferty , Andrew McCallum , Fernando Pereira , et al . 2001 .", "entities": []}, {"text": "Conditional random \ufb01elds : Probabilistic models for segmenting and labeling sequence data .", "entities": []}, {"text": "In Proceedings of the eighteenth international conference on machine learning , ICML , volume 1 , pages 282\u2013289 .", "entities": []}, {"text": "Lemao Liu , Masao Utiyama , Andrew Finch , and Eiichiro Sumita . 2016 .", "entities": []}, {"text": "Agreement on targetbidirectional neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of NAACL - HLT , pages 411\u2013416 .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016a .", "entities": []}, {"text": "Edinburgh neural machine translation systems for wmt 16 .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1606.02891 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016b .", "entities": []}, {"text": "Improving neural machine translation models with monolingual data .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceddings of ACL .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016c .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "Nitish Srivastava , Geoffrey Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov .", "entities": [[13, 14, "DatasetName", "Ruslan"]]}, {"text": "2014 .", "entities": []}, {"text": "Dropout : A simple way to prevent neural networks from over\ufb01tting .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "The Journal of Machine Learning Research , 15(1):1929\u20131958 .", "entities": []}, {"text": "Ilya Sutskever , Oriol Vinyals , and Quoc V Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "In Advances in neural information processing systems , pages 3104\u20133112 .", "entities": []}, {"text": "Mingxuan Wang , Zhengdong Lu , Jie Zhou , and Qun Liu . 2017 .", "entities": []}, {"text": "Deep Neural Machine Translation with Linear Associative Unit .", "entities": [[2, 4, "TaskName", "Machine Translation"]]}, {"text": "arXiv preprint arXiv:1705.00861 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , et al . 2016 .", "entities": []}, {"text": "Google \u2019s neural machine translation system : Bridging the gap between human and machine translation .", "entities": [[0, 1, "DatasetName", "Google"], [3, 5, "TaskName", "machine translation"], [13, 15, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:1609.08144 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jie Zhou , Ying Cao , Xuguang Wang , Peng Li , and Wei Xu .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Deep recurrent models with fast - forward connections for neural machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 4:371\u2013383.404", "entities": []}]