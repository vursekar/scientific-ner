[{"text": "Findings of the Association for Computational Linguistics : ACL 2022 , pages 3993 - 4007 May 22 - 27 , 2022 c", "entities": []}, {"text": "2022 Association for Computational Linguistics Interpreting the Robustness of Neural NLP Models to Textual Perturbations Yunxiang Zhang1 , Liangming Pan2 , Samson Tan2 , Min - Yen Kan2 1Wangxuan Institute of Computer Technology , Peking University 2School of Computing , National University of Singapore yx.zhang@pku.edu.cn , liangmingpan@u.nus.edu , { samson.tmr,kanmy}@comp.nus.edu.sg Abstract Modern Natural Language Processing ( NLP ) models are known to be sensitive to input perturbations and their performance can decrease when applied to real - world , noisy data .", "entities": []}, {"text": "However , it is still unclear why models are less robust to some perturbations than others .", "entities": []}, {"text": "In this work , we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation ( robustness ) can be explained by the learnability of the perturbation ( defined as how well the model learns to identify the perturbation with a small amount of evidence ) .", "entities": []}, {"text": "We further give a causal justification for the learnability metric .", "entities": []}, {"text": "We conduct extensive experiments with four prominent NLP models \u2014 TextRNN , BERT , RoBERTa and XLNet \u2014 over eight types of textual perturbations on three datasets .", "entities": [[12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "RoBERTa"], [16, 17, "MethodName", "XLNet"]]}, {"text": "We show that a model which is better at identifying a perturbation ( higher learnability ) becomes worse at ignoring such a perturbation at test time ( lower robustness ) , providing empirical support for our hypothesis .", "entities": []}, {"text": "1 Introduction Despite the success of deep neural models on many Natural Language Processing ( NLP ) tasks ( Liu et al . , 2016 ; Devlin et", "entities": []}, {"text": "al . , 2019 ; Liu et al . , 2019b ) , recent work has discovered that these models are not robust to noisy input from the real world and thus their performance will decrease ( Prabhakaran et al . , 2019 ; Niu et al . , 2020 ; Ribeiro et al . , 2020 ; Moradi and Samwald , 2021 ) .", "entities": []}, {"text": "A reliable NLP system should not be easily fooled by slight noise in the text .", "entities": []}, {"text": "Although a wide range of evaluation approaches for robust NLP models have been proposed ( Ribeiro et al . , 2020 ; Morris et al . , 2020 ; Goel et al . , 2021 ; Wang et al . , 2021 ) , few attempts have been made to understand these benchmark results .", "entities": []}, {"text": "Given the difference of robustness between models and perturbations , it is a natural question why models are more sensitive to some perturbations than others .", "entities": []}, {"text": "It is crucial to avoidover - sensitivity to input perturbations , and understanding why it happens is useful for revealing the weaknesses of current models and designing more robust training methods .", "entities": []}, {"text": "To the best of our knowledge , a quantitative measure to interpret the robustness of NLP models to textual perturbations has yet to be proposed .", "entities": []}, {"text": "To improve the robustness under perturbation , it is common practice to leverage data augmentation ( Li and Specia , 2019 ; Min et al . , 2020 ; Tan and Joty , 2021 ) .", "entities": [[13, 15, "TaskName", "data augmentation"]]}, {"text": "Similarly , how much data augmentation through the perturbation improves model robustness varies between models and perturbations .", "entities": [[4, 6, "TaskName", "data augmentation"]]}, {"text": "In this work , we aim to investigate two Research Questions ( RQ ): \u2022RQ1 : Why are NLP models less robust to some perturbations than others ?", "entities": []}, {"text": "\u2022RQ2 : Why does data augmentation work better at improving the model robustness to some perturbations than others ?", "entities": [[4, 6, "TaskName", "data augmentation"]]}, {"text": "We test a hypothesis for RQ1 that the extent to which a model is affected by an unseen textual perturbation ( robustness ) can be explained by the learnability of the perturbation ( defined as how well the model learns to identify the perturbation with a small amount of evidence ) .", "entities": []}, {"text": "We also validate another hypothesis for RQ2 that the learnability metric is predictive of the improvement on robust performance brought by data augmentation along a perturbation .", "entities": [[21, 23, "TaskName", "data augmentation"]]}, {"text": "Our proposed learnability is inspired by the concepts of Randomized Controlled Trial ( RCT ) and Average Treatment Effect ( ATE ) from Causal Inference ( Rubin , 1974 ; Holland , 1986 ) .", "entities": [[23, 25, "MethodName", "Causal Inference"]]}, {"text": "Estimation of perturbation learnability for a model consists of three steps : \u2460randomly labelling a dataset , \u2461perturbing examples of a particular pseudo class with probabilities , and \u2462using ATE to measure the ease with which the model learns the perturbation .", "entities": []}, {"text": "The core intuition for our method is to frame an RCT as a perturbation identification task and formalize the notion of learnability3993", "entities": []}, {"text": "Exp No . Measurement Label Perturbation Training Examples Test Examples 0", "entities": [[10, 11, "DatasetName", "0"]]}, {"text": "Standard original l\u2208\u2205 ( xi,0),(xj,1 ) ( xi,0),(xj,1 ) 1 Robustness original l\u2208{0,1 } ( xi,0),(xj,1 ) ( x\u2217 i,0),(x\u2217 j,1 ) 2 Data Augmentation original l\u2208{0,1}(xi,0),(xj,1 ) ( x\u2217 i,0),(x\u2217 j,1)(x\u2217 i,0),(x\u2217 j,1 ) 3Learnabilityrandom l\u2032\u2208{1\u2032 } ( xj,0\u2032),(x\u2217 i,1\u2032 ) ( x\u2217 i,1\u2032 ) 4 random l\u2032\u2208{1\u2032 } ( xj,0\u2032),(x\u2217 i,1\u2032 ) ( xi,1\u2032 ) Table 1 : Example experiment settings for measuring learnability , robustness and improvement by data augmentation .", "entities": [[23, 25, "TaskName", "Data Augmentation"], [71, 73, "TaskName", "data augmentation"]]}, {"text": "We perturb an example if its label falls in the set of label(s ) in \u201c Perturbation \u201d column .", "entities": []}, {"text": "\u2205means no perturbation at all .", "entities": []}, {"text": "Training / test examples are the expected input data , assuming we have only one negative ( xi,0)and positive ( xj,1)example in our original training / test set .", "entities": []}, {"text": "l\u2032is a random label and x\u2217is a perturbed example .", "entities": []}, {"text": "as a causal estimand based on ATE .", "entities": []}, {"text": "We conduct extensive experiments on four neural NLP models with eight different perturbations across three datasets and find strong evidence for our two hypotheses .", "entities": []}, {"text": "Combining these two findings , we further show that data augmentation is only more effective at improving robustness against perturbations that a model is more sensitive to , contributing to the interpretation of robustness and data augmentation .", "entities": [[9, 11, "TaskName", "data augmentation"], [35, 37, "TaskName", "data augmentation"]]}, {"text": "Learnability provides a clean setup for analysis of the model behaviour under perturbation , which contributes better model interpretation as well .", "entities": []}, {"text": "Contribution .", "entities": []}, {"text": "This work provides an empirical explanation for why NLP models are less robust to some perturbations than others .", "entities": []}, {"text": "The key to this question is perturbation learnability , which is grounded in the causality framework .", "entities": []}, {"text": "We show a statistically significant inverse correlation between learnability and robustness .", "entities": []}, {"text": "2 Setup and Terminology As a pilot study , we consider the task of binary text classification .", "entities": [[15, 17, "TaskName", "text classification"]]}, {"text": "The training set is denoted as Dtrain={(x1 , l1 ) , ... , ( xn , ln ) } , where xiis the i - th example and li\u2208{0,1}is the corresponding label .", "entities": []}, {"text": "We fit a model f\u2236(x;\u03b8)\u21a6{0,1}with parameters \u03b8on the training data .", "entities": []}, {"text": "A textual perturbation is a transformation g\u2236(x;\u03b2)\u2192x\u2217that injects a specific type of noise into an example x with parameters \u03b2and the resulting perturbed example is x\u2217.", "entities": []}, {"text": "We design several experiment settings ( Table 1 ) to answer our research questions .", "entities": []}, {"text": "Experiment 0 in Table 1 is the standard learning setup , where we train and evaluate a model on the original dataset .", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "Below we detail other experiment settings.2.1 Definitions Robustness .", "entities": []}, {"text": "We apply the perturbations to test examples and measure the robustness of model to said perturbations as the decrease in accuracy .", "entities": [[20, 21, "MetricName", "accuracy"]]}, {"text": "In Table 1 , Experiment 1 is related to robustness measurement , where we train a model on unperturbed dataset and test it on perturbed examples .", "entities": []}, {"text": "We denote the test accuracy of a model f(\u22c5)on examples perturbed by g(\u22c5)in Experiment 1 as A1(f , g , D\u2217 test ) .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "Similarly , the test accuracy in Experiment 0 is A0(f , D test ) .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 8, "DatasetName", "0"]]}, {"text": "Consequently , the robustness is calculated as the difference of test accuracies : robustness ( f , g , D)=A1(f , g , D\u2217 test ) \u2212A0(f , D test).(1 ) Models usually suffer a performance drop when encountering perturbations , therefore the robustness is usually negative , where lower values indicate decreased robustness .", "entities": []}, {"text": "Improvement by Data Augmentation ( Post Augmentation \u2206).To improve robust accuracy ( Tu et al . , 2020 ) ( i.e. , accuracy on the perturbed test set ) , it is a common practice to leverage data augmentation ( Li and Specia , 2019 ; Min et al . , 2020 ; Tan and Joty , 2021 ) .", "entities": [[2, 4, "TaskName", "Data Augmentation"], [10, 11, "MetricName", "accuracy"], [22, 23, "MetricName", "accuracy"], [37, 39, "TaskName", "data augmentation"]]}, {"text": "We simulate the data augmentation process by appending perturbed data to the training set ( Experiment 2 of Table 1 ) .", "entities": [[3, 5, "TaskName", "data augmentation"]]}, {"text": "We calculate the improvement on performance after data augmentation as the difference of test accuracies : \u2206post_aug(f , g , D)=A2(f , g , D\u2217 test ) \u2212A1(f , g , D\u2217 test).(2 ) whereA2(f , g , D\u2217 test)denotes the test accuracy of Experiment 2 . \u2206post_aug is the higher the better.3994", "entities": [[7, 9, "TaskName", "data augmentation"], [42, 43, "MetricName", "accuracy"]]}, {"text": "Learnability .", "entities": []}, {"text": "We want to compare perturbations in terms of how well the model learns to identify them with a small amount of evidence .", "entities": []}, {"text": "We cast learnability estimation as a perturbation classification task , where a model is trained to identify the perturbation in an example .", "entities": []}, {"text": "We define that the learnability estimation consists of three steps , namely \u2460assigning random labels , \u2461perturbing with probabilities , and\u2462estimating model performance .", "entities": []}, {"text": "Below we introduce the procedure and intuition for each step .", "entities": []}, {"text": "This estimation framework is further grounded in concepts from the causality literature in Section 3 , which justifies our motivations .", "entities": []}, {"text": "We summarize our estimation approach formally in Algorithm 1 ( Appendix A ) .", "entities": []}, {"text": "\u2460Assigning Random Labels .", "entities": []}, {"text": "We randomly assign pseudo labels to each training example regardless of its original label .", "entities": []}, {"text": "Each data point has equal probability of being assigned to positive ( l\u2032=1 ) or negative ( l\u2032=0 ) pseudo label .", "entities": []}, {"text": "This results in a randomly labeled dataset D\u2032 train={(x1;l\u2032 1 ) , ... , ( xn , l\u2032 n ) } , where L\u2032\u223c Bernoulli ( 1,0.5 ) .", "entities": []}, {"text": "In this way , we ensure that there is no difference between the two pseudo groups since the data are randomly split .", "entities": []}, {"text": "\u2461Perturbing with Probabilities .", "entities": []}, {"text": "We apply the perturbation g(\u22c5)to each training example in one of the pseudo groups ( e.g. , l\u2032=1 in Algorithm 1)1 .", "entities": []}, {"text": "In this way , we create a correlation between the existence of perturbation and label ( i.e. , the perturbation occurrence is predictive of the label ) .", "entities": []}, {"text": "We control the perturbation probability p\u2208[0,1 ] , i.e. , an example has a specific probability pof being perturbed .", "entities": []}, {"text": "This results in a perturbed training set D\u2032\u2217 train={(x\u2217 1 , l\u2032 1 ) , ... , ( x\u2217 n , l\u2032 n ) } , where the perturbed example x\u2217 iis : Z\u223cU(0,1),\u2200i\u2208{1,2 , ... , n } x\u2217", "entities": []}, {"text": "i=\u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9g(xi)l\u2032 i=1\u2227z < p , xi otherwise.(3 )", "entities": []}, {"text": "HereZis a random variable drawn from a uniform distribution U(0,1 ) .", "entities": []}, {"text": "Due to randomization in the formal step , now the only difference between the two pseudo groups is the occurrence of perturbation .", "entities": []}, {"text": "\u2462Estimating Model Performance .", "entities": []}, {"text": "We train a model on the randomly labeled dataset with per1Because the training data is randomly split into two pseudo groups , applying perturbations to any one of the groups should yield same result .", "entities": []}, {"text": "We assume that we always perturb into the first group ( l\u2032=1 ) hereafter.turbed examples .", "entities": []}, {"text": "Since the only difference between the two pseudo groups is the existence of the perturbation , the model is trained to identify the perturbation .", "entities": []}, {"text": "The original test examples Dtestare also assigned random labels and become D\u2032 test .", "entities": []}, {"text": "We perturb all of the test examples in one pseudo group ( e.g. , l\u2032=1 , as in step 2.1 ) to produce a perturbed test set D\u2032\u2217 test .", "entities": []}, {"text": "Finally , the perturbation learnability is calculated as the difference of accuracies on D\u2032\u2217 testandD\u2032 test , which indicates how much the model learns from the perturbation \u2019s co - occurrence with pseudo label : learnability ( f , g , p , D ) = A3(f , g , p , D\u2032\u2217 test ) \u2212A4(f , g , p , D\u2032 test).(4 ) A4(f , g , p , D\u2032\u2217 test)andA3(f , g , p , D\u2032 test)are accuracies measured by Experiment 4 and 3 of Table 1 , respectively .", "entities": []}, {"text": "We observe that the learnability depends on perturbation probability p.", "entities": []}, {"text": "For each model \u2013 perturbation pair , we obtain multiple learnability estimates by varying the perturbation probability ( Figure 3 ) .", "entities": []}, {"text": "However , we expect that learnability of the perturbation ( as a concept ) should be independent of perturbation probability .", "entities": []}, {"text": "To this end , we use thelogAUC ( area under the curve in log scale ) of thep\u2212learnability curve ( Figure 3 ) , termed as \u201c average learnability \u201d , which summarizes the overall learnability across different perturbation probabilitiesp1 , ... , p t : avg_learnability ( f , g , D)\u2236=logAUC({(pi , learnability ( f , g , p i , D ) ) \u2223i\u2208{1,2 , ... , t}}).(5 )", "entities": []}, {"text": "We use logAUC rather than AUC because we empirically find that the learnability varies substantially between perturbations when pis small , and a log scale can better capture this nuance .", "entities": [[5, 6, "MetricName", "AUC"]]}, {"text": "We also introduce learnability at a specific perturbation probability ( Learnability @ p ) as an alternate summary metric and provide a comparison of this metric against logAUC in Appendix D. 2.2 Hypothesis With the above - defined terminologies , we propose hypotheses for RQ1 and RQ2 in Section 1 , respectively .", "entities": []}, {"text": "Hypothesis 1 ( H1 ): A model for which a perturbation is more learnable is less robust against the same perturbation at the test time.3995", "entities": []}, {"text": "This is notobvious because the model encounters this perturbation during training in learnability estimation while they do not in robustness measurement .", "entities": []}, {"text": "Hypothesis 2 ( H2 ): A model for which a perturbation is more learnable experiences bigger robustness gains with data augmentation along such a perturbation .", "entities": [[19, 21, "TaskName", "data augmentation"]]}, {"text": "We validate both Hypotheses 1 and 2 with experiments on several perturbations and models described in Section 4.1 and 4.2 .", "entities": []}, {"text": "3 A Causal View on Perturbation Learnability In Section 2.1 , we introduce the term \u201c learnability \u201d in an intuitive way .", "entities": []}, {"text": "Now we map it to a formal , quantitative measure in standard statistical frameworks .", "entities": []}, {"text": "Learnability is actually motivated by concepts from the causality literature .", "entities": []}, {"text": "We provide a brief introduction to basic concepts of causal inference in Appendix B.", "entities": [[9, 11, "MethodName", "causal inference"]]}, {"text": "In fact , learnability is the causal effect of perturbation on models , which is often difficult to measure due to the confounding latent features .", "entities": []}, {"text": "In the language of causality , this is \u201c correlation is not causation \u201d .", "entities": []}, {"text": "Causality provides insight on how to fully decouple the effect of perturbation and other latent features .", "entities": []}, {"text": "We introduce the causal motivations for step 2.1 and 2.1 of learnability estimation in the following Section 3.1 and 3.2 , respectively .", "entities": []}, {"text": "3.1 A Causal Explanation for Random Label Assignment Natural noise ( simulated by perturbations in this work ) usually co - occurs with latent features in an example .", "entities": []}, {"text": "If we did not assign random labels and simply perturbed one of the original groups , there would be confounding latent features that would prevent us from estimating the causal effect of the perturbation .", "entities": []}, {"text": "Figure 1a illustrates this scenario .", "entities": []}, {"text": "Both perturbation Pand latent feature Tmay affect the outcome Y,2while the latent feature is predictive of label L. Since we make the perturbation P on examples with the same label , Pis decided by L.", "entities": []}, {"text": "It therefore follows that Tis a confounder of the effect of PonY , resulting in non - causal association flowing along the path P\u2190L\u2190T\u2192Y.", "entities": []}, {"text": "However , if we do randomize the labels , Pno longer has any causal parents ( i.e. , incoming edges ) ( Figure 1b ) .", "entities": []}, {"text": "This is because perturbation is purely 2Yis later defined in Section 3.2 0.75 0.50 0.25 0.000.250.500.751.00sensitivity @ p dataset = IMDB | model", "entities": [[20, 21, "DatasetName", "IMDB"]]}, {"text": "= T extRNN   dataset = IMDB | model = BERT   dataset = IMDB | model = RoBERT a   dataset = IMDB | model = XLNet 0.75 0.50 0.25 0.000.250.500.751.00sensitivity @ p dataset = YELP | model", "entities": [[6, 7, "DatasetName", "IMDB"], [10, 11, "MethodName", "BERT"], [14, 15, "DatasetName", "IMDB"], [23, 24, "DatasetName", "IMDB"], [27, 28, "MethodName", "XLNet"]]}, {"text": "= T extRNN   dataset = YELP | model = BERT   dataset = YELP | model = RoBERT a   dataset = YELP | model = XLNet 0.001 0.005 0.01 0.02 0.05 0.1 0.5 1.0 injection probability p0.75 0.50 0.25 0.000.250.500.751.00sensitivity @ p dataset = QQP | model = T extRNN 0.001 0.005 0.01 0.02 0.05 0.1 0.5 1.0 injection probability p dataset = QQP | model = BERT 0.001 0.005 0.01 0.02 0.05 0.1 0.5 1.0 injection probability p dataset = QQP | model = RoBERT a 0.001 0.005 0.01 0.02 0.05 0.1 0.5 1.0 injection probability p dataset = QQP | model", "entities": [[10, 11, "MethodName", "BERT"], [27, 28, "MethodName", "XLNet"], [46, 47, "DatasetName", "QQP"], [65, 66, "DatasetName", "QQP"], [69, 70, "MethodName", "BERT"], [83, 84, "DatasetName", "QQP"], [102, 103, "DatasetName", "QQP"]]}, {"text": "= XLNetspurious feature duplicate_punctuations", "entities": []}, {"text": "butter_fingers_perturbation shuffle_word random_upper_transformation insert_abbreviation whitespace_perturbation visual_attack_letters leet_lettersFigure 3 : Learnability of eight perturbations for four NLP models on three datasets , as a function of perturbation probability .", "entities": []}, {"text": "the language of causality , this is \u201c correlation is not 288 causation \" .", "entities": []}, {"text": "Causality provides insight on how to 289 fully decouple the effect of perturbation and other 290 latent features .", "entities": []}, {"text": "We introduce the causal motiva- 291 tions for step 1 and 3 of learnability estimation in 292 the following Section 3.1 and 3.2 respectively .", "entities": []}, {"text": "293 3.1 A Causal Explanation for Random Label 294 Assignment 295 Natural noise ( simulated by perturbations in this 296 work ) usually co - occurs with latent features in an 297 example .", "entities": []}, {"text": "If we did not assign random labels and 298 simply perturbed one of the original groups , there 299 would be confounding latent features that would 300 prevent us from estimating the causal effect of the 301 perturbation .", "entities": []}, {"text": "Figure 4a illustrates this scenario .", "entities": []}, {"text": "302", "entities": []}, {"text": "Both perturbation Pand latent feature Tmay affect 303 the outcome Y,3while the latent feature is predic- 304 tive of label L. Since we make perturbation Pon 305 examples with the same label , Pis decided by L. 306 It therefore follows that Tis a confounder of the ef-", "entities": []}, {"text": "307 fect of PonY , resulting in non - causal association 308 flowing along the path P\u2190L\u2190T\u2192Y.", "entities": []}, {"text": "How- 309 ever , if we do randomize the labels , Pno longer 310 has any causal parents ( i.e. , incoming edges ) ( Fig- 311 ure 4b ) .", "entities": []}, {"text": "This is because perturbation is purely ran- 312 dom .", "entities": []}, {"text": "Without the path represented by P\u2190L , all 313 of the association that flows from PtoYis causal .", "entities": []}, {"text": "314 As a result , we can directly calculate the causal 315 effect from the observed outcomes ( Section 3.2 ) .", "entities": []}, {"text": "316 Our randomization experiments allow us to dis- 317 3Yis later defined in Section 3.2P YT L causal associationconfounding association ( a ) Before randomization .", "entities": []}, {"text": "P YT L causal association ( b ) After randomization .", "entities": []}, {"text": "Figure 4 : Causal graph explanation for decoupling perturbation and latent feature with randomization .", "entities": []}, {"text": "Pis the perturbation and Tis the latent feature .", "entities": []}, {"text": "Lis the original label and Yis the correctness of the predicted label .", "entities": []}, {"text": "5(a )", "entities": []}, {"text": "Before randomization .", "entities": []}, {"text": "0.75 0.50 0.25 0.000.250.500.751.00sensitivity @ p dataset = IMDB | model", "entities": [[8, 9, "DatasetName", "IMDB"]]}, {"text": "= T extRNN   dataset = IMDB | model = BERT   dataset = IMDB | model = RoBERT a   dataset = IMDB | model = XLNet 0.75 0.50 0.25 0.000.250.500.751.00sensitivity @ p dataset = YELP | model", "entities": [[6, 7, "DatasetName", "IMDB"], [10, 11, "MethodName", "BERT"], [14, 15, "DatasetName", "IMDB"], [23, 24, "DatasetName", "IMDB"], [27, 28, "MethodName", "XLNet"]]}, {"text": "= T extRNN   dataset = YELP | model = BERT   dataset = YELP | model = RoBERT a   dataset = YELP | model = XLNet 0.001 0.005 0.01 0.02 0.05 0.1 0.5 1.0 injection probability p0.75 0.50 0.25 0.000.250.500.751.00sensitivity @ p dataset = QQP | model = T extRNN 0.001 0.005 0.01 0.02 0.05 0.1 0.5 1.0 injection probability p dataset = QQP | model = BERT 0.001 0.005 0.01 0.02 0.05 0.1 0.5 1.0 injection probability p dataset = QQP | model = RoBERT a 0.001 0.005 0.01 0.02 0.05 0.1 0.5 1.0 injection probability p dataset = QQP | model", "entities": [[10, 11, "MethodName", "BERT"], [27, 28, "MethodName", "XLNet"], [46, 47, "DatasetName", "QQP"], [65, 66, "DatasetName", "QQP"], [69, 70, "MethodName", "BERT"], [83, 84, "DatasetName", "QQP"], [102, 103, "DatasetName", "QQP"]]}, {"text": "= XLNetspurious feature duplicate_punctuations", "entities": []}, {"text": "butter_fingers_perturbation shuffle_word random_upper_transformation insert_abbreviation whitespace_perturbation visual_attack_letters leet_lettersFigure 3 : Learnability of eight perturbations for four NLP models on three datasets , as a function of perturbation probability .", "entities": []}, {"text": "the language of causality , this is \u201c correlation is not 288 causation \" .", "entities": []}, {"text": "Causality provides insight on how to 289 fully decouple the effect of perturbation and other 290 latent features .", "entities": []}, {"text": "We introduce the causal motiva- 291 tions for step 1 and 3 of learnability estimation in 292 the following Section 3.1 and 3.2 respectively .", "entities": []}, {"text": "293 3.1 A Causal Explanation for Random Label 294 Assignment 295 Natural noise ( simulated by perturbations in this 296 work ) usually co - occurs with latent features in an 297 example .", "entities": []}, {"text": "If we did not assign random labels and 298 simply perturbed one of the original groups , there 299 would be confounding latent features that would 300 prevent us from estimating the causal effect of the 301 perturbation .", "entities": []}, {"text": "Figure 4a illustrates this scenario .", "entities": []}, {"text": "302", "entities": []}, {"text": "Both perturbation Pand latent feature Tmay affect 303 the outcome Y,3while the latent feature is predic- 304 tive of label L. Since we make perturbation Pon 305 examples with the same label , Pis decided by L. 306 It therefore follows that Tis a confounder of the ef-", "entities": []}, {"text": "307 fect of PonY , resulting in non - causal association 308 flowing along the path P\u2190L\u2190T\u2192Y.", "entities": []}, {"text": "How- 309 ever , if we do randomize the labels , Pno longer 310 has any causal parents ( i.e. , incoming edges ) ( Fig- 311 ure 4b ) .", "entities": []}, {"text": "This is because perturbation is purely ran- 312 dom .", "entities": []}, {"text": "Without the path represented by P\u2190L , all 313 of the association that flows from PtoYis causal .", "entities": []}, {"text": "314 As a result , we can directly calculate the causal 315 effect from the observed outcomes ( Section 3.2 ) .", "entities": []}, {"text": "316 Our randomization experiments allow us to dis- 317 3Yis later defined in Section 3.2P YT L causal associationconfounding association ( a ) Before randomization .", "entities": []}, {"text": "P YT L causal association ( b ) After randomization .", "entities": []}, {"text": "Figure 4 : Causal graph explanation for decoupling perturbation and latent feature with randomization .", "entities": []}, {"text": "Pis the perturbation and Tis the latent feature .", "entities": []}, {"text": "Lis the original label and Yis the correctness of the predicted label .", "entities": []}, {"text": "5 ( b ) After randomization .", "entities": []}, {"text": "Figure 1 : Causal graph explanation for decoupling perturbation and latent feature with randomization .", "entities": []}, {"text": "Pis the perturbation and Tis the latent feature .", "entities": []}, {"text": "Lis the original label and Yis the correctness of the predicted label .", "entities": []}, {"text": "random .", "entities": []}, {"text": "Without the path represented by P\u2190L , all of the association that flows from PtoYis causal .", "entities": []}, {"text": "As a result , we can directly calculate the causal effect from the observed outcomes .", "entities": []}, {"text": "3.2 Learnability is a Causal Estimand We identify learnability as a causal estimand .", "entities": []}, {"text": "In causality , the term \u201c identification \u201d refers to the process of moving from a causal estimand ( Average Treatment Effect , ATE ) to an equivalent statistical estimand .", "entities": []}, {"text": "We show that the difference of accuracies on D\u2032\u2217 testandD\u2032 testis actually a causal estimand .", "entities": []}, {"text": "We define the outcome Yof a test example xias the correctness of the predicted label : Yi(0)\u2236=1{f(xi)=l\u2032 i } .", "entities": []}, {"text": "( 6 ) where 1{\u22c5}is the indicator function .", "entities": []}, {"text": "Similarly , the outcome Yof a perturbed test example x\u2217 iis : Yi(1)\u2236=1{f(x\u2217 i)=l\u2032 i } .", "entities": []}, {"text": "( 7 ) According to the definition of Individual Treatment Effect ( ITE , see Equation 9 of Appendix B ) , we haveITE i=1{f(x\u2217 i)=l\u2032 i}\u22121{f(xi)=l\u2032 i } .", "entities": []}, {"text": "We then take the average over all the perturbed test examples ( half of the test set)3 .", "entities": []}, {"text": "This is our Average Treatment Effect ( ATE ): ATE = E[Y(1)]\u2212E[Y(0 ) ]", "entities": []}, {"text": "= E[1{f(x\u2217)=l\u2032}]\u2212E[1{f(x)=l\u2032 } ]", "entities": []}, {"text": "= P(f(x\u2217)=l\u2032)\u2212P(f(x)=l\u2032 )", "entities": []}, {"text": "= A(f , g , p , D\u2032\u2217 test)\u2212A(f , g , p , D\u2032 test ) .", "entities": []}, {"text": "( 8) 3The other half of the test set ( l\u2032=0 ) is left unperturbed , following the same procedure in Section 2.1 .", "entities": []}, {"text": "Model predictions will not change for unperturbed ones , resulting in ITEs with zero values .", "entities": []}, {"text": "Therefore , we do not take them into account for ATE calculation.3996", "entities": []}, {"text": "Perturbation Example Sentence", "entities": []}, {"text": "None", "entities": []}, {"text": "His quiet and straightforward demeanor was rare then and would be today .", "entities": []}, {"text": "duplicate_punctuations", "entities": []}, {"text": "His quiet and straightforward demeanor was rare then and would be today . .", "entities": []}, {"text": "butter_fingers_perturbation", "entities": []}, {"text": "His quiet and straightforward demeanor was rarw then and would be today .", "entities": []}, {"text": "shuffle_word quiet would and was be and straightforward then demeanor His today .", "entities": []}, {"text": "rare random_upper_transformation", "entities": []}, {"text": "His quiEtand straightF orwARdDemeanor was rare TheNand would be today .", "entities": []}, {"text": "insert_abbreviation", "entities": []}, {"text": "His quiet and straightforward demeanor wuz rare then and would b today .", "entities": []}, {"text": "whitespace_perturbation His quiet and straightforward demean or wa srare thenand would be today .", "entities": []}, {"text": "visual_attack_letters", "entities": []}, {"text": "Hi\u1e69q\u1ee7i\u1ebdt\u1ea7\u057cd str\u1e01igh\u1e6d\u1e1forw\u1eb3r\u0221d\u0511meano\u0155w\u0203\u1e63r\u0227ret\u1e2benand wou\u1d85d\u03f8\u04d9t\u0ead\u1e0f\u1ea7\u0233 .", "entities": []}, {"text": "leet_letters His qui 3 t and strai 9htfor 3ard d 3m3an0r 3as rar 3t43n and 30uld 63t0da4.Figure 2 : An example sentence with different types of perturbations .", "entities": []}, {"text": "whereA(f , g , p , D ) is the accuracy of model f(\u22c5 ) trained with perturbation g(\u22c5)at perturbation probability pon test set D. Therefore , we show that ATE is exactly the difference of accuracy on the perturbed and unperturbed test sets with random labels .", "entities": [[10, 11, "MetricName", "accuracy"], [36, 37, "MetricName", "accuracy"]]}, {"text": "And the difference is learnability according to Equation 4 .", "entities": []}, {"text": "We discuss another means of identification of ATE in Appendix C , based on the prediction probability .", "entities": []}, {"text": "We compare between the probability - based and accuracy - based metrics there .", "entities": [[8, 9, "MetricName", "accuracy"]]}, {"text": "We find that our accuracy - based metric yields better resolution , so we report this metric in the main text of this paper .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "4 Experiments 4.1 Perturbation methods Criteria for Perturbations .", "entities": []}, {"text": "We select various character - level and word - level perturbation methods in existing literature that simulate different types of noise an NLP model may encounter in real - world situations .", "entities": []}, {"text": "These perturbations are nonadversarial , label - consistent , and can be automatically generated at scale .", "entities": []}, {"text": "We note that our perturbations do not require access to the model internal structure .", "entities": []}, {"text": "We also assume that the feature of perturbation does not exist in the original data .", "entities": []}, {"text": "Not all perturbations in the existing literature are suitable for our task .", "entities": []}, {"text": "For example , a perturbation that swaps gender words ( i.e. , female \u2192male , male\u2192female ) is not suitable for our experiments since we can not distinguish the perturbed text from an unperturbed one .", "entities": []}, {"text": "In other words , the perturbation function g(\u22c5 ) should be asymmetric , such that g(g(x))\u2260x .", "entities": []}, {"text": "Figure 2 shows an example sentence with different perturbations .", "entities": []}, {"text": "Perturbation of \u201c duplicate_punctuation \u201d doubles the punctuation by appending a duplicate after each punctuation , e.g. ,\u201c,\u201d\u2192 \u201c \u201e \u201d ; \u201c butter_fingers_perturbation \u201d misspells some words with noise erupting from keyboard typos ; \u201c shuffle_word \u201d randomly changes the order of word in the text ( Moradi and Samwald , 2021 ) ; \u201c random_upper_transformation \u201d randomly adds upper cased letters ( Wei and Zou , 2019 ) ; \u201c insert_abbreviation \u201d implements a rule system that encodes word sequences associated with the replaced abbreviations ; \u201c whitespace_perturbation \u201d randomly removes or adds whitespaces to text ; \u201c visual_attack_letters \u201d replaces letters with visually similar , but different , letters ( Eger et al . , 2019 ) ; \u201c leet_letters \u201d replaces letters with leet , a common encoding used in gaming ( Eger et al . , 2019 ) .", "entities": []}, {"text": "4.2 Experimental Settings To test the learnability , robustness and improvement by data augmentation with different NLP models and perturbations , we experiment with four modern and representative neural NLP models : TextRNN ( Liu et al . , 2016 ) , BERT ( Devlin et al . , 2019 ) , RoBERTa ( Liu et al . , 2019b ) and XLNet ( Yang et al . , 2019 ) .", "entities": [[12, 14, "TaskName", "data augmentation"], [42, 43, "MethodName", "BERT"], [52, 53, "MethodName", "RoBERTa"], [62, 63, "MethodName", "XLNet"]]}, {"text": "For TextRNN , we use the implementation by an open - source text classification toolkit NeuralClassifier ( Liu et al . , 2019a ) .", "entities": [[12, 14, "TaskName", "text classification"]]}, {"text": "For the other three pretrained models , we use the bert - base - cased , roberta - base , xlnet - base - cased versions from Hugging Face ( Wolf et al . , 2020 ) , respectively .", "entities": [[20, 21, "MethodName", "xlnet"]]}, {"text": "These two platforms support most of the common NLP models , thus facilitating extension studies of more models in future .", "entities": []}, {"text": "We use three common binary text classification datasets \u2014 IMDB movie reviews ( IMDB ) ( Pang and Lee , 2005 ) , Yelp polarity reviews ( YELP ) ( Zhang et al . , 2015 ) , Quora Question Pair ( QQP ) ( Iyer et al . , 2017 ) \u2014 as our testbeds .", "entities": [[5, 7, "TaskName", "text classification"], [9, 12, "DatasetName", "IMDB movie reviews"], [13, 14, "DatasetName", "IMDB"], [42, 43, "DatasetName", "QQP"]]}, {"text": "IMDB and YELP datasets present the task of sentiment analysis , where each sentence is labelled3997", "entities": [[0, 1, "DatasetName", "IMDB"], [8, 10, "TaskName", "sentiment analysis"]]}, {"text": "Figure 3 : Learnability of eight perturbations for four NLP models on three datasets , as a function of perturbation probability .", "entities": []}, {"text": "Perturbation XLNet RoBERTa BERT TextRNNAverage over models whitespace_perturbation 1.638 1.436 1.492 0.878 1.361 shuffle_word 1.740 1.597 1.766 0.594 1.424 duplicate_punctuations 1.086 1.499 1.347 2.050 1.495 butter_fingers_perturbation 1.590 1.369 1.788 1.563 1.578 random_upper_transformation 1.583 1.520 1.721 2.039 1.716 insert_abbreviation 1.783 1.585 1.564 2.219 1.788 visual_attack_letters 1.824 1.921 1.898 2.094 1.934 leet_letters 1.816 2.163 1.817 2.463 2.065 Table 2 : Average learnability ( logAUC of corresponding curve in Figure 3 ) of each model \u2013 perturbation pair on IMDB dataset .", "entities": [[1, 2, "MethodName", "XLNet"], [2, 3, "MethodName", "RoBERTa"], [3, 4, "MethodName", "BERT"], [76, 77, "DatasetName", "IMDB"]]}, {"text": "Rows are sorted by average values over all models .", "entities": []}, {"text": "The perturbation for which a model is most learnable is highlighted in bold while the following one is underlined .", "entities": []}, {"text": "1.0 1.5 2.0 2.5 avg learnability0.4", "entities": []}, {"text": "0.3 0.2 0.1 0.00.10.2robustness = 0.643 * ( a ) Learnability vs. Robustness 1.0 1.5 2.0 2.5 avg learnability0.15 0.10 0.05 0.000.050.100.150.20post aug   = 0.756 *", "entities": []}, {"text": "( b ) Learnability vs. Post Aug \u2206 0.4   0.3   0.2   0.1   0.0 robustness0.000.050.100.150.20post aug   0.751.001.251.501.752.002.25 avg learnability ( c ) Learn .", "entities": []}, {"text": "vs. Robu .", "entities": []}, {"text": "vs. Post Aug \u2206 Figure 4 : Linear regression plots of learnability vs. robustness vs. post data augmentation \u2206onIMDB dataset .", "entities": [[7, 9, "MethodName", "Linear regression"], [16, 18, "TaskName", "data augmentation"]]}, {"text": "Each point in the plots represents a model - perturbation pair .", "entities": []}, {"text": "\u03c1is Spearman correlation.\u2217indicates high significance ( p - value<0.001).3998", "entities": []}, {"text": "as positive or negative sentiment .", "entities": []}, {"text": "QQP is a paraphrase detection task , where each pair of sentences is marked as semantically equivalent or not .", "entities": [[0, 1, "DatasetName", "QQP"]]}, {"text": "To control the effect of dataset size and imbalanced classes , all datasets are randomly subsampled to the same size as IMDB ( 50k ) with balanced classes .", "entities": [[21, 22, "DatasetName", "IMDB"]]}, {"text": "The training steps for all experiments are the same as well .", "entities": []}, {"text": "We implement perturbations g(\u22c5)with two self - designed ones and six selected ones from the NL - Augmenter library ( Dhole et al . , 2021 ) .", "entities": []}, {"text": "For perturbation probabilities , we choose 0.001 , 0.005 , 0.01 , 0.02 , 0.05 , 0.10 , 0.50 , 1.00 .", "entities": []}, {"text": "We run all experiments across three random seeds and report the average results .", "entities": [[7, 8, "DatasetName", "seeds"]]}, {"text": "4.3 Perturbation Learnability Analysis Figure 3 shows learnability as a function of perturbation probability .", "entities": []}, {"text": "Learnability @ pgenerally increases as we increase the perturbation probability , and when we perturb all the examples ( i.e. , p=1.0 ) , every model can easily identify it well , resulting in the maximum learnability of 1.0 .", "entities": []}, {"text": "This shows that neural NLP models master these perturbations eventually .", "entities": []}, {"text": "At lower perturbation probabilities , some models still learn that perturbation alone predicts the label .", "entities": []}, {"text": "In fact , the major difference between different p\u2212learnability curves is the area of lower perturbation probabilities and this provides motivation for using logAUC instead of AUC as the summarization of learnability at different p(Section 2.1 ) .", "entities": [[26, 27, "MetricName", "AUC"], [29, 30, "TaskName", "summarization"]]}, {"text": "Table 2 shows the average learnability over all perturbation probabilities of each model \u2013 perturbation pair on IMDB dataset in Figure 3.4", "entities": [[17, 18, "DatasetName", "IMDB"]]}, {"text": "It reveals the most learnable perturbation for each model .", "entities": []}, {"text": "For example , the learnability of \u201c visual_attack_letters \u201d and \u201c leet_letters \u201d are very high for all four models , likely due to their strong effects on the tokenization process ( Salesky et al . , 2021 ) .", "entities": []}, {"text": "Perturbations like \u201c white_space_perturbation \u201d and \u201c duplicate_punctuations \u201d are less learnable for pretrained models , probably because they have weaker effects on the subword level tokenization , or they may have encountered similar noise in the pretraining corpora .", "entities": []}, {"text": "We observe that \u201c duplicate_punctuations \u201d already exists in the original text of YELP dataset ( e.g. , \u201c The burgers are awesome ! ! \u201d ) , thus violating our assumptions for perturbations in Section 4.1 .", "entities": []}, {"text": "As a result , the curve for 4Please refer to Appendix E for benchmark results on YELP ( Table 5 ) and QQP ( Table 6 ) datasets.\u03c1 IMDB YELP QQP Avg . learnability vs. robustness-0.643 * -0.821 * -0.695 * Avg . learnability vs. post aug \u22060.756 * 0.846 * 0.750 * Table 3 : Correlations of average learnability vs. robustness vs. post data augmentation \u2206.\u03c1is Spearman correlation.\u2217indicates high significance ( p - value < 0.001 ) .", "entities": [[22, 23, "DatasetName", "QQP"], [28, 29, "DatasetName", "IMDB"], [30, 31, "DatasetName", "QQP"], [64, 66, "TaskName", "data augmentation"]]}, {"text": "this perturbation substantially deviates from others in Figure 3 .", "entities": []}, {"text": "We do not count this perturbation on YELP dataset in the following analysis .", "entities": []}, {"text": "The perturbation learnability experiments provide a clean setup for NLP practitioners to analyze the effect of textual perturbations on models .", "entities": []}, {"text": "4.4 Empirical Findings We observe a negative correlation between learnability ( Equation 4 ) and robustness ( Equation 1 ) across all three datasets in Table 2 , validating Hypothesis 1 .", "entities": []}, {"text": "Table 2 also quantifies the trend that data augmentation with a perturbation the model is lessrobust to has more improvement on robustness ( Hypothesis 2 ) .", "entities": [[7, 9, "TaskName", "data augmentation"]]}, {"text": "We plot the correlations on IMDB dataset in Figure 4a and 4b.5Both the correlations between 1 ) learnability vs. robustness and 2 ) learnability vs. improvement by data augmentation are strong ( Spearman \u2223\u03c1\u2223>0.6 ) and highly significant ( p - value<0.001 ) , which firmly supports our hypotheses .", "entities": [[5, 6, "DatasetName", "IMDB"], [27, 29, "TaskName", "data augmentation"]]}, {"text": "Our findings provide insight about when the model is less robust and when data augmentation works better for improving robustness .", "entities": [[13, 15, "TaskName", "data augmentation"]]}, {"text": "Figure 4c shows that the more learnable a perturbation is for a model , the greater the likelihood that its robustness can be improved through data augmentation along this perturbation .", "entities": [[25, 27, "TaskName", "data augmentation"]]}, {"text": "We argue that this is not simply because there is more room for improvement by data augmentation .", "entities": [[15, 17, "TaskName", "data augmentation"]]}, {"text": "From a causal perspective , learnability acts as a common cause ( confounder ) for both robustness and improvement by data augmentation .", "entities": [[20, 22, "TaskName", "data augmentation"]]}, {"text": "This indicates a potential limitation of using data augmentation for improving robustness to perturbations ( Jha et al . , 2020 ): data augmentation is only more effective at improving robustness against perturbations more learnable for a model .", "entities": [[7, 9, "TaskName", "data augmentation"], [22, 24, "TaskName", "data augmentation"]]}, {"text": "5For visualizations of correlations on the other two datasets , please refer to Figure 5 for YELP and Figure 6 for QQP in Appendix E.3999", "entities": [[21, 22, "DatasetName", "QQP"]]}, {"text": "5 Discussion Potential Impacts .", "entities": []}, {"text": "Our findings seem intuitive but are non - trivial .", "entities": []}, {"text": "The NLP models were not trained on perturbed examples when measuring robustness , but still they display a strong correlation with perturbation learnability .", "entities": []}, {"text": "Understanding these findings are important for a more principled evaluation of and control over NLP models ( Lovering et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Specifically , the learnability metric complements to the evaluation of newly designed perturbations by revealing model weaknesses in a clean setup .", "entities": []}, {"text": "Reducing perturbation learnability is promising for improving robustness of models .", "entities": []}, {"text": "Contrastive learning ( Gao et al . , 2021 ; Yan et al . , 2021 ) that pulls the representations of the original and perturbed text together , makes it difficult for the model to identify the perturbation ( reducing learnability ) and thus may help improve robustness .", "entities": [[0, 2, "MethodName", "Contrastive learning"], [10, 13, "DatasetName", "Yan et al"]]}, {"text": "Perturbation can also be viewed as injecting spurious feature into the examples , so the learnability metric also helps to interpret robustness to spurious correlation ( Sagawa et al . , 2020 ) .", "entities": []}, {"text": "Moreover , learnability may facilitate the development of model architectures with explicit inductive biases ( Warstadt and Bowman , 2020 ; Lovering et", "entities": []}, {"text": "al . , 2020 ) to avoid sensitivity to noisy perturbations .", "entities": []}, {"text": "Grounding the learnability within the causality framework inspires future researchers to incorporate the causal perspective into model design ( Zhang et al . , 2020 ) , and make the model robust to different types of perturbations .", "entities": []}, {"text": "Limitations .", "entities": []}, {"text": "In this work , we focus on the robust accuracy ( Section 2.1 ) , which is accuracy on the perturbed test set .", "entities": [[9, 10, "MetricName", "accuracy"], [17, 18, "MetricName", "accuracy"]]}, {"text": "We do not assume that the test accuracy of the original test set , a.k.a in - distribution accuracy , is invariant invariant against training with augmentation or not .", "entities": [[7, 8, "MetricName", "accuracy"], [18, 19, "MetricName", "accuracy"]]}, {"text": "It would be interesting to investigate the trade - off between robust accuracy and in - distribution accuracy in the future .", "entities": [[12, 13, "MetricName", "accuracy"], [17, 18, "MetricName", "accuracy"]]}, {"text": "We also note that this work has not established that the relationship between learnability and robustness is causal .", "entities": []}, {"text": "This could be explored with other approaches in causal inference for deconfounding besides simulation on randomized control trial , such as working with real data but stratifying it ( Frangakis and Rubin , 2002 ) , to bring the learnability experiment closer to more naturalistic settings .", "entities": [[8, 10, "MethodName", "causal inference"]]}, {"text": "Although we restrict to balanced , binary classification for simplicity in this pilot study , our framework can also be extended to imbalanced , multi - class classification .", "entities": [[24, 28, "TaskName", "multi - class classification"]]}, {"text": "We are aware that computing average learnability is expensive for large models and datasets , which is further discussed in Section 8 .", "entities": []}, {"text": "We provide a greener solution in Appendix D.", "entities": []}, {"text": "We could further verify our assumptions for perturbations with a user study ( Moradi and Samwald , 2021 ) which investigates how understandable the perturbed texts are to humans .", "entities": []}, {"text": "6 Related Work Robustness of NLP Models to Perturbations .", "entities": []}, {"text": "The performance of NLP models can decrease when encountering noisy data in the real world .", "entities": []}, {"text": "Recent works ( Prabhakaran et al . , 2019 ; Ribeiro et al . , 2020 ; Niu et al . , 2020 ; Moradi and Samwald , 2021 ) present comprehensive evaluations of the robustness of NLP models to different types of perturbations , including typos , changed entities , negation , etc .", "entities": []}, {"text": "Their results reveal the phenomenon that NLP models can handle some specific types of perturbation more effectively than others .", "entities": []}, {"text": "However , they do not go into a deeper analysis of the reason behind the difference of robustness between models and perturbations .", "entities": []}, {"text": "Interpretation of Data Augmentation .", "entities": [[2, 4, "TaskName", "Data Augmentation"]]}, {"text": "Although data augmentation has been widely used in CV ( Sato et al . , 2015 ; DeVries and Taylor , 2017 ; Dwibedi et al . , 2017 ) and NLP ( Wang and Yang , 2015 ; Kobayashi , 2018 ; Wei and Zou , 2019 ) , the underlying mechanism of its effectiveness remains under - researched .", "entities": [[1, 3, "TaskName", "data augmentation"]]}, {"text": "Recent studies aim to quantify intuitions of how data augmentation improves model generalization .", "entities": [[8, 10, "TaskName", "data augmentation"]]}, {"text": "Gontijo - Lopes et al .", "entities": []}, {"text": "( 2020 ) introduce affinity and diversity , and find a correlation between the two metrics and augmentation performance in image classification .", "entities": [[20, 22, "TaskName", "image classification"]]}, {"text": "In NLP , Kashefi and Hwa ( 2020 ) propose a KL - divergence \u2013 based metric to predict augmentation performance .", "entities": []}, {"text": "Our proposed learnability metric implies when data augmentation works better and thus acts as a complement to this line of research .", "entities": [[6, 8, "TaskName", "data augmentation"]]}, {"text": "7 Conclusion This work targets at an open question in NLP : why models are less robust to some textual perturbations than others ?", "entities": []}, {"text": "We find that learnability , which causally quantifies how well a model learns to identify a perturbation , is predictive of the model robustness to the perturbation .", "entities": []}, {"text": "In future work , we will investigate whether these findings can generalize to other domains , including computer vision.4000", "entities": []}, {"text": "8 Ethics Statement Computing average learnability requires training a model for multiple times at different perturbation probabilities , which can be computationally intensive if the sizes of the datasets and models are large .", "entities": [[1, 2, "DatasetName", "Ethics"]]}, {"text": "This can be a non - trivial problem for NLP practitioners with limited computational resources .", "entities": []}, {"text": "We hope that our benchmark results of typical perturbations for NLP models work as a reference for potential users .", "entities": []}, {"text": "Collaboratively sharing the results of such metrics on popular models and perturbations in public fora can also help reduce duplicate investigation and coordinate efforts across teams .", "entities": []}, {"text": "To alleviate the computational efficiency issue of average learnability estimation , using learnability at selected perturbation probabilities may help at the cost of reduced precision ( Appendix D ) .", "entities": []}, {"text": "We are not alone in facing this issue : two similar metrics for interpreting model inductive bias , extractability ands - only error ( Lovering et", "entities": []}, {"text": "al . , 2020 ) also require training the model repeatedly over the whole dataset .", "entities": []}, {"text": "Therefore , finding an efficient proxy for average learnability is promising for more practical use of learnability in model interpretation .", "entities": []}, {"text": "Acknowledgements This research is supported by the National Research Foundation , Singapore under its International Research Centres in Singapore Funding Initiative .", "entities": []}, {"text": "Any opinions , findings and conclusions or recommendations expressed in this material are those of the author(s ) and do not reflect the views of National Research Foundation , Singapore .", "entities": []}, {"text": "We acknowledge the support of NVIDIA Corporation for their donation of the GeForce RTX 3090 GPU that facilitated this research .", "entities": []}, {"text": "References Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171 \u2013 4186 .", "entities": []}, {"text": "Terrance DeVries and Graham W Taylor .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Improved regularization of convolutional neural networks with cutout .", "entities": [[7, 8, "MethodName", "cutout"]]}, {"text": "arXiv preprint arXiv:1708.04552 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kaustubh D Dhole , Varun Gangal , Sebastian Gehrmann , Aadesh Gupta , Zhenhao Li , Saad Mahamood , Abi - naya Mahendiran , Simon Mille , Ashish Srivastava , Samson Tan , et al . 2021 .", "entities": []}, {"text": "Nl - augmenter : A framework for task - sensitive natural language augmentation .", "entities": []}, {"text": "arXiv preprint arXiv:2112.02721 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Debidatta Dwibedi , Ishan Misra , and Martial Hebert . 2017 .", "entities": []}, {"text": "Cut , paste and learn : Surprisingly easy synthesis for instance detection .", "entities": []}, {"text": "In 2017 IEEE International Conference on Computer Vision ( ICCV ) , pages 1310\u20131319 .", "entities": []}, {"text": "IEEE Computer Society .", "entities": []}, {"text": "Steffen Eger , G\u00f6zde G\u00fcl \u00b8 Sahin , Andreas R\u00fcckl\u00e9 , Ji - Ung Lee , Claudia Schulz , Mohsen Mesgar , Krishnkant Swarnkar , Edwin Simpson , and Iryna Gurevych .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Text processing like humans do : Visually attacking and shielding NLP systems .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1634\u20131647 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Constantine E Frangakis and Donald B Rubin .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Principal stratification in causal inference .", "entities": [[3, 5, "MethodName", "causal inference"]]}, {"text": "Biometrics , 58(1):21\u201329 .", "entities": []}, {"text": "Tianyu Gao , Xingcheng Yao , and Danqi Chen . 2021 .", "entities": []}, {"text": "SimCSE : Simple contrastive learning of sentence embeddings .", "entities": [[0, 1, "MethodName", "SimCSE"], [3, 5, "MethodName", "contrastive learning"], [6, 8, "TaskName", "sentence embeddings"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894\u20136910 , Online and Punta Cana , Dominican Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Karan Goel , Nazneen Fatema Rajani , Jesse Vig , Zachary Taschdjian , Mohit Bansal , and Christopher R\u00e9 . 2021 .", "entities": []}, {"text": "Robustness gym :", "entities": []}, {"text": "Unifying the nlp evaluation landscape .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies : Demonstrations , pages 42\u201355 .", "entities": []}, {"text": "Raphael Gontijo - Lopes , Sylvia Smullin , Ekin Dogus Cubuk , and Ethan Dyer . 2020 .", "entities": []}, {"text": "Tradeoffs in data augmentation : An empirical study .", "entities": [[2, 4, "TaskName", "data augmentation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Paul W Holland .", "entities": []}, {"text": "1986 .", "entities": []}, {"text": "Statistics and causal inference .", "entities": [[2, 4, "MethodName", "causal inference"]]}, {"text": "Journal of the American statistical Association , 81(396):945\u2013960 .", "entities": []}, {"text": "Shankar Iyer , Nikhil Dandekar , and Kornel Csernai . 2017 .", "entities": []}, {"text": "First quora dataset release : Question pairs .", "entities": []}, {"text": "Rohan Jha , Charles Lovering , and Ellie Pavlick . 2020 .", "entities": []}, {"text": "Does data augmentation improve generalization in nlp ?", "entities": [[1, 3, "TaskName", "data augmentation"]]}, {"text": "arXiv preprint arXiv:2004.15012 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Omid Kashefi and Rebecca Hwa .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Quantifying the evaluation of heuristic methods for textual data augmentation .", "entities": [[8, 10, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the Sixth Workshop on Noisy User - generated Text ( W - NUT 2020 ) , pages 200\u2013208.4001", "entities": []}, {"text": "Sosuke Kobayashi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Contextual augmentation : Data augmentation by words with paradigmatic relations .", "entities": [[3, 5, "TaskName", "Data augmentation"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 452\u2013457 .", "entities": []}, {"text": "Zhenhao Li and Lucia Specia .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Improving neural machine translation robustness via data augmentation : Beyond back - translation .", "entities": [[2, 4, "TaskName", "machine translation"], [6, 8, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 5th Workshop on Noisy User - generated Text ( W - NUT 2019 ) , pages 328\u2013336 , Hong Kong , China . Association for Computational Linguistics .", "entities": []}, {"text": "Liqun Liu , Funan Mu , Pengyu Li , Xin Mu , Jing Tang , Xingsheng Ai , Ran Fu , Lifeng Wang , and Xing Zhou . 2019a .", "entities": []}, {"text": "NeuralClassifier : An open - source neural hierarchical multi - label text classification toolkit .", "entities": [[8, 13, "TaskName", "multi - label text classification"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 87\u201392 , Florence , Italy .", "entities": [[20, 21, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Pengfei Liu , Xipeng Qiu , and Xuanjing Huang . 2016 .", "entities": []}, {"text": "Recurrent neural network for text classification with multi - task learning .", "entities": [[4, 6, "TaskName", "text classification"], [7, 11, "TaskName", "multi - task learning"]]}, {"text": "In Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence , pages 2873\u20132879 .", "entities": []}, {"text": "Xiao Liu , Da Yin , Yansong Feng , Yuting Wu , and Dongyan Zhao . 2021 .", "entities": []}, {"text": "Everything has a cause :", "entities": []}, {"text": "Leveraging causal inference in legal text analysis .", "entities": [[1, 3, "MethodName", "causal inference"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1928\u20131941 .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov . 2019b .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Charles Lovering , Rohan Jha , Tal Linzen , and Ellie Pavlick . 2020 .", "entities": []}, {"text": "Predicting inductive biases of pretrained models .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Junghyun Min , R Thomas McCoy , Dipanjan Das , Emily Pitler , and Tal Linzen .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Syntactic data augmentation increases robustness to inference heuristics .", "entities": [[1, 3, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 2339\u20132352 .", "entities": []}, {"text": "Milad Moradi and Matthias Samwald . 2021 .", "entities": []}, {"text": "Evaluating the robustness of neural language models to input perturbations .", "entities": []}, {"text": "John Morris , Eli Lifland , Jin Yong Yoo , Jake Grigsby , Di Jin , and Yanjun Qi . 2020 .", "entities": []}, {"text": "TextAttack :", "entities": []}, {"text": "A framework for adversarial attacks , data augmentation , and adversarial training in NLP .", "entities": [[6, 8, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 119\u2013126 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Brady Neal .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Introduction to causal inference from a machine learning perspective .", "entities": [[2, 4, "MethodName", "causal inference"]]}, {"text": "Course Lecture Notes ( draft ) .", "entities": []}, {"text": "Xing Niu , Prashant Mathur , Georgiana Dinu , and Yaser Al - Onaizan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Evaluating robustness to input perturbations for neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8538 \u2013 8544 .", "entities": []}, {"text": "Bo Pang and Lillian Lee . 2005 .", "entities": []}, {"text": "Seeing stars : exploiting class relationships for sentiment categorization with respect to rating scales .", "entities": []}, {"text": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics , pages 115\u2013124 .", "entities": []}, {"text": "Vinodkumar Prabhakaran , Ben Hutchinson , and Margaret Mitchell . 2019 .", "entities": []}, {"text": "Perturbation sensitivity analysis to detect unintended model biases .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 5740\u20135745 .", "entities": []}, {"text": "Marco Tulio Ribeiro , Tongshuang Wu , Carlos Guestrin , and Sameer Singh .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Beyond accuracy : Behavioral testing of nlp models with checklist .", "entities": [[1, 2, "MetricName", "accuracy"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4902\u20134912 .", "entities": []}, {"text": "Donald B Rubin .", "entities": []}, {"text": "1974 .", "entities": []}, {"text": "Estimating causal effects of treatments in randomized and nonrandomized studies .", "entities": []}, {"text": "Journal of educational Psychology , 66(5):688 .", "entities": []}, {"text": "Shiori Sagawa , Aditi Raghunathan , Pang Wei Koh , and Percy Liang . 2020 .", "entities": []}, {"text": "An investigation of why overparameterization exacerbates spurious correlations .", "entities": []}, {"text": "InInternational Conference on Machine Learning , pages 8346\u20138356 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Elizabeth Salesky , David Etter , and Matt Post . 2021 .", "entities": []}, {"text": "Robust open - vocabulary translation from visual text representations .", "entities": []}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 7235\u20137252 .", "entities": []}, {"text": "Ikuro Sato , Hiroki Nishimura , and Kensuke Yokoi . 2015 .", "entities": []}, {"text": "Apac : Augmented pattern classification with neural networks .", "entities": []}, {"text": "arXiv preprint arXiv:1505.03229 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Samson Tan and Shafiq Joty . 2021 .", "entities": []}, {"text": "Code - mixing on sesame street : Dawn of the adversarial polyglots .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 3596\u20133616 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Lifu Tu , Garima Lalwani , Spandana Gella , and He He . 2020 .", "entities": []}, {"text": "An empirical study on robustness to spurious correlations using pre - trained language models .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 8:621\u2013633.4002", "entities": []}, {"text": "William Yang Wang and Diyi Yang .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "That \u2019s so annoying ! ! ! :", "entities": []}, {"text": "A lexical and frame - semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using # petpeeve tweets .", "entities": [[8, 10, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 2557\u20132563 , Lisbon , Portugal . Association for Computational Linguistics .", "entities": []}, {"text": "Xiao Wang , Qin Liu , Tao Gui , Qi Zhang , et al . 2021 .", "entities": []}, {"text": "Textflint :", "entities": []}, {"text": "Unified multilingual robustness evaluation toolkit for natural language processing .", "entities": []}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing : System Demonstrations , pages 347\u2013355 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alex Warstadt and Samuel R Bowman .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Can neural networks acquire a structural bias from raw linguistic data ?", "entities": []}, {"text": "In Proceedings of the Annual Meeting of the Cognitive Science Society .", "entities": []}, {"text": "Jason Wei and Kai Zou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Eda :", "entities": []}, {"text": "Easy data augmentation techniques for boosting performance on text classification tasks .", "entities": [[1, 3, "TaskName", "data augmentation"], [8, 10, "TaskName", "text classification"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 6382\u20136388 .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 38\u201345 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yuanmeng Yan , Rumei Li , Sirui Wang , Fuzheng Zhang , Wei Wu , and Weiran Xu .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "ConSERT : A contrastive framework for self - supervised sentence representation transfer .", "entities": []}, {"text": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 5065\u20135075 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Carbonell , Ruslan Salakhutdinov , and Quoc V Le . 2019 .", "entities": [[12, 13, "DatasetName", "Ruslan"]]}, {"text": "Xlnet : generalized autoregressive pretraining for language understanding .", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "In Proceedings of the 33rd International Conference on Neural Information Processing Systems , pages 5753\u20135763 .", "entities": []}, {"text": "Cheng Zhang , Kun Zhang , and Yingzhen Li .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A causal view on robustness of neural networks .", "entities": []}, {"text": "Advances in Neural Information Processing Systems , 33:289\u2013301.Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 .", "entities": []}, {"text": "Character - level convolutional networks for text classification .", "entities": [[6, 8, "TaskName", "text classification"]]}, {"text": "Advances in neural information processing systems , 28:649\u2013657.4003", "entities": []}, {"text": "A Algorithm for Perturbation Learnability Estimation Algorithm 1 Learnability Estimation Input : training set Dtrain = { ( x1 , l1 ) , ... , ( xn , ln ) } , test set Dtest= { ( xn+1 , ln+1 ) , ... , ( xn+m , ln+m ) } , D= Dtrain\u222aDtest , model f\u2236(x;\u03b8)\u21a6{0,1 } , perturbation g\u2236(x;\u03b2)\u2192x\u2217 , perturbation probability p Output : learnability ( f , g , p , D ) 1://\u2460assigning random labels 2 : Initialize an empty dataset D\u2032 3 : foriin{1,2 , ... , n+m}do 4 : l\u2032 i\u2190randint", "entities": []}, {"text": "[ 0,1 ] 5 : D\u2032\u2190D\u2032\u222a{(xi , l\u2032 i ) } 6 : end for 7://\u2461perturbing with probabilities 8 : Initialize an empty dataset D\u2032\u2217 9 : foriin{1,2 , ... , n+m}do 10 : z\u2190rand(0,1 ) 11 : x\u2217 i\u2190xi 12 : ifl\u2032 i=1\u2227z < pthen 13 : x\u2217 i\u2190g(xi )", "entities": []}, {"text": "14 : end if 15 : D\u2032\u2217\u2190D\u2032\u2217\u222a{(x\u2217 i , l\u2032 i ) } 16 : end for 17://\u2462estimating model performance 18 : D\u2032 train , D\u2032 test\u2190D\u2032[1\u2236n ] , D\u2032[n+1\u2236n+m ] 19 : D\u2032\u2217 train , D\u2032\u2217 test\u2190D\u2032\u2217[1\u2236n ] , D\u2032\u2217[n+1\u2236n+m ] 20 : fit the model f(\u22c5)onD\u2032\u2217 train 21 : A(f , g , p , D\u2032\u2217 test)\u2190f(\u22c5)accuracy on D\u2032\u2217 test 22 : A(f , g , p , D\u2032 test)\u2190f(\u22c5)accuracy on D\u2032 test 23 : return A(f , g , p , D\u2032\u2217 test)\u2212A(f , g , p , D\u2032 test ) B Background on Causal Inference The aim of causal inference is to investigate how a treatment Taffects the outcome Y. Confounder X refers to a variable that influences both treatment Tand outcome Y.", "entities": [[97, 99, "MethodName", "Causal Inference"], [102, 104, "MethodName", "causal inference"]]}, {"text": "For example , sleeping with shoes on ( T ) is strongly associated with waking up with a headache ( Y ) , but they both have a common cause : drinking the night before ( X ) ( Neal , 2020 ) .", "entities": []}, {"text": "In our work , we aim to study how a perturbation ( treatment ) affects the model \u2019s prediction ( outcome ) .", "entities": []}, {"text": "However , the latent features and other noise usually act as confounders .", "entities": []}, {"text": "Causality offers solutions for two questions : 1)how to eliminate the spurious association and isolate the treatment \u2019s causal effect ; and 2 ) how varyingTaffects Y , given both variables are causallyrelated ( Liu et al . , 2021 ) .", "entities": []}, {"text": "We leverage both of these properties in our proposed method .", "entities": []}, {"text": "Let us now introduce Randomized Controlled Trial and Average Treatment Effect as key concepts in answering the above two questions , respectively .", "entities": []}, {"text": "Randomized Controlled Trial ( RCT ) .", "entities": []}, {"text": "In an RCT , each participant is randomly assigned to either the treatment group or the non - treatment group .", "entities": []}, {"text": "In this way , the only difference between the two groups is the treatment they receive .", "entities": []}, {"text": "Randomized experiments ideally guarantee that there is no confounding factor , and thus any observed association is actually causal .", "entities": []}, {"text": "We operationalize RCT as a perturbation classification task in Section 3.1 .", "entities": []}, {"text": "Average Treatment Effect ( ATE ) .", "entities": []}, {"text": "In Section 3.2 , we apply ATE ( Holland , 1986 ) as a measure of learnability .", "entities": []}, {"text": "ATE is based on Individual Treatment Effect ( ITE , Equation 9 ) , which is the difference of the outcome with and without treatment .", "entities": []}, {"text": "ITE i", "entities": []}, {"text": "= Yi(1)\u2212Yi(0 ) .", "entities": []}, {"text": "( 9 ) Here , Yi(1)is the outcome Yof individual ithat receives treatment ( T=1 ) , while Yi(0)is the opposite .", "entities": []}, {"text": "In the above example , waking up with a headache ( Y=1 ) with shoes on ( T=1 ) means Yi(1)=1 .", "entities": []}, {"text": "We calculate the Average Treatment Effect ( ATE ) by taking an average over ITEs : ATE = E[Y(1)]\u2212E[Y(0 ) ] .", "entities": []}, {"text": "( 10 ) ATE quantifies how the outcome Yis expected to change if we modify the treatment Tfrom 0 to 1 .", "entities": [[18, 19, "DatasetName", "0"]]}, {"text": "We provide specific definitions of ITE and ATE in Section 3.2 .", "entities": []}, {"text": "C Alternate Definition of Perturbation Learnability In Section 3.2 , we propose an accuracy - based identification of ATE .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "Now we discuss another probability - based identification and compare between them .", "entities": []}, {"text": "We can also define the outcome Yof a test example xias the predicted probability of ( pseudo ) true label given by the trained model f(\u22c5 ): Yi(0)\u2236=Pf(L\u2032=l\u2032 i\u2223X = xi)\u2208(0,1).(11)4004", "entities": []}, {"text": "Similarly , the performance outcome Yof a perturbed test data point x\u2217 iis : Yi(1)\u2236=Pf(L\u2032=l\u2032 i\u2223X = x\u2217 i)\u2208(0,1).(12 )", "entities": []}, {"text": "For example , for a test example ( xi , l\u2032 i)which receives treatment ( l\u2032 i=1 ) , the trained model f(\u22c5)predicts its label as 1 with only a small probability 0.1 before treatment ( it has not been perturbed yet ) , and 0.9 after treatment .", "entities": []}, {"text": "So the Individual Treatment Effect ( ITE , see Equation 9 ) of this example is calculated as ITE i", "entities": []}, {"text": "= Yi(1)\u2212Yi(0)=0.9\u22120.1=0.8 .", "entities": []}, {"text": "We then take an average over all the perturbed test examples ( half of the test set ) as Average Treatment Effect ( ATE , see Equation 10 ) , which is exactly the learnability of a perturbation for a model .", "entities": []}, {"text": "To clarify , the two operands in Equation 10 are defined as follows : E[Y(1)]\u2236=P(f , g , p , D\u2032\u2217 test ) .", "entities": []}, {"text": "( 13 ) It means the average predicted probability of ( pseudo ) true label given by the trained model f(\u22c5 ) on the perturbed test set D\u2032\u2217 test .", "entities": []}, {"text": "E[Y(0)]\u2236=P(f , g , p , D\u2032 test ) .", "entities": []}, {"text": "( 14 ) Similarly , this is the average predicted probability on the randomly labeled test set D\u2032 test .", "entities": []}, {"text": "Notice that the accuracy - based definition of outcome Y(Equation 6 ) can also be written in a similar form to the probability - based one ( Equation 11 ): Yi(0)\u2236=1{f(xi)=l\u2032 i}=1{Pf(L\u2032=l\u2032 i\u2223X = xi)>0.5}\u2208{0,1 } .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "( 15 ) because the correctness of the prediction is equal to whether the predicted probability of true ( pseudo ) label exceeds a certain threshold ( i.e. , 0.5 ) .", "entities": []}, {"text": "The major difference is that , accuracy - based ITE is a discrete variable falling in { \u22121,0,1 } , while probability - based ITE is a continuous one ranging from -1 to 1 .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "For example , if a model learns to identify a perturbation and thus changes its prediction from wrong ( before perturbation ) to correct ( after perturbation ) , accuracy - based ITE will be 1\u22120=1while probability - based ITE will be less than 1 .", "entities": [[29, 30, "MetricName", "accuracy"]]}, {"text": "That is to say , accuracy - based ATE tends to vary more drastically than probability - based if inconsistent predictions occur more often , and thus can better capture the nuance of perturbation learnability .", "entities": [[5, 6, "MetricName", "accuracy"]]}, {"text": "Empirically , we find that accuracy - basedaverage learnability varies greatly ( \u03c3=0.375 , Table 4 ) and thus can better distinguish between different model - perturbation pairs than probabilitybased one ( \u03c3=0.288 , Table 4 ) .", "entities": [[5, 6, "MetricName", "accuracy"]]}, {"text": "As a result , we choose accuracy - based ATE as the primary measurement of learnability in this paper .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "D Investigating Learnability at a Specific Perturbation Probability Inspired by Precision @ K in Information Retrieval ( IR ) , we propose a similar metric dubbed Learnability @ p , which is the learnability of a perturbation for a model at a specific perturbation probability p.", "entities": [[8, 9, "DatasetName", "Inspired"], [10, 11, "MetricName", "Precision"], [14, 16, "TaskName", "Information Retrieval"]]}, {"text": "We are primarily interested in whether a selected pcan represent the learnability over different perturbation probabilities and correlates well with robustness and post data augmentation \u2206. We calculate the standard deviation ( \u03c3 ) of Learnability @ pand average learnability ( logAUC ) over all model - perturbation pairs to measure how well it can distinguish between different models and perturbations .", "entities": [[23, 25, "TaskName", "data augmentation"]]}, {"text": "Table 4 shows that average learnability is more diversified than all Learnability @ pand diversity ( \u03c3 ) peaks at p=0.01for accuracybased / probability - based measurement .", "entities": []}, {"text": "Accuracybased Learnability @ pis generally more diversified across models and perturbations than its counterpart .", "entities": []}, {"text": "To investigate the strength of the correlations , we also calculate Spearman \u03c1between accuracy - based / probability - based learnability @ p vs. average learnability / robustness / post data augmentation \u2206over all model - perturbation pairs .", "entities": [[13, 14, "MetricName", "accuracy"], [30, 32, "TaskName", "data augmentation"]]}, {"text": "Table 4 shows that generally average learnability has stronger correlation than Learnability @ p. Correlations with both robustness and post data augmentation \u2206peak at p=0.02for accuracybased / probability - based measurements , and the correlations with average learnability ( 0.816*/0.886 * ) are also strong at these perturbation probabilities .", "entities": [[20, 22, "TaskName", "data augmentation"]]}, {"text": "Overall , Learnability @ pwith higher standard deviation correlates better with average learnability , robustness and post data augmentation \u2206. Our analysis shows that if pis carefully selected by \u03c3 , Learnability @ pis also a promising metric , though not as accurate as average learnability .", "entities": [[17, 19, "TaskName", "data augmentation"]]}, {"text": "One advantage of Learnability @ pover average learnability is that it costs less time to obtain learnability at a single perturbation probability .", "entities": []}, {"text": "E Additional Experiment Results4005", "entities": []}, {"text": "pAccuracy - based Learnability @ p Probability - based Learnability @ p \u03c3 Avg Learn .", "entities": []}, {"text": "Robu .", "entities": []}, {"text": "Post Aug \u2206 \u03c3 Avg Learn .", "entities": []}, {"text": "Robu .", "entities": []}, {"text": "Post Aug \u2206 Avg . 0.375 1.000 * -0.643 * 0.756 * 0.288 1.000 * -0.652 * 0.727 * 0.001 0.182 0.426 * -0.265 0.259 0.114 0.367 * -0.279 0.288 0.005 0.235 0.637 * -0.383 * 0.522 * 0.192 0.925 *", "entities": []}, {"text": "-0.620 * 0.702 * 0.01 0.263 0.741 * -0.530 * 0.635 * 0.192 0.893 * -0.567 * 0.586 * 0.02 0.257 0.816 * -0.636 *", "entities": []}, {"text": "0.743 * 0.192 0.886 * -0.686 * 0.690 * 0.05 0.236 0.279 -0.158 0.136 0.121 0.576 * -0.371 * 0.350 *", "entities": []}, {"text": "0.1 0.241 0.354 * -0.162 0.192 0.115 0.543 * -0.288 0.258 0.5 0.094 0.024 0.155 -0.179 0.037 -0.080 0.114 -0.258 1.0 0.011 -0.199 0.252 -0.332 0.019 -0.220", "entities": []}, {"text": "0.294 -0.402 *", "entities": []}, {"text": "Table 4 : Standard deviations ( \u03c3 ) of Learnability @ pand Spearman correlations between accuracy - based / probabilitybased learnability @ pvs .", "entities": [[15, 16, "MetricName", "accuracy"]]}, {"text": "average learnability / robustness / post data augmentation \u2206over all model - perturbation pairs on IMDB dataset.\u2217indicates significance ( p - value < 0.05 ) .", "entities": [[6, 8, "TaskName", "data augmentation"], [15, 16, "DatasetName", "IMDB"]]}, {"text": "0.5 1.0 1.5 2.0 avg learnability0.4", "entities": []}, {"text": "0.3 0.2 0.1 0.00.10.20.30.4robustness", "entities": []}, {"text": "= 0.821 * ( a ) Learnability vs. Robustness 0.5 1.0 1.5 2.0 avg learnability0.2 0.1 0.00.10.2post aug   = 0.846 *   ( b ) Learnability vs. Post Aug \u2206 0.4   0.3   0.2   0.1   0.0 robustness0.000.050.100.150.200.25post aug   0.500.751.001.251.501.752.002.25 avg learnability ( c ) Learn .", "entities": []}, {"text": "vs. Robu .", "entities": []}, {"text": "vs. Post Aug \u2206", "entities": []}, {"text": "Figure 5 : Linear regression plots of learnability vs. robustness vs. post data augmentation \u2206onYELP dataset .", "entities": [[3, 5, "MethodName", "Linear regression"], [12, 14, "TaskName", "data augmentation"]]}, {"text": "Each point in the plots represents a model - perturbation pair .", "entities": []}, {"text": "\u03c1is Spearman correlation.\u2217indicates high significance ( p - value<0.001 ) .", "entities": []}, {"text": "0.5 1.0 1.5 avg learnability0.3 0.2 0.1 0.0robustness = 0.695 * ( a ) Learnability vs. Robustness 0.5 1.0 1.5 avg learnability0.2 0.1 0.00.10.2post aug   = 0.75 *   ( b ) Learnability vs. Post Aug \u2206 0.3   0.2   0.1   0.0 robustness0.2 0.1 0.00.10.2post aug   0.40.60.81.01.21.41.61.8 avg learnability ( c ) Learn .", "entities": []}, {"text": "vs. Robu .", "entities": []}, {"text": "vs. Post Aug \u2206", "entities": []}, {"text": "Figure 6 : Linear regression plots of learnability vs. robustness vs. post data augmentation \u2206onQQP dataset .", "entities": [[3, 5, "MethodName", "Linear regression"], [12, 14, "TaskName", "data augmentation"]]}, {"text": "Each point in the plots represents a model - perturbation pair .", "entities": []}, {"text": "\u03c1is Spearman correlation.\u2217indicates high significance ( p - value<0.001).4006", "entities": []}, {"text": "Perturbation RoBERTa XLNet TextRNN BERTAverage over models shuffle_word 1.538 1.586 0.401 1.854 1.345 butter_fingers_perturbation 1.301 1.433 1.425 1.758 1.479 whitespace_perturbation 1.276 1.449 1.720 1.569 1.504 insert_abbreviation 1.437 1.370 2.241 1.572 1.655 random_upper_transformation 1.432 1.828 1.733 1.715 1.677 visual_attack_letters 2.060 2.006 2.030 1.808 1.976 leet_letters 2.083 1.947 2.359 1.824 2.053 Table 5 : Average learnability ( logAUC of corresponding curve in Figure 3 ) of each model \u2013 perturbation pair on YELP dataset .", "entities": [[1, 2, "MethodName", "RoBERTa"], [2, 3, "MethodName", "XLNet"]]}, {"text": "Rows are sorted by average values over all models .", "entities": []}, {"text": "The perturbation for which a model is most learnable is highlighted in bold while the following one is underlined .", "entities": []}, {"text": "Perturbation RoBERTa TextRNN XLNet BERTAverage over models whitespace_perturbation 0.732 0.399 0.562 0.711 0.601 duplicate_punctuations 0.722 0.823 0.640 0.872 0.764 butter_fingers_perturbation 0.555 0.878 0.775 1.022 0.808 insert_abbreviation 0.820 1.440 0.960 1.206 1.107 random_upper_transformation 1.062 0.664 1.392 1.483 1.150 shuffle_word 1.231 0.816 1.552 1.623 1.306 visual_attack_letters 1.429 1.810 1.744 1.608 1.648 leet_letters 1.720 1.676 1.840 1.718 1.738 Table 6 : Average learnability ( logAUC of corresponding curve in Figure 3 ) of each model \u2013 perturbation pair on QQP dataset .", "entities": [[1, 2, "MethodName", "RoBERTa"], [3, 4, "MethodName", "XLNet"], [76, 77, "DatasetName", "QQP"]]}, {"text": "Rows are sorted by average values over all models .", "entities": []}, {"text": "The perturbation for which a model is most learnable is highlighted in bold while the following one is underlined .4007", "entities": []}]