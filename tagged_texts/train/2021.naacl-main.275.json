[{"text": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 3500\u20133510 June 6\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics3500Multi - Style Transfer with Discriminative Feedback on Disjoint Corpus Navita Goyal , Balaji Vasan Srinivasan , Anandhavelu N , Abhilasha Sancheti Adobe Research , India { navgoyal , balsrini , anandvn , sancheti}@adobe.com Abstract Style transfer has been widely explored in natural language generation with non - parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus .", "entities": [[7, 9, "TaskName", "Style Transfer"], [40, 42, "TaskName", "Style transfer"]]}, {"text": "A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration .", "entities": []}, {"text": "Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions .", "entities": []}, {"text": "While cascading single - dimensional models across multiple styles is a possibility , it suffers from content loss , especially when the style dimensions are not completely independent of each other .", "entities": [[17, 18, "MetricName", "loss"]]}, {"text": "In our work , we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations .", "entities": []}, {"text": "We initialize an encoder - decoder setup with transformerbased language model pre - trained on a generic corpus and enhance its re - writing capability to multiple target style dimensions by employing multiple style - aware language models as discriminators .", "entities": []}, {"text": "Through quantitative and qualitative evaluation , we show the ability of our model to control styles across multiple style dimensions while preserving content of the input text .", "entities": []}, {"text": "We compare it against baselines involving cascaded state - of - the - art uni - dimensional style transfer models .", "entities": [[17, 19, "TaskName", "style transfer"]]}, {"text": "1 Introduction Style transfer is a popular task in natural language processing and has been studied on attributes like age or gender ( Subramanian et al . , 2018 ) , styles emanating from social construct like formality ( Rao and Tetreault , 2018 ) and politeness ( Madaan et al . , 2020 ) , linguistic styles based on author writing style ( Syed et al . , 2020 ) , or psycho - linguistic styles based on personality types ( Mairesse and Walker , 2011 ) .", "entities": [[2, 4, "TaskName", "Style transfer"]]}, {"text": "While early style transfer frameworks were modeled as a supervised learning taskon a parallel corpus , state - of - the - art models are semi - supervised / unsupervised and operate on nonparallel corpus .", "entities": [[2, 4, "TaskName", "style transfer"]]}, {"text": "These models achieve style transfer by aligning source and target distribution of sentences from non - parallel corpus ( Shen et al . , 2017 ) , disentangling content space from style space in latent representation ( Hu et al . , 2017 ) or employing self - reconstruction ( Dai et al . , 2019 ) and back translation ( Lample et al . , 2018 ) objectives to achieve pseudo - supervision with non - parallel corpus .", "entities": [[3, 5, "TaskName", "style transfer"]]}, {"text": "Recent works have also modeled this in a self - supervised manner where rewriting ( transfer ) is achieved by utilizing corpus from the target style alone ( Syed et al . , 2020 ) .", "entities": []}, {"text": "These wide studies have also led to the curation and benchmarking of non - parallel dataset for various style dimensions , such as sentiment ( Li et al . , 2018 ) , formality ( Rao and Tetreault , 2018 ) , politeness ( DanescuNiculescu - Mizil et al . , 2013 ) , excitement ( Sancheti et al . , 2020 ) , etc .", "entities": []}, {"text": "But availability of data with joint tagging across multiple styles is limited and has restricted the ability of existing approaches to scale from single - dimensional transfer to multiple style dimensions .", "entities": []}, {"text": "In this paper , we propose a multidimensional style transfer approach that can work off partially labelled data for style transfer across multiple dimensions simultaneously .", "entities": [[8, 10, "TaskName", "style transfer"], [19, 21, "TaskName", "style transfer"]]}, {"text": "The work by Subramanian et al .", "entities": []}, {"text": "( 2018 ) attempts style transfer with multiple attributes such as age , gender , and sentiment simultaneously .", "entities": [[4, 6, "TaskName", "style transfer"]]}, {"text": "However , their approach avails corpus tagged with each of these three style dimensions .", "entities": []}, {"text": "In contrast to this and other similar explorations in multi - style transfer , our approach does not require jointly labelled data across all the stylistic dimensions in source and/or target corpus .", "entities": [[11, 13, "TaskName", "style transfer"]]}, {"text": "We focus on the problem where independent corpus is available across different stylistic dimensions ( say sentiment andformality ) and we achieve style transfer spanning different stylistic dimensions ( say make a sentence more positive and formal ) .", "entities": [[22, 24, "TaskName", "style transfer"]]}, {"text": "While state - of - the - art approaches can be", "entities": []}, {"text": "3501extended to achieve this by sequentially transferring one style after another , it is limited as different style dimensions are not necessarily independent of each other .", "entities": []}, {"text": "In aspects that are not independent , changing one style aspect of the text might affect another aspect considered , making a sequential brute - force approach non - ideal .", "entities": []}, {"text": "As we show in our experiments later , the cascaded setup also lacks common grounding between the content from different styles leading to erratic changes in content .", "entities": []}, {"text": "We circumvent this by grounding our framework on the linguistic understanding of a large language model .", "entities": []}, {"text": "Our model builds understanding of interplay between the different styles by incorporating multiple discriminative language models ( LM ) with language model - based encoder - decoder setup .", "entities": []}, {"text": "The key contributions of this paper are : 1)An encoder - decoder setup with multiple language models as discriminator , with each entity harnessing the language understanding from a large pre - trained transformer model .", "entities": []}, {"text": "2)Relaxing the requirement of jointly labelled data for multi - style transfer , by leveraging independently acquired disjoint corpus for different styles .", "entities": [[10, 12, "TaskName", "style transfer"]]}, {"text": "3)Achieving better style control with better content preservation in multi - dimensional style transfer than a cascaded setup of state - of - the - art unidimensional style transfer models .", "entities": [[12, 14, "TaskName", "style transfer"], [27, 29, "TaskName", "style transfer"]]}, {"text": "2 Related Work One line of work in style transfer attempts to learn disentangled latent representation for style and content , and transfer style by manipulating latent representation of style ( Shen et al . , 2017 ) .", "entities": [[8, 10, "TaskName", "style transfer"]]}, {"text": "Although these approaches perform well with one style at a time , they do not trivially scale to multidimensional style transfer .", "entities": [[19, 21, "TaskName", "style transfer"]]}, {"text": "Several other works develop unsupervised approach for style transfer by employing Denoising Autoencoding ( DAE ) ( Fu et al . , 2017 ) and back - translation ( BT ) ( Lample et al . , 2018 ) loss to develop interaction and hence transfer between the source and target domain .", "entities": [[7, 9, "TaskName", "style transfer"], [11, 12, "TaskName", "Denoising"], [39, 40, "MetricName", "loss"]]}, {"text": "Subramanian et al .", "entities": []}, {"text": "( 2018 ) extend this approach to multiple styles by conditioning on average of embedding of each target attribute and using combination of DAE and back - translation techniques .", "entities": []}, {"text": "DAE takes as input a sentence xfrom stylesand tries to reconstruct sentence xfrom its corrupted version ~ x.", "entities": []}, {"text": "This relies on the assumption that the input sentence xis from a certain style combination s = fs1;s2;:::;skg .", "entities": []}, {"text": "Similarly back translation(BT ) objective with input sentence xfrom style s , \ufb01rst estimates x0 = f(x;s0 ) , wheres6 = s0and then reconstruct xfrom ~x = f(x0;s ) .", "entities": []}, {"text": "Thus , these approaches are inherently dependent on knowledge of annotation of each sentence with all the style combinations .", "entities": []}, {"text": "Dai et al .", "entities": []}, {"text": "( 2019 ) achieve state - ofthe - art style transfer in single style dimensions by employing transformer - based model in conjunction with classi\ufb01er - based discriminator .", "entities": [[9, 11, "TaskName", "style transfer"]]}, {"text": "In addition to discriminator losses , their proposed technique uses self - reconstruction and cycle reconstruction losses , which similar to DAE and BT losses are also reliant on availability of jointly annotated data to be extendable to multiple style setup .", "entities": []}, {"text": "Language modeling is integral to several natural language generation ( NLG ) tasks like text summarization , spelling correction , image captioning , etc .", "entities": [[14, 16, "TaskName", "text summarization"], [17, 19, "TaskName", "spelling correction"], [20, 22, "TaskName", "image captioning"]]}, {"text": "The model architecture for these tasks has evolved from n - gram based methods to Recurrent Neural Networks to transformer architectures .", "entities": []}, {"text": "The introduction of Transformer - based architecture accompanied with generative pre - training ( Radford , 2018 ) capabilities have led to strong improvements in many downstream generation and GLUE ( Wang et al . , 2018 ) tasks .", "entities": [[3, 4, "MethodName", "Transformer"], [29, 30, "DatasetName", "GLUE"]]}, {"text": "Generative pre - training aims to adapt a large Transformer language model to large unsupervised corpus .", "entities": [[9, 10, "MethodName", "Transformer"]]}, {"text": "This capability of generative pre - training is exploited in many large language models like BERT ( Devlin et al . , 2019 ) , GPT-2 ( Radford et al . , 2018 ) , ERNIE 2.0 ( Sun et al . , 2020 ) which have the ability to perform tasks like reading comprehension ( Xu et al . , 2019 ) , summarization ( Liu and Lapata , 2019 ) , question - answering ( Rajpurkar et al . , 2016 ) and translation ( Clinchant et", "entities": [[15, 16, "MethodName", "BERT"], [25, 26, "MethodName", "GPT-2"], [53, 55, "TaskName", "reading comprehension"], [64, 65, "TaskName", "summarization"]]}, {"text": "al . , 2019 ) in zero - shot and few - shot settings", "entities": []}, {"text": ".", "entities": []}, {"text": "Recently these pre - trained generative language models have been explored in translation ( Conneau and Lample , 2019 ) and style transfer tasks ( Syed et al . , 2020 ) .", "entities": [[21, 23, "TaskName", "style transfer"]]}, {"text": "Conneau and Lample ( 2019 ) develop cross - lingual models for unsupervised machine translation by initializing encoder and decoder with a pre - trained language model trained on Masked Language Modeling ( MLM ) ( Devlin et al . , 2019 ) objective and \ufb01ne - tuning the encoderdecoder framework with adversarial training .", "entities": [[12, 15, "TaskName", "unsupervised machine translation"], [29, 32, "TaskName", "Masked Language Modeling"], [33, 34, "DatasetName", "MLM"]]}, {"text": "Syed et al .", "entities": []}, {"text": "( 2020 ) extend this to stylized re - writing task by employing DAE during \ufb01ne - tuning .", "entities": []}, {"text": "The joint encoder - decoder framework learns to reconstruct sentences in target - domain from its noisy version using DAE objective .", "entities": []}, {"text": "As previously discussed , the DAE objective is reliant on the corpus being tagged", "entities": []}, {"text": "3502for the target domain style ( or combination of style ) and restricts the generalization of this setup to multiple attributes .", "entities": []}, {"text": "We overcome this by employing discriminative language models to assist the decoder with feedback for various target styles .", "entities": []}, {"text": "Shen et al .", "entities": []}, {"text": "( 2017 ) show that even with nonparallel data , the content distribution across source and target style is shared .", "entities": []}, {"text": "Based on this , a language model trained on target style will have high perplexity on transferred text if it does not match target style and low perplexity otherwise .", "entities": [[14, 15, "MetricName", "perplexity"], [27, 28, "MetricName", "perplexity"]]}, {"text": "Yang et al .", "entities": []}, {"text": "( 2018 ) exploit this ability of language models to replace standard binary classi\ufb01er - based discriminators with an implicitly trained language model as discriminator .", "entities": []}, {"text": "They show that using the language model as structured discriminator allows for more stable training by eliminating the adversarial step .", "entities": []}, {"text": "We extend this idea to a multi - discriminator approach .", "entities": []}, {"text": "Training a LM on combination of target styles is not possible in absence of jointly labelled dataset .", "entities": []}, {"text": "Due to this , we attempt to use multiple discriminators for each of the target styles .", "entities": []}, {"text": "Since with multiple styles , the underlying corpus is independently acquired , the variation in content distribution across different styles is more noticeable .", "entities": []}, {"text": "Consequently , an independently trained LM on one of the target styles might have high perplexity even if the transferred sentence \ufb01ts in the corresponding target style , due to the content space of source sentence .", "entities": [[15, 16, "MetricName", "perplexity"]]}, {"text": "To equip discriminative LM with more generalized notion of content , we use large transformer - based LM pre - trained on large unsupervised corpus to establish generic content distribution before style - oriented \ufb01ne - tuning .", "entities": []}, {"text": "3 Approach Our proposed approach has two key elements \u2014 a Transformer - based encoder - decoder model initialized with a pre - trained Transformer Language Model and \ufb01ne - tuned on DAE loss to achieve style transfer ( Section 3.1 ) and the multiple language models as discriminators stacked together to enable multi - style transfer ( Section 3.2 ) .", "entities": [[11, 12, "MethodName", "Transformer"], [24, 25, "MethodName", "Transformer"], [33, 34, "MetricName", "loss"], [36, 38, "TaskName", "style transfer"], [55, 57, "TaskName", "style transfer"]]}, {"text": "3.1 Pre - trained LM as Encoder - Decoder Similar to Syed et al .", "entities": []}, {"text": "( 2020 ) , we \ufb01rst pre - train a Transformer - based language model with Masked Language Modeling ( MLM ) objective on English Wikipedia data extracted using WikiExtractor.1", "entities": [[10, 11, "MethodName", "Transformer"], [16, 19, "TaskName", "Masked Language Modeling"], [20, 21, "DatasetName", "MLM"]]}, {"text": "This equips LM with the ability to predict masked 1https://github.com/attardi/wikiextractorwords over a large corpus .", "entities": []}, {"text": "Masked Language Modeling leverages bidirectional context of the input , thus enabling better language understanding .", "entities": [[0, 3, "TaskName", "Masked Language Modeling"]]}, {"text": "Following Masked Language Modeling objective from Devlin et al .", "entities": [[1, 4, "TaskName", "Masked Language Modeling"]]}, {"text": "( 2019 ) , we randomly sample 15 % of the tokens from the text stream and replace them with the [ MASK ] token 80 % of the time , by a random token 10 % of the time and keep them unchanged 10 % of the time , with the objective of predicting the original identity of the masked word based on its bidirectional context .", "entities": []}, {"text": "To enable style transfer from a given sentence to target style , we use independently trained language models ( LMs ) to initialize the encoder and decoder and connect these with randomly initialized attention layers to arrive at a encoder - decoder setup .", "entities": [[2, 4, "TaskName", "style transfer"], [33, 35, "HyperparameterName", "attention layers"]]}, {"text": "As discussed by Syed et al .", "entities": []}, {"text": "( 2020 )", "entities": []}, {"text": ", the Transformer architecture ( Vaswani et al . , 2017 ) allows such independent initialization by implicitly aligning encoder - decoder layers via attention mechanism .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "Pre - training an encoder only transformer on generative task and then leveraging it to initialize as both encoder and decoder as opposed to pretraining a joint encoder - decoder model has several advantages .", "entities": []}, {"text": "Transformer - based models with encoder - only ( Devlin et al . , 2019 ) or decoder - only ( Radford et al . , 2018 ) blocks have been shown to perform well in generative pre - training task .", "entities": [[0, 1, "MethodName", "Transformer"]]}, {"text": "Clearly , pre - training a single transformer block on generative task and then utilizing it as both encoder and decoder blocks has lower computational cost than training the entire encoder - decoder block jointly .", "entities": []}, {"text": "Moreover , this also enables us to use the same pre - trained model to initialize both style transfer module and the discriminator models , explained in the following section .", "entities": [[17, 20, "MethodName", "style transfer module"]]}, {"text": "This is not only computationally more ef\ufb01cient but it also closely ties the underlying language distribution of the two modules .", "entities": []}, {"text": "This is expected to make the discriminative feedback more effective while \ufb01ne tuning the transfer model for multiple styles .", "entities": []}, {"text": "In Syed et al .", "entities": []}, {"text": "( 2020 ) \u2019s setup , both encoder and decoder in the style transfer module are initialized with the pre - trained language model ( trained on MLM objective ) .", "entities": [[12, 15, "MethodName", "style transfer module"], [27, 28, "DatasetName", "MLM"]]}, {"text": "Instead , we initialize the decoder with the language model \ufb01ne - tuned with the target style using Causal Language Modeling ( CLM ) objective , before training the joint encoder - decoder model , as detailed in Section 3.2 .", "entities": []}, {"text": "The encoder is initialized with the pre - trained model directly .", "entities": []}, {"text": "Aligning the decoder to the distribution of the tar-", "entities": []}, {"text": "3503 .", "entities": []}, {"text": ". . .Transformer", "entities": []}, {"text": "Layer 12 012n < bos >", "entities": []}, {"text": "The < mask > citycity \u2026 .", "entities": []}, {"text": "\u2026 .Position", "entities": []}, {"text": "EmbeddingsText EmbeddingsPre - trainingMasked Language ModelingMasked xDiscriminator   loss - kDiscriminator   loss - 2 012nThecat < drop><bos > Thebig \u2026 .", "entities": [[8, 9, "MetricName", "loss"], [12, 13, "MetricName", "loss"]]}, {"text": "\u2026 .Encoder - decoder Fine - tuningEncoderDecoderDe - noising Autoencoding Objective Noisy xTransferred x 01n < bos > Thebig \u2026 .", "entities": []}, {"text": "\u2026 .DAE", "entities": []}, {"text": "lossDiscriminator   loss - 1\u2295 Transformer Layer 3Transformer", "entities": [[2, 3, "MetricName", "loss"], [5, 6, "MethodName", "Transformer"]]}, {"text": "Layer 2Transformer", "entities": []}, {"text": "Layer 1 < bos >", "entities": []}, {"text": "The < bos>012n < bos > Thecatcitycity \u2026 .", "entities": []}, {"text": "\u2026 .Discriminator", "entities": []}, {"text": "Fine - tuningCausal Language Modeling Original x < eos > The .", "entities": []}, {"text": ". . .Transformer", "entities": []}, {"text": "Layer 12Transformer Layer 1Figure 1 : Model Architecture - Left : Generative pre - training using MLM objective , and Fine - tuning encoderdecoder LM with multiple discriminative losses and Right : Discriminator \ufb01ne - tuning with language modeling ( next token prediction ) objective .", "entities": [[16, 17, "DatasetName", "MLM"]]}, {"text": "Color for model blocks represents the pre - trained model used for initialization prior to \ufb01ne - tuning .", "entities": []}, {"text": "get style helps speed up the \ufb01ne - tuning process as decoder is more adept at generating stylized outputs .", "entities": []}, {"text": "This does not add to computational overhead as these \ufb01ne - tuned models are repurposed as discriminators for stylistic feedback ( Section 3.2 ) .", "entities": []}, {"text": "To instill style - awareness to the encoder - decoder setup initialized with pre - trained Transformer models , we \ufb01ne - tune it with Denoising Autoencoder ( DAE ) loss using the target - domain corpus .", "entities": [[16, 17, "MethodName", "Transformer"], [25, 27, "MethodName", "Denoising Autoencoder"], [30, 31, "MetricName", "loss"]]}, {"text": "In case of multiple styles , we use a randomized mixture of target - domain corpus from each of the target styles .", "entities": []}, {"text": "Under the DAE objective , the encoder takes a noisy masked version ~xof the textxas input and attempts to \ufb01ll in the mask token as per the MLM objective that it was pre - trained on .", "entities": [[27, 28, "DatasetName", "MLM"]]}, {"text": "In turn , the decoder re - creates stylistic version of original sentence from this noisy output from the encoder .", "entities": []}, {"text": "The overall training objective is LDAE(\u0012G )", "entities": []}, {"text": "= Ex\u0018T[\u0000logP\u0012G(xj ~ x)];(1 ) where\u0012Gare the trainable parameters of the encoder - decoder model .", "entities": []}, {"text": "The noisy version of sentencexfrom the target corpus Tis obtained after dropping tokens from xwith probability pdropand masking with a probability of pmask .", "entities": []}, {"text": "In conjunction , the encoder and decoder enable style transfer to the target style .", "entities": [[8, 10, "TaskName", "style transfer"]]}, {"text": "The noteworthy aspect here is that the model has no sense of source style and is trained to generate sentences to match the style of the target - domain corpus with which it is trained.3.2", "entities": []}, {"text": "Fine - tuned LM as discriminators To extend the single - dimensional style transfer setup above to multi - dimensional setting", "entities": [[12, 14, "TaskName", "style transfer"]]}, {"text": ", we use language models as discriminators to provide the feedback to the model for partially annotated nature of input data .", "entities": []}, {"text": "As opposed to a classi\ufb01er - based discriminator , the language model as discriminator takes into account the wider language distribution of the target style .", "entities": []}, {"text": "Additionally , such a setup allows us to use only the target style corpus for training the transfer model , whereas the classi\ufb01er would require both source and target style corpus to distinguish between a sentence as being from one style or another .", "entities": []}, {"text": "Inspired by Yang et al .", "entities": [[0, 1, "DatasetName", "Inspired"]]}, {"text": "( 2018 ) , we \ufb01ne - tune a language model on the target style si , so that the language model is equipped with language distribution of target domain data .", "entities": []}, {"text": "This entails generating the probability of next token , given the previous tokens \u2014 also known as Causal Language Modeling objective ( Conneau and Lample , 2019 ) .", "entities": []}, {"text": "The training loss for the LM for target style siwith corresponding corpus Tiis Ex\u0018Ti\u0014nX t=1[\u0000logPLM(xtjx1;:::;xt\u00001)]\u0015 ( 2 ) We show in our experiments that such a \ufb01netuning step transforms language distribution of this language model to style siand hence serve as softdiscriminator for our framework .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "We exploit this capability of language models to imbibe style of", "entities": []}, {"text": "3504\ufb01ne - tuning corpus by employing language models as style discriminators for transferred sentences .", "entities": []}, {"text": "This is based on the idea that if the transferred sentence does not \ufb01t well in the target style , then the perplexity of language model \ufb01ne - tuned on that style will be high ( Section 4.1 ) .", "entities": [[22, 23, "MetricName", "perplexity"]]}, {"text": "Fork - dimensional style transfer with target styless = fs1;s2;:::;skg , we independently \ufb01netuneklanguage models on each of the target styles .", "entities": [[3, 5, "TaskName", "style transfer"]]}, {"text": "As discussed in Yang et al .", "entities": []}, {"text": "( 2018 ) , we are able to forgo the adversarial training for the discriminator , since the \ufb01ne - tuned discriminative language model is implicitly capable of assigning high perplexity to negative samples ( out - of - style samples ) , as shown in Section 4.1 .", "entities": [[30, 31, "MetricName", "perplexity"]]}, {"text": "For the transferred sentence x0 , the training objective for each target style siis , argmin \u0012GLsi", "entities": []}, {"text": "= Ex\u0018T;x0\u0018P\u0012G(x )", "entities": []}, {"text": "\u0014nX t=1\u0000logPLMi(x0 tjx0 1;::;x0 t\u00001)\u0015(3 )", "entities": []}, {"text": "This dictates that transferred sentence x0has low perplexity on the language model \ufb01ne - tuned on stylesi , for each target style si .", "entities": [[7, 8, "MetricName", "perplexity"]]}, {"text": "However , we can not directly \ufb01nd the argmin\u0012Gusing gradient descent because of discrete sampling of x0\u0018P\u0012G(x ) .", "entities": []}, {"text": "To account for this , we use a policy gradient reinforcement learning approach using REINFORCE algorithm ( Sutton et al . , 1999 ) .", "entities": [[14, 15, "MethodName", "REINFORCE"]]}, {"text": "The reward for an input sequence xto the style discriminator LMiis calculated as , r(x ) = nX t=1logPLMi(xtjx1;::;xt\u00001)(4 )", "entities": []}, {"text": "Using these rewards , the RL objective is to minimize the lossLsigiven by , Lsi = Ex\u0018T;x0\u0018P\u0012G(x)(r(x0)\u0000r(x ) )", "entities": []}, {"text": "[ \u0000logP\u0012G(x0j ~ x)](5 ) for stylesi , whereP\u0012G(xj ~ x)is as in Equation 1 andr(x0)is the reward in the Equation 4 for the transferred sentence x0 .", "entities": []}, {"text": "The rewards r(x)represents the baseline reward of greedily sampling the input sequence xby", "entities": []}, {"text": "the style discriminator LMi .", "entities": []}, {"text": "For the style combination s = fs1;s2;:::;skg , the joint encoder - decoder model is trained on randomized mixture of data from each of the targetdomain corpus .", "entities": []}, {"text": "The mixture is thus agnostic of individual style of each of the sentence and thediscriminative LM for each style guides the generation towards that speci\ufb01c style by rewarding style adherence in the transferred sentence .", "entities": []}, {"text": "Randomized mixture of training corpus across styles allows for uni\ufb01ed and cohesive understanding of multiple styles by diversifying rewards from different discriminators across samples .", "entities": []}, {"text": "The overall training loss for the joint encoder - decoder model is L=\u0015DAEEx\u0018T[\u0000logP\u0012(xj ~ x ) ]", "entities": [[3, 4, "MetricName", "loss"]]}, {"text": "+ kX i=1\u0015iLsi;(6 ) whereLsiis as de\ufb01ned in Equation 5 , and \u0015DAE andf\u0015igk i=1are hyper - parameters .", "entities": []}, {"text": "The overall training process is summarized in Figure 1 .", "entities": []}, {"text": "First , we pre - train a transformer model with Masked language modeling objective as shown in Figure 1(Left ) .", "entities": [[10, 13, "TaskName", "Masked language modeling"]]}, {"text": "We then initialize discriminator model with this pre - trained language model and \ufb01ne - tune it with Causal language modeling objective , shown in Figure 1(Right ) , for each target style .", "entities": []}, {"text": "Finally , we initialize the encoder and decoder of the style transfer module with the pretrained and style - speci\ufb01c \ufb01ne - tuned language models , respectively .", "entities": [[10, 13, "MethodName", "style transfer module"]]}, {"text": "In case of multiple styles , the decoder can be initialized with the language model which is \ufb01ne - tuned with CLM loss on the mixture of data from target styles , i.e. , CLM loss in Equation 2 with x\u0018T.", "entities": [[22, 23, "MetricName", "loss"], [35, 36, "MetricName", "loss"]]}, {"text": "The joint encoder - decoder model ( Figure 1(Centre ) ) is then trained with a combination of DAE objective and rewards from \ufb01ne - tuned discriminators of respective target styles .", "entities": []}, {"text": "4 Experiments We experiment with a combination of sentiment and formality styles .", "entities": []}, {"text": "For sentiment , we use a mixture of IMDB ( Maas et al . , 2011 ) and Yelp dataset ( Li et al . , 2018 ) with 300kexamples in the positive and negative sentiment each .", "entities": [[8, 9, "DatasetName", "IMDB"]]}, {"text": "For formality , we use GYAFC corpus ( Rao and Tetreault , 2018 ) which has104kexamples in each formal and informal class .", "entities": [[5, 6, "DatasetName", "GYAFC"]]}, {"text": "The test set has 3000 and4849 examples for sentiment and formality respectively , following the data split available in Dai et al .", "entities": []}, {"text": "( 2019 ) ; Rao and Tetreault ( 2018 ) .", "entities": []}, {"text": "For both datasets , the training corpus is non - parallel and the test corpus has human written references available , which we use for content evaluation ( Section 4.2 ) .", "entities": []}, {"text": "For pre - training , we use 12 - layer Transformer model with 512hidden units , 16heads , a dropout rate of 0:1and learned positional embedding .", "entities": [[10, 11, "MethodName", "Transformer"]]}, {"text": "We train our models with the Adam optimizer , and", "entities": [[6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "optimizer"]]}, {"text": "3505Style / Dimension Sentiment % Formality % Positive 71.41 67.09 Negative 76.17 75.59 Table 1 : Accuracy of sentences generated by model \ufb01ne - tuned on style sias % of generated sentences labelled as class siby the classi\ufb01er trained on the corresponding style dimension .", "entities": [[16, 17, "MetricName", "Accuracy"]]}, {"text": "Fine - tuning Test Corpus corpus Same#Opposite \" Positive 6.9275 9.6850 Negative 7.7131 9.9637 Table 2 : Perplexity of test corpus on models \ufb01ne - tuned positive and negative corpus ( rows ) .", "entities": [[17, 18, "MetricName", "Perplexity"]]}, {"text": "The column Same represents that test corpus is same as \ufb01ne - tuning corpus , leading to lower perplexities and Opposite represent test corpus from opposite polarity as \ufb01ne - tuning corpus leading to higher perplexity .", "entities": [[35, 36, "MetricName", "perplexity"]]}, {"text": "a learning rate of 10\u00004 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "To handle large vocabulary sizes , we use Byte Pair Encoding ( BPE ) ( Sennrich et al . , 2016 ) learned on the Wikipedia dataset .", "entities": [[8, 11, "MethodName", "Byte Pair Encoding"], [12, 13, "MethodName", "BPE"]]}, {"text": "The\u0015s in Equation 6 are determined using hyperparameter tuning on validation set , with style transfer accuracy ( Section 4.2 ) as search criteria .", "entities": [[14, 16, "TaskName", "style transfer"], [16, 17, "MetricName", "accuracy"]]}, {"text": "4.1 Style - awareness of Language Models To evaluate style variation across language models \ufb01ne - tuned on different styles , we compare the generations of the \ufb01ne - tuned models .", "entities": []}, {"text": "For singledimensional style evaluation , we generate sentences from models \ufb01ne - tuned on negative corpus and positive corpus and compare the style accuracy of generated sentences .", "entities": [[23, 24, "MetricName", "accuracy"]]}, {"text": "The style accuracy is evaluated by employing a FastText ( Joulin et al . , 2016 ) classi\ufb01er trained on the corresponding style dimension .", "entities": [[2, 3, "MetricName", "accuracy"], [8, 9, "MethodName", "FastText"]]}, {"text": "For instance , the classi\ufb01er for evaluating sentiment accuracy is trained on sentiment corpus tagged with positive and negative class in IMDB and Yelp data .", "entities": [[8, 9, "MetricName", "accuracy"], [21, 22, "DatasetName", "IMDB"]]}, {"text": "Table 1 shows the accuracy of sentences generated by a model \ufb01ne - tuned on stylesias belonging to the class si .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "For both sentiment and formality , the \ufb01ne - tuned language models are able to generate text faithful to the target style dimension .", "entities": []}, {"text": "Thus , we conclude that the language models trained on style siare able to capture the essence of the corresponding style reasonably well .", "entities": []}, {"text": "These accuracies are an indication of the style awareness in these \ufb01ne - tuned LMs .", "entities": []}, {"text": "We , therefore , employ the perplexities of these \ufb01ne - tuned language models to gauge the style of the input textto guide our style transfer model .", "entities": [[24, 26, "TaskName", "style transfer"]]}, {"text": "As discussed in discriminative modeling ( Section 3.2 ) , the model \ufb01ne - tuned with corpus from a certain style is expected to have high perplexity on sentence not from that style and low perplexity otherwise .", "entities": [[26, 27, "MetricName", "perplexity"], [35, 36, "MetricName", "perplexity"]]}, {"text": "To this end , we experiment with two models independently \ufb01netuned on positive and negative corpus .", "entities": []}, {"text": "We calculate the perplexity of each of these models on the test corpus from the same style and from the opposite style .", "entities": [[3, 4, "MetricName", "perplexity"]]}, {"text": "As seen in Table 2 , the perplexity for each model is substantially lower on the same corpus as compared to that on the opposite corpus .", "entities": [[7, 8, "MetricName", "perplexity"]]}, {"text": "This implies that a language model \ufb01ne - tuned on positive corpus shows higher perplexity for negative sentences and lower for positive sentences and vice versa .", "entities": [[14, 15, "MetricName", "perplexity"]]}, {"text": "This corroborates the effectiveness of these \ufb01ne - tuned language models to serve as discriminators for training the style transfer module .", "entities": [[18, 21, "MethodName", "style transfer module"]]}, {"text": "4.2 Evaluation metrics We measure the performance of our model and the baselines based on the style control , content preservation and \ufb02uency .", "entities": []}, {"text": "To measure the accuracy of style transfer , we train two Fasttext2classi\ufb01ers independently for sentiment and formality using the train corpus , as described in Section 4.1 .", "entities": [[3, 4, "MetricName", "accuracy"], [5, 7, "TaskName", "style transfer"]]}, {"text": "These classi\ufb01ers have accuracy of 93:74 % and88:95 % respectively on test corpus of respective datasets .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "We note that formality as a style is more intricately designed , so we also check lexical scoring by Brooke et al .", "entities": []}, {"text": "( 2010 ) to evaluate formality , which uses a formality lexicon to assign formality score between \u00001(informal ) and 1(formal ) to each word and averages it .", "entities": []}, {"text": "We scale these scores between 0\u2013100 , where higher ( 100 ) lexical score signi\ufb01es formal style and lower ( 0)score signi\ufb01es informal style .", "entities": []}, {"text": "For informal target style , we report lexical score as100\u0000n , so that a higher average lexical score signi\ufb01es a better transfer for either polarity .", "entities": []}, {"text": "To measure content preservation on transfer , we calculate the BLEU score ( Papineni et al . , 2002 ) between the transferred sentence and the input sentence ( self - BLEU ) .", "entities": [[10, 12, "MetricName", "BLEU score"], [31, 32, "MetricName", "BLEU"]]}, {"text": "Besides this , we also calculate BLEU score between the transferred sentence generated by our model and the corresponding human reference transferred sentence , available for GYAFC and Yelp corpus ( ref - BLEU ) .", "entities": [[6, 8, "MetricName", "BLEU score"], [26, 27, "DatasetName", "GYAFC"], [33, 34, "MetricName", "BLEU"]]}, {"text": "Since both these corpus account for transfer across only one style dimension each , the provided references are only partial indication of expected outcome .", "entities": []}, {"text": "This 2https://github.com/facebookresearch/fastText", "entities": []}, {"text": "3506ModelStyle Accuracy Content Preservation Fluency Classi\ufb01er \" Lexical Scoring \" BLEU\"Perplexity#Sentiment Formality Formality -self -ref Cascaded Style Transformer72:17 64 : 08 81:29 0:6066 0:3479", "entities": [[1, 2, "MetricName", "Accuracy"]]}, {"text": "8:8657(Dai et", "entities": []}, {"text": "al . , 2019 ) Adapted Rewriting LM52:59 36 : 39 72:21 0.7917 0.4259 6:5963(Syed et al . , 2020 )", "entities": []}, {"text": "Cascaded Discriminative LM 69:30 48 : 18 83:02 0:6634 0:3579", "entities": []}, {"text": "6:6846 Joint Discriminative LM 79.78 65.33 85.39 0:7710 0:4136", "entities": []}, {"text": "6.4574 Table 3 : Quantitative Comparison of our proposed approach ( Joint Discriminative LM ) against Cascaded Style Transformer ( Dai et al . , 2019 ) , Cascaded Discriminative LM method and multi - style transfer using Adapted Rewriting LM ( Syed et al . , 2020 ) .", "entities": [[18, 19, "MethodName", "Transformer"], [35, 37, "TaskName", "style transfer"]]}, {"text": "The upward arrow signi\ufb01es that higher is better and vice versa .", "entities": []}, {"text": "Score of near 100on formality lexical scoring imply the transferred text is close in formality to the target corpus .", "entities": [[0, 1, "MetricName", "Score"]]}, {"text": "is also apparent from low ref - BLEU scores for our model as well as baselines .", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "Since , the results are presented on aggregated dataset from both these style dimensions , this evaluation is still able to provide reasonable indication of content preservation .", "entities": []}, {"text": "To measure the \ufb02uency of the text , we calculate perplexity assigned to the generated text sequence by a language model trained on the train corpus , as is standard in style transfer literature ( Dai et al . , 2019 ; Subramanian et al . , 2018 ) .", "entities": [[10, 11, "MetricName", "perplexity"], [31, 33, "TaskName", "style transfer"]]}, {"text": "The perplexity is the measure of log likelihood of the generated sentence on the language model .", "entities": [[1, 2, "MetricName", "perplexity"]]}, {"text": "A lower perplexity is indicative of a more \ufb02uent sentence .", "entities": [[2, 3, "MetricName", "perplexity"]]}, {"text": "We use a generative transformer - based language model trained on the dataset combined from two styles .", "entities": []}, {"text": "4.3 Automatic Evaluation Dai et al .", "entities": []}, {"text": "( 2019 ) use transformer - based model ( Style Transformer ) for single - dimensional style transfer .", "entities": [[10, 11, "MethodName", "Transformer"], [16, 18, "TaskName", "style transfer"]]}, {"text": "We train two independent Style Transformer models for sentiment and formality transfer and then perform transfer one after another to compare results with our model .", "entities": [[5, 6, "MethodName", "Transformer"]]}, {"text": "We term this as Cascaded Style Transformer setup .", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "The Style Transformer model is shown to have state - of - the - art performance in single - dimensional style transfer ; thus it provides an estimate of the performance of sequential single style transfer .", "entities": [[2, 3, "MethodName", "Transformer"], [20, 22, "TaskName", "style transfer"], [34, 36, "TaskName", "style transfer"]]}, {"text": "We also experiment with Adapted Rewriting LM ( Syed et al . , 2020 ) as another baseline .", "entities": []}, {"text": "Their work on style rewriting to match author - speci\ufb01c style does not require explicit annotations for the various aspects that constitutes an author \u2019s style , but is based on the assumption that the training corpus re\ufb02ects the target style .", "entities": []}, {"text": "In this context , we train their framework on the mixture of data from the respective target styles and report the performance .", "entities": []}, {"text": "These are the closest baselines to our proposed approach , since other works dealing with multi - style transfer assume presenceof jointly annotated dataset , which is a stronger assumption that we aim to relax .", "entities": [[17, 19, "TaskName", "style transfer"]]}, {"text": "In addition to our proposed model with multiple style transfer , we also train our encoder - decoder architecture with single discriminative LM for one style at a time and perform two stage transfer , similar to one with Cascaded Style Transformer ( Dai et al . , 2019 ) setup .", "entities": [[8, 10, "TaskName", "style transfer"], [41, 42, "MethodName", "Transformer"]]}, {"text": "The results in Table 3 show that our model achieves better style control than the Cascaded Style Transformer ( Dai et al . , 2019 ) as well as the joint transfer using Syed et al .", "entities": [[17, 18, "MethodName", "Transformer"]]}, {"text": "( 2020 ) for both sentiment and formality .", "entities": []}, {"text": "As seen in Table 3 , cascaded style transfer models perform poorly on content preservation .", "entities": [[7, 9, "TaskName", "style transfer"]]}, {"text": "This is because transferring style one after other leads to huge loss in content , thus both the two - stage models score lower on content preservation metrics , both w.r.t .", "entities": [[11, 12, "MetricName", "loss"]]}, {"text": "the input text and the reference transferred text .", "entities": []}, {"text": "This demonstrates the advantage of using single model to control for multiple styles .", "entities": []}, {"text": "The effect can also be observed in Table 4 which demonstrates qualitative results for Cascaded Style Transformer model and our model .", "entities": [[16, 17, "MethodName", "Transformer"]]}, {"text": "We can see in many cases content loses the underlying meaning of source sentence during the twostage transfer , whereas our model is able to retain original meaning of the sentence well , corroborating the \ufb01ndings of automatic evaluation .", "entities": []}, {"text": "Among the cascaded models , the Discriminative LM scores marginally better on content preservation than the Style Transformer model .", "entities": [[17, 18, "MethodName", "Transformer"]]}, {"text": "We attribute this to initialization with the same pre - trained LM resulting in shared content space in the underlying single style transfer models .", "entities": [[21, 23, "TaskName", "style transfer"]]}, {"text": "However , due to independent training of the two single style transfer models , they are not able to model interplay between these styles and hence perform worse on style control than our proposed model trained jointly on multiple styles .", "entities": [[10, 12, "TaskName", "style transfer"]]}, {"text": "Our model also scores better on \ufb02uency , as seen in Table 3 .", "entities": []}, {"text": "This is also apparent from the exam-", "entities": []}, {"text": "3507Target style Source sentenceTransferred Sentence Style Transformer Our model ( multi - style ) Positive+Formal", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "That \u2019s not funny .", "entities": []}, {"text": "I do n\u2019t think she \u2019ll like it .So", "entities": []}, {"text": "funny movie .", "entities": []}, {"text": "I really like it .", "entities": []}, {"text": "That was very funny .", "entities": []}, {"text": "I am sure she will appreciate it .", "entities": []}, {"text": "Give your brother some money and tell him to take a hike .Just give your brother some time and it will be good again .", "entities": []}, {"text": "Give your brother some money andrequest him to leave .", "entities": []}, {"text": "Negative+Formal", "entities": []}, {"text": "An intelligent , rewarding \ufb01lm that I look forward to watching again.ludicrous , shallow \ufb01lm that look forward to watching again .", "entities": []}, {"text": "An unintelligent , poor \ufb01lm that I would not look forward to watching again .", "entities": []}, {"text": "super friendly staff , quick service and amazing and simple food was done right!says wait staff , quick not amazing before overcooked food done were okay.dirty staff and slow service and simple food was not done right .", "entities": []}, {"text": "Positive+Informal You need to separate the bad thing and move on.need to the great thing and move on .", "entities": []}, {"text": "You need to enjoy the good stuff and move on .", "entities": []}, {"text": "The evening started out slow .", "entities": []}, {"text": "The evening spent in professional show .", "entities": []}, {"text": "The evening began amazing .", "entities": []}, {"text": "Negative+Informal Great food recommendations steak and tuna were both great.terrible food 9 am steak and were both terrible .", "entities": []}, {"text": "Disappointing food recommendations steak and tuna were horrible .", "entities": []}, {"text": "That person in hilarious .", "entities": []}, {"text": "You person in worse !", "entities": []}, {"text": "That guy in so boring .", "entities": []}, {"text": "Table 4 : Qualitative results for transfer to different target style combination across different models .", "entities": []}, {"text": "( Different colors highlight the transferred segments corresponding to underlined input sentence ; Text in bold highlights adherence to target formality in text generated by our model . )", "entities": []}, {"text": "ModelStyle Accuracy ContentFluencyTransfer Sentiment Formality Preservation Quality Cascaded Style Transformer3:5909 2 : 7424 3:2803 2:7424", "entities": [[1, 2, "MetricName", "Accuracy"]]}, {"text": "2:9318(Dai et al . , 2019 )", "entities": []}, {"text": "Joint Discriminative LM3.8561 3.0379 4.1061 4.1894 4.1091(Our Model ) Table 5 : Results for Human Evaluation across different metrics .", "entities": []}, {"text": "Each value represents the average of rating between 1 ( Very bad ) and 5 ( Very good ) .", "entities": []}, {"text": "ples in Table 4 , where sentences generated by Cascaded Style Transformer are much less coherent .", "entities": [[11, 12, "MethodName", "Transformer"]]}, {"text": "Qualitative experiments also highlight the ability of our model to incorporate intricacies of formality stylistic dimension ( shown in bold ) better than the Cascaded Style Transformer model .", "entities": [[26, 27, "MethodName", "Transformer"]]}, {"text": "Among single step transfer models ( Syed et al .", "entities": []}, {"text": "( 2020 ) and our proposed approach ) , we note that content preservation is marginally better for Syed et al .", "entities": []}, {"text": "( 2020 ) \u2019s model , however , our model is able to yield much better style transfer owing to feedback on style control by multiple discriminators .", "entities": [[16, 18, "TaskName", "style transfer"]]}, {"text": "4.4 Human evaluation To augment automatic evaluation results , we conduct a human study to evaluate the model outputs across various dimensions such as content preservation , style control , \ufb02uency , and overall trans - fer quality .", "entities": []}, {"text": "Based on comparable style control in Cascaded Style Transformer and our proposed approach on automatic metrics , we compare the transfer quality across these two models by a small - scale human study .", "entities": [[8, 9, "MethodName", "Transformer"]]}, {"text": "We select 40sentences , with 10examples from each combinations of sentiment and formality as target style , and collect annotations from 4\u20135participants for each example .", "entities": []}, {"text": "Out of resulting annotations , more than 85 % annotations favoured our results over baseline .", "entities": []}, {"text": "The average participant rating across different dimensions is shown in Table 5 .", "entities": []}, {"text": "We test the statistical signi\ufb01cance of these results using z - test statistic .", "entities": []}, {"text": "With \u000b = 0:05 , the preferences indicated in human study are signi\ufb01cant across all metrics .", "entities": []}, {"text": "These results are in line with our automatic evaluations and add con\ufb01dence to the ef\ufb01cacy of our proposed approach in achieving style transfer across multiple dimensions .", "entities": [[21, 23, "TaskName", "style transfer"]]}, {"text": "35085 Conclusion and Future Work We propose an approach to extend currently existing style transfer work to multiple style setting without imposing any extra constraints on availability of dataset .", "entities": [[13, 15, "TaskName", "style transfer"]]}, {"text": "Our method makes use of disjoint corpus from separate styles to enable one step transfer across multiple target styles .", "entities": []}, {"text": "We exploit multiple discriminative language models with an encoder - decoder framework , all emerging from large transformer - based language models pretrained on Masked Language Modeling objective and \ufb01ne - tuned separately for transfer and discriminative purposes .", "entities": [[24, 27, "TaskName", "Masked Language Modeling"]]}, {"text": "We show that uni\ufb01ed single step transfer approach is able to achieve better transfer while offering much better content preservation which is paramount to any style transfer task .", "entities": [[25, 27, "TaskName", "style transfer"]]}, {"text": "Further improvements are in scope for adding modularity to the proposed transfer module .", "entities": []}, {"text": "In the current setup , each version of model is trained for a speci\ufb01c combination of target style(s ) .", "entities": []}, {"text": "The utility of such a model increases manifold with added ease of transfer across multiple style combinations within a single model .", "entities": []}, {"text": "This could be attempted by employing a controlled language model as a uni\ufb01ed discriminator for multiple styles , which would be the subject of further research .", "entities": []}, {"text": "Ethics Statement .", "entities": [[0, 1, "DatasetName", "Ethics"]]}, {"text": "We recognise the ethical implication of employing large language models trained on data infused with unchecked biases .", "entities": []}, {"text": "As with any generative task , style transfer too suffers from the potential misuse for fact distortion , plagiarism and more .", "entities": [[6, 8, "TaskName", "style transfer"]]}, {"text": "The paper aims at establishing academic utility of proposed framework .", "entities": []}, {"text": "To meet ethical standards , this solution has to coupled with strict misrepresentation , offensiveness and bias checks .", "entities": []}, {"text": "References Julian Brooke , Tong Wang , and Graeme Hirst .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Automatic acquisition of lexical formality .", "entities": []}, {"text": "In Coling 2010 : Posters , pages 90\u201398 , Beijing , China .", "entities": []}, {"text": "Coling 2010 Organizing Committee .", "entities": []}, {"text": "Stephane Clinchant , Kweon Woo Jung , and Vassilina Nikoulina .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "On the use of BERT for neural machine translation .", "entities": [[4, 5, "MethodName", "BERT"], [7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 3rd Workshop on Neural Generation and Translation , pages 108\u2013117 , Hong Kong . Association for Computational Linguistics .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Alexis Conneau and Guillaume Lample .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crosslingual language model pretraining .", "entities": []}, {"text": "In H. Wallach , H. Larochelle , A. Beygelzimer , F. d'Alch\u00e9 - Buc , E. Fox , and R. Garnett , editors , Advances in Neural Information Processing Systems 32 , pages 7059 \u2013 7069 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Ning Dai , Jianze Liang , Xipeng Qiu , and Xuanjing Huang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Style transformer : Unpaired text style transfer without disentangled latent representation .", "entities": [[4, 7, "TaskName", "text style transfer"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5997\u20136007 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Cristian Danescu - Niculescu - Mizil , Moritz Sudhof , Dan Jurafsky , Jure Leskovec , and Christopher Potts .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "A computational approach to politeness with application to social factors .", "entities": []}, {"text": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 250\u2013259 , So\ufb01a , Bulgaria .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zhenxin Fu , Xiaoye Tan , Nanyun Peng , Dongyan Zhao , and Rui Yan . 2017 .", "entities": []}, {"text": "Style Transfer in Text : Exploration and Evaluation .", "entities": [[0, 2, "TaskName", "Style Transfer"]]}, {"text": "arXiv e - prints , page arXiv:1711.06861 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhiting Hu , Zichao Yang , Xiaodan Liang , Ruslan Salakhutdinov , and Eric P. Xing . 2017 .", "entities": [[9, 10, "DatasetName", "Ruslan"]]}, {"text": "Toward controlled generation of text .", "entities": []}, {"text": "In Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 1587\u20131596 , International Convention Centre , Sydney , Australia .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Armand Joulin , Edouard Grave , Piotr Bojanowski , Matthijs Douze , H\u00e9rve J\u00e9gou , and Tomas Mikolov .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Fasttext.zip : Compressing text classi\ufb01cation models .", "entities": []}, {"text": "arXiv preprint arXiv:1612.03651 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Guillaume Lample , Myle Ott , Alexis Conneau , Ludovic Denoyer , and Marc\u2019Aurelio Ranzato .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Phrase - based & neural unsupervised machine translation .", "entities": [[5, 8, "TaskName", "unsupervised machine translation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 5039\u20135049 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Juncen Li , Robin Jia , He He , and Percy Liang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Delete , retrieve , generate : a simple approach to sentiment and style transfer .", "entities": [[12, 14, "TaskName", "style transfer"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1865\u20131874 , New Orleans , Louisiana . Association for Computational Linguistics .", "entities": []}, {"text": "3509Yang Liu and Mirella Lapata .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Text summarization with pretrained encoders .", "entities": [[0, 2, "TaskName", "Text summarization"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3730\u20133740 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Andrew L. Maas , Raymond E. Daly , Peter T. Pham , Dan Huang , Andrew Y .", "entities": []}, {"text": "Ng , and Christopher Potts . 2011 .", "entities": []}, {"text": "Learning word vectors for sentiment analysis .", "entities": [[4, 6, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , pages 142\u2013150 , Portland , Oregon , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Aman Madaan , Amrith Setlur , Tanmay Parekh , Barnabas Poczos , Graham Neubig , Yiming Yang , Ruslan Salakhutdinov , Alan W Black , and Shrimai Prabhumoye .", "entities": [[18, 19, "DatasetName", "Ruslan"]]}, {"text": "2020 .", "entities": []}, {"text": "Politeness Transfer : A Tag and Generate Approach .", "entities": []}, {"text": "arXiv e - prints , page arXiv:2004.14257 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Fran\u00e7ois Mairesse and Marilyn A. Walker . 2011 .", "entities": []}, {"text": "Controlling user perceptions of linguistic style : Trainable generation of personality traits .", "entities": []}, {"text": "Computational Linguistics , 37(3):455\u2013488 .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311\u2013318 , Philadelphia , Pennsylvania , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training .", "entities": []}, {"text": "Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "SQuAD : 100,000 + questions for machine comprehension of text .", "entities": [[0, 1, "DatasetName", "SQuAD"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383\u20132392 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sudha Rao and Joel Tetreault .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Dear sir or madam , may I introduce the GYAFC dataset : Corpus , benchmarks and metrics for formality style transfer .", "entities": [[9, 10, "DatasetName", "GYAFC"], [19, 21, "TaskName", "style transfer"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 129\u2013140 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Abhilasha Sancheti , Kundan Krishna , Balaji Vasan Srinivasan , and Anandhavelu Natarajan . 2020 .", "entities": []}, {"text": "Reinforced rewards framework for text style transfer .", "entities": [[4, 7, "TaskName", "text style transfer"]]}, {"text": "In Advances in Information Retrieval , pages 545\u2013560 , Cham .", "entities": [[3, 5, "TaskName", "Information Retrieval"]]}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Tianxiao Shen , Tao Lei , Regina Barzilay , and Tommi Jaakkola . 2017 .", "entities": []}, {"text": "Style transfer from non - parallel text by cross - alignment .", "entities": [[0, 2, "TaskName", "Style transfer"]]}, {"text": "In I. Guyon , U. V .", "entities": []}, {"text": "Luxburg , S. Bengio , H. Wallach , R. Fergus , S. Vishwanathan , and R. Garnett , editors , Advances in Neural Information Processing Systems 30 , pages 6830\u20136841 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Sandeep Subramanian , Guillaume Lample , Eric Michael Smith , Ludovic Denoyer , Marc\u2019Aurelio Ranzato , and Y - Lan Boureau .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Multiple - attribute text style transfer .", "entities": [[3, 6, "TaskName", "text style transfer"]]}, {"text": "CoRR , abs/1811.00552 .", "entities": []}, {"text": "Yu Sun , Shuohuan Wang , Yukun Li , Shikun Feng , Hao Tian , Hua Wu , and Haifeng Wang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Ernie 2.0 : A continual pre - training framework for language understanding .", "entities": []}, {"text": "Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 34(05):8968\u20138975 .", "entities": []}, {"text": "Richard S. Sutton , David McAllester , Satinder Singh , and Yishay Mansour . 1999 .", "entities": []}, {"text": "Policy gradient methods for reinforcement learning with function approximation .", "entities": [[0, 3, "TaskName", "Policy gradient methods"]]}, {"text": "In Proceedings of the 12th International Conference on Neural Information Processing Systems , NIPS\u201999 , page 1057\u20131063 , Cambridge , MA , USA . MIT Press .", "entities": [[18, 19, "DatasetName", "Cambridge"]]}, {"text": "Bakhtiyar Syed , Gaurav Verma , Balaji Vasan Srinivasan , Anandhavelu Natarajan , and Vasudeva Varma . 2020 .", "entities": []}, {"text": "Adapting language models for non - parallel author - stylized rewriting .", "entities": []}, {"text": "Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 34(05):9008 \u2013 9015 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In NIPS , pages 6000\u20136010 .", "entities": []}, {"text": "Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "GLUE :", "entities": [[0, 1, "DatasetName", "GLUE"]]}, {"text": "A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[9, 12, "TaskName", "natural language understanding"]]}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 353\u2013355 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hu Xu , Bing Liu , Lei Shu , and Philip Yu .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "BERT post - training for review reading comprehension and aspect - based sentiment analysis .", "entities": [[0, 1, "MethodName", "BERT"], [6, 8, "TaskName", "reading comprehension"], [9, 14, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 2324\u20132335 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "3510Zichao Yang , Zhiting Hu , Chris Dyer , Eric P Xing , and Taylor Berg - Kirkpatrick .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Unsupervised text style transfer using language models as discriminators .", "entities": [[1, 4, "TaskName", "text style transfer"]]}, {"text": "In S. Bengio , H. Wallach , H. Larochelle , K. Grauman , N. Cesa - Bianchi , and R. Garnett , editors , Advances in Neural Information Processing Systems 31 , pages 7287\u20137298 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}]