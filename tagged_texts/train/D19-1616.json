[{"text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 5994\u20135998 , Hong Kong , China , November 3\u20137 , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics5994Abstract Text Summarization : A Low Resource Challenge Shantipriya Parida Petr Motlicek Idiap Research Institute Rue Marconi 19 , 1920 Martigny , Switzerland fshantipriya.parida , petr.motlicek g@idiap.ch", "entities": [[5, 7, "TaskName", "Text Summarization"]]}, {"text": "Abstract Text summarization is considered as a challenging task in the NLP community .", "entities": [[1, 3, "TaskName", "Text summarization"]]}, {"text": "The availability of datasets for the task of multilingual text summarization is rare , and such datasets are dif\ufb01cult to construct .", "entities": [[9, 11, "TaskName", "text summarization"]]}, {"text": "In this work , we build an abstract text summarizer for the German language text using the state - of - the - art \u201c Transformer \u201d model .", "entities": [[25, 26, "MethodName", "Transformer"]]}, {"text": "We propose an iterative data augmentation approach which uses synthetic data along with the real summarization data for the German language .", "entities": [[4, 6, "TaskName", "data augmentation"], [15, 16, "TaskName", "summarization"]]}, {"text": "To generate synthetic data , the Common Crawl ( German ) dataset is exploited , which covers different domains .", "entities": [[6, 8, "DatasetName", "Common Crawl"]]}, {"text": "The synthetic data is effective for the low resource condition and is particularly helpful for our multilingual scenario where availability of summarizing data is still a challenging issue .", "entities": []}, {"text": "The data are also useful in deep learning scenarios where the neural models require a large amount of training data for utilization of its capacity .", "entities": []}, {"text": "The obtained summarization performance is measured in terms of ROUGE and BLEU score .", "entities": [[2, 3, "TaskName", "summarization"], [11, 13, "MetricName", "BLEU score"]]}, {"text": "We achieve an absolute improvement of +1.5 and +16.0 in ROUGE1 F1 ( R1 F1 ) on the development and test sets , respectively , compared to the system which does not rely on data augmentation .", "entities": [[11, 12, "MetricName", "F1"], [14, 15, "MetricName", "F1"], [34, 36, "TaskName", "data augmentation"]]}, {"text": "1 Introduction Automatic text summarization is considered as a challenging task because while summarizing a piece of text , we read it entirely to develop our understanding to prepare highlighting its main points .", "entities": [[3, 5, "TaskName", "text summarization"]]}, {"text": "Due to the lack of human knowledge and language processing abilities in computers , automatic text summarization is a major non - trivial task ( Allahyari et al . , 2017 ) .", "entities": [[15, 17, "TaskName", "text summarization"]]}, {"text": "Two major approaches for automatic summarization are : extractive and abstractive .", "entities": [[5, 6, "TaskName", "summarization"]]}, {"text": "The extractive summarization approach produces summaries by choosing a subset of sentences in theoriginal text .", "entities": [[1, 3, "TaskName", "extractive summarization"]]}, {"text": "The abstract text summarization approach aims to shorten the long text into a humanreadable form that contains the most important fact from the original text ( Allahyari et al . , 2017 ; Kry\u00b4sci\u00b4nski et al . , 2018 ) .", "entities": [[2, 4, "TaskName", "text summarization"]]}, {"text": "The deep learning - based neural attention model when applying to abstract text summarization performs well compared to standard learning - based approaches ( Rush et al . , 2015 ) .", "entities": [[12, 14, "TaskName", "text summarization"]]}, {"text": "Abstract text summarization using the attentional encoder - decoder recurrent neural network approach shows a stateof - the - art performance and sets a baseline model ( Nallapati et al . , 2016 ) .", "entities": [[1, 3, "TaskName", "text summarization"]]}, {"text": "Further improvements are introduced to the baseline model by using the pointer generator network and coverage mechanism using reinforcement learning based training procedure ( See et al . , 2017 ; Paulus et al . , 2017 ) .", "entities": []}, {"text": "There is an inherent limitation to natural language processing tasks such as text summarization for resource - poor and morphological complex languages owing to a shortage of quality linguistic data available ( Kurniawan and Louvan , 2018 ) .", "entities": [[12, 14, "TaskName", "text summarization"]]}, {"text": "The use of synthetic data along with the real data is one of the popular approaches followed in machine translation domain for the low resource conditions to improve the translation quality ( Bojar and Tamchyna , 2011 ; Hoang et al . , 2018 ; ChineaR\u0131os et al . , 2017 ) .", "entities": [[18, 20, "TaskName", "machine translation"]]}, {"text": "The iterative back - translation ( e.g. training back - translation systems multiple times ) were also found effective in machine translation ( Hoang et al . , 2018 ) .", "entities": [[20, 22, "TaskName", "machine translation"]]}, {"text": "We explore similar approaches in our experiments for the text summarization task .", "entities": [[9, 11, "TaskName", "text summarization"]]}, {"text": "The organizations of this paper is as follows : Section 1 describes related work on abstract text summarization .", "entities": [[16, 18, "TaskName", "text summarization"]]}, {"text": "Section 2 explains the techniques followed in our work .", "entities": []}, {"text": "Section 3 describes the dataset used in our experiment .", "entities": []}, {"text": "Section 4 explains the experimental settings : models and their parameters .", "entities": []}, {"text": "Section 5 provides evaluation results with", "entities": []}, {"text": "5995analysis and discussion .", "entities": []}, {"text": "Section 6 provides conclusion to the paper .", "entities": []}, {"text": "2 Method Description Across all experiments performed in this paper , we have used the Transformer model as implemented in OpenNMT - py1(Vaswani et al . , 2018 ; See et al . , 2017 ) .", "entities": [[15, 16, "MethodName", "Transformer"]]}, {"text": "The Transformer model is based on encoder / decoder architecture .", "entities": [[1, 2, "MethodName", "Transformer"]]}, {"text": "In context to summarize , it takes text as input and provides its summary .", "entities": []}, {"text": "We use synthetic data as shown in Figure 1 to increase the size of the training data .", "entities": []}, {"text": "T ext Summary Summary T extReal SyntheticReal + Synthetic Reverse System Final SystemSummary T ext Figure 1 : Generation of synthetic data using a reverse system .", "entities": []}, {"text": "To generate synthetic data , \ufb01rst , a system in the reverse direction ( i.e. source as summary and target as text ) is trained and then used to generate text for the given summary .", "entities": []}, {"text": "Then both the real and synthetic data acts as input to the \ufb01nal system .", "entities": []}, {"text": "3 Dataset Description We use German wiki data ( spread across different domain ) collected from the SwissText 20192(real data ) and Common Crawl3data ( synthetic data ) in our experiment .", "entities": []}, {"text": "The statistics of all the datasets are shown in Table 1 . 3.1 SwissText datasets used as real data We divide the 100 K SwissText dataset ( downloaded from SwissText 2019 website ) into three subsets : train , dev , and test in 90:5:5 ratio ( i.e. 90 K for training , 5 K for development and 5 K for 1http://opennmt.net/OpenNMT-py/ Summarization.html 2https://www.swisstext.org/ 3http://commoncrawl.org/Dataset # Text # Summ Train Real(SwissText ) 90 K 90 K Train RealSynth(Swiss+CC ) 190 K 190 K Train RealSynthRegen(Swiss+CC ) 190 K 190 K Dev(SwissText ) 5 K 5 K Test(SwissText ) 5 K 5 K Table 1 : Statistics of the experimental data which include the number of texts and their summaries .", "entities": []}, {"text": "the test data ) .", "entities": []}, {"text": "The experiments performed over these datasets are described in Section 4.3 ( denoted as S1 experimental setup ) .", "entities": []}, {"text": "3.2 Common Crawl dataset used as synthetic data", "entities": [[1, 3, "DatasetName", "Common Crawl"]]}, {"text": "The data crawled from the Internet ( Common Crawl ) used to prepare synthetic data to boost the training .", "entities": [[7, 9, "DatasetName", "Common Crawl"]]}, {"text": "The steps followed to create the synthetic dataset as follows : Step 1 : Build vocab : We create vocabulary using SwissText based on the occurrence of the most frequent ( top N ) German words .", "entities": []}, {"text": "Step 2 : Sentence selection : The sentences from the Common Crawl data are selected with respect to the vocabulary based on the threshold we provide ( e.g. a sentence has 10 words and the threshold is 10 % ( 0.1 ) ) .", "entities": [[10, 12, "DatasetName", "Common Crawl"]]}, {"text": "For a sentence to be selected , at least 1 out of 10 words should be in the vocabulary .", "entities": []}, {"text": "Step 3 : Filtering :", "entities": []}, {"text": "Select random sentences ( e.g. 100 K ) from the selected Common Crawl data in the previous step .", "entities": [[11, 13, "DatasetName", "Common Crawl"]]}, {"text": "Step 4 : Generate summary : The 100 K data obtained from the previous step are used as a summary and required to generate corresponding text .", "entities": []}, {"text": "We use the reverse trained model where we provide the summary as source and target as text .", "entities": []}, {"text": "This results in the text as well as the corresponding summary as additional data to be utilized along with real data ( SwissText ) .", "entities": []}, {"text": "Eventually , the 190 K dataset is created ( denote as Train RealSynth ) as a combination of 90 K SwissText train data ( real ) and 100 K synthetic data .", "entities": []}, {"text": "This dataset is used in the experimental setup S2 ( described in details in Section 4.3 ) .", "entities": []}, {"text": "4 Experimental Setup This section describes our experiments conducted for the text summarization task .", "entities": [[11, 13, "TaskName", "text summarization"]]}, {"text": "5996Setting Dataset R1 F1 R2 F1 RL F1 BLEU S1 Dev 43.9 28.5 46.3 12.6 Test 39.7 22.9 42.2 9.0 S2 Dev 45.4 29.8 47.4 14.0 Test 55.7 41.8 57.6 20.8 S3 Dev 44.3 28.5 46.4 13.1 Test 40.0 23.0 42.3 9.4 Table 2 : Evaluation results of our models on development ( dev ) and testing ( test ) sets .", "entities": [[3, 4, "MetricName", "F1"], [5, 6, "MetricName", "F1"], [7, 8, "MetricName", "F1"], [8, 9, "MetricName", "BLEU"]]}, {"text": "The automatic evaluation scores in terms of Rouge ( R1 F1 , R2 F1 , RL F1 ) and BLEU for the output summaries are shown in the table .", "entities": [[10, 11, "MetricName", "F1"], [13, 14, "MetricName", "F1"], [16, 17, "MetricName", "F1"], [19, 20, "MetricName", "BLEU"]]}, {"text": "4.1", "entities": []}, {"text": "Prepossessing The preprocess step involves preprocessing the dataset such that source and target are aligned and use the same dictionary .", "entities": []}, {"text": "Additionally , we truncate the source length at 400 tokens and the target length at 100 tokens to expedite training ( See et al . , 2017 ) .", "entities": []}, {"text": "4.2 Model Parameters The Transformer model is implemented in OpenNMT - py .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "To train the model , we use a single GPU .", "entities": []}, {"text": "To \ufb01t the model to the GPU cluster , a batch size equal to 4,096 is selected for training .", "entities": [[10, 12, "HyperparameterName", "batch size"]]}, {"text": "The validation batch size is set to 8 .", "entities": [[2, 4, "HyperparameterName", "batch size"]]}, {"text": "We use an initial learning rate of 2 , drop out of 0.2 and 8,000 warm - up steps .", "entities": [[4, 6, "HyperparameterName", "learning rate"]]}, {"text": "Decoding uses a beam size of 10 and we did not set any minimum length of output summary .", "entities": []}, {"text": "4.3 Model Setup We use 3 settings : ( i ) real data ( we set this as the baseline in our experiment ) , ( ii ) real data and synthetic data , and ( iii ) real and regenerated synthetic data for the summarization task , described as follows :", "entities": [[45, 46, "TaskName", "summarization"]]}, {"text": "1.S1 : Transformer model using Train Real data In this setup , we use the \u201c Train Real \u201d data for training the Transformer model .", "entities": [[2, 3, "MethodName", "Transformer"], [23, 24, "MethodName", "Transformer"]]}, {"text": "2.S2 :", "entities": []}, {"text": "Transformer Model using Train RealSynth data In this setup , we use the \u201c Train RealSynth \u201d data for training the Transformer model .", "entities": [[0, 1, "MethodName", "Transformer"], [21, 22, "MethodName", "Transformer"]]}, {"text": "As the balance between real and synthetic data is an important factor , we maintain a 1:1 ratio ( e.g. 1 ( real ) :1 ( synthetic ) ) for our experiment ( Sennrich et al . , 2016).3.S3 : Transformer Model using Train RealSynthRegen data We propose an iterative approach to improve the quality of synthetic summaries .", "entities": [[40, 41, "MethodName", "Transformer"]]}, {"text": "In this setup , after training a system with ( real+synthetic ) data , it is used to regenerate synthetic data for the \ufb01nal system .", "entities": []}, {"text": "As a result , the input data to the \ufb01nal system is a combination of real and regenerated synthetic data as shown in Figure 2 . 4.4 Training Procedure The copying mechanism is applied during training .", "entities": []}, {"text": "It allows the summarizer to fall back and copy the source text when encounters < unk > tokens by referencing to the softmax of the multiplication between attention scores of the output with the attention scores of the source ( See et al . , 2017 ) .", "entities": [[22, 23, "MethodName", "softmax"]]}, {"text": "The systems are trained for 300 K iterations .", "entities": []}, {"text": "5 Evaluation and Discussion We evaluate the results for every 10,000 iterations on the dev and test set .", "entities": []}, {"text": "The automatic evaluation results based on the dev and test set are shown in Table 2 with sample summaries in Table 3 .", "entities": []}, {"text": "To evaluate the proposed algorithms , we use ROUGE ( Recall - Oriented Understudy for Gisting Evaluation ) score , which is a popular metric for text summarization task , and has several variants like ROUGE - N , and ROUGE - L , which measure the overlap of n - grams between the system and reference summary ( LIN , 2004 ) .", "entities": [[10, 11, "MetricName", "Recall"], [26, 28, "TaskName", "text summarization"], [40, 43, "MetricName", "ROUGE - L"]]}, {"text": "We use ROUGE 1 F1 ( R1F1 ) , ROUGE 2 F1 ( R2 F1 ) , and ROUGE L F1 ( RL F1 ) for scoring the generated summary .", "entities": [[4, 5, "MetricName", "F1"], [11, 12, "MetricName", "F1"], [14, 15, "MetricName", "F1"], [20, 21, "MetricName", "F1"], [23, 24, "MetricName", "F1"]]}, {"text": "In addition , we also use the SacreBLEU4evaluation metric ( Post , 2018 ) .", "entities": []}, {"text": "Figure 3 presents the learning curves for the models ( S1 and S2 ) on the development set .", "entities": []}, {"text": "It can be seen that there is a variance ( e.g. word 4https://github.com/mjpost/sacreBLEU", "entities": []}, {"text": "5997 Summary Summary T extReal SyntheticReal + Synthetic Reverse System1Summary T ext Summary T ext Synthetic Summary T ext Final SystemReal + Synthetic T ext Reverse System2Figure 2 : Regeneration of synthetic data .", "entities": []}, {"text": "After training a system with real+synthetic data ( Reverse System2 above ) , used to create synthetic summarization data for the \ufb01nal system .", "entities": [[17, 18, "TaskName", "summarization"]]}, {"text": "42.5 43 43.5 44 44.5 45 45.5 0 50k 100k 150k 200k 250k 300kRouge1 F1 IterationS1 : Real S2 : Real+Synth Figure 3 : Learning curves in terms of Rouge1 F1 ( R1F1 ) Score on dev set .", "entities": [[7, 8, "DatasetName", "0"], [14, 15, "MetricName", "F1"], [30, 31, "MetricName", "F1"], [34, 35, "MetricName", "Score"]]}, {"text": "selection , summary length ) for model S2 generated summary as compared with model S1 .", "entities": []}, {"text": "During manual veri\ufb01cation , we found that the summaries generated without a minimum length constraint appear better compared to summaries with minimum length constraint .", "entities": []}, {"text": "Although we do not explicitly specify a minimum length parameter for generating summaries for the models , the average length of words generated by model S2 ( e.g. 41.42 words ) is longer than the model S1 ( e.g. 39.81 words ) .", "entities": []}, {"text": "Some data ( e.g. name , year ) were found inconsistent during a comparison of the generated summary with respect to the reference .", "entities": []}, {"text": "There is a variance in summaries generated by model S3 as compared to S2 and S1 .", "entities": []}, {"text": "In terms of Rouge score model S3 outperforms model S1 but perform worse than model S2 ( see Table 2 ) .", "entities": []}, {"text": "6 Conclusion In this paper , we highlighted the implementation of synthetic data for the abstract text summariza - Ref Summary : \u201c Das Feuerschiff Relandersgrund war ein \ufb01nnisches Feuerschiff , das von 1888 bis 1914", "entities": []}, {"text": "i m Schrenmeer bei Rauma positioniert war .", "entities": []}, {"text": "Heute dient es als Restaurantschiff in Helsinki . \u201d", "entities": []}, {"text": "Gloss : The lightship Relandersgrund was a Finnish lightship , which was built from 1888 to 1914 Schrenmeer was positioned at Rauma .", "entities": []}, {"text": "Today serves it as a restaurant ship in Helsin S1 Summary : : \u201c Die \u201c Rauma \u201d .", "entities": []}, {"text": "ist ein 1886\u20141888 Feuerschiff der norwegischen Reederei \u201c Libauskij \u201d , Das Schiff wurde in den 1930er Jahren gebaut und in den 2000er Jahren als Museumsschiff als \u201d Gloss : \u201c The \u201c Rauma \u201d .", "entities": []}, {"text": "is a 1886 - 1888 Lightship of the Norwegian shipping company \u201c Libauskij\u201d,The ship was built in the 1930s and in the 2000s as a museum ship as S2 Summary : : \u201c", "entities": []}, {"text": "Das Feuerschiff Relandersgrund war ein Feuerschiff des das von 1888 bis 1914", "entities": []}, {"text": "i m Einsatz war .", "entities": []}, {"text": "Heute dient es als Restaurantschiff in Kotka , \u201d Gloss : The lightship Relandersgrund was on Lightship of the 1888 to 1914 was in use .", "entities": []}, {"text": "Today it serves as a restaurant ship in Kotka S3 Summary : : \u201c Das Kotka . \u201d", "entities": []}, {"text": "ist ein \ufb01nnischer Museumsschiff der i m Zweiten Weltkrieg von der russischen Marine als Restaurantschiff 1 \u201d eingesetzt wurde .", "entities": []}, {"text": "I m Mittelalter war das Schiff unter dem Namen \u201c Vuolle \u201d 1 \u201d fr die \ufb01nnische Marine 1 \u201d Gloss : The Kotka .", "entities": []}, {"text": "\u201c Is a Finnish one Museum ship of the World War II Russian Navy used as a restaurant ship 1 \u201d has been .", "entities": []}, {"text": "In the Middle Ages , the ship was under the name \u201c Vuolle \u201d 1 \u201c for the Finnish Navy 1 Table 3 : Sample summaries on test set .", "entities": []}, {"text": "The matching words of generating summaries with respect to references are shown in color blue .", "entities": []}, {"text": "tion task under low resource condition , which helps improving the text summarization system in terms of automatic evaluation metrics .", "entities": [[11, 13, "TaskName", "text summarization"]]}, {"text": "As the next step , we plan to investigate : i ) synthetic summarization data , and ii ) applying transfer learning on text summarization for the multilingual low resource data set with little or no ground truth summaries ( Keneshloo et al . , 2018 ) .", "entities": [[13, 14, "TaskName", "summarization"], [20, 22, "TaskName", "transfer learning"], [23, 25, "TaskName", "text summarization"]]}, {"text": "5998Acknowledgments The work is supported by an innovation project ( under an InnoSuisse grant ) oriented to improve the automatic speech recognition and natural language understanding technologies for German .", "entities": [[19, 22, "TaskName", "automatic speech recognition"], [23, 26, "TaskName", "natural language understanding"]]}, {"text": "Title : \u201c SM2 : Extracting Semantic Meaning from Spoken Material \u201d funding application no . 29814.1 IP - ICT .", "entities": []}, {"text": "References Mehdi Allahyari , Seyedamin Pouriyeh , Mehdi Asse\ufb01 , Saeid Safaei , Elizabeth D Trippe , Juan B Gutierrez , and Krys Kochut . 2017 .", "entities": []}, {"text": "Text summarization techniques : a brief survey .", "entities": [[0, 2, "TaskName", "Text summarization"]]}, {"text": "arXiv preprint arXiv:1707.02268 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ondrej Bojar and Ale \u02c7s Tamchyna .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Improving translation model by monolingual data .", "entities": []}, {"text": "In Sixth Workshop on Statistical Machine Translation , page 330 .", "entities": [[5, 7, "TaskName", "Machine Translation"]]}, {"text": "Mara Chinea - R\u0131os , Alvaro Peris , and Francisco Casacuberta .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Adapting neural machine translation with parallel synthetic data .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "WMT 2017 , page 138 .", "entities": []}, {"text": "Cong Duy Vu Hoang , Philipp Koehn , Gholamreza Haffari , and Trevor Cohn .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Iterative backtranslation for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "ACL 2018 , 23(32.5):18 .", "entities": []}, {"text": "Yaser Keneshloo , Naren Ramakrishnan , and Chandan K Reddy .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep transfer reinforcement learning for text summarization .", "entities": [[1, 4, "TaskName", "transfer reinforcement learning"], [5, 7, "TaskName", "text summarization"]]}, {"text": "arXiv preprint arXiv:1810.06667 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Wojciech Kry \u00b4 sci\u00b4nski , Romain Paulus ,", "entities": []}, {"text": "Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Improving abstraction in text summarization .", "entities": [[3, 5, "TaskName", "text summarization"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1808\u20131817 .", "entities": []}, {"text": "Kemal Kurniawan and Samuel Louvan .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Indosum : A new benchmark dataset for indonesian text summarization .", "entities": [[0, 1, "DatasetName", "Indosum"], [8, 10, "TaskName", "text summarization"]]}, {"text": "In 2018 International Conference on Asian Language Processing ( IALP ) , pages 215 \u2013 220 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "C - Y LIN .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Rouge :", "entities": []}, {"text": "A package for automatic evaluation of summaries .", "entities": []}, {"text": "In Proc .", "entities": []}, {"text": "of Workshop on Text Summarization Branches", "entities": [[3, 5, "TaskName", "Text Summarization"]]}, {"text": "Out , Post Conference Workshop of ACL 2004 .", "entities": []}, {"text": "Ramesh Nallapati , Bowen Zhou , Cicero dos Santos , Caglar Gulcehre , and Bing Xiang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Abstractive text summarization using sequence - tosequence rnns and beyond .", "entities": [[0, 3, "TaskName", "Abstractive text summarization"]]}, {"text": "In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning , pages 280\u2013290.Romain", "entities": []}, {"text": "Paulus , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A deep reinforced model for abstractive summarization .", "entities": [[6, 7, "TaskName", "summarization"]]}, {"text": "arXiv preprint arXiv:1705.04304 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matt Post . 2018 .", "entities": []}, {"text": "A call for clarity in reporting BLEU scores .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 186 \u2013 191 .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alexander M Rush , Sumit Chopra , and Jason Weston .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A neural attention model for abstractive sentence summarization .", "entities": [[6, 8, "TaskName", "sentence summarization"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 379\u2013389 .", "entities": []}, {"text": "Abigail See , Peter J Liu , and Christopher D Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Get to the point : Summarization with pointergenerator networks .", "entities": [[5, 6, "TaskName", "Summarization"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1073 \u2013 1083 .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Improving neural machine translation models with monolingual data .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , volume 1 , pages 86\u201396 .", "entities": []}, {"text": "Ashish Vaswani , Samy Bengio , Eugene Brevdo , Francois Chollet , Aidan Gomez , Stephan Gouws , Llion Jones , \u0141ukasz Kaiser , Nal Kalchbrenner , Niki Parmar , Ryan Sepassi , Noam Shazeer , and Jakob Uszkoreit .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Tensor2tensor for neural machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas ( Volume 1 : Research Papers ) , pages 193\u2013199 .", "entities": [[10, 12, "TaskName", "Machine Translation"]]}, {"text": "Association for Machine Translation in the Americas .", "entities": [[2, 4, "TaskName", "Machine Translation"]]}]