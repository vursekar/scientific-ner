[{"text": "Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio - political Events from Text ( CASE ) , pages 98\u2013104", "entities": []}, {"text": "August 5\u20136 , 2021 , \u00a9 2021 Association for Computational Linguistics98IIITT at CASE 2021 Task 1 : Leveraging Pretrained Language Models for Multilingual Protest Detection Pawan Kalyan Jada1 , Duddukunta Sashidhar Reddy1 , Adeep Hande1 ,", "entities": [[18, 21, "TaskName", "Pretrained Language Models"]]}, {"text": "Ruba Priyadharshini2,Ratnasingam", "entities": []}, {"text": "Sakuntharaj3,Bharathi", "entities": []}, {"text": "Raja Chakravarthi4 1Indian Institute of Information Technology Tiruchirappalli 2ULTRA Arts and Science College , India,3Eastern University , Sri Lanka 4Insight SFI Research Centre for Data Analytics , National University of Ireland Galway pawankj19c@iiitt.ac.in", "entities": []}, {"text": "Abstract In a world abounding in constant protests resulting from events like a global pandemic , climate change , religious or political con\ufb02icts , there has always been a need to detect events / protests before getting ampli\ufb01ed by news media or social media .", "entities": []}, {"text": "This paper demonstrates our work on the sentence classi\ufb01cation subtask of multilingual protest detection in CASE@ACL - IJCNLP 2021 .", "entities": []}, {"text": "We approached this task by employing various multilingual pre - trained transformer models to classify if any sentence contains information about an event that has transpired or not .", "entities": []}, {"text": "Furthermore , we performed soft voting over the models , achieving the best results among the models , accomplishing a macro F1 - Score of 0.8291 , 0.7578 , and 0.7951 in English , Spanish , and Portuguese , respectively .", "entities": [[21, 24, "MetricName", "F1 - Score"]]}, {"text": "The source codes for our systems are published1 .", "entities": []}, {"text": "1 Introduction The recent surge in social media users has led many people to express their opinions on various global issues .", "entities": []}, {"text": "These opinions travel far and wide within a matter of seconds ( Hossny et al . , 2018 ) .", "entities": []}, {"text": "This can in\ufb02uence many people and may engage public movements ( Won et al . , 2017a ) .", "entities": []}, {"text": "Therefore , there is a de\ufb01nite need to detect these protests and analyse them to know the signi\ufb01cant areas of disinterest .", "entities": []}, {"text": "Being a free and easy to use platform , social media has become a part of our day to day life .", "entities": []}, {"text": "It incorporates people of different ages , gender , location , religions , background , and so on .", "entities": []}, {"text": "The enormous number of rich and diversi\ufb01ed users results in an enormous amount of information being generated , which is helpful in many ways ( Kapoor et al . , 2018 ) .", "entities": []}, {"text": "Some of this even contains private information about the users , which others could misuse .", "entities": []}, {"text": "Cases were also found where certain users 1https://github.com/adeepH/ CASE-2021 - Task-1were being targeted and harassed by people using this platform , a common scenario in cyberbullying ( Abaido , 2020 ) .", "entities": []}, {"text": "Social media plays a crucial role in amplifying these protests and movements ( Won et al . , 2017b ) .", "entities": []}, {"text": "It enables political groups and protesters to organise protest movements and share information .", "entities": []}, {"text": "It acts as a platform for the people who are underrepresented by giving a voice to them .", "entities": []}, {"text": "It also offers new opportunities for people to engage in activism , political resistance , and protest outside the political groups and civic institutions .", "entities": []}, {"text": "Thus , it has a social impact on everyone ( Pulido et al . , 2018 ) .", "entities": []}, {"text": "It is to be noted that social media , similar to news media , plays a vital role in its social and political events worldwide ( Holt et al . , 2013 ) .", "entities": []}, {"text": "For the above reasons , we can state that social media plays a crucial role in most worldwide events .", "entities": []}, {"text": "The English language is widely regarded as the \ufb01rstLingua Franca .", "entities": []}, {"text": "Statistically , it is one of the most widely spoken languages globally , having of\ufb01cial status in over 53 countries ( Crystal , 2008 ) .", "entities": []}, {"text": "Over 400 million people speak English as their primary language and widely spoken in the United States and the United Kingdom .", "entities": []}, {"text": "BlackLivesMatter(Dave et", "entities": []}, {"text": "al . , 2020 ) , EarthDay ( Rome , 2010 ) are some of the major protests that have occurred in these countries .", "entities": []}, {"text": "Espa \u02dcnol commonly referred to as Spanish , is spoken by over 360 million people worldwide , with most of its speakers residing in Mexico , Argentina , Spain .", "entities": []}, {"text": "15 - M Movement ( Casero - Ripoll \u00b4 es and Feenstra , 2012 ) and YoSoy132 ( Garc \u00b4 \u0131a and Trer \u00b4 e , 2014 ) are some of the recent protests where people have been vocal about in the Spanish language .", "entities": []}, {"text": "Portuguese has over 220 million native speakers .", "entities": []}, {"text": "Brazil , Portugal , Angola are some of the major countries where this language is spoken .", "entities": []}, {"text": "Protests like Racism Kills , May 68(Ross , 2008 ) are the recent ones that occurred in the Portuguese language .", "entities": []}, {"text": "The recent upheavals of protests are due to so-", "entities": []}, {"text": "99Sentence Language Label Fabius ran against Royal for the presidential nomination in 2007 .", "entities": []}, {"text": "English Event He planned to start a race war .", "entities": []}, {"text": "English Event Metro police intervened and the \ufb01re was put out .", "entities": []}, {"text": "English Not - event Pero", "entities": []}, {"text": "no es \u00b4 ese el mayor problema .", "entities": []}, {"text": "Spanish Event La Argentina retroceder \u00b4 \u0131a un paso todos los d \u00b4 \u0131as .", "entities": []}, {"text": "Spanish Event Carri \u00b4 o", "entities": []}, {"text": "no objet \u00b4 o que se trat \u00b4 o de un secuestro .", "entities": []}, {"text": "Spanish Not - event Os servidores do Piau \u00b4 \u0131 est\u02dcao em greve", "entities": []}, {"text": "h", "entities": []}, {"text": "\u00b4 a 17 dias .", "entities": []}, {"text": "Portuguese Not - event \u00b4 E uma nova experi \u02c6encia mobilizat \u00b4 oria .", "entities": []}, {"text": "Portuguese Event \u00b4 E decidiram ir ` as aulas e passar o dia de saia .", "entities": []}, {"text": "Portuguese Not - event Table 1 : Examples of the dataset indicating events of the past and not - events .", "entities": []}, {"text": "cial media , youth , exaggeration of certain events .", "entities": []}, {"text": "( Basile and Caselli , 2020 ) .", "entities": []}, {"text": "Any early detection of mass protest detection through social media platforms such as Facebook , Twitter , and Instagram to help minimizing the aftermath of the protests ( Wilson , 2017 ) .", "entities": []}, {"text": "This has motivated Natural Language Processing ( NLP ) researchers to develop NLP systems to generalize on data coming from diverse sources to leverage the NLP systems to more realistic environments ( B \u00a8uy\u00a8uk\u00a8oz et al . , 2020 ) .", "entities": []}, {"text": "Hence , there is a need to develop NLP systems that could be generalized to any protest / events ( Peng et al . , 2013 ) , which has motivated us to participate in the shared task for multilingual protest detection ( H\u00a8urriyeto \u02d8glu et", "entities": []}, {"text": "al . , 2019a , 2021 )", "entities": []}, {"text": "The objective of the task is to identify if any sentence talks about any mentions of protests or events in three languages , namely , English , Spanish , and Portuguese .", "entities": []}, {"text": "Hence , we treat this as a sequence classi\ufb01cation task .", "entities": []}, {"text": "The rest of the paper is organized as follows , Section 2 presents previous work on protest detection and analysis .", "entities": []}, {"text": "Section 3 entails a comprehensive analysis of the dataset used for our cause .", "entities": []}, {"text": "Next , section 4 gives a detailed description of the models used for the multilingual event detection .", "entities": [[15, 17, "TaskName", "event detection"]]}, {"text": "Finally , section 5 analyses the results obtained , and Section 6 concludes our work while discussing the potential directions for future work .", "entities": []}, {"text": "2 Related Work The need to detect events that could lead to protests is of prime interest to sociologists and governments ( Danilova et al . , 2016 ) .", "entities": []}, {"text": "There are several active ongoing projects for socio - political event systems such as KEDS ( Kansas Event Data System ) ( Schrodt and Hall , 2006 ) , CAMEO ( Con\ufb02ict and Mediation Event Observation ) ( Gerner et al . , 2002 ) , and several other databases for protest de - tection systems ( Danilova , 2015 ) .", "entities": []}, {"text": "These methods have focused on news data as they have traditionally been the most reliant source of events .", "entities": []}, {"text": "Protest detection has been one of the major issues in the context of social and political ( Ettinger et al . , 2017 ) .", "entities": []}, {"text": "Papanikolaou and Papageorgiou ( 2020 ) presented a computational social science methodology to analyse protests in Greece .", "entities": []}, {"text": "H \u00a8urriyeto \u02d8glu et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2021 ) constructed a corpus of protest events comprising various language sources from various countries .", "entities": []}, {"text": "Several systems were submitted to the CLEF ProtestNews Track that consisted of three shared tasks , primarily aimed at identifying and extracting event information spanning to multiple countries ( H\u00a8urriyeto \u02d8glu et", "entities": []}, {"text": "al . , 2019b , 2020 ) .", "entities": []}, {"text": "3 Dataset This dataset comprises 26,208 sentences in three languages , namely English , Spanish , and Portuguese .", "entities": []}, {"text": "The dataset consists of two classes : \u2022Event : The sentence indicates an event of the past .", "entities": []}, {"text": "\u2022Not - event : The sentence does not talk about any event .", "entities": []}, {"text": "The volume of sequences indicating Not - event is higher in contrast to that of the Event label .", "entities": []}, {"text": "Therefore , the dataset distribution is quite imbalanced .", "entities": []}, {"text": "We can also notice that the number of English samples exceeds that of Spanish and Portuguese ones .", "entities": []}, {"text": "Refer to Table 1 for examples of sentences talking about events and not talking about events displayed in English , Spanish , and Portuguese .", "entities": []}, {"text": "The dataset distribution is displayed in Table 2 .", "entities": []}, {"text": "For our cause , we split the training and validation set in the ratio of 80:20 .", "entities": []}, {"text": "100 [ CLS ] Tok 1 Tok 2 Tok N ..... E[CLS ] E1 E2 EN ..... C TN T2", "entities": []}, {"text": "T1 .....", "entities": []}, {"text": ".......... LSTM LSTM LSTM LSTM .....", "entities": [[1, 2, "MethodName", "LSTM"], [2, 3, "MethodName", "LSTM"], [3, 4, "MethodName", "LSTM"], [4, 5, "MethodName", "LSTM"]]}, {"text": "Global A verage PoolingDropout .....", "entities": []}, {"text": "..... Dense Layer(128 Neurons)Dense Layer(1 Neuron )", "entities": []}, {"text": "Pretrained Language ModelNot - event EventFigure 1 : System Architecture based on BERT ( Devlin et al . , 2019 )", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "Language English Spanish Portuguese Not - event 18,602 2,291 901 Event 4,223 450 281 Total 22,285 2,741 1,182 Table 2 : Classwise distribution of the training set 4 Methodology We used pretrained transformer - based models for identifying if a sentence talks about an event or not .", "entities": []}, {"text": "The models that were used are BERT ( Devlin et", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "al . , 2019 ) , RoBERTa ( Liu et al . , 2019 ) and DistilBERT ( Sanh et al . , 2019 )", "entities": [[6, 7, "MethodName", "RoBERTa"], [16, 17, "MethodName", "DistilBERT"]]}, {"text": ".", "entities": []}, {"text": "Even though there are 3 different languages , we used a single model for all three due to memory constraints and reduced training time .", "entities": []}, {"text": "We \ufb01ne - tuned these models for sequence classi\ufb01cation .", "entities": []}, {"text": "Soft V oting is done on all these models to produce the respective \ufb01nal outputs for the languages .", "entities": []}, {"text": "In soft voting , each classi\ufb01er predicts that a speci\ufb01c data point belongs to the particular target class .", "entities": []}, {"text": "A weighted sum of the predictions is done based on the importance of the classi\ufb01er ( all models have equal weights ) .", "entities": []}, {"text": "The overall prediction is chosen as the target with the greatest sumof the weighted probability , thus winning the vote ( Beyeler , 2017 ; Hande et al . , 2021 ) .", "entities": []}, {"text": "4.1 BERT Bidirectional Encoder Representations from Transformers ( BERT ) ( Devlin et al . , 2019 ) is a pretrained language model which was created with the objective that \ufb01ne - tuning a pretrained model yields better performance .", "entities": [[1, 2, "MethodName", "BERT"], [8, 9, "MethodName", "BERT"]]}, {"text": "BERT \u2019s pretraining phase includes two tasks .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "Firstly , Masked Language Modeling ( MLM ) is where certain words are randomly masked in a sequence .", "entities": [[2, 5, "TaskName", "Masked Language Modeling"], [6, 7, "DatasetName", "MLM"]]}, {"text": "About 15 % of the words in a sequence is masked .", "entities": []}, {"text": "The model then attempts to predict the masked words .", "entities": []}, {"text": "Secondly , Next Sentence Prediction ( NSP ) , where the model has an additional loss function , NSP loss , indicates if the second sequence follows the \ufb01rst one .", "entities": [[15, 16, "MetricName", "loss"], [19, 20, "MetricName", "loss"]]}, {"text": "Around 50 % of the inputs are a pair , and they randomly chose the other 50 .", "entities": []}, {"text": "Here , we use a bert - base - multilingual - cased ( Pires et al . , 2019 ) trained on top of 104 languages in the largest Wikipedia corpus .", "entities": []}, {"text": "This model has 12 layers , 12 Attention heads with over 179 million parameters .", "entities": []}, {"text": "101Event Not - event Overall Model P R F1 P R F1 Acc M(P ) M(R ) M(F1 )", "entities": [[8, 9, "MetricName", "F1"], [11, 12, "MetricName", "F1"], [12, 13, "MetricName", "Acc"]]}, {"text": "mBERT 0.928 0.917 0.922 0.641 0.675 0.658 0.873 0.784 0.796 0.790 DistilmBERT 0.924 0.947 0.936 0.729 0.646 0.685 0.893 0.827 0.797 0.810 RoBERTa 0.910 0.938 0.924 0.670 0.578 0.621 0.873 0.790 0.758 0.772 SoftV oting 0.937 0.939 0.938 0.720 0.713 0.717 0.899 0.829 0.826 0.827", "entities": [[0, 1, "MethodName", "mBERT"], [22, 23, "MethodName", "RoBERTa"]]}, {"text": "Table 3 : Precision ( P ) , recall ( R ) , and F1 - Score of the models on the validation set ; M(P ) , M(R ) , and M(F1 ) are the Macro averages of precision , recall , and F1 - Score respectively 4.2 DistilmBERT DistilBERT ( Sanh et al . , 2019 ) is the distilled version of BERT .", "entities": [[3, 4, "MetricName", "Precision"], [14, 17, "MetricName", "F1 - Score"], [44, 47, "MetricName", "F1 - Score"], [50, 51, "MethodName", "DistilBERT"], [64, 65, "MethodName", "BERT"]]}, {"text": "DistilBERT employs a triple loss language modelling , where it integrates cosine distance loss with knowledge distillation .", "entities": [[0, 1, "MethodName", "DistilBERT"], [4, 5, "MetricName", "loss"], [5, 7, "TaskName", "language modelling"], [13, 14, "MetricName", "loss"], [15, 17, "MethodName", "knowledge distillation"]]}, {"text": "DistilBERT has 40 % fewer parameters than BERT but still promises 97 % of the latter \u2019s performance .", "entities": [[0, 1, "MethodName", "DistilBERT"], [7, 8, "MethodName", "BERT"]]}, {"text": "It is also 60 % faster than BERT .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "In this system , we used a cased multilingual DistilBERT model as they are three different languages .", "entities": [[9, 10, "MethodName", "DistilBERT"]]}, {"text": "For our cause , we \ufb01netune distilbert - base - multilingual - cased , which is distilled from the mBERT checkpoint .", "entities": [[6, 7, "MethodName", "distilbert"], [19, 20, "MethodName", "mBERT"]]}, {"text": "The model has 6 layers , 768 dimensions , and 12 Attention heads , totalizing about 134 million parameters .", "entities": []}, {"text": "4.3 RoBERTa Robustly Optimized BERT ( RoBERTa ) ( Liu et al . , 2019 ) follows the same architecture of BERT while differing in the pretraining strategy .", "entities": [[1, 2, "MethodName", "RoBERTa"], [4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "RoBERTa"], [21, 22, "MethodName", "BERT"]]}, {"text": "It is pretrained with MLM as its objective where the model tries to predict the masked words .", "entities": [[4, 5, "DatasetName", "MLM"]]}, {"text": "RoBERTa model is trained on the vast English Wikipedia and CCNews datasets .", "entities": [[0, 1, "MethodName", "RoBERTa"]]}, {"text": "The NSP is not employed as a pretraining strategy , and the tokens are dynamically masked , making the model slightly different to BERT .", "entities": [[23, 24, "MethodName", "BERT"]]}, {"text": "During tokenization , RoBERTa follows byte - pair encoding ( BPE ) ( Gall \u00b4 e , 2019 ) as opposed to WordPiece employed in BERT .", "entities": [[3, 4, "MethodName", "RoBERTa"], [10, 11, "MethodName", "BPE"], [22, 23, "MethodName", "WordPiece"], [25, 26, "MethodName", "BERT"]]}, {"text": "We use robertabase , a pretrained language model consisting of 12 layers , 768 hidden , 12 attention heads , and 125 million parameters .", "entities": []}, {"text": "4.4 System Description For our system , we \ufb01ne - tune the pretrained models discussed in Section 4.1 , 4.2 , and 4.3 .", "entities": []}, {"text": "We combine the three datasets as the number of samples for Spanish and Portuguese are quite low .", "entities": [[7, 10, "HyperparameterName", "number of samples"]]}, {"text": "After combining the models , we split the validation set accordingly , maintaining the split \u2019s ratio and tabulating the results on the concatenated dataset in Table3 .", "entities": []}, {"text": "The embeddings are extracted fromthese models to be fed as input to the LSTM layer , ( Hochreiter and Schmidhuber , 1997 ) as shown in Figure1 .", "entities": [[13, 14, "MethodName", "LSTM"]]}, {"text": "The resulting output is fed into a global average pooling layer ( Lin et al . , 2014 ) and then passed into fully connected layers , followed by a sigmoid activation function to obtain the resulting probability score for the input sentences .", "entities": [[7, 10, "MethodName", "global average pooling"], [30, 32, "MethodName", "sigmoid activation"]]}, {"text": "The same parameters are used for all three models .", "entities": []}, {"text": "A dropout layer ( Srivastava et al . , 2014 ) is also added in between the fully connected layers for regularization .", "entities": []}, {"text": "Refer Table 4 for the parameters used in the model .", "entities": []}, {"text": "Parameters Values Number of LSTM units 128 Dropout Rate 0.2 Batch Size 16 Max Length 128 Optimizer Adam Learning Rate 3e-5 Activation Function Sigmoid Loss Function cross - entropy Table 4 : Parameters used for training the Models 5 Results and Analysis All pretrained language models are \ufb01ne - tuned in Google Colab2for ten epochs .", "entities": [[4, 5, "MethodName", "LSTM"], [7, 8, "MethodName", "Dropout"], [10, 12, "HyperparameterName", "Batch Size"], [16, 17, "HyperparameterName", "Optimizer"], [17, 18, "MethodName", "Adam"], [18, 20, "HyperparameterName", "Learning Rate"], [21, 23, "HyperparameterName", "Activation Function"], [43, 46, "TaskName", "pretrained language models"], [51, 52, "DatasetName", "Google"]]}, {"text": "We use the Tensor\ufb02ow implementation of the models3on the Huggingface transformers library4 .", "entities": []}, {"text": "We compare the macro F1 - Scores of our \ufb01ne - tuned models on the validation set , which were created by splitting the given dataset .", "entities": [[3, 5, "MetricName", "macro F1"]]}, {"text": "The remaining split is the training data .", "entities": []}, {"text": "The validation set contains samples from all three languages .", "entities": []}, {"text": "It has 4,387 Not - event sequences and 963 Event sequences making a combined total of 5,350 .", "entities": []}, {"text": "The results are shown in Table3 .", "entities": []}, {"text": "We \ufb01ne - tuned BERT , DistilBERT , and RoBERTa models on the training set .", "entities": [[4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "DistilBERT"], [9, 10, "MethodName", "RoBERTa"]]}, {"text": "We have combined the 2https://colab.research.google.com/ 3https://huggingface.co/transformers/pretrained models.html 4https://huggingface.co/", "entities": []}, {"text": "102Language Macro F1 - Score English 0.8291 Spanish 0.7578 Portuguese 0.7951 Table 5 : Macro F1 - Scores on the Test Set three language corpora into a single corpus comprising of all the three languages together .", "entities": [[2, 5, "MetricName", "F1 - Score"], [14, 16, "MetricName", "Macro F1"]]}, {"text": "The main intention towards using a multilingual model is that the representations learnt during one language \u2019s pretraining would help the other .", "entities": []}, {"text": "We can observe that DistilBERT achieved a better F1 - Score among the models mentioned in the previous sections .", "entities": [[4, 5, "MethodName", "DistilBERT"], [8, 11, "MetricName", "F1 - Score"]]}, {"text": "RoBERTa gave the lowest score among these .", "entities": [[0, 1, "MethodName", "RoBERTa"]]}, {"text": "The reason could be that the RoBERTa model was not multilingual , unlike the other two ; however , it still managed to get a score very close to the BERT model .", "entities": [[6, 7, "MethodName", "RoBERTa"], [30, 31, "MethodName", "BERT"]]}, {"text": "It is imperative that performing soft voting on all three models has managed to increase the score .", "entities": []}, {"text": "One of the reasons for the poor performance of the models is the imbalance in the distribution of the classes .", "entities": []}, {"text": "In the dataset , there are 21,794 Not - event sentences and only 4,954 Event ones .", "entities": []}, {"text": "The models performed very well in the majority class and poorly in the minority class .", "entities": []}, {"text": "Having more Event samples could have certainly helped the model in distinguishing better among the classes .", "entities": []}, {"text": "Based on the performance of soft voting on the validation set , we have used the same for the test set .", "entities": []}, {"text": "The results for the test set are shown in Table5 .", "entities": []}, {"text": "The reason for relatively low scores of Spanish and Portuguese could be due to the inadequate support of the training set ( 2,741 and 1,182 ) instead of English ( 22,825 ) .", "entities": []}, {"text": "We also believe that our approach of combining datasets could have in\ufb02uenced the performance of the low support datasets .", "entities": []}, {"text": "6 Conclusion The need to develop automated systems to detect any event is an active protest has constantly been increasing because of the escalation of social media users and several platforms to support them .", "entities": []}, {"text": "In this paper , we have explored several multilingual language models to classify if a given sentence talks about an event that has happened ( Event ) or not ( Not - event ) in three languages .", "entities": []}, {"text": "Our work primarily focuses on \ufb01ne - tuning language models and feeding them to an architecture we created .", "entities": []}, {"text": "We also observe that the problem of class imbalance has had a signi\ufb01cant impact on the performance of themodels .", "entities": []}, {"text": "The soft voting approach has achieved macro F1 - Scores of 0.8291 , 0.7578 , and 0.7951 for English , Spanish , and Portuguese , respectively .", "entities": [[6, 8, "MetricName", "macro F1"]]}, {"text": "For future work , we intend to explore class weighting techniques and semi - supervised approaches to improve our performance .", "entities": []}, {"text": "References Ghada Abaido .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Cyberbullying on social media platforms among university students in the united arab emirates .", "entities": []}, {"text": "International journal of adolescence and youth , 25:407\u2013420 .", "entities": []}, {"text": "Angelo Basile and Tommaso Caselli .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Protest event detection : When task - speci\ufb01c models outperform an event - driven method .", "entities": [[1, 3, "TaskName", "event detection"]]}, {"text": "In Experimental IR Meets Multilinguality , Multimodality , and Interaction , pages 97\u2013111 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Michael Beyeler .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Machine Learning for OpenCV .", "entities": []}, {"text": "Packt Publishing Ltd. Berfu B \u00a8uy\u00a8uk\u00a8oz , Ali H \u00a8urriyeto \u02d8glu , and Arzucan \u00a8Ozg\u00a8ur .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Analyzing ELMo and DistilBERT on sociopolitical news classi\ufb01cation .", "entities": [[1, 2, "MethodName", "ELMo"], [3, 4, "MethodName", "DistilBERT"]]}, {"text": "In Proceedings of the Workshop on Automated Extraction of Sociopolitical Events from News 2020 , pages 9\u201318 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Andreu Casero - Ripoll \u00b4 es and Ram \u00b4 on Feenstra .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "The 15 - m movement and the new media : A case study of how new themes were introduced into spanish political discourse .", "entities": []}, {"text": "Media International Australia incorporating Culture and Policy , 144:68 .", "entities": []}, {"text": "David Crystal . 2008 .", "entities": []}, {"text": "Two thousand million ?", "entities": []}, {"text": "English Today , 24(1):3\u20136 .", "entities": []}, {"text": "Vera Danilova . 2015 .", "entities": []}, {"text": "A pipeline for multilingual protest event selection and annotation .", "entities": []}, {"text": "In 2015 26th International Workshop on Database and Expert Systems Applications ( DEXA ) , pages 309\u2013313 .", "entities": []}, {"text": "Vera Danilova , Svetlana Popova , and Mikhail Alexandrov .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Multilingual protest event data collection with gate .", "entities": []}, {"text": "In Natural Language Processing and Information Systems , pages 115\u2013126 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Dhaval M Dave , Andrew I Friedson , Kyutaro Matsuzawa , Joseph J Sabia , and Samuel Safford .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Black lives matter protests and risk avoidance : The case of civil unrest during a pandemic .", "entities": []}, {"text": "Working Paper 27408 , National Bureau of Economic Research .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association", "entities": []}, {"text": "103for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Allyson Ettinger , Sudha Rao , Hal Daum \u00b4 e III , and Emily M. Bender .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Towards linguistically generalizable NLP systems : A workshop and shared task .", "entities": []}, {"text": "In Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems , pages 1\u201310 , Copenhagen , Denmark .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Matthias Gall \u00b4 e. 2019 .", "entities": []}, {"text": "Investigating the effectiveness of BPE : The power of shorter sequences .", "entities": [[4, 5, "MethodName", "BPE"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 1375\u20131381 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rodrigo G \u00b4 omez Garc \u00b4 \u0131a and Emiliano Trer \u00b4 e. 2014 .", "entities": []}, {"text": "The # yosoy132 movement and the struggle for media democratization in mexico .", "entities": []}, {"text": "Convergence , 20(4):496\u2013510 .", "entities": []}, {"text": "Deborah J Gerner , Philip A Schrodt , Omur Yilmaz , and Rajaa Abu - Jabr .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "The creation of cameo ( con\ufb02ict and mediation event observations ): An event data framework for a post cold war world .", "entities": []}, {"text": "In annual meeting of the American Political Science Association , volume 29 .", "entities": []}, {"text": "Adeep Hande , Karthik Puranik , Ruba Priyadharshini , and Bharathi Raja Chakravarthi .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Domain identi\ufb01cation of scienti\ufb01c articles using transfer learning and ensembles .", "entities": [[6, 8, "TaskName", "transfer learning"]]}, {"text": "In Trends and Applications in Knowledge Discovery and Data Mining , pages 88\u201397 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Sepp Hochreiter and J \u00a8urgen Schmidhuber .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Long short - term memory .", "entities": [[0, 5, "MethodName", "Long short - term memory"]]}, {"text": "Neural Computation , 9(8):1735\u20131780 .", "entities": []}, {"text": "Kristoffer Holt , Adam Shehata , Jesper Str \u00a8omb\u00a8ack , and Elisabet Ljungberg .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "2013 .", "entities": []}, {"text": "Age and the effects of news media attention and social media use on political interest and participation : Do social media function as leveller ?", "entities": []}, {"text": "European Journal of Communication , 28(1):19\u201334 .", "entities": []}, {"text": "Ahmad Hany Hossny , Terry Moschuo , Grant Osborne , Lewis Mitchell , and Nick Lothian .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Enhancing keyword correlation for event detection in social networks using svd and k - means : Twitter case study .", "entities": [[4, 6, "TaskName", "event detection"], [10, 11, "DatasetName", "svd"]]}, {"text": "Social Network Analysis and Mining , 8(1):1\u201310 .", "entities": []}, {"text": "Ali H \u00a8urriyeto \u02d8glu , Osman Mutlu , Farhana Ferdousi Liza , Erdem Y \u00a8or\u00a8uk , Ritesh Kumar , and Shyam Ratan .", "entities": [[17, 18, "DatasetName", "Kumar"]]}, {"text": "2021 .", "entities": []}, {"text": "Multilingual protest news detection shared task 1 , case 2021 .", "entities": []}, {"text": "In Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio - political Events from Text ( CASE 2021 ) , online .", "entities": []}, {"text": "Association for Computational Linguistics ( ACL).Ali H \u00a8urriyeto \u02d8glu , Erdem Y \u00a8or\u00a8uk , Deniz Y \u00a8uret , C \u00b8 a \u02d8gr\u0131 Yoltar , Burak G \u00a8urel , F\u0131rat Durus \u00b8an , and Osman Mutlu .", "entities": []}, {"text": "2019a .", "entities": []}, {"text": "A task set proposal for automatic protest information collection across multiple countries .", "entities": []}, {"text": "In Advances in Information Retrieval , pages 316\u2013323 , Cham .", "entities": [[3, 5, "TaskName", "Information Retrieval"]]}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Ali H \u00a8urriyeto \u02d8glu , Erdem Y \u00a8or\u00a8uk , Deniz Y \u00a8uret , C \u00b8 a \u02d8gr\u0131 Yoltar , Burak G \u00a8urel , F\u0131rat Durus \u00b8an , Osman Mutlu , and Arda Akdemir .", "entities": []}, {"text": "2019b .", "entities": []}, {"text": "Overview of clef 2019 lab protestnews : Extracting protests from news in a cross - context setting .", "entities": []}, {"text": "In Experimental IR Meets Multilinguality , Multimodality , and Interaction , pages 425\u2013432 , Cham .", "entities": []}, {"text": "Springer International Publishing .", "entities": []}, {"text": "Ali H \u00a8urriyeto \u02d8glu , Vanni Zavarella , Hristo Tanev , Erdem Y \u00a8or\u00a8uk , Ali Safaya , and Osman Mutlu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Automated extraction of socio - political events from news ( AESPEN ): Workshop and shared task report .", "entities": []}, {"text": "InProceedings of the Workshop on Automated Extraction of Socio - political Events from News 2020 , pages 1\u20136 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Ali H \u00a8urriyeto \u02d8glu , Erdem Y \u00a8or\u00a8uk , Osman Mutlu , F\u0131rat Durus \u00b8an , C \u00b8 a \u02d8gr\u0131 Yoltar , Deniz Y \u00a8uret , and Burak G\u00a8urel . 2021 .", "entities": []}, {"text": "Cross - Context News Corpus for Protest Event - Related Knowledge Base Construction .", "entities": []}, {"text": "Data Intelligence , pages 1\u201328 .", "entities": []}, {"text": "Kawal Kapoor , Kuttimani Tamilmani , Nripendra Rana , Pushp Patil , Yogesh Dwivedi , and Sridhar Nerur .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Advances in social media research : Past , present and future .", "entities": []}, {"text": "Information Systems Frontiers , 20 .", "entities": []}, {"text": "Min Lin , Qiang Chen , and Shuicheng Yan . 2014 .", "entities": []}, {"text": "Network in network .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized bert pretraining approach .", "entities": []}, {"text": "Konstantina Papanikolaou and Haris Papageorgiou .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Protest event analysis : A longitudinal analysis for Greece .", "entities": []}, {"text": "In Proceedings of the Workshop on Automated Extraction of Socio - political Events from News 2020 , pages 57\u201362 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Yifan Peng , Manabu Torii , Cathy H. Wu , and K. VijayShanker .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "A generalizable nlp framework for fast development of pattern - based biomedical relation extraction systems .", "entities": [[12, 14, "TaskName", "relation extraction"]]}, {"text": "BMC Bioinformatics , 15 .", "entities": []}, {"text": "Telmo Pires , Eva Schlinger , and Dan Garrette .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "How multilingual is multilingual BERT ?", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4996 \u2013 5001 , Florence , Italy . Association for Computational Linguistics .", "entities": [[19, 20, "MethodName", "Florence"]]}, {"text": "104Cristina M. Pulido , Gisela Redondo - Sama , Teresa Sord \u00b4 e - Mart \u00b4 \u0131 , and Ramon Flecha .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Social impact in social media : A new method to evaluate the social impact of research .", "entities": []}, {"text": "PLOS ONE , 13(8):1\u201320 .", "entities": [[0, 1, "DatasetName", "PLOS"]]}, {"text": "Adam Rome .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "2010 .", "entities": []}, {"text": "The genius of earth day .", "entities": []}, {"text": "Environmental History , 15(2):194\u2013205 .", "entities": []}, {"text": "Kristin Ross . 2008 .", "entities": []}, {"text": "May\u201968 and its Afterlives .", "entities": []}, {"text": "University of Chicago Press .", "entities": []}, {"text": "Victor Sanh , Lysandre Debut , Julien Chaumond , and Thomas Wolf .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Distilbert , a distilled version of bert : smaller , faster , cheaper and lighter .", "entities": [[0, 1, "MethodName", "Distilbert"]]}, {"text": "ArXiv , abs/1910.01108 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Philip A Schrodt and Blake Hall .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Twenty years of the kansas event data system project .", "entities": []}, {"text": "The political methodologist , 14(1):2\u20138 .", "entities": []}, {"text": "Nitish Srivastava , Geoffrey E. Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov .", "entities": [[14, 15, "DatasetName", "Ruslan"]]}, {"text": "2014 .", "entities": []}, {"text": "Dropout : a simple way to prevent neural networks from over\ufb01tting .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "Journal of Machine Learning Research , 15(1):1929\u20131958 . S. Wilson .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Detecting mass protest through social media .", "entities": []}, {"text": "Social media and society , 6:5\u201325 .", "entities": []}, {"text": "Donghyeon Won , Zachary C Steinert - Threlkeld , and Jungseock Joo . 2017a .", "entities": []}, {"text": "Protest activity detection and perceived violence estimation from social media images .", "entities": [[1, 3, "TaskName", "activity detection"]]}, {"text": "In Proceedings of the 25th ACM international conference on Multimedia , pages 786\u2013794 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Donghyeon Won , Zachary C. Steinert - Threlkeld , and Jungseock Joo . 2017b .", "entities": []}, {"text": "Protest activity detection and perceived violence estimation from social media images .", "entities": [[1, 3, "TaskName", "activity detection"]]}, {"text": "CoRR , abs/1709.06204 .", "entities": []}]