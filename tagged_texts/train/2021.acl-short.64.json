[{"text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Short Papers ) , pages 504\u2013510 August 1\u20136 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics504Towards Generative Aspect - Based Sentiment Analysis\u0003 Wenxuan Zhang1 , Xin Li2 , Yang Deng1 , Lidong", "entities": []}, {"text": "Bing2and Wai Lam1 1The Chinese University of Hong Kong 2DAMO Academy , Alibaba Group fwxzhang , ydeng , wlam g@se.cuhk.edu.hk fxinting.lx , l.bing g@alibaba-inc.com Abstract Aspect - based sentiment analysis ( ABSA ) has received increasing attention recently .", "entities": [[25, 30, "TaskName", "Aspect - based sentiment analysis"]]}, {"text": "Most existing work tackles ABSA in a discriminative manner , designing various task - speci\ufb01c classi\ufb01cation networks for the prediction .", "entities": []}, {"text": "Despite their effectiveness , these methods ignore the rich label semantics in ABSA problems and require extensive task - speci\ufb01c designs .", "entities": []}, {"text": "In this paper , we propose to tackle various ABSA tasks in a uni\ufb01ed generative framework .", "entities": []}, {"text": "Two types of paradigms , namely annotation - style and extraction - style modeling , are designed to enable the training process by formulating each ABSA task as a text generation problem .", "entities": [[29, 31, "TaskName", "text generation"]]}, {"text": "We conduct experiments on four ABSA tasks across multiple benchmark datasets where our proposed generative approach achieves new state - of - the - art results in almost all cases .", "entities": []}, {"text": "This also validates the strong generality of the proposed framework which can be easily adapted to arbitrary ABSA task without additional taskspeci\ufb01c model design.1 1 Introduction Aspect - based sentiment analysis ( ABSA ) , aiming at mining \ufb01ne - grained opinion information towards speci\ufb01c aspects , has attracted increasing attention in recent years ( Liu , 2012 ) .", "entities": [[26, 31, "TaskName", "Aspect - based sentiment analysis"]]}, {"text": "Multiple fundamental sentiment elements are involved in ABSA , including the aspect term , opinion term , aspect category , and sentiment polarity .", "entities": []}, {"text": "Given a simple example sentence \u201c The pizza is delicious .", "entities": []}, {"text": "\u201d , the corresponding elements are \u201c pizza \u201d , \u201c delicious \u201d , \u201c food quality \u201d and \u201c positive \u201d , respectively .", "entities": []}, {"text": "\u0003Work done when Wenxuan Zhang was an intern at Alibaba .", "entities": []}, {"text": "The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project Code : 14200719 ) .", "entities": []}, {"text": "1The data and code can be found at https://github . com / IsakZhang / Generative - ABSAThe main research line of ABSA focuses on the identi\ufb01cation of those sentiment elements such as extracting the aspect term ( Liu et al . , 2015 ;", "entities": []}, {"text": "Yin et al . , 2016 ; Li et al . , 2018 ; Ma et al . , 2019 ) or classifying the sentiment polarity for a given aspect ( Wang et al . , 2016 ; Chen et al . , 2017 ; Jiang et al . , 2019 ; Zhang and Qian , 2020 ) .", "entities": []}, {"text": "To provide more detailed information , many recent studies propose to jointly predict multiple elements simultaneously ( Li et al . , 2019a ;", "entities": []}, {"text": "Wan et al . , 2020 ; Peng et al . , 2020 ; Zhao et", "entities": []}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Taking the Uni\ufb01ed ABSA ( UABSA , also called End - to - End ABSA ) task as an example , it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities ( Luo et al . , 2019 ; He et al . , 2019 ) .", "entities": []}, {"text": "In general , most ABSA tasks are formulated as either sequence - level or token - level classi\ufb01cation problems ( Li et al . , 2019b ) .", "entities": []}, {"text": "By designing taskspeci\ufb01c classi\ufb01cation networks , the prediction is made in a discriminative manner , using the class index as labels for training ( Huang and Carley , 2018 ; Wan et al . , 2020 ) .", "entities": []}, {"text": "However , these methods ignore the label semantics , i.e. , the meaning of the natural language labels , during the training process .", "entities": []}, {"text": "Intuitively , knowing the meaning of \u201c food quality \u201d and \u201c restaurant ambiance \u201d , it can be much easier to identify that the former one is more likely to be the correct aspect category for the concerned aspect \u201c pizza \u201d .", "entities": []}, {"text": "Such semantics of the label can be more helpful for the joint extraction of multiple sentiment elements , due to the complicated interactions of those involved elements .", "entities": []}, {"text": "For example , understanding \u201c delicious \u201d is an adjective for describing the food such as \u201c pizza \u201d could better lead to the prediction of aspect opinion pair ( \u201c pizza \u201d , \u201c delicious \u201d ) .", "entities": []}, {"text": "Another issue is that different classi\ufb01cation models are proposed to suit the need of different ABSA problems , making it dif\ufb01cult to adapt the model from one to another .", "entities": []}, {"text": "Motivated by recent success in formulating sev-", "entities": []}, {"text": "505eral language understanding problems such as named entity recognition , question answering , and text classi\ufb01cation as generation tasks ( Raffel et al . , 2020 ; Athiwaratkun et al . , 2020 ) , we propose to tackle various ABSA problems in a uni\ufb01ed generative approach in this paper .", "entities": [[6, 9, "TaskName", "named entity recognition"], [10, 12, "TaskName", "question answering"]]}, {"text": "It can fully utilize the rich label semantics by encoding the natural language label into the target output .", "entities": []}, {"text": "Moreover , this uni\ufb01ed generative model can be seamlessly adapted to multiple tasks without introducing additional task - speci\ufb01c model designs .", "entities": []}, {"text": "In order to enable the Generative Aspect - based Sentiment analysis ( GAS ) , we tailor - make two paradigms , namely annotation - style and extractionstyle modeling to transform the original task as a generation problem .", "entities": [[6, 11, "TaskName", "Aspect - based Sentiment analysis"]]}, {"text": "Given a sentence , the former one adds annotations on it to include the label information when constructing the target sentence ; while the latter directly adopts the desired natural language label of the input sentence as the target .", "entities": []}, {"text": "The original sentence and the target sentence produced by either paradigm can then be paired as a training instance of the generation model .", "entities": []}, {"text": "Furthermore , we propose a prediction normalization strategy to handle the issue that the generated sentiment element falls out of its corresponding label vocabulary set .", "entities": []}, {"text": "We investigate four ABSA tasks including Aspect Opinion Pair Extraction ( AOPE ) , Uni\ufb01ed ABSA ( UABSA ) , Aspect Sentiment Triplet Extraction ( ASTE ) , and Target Aspect Sentiment Detection ( TASD ) with the proposed uni\ufb01ed GASframework to verify its effectiveness and generality .", "entities": [[20, 24, "TaskName", "Aspect Sentiment Triplet Extraction"]]}, {"text": "Our main contributions are 1 ) We tackle various ABSA tasks in a novel generative manner ; 2 ) We propose two paradigms to formulate each task as a generation problem and a prediction normalization strategy to re\ufb01ne the generated outputs ; 3 ) We conduct experiments on multiple benchmark datasets across four ABSA tasks and our approach surpasses previous state - of - the - art in almost all cases .", "entities": []}, {"text": "Specifically , we obtain 7.6 and 3.7 averaged gains on the challenging ASTE and TASD task respectively .", "entities": []}, {"text": "2 Generative ABSA ( G AS ) 2.1 ABSA with Generative Paradigm", "entities": []}, {"text": "In this section , we describe the investigated ABSA tasks and the proposed two paradigms , namely , annotation - style and extraction - style modeling .", "entities": []}, {"text": "Aspect Opinion Pair Extraction ( AOPE ) aims to extract aspect terms and their correspondingopinion terms as pairs ( Zhao et al . , 2020 ; Chen et al . , 2020 ) .", "entities": []}, {"text": "Here is an illustrative example of our generative formulations for the AOPE task :", "entities": []}, {"text": "Input : Salads were fantastic , our server was also very helpful .", "entities": []}, {"text": "Target ( Annotation - style ) :", "entities": []}, {"text": "[ Salads jfantastic ] were fantastic here , our [ server j helpful ] was also very helpful .", "entities": []}, {"text": "Target ( Extraction - style ) : ( Salads , fantastic ) ; ( server , helpful )", "entities": []}, {"text": "In the annotation - style paradigm , to indicate the pair relations between the aspect and opinion terms , we append the associated opinion modi\ufb01er to each aspect term in the form of [ aspectjopinion ] for constructing the target sentence , as shown in the above example .", "entities": []}, {"text": "The prediction of the coupled aspect and opinion term is thus achieved by including them in the same bracket .", "entities": []}, {"text": "For the extraction - style paradigm , we treat the desired pairs as the target , which resembles direct extraction of the expected sentiment elements but in a generative manner .", "entities": []}, {"text": "Uni\ufb01ed ABSA ( UABSA ) is the task of extracting aspect terms and predicting their sentiment polarities at the same time ( Li et al . , 2019a ; Chen and Qian , 2020 ) .", "entities": []}, {"text": "We also formulate it as an ( aspect , sentiment polarity ) pair extraction problem .", "entities": []}, {"text": "For the same example given above , we aim to extract two pairs : ( Salads , positive ) and ( server , positive ) .", "entities": []}, {"text": "Similarly , we replace each aspect term as [ aspectj sentiment polarity ] under the annotation - style formulation and treat the desired pairs as the target output in the extraction - style paradigm to reformulate the UABSA task as a text generation problem .", "entities": [[41, 43, "TaskName", "text generation"]]}, {"text": "Aspect Sentiment Triplet Extraction ( ASTE ) aims to discover more complicated ( aspect , opinion , sentiment polarity ) triplets ( Peng et al . , 2020 ):", "entities": [[0, 4, "TaskName", "Aspect Sentiment Triplet Extraction"]]}, {"text": "Input : The Unibody construction is solid , sleek and beautiful .", "entities": []}, {"text": "Target ( Annotation - style ) :", "entities": []}, {"text": "The [ Unibody construction jpositivejsolid , sleek , beautiful ] is solid , sleek and beautiful .", "entities": []}, {"text": "Target ( Extraction - style ) : ( Unibody construction , solid , positive ) ; ( Unibody construction , sleek , positive ) ; ( Unibody construction , beautiful , positive ) ; As shown above , we annotate each aspect term with its corresponding sentiment triplet wrapped in the bracket , i.e. , [ aspectjopinionjsentiment polarity ] for the annotation - style modeling .", "entities": []}, {"text": "Note that", "entities": []}, {"text": "506we will include all the opinion modi\ufb01ers of the same aspect term within the same bracket to predict the sentiment polarities more accurately .", "entities": []}, {"text": "For the extraction - style paradigm , we just concatenate all triplets as the target output .", "entities": []}, {"text": "Target Aspect Sentiment Detection ( TASD ) is the task to detect all ( aspect term , aspect category , sentiment polarity ) triplets for a given sentence ( Wan et al . , 2020 ) , where the aspect category belongs to a pre - de\ufb01ned category set .", "entities": []}, {"text": "For example , Input : A big disappointment , all around .", "entities": []}, {"text": "The pizza was cold and the cheese was n\u2019t even fully melted .", "entities": []}, {"text": "Target ( Annotation - style ) :", "entities": []}, {"text": "A big disappointment , all around .", "entities": []}, {"text": "The [ pizza jfood qualityjnegative ] was cold and the [ cheese j food qualityjnegative ] was n\u2019t even fully melted [ nulljrestaurant generaljnegative ] .", "entities": []}, {"text": "Target ( Extraction - style ) : ( pizza , food quality , negative ) ; ( cheese , food quality , negative ) ; ( null , restaurant general , negative ) ; Similarly , we pack each aspect term , the aspect category it belongs to , and its sentiment polarity into a bracket to build the target sentence for the annotation - style method .", "entities": []}, {"text": "Note that we use a bigram expression for the aspect category instead of the original uppercase form \u201c FOOD#QUALITY \u201d to make the annotated target sentence more natural .", "entities": []}, {"text": "As presented in the example , some triplets may not have explicitly - mentioned aspect terms , we thus use \u201c null \u201d to represent it and put such triplets at the end of the target output .", "entities": []}, {"text": "For the extraction - style paradigm , we concatenate all the desired triplets , including those with implicit aspect terms , as the target sentence for sequence - to - sequence learning .", "entities": []}, {"text": "2.2 Generation Model Given the input sentence x , we generate a target sequence y0 , which is either based on the annotationstyle or extraction - style paradigm as described in the last section , with a text generation model f(\u0001 ) .", "entities": [[37, 39, "TaskName", "text generation"]]}, {"text": "Then the desired sentiment pairs or triplets scan be decoded from the generated sequence y0 .", "entities": []}, {"text": "Specifically , for the annotation - style modeling , we extract the contents included in the bracket \u201c [ ] \u201d from y0 , and separate different sentiment elements with the vertical bar \u201c j \u201d .", "entities": []}, {"text": "If such decoding fails , e.g. , we can not \ufb01nd any bracket in the output sentence or the number of vertical bars is not as expected , L14 R14 R15 R16 HAST+TOWEy53.41 62.39 58.12 63.84 JERE - MHSy52.34 66.02 59.64 67.65 SpanMlt ( Zhao et al . , 2020 ) 68.66 75.60 64.68 71.78 SDRN ( Chen et al . , 2020 ) 66.18 73.30 65.75 73.67 GAS - ANNOTATION -R 68.74 72.66 65.03 73.75 GAS - EXTRACTION -R 67.58 73.22 65.83 74.12 GAS - ANNOTATION 69.55 75.15 67.93 75.42 GAS - EXTRACTION 68.08 74.12 67.19 74.54 Table 1 : Main results of the AOPE task .", "entities": []}, {"text": "The best results are in bold , second best results are underlined .", "entities": []}, {"text": "Results are the average F1 scores over 5 runs.ydenotes results are from Zhao", "entities": [[3, 5, "MetricName", "average F1"]]}, {"text": "et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "L14 R14 R15 R16 BERT+GRU ( Li et al . , 2019b ) 61.12 73.17 59.60 70.21 SPAN - BERT ( Hu et al . , 2019 ) 61.25 73.68 62.29 IMN - BERT ( He et al . , 2019 ) 61.73 70.72 60.22 RACL ( Chen and Qian , 2020 ) 63.40 75.42 66.05 Dual - MRC ( Mao et al . , 2021 ) 65.94 75.95 65.08 GAS - ANNOTATION -R 67.37 75.77 65.75 71.87 GAS - EXTRACTION -R 66.71 76.30 64.00 72.39 GAS - ANNOTATION 68.64 76.58 66.78 73.21 GAS - EXTRACTION 68.06 77.13 65.96 73.64 Table 2 : Main results of the UABSA task .", "entities": [[19, 20, "MethodName", "BERT"], [33, 34, "MethodName", "BERT"]]}, {"text": "The best results are in bold , second best results are underlined .", "entities": []}, {"text": "Results are the average F1 scores over 5 runs .", "entities": [[3, 5, "MetricName", "average F1"]]}, {"text": "we ignore such predictions .", "entities": []}, {"text": "For the extractionstyle paradigm , we separate the generated pairs or triplets from the sequence y0and ignore those invalid generations in a similar way .", "entities": []}, {"text": "We adopt the pre - trained T5 model ( Raffel et al . , 2020 ) as the generation model f(\u0001 ) , which closely follows the encoder - decoder architecture of the original Transformer ( Vaswani et", "entities": [[6, 7, "MethodName", "T5"], [34, 35, "MethodName", "Transformer"]]}, {"text": "al . , 2017 ) .", "entities": []}, {"text": "Therefore , by formulating these ABSA tasks as a text generation problem , we can tackle them in a uni\ufb01ed sequence - to - sequence framework without taskspeci\ufb01c model design .", "entities": [[9, 11, "TaskName", "text generation"]]}, {"text": "2.3 Prediction Normalization Ideally , the generated element e2safter decoding is supposed to exactly belong to the vocabulary set it is meant to be .", "entities": []}, {"text": "For example , the predicted aspect term should explicitly appear in the input sentence .", "entities": []}, {"text": "However , this might not always hold since each element is generated from the vocabulary set containing all tokens instead of its speci\ufb01c vocabulary set .", "entities": []}, {"text": "Thus , the predictions of a generation model may exhibit morphology shift from the ground - truths , e.g. , from single toplural nouns .", "entities": []}, {"text": "507L14 R14 R15 R16 CMLA+ ( Wang et al . , 2017 ) 33.16 42.79 37.01 41.72 Li - uni\ufb01ed - R ( Li et al . , 2019a ) 42.34 51.00 47.82 44.31 Pipeline ( Peng et al . , 2020 ) 42.87 51.46 52.32 54.21 Jet(Xu et al . , 2020 ) 43.34 58.14 52.50 63.21 Jet+BERT ( Xu et", "entities": []}, {"text": "al . , 2020 ) 51.04 62.40 57.53 63.83 GAS - ANNOTATION -R 52.80 67.35 56.95 67.43 GAS - EXTRACTION -R 58.19 70.52 60.23 69.05 GAS - ANNOTATION 54.31 69.30 61.02 68.65 GAS - EXTRACTION 60.78 72.16 62.10 70.10 Table 3 : Main results of the ASTE task .", "entities": []}, {"text": "The best results are in bold , second best results are underlined .", "entities": []}, {"text": "Results are the average F1 scores over 5 runs .", "entities": [[3, 5, "MetricName", "average F1"]]}, {"text": "We propose a prediction normalization strategy to re\ufb01ne the incorrect predictions resulting from such issue .", "entities": []}, {"text": "For each sentiment type cdenoting the type of the element esuch as the aspect term or sentiment polarity , we \ufb01rst construct its corresponding vocabulary set Vc .", "entities": []}, {"text": "For aspect term and opinion term , Vccontains all words in the current input sentence x ; for aspect category , Vcis a collection of all categories in the dataset ; for sentiment polarity , Vccontains all possible polarities .", "entities": []}, {"text": "Then for a predicted element eof the sentiment type c , if it does not belong to the corresponding vocabulary set Vc , we use \u0016e2Vc , which has the smallest Levenshtein distance ( Levenshtein , 1966 ) with e , to replace e. 3 Experiments 3.1 Experimental Setup Datasets We evaluate the proposed GASframework on four popular benchmark datasets including Laptop14 , Rest14 , Rest15 , and Rest16 , originally provided by the SemEval shared challenges ( Pontiki et al . , 2014 , 2015 , 2016 ) .", "entities": []}, {"text": "For each ABSA task , we use the public datasets derived from them with more sentiment annotations .", "entities": []}, {"text": "Speci\ufb01cally , we adopt the dataset provided by Fan et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) , Li et al . ( 2019a ) , Xu et", "entities": []}, {"text": "al . ( 2020 ) , Wan et al . ( 2020 ) for the AOPE , UABSA , ASTE , TASD task respectively .", "entities": []}, {"text": "For a fair comparison , we use the same data split as previous works .", "entities": []}, {"text": "Evaluation Metrics We adopt F1 scores as the main evaluation metrics for all tasks .", "entities": [[4, 5, "MetricName", "F1"]]}, {"text": "A prediction is correct if and only if all its predicted sentiment elements in the pair or triplet are correct .", "entities": []}, {"text": "Experiment Details", "entities": []}, {"text": "We adopt the T5 base model from huggingface Transformer library2for 2https://github.com/huggingface/ transformersRest15 Rest16 Baseline ( Brun and Nikoulina , 2018 ) - 38.10 TAS - LPM - CRF ( Wan et al . , 2020 ) 54.76 64.66 TAS - SW - CRF ( Wan et al . , 2020 ) 57.51 65.89 TAS - SW - TO ( Wan et al . , 2020 ) 58.09 65.44 GAS - ANNOTATION -R 59.27 66.54", "entities": [[3, 4, "MethodName", "T5"], [8, 9, "MethodName", "Transformer"], [25, 26, "MethodName", "LPM"], [27, 28, "MethodName", "CRF"], [42, 43, "MethodName", "CRF"]]}, {"text": "GAS - EXTRACTION -R 60.63 68.31 GAS - ANNOTATION 60.06 67.70 GAS - EXTRACTION 61.47 69.42 Table 4 : Main results of the TASD task .", "entities": []}, {"text": "The best results are in bold , second best results are underlined .", "entities": []}, {"text": "Results are the average F1 scores over 5 runs .", "entities": [[3, 5, "MetricName", "average F1"]]}, {"text": "all experiments .", "entities": []}, {"text": "T5 closely follows the original encoder - decoder architecture of the Transformer model , with some slight differences such as different position embedding schemes .", "entities": [[0, 1, "MethodName", "T5"], [11, 12, "MethodName", "Transformer"]]}, {"text": "Therefore , the encoder and decoder of it have similar parameter size as the BERT - BASE model .", "entities": [[14, 15, "MethodName", "BERT"], [16, 17, "MethodName", "BASE"]]}, {"text": "For all tasks , we use similar experimental settings for simplicity : we train the model with the batch size of 16 and accumulate gradients every two batches .", "entities": [[18, 20, "HyperparameterName", "batch size"]]}, {"text": "The learning rate is set to be 3e-4 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}, {"text": "The model is trained up to 20 epochs for the AOPE , UABSA , and ASTE task and 30 epochs for the TASD task .", "entities": []}, {"text": "3.2 Main Results The main results for the AOPE , UABSA , ASTE , TASD task are reported in Tables 1 , 2 , 3 , 4 respectively .", "entities": []}, {"text": "For our proposed GASframework , we also present the raw results without the proposed prediction normalization strategy ( with the suf\ufb01x \u201c -R \u201d ) .", "entities": []}, {"text": "All results are the average F1 scores across 5 runs with different random seeds .", "entities": [[4, 6, "MetricName", "average F1"], [13, 14, "DatasetName", "seeds"]]}, {"text": "It is noticeable that our proposed methods , based on either annotation - style or extraction - style modeling , establish new state - of - the - art results in almost all cases .", "entities": []}, {"text": "The only exception is on the Rest15 dataset for the AOPE task , our method is still on par with the previous best performance .", "entities": []}, {"text": "It shows that tackling various ABSA tasks with the proposed uni\ufb01ed generative method is an effective solution .", "entities": []}, {"text": "Moreover , we can see that our method performs especially well on the ASTE and TASD tasks , the proposed extraction - style method outperforms the previous best models by 7.6 and 3.7 average F1 scores ( across different datasets ) on them respectively .", "entities": [[33, 35, "MetricName", "average F1"]]}, {"text": "It implies that incorporating the label semantics and appropriately modeling the interactions among those sentiment elements are essential for tackling complex ABSA problems .", "entities": []}, {"text": "508BEFORE AFTER LABEL # 1 Bbq rib BBQ rib BBQ rib # 2 repeat repeats repeats # 3 chicken peas chick peas chick peas # 4 bodys bodies None # 5 cafe coffee coffee # 6 vegetarian vegan vegetarian # 7 salmon not spinach # 8 \ufb02ight cookie might cookie fortune cookie Table 5 : Example cases of the predictions before and after the prediction normalization .", "entities": [[5, 6, "DatasetName", "Bbq"], [7, 8, "DatasetName", "BBQ"], [9, 10, "DatasetName", "BBQ"]]}, {"text": "3.3 Discussions Annotation - style & Extraction - style As shown in result tables , the annotation - style method generally performs better than the extraction - style method on the AOPE and UASA task .", "entities": []}, {"text": "However , the former one becomes inferior to the latter on the more complex ASTE and TASD tasks .", "entities": []}, {"text": "One possible reason is that , on the ASTE and TASD tasks , the annotation - style method introduces too much content , such as the aspect category and sentiment polarity , into the target sentence , which increases the dif\ufb01culty of sequence - to - sequence learning .", "entities": []}, {"text": "Why Prediction Normalization Works To better understand the effectiveness of the proposed prediction normalization strategy , we randomly sample some instances from the ASTE task that have different raw prediction and normalized prediction ( i.e. , corrected by our strategy ) .", "entities": []}, {"text": "The predicted sentiment elements before and after the normalization , as well as the gold label of some example cases are shown in Table 5 .", "entities": []}, {"text": "We \ufb01nd that the normalization mainly helps on two occasions :", "entities": []}, {"text": "The \ufb01rst one is the morphology shift where two words have minor lexical differences .", "entities": []}, {"text": "For example , the method \ufb01xes \u201c Bbq rib \u201d to \u201c BBQ rib \u201d ( # 1 ) and \u201c repeat \u201d to \u201c repeats \u201d ( # 2 ) .", "entities": [[7, 8, "DatasetName", "Bbq"], [12, 13, "DatasetName", "BBQ"]]}, {"text": "Another case is orthographic alternatives where the model might generate words with the same etyma but different word types , e.g. , it outputs \u201c vegetarian \u201d rather than \u201c vegan \u201d ( # 6 ) .", "entities": []}, {"text": "Our proposed prediction normalization , which \ufb01nds the replacement from the corresponding vocabulary set via Levenshtein distance , is a simple yet effective strategy to alleviate this issue .", "entities": []}, {"text": "We also observe that our prediction strategy may fail if the raw predictions are quite lexically different or even semantically different from the goldstandard labels ( see Case # 4 , # 7 and # 8) .", "entities": []}, {"text": "In thesecases , the dif\ufb01culty does not come from the way of performing prediction normalization but the generation of labels close to the ground truths , especially for the examples containing implicit aspects or opinions ( Case # 4 ) .", "entities": []}, {"text": "4 Conclusions and Future Work We tackle various ABSA tasks in a novel generative framework in this paper .", "entities": []}, {"text": "By formulating the target sentences with our proposed annotation - style and extraction - style paradigms , we solve multiple sentiment pair or triplet extraction tasks with a uni\ufb01ed generation model .", "entities": []}, {"text": "Extensive experiments on multiple benchmarks across four ABSA tasks show the effectiveness of our proposed method .", "entities": []}, {"text": "Our work is an initial attempt on transforming ABSA tasks , which are typically treated as classi\ufb01cation problems , into text generation problems .", "entities": [[20, 22, "TaskName", "text generation"]]}, {"text": "Experimental results indicate that such transformation is an effective solution to tackle various ABSA tasks .", "entities": []}, {"text": "Following this direction , designing more effective generation paradigms and extending such ideas to other tasks can be interesting research problems for future work .", "entities": []}, {"text": "References Ben Athiwaratkun , C \u00b4 \u0131cero Nogueira dos Santos , Jason Krone , and Bing Xiang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Augmented natural language for generative sequence labeling .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , EMNLP 2020 , pages 375\u2013385 .", "entities": []}, {"text": "Caroline Brun and Vassilina Nikoulina .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Aspect based sentiment analysis into the wild .", "entities": [[2, 4, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 9th Workshop on Computational Approaches to Subjectivity , Sentiment and Social Media Analysis , WASSA@EMNLP 2018 , pages 116 \u2013 122 .", "entities": []}, {"text": "Peng Chen , Zhongqian Sun , Lidong Bing , and Wei Yang . 2017 .", "entities": []}, {"text": "Recurrent attention network on memory for aspect sentiment analysis .", "entities": [[7, 9, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , EMNLP 2017 , pages 452 \u2013 461 .", "entities": []}, {"text": "Shaowei Chen , Jie Liu , Yu Wang , Wenzheng Zhang , and Ziming Chi . 2020 .", "entities": []}, {"text": "Synchronous doublechannel recurrent network for aspect - opinion pair extraction .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , ACL 2020 , pages 6515\u20136524 .", "entities": []}, {"text": "Zhuang Chen and Tieyun Qian .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Relation - aware collaborative learning for uni\ufb01ed aspect - based sentiment analysis .", "entities": [[7, 12, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "In Proceedings of the 58th Annual", "entities": []}, {"text": "509Meeting of the Association for Computational Linguistics , ACL 2020 , pages 3685\u20133694 .", "entities": []}, {"text": "Zhifang Fan , Zhen Wu , Xin - Yu Dai , Shujian Huang , and Jiajun Chen . 2019 .", "entities": []}, {"text": "Target - oriented opinion words extraction with target - fused neural sequence labeling .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , NAACL - HLT 2019 , pages 2509\u20132518 .", "entities": []}, {"text": "Ruidan He , Wee Sun Lee , Hwee Tou Ng , and Daniel Dahlmeier .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "An interactive multi - task learning network for end - to - end aspect - based sentiment analysis .", "entities": [[2, 6, "TaskName", "multi - task learning"], [13, 18, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "In Proceedings of the 57th Conference of the Association for Computational Linguistics , ACL 2019 , pages 504\u2013515 .", "entities": []}, {"text": "Minghao Hu , Yuxing Peng , Zhen Huang , Dongsheng Li , and Yiwei Lv . 2019 .", "entities": []}, {"text": "Open - domain targeted sentiment analysis via span - based extraction and classi\ufb01cation .", "entities": [[4, 6, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 57th Conference of the Association for Computational Linguistics , ACL 2019 , pages 537\u2013546 .", "entities": []}, {"text": "Binxuan Huang and Kathleen M. Carley .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Parameterized convolutional neural networks for aspect level sentiment classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1091\u20131096 .", "entities": []}, {"text": "Qingnan Jiang , Lei Chen , Ruifeng Xu , Xiang Ao , and Min Yang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A challenge dataset and effective models for aspect - based sentiment analysis .", "entities": [[7, 12, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , EMNLP - IJCNLP 2019 , pages 6279\u20136284 .", "entities": []}, {"text": "Vladimir I Levenshtein .", "entities": []}, {"text": "1966 .", "entities": []}, {"text": "Binary codes capable of correcting deletions , insertions , and reversals .", "entities": []}, {"text": "Xin Li , Lidong Bing , Piji Li , and Wai Lam . 2019a .", "entities": []}, {"text": "A uni\ufb01ed model for opinion target extraction and target sentiment prediction .", "entities": []}, {"text": "In The Thirty - Third AAAI Conference on Arti\ufb01cial Intelligence , AAAI 2019 , pages 6714\u20136721 .", "entities": []}, {"text": "Xin Li , Lidong Bing , Piji Li , Wai Lam , and Zhimou Yang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Aspect term extraction with history attention and selective transformation .", "entities": [[1, 3, "TaskName", "term extraction"]]}, {"text": "In Proceedings of the Twenty - Seventh International Joint Conference on Arti\ufb01cial Intelligence , IJCAI 2018 , pages 4194\u20134200 .", "entities": []}, {"text": "Xin Li , Lidong Bing , Wenxuan Zhang , and Wai Lam . 2019b .", "entities": []}, {"text": "Exploiting BERT for end - to - end aspectbased sentiment analysis .", "entities": [[1, 2, "MethodName", "BERT"], [9, 11, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 5th Workshop on Noisy User - generated Text , WNUT@EMNLP 2019 , pages 34\u201341 .", "entities": []}, {"text": "Bing Liu .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Sentiment Analysis and Opinion Mining .", "entities": [[0, 2, "TaskName", "Sentiment Analysis"], [3, 5, "TaskName", "Opinion Mining"]]}, {"text": "Synthesis Lectures on Human Language Technologies .", "entities": []}, {"text": "Pengfei Liu , Sha\ufb01q R. Joty , and Helen M. Meng .", "entities": [[8, 9, "DatasetName", "Helen"]]}, {"text": "2015 .", "entities": []}, {"text": "Fine - grained opinion mining with recurrent neural networks and word embeddings .", "entities": [[3, 5, "TaskName", "opinion mining"], [10, 12, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , EMNLP 2015 , pages 1433\u20131443 .", "entities": []}, {"text": "Huaishao Luo , Tianrui Li , Bing Liu , and Junbo Zhang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "DOER : dual cross - shared RNN for aspect term - polarity co - extraction .", "entities": []}, {"text": "In Proceedings of the 57th Conference of the Association for Computational Linguistics , ACL 2019 , pages 591\u2013601 .", "entities": []}, {"text": "Dehong Ma , Sujian Li , Fangzhao Wu , Xing Xie , and Houfeng Wang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Exploring sequence - tosequence learning in aspect term extraction .", "entities": [[7, 9, "TaskName", "term extraction"]]}, {"text": "In Proceedings of the 57th Conference of the Association for Computational Linguistics , ACL 2019 , pages 3538\u20133547 .", "entities": []}, {"text": "Yue Mao , Yi Shen , Chao Yu , and Longjun Cai . 2021 .", "entities": []}, {"text": "A joint training dual - mrc framework for aspect based sentiment analysis .", "entities": [[10, 12, "TaskName", "sentiment analysis"]]}, {"text": "CoRR , abs/2101.00816 .", "entities": []}, {"text": "Haiyun Peng , Lu Xu , Lidong Bing , Fei Huang , Wei Lu , and Luo Si . 2020 .", "entities": []}, {"text": "Knowing what , how and why : A near complete solution for aspect - based sentiment analysis .", "entities": [[12, 17, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "In The Thirty - Fourth AAAI Conference on Arti\ufb01cial Intelligence , AAAI 2020 , pages 8600 \u2013 8607 .", "entities": []}, {"text": "Maria Pontiki , Dimitris Galanis , Haris Papageorgiou , Ion Androutsopoulos , Suresh Manandhar , Mohammad Al - Smadi , Mahmoud Al - Ayyoub , Yanyan Zhao , Bing Qin , Orph \u00b4 ee De Clercq , V \u00b4 eronique Hoste , Marianna Apidianaki , Xavier Tannier , Natalia V .", "entities": []}, {"text": "Loukachevitch , Evgeniy V .", "entities": []}, {"text": "Kotelnikov , N\u00b4uria Bel , Salud Mar \u00b4 \u0131a Jim \u00b4 enez Zafra , and G \u00a8ulsen Eryigit .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semeval-2016 task 5 : Aspect based sentiment analysis .", "entities": [[6, 8, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 10th International Workshop on Semantic Evaluation , SemEval@NAACL - HLT 2016 , pages 19\u201330 .", "entities": []}, {"text": "Maria Pontiki , Dimitris Galanis , Haris Papageorgiou , Suresh Manandhar , and Ion Androutsopoulos . 2015 .", "entities": []}, {"text": "Semeval-2015 task 12 : Aspect based sentiment analysis .", "entities": [[6, 8, "TaskName", "sentiment analysis"]]}, {"text": "In SemEval@NAACL - HLT , pages 486\u2013495 .", "entities": []}, {"text": "Maria Pontiki , Dimitris Galanis , John Pavlopoulos , Harris Papageorgiou , Ion Androutsopoulos , and Suresh Manandhar .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Semeval-2014 task 4 : Aspect based sentiment analysis .", "entities": [[6, 8, "TaskName", "sentiment analysis"]]}, {"text": "In SemEval@COLING 2014 , pages 27\u201335 .", "entities": []}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J. Liu . 2020 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the limits of transfer learning with a uni\ufb01ed text - to - text transformer .", "entities": [[4, 6, "TaskName", "transfer learning"]]}, {"text": "J. Mach .", "entities": []}, {"text": "Learn .", "entities": []}, {"text": "Res . , 21:140:1\u2013140:67 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 30 : Annual Conference on Neural", "entities": []}, {"text": "510Information Processing Systems 2017 , pages 5998 \u2013 6008 .", "entities": []}, {"text": "Hai Wan , Yufei Yang , Jianfeng Du , Yanan Liu , Kunxun Qi , and Jeff Z. Pan . 2020 .", "entities": []}, {"text": "Target - aspect - sentiment joint detection for aspect - based sentiment analysis .", "entities": [[8, 13, "TaskName", "aspect - based sentiment analysis"]]}, {"text": "InThe Thirty - Fourth AAAI Conference on Arti\ufb01cial Intelligence , AAAI 2020 , pages 9122\u20139129 .", "entities": []}, {"text": "Wenya Wang , Sinno Jialin Pan , Daniel Dahlmeier , and Xiaokui Xiao . 2017 .", "entities": []}, {"text": "Coupled multi - layer attentions for co - extraction of aspect and opinion terms .", "entities": []}, {"text": "In Proceedings of the Thirty - First AAAI Conference on Arti\ufb01cial Intelligence , pages 3316\u20133322 .", "entities": []}, {"text": "Yequan Wang , Minlie Huang , Xiaoyan Zhu , and Li Zhao .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Attention - based LSTM for aspectlevel sentiment classi\ufb01cation .", "entities": [[3, 4, "MethodName", "LSTM"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , EMNLP 2016 , pages 606 \u2013 615 .", "entities": []}, {"text": "Lu Xu , Hao Li , Wei Lu , and Lidong Bing .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Position - aware tagging for aspect sentiment triplet extraction .", "entities": [[5, 9, "TaskName", "aspect sentiment triplet extraction"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , EMNLP 2020 , pages 2339\u20132349 .", "entities": []}, {"text": "Yichun Yin , Furu Wei , Li Dong , Kaimeng Xu , Ming Zhang , and Ming Zhou .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Unsupervised word and dependency path embeddings for aspect term extraction .", "entities": [[8, 10, "TaskName", "term extraction"]]}, {"text": "In Proceedings of the Twenty - Fifth International Joint Conference on Arti\ufb01cial Intelligence , IJCAI 2016 , pages 2979\u20132985 .", "entities": []}, {"text": "Mi Zhang and Tieyun Qian .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis .", "entities": [[0, 1, "MethodName", "Convolution"], [10, 12, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , EMNLP 2020 , pages 3540\u20133549 .", "entities": []}, {"text": "He Zhao , Longtao Huang , Rong Zhang , Quan Lu , and Hui Xue . 2020 .", "entities": []}, {"text": "SpanMlt :", "entities": []}, {"text": "A span - based multi - task learning framework for pair - wise aspect and opinion terms extraction .", "entities": [[4, 8, "TaskName", "multi - task learning"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , ACL 2020 , pages 3239\u20133248 .", "entities": []}]