[{"text": "Proceedings of the 29th International Conference on Computational Linguistic , pages 176\u2013181 October 12\u201317 , 2022.176CompLx@SMM4H\u201922 : In - domain pretrained language models for detection of adverse drug reaction mentions in English tweets Orest Xherija", "entities": [[20, 23, "TaskName", "pretrained language models"]]}, {"text": "Independent Researcher xherija.orest@gmail.comHojoon Choi Nielsen hojoon.choi@nielsen.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "The paper describes the system that team CompLx developed for sub - task 1a of the Social Media Mining for Health 2022 ( # SMM4H ) Shared Task .", "entities": [[24, 25, "DatasetName", "SMM4H"]]}, {"text": "We finetune a RoBERTa model , a pretrained , transformer - based language model , on a provided dataset to classify English tweets for mentions of Adverse Drug Reactions ( ADRs ) , i.e. negative side effects related to medication intake .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}, {"text": "With only a simple finetuning , our approach achieves competitive results , significantly outperforming the average score across submitted systems .", "entities": []}, {"text": "We make the model checkpoints1and code2publicly available .", "entities": []}, {"text": "We also create a web application3to provide a userfriendly , readily accessible interface for anyone interested in exploring the model \u2019s capabilities .", "entities": []}, {"text": "1 Introduction The Shared Task ( Weissenbacher et al . , 2022 ) of the 2022 Social Media Mining for Health Applications ( # SMM4H ) workshop proposed ten sub - tasks in the domain of social media mining for health monitoring and surveillance .", "entities": [[24, 25, "DatasetName", "SMM4H"]]}, {"text": "From the perspective of Natural Language Processing ( NLP ) , these tasks present a considerable challenge since the nature of social media posts requires dealing with both a significant level of language variation ( informal and colloquial expressions , ambiguity , multilingual posts ) and data sparsity , as well as a widespread presence of noise such as misspellings of clinical concepts and syntactic errors .", "entities": []}, {"text": "In the 2022 instantiation of the # SMM4H Shared Task , our team participated in : ( i ) sub - task 1a , the classification of English tweets containing mentions of Adverse Drug Reactions ( ADRs ) ( Magge et al . , 2021 ) , ( ii ) sub - task 3 , the classification of English tweets ( 3a ) and WebMD reviews ( 3b ) contain1https://huggingface.co/orestxherija/roberta-base-adrsmm4h2022 2https://github.com/orestxherija/CompLx-SMM4H2022 3https://huggingface.co/spaces/orestxherija/adr-mentionclassifiering mentions of changes in medication treatments , and ( iii ) sub - task 8 , the classification of English tweets self - reporting chronic stress .", "entities": [[7, 8, "DatasetName", "SMM4H"]]}, {"text": "In this paper we primarily describe our approach for task 1a , as that constituted the major focus of our efforts .", "entities": []}, {"text": "To address these challenges , we finetune a variant of a RoBERTa ( Liu et al . , 2019 ) model , a transformer - based ( Vaswani et al . , 2017 ) language model pretrained on approximately 128 million tweets ( Loureiro et al . , 2022 ) on each sub - task \u2019s provided dataset .", "entities": [[11, 12, "MethodName", "RoBERTa"]]}, {"text": "Without any domain adaptation efforts ( apart from standard finetuning on the downstream task ) or hyperparameter optimizations , the model outperforms the average of all submissions for sub - task 1a by a 9%absolute difference in F1score .", "entities": [[2, 4, "TaskName", "domain adaptation"]]}, {"text": "In the following sections , we introduce the subtasks \u2019 datasets , describe the model architecture and training setup , report our results , and conclude with a discussion of related research and potential avenues for future work .", "entities": []}, {"text": "2 Datasets In Section [ 1 ] we provided a brief summary of each sub - task in which we participated .", "entities": []}, {"text": "For each of them , participants were given access to a labeled training and validation set , as well as an unlabeled evaluation set that was used to determined the final performance of the submitted systems .", "entities": []}, {"text": "Table", "entities": []}, {"text": "[ 1 ] summarizes the number of samples per dataset per task .", "entities": [[5, 8, "HyperparameterName", "number of samples"]]}, {"text": "Additionally , Table [ 2 ] provides representative samples from sub - task 1a .", "entities": []}, {"text": "As can be noted upon quick inspection , merely depending 1a 3a 3b 8 training 17174 5898 10378 2936 validation 909 1572 1297 420 evaluation 10969 15360 13132 839", "entities": []}, {"text": "Table 1 : Number of samples per split per task .", "entities": [[3, 6, "HyperparameterName", "Number of samples"]]}, {"text": "177Sentence Label vyvanse make me so hyper and creative and i think of so many tweets ADR feed an ocd vyvanse and cover him in crayons No ADR trazodone has screwed up my sleep schedule .", "entities": [[19, 20, "DatasetName", "ocd"]]}, {"text": "its helping tho .", "entities": []}, {"text": "ADR Table 2 : Selection of samples from training set of sub - task 1a .", "entities": []}, {"text": "on medication - related keywords for label assignment is going to be problematic : both the first and the second example contain the medication term \u201c vyvanse \" but they have been assigned different labels , \u201c ADR \" and \u201c No ADR \" respectively .", "entities": []}, {"text": "This motivates the use of a modeling approach that leverages the overall semantic content of the sentence , rather than keyword matching with individual constituents .", "entities": []}, {"text": "3 Modeling Approach 3.1 Model Architecture", "entities": []}, {"text": "The establishment of language modeling as the pretraining step in the transfer learning pipeline revolutionized modern NLP with models such as ULMFiT ( Howard and Ruder , 2018 ) , ELMo ( Peters et al . , 2018 )", "entities": [[11, 13, "TaskName", "transfer learning"], [21, 22, "MethodName", "ULMFiT"], [30, 31, "MethodName", "ELMo"]]}, {"text": "and , most notably , transformerbased language models such as GPT ( Radford et al . , 2018 ) and BERT ( Devlin et al . , 2019 ) .", "entities": [[10, 11, "MethodName", "GPT"], [20, 21, "MethodName", "BERT"]]}, {"text": "In recent years , there have been intensive efforts in the research community to produce ever - larger transformer - based pretrained language models that are trained using a variety of datasets , transformermodel architectures , training objectives and optimization techniques .", "entities": [[21, 24, "TaskName", "pretrained language models"]]}, {"text": "This should come as no surprise , since such language models have dominated virtually all NLP leaderboards , most notably GLUE ( Wang et al . , 2018 ) and SuperGLUE ( Wang et al . , 2019 ) .", "entities": [[20, 21, "DatasetName", "GLUE"], [30, 31, "DatasetName", "SuperGLUE"]]}, {"text": "Considering this overwhelming success , we opt for a RoBERTa ( Liu et al . , 2019 ) model4that has been trained on approximately 128 million tweets ( Loureiro et al . , 2022 ) .", "entities": [[9, 10, "MethodName", "RoBERTa"]]}, {"text": "Our exact modeling approach is depicted in Figure [ 1 ] .", "entities": []}, {"text": "We opt for a model that has been trained on an in - domain corpus , namely tweets , as transfer learning has been shown to yield improved results when there is indomain pretraining ( Gururangan et al . , 2020 ) .", "entities": [[20, 22, "TaskName", "transfer learning"]]}, {"text": "We do not use any text normalization steps .", "entities": []}, {"text": "4https://huggingface.co/cardiffnlp/twitter-roberta-basemar20223.2", "entities": []}, {"text": "Training Regime We train the model to minimize the negative log - likelihood loss using back - propagation with stochastic gradient descent and a mini - batch size of16 .", "entities": [[10, 13, "MetricName", "log - likelihood"], [13, 14, "MetricName", "loss"], [19, 22, "MethodName", "stochastic gradient descent"], [24, 28, "HyperparameterName", "mini - batch size"]]}, {"text": "To monitor model performance , we use the train / validation split provided by the organizers .", "entities": []}, {"text": "For optimization , we use the AdamW optimizer ( Loshchilov and Hutter , 2019 ) with gradient clipping ( Pascanu et al . , 2013 ) and a linear scheduler with no warm - up .", "entities": [[6, 7, "MethodName", "AdamW"], [7, 8, "HyperparameterName", "optimizer"], [16, 18, "MethodName", "gradient clipping"]]}, {"text": "We use FP-16 mixed precision ( Micikevicius et al . , 2018 ) training ( and inference ) in order to afford a larger batch size and increased training speed .", "entities": [[24, 26, "HyperparameterName", "batch size"]]}, {"text": "To optimize GPU use by minimizing the amount of memory allocated for padding tokens , we use dynamic padding and length - based batching in the sense of ( Skinner , 2018 ) .", "entities": []}, {"text": "Finally , we employ label smoothing ( Szegedy et al . , 2016 ) with a smoothing factor of 0.1 .", "entities": [[4, 6, "MethodName", "label smoothing"]]}, {"text": "3.3 Hyperparameters As mentioned in Section [ 1 ] , we do not experiment with hyperparameter tuning but rather keep the default parameters of the Trainer API in the Hugging Face transformers library .", "entities": []}, {"text": "More specifically , we use \u03b21= 0.9 , \u03b22= 0.999and \u03f5=", "entities": []}, {"text": "10\u22128for the AdamW optimizer parameter values and a learning rate of 0.00005 .", "entities": [[2, 3, "MethodName", "AdamW"], [3, 4, "HyperparameterName", "optimizer"], [8, 10, "HyperparameterName", "learning rate"]]}, {"text": "We train the models for a maximum of 25epochs with an early stopping patience level set to 0.001for3epochs .", "entities": [[11, 13, "MethodName", "early stopping"]]}, {"text": "Finally , we set a maximum sequence length of 128 since input sentences are generally short and we would like to avoid consuming GPU memory for padding tokens .", "entities": []}, {"text": "4 Experiments and Results In this section , we give a brief description of the system we used to conduct our experiments , share our results and provide a brief discussion .", "entities": []}, {"text": "4.1 Setup The model was developed using the PyTorch ( Paszke et al . , 2019 ) implementation of the Hugging Face transformers ( Wolf et al . , 2020 ) library .", "entities": []}, {"text": "The experiments were executed on a", "entities": []}, {"text": "178 0", "entities": [[1, 2, "DatasetName", "0"]]}, {"text": "[ CLS]2362 no5840 \u0120taste13", "entities": []}, {"text": "\u0120for47 \u0120you111 \u0120-11454 vy9965 van1090 se2 [ SEP]Finetuned RoBERT a - Base Language Model BackboneFeed Forward Layer no taste for you -vyvanse1.53 -0.68ADR", "entities": []}, {"text": "No ADRADRModel Output Classification Featurization TokenizationFigure 1 : Illustration of modeling approach ( inference step ): the string input is tokenized and the tokens are passed through the language model backbone so as to obtain contextualized ( vector ) representations of the tokens .", "entities": [[3, 4, "TaskName", "Classification"]]}, {"text": "The vector associated with the [ CLS ] token is passed through a feed - forward layer and the logit outputs are used to decide the sample \u2019s class label , \" ADR \" or \" No ADR \" .", "entities": []}, {"text": "machine with an Intel Core i9 - 9820X CPU @ 3.30GHz and a NVIDIA GeForce RTX 2080", "entities": []}, {"text": "Ti GPU with 11 GB of memory .", "entities": []}, {"text": "4.2 Results Table", "entities": []}, {"text": "[ 3 ] summarizes the performance of our approach in the validation set for each sub - task .", "entities": []}, {"text": "Note that in this set of experiments , the validation set was used both during training ( e.g. for early stopping or selection of batch size ) as well as for the reporting of the systems \u2019 performance .", "entities": [[19, 21, "MethodName", "early stopping"], [24, 26, "HyperparameterName", "batch size"]]}, {"text": "Table", "entities": []}, {"text": "[ 4 ] summarizes the performance of our approach in the evaluation set for each sub - task .", "entities": []}, {"text": "The organizers chose to disclose to each team only their respective score along with the average score of all submitted systems .", "entities": []}, {"text": "Our system performed considerably better than the average in sub - task 1a and surpassed the existing state - of - the - art F1 - score of 0.63reported in ( Magge et al . , 2021 ) .", "entities": [[24, 27, "MetricName", "F1 - score"]]}, {"text": "Performance was considerably poorer for sub - tasks 3 and 8 .", "entities": []}, {"text": "As mentioned in P R F1 1a 0.769 0.769 0.769 3a 0.030 0.312 0.055 3b 0.571 0.995 0.725 8 0.372 1.0 0.543 Table 3 : Validation set results for sub - tasks 1a , 3 and 8.P R F1 Subtask-1a0.737 0.585 0.652 ( 0.646 ) ( 0.497 ) ( 0.562 )", "entities": [[5, 6, "MetricName", "F1"], [38, 39, "MetricName", "F1"]]}, {"text": "Subtask-3a0.034 0.341 0.061 ( 0.535 ) ( 0.458 ) ( 0.456 ) Subtask-3b0.567 1.0 0.723 ( 0.778 ) ( 0.888 ) ( 0.818 )", "entities": []}, {"text": "Subtask-80.372 1.0 0.542 ( 0.720 ) ( 0.760 ) ( 0.750 )", "entities": []}, {"text": "Table 4 : Results on the evaluation set for sub - tasks 1a , 3 and 8 .", "entities": []}, {"text": "Average score of all participating systems in parentheses .", "entities": []}, {"text": "Metric is F1 - score for class 1 .", "entities": [[2, 5, "MetricName", "F1 - score"]]}, {"text": "Section [ 1 ] , our main efforts were dedicated to subtask 1a and the system developed did not transfer well to the remaining sub - tasks .", "entities": []}, {"text": "5 Conclusion and Future Directions", "entities": []}, {"text": "We demonstrated that a RoBERTa model ( Liu et al . , 2019 ) pretrained on approximately 128 million tweets performs very competitively when finetuned on English tweet classification for ADRs .", "entities": [[4, 5, "MethodName", "RoBERTa"]]}, {"text": "Using only a standard finetuning approach , our model obtained competitive results , outperforming the average of all submissions for sub - task 1 by a 9 % absolute difference in F1 - score .", "entities": [[31, 34, "MetricName", "F1 - score"]]}, {"text": "This constitutes yet another testament of the fact that large pre-", "entities": []}, {"text": "179trained language models have rightfully become the default approach in virtually all NLP tasks .", "entities": []}, {"text": "With respect to potential future work , there is a large collection of available options .", "entities": []}, {"text": "Text classification and , more generally , binary classification is one of the oldest and most widely researched topics in NLP .", "entities": [[0, 2, "TaskName", "Text classification"]]}, {"text": "Most approaches aiming to improve performance of classification models can be broadly categorized into three groups , depending on the segment of the machine learning workflow that they are targeting .", "entities": []}, {"text": "Data augmentation methods typically target the initial part of the workflow , the data , aiming to increase the quantity , quality and diversity of the training dataset to ensure that model performance is robust to small syntactic or semantic perturbations in the inputs .", "entities": [[0, 2, "TaskName", "Data augmentation"]]}, {"text": "Transformations acting directly on strings , such random token insertions or deletions , synonym / antonym replacements and related techniques ( Wei and Zou , 2019 ; Karimi et al . , 2021 , inter alia ) have shown significant performance improvements , especially in lowresource scenarios much like the one in this shared task .", "entities": []}, {"text": "A second approach , evidently a natural extension of the previous technique , would be to target vector encodings of the tokens and/or documents that are produced by the various layers of the neural networks .", "entities": []}, {"text": "We can distinguish two different approaches here : ( i ) improve the language model backbone during the pretraining phase , or ( ii ) improve the weights of the language model backbone during finetuning .", "entities": []}, {"text": "The research community has devoted intensive efforts in the former approach , as can be observed by the ever - increasing list of transformerbased pretrained language models ( Devlin et al . , 2019 ;", "entities": [[24, 27, "TaskName", "pretrained language models"]]}, {"text": "Joshi et al . , 2020 ; Kitaev et al . , 2020 ; Raffel et al . , 2020 ; Brown et al . , 2020 , inter multi alia ) released .", "entities": []}, {"text": "Model size , in terms of total number of trainable parameters , has been consistently shown to correlate strongly with downstream performance , so opting for a larger pretrained model would be a reasonable first steps towards more transferable vector representations ( and hence improved performance ) in the downstream task .", "entities": []}, {"text": "The latter approach would include domain adaptation techniques , such as continued self - supervised pretraining followed by supervised finetuning , which has been shown ( Gururangan et al . , 2020 ) to consistently lead to superior results relative to direct finetuning .", "entities": [[5, 7, "TaskName", "domain adaptation"]]}, {"text": "Finally , one could aim to improve performance by modifying aspects of the objective function.(Hui and Belkin , 2021 ) , in an extensive series of experiments , show that the established practice of using a cross - entropy loss for classification is not well - founded and show through a variety of diverse experiments that a square loss can , in many cases , significantly improve performance .", "entities": [[39, 40, "MetricName", "loss"], [58, 59, "MetricName", "loss"]]}, {"text": "References Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - V oss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel Ziegler , Jeffrey Wu , Clemens Winter , Chris Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Language Models are Few - Shot Learners .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 33 , pages 1877\u20131901 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Suchin Gururangan , Ana Marasovi \u00b4 c , Swabha Swayamdipta , Kyle Lo , Iz Beltagy , Doug Downey , and Noah A. Smith .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Do n\u2019t Stop Pretraining : Adapt Language Models to Domains and Tasks .", "entities": []}, {"text": "InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8342\u20138360 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jeremy Howard and Sebastian Ruder .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Universal Language Model Fine - tuning for Text Classification .", "entities": [[0, 6, "MethodName", "Universal Language Model Fine - tuning"], [7, 9, "TaskName", "Text Classification"]]}, {"text": "InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 328\u2013339 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Like Hui and Mikhail Belkin .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Evaluation of neural architectures trained with square loss vs crossentropy in classification .", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Mandar Joshi , Danqi Chen , Yinhan Liu , Daniel S. Weld , Luke Zettlemoyer , and Omer Levy . 2020 .", "entities": []}, {"text": "SpanBERT :", "entities": []}, {"text": "Improving Pre - training by Representing and Predicting Spans .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 8:64\u201377 .", "entities": []}, {"text": "Akbar Karimi , Leonardo Rossi , and Andrea Prati . 2021 .", "entities": []}, {"text": "AEDA :", "entities": [[0, 1, "MethodName", "AEDA"]]}, {"text": "An Easier Data Augmentation Technique for", "entities": [[0, 4, "MethodName", "An Easier Data Augmentation"]]}, {"text": "180Text Classification .", "entities": [[1, 2, "TaskName", "Classification"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2021 , pages 2748\u20132754 , Punta Cana , Dominican Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nikita Kitaev , Lukasz Kaiser , and Anselm Levskaya .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Reformer : The Efficient Transformer .", "entities": [[0, 1, "MethodName", "Reformer"], [4, 5, "MethodName", "Transformer"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "RoBERTa : A Robustly Optimized BERT Pretraining Approach .", "entities": [[0, 1, "MethodName", "RoBERTa"], [5, 6, "MethodName", "BERT"]]}, {"text": "CoRR , abs/1907.11692 .", "entities": []}, {"text": "Ilya Loshchilov and Frank Hutter .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Decoupled Weight Decay Regularization .", "entities": [[1, 3, "MethodName", "Weight Decay"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Daniel Loureiro , Francesco Barbieri , Leonardo Neves , Luis Espinosa Anke , and Jose Camacho - collados . 2022 .", "entities": []}, {"text": "TimeLMs :", "entities": []}, {"text": "Diachronic Language Models from Twitter .", "entities": []}, {"text": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 251\u2013260 , Dublin , Ireland .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Arjun Magge , Elena Tutubalina , Zulfat Miftahutdinov , Ilseyar Alimova , Anne Dirkson , Suzan Verberne , Davy Weissenbacher , and Graciela GonzalezHernandez . 2021 .", "entities": []}, {"text": "DeepADEMiner : a deep learning pharmacovigilance pipeline for extraction and normalization of adverse drug event mentions on twitter .", "entities": []}, {"text": "Journal of the American Medical Informatics Association , 28(10):2184\u20132192 .", "entities": []}, {"text": "Paulius Micikevicius , Sharan Narang , Jonah Alben , Gregory Diamos , Erich Elsen , David Garcia , Boris Ginsburg , Michael Houston , Oleksii Kuchaiev , Ganesh Venkatesh , and Hao Wu .", "entities": [[22, 23, "DatasetName", "Houston"]]}, {"text": "2018 .", "entities": []}, {"text": "Mixed Precision Training .", "entities": [[1, 2, "MetricName", "Precision"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Razvan Pascanu , Tomas Mikolov , and Yoshua Bengio .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "On the Difficulty of Training Recurrent Neural Networks .", "entities": []}, {"text": "In Proceedings of the 30th International Conference on Machine Learning , volume 28 ofProceedings of Machine Learning Research , pages 1310\u20131318 , Atlanta , GA , USA . PMLR .", "entities": [[24, 25, "MethodName", "GA"]]}, {"text": "Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , Alban Desmaison , Andreas Kopf , Edward Yang , Zachary DeVito , Martin Raison , Alykhan Tejani , Sasank Chilamkurthy , Benoit Steiner , Lu Fang , Junjie Bai , and Soumith Chintala . 2019 .", "entities": [[0, 1, "MethodName", "Adam"], [9, 10, "MethodName", "Adam"]]}, {"text": "PyTorch :", "entities": []}, {"text": "An Imperative Style , High - Performance Deep Learning Library .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 32 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Matthew E. Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep Contextualized Word Representations .", "entities": []}, {"text": "In Proceedings of the 2018 Conferenceof the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 2227\u20132237 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Karthik Narasimha , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving Language Understanding by Generative Pre - Training .", "entities": []}, {"text": "Blog post , OpenAI .", "entities": []}, {"text": "Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J. Liu . 2020 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "Exploring the Limits of Transfer Learning with a Unified Text - to - Text Transformer .", "entities": [[4, 6, "TaskName", "Transfer Learning"], [14, 15, "MethodName", "Transformer"]]}, {"text": "Journal of Machine Learning Research , 21(140):1\u201367 .", "entities": []}, {"text": "Michael Skinner .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Product Categorization with LSTMs and Balanced Pooling Views .", "entities": [[0, 2, "TaskName", "Product Categorization"]]}, {"text": "In SIGIR 2018", "entities": []}, {"text": "Workshop on eCommerce ( ECOM 18 ) , SIGIR \u2019 18 , Ann Arbor , MI , USA . ACM .", "entities": [[19, 20, "DatasetName", "ACM"]]}, {"text": "Christian Szegedy , Vincent Vanhoucke , Sergey Ioffe , Jon Shlens , and Zbigniew Wojna .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Rethinking the Inception Architecture for Computer Vision .", "entities": []}, {"text": "In2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 2818\u20132826 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is All you Need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Alex Wang , Yada Pruksachatkun , Nikita Nangia , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman . 2019 . SuperGLUE :", "entities": [[27, 28, "DatasetName", "SuperGLUE"]]}, {"text": "A Stickier Benchmark for General - Purpose Language Understanding Systems .", "entities": [[4, 5, "DatasetName", "General"]]}, {"text": "In Advances in Neural Information Processing Systems , volume 32 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "GLUE :", "entities": [[0, 1, "DatasetName", "GLUE"]]}, {"text": "A Multi - Task Benchmark and Analysis Platform for Natural Language Understanding .", "entities": [[9, 12, "TaskName", "Natural Language Understanding"]]}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 353\u2013355 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jason Wei and Kai Zou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "EDA :", "entities": []}, {"text": "Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks .", "entities": [[1, 3, "TaskName", "Data Augmentation"], [8, 10, "TaskName", "Text Classification"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 6382\u20136388 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Davy Weissenbacher , Ari Z. Klein , Luis Gasc\u00f3 , Darryl Estrada - Zavala , Martin Krallinger , Yuting Guo , Yao Ge , Abeed Sarker , Ana Lucia Schmidt , Raul Rodriguez - Esteban , Mathias Leddin , Arjun Magge ,", "entities": []}, {"text": "181Juan M. Banda , Vera Davydova , Elena Tutubalina , and Graciela Gonzalez - Hernandez .", "entities": []}, {"text": "2022 .", "entities": []}, {"text": "Overview of the Seventh Social Media Mining for Health Applications ( # SMM4H ) Shared Tasks at COLING 2022 .", "entities": [[12, 13, "DatasetName", "SMM4H"]]}, {"text": "InProceedings of the Seventh Social Media Mining for Health ( # SMM4H ) Workshop and Shared Task , Gyeongju , Republic of Korea .", "entities": [[11, 12, "DatasetName", "SMM4H"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , R\u00e9mi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander M. Rush .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Transformers : State - of - the - Art Natural Language Processing .", "entities": []}, {"text": "InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 38\u201345 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}]