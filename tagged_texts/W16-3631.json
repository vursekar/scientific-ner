[{"text": "Proceedings of the SIGDIAL 2016 Conference , pages 242\u2013251 , Los Angeles , USA , 13 - 15 September 2016 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2016 Association for Computational Linguistics Supporting Spoken Assistant Systems with a Graphical User Interface that Signals Incremental Understanding and Prediction State Casey Kennington andDavid Schlangen Boise State University and DSG / CITEC /", "entities": []}, {"text": "Bielefeld University caseykennington@boisestate.edu and david.schlangen@uni-bielefeld.de Abstract Arguably , spoken dialogue systems are most often used not in hands / eyes - busy situations , but rather in settings where a graphical display is also available , such as a mobile phone .", "entities": [[8, 11, "TaskName", "spoken dialogue systems"]]}, {"text": "We explore the use of a graphical output modality for signalling incremental understanding and prediction state of the dialogue system .", "entities": []}, {"text": "By visualising the current dialogue state and possible continuations of it as a simple tree , and allowing interaction with that visualisation ( e.g. , for con\ufb01rmations or corrections ) , the system provides both feedback on past user actions and guidance on possible future ones , and it can span the continuum from slot \ufb01lling to full prediction of user intent ( such as GoogleNow ) .", "entities": []}, {"text": "We evaluate our system with real users and report that they found the system intuitive and easy to use , and that incremental and adaptive settings enable users to accomplish more tasks .", "entities": []}, {"text": "1 Introduction Current virtual personal assistants ( PAs ) require users to either formulate complex intents in one utterance ( e.g. , \u201c call Peter Miller on his mobile phone \u201d ) or go through tedious sub - dialogues ( e.g. , \u201c phone call \u201d \u2013 who would you like to call ?", "entities": []}, {"text": "\u2013 \u201c Peter Miller \u201d \u2013 I have a mobile number and a work number .", "entities": []}, {"text": "Which one do you want ? ) .", "entities": []}, {"text": "This is not how one would interact with a human assistant , where the request would be naturally structured into smaller chunks that individually get acknowledged ( e.g. , \u201c Can you make a connection for me ? \u201d", "entities": []}, {"text": "\u2013 sure \u2013 \u201c with Peter Miller \u201d - uh huh - \u201c on his mobile \u201d - dialling now ) .", "entities": []}, {"text": "Current PAs signal ongoing understanding by displaying the state ofthe recognised speech ( ASR ) to the user , but not their semantic interpretation of it .", "entities": []}, {"text": "Another type of assistant system forgoes enquiring user intent altogether and infers likely intents from context .", "entities": []}, {"text": "GoogleNow , for example , might present traf\ufb01c information to a user picking up their mobile phone at their typical commute time .", "entities": []}, {"text": "These systems display their \u201c understanding \u201d state , but do not allow any type of interaction with it apart from dismissing the provided information .", "entities": []}, {"text": "In this work , we explore adding a graphical user interface ( GUI ) modality that makes it possible to see these interaction styles as extremes on a continuum , and to realise positions between these extremes and present a mixed graphical / voice enabled PAthat can provide feedback of understanding to the user incrementally as the user \u2019s utterance unfolds \u2013 allowing users to make requests in instalments instead of fully thought - out requests .", "entities": []}, {"text": "It does this by signalling ongoing understanding in an intuitive tree - like GUIthat can be displayed on a mobile device .", "entities": []}, {"text": "We evaluate our system by directing users to perform tasks using it under nonincremental ( i.e. , ASR endpointing ) and incremental conditions and then compare the two conditions .", "entities": []}, {"text": "We further compare a non - adaptive with an adaptive ( i.e. , infers likely events ) version of our system .", "entities": []}, {"text": "We report that the users found the interface intuitive and easy to use , and that users were able to perform tasks more ef\ufb01ciently with incremental as well as adaptive variants of the system .", "entities": []}, {"text": "2 Related Work This work builds upon several threads of previous research :", "entities": []}, {"text": "Chai et al .", "entities": []}, {"text": "( 2014 ) addressed misalignments in understanding ( i.e. , common ground ( Clark and Schaefer , 1989 ) ) between robots and humans by informing the human of the internal system state via speech .", "entities": []}, {"text": "We take this idea and ap-242", "entities": []}, {"text": "ply it to a PAby displaying the internal state of the system to the user via a GUI(explained in Section 3.5 ) , allowing the user to determine if system understanding has taken place \u2013 a way of providing feedback and backchannels to the user .", "entities": []}, {"text": "Dethlefs et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2016 ) provide a good review of work that show how backchannels facilitate grounding , feedback , and clari\ufb01cations in human spoken dialogue , and apply an information density approach to determine when to backchannel using speech .", "entities": []}, {"text": "Because we do n\u2019t backchannel using speech here , there is no potential overlap between the user and the system ; rather , our system can display backchannels and ask clari\ufb01cations without frustrating the user through inadvertent overlaps .", "entities": []}, {"text": "Though different in many ways , our work is similar in some regards to Larsson et al .", "entities": []}, {"text": "( 2011 ) , which displays information to the user and allows the user to navigate the display itself ( e.g. , by sayingupordown in a menu list)\u2013functionality that we intend to apply to our GUIin future work .", "entities": []}, {"text": "Our work is also comparable to SDS toolkits such as IrisTK ( Skantze and Moubayed , 2012 ) and OpenDial ( Lison , 2015 ) which enable SDSdesigners to visualise the internal state of their systems , though not for end user interpretability .", "entities": []}, {"text": "Some of the work here is inspired by the Microsoft Language Understanding Intelligent Service(LUIS ) project ( Williams et al . , 2015 ) .", "entities": []}, {"text": "While our system by no means achieves the scale that LUIS does , we offer here an additional contribution of an open source LUIS - like system ( with the important addition of the graphical interface ) that is authorable ( using JSON \ufb01les ; we leave authoring using a web interface like that of LUIS to future work ) , extensible ( affordances can be easily added ) , incremental ( in that respect going beyond LUIS ) , trainable ( i.e. , can learn from examples , but can still function well without examples ) , and can learn through interacting ( here we apply a user model that learns during interaction ) .", "entities": []}, {"text": "3 System Description This section introduces and describes our SDS , which is modularised into four main components : ASR , natural language understanding ( NLU ) , dialogue management ( DM ) , and the graphical user interface ( GUI ) which , as explained below , is visualised as a right - branching tree .", "entities": [[21, 24, "TaskName", "natural language understanding"], [28, 30, "TaskName", "dialogue management"]]}, {"text": "The overall system is represented in Figure 1 .", "entities": []}, {"text": "For the remainder of this section , each module is explained inturn .", "entities": []}, {"text": "As each module processes input incrementally ( i.e. , word for word ) , we \ufb01rst explain our framework for incremental processing .", "entities": []}, {"text": "ASR NLU DM w1 ... wnw1 ... wnd1dms1 : v1sm : vm ... s1 : v1sm : vm ...... GUI Figure 1 : Overview of system made up of ASRwhich takes in a speech signal and produces transcribed words , NLU , which takes words and produces a slots in a frame , DMwhich takes slots and produces a decision for each , and the GUI which displays the state of the system .", "entities": []}, {"text": "3.1 Incremental Dialogue An aspect of our SDS that sets it apart from others is the requirement that it process incrementally .", "entities": []}, {"text": "One potential concern with incremental processing is regarding informativeness : why act early when waiting might provide additional information , resulting in better - informed decisions ?", "entities": []}, {"text": "The trade off is naturalness as perceived by the user who is interacting with the SDS .", "entities": []}, {"text": "Indeed , it has been shown that human users perceive incremental systems as being more natural than traditional , turn - based systems ( Aist et al . , 2006 ; Skantze and Schlangen , 2009 ; Skantze and Hjalmarsson , 1991 ; Asri et al . , 2014 ) , offer a more human - like experience ( Edlund et al . , 2008 ) and are more satisfying to interact with than non - incremental systems ( Aist et al . , 2007 ) .", "entities": []}, {"text": "Psycholinguistic research has also shown that humans comprehend utterances as they unfold and do not wait until the end of an utterance to begin the comprehension process ( Tanenhaus et al . , 1995 ; Spivey et al . , 2002 ) .", "entities": []}, {"text": "The trade - off between informativeness and naturalness can be reconciled when mechanisms are in place that allow earlier decisions to be repaired .", "entities": []}, {"text": "Such mechanisms are offered by the incremental unit ( IU ) framework for SDS ( Schlangen and Skantze , 2011 ) , which we apply here .", "entities": []}, {"text": "Following Kennington et al .", "entities": []}, {"text": "( 2014 ) , the IUframework consists of a network of processing modules .", "entities": []}, {"text": "A typical module takes input , performs some kind of processing on that data , and produces output.243", "entities": []}, {"text": "Figure 2 : Example of IU network ; part - of - speech tags are grounded into words , tags and words have same level links with left IU ; four is revoked and replaced with forty .", "entities": [[8, 11, "DatasetName", "part - of"]]}, {"text": "The data are packaged as the payload of incremental units ( IUs ) which are passed between modules .", "entities": []}, {"text": "The IUs themselves are interconnected via so - called same level links ( SLL ) and groundedin links ( GRIN ) , the former allowing the linking ofIUs as a growing sequence , the latter allowing that sequence to convey what IUs directly affect it ( see Figure 2 for an example of incremental ASR ) .", "entities": [[19, 20, "MethodName", "GRIN"]]}, {"text": "Thus IUs can be added , but can be later revoked and replaced in light of new information .", "entities": []}, {"text": "The IU framework can take advantage of up - to - date information , but have the potential to function in such a way that users perceive as more natural .", "entities": []}, {"text": "The modules explained in the remainder of this section are implemented as IU - modules and process incrementally .", "entities": []}, {"text": "Each will now be explained .", "entities": []}, {"text": "3.2 Speech Recognition The module that takes speech input from the user in our SDS is the ASR component .", "entities": [[1, 3, "TaskName", "Speech Recognition"]]}, {"text": "IncrementalASR must transcribe uttered speech into words which must be forthcoming from the ASR as early as possible ( i.e. , the ASR must not wait for endpointing to produce output ) .", "entities": []}, {"text": "Each module that follows must also process incrementally , acting in lock - step upon input as it is received .", "entities": []}, {"text": "IncrementalASR is not new ( Baumann et al . , 2009 ) and many of the current freely - accessible ASR systems can produce output ( semi- ) incrementally .", "entities": []}, {"text": "We opt for Google ASR for its vocabulary coverage of our evaluation language ( German ) .", "entities": [[3, 4, "DatasetName", "Google"]]}, {"text": "Following , Baumann et al .", "entities": []}, {"text": "( 2016 ) , we package output from the Google service into IUs which are passed to the NLU module , which we now explain .", "entities": [[9, 10, "DatasetName", "Google"]]}, {"text": "3.3 Language Understanding We approach the task of NLU as a slot-\ufb01lling task ( a very common approach ; see Tur et al .", "entities": []}, {"text": "( 2012 ) ) where an intent is complete when all slots of a frame are \ufb01lled .", "entities": []}, {"text": "The main driver of the NLU inour SDS is the SIUM model of NLU introduced in Kennington et al .", "entities": []}, {"text": "( 2013 ) .", "entities": []}, {"text": "SIUM has been used in several systems which have reported substantial results in various domains , languages , and tasks ( Han et al . , 2015 ; Kennington et al . , 2015 ; Kennington and Schlangen , 2017 )", "entities": []}, {"text": "Though originally a model of reference resolution , it was always intended to be used for general NLU , which we do here .", "entities": []}, {"text": "The model is formalised as follows : P(I|U ) = 1 P(U)P(I)X r\u2208RP(U|R = r)P(R = r|I)(1 ) That is , P(I|U)is the probability of the intentI(i.e . , a frame slot ) behind the speaker \u2019s ( ongoing ) utterance U.", "entities": []}, {"text": "This is recovered using the mediating variable R , a set of properties which map between aspects of Uand aspects of I. We opt for abstract properties here ( e.g. , the frame forrestaurant might be \ufb01lled by a certain type of cuisine intent such as italian which has properties like pasta , mediterranean , vegetarian , etc . ) .", "entities": []}, {"text": "Properties are pre - de\ufb01ned by a system designer and can match words that might be uttered to describe the intent in question .", "entities": []}, {"text": "ForP(R|I ) , probability is distributed uniformly over all properties that a given intent is speci\ufb01ed to have .", "entities": []}, {"text": "( If other information is available , more informative priors could be used as well . )", "entities": []}, {"text": "The mapping between properties and aspects of Ucan be learned from data .", "entities": []}, {"text": "During application , Ris marginalised over , resulting in a distribution over possible intents.1This occurs at each word increment , where the distribution from the previous increment is combined via P(I ) , keeping track of the distribution over time .", "entities": []}, {"text": "We further apply a simple rule to add in apriori knowledge : if some r\u2208Randw\u2208U are such that r.=w(where.=is string equality ; e.g. , an intent has the property of pasta and the word pasta is uttered ) , then we set C(U = w|R = r)=1 .", "entities": []}, {"text": "To allow for possible ASR confusions , we also apply C(U = w|R = r)=1\u2212 ld(w , r)/max ( len(w),len ( r ) ) , whereldis the Levenshtein distance ( but we only apply this if the calculated value is above a threshold of 0.6 ; i.e. , the two strings are mostly similar ) .", "entities": []}, {"text": "For all other w , C(w|r)=0 .", "entities": []}, {"text": "This results in a distribution C , which we renormalise and blend with learned distribution to yieldP(U|R ) .", "entities": []}, {"text": "1In Kennington et al .", "entities": []}, {"text": "( 2013 ) the authors apply Bayes \u2019 Rule to allow P(U|R)to produce a distribution over properties , which we adopt here.244", "entities": []}, {"text": "We apply an instantiation of SIUM for each slot .", "entities": []}, {"text": "The candidate slots which are processed depends on the state of the dialogue ; only slots represented by visible nodes are considered , thereby reducing the possible frames that could be predicted .", "entities": []}, {"text": "At each word increment , the updated slots ( and their corresponding ) distributions are given to the DM , which will now be explained .", "entities": []}, {"text": "3.4 Dialogue Manager The DMplays a crucial role in our SDS : as well as determining how to act , the DMis called upon to decide when to act , effectively giving the DM the control over timing of actions rather than relying on ASR endpointing \u2013 further separating our SDS from other systems .", "entities": []}, {"text": "The DMpolicy is based on a con\ufb01dence score derived from the NLU ( in this case , we used the distribution \u2019s argmax value ) using thresholds for the actions ( see below ) , set by hand ( i.e. , trial and error ) .", "entities": []}, {"text": "At each word and resulting distribution from NLU , the DMneeds to choose one of the following : \u2022wait \u2013 wait for more information ( i.e. , for the next word ) \u2022select \u2013 as the NLU is con\ufb01dent enough , \ufb01ll the slot can with the argmax from NLU \u2022request \u2013 signal a ( yes / no ) clari\ufb01cation request on the current slot and the proposed \ufb01ller \u2022confirm \u2013 act on the con\ufb01rmation of the user ; in effect , select the proposed slot value Though the thresholds are statically set , we applied OpenDial ( Lison , 2015 ) as an IU - module to perform the task of the DMwith the future goal that these values could be adjusted through reinforcement learning ( which OpenDial could provide ) .", "entities": []}, {"text": "The DMprocesses and makes a decision foreach slot , with the assumption that only one slot out of all that are processed will result in an non - wait action ( though this is not enforced ) .", "entities": []}, {"text": "3.5 Graphical User Interface The goal of the GUI is to intuitively inform the user about the internal state of the ongoing understanding .", "entities": []}, {"text": "One motivation for this is that the user can determine if the system understood the user \u2019s intent before providing the user with a response(e.g . , a list of restaurants of a certain type ) ; i.e. , if any misunderstanding takes place , it happens before the system commits to an action and is potentially more easily repaired .", "entities": []}, {"text": "Figure 3 : Example tree as branching from the root ; each branch represents a system affordance ( i.e. , making a phone call , reminder , \ufb01nding a restaurant , leaving a message , and \ufb01nding a route).The display is a rightbranching tree , where the branches directly off the root node display the affordances of the system ( i.e. , what domains of things it can understand and do something about ) .", "entities": []}, {"text": "When the \ufb01rst tree is displayed , it represents a state of the NLU where none of the slots are \ufb01lled , as in Figure 3 .", "entities": []}, {"text": "When a user verbally selects a domain to ask about , the tree is adjusted to make that domain the only one displayed and the slots that are required for that domain are shown as branches .", "entities": []}, {"text": "The user can then \ufb01ll those slots ( i.e. , branches ) by uttering the displayed name , or , alternatively , by uttering the item to \ufb01ll the slot directly .", "entities": []}, {"text": "For example , at a minimum , the user could utter the name of the domain then an item for each slot ( e.g. , food Thai downtown ) or the speech could be more natural ( e.g. , I \u2019m quite hungry , I am looking for some Thai food maybe in the downtown area ) .", "entities": []}, {"text": "Crucially , the user can also hesitate within and between chunks , as advancement is not triggered by silence thresholding , but rather semantically .", "entities": []}, {"text": "When something is uttered that falls into the request state of the DMas explained above , the display expands the subtree under question and marks the item with a question mark ( see Figure 4 ) .", "entities": []}, {"text": "At this point , the user can utter any kind of con\ufb01rmation .", "entities": []}, {"text": "A positive con\ufb01rmation \ufb01lls the slot with the item in question .", "entities": []}, {"text": "A negative con\ufb01rmation retracts the question , but leaves the branch expanded .", "entities": []}, {"text": "The expanded branches are displayed according to their rank as given by the NLU \u2019s probability distribution .", "entities": []}, {"text": "Though a branch in the display can theoretically display an unlimited number of children , we opted to only show 7 children ; if a branch had more , the \ufb01nal child displayed as an ellipsis .", "entities": []}, {"text": "A completed branch is collapsed , visually marking its corresponding slot as \ufb01lled .", "entities": []}, {"text": "At any245", "entities": []}, {"text": "Figure 4 : Example tree asking for con\ufb01rmation on a speci\ufb01c node ( in red with a question mark ) .", "entities": []}, {"text": "time , a user can backtrack by saying no(or equivalent ) or start the entire interaction over from the beginning with a keyword , e.g. , restart .", "entities": []}, {"text": "To aid the user \u2019s attention , the node under question is marked in red , where completed slots are represented by outlined nodes , and \ufb01lled nodes represent candidates for the current slot in question ( see examples of all three in Figure 4 ) .", "entities": []}, {"text": "For cases where the system is in the wait state for several words ( during which there is no change in the tree ) , the system signals activity at each word by causing the red node in question to temporarily change to white , then back to red ( i.e. , appearing as a blinking node to the user ) .", "entities": []}, {"text": "Figure 5 shows a \ufb01lled frame , represented as tree with one branch for each \ufb01lled slot .", "entities": []}, {"text": "Figure 5 : Example tree where all of the slots are \ufb01lled .", "entities": []}, {"text": "( i.e. , domain : food , location : university , type : thai )", "entities": []}, {"text": "Such an interface clearly shows the internal state of the SDS and whether or not it has understood the request so far .", "entities": []}, {"text": "It is designed to aid the user \u2019s attention to the slot in question , and clearly indicates the affordances that the system has .", "entities": []}, {"text": "The interface is currently a read - only display that is purely speech - driven , but it could be augmented with additional functionalities , such as tapping a node for expansion or typing input that the system might not yet display .", "entities": []}, {"text": "It is currently implemented as a web - based interface ( using the JavaScript D3 library ) , allowing it to be usable as a web application on any machine or mobile device .", "entities": [[14, 15, "DatasetName", "D3"]]}, {"text": "Adaptive Branching The GUI as explained affords an additional straight - forward extension : in order to move our system towards adaptivity on the above - mentioned continuum , the GUI can be used to signal what the system thinks the user might say next .", "entities": []}, {"text": "This is done by expanding a branch and displaying a con\ufb01rmation on that branch , signalling that the system predicts that the user will choose that particular branch .", "entities": []}, {"text": "Alternatively , if the system is con\ufb01dent that a user will \ufb01ll a slot with a particular value , that particular slot can be \ufb01lled without con\ufb01rmation .", "entities": []}, {"text": "This is displayed as a collapsed tree branch .", "entities": []}, {"text": "A system that perfectly predicts a user \u2019s intent would \ufb01ll an entire tree ( i.e. , all slots ) only requiring the user to con\ufb01rm once .", "entities": []}, {"text": "A more careful system would con\ufb01rm at each step ( such an interaction would only require the user to utter con\ufb01rmations and nothing else ) .", "entities": []}, {"text": "We applied this adaptive variant of the tree in one of our experiments explained below .", "entities": []}, {"text": "4 Experiments In this section , we describe two experiments where we evaluated our system .", "entities": []}, {"text": "It is our primary goal to show that our GUIis useful and signals understanding to the user .", "entities": []}, {"text": "We also wish to show that incremental presentation of such a GUI is more effective than an endpointed system .", "entities": []}, {"text": "We further want to show that an adaptive system is more effective than a non - adaptive system ( though both would process incrementally ) .", "entities": []}, {"text": "In order to best evaluate our system , we recruited participants to interact with our system in varied settings to compare endpointed ( i.e. , non - incremental ) and nonadaptive as well as adaptive versions .", "entities": []}, {"text": "We describe how the data were collected from the participants , then explain each experiment and give results .", "entities": []}, {"text": "4.1 Task & Procedure", "entities": []}, {"text": "The participants were seated at a desk and given written instructions indicating that they were to use the system to perform as many tasks as possible in the allotted time .", "entities": []}, {"text": "Figure 6 shows some example tasks as they would be displayed ( one at a time ) to the user .", "entities": []}, {"text": "A screen , tablet , and keyboard were on the desk in front of the user ( see Figure 7).2The user was instructed to convey the task presented on the screen to the system such 2We used a Samsung 8.4 Pro tablet turned to its side to show a larger width for the tree to grow to the right .", "entities": []}, {"text": "The tablet only showed the GUI ; the SDSran on a separate computer.246", "entities": []}, {"text": "that the GUIon the tablet would have a completed tree ( e.g. , as in Figure 5 ) .", "entities": []}, {"text": "When the participant was satis\ufb01ed that the system understood her intent , she was to press space bar on the keyboard which triggered a new task to be displayed on the screen and reset the tree to its start state on the tablet ( as in Figure 3 ) .", "entities": []}, {"text": "Figure 6 : Examples of tasks , as presented to each participant .", "entities": []}, {"text": "Each icon represents a speci\ufb01c task domain ( i.e. , call , reminder , \ufb01nd a restaurant , leave a message , or directions).The possible task domains were call , which had a single slot for name to be \ufb01lled ( i.e. , one out of the 22 most common German given names ) ; message which had a slot for name and a slot for the message ( which , when invoked , would simply \ufb01ll in directly from the ASR until 1 second of silence was detected ) ; eat which had slots for type ( in this case , 6 possible types ) and location ( in this case , 6 locations based around the city of Bielefeld ) ; route which had slots for source city and the destination city ( which shared the same list of the top 100 most populous German cities ) ; and reminder which had a slot for message .", "entities": []}, {"text": "For each task , the domain was \ufb01rst randomly chosen from the 5 possible domains , and then each slot value to be \ufb01lled was randomly chosen ( the message slot for the name andmessage domains was randomly selected from a list of 6 possible \u201c messages \u201d , each with 2 - 3 words ; e.g. , feed the cat , visit grandma , etc . ) .", "entities": []}, {"text": "The system kept track of which tasks were already presented to the participant .", "entities": []}, {"text": "At any time after the \ufb01rst task , the system could choose a task that was previously presented and present it again to the participant ( with a 50 % chance ) so the user would often see tasks that she had seen before ( with the assumption that humans who use PAs often do perform similar , if not the same , tasks more than once ) .", "entities": []}, {"text": "The participant was told that she would interact with the system in three different phases , each for 4 minutes , and to accomplish as many tasks as possible in that time allotment .", "entities": []}, {"text": "The participant was not told what the different phases were .", "entities": []}, {"text": "The experiments described in Sections 4.2 and screentabletkeyboardparticipantFigure 7 : Bird \u2019s eye view of the experiment : the participant sat at a table with a screen , tablet , and keyboard in front of them .", "entities": []}, {"text": "4.3 respectively describe and report a comparison \ufb01rst between the Phase 1 and 2 ( denoted as theendpointed and incremental variants of the system ) in order to establish whether or not the incremental variant produced better results than the endpointed variant .", "entities": []}, {"text": "We also report a comparison between Phase 2 and 3 ( incremental and incremental - adaptive phases ) .", "entities": []}, {"text": "Phase 1 and Phase 3 are not directly comparable to each other as Phase 3 is really a variant of Phase 2 .", "entities": []}, {"text": "Because of this , we \ufb01xed the order of the phase presentation for all participants .", "entities": []}, {"text": "Each of these phases are described below .", "entities": []}, {"text": "Before the participant began Phase 1 , they were able to try it out for up to 4 minutes ( in Phase 1 settings ) and ask for help from the experimenter , allowing them to get used to the Phase 1 interface before the actual experiment began .", "entities": []}, {"text": "After this trial phase , the experiment began with Phase 1 .", "entities": []}, {"text": "Phase 1 : Non - incremental In this phase , the system did not appear to work incrementally ; i.e. , the system displayed tree updates after ASR endpointing ( of 1.2 seconds \u2013 a reasonable amount of time to expect a response from a commercial spoken PA ) .", "entities": []}, {"text": "The system displayed the ongoing ASR on the tablet as it was recognised ( as is often done in commercial PAs ) .", "entities": []}, {"text": "At the end of Phase 1 , a pop up window noti\ufb01ed the user that the phase was complete .", "entities": []}, {"text": "They then moved onto Phase 2 . Phase 2 : Incremental", "entities": []}, {"text": "In this phase , the system displayed the tree information incrementally without endpointing .", "entities": []}, {"text": "The ASR was no longer displayed ; only the tree provided feedback in understanding , as explained in Section 3.5 .", "entities": []}, {"text": "After Phase 2 , a 10 - question questionnaire was displayed on the screen for the participant to \ufb01ll out comparing Phase 1 and Phase 2 .", "entities": []}, {"text": "For each question , they had the choice of Phase 1 , Phase247", "entities": []}, {"text": "2,Both , and Neither .", "entities": []}, {"text": "( See Appendix for full list of questions . )", "entities": []}, {"text": "After completing the questionnaire , they moved onto Phase 3 .", "entities": []}, {"text": "Phase 3 : Incremental - adaptive In this phase , the incremental system was again presented to the participant with an added user model that \u201c learned \u201d about the user .", "entities": []}, {"text": "If the user saw a task more than once , the user model would predict that , if the user chose that task domain again ( e.g. , route )", "entities": []}, {"text": "then the system would automatically ask a clari\ufb01cation using the previously \ufb01lled values ( except for the message slot , which the user always had to \ufb01ll ) .", "entities": []}, {"text": "If the user saw a task more than 3 times , the system skipped asking for clari\ufb01cations and \ufb01lled in the domain slots completely , requiring the user only to press the space bar to con\ufb01rm it was the correct one ( i.e. , to complete the task ) .", "entities": []}, {"text": "An example progression might be as follows : a participant is presented with the task route from Bielefeld to Berlin , then the user would attempt to get the system to \ufb01ll in the tree ( i.e. , slots ) with those values .", "entities": []}, {"text": "After some interaction in other domains , the user sees the same task again , and now after indicating the intent type route , the user must only say \u201c yes \u201d for each slot to con\ufb01rm the system \u2019s prediction .", "entities": []}, {"text": "Later , if the task is presented a third time , when entering that domain ( i.e , route ) , the two slots would already be \ufb01lled .", "entities": []}, {"text": "If later a different route task was presented , e.g. , route from Bielefeld to Hamburg , the system would already have the two slots \ufb01lled , but the user could backtrack by saying \u201c no , to Hamburg \u201d which would trigger the system to \ufb01ll the appropriate slot with the corrected value .", "entities": []}, {"text": "Later interactions within the route domain would ask for a clari\ufb01cation on the destination slot since it has had several possible values given by the participant , but continue to \ufb01ll thefrom slot with Bielefeld .", "entities": []}, {"text": "After Phase 3 , the participants were presented with another questionnaire on the screen to \ufb01ll out with the same questions ( plus two additional questions ) , this time comparing Phase 2 and Phase 3 .", "entities": []}, {"text": "For each item , they had the choice of Phase 2 , Phase 3 , Both , and Neither .", "entities": []}, {"text": "At the end of the three phases and questionnaires , the participants were given a \ufb01nal questionnaire to \ufb01ll out by hand on their general impressions of the systems .", "entities": []}, {"text": "We recruited 14 participants for the evaluation .", "entities": []}, {"text": "We used the Mint tools data collection framework ( Kousidis et al . , 2012 ) to log the interactions .", "entities": [[3, 4, "DatasetName", "Mint"]]}, {"text": "Due to some technical issues , one of the participantsdid not log interactions .", "entities": []}, {"text": "We collected data from 13 participants , post - Phase 2 questionnaires from 12 participants , post - Phase 3 questionnaires from all 14 participants , and general questionnaires from all 14 participants .", "entities": []}, {"text": "In the experiments that follow , we report objective and subjective measures to determine the settings that produced superior results .", "entities": []}, {"text": "Metrics We report the subjective results of the participant questionnaires .", "entities": []}, {"text": "We only report those items that were statistically signi\ufb01cant ( see Appendix for a full list of the questions ) .", "entities": []}, {"text": "We further report objective measures for each system variant : total number of completed tasks , fully correct frames , average frame f - score , and average time elapsed ( averages are taken over all participants for each variant ; we only used the 10 participants who fully interacted with all three phases ) .", "entities": []}, {"text": "Discussion is left to the end of this section .", "entities": []}, {"text": "4.2 Experiment 1 : Endpointed vs. Incremental", "entities": []}, {"text": "In this section we report the results of the evaluation between the endpointed ( i.e. , nonincremental ; Phase 1 ) variant vs the incremental ( Phase 2 ) variant of our system .", "entities": []}, {"text": "Subjective Results We applied a multinomial test of signi\ufb01cance to the results , treating all four possible answers as equally likely ( with Bonferroni correction of 10 ) .", "entities": []}, {"text": "The item The interface was useful and easy to understand with the answer of Both was signi\ufb01cant ( \u03c72(4 , N = 12 ) = 9.0 , p < .005 ) , as was The assistant was easy and intuitive to use also with the answer Both ( \u03c72(4 , N = 12 ) = 9.0 , p < .005 ) .", "entities": []}, {"text": "The item I always understood what the system wanted from me was also answered Both signi\ufb01cantly more times than other answers ( \u03c72(4 , N = 14 ) = 9.0 , p < .005 ) , similarly forIt was sometimes unclear to me if the assistant understood me with the answer of Both ( \u03c72 ( 4 , N = 12 )", "entities": []}, {"text": "= 10.0 , p < .005 ) .", "entities": []}, {"text": "These responses tell us that though the participants did not report preference for either system variant , they reported a general positive impression of the GUI(in both variants ) .", "entities": []}, {"text": "This is a nice result ; the GUI could be used in either system with bene\ufb01t to the users .", "entities": []}, {"text": "Objective Results The endpointed ( Phase 1 ) and incremental ( Phase 2 ) columns in Table 1 show the results of the objective evaluation .", "entities": []}, {"text": "Though the average time per task and fscore for the endpointed variant are better than those of the248", "entities": []}, {"text": "endpointed incr .", "entities": []}, {"text": "adaptive tasks 105 122 124 frames 46 46 59 fscore 0.81 0.74 0.80 time 19.1 19.6 19.5 Table 1 : Objective measures for Experiments 1 & 2 : count of completed tasks , number of fully correct frames , average fscore ( over all participants ) , and average elapsed time per task ( over all participants ) .", "entities": []}, {"text": "incremental variant , the total number of tasks for the incremental variant was higher .", "entities": []}, {"text": "Manual inspection of logs indicate that participants took advantage of the system \u2019s \ufb02exibility of understanding instalments ( i.e. , \ufb01lling frames incrementally ) .", "entities": []}, {"text": "This is evidenced in that participants often uttered words understood by the system as being negative ( e.g. , nein / no ) , either as a result of an explicit con\ufb01rmation request by the system ( e.g. , Thai ? )", "entities": []}, {"text": "or after a slot was incorrectly \ufb01lled ( something very easily determined through the GUI ) .", "entities": []}, {"text": "This is a desired outcome of using our system ; participants were able to repair local areas of misunderstanding as they took place instead of needing to correct an entire intent ( i.e. , frame ) .", "entities": []}, {"text": "However , we can not fully empirically measure these tendencies given our data .", "entities": []}, {"text": "4.3 Experiment 2 : Incremental vs. Incremental - Adaptive In this section we report results for the evaluation between the incremental ( Phase 2 ) and incremental - adaptive ( henceforth just adaptive ; Phase 3 ) systems .", "entities": []}, {"text": "Subjective Results We applied the same significance test as Experiment 1 ( with Bonferroni correction of 12 ) .", "entities": []}, {"text": "The item The interface was useful and easy to understand was answered with Both signi\ufb01cantly ( \u03c72(4 , N = 14 )", "entities": []}, {"text": "= 10.0 , p < .0042 ) ,", "entities": []}, {"text": "The item I had the feeling that the assistant attempted to learn about me was answered with Neither ( \u03c72(4 , N = 14 ) = 8.0 , p < .0042 ) , though Phase 3 was also marked ( 6 times ) .", "entities": []}, {"text": "All other items were not signi\ufb01cant .", "entities": []}, {"text": "Here again we see that there is a general positive impression of the GUIunder all conditions .", "entities": []}, {"text": "If anyone noticed that a system variant was attempting to learn a user model at all , they noticed that it was in Phase 3 , as expected .", "entities": []}, {"text": "Objective Results", "entities": []}, {"text": "The incremental ( Phase 2 ) andadaptive ( Phase 3 ) columns in Table 1 showthe results for the objective evaluation for this experiment .", "entities": []}, {"text": "There is a clear difference between the two variants , with the adaptive showing more completed tasks , more fully correct frames , and a higher average fscore ( all three likely due to the fact that frames were potentially pre-\ufb01lled ) .", "entities": []}, {"text": "4.4 Discussion While the responses do n\u2019t express any preference for a particular system variant , the overall impression of the GUIwas positive .", "entities": []}, {"text": "The objective measures show that there are gains to be made when the system signals understanding at a more \ufb01negrained interval than at the utterance level , due to the higher number of completed tasks and locallymade repairs .", "entities": []}, {"text": "There are further gains to be made when the system applies simple user modelling ( i.e. , adaptivity ) by attempting to predict what the user might want to do in a chosen domain , decreasing the possibility of user error and allowing the system to accurately and quickly complete more tasks .", "entities": []}, {"text": "Participants also did n\u2019t just get used to the system over time , as the average time per episode was fairly similar in all three phases .", "entities": []}, {"text": "The open - ended questionnaire sheds additional light .", "entities": []}, {"text": "Most of the suggestions for improvement related to ASR misrecognition and speed ( i.e. , not about the system itself ) .", "entities": []}, {"text": "Two participants suggested an ability to add \u201c free input \u201d or select alternatives from the tree .", "entities": []}, {"text": "Two participants suggested that the system be more responsive ( i.e. , in wait states ) , and give more feedback ( i.e. , backchannels ) more often .", "entities": []}, {"text": "For those participants that expressed preference to the non - incremental system ( Phase 1 ) , none of them had used a speech - based PAbefore , whereas those that expressed preference to the incremental versions ( Phases 2 and 3 ) use them regularly .", "entities": []}, {"text": "We conjecture that people without SDSexperience equate understanding with ASR , whereas those that are more familiar with PAs know that perfect ASR does n\u2019t translate to perfect understanding \u2013 hence the need for a GUI .", "entities": []}, {"text": "A potential remedy would be to display ASR with the tree , signalling understanding despite ASR errors .", "entities": []}, {"text": "5 Conclusion & Future Work Given the results and analysis , we conclude that an intuitive presentation that signals a system \u2019s ongoing understanding bene\ufb01ts end users who perform simple tasks which might be performed by a PA .", "entities": []}, {"text": "The GUIthat we provided , using a right - branching249", "entities": []}, {"text": "tree , worked well ; indeed , the participants who used it found it intuitive and easy to understand .", "entities": []}, {"text": "There are gains to be made when the system signals understanding at \ufb01ner - grained levels than just at the end of a pre - formulated utterance .", "entities": []}, {"text": "There are further gains to be made when a PAattempts to learn ( even a rudimentary ) user model to predict what the user might want to do next .", "entities": []}, {"text": "The adaptivity moves our system from one extreme of the continuum \u2013 simple slot \ufb01lling \u2013 closer towards the extreme that is fully predictive , with the additional bene\ufb01t of being able to easily correct mistakes in the predictions .", "entities": []}, {"text": "For future work , we intend to provide simple authoring tools for the system to make building simple PAs using our GUI easy .", "entities": []}, {"text": "We want to improve the NLU and scale to larger domains.3We also plan on implementing this as a standalone application that could be run on a mobile device , which could actually perform the tasks .", "entities": []}, {"text": "It would further be bene\ufb01cial to compare the GUI with a system that responds with speech ( i.e. , without a GUI ) .", "entities": []}, {"text": "Lastly , we will investigate using touch as an additional input modality to select between possible alternatives that are offered by the system .", "entities": []}, {"text": "Acknowledgements Thanks to the anonymous reviewers who provided useful comments and suggestions .", "entities": []}, {"text": "Thanks also to Julian Hough for helping with experiments .", "entities": []}, {"text": "We acknowledge support by the Cluster of Excellence \u201c Cognitive Interaction Technology \u201d ( CITEC ; EXC 277 ) at Bielefeld University , which is funded by the German Research Foundation ( DFG ) , and the BMBF KogniHome project .", "entities": []}, {"text": "Appendix The following questions were asked on both questionnaires following Phase 2 and Phase 3 ( comparing the two most latest used system versions ; as translated into English ): \u2022The interface was useful and easy to understand .", "entities": []}, {"text": "\u2022The assistant was easy and intuitive to use .", "entities": []}, {"text": "\u2022The assistant understood what I wanted to say .", "entities": []}, {"text": "\u2022I always understood what the system wanted from me .", "entities": []}, {"text": "\u2022The assistant made many mistakes .", "entities": []}, {"text": "\u2022The assistant did not respond while I spoke .", "entities": []}, {"text": "3Kennington and Schlangen ( 2017 ) showed that our chosen NLU approach can scale fairly well , but the GUI has some limits when applied to larger domains with thousands of items .", "entities": []}, {"text": "We leave improved scaling to future work.\u2022It was sometimes unclear to me if the assistant understood me .", "entities": []}, {"text": "\u2022The assistant responded while I spoke .", "entities": []}, {"text": "\u2022The assistant sometimes did things that I did not expect .", "entities": []}, {"text": "\u2022When", "entities": []}, {"text": "the assistant made mistakes , it was easy for me to correct them .", "entities": []}, {"text": "In addition to the above 10 questions , the following were also asked on the questionnaire following Phase 3 : \u2022I had the feeling that the assistant attempted to learn about me .", "entities": []}, {"text": "\u2022I had the feeling that the assistant made incorrect guesses .", "entities": []}, {"text": "The following questions were used on the general questionnaire : \u2022I regularly use personal assistants such as Siri , Cortana , Google now or Amazon Echo :", "entities": [[21, 22, "DatasetName", "Google"]]}, {"text": "Yes / No \u2022I have never used a speech - based personal assistant :", "entities": []}, {"text": "Yes / No \u2022What was your general impression of our personal assistants ?", "entities": []}, {"text": "\u2022Would you use one of these assistants on a smart phone or tablet if it were available ?", "entities": []}, {"text": "If yes , which one ?", "entities": []}, {"text": "\u2022Do you have suggestions that you think would help us improve our assistants ?", "entities": []}, {"text": "\u2022If you have used other speech - based interfaces before , do you prefer this interface ?", "entities": []}, {"text": "References Gregory Aist , James Allen , Ellen Campana , Lucian Galescu , Carlos Gallo , Scott Stoness , Mary Swift , and Michael Tanenhaus .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Software architectures for incremental understanding of human speech .", "entities": []}, {"text": "In Proceedings of CSLP , pages 1922\u20141925 .", "entities": []}, {"text": "Gregory Aist , James Allen , Ellen Campana , Carlos Gomez Gallo , Scott Stoness , and Mary Swift .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Incremental understanding in humancomputer dialogue and experimental evidence for advantages over nonincremental methods .", "entities": []}, {"text": "In Pragmatics , volume 1 , pages 149\u2013154 , Trento , Italy .", "entities": []}, {"text": "Layla El Asri , Romain Laroche , Olivier Pietquin , and Hatim Khouzaimi .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "NASTIA :", "entities": []}, {"text": "Negotiating Appointment Setting Interface .", "entities": []}, {"text": "In Proceedings of LREC , pages 266\u2013271 .", "entities": []}, {"text": "Timo Baumann , Michaela Atterer , and David Schlangen . 2009 .", "entities": [[0, 1, "DatasetName", "Timo"]]}, {"text": "Assessing and Improving the Performance of Speech Recognition for Incremental Systems .", "entities": [[6, 8, "TaskName", "Speech Recognition"]]}, {"text": "In Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics , pages 380\u2013388 , Boulder , USA , jun.250", "entities": []}, {"text": "Timo Baumann , Casey Kennington , Julian Hough , and David Schlangen . 2016 .", "entities": [[0, 1, "DatasetName", "Timo"]]}, {"text": "Recognising Conversational Speech : What an Incremental ASR Should Do for a Dialogue System and How to Get There .", "entities": []}, {"text": "InProceedings of the International Workshop Series on Spoken Dialogue Systems Technology ( IWSDS ) 2016 .", "entities": [[7, 10, "TaskName", "Spoken Dialogue Systems"]]}, {"text": "Joyce Y Chai , Lanbo She , Rui Fang , Spencer Ottarson , Cody Littley , Changsong Liu , and Kenneth Hanson .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Collaborative effort towards common ground in situated human - robot dialogue .", "entities": []}, {"text": "In Proceedings of the 2014 ACM / IEEE international conference on Human - robot interaction , pages 33\u201340 , Bielefeld , Germany .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Herbert H. Clark and Edward F. Schaefer .", "entities": []}, {"text": "1989 .", "entities": []}, {"text": "Contributing to discourse .", "entities": []}, {"text": "Cognitive Science , 13(2):259\u2013294 .", "entities": []}, {"text": "Nina Dethlefs , Helen Hastie , Heriberto Cuay \u00b4 ahuitl , Yanchao Yu , Verena Rieser , and Oliver Lemon .", "entities": [[3, 4, "DatasetName", "Helen"]]}, {"text": "2016 .", "entities": []}, {"text": "Information density and overlap in spoken dialogue .", "entities": []}, {"text": "Computer Speech and Language , 37:82\u201397 .", "entities": []}, {"text": "Jens Edlund , Joakim Gustafson , Mattias Heldner , and Anna Hjalmarsson . 2008 .", "entities": []}, {"text": "Towards human - like spoken dialogue systems .", "entities": [[4, 7, "TaskName", "spoken dialogue systems"]]}, {"text": "Speech Communication , 50(8 - 9):630\u2013645 .", "entities": []}, {"text": "Ting Han , Casey Kennington , and David Schlangen . 2015 .", "entities": []}, {"text": "Building and Applying PerceptuallyGrounded Representations of Multimodal Scene Descriptions .", "entities": []}, {"text": "In Proceedings of SEMDial , Gothenburg , Sweden .", "entities": []}, {"text": "Casey Kennington and David Schlangen . 2017 .", "entities": []}, {"text": "A Simple Generative Model of Incremental Reference Resolution in Situated Dialogue .", "entities": []}, {"text": "Comptuer Speech & Language .", "entities": []}, {"text": "Casey Kennington , Spyros Kousidis , and David Schlangen .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Interpreting Situated Dialogue Utterances : an Update Model that Uses Speech , Gaze , and Gesture Information .", "entities": []}, {"text": "In SIGdial 2013 , number August , pages 173\u2013182 .", "entities": []}, {"text": "Casey Kennington , Spyros Kousidis , and David Schlangen . 2014 .", "entities": []}, {"text": "InproTKs :", "entities": []}, {"text": "A Toolkit for Incremental Situated Processing .", "entities": []}, {"text": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ( SIGDIAL ) , pages 84 \u2013 88 , Philadelphia , PA , U.S.A. Association for Computational Linguistics .", "entities": []}, {"text": "Casey Kennington , Ryu Iida , Takenobu Tokunaga , and David Schlangen . 2015 .", "entities": []}, {"text": "Incrementally Tracking Reference in Human / Human Dialogue Using Linguistic and Extra - Linguistic Information .", "entities": []}, {"text": "In HLTNAACL 2015 - Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics , Proceedings of the Main Conference , pages 272\u2013282 , Denver , U.S.A. Association for Computational Linguistics .", "entities": []}, {"text": "Spyridon Kousidis , Thies Pfeiffer , Zo\ufb01a Malisz , Petra Wagner , and David Schlangen . 2012 .", "entities": []}, {"text": "Evaluating a minimally invasive laboratory architecture for recording multimodal conversational data .", "entities": []}, {"text": "In Proceedings of the Interdisciplinary Workshop on Feedback Behaviors in Dialog , Interspeech Satellite Workshop , pages 39\u201342 .", "entities": []}, {"text": "Staffan Larsson , Alexander Berman , and Jessica Villing .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Multimodal Menu - based Dialogue with Speech Cursor in DICO II", "entities": []}, {"text": "+ .", "entities": []}, {"text": "In Computational Linguistics , number June , pages 92\u201396 .", "entities": []}, {"text": "Pierre Lison .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A hybrid approach to dialogue management based on probabilistic rules .", "entities": [[4, 6, "TaskName", "dialogue management"]]}, {"text": "Computer Speech and Language , 34(1):232\u2013255 .", "entities": []}, {"text": "David Schlangen and Gabriel Skantze .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "A General , Abstract Model of Incremental Dialogue Processing .", "entities": [[1, 2, "DatasetName", "General"]]}, {"text": "In Dialogue & Discourse , volume 2 , pages 83\u2013111 .", "entities": []}, {"text": "Gabriel Skantze and Anna Hjalmarsson .", "entities": []}, {"text": "1991 .", "entities": []}, {"text": "Towards Incremental Speech Production in Dialogue Systems .", "entities": []}, {"text": "In Word Journal Of The International Linguistic Association , pages 1\u20138 , Tokyo , Japan , sep .", "entities": []}, {"text": "Gabriel Skantze and Samer Al Moubayed .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "IrisTK : a Statechart - based Toolkit for Multi - party Face - to - face Interaction .", "entities": []}, {"text": "In ICMI .", "entities": []}, {"text": "Gabriel Skantze and David Schlangen . 2009 .", "entities": []}, {"text": "Incremental dialogue processing in a micro - domain .", "entities": []}, {"text": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics on EACL 09 , ( April):745\u2013753 .", "entities": []}, {"text": "Michael J. Spivey , Michael K. Tanenhaus , Kathleen M. Eberhard , and Julie C. Sedivy .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Eye movements and spoken language comprehension : Effects of visual context on syntactic ambiguity resolution .", "entities": []}, {"text": "Cognitive Psychology , 45(4):447\u2013481 .", "entities": []}, {"text": "Michael Tanenhaus , Michael Spivey - Knowlton , Kathleen Eberhard , and Julie Sedivy .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Integration of visual and linguistic information in spoken language comprehension .", "entities": []}, {"text": "Science ( New York , N.Y. ) , 268(5217):1632\u20131634 .", "entities": []}, {"text": "Gokhan Tur , Li Deng , Dilek Hakkani - T \u00a8ur , and Xiaodong He . 2012 .", "entities": []}, {"text": "Towards deeper understanding : Deep convex networks for semantic utterance classi\ufb01cation .", "entities": []}, {"text": "In ICASSP , IEEE International Conference on Acoustics , Speech and Signal Processing Proceedings , pages 5045\u20135048 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Jason D Williams , Eslam Kamal , Mokhtar Ashour , Hani Amr , Jessica Miller , and Geoff Zweig .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Fast and easy language understanding for dialog systems with Microsoft Language Understanding Intelligent Service ( LUIS ) .", "entities": []}, {"text": "In 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue . 2015 .", "entities": []}, {"text": ", pages 159\u2013161 .", "entities": []}, {"text": "ACL \u2013 Association for Computational Linguistics , sep.251", "entities": []}]