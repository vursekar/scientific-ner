[{"text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing , pages 3252\u20133262 August 1\u20136 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics3252Joint Models for Answer Veri\ufb01cation in Question Answering Systems", "entities": [[11, 13, "TaskName", "Question Answering"]]}, {"text": "Zeyu", "entities": []}, {"text": "Zhang\u0003Thuy", "entities": []}, {"text": "Vu Alessandro Moschitti School of Information , The University of Arizona , Tucson , AZ , USA Amazon Alexa AI , Manhattan Beach , CA , USA zeyuzhang@email.arizona.edu , fthuyvu , amosch g@amazon.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "This paper studies joint models for selecting correct answer sentences among the top kprovided by answer sentence selection ( AS2 ) modules , which are core components of retrievalbased Question Answering ( QA ) systems .", "entities": [[29, 31, "TaskName", "Question Answering"]]}, {"text": "Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers .", "entities": []}, {"text": "For this purpose , we build a three - way multiclassi\ufb01er , which decides if an answer supports , refutes , or is neutral with respect to another one .", "entities": []}, {"text": "More speci\ufb01cally , our neural architecture integrates a state - of - the - art AS2 module with the multi - classi\ufb01er , and a joint layer connecting all components .", "entities": []}, {"text": "We tested our models on WikiQA , TREC - QA , and a real - world dataset .", "entities": [[5, 6, "DatasetName", "WikiQA"], [7, 8, "DatasetName", "TREC"]]}, {"text": "The results show that our models obtain the new state of the art in AS2 .", "entities": []}, {"text": "1 Introduction Automated Question Answering ( QA ) research has received a renewed attention thanks to the diffusion of Virtual Assistants .", "entities": [[3, 5, "TaskName", "Question Answering"]]}, {"text": "Among the different types of methods to implement QA systems , we focus on Answer Sentence Selection ( AS2 ) research , originated from TREC - QA track ( V oorhees and Tice , 1999 ) , as it proposes ef\ufb01cient models that are more suitable for a production setting , e.g. , they are more ef\ufb01cient than those developed in machine reading ( MR ) work ( Chen et al . , 2017 ) .", "entities": [[24, 25, "DatasetName", "TREC"], [64, 65, "DatasetName", "MR"]]}, {"text": "Garg et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) proposed the T AND A approach based on pre - trained Transformer models , obtaining impressive improvement over the state of the art for AS2 , measured on the two most used datasets , WikiQA ( Yang et al . , 2015 ) and TRECQA ( Wang et al . , 2007 ) .", "entities": [[14, 15, "MethodName", "Transformer"], [37, 38, "DatasetName", "WikiQA"], [47, 48, "DatasetName", "TRECQA"]]}, {"text": "However , T AND A was applied only to pointwise rerankers ( PR ) , e.g. , simple binary classi\ufb01ers .", "entities": []}, {"text": "Bonadiman and Moschitti \u0003Work done while the author was an intern at Amazon AlexaClaim : Joe Walsh was inducted in 2001 .", "entities": []}, {"text": "Ev1 : As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the V ocal Group Hall of Fame in 2001 .", "entities": []}, {"text": "Ev2 : Joseph Fidler Walsh ( born November 20 , 1947 ) is an American singer songwriter , composer , multiinstrumentalist and record producer .", "entities": []}, {"text": "Ev3 : Walsh was awarded with the Vocal Group Hall of Fame in 2001 .", "entities": []}, {"text": "Table 1 : A claim veri\ufb01cation example from FEVER .", "entities": [[8, 9, "DatasetName", "FEVER"]]}, {"text": "( 2020 ) tried to improve this model by jointly modeling all answer candidates with listwise methods , e.g. , ( Bian et al . , 2017 ) .", "entities": []}, {"text": "Unfortunately , merging the embeddings from all candidates with standard approaches , e.g. , CNN or LSTM , did not improve over TANDA .", "entities": [[16, 17, "MethodName", "LSTM"]]}, {"text": "A more structured approach to building joint models over sentences can instead be observed in Fact Veri\ufb01cation Systems , e.g. , the methods developed in the FEVER challenge ( Thorne et al . , 2018a ) .", "entities": [[26, 27, "DatasetName", "FEVER"]]}, {"text": "Such systems take a claim , e.g. , Joe Walsh was inducted in 2001 , as input ( see Tab . 1 ) , and verify if it is valid , using related sentences called evidences ( typically retrieved by a search engine ) .", "entities": []}, {"text": "For example , Ev1,As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the Vocal Group Hall of Fame in 2001 , andEv3,Walsh was awarded with the Vocal Group Hall of Fame in 2001 , support the veracity of the claim .", "entities": []}, {"text": "In contrast , Ev2is neutral as it describes who Joe Walsh is but does not contribute to establish the induction .", "entities": []}, {"text": "We conjecture that supporting evidence for answer correctness in AS2 task can be modeled with a similar rationale .", "entities": []}, {"text": "In this paper , we design joint models for AS2 based on the assumption that , given qand a target answer candidate t , the other answer candidates , ( c1;::ck ) can provide positive , negative , or neutral support to decide the correctness of t. Our \ufb01rst approach exploits Fact Checking research : we adapted a state - of - the - art FEVER system , KGAT ( Liu et al . , 2020 ) , for AS2 .", "entities": [[51, 53, "TaskName", "Fact Checking"], [65, 66, "DatasetName", "FEVER"]]}, {"text": "We de\ufb01ned a claim as", "entities": []}, {"text": "3253a pair constituted of the question and one target answer , while considering all the other answers as evidences .", "entities": []}, {"text": "We re - trained and rebuilt all its embeddings for the AS2 task .", "entities": []}, {"text": "Our second method , Answer Support - based Reranker ( ASR ) , is completely new , it is based on the representation of the pair , ( q , t ) , generated by state - of - the - art AS2 models , concatenated with the representation of all the pairs ( t;ci ) .", "entities": []}, {"text": "The latter summarizes the contribution of each citotusing a maxpooling operation .", "entities": []}, {"text": "cican be unrelated to ( q;t ) since the candidates are automatically retrieved , thus it may introduce just noise .", "entities": []}, {"text": "To mitigate this problem , we use an Answer Support Classi\ufb01er ( ASC ) to learn the relatedness between tandciby classifying their embedding , which we obtain by applying a transformer network to their concatenated text .", "entities": []}, {"text": "ASC tunes the ( t;ci ) embedding parameters according to the evidence that ciprovides tot .", "entities": []}, {"text": "Our Answer Support - based Reranker ( ASR ) signi\ufb01cantly improves the state of the art , and is also simpler than our approach based on KGAT .", "entities": []}, {"text": "Our third method is an extension of ASR .", "entities": []}, {"text": "It should be noted that , although ASR exploits the information from the kcandidates , it still produces a score for a target twithout knowing the scores produced for the other target answers .", "entities": []}, {"text": "Thus , we jointly model the representation obtained for each target in a multi - ASR ( MASR ) architecture , which can then carry out a complete global reasoning over all target answers .", "entities": []}, {"text": "We experimented with our models over three datasets , WikiQA , TREC - QA and WQA , where the latter is an internal dataset built on anonymized customer questions .", "entities": [[9, 10, "DatasetName", "WikiQA"], [11, 12, "DatasetName", "TREC"]]}, {"text": "The results show that : \u2022ASR improves the best current model for AS2 , i.e. , T AND A by\u00183 % , corresponding to an error reduction of 10 % in Accuracy , on both WikiQA and TREC - QA .", "entities": [[31, 32, "MetricName", "Accuracy"], [35, 36, "DatasetName", "WikiQA"], [37, 38, "DatasetName", "TREC"]]}, {"text": "\u2022We also obtain a relative improvement of \u00183 % over T AND A on WQA , con\ufb01rming that ASR is a general solution to design accurate QA systems .", "entities": []}, {"text": "\u2022Most interestingly , MASR improves ASR by additional 2 % , con\ufb01rming the bene\ufb01t of joint modeling .", "entities": []}, {"text": "Finally , it is interesting to mention that MASR improvement is also due to the use of FEVER data forpre-\ufb01ne - tuning ASC , suggesting that the fact veri\ufb01cation inference and the answer support inference are similar .", "entities": [[17, 18, "DatasetName", "FEVER"]]}, {"text": "2 Problem de\ufb01nition and related work We consider retrieval - based QA systems , which are mainly constituted by ( i ) a search engine , retrieving documents related to the questions ; and ( ii ) an AS2 model , which reranks passages / sentences extracted from the documents .", "entities": []}, {"text": "The top sentence is typically used as \ufb01nal answer for the users .", "entities": []}, {"text": "2.1 Answer Sentence Selection ( AS2 )", "entities": []}, {"text": "The task of reranking answer - sentence candidates provided by a retrieval engine can be modeled with a classi\ufb01er scoring the candidates .", "entities": []}, {"text": "Let qbe an element of the question set , Q , andA = fc1;:::;c ng be a set of candidates for q , a reranker can be de\ufb01ned asR : Q\u0002\u0005(A)!\u0005(A ) , where \u0005(A)is the set of all permutations of A. Previous work targeting ranking problems in the text domain has classi\ufb01ed reranking functions into three buckets : pointwise , pairwise , and listwise methods .", "entities": []}, {"text": "Pointwise reranking : This approach learns p(q;ci ) , which is the probability of cicorrectly answeringq , using a standard binary classi\ufb01cation setting .", "entities": []}, {"text": "The \ufb01nal rank is simply obtained sorting ci , based onp(q;ci ) .", "entities": []}, {"text": "Previous work estimates p(q;ci ) with neural models ( Severyn and Moschitti , 2015 ) , also using attention mechanisms , e.g. , CompareAggregate ( Yoon et al . , 2019 ) , inter - weighted alignment networks ( Shen et al . , 2017 ) , and pre - trained Transformer models , which are the state of the art .", "entities": [[51, 52, "MethodName", "Transformer"]]}, {"text": "Garg et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) proposed T AND A , which is the current most accurate model on WikiQA and TREC - QA .", "entities": [[16, 17, "DatasetName", "WikiQA"], [18, 19, "DatasetName", "TREC"]]}, {"text": "Pairwise reranking : The method considers binary classi\ufb01ers of the form \u001f ( q;ci;cj)for determining the partial rank between ciandcj , then the scoring function p(q;ci)is obtained by summing up all the contributions with respect to the target candidatet = ci , e.g. ,p(q;ci )", "entities": []}, {"text": "= P j \u001f ( q;ci;cj ) .", "entities": []}, {"text": "There has been a large body of work preceding Transformer models , e.g. , ( Laskar et al . , 2020 ; Tayyar Madabushi et al . , 2018 ; Rao et", "entities": [[9, 10, "MethodName", "Transformer"]]}, {"text": "al . , 2016 ) .", "entities": []}, {"text": "However , these methods are largely outperformed by the pointwise TANDA model .", "entities": []}, {"text": "Listwise reranking : This approach , e.g. , ( Bian et al . , 2017 ; Cao et al . , 2007 ;", "entities": []}, {"text": "Ai et al . , 2018 ) , aims at learningp(q;\u0019);\u00192\u0005(A ) , using the information on the entire set of candidates .", "entities": []}, {"text": "The loss function for training such networks is constituted by the", "entities": [[1, 2, "MetricName", "loss"]]}, {"text": "3254contribution of all elements of its ranked items .", "entities": []}, {"text": "The closest work to our research is by Bonadiman and Moschitti ( 2020 ) , who designed several joint models .", "entities": []}, {"text": "These improved early neural networks based on CNN and LSTM for AS2 , but failed to improve the state of the art using pre - trained Transformer models .", "entities": [[9, 10, "MethodName", "LSTM"], [26, 27, "MethodName", "Transformer"]]}, {"text": "2.2 Joint Models in Question Answering MR is a popular QA task that identi\ufb01es an answer string in a paragraph or a text of limited size for a question .", "entities": [[4, 6, "TaskName", "Question Answering"], [6, 7, "DatasetName", "MR"]]}, {"text": "Its application to retrieval scenario has also been studied ( Chen et al . , 2017 ;", "entities": []}, {"text": "Hu et al . , 2019 ; Kratzwald and Feuerriegel , 2018 ) .", "entities": []}, {"text": "However , the large volume of retrieved content makes their use not practical yet .", "entities": []}, {"text": "Moreover , the joint modeling aspect of MR regards sentences from the same paragraphs .", "entities": [[7, 8, "DatasetName", "MR"]]}, {"text": "Jin et al .", "entities": []}, {"text": "( 2020 ) use the relation between candidates in Multi - task learning approach for AS2 .", "entities": [[9, 13, "TaskName", "Multi - task learning"]]}, {"text": "However , they do not exploit transformer models , thus their results are rather below the state of the art .", "entities": []}, {"text": "In contrast with the work above , our modeling is driven by an answer support strategy , where the pieces of information are taken from different documents .", "entities": []}, {"text": "This makes our model even more unique ; it allows us to design innovative joint models , which are still not designed in any MR systems .", "entities": [[24, 25, "DatasetName", "MR"]]}, {"text": "2.3 Fact Veri\ufb01cation for Question Answering Fact veri\ufb01cation has become a social need given the massive amount of information generated daily .", "entities": [[4, 6, "TaskName", "Question Answering"]]}, {"text": "The problem is , therefore , becoming increasingly important in NLP context ( Mihaylova et al . , 2018 ) .", "entities": []}, {"text": "In QA , answer veri\ufb01cation is directly relevant due to its nature of content delivery ( Mihaylova et al . , 2019 ) .", "entities": []}, {"text": "The problem has been explored in MR setting ( Wang et al . , 2018 ) .", "entities": [[6, 7, "DatasetName", "MR"]]}, {"text": "Zhang et al .", "entities": []}, {"text": "( 2020a ) also proposed to fact check for product questions using additional associated evidence sentences .", "entities": []}, {"text": "The latter are retrieved based on similarity scores computed with both TF - IDF and sentence - embeddings from pre - trained BERT models .", "entities": [[22, 23, "MethodName", "BERT"]]}, {"text": "While the process is technically sound , the retrieval of evidence is an expensive process , which is prohibitive to scale in production .", "entities": []}, {"text": "We instead address this problem by leveraging the top answer candidates .", "entities": []}, {"text": "3 Baseline Models for AS2 In this section , we describe our baseline models , which are constituted by pointwise , pairwise , andlistwise strategies .", "entities": []}, {"text": "3.1 Pointwise Models One simple and effective method to build an answer selector is to use a pre - trained Transformer model , adding a simple classi\ufb01cation layer to it , and \ufb01ne - tuning the model on the AS2 task .", "entities": [[20, 21, "MethodName", "Transformer"]]}, {"text": "Specifically , q = Tokq 1, ... ,Tokq Nandc = Tokc 1, ... ,Tokc M are encoded in the input of the Transformer by delimiting them using three tags :", "entities": [[26, 27, "MethodName", "Transformer"]]}, {"text": "[ CLS ] , [ SEP ] and [ EOS ] , inserted at the beginning , as separator , and at the end , respectively .", "entities": []}, {"text": "This input is encoded as three embeddings based on tokens , segments and their positions , which are fed as input to several layers ( up to 24 ) .", "entities": []}, {"text": "Each of them contains sublayers for multi - head attention , normalization and feed forward processing .", "entities": [[6, 10, "MethodName", "multi - head attention"]]}, {"text": "The result of this transformation is an embedding , E , representing ( q;c ) , which models the dependencies between words and segments of the two sentences .", "entities": []}, {"text": "For the downstream task , Eis fed ( after applying a non - linearity function ) to a fully connected layer having weights : WandB. The output layer can be used to implement the task function .", "entities": []}, {"text": "For example , a softmax can be used to model the probability of the question / candidate pair classi\ufb01cation , as : p(q;c ) = softmax ( W\u0002tanh(E(q;c ) )", "entities": [[4, 5, "MethodName", "softmax"], [25, 26, "MethodName", "softmax"]]}, {"text": "+ B ) .", "entities": []}, {"text": "We can train this model with log cross - entropy loss : L=\u0000P l2f0;1gyl\u0002log(^yl)on pairs of texts , whereylis the correct and incorrect answer label,^y1 = p(q;c ) , and ^y0= 1\u0000p(q;c ) .", "entities": [[10, 11, "MetricName", "loss"]]}, {"text": "Training the Transformer from scratch requires a large amount of labeled data , but it can be pre - trained using a masked language model , and the next sentence prediction tasks , for which labels can be automatically generated .", "entities": [[2, 3, "MethodName", "Transformer"]]}, {"text": "Several methods for pretraining Transformer - based language models have been proposed , e.g. , BERT ( Devlin et al . , 2018 ) , RoBERTa ( Liu et al . , 2019 ) , XLNet ( Yang et al . , 2019 ) , AlBERT ( Lan et al . , 2020 ) .", "entities": [[4, 5, "MethodName", "Transformer"], [15, 16, "MethodName", "BERT"], [25, 26, "MethodName", "RoBERTa"], [35, 36, "MethodName", "XLNet"]]}, {"text": "3.2 Our joint model baselines To better show the potential of our approach and the complexity of the task , we designed three joint model baselines based on : ( i ) a multiclassi\ufb01er approach ( a listwise method ) , and ( ii ) a pairwise joint model operating over k+ 1 candidates , and our adaptation of KGAT model ( a pairwise method ) .", "entities": []}, {"text": "Joint Model Multi", "entities": []}, {"text": "- classi\ufb01er", "entities": []}, {"text": "The \ufb01rst baseline is also a Transformer - based architecture : we concatenate the question with the top k+1answer can-", "entities": [[6, 7, "MethodName", "Transformer"]]}, {"text": "3255didates , i.e. , ( q[SEP ] c1[SEP ] c2:::[SEP ] ck+1 ) , and provide this input to the same Transformer model used for pointwise reranking .", "entities": [[21, 22, "MethodName", "Transformer"]]}, {"text": "We use the \ufb01nal hidden vector Ecorresponding to the \ufb01rst input token", "entities": []}, {"text": "[ CLS ] generated by the Transformer , and a classi\ufb01cation layer with weights W2R(k+1)\u0002jEj , and train the model using a standard cross - entropy classi\ufb01cation loss : y\u0002log(softmax ( EWT ) ) , whereyis a one - hot vector representing labels for thek+ 1candidates , i.e. ,jyj = k+ 1 .", "entities": [[6, 7, "MethodName", "Transformer"], [27, 28, "MetricName", "loss"]]}, {"text": "We use a transformer model \ufb01ne - tuned with the TANDARoBERTa - base or large models , i.e. , RoBERTa models \ufb01ne - tuned on ASNQ ( Garg et al . , 2020 ) .", "entities": [[19, 20, "MethodName", "RoBERTa"], [25, 26, "DatasetName", "ASNQ"]]}, {"text": "The scores for the candidate answers are calculated as\u0000 p(c1);::;p(ck+1)\u0001 = softmax ( EWT ) .", "entities": [[11, 12, "MethodName", "softmax"]]}, {"text": "Then , we rerankciaccording their probability .", "entities": []}, {"text": "Joint Model Pairwise Our second baseline is similar to the \ufb01rst .", "entities": []}, {"text": "We concatenate the question with eachcito constitute the ( q;ci)pairs , which are input to the Transformer , and we use the \ufb01rst input token", "entities": [[16, 17, "MethodName", "Transformer"]]}, {"text": "[ CLS ] as the representation of each ( q;ci)pair .", "entities": []}, {"text": "Then , we concatenate the embedding of the pair containing the target candidate , ( q;t)with the embedding of all the other candidates \u2019", "entities": []}, {"text": "[ CLS ] .", "entities": []}, {"text": "( q;t)is always in the \ufb01rst position .", "entities": []}, {"text": "We train the model using a standard classi\ufb01cation loss .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "At classi\ufb01cation time , we select one target candidate at a time , and set it in the \ufb01rst position , followed by all the others .", "entities": []}, {"text": "We classify all k+ 1candidates and use their score for reranking them .", "entities": []}, {"text": "It should be noted that to qualify for a pairwise approach , Joint Model Pairwise should use a ranking loss .", "entities": [[19, 20, "MetricName", "loss"]]}, {"text": "However , we always use standard cross - entropy loss as it is more ef\ufb01cient and the different is performance is negligible .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "Joint Model with KGAT Liu et al .", "entities": []}, {"text": "( 2020 ) presented an interesting model , Kernel Graph Attention Network ( KGAT ) , for fact veri\ufb01cation : given a claimed fact f , and a set of evidencesEv= fev1;ev 2;:::;ev mg , their model carries out joint reasoning over Ev , e.g. , aggregating information to estimate the probability of fto be true or false , p(yjf;Ev ) , wherey2ftrue , falseg .", "entities": [[9, 12, "MethodName", "Graph Attention Network"]]}, {"text": "The approach is based on a fully connected graph , G , whose nodes are the ni= ( f;ev i)pairs , andp(yjf;Ev )", "entities": []}, {"text": "= p(yjf;ev i;Ev)p(evijf;Ev ) , wherep(yjf;ev i;Ev )", "entities": []}, {"text": "= p(yjni;G)is the label probability in each node iconditioned on the whole graph , andp(evijf;Ev )", "entities": []}, {"text": "= p(nijG)is the probability of selecting the most informative evidence .", "entities": []}, {"text": "KGAT uses an edge kernel to perform a hierarchi - cal attention mechanism , which propagates information between nodes and aggregate evidences .", "entities": []}, {"text": "We built a KGAT model for AS2 as follows : we replace ( i)eviwith the set of candidate answers ci , and ( ii ) the claim fwith the question and a target answer pair , ( q;t ) .", "entities": []}, {"text": "KGAT constructs the evidence graph Gby using each claim - evidence pair as a node , which , in our case , is ( ( q;t);ci ) , and connects all node pairs with edges , making it a fully - connected evidence graph .", "entities": []}, {"text": "This way , sentence and token attention operate over the triplets , ( q;t;c i ) , establishing semantic links , which can help to support or undermine the correctness of t.", "entities": []}, {"text": "The original KGAT aggregates all the pieces of information we built , based on their relevance , to determine the probability of t. As we use AS2 data , the probability will be about the correctness oft .", "entities": []}, {"text": "More in detail , we initialize the node representation using the contextual embeddings obtained with two TANDA - RoBERTa - base models1 : the \ufb01rst produces the embedding of ( q;t ) , while the second outputs the embedding of ( q;ci ) .", "entities": [[18, 19, "MethodName", "RoBERTa"]]}, {"text": "Then , we apply a max - pooling operation on these two to get the \ufb01nal node representation .", "entities": []}, {"text": "The rest of the architecture is identical to the original KGAT .", "entities": []}, {"text": "Finally , at test time , we select one ciat a time , as the target t , and compute its probability , which ranks ci .", "entities": []}, {"text": "4 Joint Answer Support Models for AS2 We proposed the Answer Support Reranker ( ASR ) , which uses an answer pair classi\ufb01er to provide evidence to a target answer t. Given a question q , and a subset of its top- k+1 ranked answer candidates , A(reranked by an AS2 model ) , we build a function , \u001b:Q\u0002C\u0002Ck!Rsuch that\u001b(q;t;Anftg ) provides the probability of tto be correct , where C is the set of sentence - candidates .", "entities": []}, {"text": "We also design a multi - classi\ufb01er MASR , which combines kASR models , one for each different target answer .", "entities": []}, {"text": "4.1 Answer Support - based Reranker ( ASR ) We developed ASR architecture described in Figure 1c .", "entities": []}, {"text": "This consists of three main components : 1.aPointwise Reranker ( PR ) , which provides the embedding of the input ( q;t ) , described in Figure 1a .", "entities": []}, {"text": "This is essentially the state - of - the - art AS2 model based on the T AND A approach applied to RoBERTa pre - trained transformer .", "entities": [[22, 23, "MethodName", "RoBERTa"]]}, {"text": "1https://github.com/alexa/wqa tanda", "entities": []}, {"text": "3256 ( a ) Baseline Reranker using Transformers .", "entities": []}, {"text": "( b ) PairWise Representation using Transformers .", "entities": []}, {"text": "( c ) Answer Support Reranker ( d ) Multi Answer Support Reranker Figure 1 : Multi - Answer Support Reranker and its building blocks .", "entities": []}, {"text": "2.To reduce the noise that may be introduced by irrelevantci , we use the Answer Support Classi\ufb01er(ASC ) , which classi\ufb01es each ( t;ci)in one of the following four classes : 0 : tandciare both correct , 1 : tis correct while ciis not , 2 : vice versa , and 3 : both incorrect .", "entities": [[31, 32, "DatasetName", "0"]]}, {"text": "This multi - classi\ufb01er , described in Figure 1b , is built on top a RoBERTa Transformer , which produced a PairWise Representation ( PWR ) .", "entities": [[15, 16, "MethodName", "RoBERTa"], [16, 17, "MethodName", "Transformer"]]}, {"text": "ASC is trained end - to - end with the rest of the network in a multi - task learning fashion , using its speci\ufb01c cross - entropy loss , computed with the labels above .", "entities": [[16, 20, "TaskName", "multi - task learning"], [28, 29, "MetricName", "loss"]]}, {"text": "3.The ASR ( see Figure 1c ) uses the joint representation of ( q;t)with(t;ci),i= 1;::;k , wheretandciare the top - candidates reranked by PR .", "entities": []}, {"text": "Thekrepresentations are summarized by applying a max - pooling operation , which will aggregate all the supporting or not supporting properties of the candidates with respect to the target answer .", "entities": []}, {"text": "The concatenation of the PR embedding with the max - pooling embedding is given as input to the \ufb01nal classi\ufb01cation layer , which scorestwith respect to q , also using the information from the other candidates .", "entities": []}, {"text": "For training and testing , we select a tfrom thek+", "entities": []}, {"text": "1candidates ofqat a time , and compute its score .", "entities": []}, {"text": "This way , we can rerank all the k+ 1candidates with their scores .", "entities": []}, {"text": "Implementation details : ASR is a PR that also exploits the relation between tandAnftg .", "entities": []}, {"text": "We use RoBERTa to generate the [ CLS ] 2Rdembedding of(q;t ) = Et .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}, {"text": "We denote with ^Ejthe[CLS ] output by another RoBERTa Transformer applied to answer pairs , i.e. , ( t;cj ) .", "entities": [[8, 9, "MethodName", "RoBERTa"], [9, 10, "MethodName", "Transformer"]]}, {"text": "Then , we concatenate Etto the max - pooling tensor from ^E1;::;^Ek : V=", "entities": []}, {"text": "[ Et : Maxpool ( [ ^E1;::;^Ek ] ) ] ; ( 1 ) whereV2R2dis the \ufb01nal representation of the target answer t.", "entities": []}, {"text": "Then , we use a standard feedforward network to implement a binary classi\ufb01cation layer : p(yijq;t;C k ) = softmax ( VWT+B ) , whereW2R2\u00022dandBare parameters to transform the representation of the target answer tfrom dimension 2dto dimension 2 , which represents correct or incorrect labels .", "entities": [[6, 8, "MethodName", "feedforward network"], [19, 20, "MethodName", "softmax"]]}, {"text": "ASC labels There can be different interpretations when attempting to de\ufb01ne labels for answer pairs .", "entities": []}, {"text": "An alternative to the de\ufb01nition illustrated above is to use the following FEVER compatible encoding : 0 : tis correct , while cican be any value , as also an incorrect cimay provide important context ( corresponding to FEVER Support label ) ; 1 : tis incorrect , cicorrect , since cican provide evidence that tis not similar to a correct answer ( corresponding to FEVER Refutal label ) ; and 2 : both are incorrect , in this case , nothing can be told ( corresponding to FEVER Neutral label ) .", "entities": [[12, 13, "DatasetName", "FEVER"], [16, 17, "DatasetName", "0"], [38, 39, "DatasetName", "FEVER"], [65, 66, "DatasetName", "FEVER"], [88, 89, "DatasetName", "FEVER"]]}, {"text": "4.2 Multi - Answer Support Reranker ( MASR ) ASR still selects answers with a pointwise approach2 .", "entities": []}, {"text": "This means that we can improve it by 2Again , using ranking loss did not provide a signi\ufb01cant improvment .", "entities": [[12, 13, "MetricName", "loss"]]}, {"text": "3257DatasetTrain Dev Test # Q # A+ # A- # Q # A+ # A- # Q # A+ # AWikiQA 873 1,040 7,632 121 140 990 237 293 2,058 TREC - QA 1,229 6,403 47,014 65 205 912 68 248 1,194 WQA 5,000 42,962 163,289 905 8,179 28,096 1,000 8,256 30,123 Table 2 : AS2 dataset statistics building a listwise model , to select the best answer for each question , by utilizing the information from all target answers .", "entities": [[30, 31, "DatasetName", "TREC"]]}, {"text": "In particular , the architecture of MASR shown in Figure 1d is made up of two parts : ( i ) a list of ASR containing k+ 1ASR blocks , in which each ASR block provides the representation of a target answer t. ( ii ) A \ufb01nal multiclassi\ufb01er and a softmax function , which scores each tfromk+ 1 embedding concatenation and selects the one with highest score .", "entities": [[51, 52, "MethodName", "softmax"]]}, {"text": "For training and testing , we select thetfrom thek+ 1 candidates of qbased on a softmax output at a time .", "entities": [[15, 16, "MethodName", "softmax"]]}, {"text": "Implementation details : The goal of MASR is to measure the relation between k+ 1target answers , t0 , .. , tk .", "entities": []}, {"text": "The representation of each target answer is the embedding V2R2dfrom", "entities": []}, {"text": "Equation 1 in ASR .", "entities": []}, {"text": "Then , we concatenate the hidden vectors ofk+ 1target answers to form a matrix V(q;k+1)2R(k+1)\u00022d .", "entities": []}, {"text": "We use this matrix and a classi\ufb01cation layer weights W2R2d , and compute a standard multi - class classi\ufb01cation loss : LMASR = y\u0003log(softmax ( V(q;k+1)WT);(2 ) whereyis a one - hot - vector , and jyj = jk+ 1j .", "entities": [[19, 20, "MetricName", "loss"]]}, {"text": "5 Experiments In these experiments , we compare our models : KGAT , ASR and MASR with pointwise models , which are the state of the art for AS2 .", "entities": []}, {"text": "We also compare them with our joint model baselines ( pairwise and listwise ) .", "entities": []}, {"text": "Finally , we provide an error analysis .", "entities": []}, {"text": "5.1 Datasets We used two most popular AS2 datasets , and one real world application dataset we built to test the generality of our approach .", "entities": []}, {"text": "WikiQA is a QA dataset ( Yang et al . , 2015 ) containing a sample of questions and answer - sentence candidates from Bing query logs over Wikipedia .", "entities": [[0, 1, "DatasetName", "WikiQA"]]}, {"text": "The answers are manually labeled .", "entities": []}, {"text": "We follow the most used setting : training with all the questions that have at least one correct answer , and validating and testing with all the questions having at least one correct and one incorrect answer .", "entities": []}, {"text": "Data split Supported Refuted Not Enough Info Train 80,035 29,775 35,639 Dev 6,666 6,666 6,666 Test 6,666 6,666 6,666 Table 3 : FEVER dataset statistics", "entities": [[22, 23, "DatasetName", "FEVER"]]}, {"text": "TREC - QA is another popular QA benchmark by Wang et al .", "entities": [[0, 1, "DatasetName", "TREC"]]}, {"text": "( 2007 ) .", "entities": []}, {"text": "We use the same splits of the original data , following the common setting of previous work , e.g. , ( Garg et al . , 2020 ) .", "entities": []}, {"text": "WQA The Web - based Question Answering is a dataset built by Alexa AI as part of the effort to improve understanding and benchmarking in QA systems .", "entities": [[5, 7, "TaskName", "Question Answering"]]}, {"text": "The creation process includes the following steps : ( i ) given a set of questions we collected from the web , a search engine is used to retrieve up to 1,000 web pages from an index containing hundreds of millions pages .", "entities": []}, {"text": "( ii ) From the set of retrieved documents , all candidate sentences are extracted and ranked using AS2 models from ( Garg et al . , 2020 ) .", "entities": []}, {"text": "Finally , ( iii ) top candidates for each question are manually assessed as correct or incorrect by human judges .", "entities": []}, {"text": "This allowed us to obtain a richer variety of answers from multiple sources with a higher average number of answers .", "entities": []}, {"text": "Table 2 reports the corpus statistics of WikiQA , TREC - QA , and WQA3 .", "entities": [[7, 8, "DatasetName", "WikiQA"], [9, 10, "DatasetName", "TREC"]]}, {"text": "FEVER is a large - scale public corpus , proposed by Thorne et", "entities": [[0, 1, "DatasetName", "FEVER"]]}, {"text": "al . ( 2018a ) for fact veri\ufb01cation task , consisting of 185,455 annotated claims from 5,416,537 documents from the Wikipedia dump in June 2017 .", "entities": []}, {"text": "All claims are labelled as Supported , Refuted or Not Enough Info by annotators .", "entities": []}, {"text": "Table 3 shows the statistics of the dataset , which remains the same as in ( Thorne et al . , 2018b ) .", "entities": []}, {"text": "5.2 Training and testing details Metrics The performance of QA systems is typically measured with Accuracy in providing correct answers , i.e. , the percentage of correct responses .", "entities": [[15, 16, "MetricName", "Accuracy"]]}, {"text": "This is also referred to Precision - at-1 ( P@1 ) in the context of reranking , while standard Precision and Recall are not essential in our case as we assume the system does not abstain from providing answers .", "entities": [[5, 6, "MetricName", "Precision"], [9, 10, "MetricName", "P@1"], [19, 20, "MetricName", "Precision"], [21, 22, "MetricName", "Recall"]]}, {"text": "We also use Mean Average Precision ( MAP ) and Mean Reciprocal Recall ( MRR ) evaluated on the test set , using the entire set of candidates for each 3The public version of WQA will be released in the short - term future .", "entities": [[4, 6, "MetricName", "Average Precision"], [7, 8, "DatasetName", "MAP"], [12, 13, "MetricName", "Recall"], [14, 15, "MetricName", "MRR"]]}, {"text": "Please search for a publication with titleWQA : A Dataset for Web - based Question Answering Tasks on arXiv.org .", "entities": [[14, 16, "TaskName", "Question Answering"]]}, {"text": "3258RoBERTa Base WikiQA TREC - QA WQA P@1 MAP MRR P@1 MAP MRR P@1 MAP MRR Reranker by Garg et", "entities": [[2, 3, "DatasetName", "WikiQA"], [3, 4, "DatasetName", "TREC"], [7, 8, "MetricName", "P@1"], [8, 9, "DatasetName", "MAP"], [9, 10, "MetricName", "MRR"], [10, 11, "MetricName", "P@1"], [11, 12, "DatasetName", "MAP"], [12, 13, "MetricName", "MRR"], [13, 14, "MetricName", "P@1"], [14, 15, "DatasetName", "MAP"], [15, 16, "MetricName", "MRR"]]}, {"text": "al . , 2020 \u2013 0.8890 0.9010 \u2013 0.9140 0.9520 \u2013 \u2013 \u2013 Our Reranker 0.8189y0.8860 0.8983 0.9118 0.9043 0.9498 \u2013 \u2013 \u2013 Joint Model Multi - classi\ufb01er ( k=5 ) 0.7819y0.8542 0.8684 0.8971 0.9052 0.9424 -2.29y% -1.00 % -1.23 % Joint Model Pairwise ( k=3 ) 0.8272y0.8927 0.9045 0.9559 0.9196 0.9743 2.67y% 0.39 % 1.39 % KGAT ( k=2 ) 0.8436 0.8991 0.9120 0.9412 0.9155 0.9645 2.10 % 0.39 % 0.93 % ASR ( k=3)y0.8436 0.9014 0.9123 0.9706 0.9257 0.9816y2.86 % 0.86 % 1.39 % MASR ( k=3 )", "entities": []}, {"text": "0.8230 0.8891", "entities": []}, {"text": "0.9017 0.9265 0.9200 0.9632 3.82 % 0.70 % 1.67 % MASR - F ( k=3 ) 0.8272 0.8918 0.9031 0.9412 0.9222 0.9706 2.67 % 0.55 % 1.47 % MASR - FP ( k=3 ) 0.8436 0.8998 0.9113 0.9559 0.9191 0.9743 4.96 % 0.94 % 2.43 % Table 4 : Results on WikiQA , TREC - QA and WQA , using RoBERTa base Transformer.yis used to indicate that the difference in P@1 between ASR and the other marked systems is statistically signi\ufb01cant at 95 % .", "entities": [[51, 52, "DatasetName", "WikiQA"], [53, 54, "DatasetName", "TREC"], [60, 61, "MethodName", "RoBERTa"], [70, 71, "MetricName", "P@1"]]}, {"text": "JOINT -MULTI CLASSIFER JOINT -PAIR KGAT ASR 1 2 3 4 5\u00003\u00002\u00001012345 kImprovement ( % ) Figure 2 : Impact of kon the WQA dev .", "entities": []}, {"text": "set question ( this varies according to the dataset ) , to have a direct comparison with the state of the art .", "entities": []}, {"text": "Models We use the pre - trained RoBERTa - Base ( 12 layer ) and RoBERTa - Large - MNLI ( 24 layer ) models , which were released as checkpoints for use in downstream tasks4 .", "entities": [[7, 8, "MethodName", "RoBERTa"], [15, 16, "MethodName", "RoBERTa"], [19, 20, "DatasetName", "MNLI"]]}, {"text": "Reranker training We adopt Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 2e-5 for the transfer step on the ASNQ dataset ( Garg et al . , 2020 ) , and a learning rate of 1e-6 for the adapt step on the target dataset .", "entities": [[4, 5, "MethodName", "Adam"], [5, 6, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"], [25, 26, "DatasetName", "ASNQ"], [38, 40, "HyperparameterName", "learning rate"]]}, {"text": "We apply early stopping on the development set of the target corpus for both \ufb01ne - tuning steps based on the highest MAP score .", "entities": [[2, 4, "MethodName", "early stopping"], [22, 23, "DatasetName", "MAP"]]}, {"text": "We set the max number of epochs equal to 3 and 9 for the adapt and transfer steps , respectively .", "entities": [[4, 7, "HyperparameterName", "number of epochs"]]}, {"text": "We set the maximum sequence length for RoBERTa to 128 tokens .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}, {"text": "KGAT and ASR training Again , we use the Adam optimizer with a learning rate of 2e-6 for training the ASR model on the target dataset .", "entities": [[9, 10, "MethodName", "Adam"], [10, 11, "HyperparameterName", "optimizer"], [13, 15, "HyperparameterName", "learning rate"]]}, {"text": "We utilize 1 Tesla V100 GPU with 32 GB memory and a train batch size of eight .", "entities": [[13, 15, "HyperparameterName", "batch size"]]}, {"text": "We set the maximum sequence length for RoBERTa Base / Large to 130 tokens and the number of training epochs to 20 .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}, {"text": "The other training con\ufb01gurations are the same of the original KGAT model from ( Liu et al . , 2020 ) .", "entities": []}, {"text": "We use two transformer models for ASR : a RoBERTa 4https://github.com/pytorch/fairseqBase/Large for PR , and one for ASC .", "entities": [[9, 10, "MethodName", "RoBERTa"]]}, {"text": "We set the maximum sequence length for RoBERTa to 128 tokens and the number of epochs to 20 .", "entities": [[7, 8, "MethodName", "RoBERTa"], [13, 16, "HyperparameterName", "number of epochs"]]}, {"text": "MASR training We use the same con\ufb01guration of the ASR training , including the optimizer type , learning rate , the number of epochs , GPU type , maximum sequence length , etc .", "entities": [[14, 15, "HyperparameterName", "optimizer"], [17, 19, "HyperparameterName", "learning rate"], [21, 24, "HyperparameterName", "number of epochs"]]}, {"text": "Additionally , we design two different models MASR - F , using an ASC classi\ufb01er targeting the FEVER labels , and MASR - FP , which initializes ASC with the data from FEVER .", "entities": [[17, 18, "DatasetName", "FEVER"], [32, 33, "DatasetName", "FEVER"]]}, {"text": "This is possible as the labels are compatible .", "entities": []}, {"text": "5.3 Choosing the best k", "entities": []}, {"text": "The selection of the hyper - parameter k , i.e. , the number of candidates to consider for supporting a target answer is rather tricky .", "entities": []}, {"text": "Indeed , the standard validation set is typically used for tuning PR .", "entities": []}, {"text": "This means that the candidates PR moves to the top k+1 positions are optimistically accurate .", "entities": []}, {"text": "Thus , when selecting also the optimal kon the same validation set , there is high risk to over\ufb01t the model .", "entities": []}, {"text": "We solved this problem by running a PR version not heavily optimized on the dev .", "entities": []}, {"text": "set , i.e. , we randomly choose a checkpoint after the standard three epochs of \ufb01ne - tuning of RoBERTa transformer .", "entities": [[19, 20, "MethodName", "RoBERTa"]]}, {"text": "Additionally , we tuned konly using the WQA dev .", "entities": []}, {"text": "set , which contains\u001836;000Q / A pairs .", "entities": []}, {"text": "WikiQA and TREC - QA dev .", "entities": [[0, 1, "DatasetName", "WikiQA"], [2, 3, "DatasetName", "TREC"]]}, {"text": "sets are too small to be used ( 121 and 65 questions , respectively ) .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "2 plots the improvement of four different models , Joint Model Multi - classi\ufb01er , Joint Model Pairwise , KGAT , and ASR , when using different kvalues .", "entities": []}, {"text": "Their best results are reached for 5 , 3 , 2 , and 3 , respectively .", "entities": []}, {"text": "We note that the most reliable curve shape ( convex ) is the one of ASR and Joint Model Pairwise .", "entities": []}, {"text": "32595.4 Comparative Results Table 4 reports the P@1 , MAP and MRR of the rerankers , and different answer supporting models on WikiQA , TREC - QA and WQA datasets .", "entities": [[7, 8, "MetricName", "P@1"], [9, 10, "DatasetName", "MAP"], [11, 12, "MetricName", "MRR"], [22, 23, "DatasetName", "WikiQA"], [24, 25, "DatasetName", "TREC"]]}, {"text": "As WQA is an internal dataset , we only report the improvement over PR in the tables .", "entities": []}, {"text": "All models use RoBERTa - Base pre - trained checkpoint and start from the same set of kcandidates reranked by PR ( state - of - the - art model ) .", "entities": [[3, 4, "MethodName", "RoBERTa"]]}, {"text": "The table shows that : \u2022PR replicates the MAP and MRR of the stateof - the - art reranker by Garg et al .", "entities": [[8, 9, "DatasetName", "MAP"], [10, 11, "MetricName", "MRR"]]}, {"text": "( 2020 ) on WikiQA .", "entities": [[4, 5, "DatasetName", "WikiQA"]]}, {"text": "\u2022Joint Model Multi - classi\ufb01er performs lower than PR for all measures and all datasets .", "entities": []}, {"text": "This is in line with the \ufb01ndings of Bonadiman and Moschitti ( 2020 ) , who also did not obtain improvement when jointly used all the candidates altogether in a representation .", "entities": []}, {"text": "\u2022Joint Model Pairwise differs from ASR as it concatenates the embeddings of the ( q;ci ) , instead of using max - pooling , and does not use any Answer Support Classi\ufb01er ( ASC ) .", "entities": []}, {"text": "Still , it exploits the idea of aggregating the information of all pairs ( q;ci)with respect to a target answer t , which proves to be effective , as the model improves on PR over all measures and datasets .", "entities": []}, {"text": "\u2022Our KGAT version for AS2 also improves PR over all datasets and almost all measures , con\ufb01rming that the idea of using candidates as support of the target answer is generally valid .", "entities": []}, {"text": "However , it is not superior to Joint Model Pairwise .", "entities": []}, {"text": "\u2022ASR achieves the highest performance among all models ( but MASR - FP on WQA ) , all datasets , and all measures .", "entities": []}, {"text": "For example , it outperforms PR by almost 3 absolute percent points in P@1 on WikiQA , and by almost 6 points on TREC from 91.18 % to 97.06 % , which corresponds to an error reduction of 60 % .", "entities": [[13, 14, "MetricName", "P@1"], [15, 16, "DatasetName", "WikiQA"], [23, 24, "DatasetName", "TREC"]]}, {"text": "\u2022MASR and MASR - F do not achieve better performance than Joint Model Pairwise on WikiQA and TREC , although MASR outperforms all baselines and even ASR on WQA .", "entities": [[15, 16, "DatasetName", "WikiQA"], [17, 18, "DatasetName", "TREC"]]}, {"text": "This suggests that the signi\ufb01cantly higher number of parameters of MASR can not be trained on small corpus , while WQA has a suf\ufb01cient number of examples .", "entities": [[6, 9, "HyperparameterName", "number of parameters"]]}, {"text": "RoBERTa LargeWikiQA TREC - QA P@1 MAP MRR P@1 MAP MRR Garg et al . , \u2013 0.9200 0.9330 \u2013 0.9430 0.9740 Our Reranker 0.8724 0.9151 0.9266 0.9706 0.9481 0.9816 KGAT ( K=2 )", "entities": [[0, 1, "MethodName", "RoBERTa"], [2, 3, "DatasetName", "TREC"], [5, 6, "MetricName", "P@1"], [6, 7, "DatasetName", "MAP"], [7, 8, "MetricName", "MRR"], [8, 9, "MetricName", "P@1"], [9, 10, "DatasetName", "MAP"], [10, 11, "MetricName", "MRR"]]}, {"text": "0.8642 0.9094 0.9218 0.9559 0.9407 0.9743 ASR ( K=3 ) 0.8971 0.9280 0.9399 0.9706 0.9488 0.9816 Table 5 : Results on WikiQA and TREC - QA , using RoBERTa Large Transformer .", "entities": [[21, 22, "DatasetName", "WikiQA"], [23, 24, "DatasetName", "TREC"], [28, 29, "MethodName", "RoBERTa"], [30, 31, "MethodName", "Transformer"]]}, {"text": "\u2022MASR - FP exploiting FEVER for the initialization of ASC performs better than MASR and MASR - F on WikiQA and TREC .", "entities": [[4, 5, "DatasetName", "FEVER"], [19, 20, "DatasetName", "WikiQA"], [21, 22, "DatasetName", "TREC"]]}, {"text": "Interestingly , it signi\ufb01cantly outperforms ASR by 2 % on WQA .", "entities": []}, {"text": "This con\ufb01rms the potential of the model when enough training data is available .", "entities": []}, {"text": "\u2022We perform randomization test ( Yeh , 2000 ) to verify if the models signi\ufb01cantly differ in terms of prediction outcome .", "entities": []}, {"text": "We use 100,000 trials for each calculation .", "entities": []}, {"text": "The results con\ufb01rm the statistically signi\ufb01cant difference between ASR and all the baselines , with p < 0.05 for WikiQA , and between ASR and all models ( i.e. , including also KGAT ) on WQA .", "entities": [[19, 20, "DatasetName", "WikiQA"]]}, {"text": "5.5 Of\ufb01cial State of the art As the state of the art for AS2 is obtained using RoBERTa Large , we trained KGAT and ASR using this pre - trained language model .", "entities": [[17, 18, "MethodName", "RoBERTa"]]}, {"text": "Table 5 also reports the comparison with PR , which is the of\ufb01cial state of the art .", "entities": []}, {"text": "Again , our PR replicates the results of Garg et al .", "entities": []}, {"text": "( 2020 ) , obtaining slightly lower performance on WikiQA but higher on TREC - QA .", "entities": [[9, 10, "DatasetName", "WikiQA"], [13, 14, "DatasetName", "TREC"]]}, {"text": "KGAT performs lower than PR on both datasets .", "entities": []}, {"text": "ASR establishes the new state of the art on WikiQA with an MAP of 92.80 vs. 92.00 .", "entities": [[9, 10, "DatasetName", "WikiQA"], [12, 13, "DatasetName", "MAP"]]}, {"text": "The P@1 also signi\ufb01cantly improves by 2 % , i.e. , achieving 89.71 , which is impressively high .", "entities": [[1, 2, "MetricName", "P@1"]]}, {"text": "Also , on TRECQA , ASR outperforms all models , being on par with PR regarding P@1 .", "entities": [[3, 4, "DatasetName", "TRECQA"], [16, 17, "MetricName", "P@1"]]}, {"text": "The latter is 97.06 , which corresponds to mistaking the answers of only two questions .", "entities": []}, {"text": "We manually checked these and found out that these were two annotation errors : ASR achieves perfect accuracy while PR only mistakes one answer .", "entities": [[17, 18, "MetricName", "accuracy"]]}, {"text": "Of course , this just provides evidence that PR based on RoBERTa - Large solves the task of selecting the best answers ( i.e. , measuring P@1 on this dataset is not meaningful anymore ) .", "entities": [[11, 12, "MethodName", "RoBERTa"], [26, 27, "MetricName", "P@1"]]}, {"text": "5.6 Model Discussion Table 6 reports the accuracy of ASC inside to different models .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "In ASR , it uses 4 - way categories , while in MASR - based models , it uses the three", "entities": []}, {"text": "3260WikiQA TREC - QA WQA ACC F1 ACC F1 ACC F1 ASR 0.59 0.00 0.56 0.80 0.58 0.64 MASR 0.46 0.00 0.45 0.62 0.53 0.61 MASR - F 0.46 0.00 0.64 0.78 0.58 0.68 MASR - FP 0.49 0.37 0.65 0.73 0.59 0.69 Table 6 : The Accuracy and F1 of category 0 for ASC FEVER labels ( see Sec . 4.1 ) .", "entities": [[1, 2, "DatasetName", "TREC"], [5, 6, "MetricName", "ACC"], [6, 7, "MetricName", "F1"], [7, 8, "MetricName", "ACC"], [8, 9, "MetricName", "F1"], [9, 10, "MetricName", "ACC"], [10, 11, "MetricName", "F1"], [47, 48, "MetricName", "Accuracy"], [49, 50, "MetricName", "F1"], [52, 53, "DatasetName", "0"], [55, 56, "DatasetName", "FEVER"]]}, {"text": "ACC is the overall accuracy while F1 refers to the category 0 .", "entities": [[0, 1, "MetricName", "ACC"], [3, 5, "MetricName", "overall accuracy"], [6, 7, "MetricName", "F1"], [11, 12, "DatasetName", "0"]]}, {"text": "We note that ASC in MASR - FP achieves the highest accuracy with respect to the average over all datasets .", "entities": [[11, 12, "MetricName", "accuracy"]]}, {"text": "This happens since we pre-\ufb01ne - tuned it with the FEVER data .", "entities": [[10, 11, "DatasetName", "FEVER"]]}, {"text": "We analyzed examples for which ASR is correct and PR is not .", "entities": []}, {"text": "Tab .", "entities": []}, {"text": "7 shows that , given qandk= 3 candidates , PR chooses c1 , a suitable but wrong answer .", "entities": []}, {"text": "This probably happens since the answer best matches the syntactic / semantic pattern of the question , which asks for a type of color , indeed , the answer offers such type , primary colors .", "entities": []}, {"text": "PR does not rely on any background information that can support the set of colors in the answer .", "entities": []}, {"text": "In contrast , ASR selects c2as it can rely on the support of other answers .", "entities": []}, {"text": "Its ASC provides an average score for the category 0(both members are correct ) of c2 , i.e. ,1 kP i6=2ASC(c2;ci ) = 0:653 , while forc1the average score is signi\ufb01cant lower , i.e. , 0.522 .", "entities": [[20, 21, "MethodName", "kP"]]}, {"text": "This provides higher support for c2 , which is used by ASR to rerank the output of PR .", "entities": []}, {"text": "Tab . 8 shows an interesting case where all the sentences contain the required information , i.e. , February .", "entities": []}, {"text": "However , PR and ASR both choose answerc0 , which is correct but not natural , as it provides the requested information indirectly .", "entities": []}, {"text": "Also , it contains a lot of ancillary information .", "entities": []}, {"text": "In contrast , MASR is able to rerank the best answer , c1 , in the top position .", "entities": []}, {"text": "6 Conclusion We have proposed new joint models for AS2 .", "entities": []}, {"text": "ASR encodes the relation between the target answer and all the other candidates , using an additional Transformer model , and an Answer Support Classi\ufb01er , while MASR jointly models the ASR representations for all target answers .", "entities": [[17, 18, "MethodName", "Transformer"]]}, {"text": "We extensively tested KGAT , ASR , MASR , and other joint model baselines we designed .", "entities": []}, {"text": "The results show that our models can outperform the state of the art .", "entities": []}, {"text": "Most interestingly , ASR constantly outperforms all the models ( but MASR - FP ) , on all datasets , through all measures , and for both base and large transformers .", "entities": []}, {"text": "For example , ASRq : What kind of colors are in the rainbow ?", "entities": []}, {"text": "c1 :", "entities": []}, {"text": "Red , yellow , and blue are called the primary colors .", "entities": []}, {"text": "c2 : The order of the colors in the rainbow goes : red , orange , yellow , green , blue , indigo and violet .", "entities": []}, {"text": "c3 : The colors in all rainbows are present in the same order : red , orange , yellow , green , blue , indigo , and violet .", "entities": [[0, 1, "DatasetName", "c3"]]}, {"text": "c4 : A rainbow occurs when white light bends and separates into red , orange , yellow , green blue , indigo and violet .", "entities": [[0, 1, "DatasetName", "c4"]]}, {"text": "Table 7 : A question with answer candidates ranked by PR ; ASR chose c2 .", "entities": []}, {"text": "q : What \u2019s the month of Valentine \u2019s day ?", "entities": []}, {"text": "c0 : Celebrated on February 14 every year , saint Valentine \u2019s day or Valentine \u2019s day is the traditional day on which lovers convey their love to each other by sending Valentine \u2019s cards , sometimes even anonymously .", "entities": []}, {"text": "c1 : February is historically chosen to be the month of love and romance and the month to celebrate Valentine \u2019s day .", "entities": []}, {"text": "c2 : In order for today to be Valentine \u2019s day , it \u2019s necessary that today is in the month of February .", "entities": []}, {"text": "c3 : Every year , Valentine \u2019s day is celebrated on February 14 in many countries around the world .", "entities": [[0, 1, "DatasetName", "c3"]]}, {"text": "Table 8 : A question with answer candidates fc0;c1;c2;c3granked by PR ; ASR reranks as fc0;c3;c2;c1 g ; and MASR reranks as fc1;c3;c0;c2 g ; c1is the natural correct answer .", "entities": []}, {"text": "achieves the best reported results , i.e. , MAP values of 92.80 % and 94.88 , on WikiQA and TRECQA , respectively .", "entities": [[8, 9, "DatasetName", "MAP"], [17, 18, "DatasetName", "WikiQA"], [19, 20, "DatasetName", "TRECQA"]]}, {"text": "MASR improves ASR by 2 % on WQA , since this contains enough data to train the ASR representations jointly .", "entities": []}, {"text": "References Qingyao Ai , Keping Bi , Jiafeng Guo , and W. Bruce Croft .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning a deep listwise context model for ranking re\ufb01nement .", "entities": []}, {"text": "CoRR , abs/1804.05936 .", "entities": []}, {"text": "Weijie Bian , Si Li , Zhao Yang , Guang Chen , and Zhiqing Lin . 2017 .", "entities": []}, {"text": "A compare - aggregate model with dynamic - clip attention for answer selection .", "entities": [[11, 13, "TaskName", "answer selection"]]}, {"text": "In CIKM , pages 1987\u20131990 . ACM .", "entities": [[6, 7, "DatasetName", "ACM"]]}, {"text": "Daniele Bonadiman and Alessandro Moschitti . 2020 .", "entities": []}, {"text": "A study on ef\ufb01ciency , accuracy and document structure for answer sentence selection .", "entities": [[5, 6, "MetricName", "accuracy"]]}, {"text": "CoRR , abs/2003.02349 .", "entities": []}, {"text": "Zhe Cao , Tao Qin , Tie - Yan Liu , Ming - Feng Tsai , and Hang Li . 2007 .", "entities": []}, {"text": "Learning to rank : from pairwise approach to listwise approach .", "entities": []}, {"text": "In Proceedings of the 24th international conference on Machine learning , ICML \u2019 07 , pages 129\u2013136 , New York , NY , USA . ACM .", "entities": [[25, 26, "DatasetName", "ACM"]]}, {"text": "Danqi Chen , Adam Fisch , Jason Weston , and Antoine Bordes . 2017 .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "Reading wikipedia to answer opendomain questions .", "entities": []}, {"text": "CoRR , abs/1704.00051 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "BERT : pre - training of", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "3261deep bidirectional transformers for language understanding .", "entities": []}, {"text": "CoRR , abs/1810.04805 .", "entities": []}, {"text": "Siddhant Garg , Thuy Vu , and Alessandro Moschitti .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "TANDA : transfer and adapt pre - trained transformer models for answer sentence selection .", "entities": []}, {"text": "In The Thirty - Fourth AAAI Conference on Arti\ufb01cial Intelligence , AAAI 2020 , The Thirty - Second Innovative Applications of Arti\ufb01cial Intelligence Conference , IAAI 2020 , The Tenth AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence , EAAI 2020 , New York , NY , USA , February 7 - 12 , 2020 , pages 7780 \u2013 7788 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Minghao Hu , Yuxing Peng , Zhen Huang , and Dongsheng Li . 2019 .", "entities": []}, {"text": "Retrieve , read , rerank : Towards end - to - end multi - document reading comprehension .", "entities": [[15, 17, "TaskName", "reading comprehension"]]}, {"text": "CoRR , abs/1906.04618 .", "entities": []}, {"text": "Zan - Xia Jin , Bo - Wen Zhang , Fang Zhou , Jingyan Qin , and Xu - Cheng Yin .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Ranking via partial ordering for answer selection .", "entities": [[5, 7, "TaskName", "answer selection"]]}, {"text": "Information Sciences .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "CoRR , abs/1412.6980 .", "entities": []}, {"text": "Bernhard Kratzwald and Stefan Feuerriegel .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Adaptive document retrieval for deep question answering .", "entities": [[5, 7, "TaskName", "question answering"]]}, {"text": "In EMNLP\u201918 , pages 576\u2013581 .", "entities": []}, {"text": "Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , and Radu Soricut .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Albert :", "entities": []}, {"text": "A lite bert for self - supervised learning of language representations .", "entities": [[4, 8, "TaskName", "self - supervised learning"]]}, {"text": "In ICLR 2020 .", "entities": []}, {"text": "Md Tahmid Rahman Laskar , Jimmy Xiangji Huang , and Enamul Hoque .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Contextualized embeddings based transformer encoder for sentence similarity modeling in answer selection task .", "entities": [[10, 12, "TaskName", "answer selection"]]}, {"text": "In Proceedings of The 12th Language Resources and Evaluation Conference , pages 5505\u20135514 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Roberta : A robustly optimized BERT pretraining approach .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "CoRR , abs/1907.11692 .", "entities": []}, {"text": "Zhenghao Liu , Chenyan Xiong , Maosong Sun , and Zhiyuan Liu . 2020 .", "entities": []}, {"text": "Fine - grained fact veri\ufb01cation with kernel graph attention network .", "entities": [[7, 10, "MethodName", "graph attention network"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7342\u20137351 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tsvetomila Mihaylova , Georgi Karadzhov , Pepa Atanasova , Ramy Baly , Mitra Mohtarami , and Preslav Nakov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SemEval-2019 task 8 : Fact checking in community question answering forums .", "entities": [[4, 6, "TaskName", "Fact checking"], [7, 10, "TaskName", "community question answering"]]}, {"text": "InProceedings of the 13th International Workshop on Semantic Evaluation , pages 860\u2013869 , Minneapolis , Minnesota , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Tsvetomila Mihaylova , Preslav Nakov , Llu \u00b4 \u0131s M`arquez , Alberto Barr \u00b4 on - Cede \u02dcno , Mitra Mohtarami , Georgi Karadzhov , and James R. Glass .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Fact checking in community forums .", "entities": [[0, 2, "TaskName", "Fact checking"]]}, {"text": "AAAI-2018 .", "entities": []}, {"text": "Jinfeng Rao , Hua He , and Jimmy J. Lin . 2016 .", "entities": []}, {"text": "Noisecontrastive estimation for answer selection with deep neural networks .", "entities": [[3, 5, "TaskName", "answer selection"]]}, {"text": "In CIKM , pages 1913\u20131916 .", "entities": []}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Aliaksei Severyn and Alessandro Moschitti . 2015 .", "entities": []}, {"text": "Learning to rank short text pairs with convolutional deep neural networks .", "entities": []}, {"text": "In SIGIR\u201915 .", "entities": []}, {"text": "Gehui Shen , Yunlun Yang , and Zhi - Hong Deng . 2017 .", "entities": []}, {"text": "Inter - weighted alignment network for sentence pair modeling .", "entities": [[6, 9, "TaskName", "sentence pair modeling"]]}, {"text": "In EMNLP\u201917 , pages 1179\u20131189 , Copenhagen , Denmark .", "entities": []}, {"text": "Harish Tayyar Madabushi , Mark Lee , and John Barnden .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Integrating question classi\ufb01cation and deep learning for improved answer selection .", "entities": [[8, 10, "TaskName", "answer selection"]]}, {"text": "In COLING\u201918 , pages 3283\u20133294 .", "entities": []}, {"text": "James Thorne , Andreas Vlachos , Christos Christodoulopoulos , and Arpit Mittal .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "FEVER : a large - scale dataset for fact extraction and VERi\ufb01cation .", "entities": [[0, 1, "DatasetName", "FEVER"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 809\u2013819 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "James Thorne , Andreas Vlachos , Oana Cocarascu , Christos Christodoulopoulos , and Arpit Mittal .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "The fact extraction and VERi\ufb01cation ( FEVER ) shared task .", "entities": [[6, 7, "DatasetName", "FEVER"]]}, {"text": "In Proceedings of the First Workshop on Fact Extraction and VERi\ufb01cation ( FEVER ) , pages 1\u20139 , Brussels , Belgium .", "entities": [[12, 13, "DatasetName", "FEVER"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "E. V oorhees and D. Tice .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "The TREC-8 Question Answering Track Evaluation , pages 77\u201382 . Department of Commerce , National Institute of Standards and Technology .", "entities": [[2, 4, "TaskName", "Question Answering"]]}, {"text": "Mengqiu Wang , Noah A. Smith , and Teruko Mitamura .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "What is the Jeopardy model ?", "entities": []}, {"text": "a quasi - synchronous grammar for QA .", "entities": []}, {"text": "In EMNLPCoNLL\u201907 , pages 22\u201332 , Prague , Czech Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yizhong Wang , Kai Liu , Jing Liu , Wei He , Yajuan Lyu , Hua Wu , Sujian Li , and Haifeng Wang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Multipassage machine reading comprehension with crosspassage answer veri\ufb01cation .", "entities": [[1, 4, "TaskName", "machine reading comprehension"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1918\u20131927 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yi Yang , Wen - tau Yih , and Christopher Meek . 2015 .", "entities": []}, {"text": "Wikiqa : A challenge dataset for open - domain question answering .", "entities": [[0, 1, "DatasetName", "Wikiqa"], [6, 11, "TaskName", "open - domain question answering"]]}, {"text": "In Proceedings of the 2015 conference on empirical methods in natural language processing , pages 2013\u20132018 .", "entities": []}, {"text": "3262Zhilin Yang , Zihang Dai , Yiming Yang , Jaime G. Carbonell , Ruslan Salakhutdinov , and Quoc V .", "entities": [[13, 14, "DatasetName", "Ruslan"]]}, {"text": "Le . 2019 .", "entities": []}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "CoRR , abs/1906.08237 .", "entities": []}, {"text": "Alexander S. Yeh .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "More accurate tests for the statistical signi\ufb01cance of result differences .", "entities": []}, {"text": "CoRR , cs . CL/0008005 .", "entities": []}, {"text": "Seunghyun Yoon , Franck Dernoncourt , Doo Soon Kim , Trung Bui , and Kyomin Jung .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A compareaggregate model with latent clustering for answer selection .", "entities": [[7, 9, "TaskName", "answer selection"]]}, {"text": "CoRR , abs/1905.12897 .", "entities": []}, {"text": "Wenxuan Zhang , Yang Deng , Jing Ma , and Wai Lam . 2020a .", "entities": []}, {"text": "AnswerFact :", "entities": []}, {"text": "Fact checking in product question answering .", "entities": [[0, 2, "TaskName", "Fact checking"], [4, 6, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 2407\u20132417 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yingxue Zhang , Fandong Meng , Peng Li , Ping Jian , and Jie Zhou .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Ms - ranker : Accumulating evidence from potentially correct candidates for answer selection .", "entities": [[11, 13, "TaskName", "answer selection"]]}, {"text": "CoRR , abs/2010.04970 .", "entities": []}]