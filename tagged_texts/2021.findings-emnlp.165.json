[{"text": "Findings of the Association for Computational Linguistics : EMNLP 2021 , pages 1920\u20131934 November 7\u201311 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics1920Japanese Zero Anaphora Resolution Can Bene\ufb01t from Parallel Texts Through Neural Transfer Learning Masato Umakoshi , Yugo Murawaki , Sadao Kurohashi Graduate School of Informatics , Kyoto University Yoshida - honmachi , Sakyo - ku , Kyoto , 606 - 8501 , Japan { umakoshi , murawaki , kuro}@nlp.ist.i.kyoto-u.ac.jp Abstract Parallel texts of Japanese and a non - pro - drop language have the potential of improving the performance of Japanese zero anaphora resolution ( ZAR ) because pronouns dropped in the former are usually mentioned explicitly in the latter .", "entities": [[16, 18, "TaskName", "Transfer Learning"]]}, {"text": "However , rule - based cross - lingual transfer is hampered by error propagation in an NLP pipeline and the frequent lack of transparency in translation correspondences .", "entities": [[5, 9, "TaskName", "cross - lingual transfer"]]}, {"text": "In this paper , we propose implicit transfer by injecting machine translation ( MT ) as an intermediate task between pretraining and ZAR .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "We employ a pretrained BERT model to initialize the encoder part of the encoder - decoder model for MT , and eject the encoder part for \ufb01netuning on ZAR .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "The proposed framework empirically demonstrates that ZAR performance can be improved by transfer learning from MT .", "entities": [[12, 14, "TaskName", "transfer learning"]]}, {"text": "In addition , we \ufb01nd that the incorporation of the masked language model training into MT leads to further gains .", "entities": []}, {"text": "1 Introduction Figuring out who did what to whom is an essential part of natural language understanding .", "entities": [[14, 17, "TaskName", "natural language understanding"]]}, {"text": "This is , however , especially challenging for so - called prodrop languages like Japanese and Chinese because they usually omit pronouns that are inferable from context .", "entities": []}, {"text": "The task of identifying the referent of such a dropped element , as illustrated in Figure 1(a ) , is referred to as zero anaphora resolution ( ZAR ) .", "entities": []}, {"text": "Although Japanese ZAR saw a performance boost with the introduction of BERT ( Ueda et al . , 2020 ; Konno et al . , 2020 ) , there is still a good amount of room for improvement .", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "A major barrier to improvement is the scarcity of training data .", "entities": []}, {"text": "The number of annotated sentences is the order of tens of thousands or less ( Kawahara et al . , 2002 ;", "entities": []}, {"text": "Hangyo et al . , 2012 ; Iida et al . , 2017 ) , and the considerable linguistic expertise required for annotation makes drastic corpus expansion impractical .", "entities": []}, {"text": "Previous attempts to overcome this limitation exploit orders - of - magnitude larger parallel texts of Japanese and English , a non - pro - drop language ( Nakaiwa , 1999 ; Furukawa et al . , 2017 ) .", "entities": []}, {"text": "The key idea is that Japanese zero pronouns can be recovered from parallel texts because they are usually mentioned explicitly in English , as in Figure 1(b ) .", "entities": []}, {"text": "If translation correspondences are identi\ufb01ed and the anaphoric relation in English is identi\ufb01ed , then we can identify the antecedent of the omitted argument in Japanese .", "entities": []}, {"text": "Their rule - based transfer from English to Japanese had met with limited success , however .", "entities": []}, {"text": "It is prone to error propagation due to its dependence on word alignment , parsing , and English coreference resolution .", "entities": [[11, 13, "TaskName", "word alignment"], [18, 20, "TaskName", "coreference resolution"]]}, {"text": "More importantly , the great linguistic differences between the two language often lead to parallel sentences without transparent syntactic correspondences ( Figure 1(c ) ) .", "entities": []}, {"text": "In this paper , we propose neural transfer learning from machine translation ( MT ) .", "entities": [[7, 9, "TaskName", "transfer learning"], [10, 12, "TaskName", "machine translation"]]}, {"text": "By generating English translations , a neural MT model should be able to implicitly recover omitted Japanese pronouns , thanks to its expressiveness and large training data .", "entities": []}, {"text": "We expect the knowledge gained during MT training to be transferred to ZAR .", "entities": []}, {"text": "Given that state - of - the - art ZAR models are based on BERT ( Ueda et al . , 2020 ; Konno et al . , 2020 , 2021 ) , it is a natural choice to explore intermediate task transfer learning ( Phang et al . , 2018 ; Wang et al . , 2019a ;", "entities": [[14, 15, "MethodName", "BERT"], [42, 44, "TaskName", "transfer learning"]]}, {"text": "Pruksachatkun et al . , 2020 ; Vu et al . , 2020 ):", "entities": []}, {"text": "A pretrained BERT model is \ufb01rst trained on MT and the resultant model is then \ufb01ne - tuned on", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "ZAR.1", "entities": []}, {"text": "A key challenge to this approach is a mismatch in model architectures .", "entities": []}, {"text": "While BERT is an encoder , the dominant paradigm of neural MT is the encoder - decoder .", "entities": [[1, 2, "MethodName", "BERT"]]}, {"text": "Although both share Transformer ( Vaswani et al . , 2017 ) as the building block , 1In our preliminary experiments , we tested encoderdecoder pretrained models with no success .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "We brie\ufb02y revisit this in Section 4.7 .", "entities": []}, {"text": "1921 Figure 1 : ( a ) An example of Japanese zero anaphora .", "entities": []}, {"text": "The nominative argument of the underlined predicate is omitted .", "entities": []}, {"text": "The goal of the task is to detect the omission and to identify its antecedent \u201c son \u201d .", "entities": []}, {"text": "( b ) The corresponding English text .", "entities": []}, {"text": "The omitted argument in Japanese is present as a pronoun in English .", "entities": []}, {"text": "( c ) A Japanese - English pair ( Nabeshima and Brooks , 2020 , p. 74 ) whose correspondences are too obscure for rule - based transfer .", "entities": []}, {"text": "Because Japanese generally avoids having inanimate agents with animate patients , the English inanimate - subject sentence corresponds to two animate - subject clauses in Japanese , with two exophoric references to the reader ( i.e. , you ) .", "entities": []}, {"text": "it is non - trivial to combine the two distinct architectures , with the goal to help the former .", "entities": []}, {"text": "We use a pretrained BERT model to initialize the encoder part of the encoder - decoder model for MT .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "While this technique was previously used by Imamura and Sumita ( 2019 ) and Clinchant et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) , they both aimed at improving MT performance .", "entities": []}, {"text": "We show that by ejecting the encoder part for use in \ufb01ne - tuning ( Figure 2 ) , we can achieve performance improvements in ZAR .", "entities": []}, {"text": "We also demonstrate further improvements can be brought by incorporating encoder - side masked language model ( MLM ) training into the intermediate training on MT .", "entities": [[17, 18, "DatasetName", "MLM"]]}, {"text": "2 Related Work 2.1 Zero Anaphora Resolution ( ZAR ) ZAR has been extensively studied in major East Asian languages , Chinese and Korean as well as Japanese , which not only omit contextually inferable pronouns but also show no verbal agreement for person , number , or gender ( Park et", "entities": []}, {"text": "al . , 2015 ; Yin et al . , 2017 ; Song et al . , 2020 ; Kim et al . , 2021 ) .", "entities": []}, {"text": "While supervised learning is the standard approach to ZAR ( Iida et al . , 2016 ; Ouchi et al . , 2017 ; Shibata and Kurohashi , 2018 ) , training data are so small that additional resources are clearly needed .", "entities": []}, {"text": "Early studies work on case frame construction from a large raw corpus ( Sasano et al . , 2008 ; Sasano and Kurohashi , 2011 ; Yamashiro et al . , 2018 ) , pseudo training data generation ( Liu et al . , 2017 ) , and adversarial training ( Kurita et al . , 2018 ) .", "entities": []}, {"text": "These efforts are , however , overshadowed by the surprising effectiveness of BERT \u2019s pretraining ( Ueda et al . , 2020 ; Konno et al . , 2020 ) .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "Adopting BERT , recent studies seek gains through multi - task learning ( Ueda et al . , 2020),data augmentation ( Konno et al . , 2020 ) , and an intermediate task tailored to ZAR ( Konno et al . , 2021 ) .", "entities": [[1, 2, "MethodName", "BERT"], [8, 12, "TaskName", "multi - task learning"]]}, {"text": "The multi - task learning approach of Ueda et al .", "entities": [[1, 5, "TaskName", "multi - task learning"]]}, {"text": "( 2020 ) covers verbal predicate analysis ( which subsumes ZAR ) , and nominal predicate analysis , coreference resolution , and bridging anaphora resolution .", "entities": [[18, 20, "TaskName", "coreference resolution"], [22, 25, "TaskName", "bridging anaphora resolution"]]}, {"text": "Their method is used as a state - of - the - art baseline in our experiments .", "entities": []}, {"text": "Konno et al .", "entities": []}, {"text": "( 2020 ) perform data augmentation by simply masking some tokens .", "entities": [[4, 6, "TaskName", "data augmentation"]]}, {"text": "They found that performance gains were achieved by selecting target tokens by part of speech .", "entities": []}, {"text": "Konno et al .", "entities": []}, {"text": "( 2021 ) introduce a more elaborate masking strategy as a ZAR - speci\ufb01c intermediate task They spot multiple occurrences of the same noun phrase , mask one of them , and force the model to identify the pseudo - antecedent .", "entities": []}, {"text": "Our use of parallel texts in ZAR is inspired by Nakaiwa ( 1999 ) and Furukawa et al . ( 2017 ) , who identify a multi - hop link from a Japanese zero pronoun to its Japanese antecedent via English counterparts .", "entities": []}, {"text": "Their rule - based methods suffer from accumulated errors and syntactically non - transparent correspondences .", "entities": []}, {"text": "In addition , they do not handle inter - sentential anaphora , a non - negligible subtype of anaphora we cover in this paper .", "entities": []}, {"text": "While we exploit MT to improve the performance of ZAR , the exploitation in the reverse direction has been studied .", "entities": []}, {"text": "A line of research has been done on Chinese zero pronoun prediction ( ZPP ) with a primary aim of improving Chinese - English translation ( Wang et al . , 2016 , 2018 , 2019b ) .", "entities": []}, {"text": "ZPP is different from ZAR in that it does not identify antecedents .", "entities": []}, {"text": "This is understandable given that classi\ufb01cation of zero pronouns into overt ones suf\ufb01ces for MT .", "entities": []}, {"text": "Although Wang et al .", "entities": []}, {"text": "( 2019b ) report mutual bene\ufb01ts between MT and ZPP , it remains an", "entities": []}, {"text": "1922 BERTDecoder\ud835\udc61!BERT\ud835\udc61\"\ud835\udc61#\ud835\udc61\"\ud835\udc61#\ud835\udc61$\ud835\udc61$[MASK][MASK];\u02bc\ud835\udc61![MASK];\u02bc", "entities": []}, {"text": "[ MASK]\ud835\udc61% [ reader]Pretraining ( MLM)Intermediate task ( MT w/ MLM ) Target task ( ZAR w/ related tasks)[reader]\ud835\udc61\"\ud835\udc61#\ud835\udc61%[reader]\ud835\udc61\"\ud835\udc61#\ud835\udc61%BERT;\u02bc\ud835\udc61$[MASK]\ud835\udc62%\ud835\udc62\"\ud835\udc62#\ud835\udc62$ \ud835\udc61%\ud835\udc61\"\ud835\udc61#\ud835\udc61%\ud835\udc61\"\ud835\udc61#\ud835\udc61\"[MASK];\u02bc\ud835\udc61#\ud835\udc61%\ud835\udc61$[MASK]\ud835\udc61\"[MASK]\ud835\udc61%[reader]Figure 2 : Overview of the proposed method .", "entities": [[10, 11, "DatasetName", "MLM"]]}, {"text": "Left : The model is pretrained with the masked language model ( MLM ) objective ( known as BERT ) .", "entities": [[12, 13, "DatasetName", "MLM"], [18, 19, "MethodName", "BERT"]]}, {"text": "Center :", "entities": []}, {"text": "The pretrained BERT is used to initialize the encoder part of the encoder - decoder , which is trained on MT with the MLM objective .", "entities": [[2, 3, "MethodName", "BERT"], [23, 24, "DatasetName", "MLM"]]}, {"text": "Right : The encoder is extracted from the MT model and is \ufb01ne - tuned on ZAR and related tasks .", "entities": []}, {"text": "Note that some special tokens are omitted for simplicity .", "entities": []}, {"text": "open question whether MT helps ZAR as well .", "entities": []}, {"text": "2.2 MT as an Intermediate Task Inspired by the great success of the pretraining/\ufb01netuning paradigm on a broad range of tasks ( Peters et al . , 2018 ; Devlin et", "entities": [[6, 7, "DatasetName", "Inspired"]]}, {"text": "al . , 2019 ) , a line of research inserts an intermediate task between pretraining and \ufb01ne - tuning on a target task ( Phang et al . , 2018 ; Wang et al . , 2019a ; Pruksachatkun et al . , 2020 ) .", "entities": []}, {"text": "However , Wang et al . ( 2019a ) found that MT used as an intermediate task led to performance degeneration in various target tasks , such as natural language inference and sentiment classi\ufb01cation.2They argue that the considerable difference between MLM pretraining and MT causes catastrophic forgetting ( CF ) .", "entities": [[28, 31, "TaskName", "natural language inference"], [40, 41, "DatasetName", "MLM"]]}, {"text": "Pruksachatkun et al .", "entities": []}, {"text": "( 2020 ) suggest injecting the MLM objective during intermediate training as a possible way to mitigate CF , which we empirically test in this paper .", "entities": [[6, 7, "DatasetName", "MLM"]]}, {"text": "2.3 Use of BERT in MT Motivated by BERT \u2019s success in a wide range of applications , some studies incorporate BERT into MT models .", "entities": [[3, 4, "MethodName", "BERT"], [8, 9, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"]]}, {"text": "A straightforward way to do this is to initialize the encoder part of the encoder - decoder with pretrained BERT , but it has had mixed success at best ( Clinchant et al . , 2019 ; Zhu et al . , 2020 ) .", "entities": [[19, 20, "MethodName", "BERT"]]}, {"text": "Abandoning this approach , Zhang et al .", "entities": []}, {"text": "( 2020 ) simply use BERT as a supplier of context - aware embeddings to their own encoder - decoder model .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "Similarly , Guo et al .", "entities": []}, {"text": "( 2020 ) stack adapter layers on top of two frozen BERT models to use them as the encoder and decoder of a non - autoregressive MT 2We suspect that the poor performance resulted in part from their excessively simple decoder , a single - layer LSTM .", "entities": [[11, 12, "MethodName", "BERT"], [46, 47, "MethodName", "LSTM"]]}, {"text": "BERT[CLS][author][NA]\ud835\udc95\ud835\udc8a\ud835\udc61![CLS][author][NA]\ud835\udc95\ud835\udc8a\ud835\udc61!Figure 3 : ZAR as argument selection .", "entities": []}, {"text": "model .", "entities": []}, {"text": "However , these methods can not be adopted for our purpose because we want BERT itself to learn from MT .", "entities": [[14, 15, "MethodName", "BERT"]]}, {"text": "Imamura and Sumita ( 2019 ) manage to maintain the straightforward approach by adopting a twostage training procedure :", "entities": []}, {"text": "In the \ufb01rst stage , only the decoder is updated with the encoder frozen , while in the second stage , the entire model is updated .", "entities": []}, {"text": "Although they offer some insights , it remains unclear how best to exploit BERT when MT is an intermediate task , not the target task .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "3 Proposed Method We adopt a ZAR model of Ueda et al .", "entities": []}, {"text": "( 2020 ) , which adds a thin layer on top of BERT during \ufb01ne - tuning to solve ZAR and related tasks ( Section 3.1 ) .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "Instead of directly moving from MLM pretraining to \ufb01ne - tuning on ZAR , we inject MT as an intermediate task ( Section 3.2 ) .", "entities": [[5, 6, "DatasetName", "MLM"]]}, {"text": "In addition , we introduce the MLM training objective during the intermediate training ( Section 3.3 ) .", "entities": [[6, 7, "DatasetName", "MLM"]]}, {"text": "19233.1 BERT - based Model for ZAR ZAR as argument selection As illustrated in Figure 3 , the basic idea behind BERT - based ZAR is that given the powerful neural encoder , the joint task of omission detection and antecedent identi\ufb01cation can be formalized as argument selection ( Shibata and Kurohashi , 2018 ; Kurita et al . , 2018 ; Ueda et al . , 2020 ) .", "entities": [[1, 2, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"]]}, {"text": "Omission detection concerns whether a given predicate has an argument for a given case ( relation ) .", "entities": []}, {"text": "If not , the model must point to the special token [ NULL ] .", "entities": []}, {"text": "Otherwise the model must identify the antecedent of the zero pronoun by pointing either to a token in the given text or to a special token reserved for exophora .", "entities": []}, {"text": "Note that by getting the entire document as the input , the model can handle inter - sentential anaphora as well as intra - sentential anaphora .", "entities": []}, {"text": "In practice , the input length limitation of BERT forces us to implement a sliding window approach .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "Also note that in this formulation , ZAR is naturally subsumed into verbal predicate analysis ( VPA ) , which also covers instances where the predicate and the argument have a dependency relation and only the case marker is absent .", "entities": []}, {"text": "Formally , the probability of the token tjbeing the argument of the predicate tifor casecis : P(tjjti;c )", "entities": []}, {"text": "= exp(sc(tj;ti))P j0exp(sc(tj0;ti))(1 ) sc(tj;ti )", "entities": []}, {"text": "= v|tanh(Wctj+Ucti)(2 ) where tiis the context - aware embedding of tiprovided by BERT , WcandUcare case - speci\ufb01c weight matrices , and vis a weight vector shared among cases .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "We output tjwith the highest probability .", "entities": []}, {"text": "For each predicate , we repeat this for the nominative ( NOM ) , accusative ( ACC ) , and dative ( DAT ) cases , and another nominative case for the double nominative construction ( NOM2 ) .", "entities": [[16, 17, "MetricName", "ACC"]]}, {"text": "Input representations We append some special tokens at the end of the input sequence :", "entities": []}, {"text": "[ NULL ] for null arguments , and [ author ] , [ reader ] , and[unspecified person ] for exophora .", "entities": []}, {"text": "The special token [ NA ] is also supplied for the reason given in the next paragraph .", "entities": []}, {"text": "As is usual for BERT , the special tokens [ CLS ] and[SEP ] are inserted at the beginning and end of the sequence , respectively .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "If a predicate or argument candidate is split into two or more subwords , the initial subword is used for argument selection .", "entities": []}, {"text": "Multi - task learning Following Ueda et al .", "entities": [[0, 4, "TaskName", "Multi - task learning"]]}, {"text": "( 2020 ) , we use a single model to simultaneously perform verbal predicate analysis ( VPA ) , nominal predicate analysis ( NPA ) , bridging anaphora resolution ( BAR ) , and coreference resolution ( CR ) .", "entities": [[26, 29, "TaskName", "bridging anaphora resolution"], [30, 31, "DatasetName", "BAR"], [34, 36, "TaskName", "coreference resolution"]]}, {"text": "NPA is a variant of VPA in which verb - like nouns serve as predicates taking arguments .", "entities": []}, {"text": "BAR is a special kind of anaphora resolution in which the antecedent \ufb01lls a semantic gap of the anaphor ( e.g. , \" price \" takes something priced as its argument ) .", "entities": [[0, 1, "DatasetName", "BAR"]]}, {"text": "CR identi\ufb01es the antecedent and anaphor that refer to the same real - world entity , with the special token [ NA ] reserved for nouns without coreferent mentions .", "entities": []}, {"text": "All of the four tasks can be formalized as argument selection as in Eq .", "entities": []}, {"text": "( 1 ) .", "entities": []}, {"text": "By sharing the BERT encoder , these interrelated tasks have an in\ufb02uence on each other during training .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "In addition , case - speci\ufb01c weights are shared between VPA and NPA while separate weights are used for BAR and CR .", "entities": [[19, 20, "DatasetName", "BAR"]]}, {"text": "During training , we compute the losses equally for the four tasks .", "entities": []}, {"text": "3.2 MT as an Intermediate Task Our main proposal is to use MT as an intermediate task prior to \ufb01ne - tuning on ZAR .", "entities": []}, {"text": "Following Imamura and Sumita ( 2019 ) and Clinchant et al .", "entities": []}, {"text": "( 2019 ) , we use a pretrained BERT to initialize the encoder part of the Transformer - based encoderdecoder model while the decoder is randomly initialized .", "entities": [[8, 9, "MethodName", "BERT"], [16, 17, "MethodName", "Transformer"]]}, {"text": "After the intermediate training on MT , we extract the encoder and move on to \ufb01ne - tuning on ZAR and related tasks ( Figure 2 ) .", "entities": []}, {"text": "Speci\ufb01cally , we test the following two procedures for intermediate training : One - stage optimization The entire model is updated throughout the training .", "entities": []}, {"text": "Two - stage optimization In the \ufb01rst stage", "entities": []}, {"text": ", the encoder is frozen and only the decoder is updated .", "entities": []}, {"text": "In the second stage , the entire model is updated ( Imamura and Sumita , 2019 ) .", "entities": []}, {"text": "3.3 Incorporating MLM into MT As discussed in Section 2.2 , MT as an intermediate task reportedly harms target - task performance , probably because MT forces the model to forget what it has learned from MLM pretraining ( catastrophic forgetting ) .", "entities": [[2, 3, "DatasetName", "MLM"], [36, 37, "DatasetName", "MLM"]]}, {"text": "To overcome this problem , we incorporate the MLM training objective into MT , as suggested by Pruksachatkun et al .", "entities": [[8, 9, "DatasetName", "MLM"]]}, {"text": "( 2020 ) .", "entities": []}, {"text": "Speci\ufb01cally , we mask some input tokens on the encoder", "entities": []}, {"text": "1924Web News # of sentences 16,038 11,276 # of zeros 30,852 27,062 Table 1 : The numbers of sentences and zero anaphors in each corpus .", "entities": []}, {"text": "side and force the model to recover the original tokens , as depicted in the center of Figure 2 .", "entities": []}, {"text": "Our masking strategy is the same as BERT \u2019s ( Devlin et al . , 2019 ):", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "We choose 15 % of the tokens at random and 80 % of them are replaced with [ MASK ] , 10 % of them with a random token , and the rest are unchanged .", "entities": []}, {"text": "The corresponding losses are added to the MT loss function .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "4 Experiments 4.1 Datasets ZAR", "entities": []}, {"text": "We used two corpora in our experiments : the Kyoto University Web Document Lead Corpus ( Hangyo et al . , 2012 ) and the Kyoto University Text Corpus ( Kawahara et al . , 2002 ) .", "entities": []}, {"text": "Based on their genres , we refer to them as the Web andNews , respectively .", "entities": []}, {"text": "These corpora have been widely used in previous studies ( Shibata and Kurohashi , 2018 ;", "entities": []}, {"text": "Kurita et al . , 2018 ; Ueda et al . , 2020 ) .", "entities": []}, {"text": "They contained manual annotation for predicate - argument structures ( including zero anaphora ) as well as word segmentation , part - of - speech tags , dependency relations , and coreference chains .", "entities": [[20, 23, "DatasetName", "part - of"]]}, {"text": "We split the datasets into training , validation , and test sets following the published setting , where the ratio was around 0:75:0:1:0:15 .", "entities": []}, {"text": "Key statistics are shown in Table 1 .", "entities": []}, {"text": "MT We used a Japanese - English parallel corpus of newspaper articles distributed by the Yomiuri Shimbun.3It consisted of about 1.3 million sentence pairs4with sentence alignment scores .", "entities": []}, {"text": "We discarded pairs with scores of 0 .", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "Because the task of interest , ZAR , required inter - sentential reasoning , consecutive sentences were concatenated into chunks , with the maximum number of tokens equal to that of ZAR .", "entities": []}, {"text": "As a result , we obtained around 373,000 , 21,000 , and 21,000 chunks for the training , validation , and test data , respectively .", "entities": []}, {"text": "Japanese sentences were split into words using the morphological analyzer MeCab with the Juman 3https://database.yomiuri.co.jp/about/ glossary/", "entities": []}, {"text": "4We counted the Japanese sentences since there were oneto - many mappings.dictionary ( Kudo et al . , 2004).5Both Japanese and English texts underwent subword tokenization .", "entities": []}, {"text": "We used Subword - NMT ( Sennrich et al . , 2016 ) for Japanese and SentencePiece ( Kudo and Richardson , 2018 ) for English .", "entities": [[16, 17, "MethodName", "SentencePiece"]]}, {"text": "We used separate vocabularies for Japanese and English , with the vocabulary sizes of around 32,000 and 16,000 , respectively .", "entities": []}, {"text": "4.2 Model Settings BERT We employed a Japanese BERT model with BPE segmentation distributed by NICT.6", "entities": [[3, 4, "MethodName", "BERT"], [8, 9, "MethodName", "BERT"], [11, 12, "MethodName", "BPE"]]}, {"text": "It had the same architecture as Google \u2019s BERTBase ( Devlin et al . , 2019 ): 12 layers , 768 hidden units , and 12 attention heads .", "entities": [[6, 7, "DatasetName", "Google"]]}, {"text": "It was trained on the full text of Japanese Wikipedia for approximately 1 million steps .", "entities": []}, {"text": "MT We used the Transformer encoder - decoder architecture ( Vaswani et al . , 2017 ) .", "entities": [[4, 5, "MethodName", "Transformer"]]}, {"text": "The encoder was initialized with BERT while the decoder was a randomly initialized six - layer Transformer .", "entities": [[5, 6, "MethodName", "BERT"], [16, 17, "MethodName", "Transformer"]]}, {"text": "The numbers of hidden units and heads were set to be the same as BERT \u2019s ( i.e. , 768 units and 12 attention heads ) .", "entities": [[14, 15, "MethodName", "BERT"]]}, {"text": "We adopted Adam ( Kingma and Ba , 2017 ) as the optimizer .", "entities": [[2, 3, "MethodName", "Adam"], [12, 13, "HyperparameterName", "optimizer"]]}, {"text": "We set the total number of epochs to 50 .", "entities": [[4, 7, "HyperparameterName", "number of epochs"]]}, {"text": "In two - stage optimization , the encoder was frozen during the \ufb01rst 15 epochs , then the entire model was updated for the remaining 35 epochs .", "entities": []}, {"text": "We set a mini - batch size to about 500 .", "entities": [[3, 7, "HyperparameterName", "mini - batch size"]]}, {"text": "The details of hyper - parameters are given in Appendix A. ZAR For a fair comparison with Ueda et al .", "entities": []}, {"text": "( 2020 ) , we used almost the same con\ufb01guration as theirs .", "entities": []}, {"text": "We dealt with all subtypes of ZAR : intra - sentential anaphora , inter - sentential anaphora , and exophora .", "entities": []}, {"text": "For exophora , we targeted [ author ] , [ reader ] , and [ unspecified person ] .", "entities": []}, {"text": "We set the maximum sequence length to 128.7All documents from the Web met this limitation .", "entities": []}, {"text": "In the News corpus , however , many documents exceeded the sequence length of 128 .", "entities": []}, {"text": "For such documents , we divided the document into multiple parts such that it had the longest preceding contexts .", "entities": []}, {"text": "The evaluation of ZAR was relaxed using a gold coreference chain .", "entities": []}, {"text": "The model was trained on the mixture of both corpora and evaluated on each corpus .", "entities": []}, {"text": "We used almost the same 59 pairs for which morphological analysis failed were removed .", "entities": [[9, 11, "TaskName", "morphological analysis"]]}, {"text": "6https://alaginrc.nict.go.jp/ nict - bert / index.html 7We tested longer maximum sequence lengths ( 256 and 512 ) but ended up with poorer performance .", "entities": []}, {"text": "1925Models Web News Shibata and Kurohashi ( 2018 ) 58.1 35.6 Kurita et al .", "entities": []}, {"text": "( 2018)858.4 Ueda et al .", "entities": []}, {"text": "( 2020)970.3 56.7 + MT 71.4 * 57.7 + MT w/ MLM 71.9 58.3 Table 2 : F1 scores on the test sets .", "entities": [[11, 12, "DatasetName", "MLM"], [17, 18, "MetricName", "F1"]]}, {"text": "* : with two - stage optimization .", "entities": []}, {"text": "hyper - parameters as Ueda et al .", "entities": []}, {"text": "( 2020 ) , which are included in Appendix", "entities": []}, {"text": "B. We decided to tune the training epochs for MT since we found that it slightly affected ZAR performance .", "entities": []}, {"text": "We collected checkpoints at the interval of 5 epochs out of 45 epochs , in addition to the one with the lowest validation loss .", "entities": [[23, 24, "MetricName", "loss"]]}, {"text": "They were all trained on ZAR , and we chose the one with the highest score on the validation set .", "entities": []}, {"text": "We ran the model with 3 seeds on MT and with 3 seeds on ZAR , which resulted in 9 seed combinations .", "entities": [[6, 7, "DatasetName", "seeds"], [12, 13, "DatasetName", "seeds"]]}, {"text": "We report the mean and the standard deviation of the 9 runs .", "entities": []}, {"text": "4.3 Results Table 2 summarizes the experimental results .", "entities": []}, {"text": "Our baseline is Ueda et al .", "entities": []}, {"text": "( 2020 ) , who drastically outperformed previous models , thanks to BERT .", "entities": [[12, 13, "MethodName", "BERT"]]}, {"text": "+ MT refers to the model with intermediate training on MT while + MT w/ MLM corresponds to the model that incorporated the MLM objective into MT .", "entities": [[15, 16, "DatasetName", "MLM"], [23, 24, "DatasetName", "MLM"]]}, {"text": "We can see that MT combined with MLM performed the best and that the gains reached 1.6 points for both the Web and News .", "entities": [[7, 8, "DatasetName", "MLM"]]}, {"text": "Tables 3 and 4 provide more detailed results .", "entities": []}, {"text": "For comparison , we performed additional pretraining with ordinary MLM on the Japanese part of the parallel corpus ( denoted as + MLM ) , because the possibility remained that the model simply took advantage of additional data .", "entities": [[9, 10, "DatasetName", "MLM"], [22, 23, "DatasetName", "MLM"]]}, {"text": "The subsequent two blocks compare one - stage ( unmarked ) optimization with two - stage optimization .", "entities": []}, {"text": "MT yielded gains on all settings .", "entities": []}, {"text": "The gains were consistent across anaphora categories .", "entities": []}, {"text": "Although + MLM somehow beat the baseline , it was outperformed by most models trained on MT , ruling out the possibility that the gains were solely attributed to extra data .", "entities": [[2, 3, "DatasetName", "MLM"]]}, {"text": "We can 8Not a strict comparison since Kurita et al .", "entities": []}, {"text": "( 2018 ) ignored inter - sentential anaphora .", "entities": []}, {"text": "9We refer to errata posted on the \ufb01rst author \u2019s website : https://nobu-g.github.io/pub/COLING2020 _ errata.pdfconclude that Japanese ZAR bene\ufb01ts from parallel texts through neural transfer learning .", "entities": [[24, 26, "TaskName", "transfer learning"]]}, {"text": "Two - stage optimization showed mixed results .", "entities": []}, {"text": "It worked for the Web but did not for the News .", "entities": []}, {"text": "What is worse , its combination with MLM led to performance degeneration on both datasets .", "entities": [[7, 8, "DatasetName", "MLM"]]}, {"text": "MLM achieved superior performance as it worked well in all settings .", "entities": [[0, 1, "DatasetName", "MLM"]]}, {"text": "The gains were larger with one - stage optimization than with two - stage optimization ( 1.4 vs. 0.3 on the Web ) .", "entities": []}, {"text": "4.4 Translation of Zero Pronouns The experimental results demonstrate that MT helps ZAR , but why does it work ?", "entities": [[1, 2, "TaskName", "Translation"]]}, {"text": "Unfortunately , conventional evaluation metrics for MT ( e.g. , BLEU ) reveal little about the model \u2019s ability to handle zero anaphora .", "entities": [[10, 11, "MetricName", "BLEU"]]}, {"text": "To address this problem , Shimazu et al .", "entities": []}, {"text": "( 2020 ) and", "entities": []}, {"text": "Nagata and Morishita ( 2020 ) constructed Japanese - English parallel datasets that were designed to automatically evaluate MT models with regard to the translation of Japanese zero pronouns ( ZPT ) .", "entities": []}, {"text": "We used Shimazu et al . \u2019s dataset for its larger data size.10 To facilitate automatic evaluation of ZPT , this dataset paired a correct English sentence with an incorrect one .", "entities": []}, {"text": "All we had to do was to calculate the ratio of instances for which the model assigned higher translation scores to the correct candidates .", "entities": []}, {"text": "The only difference between the two sentences involved the translation of a Japanese zero pronoun .", "entities": []}, {"text": "To choose the correct one , the MT model must sometimes refer to preceding sentences .", "entities": []}, {"text": "As in intermediate training , multiple source sentences were fed to the model to generate multiple target sentences .", "entities": []}, {"text": "We prepended as many preceding sentences as possible given the limit of 128 tokens .", "entities": []}, {"text": "In addition , this dataset recorded d , the sentencelevel distance between the zero pronoun in question and its antecedent .", "entities": []}, {"text": "The number of instances with d= 0 was 218 while the number of remaining instances was 506 .", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "We regarded the former as the instances of intra - sentential anaphora and the latter as the instances of inter - sentential anaphora .", "entities": []}, {"text": "We chose the model with the best performance ( i.e. , one - stage optimization with MLM ) .", "entities": [[16, 17, "DatasetName", "MLM"]]}, {"text": "For each checkpoint we collected during intermediate training , we ( 1 ) measured the ZPT accuracy and ( 2 ) \ufb01netuned it to obtain the F1 score for ZAR .", "entities": [[16, 17, "MetricName", "accuracy"], [26, 28, "MetricName", "F1 score"]]}, {"text": "As before , 10In this datasets , Japanese texts were translated from English broadcast conversations .", "entities": []}, {"text": "Despite the multifold domain mismatch ( i.e. , spoken and translationese ) , to our knowledge , this was the best dataset available for our purpose .", "entities": []}, {"text": "1926MethodsWeb all intra inter exophora Ueda et al .", "entities": []}, {"text": "( 2020)970.3 - - + MLM 71.0\u00060:716 63.9\u00061:27 65.1\u00061:14 75.8\u00060:764 +", "entities": [[5, 6, "DatasetName", "MLM"]]}, {"text": "MT 70.5\u00060:410 64.0\u00060:868 63.8\u00060:536 75.4\u00060:565", "entities": []}, {"text": "+", "entities": []}, {"text": "MT w/ MLM", "entities": [[2, 3, "DatasetName", "MLM"]]}, {"text": "71.9\u00060:416 65.4\u00060:697 65.2\u00061:07 76.7\u00060:468 + MT ( Two - stage )", "entities": []}, {"text": "71.4\u00060:511 65.3\u00060:830 64.6\u00060:479 76.1\u00060:633 + MT w/ MLM ( Two - stage ) 71.7\u00060:393 65.0\u00060:641 64.4\u00060:844 76.7\u00060:478 Table 3 : Breakdown of the F1 scores with standard deviations on the Web test set .", "entities": [[7, 8, "DatasetName", "MLM"], [23, 24, "MetricName", "F1"]]}, {"text": "Boldfaced scores indicate the best results in the corresponding categories .", "entities": []}, {"text": "One - stage optimization with the MLM objective performed the best on all categories .", "entities": [[6, 7, "DatasetName", "MLM"]]}, {"text": "MethodsNews all intra inter exophora Ueda et al .", "entities": []}, {"text": "( 2020)956.7 - - + MLM 57.1\u00060:359 62.7\u00060:723 50.2\u00060:880 55.6\u00061:30 + MT 57.7\u00060:442 63.8\u00060:652 49.8\u00060:811 57.1\u00060:910 + MT w/ MLM 58.3\u00060:383 65.0\u00060:544 50.6\u00060:667 56.3\u00061:07 + MT ( Two - stage )", "entities": [[5, 6, "DatasetName", "MLM"], [19, 20, "DatasetName", "MLM"]]}, {"text": "57.3\u00060:466 63.2\u00060:723 50.1\u00060:761 55.7\u00060:700 + MT w/ MLM ( Two - stage ) 57.7\u00060:549 63.8\u00060:597 50.2\u00060:628 56.2\u00061:37 Table 4 : Breakdown of the F1 scores with standard deviations on the News test set .", "entities": [[7, 8, "DatasetName", "MLM"], [23, 24, "MetricName", "F1"]]}, {"text": "Boldfaced scores indicate the best results in the corresponding categories .", "entities": []}, {"text": "One - stage optimization with the MLM objective performed the best on all categories but exophora .", "entities": [[6, 7, "DatasetName", "MLM"]]}, {"text": "Web News intra", "entities": []}, {"text": "- sentential anaphora 0.758 0.763 inter - sentential anaphora 0.871 0.879 Table 5 : Pearson \u2019s correlation coef\ufb01cient between ZPT accuracies and ZAR F1 scores .", "entities": [[23, 24, "MetricName", "F1"]]}, {"text": "scores were averaged over 3 different seeds .", "entities": [[6, 7, "DatasetName", "seeds"]]}, {"text": "Through the course of intermediate training , we observed almost steady increase in ZPT accuracies and ZAR F1 scores until around the 30th epoch ( the four \ufb01gures in Appendix D ) .", "entities": [[17, 18, "MetricName", "F1"]]}, {"text": "Table 5 shows the strong positive correlations between the two performance measures , especially the very strong correlation for inter - sentential anaphora .", "entities": []}, {"text": "These results were in line with our speculation that the performance gains in ZAR stemmed from the model \u2019s increased ability to translate zero pronouns .", "entities": []}, {"text": "4.5 Why Is MLM so Effective ?", "entities": [[3, 4, "DatasetName", "MLM"]]}, {"text": "The MLM objective during intermediate training on MT is shown to be very effective , but why ?", "entities": [[1, 2, "DatasetName", "MLM"]]}, {"text": "Pruksachatkun et al .", "entities": []}, {"text": "( 2020 ) conjecture that it would mitigate catastrophic forgetting ( CF ) , but this is not the sole explanation .", "entities": []}, {"text": "In fact , Konno et al .", "entities": []}, {"text": "( 2020 ) see token masking as a way to augment data .", "entities": []}, {"text": "MethodsWeb News F14 F14 + MT 70.5 - 57.7 + MT w/ masking 71.1 0.6 57.8 0.1 + MT w/ MLM", "entities": [[20, 21, "DatasetName", "MLM"]]}, {"text": "71.9 1.4 58.3 0.6 Table 6 : Ablation study focusing on MLM .", "entities": [[11, 12, "DatasetName", "MLM"]]}, {"text": "All models were trained with one - stage optimization .", "entities": []}, {"text": "w/ masking indicates token masking without the corresponding loss function .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "To dig into this question , we conducted an ablation study by introducing a model with token masking but without the corresponding loss function ( denoted as + MT w/ masking ) .", "entities": [[22, 23, "MetricName", "loss"]]}, {"text": "We assume that this model was largely deprived of the power to mitigate CF while token masking still acted as a data augmenter .", "entities": []}, {"text": "Table 6 shows the results .", "entities": []}, {"text": "Not surprisingly , + MT w/ masking was beaten by + MT w/ MLM with large margins .", "entities": [[13, 14, "DatasetName", "MLM"]]}, {"text": "However , it did outperform + MT , and the gain was particularly large for the Web .", "entities": []}, {"text": "The fact that the contribution of the loss function was larger than that of token masking indicates that the improvements were mainly attributed to CF mitigation , but the contribution of token masking", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "1927alone should not be overlooked .", "entities": []}, {"text": "4.6 Case Studies To gain further insights , we compared ZAR results with English translations automatically generated by the corresponding MT model .", "entities": []}, {"text": "Figure 4 gives two examples .", "entities": []}, {"text": "It is no great surprise that the translation quality was not satisfactory because we did not fully optimize the model for it .", "entities": []}, {"text": "In the exmple of Figure 4(a ) , MT seems to have helped ZAR .", "entities": []}, {"text": "The omitted nominative argument of \u201c \u3042\u308a \u201d ( is ) was correctly translated as \u201c the school \u201d , and the model successfully identi\ufb01ed its antecedent \u201c \u5b66\u6821 \u201d ( school ) while the baseline failed .", "entities": []}, {"text": "Figure 4(b ) illustrates a limitation of the proposed approach .", "entities": []}, {"text": "The omitted nominative argument of the predicate , \u201c \u3067 \u201d ( be ) , points to \u201c \u5b9a\u5409 \u201d ( Sadakichi , the father of Jutaro ) .", "entities": []}, {"text": "Although the model correctly translated the zero pronoun as \u201c He \u201d , it failed in ZAR .", "entities": []}, {"text": "This is probably because not only \u201c \u5b9a\u5409(Sadakichi ) \u201d", "entities": []}, {"text": "but also \u201c \u9f8d\u99ac \u201d ( Ryoma ) and \u201c \u91cd\u592a\u90ce \u201d ( Jutaro ) can be referred to as \u201c He \u201d .", "entities": []}, {"text": "When disambiguation is not required to generate an overt pronoun , MT is not very helpful .", "entities": []}, {"text": "4.7 Note on Other Pretrained Models Due to space limitation , we have limited our focus to BERT , but for the sake of future practitioners , we would like to brie\ufb02y note that we extensively tested BART ( Lewis et al . , 2020 ) and its variants before switching to BERT .", "entities": [[17, 18, "MethodName", "BERT"], [37, 38, "MethodName", "BART"], [52, 53, "MethodName", "BERT"]]}, {"text": "Unlike BERT , BART is an encoder - decoder model pretrained on a monolingual corpus ( original ) or a non - parallel multilingual corpus ( mBART ) ( Liu et al . , 2020 ) .", "entities": [[1, 2, "MethodName", "BERT"], [3, 4, "MethodName", "BART"], [26, 27, "MethodName", "mBART"]]}, {"text": "Because MT requires the encoder - decoder architecture , maintaining the model architecture between pretraining and intermediate training looked promising to us .", "entities": []}, {"text": "We speci\ufb01cally tested ( 1 ) the of\ufb01cially distributed mBART model , ( 2 ) a BART model we pretrained on Japanese Wikipedia , and ( 3 ) an mBART model we pretrained on Japanese and English texts .", "entities": [[9, 10, "MethodName", "mBART"], [16, 17, "MethodName", "BART"], [29, 30, "MethodName", "mBART"]]}, {"text": "During \ufb01ne - tuning , we added the ZAR argument selection layer on top of either the encoder or the decoder .", "entities": []}, {"text": "Unfortunately , gains from MT intermediate training were marginal for these models .", "entities": []}, {"text": "A more serious problem was that they came close to but rarely outperformed the strong BERT baseline .", "entities": [[15, 16, "MethodName", "BERT"]]}, {"text": "We gave up identifying the cause of poorer performance because it was extremely hard to apply comparable experimental conditions to large pretrained models.5 Conclusion In this paper , we proposed to exploit parallel texts for Japanese zero anaphora resolution ( ZAR ) by inserting machine translation ( MT ) as an intermediate task between masked language model ( MLM ) pretraining and \ufb01ne - tuning on ZAR .", "entities": [[44, 46, "TaskName", "machine translation"], [58, 59, "DatasetName", "MLM"]]}, {"text": "Although previous studies reported negative results on the use of MT as an intermediate task , we demonstrated that it did work for Japanese ZAR .", "entities": []}, {"text": "Our analysis suggests that the intermediate training on MT simultaneously improved the model \u2019s ability to translate Japanese zero pronouns and the ZAR performance .", "entities": []}, {"text": "We bridged the gap between BERT - based ZAR and the encoder - decoder architecture for MT by initializing the encoder part of the MT model with a pretrained BERT .", "entities": [[5, 6, "MethodName", "BERT"], [29, 30, "MethodName", "BERT"]]}, {"text": "Previous studies focusing on MT reported mixed results on this approach , but again , we demonstrated its considerable positive impact on ZAR .", "entities": []}, {"text": "We found that incorporating the MLM objective into the intermediate training was particularly effective .", "entities": [[5, 6, "DatasetName", "MLM"]]}, {"text": "Our experimental results were consistent with the speculation that MLM mitigated catastrophic forgetting during intermediate training .", "entities": [[9, 10, "DatasetName", "MLM"]]}, {"text": "With neural transfer learning , we successfully revived the old idea that Japanese ZAR can bene\ufb01t from parallel texts ( Nakaiwa , 1999 ) .", "entities": [[2, 4, "TaskName", "transfer learning"]]}, {"text": "Thanks to the astonishing \ufb02exibility of neural networks , we would probably be able to connect ZAR to other tasks through transfer learning .", "entities": [[21, 23, "TaskName", "transfer learning"]]}, {"text": "Acknowledgments We thank the Yomiuri Shimbun for providing Japanese - English parallel texts .", "entities": []}, {"text": "We are grateful for Nobuhiro Ueda \u2019 help in setting up the baseline model .", "entities": []}, {"text": "We thank the anonymous reviewers for their insightful comments .", "entities": []}, {"text": "References Stephane Clinchant , Kweon Woo Jung , and Vassilina Nikoulina .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "On the use of BERT for neural machine translation .", "entities": [[4, 5, "MethodName", "BERT"], [7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 3rd Workshop on Neural Generation and Translation , pages 108\u2013117 , Hong Kong . Association for Computational Linguistics .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language", "entities": []}, {"text": "1928 OURSBASELINE(b )", "entities": []}, {"text": "\u6c5f\u6238\u306b\u5263\u8853\u4fee\u884c\u306b\u6765\u305f\u9f8d\u99ac\u306f\u3001\u5b9a\u5409\u306e\u9053\u5834\u306b\u5165\u9580\u3059\u308b\u3002Edo = DATswordplay training = DATcome .", "entities": []}, {"text": "PSTRyoma = TOP\u9053\u5834\u306e\u7d4c\u55b6\u306f\u606f\u5b50\u306e\u91cd\u592a\u90ce\u306b\u4efb\u305b\u3066\u3044\u308b\u3002\ud835\udf19!-NOM        \u9f8d\u99ac\u3084\u4f50\u90a3\u306e\u6210\u9577\u3092\u3058\u3063\u3068\u898b\u5b88\u308b\u5fc3\u512a\u3057\u3044\u7236\u89aa\u3067\u3082\u3042\u308b\u3002gym = GENRyoma , who came to the Edo period [ UNK ] 1603 - 1867 [ UNK ] to learn swords , entered a training school run by Sakakichi .", "entities": []}, {"text": "He is left to his son , Shigeta .", "entities": []}, {"text": "Heisa gentle father who watches the growth of Ryomaand Sana .[NULL](a )", "entities": []}, {"text": "\u7b2c\u4e03\u5341\u56db\u56de\u5168\u56fd\u9ad8\u6821\u30e9\u30b0\u30d3\u30fc\u30d5\u30c3\u30c8\u30dc\u30fc\u30eb\u5927\u4f1a\u6e96\u6c7a\u52dd\u306e\u4e94\u65e5\u3001\u8fd1\u9244\u82b1\u5712\u30e9\u30b0\u30d3\u30fc\u5834\u306e\u30b9\u30bf\u30f3\u30c9\u3067\u306f\u5927\u962a\u671d\u9bae\u9ad8\u7d1a\u5b66\u6821\u30e9\u30b0\u30d3\u30fc\u90e8\u54e1\u4e8c\u5341\u4e94\u4eba\u304c\u9752\u3044\u30a6\u30a4\u30f3\u30c9\u30d6\u30ec\u30fc\u30ab\u30fc\u59ff\u3067\u89b3\u6226\u3057\u305f\u3002\ud835\udf19!-NOM    \u540c\u30e9\u30b0\u30d3\u30fc\u5834\u304b\u3089\u308f\u305a\u304b\u4e00\u30fb\u4e94\u30ad\u30ed\u306e\u81f3\u8fd1\u8ddd\u96e2\u306b\u3042\u308a\u306a\u304c\u3089\u3001samerugby\ud835\udf19!-NOM1.5km = GENdistance = DATbe . GERonlyTwenty - two students ofOsaka Korean pro - Pyongyang Korean high school students watched the final at Kintetsu National HighSchool 's HanazonoStadium on Sunday .Although", "entities": []}, {"text": "the school isonly about five kilometers away from the stadium , \u300c \u5404\u7a2e\u5b66\u6821\u300d\u6271\u3044\u306e\u305f\u3081\u5927\u4f1a\u306b\u51fa\u5834\u3067\u304d\u306a\u304b\u3063\u305f\u304c\u3001\u4eca\u5e74\u3088\u3046\u3084\u304f\u82b1\u5712\u3078\u306e\u5922\u304c\u5b9f\u73fe\u3057\u305f\u3002it was unable to participate in the tournament because it was treated as a special - needs school.miscellaneousschooltreatment = GENtournament = DATenter = can .", "entities": []}, {"text": "NEG.PSTbecause.ofORD 74 CLFrugbynationalhigh.schoolfootballtournamentsemi final = GEN5dayKintetsu Hanazonorugby field = GENstands = LOC = TOPOsakaKoreahigh25 CLF = NOMbluewind breakerrugbyclub.memberappearance = INSwatch = do .", "entities": []}, {"text": "PST GOLDfield = ABLbutbutthis.yearfinallyHanazono = DAT = GENdream = NOMrealize = do .", "entities": []}, {"text": "PST \ud835\udf19!-NOMSadakichi = GENgym = DATenter .", "entities": []}, {"text": "NPSToperation = TOPson = GENJutaro = DATdelegate - GER = be .", "entities": [[8, 9, "MethodName", "GER"]]}, {"text": "NPSTRyoma = andSana = GENgrowth = ACCsteadilywatch . NPSTheartkindfatherbe .", "entities": []}, {"text": "GERalsobe . NPSTproximateschoolFigure 4 : Two examples of ZAR and MT .", "entities": []}, {"text": "Green , blue , and orange dotted lines represent the output of the baseline model , that of ours , and the gold standard , respectively .", "entities": []}, {"text": "English sentences are generated by the corresponding MT ( encoder - decoder ) model .", "entities": []}, {"text": "( a ) The example in which MT apparently helped ZAR .", "entities": []}, {"text": "The nominative zero pronoun of \u201c \u3042\u308a \u201d ( is ) was correctly translated as \u201c the school \u201d .", "entities": []}, {"text": "The model also succeeded in identifying its antecedent \u201c \u5b66 \u6821 \u201d ( school ) .", "entities": []}, {"text": "( b ) The example in which MT was not helpful .", "entities": []}, {"text": "The model successfully translated the nominative zero pronoun of the underlined predicate , \u201c \u3067 \u201d ( be ) , as \u201c He \u201d .", "entities": []}, {"text": "It misidenti\ufb01ed its antecedent , however .", "entities": []}, {"text": "Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tomomasa Furukawa , Toshiaki Nakazawa , Tomohide Shibata , Daisuke Kawahara , and Sadao Kuroahshi . 2017 .", "entities": []}, {"text": "Automatic construction of a pseudoannotatedzero anaphora corpus using a bilingual corpus .", "entities": []}, {"text": "In Proceedings of the Twenty - third Annual Meeting of the Association for Natural Language Processing .", "entities": []}, {"text": "( in Japanese ) .", "entities": []}, {"text": "Junliang Guo , Zhirui Zhang , Linli Xu , Hao - Ran Wei , Boxing Chen , and Enhong Chen . 2020 .", "entities": []}, {"text": "Incorporating BERT into parallel sequence decoding with adapters .", "entities": [[1, 2, "MethodName", "BERT"]]}, {"text": "In Advances in Neural Information Processing Systems , volume 33 , pages 10843\u201310854 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Masatsugu Hangyo , Daisuke Kawahara , and Sadao Kurohashi .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Building a diverse document leads corpus annotated with semantic relations .", "entities": []}, {"text": "In Proceedings of the 26th Paci\ufb01c Asia Conference on Language , Information , and Computation , pages 535 \u2013 544 , Bali , Indonesia .", "entities": []}, {"text": "Faculty of Computer Science , Universitas Indonesia .", "entities": []}, {"text": "Ryu Iida , Mamoru Komachi , Naoya Inoue , Kentaro Inui , and Yuji Matsumoto . 2017 .", "entities": []}, {"text": "NAIST text corpus :", "entities": []}, {"text": "Annotating predicate- argument and coreference relations in Japanese .", "entities": []}, {"text": "In Nancy Ide and James Puste - jovsky , editors , Handbook of Linguistic Annotation , pages 1177\u20131196 .", "entities": []}, {"text": "Springer , Dordrecht .", "entities": []}, {"text": "Ryu Iida , Kentaro Torisawa , Jong - Hoon Oh , Canasai Kruengkrai , and Julien Kloetzer .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Intrasentential subject zero anaphora resolution using multi - column convolutional neural network .", "entities": []}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1244 \u2013 1254 , Austin , Texas .", "entities": [[21, 22, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kenji Imamura and Eiichiro Sumita .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Recycling a pre - trained BERT encoder for neural machine translation .", "entities": [[5, 6, "MethodName", "BERT"], [9, 11, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 3rd Workshop on Neural Generation and Translation , pages 23\u201331 , Hong Kong . Association for Computational Linguistics .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Daisuke Kawahara , Sadao Kurohashi , and K\u00f4iti Hasida .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Construction of a Japanese relevance - tagged corpus .", "entities": []}, {"text": "In Proceedings of the Third International Conference on Language Resources and Evaluation ( LREC\u201902 ) , Las Palmas , Canary Islands - Spain .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Youngtae Kim , Dongyul Ra , and Soojong Lim . 2021 .", "entities": []}, {"text": "Zero - anaphora resolution in Korean based on deep language representation model : BERT .", "entities": [[13, 14, "MethodName", "BERT"]]}, {"text": "ETRI Journal , 43(2):299\u2013312 .", "entities": []}, {"text": "1929Diederik P. Kingma and Jimmy Ba . 2017 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv:1412.6980 .", "entities": []}, {"text": "Ryuto Konno , Shun Kiyono , Yuichiroh Matsubayashi , Hiroki Ouchi , and Kentaro Inui .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Pseudo zero pronoun resolution improves zero anaphora resolution .", "entities": []}, {"text": "arXiv:2104.07425 .", "entities": []}, {"text": "Ryuto Konno , Yuichiroh Matsubayashi , Shun Kiyono , Hiroki Ouchi , Ryo Takahashi , and Kentaro Inui .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "An empirical study of contextual data augmentation for Japanese zero anaphora resolution .", "entities": [[5, 7, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics , pages 4956\u20134968 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Taku Kudo and John Richardson .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SentencePiece : A simple and language independent subword tokenizer and detokenizer for neural text processing .", "entities": [[0, 1, "MethodName", "SentencePiece"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 66\u201371 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Taku Kudo , Kaoru Yamamoto , and Yuji Matsumoto .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "Applying conditional random \ufb01elds to Japanese morphological analysis .", "entities": [[6, 8, "TaskName", "morphological analysis"]]}, {"text": "In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing , pages 230\u2013237 , Barcelona , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shuhei Kurita , Daisuke Kawahara , and Sadao Kurohashi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural adversarial training for semisupervised Japanese predicate - argument structure analysis .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 474\u2013484 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "BART :", "entities": [[0, 1, "MethodName", "BART"]]}, {"text": "Denoising sequence - to - sequence pretraining for natural language generation , translation , and comprehension .", "entities": [[0, 1, "TaskName", "Denoising"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871\u20137880 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ting Liu , Yiming Cui , Qingyu Yin , Wei - Nan Zhang , Shijin Wang , and Guoping Hu . 2017 .", "entities": []}, {"text": "Generating and exploiting large - scale pseudo training data for zero pronoun resolution .", "entities": []}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 102\u2013111 , Vancouver , Canada . Association for Computational Linguistics .", "entities": []}, {"text": "Yinhan Liu , Jiatao Gu , Naman Goyal , Xian Li , Sergey Edunov , Marjan Ghazvininejad , Mike Lewis , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multilingual denoisingpre - training for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "Transactions of the Association for Computational Linguistics , 8:726\u2013742 .", "entities": []}, {"text": "Kojiro Nabeshima and Michael N. Brooks .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Techniques of English - Japanese translation .", "entities": []}, {"text": "Kurosio Publishers .", "entities": []}, {"text": "( in Japanese ) .", "entities": []}, {"text": "Masaaki Nagata and Makoto Morishita .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A test set for discourse translation from Japanese to English .", "entities": []}, {"text": "In Proceedings of the 12th Language Resources and Evaluation Conference , pages 3704 \u2013 3709 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Hiromi Nakaiwa .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "Automatic extraction of rules for anaphora resolution of Japanese zero pronouns in Japanese - English machine translation from aligned sentence pairs .", "entities": [[15, 17, "TaskName", "machine translation"]]}, {"text": "Machine Translation , 14(14):247 \u2013 279 .", "entities": [[0, 2, "TaskName", "Machine Translation"]]}, {"text": "Hiroki Ouchi , Hiroyuki Shindo , and Yuji Matsumoto . 2017 .", "entities": []}, {"text": "Neural modeling of multi - predicate interactions for Japanese predicate argument structure analysis .", "entities": []}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1591\u20131600 , Vancouver , Canada .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Arum Park , Seunghee Lim , and Munpyo Hong . 2015 .", "entities": []}, {"text": "Zero object resolution in Korean .", "entities": []}, {"text": "In Proceedings of the 29th Paci\ufb01c Asia Conference on Language , Information and Computation , pages 439\u2013448 , Shanghai , China .", "entities": []}, {"text": "Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 2227\u20132237 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jason Phang , Thibault F\u00e9vry , and Samuel R. Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Sentence encoders on STILTs : Supplementary training on intermediate labeled - data tasks .", "entities": []}, {"text": "arXiv:1811.01088 .", "entities": []}, {"text": "Yada Pruksachatkun , Jason Phang , Haokun Liu , Phu Mon Htut , Xiaoyi Zhang , Richard Yuanzhe Pang , Clara Vania , Katharina Kann , and Samuel R. Bowman . 2020 .", "entities": []}, {"text": "Intermediate - task transfer learning with pretrained language models : When and why does it work ?", "entities": [[3, 5, "TaskName", "transfer learning"], [6, 9, "TaskName", "pretrained language models"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5231\u20135247 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ryohei Sasano , Daisuke Kawahara , and Sadao Kurohashi . 2008 .", "entities": []}, {"text": "A fully - lexicalized probabilistic model for Japanese zero anaphora resolution .", "entities": []}, {"text": "In Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008 ) , pages 769 \u2013 776 , Manchester , UK .", "entities": []}, {"text": "Coling 2008 Organizing Committee .", "entities": []}, {"text": "1930Ryohei Sasano and Sadao Kurohashi . 2011 .", "entities": []}, {"text": "A discriminative approach to Japanese zero anaphora resolution with large - scale lexicalized case frames .", "entities": []}, {"text": "In Proceedings of 5th International Joint Conference on Natural Language Processing , pages 758\u2013766 , Chiang Mai , Thailand .", "entities": []}, {"text": "Asian Federation of Natural Language Processing .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Tomohide Shibata and Sadao Kurohashi .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Entitycentric joint modeling of Japanese coreference resolution and predicate argument structure analysis .", "entities": [[5, 7, "TaskName", "coreference resolution"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 579\u2013589 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Sho Shimazu , Sho Takase , Toshiaki Nakazawa , and Naoaki Okazaki .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Evaluation dataset for zero pronoun in Japanese to English translation .", "entities": []}, {"text": "In Proceedings of the 12th Language Resources and Evaluation Conference , pages 3630\u20133634 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Linfeng Song , Kun Xu , Yue Zhang , Jianshu Chen , and Dong Yu . 2020 .", "entities": []}, {"text": "ZPR2 : Joint zero pronoun recovery and resolution using multi - task learning and BERT .", "entities": [[9, 13, "TaskName", "multi - task learning"], [14, 15, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5429\u20135434 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nobuhiro Ueda , Daisuke Kawahara , and Sadao Kurohashi . 2020 .", "entities": []}, {"text": "BERT - based cohesion analysis of Japanese texts .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics , pages 1323\u20131333 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Tu Vu , Tong Wang , Tsendsuren Munkhdalai , Alessandro Sordoni , Adam Trischler , Andrew MattarellaMicke , Subhransu Maji , and Mohit Iyyer .", "entities": [[12, 13, "MethodName", "Adam"]]}, {"text": "2020 .", "entities": []}, {"text": "Exploring and predicting transferability across NLP tasks .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 7882\u20137926 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alex Wang , Jan Hula , Patrick Xia , Raghavendra Pappagari , R. Thomas McCoy , Roma Patel , Najoung Kim , Ian Tenney , Yinghui Huang , Katherin Yu , Shuning Jin , Berlin Chen , Benjamin Van Durme , Edouard Grave , Ellie Pavlick , and Samuel R. Bowman . 2019a .", "entities": []}, {"text": "Can you tell me how to get past sesame street ?", "entities": []}, {"text": "sentence - level pretraining beyond language modeling .", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4465\u20134476 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Longyue Wang , Zhaopeng Tu , Xing Wang , and Shuming Shi . 2019b .", "entities": []}, {"text": "One model to learn both : Zero pronoun prediction and translation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 921\u2013930 , Hong Kong , China . Association for Computational Linguistics .", "entities": []}, {"text": "Longyue Wang , Zhaopeng Tu , Andy Way , and Qun Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning to jointly translate and predict dropped pronouns with a shared reconstruction mechanism .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2997\u20133002 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Longyue Wang , Zhaopeng Tu , Xiaojun Zhang , Hang Li , Andy Way , and Qun Liu . 2016 .", "entities": []}, {"text": "A novel approach to dropped pronoun translation .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 983\u2013993 , San Diego , California . Association for Computational Linguistics .", "entities": []}, {"text": "Souta Yamashiro , Hitoshi Nishikawa , and Takenobu Tokunaga .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural Japanese zero anaphora resolution using smoothed large - scale case frames with word embedding .", "entities": []}, {"text": "In Proceedings of the 32nd Paci\ufb01c Asia Conference on Language , Information and Computation , Hong Kong . Association for Computational Linguistics .", "entities": []}, {"text": "Qingyu Yin , Yu Zhang , Weinan Zhang , and Ting Liu . 2017 .", "entities": []}, {"text": "Chinese zero pronoun resolution with deep memory network .", "entities": [[6, 8, "MethodName", "memory network"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 1309\u20131318 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Jia - Rui Zhang , Hongzheng Li , Shumin Shi , Heyan Huang , Yue Hu , and Xiangpeng Wei .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Dynamic attention aggregation with bert for neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In IJCNN , pages 1\u20138 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Jinhua Zhu , Yingce Xia , Lijun Wu , Di He , Tao Qin , Wengang Zhou , Houqiang Li , and Tieyan Liu . 2020 .", "entities": []}, {"text": "Incorporating BERT into neural machine translation .", "entities": [[1, 2, "MethodName", "BERT"], [4, 6, "TaskName", "machine translation"]]}, {"text": "InInternational Conference on Learning Representations .", "entities": []}, {"text": "1931A Hyper - parameters for MT Options Values Optimizer Adam Adam params \f 1=0.9 , \f 2= 0:98 Optimizer eps 1\u000210\u00006 Weight decay 0.01 Epochs 50 First - stage epochs * 15 Batch size Approx .", "entities": [[8, 9, "HyperparameterName", "Optimizer"], [9, 10, "MethodName", "Adam"], [10, 11, "MethodName", "Adam"], [11, 12, "MetricName", "params"], [18, 19, "HyperparameterName", "Optimizer"], [19, 20, "HyperparameterName", "eps"], [21, 23, "MethodName", "Weight decay"], [32, 34, "HyperparameterName", "Batch size"]]}, {"text": "500 Learning rate 3:0\u000210\u00004 Warm - up 5 epochs Loss function Lable - smoothed cross entropy Label smoothing 0.1 Dropout ( BERT & Dec. ) 0.1 LR Scheduler polynomial decay Table 7 : Hyper - parameters for MT .", "entities": [[1, 3, "HyperparameterName", "Learning rate"], [16, 18, "MethodName", "Label smoothing"], [19, 20, "MethodName", "Dropout"], [21, 22, "MethodName", "BERT"]]}, {"text": "* :", "entities": []}, {"text": "For two - stage optimization .", "entities": []}, {"text": "B Hyper - parameters for ZAR Options Values", "entities": []}, {"text": "Optimizer AdamW", "entities": [[0, 1, "HyperparameterName", "Optimizer"], [1, 2, "MethodName", "AdamW"]]}, {"text": "Optimizer eps 1\u000210\u00008 Weight decay 0.01 Epochs 4 Batch size 8 Learning rate 5:0\u000210\u00005 Warmup proportion 0.1 Loss function Cross entropy loss Dropout ( BERT layer )", "entities": [[0, 1, "HyperparameterName", "Optimizer"], [1, 2, "HyperparameterName", "eps"], [3, 5, "MethodName", "Weight decay"], [8, 10, "HyperparameterName", "Batch size"], [11, 13, "HyperparameterName", "Learning rate"], [21, 22, "MetricName", "loss"], [22, 23, "MethodName", "Dropout"], [24, 25, "MethodName", "BERT"]]}, {"text": "0.1 Dropout ( output layer ) 0.0 LR Scheduler linear_schedule_with_warmup11 Table 8 : Hyper - parameters for ZAR .", "entities": [[1, 2, "MethodName", "Dropout"]]}, {"text": "Although we followed Ueda et al .", "entities": []}, {"text": "( 2020 ) with respect to hyper - parameter settings , there was one exception .", "entities": []}, {"text": "Verbal predicate analysis is conventionally divided into three types : overt , covert , and zero .", "entities": []}, {"text": "While Ueda et al .", "entities": []}, {"text": "( 2020 ) excluded the easiestovert type from training , we targeted all the three types because we found slight performance improvements .", "entities": []}, {"text": "The overt type covers situations 11https://github.com/huggingface/ transformers / blob / v2.10.0 / src/ transformers / optimization.py#L47where the predicate and the argument have a dependency relation and their relation is marked explicitly with a case marker .", "entities": []}, {"text": "C Results on Validation Sets Tables 9 and 10 show the performance on the validation sets .", "entities": []}, {"text": "D Relationship between Zero Anaphora Resolution and Zero Pronoun Translatoin Figure 5 shows the relationship between zero anaphora resolution ( ZAR ) and zero pronoun translation ( ZPT ) in the course on intermediate training on MT .", "entities": []}, {"text": "We observed almost steady increase in ZPT accuracies and ZAR F1 scores until around the 30th epoch .", "entities": [[10, 11, "MetricName", "F1"]]}, {"text": "1932MethodsWeb all intra inter exophora + MLM 62.9\u00060:812 56.8\u00061:03 53.5\u00060:827 68.5\u00060:962 + MT", "entities": [[6, 7, "DatasetName", "MLM"]]}, {"text": "62.9\u00060:668 56.6\u00060:830 51.4\u00060:875 68.9\u00060:825 + MT w/ MLM 64.1\u00060:475 58.4\u00060:883", "entities": [[7, 8, "DatasetName", "MLM"]]}, {"text": "52.9\u00062:01 69.8\u00060:993 + MT ( Two - stage ) 63.6\u00060:437 57.5\u00061:36 52.7\u00062:15 69.4\u00060:514 + MT w/ MLM ( Two - stage ) 63.9\u00060:488 57.4\u00061:07 53.1\u00061:56 69.9\u00060:831", "entities": [[16, 17, "DatasetName", "MLM"]]}, {"text": "Table 9 : Breakdown of the F1 scores with standard deviations on the Web validation set .", "entities": [[6, 7, "MetricName", "F1"]]}, {"text": "Boldfaced scores indicate the best results in the corresponding categories .", "entities": []}, {"text": "MethodsNews all intra inter exophora", "entities": []}, {"text": "+ MLM 57.8\u00060:586 64.1\u00060:80 48.1\u00060:965 56.3\u00061:81", "entities": [[1, 2, "DatasetName", "MLM"]]}, {"text": "+ MT 57.0\u00060:710 62.9\u00060:819 46.0\u00061:23 58.3\u00061:80 + MT w/ MLM 58.7\u00060:438 65.5\u00061:08 48.3\u00060:843 57.6\u00061:48 + MT ( Two - stage ) 58.4\u00060:579 64.1\u00060:714 48.8\u00061:57 57.9\u00061:20 + MT w/ MLM ( Two - stage ) 58.6\u00060:381 64.9\u00060:543 48.7\u00060:767 58.0\u00061:16 Table 10 : Breakdown of the F1 scores with standard deviations on the News validation set .", "entities": [[9, 10, "DatasetName", "MLM"], [28, 29, "DatasetName", "MLM"], [44, 45, "MetricName", "F1"]]}, {"text": "Boldfaced scores indicate the best results in the corresponding categories .", "entities": []}, {"text": "1933 5 10 15 20 25 30 35 40 45 # of epochs on MT0.6400.6420.6440.6460.6480.6500.6520.654ZAR F1 score 0.800.820.840.860.88 Zero pronoun test acc.(a )", "entities": [[15, 17, "MetricName", "F1 score"]]}, {"text": "Relationship between ZAR and ZPT for intra - sentential on the Web test set .", "entities": []}, {"text": "5 10 15 20 25 30 35 40 45 # of epochs on MT0.6300.6350.6400.6450.650ZAR F1 score 0.800.820.840.860.88 Zero pronoun test acc .", "entities": [[14, 16, "MetricName", "F1 score"], [20, 21, "MetricName", "acc"]]}, {"text": "( b ) Relationship between ZAR and ZPT for intra - sentential on the News test set .", "entities": []}, {"text": "Figure 5 : Relationships between ZAR and ZPT .", "entities": []}, {"text": "1934 5 10 15 20 25 30 35 40 45 # of epochs on MT0.6380.6400.6420.6440.6460.6480.6500.652ZAR F1 score 0.640.660.680.700.720.74 Zero pronoun test acc.(c ) Relationship between ZAR and ZPT for inter - sentential on the Web test set .", "entities": [[15, 17, "MetricName", "F1 score"]]}, {"text": "5 10 15 20 25 30 35 40 45 # of epochs on MT0.49500.49750.50000.50250.50500.50750.51000.5125ZAR F1 score 0.640.660.680.700.720.74 Zero pronoun test acc .", "entities": [[14, 16, "MetricName", "F1 score"], [20, 21, "MetricName", "acc"]]}, {"text": "( d ) Relationship between ZAR and ZPT for inter - sentential on the News test set .", "entities": []}, {"text": "Figure 5 : Relationships between ZAR and ZPT .", "entities": []}]