[{"text": "Proceedings of the Workshop on Machine Reading for Question Answering , pages 89\u201397 Melbourne , Australia , July 19 , 2018 .", "entities": [[8, 10, "TaskName", "Question Answering"]]}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics89Comparative Analysis of Neural QA models on SQuAD Soumya Wadhwa Khyathi Raghavi Chandu Eric Nyberg Language Technologies Institute , Carnegie Mellon University fsoumyaw , kchandu , en09 g@andrew.cmu.edu", "entities": [[11, 12, "DatasetName", "SQuAD"]]}, {"text": "Abstract The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language .", "entities": [[4, 6, "TaskName", "Question Answering"]]}, {"text": "Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks .", "entities": [[21, 23, "TaskName", "information retrieval"]]}, {"text": "Different components in these neural architectures are intended to tackle different challenges .", "entities": []}, {"text": "As a \ufb01rst step towards achieving generalization across multiple domains , we attempt to understand and compare the peculiarities of existing end - to - end neural models on the Stanford Question Answering Dataset ( SQuAD ) by performing quantitative as well as qualitative analysis of the results attained by each of them .", "entities": [[29, 34, "DatasetName", "the Stanford Question Answering Dataset"], [35, 36, "DatasetName", "SQuAD"]]}, {"text": "We observed that prediction errors re\ufb02ect certain model - speci\ufb01c biases , which we further discuss in this paper .", "entities": []}, {"text": "1 Introduction Machine Reading is a task in which a model reads a piece of text and attempts to formally represent it or performs a downstream task like Question Answering ( QA ) .", "entities": [[28, 30, "TaskName", "Question Answering"]]}, {"text": "Neural approaches to the latter have gained a lot of prominence especially owing to the recent spur in developing and publicly releasing large datasets on Machine Reading and Comprehension ( MRC ) .", "entities": []}, {"text": "These datasets are created from different underlying sources such as web resources in MS MARCO ( Nguyen et al . , 2016 ) ; trivia and web in QUASAR - S and QUASAR - T ( Dhingra et al . , 2017 ) , SearchQA ( Dunn et al . , 2017 ) , TriviaQA ( Joshi et al . , 2017 ) ; news articles in CNN / Daily Mail ( Chen et al . ) , NewsQA ( Trischler et al . , 2016 ) and stories in NarrativeQA ( Ko \u02c7cisk`yet al . , 2017 ) .", "entities": [[13, 15, "DatasetName", "MS MARCO"], [28, 31, "DatasetName", "QUASAR - S"], [32, 35, "DatasetName", "QUASAR - T"], [44, 45, "DatasetName", "SearchQA"], [54, 55, "DatasetName", "TriviaQA"], [67, 71, "DatasetName", "CNN / Daily Mail"], [78, 79, "DatasetName", "NewsQA"], [90, 91, "DatasetName", "NarrativeQA"]]}, {"text": "Another common source is large unstructured text documents from Wikipedia such as in SQuAD ( Rajpurkar et al . , 2016 ) , WikiReading ( Hewlett et al . , 2016 ) and WikiHop ( Welbl et al . , 2017 ) .", "entities": [[13, 14, "DatasetName", "SQuAD"], [23, 24, "DatasetName", "WikiReading"], [33, 34, "DatasetName", "WikiHop"]]}, {"text": "These different sources implicitly affect the nature and properties of questions and answers in these datasets .", "entities": []}, {"text": "Based on the dataset , certain neural models capitalize on these biases while others are unable to .", "entities": []}, {"text": "The ability to generalize across different sources and domains is a desirable characteristic for any machine reading system .", "entities": []}, {"text": "Evaluating and analyzing systems on QA tasks can lead to insights for advancements in machine reading and natural language understanding , and Pe \u02dcnas et al .", "entities": [[17, 20, "TaskName", "natural language understanding"]]}, {"text": "( 2011 ) have also previously worked on this .", "entities": []}, {"text": "One of the \ufb01rst large MRC datasets ( over 100k QA pairs ) is the Stanford Question Answering Dataset ( SQuAD ) ( Rajpurkar et al . , 2016 ) .", "entities": [[14, 19, "DatasetName", "the Stanford Question Answering Dataset"], [20, 21, "DatasetName", "SQuAD"]]}, {"text": "For its collection , different sets of crowd - workers formulated questions and answers using passages obtained from\u0018500 Wikipedia articles .", "entities": []}, {"text": "The answer to each question is a span in the given passage , and many effective neural QA models have been developed for this dataset .", "entities": []}, {"text": "Our main focus in this work is to perform comparative subjective and empirical analysis of errors in answer predictions by four top performing models on the SQuAD leaderboard1 .", "entities": [[26, 27, "DatasetName", "SQuAD"]]}, {"text": "We focused on Bi - Directional Attention Flow ( BiDAF ) ( Seo et al . , 2016 ) , Gated Self - Matching Networks ( R - Net ) ( Wang et al . , 2017 ) , Document Reader ( DrQA ) ( Chen et al . , 2017 ) , MultiParagraph Reading Comprehension ( DocQA ) ( Clark and Gardner , 2017 ) , and the Logistic Regression baseline model ( Rajpurkar et al . , 2016 )", "entities": [[54, 56, "TaskName", "Reading Comprehension"], [69, 71, "MethodName", "Logistic Regression"]]}, {"text": "We mainly choose these models since they have comparable high performance on the evaluation metrics and it is easy to replicate their results due to availability of open source implementations .", "entities": []}, {"text": "1https://rajpurkar.github.io/ SQuAD - explorer/", "entities": [[1, 2, "DatasetName", "SQuAD"]]}, {"text": "90While we limit ourselves to in - domain analysis of the performance of these models on SQuAD in this paper , similar principles can be used to extend this work to study biases of combinations of different models on different datasets and thereby understand the generalization capabilities of these neural architectures .", "entities": [[16, 17, "DatasetName", "SQuAD"]]}, {"text": "The organization of the paper is as follows .", "entities": []}, {"text": "Section 2 gives a comprehensive overview of the models that are compared in further sections .", "entities": []}, {"text": "Section 3 describes the different experiments we conducted , and discusses our observations .", "entities": []}, {"text": "In Section 4 , we summarize our main conclusions from this work and describe our vision for the future .", "entities": []}, {"text": "2 Relevant Neural Models We present a brief overview of the models which we considered for our analysis in this section .", "entities": []}, {"text": "Bi - Directional Attention Flow ( BiDAF ): This model , proposed by Seo et al . ( 2016 ) , is a hierarchical multi - stage end - to - end neural network which takes inputs of different granularity ( character , word and phrase ) to obtain a query - aware context representation using memory - less contextto - query ( C2Q ) and query - to - context ( Q2C ) attention .", "entities": []}, {"text": "This representation can then be used for different \ufb01nal tasks .", "entities": []}, {"text": "Many versions of this model ( with different types of input features ) exist on the SQuAD leaderboard , but the basic architecture2(which we use for our experiments in this paper ) contains character , word and phrase embedding layers , followed by an attention \ufb02ow layer , a modeling layer and an output layer .", "entities": [[16, 17, "DatasetName", "SQuAD"]]}, {"text": "Gated Self - Matching Networks ( R - Net ): This model , proposed by Wang et al . ( 2017 ) , is a multilayer end - to - end neural network whose novelty lies in the use of a gated attention mechanism so as to give different levels of importance to different passage parts .", "entities": []}, {"text": "It also uses self - matching attention for the context to aggregate evidence from the entire passage to re\ufb01ne the query - aware context representation obtained .", "entities": []}, {"text": "The architecture contains character and word embedding layers , followed by question - passage encoding and matching layers , a passage self - matching layer and an output layer .", "entities": []}, {"text": "The implementation we used3had some minor changes for increased ef\ufb01ciency .", "entities": []}, {"text": "2https://allenai.github.io/bi-att-flow/ 3https://github.com/HKUST-KnowComp/R-NetDocument Reader ( DrQA ): This model , proposed by Chen et al . ( 2017 ) , focuses on answering open - domain factoid questions using Wikipedia , but also performs well on SQuAD ( skipping the document retrieval stage ) .", "entities": [[35, 36, "DatasetName", "SQuAD"]]}, {"text": "Its implementation4 has paragraph and question encoding layers , and an output layer .", "entities": []}, {"text": "The paragraph encoding is computed by representing each context as a sequence of feature vectors derived from tokens : word embedding , exact match with question word , POS / NER / TF and aligned question embedding , and passing these as inputs to a recurrent neural network .", "entities": [[22, 24, "MetricName", "exact match"], [30, 31, "TaskName", "NER"]]}, {"text": "The question encoding is obtained by using word embeddings as inputs to a recurrent neural network .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "Multi - Paragraph Reading Comprehension ( DocQA ): This model , proposed by Clark and Gardner ( 2017 ) , aims to answer questions based on entire documents ( multiple paras ) rather than speci\ufb01c paragraphs , but also gives good results for SQuAD ( considering the given paragraph as the document ) .", "entities": [[3, 5, "TaskName", "Reading Comprehension"], [43, 44, "DatasetName", "SQuAD"]]}, {"text": "The implementation5contains input , embedding ( character and word - level ) , pre - processing ( shared bidirectional GRU between question and passage ) , attention ( similar to BiDAF ) , self - attention ( residual ) and output ( bidirectional GRU and linear scoring ) layers .", "entities": [[18, 20, "MethodName", "bidirectional GRU"], [42, 44, "MethodName", "bidirectional GRU"]]}, {"text": "Logistic Regression ( LR ): This model was proposed as a baseline in the SQuAD dataset paper ( Rajpurkar et al . , 2016 ) and uses features based on n - gram frequencies , lengths , part - of - speech tags , constituency and dependency parse trees of questions and passages as inputs to a logistic regression classi\ufb01er6to predict whether each constituent span is an answer or not .", "entities": [[0, 2, "MethodName", "Logistic Regression"], [14, 15, "DatasetName", "SQuAD"], [37, 40, "DatasetName", "part - of"], [57, 59, "MethodName", "logistic regression"]]}, {"text": "3 Experiments and Discussion We trained the aforementioned end - to - end neural models and compare their performance on the SQuAD development set which contains 10,570 question - answer pairs based on Wikipedia articles .", "entities": [[21, 22, "DatasetName", "SQuAD"]]}, {"text": "3.1 Quantitative Analysis To perform a systematic comparison of errors across different models , we investigate the predictions based on the following criteria .", "entities": []}, {"text": "4https://github.com/facebookresearch/DrQA 5https://github.com/allenai/document-qa 6https://worksheets.codalab.org/worksheets/ 0xd53d03a48ef64b329c16b9baf0f99b0c/", "entities": []}, {"text": "913.1.1 Span - Level Performance The span - level performance is measured typically by Exact Match ( EM ) and F1 metrics which are reported with respect to the ground truth answer spans .", "entities": [[14, 16, "MetricName", "Exact Match"], [17, 18, "MetricName", "EM"], [20, 21, "MetricName", "F1"]]}, {"text": "These results are summarized in Table 1 .", "entities": []}, {"text": "The DocQA model gives the best overall performance which aligns well with our expectation , owing to the usage of and improvements in the prior mechanisms introduced in BiDAF and R - Net .", "entities": []}, {"text": "Model BiDAF R - Net DrQA DocQA LR EM ( % ) 67.67 70.12 66.00 71.60 40.14 F1 ( % ) 77.31 78.94 76.28 80.78 50.98 Correct Sentence ( % ) 91.05 92.37 92.40 93.77 83.30 Table 1 : Span and Sentence Level Performance 3.1.2 Sentence - Level Performance To investigate trends at different granularities , we also measure sentence retrieval performance .", "entities": [[8, 9, "MetricName", "EM"], [17, 18, "MetricName", "F1"]]}, {"text": "The context given for each question - answer pair is split into sentences using the NLTK sentence tokenizer7 , and the sentence - level accuracy of each of the models is computed ( Table 1 ) .", "entities": [[24, 25, "MetricName", "accuracy"]]}, {"text": "Since the default sentence tokenizer for English in NLTK is pre - trained on Penn Treebank data which contains formal language ( news articles ) , we expect it to perform reasonably well on Wikipedia articles too .", "entities": [[14, 16, "DatasetName", "Penn Treebank"]]}, {"text": "We observe that all the models have high sentencelevel accuracy , with DocQA outperforming the other models with respect to this metric as well .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "Interestingly , DrQA performs better on sentence retrieval accuracy than both BiDAF and R - Net , but has a worse span - level exact match score , which is probably because of the rich feature vector representation of the passage due to the model \u2019s focus on open domain QA ( and hence retrieval ) .", "entities": [[8, 9, "MetricName", "accuracy"], [24, 26, "MetricName", "exact match"]]}, {"text": "But , none of these neural models have near - perfect ability to identify the correct sentence , and \u001890 % accuracy indicates that even if we have a perfect answer selection method , this is the best EM score we can achieve .", "entities": [[21, 22, "MetricName", "accuracy"], [30, 32, "TaskName", "answer selection"], [38, 39, "MetricName", "EM"]]}, {"text": "However , incorrect span identi\ufb01cation contributes more to errors in prediction for all the models , as seen from the disparity between the sentence - level accuracies and the \ufb01nal spanlevel exact match score values .", "entities": [[31, 33, "MetricName", "exact match"]]}, {"text": "3.1.3 Passage Length Distribution We analyze the impact of passage length on errors , since this can be an important factor in determining the dif\ufb01culty of understanding the passage .", "entities": []}, {"text": "As seen in Figure 1 , DocQA performs the best on 7http://www.nltk.org/api/nltk.tokenize.htmlshorter passages , while R - Net and BiDAF are observed to be better for longer passages .", "entities": []}, {"text": "However , there are no systematic error patterns and overall error rates , surprisingly , are not much higher for longer passages .", "entities": []}, {"text": "This means that predictions on long passages are almost as good as on short ( presumably easier to understand ) passages .", "entities": []}, {"text": "3.1.4 Question Length Distribution", "entities": []}, {"text": "We also do a similar error analysis for questions of different lengths .", "entities": []}, {"text": "Since there are very few questions which have length greater than 30 , the estimate for range 30 - 34 is not very reliable .", "entities": []}, {"text": "In Figure 2 , we observe that the error rate \ufb01rst decreases and then increases for BiDAF , DrQA and DocQA .", "entities": []}, {"text": "A plausible explanation for this is that shorter questions contain insuf\ufb01cient information in order to be able to select the correct answer span and can hence be confusing , but it also becomes dif\ufb01cult for end - to - end neural models to learn a good representation when the question becomes longer and syntactically more complicated .", "entities": []}, {"text": "However , R - Net has an irregular trend with respect to question length , which is dif\ufb01cult to explain .", "entities": []}, {"text": "3.1.5 Answer Length Distribution For answers of varying lengths , the error rates are shown in Figure 3 .", "entities": []}, {"text": "Again , estimates for answers with length > 16 are not very reliable since data is sparse for high answer lengths .", "entities": []}, {"text": "Here , we observe an increasing trend initially and then a slight decrease ( bell shape ) .", "entities": []}, {"text": "This conforms to the hypothesis that shorter answers are easier to predict than longer answers , but only up to a certain answer length ( observed to be around 7 for most models ) .", "entities": []}, {"text": "The slightly better performance for very long answers is likely due to such answers having a higher chance of being ( almost ) entire sentences with simpler questions being asked about them .", "entities": []}, {"text": "3.1.6 Error Overlap In Table 2 , we analyze the number of erroneous predictions which overlap for different pairs of models , i.e. , which belong to the intersection of the sets of incorrect answers generated by models in each ( row , column ) pair .", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "Thus , the values in the table represent a symmetric matrix with diagonal elements indicating the number of errors which each model commits .", "entities": []}, {"text": "This analysis can be useful while determining suitable models for creating meta ensembles since a low incorrect answer overlap indicates that the combined predictive power", "entities": []}, {"text": "92 Figure 1 : Percentage of total QA pairs for each range of passage lengths which have incorrect predictions by different models Figure 2 : Percentage of total QA pairs for each range of question lengths which have incorrect predictions by different models of the pair of models under consideration is high .", "entities": []}, {"text": "We observe that most overlap values are in therange 20 - 25 % indicating that an ensemble might give considerably better performance than individ-", "entities": []}, {"text": "93 Figure 3 : Percentage of total QA pairs for each answer length which have incorrect predictions by different models ual models .", "entities": []}, {"text": "DocQA paired with other models generates low values , as expected , but the least value is observed for the DocQA - DrQA pair probably because they both use very different feature representations and architectures , and hence generate diverse outputs .", "entities": []}, {"text": "Note that DrQA is not the second best performing model ( among the ones we analyzed ) when considered independently , but might add more value to an ensemble because of the observed answer overlap trends .", "entities": []}, {"text": "Model BiDAF R - Net DrQA DocQA LR BiDAF 32.33 21.97 22.56 21.22 26.58 R - Net 21.97 29.88 22.06 21.35 24.99 DrQA 22.56 22.06 34.00 20.95 27.49 DocQA 21.22 21.35 20.95 28.40 23.59 LR 26.58 24.99 27.49 23.59 59.86 Table 2 : Incorrect Answer Overlap ( % )", "entities": []}, {"text": "One way in which this analysis can help in exploring ensemble - based methods is that instead of trying all possible combinations of models , we can adopt a greedy approach based on the incorrect answer overlap metric to decide which model to add to the ensemble ( and only if it leads to a statistically signi\ufb01cant difference in this overlap ) .", "entities": []}, {"text": "After determining an approximately optimal set of models which such an ensemble should be composedof , each of these models can be trained independently followed by multi - label classi\ufb01cation ( to select one of the generated answers ) using techniques like logistic regression , a feed - forward neural network or a recurrent or convolutional neural network with input features based on the question , the passage and their token overlap .", "entities": [[42, 44, "MethodName", "logistic regression"]]}, {"text": "The entire network can also be trained end - to - end .", "entities": []}, {"text": "Also , all 5 models combined have an error overlap of 13.68 % , i.e. , if we had a mechanism to perfectly choose between these models , we would get an Exact Match score of 86.32 % .", "entities": [[32, 34, "MetricName", "Exact Match"]]}, {"text": "This indicates that future work based on ensembling different neural models can give promising results and is worth exploring .", "entities": []}, {"text": "An example of a passage - question - answer that all of the models get wrong is : Passage : The University of Warsaw was established in 1816 , when the partitions of Poland separated Warsaw from the oldest and most in\ufb02uential Polish academic center , in Krakow .", "entities": []}, {"text": "Warsaw University of Technology is the second academic school of technology in the country , and one of the largest in East - Central Europe , employing 2,000 professors .", "entities": []}, {"text": "Other institutions for higher education include the Medical University of Warsaw , the largest medical school", "entities": []}, {"text": "94 in Poland and one of the most prestigious , the National Defence University , highest military academic institution in Poland , the Fryderyk Chopin University of Music the oldest and largest music school in Poland , and one of the largest in Europe , the Warsaw School of Economics , the oldest and most renowned economic university in the country , and the Warsaw University of Life Sciences the largest agricultural university founded in 1818 .", "entities": []}, {"text": "Question : What is one of the largest music schools in Europe ?", "entities": []}, {"text": "Answer :", "entities": []}, {"text": "Fryderyk Chopin University of Music This passage - question - answer is dif\ufb01cult for automatic processing because there several entities of the same type ( school / university ) in the passage , and the question is a paraphrase of one segment of a very long , syntactically complicated sentence which contains the information required to be able to infer the correct answer .", "entities": []}, {"text": "This presents an interesting challenge , and such qualitative observations can be used to formulate a general technique for effectively testing machine reading systems .", "entities": []}, {"text": "3.2 Qualitative Analysis For qualitative error analysis , we sample 100 incorrect predictions ( based on EM ) from each model and try to \ufb01nd common error categories .", "entities": [[16, 17, "MetricName", "EM"]]}, {"text": "Broadly , the errors observed were either because of incorrect answer span boundaries or inability to infer the meaning of the question / passage .", "entities": []}, {"text": "Examples of each error type are shown in Table 3 , and these are further described below .", "entities": []}, {"text": "3.2.1 Boundary - Based Errors Incorrect answer boundary ( longer ): This error category includes those cases where the predicted span is longer than the ground truth answer , but contains the answer .", "entities": []}, {"text": "Incorrect answer boundary ( shorter ): This error category includes those cases where the predicted span is shorter than the ground truth answer , and is a substring of the answer .", "entities": []}, {"text": "Soft Correct : This error category includes those cases where the prediction is actually correct , but due to inclusion / exclusion of certain question terms ( such as units ) along with the answer , it is deemed incorrect.3.2.2 Inference - Based Errors Multi - Sentence : This error category includes those cases where inference is required to be performed across 2 or more sentences in the given passage to be able to arrive at the answer , which leads to an incorrect prediction based on only 1 passage sentence .", "entities": []}, {"text": "Paraphrase :", "entities": []}, {"text": "This error category includes those cases where the question paraphrases certain parts of the sentence that it is asking about which makes lexical pattern matching dif\ufb01cult and leads to errors in prediction .", "entities": []}, {"text": "Same Entity Type Confusion / Unit Confusion :", "entities": []}, {"text": "This error category includes those cases where the question is about an entity type which is present multiple times in the passage and the model returns a different entity than the ground truth entity but of the same type .", "entities": []}, {"text": "Requires World Knowledge : This error category includes questions which can not be answered using the given passage alone and require external knowledge to solve , leading to incorrect predictions .", "entities": []}, {"text": "Missing Inference : This category includes inference - related errors which do n\u2019t belong to any of the other categories mentioned above .", "entities": []}, {"text": "3.2.3 Observations In this section , we record the main observations from our qualitative error analysis and analyze potential reasons for the error trends observed .", "entities": []}, {"text": "Figure 4 shows the different types of errors in predictions by various models .", "entities": []}, {"text": "We observe that BiDAF makes many boundarybased errors which indicates that a better output layer ( since this is responsible for span identi\ufb01cation \u2013 although errors might have percolated from previous layers , most of these are cases where the model almost got the correct answer but not exactly ) or some post - processing of the answer might help improve performance .", "entities": []}, {"text": "Paraphrases also contribute to almost 15 % of errors observed which indicates that the question and the relevant parts of the context are not effectively matched in these cases .", "entities": []}, {"text": "We observe that R - Net makes fewer boundary errors , perhaps because self - attention enables it to accumulate evidence and return better answer spans , although this leads to more errors of the", "entities": []}, {"text": "95Error Type Passage Question Predicted Answer Incorrect answer boundary ( longer ) ...", "entities": []}, {"text": "survey of 4,745 North American Lutherans aged 15 - 65 found that , compared to the other minority groups under consideration , Lutherans were the least prejudiced toward Jews .", "entities": []}, {"text": "Nevertheless , Professor Richard ( Dick ) Geary , ... What did a survey of North American Lutherans \ufb01nd that Lutherans felt about Jews compared to other minority groups?15 - 65 found that , compared to the other minority groups under consideration , Lutherans were the least prejudiced toward Jews Incorrect answer boundary ( shorter ) ...", "entities": []}, {"text": "In the United States , in order for a prescription for a controlled substance to be valid , it must be issued for a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor - patient relationship .", "entities": []}, {"text": "The \ufb01lling ... What conditions must be met to prescribe a controlled substance?issued for a legitimate medical purpose Soft Correct ... for that time .", "entities": []}, {"text": "The vBNS installed one of the \ufb01rst ever production OC-48c ( 2.5 Gbit / s ) IP links in February 1999 and went on to upgrade the entire backbone ...", "entities": []}, {"text": "What did the network install in 1999?OC-48c ( 2.5 Gbit / s ) IP links MultiSentence ...", "entities": []}, {"text": "User Datagram Protocol ( UDP ) is an example of a datagram protocol .", "entities": []}, {"text": "In the virtual call system ... model .", "entities": []}, {"text": "The X.25 protocol suite uses this network type .", "entities": []}, {"text": "X.25 uses what type network", "entities": []}, {"text": "type?protocol suite Paraphrase ... rather than consumers .", "entities": []}, {"text": "There is no known case of any U.S. citizens buying Canadian drugs for personal use with a prescription , who has ever been charged by authorities .", "entities": []}, {"text": "Has there ever been anyone charged with importing drugs from Canada for personal medicinal use?has ever been charged by authorities Same Entity Type / Unit Confusion ... after the 1973 oil crisis , Honda , Toyota and Nissan , affected by the 1981 voluntary export restraints , opened US assembly plants and established their luxury divisions ( Acura , Lexus and In\ufb01niti , respectively ) to distinguish themselves from their mass - market brands .", "entities": []}, {"text": "Name a luxury division of Toyota .", "entities": []}, {"text": "Acura , Lexus and In\ufb01niti Requires World Knowledge ... disobedience in opposition to the decisions of nongovernmental agencies such as trade unions , banks , and private universities can be justi\ufb01ed if ...", "entities": []}, {"text": "What public entity of learning is often target of civil disobedience?governmental Missing Inference ...", "entities": []}, {"text": "Killer T cells are a sub - group of T cells that kill cells that are infected with viruses ( and other pathogens ) , or are otherwise damaged or dysfunctional .", "entities": []}, {"text": "As with B cells ... What kind of T cells kill cells that are infected with pathogens?sub - group Table 3 : Examples of error types observed in the qualitative analysis - blue indicates ground truth Figure 4 : Distribution of errors by various models across different categories using manual inspection \u2018 shorter \u2019 answer type than \u2018 longer \u2019 .", "entities": []}, {"text": "Also , missing inference contributes to almost 20 % of the observed errors ( not including multiple sentences or paraphrases ) .", "entities": []}, {"text": "Paraphrasing is the most frequent error category observed for DrQA , which makes sense if we con - sider the features used to represent each passage , such as exact match with a question word , which depend on lexical overlap between the question and passage .", "entities": [[29, 31, "MetricName", "exact match"]]}, {"text": "We observe that DocQA makes many boundary errors too , again making more mistakes by pre-", "entities": []}, {"text": "96dicting shorter answers than expected in most of the observed cases .", "entities": []}, {"text": "A better root cause analysis can be performed by visualizing outputs from different layers and evaluating these , and we leave this in - depth investigation to future work .", "entities": []}, {"text": "Also , the high number of Soft Correct outputs across all models points to some de\ufb01ciencies in the SQuAD annotations , which might limit the reliability of the performance evaluation metrics .", "entities": [[18, 19, "DatasetName", "SQuAD"]]}, {"text": "Although these state - of - the - art deep learning models for machine reading are supposed to have inference capabilities , our error analysis above points to their limitations .", "entities": []}, {"text": "These insights can be useful for developing benchmarks and datasets which enable realistic evaluation of systems which aim to \u2018 solve \u2019 the RC task .", "entities": []}, {"text": "In Wadhwa et al . ( 2018 ) , we take a \ufb01rst step in this direction by proposing a method focused on questions involving referential inference , a setting to which these models fail to generalize well .", "entities": []}, {"text": "4 Conclusion and Future Work", "entities": []}, {"text": "In this work , we analyze - both quantitatively and qualitatively - results generated by 4 end - to - end neural models on the Stanford Question Answering Dataset .", "entities": [[24, 29, "DatasetName", "the Stanford Question Answering Dataset"]]}, {"text": "We observe interesting trends in the analysis , with some error patterns which are consistent across different models and some others which are speci\ufb01c to each model due to their different input features and architectures .", "entities": []}, {"text": "This is important to be able to interpret and gain an intuition for the effective functions that different components in a neural model architecture perform versus their intended functions , and also to understand model - speci\ufb01c biases .", "entities": []}, {"text": "Eventually , this can enable us to come up with new models including speci\ufb01c components which tackle these errors .", "entities": []}, {"text": "Alternatively , the overlap analysis demonstrates that learning ensembles of different neural models to combine their individual strengths and quirks might be an interesting direction to explore to achieve better performance .", "entities": []}, {"text": "Even though the scope of this paper is restricted to SQuAD , similar analysis can be done for any datasets / models / features , to gain a better understanding and enable a better assessment of stateof - the - art in neural machine reading .", "entities": [[10, 11, "DatasetName", "SQuAD"]]}, {"text": "To this end , we also performed some preliminary experiments on TriviaQA so as to analyze the difference between the properties of the two datasets , but were unable to replicate the published results owing topre - processing / hyperparameters .", "entities": [[11, 12, "DatasetName", "TriviaQA"]]}, {"text": "We will continue to work on this since the ability of a model to generalize and to be able to learn from a particular domain and transfer some knowledge to a different domain is a very exciting research area .", "entities": []}, {"text": "We also believe that such analysis can help curate datasets which are better indicators of the actual natural language \u2018 reading \u2019 and \u2018 comprehending \u2019 capabilities of models rather than falling prey to shallow pattern matching .", "entities": []}, {"text": "One way to achieve this is by building new challenges that are specifically designed to put pressure on the identi\ufb01ed weaknesses of neural models .", "entities": []}, {"text": "Thus , we can move towards the development of datasets and models which truly push the envelope of the challenging machine reading task .", "entities": []}, {"text": "Acknowledgments We would like to thank Chaitanya Malaviya and Abhishek Chinni for their valuable feedback , and the Language Technologies Institute at CMU for the GPU resources used in this work .", "entities": []}, {"text": "We are also very grateful to the anonymous reviewers for their insightful comments and suggestions , which helped us polish the presentation of our work .", "entities": []}, {"text": "References Danqi Chen , Jason Bolton , and Christopher D Manning .", "entities": []}, {"text": "A thorough examination of the cnn / daily mail reading comprehension task .", "entities": [[5, 9, "DatasetName", "cnn / daily mail"], [9, 11, "TaskName", "reading comprehension"]]}, {"text": "Danqi Chen , Adam Fisch , Jason Weston , and Antoine Bordes . 2017 .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "Reading wikipedia to answer open - domain questions .", "entities": []}, {"text": "arXiv preprint arXiv:1704.00051 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Christopher Clark and Matt Gardner . 2017 .", "entities": []}, {"text": "Simple and effective multi - paragraph reading comprehension .", "entities": [[6, 8, "TaskName", "reading comprehension"]]}, {"text": "arXiv preprint arXiv:1710.10723 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Bhuwan Dhingra , Kathryn Mazaitis , and William W Cohen .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Quasar : Datasets for question answering by search and reading .", "entities": [[0, 1, "DatasetName", "Quasar"], [4, 6, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1707.03904 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Matthew Dunn , Levent Sagun , Mike Higgins , Ugur Guney , V olkan Cirik , and Kyunghyun Cho . 2017 .", "entities": []}, {"text": "Searchqa :", "entities": [[0, 1, "DatasetName", "Searchqa"]]}, {"text": "A new q&a dataset augmented with context from a search engine .", "entities": []}, {"text": "arXiv preprint arXiv:1704.05179 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Daniel Hewlett , Alexandre Lacoste , Llion Jones , Illia Polosukhin , Andrew Fandrianto , Jay Han , Matthew Kelcey , and David Berthelot .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Wikireading :", "entities": [[0, 1, "DatasetName", "Wikireading"]]}, {"text": "A novel large - scale language understanding task over wikipedia .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "entities": []}, {"text": "97(Volume 1 : Long Papers ) , pages 1535\u20131545 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mandar Joshi , Eunsol Choi , Daniel S Weld , and Luke Zettlemoyer . 2017 .", "entities": []}, {"text": "Triviaqa :", "entities": [[0, 1, "DatasetName", "Triviaqa"]]}, {"text": "A large scale distantly supervised challenge dataset for reading comprehension .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}, {"text": "arXiv preprint arXiv:1705.03551 . Tom\u00b4a\u02c7s", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ko \u02c7cisk`y , Jonathan Schwarz , Phil Blunsom , Chris Dyer , Karl Moritz Hermann , G \u00b4 abor Melis , and Edward Grefenstette . 2017 .", "entities": []}, {"text": "The narrativeqa reading comprehension challenge .", "entities": [[1, 2, "DatasetName", "narrativeqa"], [2, 4, "TaskName", "reading comprehension"]]}, {"text": "arXiv preprint arXiv:1712.07040 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tri Nguyen , Mir Rosenberg , Xia Song , Jianfeng Gao , Saurabh Tiwary , Rangan Majumder , and Li Deng . 2016 .", "entities": []}, {"text": "Ms marco : A human generated machine reading comprehension dataset .", "entities": [[0, 2, "DatasetName", "Ms marco"], [6, 9, "TaskName", "machine reading comprehension"]]}, {"text": "arXiv preprint arXiv:1611.09268 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Anselmo Pe \u02dcnas , Eduard H Hovy , Pamela Forner , \u00b4 Alvaro Rodrigo , Richard FE Sutcliffe , Corina Forascu , and Caroline Sporleder .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Overview of qa4mre at clef 2011 :", "entities": []}, {"text": "Question answering for machine reading evaluation .", "entities": [[0, 2, "TaskName", "Question answering"]]}, {"text": "In CLEF ( Notebook Papers / Labs / Workshop ) , pages 1\u201320 .", "entities": []}, {"text": "Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Squad : 100,000 + questions for machine comprehension of text .", "entities": []}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383\u20132392 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Minjoon Seo , Aniruddha Kembhavi , Ali Farhadi , and Hannaneh Hajishirzi . 2016 .", "entities": []}, {"text": "Bidirectional attention \ufb02ow for machine comprehension .", "entities": []}, {"text": "arXiv preprint arXiv:1611.01603 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Adam Trischler , Tong Wang , Xingdi Yuan , Justin Harris , Alessandro Sordoni , Philip Bachman , and Kaheer Suleman .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "2016 .", "entities": []}, {"text": "Newsqa : A machine comprehension dataset .", "entities": [[0, 1, "DatasetName", "Newsqa"]]}, {"text": "arXiv preprint arXiv:1611.09830 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "S. Wadhwa , V .", "entities": []}, {"text": "Embar , M. Grabmair , and E. Nyberg .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Towards Inference - Oriented Reading Comprehension : ParallelQA .", "entities": [[4, 6, "TaskName", "Reading Comprehension"]]}, {"text": "ArXiv e -", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "prints .", "entities": []}, {"text": "Wenhui Wang , Nan Yang , Furu Wei , Baobao Chang , and Ming Zhou . 2017 .", "entities": []}, {"text": "Gated self - matching networks for reading comprehension and question answering .", "entities": [[6, 8, "TaskName", "reading comprehension"], [9, 11, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , volume 1 , pages 189\u2013198 .", "entities": []}, {"text": "Johannes Welbl , Pontus Stenetorp , and Sebastian Riedel . 2017 .", "entities": []}, {"text": "Constructing datasets for multi - hop reading comprehension across documents .", "entities": [[3, 8, "TaskName", "multi - hop reading comprehension"]]}, {"text": "arXiv preprint arXiv:1710.06481 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}]