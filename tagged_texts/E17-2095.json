[{"text": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers , pages 599\u2013604 , Valencia , Spain , April 3 - 7 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics Context - Aware Graph Segmentation for Graph - Based Translation Liangyou Li and Andy Way and Qun Liu ADAPT Centre , School of Computing Dublin City University , Ireland { liangyou.li , andy.way , qun.liu } @adaptcentre.ie Abstract In this paper , we present an improved graph - based translation model which segments an input graph into node - induced subgraphs by taking source context into consideration .", "entities": [[14, 15, "TaskName", "Translation"]]}, {"text": "Translations are generated by combining subgraph translations leftto - right using beam search .", "entities": []}, {"text": "Experiments on Chinese \u2013 English and German \u2013 English demonstrate that the context - aware segmentation signi\ufb01cantly improves the baseline graph - based model .", "entities": []}, {"text": "1 Introduction The well - known phrase - based statistical translation model ( Koehn et al . , 2003 ) extends the basic translation units from single words to continuous phrases to capture local phenomena .", "entities": []}, {"text": "However , one of its signi\ufb01cant weaknesses is that it can not learn generalizations ( Quirk et al . , 2005 ; Galley and Manning , 2010 ) .", "entities": []}, {"text": "To allow discontinuous phrases ( any subset of words of an input sentence ) , dependency treelets ( Menezes and Quirk , 2005 ; Quirk et al . , 2005 ; Xiong et al . , 2007 ) can be used , which are connected subgraphs on trees .", "entities": []}, {"text": "However , continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance ( Koehn et al . , 2003 ; Hanneman and Lavie , 2009 ) .", "entities": []}, {"text": "To make use of the merits of both phrase - based models and treelet - based models , Li et al .", "entities": []}, {"text": "( 2016 ) proposed a graph - based translation model as in Equation ( 1 ): p(tI 1|gI 1 ) = I / productdisplay i=1p(ti|gai)\u00d7d(gai , gai\u22121)(1 ) where tiis a continuous target phrase which is the translation of a node - induced and connectedsource subgraph gai.1dis a distance - based reordering function which penalizes discontinuous phrases that have relatively long gaps ( Galley and Manning , 2010 ) .", "entities": []}, {"text": "The model translates an input graph by segmenting it into subgraphs and generates a complete translation by combining subgraph translations left - to - right .", "entities": []}, {"text": "However , the model treats different graph segmentations equally .", "entities": []}, {"text": "Therefore , in this paper we propose a contextaware graph segmentation ( Section 2 ): ( i ) we add contextual information to each translation rule during training ( Section 2.2 ) ; ( ii ) during decoding , when a rule is applied , the input context should match with the rule context ( Section 2.3 ) .", "entities": []}, {"text": "Experiments ( Section 3 ) on Chinese \u2013 English ( ZH \u2013 EN ) and German \u2013 English ( DE \u2013 EN ) tasks show that our method signi\ufb01cantly improves the graphbased model .", "entities": []}, {"text": "As observed in our experiments , the context - aware segmentation brings two bene\ufb01ts to our system : ( i ) it helps to select a better subgraph to translate ; and ( ii ) it selects a better target phrase for a subgraph .", "entities": []}, {"text": "2 Context - Aware Graph Segmentation and Translation Our model extends the graph - based translation model by considering source context during segmenting input graphs , as in Equation ( 2 ): p(tI 1|gI 1 )", "entities": [[7, 8, "TaskName", "Translation"]]}, {"text": "= I / productdisplay i=1p(ti|gai , cai ) \u00d7d(gai , gai\u22121)(2 ) where caidenotes the context of the subgraph gai , which is represented as a set of connections ( i.e. edges ) between gaiand[gai+1,\u00b7\u00b7\u00b7,gaI ] .", "entities": []}, {"text": "1All subgraphs in this paper are connected and nodeinduced.599", "entities": []}, {"text": "2010Nian FIFA Shijiebei ZaiNanfei Chenggong Juxing Figure 1 : An example graph for a Chinese sentence .", "entities": []}, {"text": "Dotted lines are bigram relations .", "entities": []}, {"text": "Solid lines are dependency relations .", "entities": []}, {"text": "Dashed lines are shared by bigram and dependency relations .", "entities": []}, {"text": "2.1 Building Graphs The graph used in this paper combines a sequence and a dependency tree as in Li et", "entities": []}, {"text": "al . ( 2016 ) .", "entities": []}, {"text": "Each graph contains two kinds of links : dependency links from dependency trees which model syntactic and semantic relations between words , and bigram links which provide local and sequential information on pairs of continuous words .", "entities": []}, {"text": "Figure 1 shows an example graph .", "entities": []}, {"text": "Given such graphs , we can make use of both continuous and linguistically informed discontinuous phrases as long as they are connected on graphs .", "entities": []}, {"text": "In this paper , we do not distinguish the two kinds of relations , because our preliminary experiments showed no improvement when considering edge types .", "entities": []}, {"text": "2.2 Training During training , given a word - aligned graph \u2013 string pair / angbracketleftg , t , a / angbracketright , we extract translation rules /angbracketleftgai , cai , ti / angbracketright , each of which consists of a continuous target phrase ti , a source subgraph gaialigned toti , and a source context cai .", "entities": []}, {"text": "We \ufb01rst \ufb01nd initial pairs ./angbracketleft\u02dcsai , ti / angbracketrightis an initial pair , iff it is consistent with the word alignment a(Och and Ney , 2004 ) .", "entities": [[20, 22, "TaskName", "word alignment"]]}, {"text": "\u02dcsajis a set of source words which are aligned to ti .", "entities": []}, {"text": "Then , the set of rules satis\ufb01es the following : 1.If / angbracketleft\u02dcsai , ti / angbracketrightis an initial pair and \u02dcsaiis covered by a subgraph gaiwhich is connected , then /angbracketleftgai,\u2217,ti / angbracketrightis abasic rule .cai=\u2217means", "entities": []}, {"text": "that a basic rule is applied without considering context to make sure that at least one translation is produced for any inputs during decoding .", "entities": []}, {"text": "Therefore , basic rules are the same as rules in the conventional graph - based model .", "entities": []}, {"text": "Rule ( 3 ) shows an example of a basic rule : 2010Nian FIFA Shijiebei 2010 FIFA World Cup ( 3)2.Assume / angbracketleftgai,\u2217,ti / angbracketrightis a basic rule and /angbracketleft\u02dcsai+1,ti+1 / angbracketrightis an initial pair where ti+1is on the right of and adjacent to ti .", "entities": []}, {"text": "If there are edges between gaiand\u02dcsai+1 , then / angbracketleftgai , cai , ti / angbracketrightis a segmenting rule , where caiis the set of edges between gaiand\u02dcsai+1by treating \u02dcsai+1as a single node x. Rule ( 4 ) is an example of a segmenting rule : 2010Nian FIFA x 2010 FIFA(4 ) where dashed links are contextual connections .", "entities": []}, {"text": "During decoding , when the context matches , rule ( 4 ) translates a subgraph over 2010Nian FIFA into a target phrase 2010 FIFA .", "entities": []}, {"text": "For example , it can be applied to graph ( 5 ) where Shijiebei Zai Nanfei ( in the dashed rectangle ) is treated as x : 2010Nian FIFA Shijiebei Zai Nanfei(5 ) 3.If there are no edges between gaiand\u02dcsai+1 , then caiis equal to\u2205and /", "entities": []}, {"text": "angbracketleftgai,\u2205,ti / angbracketrightis a translation rule , called a selecting rule in this paper .", "entities": []}, {"text": "During decoding , the untranslated input could be a set of subgraphs which are disjoint with each other .", "entities": []}, {"text": "A selecting rule is used to select one of them .", "entities": []}, {"text": "For example , rule ( 6 ) can be applied to ( 7 ) to translate 2010Nian FIFA to2010 FIFA .", "entities": []}, {"text": "In this example , the xin rule ( 6 ) matches with Chenggong Juxing ( in the dashed rectangle ) in ( 7 ) .", "entities": []}, {"text": "2010Nian FIFA x 2010 FIFA(6 ) 2010Nian FIFA Chenggong Juxing(7 ) By comparing these three types of rules , we observe that both segmenting rules and selecting rules are based on basic rules .", "entities": []}, {"text": "They extend basic rules by adding contextual information to their source subgraphs so that basic rules are split into different groups according to the context .", "entities": []}, {"text": "During decoding , the context will help to select target phrases as well .", "entities": []}, {"text": "Algorithm 1 illustrates a simple process for rule extraction .", "entities": []}, {"text": "Given a word - aligned graph \u2013 string pair , we \ufb01rst extract all initial pairs ( Line 1 ) .", "entities": []}, {"text": "Then , we \ufb01nd basic rules from these pairs ( Lines 3\u20134 ) .", "entities": []}, {"text": "Basic600", "entities": []}, {"text": "Algorithm 1 : An algorithm for extracting translation rules from a graph \u2013 string pair .", "entities": []}, {"text": "Data : Word - aligned graph \u2013 string pair /angbracketleftg , t , a / angbracketright Result : A set of translation rules R 1\ufb01nd a set of initial pairs P ; 2foreachp=/angbracketleftsai , ti / angbracketrightinPdo 3 ifsj iis connected then // basic rules 4 add / angbracketleftgai),\u2217,ti / angbracketrighttoR ; // segmenting and selecting rules 5 forq=/angbracketleftsai+1,ti+1 / angbracketrightinPdo 6 cis the set of edges between gai andsai+1 ; 7 add / angbracketleftgai , c , ti / angbracketrighttoR ; 8 end 9 end 10end rules are then used to generate segmenting and selecting rules by extending them with contextual connections ( Lines 5\u20138 ) .", "entities": []}, {"text": "2.3 Model and Decoding Following Li et al . ( 2016 ) , we de\ufb01ne our model in the well - known log - linear framework ( Och and Ney , 2002 ) .", "entities": []}, {"text": "In our experiments , we use the following standard features : two translation probabilities p(g , c|t)andp(t|g , c ) , two lexical translation probabilities plex(g , c|t)andplex(t|g , c ) , a language model p(t ) , a rule penalty , a word penalty , and a distortion function as de\ufb01ned in Galley and Manning ( 2010 ) .", "entities": []}, {"text": "In addition , we add one more feature into our system : a basic - rule penalty to distinguish basic rules from segmenting and selecting rules .", "entities": []}, {"text": "Our decoder is very similar to the one in the conventional graph - based model , which generates hypotheses left - to - right using beam search .", "entities": []}, {"text": "A hypothesis can be extended on the right by translating an uncovered source subgraph .", "entities": []}, {"text": "The translation process ends when all source words have been translated .", "entities": []}, {"text": "However , when extending a hypothesis , our decoder considers the context of the translated subgraph , i.e. edges connecting it with the remaining untranslated source words .", "entities": []}, {"text": "Figure 2 shows a derivation which translates an input graph in Chinese to an English string .", "entities": []}, {"text": "In this example , both rules r1 andr2are segmenting", "entities": []}, {"text": "rules.2010Nian FIFA Shijiebei ZaiNanfei", "entities": []}, {"text": "Chenggong Juxing r1:2010Nian FIFA x 2010 FIFA h1 : 2010 FIFA Shijiebei Zai Nanfei Chenggong Juxing r2 :", "entities": []}, {"text": "Shijiebei Juxing x World Cup was held h2 : 2010 FIFA World Cup was held Zai Nanfei Chenggong r3 : Zai Nanfei Chenggongsuccessfully in South Africa h3 : 2010 FIFA World Cup was held successfully in South Africa \u2205", "entities": []}, {"text": "Figure 2 : Example of translating an input graph .", "entities": []}, {"text": "Each rule rigenerates a new hypothesis hiby appending translations on the right .", "entities": []}, {"text": "Edges connected to xdenote contextual information .", "entities": []}, {"text": "Nodes in dashed rectangles are treated as xduring decoding for matching contexts .", "entities": []}, {"text": "3 Experiments We conduct experiments on ZH \u2013 EN and DE \u2013 EN corpora .", "entities": []}, {"text": "3.1 Data and Settings The ZH \u2013 EN training corpus contains 1.5M+ sentences from LDC .", "entities": []}, {"text": "NIST 2002 is taken as a development set to tune weights .", "entities": []}, {"text": "NIST 2004 ( MT04 ) and NIST 2005 ( MT05 ) are two test sets to evaluate systems .", "entities": []}, {"text": "The DE \u2013 EN training corpus ( 2M+ sentence pairs ) is from WMT 2014 , including Europarl V7 and News Commentary .", "entities": [[13, 15, "DatasetName", "WMT 2014"]]}, {"text": "News - Test 2011 is taken as a development set while News - Test 2012 ( WMT12 ) and News - Test 2013 ( WMT13 ) are our test sets.601", "entities": []}, {"text": "SystemZH \u2013 EN DE \u2013 EN MT04 MT05 WMT12 WMT13 PBMT 33.2 31.8 19.5 21.9 TBMT 33.8\u221731.7 19.6 22.1\u2217 GBMT 34.7\u2217+32.4\u2217+19.8\u2217+22.4\u2217+ GBMT ctx 35.4\u2217+33.7\u2217+20.1\u2217+22.8\u2217+ Table 1 : BLEU scores of all systems .", "entities": [[27, 28, "MetricName", "BLEU"]]}, {"text": "Bold \ufb01gures mean GBMT ctxis signi\ufb01cantly better than GBMT atp\u22640.01.\u2217means", "entities": []}, {"text": "a system is signi\ufb01cantly better than PBMT at p\u22640.01.+means a system is signi\ufb01cantly better than TBMT at p\u22640.01 .", "entities": []}, {"text": "Following Li et al .", "entities": []}, {"text": "( 2016 ) , Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges .", "entities": []}, {"text": "Word alignment is performed by GIZA++ ( Och and Ney , 2003 ) with the heuristic function grow - diag-\ufb01nal - and .", "entities": [[0, 2, "TaskName", "Word alignment"]]}, {"text": "We use SRILM ( Stolcke , 2002 ) to train a 5 - gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modi\ufb01ed Kneser - Ney discounting ( Chen and Goodman , 1996 ) .", "entities": []}, {"text": "Batch MIRA ( Cherry and Foster , 2012 ) is used to tune feature weights .", "entities": []}, {"text": "We report BLEU ( Papineni et al . , 2002 ) scores averaged on three runs of MIRA ( Clark et al . , 2011 ) .", "entities": [[2, 3, "MetricName", "BLEU"]]}, {"text": "We compare our system GBMT ctxwith several other systems .", "entities": []}, {"text": "A system PBMT is built using the phrase - based model in Moses ( Koehn et al . , 2007 ) .", "entities": []}, {"text": "GBMT is the graph - based translation system described in Li et", "entities": []}, {"text": "al . ( 2016 ) .", "entities": []}, {"text": "To examine the in\ufb02uence of bigram links , GBMT is also used to translate dependency trees where treelets ( Menezes and Quirk , 2005 ; Quirk et al . , 2005 ; Xiong et al . , 2007 ) are the basic translation units .", "entities": []}, {"text": "Accordingly , we name the system TBMT .", "entities": []}, {"text": "All systems are implemented in Moses .", "entities": []}, {"text": "3.2 Results and Discussion Table 1 shows BLEU scores of all systems .", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "We found that GBMT ctxis better than PBMT across all test sets .", "entities": []}, {"text": "Speci\ufb01cally , the improvements are +2.0/+0.7 BLEU on average on ZH \u2013 EN and DE \u2013 EN , respectively .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "This improvement is reasonable as our system allows discontinuous phrases which can reduce data sparsity and handle longdistance relations ( Galley and Manning , 2010 ) .", "entities": []}, {"text": "In addition , the system TBMT does not show consistent improvements over PBMT while both GBMT and GBMT ctxachieve better BLEU scores than TBMT on both ZH \u2013 EN ( +1.8 BLEU , in terms ofRule Type # Rules ZH \u2013 EN DE \u2013 EN Basic Rule 84.7M+ 115.7M+ Segmenting Rule 128.4M+ 167.3M+", "entities": [[20, 21, "MetricName", "BLEU"], [31, 32, "MetricName", "BLEU"]]}, {"text": "Selecting Rule 30.2M+ 35.7M+ Total 243.5M+ 318.9M+ Table 2 : The number of rules in GBMT ctxaccording to their type GBMT ctx ) and DE \u2013 EN ( +0.6 BLEU , in terms of GBMT ctx ) .", "entities": [[29, 30, "MetricName", "BLEU"]]}, {"text": "This suggests that continuous phrases connected by bigram links are essential to system performance since they help to improve phrase coverage ( Hanneman and Lavie , 2009 ) .", "entities": []}, {"text": "We also found that GBMT ctxis signi\ufb01cantly better than GBMT on both ZH \u2013 EN ( +1.0 BLEU ) and DE \u2013 EN ( +0.4 BLEU ) , which indicates that explicitly modeling a segmentation using context is helpful .", "entities": [[17, 18, "MetricName", "BLEU"], [25, 26, "MetricName", "BLEU"]]}, {"text": "The main reason for the improvement is that context helps to select proper subgraphs and target phrases .", "entities": []}, {"text": "Figure 3 shows example translations .", "entities": []}, {"text": "We found that in Figure 3a , after translating a parenthesis , GBMT ctxcorrectly selects a subgraph Gang Ao Tai and generates a target phrase hong kong , macao and taiwan .", "entities": []}, {"text": "In Figure 3b , both GBMT and GBMT ctxchoose to translate the subgraph WoMen Ye ZhiLi .", "entities": []}, {"text": "However , given the context of the subgraph , GBMT ctxselects a correct target phrase we are also committed to for it .", "entities": []}, {"text": "3.3 In\ufb02uence of Different Types of Rules Recall that , compared with GBMT , GBMT ctxcontains three types of rules : basic rules , segmenting rules , and selecting rules .", "entities": [[7, 8, "MetricName", "Recall"]]}, {"text": "While basic rules exist in both systems , segmenting and selecting rules make GBMT ctxcontext - aware .", "entities": []}, {"text": "Table 2 shows the number of rules in GBMT ctxaccording to their types .", "entities": []}, {"text": "We found that on both language pairs 35%\u201336 % of rules are basic rules .", "entities": []}, {"text": "While the proportion of segmenting rules is \u223c53 % , selecting rules only account for 11%\u201312 % .", "entities": []}, {"text": "This is because segmenting rules contain richer contextual information than selecting rules .", "entities": []}, {"text": "Table 3 shows BLEU scores of GBMT ctxwhen different types of rules are used .", "entities": [[3, 4, "MetricName", "BLEU"]]}, {"text": "Note that when only basic rules are allowed , our system degrades to the conventional GBMT system .", "entities": []}, {"text": "The results in Table 3 suggest that both segmenting and selecting rules consistently improve GBMT on both language pairs .", "entities": []}, {"text": "However , segmenting rules are more useful than selecting rules .", "entities": []}, {"text": "This is reasonable since602", "entities": []}, {"text": "( hong kong macao taiwan ) hong kong spring festival retail business rise 10 % ( Gang Ao Tai )", "entities": []}, {"text": "XiangGang XinChun LingShou ShengYi ShangSheng YiCheng Ref : GBMT : GBMT ctx:(hong kong , macao and taiwan )", "entities": []}, {"text": "hong kong \u2019s retail sales up 10 % during spring festival ( the spring festival ) hong kong retail business in hong kong , macao and taiwan rose by 10 % ( hong kong , macao and taiwan )", "entities": []}, {"text": "hong kong spring retail business will increase by 10 % ( a ) subgraph selection we also dedicate protect and improve living emvironment .", "entities": []}, {"text": "WoMen Ye ZhiLi", "entities": []}, {"text": "BaoHu He GaiShan JuZhu HuanJing .", "entities": []}, {"text": "Ref : GBMT : GBMT ctx : we are also committed to protect and improve our living environment .", "entities": []}, {"text": "we have worked hard to protect and improve the living environment .", "entities": []}, {"text": "we are also committed to protect and improve the living environment .", "entities": []}, {"text": "( b ) target - phrase selection Figure 3 : Example translations of GBMT and GBMT ctx SystemZH \u2013 EN DE \u2013", "entities": []}, {"text": "EN MT04 MT05 WMT12 WMT13 Basic Rule 34.7 32.4 19.8 22.4 + Seg .", "entities": []}, {"text": "Rule 34.9 33.0 20.2 23.0 + Sel .", "entities": []}, {"text": "Rule 34.8 32.5 20.0 22.7 All 35.4 33.7 20.1 22.8 Table 3 : BLEU scores of GBMT ctxwhen different types of rules are used , including Basic Rule , Segmenting ( Seg . )", "entities": [[13, 14, "MetricName", "BLEU"]]}, {"text": "Rule , and Selecting ( Sel . )", "entities": []}, {"text": "Rule .", "entities": []}, {"text": "Bold \ufb01gures mean a system is signi\ufb01cantly better than the one only using basic rules at p\u22640.01 .", "entities": []}, {"text": "the number of segmenting rules is much larger than the number of selecting rules .", "entities": []}, {"text": "We further observed that , while our system achieves the best performance when all rules are used on ZH \u2013 EN , the combination of basic rules and segmenting rules on DE \u2013 EN results in the best system .", "entities": []}, {"text": "This is probably because reordering ( including long - distance reordering ) is performed less often in DE \u2013 EN than in ZH \u2013 EN ( Li et al . , 2016 ) which makes selecting rules less preferable on DE \u2013 EN . 4 Conclusion In this paper , we present a graph - based model which takes subgraphs as the basic translation units and considers source context during segmenting graphs into subgraphs .", "entities": []}, {"text": "Experiments on Chinese \u2013 English and German \u2013 English show that our model is signi\ufb01cantly better than the conventional graphbased model which equally treats different graph segmentations .", "entities": []}, {"text": "In this paper , source context is used as hard constraints during decoding .", "entities": []}, {"text": "In future , we would like to try soft constraints .", "entities": []}, {"text": "In addition , it would also be interesting to extend this model using a synchronous graph grammar .", "entities": []}, {"text": "Acknowledgments This research has received funding from the European Union \u2019s Horizon 2020 research and innovation programme under grant agreement no645452 ( QT21 ) .", "entities": []}, {"text": "The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme ( Grant 13 / RC/2106 ) and is cofunded under the European Regional Development Fund .", "entities": []}, {"text": "The authors thank all anonymous reviewers for their insightful comments and suggestions .", "entities": []}, {"text": "References Stanley F. Chen and Joshua Goodman .", "entities": []}, {"text": "1996 .", "entities": []}, {"text": "An Empirical Study of Smoothing Techniques for Language Modeling .", "entities": []}, {"text": "In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics , ACL \u2019 96 , pages 310\u2013318 , Santa Cruz , California , June.603", "entities": []}, {"text": "Colin Cherry and George Foster .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Batch Tuning Strategies for Statistical Machine Translation .", "entities": [[5, 7, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 427\u2013436 , Montreal , Canada , June .", "entities": []}, {"text": "Jonathan H. Clark , Chris Dyer , Alon Lavie , and Noah A. Smith .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Better Hypothesis Testing for Statistical Machine Translation : Controlling for Optimizer Instability .", "entities": [[5, 7, "TaskName", "Machine Translation"], [10, 11, "HyperparameterName", "Optimizer"]]}, {"text": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies :", "entities": []}, {"text": "Short Papers - Volume 2 , pages 176\u2013181 , Portland , Oregon , June .", "entities": []}, {"text": "Michel Galley and Christopher D. Manning .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Accurate Non - hierarchical Phrase - Based Translation .", "entities": [[7, 8, "TaskName", "Translation"]]}, {"text": "InHuman Language Technologies : The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics , pages 966\u2013974 , Los Angeles , California , June .", "entities": []}, {"text": "Greg Hanneman and Alon Lavie .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Decoding with Syntactic and Non - syntactic Phrases in a Syntaxbased Machine Translation System .", "entities": [[11, 13, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation , pages 1\u20139 , Boulder , Colorado , June .", "entities": [[12, 13, "TaskName", "Translation"]]}, {"text": "Philipp Koehn , Franz Josef Och , and Daniel Marcu .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Statistical Phrase - Based Translation .", "entities": [[4, 5, "TaskName", "Translation"]]}, {"text": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 , pages 48\u201354 , Edmonton , Canada , July .", "entities": []}, {"text": "Philipp Koehn , Hieu Hoang , Alexandra Birch , Chris Callison - Burch , Marcello Federico , Nicola Bertoldi , Brooke Cowan , Wade Shen , Christine Moran , Richard Zens , Chris Dyer , Ondej Bojar , Alexandra Constantin , and Evan Herbst .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Moses : Open Source Toolkit for Statistical Machine Translation .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions , pages 177\u2013180 , Prague , Czech Republic , June .", "entities": []}, {"text": "Liangyou Li , Andy Way , and Qun Liu . 2016 .", "entities": []}, {"text": "GraphBased Translation Via Graph Segmentation .", "entities": [[1, 2, "TaskName", "Translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 97\u2013107 , Berlin , Germany , August .", "entities": []}, {"text": "Arul Menezes and Chris Quirk .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Dependency Treelet Translation : The Convergence of Statistical and Example - Based Machine - translation ?", "entities": [[2, 3, "TaskName", "Translation"]]}, {"text": "In Proceedings of the Workshop on Example - based Machine Translation at MT Summit X , September .", "entities": [[9, 11, "TaskName", "Machine Translation"]]}, {"text": "Franz Josef Och and Hermann Ney .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Discriminative Training and Maximum Entropy Models for Statistical Machine Translation .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics , pages 295\u2013302 , Philadelphia , PA , USA , July .", "entities": []}, {"text": "Franz Josef Och and Hermann Ney . 2003 .", "entities": []}, {"text": "A Systematic Comparison of Various Statistical Alignment Models .", "entities": []}, {"text": "Computational Linguistics , 29(1):19\u201351 , March .", "entities": []}, {"text": "Franz Josef Och and Hermann Ney .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "The Alignment Template Approach to Statistical Machine Translation .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "Computational Linguistics , 30(4):417 \u2013 449 , December .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "BLEU :", "entities": [[0, 1, "MetricName", "BLEU"]]}, {"text": "A Method for Automatic Evaluation of Machine Translation .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics , pages 311\u2013318 , Philadelphia , Pennsylvania , July .", "entities": []}, {"text": "Chris Quirk , Arul Menezes , and Colin Cherry . 2005 .", "entities": []}, {"text": "Dependency Treelet Translation : Syntactically Informed Phrasal SMT .", "entities": [[2, 3, "TaskName", "Translation"]]}, {"text": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ( ACL\u201905 ) , pages 271\u2013279 , Ann Arbor , Michigan , June .", "entities": []}, {"text": "Andreas Stolcke . 2002 .", "entities": []}, {"text": "SRILM An Extensible Language Modeling Toolkit .", "entities": []}, {"text": "In Proceedings of the International Conference Spoken Language Processing , pages 901\u2013904 , Denver , CO .", "entities": []}, {"text": "Deyi Xiong , Qun Liu , and Shouxun Lin .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "A Dependency Treelet String Correspondence Model for Statistical Machine Translation .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "In Proceedings of the Second Workshop on Statistical Machine Translation , pages 40\u201347 , Prague , Czech Republic , June.604", "entities": [[8, 10, "TaskName", "Machine Translation"]]}]