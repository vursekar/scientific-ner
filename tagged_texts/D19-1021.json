[{"text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 219\u2013228 , Hong Kong , China , November 3\u20137 , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics219Open Relation Extraction : Relational Knowledge Transfer from Supervised Data to Unsupervised Data Ruidong Wu1\u2217 ,", "entities": [[5, 7, "TaskName", "Relation Extraction"]]}, {"text": "Yuan Yao1\u2217,X uH a n1 , Ruobing Xie2 , Zhiyuan Liu1\u2020 , Fen Lin2 , Leyu Lin2 , Maosong Sun1 1Department of Computer Science and Technology , Tsinghua University , Beijing , China Institute for Arti\ufb01cial Intelligence , Tsinghua University , Beijing , China State Key Lab on Intelligent Technology and Systems , Tsinghua University , Beijing , China 2Search Product Center , WeChat Search Application Department , Tencent , China mooninsiderain@gmail.com", "entities": [[63, 64, "DatasetName", "WeChat"]]}, {"text": "Abstract Open relation extraction ( OpenRE ) aims to extract relational facts from the open - domaincorpus .", "entities": [[2, 4, "TaskName", "relation extraction"]]}, {"text": "To this end , it discovers relation pat - terns between named entities and then clustersthose semantically equivalent patterns into aunited relation cluster .", "entities": []}, {"text": "Most OpenRE meth - ods typically con\ufb01ne themselves to unsuper - vised paradigms , without taking advantage ofexisting relational facts in knowledge bases ( KBs ) and their high - quality labeled instances .", "entities": []}, {"text": "To address this issue , we propose RelationalSiamese Networks ( RSNs ) to learn similar - ity metrics of relations from labeled data ofpre - de\ufb01ned relations , and then transfer therelational knowledge to identify novel relations in unlabeled data .", "entities": []}, {"text": "Experiment results on two real - world datasets show that our frame - work can achieve signi\ufb01cant improvements ascompared with other state - of - the - art methods .", "entities": []}, {"text": "Our code is available at https://github . com / thunlp / RSN .", "entities": []}, {"text": "1 Introduction Relation extraction ( RE ) aims to extract relational facts between two entities from plain texts .", "entities": [[2, 4, "TaskName", "Relation extraction"]]}, {"text": "Forexample , with the sentence \u201c Hayao Miyazaki is the director of the \ufb01lm \u2018 The Wind Rises \u2019 \" , we can extract a relation \u201c director_of \" between two entities \u201c Hayao Miyazaki \" and \u201c The Wind Rises \" .", "entities": []}, {"text": "Recent progress in supervised methods to RE has achieved great successes .", "entities": []}, {"text": "Supervised meth - ods can effectively learn signi\ufb01cant relation se - mantic patterns based on existing labeled data , but the data constructions are time - consuming andhuman - intensive .", "entities": []}, {"text": "To lower the level of super - vision , several semi - supervised approaches havebeen developed , including bootstrapping , activelearning , label propagation ( Pawar et al . , 2017 ) .", "entities": []}, {"text": "\u2217indicates equal contribution \u2020Corresponding author : Z.Liu(liuzy@tsinghua.edu.cn)Relational Siamese NetworkNovel Relations ( Unlabeled ) Pre - defined Relations ( Auto - labeled / Labeled ) 1 \u2026 2 \u2026 n \u2026 \u2026 Testing Instances Novel Relations ( Extracted ) n+1 \u2026 n+2 \u2026 n+m \u2026 \u2026 clustering \u2026 every rectangle indicates an unlabeled instance of a   novel relation \u2026 metric learning every rectangle indicates   a pre - defined relation   instancerelational   knowledge transfer Figure 1 : A \ufb02owchart of our framework .", "entities": [[59, 61, "TaskName", "metric learning"]]}, {"text": "Our model RSN learns from both labeled instances of pre - de\ufb01nedrelations and unlabeled instances of new relations , andtries to cluster testing instances of new relations .", "entities": []}, {"text": "1 Mintz ( 2009 ) also proposes distant supervision to generate training data automatically .", "entities": []}, {"text": "It assumesthat if two entities have a relation in KBs , all sen - tences that contain these two entities will expressthis relation .", "entities": []}, {"text": "Still , all these approaches can onlyextract pre - de\ufb01ned relations that have already ap - peared either in human - annotated datasets or KBs .", "entities": []}, {"text": "It is hard for them to cover the great variety ofnovel relational facts in the open - domain corpora .", "entities": []}, {"text": "Open relation extraction ( OpenRE ) aims to extract relational facts on the open - domain cor - pus , where the relation types may not be pre - de\ufb01ned .", "entities": [[1, 3, "TaskName", "relation extraction"]]}, {"text": "There are some efforts concentrating onextracting triples with new relation types .", "entities": []}, {"text": "Banko(2008 ) directly extracts words or phrases in sentences to represent new relation types .", "entities": []}, {"text": "However , some relations can not be explicitly represented with tokens in sentences , and it is hardto align different relational tokens that exactlyhave the same meanings .", "entities": []}, {"text": "Yao ( 2011 ) consid1To highlight our model \u2019s ability to extract new relations , testing instances only contain new relations .", "entities": []}, {"text": "220ers OpenRE as a clustering task for extracting triples with new relation types .", "entities": []}, {"text": "However , previ - ous clustering - based OpenRE methods ( Yao et al . , 2011 , 2012 ; Marcheggiani and Titov , 2016 ; Elsahar et al . , 2017 ) are mostly unsupervised , and can not effectively select meaningful relation patternsand discard irrelevant information .", "entities": []}, {"text": "In this paper , we propose to take advantage of high - quality supervised data of pre - de\ufb01ned rela - tions for OpenRE .", "entities": []}, {"text": "The approach is non - trivial , however , due to the considerable gap between thepre - de\ufb01ned relations and novel relations of inter - est in open domain .", "entities": []}, {"text": "To bridge the gap , we proposeRelational Siamese Networks ( RSNs ) to learn transferable relational knowledge from superviseddata for OpenRE .", "entities": []}, {"text": "Speci\ufb01cally , RSNs learn re - lational similarity metrics from labeled data ofpre - de\ufb01ned relations , and then transfer the metrics to measure the similarity of unlabeled sentences for open relation clustering .", "entities": []}, {"text": "We describethe \ufb02owchart of our framework in Figure 1 .", "entities": []}, {"text": "Moreover , we show that RSNs can also be generalized to various weakly - supervised scenar - ios .", "entities": []}, {"text": "We propose Semi - supervised RSN to learn from both supervised data of pre - de\ufb01ned rela - tions and unsupervised data with novel relations , and Distantly - supervised RSN to learn from distantly - supervised data and unsupervised data .", "entities": []}, {"text": "We conduct experiments on real - world RE datasets , FewRel and FewRel - distant , by split - ting relations into seen and unseen set , and eval - uate our models in supervised , semi - supervised , and distantly - supervised scenarios .", "entities": [[10, 11, "DatasetName", "FewRel"], [12, 13, "DatasetName", "FewRel"]]}, {"text": "The resultsdemonstrate that our models signi\ufb01cantly outper - form state - of - the - art baseline methods in all sce - narios without using external linguistic tools .", "entities": []}, {"text": "Tosummarize , the main contributions of this work areas follows : ( 1 ) We develop a novel relational knowledge transfer framework RSN for OpenRE , which caneffectively transfer existing relational knowledgeto novel - relation data and accurately identifynovel relations .", "entities": []}, {"text": "To the best of our knowledge , RSN is the \ufb01rst model to consider knowledgetransfer in clustering - based OpenRE task .", "entities": []}, {"text": "( 2 ) We further propose Semi - supervised RSNs and Distantly - supervised RSNs that can learnfrom various weakly supervised scenarios .", "entities": []}, {"text": "Theexperimental results show that all these RSN mod - els achieve signi\ufb01cant improvements in F - measurecompared with state - of - the - art baselines.2 Related Work Open Relation Extraction .", "entities": [[29, 31, "TaskName", "Relation Extraction"]]}, {"text": "Relation extraction ( RE ) is an important task in NLP .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}, {"text": "Traditional REmethods mainly concentrate on classifying rela - tional facts into pre - de\ufb01ned relation types ( Mintz et al . , 2009 ; Y u et al . , 2017 ) .", "entities": []}, {"text": "Zeng ( 2014 ) utilizes CNN encoders to build sentence representationswith the help of position embeddings .", "entities": []}, {"text": "Lin ( 2016 ) further improves RE performance on distantly - supervised data via instance - level attention .", "entities": []}, {"text": "Thesemethods take advantage of supervised or distantly - supervised data to learn neural sentence encodersfor distributed representations , and have achievedpromising results .", "entities": []}, {"text": "However , these methods can - not handle the open - ended growth of new relationtypes in the open - domain corpora .", "entities": []}, {"text": "To solve this problem , recently many efforts have been invested in exploring methods foropen relation extraction ( OpenRE ) , which aimsto discover new relation types from unsupervisedopen - domain corpora .", "entities": [[15, 17, "TaskName", "relation extraction"]]}, {"text": "OpenRE methods can beroughly divided into two categories : tagging - based and clustering - based .", "entities": []}, {"text": "Tagging - based meth - ods cast OpenRE as a sequence labeling prob - lem , and extract relational phrases consisting ofwords from sentences in unsupervised ( Banko et al . , 2007 ; Banko and Etzioni , 2008 ) or supervised paradigms ( Jia et al . , 2018 ; Cui et", "entities": []}, {"text": "al . , 2018 ; Stanovsky et al . , 2018 ) .", "entities": []}, {"text": "However , tagging - based methods often extract multiple overly - speci\ufb01c re - lational phrases for the same relation type , and can not be readily utilized for downstream tasks .", "entities": []}, {"text": "In comparison , conventional clustering - based OpenRE methods extract rich features for relationinstances via external linguistic tools , and clus - ter semantic patterns into several relation types(Lin and Pantel , 2001 ; Yao et al . , 2011 , 2012 ) .", "entities": []}, {"text": "Marcheggiani ( 2016 ) proposes a reconstructionbased model discrete - state variational autoencoder for OpenRE via unlabeled instances .", "entities": [[11, 13, "MethodName", "variational autoencoder"]]}, {"text": "Elsahar(2017 ) utilizes a clustering algorithm over linguistic features .", "entities": []}, {"text": "In this paper , we focus on theclustering - based OpenRE methods , which have the advantage of discovering highly distinguishable relation types .", "entities": []}, {"text": "Few - shot Learning .", "entities": [[0, 4, "TaskName", "Few - shot Learning"]]}, {"text": "Few - shot learning aims to classify instances with a handful of labeled sam - ples .", "entities": [[0, 4, "TaskName", "Few - shot learning"]]}, {"text": "Many efforts are devoted to few - shot image classi\ufb01cation ( Koch et al . , 2015 ) and relation classi\ufb01cation ( Y uan et al . , 2017 ; Han et al . , 2018 ) .", "entities": []}, {"text": "Notably , ( Koch et al . , 2015 ) introduces Convolu-", "entities": []}, {"text": "221Twain    was   a   writer   of   America   FC   distance classifier   0.7 vl   vr vd p max   word   embeddings position   embeddings FC max Kenji    was   a   poet   of   Japan    Figure 2 : The architecture of Relational Siamese Networks .", "entities": []}, {"text": "The output is the similarity between two rela - tional instances .", "entities": []}, {"text": "tional Siamese Neural Network for image metric learning , which inspires us to learn relational sim - ilarity metrics for OpenRE .", "entities": [[6, 8, "TaskName", "metric learning"]]}, {"text": "Semi - supervised Clustering .", "entities": []}, {"text": "Semi - supervised clustering aims to cluster semantic patterns giveninstance seeds of target categories ( Bair , 2013 ; Hongtao Lin , 2019 ) .", "entities": [[10, 11, "DatasetName", "seeds"]]}, {"text": "Differently , our proposed Semi - supervised RSN only leverages labeled in - stances of pre - de\ufb01ned relations , and does not needany seed of new relations .", "entities": []}, {"text": "3 Methodology Our OpenRE framework mainly consists of twomodules , the relation similarity calculation mod - ule and the relation clustering module .", "entities": []}, {"text": "For rela - tion similarity calculation , we propose RelationalSiamese Networks ( RSNs ) , which learn to pre - dict whether two sentences mention the same re - lation .", "entities": []}, {"text": "To utilize large - scale unsupervised dataand distantly - supervised data , we further proposeSemi - supervised RSN and Distantly - supervisedRSN .", "entities": []}, {"text": "Finally , in the relation clustering module , with the learned relation metric , we utilize hierar - chical agglomerative clustering ( HAC ) and Lou - vain clustering algorithms to cluster target relationinstances of new relation types .", "entities": []}, {"text": "3.1 Relational Siamese Network ( RSN ) The architecture of our Relational Siamese Networks is shown in Figure 2 .", "entities": [[2, 4, "MethodName", "Siamese Network"]]}, {"text": "CNN modules encode a pair of relational instances into vectors , and several shared layers compute their similarity .", "entities": []}, {"text": "Sentence Encoder .", "entities": []}, {"text": "We use a CNN module as the sentence encoder .", "entities": []}, {"text": "The CNN module in - cludes an embedding layer , a convolutional layer , a max - pooling layer , and a fully - connected ( FC ) layer .", "entities": []}, {"text": "The embedding layer transforms the words in a sentence xand the positions of entities e head andetail into pre - trained word embeddings and random - initialized position embeddings .", "entities": [[21, 23, "TaskName", "word embeddings"]]}, {"text": "Follow - ing ( Zeng et al . , 2014 ) , we concatenate these embeddings to form a vector sequence .", "entities": []}, {"text": "Next , a one - dimensional convolutional layer and a maxpooling layer transform the vector sequence intofeatures .", "entities": []}, {"text": "Finally , an FC layer with sigmoid ac - tivation maps features into a relational vector v. To summarize , we obtain a vector representationvfor a relational sentence with our CNN module : v= CNN(s ) , ( 1 ) in which we denote the joint information of a sentencexand two entities in it ehead andetailas a data sample s.", "entities": []}, {"text": "And with paired input relational instances , we have : vl= CNN(s l),vr= CNN(s r ) , ( 2 ) in which two CNN modules are identical and share all the parameters .", "entities": []}, {"text": "Similarity Computation .", "entities": []}, {"text": "Next , to measure the similarity of two relational vectors , we calculatetheir absolute distance and transform it into a real - number similarity p\u2208[0,1 ] .", "entities": []}, {"text": "First , a distance layer computes the element - wise absolute distance oftwo vectors : vd=|vl\u2212vr| .", "entities": []}, {"text": "( 3 ) Then , a classi\ufb01er layer calculates a metric pfor relation similarity .", "entities": []}, {"text": "The layer is a one - dimensional - output FC layer with sigmoid activation : p = \u03c3(kvd+b ) , ( 4 ) in which \u03c3denotes the sigmoid function , kandb denote the weights and bias .", "entities": [[12, 14, "MethodName", "sigmoid activation"]]}, {"text": "To summarize , we obtain a good similarity metric pof relational instances .", "entities": []}, {"text": "Cross Entropy Loss .", "entities": []}, {"text": "The output of RSN pcan also be explained as the probability of two sen - tences mentioning two different relations .", "entities": []}, {"text": "Thus , we can use binary labels qand binary cross entropy loss to train our RSN : Ll = Edl\u223cD l[qln(p\u03b8(dl ) )", "entities": [[11, 12, "MetricName", "loss"]]}, {"text": "+ ( 1 \u2212q)l n ( 1 \u2212p\u03b8(dl))],(5 ) in which\u03b8indicates all the parameters in the RSN .", "entities": []}, {"text": "222labeled data dl     p = 0.7 q = 0   Cross Entropy   Relational   Siamese   Network \u2026   ( a ) Supervised RSN ( auto)-labeled data", "entities": [[9, 10, "DatasetName", "0"]]}, {"text": "dl     q = 0   Cross Entropy    ( + V AT ) Relational   Siamese   Network \u2026     unlabeled data du \u2026 p =", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "0.7 p = 0.6   Conditional Entropy   + V AT   ( b ) Weakly - supervised RSNs Figure 3 : The comparison of ( a ) Supervised RSN and ( b ) Weakly - supervised RSNs .", "entities": []}, {"text": "Weakly - supervised RSNs , including Semi - supervised RSN and Distantlysupervised RSN , further learn from unlabeled data withconditional entropy minimization and virtual adversar - ial training ( V A T ) .", "entities": []}, {"text": "In \ufb01gures , pindicates the predicted similarity of two relational sentences , while qindicates the ground - truth label between them .", "entities": []}, {"text": "3.2 Semi - supervised RSN To discover relation clusters in the open - domain corpus , it is bene\ufb01cial to not only learn from la - beled data , but also capture the manifold of unla - beled data in the semantic space .", "entities": []}, {"text": "To this end , weneed to push the decision boundaries away fromhigh - density areas , which is known as the clusterassumption ( Chapelle and Zien , 2005 ) .", "entities": []}, {"text": "We try to achieve this goal with several additional loss functions .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "In the following paragraphs , we denote the labeled training dataset as D land a couple of labeled relational instances as dl .", "entities": []}, {"text": "Similarly , we denote the unlabeled training dataset as Duand a couple of unlabeled instances as du .", "entities": []}, {"text": "Conditional Entropy Loss .", "entities": []}, {"text": "In classi\ufb01cation problems , a well - classi\ufb01ed embedding space usu - ally reserves large margins between different clas - si\ufb01ed clusters , and optimizing margin can be apromising way to facilitate training .", "entities": []}, {"text": "However , inclustering problems , type labels are not availableduring training .", "entities": []}, {"text": "To optimize margin without ex - plicit supervision , we can push the data pointsaway from the decision boundaries .", "entities": []}, {"text": "Intuitively , when the distance similarity pbetween two relational instances equals 0.5 , there is a high prob - ability that at least one of two instances is near the decision boundary between relation clusters .", "entities": []}, {"text": "Thus , we use the conditional entropy loss ( Grandvalet and Bengio , 2005 ) , which reaches the maximum when p=0.5 , to penalize close - boundary distribution of data points :", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "Lu = Edu\u223cD u[p\u03b8(du)l n ( p\u03b8(du))+ ( 1\u2212p\u03b8(du ) ) ln(1 \u2212p\u03b8(du))].(6 )", "entities": []}, {"text": "Virtual Adversarial Loss .", "entities": []}, {"text": "Despite its theoretical promise , conditional entropy minimizationsuffers from shortcomings in practice .", "entities": []}, {"text": "Due to neu - ral networks \u2019 strong \ufb01tting ability , a very complexdecision hyperplane might be learned so as to keepaway from all the training samples , which lacksgeneralizability .", "entities": []}, {"text": "As a solution , we can smooththe relational representation space with locally - Lipschitz constraint .", "entities": []}, {"text": "To satisfy this constraint , we introduce virtual adversarial training ( Miyato et al . , 2016 ) on both branches of RSN .", "entities": []}, {"text": "Virtual adversarial training cansearch through data point neighborhoods , and pe - nalize most sharp changes in distance prediction .", "entities": []}, {"text": "For labeled data , we have Lvl = Edl\u223cD l[DKL(p\u03b8(dl)||p\u03b8(dl , t1,t2 ) ) ] , ( 7 ) in which D KLindicates the Kullback - Leibler divergence , p\u03b8(dl , t1,t2)indicates a new distance estimation with perturbations t1andt2on both input instances respectively .", "entities": []}, {"text": "Speci\ufb01cally , t1andt2 are worst - case perturbations that maximize the KL divergence between p\u03b8(dl)and p\u03b8(dl , t1,t2)with a limited length .", "entities": []}, {"text": "Empirically , we approximate theperturbations the same as the original paper ( Miyato et al . , 2016 ) .", "entities": []}, {"text": "Speci\ufb01cally , we \ufb01rst add a random noise to the input , and calculate the gradientof the KL - divergence between the outputs of theoriginal input and the noisy input .", "entities": []}, {"text": "We then addthe normalized gradient to the original input andget the perturbed input .", "entities": []}, {"text": "And for unlabeled data , we have Lvu = Edu\u223cD u[DKL(p\u03b8(du)||p\u03b8(du , t1,t2 ) ) ] , ( 8) in which the perturbations t1andt2are added to word embeddings rather than the words them - selves .", "entities": [[26, 28, "TaskName", "word embeddings"]]}, {"text": "To summarize , we use the following loss function to train Semi - supervised RSN , which learnsfrom both labeled and unlabeled data :", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "Lall = Ll+\u03bbvLvl+\u03bbu(Lu+\u03bbvLvu ) , ( 9 ) in which\u03bbvand\u03bbuare two hyperparameters .", "entities": []}, {"text": "2233.3 Distantly - supervised RSN To alleviate the intensive human labor for annotation , the topic of distantly - supervised learning hasattracted much attention in RE .", "entities": []}, {"text": "Here , we proposeDistantly - supervised RSN , which can learn fromboth distantly - supervised data and unsuperviseddata for relational knowledge transfer .", "entities": []}, {"text": "Speci\ufb01-cally , we use the following loss function :", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "Lall = Ll+\u03bbu(Lu+\u03bbvLvu ) , ( 10 ) which treats auto - labeled data as labeled data but removes the virtual adversarial loss on the auto - labeled data .", "entities": [[22, 23, "MetricName", "loss"]]}, {"text": "The reason to remove the loss is simple : virtual adversarial training on auto - labeled data canamplify the noise from false labels .", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "Indeed , wedo \ufb01nd that the virtual adversarial loss on auto - labeled data can harm our model \u2019s performance inexperiments .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "We do not use more denoising methods , since we think RSN has some inherent advantages oftolerating such noise .", "entities": [[5, 6, "TaskName", "denoising"]]}, {"text": "Firstly , the noise will beoverwhelmed by the large proportion of negativesampling during training .", "entities": []}, {"text": "Secondly , during clus - tering , the prediction of a new relation cluster isbased on areas where the density of relational in - stances is high .", "entities": []}, {"text": "Outliers from noise , as a result , will not in\ufb02uence the prediction process so much .", "entities": []}, {"text": "3.4 Open Relation Clustering After RSN is learned , we can use RSN to calculate the similarity matrix of testing instances .", "entities": []}, {"text": "With thismatrix , several clustering methods can be appliedto extract new relation clusters .", "entities": []}, {"text": "Hierarchical Agglomerative Clustering .", "entities": []}, {"text": "The \ufb01rst clustering method we adopt is hierarchical ag - glomerative clustering ( HAC ) .", "entities": []}, {"text": "HAC is a bottom - up clustering algorithm .", "entities": []}, {"text": "At the start , every testing instance is regarded as a cluster .", "entities": []}, {"text": "For every step , itagglomerates two closest instances .", "entities": []}, {"text": "There are sev - eral criteria to evaluate the distance between twoclusters .", "entities": []}, {"text": "Here , we adopt the complete - linkage cri - terion , which is more robust to extreme instances .", "entities": []}, {"text": "However , there is a signi\ufb01cant shortcoming of HAC : it needs the exact number of clusters in ad - vance .", "entities": []}, {"text": "A potential solution is to stop agglomerat - ing according to an empirical distance threshold , but it is hard to determine such a threshold .", "entities": []}, {"text": "Thisproblem leads us to consider another clustering al - gorithm Louvain ( Blondel et al . , 2008 ) .Louvain .", "entities": []}, {"text": "Louvain is a graph - based clustering algorithm traditionally used for detecting commu - nities .", "entities": []}, {"text": "To construct the graph , we use the binaryapproximation of RSN \u2019s output , with 0indicating an edge between two nodes .", "entities": []}, {"text": "The advantageof Louvain is that it does not need the number ofpotential clusters beforehand .", "entities": []}, {"text": "It will automatically\ufb01nd proper sizes of clusters by optimizing commu - nity modularity .", "entities": []}, {"text": "According to the experiments weconduct , Louvain performs better than HAC .", "entities": []}, {"text": "After running , Louvain might produce a number of singleton clusters with few instances .", "entities": []}, {"text": "It is notproper to call these clusters new relation types , sowe label these instances the same as their closestlabeled neighbors .", "entities": []}, {"text": "Finally , we want to explain the reason why we do not use some other common clustering methodslike K - Means , Mean - Shift and Ward \u2019s ( Ward Jr , 1963 ) method of HAC : these methods calculate the centroid of several points during clustering by merely averaging them .", "entities": []}, {"text": "However , the relation vectors in our model are high - dimensional , and thedistance metric described by RSN is non - linear .", "entities": []}, {"text": "Consequently , it is not proper to calculate the cen - troid by simply averaging the vectors .", "entities": []}, {"text": "4 Experiments In this section , we conduct several experiments onreal - world RE datasets to show the effectivenessof our models , and give a detailed analysis to showits advantages .", "entities": []}, {"text": "4.1 Dataset In experiments , we use FewRel ( Han et al . , 2018 ) as our \ufb01rst dataset .", "entities": [[7, 8, "DatasetName", "FewRel"]]}, {"text": "FewRel is a human - annotateddataset containing 80types of relations , each with 700 instances .", "entities": [[0, 1, "DatasetName", "FewRel"]]}, {"text": "An advantage of FewRel is that every instance contains a unique entity pair , so REmodels can not choose the easy way to memorizethe entities .", "entities": [[3, 4, "DatasetName", "FewRel"]]}, {"text": "We use the original train set of FewRel , which contains 64relations , as labeled set with prede\ufb01ned relations , and the original validation set ofFewRel , which contains 16new relations , as the unlabeled set with novel relations to extract .", "entities": [[7, 8, "DatasetName", "FewRel"]]}, {"text": "Wethen randomly choose 1,600 instances from the unlabeled set as the test set , with the rest labeledand unlabeled instances considered as the train set .", "entities": []}, {"text": "The second dataset we use is FewRel - distant , which contains the distantly - supervised data ob - tained by the authors of FewRel before human an-", "entities": [[6, 7, "DatasetName", "FewRel"], [24, 25, "DatasetName", "FewRel"]]}, {"text": "224notation .", "entities": []}, {"text": "We follow the split of FewRel to obtain the auto - labeled train set and unlabeled train set .", "entities": [[5, 6, "DatasetName", "FewRel"]]}, {"text": "For evaluation , we use the human - annotated test set of FewRel with 1,600 instances .", "entities": [[12, 13, "DatasetName", "FewRel"]]}, {"text": "Unlabeled instances already existing in this test set are removedfrom the unlabeled train set of FewRel - distant .", "entities": [[15, 16, "DatasetName", "FewRel"]]}, {"text": "Fi - nally , the auto - labeled train set contains 323,549 relational instances , and the unlabeled train set contains 60,581 instances .", "entities": []}, {"text": "A previous OpenRE work reports performance on an unpublic dataset called NYT - FB ( Marcheggiani and Titov , 2016 ) .", "entities": []}, {"text": "However , it has several shortcomings compared with FewRel - distant .", "entities": [[8, 9, "DatasetName", "FewRel"]]}, {"text": "First , NTY -FB \u2019s test set is distantly - supervisedand is noisy for instance - level RE .", "entities": []}, {"text": "Moreover , in - stances in NYT - FB often share entity pairs or re - lational phrases , which makes it much easier forrelation clustering .", "entities": []}, {"text": "Therefore , we think the re - sults on FewRel - distant are convincing enough forDistantly - supervised OpenRE .", "entities": [[9, 10, "DatasetName", "FewRel"]]}, {"text": "4.2 Implementation Details Data Sampling .", "entities": []}, {"text": "The input of RSN should be a pair of sampled instances .", "entities": []}, {"text": "For the unlabeledset , the only possible sampling method is to selecttwo instances randomly .", "entities": []}, {"text": "For the labeled set , how - ever , random selection would result in too manydifferent - relation pairs , and cause severe biasesfor RSN .", "entities": []}, {"text": "To solve this problem , we use down - sampling .", "entities": []}, {"text": "In our experiments , we \ufb01x the percent - age of same - relation pairs in every labeled databatch as 6 % .", "entities": []}, {"text": "Let us denote this percentage number as the sample ratio for convenience .", "entities": []}, {"text": "Experimental re - sults show that the sample ratio decides RSN\u2019stendency to predict larger or smaller clusters .", "entities": []}, {"text": "Inother words , it controls the granularity of the pre - dicted relation types .", "entities": []}, {"text": "This phenomenon suggests apotential application of our model in hierarchicalrelation extraction .", "entities": []}, {"text": "However , we leave any seriousdiscussion to future work .", "entities": []}, {"text": "Hyperparameter Settings .", "entities": []}, {"text": "Following ( Lin et al . , 2016 ) and ( Zeng et al . , 2014 ) , we \ufb01x the less in\ufb02uencing hyperparameters for sentence en - coding as their reported optimal values .", "entities": []}, {"text": "For wordembeddings , we use pre - trained 50 - dimensional Glove ( Pennington et al . , 2014 ) word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "For position embeddings , we use randominitialized 5 - dimensional position embeddings .", "entities": []}, {"text": "During training , all the embeddings are trainable .", "entities": []}, {"text": "For the neural network , the number of featuremaps in the convolutional layer is 230 .", "entities": []}, {"text": "The \ufb01lter length is 3 .", "entities": []}, {"text": "The activation function after the max - pooling layer is ReLU , and the activationfunctions after FC layers are sigmoid .", "entities": [[1, 3, "HyperparameterName", "activation function"], [10, 11, "MethodName", "ReLU"]]}, {"text": "Besides , we adopt two regularization methods in the CNNmodule .", "entities": []}, {"text": "We put a dropout layer right after theembedding layer as ( Miyato et al . , 2016 ) .", "entities": []}, {"text": "The dropout rate is 0.2 .", "entities": []}, {"text": "We also impose L2 regularization on the convolutional layer and the FC layer , with parameters of 0.0002 and0.001 respectively .", "entities": [[3, 5, "HyperparameterName", "L2 regularization"]]}, {"text": "Hyperparameters for virtual adversarial trainingare just the same as ( Miyato et al . , 2016 ) proposed .", "entities": []}, {"text": "At the same time , major hyperparameters are selected with grid search according to the modelperformance on a validation set .", "entities": []}, {"text": "Speci\ufb01cally , thevalidation set contains 10,000 randomly chosensentence pairs from the unlabeled set ( i.e. 16 novelrelations ) and does not overlap with the test set .", "entities": []}, {"text": "The model is evaluated according to the precisionof binary classi\ufb01cation of sentence pairs on thevalidation set , which is an estimation for models\u2019clustering ability .", "entities": []}, {"text": "We do not use F1 during modelvalidation because the clustering steps are time - consuming .", "entities": [[4, 5, "MetricName", "F1"]]}, {"text": "For optimization , we use Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0001 , which is selected from{0.1,0 .01,0.001,0.0001,0.00001 } .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [16, 18, "HyperparameterName", "learning rate"]]}, {"text": "The batch size is 100 selected from { 25,50,100 } .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "For hyperparameters in Equation 9and Equation 10,\u03bbv is1.0selected from { 0.1,0.5,1.0,2.0}and\u03bbuis 0.03selected from { 0.01,0.02,0.03,0.04,0.05 } .", "entities": []}, {"text": "For baseline models , original papers do grid search for all possible hyperparameters and reportthe best result during testing .", "entities": []}, {"text": "We follow their set - tings and do grid search directly on the test set .", "entities": []}, {"text": "4.3 Experiment Results on OpenRE In this section , we demonstrate the effectiveness of our RSN models by comparing our mod - els with state - of - the - art clustering - based OpenREmethods .", "entities": []}, {"text": "We also conduct ablation experimentsto detailedly investigate the contributions of dif - ferent mechanisms of Semi - supervised RSN andDistantly - supervised RSN .", "entities": []}, {"text": "Baselines .", "entities": []}, {"text": "Conventional clustering - based OpenRE models usually cluster instances by either clustering their linguistic features ( Lin and Pantel , 2001 ;", "entities": []}, {"text": "Yao et al . , 2012 ; Elsahar et al . , 2017 ) or imposing reconstruction constraints ( Yao et al . , 2011 ; Marcheggiani and Titov , 2016 ) .", "entities": []}, {"text": "To demonstrate", "entities": []}, {"text": "225the effectiveness of our RSN models , we compare our models with two state - of - the - art models : ( 1 ) HAC with re - weighted word embeddings ( RW - HAC ) ( Elsahar et al . , 2017 ): RW - HAC is the state - of - the - art feature clustering model forOpenRE .", "entities": [[30, 32, "TaskName", "word embeddings"]]}, {"text": "The model \ufb01rst extracts KB types andNER tags of entities as well as re - weighted wordembeddings from sentences , then adopts principalcomponent analysis ( PCA ) to reduce feature di - mensionality , and \ufb01nally uses HAC to cluster theconcatenation of reduced feature representations .", "entities": [[25, 26, "MethodName", "PCA"]]}, {"text": "( 2 ) Discrete - state variational autoencoder ( V AE ) ( Marcheggiani and Titov , 2016 ): V AE is the state - of - the - art reconstruction - based modelfor OpenRE via unlabeled instances .", "entities": [[6, 8, "MethodName", "variational autoencoder"], [10, 11, "MethodName", "AE"], [20, 21, "MethodName", "AE"]]}, {"text": "It optimizesa relation classi\ufb01er by reconstructing entities frompairing entities and predicted relation types .", "entities": []}, {"text": "Richfeatures including entity words , context words , trigger words , dependency paths , and contextPOS tags are used to predict the relation type .", "entities": []}, {"text": "RW - HAC and V AE both rely on external linguistic tools to extract rich features from plaintexts .", "entities": [[5, 6, "MethodName", "AE"]]}, {"text": "Speci\ufb01cally , we \ufb01rst align entities to Wiki - data and get their KB types .", "entities": []}, {"text": "Next , we preprocessthe instances with part - of - speech ( POS ) tagging , named - entity recognition ( NER ) , and dependencyparsing with Stanford CoreNLP ( Manning et al . , 2014 ) .", "entities": [[6, 9, "DatasetName", "part - of"], [21, 22, "TaskName", "NER"]]}, {"text": "It is worth noting that these features are only used by baseline models .", "entities": []}, {"text": "Our models , in con - trast , only use sentences and entity pairs as inputs .", "entities": []}, {"text": "Evaluation Protocol .", "entities": []}, {"text": "In evaluation , we use B 3 metric ( Bagga and Baldwin , 1998 ) as the scoring function .", "entities": []}, {"text": "B3metric is a standard measure to balance the precision and recall of clusteringtasks , and is commonly used in previous OpenREworks ( Marcheggiani and Titov , 2016 ; Elsahar et al . , 2017 ) .", "entities": []}, {"text": "To be speci\ufb01c , we use F 1measure , the harmonic mean of precision and recall .", "entities": []}, {"text": "First , we report the result of supervised RSN with different clustering methods .", "entities": []}, {"text": "Speci\ufb01cally , SN represents the original RSN structure , HAC and Lindicate HAC and Louvain clustering introduced in Sec .", "entities": []}, {"text": "3.3 .", "entities": []}, {"text": "The result shows that Louvain performs better than HAC , so in the following ex - periments we focus on using Louvain clustering .", "entities": []}, {"text": "Next , for Semi - supervised and Distantlysupervised RSN , we conduct various combina - tions of different mechanisms to verify the contri - bution of each part .", "entities": []}, {"text": "( + C ) indicates that the model is powered up with conditional entropy minimiza - tion , while ( + V ) indicates that the model is pow - FewRel FewRel - distant Approach PRF 1PRF 1 V AE 17.9", "entities": [[30, 31, "DatasetName", "FewRel"], [31, 32, "DatasetName", "FewRel"], [39, 40, "MethodName", "AE"]]}, {"text": "69.7 28.5 17.9 69.7 28.5 RW - HAC 31.8 46.0 37.6 31.8 46.0 37.6 SN - HAC 36.2 53.3 43.1 34.5 53.3 41.5SN - L 36.5 69.2 47.8 34.6 59.8 43.9SN - L+V 46.1 77.3 57.8 40.7 52.4 45.8SN - L+C 47.1 78.1 58.8 42.3 66.0 51.5 SN - L+CV 148.9 77.5 59.9 40.8 74.0 52.6 Table 1 : Precision , recall and F1 results ( % ) for different models .", "entities": [[59, 60, "MetricName", "Precision"], [63, 64, "MetricName", "F1"]]}, {"text": "The \ufb01rst two models are baselines .", "entities": []}, {"text": "The next \ufb01ve models are different variants of our model .", "entities": []}, {"text": "ered up with virtual adversarial training .", "entities": []}, {"text": "Experimental Result Analysis .", "entities": []}, {"text": "Table 1shows the experimental results , from which we can ob - serve that : ( 1 ) RSN models outperform all baseline models on precision , recall , and F1 - score , among whichWeakly - supervised RSN ( SN - L+CV ) achievesstate - of - the - art performances .", "entities": [[30, 33, "MetricName", "F1 - score"]]}, {"text": "This indicates thatRSN is capable of understanding new relations\u2019semantic meanings within sentences .", "entities": []}, {"text": "( 2 ) Supervised and distantly - supervised relational representations improve clustering performances .", "entities": []}, {"text": "Compared with RW - HAC , SN - HAC achieves better clustering results because of itssupervised relational representation and similar - ity metric .", "entities": []}, {"text": "Speci\ufb01cally , unsupervised baselinesmainly use sparse one - hot features .", "entities": []}, {"text": "RW - HAC usesword embeddings , but integrates them in a rule - based way .", "entities": []}, {"text": "In contrast , RSN uses distributed fea - ture representations , and can optimize informationintegration process according to supervision .", "entities": []}, {"text": "( 3 ) Louvain outperforms HAC for clustering with RSN , comparing SN - HAC with SN - L. Oneexplanation is that our model does not put addi - tional constraints on the prior distribution of rela - tional vectors , and therefore the relation clustersmight have odd shapes in violation of HAC \u2019s as - sumption .", "entities": []}, {"text": "Moreover , when representations are notdistinguishable enough , forcing HAC to \ufb01nd \ufb01ne - grained clusters may harm recall while contributing minimally to precision .", "entities": []}, {"text": "In practice , we do observe that the number of relations SN - L extracts isconstantly less than the true number 16 .", "entities": []}, {"text": "( 4 ) Both SN - L+V and SN - L+C improve the performance of supervised or distantly - supervised 1Here for FewRel - distant we use Equation 10rather than Equation 9as loss , which corresponds to Distantlysupervised RSN , and this brings a minor improvement on F1 from 52.0%to52.6 % .", "entities": [[22, 23, "DatasetName", "FewRel"], [32, 33, "MetricName", "loss"], [47, 48, "MetricName", "F1"]]}, {"text": "226 ( a ) RSN ( b ) Semi - supervised RSN ( c ) Supervised CNN Figure 4 : The t - SNE visualization of the output vectors of CNN modules in our ( a ) OpenRE model RSN , ( b ) Semi - supervised RSN facilitated by unlabeled novel - relation data and in ( c ) a classical RE baseline trained withlabeled novel - relation data .", "entities": []}, {"text": "All \ufb01gures visualize the clustering result for 402 instances of 4novel relations .", "entities": []}, {"text": "40 48 56 64 Number of Pre - defined Training Relations40.042.545.047.550.052.555.057.560.0F1 Score SN - L+CV SN - L+V SN - L+C", "entities": [[11, 12, "MetricName", "Score"]]}, {"text": "SN - L Figure 5 : The clustering results with different numbers of pre - de\ufb01ned training relations .", "entities": []}, {"text": "RSN by further utilizing unsupervised corpora .", "entities": []}, {"text": "Both semi - supervised approaches bring signi\ufb01cantimprovements for F 1scores by increasing the precision and recall , and combining both can furtherincrease the F 1score .", "entities": []}, {"text": "( 5 ) One interesting observation is that SN - L+V does not outperform SN - L so much on FewRel - distant .", "entities": [[20, 21, "DatasetName", "FewRel"]]}, {"text": "This is probably because V A T on the noisy data might amplify the noise .", "entities": []}, {"text": "In further experiments , we perform V A T only on unlabeled setand observe improvements on F 1 , with SN - L+V from 45.8%to49.2%and SN - L+CV from 52.0 % to52.6 % , which proves this conjecture .", "entities": []}, {"text": "4.4", "entities": []}, {"text": "The In\ufb02uence of Pre - de\ufb01ned Relation Diversity on Generalizability In this subsection , we mainly focus on analyzing the in\ufb02uence of pre - de\ufb01ned relation diversity , i.e. ,the number of relations in the labeled train set .", "entities": []}, {"text": "Tostudy this in\ufb02uence , we use FewRel for evaluationand change the number of relations in the labeledtrain set from 40to64while \ufb01xing the total num - ber of labeled instances to 25,000 , and report the clustering results in Figure 5 .", "entities": [[6, 7, "DatasetName", "FewRel"]]}, {"text": "Several conclusions can be drawn according to Figure 5 .", "entities": []}, {"text": "Firstly , a rich variety of labeled relations do improve the performance of our models , especially RSN .", "entities": []}, {"text": "The models trained on 64 rela - tions perform better than those trained on 40relations constantly .", "entities": []}, {"text": "Secondly , while the performanceof supervised RSN is very sensitive to pre - de\ufb01nedrelation diversity , its semi - supervised counterpartssuffer much less from the relation number limit .", "entities": []}, {"text": "This phenomenon suggests that Semi - supervisedRSNs succeed in learning from unlabeled novel - relation data and are more generalizable to novelrelations .", "entities": []}, {"text": "4.5 Relational Knowledge Representation Visualization To intuitively evaluate the knowledge transfer effects of RSN and Semi - supervised RSN , we visualize their relational knowledge representation spaces in the last layer of CNN encoders with t - SNE ( Maaten and Hinton , 2008 ) in Figure 4.W e also compare with a supervised CNN trained on9,600 labeled instances of novel relations , which suggests the optimal relational knowledge representation .", "entities": []}, {"text": "In each \ufb01gure , we plot 402 relation instances of 4randomly - chosen relation types in the test set , and points are colored according to theirground - truth labels .", "entities": []}, {"text": "As we can see from Figure 4 , RSN is able to roughly distinguish different relations , andSemi - supervised RSN further facilitated knowl - edge transfer by optimizing the margin betweenpotential relation clusters during training .", "entities": []}, {"text": "As a re - sult , Semi - supervised RSN can extract more dis - tinguishable novel relations , and gains comparable", "entities": []}, {"text": "227relational knowledge representation ability with supervised CNN .", "entities": []}, {"text": "5 Conclusions and Future Work", "entities": []}, {"text": "In this paper , we propose a new model Rela - tional Siamese Network ( RSN ) for OpenRE .", "entities": [[12, 14, "MethodName", "Siamese Network"]]}, {"text": "Dif - ferent from conventional unsupervised models , our model learns to measure relational similarityfrom supervised / distantly - supervised data of pre - de\ufb01ned relations , as well as unsupervised data ofnovel relations .", "entities": []}, {"text": "There are mainly two innovativepoints in our model .", "entities": []}, {"text": "First , we propose to transferrelational similarity knowledge with RSN struc - ture .", "entities": []}, {"text": "To the best of our knowledge , we are the \ufb01rstto propose knowledge transfer for OpenRE .", "entities": []}, {"text": "Sec - ond , we propose Semi / Distantly - supervised RSN , to further perform semi - supervised and distantly - supervised transfer learning .", "entities": [[23, 25, "TaskName", "transfer learning"]]}, {"text": "Experiments show that our models signi\ufb01cantly surpass conventionalOpenRE models and achieve new state - of - the - artperformance .", "entities": []}, {"text": "For future research , we plan to explore the following directions : ( 1 ) Besides CNN , thereare some other popular sentence encoder struc - tures like piecewise convolutional neural network(PCNN ) and Long Short - Term Memory ( LSTM)for RE .", "entities": [[34, 39, "MethodName", "Long Short - Term Memory"]]}, {"text": "In the future , we can try different sentenceencoders in our model .", "entities": []}, {"text": "( 2 ) As mentioned above , our model has the potential ability to discover thehierarchical structure of relations .", "entities": []}, {"text": "In the future , we will try to explore this application with addi - tional experiments .", "entities": []}, {"text": "6 Acknowledgement This work is supported by the National Key Re - search and Development Program of China ( No.2018YFB1004503 ) and the National Natural Sci - ence Foundation of China ( NSFC No . 61572273,61661146007 ) .", "entities": []}, {"text": "Ruidong Wu is also supported byTsinghua University Initiative Scienti\ufb01c ResearchProgram .", "entities": []}, {"text": "References Amit Bagga and Breck Baldwin .", "entities": []}, {"text": "1998 .", "entities": []}, {"text": "Algorithms for scoring coreference chains .", "entities": []}, {"text": "In Proceedings of the First Iternational Conference on Language Re - sources and Evaluation Workshop on LinguisticsCoreference .", "entities": []}, {"text": "Eric Bair .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Semi - supervised clustering methods .", "entities": []}, {"text": "Wiley Interdisciplinary Reviews : Computational Statistics .Michele", "entities": []}, {"text": "Banko , Michael J Cafarella , Stephen Soderland , Matthew Broadhead , and Oren Etzioni . 2007.Open", "entities": []}, {"text": "information extraction from the web .", "entities": []}, {"text": "In Proceedings of IJCAI .", "entities": []}, {"text": "Michele Banko and Oren Etzioni . 2008 .", "entities": []}, {"text": "The tradeoffs between open and traditional relation extraction .", "entities": [[6, 8, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of ACL - HLT .", "entities": []}, {"text": "Vincent D Blondel , Jean Loup Guillaume , Renaud Lambiotte , and Etienne Lefebvre . 2008 .", "entities": []}, {"text": "Fast un - folding of communities in large networks .", "entities": []}, {"text": "Journal of Statistical Mechanics .", "entities": []}, {"text": "Olivier Chapelle and Alexander Zien .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Semisupervised classi\ufb01cation by low density separation .", "entities": []}, {"text": "InProceedings of AISTATS .", "entities": []}, {"text": "Lei Cui , Furu Wei , and Ming Zhou .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural open information extraction .", "entities": [[1, 4, "TaskName", "open information extraction"]]}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Hady Elsahar , Elena Demidova , Simon Gottschalk , Christophe Gravier , and Frederique Laforest .", "entities": []}, {"text": "2017.Unsupervised open relation extraction .", "entities": [[2, 4, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of European Semantic Web Conference .", "entities": []}, {"text": "Yves Grandvalet and Y oshua Bengio .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Semisupervised learning by entropy minimization .", "entities": []}, {"text": "In Proceedings of NIPS .", "entities": []}, {"text": "Xu Han , Hao Zhu , Pengfei Y u , Ziyun Wang , Y uan Yao , Zhiyuan Liu , and Maosong Sun .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Fewrel : Alarge - scale supervised few - shot relation classi\ufb01cation dataset with state - of - the - art evaluation .", "entities": [[0, 1, "DatasetName", "Fewrel"]]}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Meng Qu Xiang Ren Hongtao Lin , Jun Yan . 2019 .", "entities": []}, {"text": "Learning dual retrieval module for semi - supervisedrelation extraction .", "entities": []}, {"text": "In Proceedings of WWW .", "entities": []}, {"text": "Shengbin Jia , Yang Xiang , and Xiaojun Chen .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Supervised neural models revitalize the open rela - tion extraction .", "entities": []}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Diederik P Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Gregory Koch , Richard Zemel , and Ruslan Salakhutdinov .", "entities": [[7, 8, "DatasetName", "Ruslan"]]}, {"text": "2015 .", "entities": []}, {"text": "Siamese neural networks for one - shotimage recognition .", "entities": []}, {"text": "In Proceedings of ICML Deep Learning Workshop , volume 2 .", "entities": []}, {"text": "Dekang Lin and Patrick Pantel . 2001 .", "entities": []}, {"text": "Dirt - discovery of inference rules from text .", "entities": []}, {"text": "In Proceedings of KDD .", "entities": []}, {"text": "Yankai Lin , Shiqi Shen , Zhiyuan Liu , Huanbo Luan , and Maosong Sun . 2016 .", "entities": []}, {"text": "Neural relation extractionwith selective attention over instances .", "entities": []}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "Laurens van der Maaten and Geoffrey Hinton .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Visualizing data using t - sne .", "entities": []}, {"text": "JMLRJournal of Statistical Mechanics .", "entities": []}, {"text": "Christopher D. Manning , Mihai Surdeanu , John Bauer , Jenny Finkel , Steven J. Bethard , and David Mc - Closky .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The Stanford CoreNLP natural language processing toolkit .", "entities": []}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "228Diego Marcheggiani and Ivan Titov .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Discretestate variational autoencoders for joint discovery andfactorization of relations .", "entities": [[2, 3, "MethodName", "autoencoders"]]}, {"text": "Transactions of ACL .", "entities": []}, {"text": "Mike Mintz , Steven Bills , Rion Snow , and Daniel Jurafsky . 2009 .", "entities": []}, {"text": "Distant supervision for relation extrac - tion without labeled data .", "entities": []}, {"text": "In Proceedings of ACLIJCNLP .", "entities": []}, {"text": "Takeru Miyato , Andrew M Dai , and Ian Goodfellow .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Adversarial training methods for semi - supervised text classi\ufb01cation .", "entities": []}, {"text": "arXiv .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sachin Pawar , Girish K. Palshikar , and Pushpak Bhattacharyya .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Relation extraction : A survey.arXiv .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}, {"text": "Jeffrey Pennington , Richard Socher , and Christoper Manning . 2014 .", "entities": []}, {"text": "Glove : Global vectors for wordrepresentation .", "entities": []}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Gabriel Stanovsky , Julian Michael , Luke Zettlemoyer , and Ido Dagan .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Supervised open informationextraction .", "entities": []}, {"text": "In Proceedings of NAACL .", "entities": []}, {"text": "Joe H Ward Jr. 1963 .", "entities": []}, {"text": "Hierarchical grouping to optimize an objective function .", "entities": []}, {"text": "Journal of the American Statistical Association .", "entities": []}, {"text": "Limin Yao , Aria Haghighi , Sebastian Riedel , and Andrew Mccallum .", "entities": [[3, 4, "MethodName", "Aria"]]}, {"text": "2011 .", "entities": []}, {"text": "Structured relation discov - ery using generative models .", "entities": []}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Limin Yao , Sebastian Riedel , and Andrew McCallum .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Unsupervised relation discovery with sensedisambiguation .", "entities": []}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "Dian Y u , Lifu Huang , and Heng Ji . 2017 .", "entities": []}, {"text": "Open relation extraction and grounding .", "entities": [[1, 3, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of IJCNLP .", "entities": []}, {"text": "Jianbo Y uan , Han Guo , Zhiwei Jin , Hongxia Jin , Xianchao Zhang , and Jiebo Luo . 2017 .", "entities": []}, {"text": "One - shot learn - ing for \ufb01ne - grained relation extraction via convolu - tional siamese neural network .", "entities": [[10, 12, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of BigData .", "entities": []}, {"text": "Daojian Zeng , Kang Liu , Siwei Lai , Guangyou Zhou , Jun Zhao , et al . 2014 .", "entities": []}, {"text": "Relation classi\ufb01cation viaconvolutional deep neural network .", "entities": []}, {"text": "In Proceedings of COLING .", "entities": []}]