[{"text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Short Papers ) , pages 138\u2013143 Melbourne , Australia , July 15 - 20 , 2018 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics138Analogical Reasoning on Chinese Morphological and Semantic Relations Shen Li1,2,\u007fZhe Zhao3,|Renfen Hu1,2,\u007f,yWensi Li1,2,\u0007Tao Liu3,|Xiaoyong Du3,| \u007f{shen , irishere}@mail.bnu.edu.cn |{helloworld , tliu , duyong}@ruc.edu.cn \u0007zjklws@163.com 1Institute of Chinese Information Processing , Beijing Normal University 2UltraPower - BNU Joint Laboratory for Arti\ufb01cial Intelligence , Beijing Normal University 3School of Information , Renmin University of China Abstract Analogical reasoning is effective in capturing linguistic regularities .", "entities": []}, {"text": "This paper proposes an analogical reasoning task on Chinese .", "entities": []}, {"text": "After delving into Chinese lexical knowledge , we sketch 68 implicit morphological relations and 28 explicit semantic relations .", "entities": []}, {"text": "A big and balanced dataset CA8 is then built for this task , including 17813 questions .", "entities": []}, {"text": "Furthermore , we systematically explore the in\ufb02uences of vector representations , context features , and corpora on analogical reasoning .", "entities": []}, {"text": "With the experiments , CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings .", "entities": [[15, 17, "TaskName", "word embeddings"]]}, {"text": "1 Introduction Recently , the boom of word embedding draws our attention to analogical reasoning on linguistic regularities .", "entities": []}, {"text": "Given the word representations , analogy questions can be automatically solved via vector computation , e.g. \u201c apples - apple + car \u0019 cars \u201d for morphological regularities and \u201c king man + woman\u0019queen \u201d for semantic regularities ( Mikolov et al . , 2013 ) .", "entities": []}, {"text": "Analogical reasoning has become a reliable evaluation method for word embeddings .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "In addition , It can be used in inducing morphological transformations ( Soricut and Och , 2015 ) , detecting semantic relations ( Herdagdelen and Baroni , 2009 ) , and translating unknown words ( Langlais and Patry , 2007 ) .", "entities": []}, {"text": "It is well known that linguistic regularities vary a lot among different languages .", "entities": []}, {"text": "For example , Chinese is a typical analytic language which lacks in\ufb02ection .", "entities": []}, {"text": "Figure 1 shows that function words and reduplication are used to denote grammatical and semantic information .", "entities": []}, {"text": "In addition , many semantic yCorresponding author .", "entities": []}, {"text": "r\u00e9n \t \u4eba r\u00e9n \t r\u00e9n \t \u4eba\u4eba person every \t person + \u0101n \t \u5929", "entities": []}, {"text": "+ \u0101n \t + \u0101n \t \u5929\u5929 day every \t day ( a ) ( b ) easier g\u00e8ng \t \u66f4 ji\u01cen \t d\u0101n \t \u7b80\u5355 easy ji\u01cen \t d\u0101n \t \u7b80\u5355", "entities": []}, {"text": "xi\u0113 \t \u4e9b easiest zu\u00ec \t \u6700 ji\u01cen \t d\u0101n \t \u7b80\u5355 ji\u01cen \t d\u0101n \t \u7b80\u5355", "entities": []}, {"text": "Figure 1 : Examples of Chinese lexical knowledge : ( a ) function words ( in orange boxes ) are used to indicate the comparative and superlative degrees ; ( b ) reduplication yields the meaning of \u201c every \u201d .", "entities": []}, {"text": "relations are closely related with social and cultural factors , e.g. in Chinese \u201c sh\u00af\u0131 - xi\u00afan \u201d ( god of poetry ) refers to the poet Li - bai and\u201csh\u00af\u0131 - sh\u00e8ng \u201d ( saint of poetry ) refers to the poet Du - fu .", "entities": []}, {"text": "However , few attempts have been made in Chinese analogical reasoning .", "entities": []}, {"text": "The only Chinese analogy dataset is translated from part of an English dataset ( Chen et al . , 2015 ) ( denote as CA_translated ) .", "entities": []}, {"text": "Although it has been widely used in evaluation of word embeddings ( Yang and Sun , 2015 ; Yin et", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "al . , 2016 ; Su and Lee , 2017 ) , it could not serve as a reliable benchmark since it includes only 134 unique Chinese words in three semantic relations ( capital , state , and family ) , and morphological knowledge is not even considered .", "entities": []}, {"text": "Therefore , we would like to investigate linguistic regularities beneath Chinese .", "entities": []}, {"text": "By modeling them as an analogical reasoning task , we could further examine the effects of vector offset methods in detecting Chinese morphological and semantic relations .", "entities": []}, {"text": "As far as we know , this is the \ufb01rst study focusing on Chinese analogical reasoning .", "entities": []}, {"text": "Moreover , we release a standard benchmark for evaluation of Chinese word embedding , together with 36 open - source pre - trained embeddings at", "entities": []}, {"text": "139GitHub1 , which could serve as a solid basis for Chinese NLP tasks .", "entities": []}, {"text": "2 Morphological Relations Morphology concerns the internal structure of words .", "entities": []}, {"text": "There is a common belief that Chinese is a morphologically impoverished language since a morpheme mostly corresponds to an orthographic character , and it lacks apparent distinctions between roots and af\ufb01xes .", "entities": []}, {"text": "However , Packard ( 2000 ) suggests that Chinese has a different morphological system because it selects different \u201c settings \u201d on parameters shared by all languages .", "entities": []}, {"text": "We will clarify this special system by mapping its morphological analogies into two processes : reduplication and semi - af\ufb01xation .", "entities": []}, {"text": "2.1 Reduplication Reduplication means a morpheme is repeated to form a new word , which is semantically and/or syntactically distinct from the original morpheme , e.g. the word \u201c ti\u00afan - ti \u00afan\u201d(day day ) in Figure 1(b ) means \u201c everyday \u201d .", "entities": []}, {"text": "By analyzing all the word categories in Chinese , we \ufb01nd that nouns , verbs , adjectives , adverbs , and measure words have reduplication abilities .", "entities": []}, {"text": "Given distinct morphemes A and B , we summarize 6 repetition patterns in Figure 2 .", "entities": []}, {"text": "A - B   A A - yi - APattern 2 A - l\u00e1i - A - q\u00f9Pattern 3A - A Pattern 1 A - l\u01d0 - A - B A - B - A - BA - A - B - B Pattern 4   Pattern 5 Pattern 6 Figure 2 : Reduplication patterns of A and A - B. Each pattern may have one or more morphological functions .", "entities": [[36, 37, "DatasetName", "BA"]]}, {"text": "Taking Pattern 1 ( A!AA ) as an example , noun morphemes could form kinship terms or yield every / each meaning .", "entities": []}, {"text": "For verbs , it signals doing something a little bit or things happen brie\ufb02y .", "entities": []}, {"text": "AA reduplication could also intensify an adjective or transform it to an adverb .", "entities": []}, {"text": "\u000fb\u00e0(dad)!b\u00e0 - b\u00e0 ( dad ) \u000fti\u00afan(day)!ti\u00afan - ti \u00afan(everyday ) \u000fshu\u00afo(say)!shu\u00afo - shuo ( say a little ) \u000fk\u00e0n(look)!k\u00e0n - k\u00e0n ( have a brief look ) \u000fd\u00e0(big)!d\u00e0 - d\u00e0 ( very big ; greatly ) \u000fsh\u00afen(deep)!sh\u00afen - sh \u00afen(deeply ) 1https://github.com/Embedding/Chinese-Word-Vectors2.2", "entities": []}, {"text": "Semi - af\ufb01xation Af\ufb01xation is a morphological process whereby a bound morpheme ( an af\ufb01x ) is attached to roots or stems to form new language units .", "entities": []}, {"text": "Chinese is a typical isolating language that has few af\ufb01xes .", "entities": []}, {"text": "Liu et al .", "entities": []}, {"text": "( 2001 ) points out that although af\ufb01xes are rare in Chinese , there are some components behaving like af\ufb01xes and can also be used as independent lexemes .", "entities": []}, {"text": "They are called semi - af\ufb01xes .", "entities": []}, {"text": "To model the semi - af\ufb01xation process , we uncover 21 semi - pre\ufb01xes and 41 semi - suf\ufb01xes .", "entities": []}, {"text": "These semi - suf\ufb01xes can be used to denote changes of meaning or part of speech .", "entities": []}, {"text": "For example , the semi - pre\ufb01x \u201c d\u00ec- \u201d could be added to numerals to form ordinal numbers , and the semi - suf\ufb01x \u201c -zi \u201d is able to nominalize an adjective : \u000fy\u00af\u0131(one)!d\u00ec - y\u00af\u0131(\ufb01rst ) \u00e8r(two)!d\u00ec - \u00e8r ( second ) \u000fp\u00e0ng ( fat)!p\u00e0ng - zi ( a fat man ) sh\u00f2u ( thin)!sh\u00f2u - zi ( a thin man ) 3 Semantic Relations To investigate semantic knowledge reasoning , we present 28 semantic relations in four aspects : geography , history , nature , and people .", "entities": []}, {"text": "Among them we inherit a few relations from English datasets , e.g. country - capital and family members , while the rest of them are proposed originally on the basis of our observation of Chinese lexical knowledge .", "entities": []}, {"text": "For example , a Chinese province may have its own abbreviation , capital city , and representative drama , which could form rich semantic analogies : \u000f\u00afan - hu \u00af\u0131vszh\u00e8 - ji \u00afang(province ) \u000fw\u02c7anvszh\u00e8(abbreviation ) \u000fh\u00e9 - f\u00e9i vsh\u00e1ng - zh \u00afou(capital ) \u000fhu\u00e1ng - m\u00e9i - x\u00ec vsyu\u00e8 - j\u00f9 ( drama ) We also address novel relations that could be used for other languages , e.g. scientists and their \ufb01ndings , companies and their founders .", "entities": []}, {"text": "4 Task of Chinese Analogical Reasoning Analogical reasoning task is to retrieve the answer of the question \u201c a is to b as c is to ? \u201d .", "entities": []}, {"text": "Based on the relations discussed above , we \ufb01rstly collect word pairs for each relation .", "entities": []}, {"text": "Since there are no explicit word boundaries in Chinese , we take dictionaries and word segmentation speci\ufb01cations as references to con\ufb01rm the inclusion of each word", "entities": []}, {"text": "140Benchmark Category Type # questions # words Relation CA_translated SemanticCapital 506 46 capital - country State 175 54 city - province Family 272 34 family members CA8MorphologicalReduplication A 2554 344 A - A , A - yi - A , A - l\u00e1i - A - q\u00f9 Reduplication AB 2535 423 A - A - B - B , A - l \u02c7\u0131 - A - B , A - B - A - B Semi - pre\ufb01x 2553 656 21 semi - pre\ufb01xes : \u5927 , \u5c0f , \u8001 , \u7b2c , \u4e9a , etc .", "entities": []}, {"text": "Semi - suf\ufb01x 2535 727 41 semi - suf\ufb01xes : \u8005 , \u5f0f , \u4e3b\u4e49 , \u6027 , etc .", "entities": []}, {"text": "SemanticGeography 3192 305country - capital , country - currency , province - abbreviation , province - capital , province - dramma , etc . History 1465", "entities": []}, {"text": "177dynasty - emperor , dynasty - capital , title - emperor , celebrity - country Nature 1370 452number , time , animal , plant , body , physics , weather , reverse , color , etc .", "entities": []}, {"text": "People 1609 259\ufb01nding - scientist , work - writer , family members , etc . Table 1 : Comparisons of CA_translated and CA8 benchmarks .", "entities": []}, {"text": "More details about the relations in CA8 can be seen in GitHub .", "entities": []}, {"text": "Window ( dynamic)Iteration DimensionSubsamplingLow - frequency thresholdContext distribution smoothingNegative ( SGNS / PPMI)Vector offset 5 5 300 1e-5 50 0.75 5/1 3COSMUL Table 2 : Hyper - parameter details .", "entities": []}, {"text": "Levy and Goldberg ( 2014b ) uni\ufb01es SGNS and PPMI in a framework , which share the same hyper - parameter settings .", "entities": [[9, 10, "DatasetName", "PPMI"]]}, {"text": "We exploit 3COSMUL to solve the analogical questions suggested by Levy and Goldberg ( 2014a ) .", "entities": []}, {"text": "pair .", "entities": []}, {"text": "To avoid the imbalance problem addressed in English benchmarks ( Gladkova et al . , 2016 ) , we set a limit of 50 word pairs at most for each relation .", "entities": []}, {"text": "In this step , 1852 unique Chinese word pairs are retrieved .", "entities": []}, {"text": "We then build CA8 , a big , balanced dataset for Chinese analogical reasoning including 17813 questions .", "entities": []}, {"text": "Compared with CA_translated ( Chen et al . , 2015 ) , CA8 incorporates both morphological and semantic questions , and it brings in much more words , relation types and questions .", "entities": []}, {"text": "Table 1 shows details of the two datasets .", "entities": []}, {"text": "They are both used for evaluation in Experiments section .", "entities": []}, {"text": "5 Experiments In Chinese analogical reasoning task , we aim at investigating to what extent word vectors capture the linguistic relations , and how it is affected by three important factors : vector representations ( sparse and dense ) , context features ( character , word , and ngram ) , and training corpora ( size and domain ) .", "entities": []}, {"text": "Table 2 shows the hyper - parameters used in this work .", "entities": []}, {"text": "All the text data used in our experiments ( as shown in Table 3 ) are preprocessed via the following steps : \u000fRemove the html and xml tags from the texts and set the encoding as utf-8 .", "entities": []}, {"text": "Digits and punctuations are remained.\u000fConvert traditional Chinese characters into simpli\ufb01ed characters with Open Chinese Convert ( OpenCC)2 .", "entities": [[0, 1, "DatasetName", "Digits"]]}, {"text": "\u000fConduct Chinese word segmentation with HanLP(v_1.5.3)3 .", "entities": [[1, 4, "TaskName", "Chinese word segmentation"]]}, {"text": "5.1 Vector Representations Existing vector representations fall into two types , dense vectors and sparse vectors .", "entities": []}, {"text": "SGNS ( skipgram model with negative sampling ) ( Mikolov et al . , 2013 ) and PPMI ( Positive Pointwise Mutual Information ) ( Levy and Goldberg , 2014a ) are respectively typical methods for learning dense and sparse word vectors .", "entities": [[17, 18, "DatasetName", "PPMI"]]}, {"text": "Table 4 lists the performance of them on CA_translated and CA8 datasets under different con\ufb01gurations .", "entities": []}, {"text": "We can observe that on CA8 dataset , SGNS representations perform better in analogical reasoning of morphological relations and PPMI representations show great advantages in semantic relations .", "entities": [[19, 20, "DatasetName", "PPMI"]]}, {"text": "This result is consistent with performance of English dense and sparse vectors on MSR ( morphology - only ) , SemEval ( semanticonly ) , and Google ( mixed ) analogy datasets ( Levy and Goldberg , 2014b ; Levy et al . , 2015 ) .", "entities": [[26, 27, "DatasetName", "Google"]]}, {"text": "It is 2https://github.com/BYV oid / OpenCC 3https://github.com/hankcs/HanLP", "entities": []}, {"text": "141Corpus Size # tokens jVj Description Wikipedia 1.3 G 223 M 2129KWikipedia data obtained from https://dumps.wikimedia.org/ Baidubaike 4.1 G 745 M 5422KChinese wikipedia data from https://baike.baidu.com/ People \u2019s Daily News 3.9 G 668 M 1664KNews data from People \u2019s Daily ( 1946 - 2017 ) http://data.people.com.cn/ Sogou news 3.7 G 649 M 1226KNews data provided by Sogou Labs http://www.sogou.com/labs/", "entities": []}, {"text": "Zhihu QA 2.1 G 384 M 1117KChinese QA data from https://www.zhihu.com/ , including 32137 questions and 3239114", "entities": []}, {"text": "answers Combination 14.8 G 2668 M 8175 K", "entities": []}, {"text": "We build this corpus by combining the above corpora Table 3 : Detailed information of the corpora .", "entities": []}, {"text": "# tokens denotes the number of tokens in corpus .", "entities": []}, {"text": "jVj denotes the vocabulary size .", "entities": []}, {"text": "CA_translated CA8 Cap .", "entities": []}, {"text": "Sta .", "entities": []}, {"text": "Fam .", "entities": []}, {"text": "A AB Pre .", "entities": []}, {"text": "Suf .", "entities": []}, {"text": "Mor .", "entities": []}, {"text": "Geo .", "entities": []}, {"text": "His . Nat .", "entities": []}, {"text": "Peo .", "entities": []}, {"text": "Sem .", "entities": []}, {"text": "SGNSword .706", "entities": []}, {"text": ".966 .603 .117 .162 .181 .389 .222 .414 .345 .236 .223 .327 word+ngram .715 .977 .640 .143 .184 .197 .429 .250 .449 .308 .276 .310 .368 word+char .676 .966 .548 .358 .540 .326 .612 .455 .468 .226 .296 .305 .368 PPMIword .925 .920 .548 .103 .139 .138 .464 .226 .627 .501 .300 .515 .522 word+ngram .943 .960 .658 .102 .129 .168 .456 .230 .680 .535 .371 .626 .586 word+char .913 .886 .614 .106 .190 .173 .505 .260 .638 .502 .288 .515 .524 Table 4 : Performance of word representations learned under different con\ufb01gurations .", "entities": []}, {"text": "Baidubaike is used as the training corpus .", "entities": []}, {"text": "The top 1 results are in bold .", "entities": []}, {"text": "probably because the reasoning on morphological relations relies more on common words in context , and the training procedure of SGNS favors frequent word pairs .", "entities": [[11, 14, "DatasetName", "words in context"]]}, {"text": "Meanwhile , PPMI model is more sensitive to infrequent and speci\ufb01c word pairs , which are bene\ufb01cial to semantic relations .", "entities": [[2, 3, "DatasetName", "PPMI"]]}, {"text": "The above observation shows that CA8 is a reliable benchmark for studying the effects of dense and sparse vectors .", "entities": []}, {"text": "Compared with CA_translated and existing English analogy datasets , it offers both morphological and semantic questions which are also balanced across different types4 .", "entities": []}, {"text": "5.2 Context Features To investigate the in\ufb02uence of context features on analogical reasoning , we consider not only word features , but also ngram features inspired by statistical language models , and character ( Hanzi ) features based on the close relationship between Chinese words and their composing characters5 .", "entities": []}, {"text": "Speci\ufb01cally , we use word bigrams for ngram features , character unigrams and bigrams for character features .", "entities": []}, {"text": "4CA_translated and SemEval datasets contain only semantic questions , MSR dataset contains only morphological questions , and in Google dataset the capital : country relation constitutes 56.72 % of all semantic questions .", "entities": [[18, 20, "DatasetName", "Google dataset"]]}, {"text": "5The SGNS with word and character features are implemented by fasttext toolkit , the rest are implemented by ngram2vec toolkit .", "entities": [[10, 11, "MethodName", "fasttext"]]}, {"text": "Ngrams and Chinese characters are effective features in training word representations ( Zhao et al . , 2017 ; Chen et al . , 2015 ; Bojanowski et", "entities": []}, {"text": "al . , 2016 ) .", "entities": []}, {"text": "However , Table 4 shows that there is only a slight increase on CA_translated dataset with ngram features , and the accuracies in most cases decrease after integrating character features .", "entities": []}, {"text": "In contrast , on CA8 dataset , the introduction of ngram and character features brings signi\ufb01cant and consistent improvements on almost all the categories .", "entities": []}, {"text": "Furthermore , character features are especially advantageous for reasoning of morphological relations .", "entities": []}, {"text": "SGNS model integrating with character features even doubles the accuracy in morphological questions .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "Besides , the representations achieve surprisingly high accuracies in some categories of CA_translated , which means that there is little room for further improvement .", "entities": []}, {"text": "However it is much harder for representation methods to achieve high accuracies on CA8 .", "entities": []}, {"text": "The best con\ufb01guration only achieves 68.0 % .", "entities": []}, {"text": "5.3", "entities": []}, {"text": "Corpora We compare word representations learned upon corpora of different sizes and domains .", "entities": []}, {"text": "As shown in Table 3 , six corpora are used in the experiments : Chinese Wikipedia , Baidubaike , People \u2019s Daily News , Sogou News , Zhihu QA , and \u201c Com-", "entities": []}, {"text": "142CA_translated CA8 Cap .", "entities": []}, {"text": "Sta .", "entities": []}, {"text": "Fam .", "entities": []}, {"text": "A AB Pre .", "entities": []}, {"text": "Suf .", "entities": []}, {"text": "Mor .", "entities": []}, {"text": "Geo .", "entities": []}, {"text": "His . Nat .", "entities": []}, {"text": "Peo .", "entities": []}, {"text": "Sem .", "entities": []}, {"text": "Wikipedia 1.2 G .597 .771 .360 .029 .018 .152 .266 .180 .339 .125 .147 .079 .236 Baidubaike 4.3 G .706 .966 .603 .117 .162 .181 .389 .222 .414 .345 .236 .223 .327 People \u2019s Daily 4.2 G .925 .989 .547 .140 .158 .213 .355 .226 .694 .019 .206 .157 .455 Sogou News 4.0 G .619 .966 .496 .057 .075 .131 .176 .115 .432 .067 .150 .145 .302 Zhihu QA 2.2 G .277 .491 .625 .175 .199 .134 .251 .189 .146 .147 .250 .189 .181 Combination 15.9 G .872 .994 .710 .223 .300 .234 .518 .321 .662 .293 .310 .307 .467 Table 5 : Performance of word representations learned upon different training corpora by SGNS with context feature of word .", "entities": []}, {"text": "The top 2 results are in bold .", "entities": []}, {"text": "bination \u201d which is built by combining the \ufb01rst \ufb01ve corpora together .", "entities": []}, {"text": "Table 5 shows that accuracies increase with the growth in corpus size , e.g. Baidubaike ( an online Chinese encyclopedia ) has a clear advantage over Wikipedia .", "entities": []}, {"text": "Also , the domain of a corpus plays an important role in the experiments .", "entities": []}, {"text": "We can observe that vectors trained on news data are bene\ufb01cial to geography relations , especially on People \u2019s Daily which has a focus on political news .", "entities": []}, {"text": "Another example is Zhihu QA , an online questionanswering corpus which contains more informal data than others .", "entities": []}, {"text": "It is helpful to reduplication relations since many reduplication words appear frequently in spoken language .", "entities": []}, {"text": "With the largest size and varied domains , \u201c Combination \u201d corpus performs much better than others in both morphological and semantic relations .", "entities": []}, {"text": "Based on the above experiments , we \ufb01nd that vector representations , context features , and corpora all have important in\ufb02uences on Chinese analogical reasoning .", "entities": []}, {"text": "Also , CA8 is proved to be a reliable benchmark for evaluation of Chinese word embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "6 Conclusion In this paper , we investigate the linguistic regularities beneath Chinese , and propose a Chinese analogical reasoning task based on 68 morphological relations and 28 semantic relations .", "entities": []}, {"text": "In the experiments , we apply vector offset method to this task , and examine the effects of vector representations , context features , and corpora .", "entities": []}, {"text": "This study offers an interesting perspective combining linguistic analysis and representation models .", "entities": []}, {"text": "The benchmark and embedding sets we release could also serve as a solid basis for Chinese NLP tasks .", "entities": [[0, 2, "DatasetName", "The benchmark"]]}, {"text": "Acknowledgments This work is supported by the Fundamental Research Funds for the Central Universities , NationalNatural Science Foundation of China with Grant ( No.61472428 ) and Chinese Testing International Project ( No . CTI2017B12 ) .", "entities": []}, {"text": "References Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Enriching word vectors with subword information .", "entities": []}, {"text": "arXiv preprint arXiv:1607.04606 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Xinxiong Chen , Lei Xu , Zhiyuan Liu , Maosong Sun , and Huan - Bo Luan . 2015 .", "entities": []}, {"text": "Joint learning of character and word embeddings .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "In IJCAI .", "entities": []}, {"text": "pages 1236 \u2013 1242 .", "entities": []}, {"text": "Etienne Denoual .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Analogical translation of unknown words in a statistical machine translation framework .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "Proceedings of Machine Translation Summit XI , Copenhagen .", "entities": [[2, 4, "TaskName", "Machine Translation"]]}, {"text": "Anna Gladkova , Aleksandr Drozd , and Satoshi Matsuoka . 2016 .", "entities": []}, {"text": "Analogy - based detection of morphological and semantic relations with word embeddings : what works and what does n\u2019t .", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the NAACL Student Research Workshop .", "entities": []}, {"text": "pages 8 \u2013 15 .", "entities": []}, {"text": "Amac Herdagdelen and Marco Baroni . 2009 .", "entities": []}, {"text": "Bagpack :", "entities": []}, {"text": "A general framework to represent semantic relations .", "entities": []}, {"text": "In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics . Association for Computational Linguistics , pages 33\u201340 .", "entities": []}, {"text": "Philippe Langlais and Alexandre Patry .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Translating unknown words by analogical learning .", "entities": []}, {"text": "In EMNLP - CoNLL .", "entities": []}, {"text": "pages 877\u2013886 .", "entities": []}, {"text": "Omer Levy and Yoav Goldberg . 2014a .", "entities": []}, {"text": "Linguistic regularities in sparse and explicit word representations .", "entities": []}, {"text": "InProceedings of the eighteenth conference on computational natural language learning .", "entities": []}, {"text": "pages 171 \u2013 180 .", "entities": []}, {"text": "Omer Levy and Yoav Goldberg . 2014b .", "entities": []}, {"text": "Neural word embedding as implicit matrix factorization .", "entities": []}, {"text": "In Advances in neural information processing systems .", "entities": []}, {"text": "pages 2177\u20132185 .", "entities": []}, {"text": "Omer Levy , Yoav Goldberg , and Ido Dagan . 2015 .", "entities": []}, {"text": "Improving distributional similarity with lessons learned", "entities": []}, {"text": "143from word embeddings .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "Transactions of the Association for Computational Linguistics 3:211\u2013225 .", "entities": []}, {"text": "Yuehua Liu , Wenyu Pan , and Wei Gu . 2001 .", "entities": []}, {"text": "Practical grammar of modern Chinese .", "entities": []}, {"text": "The Commercial Press .", "entities": []}, {"text": "Tomas Mikolov , Wen - tau Yih , and Geoffrey Zweig .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Linguistic regularities in continuous space word representations .", "entities": []}, {"text": "In hlt - Naacl .", "entities": []}, {"text": "volume 13 , pages 746\u2013751 .", "entities": []}, {"text": "Jerome L Packard .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "The morphology of Chinese : A linguistic and cognitive approach .", "entities": []}, {"text": "Cambridge University Press .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "Radu Soricut and Franz Josef Och . 2015 .", "entities": []}, {"text": "Unsupervised morphology induction using word embeddings .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "In HLT - NAACL .", "entities": []}, {"text": "pages 1627\u20131637 .", "entities": []}, {"text": "Tzu - ray Su and Hung - yi Lee . 2017 .", "entities": []}, {"text": "Learning chinese word representations from glyphs of characters .", "entities": []}, {"text": "InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "pages 264\u2013273 .", "entities": []}, {"text": "Peter D Turney . 2008 .", "entities": []}, {"text": "A uniform approach to analogies , synonyms , antonyms , and associations .", "entities": []}, {"text": "In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1 . Association for Computational Linguistics , pages 905\u2013912 .", "entities": []}, {"text": "Liner Yang and Maosong Sun . 2015 .", "entities": []}, {"text": "Improved learning of chinese word embeddings with semantic knowledge .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "In Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data , Springer , pages 15\u201325 .", "entities": []}, {"text": "Rongchao Yin , Quan Wang , Peng Li , Rui Li , and Bin Wang . 2016 .", "entities": []}, {"text": "Multi - granularity chinese word embedding .", "entities": []}, {"text": "In EMNLP .", "entities": []}, {"text": "pages 981\u2013986 .", "entities": []}, {"text": "Zhe Zhao , Tao Liu , Shen Li , Bofang Li , and Xiaoyong Du . 2017 .", "entities": []}, {"text": "Ngram2vec : Learning improved word representations from ngram co - occurrence statistics .", "entities": []}, {"text": "InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "pages 244\u2013253 .", "entities": []}]