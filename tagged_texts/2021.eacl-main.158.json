[{"text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics , pages 1841\u20131851 April 19 - 23 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics1841Alignment veri\ufb01cation to improve NMT translation towards highly in\ufb02ectional languages with limited resources George Tambouratzis ILSP , Athena R.C. 6 Artemidos Str . , Maroussi , 15125 , Greece giorg t@athenarc.grMarina Vassiliou ILSP , Athena R.C. 6 Artemidos Str . , Maroussi , 15125 , Greece mvas@athenarc.gr", "entities": []}, {"text": "Abstract The present article studies translation quality when limited training data is available to translate towards morphologically rich languages .", "entities": []}, {"text": "The starting point is a neural MT system , used to train translation models with only publicly available parallel data .", "entities": []}, {"text": "An initial analysis of the translation output has shown that quality is sub - optimal , mainly due to the insuf\ufb01cient amount of training data .", "entities": []}, {"text": "To improve translation , a hybridized solution is proposed , using an ensemble of relatively simple NMT systems trained with different metrics , combined with an open source module designed for low - resource MT that measures the alignment level .", "entities": []}, {"text": "A quantitative analysis based on established metrics is complemented by a qualitative analysis of translation results .", "entities": []}, {"text": "These show that over multiple test sets , the proposed hybridized method confers improvements over ( i ) both the best individual NMT and ( ii ) the ensemble system provided in the Marian - NMT package .", "entities": []}, {"text": "Improvements over Marian - NMT are in many cases statistically signi\ufb01cant .", "entities": []}, {"text": "1 Introduction The state of the art in MT involves corpus - based systems developed with machine - learning methods .", "entities": []}, {"text": "These methods learn from corpora the models needed for translation .", "entities": []}, {"text": "A key strength of this approach is that the system is adapted speci\ufb01cally towards the data it is trained with .", "entities": []}, {"text": "For many years , the most successful data - driven approaches were phrase - based and syntax - based Statistical MT ( SMT ; Koehn , 2009 ) .", "entities": []}, {"text": "However , lately Neural MT ( NMT ) based on the encoderdecoder architecture and the concept of attention ( Sutskever et al . , 2014 ; Bahdanau et al . , 2016 ) has become very popular .", "entities": []}, {"text": "Indeed , since 2015 , in MT shared tasks ( Cettolo et al . , 2015 ; Bojar et al . , 2015 ; Bojar et al . , 2016 ) most top - performing systems have been NMT systems .", "entities": []}, {"text": "This trend is con\ufb01rmedin the most recent MT shared task ( Barrault et al . , 2019 ) , where 80 % of participating systems are of NMT type .", "entities": []}, {"text": "Though NMT represents the state of the art for MT , speci\ufb01c weaknesses have been reported : \u2022NMT performance suffers from the lack of data resources ( Koehn and Knowles , 2017 ) , giving lower translation performance , especially when training with out - of - domain rather than in - domain data .", "entities": []}, {"text": "\u2022Recent advances in NMT models have been shown ( Sennrich and Zhang , 2019 ) to allow good translations to be achieved with smaller parallel corpora of typically 105sentences , though substantial improvements are achieved when the corpus size reaches 106sentences .", "entities": []}, {"text": "However , training sets of such sizes are not available for all languages .", "entities": []}, {"text": "\u2022Translation performance is affected by nonparallel texts and non - literal translations ( Carpuat et al . , 2017 ) .", "entities": []}, {"text": "\u2022The integration of multiple algorithms into an NMT system does not necessarily improve translation ( Denkowski and Neubig , 2017 ) .", "entities": []}, {"text": "\u2022The time complexity of training a new NMT system can be very high , with training sessions of the order of weeks .", "entities": []}, {"text": "NMT requires very large amounts of parallel data , measured in millions of parallel sentences .", "entities": []}, {"text": "This is re\ufb02ected by the separate studies carried out for MT with limited resources , which includes initiatives such as Lorelei1 .", "entities": []}, {"text": "In the case of morphologically - rich languages , the requirements for parallel corpora are further exacerbated .", "entities": []}, {"text": "1https://www.darpa.mil/program/low-resourcelanguages-for-emergent-incidents", "entities": []}, {"text": "1842Proposed approaches for translating towards lowresource and morphologically - rich languages have included transfer learning ( Zoph et al . , 2016 ) as well as multilingual and multi - way NMT ( Rikters et al . , 2018 ) .", "entities": [[13, 15, "TaskName", "transfer learning"]]}, {"text": "In this paper , an effort to improve the translation quality is presented , when translating towards a morphologically - rich language , while reducing the training time .", "entities": []}, {"text": "This approach combines the output of multiple NMT systems with an NLP module developed for an example - based MT paradigm , resulting in a hybridized solution .", "entities": []}, {"text": "The latter module is fast and runs independently of its original MT system and thus the computational complexity of the proposed hybrid solution is not substantially increased over the base NMT system .", "entities": []}, {"text": "The idea of combining multiple MT models to produce a higher performing MT system has been studied extensively in the area of MT .", "entities": []}, {"text": "For instance in the recent shared task ( Barrault et al . , 2019 ) more than 20 entries consist of ensembles of multiple NMT systems .", "entities": []}, {"text": "Ensembles of weaker NMT systems of the same general architecture have been proposed by Freitag et al .", "entities": []}, {"text": "( 2017 ) to train a higher performing NMT system .", "entities": []}, {"text": "In addition ensembles of factored NMT models have been proposed for automatic post - editing and quality estimation ( for example Hokamp , 2017 ) .", "entities": [[11, 15, "TaskName", "automatic post - editing"]]}, {"text": "This base NMT system is described in section 2 .", "entities": []}, {"text": "The training data used is reported in section 3 .", "entities": []}, {"text": "The proposed hybridization is presented in section 4 , whilst the improvements attained are presented in section 5 .", "entities": []}, {"text": "Future developments are discussed in section 6 . 2 Overview of the Base NMT System Since NMT systems have achieved the highest translation quality in recent evaluation contests , the Marian - NMT package ( Junczys - Dowmunt et al . , 2018 ) is adopted for experimentation here .", "entities": []}, {"text": "MarianNMT development was funded by the European Commission to consolidate NMT research and incorporates the most recent advances in NMT .", "entities": []}, {"text": "Its code is optimized to reduce the CPU / GPU time required to complete the simulations of NMT systems .", "entities": []}, {"text": "For creating NMT systems , three of the models provided by Marian - NMT were chosen , termed as the \u201c transformer \u201d , \u201c amun \u201d and \u201c s2s \u201d models .", "entities": []}, {"text": "The \u201c transformer \u201d model has been based on the work of Vaswani et", "entities": []}, {"text": "al . ( 2017 ) and uses a simple structureincorporating attention mechanisms and dispensing with recurrence to implement a fast NMT system .", "entities": []}, {"text": "The other two models are more conventional , using a recurrent neural network to implement the translation .", "entities": []}, {"text": "The \u201c amun \u201d model follows the approach of Bahdanau et al .", "entities": []}, {"text": "( 2016 ) , employing a recurrent neural network but allowing the model to automatically search for wider ranges of the source language ( SL ) to connect with the target language side ( TL ) words .", "entities": []}, {"text": "Finally , \u201c s2s \u201d implements a recurrent neural network - based encoder - decoder model with attention mechanism , using the architecture proposed in ( Sennrich et al . , 2017 ) .", "entities": []}, {"text": "Hereafter , the three models are identi\ufb01ed via the names used within MarianNMT , which are also used in evaluations ( cf .", "entities": []}, {"text": "Bojar et al . , 2018 ) .", "entities": []}, {"text": "The main con\ufb01guration parameters used for each model are depicted in Table 1 , to enable replication of experiments .", "entities": []}, {"text": "For each model , different optimization options from Marian - NMT during the validation phase are used to create three NMT variants of each model , namely optimizing with ( i ) BLEU , ( ii ) entropy and ( iii ) word - wise normalized crossentropy ( denoted as \u201c ce - mean \u201d and representing the default optimization for Marian - NMT ) .", "entities": [[32, 33, "MetricName", "BLEU"]]}, {"text": "Regarding the main NMT parameters , all recurrent networks comprise 1,024 units in the hidden layer , an encoder depth of 6 layers and an embedding size of 512 .", "entities": []}, {"text": "All cells used both in the encoder and decoder side are gated recurrent units ( GRU ) .", "entities": [[15, 16, "MethodName", "GRU"]]}, {"text": "The transformer dimension is set to 2,048 .", "entities": []}, {"text": "To reduce the lexicon size , a total of 85,000 merge operations are allowed using the BPE ( Byte Pair Encoding ) method proposed in ( Sennrich et al . , 2016 ) , this being the default setting for marian - nmt applications .", "entities": [[16, 17, "MethodName", "BPE"], [18, 21, "MethodName", "Byte Pair Encoding"]]}, {"text": "Initially , the three Marian - NMT models are trained to provide the base NMT systems .", "entities": []}, {"text": "Typically , for a single - GPU system ( equipped with an NVIDIA Titan XP GTX1080 GPU card driven by an Intel i-9700 K CPU ) , 24 hours are required for training the transformer , 130 hours for amun and 308 hours for s2s .", "entities": [[13, 14, "DatasetName", "Titan"]]}, {"text": "This is equivalent to a ratio of 1:5:12 to train the respective systems .", "entities": []}, {"text": "3 Experimental Set - up The experiments aim to improve the translation accuracy of an NMT system , taking into account limited training data and constrained computing resources .", "entities": [[12, 13, "MetricName", "accuracy"]]}, {"text": "In order to investigate translation into a lesser - used and highly in\ufb02ectional language ,", "entities": []}, {"text": "we", "entities": []}, {"text": "1843Common to all 3 models layer - normalization yes exponential smoothing yes beam - size 6 normalize 0.6 early - stopping 5 Transformer - speci\ufb01c transformer - dropout 0.1 transformer - dropout - attention 0.1 transformer - dropout - ffn 0.1 Amun and s2s - speci\ufb01c dropout - rnn 0.2 dropout - src 0.1 dropout - trg 0.1 Table 1 : Key NMT hyper - parameters used corpus senten .", "entities": [[22, 23, "MethodName", "Transformer"]]}, {"text": "wordEn wordGr raw(Europarl ) 1.23 M 31.8 M 31.9 M raw(DGT ) 4.90 M 97.8 M 87.2 M train(DGT 6.13 M 129.6 M 119.1 M + Europarl ) devel(Eparl ) 3,000 77,681 78,610 testset2(Eparl ) 1,000 27,712 27,630 testset1(Pres . )", "entities": []}, {"text": "200 2,873 2,757 Table 2 : Corpora for training and evaluation have chosen the English - to - Greek language pair .", "entities": []}, {"text": "When selecting the training corpora , it has been decided to refrain from using expensive language resources such as specialized or hand - built parallel corpora .", "entities": []}, {"text": "Instead , only standard publicly available parallel corpora have been adopted , namely the Europarl and DGT - Acquis corpora2 , as listed in Table 2 .", "entities": []}, {"text": "The largest part of the Europarl corpus and the entire DGT - Acquis corpus are used to train the NMT system .", "entities": []}, {"text": "Three small portions of the Europarl corpus have been reserved for test and validation purposes .", "entities": []}, {"text": "More speci\ufb01cally , two independent sets of approx .", "entities": []}, {"text": "3,000 Europarl sentences each are excluded , to ensure that the NMT evaluation is unbiased .", "entities": []}, {"text": "In the present experiment , one of these sets is used for in - training validation .", "entities": []}, {"text": "The other set is reserved to allow additional cross - evaluation of experiments in the future , without invalidating the previously trained models .", "entities": []}, {"text": "Finally , a sample 2The Europarl corpus ( ver.7 ) was retrieved from https://www.statmt.org/europarl .", "entities": []}, {"text": "The DGT - Acquis corpus was retrieved from https://ec.europa.eu/jrc/en/languagetechnologies/dgt-translation-memoryof 1,000 sentences from Europarl ( Testset2 ) is retained to provide an unseen in - domain test set .", "entities": []}, {"text": "Another independent test set was drawn from the PRESEMT project resources , comprising 200 sentences which have not been used to either train an MT model or create any resources used herewith ( denoted as Testset1 ) .", "entities": []}, {"text": "A preliminary analysis of the NMT outputs has shown that translations are commendably \ufb02uent , though errors are evident .", "entities": []}, {"text": "A sample of amun translations is shown in Figure 1 .", "entities": []}, {"text": "In sentence # 1 , the term \u201c Amerikano\u00d0 \u201d ( Transl .", "entities": []}, {"text": "\u201c Americans \u201d ) is erroneously used as a translation of the terms \u201c American \u201d , \u201c European \u201d , and \u201c Japanese monopolies \u201d .", "entities": []}, {"text": "Similarly , in sentence # 2 , the phrase \u201c h katapol\u00e8mhsh thc ft\u00b8qeiac \u201d ( meaning \u201c the reduction of poverty \u201d ) is used to translate semantically diverse phrases , including \u201c genetically modi\ufb01ed organisms \u201d , and \u201c the negative social effects of unbridled , unregulated globalization \u201d .", "entities": []}, {"text": "Repetition is a widely reported weakness of NMT systems , most frequently attributed to insuf\ufb01cient training data .", "entities": []}, {"text": "An additional problem concerns the translation of rare words ( i.e. words with low frequency in the corpus ) , due to the limited vocabulary that NMT systems can directly handle .", "entities": []}, {"text": "This is especially severe when translating towards languages with complex morphology , which increases the effective vocabulary size .", "entities": []}, {"text": "For example the word \u201c ostensibly \u201d is translated into Greek as \u201c osten\ufb01gher \u201d ( ungrammatical ) .", "entities": []}, {"text": "Similarly the word \u201c room \u201d is translated as \u201c dwmate\u00d0o \u201d instead of the correct \u201c dwm\u0088tio \u201d ( meaning room ) , whilst the word \u201c indistinct \u201d is translated as \u201c \u0088qwroc \u201d which is not a valid Greek word .", "entities": []}, {"text": "Another issue is that entire phrases present in the source text may be omitted in the translation .", "entities": []}, {"text": "For instance the sentence \u201c Businesses have undertaken the education \u201d is translated by a transformer NMT as \u201c H ekpa\u00d0deush \u00e8qei anal\u0088bei \u001d , [ meaning \u201c education has undertaken \u201d ] .", "entities": []}, {"text": "Hence , the subject \u201c business \u201d has been deleted .", "entities": []}, {"text": "4 Improving NMT via the Alignment Veri\ufb01cation Method ( A VM ) 4.1 Aim of A VM To improve translation accuracy , the main errors need to be identi\ufb01ed in an automated manner .", "entities": [[20, 21, "MetricName", "accuracy"]]}, {"text": "The idea is that a poor alignment between source text and translation indicates substantial loss of meaning during translation .", "entities": [[14, 15, "MetricName", "loss"]]}, {"text": "On the contrary a high alignment score is indicative of a high likelihood that", "entities": []}, {"text": "1844 Figure 1 : Example translations generated by amun , with repetitions of texts highlighted in grey the NMT output is an accurate translation .", "entities": []}, {"text": "To this end a module will be added to implement alignment veri\ufb01cation ( A VM ) , by determining the match between the input sentence and its translation .", "entities": []}, {"text": "The establishment of representative alignment scores allows in turn the combination of multiple NMT models , using A VM to evaluate the accuracy of each candidate translation and thus select the best translation on a sentence - by - sentence basis .", "entities": [[22, 23, "MetricName", "accuracy"]]}, {"text": "For this research , MT software and tools released via open - source code have been surveyed and the Phrase Aligner Module ( PAM , cf .", "entities": []}, {"text": "Troullinos , 2013 ) has been selected .", "entities": []}, {"text": "The architecture of the proposal hybrid NMT is depicted in Figure 2 .", "entities": []}, {"text": "4.2 PRESEMT essentials PAM was developed as part of the PRESEMT hybrid MT methodology ( Tambouratzis et al . , 2017 ( Tambouratzis et al . , 2017 ) ) .", "entities": []}, {"text": "PRESEMT was designed to create MT systems requiring only very limited amounts of specialized , expensive linguistic resources .", "entities": []}, {"text": "Frequently , the most expensive resource is the parallel corpus of SL \u2013 TL sentences .", "entities": []}, {"text": "PRESEMT uses parallel corpora of only a few hundred sentences , augmented by very extensive but comparatively inexpensive monolingual corpora .", "entities": []}, {"text": "Within the PRESEMT methodology , the small parallel corpus serves to establish the transformation from the SL structure to the TL one , using the Phrase Aligner module .", "entities": []}, {"text": "This module , handling sentence pairs from this parallel corpus , identi\ufb01es the correspondence of words and phrases from SL to TL , to determine the translation accuracy .", "entities": [[27, 28, "MetricName", "accuracy"]]}, {"text": "4.3 Description of the PAM module PAM utilizes a limited - size bilingual lexicon ( of typically 30 to 40 thousand token pairs ) together with a publicly available parser .", "entities": []}, {"text": "Details on these resources are reported in section 4.4 , as their choices are language - speci\ufb01c .", "entities": []}, {"text": "Based on these resources , PAM establishes for the set of parallel sentences the alignment of both words and phrases from SL to TL , in three hierarchically ordered stages : 1.Within the \ufb01rst stage , the alignment of words is based on equivalences provided by the bilingual lexicon .", "entities": []}, {"text": "Dedicated PAM processes resolve cases where ( i ) words have multiple appearances within a sentence and ( ii ) multiple potential translations of an SL word exist in the TL side .", "entities": []}, {"text": "2.Within the second stage , words are aligned by establishing statistical correspondences between grammatical features across the SL and TL pair .", "entities": []}, {"text": "These correspondences are automatically extracted from the lexicon .", "entities": []}, {"text": "3.Within the third stage , any remaining words are aligned and grouped into phrases on the basis of the alignments of their neighboring words that are successfully aligned .", "entities": []}, {"text": "To implement this , the principle of locality across languages is adopted ( words at a small distance to each other in SL also tend to be located close to each other in TL ) .", "entities": []}, {"text": "The key PAM principle is that decisions made at a later stage have a lower degree of con\ufb01dence than those made at an earlier stage ( Troullinos , 2013 ) .", "entities": []}, {"text": "4.4 Using PAM for Alignment Veri\ufb01cation In the current application , PAM determines the suitability of each candidate translation , based on its match with the source sentence .", "entities": []}, {"text": "Thus , the assumption made is that the input sentence and the candidate translation represent the corresponding SL and TL entries of a parallel corpus and PAM determines their level of parallelism .", "entities": []}, {"text": "1845As the requirement is to grade various translations , the PAM operation is reversed , to identify the quality of match between the input sentence and the generated translations .", "entities": []}, {"text": "When PAM was used in PRESEMT , sentence pairs from the parallel corpus with a very low percentage of successful alignments were discarded without measuring their degree of parallelism , as poor exemplars of the structural transformations from SL to TL .", "entities": []}, {"text": "Here , PAM is modi\ufb01ed so that for all pairs of input sentence and NMT - translation the word alignments and assignments of words to phrases are calculated .", "entities": []}, {"text": "This allows the re - roled PAM to grade any source / translation pair , no matter how poor the match of the two sentences is .", "entities": []}, {"text": "Two metrics have been established to calculate divergence between the SL sentence and its NMTderived translations .", "entities": []}, {"text": "The \ufb01rst metric ( Uscore ) calculates the number of unaligned words of the source sentence , after PAM is applied .", "entities": []}, {"text": "The aim is to have as few unaligned words as possible , so the lower Uscore is , the better the translation is .", "entities": []}, {"text": "Uscore = # unaligned words ( 1 ) The second metric ( Wscore ) is a weighted combination of several indicators of alignment between source sentence and candidate translation .", "entities": []}, {"text": "This summarizes in one measurement the type of alignments and the stage at which they were achieved .", "entities": []}, {"text": "Hence , for a sentence with Kwords , Wscore is de\ufb01ned as : W score = KX i=1(wi\u0003align stage i ) ( 2 ) In equation ( 2 ) , align - stage idenotes the stage ( cf . section 4.3 for the different stages ) at which the i - th SL word is aligned successfully to a TL word , and widenotes the relevant weight for this stage .", "entities": []}, {"text": "In the case of the weighted metric Wscore , the higher the score , the more accurate the corresponding translation is .", "entities": []}, {"text": "The actual weight values must reward the establishment of alignments at an earlier rather than a later stage .", "entities": []}, {"text": "Thus , wishould be larger than wj , for ismaller than j. For the purposes of the present article , wiis set to integer values of 5 , 2 and 1 for the \ufb01rst , second and third stage respectively ( other sets of weight values that follow this reasoning produce similar results to those reported here ) .", "entities": []}, {"text": "The code of PAM has been modi\ufb01ed to integrate Wscore and Uscore calculation , though the actual alignment Figure 2 : Proposed hybrid NMT approach algorithms within PAM have remained intact .", "entities": []}, {"text": "The associated PAM resources ( the TL - side parser and bilingual lexicon ) remain unchanged .", "entities": []}, {"text": "For processing the SL language , Treetagger ( Schmid , 1994 ) is used , with a reported tagging accuracy exceeding 96 % .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "5 Experimental Results To determine the quality of the NMT - based translations ( amun- , s2s- and transformer - based models ) , two widely used MT evaluation metrics are utilised , namely BLEU ( Papineni et al . , 2002 ) and NIST ( Doddington , 2002 ) .", "entities": [[34, 35, "MetricName", "BLEU"]]}, {"text": "To calculate both these metrics , the mt - eval package ( version 13a ) is used .", "entities": []}, {"text": "For PAM , the PRESEMT bilingual lexicon from Greek to English is used , which contains approx .", "entities": []}, {"text": "8,000 lemmas and 40,000 Greek - English token pairs .", "entities": []}, {"text": "This lexicon is from the same domain as testset1 and is thus out - of - domain for testset2 , providing a more limited coverage for this testset .", "entities": []}, {"text": "Two different types of experiments are possible , depending on whether the ensemble comprises multiple NMT architectures , or only one type of architecture .", "entities": []}, {"text": "The \ufb01rst experiment reported here involves NMT ensembles that all share the same architecture , but are optimized with different criteria .", "entities": []}, {"text": "The second type of experiment studies ensembles which consist of systems with different architectures , to investigate if their combination results in a better translation quality .", "entities": []}, {"text": "5.1 NMT Ensembles of a single architecture The results obtained for testset1 of the Englishto - Greek translation pair are depicted in Table 3 , when running the single transformer , amun and s2s models respectively , as well as their ensembles .", "entities": []}, {"text": "The corresponding results for testset2 are depicted in Table 4 .", "entities": []}, {"text": "In Tables 3 and 4 , the \ufb01rst 3 rows correspond to", "entities": []}, {"text": "1846single NMT models generated when Marian - NMT is trained to optimise ( i ) BLEU , ( ii ) entropy and ( iii ) the word - wise normalised cross - entropy ( this is denoted as \u201c ce - mean \u201d ) .", "entities": [[15, 16, "MetricName", "BLEU"]]}, {"text": "The \ufb01nal three rows of Tables 3 and 4 report the accuracy of translations obtained by NMT ensembles .", "entities": [[11, 12, "MetricName", "accuracy"]]}, {"text": "Marian - NMT implements a standard ensemble method , which allows the user to combine different models provided they use the same lexicon .", "entities": []}, {"text": "The user may specify weighting factors to boost selection of the models deemed to be better .", "entities": []}, {"text": "For this article , this ensemble combines the three aforementioned NMT models ( i ) , ( ii ) and ( iii ) , with equal weights for all NMTs .", "entities": []}, {"text": "The last two rows report the accuracy of ensembles using PAM with ( i ) Uscore and ( ii ) Wscore , respectively .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "A key difference of the Marian - NMT ensemble is that it is able to recombine partial results of the translation process from each NMT model and thus may generate a new translation that is different from all the translations of single - NMT systems .", "entities": []}, {"text": "On the contrary , Uscore and Wscore grade the translations generated by single - NMT models in the ensemble , and then select the highest - scoring translation to be the translation produced by the PAM - based ensemble .", "entities": []}, {"text": "To evaluate the quality of translations produced by the PAM - based ensembles , two baselines are selected .", "entities": []}, {"text": "The \ufb01rst baseline is the \u201c ce - mean \u201d option of the Marian - NMT translation system .", "entities": []}, {"text": "The second , and stronger , baseline is the Marian - NMT ensemble ( referred to as \u201c Marian - ensemble \u201d hereafter ) .", "entities": []}, {"text": "Entries that exceed the \ufb01rst baseline are depicted in bold .", "entities": []}, {"text": "Entries with scores that exceed the stronger Marian - ensemble baseline are annotated with an asterisk .", "entities": []}, {"text": "Based on Table 3 , for testset1 the best BLEU scores are achieved by the Marian - NMT ensemble in comparison to single - NMT models .", "entities": [[9, 10, "MetricName", "BLEU"]]}, {"text": "The PAM - Wscore ensemble gives a higher accuracy than the Marian - NMT ensemble , whilst the accuracy of PAM - Uscore is lower than PAM - Wscore .", "entities": [[8, 9, "MetricName", "accuracy"], [18, 19, "MetricName", "accuracy"]]}, {"text": "On the whole , it is Marian - ensemble and PAMWscore that generate the best NIST and BLEU scores .", "entities": [[17, 18, "MetricName", "BLEU"]]}, {"text": "A broadly similar situation is found when using testset2 ( Table 4 ) .", "entities": []}, {"text": "Here , the improvement conferred by the ensemble methods over the three base models is much more marked .", "entities": []}, {"text": "For instance , for BLEU , the score is only 19.0 to 20.0 for single NMT models , but rises to more than 28.0 for theensembles , which equates to more than eight BLEU percentage points of improvement .", "entities": [[4, 5, "MetricName", "BLEU"], [33, 34, "MetricName", "BLEU"]]}, {"text": "5.2 Statistical analysis of ensemble results One question is whether the improvements conferred by the ensembles are statistically signi\ufb01cant .", "entities": []}, {"text": "To that end , the BLEU and NIST scores of all the independent sentences are assembled , forming two populations of scores ( one for BLEU and one for NIST ) for each experimental run .", "entities": [[5, 6, "MetricName", "BLEU"], [25, 26, "MetricName", "BLEU"]]}, {"text": "Then the Wilcoxon and sign tests are used to determine if these populations have signi\ufb01cant differences .", "entities": []}, {"text": "For testset1 , the scores of the single NMT systems and the NMT - ensembles are relatively close , differing by less than 2 BLEU points .", "entities": [[24, 25, "MetricName", "BLEU"]]}, {"text": "Applying the sign and Wilcoxon tests , Marian - ensemble produces statistically better NIST scores ( at a 0.05 level ) than the default Marian - NMT output for amun and s2s models , but not for the transformer model .", "entities": []}, {"text": "For the transformer and s2s models , the scores generated by PAM - Wscore are signi\ufb01cantly better that those of single - model Marian - NMT , according to both the Wilcoxon and sign tests ( at a 0.05 level ) .", "entities": []}, {"text": "Similarly , PAM - Uscore gives statistically superior results to Marian - NMT ( ce - mean optimization ) for the s2s model ( at a signi\ufb01cance level of 0.05 ) .", "entities": []}, {"text": "Comparing the ensembles to each other , Wscore consistently produces higher scores than Uscore .", "entities": []}, {"text": "This superiority is statistically signi\ufb01cant at a 0.05 level according to both Wilcoxon and sign tests , for the transformer and the amun models .", "entities": []}, {"text": "PAM - Wscore achieves consistently higher translation scores than Marian - ensemble for both BLEU and NIST .", "entities": [[14, 15, "MetricName", "BLEU"]]}, {"text": "According to the Wilcoxon test , these differences are statistically signi\ufb01cant , at a 0.05 level , only for the s2s ( BLEU score ) and the transformer model ( both BLEU and NIST scores ) .", "entities": [[22, 24, "MetricName", "BLEU score"], [31, 32, "MetricName", "BLEU"]]}, {"text": "Turning to testset2 , the results are more clearly separated .", "entities": []}, {"text": "All three ensembles ( i.e. PAM - Wscore , PAM - Uscore and Marian - ensemble ) have statistically superior scores to Marian ( optimised with ce - mean ) for both BLEU and NIST , at a signi\ufb01cance level of 0.01 .", "entities": [[32, 33, "MetricName", "BLEU"]]}, {"text": "This extends to all three NMT models ( amun , transformer and s2s ) , and indicates that both Marian - ensemble and the two PAM - based ensembles give substantially higher scores than single Marian - NMT models .", "entities": []}, {"text": "On the other hand , when comparing PAMWscore to PAM - Uscore for testset2 , no statistically signi\ufb01cant difference ( at a 0.05 level of signi\ufb01-", "entities": []}, {"text": "1847cance ) between the two systems is discerned by either the Wilcoxon or sign test .", "entities": []}, {"text": "Similarly , no statistically signi\ufb01cant differences at a 0.05 level are found between the PAM - based ensembles and the Marian - ensemble and only small differences at a 0.10 level .", "entities": []}, {"text": "Thus , even though PAM - based ensembles achieve scores higher than Marian - ensemble , differences are not signi\ufb01cant .", "entities": []}, {"text": "5.3 Measuring improvement over baselines To quantize the improvements achieved by the proposed PAM - Wscore approach , in this section the computational requirements posed by each NMT system are also considered .", "entities": []}, {"text": "To this end , the most accurate NMT system is de\ufb01ned for each dataset and metric combination .", "entities": []}, {"text": "Two baselines are chosen , namely the most accurate NMT model and the most accurate Marian - ensemble .", "entities": []}, {"text": "We focus on the transformer model , which is the least expensive model to train .", "entities": []}, {"text": "For each ensemble using transformers , the aim is to determine how close to the Marian - ensemble baseline this is .", "entities": []}, {"text": "Results are shown in Table 5 , where the accuracy of each transformer NMT is expressed as a fraction of the Marian - ensemble score .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "The best single transformer model achieves for testset1 88.7 % of the baseline BLEU score and 93.1 % of the NIST score .", "entities": [[13, 15, "MetricName", "BLEU score"]]}, {"text": "Using the Wscore ensembling method , this rises to 90.7 % for BLEU and 95.2 % for NIST , showing a gain of 2 % .", "entities": [[12, 13, "MetricName", "BLEU"]]}, {"text": "Turning to dataset2 , the single transformer scores just 70.5 % in comparison to the baseline BLEU score and 73.4 % of the NIST score ( therefore it is 27 % to 30 % lower ) .", "entities": [[16, 18, "MetricName", "BLEU score"]]}, {"text": "The Wscore ensemble improves relative scores , reaching 92.7 % and 94.5 % of the baseline scores for BLEU and NIST respectively .", "entities": [[18, 19, "MetricName", "BLEU"]]}, {"text": "This equates to an increase of ca .", "entities": []}, {"text": "22 % in both scores , making the \ufb01nal result directly comparable to s2s , though GPU training requirements are reduced by a factor of 12 .", "entities": []}, {"text": "5.4 Subjective studies A second type of evaluation moves away from metrics to focus on analysing the translation errors by different models , with subjective methods .", "entities": []}, {"text": "For instance , when transformer NMT models are tasked to translate testset1 , the BLEU - optimised NMT generates 26 ungrammatical words , the entropyoptimised NMT generates 24 ungrammatical words and the cross - entropy optimised model produces 23 ungrammatical words .", "entities": [[14, 15, "MetricName", "BLEU"]]}, {"text": "The Wscore - ensemble reduces the ungrammatical words to 21 , improving Figure 3 : Examples of poor translations produced by Marian - ensemble ( omitted parts are underlined in source ) .", "entities": []}, {"text": "translation .", "entities": []}, {"text": "The ungrammatical words were determined in all cases by visual inspection of the body of translations complemented by spell - checking tools to aid detection .", "entities": []}, {"text": "Further inspection of translation quality has involved comparing the Marian - ensemble and Wscore - ensemble outputs .", "entities": []}, {"text": "The length ( in words ) of translations per test sentence is found to differ substantially between the two ensembles , with the difference being more than 1/10 for 9 % of sentences , more than 1/4 for 2.5 % of sentences and more than 1/2 for 1 % of sentences ( close to identical results are obtained for testset1 and testset2 ) .", "entities": []}, {"text": "As such deviations are unexpectedly large , an analysis was performed , with typical examples being shown in Figure 3 .", "entities": []}, {"text": "As can be seen , PAM assists the Wscore - ensemble in retaining all phrases of the sentence .", "entities": []}, {"text": "On the contrary , Marian - ensemble fails to ensure this , and frequently discards portions of the input sentence .", "entities": []}, {"text": "In one case ( sentence # 774 ) Marianensemble results in a null - length translation , and in another ( sentence # 648 ) the \ufb01nal translation covers less than 10 % of the input text , radically distorting meaning .", "entities": []}, {"text": "Both PAM - ensembles are unaffected by such phenomena .", "entities": []}, {"text": "18486 Conclusions and Future Work", "entities": []}, {"text": "This article has studied the creation of translation systems towards highly in\ufb02ectional languages , when the amount of in - domain training data is limited .", "entities": []}, {"text": "Emphasis has been placed on improving the translation accuracy of NMT models that can be trained more rapidly and cost - effectively ( in terms of CPU processing power ) and rendering this performance comparable to that of more complex models .", "entities": [[8, 9, "MetricName", "accuracy"]]}, {"text": "The Marian - NMT package has been chosen as the starting point to create NMT models for the English to Greek language pair .", "entities": []}, {"text": "Using only publicly available text corpora , the NMT models produce commendably \ufb02uent translations .", "entities": []}, {"text": "Identi\ufb01ed errors in the NMT translations are typical of a lack of training data .", "entities": []}, {"text": "A hybrid methodology has been proposed that samples an ensemble of NMT models to select the \ufb01nal translation , chosen by a module calculating the alignment level between the input sentence and each translation .", "entities": []}, {"text": "This module was developed for resource - poor MT systems .", "entities": []}, {"text": "The proposed hybrid approach has resulted in higher BLEU and NIST scores , compared to those of single NMT models .", "entities": [[8, 9, "MetricName", "BLEU"]]}, {"text": "Improvements are in many cases statistically signi\ufb01cant even over the ensemble system provided within the Marian - NMT package , indicating the promising nature of the hybrid approach .", "entities": []}, {"text": "Also , the translation process is found to be more robust , giving more consistent translations in comparison to the Marian - NMT ensemble system , which occasionally omits large portions of the input text from the translation .", "entities": []}, {"text": "One of the advantages of the proposed method is that it is general - purpose and does not rely on the use of ensembles of Neural MT systems with a speci\ufb01c architecture .", "entities": []}, {"text": "Instead , it can be used to combine the results of different types of Neural MT systems , or MT systems that belong to different paradigms , or even to combine human translations .", "entities": []}, {"text": "In addition the proposed method can be used to clean up a corpus of parallel sentences or several such corpora , by removing sentence pairs for which the source and target - language texts do not have a high degree of parallelism .", "entities": []}, {"text": "Similarly , the proposed method may be used to \ufb01lter a corpus consisting of original text and its MT - derived translation , to produce a parallel corpus for training of other MT systems , ful\ufb01lling a role similar to that proposed by ( Rikters and Fishel , 2017 ) .", "entities": []}, {"text": "One point for future research is how effective a \ufb01ltering system based onPAM would be , in comparison to already proposed systems .", "entities": []}, {"text": "Future work involves some relatively simple activities that can be imminently implemented , such as releasing the modi\ufb01ed version of PAM for experimentation by interested parties .", "entities": []}, {"text": "Another short term activity involves using the proposed method with sacreBLEU instead of the BLEU and NIST metrics provided by mt - eval .", "entities": [[10, 11, "MetricName", "sacreBLEU"], [14, 15, "MetricName", "BLEU"]]}, {"text": "Future experiments will investigate the effectiveness of this hybrid approach for other language pairs .", "entities": []}, {"text": "One area of interest would be to determine the effectiveness of the PAM - based method when very limited dictionaries are available as well as the limitations when the accuracy of the parser used is relatively low .", "entities": [[29, 30, "MetricName", "accuracy"]]}, {"text": "All these represent issues for the future .", "entities": []}, {"text": "It is also planned to study the approach using systematic optimisation of the PAM parameters , to identify in more detail con\ufb01gurations that produce more accurate translations .", "entities": []}, {"text": "Another possibility is to use PAM to detect sub - sentential parts of the translated sentences with particularly poor alignments between input and translation and seek better translations of only these speci\ufb01c parts .", "entities": []}, {"text": "Another direction is to investigate more extensively cases where the translation is not suf\ufb01ciently close to the input sentence .", "entities": []}, {"text": "Then , comparisons to other low - scored translations are more dif\ufb01cult and result in a reduced level of con\ufb01dence of the chosen translation .", "entities": []}, {"text": "Such a line of study will evaluate more thoroughly the robustness of the proposed method .", "entities": []}, {"text": "Acknowledgements The authors acknowledge support of this work by the project \u201c DRASSI \u201d ( MIS5002437 ) which is implemented under the Action \u201c Reinforcement of the Research and Innovation Infrastructure \u201d , funded by the Operational Programme \u201c Competitiveness , Entrepreneurship and Innovation \u201d ( NSRF2014 - 2020 ) and co-\ufb01nanced by Greece and the European Commission ( European Regional Development Fund ) .", "entities": []}, {"text": "The authors wish to acknowledge the contribution of NVIDIA who donated for research purposes in the area of Machine Translation a Titan XP GPU card under the NVIDIA Academic Support Programme to the MT group of ILSP / Athena R.C.", "entities": [[18, 20, "TaskName", "Machine Translation"], [21, 22, "DatasetName", "Titan"]]}, {"text": "1849BLEU", "entities": []}, {"text": "NIST criterion transformer amun s2s transformer amun s2s", "entities": []}, {"text": "BLEU - optimised ( 1 ) 35.22 36.92 35.28 6.238 6.440 6.459 Entropy - optimised ( 2 ) 34.80 37.91 35.25 6.213 6.532 6.428 Ce - mean - optimised ( 3 ) 34.96 37.77 35.58 6.222 6.524 6.476 Marian - ensemble ( 1,2,3 ) 35.63 38.38 39.70 6.305 6.590 6.707 PAM \u2013 ensemble + Uscore 33.94 37.25 39.57 6.128 6.463 6.721 * PAM \u2013 ensemble + Wscore 35.99 * 38.56 * 39.79 * 6.384 * 6.584 6.758 * Table 3 : Translation accuracy of NMT models and ensembles , where each ensemble consists of identically structured NMTs that have been optimized with different criteria ( using testset1 )", "entities": [[0, 1, "MetricName", "BLEU"], [81, 82, "TaskName", "Translation"], [82, 83, "MetricName", "accuracy"]]}, {"text": "BLEU NIST criterion transformer amun s2s transformer amun s2s BLEU - optimised ( 1 ) 18.27 19.11 18.92 4.100 4.271 3.894 Entropy - optimised ( 2 ) 20.41 19.07 20.04 4.944 4.260 4.396 Ce - mean - optimised ( 3 ) 18.84 18.83 20.48 4.254 4.129 4.526 Marian - ensemble ( 1,2,3 ) 26.48 26.9 5 28.79 6.454 6.507 6.735 PAM \u2013 ensemble + Uscore 26.35 27.61 * 28.96 * 6.407 6.467 6.684 PAM \u2013 ensemble + Wscore 26.68 * 27.45 * 28.85 * 6.363 6.513 * 6.716 Table 4 : Translation accuracy of NMT models and ensembles , where each ensemble consists of identically structured NMTs that have been optimized with different criteria ( using testset2 ) Testset1 Testset1", "entities": [[0, 1, "MetricName", "BLEU"], [9, 10, "MetricName", "BLEU"], [91, 92, "TaskName", "Translation"], [92, 93, "MetricName", "accuracy"]]}, {"text": "Testset2 Testset2 Model BLEU NIST BLEU", "entities": [[3, 4, "MetricName", "BLEU"], [5, 6, "MetricName", "BLEU"]]}, {"text": "NIST Best NMT ( single model ) 95.5 % ( amun ) 97.4 % ( amun ) 71.1 % ( s2s ) 73.4 % ( transf ) Best NMT ( Marian - ensemble ) 100 % ( s2s ) 100 % ( s2s ) 100 % ( s2s ) 100 % ( s2s ) Transformer ( single model ) 88.7 % 93.1 % 70.9 % 73.4 % Transformer ( Marian - ensem ) 89.7 % 94.0 % 92.0 % 95.8 % Transf PAM + Uscore 85.5 % 91.4 % 91.5 % 95.2 % Transf PAM + Wscore 90.7 % 95.2 % 92.7 % 94.5 % Table 5 : Scores achieved for testset1 by different transformer models in comparison to the two baseline models , reported in the \ufb01rst two rows .", "entities": [[54, 55, "MethodName", "Transformer"], [67, 68, "MethodName", "Transformer"]]}, {"text": "Scores are normalized over the Marian - ensemble score ( cf . row 2 ) .", "entities": []}, {"text": "1850References Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "Computing Research Repository , arXiv:1409.0473 .", "entities": []}, {"text": "Version 7 . Lo\u00a8\u0131c Barrault , Ond \u02c7rej Bojar , Marta R. Costa - juss ` a , Christian Federmann , Mark Fishel , Yvette Graham , Barry Haddow , Matthias Huck , Philipp Koehn , Shervin Malmasi , Christof Monz , Mathias M \u00a8uller , Santanu Pal , Matt Post , and Marcos Zampieri .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Findings of the 2019 conference on machine translation ( WMT19 ) .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Fourth Conference on Machine Translation ( Volume 2 : Shared Task Papers , Day 1 ) , pages 1\u201361 , Florence , Italy .", "entities": [[7, 9, "TaskName", "Machine Translation"], [24, 25, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ond\u02c7rej Bojar , Rajen Chatterjee , Christian Federmann , Yvette Graham , Barry Haddow , Matthias Huck , Antonio Jimeno Yepes , Philipp Koehn , Varvara Logacheva , Christof Monz , Matteo Negri , Aur \u00b4 elie N\u00b4ev\u00b4eol , Mariana Neves , Martin Popel , Matt Post , Raphael Rubino , Carolina Scarton , Lucia Specia , Marco Turchi , Karin Verspoor , and Marcos Zampieri .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Findings of the 2016 conference on machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the First Conference on Machine Translation : Volume 2 , Shared Task Papers , pages 131\u2013198 , Berlin , Germany .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ond\u02c7rej Bojar , Rajen Chatterjee , Christian Federmann , Barry Haddow , Matthias Huck , Chris Hokamp , Philipp Koehn , Varvara Logacheva , Christof Monz , Matteo Negri , Matt Post , Carolina Scarton , Lucia Specia , and Marco Turchi .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Findings of the 2015 workshop on statistical machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Tenth Workshop on Statistical Machine Translation , pages 1\u201346 , Lisbon , Portugal . Association for Computational Linguistics .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Ond\u02c7rej Bojar , Christian Federmann , Mark Fishel , Yvette Graham , Barry Haddow , Philipp Koehn , and Christof Monz .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Findings of the 2018 conference on machine translation ( WMT18 ) .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Shared Task Papers , pages 272\u2013303 , Belgium , Brussels . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Marine Carpuat , Yogarshi Vyas , and Xing Niu . 2017 .", "entities": []}, {"text": "Detecting cross - lingual semantic divergence for neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the First Workshop on Neural Machine Translation , pages 69 \u2013 79 , Vancouver .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mauro Cettolo , Jan Niehues , Sebastian Stuker , Luisa Bentivogli , Roldano Cattoni , and Marcello Federico . 2015 .", "entities": []}, {"text": "The iwslt 2015 evaluation campaign .", "entities": []}, {"text": "In IWSLT Proceedings , Da Nang .", "entities": []}, {"text": "Michael Denkowski and Graham Neubig .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Stronger baselines for trustable results in neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the First Workshop on Neural Machine Translation , pages 18\u201327,Vancouver .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "George Doddington .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Automatic evaluation of machine translation quality using n - gram cooccurrence statistics .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In HLT \u2019 02 : Proceedings of the Second International Conference on Human Language Technology Research , pages 138\u2013145 .", "entities": []}, {"text": "Chris Hokamp .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Ensembling factored neural machine translation models for automatic post - editing and quality estimation .", "entities": [[3, 5, "TaskName", "machine translation"], [7, 11, "TaskName", "automatic post - editing"]]}, {"text": "In Proceedings of the Second Conference on Machine Translation , pages 647 \u2013 654 , Copenhagen , Denmark .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Marcin Junczys - Dowmunt , Roman Grundkiewicz , Tomasz Dwojak , Hieu Hoang , Kenneth Hea\ufb01eld , Tom Neckermann , Frank Seide , Ulrich Germann , Alham Fikri Aji , Nikolay Bogoychev , Andr \u00b4 e F. T. Martins , and Alexandra Birch .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Marian :", "entities": []}, {"text": "Fast neural machine translation in C++ .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of ACL 2018 , System Demonstrations , pages 116 \u2013 121 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Philipp Koehn .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Statistical Machine Translation .", "entities": [[1, 3, "TaskName", "Machine Translation"]]}, {"text": "Cambridge University Press , Cambridge , UK .", "entities": [[0, 1, "DatasetName", "Cambridge"], [4, 5, "DatasetName", "Cambridge"]]}, {"text": "Philipp Koehn and Rebecca Knowles .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Six challenges for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the First Workshop on Neural Machine Translation , pages 28\u201339 , Vancouver .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311\u2013318 , Philadelphia , Pennsylvania , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Matiss Rikters and Mark Fishel .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Con\ufb01dence through attention .", "entities": []}, {"text": "CoRR , abs/1710.03743 .", "entities": []}, {"text": "Mat\u00af\u0131ss Rikters , M \u00afarcis Pinnis , and Rihards Kri \u02c7slauks .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Training and adapting multilingual NMT for less - resourced and morphologically rich languages .", "entities": []}, {"text": "In Proceedings of the Eleventh International Conference on Language Resources and Evaluation ( LREC-2018 ) , Miyazaki , Japan .", "entities": []}, {"text": "European Languages Resources Association ( ELRA ) .", "entities": []}, {"text": "Helmut Schmid .", "entities": []}, {"text": "1994 .", "entities": []}, {"text": "Probabilistic part - of - speech tagging using decision trees .", "entities": [[1, 7, "TaskName", "part - of - speech tagging"]]}, {"text": "In Proceedings of International Conference on New Methods in Language Processing , Manchester , UK .", "entities": []}, {"text": "Rico Sennrich , Orhan Firat , Kyunghyun Cho , Alexandra Birch , Barry Haddow , Julian Hitschler , Marcin Junczys - Dowmunt , Samuel L \u00a8aubli , Antonio Valerio Miceli Barone , Jozef Mokry , and Maria N \u02d8adejde . 2017 .", "entities": []}, {"text": "Nematus : a toolkit for neural machine translation .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics ,", "entities": []}, {"text": "1851pages 65\u201368 , Valencia , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich and Biao Zhang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Revisiting lowresource neural machine translation : A case study .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 211 \u2013 221 , Florence , Italy .", "entities": [[18, 19, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ilia Sutskever , Oriol Vinyals , and Quok V .", "entities": []}, {"text": "Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "In Proceedings of the NIPS Conference , page 3104\u20133112 .", "entities": []}, {"text": "NIPS .", "entities": []}, {"text": "George Tambouratzis , Marina Vassiliou , and Sokratis So\ufb01anopoulos . 2017 .", "entities": []}, {"text": "Machine Translation with Minimal Reliance on Parallel Resources .", "entities": [[0, 2, "TaskName", "Machine Translation"]]}, {"text": "SpringerVerlag , Berlin .", "entities": []}, {"text": "Michalis Troullinos .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Phrase aligner .", "entities": []}, {"text": "technical report .", "entities": []}, {"text": "Faculty of Informatics , Masaryk University Brno , FI MU Report Series FIMU - RS-2013 - 2 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 , pages 5998\u20136008 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Barret Zoph , Deniz Yuret , Jonathan May , and Kevin Knight .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Transfer learning for low - resource neural machine translation .", "entities": [[0, 2, "TaskName", "Transfer learning"], [3, 9, "TaskName", "low - resource neural machine translation"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1568\u20131575 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}]