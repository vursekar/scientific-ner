[{"text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 465\u2013476 July 5 - 10 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics465Dice Loss for Data - imbalanced NLP Tasks Xiaoya Li| , Xiaofei Sun| , Yuxian Meng| , Junjun Liang| , Fei Wu\u007fand", "entities": []}, {"text": "Jiwei Li\u007f| \u007fDepartment of Computer Science and Technology , Zhejiang University |Shannon . AI fxiaoya li , xiaofei sun , yuxian meng , jiwei lig@shannonai.com , wufei@cs.zju.edu.cn Abstract Many NLP tasks such as tagging and machine reading comprehension ( MRC ) are faced with the severe data imbalance issue : negative examples signi\ufb01cantly outnumber positive ones , and the huge number of easy - negative examples overwhelms training .", "entities": [[35, 38, "TaskName", "machine reading comprehension"]]}, {"text": "The most commonly used cross entropy criteria is actually accuracy - oriented , which creates a discrepancy between training and test .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "At training time , each training instance contributes equally to the objective function , while at test time F1 score concerns more about positive examples .", "entities": [[18, 20, "MetricName", "F1 score"]]}, {"text": "In this paper , we propose to use dice loss in replacement of the standard cross - entropy objective for data - imbalanced NLP tasks .", "entities": [[8, 10, "MethodName", "dice loss"]]}, {"text": "Dice loss is based on the S\u00f8rensen \u2013 Dice coef\ufb01cient ( Sorensen , 1948 ) or Tversky index ( Tversky , 1977 ) , which attaches similar importance to false positives and false negatives , and is more immune to the data - imbalance issue .", "entities": [[0, 2, "MethodName", "Dice loss"], [8, 9, "MetricName", "Dice"]]}, {"text": "To further alleviate the dominating in\ufb02uence from easy - negative examples in training , we propose to associate training examples with dynamically adjusted weights to deemphasize easy - negative examples .", "entities": []}, {"text": "Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training .", "entities": [[12, 14, "MetricName", "F1 score"], [18, 20, "MethodName", "dice loss"]]}, {"text": "With the proposed training objective , we observe signi\ufb01cant performance boosts over a wide range of data imbalanced NLP tasks .", "entities": []}, {"text": "Notably , we are able to achieve SOTA results on CTB5 , CTB6 and UD1.4 for the part of speech tagging task , and competitive or even better results on CoNLL03 , OntoNotes5.0 , MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identi\ufb01cation tasks .", "entities": [[30, 31, "DatasetName", "CoNLL03"], [39, 42, "TaskName", "named entity recognition"], [46, 49, "TaskName", "machine reading comprehension"]]}, {"text": "The code can be found athttps://github.com/ShannonAI/ dice_loss_for_NLP .Task", "entities": []}, {"text": "# neg # pos ratio CoNLL03", "entities": [[5, 6, "DatasetName", "CoNLL03"]]}, {"text": "NER 170 K 34 K 4.98 OntoNotes5.0", "entities": [[0, 1, "TaskName", "NER"]]}, {"text": "NER 1.96 M 239 K 8.18 SQuAD 1.1 ( Rajpurkar et al . , 2016 ) 10.3 M 175 K 55.9 SQuAD 2.0 ( Rajpurkar et al . , 2018 ) 15.4 M 188 K 82.0 QUOREF ( Dasigi et al . , 2019 ) 6.52 M 38.6 K 169 Table 1 : Number of positive and negative examples and their ratios for different data - imbalanced NLP tasks .", "entities": [[0, 1, "TaskName", "NER"], [6, 7, "DatasetName", "SQuAD"], [21, 22, "DatasetName", "SQuAD"], [36, 37, "DatasetName", "QUOREF"]]}, {"text": "1 Introduction Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension .", "entities": [[18, 21, "TaskName", "machine reading comprehension"]]}, {"text": "Table 1 gives concrete examples : for the Named Entity Recognition ( NER ) task ( Sang and De Meulder , 2003 ; Nadeau and Sekine , 2007 ) , most tokens are backgrounds with tagging class O. Speci\ufb01cally , the number of tokens with tagging class Ois 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset ; Dataimbalanced issue is more severe for MRC tasks ( Rajpurkar et al . , 2016 ;", "entities": [[8, 11, "TaskName", "Named Entity Recognition"], [12, 13, "TaskName", "NER"], [59, 60, "DatasetName", "CoNLL03"]]}, {"text": "Nguyen et al . , 2016 ; Rajpurkar et al . , 2018 ; Ko \u02c7cisk`y et al . , 2018 ; Dasigi et al . , 2019 ) with the value of negative - positive ratio being 50 - 200 , which is due to the reason that the task of MRC is usually formalized as predicting thestarting andending indexes conditioned on the query and the context , and given a chunk of text of an arbitrary length , only two tokens are positive ( or of interest ) with all the rest being background .", "entities": []}, {"text": "Data imbalance results in the following two issues : ( 1)the training - test discrepancy : Without balancing the labels , the learning process tends to converge to a point that strongly biases towards class with the majority label .", "entities": []}, {"text": "This actually creates a discrepancy between training and test : at training time , each training instance contributes equally to the objective function , whereas at test time , F1 gives equal weight to positive and negative examples ; ( 2 ) the overwhelming effect of easy - negative examples .", "entities": [[29, 30, "MetricName", "F1"]]}, {"text": "As pointed out by Meng et al .", "entities": []}, {"text": "( 2019 ) , a signi\ufb01cantly large number of negative examples also", "entities": []}, {"text": "466means that the number of easy - negative example is large .", "entities": []}, {"text": "The huge number of easy examples tends to overwhelm the training , making the model not suf\ufb01ciently learn to distinguish between positive examples and hard - negative examples .", "entities": []}, {"text": "The crossentropy objective ( CE for short ) or maximum likelihood ( MLE ) objective , which is widely adopted as the training objective for data - imbalanced NLP tasks ( Lample et al . , 2016 ; Wu et al . , 2019 ; Devlin et al . , 2018 ; Yu et al . , 2018a ; McCann et al . , 2018 ; Ma and Hovy , 2016 ; Chen et al . , 2017 ) , handles neither of the issues .", "entities": []}, {"text": "To handle the \ufb01rst issue , we propose to replace CE or MLE with losses based on the S\u00f8rensen \u2013 Dice coef\ufb01cient ( Sorensen , 1948 ) or Tversky index ( Tversky , 1977 ) .", "entities": [[20, 21, "MetricName", "Dice"]]}, {"text": "The S\u00f8rensen \u2013 Dice coef\ufb01cient , dice loss for short , is the harmonic mean of precision and recall .", "entities": [[3, 4, "MetricName", "Dice"], [6, 8, "MethodName", "dice loss"]]}, {"text": "It attaches equal importance to false positives ( FPs ) and false negatives ( FNs ) and is thus more immune to data - imbalanced datasets .", "entities": []}, {"text": "Tversky index extends dice loss by using a weight that trades precision and recall , which can be thought as the approximation of the F \f score , and thus comes with more \ufb02exibility .", "entities": [[3, 5, "MethodName", "dice loss"]]}, {"text": "Therefore , we use dice loss or Tversky index to replace CE loss to address the \ufb01rst issue .", "entities": [[4, 6, "MethodName", "dice loss"], [12, 13, "MetricName", "loss"]]}, {"text": "Only using dice loss or Tversky index is not enough since they are unable to address the dominating in\ufb02uence of easy - negative examples .", "entities": [[2, 4, "MethodName", "dice loss"]]}, {"text": "This is intrinsically because dice loss is actually a soft version of the F1 score .", "entities": [[4, 6, "MethodName", "dice loss"], [13, 15, "MetricName", "F1 score"]]}, {"text": "Taking the binary classi\ufb01cation task as an example , at test time , an example will be classi\ufb01ed as negative as long as its probability is smaller than 0.5 , but training will push the value to 0 as much as possible .", "entities": [[37, 38, "DatasetName", "0"]]}, {"text": "This gap is n\u2019t a big issue for balanced datasets , but is extremely detrimental if a big proportion of training examples are easynegative ones : easy - negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily .", "entities": [[41, 42, "DatasetName", "0"]]}, {"text": "Meanwhile , the model can hardly distinguish between hard - negative examples and positive ones .", "entities": []}, {"text": "Inspired by the idea of focal loss ( Lin et al . , 2017 ) in computer vision , we propose a dynamic weight adjusting strategy , which associates each training example with a weight in proportion to ( 1\u0000p ) , and this weight dynamically changes as training proceeds .", "entities": [[0, 1, "DatasetName", "Inspired"], [5, 7, "MethodName", "focal loss"]]}, {"text": "This strategy helps deemphasize con\ufb01dent examples during training as their probability papproaches 1 , making the model attentive to hard - negative examples , and thus alleviates the dominating effect of easy - negative exam - ples .", "entities": []}, {"text": "Combing both strategies , we observe significant performance boosts on a wide range of data imbalanced NLP tasks .", "entities": []}, {"text": "The rest of this paper is organized as follows : related work is presented in Section 2 .", "entities": []}, {"text": "We describe different proposed losses in Section 3 .", "entities": []}, {"text": "Experimental results are presented in Section 4 .", "entities": []}, {"text": "We perform ablation studies in Section 5 , followed by a brief conclusion in Section 6 . 2 Related Work 2.1 Data Resampling The idea of weighting training examples has a long history .", "entities": []}, {"text": "Importance sampling ( Kahn and Marshall , 1953 ) assigns weights to different samples and changes the data distribution .", "entities": []}, {"text": "Boosting algorithms such as AdaBoost ( Kanduri et al . , 2018 ) select harder examples to train subsequent classi\ufb01ers .", "entities": []}, {"text": "Similarly , hard example mining ( Malisiewicz et al . , 2011 ) downsamples the majority class and exploits the most dif\ufb01cult examples .", "entities": []}, {"text": "Oversampling ( Chen et al . , 2010 ; Chawla et al . , 2002 ) is used to balance the data distribution .", "entities": []}, {"text": "Another line of data resampling is to dynamically control the weights of examples as training proceeds .", "entities": []}, {"text": "For example , focal loss ( Lin et al . , 2017 ) used a soft weighting scheme that emphasizes harder examples during training .", "entities": [[3, 5, "MethodName", "focal loss"]]}, {"text": "In self - paced learning ( Kumar et al . , 2010 ) , example weights are obtained through optimizing the weighted training loss which encourages learning easier examples \ufb01rst .", "entities": [[6, 7, "DatasetName", "Kumar"], [23, 24, "MetricName", "loss"]]}, {"text": "At each training step , selfpaced learning algorithm optimizes model parameters and example weights jointly .", "entities": []}, {"text": "Other works ( Chang et al . , 2017 ; Katharopoulos and Fleuret , 2018 ) adjusted the weights of different training examples based on training loss .", "entities": [[26, 27, "MetricName", "loss"]]}, {"text": "Besides , recent work ( Jiang et al . , 2017 ; Fan et al . , 2018 ) proposed to learn a separate network to predict sample weights .", "entities": []}, {"text": "2.2 Data Imbalance Issue in Computer Vision The background - object label imbalance issue is severe and thus well studied in the \ufb01eld of object detection ( Li et al . , 2015 ; Girshick , 2015 ; He et al . , 2015 ;", "entities": [[24, 26, "TaskName", "object detection"]]}, {"text": "Girshick et al . , 2013 ; Ren et al . , 2015 ) .", "entities": []}, {"text": "The idea of hard negative mining ( HNM ) ( Girshick et al . , 2013 ) has gained much attention recently .", "entities": []}, {"text": "Pang et al .", "entities": []}, {"text": "( 2019 ) proposed a novel method called IoU - balanced sampling and Chen et al .", "entities": [[8, 12, "MethodName", "IoU - balanced sampling"]]}, {"text": "( 2019 ) designed a ranking model to replace the conventional classi\ufb01cation task with an average - precision loss", "entities": [[18, 19, "MetricName", "loss"]]}, {"text": "467to alleviate the class imbalance issue .", "entities": []}, {"text": "The efforts made on object detection have greatly inspired us to solve the data imbalance issue in NLP .", "entities": [[4, 6, "TaskName", "object detection"]]}, {"text": "Sudre et al .", "entities": []}, {"text": "( 2017 ) addressed the severe class imbalance issue for the image segmentation task .", "entities": []}, {"text": "They proposed to use the class re - balancing property of the Generalized Dice Loss as the training objective for unbalanced tasks .", "entities": [[13, 15, "MethodName", "Dice Loss"]]}, {"text": "Shen et al .", "entities": []}, {"text": "( 2018 ) investigated the in\ufb02uence of Dice - based loss for multi - class organ segmentation using a dataset of abdominal CT volumes .", "entities": [[7, 8, "MetricName", "Dice"], [10, 11, "MetricName", "loss"]]}, {"text": "Kodym et al .", "entities": []}, {"text": "( 2018 ) proposed to use the batch soft Dice loss function to train the CNN network for the task of segmentation of organs at risk ( OAR ) of medical images .", "entities": [[9, 11, "MethodName", "Dice loss"]]}, {"text": "Shamir et al .", "entities": []}, {"text": "( 2019 ) extended the de\ufb01nition of the classical Dice coef\ufb01cient to facilitate the direct comparison of a ground truth binary image with a probabilistic map .", "entities": [[9, 10, "MetricName", "Dice"]]}, {"text": "In this paper , we introduce dice loss into NLP tasks as the training objective and propose a dynamic weight adjusting strategy to address the dominating in\ufb02uence of easy - negative examples .", "entities": [[6, 8, "MethodName", "dice loss"]]}, {"text": "3 Losses 3.1 Notation For illustration purposes , we use the binary classi\ufb01cation task to demonstrate how different losses work .", "entities": []}, {"text": "The mechanism can be easily extended to multi - class classi\ufb01cation .", "entities": []}, {"text": "Let Xdenote a set of training instances and each instance xi2Xis associated with a golden binary label yi=", "entities": []}, {"text": "[ yi0;yi1 ] denoting the ground - truth class xibelongs to ,", "entities": []}, {"text": "andpi=", "entities": []}, {"text": "[ pi0;pi1]is the predicted probabilities of the two classes respectively ,", "entities": []}, {"text": "where yi0;yi12 f0;1g;pi0;pi12[0;1]andpi1+pi0= 1 . 3.2 Cross Entropy Loss", "entities": []}, {"text": "The vanilla cross entropy ( CE ) loss is given by : CE=\u00001 NX iX", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "j2f0;1gyijlogpij ( 1 ) As can be seen from Eq.1 , each xicontributes equally to the \ufb01nal objective .", "entities": []}, {"text": "Two strategies are normally used to address the the case where we wish that not all xiare treated equally : associating different classes with different weighting factor \u000b  or resampling the datasets .", "entities": []}, {"text": "For the former , Eq.1 is adjusted as follows :", "entities": []}, {"text": "Weighted CE = \u00001", "entities": []}, {"text": "NX i \u000b iX j2f0;1gyijlogpij ( 2)where \u000b i2[0;1]may be set by the inverse class frequency or treated as a hyperparameter to set by cross validation .", "entities": []}, {"text": "In this work , we use lg(n\u0000nt nt+K ) to calculate the coef\ufb01cient \u000b , wherentis the number of samples with class tandnis the total number of samples in the training set .", "entities": [[17, 20, "HyperparameterName", "number of samples"], [25, 28, "HyperparameterName", "number of samples"]]}, {"text": "Kis a hyperparameter to tune .", "entities": []}, {"text": "Intuitively , this equation assigns less weight to the majority class and more weight to the minority class .", "entities": []}, {"text": "The data resampling strategy constructs a new dataset by sampling training examples from the original dataset based on human - designed criteria , e.g. extracting equal training samples from each class .", "entities": []}, {"text": "Both strategies are equivalent to changing the data distribution during training and thus are of the same nature .", "entities": []}, {"text": "Empirically , these two methods are not widely used due to the trickiness of selecting \u000b especially for multi - class classi\ufb01cation tasks and that inappropriate selection can easily bias towards rare classes ( Valverde et al . , 2017 ) .", "entities": []}, {"text": "3.3 Dice Coef\ufb01cient and Tversky Index S\u00f8rensen \u2013 Dice coef\ufb01cient ( Sorensen , 1948 ; Dice , 1945 ) , dice coef\ufb01cient ( DSC ) for short , is an F1oriented statistic used to gauge the similarity of two sets .", "entities": [[1, 2, "MetricName", "Dice"], [8, 9, "MetricName", "Dice"], [15, 16, "MetricName", "Dice"]]}, {"text": "Given two sets AandB , the vanilla dice coef\ufb01cient between them is given as follows : DSC(A;B ) = 2jA\\Bj jAj+jBj(3 )", "entities": []}, {"text": "In our case , Ais the set that contains all positive examples predicted by a speci\ufb01c model , and Bis the set of all golden positive examples in the dataset .", "entities": []}, {"text": "When applied to boolean data with the de\ufb01nition of true positive ( TP ) , false positive ( FP ) , and false negative ( FN ) , it can be then written as follows :", "entities": []}, {"text": "DSC = 2TP 2TP+FN+FP=2TP TP+FNTP TP+FP TP TP+FN+TP", "entities": []}, {"text": "TP+FP", "entities": []}, {"text": "= 2Pre\u0002Rec Pre+Rec = F1 ( 4 ) For an individual example xi , its corresponding dice coef\ufb01cient is given as follows : DSC(xi ) = 2pi1yi1 pi1+yi1(5 )", "entities": [[4, 5, "MetricName", "F1"]]}, {"text": "As can be seen , a negative example ( yi1= 0 ) does not contribute to the objective .", "entities": [[10, 11, "DatasetName", "0"]]}, {"text": "For smoothing purposes , it is common to add a", "entities": []}, {"text": "factor to both the nominator and the denominator , making the form to be as follows ( we simply set", "entities": []}, {"text": "= 1 in the rest of", "entities": []}, {"text": "468Loss Formula ( one sample xi ) CE\u0000P", "entities": []}, {"text": "j2f0;1gyijlogpij WCE\u0000 \u000b iP", "entities": []}, {"text": "j2f0;1gyijlogpij DL 1\u00002pi1yi1 +", "entities": []}, {"text": "p2 i1+y2 i1 +", "entities": []}, {"text": "TL 1\u0000pi1yi1 +", "entities": []}, {"text": "pi1yi1 + \u000b pi1yi0 + \f  pi0yi1 +", "entities": []}, {"text": "DSC 1\u00002(1\u0000pi1)pi1\u0001yi1 +", "entities": []}, {"text": "( 1\u0000pi1)pi1+yi1 +", "entities": []}, {"text": "FL\u0000 \u000b iP j2f0;1g(1\u0000pij )", "entities": []}, {"text": "logpij Table 2 : Different losses and their formulas .", "entities": []}, {"text": "We add +1 to DL , TL and DSC so that they are positive .", "entities": []}, {"text": "this paper ): DSC(xi ) = 2pi1yi1 +", "entities": []}, {"text": "pi1+yi1 +", "entities": []}, {"text": "( 6 ) As can be seen , negative examples whose DSC is", "entities": []}, {"text": "pi1 +", "entities": []}, {"text": ", also contribute to the training .", "entities": []}, {"text": "Additionally , Milletari et al .", "entities": []}, {"text": "( 2016 ) proposed to change the denominator to the square form for faster convergence , which leads to the following dice loss ( DL ): DL=1 NX i\u0014 1\u00002pi1yi1 +", "entities": [[21, 23, "MethodName", "dice loss"]]}, {"text": "p2 i1+y2 i1 +", "entities": []}, {"text": "\u0015 ( 7 ) Another version of DL is to directly compute setlevel dice coef\ufb01cient instead of the sum of individual dice coef\ufb01cient , which is easier for optimization : DL= 1\u00002P ipi1yi1 +", "entities": []}, {"text": "P", "entities": []}, {"text": "ip2 i1+P iy2 i1", "entities": []}, {"text": "+", "entities": []}, {"text": "( 8) Tversky index ( TI ) , which can be thought as the approximation of the F \f score , extends dice coef\ufb01cient to a more general case .", "entities": []}, {"text": "Given two sets Aand B , tversky index is computed as follows : TI = jA\\Bj jA\\Bj+ \u000b jAnBj+ \f jBnAj(9 ) Tversky index offers the \ufb02exibility in controlling the tradeoff between false - negatives and falsepositives .", "entities": []}, {"text": "It degenerates to DSC if \u000b = \f = 0:5 .", "entities": []}, {"text": "The Tversky loss ( TL ) is thus given as follows : TL=1 NX i\u0014 1\u0000pi1yi1 +", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "pi1yi1 + \u000b pi1yi0 + \f pi0yi1 +", "entities": []}, {"text": "\u0015 ( 10 ) 3.4 Self - adjusting Dice Loss Consider a simple case where the dataset consists of only one example xi , which is classi\ufb01ed as positive as long as pi1is larger than 0.5 .", "entities": [[8, 10, "MethodName", "Dice Loss"]]}, {"text": "The computation ofF1score is actually as follows : F1(xi )", "entities": []}, {"text": "= 2I(pi1>0:5)yi1 I(pi1>0:5 )", "entities": []}, {"text": "+ yi1(11 ) 0", "entities": [[3, 4, "DatasetName", "0"]]}, {"text": "0.1 0.2 0.3 0.4 0.5 0.6 0.7", "entities": []}, {"text": "0.8 0.9 1 - 2 - 1.5 - 1 - 0.500.511.52Derivatives FL ( = 1 )   DL ( = 1 )   TL ( = 0.5 )   DSCFigure 1 : An illustration of derivatives of the four losses .", "entities": []}, {"text": "The derivative of DSC approaches zero right afterpexceeds 0.5 , and for the other losses , the derivatives reach 0 only if the probability is exactly 1 , which means they will push pto 1 as much as possible .", "entities": [[19, 20, "DatasetName", "0"]]}, {"text": "Comparing Eq.5 with Eq.11 , we can see that Eq.5 is actually a soft form of F1 , using a continuous p rather than the binary I(pi1>0:5 ) .", "entities": [[16, 17, "MetricName", "F1"]]}, {"text": "This gap is n\u2019t a big issue for balanced datasets , but is extremely detrimental if a big proportion of training examples are easy - negative ones : easy - negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily .", "entities": [[43, 44, "DatasetName", "0"]]}, {"text": "Meanwhile , the model can hardly distinguish between hardnegative examples and positive ones , which has a huge negative effect on the \ufb01nal F1 performance .", "entities": [[23, 24, "MetricName", "F1"]]}, {"text": "To address this issue , we propose to multiply the soft probability pwith a decaying factor ( 1\u0000p ) , changing Eq.11 to the following adaptive variant of DSC : DSC(xi ) = 2(1\u0000pi1)pi1\u0001yi1 +", "entities": []}, {"text": "( 1\u0000pi1)pi1+yi1 +", "entities": []}, {"text": "( 12 ) One can think ( 1\u0000pi1)as a weight associated with each example , which changes as training proceeds .", "entities": []}, {"text": "The intuition of changing pi1to(1\u0000pi1)pi1is to push down the weight of easy examples .", "entities": []}, {"text": "For easy examples whose probability are approaching 0 or 1,(1\u0000pi1)pi1makes the model attach signi\ufb01cantly less focus to them .", "entities": [[7, 8, "DatasetName", "0"]]}, {"text": "A close look at Eq.12 reveals that it actually mimics the idea of focal loss ( FL for short ) ( Lin et al . , 2017 ) for object detection in vision .", "entities": [[13, 15, "MethodName", "focal loss"], [29, 31, "TaskName", "object detection"]]}, {"text": "Focal loss was proposed for one - stage object detector to handle foreground - background tradeoff encountered during training .", "entities": [[0, 2, "MethodName", "Focal loss"]]}, {"text": "It down - weights the loss assigned to well - classi\ufb01ed examples by adding a ( 1\u0000p )", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "factor , leading the \ufb01nal loss to be \u0000(1\u0000p )", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "logp .", "entities": []}, {"text": "469CTB5 CTB6 UD1.4 Model Prec .", "entities": []}, {"text": "Rec . F1 Prec .", "entities": [[2, 3, "MetricName", "F1"]]}, {"text": "Rec . F1 Prec .", "entities": [[2, 3, "MetricName", "F1"]]}, {"text": "Rec . F1 Joint - POS(Sig)(Shao et", "entities": [[2, 3, "MetricName", "F1"]]}, {"text": "al . , 2017 ) 93.68 94.47 94.07 - - 90.81 89.28 89.54 89.41 Joint - POS(Ens)(Shao et al . , 2017 ) 93.95 94.81 94.38 - - - 89.67 89.86 89.75 Lattice - LSTM(Zhang and Yang , 2018 ) 94.77 95.51 95.14 92.00 90.86 91.43 90.47 89.70 90.09 BERT - Tagger(Devlin et al . , 2018 ) 95.86 96.26 96.06 94.91 94.63 94.77 95.42 94.17 94.79 BERT+FL 96.11 97.42 96.76 95.80 95.08 95.44 96.33 95.85 96.81 ( +0.70 ) ( +0.67 ) ( +2.02 ) BERT+DL 96.77 98.87 97.81 94.08 96.12 95.09 96.10 97.79 96.94 ( +1.75 ) ( +0.32 ) ( +2.15 ) BERT+DSC 97.10 98.75 97.92 96.29 96.85 96.57 96.24 97.73 96.98 ( +1.86 ) ( +1.80 ) ( +2.19 ) Table 3 : Experimental results for Chinese POS datasets including CTB5 , CTB6 and UD1.4 .", "entities": [[49, 50, "MethodName", "BERT"]]}, {"text": "English WSJ Model Prec .", "entities": []}, {"text": "Rec . F1 Meta BiLSTM(Bohnet et", "entities": [[2, 3, "MetricName", "F1"]]}, {"text": "al . , 2018 ) - - 98.23 BERT - Tagger ( Devlin et al . , 2018 ) 99.21 98.36 98.86 BERT - Tagger+FL 98.36 98.97 98.88 ( +0.02 )", "entities": [[8, 9, "MethodName", "BERT"], [22, 23, "MethodName", "BERT"]]}, {"text": "BERT - Tagger+DL 99.34 98.22 98.91 ( +0.05 ) BERT - Tagger+DSC 99.41 98.93 99.38 ( +0.52 )", "entities": [[0, 1, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"]]}, {"text": "English Tweets Model Prec .", "entities": []}, {"text": "Rec .", "entities": []}, {"text": "F1 FastText+CNN+CRF(Godin , 2019 ) - - 91.78 BERT - Tagger ( Devlin et", "entities": [[0, 1, "MetricName", "F1"], [8, 9, "MethodName", "BERT"]]}, {"text": "al . , 2018 ) 92.33 91.98 92.34 BERT - Tagger+FL 91.24 93.22 92.47 ( +0.13 ) BERT - Tagger+DL 91.44 92.88 92.52 ( +0.18 ) BERT - Tagger+DSC 92.87 93.54 92.58 ( +0.24 ) Table 4 : Experimental results for English POS datasets", "entities": [[8, 9, "MethodName", "BERT"], [17, 18, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"]]}, {"text": ".", "entities": []}, {"text": "In Table 2 , we summarize all the aforementioned losses .", "entities": []}, {"text": "Figure 1 gives an explanation from the perspective in derivative : The derivative of DSC approaches zero right after pexceeds 0.5 , which suggests the model attends less to examples once they are correctly classi\ufb01ed .", "entities": []}, {"text": "But for the other losses , the derivatives reach 0 only if the probability is exactly 1 , which means they will push pto 1 as much as possible .", "entities": [[9, 10, "DatasetName", "0"]]}, {"text": "4 Experiments We evaluated the proposed method on four NLP tasks , part - of - speech tagging , named entity recognition , machine reading comprehension and paraphrase identi\ufb01cation .", "entities": [[12, 18, "TaskName", "part - of - speech tagging"], [19, 22, "TaskName", "named entity recognition"], [23, 26, "TaskName", "machine reading comprehension"]]}, {"text": "Hyperparameters are tuned on the corresponding development set of each dataset .", "entities": []}, {"text": "More experiment details including datasets and hyperparameters are shown in supplementary material.4.1 Part - of - Speech Tagging Settings Part - of - speech tagging ( POS ) is the task of assigning a part - of - speech label ( e.g. , noun , verb , adjective ) to each word in a given text .", "entities": [[12, 18, "TaskName", "Part - of - Speech Tagging"], [19, 25, "TaskName", "Part - of - speech tagging"], [34, 37, "DatasetName", "part - of"]]}, {"text": "In this paper , we choose BERT ( Devlin et al . , 2018 ) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank ( Xue et al . , 2005 ) 5.0/6.0 and UD1.4 and English datasets including Wall Street Journal ( WSJ ) and the dataset proposed by Ritter et al . ( 2011 ) .", "entities": [[6, 7, "MethodName", "BERT"], [29, 31, "DatasetName", "Chinese Treebank"]]}, {"text": "We report the span - level micro - averaged precision , recall and F1 for evaluation .", "entities": [[13, 14, "MetricName", "F1"]]}, {"text": "Baselines We used the following baselines : \u000fJoint - POS :", "entities": []}, {"text": "Shao et al .", "entities": []}, {"text": "( 2017 ) jointly learns Chinese word segmentation and POS .", "entities": [[5, 8, "TaskName", "Chinese word segmentation"]]}, {"text": "\u000fLattice - LSTM :", "entities": [[2, 3, "MethodName", "LSTM"]]}, {"text": "Zhang and Yang ( 2018 ) constructs a word - character lattice network .", "entities": []}, {"text": "\u000fBert - Tagger : Devlin et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) treats partof - speech as a tagging task .", "entities": []}, {"text": "Results Table 3 presents the experimental results on Chinese datasets .", "entities": []}, {"text": "As can be seen , the proposed DSC loss outperforms the best baseline results by a large margin , i.e. , outperforming BERT - tagger by +1.86 in terms of F1 score on CTB5 , +1.80 on CTB6 and +2.19 on UD1.4 .", "entities": [[8, 9, "MetricName", "loss"], [22, 23, "MethodName", "BERT"], [30, 32, "MetricName", "F1 score"]]}, {"text": "As far as we know , we are achieving SOTA performances on the three datasets .", "entities": []}, {"text": "Focal loss only obtains a little performance improvement on CTB5 and CTB6 , and the dice loss obtains huge gain on CTB5 but not on CTB6 , which indicates the three losses are not consistently robust in solving the data imbalance issue .", "entities": [[0, 2, "MethodName", "Focal loss"], [15, 17, "MethodName", "dice loss"]]}, {"text": "Table 4 presents the experimental results for English datasets .", "entities": []}, {"text": "470English CoNLL 2003 Model Prec .", "entities": [[1, 3, "DatasetName", "CoNLL 2003"]]}, {"text": "Rec . F1 ELMo(Peters et", "entities": [[2, 3, "MetricName", "F1"]]}, {"text": "al . , 2018 ) - - 92.22 CVT(Clark", "entities": []}, {"text": "et al . , 2018 ) - - 92.6 BERT - Tagger(Devlin et al . , 2018 ) - - 92.8 BERT - MRC(Li et al . , 2019 ) 92.33 94.61 93.04 BERT - MRC+FL 93.13 93.09 93.11 ( +0.06 )", "entities": [[9, 10, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"], [33, 34, "MethodName", "BERT"]]}, {"text": "BERT - MRC+DL 93.22 93.12 93.17 ( +0.12 ) BERT - MRC+DSC 93.41 93.25 93.33 ( +0.29 ) English OntoNotes 5.0 Model Prec .", "entities": [[0, 1, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"], [19, 21, "DatasetName", "OntoNotes 5.0"]]}, {"text": "Rec . F1 CVT ( Clark et al . , 2018 ) - - 88.8 BERT - Tagger ( Devlin et", "entities": [[2, 3, "MetricName", "F1"], [3, 4, "MethodName", "CVT"], [15, 16, "MethodName", "BERT"]]}, {"text": "al . , 2018 ) 90.01 88.35 89.16 BERT - MRC(Li et al . , 2019 ) 92.98 89.95 91.11 BERT - MRC+FL 90.13 92.34 91.22 ( +0.11 ) BERT - MRC+DL 91.70 92.06 91.88 ( +0.77 ) BERT - MRC+DSC 91.59 92.56 92.07 ( +0.96 ) Chinese MSRA Model Prec .", "entities": [[8, 9, "MethodName", "BERT"], [20, 21, "MethodName", "BERT"], [29, 30, "MethodName", "BERT"], [38, 39, "MethodName", "BERT"]]}, {"text": "Rec . F1 Lattice - LSTM ( Zhang and Yang , 2018 ) 93.57 92.79 93.18 BERT - Tagger ( Devlin et al . , 2018 ) 94.97 94.62 94.80 Glyce - BERT ( Wu et al . , 2019 ) 95.57 95.51 95.54 BERT - MRC(Li et al . , 2019 ) 96.18 95.12 95.75 BERT - MRC+FL 95.45 95.89 95.67 ( -0.08 ) BERT - MRC+DL 96.20 96.68 96.44 ( +0.69 ) BERT - MRC+DSC 96.67 96.77 96.72 ( +0.97 ) Chinese OntoNotes 4.0 Model Prec .", "entities": [[2, 3, "MetricName", "F1"], [5, 6, "MethodName", "LSTM"], [16, 17, "MethodName", "BERT"], [32, 33, "MethodName", "BERT"], [44, 45, "MethodName", "BERT"], [56, 57, "MethodName", "BERT"], [65, 66, "MethodName", "BERT"], [74, 75, "MethodName", "BERT"], [84, 86, "DatasetName", "OntoNotes 4.0"]]}, {"text": "Rec . F1 Lattice - LSTM ( Zhang and Yang , 2018 ) 76.35 71.56 73.88 BERT - Tagger ( Devlin et al . , 2018 ) 78.01 80.35 79.16 Glyce - BERT ( Wu et al . , 2019 ) 81.87 81.40 80.62 BERT - MRC(Li et al . , 2019 ) 82.98 81.25 82.11 BERT - MRC+FL 83.63 82.97 83.30 ( +1.19 ) BERT - MRC+DL 83.97 84.05 84.01 ( +1.90 ) BERT - MRC+DSC 84.22 84.72 84.47 ( +2.36 ) Table 5 : Experimental results for NER task .", "entities": [[2, 3, "MetricName", "F1"], [5, 6, "MethodName", "LSTM"], [16, 17, "MethodName", "BERT"], [32, 33, "MethodName", "BERT"], [44, 45, "MethodName", "BERT"], [56, 57, "MethodName", "BERT"], [65, 66, "MethodName", "BERT"], [74, 75, "MethodName", "BERT"], [89, 90, "TaskName", "NER"]]}, {"text": "4.2 Named Entity Recognition Settings Named entity recognition ( NER ) is the task of detecting the span and semantic category of entities within a chunk of text .", "entities": [[1, 4, "TaskName", "Named Entity Recognition"], [5, 8, "TaskName", "Named entity recognition"], [9, 10, "TaskName", "NER"]]}, {"text": "Our implementation uses the current state - of - the - art model proposed by Li et al .", "entities": []}, {"text": "( 2019 ) as the backbone , and changes the MLE loss to DSC loss .", "entities": [[11, 12, "MetricName", "loss"], [14, 15, "MetricName", "loss"]]}, {"text": "Datasets that we use include OntoNotes4.0 ( Pradhan et al . , 2011 ) , MSRA ( Levow , 2006 ) , CoNLL2003 ( Sang and Meulder , 2003 ) and OntoNotes5.0 ( Pradhan et al . , 2013 ) .", "entities": [[22, 23, "DatasetName", "CoNLL2003"]]}, {"text": "We report span - level micro - averaged precision , recall and F1.Baselines We use the following baselines : \u000fELMo : a tagging model with pretraining from Peters et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "\u000fLattice - LSTM :", "entities": [[2, 3, "MethodName", "LSTM"]]}, {"text": "Zhang and Yang ( 2018 ) constructs a word - character lattice , only used in Chinese datasets .", "entities": []}, {"text": "\u000fCVT :", "entities": []}, {"text": "Clark et al .", "entities": []}, {"text": "( 2018 ) uses Cross - View Training(CVT ) to improve the representations of a Bi - LSTM encoder .", "entities": [[17, 18, "MethodName", "LSTM"]]}, {"text": "\u000fBert - Tagger : Devlin et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) treats NER as a tagging task .", "entities": [[4, 5, "TaskName", "NER"]]}, {"text": "\u000fGlyce - BERT : Wu et", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "al .", "entities": []}, {"text": "( 2019 ) combines Chinese glyph information with BERT pretraining .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "\u000fBERT - MRC : Li et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) formulates NER as a machine reading comprehension task and achieves SOTA results on Chinese and English NER benchmarks .", "entities": [[4, 5, "TaskName", "NER"], [7, 10, "TaskName", "machine reading comprehension"], [19, 20, "TaskName", "NER"]]}, {"text": "Results Table 5 shows experimental results on NER datasets .", "entities": [[7, 8, "TaskName", "NER"]]}, {"text": "DSC outperforms BERT - MRC(Li et al . , 2019 ) by +0.29 , +0.96 , +0.97 and +2.36 respectively on CoNLL2003 , OntoNotes5.0 , MSRA and OntoNotes4.0 .", "entities": [[2, 3, "MethodName", "BERT"], [21, 22, "DatasetName", "CoNLL2003"]]}, {"text": "As far as we are concerned , we are setting new SOTA performances on all of the four NER datasets .", "entities": [[18, 19, "TaskName", "NER"]]}, {"text": "4.3 Machine Reading Comprehension Settings The task of machine reading comprehension ( MRC ) ( Seo et al . , 2016 ; Wang et al . , 2016 ; Wang and Jiang , 2016 ;", "entities": [[1, 4, "TaskName", "Machine Reading Comprehension"], [8, 11, "TaskName", "machine reading comprehension"]]}, {"text": "Wang et al . , 2016 ; Shen et al . , 2017 ; Chen et al . , 2017 ) predicts the answer span in the passage given a question and the passage .", "entities": []}, {"text": "We followed the standard protocols in Seo et al .", "entities": []}, {"text": "( 2016 ) , in which the start and end indexes of answer are predicted .", "entities": []}, {"text": "We report Extract Match ( EM ) as well as F1 score on validation set .", "entities": [[5, 6, "MetricName", "EM"], [10, 12, "MetricName", "F1 score"]]}, {"text": "We use three datasets on this task : SQuAD v1.1 , SQuAD v2.0 ( Rajpurkar et al . , 2016 , 2018 ) and Quoref ( Dasigi et al . , 2019 ) .", "entities": [[8, 9, "DatasetName", "SQuAD"], [11, 12, "DatasetName", "SQuAD"], [24, 25, "DatasetName", "Quoref"]]}, {"text": "Baselines We used the following baselines : \u000fQANet : Yu et al . ( 2018b ) builds a model based on convolutions and self - attentions .", "entities": []}, {"text": "Convolutions are used to model local interactions and self - attention are used to model global interactions .", "entities": []}, {"text": "\u000fBERT : Devlin et al .", "entities": []}, {"text": "( 2018 ) scores each candidate span and the maximum scoring span is used as a prediction .", "entities": []}, {"text": "\u000fXLNet : Yang et al .", "entities": []}, {"text": "( 2019 ) proposes a generalized autoregressive pretraining method that", "entities": []}, {"text": "471SQuAD v1.1 SQuAD v2.0 QuoRef Model EM F1 EM F1 EM F1 QANet", "entities": [[2, 3, "DatasetName", "SQuAD"], [4, 5, "DatasetName", "QuoRef"], [6, 7, "MetricName", "EM"], [7, 8, "MetricName", "F1"], [8, 9, "MetricName", "EM"], [9, 10, "MetricName", "F1"], [10, 11, "MetricName", "EM"], [11, 12, "MetricName", "F1"]]}, {"text": "( Yu et al . , 2018b ) 73.6 82.7 - - 34.41 38.26 BERT ( Devlin et al . , 2018 ) 84.1 90.9 78.7 81.9 58.44 64.95 BERT+FL 84.67 91.25 78.92 82.20 60.78 66.19 ( +0.57 ) ( +0.35 ) ( +0.22 ) ( +0.30 ) ( +2.34 ) ( +1.24 ) BERT+DL 84.83 91.86 78.99 82.88 62.03 66.88 ( +0.73 ) ( +0.96 ) ( +0.29 ) ( +0.98 ) ( +3.59 ) ( +1.93 ) BERT+DSC 85.34 91.97 79.02 82.95 62.44 67.52 ( +1.24 ) ( +1.07 ) ( +0.32 ) ( +1.05 ) ( +4.00 ) ( +2.57 ) XLNet", "entities": [[14, 15, "MethodName", "BERT"], [104, 105, "MethodName", "XLNet"]]}, {"text": "( Yang et al . , 2019 ) 88.95 94.52 86.12 88.79 64.52 71.49 XLNet+FL 88.90 94.55 87.04 89.32 65.19 72.34 ( -0.05 ) ( +0.03 ) ( +0.92 ) ( +0.53 ) ( +0.67 ) ( +0.85 ) XLNet+DL 89.13 95.36 87.22 89.44 65.77 72.85 ( +0.18 ) ( +0.84 ) ( +1.10 ) ( +0.65 ) ( +1.25 ) ( +1.36 ) XLNet+DSC", "entities": []}, {"text": "89.79 95.77 87.65 89.51 65.98 72.90 ( +0.84 ) ( +1.25 ) ( +1.53 ) ( +0.72 ) ( +1.46 ) ( +1.41 ) Table 6 : Experimental results for MRC task .", "entities": []}, {"text": "MRPC QQP Model F1 F1 BERT ( Devlin et", "entities": [[0, 1, "DatasetName", "MRPC"], [1, 2, "DatasetName", "QQP"], [3, 4, "MetricName", "F1"], [4, 5, "MetricName", "F1"], [5, 6, "MethodName", "BERT"]]}, {"text": "al . , 2018 ) 88.0 91.3 BERT+FL 88.43 91.86 ( +0.43 ) ( +0.56 ) BERT+DL 88.71 91.92 ( +0.71 ) ( +0.62 ) BERT+DSC 88.92 92.11 ( +0.92 ) ( +0.81 ) XLNet ( Yang et al . , 2019 ) 89.2 91.8 XLNet+FL 89.25 92.31 ( +0.05 ) ( +0.51 ) XLNet+DL 89.33 92.39 ( +0.13 ) ( +0.59 ) XLNet+DSC 89.78 92.60 ( +0.58 ) ( +0.79 ) Table 7 : Experimental results for PI task .", "entities": [[34, 35, "MethodName", "XLNet"]]}, {"text": "enables learning bidirectional contexts .", "entities": []}, {"text": "Results Table 6 shows the experimental results for MRC task .", "entities": []}, {"text": "With either BERT or XLNet , our proposed DSC loss obtains signi\ufb01cant performance boost on both EM and F1 .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "XLNet"], [9, 10, "MetricName", "loss"], [16, 17, "MetricName", "EM"], [18, 19, "MetricName", "F1"]]}, {"text": "For SQuADv1.1 , our proposed method outperforms XLNet by +1.25 in terms of F1 score and +0.84 in terms of EM .", "entities": [[7, 8, "MethodName", "XLNet"], [13, 15, "MetricName", "F1 score"], [20, 21, "MetricName", "EM"]]}, {"text": "For SQuAD v2.0 , the proposed method achieves 87.65 on EM and 89.51 on F1 .", "entities": [[1, 2, "DatasetName", "SQuAD"], [10, 11, "MetricName", "EM"], [14, 15, "MetricName", "F1"]]}, {"text": "On QuoRef , the proposed method surpasses XLNet by +1.46 on EM and +1.41 on F1 .", "entities": [[1, 2, "DatasetName", "QuoRef"], [7, 8, "MethodName", "XLNet"], [11, 12, "MetricName", "EM"], [15, 16, "MetricName", "F1"]]}, {"text": "4.4 Paraphrase Identi\ufb01cation Settings Paraphrase identi\ufb01cation ( PI ) is the task of identifying whether two sentences have the same meaning or not .", "entities": []}, {"text": "We conduct experiments on the two widely - used datasets : MRPC ( Dolan and Brockett , 2005 ) and QQP .", "entities": [[11, 12, "DatasetName", "MRPC"], [20, 21, "DatasetName", "QQP"]]}, {"text": "F1 score is reported for comparison .", "entities": [[0, 2, "MetricName", "F1 score"]]}, {"text": "We use BERT ( Devlin et al . , 2018 ) and XLNet ( Yang et al . , 2019 ) as baselines .", "entities": [[2, 3, "MethodName", "BERT"], [12, 13, "MethodName", "XLNet"]]}, {"text": "Results Table 7 shows the results .", "entities": []}, {"text": "We \ufb01nd that replacing the training objective with DSC introduces performance boost for both settings , +0.58 for MRPC and +0.73 for QQP .", "entities": [[18, 19, "DatasetName", "MRPC"], [22, 23, "DatasetName", "QQP"]]}, {"text": "5 Ablation Studies 5.1 Datasets imbalanced to different extents It is interesting to see how differently the proposed objectives affect datasets imbalanced to different extents .", "entities": []}, {"text": "We use the paraphrase identi\ufb01cation dataset QQP ( 37 % positive and 63 % negative ) for studies .", "entities": [[6, 7, "DatasetName", "QQP"]]}, {"text": "To construct datasets with different imbalance degrees , we used the original QQP dataset to construct synthetic training sets with different positive - negative ratios .", "entities": [[12, 13, "DatasetName", "QQP"]]}, {"text": "Models are trained on these different synthetic sets and then test on the same original test set .", "entities": []}, {"text": "\u000fOriginal training set ( original )", "entities": []}, {"text": "The original dataset with 363,871 examples , with 37 % being positive and 63 % being negative \u000fPositive augmentation ( + positive )", "entities": []}, {"text": "We created a balanced dataset by adding positive examples .", "entities": []}, {"text": "We \ufb01rst randomly chose positive training examples in the original training set as templates .", "entities": []}, {"text": "Then we used Spacy1to retrieve entity mentions and replace them with new ones by linking mentions to their corresponding entities in DBpedia .", "entities": [[21, 22, "DatasetName", "DBpedia"]]}, {"text": "The augmented set contains 458,477 examples , with 50 % being positive and 50 % being negative .", "entities": []}, {"text": "\u000fNegative augmentation ( + negative )", "entities": []}, {"text": "We created a more imbalanced dataset .", "entities": []}, {"text": "The size of the newly constructed training set and 1https://github.com/explosion/spaCy", "entities": []}, {"text": "472original + positive + negative - negative + positive & negative BERT 91.3 92.27 90.08 89.73 93.14 BERT+FL 91.86(+0.56 ) 92.64(+0.37 ) 90.61(+0.53 ) 90.79(+1.06 ) 93.45(+0.31 ) BERT+DL 91.92(+0.62 ) 92.87(+0.60 ) 90.22(+0.14 ) 90.49(+0.76 ) 93.52(+0.38 )", "entities": [[11, 12, "MethodName", "BERT"]]}, {"text": "BERT+DSC 92.11(+0.81 ) 92.92(+0.65 ) 90.78(+0.70 ) 90.80(+1.07 ) 93.63(+0.49 ) Table 8 : The effect of different data augmentation ways for QQP in terms of F1 - score .", "entities": [[18, 20, "TaskName", "data augmentation"], [22, 23, "DatasetName", "QQP"], [26, 29, "MetricName", "F1 - score"]]}, {"text": "the data augmented technique are exactly the same as + negative , except that we chose negative training examples as templates .", "entities": []}, {"text": "The augmented training set contains 458,477 examples , with 21 % being positive and 79 % being negative .", "entities": []}, {"text": "\u000fNegative downsampling ( - negative ) We down - sampled negative examples in the original training set to get a balanced training set .", "entities": []}, {"text": "The down - sampled set contains 269,165 examples , with 50 % being positive and 50 % being negative .", "entities": []}, {"text": "\u000fPositive and negative augmentation ( + positive & + negative )", "entities": []}, {"text": "We augmented the original training data with additional positive and negative examples with the data distribution staying the same .", "entities": []}, {"text": "The augmented dataset contains 458,477 examples , with 50 % being positive and 50 % being negative .", "entities": []}, {"text": "Results are shown in Table 8 .", "entities": []}, {"text": "We \ufb01rst look at the \ufb01rst line , with all results obtained using the MLE objective .", "entities": []}, {"text": "We can see that + positive outperforms original , and + negative underperforms original .", "entities": []}, {"text": "This is in line with our expectation since + positive creates a balanced dataset while + negative creates a more imbalanced dataset .", "entities": []}, {"text": "Despite the fact that - negative creates a balanced dataset , the number of training data decreases , resulting in inferior performances .", "entities": []}, {"text": "DSC achieves the highest F1 score across all datasets .", "entities": [[4, 6, "MetricName", "F1 score"]]}, {"text": "Specially , for + positive , DSC achieves minor improvements ( +0.05 F1 ) over DL .", "entities": [[12, 13, "MetricName", "F1"]]}, {"text": "In contrast , it signi\ufb01cantly outperforms DL for + negative dataset .", "entities": []}, {"text": "This is in line with our expectation since DSC helps more on more imbalanced datasets .", "entities": []}, {"text": "The performance of FL and DL are not consistent across different datasets , while DSC consistently performs the best on all datasets .", "entities": []}, {"text": "5.2 Dice loss for accuracy - oriented tasks ?", "entities": [[1, 3, "MethodName", "Dice loss"], [4, 5, "MetricName", "accuracy"]]}, {"text": "We argue that the cross - entropy objective is actually accuracy - oriented , whereas the proposed losses perform as a soft version of F1 score .", "entities": [[10, 11, "MetricName", "accuracy"], [24, 26, "MetricName", "F1 score"]]}, {"text": "ToSST-2 SST-5 Model Acc Acc BERT+CE 94.90 55.57 BERT+DL 94.37 54.63 BERT+DSC 94.84 55.19 Table 9 : The effect of DL and DSC on sentiment classi\ufb01cation tasks .", "entities": [[3, 4, "MetricName", "Acc"], [4, 5, "MetricName", "Acc"]]}, {"text": "BERT+CE refers to \ufb01ne - tuning BERT and setting cross - entropy as the training objective .", "entities": [[6, 7, "MethodName", "BERT"]]}, {"text": "explore the effect of the dice loss on accuracyoriented tasks such as text classi\ufb01cation , we conduct experiments on the Stanford Sentiment Treebank ( SST ) datasets including SST-2 and SST-5 .", "entities": [[5, 7, "MethodName", "dice loss"], [24, 25, "DatasetName", "SST"], [28, 29, "DatasetName", "SST-2"]]}, {"text": "We \ufb01ne - tuned BERT Large with different training objectives .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "Experimental results for SST are shown in Table 9 .", "entities": [[3, 4, "DatasetName", "SST"]]}, {"text": "For SST-5 , BERT with CE achieves 55.57 in terms of accuracy , while DL and DSC perform slightly worse ( 54.63 and 55.19 , respectively ) .", "entities": [[3, 4, "MethodName", "BERT"], [11, 12, "MetricName", "accuracy"]]}, {"text": "Similar phenomenon is observed for SST-2 .", "entities": [[5, 6, "DatasetName", "SST-2"]]}, {"text": "These results verify that the proposed dice loss is not accuracy - oriented , and should not be used for accuracy - oriented tasks .", "entities": [[6, 8, "MethodName", "dice loss"], [10, 11, "MetricName", "accuracy"], [20, 21, "MetricName", "accuracy"]]}, {"text": "5.3 Hyper - parameters in Tversky Index As mentioned in Section 3.3 , Tversky index ( TI ) offers the \ufb02exibility in controlling the tradeoff between false - negatives and false - positives .", "entities": []}, {"text": "In this subsection , we explore the effect of hyperparameters ( i.e. , \u000b and \f ) in TI to test how they manipulate the tradeoff .", "entities": []}, {"text": "We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset .", "entities": [[7, 8, "TaskName", "NER"], [11, 12, "DatasetName", "QuoRef"]]}, {"text": "Experimental results are shown in Table 10 .", "entities": []}, {"text": "The highest F1 on Chinese OntoNotes4.0 is 84.67 when \u000b is set to 0.6 while for QuoRef , the highest F1 is 68.44 when \u000b is set to 0.4 .", "entities": [[2, 3, "MetricName", "F1"], [16, 17, "DatasetName", "QuoRef"], [20, 21, "MetricName", "F1"]]}, {"text": "In addition , we can observe that the performance varies a lot as \u000b changes in distinct datasets , which shows that the hyperparameters \u000b ; \f  acturally play an important role in TI . 6 Conclusion In this paper , we propose the dice - based loss to narrow down the gap between training objective and evaluation metrics ( F1 score ) .", "entities": [[47, 48, "MetricName", "loss"], [60, 62, "MetricName", "F1 score"]]}, {"text": "Experimental results show that the proposed loss function help", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "473 \u000b  Chinese Onto4.0 English QuoRef \u000b = 0:1 80.13 63.23 \u000b = 0:2 81.17 63.45 \u000b = 0:3 84.22 65.88 \u000b = 0:4 84.52 68.44 \u000b = 0:5 84.47 67.52 \u000b = 0:6 84.67 66.35 \u000b = 0:7 81.81 65.09 \u000b = 0:8 80.97 64.13 \u000b = 0:9 80.21 64.84 Table 10 : The effect of hyperparameters in Tversky Index .", "entities": [[5, 6, "DatasetName", "QuoRef"]]}, {"text": "We set \f = 1\u0000 \u000b and thus we only list \u000b here .", "entities": []}, {"text": "to achieve signi\ufb01cant performance boost without changing model architectures .", "entities": []}, {"text": "Acknowledgement We thank all anonymous reviewers , as well as Qinghong Han , Wei Wu and Jiawei Wu for their comments and suggestions .", "entities": []}, {"text": "The work is supported by the National Natural Science Foundation of China ( NSFC No . 61625107 and 61751209 ) .", "entities": []}, {"text": "References Bernd Bohnet , Ryan T. McDonald , Gonc \u00b8alo Sim \u02dcoes , Daniel Andor , Emily Pitler , and Joshua Maynez .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Morphosyntactic tagging with a meta - bilstm model over context sensitive token encodings .", "entities": [[6, 7, "MethodName", "bilstm"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , ACL 2018 , Melbourne , Australia , July 15 - 20 , 2018 , Volume 1 : Long Papers , pages 2642\u20132652 .", "entities": []}, {"text": "Haw - Shiuan Chang , Erik G. Learned - Miller , and Andrew McCallum .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Active bias : Training more accurate neural networks by emphasizing high variance samples .", "entities": []}, {"text": "In NIPS .", "entities": []}, {"text": "N. V .", "entities": []}, {"text": "Chawla , K. W. Bowyer , Lawrence O. Hall , and W. P. Kegelmeyer .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Smote : Synthetic minority over - sampling technique .", "entities": [[0, 1, "MethodName", "Smote"], [2, 9, "MethodName", "Synthetic minority over - sampling technique ."]]}, {"text": "J. Artif .", "entities": []}, {"text": "Intell .", "entities": []}, {"text": "Res . , 16:321 \u2013 357 .", "entities": []}, {"text": "Danqi Chen , Adam Fisch , Jason Weston , and Antoine Bordes . 2017 .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "Reading wikipedia to answer opendomain questions .", "entities": []}, {"text": "arXiv preprint arXiv:1704.00051 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kean Chen , Jianguo Li , Weiyao Lin , John See , Ji Wang , Lingyu Duan , Zhibo Chen , Changwei He , and Junni Zou .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Towards accurate one - stage object detection with ap - loss .", "entities": [[5, 7, "TaskName", "object detection"], [10, 11, "MetricName", "loss"]]}, {"text": "In IEEE Conference on Computer Vision and Pattern Recognition , CVPR 2019 , Long Beach , CA , USA , June 16 - 20 , 2019 , pages 5119\u20135127 .", "entities": []}, {"text": "Shijuan Chen , Haibo He , and Edwardo A. Garcia .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Ramoboost : Ranked minority oversampling in boosting .", "entities": []}, {"text": "IEEE Transactions on Neural Networks , 21:1624 \u2013 1642 .", "entities": []}, {"text": "Kevin Clark , Minh - Thang Luong , Christopher D. Manning , and Quoc V .", "entities": []}, {"text": "Le .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Semi - supervised sequencemodeling with cross - view training .", "entities": [[5, 9, "MethodName", "cross - view training"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Procfessing , Brussels , Belgium , October 31 November 4 , 2018 , pages 1914\u20131925 .", "entities": []}, {"text": "Pradeep Dasigi , Nelson F Liu , Ana Marasovic , Noah A Smith , and Matt Gardner .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Quoref : A reading comprehension dataset with questions requiring coreferential reasoning .", "entities": [[0, 1, "DatasetName", "Quoref"], [3, 5, "TaskName", "reading comprehension"]]}, {"text": "arXiv preprint arXiv:1908.05803 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .", "entities": []}, {"text": "Bert : Pre - training of deep bidirectional transformers for language understanding .", "entities": []}, {"text": "arXiv preprint arXiv:1810.04805 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Lee R Dice . 1945 .", "entities": [[2, 3, "MetricName", "Dice"]]}, {"text": "Measures of the amount of ecologic association between species .", "entities": []}, {"text": "Ecology , 26(3):297\u2013302 .", "entities": []}, {"text": "William B. Dolan and Chris Brockett .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Automatically constructing a corpus of sentential paraphrases .", "entities": []}, {"text": "InProceedings of the Third International Workshop on Paraphrasing ( IWP2005 ) .", "entities": []}, {"text": "Yang Fan , Fei Tian , Tao Qin , Xiuping Li , and Tie - Yan Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning to teach .", "entities": []}, {"text": "ArXiv , abs/1805.03643 .", "entities": [[0, 1, "DatasetName", "ArXiv"]]}, {"text": "Ross B. Girshick .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Fast r - cnn .", "entities": [[0, 4, "MethodName", "Fast r - cnn"]]}, {"text": "2015 IEEE International Conference on Computer Vision ( ICCV ) , pages 1440\u20131448 .", "entities": []}, {"text": "Ross B. Girshick , Jeff Donahue , Trevor Darrell , and Jitendra Malik .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Rich feature hierarchies for accurate object detection and semantic segmentation .", "entities": [[5, 7, "TaskName", "object detection"], [8, 10, "TaskName", "semantic segmentation"]]}, {"text": "2014 IEEE Conference on Computer Vision and Pattern Recognition , pages 580\u2013587 .", "entities": []}, {"text": "Fr\u00b4ederic Godin .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Improving and Interpreting Neural Networks for Word - Level Prediction Tasks in Natural Language Processing .", "entities": []}, {"text": "Ph.D. thesis , Ghent University , Belgium .", "entities": []}, {"text": "Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . 2015 .", "entities": []}, {"text": "Deep residual learning for image recognition .", "entities": [[4, 6, "TaskName", "image recognition"]]}, {"text": "2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 770\u2013778 .", "entities": []}, {"text": "Lu Jiang , Zhengyuan Zhou , Thomas Leung , Li - Jia Li , and Li Fei - Fei . 2017 .", "entities": []}, {"text": "Mentornet :", "entities": []}, {"text": "Learning data - driven curriculum for very deep neural networks on corrupted labels .", "entities": []}, {"text": "In ICML .", "entities": []}, {"text": "H. Kahn and A. W. Marshall .", "entities": []}, {"text": "1953 .", "entities": []}, {"text": "Methods of reducing sample size in monte carlo computations .", "entities": []}, {"text": "Operations Research , 1(5):263\u2013278 .", "entities": []}, {"text": "Anil Kanduri , Mohammad Hashem Haghbayan , Amir M. Rahmani , Muhammad Sha\ufb01que , Axel Jantsch , and Pasi Liljeberg .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "adboost :", "entities": []}, {"text": "Thermal aware performance boosting through dark silicon patterning .", "entities": []}, {"text": "IEEE Trans .", "entities": []}, {"text": "Computers , 67(8):1062\u20131077 .", "entities": []}, {"text": "Angelos Katharopoulos and Franc \u00b8ois Fleuret .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Not all samples are created equal : Deep learning with importance sampling .", "entities": []}, {"text": "In ICML . Tom\u00b4a\u02c7s", "entities": []}, {"text": "Ko \u02c7cisk`y , Jonathan Schwarz , Phil Blunsom , Chris Dyer , Karl Moritz Hermann , G \u00b4 aabor Melis , and Edward Grefenstette .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The narrativeqa reading", "entities": [[1, 2, "DatasetName", "narrativeqa"]]}, {"text": "474comprehension challenge .", "entities": []}, {"text": "Transactions of the Association of Computational Linguistics , 6:317\u2013328 .", "entities": []}, {"text": "Oldrich Kodym , Michal Spanel , and Adam Herout .", "entities": [[7, 8, "MethodName", "Adam"]]}, {"text": "2018 .", "entities": []}, {"text": "Segmentation of head and neck organs at risk using CNN with batch dice loss .", "entities": [[12, 14, "MethodName", "dice loss"]]}, {"text": "In Pattern Recognition 40th German Conference , GCPR 2018 , Stuttgart , Germany , October 9 - 12 , 2018 , Proceedings , pages 105 \u2013 114 .", "entities": []}, {"text": "M. Pawan Kumar , Benjamin Packer , and Daphne Koller .", "entities": [[2, 3, "DatasetName", "Kumar"]]}, {"text": "2010 .", "entities": []}, {"text": "Self - paced learning for latent variable models .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 23 : 24th Annual Conference on Neural Information Processing Systems 2010 .", "entities": []}, {"text": "Proceedings of a meeting held 6 - 9 December 2010 , Vancouver , British Columbia , Canada . , pages 1189\u20131197 .", "entities": []}, {"text": "Guillaume Lample , Miguel Ballesteros , Sandeep Subramanian , Kazuya Kawakami , and Chris Dyer . 2016 .", "entities": []}, {"text": "Neural architectures for named entity recognition .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}, {"text": "arXiv preprint arXiv:1603.01360 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Gina - Anne Levow .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "The third international Chinese language processing bakeoff :", "entities": []}, {"text": "Word segmentation and named entity recognition .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing , pages 108\u2013117 , Sydney , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "H. Li , Z. Lin , X. Shen , J. Brandt , and G. Hua . 2015 .", "entities": []}, {"text": "A convolutional neural network cascade for face detection .", "entities": [[6, 8, "TaskName", "face detection"]]}, {"text": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 5325\u20135334 .", "entities": []}, {"text": "Xiaoya Li , Jingrong Feng , Yuxian Meng , Qinghong Han , Fei Wu , and Jiwei Li . 2019 .", "entities": []}, {"text": "A uni\ufb01ed MRC framework for named entity recognition .", "entities": [[5, 8, "TaskName", "named entity recognition"]]}, {"text": "CoRR , abs/1910.11476 .", "entities": []}, {"text": "Tsung - Yi Lin , Priya Goyal , Ross Girshick , Kaiming He , and Piotr Doll \u00b4 ar . 2017 .", "entities": []}, {"text": "Focal loss for dense object detection .", "entities": [[0, 2, "MethodName", "Focal loss"], [3, 6, "TaskName", "dense object detection"]]}, {"text": "In Proceedings of the IEEE international conference on computer vision , pages 2980\u20132988 .", "entities": []}, {"text": "Xuezhe Ma and Eduard Hovy . 2016 .", "entities": []}, {"text": "End - to - end sequence labeling via bi - directional lstm - cnns - crf .", "entities": [[11, 12, "MethodName", "lstm"], [15, 16, "MethodName", "crf"]]}, {"text": "arXiv preprint arXiv:1603.01354 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tomasz Malisiewicz , Abhinav Gupta , and Alexei A. Efros . 2011 .", "entities": []}, {"text": "Ensemble of exemplar - svms for object detection and beyond .", "entities": [[6, 8, "TaskName", "object detection"]]}, {"text": "In IEEE International Conference on Computer Vision , ICCV 2011 , Barcelona , Spain , November 6 - 13 , 2011 , pages 89\u201396 .", "entities": []}, {"text": "Bryan McCann , Nitish Shirish Keskar , Caiming Xiong , and Richard Socher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The natural language decathlon : Multitask learning as question answering .", "entities": [[8, 10, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1806.08730 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yuxian Meng , Muyu Li , Wei Wu , and Jiwei Li .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Dsreg : Using distant supervision as a regularizer .", "entities": []}, {"text": "arXiv preprint arXiv:1905.11658 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Fausto Milletari , Nassir Navab , and Seyed - Ahmad Ahmadi .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "V - net : Fully convolutional neural networks for volumetric medical image segmentation .", "entities": [[9, 13, "TaskName", "volumetric medical image segmentation"]]}, {"text": "In2016 Fourth International Conference on 3D Vision ( 3DV ) , pages 565\u2013571 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "David Nadeau and Satoshi Sekine . 2007 .", "entities": []}, {"text": "A survey of named entity recognition and classi\ufb01cation .", "entities": [[3, 6, "TaskName", "named entity recognition"]]}, {"text": "Lingvisticae Investigationes , 30(1):3\u201326 .", "entities": []}, {"text": "Tri Nguyen , Mir Rosenberg , Xia Song , Jianfeng Gao , Saurabh Tiwary , Rangan Majumder , and Li Deng . 2016 .", "entities": []}, {"text": "Ms marco : A human generated machine reading comprehension dataset .", "entities": [[0, 2, "DatasetName", "Ms marco"], [6, 9, "TaskName", "machine reading comprehension"]]}, {"text": "arXiv preprint arXiv:1611.09268 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jiangmiao Pang , Kai Chen , Jianping Shi , Huajun Feng , Wanli Ouyang , and Dahua Lin . 2019 .", "entities": []}, {"text": "Libra R - CNN : towards balanced learning for object detection .", "entities": [[0, 4, "MethodName", "Libra R - CNN"], [9, 11, "TaskName", "object detection"]]}, {"text": "In IEEE Conference on Computer Vision and Pattern Recognition , CVPR 2019 , Long Beach , CA , USA , June 16 - 20 , 2019 , pages 821\u2013830 .", "entities": []}, {"text": "Matthew E Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "arXiv preprint arXiv:1802.05365 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sameer Pradhan , Mitchell P. Marcus , Martha Palmer , Lance A. Ramshaw , Ralph M. Weischedel , and Nianwen Xue , editors .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning : Shared Task .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Sameer Pradhan , Alessandro Moschitti , Nianwen Xue , Hwee Tou Ng , Anders Bj \u00a8orkelund , Olga Uryupina , Yuchen Zhang , and Zhi Zhong .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Towards robust linguistic analysis using OntoNotes .", "entities": [[5, 6, "DatasetName", "OntoNotes"]]}, {"text": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning , pages 143\u2013152 , So\ufb01a , Bulgaria . Association for Computational Linguistics .", "entities": []}, {"text": "Pranav Rajpurkar , Robin Jia , and Percy Liang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Know what you do n\u2019t know : Unanswerable questions for squad .", "entities": []}, {"text": "arXiv preprint arXiv:1806.03822 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Squad : 100,000 + questions for machine comprehension of text .", "entities": []}, {"text": "arXiv preprint arXiv:1606.05250 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Shaoqing Ren , Kaiming He , Ross B. Girshick , and Jian Sun . 2015 .", "entities": []}, {"text": "Faster r - cnn : Towards real - time object detection with region proposal networks .", "entities": [[0, 4, "MethodName", "Faster r - cnn"], [6, 11, "TaskName", "real - time object detection"], [12, 14, "TaskName", "region proposal"]]}, {"text": "IEEE Transactions on Pattern Analysis and Machine Intelligence , 39:1137\u20131149 .", "entities": []}, {"text": "Alan Ritter , Sam Clark , Mausam , and Oren Etzioni . 2011 .", "entities": []}, {"text": "Named entity recognition in tweets :", "entities": [[0, 3, "TaskName", "Named entity recognition"]]}, {"text": "An experimental study .", "entities": []}, {"text": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , pages 1524\u20131534 , Edinburgh , Scotland , UK .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Erik F Sang and Fien De Meulder .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Introduction to the conll-2003 shared task : Language - independent named entity recognition .", "entities": [[3, 4, "DatasetName", "conll-2003"], [10, 13, "TaskName", "named entity recognition"]]}, {"text": "arXiv preprint cs/0306050 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Erik F. Tjong Kim Sang and Fien De Meulder .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Introduction to the conll-2003 shared task : Languageindependent named entity recognition .", "entities": [[3, 4, "DatasetName", "conll-2003"], [8, 11, "TaskName", "named entity recognition"]]}, {"text": "In Proceed-", "entities": []}, {"text": "475ings of the Seventh Conference on Natural Language Learning , CoNLL 2003 , Held in cooperation with HLTNAACL 2003 , Edmonton , Canada , May 31 - June 1 , 2003 , pages 142\u2013147 .", "entities": [[10, 12, "DatasetName", "CoNLL 2003"]]}, {"text": "Minjoon Seo , Aniruddha Kembhavi , Ali Farhadi , and Hannaneh Hajishirzi . 2016 .", "entities": []}, {"text": "Bidirectional attention \ufb02ow for machine comprehension .", "entities": []}, {"text": "arXiv preprint arXiv:1611.01603 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Reuben R. Shamir , Yuval Duchin , Jinyoung Kim , Guillermo Sapiro , and Noam Harel .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Continuous dice coef\ufb01cient : a method for evaluating probabilistic segmentations .", "entities": []}, {"text": "CoRR , abs/1906.11031 .", "entities": []}, {"text": "Yan Shao , Christian Hardmeier , J \u00a8org Tiedemann , and Joakim Nivre .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Character - based joint segmentation and pos tagging for chinese using bidirectional rnncrf.arXiv preprint arXiv:1704.01314 .", "entities": []}, {"text": "Chen Shen , Holger R. Roth , Hirohisa Oda , Masahiro Oda , Yuichiro Hayashi , Kazunari Misawa , and Kensaku Mori . 2018 .", "entities": []}, {"text": "On the in\ufb02uence of dice loss function in multi - class organ segmentation of abdominal CT using 3d fully convolutional networks .", "entities": [[4, 6, "MethodName", "dice loss"]]}, {"text": "CoRR , abs/1801.05912 .", "entities": []}, {"text": "Yelong Shen , Po - Sen Huang , Jianfeng Gao , and Weizhu Chen . 2017 .", "entities": []}, {"text": "Reasonet : Learning to stop reading in machine comprehension .", "entities": []}, {"text": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 1047 \u2013 1055 .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Th A Sorensen .", "entities": []}, {"text": "1948 .", "entities": []}, {"text": "A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on danish commons .", "entities": []}, {"text": "Biol .", "entities": []}, {"text": "Skar . , 5:1\u201334 .", "entities": []}, {"text": "Carole H. Sudre , Wenqi Li , Tom Vercauteren , S\u00b4ebastien Ourselin , and M. Jorge Cardoso . 2017 .", "entities": []}, {"text": "Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations .", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support - Third International Workshop , DLMIA 2017 , and 7th International Workshop , ML - CDS 2017 , Held in Conjunction with MICCAI 2017 , Qu \u00b4 ebec City , QC , Canada , September 14 , 2017 , Proceedings , pages 240\u2013248 .", "entities": []}, {"text": "Amos Tversky .", "entities": []}, {"text": "1977 .", "entities": []}, {"text": "Features of similarity .", "entities": []}, {"text": "Psychological review , 84(4):327 .", "entities": []}, {"text": "Sergi Valverde , Mariano Cabezas , Eloy Roura , Sandra Gonz \u00b4 alez - Vill ` a , Deborah Pareto , Joan C Vilanova , Llu\u00b4\u0131s Rami \u00b4 o - Torrent ` a,`Alex Rovira , Arnau Oliver , and Xavier Llad \u00b4 o. 2017 .", "entities": []}, {"text": "Improving automated multiple sclerosis lesion segmentation with a cascaded 3d convolutional neural network approach .", "entities": [[4, 6, "TaskName", "lesion segmentation"]]}, {"text": "NeuroImage , 155:159\u2013168 .", "entities": []}, {"text": "Shuohang Wang and Jing Jiang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Machine comprehension using match - lstm and answer pointer .", "entities": [[5, 6, "MethodName", "lstm"]]}, {"text": "arXiv preprint arXiv:1608.07905 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhiguo Wang , Haitao Mi , Wael Hamza , and Radu Florian . 2016 .", "entities": []}, {"text": "Multi - perspective context match - ing for machine comprehension .", "entities": []}, {"text": "arXiv preprint arXiv:1612.04211 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Wei Wu , Yuxian Meng , Qinghong Han , Muyu Li , Xiaoya Li , Jie Mei , Ping Nie , Xiaofei Sun , and Jiwei Li .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Glyce : Glyph - vectors for chinese character representations .", "entities": []}, {"text": "arXiv preprint arXiv:1901.10125 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Naiwen Xue , Fei Xia , Fudong Choiu , and Marta Palmer .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "The penn chinese treebank : Phrase structure annotation of a large corpus .", "entities": [[2, 4, "DatasetName", "chinese treebank"]]}, {"text": "Natural Language Engineering , 11(2):207\u2013238 .", "entities": []}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime G. Carbonell , Ruslan Salakhutdinov , and Quoc V .", "entities": [[13, 14, "DatasetName", "Ruslan"]]}, {"text": "Le . 2019 .", "entities": []}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "CoRR , abs/1906.08237 .", "entities": []}, {"text": "Adams Wei Yu , David Dohan , Minh - Thang Luong , Rui Zhao , Kai Chen , Mohammad Norouzi , and Quoc V Le .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Qanet :", "entities": []}, {"text": "Combining local convolution with global self - attention for reading comprehension .", "entities": [[2, 3, "MethodName", "convolution"], [9, 11, "TaskName", "reading comprehension"]]}, {"text": "arXiv preprint arXiv:1804.09541 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Adams Wei Yu , David Dohan , Minh - Thang Luong , Rui Zhao , Kai Chen , Mohammad Norouzi , and Quoc V .", "entities": []}, {"text": "Le . 2018b .", "entities": []}, {"text": "Qanet :", "entities": []}, {"text": "Combining local convolution with global self - attention for reading comprehension .", "entities": [[2, 3, "MethodName", "convolution"], [9, 11, "TaskName", "reading comprehension"]]}, {"text": "In 6th International Conference on Learning Representations , ICLR 2018 , Vancouver , BC , Canada , April 30 - May 3 , 2018 , Conference Track Proceedings .", "entities": []}, {"text": "Yue Zhang and Jie Yang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Chinese ner using lattice lstm .", "entities": [[4, 5, "MethodName", "lstm"]]}, {"text": "arXiv preprint arXiv:1805.02023 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "A Dataset Details A.1 Part - of - Speech Tagging Datasets We conduct experiments on three widely used benchmark , i.e. , Chinese Treebank", "entities": [[4, 10, "TaskName", "Part - of - Speech Tagging"], [22, 24, "DatasetName", "Chinese Treebank"]]}, {"text": "5.02/6.03and", "entities": []}, {"text": "UD1.44 . \u000fCTB5 is a Chinese dataset for tagging and parsing , which contains 507,222 words , 824,983 characters and 18,782 sentences extracted from newswire sources , including 698 articles from Xinhua ( 1994 - 1998 ) , 55 articles from Information Services Department of HKSAR ( 1997 ) and 132 articles from Sinorama Magazine ( 1996 - 1998 & 2000 - 2001 ) .", "entities": []}, {"text": "\u000fCTB6 is an extension of CTB5 , containing 781,351 words , 1,285,149 characters and 28,295 sentences .", "entities": []}, {"text": "\u000fUDis the abbreviation of Universal Dependencies , which is a framework for consistent 2https://catalog.ldc.upenn.edu/ LDC2005T01 3https://catalog.ldc.upenn.edu/ LDC2007T36 4https://universaldependencies.org/", "entities": [[4, 6, "DatasetName", "Universal Dependencies"]]}, {"text": "476annotation of grammar ( parts of speech , morphological features , and syntactic dependencies ) across different human languages .", "entities": []}, {"text": "In this work , we use UD1.4 for Chinese POS tagging .", "entities": []}, {"text": "A.2 Named Entity Recognition Datasets For the NER task , we consider both Chinese datasets , i.e. , OntoNotes4.05and MSRA6 , and English datasets , i.e. , CoNLL20037and OntoNotes5.08 .", "entities": [[1, 4, "TaskName", "Named Entity Recognition"], [7, 8, "TaskName", "NER"]]}, {"text": "\u000fCoNLL2003 is an English dataset with 4 entity types : Location , Organization , Person and Miscellaneous .", "entities": [[16, 17, "TaskName", "Miscellaneous"]]}, {"text": "We followed data processing protocols in ( Ma and Hovy , 2016 ) .", "entities": []}, {"text": "\u000fEnglish OntoNotes5.0 consists of texts from a wide variety of sources and contains 18 entity types .", "entities": []}, {"text": "We use the standard train / dev / test split of CoNLL2012 shared task .", "entities": []}, {"text": "\u000fChinese MSRA performs as a Chinese benchmark dataset containing 3 entity types .", "entities": []}, {"text": "Data in MSRA is collected from news domain .", "entities": []}, {"text": "Since the development set is not provided in the original MSRA dataset , we randomly split the training set into training and development splits by 9:1 .", "entities": []}, {"text": "We use the of\ufb01cial test set for evaluation .", "entities": []}, {"text": "\u000fChinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain , which has 18 entity types .", "entities": []}, {"text": "In this paper , we take the same data split as Wu et al .", "entities": []}, {"text": "( 2019 ) did .", "entities": []}, {"text": "A.3 Machine Reading Comprephension Datasets For MRC task , we use three datasets : SQuADv1.1 / v2.09and Queref10datasets .", "entities": []}, {"text": "\u000fSQuAD v1.1 and SQuAD v2.0 are the most widely used QA benchmarks .", "entities": [[3, 4, "DatasetName", "SQuAD"]]}, {"text": "SQuAD1.1 is a collection of 100 K crowdsourced question - answer pairs , and SQuAD2.0 extends SQuAD1.1 allowing no short answer exists in the provided passage .", "entities": [[0, 1, "DatasetName", "SQuAD1.1"], [14, 15, "DatasetName", "SQuAD2.0"], [16, 17, "DatasetName", "SQuAD1.1"]]}, {"text": "5https://catalog.ldc.upenn.edu/ LDC2011T03 6http://sighan.cs.uchicago.edu/ bakeoff2006/ 7https://www.clips.uantwerpen.be/", "entities": []}, {"text": "conll2003 / ner/ 8https://catalog.ldc.upenn.edu/ LDC2013T19 9https://rajpurkar.github.io/ SQuAD - explorer/ 10https://allennlp.org/quoref\u000fQuoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems , containing 24 K questions over 4.7 K paragraphs from Wikipedia .", "entities": [[0, 1, "DatasetName", "conll2003"], [6, 7, "DatasetName", "SQuAD"], [21, 23, "TaskName", "reading comprehension"]]}, {"text": "A.4 Paraphrase Identi\ufb01cation Datasets Experiments are conducted on two PI datasets : MRPC11and QQP12 .", "entities": []}, {"text": "\u000fMRPC is a corpus of sentence pairs automatically extracted from online news sources , with human annotations of whether the sentence pairs are semantically equivalent .", "entities": []}, {"text": "The MRPC dataset has imbalanced classes ( 6800 pairs in total , and 68 % for positive , 32 % for negative ) .", "entities": [[1, 2, "DatasetName", "MRPC"]]}, {"text": "\u000fQQP is a collection of question pairs from the community question - answering website Quora .", "entities": []}, {"text": "The class distribution in QQP is also unbalanced ( over 400,000 question pairs in total , and 37 % for positive , 63 % for negative ) .", "entities": [[4, 5, "DatasetName", "QQP"]]}, {"text": "11https://www.microsoft.com/en-us/ download / details.aspx?id=52398 12https://www.quora.com/q/quoradata/", "entities": []}, {"text": "First - Quora - Dataset - Release - Question - Pairs", "entities": []}]