[{"text": "Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 422\u2013430", "entities": []}, {"text": "November 16 - 20 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics422KorNLI and KorSTS : New Benchmark Datasets for Korean Natural Language Understanding Jiyeon Ham\u0003 , Yo Joong Choe\u0003 , Kyubyong Park\u0003 , Ilji Choi , Hyungjoon Soh Kakao Brain fjiyeon.ham , yj.choe , kyubyong.park , ilji.choi , hj.soh g@kakaobrain.com", "entities": [[6, 7, "DatasetName", "KorSTS"], [13, 16, "TaskName", "Natural Language Understanding"]]}, {"text": "Abstract Natural language inference ( NLI ) and semantic textual similarity ( STS ) are key tasks in natural language understanding ( NLU ) .", "entities": [[1, 4, "TaskName", "Natural language inference"], [8, 11, "TaskName", "semantic textual similarity"], [12, 13, "TaskName", "STS"], [18, 21, "TaskName", "natural language understanding"]]}, {"text": "Although several benchmark datasets for those tasks have been released in English and a few other languages , there are no publicly available NLI or STS datasets in the Korean language .", "entities": [[25, 26, "TaskName", "STS"]]}, {"text": "Motivated by this , we construct and release new datasets for Korean NLI and STS , dubbed KorNLI and KorSTS , respectively .", "entities": [[14, 15, "TaskName", "STS"], [17, 18, "DatasetName", "KorNLI"], [19, 20, "DatasetName", "KorSTS"]]}, {"text": "Following previous approaches , we machine - translate existing English training sets and manually translate development and test sets into Korean .", "entities": []}, {"text": "To accelerate research on Korean NLU , we also establish baselines on KorNLI and KorSTS .", "entities": [[12, 13, "DatasetName", "KorNLI"], [14, 15, "DatasetName", "KorSTS"]]}, {"text": "Our datasets are publicly available at https://github.com/ kakaobrain / KorNLUDatasets .", "entities": []}, {"text": "1 Introduction Natural language inference ( NLI ) and semantic textual similarity ( STS ) are considered as two of the central tasks in natural language understanding ( NLU ) .", "entities": [[2, 5, "TaskName", "Natural language inference"], [9, 12, "TaskName", "semantic textual similarity"], [13, 14, "TaskName", "STS"], [24, 27, "TaskName", "natural language understanding"]]}, {"text": "They are not only featured in GLUE ( Wang et al . , 2018 ) and SuperGLUE ( Wang et al . , 2019 ) , which are two popular benchmarks for NLU , but also known to be useful for supplementary training of pre - trained language models ( Phang et al . , 2018 ) as well as for building and evaluating \ufb01xedsize sentence embeddings ( Reimers and Gurevych , 2019 ) .", "entities": [[6, 7, "DatasetName", "GLUE"], [16, 17, "DatasetName", "SuperGLUE"], [65, 67, "TaskName", "sentence embeddings"]]}, {"text": "Accordingly , several benchmark datasets have been released for both NLI ( Bowman et al . , 2015 ; Williams et al . , 2018 ) and STS ( Cer et al . , 2017 ) in the English language .", "entities": [[27, 28, "TaskName", "STS"]]}, {"text": "When it comes to the Korean language , however , benchmark datasets for NLI and STS do not exist .", "entities": [[15, 16, "TaskName", "STS"]]}, {"text": "Popular benchmark datasets for Korean NLU typically involve question answering12and sentiment \u0003Equal Contribution .", "entities": []}, {"text": "1https://korquad.github.io/ ( Lim et al . , 2019 ) 2http://www.aihub.or.kr/aidata/84analysis3 , but not NLI or STS .", "entities": [[15, 16, "TaskName", "STS"]]}, {"text": "We believe that the lack of publicly available benchmark datasets for Korean NLI and STS has led to the lack of interest for building Korean NLU models suited for these key understanding tasks .", "entities": [[14, 15, "TaskName", "STS"]]}, {"text": "Motivated by this , we construct and release KorNLI andKorSTS , two new benchmark datasets for NLI and STS in the Korean language .", "entities": [[8, 9, "DatasetName", "KorNLI"], [18, 19, "TaskName", "STS"]]}, {"text": "Following previous work ( Conneau et al . , 2018 ) , we construct our datasets by machine - translating existing English training sets and by translating English development and test sets via human translators .", "entities": []}, {"text": "We then establish baselines for both KorNLI and KorSTS to facilitate research on Korean NLU .", "entities": [[6, 7, "DatasetName", "KorNLI"], [8, 9, "DatasetName", "KorSTS"]]}, {"text": "2 Background 2.1 NLI and the fS , M , XgNLI Datasets", "entities": []}, {"text": "In an NLI task , a system receives a pair of sentences , a premise and a hypothesis , and classi\ufb01es their relationship into one out of three categories : entailment , contradiction , and neutral .", "entities": []}, {"text": "There are several publicly available NLI datasets in English .", "entities": []}, {"text": "Bowman et al .", "entities": []}, {"text": "( 2015 ) introduced the Stanford NLI ( SNLI ) dataset , which consists of 570 K English sentence pairs based on image captions .", "entities": [[8, 9, "DatasetName", "SNLI"]]}, {"text": "Williams et al .", "entities": []}, {"text": "( 2018 ) introduced the MultiGenre NLI ( MNLI ) dataset , which consists of 455 K English sentence pairs from ten genres .", "entities": [[8, 9, "DatasetName", "MNLI"]]}, {"text": "Conneau et al .", "entities": []}, {"text": "( 2018 ) released the Cross - lingual NLI ( XNLI ) dataset by extending the development and test data of the MNLI corpus to 15 languages .", "entities": [[10, 11, "DatasetName", "XNLI"], [22, 23, "DatasetName", "MNLI"]]}, {"text": "Note that Korean is not one of the 15 languages in XNLI .", "entities": [[11, 12, "DatasetName", "XNLI"]]}, {"text": "There are also publicly available NLI datasets in a few other non - English languages ( Fonseca et al . , 2016 ;", "entities": []}, {"text": "Real et al . , 2019 ; Hayashibe , 2020 ) , but none exists for Korean at the time of publication .", "entities": []}, {"text": "3https://github.com/e9t/nsmc", "entities": []}, {"text": "423 Figure 1 : Data construction process .", "entities": []}, {"text": "MT and PE indicate machine translation and post - editing , respectively .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "We translate original English data into Korean using an internal translation engine .", "entities": []}, {"text": "For development and test data , the machine translation outputs are further post - edited by human experts .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "2.2 STS and the STS - B Dataset STS is a task that assesses the gradations of semantic similarity between two sentences .", "entities": [[1, 2, "TaskName", "STS"], [4, 7, "DatasetName", "STS - B"], [8, 9, "TaskName", "STS"], [17, 19, "TaskName", "semantic similarity"]]}, {"text": "The similarity score ranges from 0 ( completely dissimilar ) to 5 ( completely equivalent ) .", "entities": [[5, 6, "DatasetName", "0"]]}, {"text": "It is commonly used to evaluate either how well a model grasps the closeness of two sentences in meaning , or how well a sentence embedding embodies the semantic representation of the sentence .", "entities": [[24, 26, "TaskName", "sentence embedding"]]}, {"text": "The STS - B dataset consists of 8,628 English sentence pairs selected from the STS tasks organized in the context of SemEval between 2012 and 2017 ( Agirre et al . , 2012 , 2013 , 2014 , 2015 , 2016 ) .", "entities": [[1, 4, "DatasetName", "STS - B"], [14, 15, "TaskName", "STS"]]}, {"text": "The domain of input sentences covers image captions , news headlines , and user forums .", "entities": []}, {"text": "For details , we refer readers to Cer et al .", "entities": []}, {"text": "( 2017 ) .", "entities": []}, {"text": "3 Data 3.1 Data Construction We explain how we develop two new Korean language understanding datasets : KorNLI and KorSTS .", "entities": [[17, 18, "DatasetName", "KorNLI"], [19, 20, "DatasetName", "KorSTS"]]}, {"text": "The KorNLI dataset is derived from three different sources : SNLI , MNLI , and XNLI , while the KorSTS dataset stems from the STS - B dataset .", "entities": [[1, 2, "DatasetName", "KorNLI"], [10, 11, "DatasetName", "SNLI"], [12, 13, "DatasetName", "MNLI"], [15, 16, "DatasetName", "XNLI"], [19, 20, "DatasetName", "KorSTS"], [24, 27, "DatasetName", "STS - B"]]}, {"text": "The overall construction process , which is applied identically to the two new datasets , is illustrated in Figure 1 .", "entities": []}, {"text": "First , we translate the training sets of the SNLI , MNLI , and STS - B datasets , as well as the development and test sets of the XNLI4and STS - B datasets , into Korean using an internal neural machine translation engine .", "entities": [[9, 10, "DatasetName", "SNLI"], [11, 12, "DatasetName", "MNLI"], [14, 17, "DatasetName", "STS - B"], [30, 33, "DatasetName", "STS - B"], [41, 43, "TaskName", "machine translation"]]}, {"text": "Then , the translation results of the development and test sets are post - edited by professional translators in order to guarantee the quality of evaluation .", "entities": []}, {"text": "This multi - stage translation strategy 4Only English examples count.aims not only to expedite the translators \u2019 work , but also to help maintain the translation consistency between the training and evaluation datasets .", "entities": []}, {"text": "It is worth noting that the post - editing procedure does not simply mean proofreading .", "entities": []}, {"text": "Rather , it refers to human translation based on the prior machine translation results , which serve as \ufb01rst drafts .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "3.1.1 Translation Quality To ensure translation quality , we hired two professional translators with at least seven years of experience who specialize in academic papers / books as well as business contracts .", "entities": [[1, 2, "TaskName", "Translation"]]}, {"text": "The two translators each post - edited half of the dataset and cross - checked each other \u2019s translation afterward .", "entities": []}, {"text": "This was further examined by one of the authors , who is \ufb02uent in both English and Korean .", "entities": []}, {"text": "We also note that the professional translators did not have to edit much during post - editing , suggesting that the machine - translated sentences were often good enough to begin with .", "entities": []}, {"text": "We found that the BLEU scores between the machine - translated and post - edited sentences were 63.30 for KorNLI and 73.26 for KorSTS , and for approximately half the time ( 47 % for KorNLI and 53 % for KorSTS ) , the translators did not have to change the machinetranslated sentence at all .", "entities": [[4, 5, "MetricName", "BLEU"], [19, 20, "DatasetName", "KorNLI"], [23, 24, "DatasetName", "KorSTS"], [35, 36, "DatasetName", "KorNLI"], [40, 41, "DatasetName", "KorSTS"]]}, {"text": "Finally , we note that translators did not see the English gold labels during post - editing , in order to expedite the post - editing process .", "entities": []}, {"text": "See Section 5 for a discussion on the effect of translation on data quality .", "entities": []}, {"text": "3.2 KorNLI Table 1 shows the statistics of the KorNLI dataset .", "entities": [[1, 2, "DatasetName", "KorNLI"], [9, 10, "DatasetName", "KorNLI"]]}, {"text": "There are 942,854 training examples translated automatically and 7,500 evaluation ( development and test ) examples translated manually .", "entities": []}, {"text": "The premises", "entities": []}, {"text": "424KorNLI Total Train Dev .", "entities": []}, {"text": "Test Source - SNLI , MNLI XNLI", "entities": [[3, 4, "DatasetName", "SNLI"], [5, 6, "DatasetName", "MNLI"], [6, 7, "DatasetName", "XNLI"]]}, {"text": "XNLI Translated by - Machine Human Human # Examples 950,354", "entities": [[0, 1, "DatasetName", "XNLI"]]}, {"text": "942,854 2,490 5,010 # Words ( P ) 13.6 13.6 13.0 13.1 # Words ( H ) 7.1 7.2 6.8 6.8 Table 1 : Statistics of KorNLI dataset .", "entities": [[26, 27, "DatasetName", "KorNLI"]]}, {"text": "The last two rows mean the average number of words in a Premise ( P ) and a Hypothesis ( H ) , respectively .", "entities": []}, {"text": "Examples Label P : \u001d  -\u008d\u000fH \u001c  \u0015l \u001b\\ \u0081e\u0094\u0012`\u0007", "entities": []}, {"text": "\u00a6 \u00809 \u0006\u00af \u00b9\\O \u001d # Q. E\u201cYou do n\u2019t have to stay there . \u201d", "entities": []}, {"text": "H:\u0000 \u0013\u0095 \u00b8\u0011", "entities": []}, {"text": "# .", "entities": []}, {"text": "\u201c You can leave . \u201d", "entities": []}, {"text": "P : \u001d  -\u008d\u000fH \u001c  \u0015l \u001b\\ \u0081e\u0094\u0012`\u0007", "entities": []}, {"text": "\u00a6 \u00809 \u0006\u00af \u00b9\\O \u001d # Q. C\u201cYou do n\u2019t have to stay there . \u201d", "entities": []}, {"text": "H :}", "entities": []}, {"text": "\u0004&\u00f1 SX\u0089y \u001b\u00d5 \u00aa\b \u0013o \u001b\\ \u0081e\u0094\u0012 # Q\u0007 \u0014K \u0017 !", "entities": []}, {"text": "\u201c You need to stay in this place exactly ! \u201d", "entities": []}, {"text": "P : \u001d  -\u008d\u000fH \u001c  \u0015l \u001b\\ \u0081e\u0094\u0012`\u0007", "entities": []}, {"text": "\u00a6 \u00809 \u0006\u00af \u00b9\\O \u001d #", "entities": []}, {"text": "Q.", "entities": []}, {"text": "N\u201cYou do n\u2019t have to stay there . \u201d", "entities": []}, {"text": "H : W 1\u0000 \u0013 \" \u00e9\u00b6", "entities": []}, {"text": "\u0013\u0080\u0016\u0004 }", "entities": []}, {"text": "\u0004|9 \b\\ \u0081\u0000 \u0013\u0095 \u00b8\u0011", "entities": []}, {"text": "# .", "entities": []}, {"text": "\u201c You can go home if you like . \u201d", "entities": []}, {"text": "Table 2 : Examples from KorNLI dataset .", "entities": [[5, 6, "DatasetName", "KorNLI"]]}, {"text": "P : Premise , H : Hypothesis .", "entities": []}, {"text": "E : Entailment , C : Contradiction , N : Neutral . are almost twice as long as the hypotheses , as reported in Conneau et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "We present a few examples in Table 2 . 3.3 KorSTS As provided in Table 3 , the KorSTS dataset comprises 5,749 training examples translated automatically and 2,879 evaluation examples translated manually .", "entities": [[10, 11, "DatasetName", "KorSTS"], [18, 19, "DatasetName", "KorSTS"]]}, {"text": "Examples are shown in Table 4 . 4 Baselines In this section , we provide baselines for the Korean NLI and STS tasks using our newly created benchmark datasets .", "entities": [[21, 22, "TaskName", "STS"]]}, {"text": "Because both tasks receive a pair of sentences as an input , there are two different approaches depending on whether the model encodes the sentences jointly ( \u201c cross - encoding \u201d ) or separately ( \u201c bi - encoding\u201d).5 4.1 Cross - encoding Approaches As illustrated with BERT ( Devlin et al . , 2019 ) and many of its variants , the de facto standard approach for NLU tasks is to pre - train a large language model and \ufb01ne - tune it on each task .", "entities": [[48, 49, "MethodName", "BERT"]]}, {"text": "In the cross - encoding 5These nomenclatures ( cross - encoding and bi - encoding ) are adopted from Humeau et al .", "entities": []}, {"text": "( 2020).KorSTS Total Train Dev .", "entities": []}, {"text": "Test Source - STS - B STS - B STS - B Translated by - Machine Human Human # Examples 8,628 5,749 1,500 1,379 Avg . #", "entities": [[3, 6, "DatasetName", "STS - B"], [6, 9, "DatasetName", "STS - B"], [9, 12, "DatasetName", "STS - B"]]}, {"text": "Words 7.7 7.5 8.7 7.6 Table 3 : Statistics of KorSTS dataset .", "entities": [[10, 11, "DatasetName", "KorSTS"]]}, {"text": "Examples Score A : \u00f4\u0014\u00c7z \u008c \u0099\b \u0013\u0000 \u00136 \u00a3 \u00a7 d\u0094\u0003`\u0007 \u00a6 \" 3 \u0003\u0093 \u00a6 e\u0094\u0012\u0002 \u0013. 4.2\u201cA man is eating food . \u201d", "entities": [[1, 2, "MetricName", "Score"]]}, {"text": "B : \u00f4\u0014\u00c7z \u008c \u0099\b \u0013\u0000 \u0013 \u001f  \u00e3\u00b6\u0000 \u0013\\\u0007 \u00a6 \" 3 \u0003\u0093 \u00a6 e\u0094\u0012\u0002 \u0013. \u201c A man is eating something . \u201d", "entities": []}, {"text": "A : \u00f4\u0014\u00c7 # \u008c$\u00ed s \u001b\u0093 \u00a6 l \u001b\\\u0007 \u00a6 \u00af \u00b9o \u001b", "entities": []}, {"text": "\u0013\u0093 \u00a6 e\u0094\u0012\u0002 \u0013. 0.0\u201cA woman is cooking meat . \u201d", "entities": []}, {"text": "B : \u00f4\u0014\u00c7z \u008c \u0099\b \u0013\u0000 \u0013 \u00b4 \u00fa \u0098", "entities": []}, {"text": "\u0013\u0093 \u00a6 e\u0094\u0012\u0002 \u0013. \u201c A man is speaking . \u201d", "entities": []}, {"text": "Table 4 : Examples from KorSTS dataset .", "entities": [[5, 6, "DatasetName", "KorSTS"]]}, {"text": "approach , the pre - trained language model takes each sentence pair as a single input for \ufb01ne - tuning .", "entities": []}, {"text": "These cross - encoding models typically achieve the state - of - the - art performance over bi - encoding models , which encode each input sentence separately .", "entities": []}, {"text": "For both KorNLI and KorSTS , we consider two pre - trained language models .", "entities": [[2, 3, "DatasetName", "KorNLI"], [4, 5, "DatasetName", "KorSTS"]]}, {"text": "We \ufb01rst pre - train a Korean RoBERTa ( Liu et al . , 2019 ) , both base and large versions , on a collection of internally collected Korean corpora ( 65 GB ) .", "entities": [[7, 8, "MethodName", "RoBERTa"]]}, {"text": "We construct a byte pair encoding ( BPE ) ( Gage , 1994 ; Sennrich et al . , 2016 ) dictionary of 32 K tokens using SentencePiece ( Kudo and Richardson , 2018 ) .", "entities": [[3, 6, "MethodName", "byte pair encoding"], [7, 8, "MethodName", "BPE"], [27, 28, "MethodName", "SentencePiece"]]}, {"text": "We train our models using fairseq ( Ott et al . , 2019 ) with 32 V100 GPUs for the base model ( 25 days ) and 64 for the large model ( 20 days ) .", "entities": []}, {"text": "We also use XLM - R ( Conneau and Lample , 2019 ) , a publicly available cross - lingual language model that was pre - trained on 2.5 TB of Common Model # Params.yKorNLI KorSTS", "entities": [[3, 4, "MethodName", "XLM"], [35, 36, "DatasetName", "KorSTS"]]}, {"text": "Fine - tuned on Korean training set Korean RoBERTa ( base ) 111 M 82.75 83.00 Korean RoBERTa ( large ) 338 M 83.67 85.27 XLM - R ( base ) 270 M 80.56 77.78 XLM - R ( large ) 550 M 83.41 84.68 Fine - tuned on English training set ( Cross - lingual Transfer ) XLM - R ( base ) 270 M 75.17 XLM - R ( large ) 550 M 80.30 Table 5 : KorNLI and KorSTS test set scores for \ufb01netuned cross - encoding language models .", "entities": [[8, 9, "MethodName", "RoBERTa"], [17, 18, "MethodName", "RoBERTa"], [25, 26, "MethodName", "XLM"], [35, 36, "MethodName", "XLM"], [53, 57, "TaskName", "Cross - lingual Transfer"], [58, 59, "MethodName", "XLM"], [67, 68, "MethodName", "XLM"], [79, 80, "DatasetName", "KorNLI"], [81, 82, "DatasetName", "KorSTS"]]}, {"text": "KorNLI scores are accuracy ( % ) and KorSTS scores are 100 \u0002Spearman correlation.yTo ensure comparability with XNLI , we only use the MNLI portion of the KorNLI dataset .", "entities": [[0, 1, "DatasetName", "KorNLI"], [3, 7, "MetricName", "accuracy ( % )"], [8, 9, "DatasetName", "KorSTS"], [17, 18, "DatasetName", "XNLI"], [23, 24, "DatasetName", "MNLI"], [27, 28, "DatasetName", "KorNLI"]]}, {"text": "425Model # Params .", "entities": [[2, 3, "MetricName", "Params"]]}, {"text": "KorSTS", "entities": [[0, 1, "DatasetName", "KorSTS"]]}, {"text": "Unsupervised Supervised -Trained on : KorNLITrained on : KorSTSTrained on : KorNLI !", "entities": [[11, 12, "DatasetName", "KorNLI"]]}, {"text": "KorSTS", "entities": [[0, 1, "DatasetName", "KorSTS"]]}, {"text": "Korean fastText - 47.96 - - M - USE CNN(base ) 68.9 M -y72.74 - M - USE CNN(large ) 85.2 M -y76.32 - Korean SRoBERTa ( base )", "entities": [[1, 2, "MethodName", "fastText"], [8, 9, "MethodName", "USE"], [17, 18, "MethodName", "USE"]]}, {"text": "111 M 48.96 74.19 78.94 80.29 Korean SRoBERTa ( large ) 338 M 51.35 75.46 79.55 80.49 SXLM - R ( base ) 270 M 45.05 73.99 68.36 79.13 SXLM - R ( large ) 550 M 39.92 77.01 77.71 81.84 Table 6 : KorSTS test set scores ( 100 \u0002Spearman correlation ) of bi - encoding models .", "entities": [[44, 45, "DatasetName", "KorSTS"]]}, {"text": "Note that the \ufb01rst two columns of results are unsupervised w.r.t .", "entities": []}, {"text": "KorSTS , and the latter two are supervised w.r.t .", "entities": [[0, 1, "DatasetName", "KorSTS"]]}, {"text": "KorSTS.yTrained on machine - translated SNLI only .", "entities": [[5, 6, "DatasetName", "SNLI"]]}, {"text": "Crawl corpora in 100 languages including Korean ( 54 GB ) .", "entities": []}, {"text": "Note that the base and large architectures of XLM - R are identical to those of RoBERTa , except that the vocabulary size is signi\ufb01cantly larger ( 250 K ) , making the embedding and output layers that much larger .", "entities": [[8, 9, "MethodName", "XLM"], [16, 17, "MethodName", "RoBERTa"]]}, {"text": "In Table 5 , we report the test set scores for crossencoding models \ufb01ne - tuned on KorNLI ( accuracy ) and KorSTS ( Spearman correlation ) .", "entities": [[17, 18, "DatasetName", "KorNLI"], [19, 20, "MetricName", "accuracy"], [22, 23, "DatasetName", "KorSTS"], [24, 26, "MetricName", "Spearman correlation"]]}, {"text": "For KorNLI , we additionally include results for XLM - R models \ufb01ne - tuned on the original MNLI training set ( also known as cross - lingual transfer in XNLI ) .", "entities": [[1, 2, "DatasetName", "KorNLI"], [8, 9, "MethodName", "XLM"], [18, 19, "DatasetName", "MNLI"], [25, 29, "TaskName", "cross - lingual transfer"], [30, 31, "DatasetName", "XNLI"]]}, {"text": "To ensure comparability across settings , we only train on the MNLI portion when \ufb01ne - tuning on KorNLI .", "entities": [[11, 12, "DatasetName", "MNLI"], [18, 19, "DatasetName", "KorNLI"]]}, {"text": "Overall , the Korean RoBERTa models outperform the XLM - R models , regardless of whether they are \ufb01ne - tuned on Korean or English training sets .", "entities": [[4, 5, "MethodName", "RoBERTa"], [8, 9, "MethodName", "XLM"]]}, {"text": "For each model , the larger variant outperforms the base one , consistent with previous \ufb01ndings .", "entities": []}, {"text": "The large version of Korean RoBERTa performs the best for both KorNLI ( 83.67 % ) and KorSTS ( 85.27 % ) among all models tested .", "entities": [[5, 6, "MethodName", "RoBERTa"], [11, 12, "DatasetName", "KorNLI"], [17, 18, "DatasetName", "KorSTS"]]}, {"text": "Among the XLM - R models for KorNLI , those \ufb01ne - tuned on the Korean training set consistently outperform the cross - lingual transfer variants .", "entities": [[2, 3, "MethodName", "XLM"], [7, 8, "DatasetName", "KorNLI"], [21, 25, "TaskName", "cross - lingual transfer"]]}, {"text": "4.2 Bi - encoding Approaches We also report the KorSTS scores of bi - encoding models .", "entities": [[9, 10, "DatasetName", "KorSTS"]]}, {"text": "The bi - encoding approach bears practical importance in applications such as semantic search , where computing pairwise similarity among a large set of sentences is computationally expensive with cross - encoding .", "entities": []}, {"text": "Here , we \ufb01rst provide two baselines that do not use pre - trained language models : Korean fastText and the multilingual universal sentence encoder ( M - USE ) .", "entities": [[18, 19, "MethodName", "fastText"], [21, 25, "MethodName", "multilingual universal sentence encoder"], [28, 29, "MethodName", "USE"]]}, {"text": "Korean fastText ( Bojanowski et al . , 2017 ) is a pre - trained word embedding model6trained on Korean text from Common Crawl .", "entities": [[1, 2, "MethodName", "fastText"], [22, 24, "DatasetName", "Common Crawl"]]}, {"text": "To produce sentence embeddings , we take the average of fastText word embeddings for each sentence .", "entities": [[2, 4, "TaskName", "sentence embeddings"], [10, 11, "MethodName", "fastText"], [11, 13, "TaskName", "word embeddings"]]}, {"text": "M - USE7(Yang et", "entities": []}, {"text": "al . , 2019 ) , is a CNN - based sentence encoder model trained for NLI , questionanswering , and translation ranking across 16 languages including Korean .", "entities": []}, {"text": "For both Korean fastText and M - USE , we compute the cosine similarity between two input sentence embeddings to make an unsupervised STS prediction .", "entities": [[3, 4, "MethodName", "fastText"], [7, 8, "MethodName", "USE"], [17, 19, "TaskName", "sentence embeddings"], [23, 24, "TaskName", "STS"]]}, {"text": "Pre - trained language models can also be used as bi - encoding models following the approach of SentenceBERT ( Reimers and Gurevych , 2019 ) , which involves \ufb01ne - tuning a BERT - like model with a Siamese network structure on NLI and/or STS .", "entities": [[33, 34, "MethodName", "BERT"], [39, 41, "MethodName", "Siamese network"], [45, 46, "TaskName", "STS"]]}, {"text": "We use the SentenceBERT approach for both Korean RoBERTa ( \u201c Korean SRoBERTa \u201d ) and XLMR ( \u201c SXLM - R \u201d ) .", "entities": [[8, 9, "MethodName", "RoBERTa"]]}, {"text": "We adopt the MEAN pooling strategy , i.e. , computing the sentence vector as the mean of all contextualized word vectors .", "entities": []}, {"text": "In Table 6 , we present the KorSTS test set scores ( 100\u0002Spearman correlation ) for the biencoding models .", "entities": [[7, 8, "DatasetName", "KorSTS"]]}, {"text": "We categorize each result based on whether the model was additionally trained on KorNLI and/or KorSTS .", "entities": [[13, 14, "DatasetName", "KorNLI"], [15, 16, "DatasetName", "KorSTS"]]}, {"text": "Note that models that are not \ufb01ne - tuned at all or only \ufb01ne - tuned to KorNLI can be considered as unsupervised w.r.t .", "entities": [[17, 18, "DatasetName", "KorNLI"]]}, {"text": "KorSTS .", "entities": [[0, 1, "DatasetName", "KorSTS"]]}, {"text": "Also note that M - USE is trained on a machinetranslated version of SNLI , which is a subset of KorNLI , as part of its pre - training step .", "entities": [[5, 6, "MethodName", "USE"], [13, 14, "DatasetName", "SNLI"], [20, 21, "DatasetName", "KorNLI"]]}, {"text": "6https://dl.fbaipublicfiles.com/ fasttext / vectors - crawl / cc.ko.300.bin.gz 7https://tfhub.dev/google/ universal - sentence - encoder - multilingual/ 3", "entities": [[1, 2, "MethodName", "fasttext"]]}, {"text": "426First , given each model , we \ufb01nd that supplementary training on KorNLI consistently improves the KorSTS scores for both unsupervised and supervised settings , as was the case with English models ( Conneau et al . , 2017 ; Reimers and Gurevych , 2019 ) .", "entities": [[12, 13, "DatasetName", "KorNLI"], [16, 17, "DatasetName", "KorSTS"]]}, {"text": "This shows that the KorNLI dataset can be an effective intermediate training source for biencoding approaches .", "entities": [[4, 5, "DatasetName", "KorNLI"]]}, {"text": "When comparing the baseline models in each setting , we \ufb01nd that both MUSE and the SentenceBERT - based models trained on KorNLI achieve competitive unsupervised KorSTS scores .", "entities": [[13, 14, "DatasetName", "MUSE"], [22, 23, "DatasetName", "KorNLI"], [26, 27, "DatasetName", "KorSTS"]]}, {"text": "Both models signi\ufb01cantly outperform the average of fastText embeddings model and the Korean SRoBERTa and SXLM - R models without \ufb01ne - tuning .", "entities": [[7, 8, "MethodName", "fastText"]]}, {"text": "Among our baselines , large SXLM - R trained on KorNLI followed by KorSTS achieves the best score ( 81.84 ) .", "entities": [[10, 11, "DatasetName", "KorNLI"], [13, 14, "DatasetName", "KorSTS"]]}, {"text": "5 Effect of Translation on Data Quality As noted in ( Conneau et al . , 2018 ) , translation quality does not necessarily guarantee that the semantic relationships between sentences are preserved .", "entities": [[3, 4, "TaskName", "Translation"]]}, {"text": "We also translated each sentence independently and took the gold labels from the original English pair , so the resulting label might no longer be \u201c gold , \u201d due to both incorrect translations and ( in rarer cases ) linguistic differences that make it dif\ufb01cult to translate speci\ufb01c concepts .", "entities": []}, {"text": "Fortunately , it was also pointed out in ( Conneau et al . , 2018 ) that annotators could recover the NLI labels at a similar accuracy in translated pairs ( 83 % in French ) as in original pairs ( 85 % in English ) .", "entities": [[26, 27, "MetricName", "accuracy"]]}, {"text": "In addition , our baseline experiments in Section 4.1 show that supplementary training on KorNLI improves KorSTS performance ( +1 % for RoBERTa and +4 - 11 % for XLM - R ) , suggesting that the labels of KorNLI are still meaningful .", "entities": [[14, 15, "DatasetName", "KorNLI"], [16, 17, "DatasetName", "KorSTS"], [22, 23, "MethodName", "RoBERTa"], [29, 30, "MethodName", "XLM"], [39, 40, "DatasetName", "KorNLI"]]}, {"text": "Another quantitative evidence is that the performance of XLM - R \ufb01ne - tuned on KorNLI ( 80.3 % with cross - lingual transfer ) is within a comparable range of the model \u2019s performance on other XNLI languages ( 80.1 % on average ) .", "entities": [[8, 9, "MethodName", "XLM"], [15, 16, "DatasetName", "KorNLI"], [20, 24, "TaskName", "cross - lingual transfer"], [37, 38, "DatasetName", "XNLI"]]}, {"text": "Nevertheless , we could also \ufb01nd some ( not many ) examples", "entities": []}, {"text": "the gold label becomes incorrect after translating input sentences to Korean .", "entities": []}, {"text": "For example , there were cases in which the two input sentences for KorSTS were so similar ( with 4 + similarity scores ) that upon translation , the two inputs simply became identical .", "entities": [[13, 14, "DatasetName", "KorSTS"]]}, {"text": "In another case , the English word sirappeared in the premise of an NLI example and was translated to \u0082\u0015\u0004\u00d2qt_\u0094\u0007 , which isa correct word translation but is a gender - neutral noun , because there is no gender - speci\ufb01c counterpart to the word in Korean .", "entities": [[24, 26, "TaskName", "word translation"]]}, {"text": "As a result , when the hypothesis referencing the entity as the man got translated intoz \u008c \u0099\b \u0013(gender - speci\ufb01c ) , the English gold label ( entailment ) was no longer correct in the translated example .", "entities": []}, {"text": "More systematically analyzing these errors is an interesting future work , although the amount of human efforts involved in this analysis would match that of labeling a new dataset .", "entities": []}, {"text": "6 Conclusion We introduced KorNLI and KorSTS \u2014 new datasets for Korean natural language understanding .", "entities": [[4, 5, "DatasetName", "KorNLI"], [6, 7, "DatasetName", "KorSTS"], [12, 15, "TaskName", "natural language understanding"]]}, {"text": "Using these datasets , we also established baselines for Korean NLI and STS with both cross - encoding and bi - encoding approaches .", "entities": [[12, 13, "TaskName", "STS"]]}, {"text": "Looking forward , we hope that our datasets and baselines will facilitate future research on not only improving Korean NLU systems but also increasing language diversity in NLU research .", "entities": []}, {"text": "Acknowledgements We thank Pulip Park for helping with hiring and contacting with the professional translators .", "entities": []}, {"text": "We would also like to acknowledge Kakao Brain Cloud , which we used for our baseline experiments .", "entities": []}, {"text": "References Eneko Agirre , Carmen Banea , Claire Cardie , Daniel Cer , Mona Diab , Aitor Gonzalez - Agirre , Weiwei Guo , Inigo Lopez - Gazpio , Montse Maritxalar , Rada Mihalcea , et al . 2015 .", "entities": []}, {"text": "Semeval-2015 task 2 : Semantic textual similarity , english , spanish and pilot on interpretability .", "entities": [[4, 7, "TaskName", "Semantic textual similarity"]]}, {"text": "In Proceedings of the 9th international workshop on semantic evaluation ( SemEval 2015 ) , pages 252\u2013263 .", "entities": []}, {"text": "Eneko Agirre , Carmen Banea , Claire Cardie , Daniel Cer , Mona Diab , Aitor Gonzalez - Agirre , Weiwei Guo , Rada Mihalcea , German Rigau , and Janyce Wiebe .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Semeval-2014 task 10 : Multilingual semantic textual similarity .", "entities": [[5, 8, "TaskName", "semantic textual similarity"]]}, {"text": "In Proceedings of the 8th international workshop on semantic evaluation ( SemEval 2014 ) , pages 81\u201391 .", "entities": []}, {"text": "Eneko Agirre , Carmen Banea , Daniel Cer , Mona Diab , Aitor Gonzalez Agirre , Rada Mihalcea , German Rigau Claramunt , and Janyce Wiebe .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Semeval-2016 task 1 : Semantic textual similarity , monolingual and cross - lingual evaluation .", "entities": [[4, 7, "TaskName", "Semantic textual similarity"]]}, {"text": "In SemEval-2016 .", "entities": []}, {"text": "10th International Workshop on Semantic Evaluation ; 2016 Jun 16 - 17 ; San Diego , CA .", "entities": []}, {"text": "427Stroudsburg ( PA ): ACL ; 2016 .", "entities": []}, {"text": "p. 497 - 511 .", "entities": []}, {"text": "ACL ( Association for Computational Linguistics ) .", "entities": []}, {"text": "Eneko Agirre , Daniel Cer , Mona Diab , and Aitor Gonzalez - Agirre . 2012 .", "entities": []}, {"text": "Semeval-2012 task 6 : A pilot on semantic textual similarity .", "entities": [[7, 10, "TaskName", "semantic textual similarity"]]}, {"text": "In * SEM 2012 : The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1 : Proceedings of the main conference and the shared task , and Volume 2 : Proceedings of the Sixth International Workshop on Semantic Evaluation ( SemEval 2012 ) , pages 385 \u2013 393 .", "entities": []}, {"text": "Eneko Agirre , Daniel Cer , Mona Diab , Aitor GonzalezAgirre , and Weiwei Guo .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "* sem 2013 shared task : Semantic textual similarity .", "entities": [[6, 9, "TaskName", "Semantic textual similarity"]]}, {"text": "In Second Joint Conference on Lexical and Computational Semantics ( * SEM ) , Volume 1 : Proceedings of the Main Conference and the Shared Task :", "entities": []}, {"text": "Semantic Textual Similarity , pages 32\u201343 .", "entities": [[0, 3, "TaskName", "Semantic Textual Similarity"]]}, {"text": "Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Enriching word vectors with subword information .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 5:135\u2013146 .", "entities": []}, {"text": "Samuel R. Bowman , Gabor Angeli , Christopher Potts , and Christopher D. Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A large annotated corpus for learning natural language inference .", "entities": [[6, 9, "TaskName", "natural language inference"]]}, {"text": "InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 632\u2013642 , Lisbon , Portugal .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Daniel Cer , Mona Diab , Eneko Agirre , I \u02dcnigo LopezGazpio , and Lucia Specia .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "SemEval-2017 task 1 : Semantic textual similarity multilingual and crosslingual focused evaluation .", "entities": [[4, 7, "TaskName", "Semantic textual similarity"]]}, {"text": "In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) , pages 1\u201314 , Vancouver , Canada . Association for Computational Linguistics .", "entities": []}, {"text": "Alexis Conneau , Douwe Kiela , Holger Schwenk , Lo \u00a8\u0131c Barrault , and Antoine Bordes .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Supervised learning of universal sentence representations from natural language inference data .", "entities": [[7, 10, "TaskName", "natural language inference"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 670\u2013680 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alexis Conneau and Guillaume Lample .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crosslingual language model pretraining .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , pages 7057\u20137067 .", "entities": []}, {"text": "Alexis Conneau , Ruty Rinott , Guillaume Lample , Adina Williams , Samuel Bowman , Holger Schwenk , and Veselin Stoyanov .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "XNLI : Evaluating cross - lingual sentence representations .", "entities": [[0, 1, "DatasetName", "XNLI"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2475\u20132485 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "E Fonseca , L Santos , Marcelo Criscuolo , and S Aluisio .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Assin :", "entities": [[0, 1, "DatasetName", "Assin"]]}, {"text": "Avaliacao de similaridade semantica e inferencia textual .", "entities": []}, {"text": "In Computational Processing of the Portuguese Language-12th International Conference , Tomar , Portugal , pages 13\u201315 .", "entities": []}, {"text": "Philip Gage . 1994 .", "entities": []}, {"text": "A new algorithm for data compression .", "entities": []}, {"text": "C Users J. , 12(2):23\u201338 .", "entities": []}, {"text": "Yuta Hayashibe .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Japanese realistic textual entailment corpus .", "entities": []}, {"text": "In Proceedings of The 12th Language Resources and Evaluation Conference , pages 6827\u20136834 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Samuel Humeau , Kurt Shuster , Marie - Anne Lachaux , and Jason Weston .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Poly - encoders : Architectures and pre - training strategies for fast and accurate multi - sentence scoring .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Taku Kudo and John Richardson .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SentencePiece : A simple and language independent subword tokenizer and detokenizer for neural text processing .", "entities": [[0, 1, "MethodName", "SentencePiece"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 66\u201371 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Seungyoung Lim , Myungji Kim , and Jooyoul Lee . 2019 .", "entities": []}, {"text": "Korquad1 .", "entities": []}, {"text": "0 : Korean qa dataset for machine reading comprehension .", "entities": [[0, 1, "DatasetName", "0"], [6, 9, "TaskName", "machine reading comprehension"]]}, {"text": "arXiv preprint arXiv:1909.07005 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "RoBERTa : A robustly optimized bert pretraining approach .", "entities": [[0, 1, "MethodName", "RoBERTa"]]}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Myle Ott , Sergey Edunov , Alexei Baevski , Angela Fan , Sam Gross , Nathan Ng , David Grangier , and Michael Auli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "fairseq :", "entities": []}, {"text": "A fast , extensible toolkit for sequence modeling .", "entities": []}, {"text": "In Proceedings of NAACL - HLT 2019 : Demonstrations .", "entities": []}, {"text": "Jason Phang , Thibault F \u00b4 evry , and Samuel R. Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Sentence encoders on stilts : Supplementary training on intermediate labeled - data tasks .", "entities": []}, {"text": "Livy Real , Erick Fonseca , and Hugo Gonc \u00b8alo Oliveira .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Organizing the assin 2 shared task .", "entities": [[2, 3, "DatasetName", "assin"]]}, {"text": "In ASSIN@ STIL , pages 1\u201313 .", "entities": []}, {"text": "428Nils", "entities": []}, {"text": "Reimers and Iryna Gurevych .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SentenceBERT : Sentence embeddings using Siamese BERTnetworks .", "entities": [[2, 4, "TaskName", "Sentence embeddings"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3982\u20133992 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 \u2013 1725 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Alex Wang , Yada Pruksachatkun , Nikita Nangia , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel R Bowman .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Superglue : A stickier benchmark for general - purpose language understanding systems .", "entities": [[0, 1, "DatasetName", "Superglue"]]}, {"text": "arXiv preprint arXiv:1905.00537 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "GLUE :", "entities": [[0, 1, "DatasetName", "GLUE"]]}, {"text": "A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[9, 12, "TaskName", "natural language understanding"]]}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 353\u2013355 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Adina Williams , Nikita Nangia , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A broad - coverage challenge corpus for sentence understanding through inference .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1112\u20131122 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yinfei Yang , Daniel Cer , Amin Ahmad , Mandy Guo , Jax Law , Noah Constant , Gustavo Hernandez Abrego , Steve Yuan , Chris Tar , Yun - Hsuan Sung , Brian Strope , and Ray Kurzweil .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Multilingual universal sentence encoder for semantic retrieval .", "entities": [[0, 4, "MethodName", "Multilingual universal sentence encoder"], [5, 7, "TaskName", "semantic retrieval"]]}, {"text": "A Korean RoBERTa Pre - training For the Korean RoBERTa baselines used in x4 , we pre - train a RoBERTa ( Liu et al . , 2019 ) model on an internal Korean corpora of size 65 GB , consisting of online news articles ( 56 GB ) , encyclopedia ( 7 GB ) , movie subtitles ( \u00181 GB ) , and the Sejong corpus8(\u00180.5 GB ) .", "entities": [[2, 3, "MethodName", "RoBERTa"], [9, 10, "MethodName", "RoBERTa"], [20, 21, "MethodName", "RoBERTa"]]}, {"text": "We use fairseq ( Ott et al . , 2019 ) , which includes the of\ufb01cial implementation for RoBERTa .", "entities": [[18, 19, "MethodName", "RoBERTa"]]}, {"text": "In Table 7 , we list all hyperparameters we use for Korean RoBERTa pre - training .", "entities": [[12, 13, "MethodName", "RoBERTa"]]}, {"text": "Note that , 8https://ithub.korean.go.kr/user/ guide / corpus / guide1.doHyperparameter Large Base Total # of Parameters 338 M 111 M Number of Layers 24 12 Hidden Size 1024 768 FFN Inner Hidden Size 4096 3072 Attention Heads 16 12 Attention Head Size 64 64 Dropout 0.1 0.1 Attention Dropout 0.1 0.1 Warmup Steps 30 K 24 K Peak Learning Rate 2e-4 6e-4 Batch Size 2048 8192 Weight Decay 0.01 0.01 Scheduled # Updates 2 M 500 K Performed # Updates*502.3 K 500 K Learning Rate Decay Linear Linear Adam \u000f 1e-6 1e-6 Adam \f 1 0.9 0.9 Adam \f 2 0.98 0.98 Gradient Clipping 0.0 0.0 Table 7 : Hyperparameters for Korean RoBERTa pretraining.*For the large model , we initially scheduled our learning rate to decay to zero at 2 M steps .", "entities": [[19, 22, "HyperparameterName", "Number of Layers"], [43, 44, "MethodName", "Dropout"], [46, 48, "MethodName", "Attention Dropout"], [57, 59, "HyperparameterName", "Learning Rate"], [61, 63, "HyperparameterName", "Batch Size"], [63, 64, "DatasetName", "2048"], [65, 67, "MethodName", "Weight Decay"], [82, 84, "HyperparameterName", "Learning Rate"], [87, 88, "MethodName", "Adam"], [91, 92, "MethodName", "Adam"], [96, 97, "MethodName", "Adam"], [101, 103, "MethodName", "Gradient Clipping"], [111, 112, "MethodName", "RoBERTa"], [121, 123, "HyperparameterName", "learning rate"]]}, {"text": "After 500 K steps , however , we observed no signi\ufb01cant improvement in the KorNLI and KorSTS \ufb01ne - tuning performance .", "entities": [[14, 15, "DatasetName", "KorNLI"], [16, 17, "DatasetName", "KorSTS"]]}, {"text": "compared to the original RoBERTa ( English ) , the model architectures are identical except for the token embedding layer , as we use different vocabularies ( 32 K sentencepiece vocab instead of 50 K byte - level BPE ) .", "entities": [[4, 5, "MethodName", "RoBERTa"], [29, 30, "MethodName", "sentencepiece"], [38, 39, "MethodName", "BPE"]]}, {"text": "After training , the base and large models achieve validation perplexities of 2.55 and 2.39 respectively , where the validation set is a random 5 % subset of the entire corpora .", "entities": []}, {"text": "B Fine - tuning with Cross - encoding Approaches To \ufb01ne - tune Korean RoBERTa and XLM - R models using the cross - encoding approach ( x4.1 ) , we follow the \ufb01ne - tuning procedures of RoBERTa ( Liu et al . , 2019 ) on MNLI and STS - B , as described in RoBERTa \u2019s code release9 .", "entities": [[14, 15, "MethodName", "RoBERTa"], [16, 17, "MethodName", "XLM"], [38, 39, "MethodName", "RoBERTa"], [48, 49, "DatasetName", "MNLI"], [50, 53, "DatasetName", "STS - B"], [57, 58, "MethodName", "RoBERTa"]]}, {"text": "Hyperparameter KorNLI", "entities": [[1, 2, "DatasetName", "KorNLI"]]}, {"text": "KorSTS Batch Size 32 16 Learning Rate Schedule", "entities": [[0, 1, "DatasetName", "KorSTS"], [1, 3, "HyperparameterName", "Batch Size"], [5, 7, "HyperparameterName", "Learning Rate"]]}, {"text": "Linear Linear Peak Learning Rate", "entities": [[3, 5, "HyperparameterName", "Learning Rate"]]}, {"text": "1e-5 2e-5 # Warmup Steps 7318 214 Total # Updates 121979 3596 Table 8 : Hyperparameters for Korean RoBERTa and XLM - R \ufb01ne - tuning using the cross - encoding approach .", "entities": [[18, 19, "MethodName", "RoBERTa"], [20, 21, "MethodName", "XLM"]]}, {"text": "9https://github.com/pytorch/fairseq/ blob / v0.9.0 / examples / roberta / README.glue .", "entities": []}, {"text": "md", "entities": []}, {"text": "429The \ufb01ne - tuning hyperparameters are summarized in Table 8 .", "entities": []}, {"text": "For each dataset and model size , we choose the hyperparameter con\ufb01gurations that are used in the corresponding English version of the dataset and model size ( except for the XLMR cross - lingual transfer using MNLI , where we also use the same hyperparameters as RoBERTa and XLM - R on KorNLI ) .", "entities": [[31, 35, "TaskName", "cross - lingual transfer"], [36, 37, "DatasetName", "MNLI"], [46, 47, "MethodName", "RoBERTa"], [48, 49, "MethodName", "XLM"], [52, 53, "DatasetName", "KorNLI"]]}, {"text": "We \ufb01nd that the hyperparameters used for English models and datasets give suf\ufb01ciently good performances on the development set , so we do not perform an additional hyperparameter search .", "entities": []}, {"text": "After training each model for 10 epochs , we choose the model checkpoint that achieve the highest score on the development set and evaluate it on the test set to obtain our \ufb01nal results inx4.1 .", "entities": []}, {"text": "We also report the development set scores for the best checkpoint in Table 9 .", "entities": []}, {"text": "We observe that the XLM - R models \ufb01ne - tuned on KorNLI and KorSTS achieve the highest scores on the development set , although the Korean RoBERTa models perform better on the test set ( Table 5 in x4.1 ) .", "entities": [[4, 5, "MethodName", "XLM"], [12, 13, "DatasetName", "KorNLI"], [14, 15, "DatasetName", "KorSTS"], [27, 28, "MethodName", "RoBERTa"]]}, {"text": "Both models outperform the cross - lingual transfer models on the development set , as is the case on the test set .", "entities": [[4, 8, "TaskName", "cross - lingual transfer"]]}, {"text": "Model # Params.yKorNLI KorSTS", "entities": [[3, 4, "DatasetName", "KorSTS"]]}, {"text": "Fine - tuned on Korean training set Korean RoBERTa ( base ) 111 M 81.97 84.97 Korean RoBERTa ( large ) 338 M 83.17 87.82 XLM - R ( base ) 270 M 79.20 83.02 XLM - R ( large ) 550 M 84.42 88.37 Fine - tuned on English training set ( Cross - lingual Transfer ) XLM - R ( base ) 270 M 74.34 XLM - R ( large ) 550 M 81.45 Table 9 : KorNLI and KorSTS development set scores for \ufb01ne - tuned cross - encoding language models .", "entities": [[8, 9, "MethodName", "RoBERTa"], [17, 18, "MethodName", "RoBERTa"], [25, 26, "MethodName", "XLM"], [35, 36, "MethodName", "XLM"], [53, 57, "TaskName", "Cross - lingual Transfer"], [58, 59, "MethodName", "XLM"], [67, 68, "MethodName", "XLM"], [79, 80, "DatasetName", "KorNLI"], [81, 82, "DatasetName", "KorSTS"]]}, {"text": "KorNLI scores are accuracy ( % ) and KorSTS scores are 100\u0002Spearman correlation.yTo ensure comparability with XNLI , we only use the MNLI portion of the KorNLI dataset .", "entities": [[0, 1, "DatasetName", "KorNLI"], [3, 7, "MetricName", "accuracy ( % )"], [8, 9, "DatasetName", "KorSTS"], [16, 17, "DatasetName", "XNLI"], [22, 23, "DatasetName", "MNLI"], [26, 27, "DatasetName", "KorNLI"]]}, {"text": "C Fine - tuning with Bi - encoding Approaches To \ufb01ne - tune Korean RoBERTa and XLM - R models using the bi - encoding approach ( x4.2 ) , we train Korean Sentence RoBERTa ( \u201c Korean SRoBERTa \u201d ) and Sentence XLM - R ( \u201c SXLM - R \u201d ) , following the \ufb01ne - tuning procedure of SentenceBERT ( Reimers and Gurevych , 2019 ) .", "entities": [[14, 15, "MethodName", "RoBERTa"], [16, 17, "MethodName", "XLM"], [34, 35, "MethodName", "RoBERTa"], [43, 44, "MethodName", "XLM"]]}, {"text": "Unless described otherwise , we follow the experimental settings , including all hyperparameters , ofSentenceBERT10 .", "entities": []}, {"text": "For each model size , we manually search among learning rates f2e-5 , 1e-5gfor training on KorNLI , f1e-5 , 2e-6gfor training on KorSTS , andf1e-5 , 2e-6gfor training on KorSTS after KorNLI .", "entities": [[16, 17, "DatasetName", "KorNLI"], [23, 24, "DatasetName", "KorSTS"], [30, 31, "DatasetName", "KorSTS"], [32, 33, "DatasetName", "KorNLI"]]}, {"text": "After training until convergence , we choose the learning rate that lead to the highest KorSTS score on the development set .", "entities": [[8, 10, "HyperparameterName", "learning rate"], [15, 16, "DatasetName", "KorSTS"]]}, {"text": "These hyperparameters are shown in Table 10 .", "entities": []}, {"text": "Model KorNLI KorSTSKorSTS ( after KorNLI ) Korean SRoBERTa ( base ) 2e-5", "entities": [[1, 2, "DatasetName", "KorNLI"], [5, 6, "DatasetName", "KorNLI"]]}, {"text": "1e-5 1e-5 Korean SRoBERTa ( large ) 2e-5", "entities": []}, {"text": "1e-5 1e-5", "entities": []}, {"text": "SXLM - R ( base ) 2e-5", "entities": []}, {"text": "1e-5 1e-5", "entities": []}, {"text": "SXLM - R ( large ) 1e-5 2e-6 1e-5 Table 10 :", "entities": []}, {"text": "Learning rates for Korean SRoBERTa and SXLM - R \ufb01ne - tuning using the bi - encoding approach .", "entities": []}, {"text": "We report the development set scores in Table 11 .", "entities": []}, {"text": "Korean SRoBERTa ( large ) achieves the best development set performance on both supervised settings , but SXLM - R ( large ) achieves the best performance for the KorNLI!KorSTS setting on test set .", "entities": []}, {"text": "10https://github.com/UKPLab/ sentence - transformers", "entities": []}, {"text": "430Model # Params .", "entities": [[2, 3, "MetricName", "Params"]]}, {"text": "KorSTS", "entities": [[0, 1, "DatasetName", "KorSTS"]]}, {"text": "Unsupervised Supervised -Trained on : KorNLITrained on : KorSTSTrained on : KorNLI !", "entities": [[11, 12, "DatasetName", "KorNLI"]]}, {"text": "KorSTS Korean SRoBERTa ( base ) 111 M 63.34 76.48 83.68 83.54 Korean SRoBERTa ( large ) 338 M 60.15 77.95 84.74 84.21 SXLM - R ( base ) 270 M 64.27 77.65 74.60 81.95 SXLM - R ( large ) 550 M 55.00 79.16 82.66 84.13 Table 11 : KorSTS development set scores ( 100\u0002Spearman correlation ) of bi - encoding models .", "entities": [[0, 1, "DatasetName", "KorSTS"], [50, 51, "DatasetName", "KorSTS"]]}, {"text": "Note that the \ufb01rst two columns of results are unsupervised w.r.t .", "entities": []}, {"text": "KorSTS , and the latter two are supervised w.r.t .", "entities": [[0, 1, "DatasetName", "KorSTS"]]}, {"text": "KorSTS .", "entities": [[0, 1, "DatasetName", "KorSTS"]]}]