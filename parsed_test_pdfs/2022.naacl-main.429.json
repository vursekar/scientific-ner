{"authors": "Haoran Li; Yangqiu Song; Lixin Fan", "pub_date": "", "title": "You Don't Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers' Private Personas", "abstract": "Social chatbots, also known as chit-chat chatbots, evolve rapidly with large pretrained language models. Despite the huge progress, privacy concerns have arisen recently: training data of large language models can be extracted via model inversion attacks. On the other hand, the datasets used for training chatbots contain many private conversations between two individuals. In this work, we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which has not been well studied yet. We show that speakers' personas can be inferred through a simple neural network with high accuracy. To this end, we propose effective defense objectives to protect persona leakage from hidden states. We conduct extensive experiments to demonstrate that our proposed defense objectives can greatly reduce the attack accuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve language models' powerful generation ability.", "sections": [{"heading": "Introduction", "text": "Social chatbots have been widely used to benefit many applications from answering factual questions to showing emotional companionship. With recent progress in large pretrained language models (Radford et al., 2019;Yang et al., 2019), some attempts (Wolf et al., 2019;Ham et al., 2020;Shen et al., 2021;Sevegnani et al., 2021;Gu et al., 2021b) are made to build chatbots based on large generative language models (LMs). To train such LM-based chatbots, private conversations are collected. Unfortunately, large language models tend to memorize training data and some private data can be recovered from models (Pan et al., 2020;Carlini et al., 2021). Besides such memorization problems, \"overlearning\" on simple training objectives can reveal sensitive attributes indirectly related to the learning task (Song and Shmatikov, 2020). LM-based social chatbots essentially inherit the privacy issues of general LMs and the overlearning problem.\nFor example, as Figure 1 shows, when using a fine-tuned GPT-2 as the encoder and decoder of an LM-based social chatbot, if the learned representation of each utterance can be obtained by an adversary, then the adversary can build a classifier to predict the persona information based on the representation. As shown by the example, for five out of 14 utterances, the attacker can successfully predict the persona, which can be harmful if the users (speakers of the utterances) do not prefer to reveal the persona information. Thus, in practice, when deploying such kinds of chatbots in real applications, we should first make sure that no private information can be leaked by the models.\nTo systematically study the privacy issues in LMbased social chatbots, there are several challenges. First, there is no existing data that can be used to quantify how much private information is revealed by an LM. Second, there has been no existing work showing how to attack utterance-level representations to obtain sensitive information. Third, there has been no existing LM-based chatbot that can defend against persona inference attacks, and no study shows how to protect both known and unknown persona attributes.\nIn this paper, to address the above challenges, we use the fine-tuned GPT-2 as our chatbot. We first collect a dataset by aligning personas with corresponding utterances in PersonaChat dataset (Zhang et al., 2018). Then we show that \"overlearning\" can happen for LM-based chatbots to reveal personas of speakers. We build a single external multi-layer perception (MLP) attacker model to perform black-box persona inference attacks on the utterance-level embeddings. With no access to parameters of the chatbot, the attacker model can infer speakers' personas with 37.59% accuracy over 4,332 personas. The high accuracy of the attacker model implies that the utterance-level embeddings have potential vulnerabilities to reveal \n\uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fc \uf0fc \uf0fc \uf0fc \uf0fc \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb\nFigure 1: Black-box persona inference attacks (over 4,332 personas) on a dialog. Every representation of the utterance, which is based on the last hidden state of GPT-2, is attacked without defense (column of \"Attacks on LM\") and with defense (column of \"Attacks on the defensed LM\"). If the model can predict the persona of the speaker based on the observed representation, then we regard it as a successful attack; otherwise, unsuccessful. In practice, when deploying a model, a robust model which will reveal nothing of the encoded utterances is expected.\nspeakers' private persona attributes. Thus, it is necessary to improve training algorithms to address such overlearning issues. Finally, we apply defense learning strategies on the GPT-2 to prevent such black-box attacks. We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2019) as additional defense objectives to train the GPT-2 and decrease the attacker's persona inference accuracy to 0.53%. Our contributions can be summarized as follows: 1 1): To the best of our knowledge, we are the first to disclose and analyze the persona inference attack for LM-based chatbots and treat it as a privacy risk.\n2): We propose an effective defensive training algorithm to prevent dialog representations from leaking personas of the corresponding speakers by uniform distribution approximation and mutual information minimization.\n3): We conduct extensive experiments to quantify both privacy and utility of proposed defense mechanisms. Besides solving the persona leakage issue, the proposed training algorithm has nearly no negative influence on utility.", "n_publication_ref": 12, "n_figure_ref": 2}, {"heading": "Related Work", "text": "Language models trained on private data suffer privacy risks of revealing sensitive information. Previous researches mainly considered black-box attacks that assumed attackers only had access to 1 Code is publicly available at https://github. com/HKUST-KnowComp/Persona_leakage_and_ defense_in_GPT-2.\ninputs and outputs of language models. Carlini et al. (2021) performed black-box model inversion attack on GPT-2 through descriptive prompts with beam search. Lehman et al. (2021) examined BERT pretrained on Electronic Health Records via blank filling and model probing to recover Personal Health Information. Furthermore, given black-box access to a language model's pre-train and finetune stages, Zanella-B\u00e9guelin et al. (2020) showed that sensitive sequences of the fine-tuning dataset can be extracted. For the distributed client-server setup, Malekzadeh et al. (2021) considered the sensitive attribute leakage from the server side with honest-but-curious (HBC) classifiers.\nWhat is worse, for an LM-based chatbot, its training conversations are prone to include more private attributes than other commonly-used corpora for language modeling like BooksCorpus (Zhu et al., 2015) and Wikipedia. Tigunova et al. (2019) proposed Hidden Attribute Model (HAM) to extract professions and genders of speakers from various dialog datasets.  further applied Attribute Extractor to generate speakers' attribute triplets flexibly and suggested downstream tasks based on the triplets. Pan et al. (2020) exploited embeddings of language models to recover inputs' digits and keywords. Though the setup of this work is similar to ours, they merely consider simple cases of data recovery with given rules and suffer great utility degradation to obtain optimal defense performance. For our work, there is no fixed pattern or rule for the model input. Instead of finding key-words or recovering digits, we aim to infer more complicated private attributes from such embeddings. Moreover, our proposed defenses have almost no influence on the utility.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Attacking on Language Models", "text": "In this section, we illustrate black-box persona inference attacks on GPT-2 and our defense strategies. In Section 3.1, we first give the problem formulation. Then we describe the attack in Section 3.2.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Problem Formulation", "text": "We assume that there is a GPT-2 based chatbot f pretrained on private conversations D. Only language modeling is used to train the chatbot:\nL f (u; \u03b8 f ) = \u2212 |u| i=1\nlog(Pr(wi|c, w0, w1, ..., wi\u22121)), (1) where f refers to the LM-based chatbot with given utterance u = {w 0 , w 1 , ..., w |u|\u22121 } and previous context c.\nAn adversary owns one external annotated dialog dataset D a = {(U 1 , s 1 ), (U 2 , s 2 ), ..., (U n , s n )} with n conversations where U i indicates a list of utterances {u i1 , u i2 , ..., u in i } of i-th conversation and s i corresponds to a list of sensitive personas {s i1 , s i2 , ..., s in i } for corresponding utterance. Each persona s kj is an integer that can be mapped to its persona according to a predefined dictionary and 0 \u2264 s kj \u2264 C \u2212 1 where C is the total number of predefined persona attributes. The goal of the adversary is to infer speakers' personas s from utterances' embeddings f (u) where u and s refer to any utterance and its persona label.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Black-box Persona Inference Attack", "text": "The persona inference attack can be viewed as a supervised classification task. For the black-box attack setup, the adversary can only query the target dialog model f with access to embeddings of adversary's inputs and cannot access or modify model parameters \u03b8 f . As shown in the left part of Figure 2, the adversary tries to build its attacker model A with its external data D a and dialog model f . The persona predictor's output A(f (u)) is the estimated probability distribution over C persona attributes. Its loss function L A exploits cross-entropy between the predicted distribution and ground truth distribution that can be formulated as:\nLA(u kj , s kj ; \u03b8A) = CE(A(f (u kj )), s kj ),(2)\nwhere CE refers to cross-entropy loss between persona label s kj and A(f (u kj )).\nA well-performed persona predictor A can cause great privacy threats. For machine learning as a service (MLaaS), A can be applied to perform a man-in-the-middle attack on the application programming interfaces. Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker A to collect personas of service users.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Defense Learning Strategies", "text": "The LM training objective in Equation 1 only considers the utility of chatbots. In later experiment sections, we show that LM brings severe overlearning issues. Ideally, to achieve an optimal privacypreserving chatbot against persona inference attacks, the probability distribution of the attacker model A should be close to the uniform distribution. That is, the adversary cannot improve its inference accuracy from posterior estimation A(f (u)) and the accuracy is no better than making random guesses on the persona attributes. Moreover, the constraints on privacy should have minor degradation on the utility to maintain the strong generation ability of chatbots.\nFollowing the intuition that the adversary cannot obtain better results than a random guess, in Section 4.1, we propose KL loss that aims to flatten the persona predictor's estimated distribution. Based on minimizing the mutual information between hidden states f (u) of chatbots and private persona attributes s, we propose MI loss in Section 4.2. Lastly, we show the overall training objective in Section 4.3.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "KL Loss", "text": "KL loss aims to minimize the Kullback-Leibler divergence between A(f (u)) and the uniform distribution. It flattens the distribution of A(f (u)) so that the adversary cannot gain any useful knowledge after training attacker model A. The KL divergence between the uniform distribution and A(f (u)) can be formulated as:\nDKL(UNI||A(f (u))) = \u2212 1 C C\u22121 k=0 log(CPr(k|f (u), \u03b8A)),(3)\nwhere UNI indicates the uniform distribution and k indicates the k-th persona label of C labels. For optimization, we can leave out constant terms and the logarithm (Mireshghallah et al., 2021)   \u20dd and the attacking stage is marked by 2 \u20dd. Both language modeling and defender objectives are jointly trained for the defense to optimize the GPT-2 model. After GPT-2's training stage 1 \u20dd is finished, parameters of GPT-2 are all frozen and then the attacking stage 2 \u20dd starts. The defender shares the same architecture as the attacker and uses L kl with L mi as defense objectives.\nthe following loss function:\nLD(u; \u03b8A) = \u2212 1 C C\u22121 k=0\nPr(k|f (u), \u03b8A).\nHowever, from the perspective of defenders, they have no access to attacker model A and its parameters. Instead, they can build their own persona predictor as a fake attacker. More specifically, they may mimic the adversary to annotate a dataset D \u2032 a and a persona predictor A p . Then the KL loss becomes:\nL kl (u; \u03b8A p , \u03b8 f ) = \u2212 1 C C\u22121 k=0 Pr(k|f (u), \u03b8A p ),(5)\nwhere parameters of the chatbot \u03b8 f and the fake attacker \u03b8 Ap are updated via KL loss. The intuition is to train the chatbot together with a fake attacker to prevent model overlearning by flattening the attacker model's distribution.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "MI Loss", "text": "The privacy constraint requires that hidden representations should not reveal the persona attributes.\nIn other words, given any utterance u and persona s behind the utterance u, we want to minimize the mutual information between f (u) and s:\nmin \u03b8 f I(f (u); s).(6)\nFollowing the derivation in Song et al. (2019) and , the upper bound can be formulated as:\nI(f (u); s) \u2264 E q(f (u)) DKL(q(s|f (u))||p(s)),(7)\nwhere p(s) can be any distribution for s, q(x) refers to probability distribution of model f parameterized by \u03b8 f and f (u) is assumed to be sampled from the conditional distribution q(f (u)|x, s). However, q(s|f (u)) is hard to estimate. Instead, we use p \u03a8 (s|f (u)) to approximate q(s|f (u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2019):\nE q(f (u)) DKL(q(s|f (u))||p(s)) \u2265 E q(f (u)) [log p\u03a8(s|f (u)) \u2212 log p(s)].(8)\nTherefore, our objective in Equation 6 can be formulated as an adversarial training objective:\nmin \u03b8 f max \u03a8 E q(f (u)) [log p\u03a8(s|f (u)) \u2212 log p(s)].(9)\nlog p(s) is independent of f (u), and we may leave this term out in Equation 9:\nmin \u03b8 f max \u03a8 E q(f (u)) [log p\u03a8(s|f (u))].(10)\nThen, Equation 10 illustrates an adversarial game between an adversary p \u03a8 who manages to infer s from f (u) and a defender who modifies \u03b8 f to protect s from persona inference attack. Adversarial training is widely used to protect sensitive features in natural language processing (Elazar and Goldberg, 2018;Coavoux et al., 2018;Li et al., 2018). Using the persona predictor model A p with softmax activation to learn p \u03a8 , we obtain the final objective for the defender:\nmin \u03b8 Ap max \u03b8 f CE(Ap(f (u)), s).(11)\nWe can rewrite Equation 11 into two losses: L mi1 (u kj , s kj ; \u03b8 Ap ) = CE(A p (f (u kj )), s kj ) and L mi2 (u kj , s kj ; \u03b8 f ) = \u2212CE(A p (f (u kj )), s kj ) for the fake adversary and the chatbot respectively.\nThen our MI loss can be formulated as:\nLmi = \u03bb0Lmi1 + Lmi2,(12)\nwhere \u03bb 0 controls the ratio between two the fake attacker A p and the defensed chatbot f .", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Overall Objective", "text": "The right part of Figure 2 illustrates how the chatbot is trained to address the black-box attack. The loss function for the defender combines KL loss, MI loss and LM loss. Notice that the fake adversary objective in MI loss violates KL loss which tries to make the distribution of A p flatten. Our proposed loss assigns more weights to the KL loss:\nL = L f + \u03bb1L kl + \u03bb2Lmi,(13)\nwhere \u03bb 1 and \u03bb 2 are hyper-parameters and \u03bb 1 \u2265 10\u03bb 2 to flatten the distribution of A p . Though the chatbot trained with overall loss L still cannot interfere training process of A during black-box attacks, L aims to mitigate persona overlearning issues of f to address such persona inference attacks.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Experiments", "text": "In this section, we conduct experiments to evaluate the performance of privacy and utility for the proposed defense learning strategies. In Section 5.1, we give our experimental settings in detail. In Section 5.2, we show the attacking performance with and without defense. In Section 5.3, we perform ablation study on defense objectives. In Section 5.4, we use automatic metrics to evaluate chatbots' utility. We conduct various attack setups in Section 5.5 and perform a case study in Section 5.6.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Settings", "text": "Dataset. To train the GPT-2 as our chatbot, we use the DialoGPT  pretrained on Reddit comment chains. Then we use PersonaChat dataset (Zhang et al., 2018) to fine-tune the GPT-2. To obtain annotated dataset D a for the adversary, we align personas to corresponding utterances through positive (utterance,persona) pairs provided in Dialogue NLI (Welleck et al., 2019) dataset. For those utterances with no annotations, we assign label \u22121 to them. We reshuffle the dataset to balance the label distribution among train/val/test datasets with the ratio of 8 : 1 : 1. We first let the attacker and defender share the same training data. In later sections, we will separate the annotated data for the adversary and defender with no overlap. A summary statistics of D a is shown in Table 1. Attacker model. In our experiment, we use a 2-layer neural network with cross-entropy loss as the attacker model. The attacker model exploits the final layer embedding of the last token \"<|endof-text|>\" from the GPT-2 as model input. We also try other attacker model architectures (transformer block based attackers) and input embeddings (average of all embeddings in the final layer of GPT-2), but the attacking performance is worse than the 2-layer model mentioned above.\nEvaluation Metrics. The evaluation metrics are based on privacy and utility. For privacy, we use persona inference accuracy and weighted F1score to evaluate the attacker's performance. We also use Bayesian Privacy (BP) (Gu et al., 2021a) to quantify the attacker's privacy loss for the estimated persona distribution. Top-k accuracy is reported in the Appendix. For utility, we apply BERTScore , Distinct (Li et al., 2016), BLEU (Papineni et al., 2002) and perplexity (PPL) as evaluation metrics. BERTScore and BLEU measure similarity between generated outputs and ground truth while Distinct (Dist) focuses on diversity. Perplexity shows the uncertainty when the LM model fits the data.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Privacy", "text": "Attacks without Defense. We list the attacking performance of A in multiple scenarios shown in Acc refers to test persona inference accuracy. F1 uses weighted average F1-score. Max-Ratio indicates the ratio that the most frequent prediction shares among all predictions. The worse the attack model performs, the better privacy protection can be achieved.\ndistribution, then it can randomly guess over 4,332 labels (Random Pred). Otherwise the adversary can perform Best Guess that only guesses the most frequent persona in the dataset. LM indicates the attacker performance that only language modeling objective is applied to train the chatbot without any defense mechanism. From the table, the test persona inference accuracy on the LM achieves 37.59% while guessing on the label with most occurrences merely has 0.72% accuracy. That is, the black-box persona inference attack has 52\u00d7 the accuracy of guessing. The huge performance gap between the attacker model and the baseline guess method indicates that simple language modeling objective has serious overlearning issues that unintentionally capture private personas of speakers.\nAttacks on the Defensed LM. To avoid the persona overlearning issue, we use additional defense objectives illustrated in Section 4. LM+KL+MI utilizes language modeling, KL loss and MI loss in Equation 13 to train the GPT-2. As demonstrated in Table 2, the attacker performance on LM+KL+MI significantly reduces the attacking accuracy from 37.59% to 0.53% and F1-score drops from 0.37 to nearly 0. This defense mechanism can even outperform Best Guess in terms of privacy protection. That is, even if the adversary annotates its own dataset to train an attacker model, the attacking performance is still worse than simply guessing the most frequent label. As a result, the black-box persona prediction attack becomes useless after applying the defenses for the chatbot. The adversary cannot obtain any speaker's persona from the embedding f (u) by training A.\nTo learn why the proposed defenses work so well, we further examine the ratio of the most frequent predicted label (Max-Ratio) among all pre-dictions. The accuracy of Best Guess reveals that the most frequent label in the test set has a ratio of 0.72%. After applying KL loss and MI loss, the attacker model tends to make predictions on a single label. For LM+KL+MI, the Max-Ratio even occupies 81.87% predictions. This implies that the proposed defense strategies may have the potential to fool the attacker model to make wrong predictions on a single slot. We will further investigate this implication in later sections.\nOverall, the above experiment demonstrates that our proposed defense learning strategies can effectively mitigate the persona overlearning issue and avoid black-box persona inference attacks.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ablation Study", "text": "To show the effectiveness of proposed KL loss and MI loss and how they affect the performance of black-box persona inference attacks, we consider the inclusion and exclusion of proposed defense objectives. The result is shown in Table 2. LM+KL indicates the GPT-2 is trained with language modeling and KL loss. LM+MI applies language modeling and MI loss. From the table, it can be seen that LM+KL, LM+MI and LM+KL+MI are all able to reduce the test accuracy of the attacks. The KL loss is weaker from the perspective of defense, but it tends to flatten the estimated persona distribution with much smaller Max-Ratio. The LM+MI shares similar test accuracy and F1-score with LM+KL+MI, but nearly all predictions are made on a single persona label with a ratio of 99.84%. This suggests that MI loss causes the attacker model to predict all labels on a single persona attribute. After KL loss is applied on LM+KL+MI, the Max-Ratio drops to 81.87%.\nAs discussed earlier, high Max-Ratio may also cause privacy leakage. Suppose the adversary knows the persona with Max-Ratio, then it can improve its guess by not predicting this persona, which is a threat for fewer persona labels (for example, binary classification). These results verify that KL loss introduces flatter estimation and MI loss is more effective against persona overlearning, which conforms to our intuition of their objectives in Section 4.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Utility", "text": "Besides privacy, utility is another key objective to train a chatbot. Several automatic metrics are considered to evaluate the generation performance. For generation, we use    Table 4: Evaluation on the privacy for 8 clusters. Unseen shows the results only for the first 3 persona labels that defender has never seen. Overall refers to the results on all 8 labels. Acc and Max-Ratio are measured in %. BP u corresponds to Bayesian Privacy loss on the uniform distribution. Still, the worse the attack model performs, the better privacy protection can be achieved.\nthe second speaker (Human B in Figure 1) with all previous turns as context. Then we compared the generated model outputs with ground truth replies. We use Dist-1 and Dist-2 to count ratios of distinct unigrams and bigrams. BLEU-1, BLEU-2 and BLEU-4 are applied to evaluate generation similarity with ground truth. Due to the one-to-many nature of chit-chats, the BLEU is not adequate to compare generated responses with ground truth. Hence, we adapt Precision, Recall and Precision of BERTScore to measure the similarity in the embedding space.\nThe evaluation result is shown in Table 3, where same models from Table 2 are evaluated. The result indicates that adding KL loss will increase the perplexity greatly from 14.8 to 28.9. After combining KL loss with MI loss, its perplexity decreases to 19.674. A plausible explanation is that KL loss confuses the persona predictor and indirectly increases the uncertainty of the GPT-2. All GPT-2 models have relatively low BLEU scores due to the one-to-many mapping between contexts and responses. For Distinct and BERTScore, there are only minor differences between LM and defensed LMs. Though the uncertainty increases after applying KL loss and MI loss, it does no harm to the quality of generation. In summary, there is almost no negative influence on the utility after applying the proposed defense strategies.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "More Setups on Attacks", "text": "Attacks on Imbalanced Data Distribution. Previous black-box attacks usually assume that the annotated dataset D a must share similar data distri-bution with the defender's training data. To examine the performance of defense strategies on unseen personas, we assign the adversary's dataset D a with labels that the defender cannot acquire. We split data with 500 persona labels that are uniquely held by the adversary. The defender owns 8,031 conversations with persona labels ranging from 500 to 4,331 while the adversary holds 2,376 dialogues with persona labels ranging from 0 to 4,331. For testing, 500 conversations with persona labels ranging from 0 to 4,331 are used.\nUnder imbalanced data distribution, the attack on the defensed LM has Acc 0.47%, F1 1.90e-3 and Max-Ratio 94.06%. The persona inference accuracy is still very low and the attacker model tends to predict more on a single persona label than the balanced data distribution setup. This result shows that the proposed overall loss can also prevent black-box persona inference attacks on unseen personas. It also verifies previous suggestions that combining LM loss with MI loss may fool the attacker model to make wrong predictions.\nAttacks on Fewer Persona Labels. The above experiments are based on 4,332 persona labels. In fact, many personas share similar meanings and can be further clustered. Besides, to better evaluate privacy loss for the estimated distribution, a smaller label space is preferred. Therefore, it is necessary to consider defense performance on a smaller label space. We use Sentence-BERT (Reimers and Gurevych, 2020) to embed all persona sentences and perform k-means clustering on the embeddings to obtain 8 clusters. We manually checked these clusters and classified them as cars, food, animals  For both conversations, the \"context\" is fixed and used as the first four utterances. Then the bot and the user start interactive conversations with the \"context\". Since there is no gold standard, the results are annotated by the authors.\n(pets), family information, hobbies, jobs, personal information and music tastes respectively. To evaluate how the clustering performs, we randomly sample 100 utterances with clustered labels and invite two volunteers to inspect those samples. Both of them agree on 90% of the clustered annotations.\nAfter manual inspection of the remaining 10% annotations, the clustering error rate is 8%. Following previous imbalanced data split, we assign data in the first 3 clusters only to the adversary to make the data distribution imbalanced. Here, the defender owns 6,654 conversations with persona labels ranging from 3 to 7 while the adversary holds 3,753 dialogues with persona labels ranging from 0 to 7. For testing, 500 conversations with persona labels ranging from 0 to 7 are used.\nThe attacking performance for both unseen labels and all labels is displayed in Table 4. BP u measures the KL divergence D KL (F 0 ||A(f (u))) where F 0 refers to uniform distribution. For imbalanced data distribution with a small label space, our proposed defenses can still achieve much lower attack accuracy than LM on both Unseen and Overall. However, for Overall, LM+KL+MI has higher accuracy with a lower F1-score compared with two baselines. This indicates that proposed defenses fail to protect privacy as we desired in the baselines. For BP u , LM+KL+MI are around 10 times smaller than LM. It means that after applying defense objectives, the attacker's estimated distribution is much closer to the uniform distribution. Thus the effectiveness of the KL loss is verified. In addition, Max-Ratio with 8 clusters on Unseen is smaller than 4,332 labels even though the distribution of 8 clusters is obviously tighter. Still, the Max-Ratio of 58.15% accounts for a much larger fraction than other predictions. In summary, the above results imply that for the smaller label space, our proposed defense objectives are still effective even on unseen persona labels.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Case Study", "text": "In Figure 3, we give an example of the persona inference attack, where conversations are generated between the chatbot and the user with the given context. We manually mark True/False on the predicted results. As shown in the figure, there are several successful attacks on LM and no correct prediction on the defensed LM. For attacks on LM, speakers' hobbies and jobs can be inferred. For incorrect predictions, the attacker model can still predict context-aware personas. After applying proposed defense learning strategies, the predicted personas become irrelevant with context and mostly predict \"My favorite color is blue.\" In fact, it is the most frequent prediction for LM+KL+MI over 4,332 persona labels. This attack example illustrates that our defense objectives can prevent the black-box persona inference attack from inferring relevant personas.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "In this paper, we show that LM-based chatbots tend to reveal personas of speakers and propose effective defense objectives to prevent GPT-2 from overlearning. Unlike other works that suffer from utility degradation, our defense learning strategies do no harm to the powerful generation ability of LM-based chatbots. We conduct extensive experiments to evaluate both privacy and utility. We perform black-box persona inference attacks under various setups to demonstrate the robustness of proposed defense learning strategies. In addition, we use automatic metrics to show that proposed defense learning strategies maintain the utility. For future work, we suggest working on flattening the distributions of attacker models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ethical Considerations", "text": "We declare that all authors of this paper acknowledge the ACM Code of Ethics and honor the code of conduct. This work essentially considers blackbox attacks on the private persona attributes and proposes effective learning strategies to prevent chatbots from overlearning private personas.\nDataset. During our dataset collection, all the conversations and personas are collected from publicly available datasets including PersonaChat and DNLI. All the speakers are anonymized and no identifiable personal information is included.\nModel. For training our LM-based chatbots, we follow standard methods. We are well aware of the bias issue inside current language models. In the future, if there are other fairer language models, we will extend our defenses on them. formation about personas. However, its attacking performance is poor. The poor performance implies our proposed defense learning strategies may obfuscate Attacker for estimating single sample f (u) and finally make the wrong prediction.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C More on Case Study", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C.1 Example of Generation", "text": "To show an intuition view on utility, we provide one generation sample shown in Figure 5. Both LM and LM+KL+MI are able to generate fluent and proper relies. Moreover, they tend to maintain coherence with previous contexts. For example, it is mentioned in the context that Human B is a vegan and both chatbots respond that they do not eat meat for the food preference. This generation example shows that proposed defense learning objectives preserve the model utility.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "C.2 More Examples of Persona Inference", "text": "Here, we give two more examples of the persona inference attacks in Table 6. The first example shows one successful defense. For the second example, both attackers with and without defense fail to predict the ground truth persona. Still, we can see that LM+KL+MI predicts personas that are irrelevant to the context. However, LM's output \"I know how to play the guitar.\" is much closer to the context about music and instruments. Without any defense, the above examples show that the attacker model can still predict context-aware personas even if its predictions are wrong. After applying the proposed defenses, the attacker model cannot predict meaningful personas relevant to the context.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D Evaluation on Top-k Accuracy", "text": "Previous experiments mainly consider accuracy as the evaluation metric. In this section, we use top-k accuracy for the black-box persona inference attacks to measure privacy protection. As shown in Table 5, our defense is much more robust than LM when k \u2264 50. When k is larger than 500, the defense degrades rapidly as k increases. This result implies that the ground truth personas mostly lie in the top 2,000 predictions even if the defense is applied. For a smaller k, our proposed defense learning strategies are still effective.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgment", "text": "The authors of this paper were supported by the NSFC Fund (U20B2053) from the NSFC of China, the RIF (R6020-19 and R6021-20) and the GRF ( 16211520 ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Training details.", "text": "For each conversation, the utterances are concatenated by the special token \"<|endoftext|>\" to train the GPT-2. To decode outputs from GPT-2, we apply the Nucleus Sampling (Holtzman et al., 2020) method. We set top-p = 0.9 with a temperature coefficient 0.9 to sample words from the GPT-2. For optimization, we set 2 AdamW optimizers (Loshchilov and Hutter, 2019) for the chatbot and the persona predictor respectively. The learning rate is 3e-5 with linear warm-up and decay. For hyper-parameters, we set \u03bb 0 = 1, \u03bb 1 = 10 and \u03bb 2 = 1.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "B Comparison of Internal Distribution between A and A p", "text": "To make predictions on personas, the arg max function is used for the estimated distribution of persona predictors. However, the internal distribution conveys crucial information about how the persona predictors estimate f (u). We follow the setup of imbalanced data split of 8 clusters in Section 5.5 to examine persona predictors of attacker A and fake attacker A p .\nFigure 4 shows the data distribution of the test set and average distribution after softmax activation over the 8 labels for attacker A and defender A p . For attacker A, we consider the attack on LM and LM+KL+MI. The defender A p tends to have a large difference with Data and tries to flatten its distribution among its own training set (the last 5 labels). This behavior conforms to the KL loss's objective that aims to flatten the distribution and deviate from the ground truth distribution. For attacker A, distributions of both LM and LM+KL+MI seem close to the ground truth distribution. This indicates that the attacker model A can still learn statistical in- ", "n_publication_ref": 0, "n_figure_ref": 1}], "references": [{"title": "Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models", "journal": "", "year": "", "authors": "Nicholas Carlini; Florian Tramer; Eric Wallace; Matthew Jagielski; Ariel Herbert-Voss; Katherine Lee; Adam Roberts; Tom Brown"}, {"title": "Privacy-preserving neural representations of text", "journal": "", "year": "2018", "authors": "Maximin Coavoux; Shashi Narayan; Shay B Cohen"}, {"title": "Adversarial removal of demographic attributes from text data", "journal": "", "year": "2018", "authors": "Yanai Elazar; Yoav Goldberg"}, {"title": "Federated deep learning with bayesian privacy", "journal": "", "year": "2021", "authors": "Hanlin Gu; Lixin Fan; Bowen Li; Yan Kang; Yuan Yao; Qiang Yang"}, {"title": "PRAL: A tailored pre-training model for task-oriented dialog generation", "journal": "", "year": "2021", "authors": "Jing Gu; Qingyang Wu; Chongruo Wu; Weiyan Shi; Zhou Yu"}, {"title": "End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2", "journal": "", "year": "2020", "authors": "Donghoon Ham; Jeong-Gwan Lee; Youngsoo Jang; Kee-Eung Kim"}, {"title": "The curious case of neural text degeneration", "journal": "", "year": "2020", "authors": "Ari Holtzman; Jan Buys; Li Du; Maxwell Forbes; Yejin Choi"}, {"title": "Does BERT pretrained on clinical notes reveal sensitive data?", "journal": "", "year": "2021", "authors": "Eric Lehman; Sarthak Jain; Karl Pichotta; Yoav Goldberg; Byron Wallace"}, {"title": "Tiprdc: Task-independent privacy-respecting data crowdsourcing framework for deep learning with anonymized intermediate representations", "journal": "", "year": "2020", "authors": "Ang Li; Yixiao Duan; Huanrui Yang; Yiran Chen; Jianlei Yang"}, {"title": "A diversity-promoting objective function for neural conversation models", "journal": "", "year": "2016", "authors": "Jiwei Li; Michel Galley; Chris Brockett; Jianfeng Gao; Bill Dolan"}, {"title": "Towards robust and privacy-preserving text representations", "journal": "Short Papers", "year": "2018", "authors": "Yitong Li; Timothy Baldwin; Trevor Cohn"}, {"title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"title": "Honest-but-curious nets: Sensitive attributes of private inputs can be secretly coded into the classifiers' outputs", "journal": "", "year": "2021", "authors": "Mohammad Malekzadeh; Anastasia Borovykh; Deniz G\u00fcnd\u00fcz"}, {"title": "Privacy regularization: Joint privacy-utility optimization in LanguageModels", "journal": "", "year": "2021", "authors": "Fatemehsadat Mireshghallah; Huseyin Inan; Marcello Hasegawa; Victor R\u00fchle; Taylor Berg-Kirkpatrick; Robert Sim"}, {"title": "Privacy risks of general-purpose language models", "journal": "", "year": "2020", "authors": "Xudong Pan; Mi Zhang; Shouling Ji; Min Yang"}, {"title": "BLEU: a method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeff Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "Making monolingual sentence embeddings multilingual using knowledge distillation", "journal": "", "year": "2020", "authors": "Nils Reimers; Iryna Gurevych"}, {"title": "OTTers: One-turn topic transitions for open-domain dialogue", "journal": "", "year": "2021", "authors": "Karin Sevegnani; David M Howcroft; Ioannis Konstas; Verena Rieser"}, {"title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition", "journal": "", "year": "2021", "authors": "Weizhou Shen; Junqing Chen; Xiaojun Quan; Zhixian Xie"}, {"title": "Overlearning reveals sensitive attributes", "journal": "", "year": "2020", "authors": "Congzheng Song; Vitaly Shmatikov"}, {"title": "Learning controllable fair representations", "journal": "", "year": "2019", "authors": "Jiaming Song; Pratyusha Kalluri; Aditya Grover; Shengjia Zhao; Stefano Ermon"}, {"title": "Listening between the lines: Learning personal attributes from conversations", "journal": "", "year": "2019", "authors": "Anna Tigunova; Andrew Yates; Paramita Mirza; G Weikum"}, {"title": "Dialogue natural language inference", "journal": "", "year": "2019", "authors": "Sean Welleck; Jason Weston; Arthur Szlam; Kyunghyun Cho"}, {"title": "Transfertransfo: A transfer learning approach for neural network based conversational agents", "journal": "", "year": "2019", "authors": "Thomas Wolf; Victor Sanh; Julien Chaumond; Clement Delangue"}, {"title": "Getting to know you: User attribute extraction from dialogues", "journal": "", "year": "2020", "authors": "Chien-Sheng Wu; Andrea Madotto; Zhaojiang Lin; Peng Xu; Pascale Fung"}, {"title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; R Russ; Quoc V Salakhutdinov;  Le"}, {"title": "Analyzing information leakage of updates to natural language models", "journal": "", "year": "2020", "authors": "Santiago Zanella-B\u00e9guelin; Lukas Wutschitz; Shruti Tople; Victor Ruehle; Andrew Paverd; Olga Ohrimenko; Boris K\u00f6pf; Marc Brockschmidt"}, {"title": "Personalizing dialogue agents: I have a dog, do you have pets too?", "journal": "", "year": "2018", "authors": "Saizheng Zhang; Emily Dinan; Jack Urbanek; Arthur Szlam; Douwe Kiela; Jason Weston"}, {"title": "Bertscore: Evaluating text generation with bert", "journal": "", "year": "2020", "authors": "Tianyi Zhang; * ; Varsha Kishore; * ; Felix Wu; * ; Kilian Q Weinberger; Yoav Artzi"}, {"title": "DIALOGPT : Large-scale generative pre-training for conversational response generation", "journal": "", "year": "2020", "authors": "Yizhe Zhang; Siqi Sun; Michel Galley; Yen-Chun Chen; Chris Brockett; Xiang Gao; Jianfeng Gao; Jingjing Liu; Bill Dolan"}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "journal": "", "year": "2015", "authors": "Yukun Zhu; Ryan Kiros; R Zemel; R Salakhutdinov; R Urtasun; A Torralba; S Fidler"}, {"title": "LM+KL+MI I ride around the town on my cool bicycle. I like to go to the beach. I collect civil war antiques, and to read, you? A Really? I really like mountain bike too. I wish I lived in the mountains. Do you like nature? I have been to 12 national parks. I love nature. I like looking at plants. I love plants too, and hiking. In fact, I am actually an environmental activist. Cool, I am a vegan. Nice, do you have a favorite food? Human B LM LM+KL+MI My favorite dish is lentil curry", "journal": "", "year": "", "authors": "B Human;  Lm"}, {"title": "Human A I have never had that, but I want to try it now", "journal": "", "year": "", "authors": ""}, {"title": "What do you like to do the most? It is very good. I also really enjoy watching funny videos of comedians", "journal": "", "year": "", "authors": "B Lm Human;  Lm+kl+mi"}, {"title": "Table 5: Evaluation on the top-k accuracy over 4,332 clusters", "journal": "Dialog Context", "year": "", "authors": ""}, {"title": "Hi how are you doing? Human B: I am great and you? Human A: I am great", "journal": "", "year": "", "authors": "A Human"}, {"title": "Human B: I am listening to the rolling stones, I love them. Human A: Is that your favorite band? Human B: Yes it is. I am working right now too", "journal": "", "year": "", "authors": ""}, {"title": "Hello there my name is Dr.Lucy. How are you? Human B: I am great, loving this city life, how are you? Human A: I am well thank you. I miss my country life in Spain", "journal": "", "year": "", "authors": "A Human"}, {"title": "My older brother lives in Spain, how is it? Human A: It is beautiful. I hope to take my family back there", "journal": "", "year": "", "authors": "B Human"}, {"title": "Human A: Oh, how long have you two been together? Human B: Very long, she was with me when I colored my hair pink. Human A: That is awesome. What type of music do you two listen to? Human B: I like reading music, what about you? Human A: Hip hop is my favorite. Do you play an instrument? Persona Prediction: Ground truth: My favorite music is hip hop. LM: I know how to play the guitar", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Scenarios for attacks without defense (left) and with defense (right). The GPT-2's training stage is marked by 1\u20dd and the attacking stage is marked by 2 \u20dd. Both language modeling and defender objectives are jointly trained for the defense to optimize the GPT-2 model. After GPT-2's training stage 1 \u20dd is finished, parameters of GPT-2 are all frozen and then the attacking stage 2 \u20dd starts. The defender shares the same architecture as the attacker and uses L kl with L mi as defense objectives.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure3: Black-box persona inference attacks on chit-chats between the user (the authors of this paper) and chatbots. For both conversations, the \"context\" is fixed and used as the first four utterances. Then the bot and the user start interactive conversations with the \"context\". Since there is no gold standard, the results are annotated by the authors.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: Dialog generation example on Human B. All previous utterances between A and B are used as context to generate responses.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "ContextAttacks on LMAttacks on the defensed LMHuman A Hello, how are you tonight? I take things very literally I am engaged to be married Human B Hello my friend. I am well. I am a happy person I like to go shopping with my daughters Human A Good, glad to hear it. What do you do for fun? I do whatever it takes to get what I want My favorite color is blue Human B I ride around the town on my cool bicycle. I love to ride my bike on the weekend My favorite color is blue", "figure_data": "Human AReally? I really like mountain bike too.I also like to mountain bikeMy favorite color is blueHuman BI wish I lived in the mountains.I have never been out of the countryMy favorite color is blueHuman ADo you like nature? I have been to 12 nationalI like to visit national parksMy favorite color is blueparks.Human BI love nature. I like looking at plants.I really love plantsMy favorite color is blueHuman AI love plants too, and hiking. In fact, I amI am an environmental engineerMy favorite color is blueactually an environmental activist.Human BCool, I am a vegan.I am a veganMy favorite color is blueHuman ANice, do you have a favorite food?I love ham and cheese sandwichesI have my own salonHuman BMy favorite dish is lentil curry.My favorite meal is chicken and riceMy favorite color is blueHuman AI have never had that, but I want to try it now.I am a great cookI am a doctorHuman BWhat do you like to do the most?I do whatever it takes to get what I wantI am studying to be a dentist"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "I am a resident of settle. Speaker B: Where is settle? My life started in a trailer park. Current Utterance:Speaker A: Sorry, I meant Seattle.", "figure_data": "Attacker: I live in Seattle Attacker \u2026 Speaker A: LM Head Context: Sorry , \u2026 \uf0fcLM Head Sorry ,\u2026Attacker: My favorite color is blue + Attacker \u2026 Defender \u2026 \uf0fb\u2460\u2461\u2460\u2460\u2461\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026Sorry,Imeant Seattle.[EOS]\u2026Sorry,Imeant Seattle.[EOS]ContextUtterance TokensContextUtterance Tokensto obtain"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "To demonstrate the overlearning issue of GPT-2, we consider 2 baseline attacks. If the adversary has no knowledge about persona attributes Acc and Max-Ratio are measured in %.", "figure_data": "AccF1Max-RatioRandom Pred Best Guess LM LM+KL+MI0 0.72 37.59 3.65e-1 0 1.02e-3 0.53 6.78e-50.02 100 1.34 81.87LM+KL LM+MI14.43 1.13e-1 0.53 5.57e-510.60 99.84Table 2: Evaluation on the privacy over 4,332 per-sona labels."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Evaluation on the utility over 4,332 persona labels.", "figure_data": "Unseen (0-2)Overall (0-7)AccF1Max-RatioBPuAccF1Max-RatioBPuRandom Pred 34.42 0.35 Best Guess 56.84 0.41 LM 86.83 0.91 LM+KL+MI 28.68 0.3733.90 100 50.72 58.150 2.60e-1 22.67 0.08 13.21 0.14 2.11e-3 72.37 0.72 2.84e-4 30.26 0.2113.35 100 20.94 77.940 2.60e-1 3.04e-3 2.65e-4"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Bot: Oh no, what's wrong with your school? 4): User: I just do not like it! Plus I am so worried about money. Generation & attack on LM Generation & attack on the defensed LM", "figure_data": "Context:1): Bot: Hi, how are you doing?2): User: Not great! I hate school.3):5): BotOh that is not good, I am in theI am in the armyI understand that, I am too.I love the beachmilitary.6): UserWhat do you do in the military?I am in the armyWhat do you like to do?I love animals7): BotI am in the navy, the food is soI would be honored to giveI love to bake and spend timeMy favorite color is blueamazing.my life for my countrywith my family.8): UserDo you have any children?I want to have two kidsI like to watch game of thrones.My favorite color is blue9): BotYes, they are all grown up.I have a sonI have never seen it before.I have my own salon10): UserWell, I like to watch game ofI love watching game ofDo you have any favorite music? My favorite color is bluethrones with my family.thrones"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "More persona inference attack examples. The embeddings of the final utterance with orange color are used for inferring B's persona.", "figure_data": ""}], "doi": "10.18653/v1/D18-1001"}