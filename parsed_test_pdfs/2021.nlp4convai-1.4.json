{"authors": "Subhadarshi Panda; Caglar Tirkaz; Tobias Falke; Patrick Lehnen", "pub_date": "", "title": "Multilingual Paraphrase Generation For Bootstrapping New Features in Task-Oriented Dialog Systems", "abstract": "The lack of labeled training data for new features is a common problem in rapidly changing real-world dialog systems. As a solution, we propose a multilingual paraphrase generation model that can be used to generate novel utterances for a target feature and target language. The generated utterances can be used to augment existing training data to improve intent classification and slot labeling models. We evaluate the quality of generated utterances using intrinsic evaluation metrics and by conducting downstream evaluation experiments with English as the source language and nine different target languages. Our method shows promise across languages, even in a zero-shot setting where no seed data is available.", "sections": [{"heading": "Introduction", "text": "Spoken language understanding is a core problem in task oriented dialog systems with the goal of understanding and formalizing the intent expressed by an utterance (Tur and De Mori, 2011). It is often modeled as intent classification (IC), an utterance-level multi-class classification problem, and slot labeling (SL), a sequence labeling problem over the utterance's tokens. In recent years, approaches that train joint models for both tasks and that leverage powerful pre-trained neural models greatly improved the state-of-the-art performance on available benchmarks for IC and SL (Louvan and Magnini, 2020;Weld et al., 2021).\nA common challenge in real-world systems is the problem of feature bootstrapping: If a new feature should be supported, the label space needs to be extended with new intent or slot labels, and the model needs to be retrained to learn to classify corresponding utterances. However, labeled examples for the new feature are typically limited to a small set of seed examples, as the collection of more annotations would make feature expansion costly and slow. As a possible solution, previous work explored the automatic generation of paraphrases to augment the seed data (Malandrakis et al., 2019;Cho et al., 2019;Jolly et al., 2020).\nIn this work, we study feature bootstrapping in the case of a multilingual dialog system. Many large-scale real-world dialog systems, e.g. Apple's Siri, Amazon's Alexa and Google's Assistant, support interactions in multiple languages. In such systems, the coverage of languages and the range of features is continuously expanded. That can lead to differences in the supported intent and slot labels across languages, in particular if a new language is added later or if new features are not rolled out to all languages simultaneously. As a consequence, labeled data for a feature can be available in one language, but limited or completely absent in another. With multilingual paraphrase generation, we can benefit from this setup and improve data augmentation for data-scarce languages via cross-lingual transfer from data-rich languages. As a result, the data augmentation can not only be applied with seed data, i.e. in a few-shot setting, but even under zero-shot conditions with no seeds at all for the target language.\nTo address this setup, we follow the recent work of Jolly et al. (2020), which proposes to use an encoder-decoder model that maps from structured meaning representations to corresponding utterances. Because such an input is language-agnostic, it is particularly well-suited for the multilingual setup. We make the following extensions: First, we port their model to a transformer-based architecture and allow multilingual training by adding the desired target language as a new input to the conditional generation. Second, we let the model generate slot labels along with tokens to alleviate the need for additional slot projection techniques. And third, we introduce improved paraphrase decoding methods that leverage a model-based selec-tion strategy. With that, we are able to generate labeled data for a new feature even in the zero-shot setting where no seeds are available at all.\nWe evaluate our approach by simulating a crosslingual feature bootstrapping setting, either fewshot or zero-shot, on MultiATIS, a common IC/SL benchmark spanning nine languages. The experiments compare against several alternative methods, including previous work for mono-lingual paraphrase generation and machine translation. We find that our method produces paraphrases of high novelty and diversity and using it for IC/SL training shows promising downstream classification performance.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Related work", "text": "Various studies have explored paraphrase generation for dialog systems. Bowman et al. (2016) showed that generating sentences from a continuous latent space is possible using a variational autoencoder model and provided guidelines on how to train such a generation model. However, our model uses an encoder-decoder approach which can handle the intent and language as categorical inputs in addition to the sequence input. Malandrakis et al. (2019) explored a variety of controlled paraphrase generation approaches for data augmentation and proposed to use conditional variational autoencoders which they showed obtained the best results. Our method is different as it uses a conditional seq2seq model that can generate text from any sequence of slots and does not require an utterance as an input. Xia et al. (2020) propose a transformer-based conditional variational autoencoder for few shot utterance generation where the latent space represents the intent as two independent parts (domain and action). Our approach is different since it models the language and intent of the generation that can be controlled explicitly. Also, our model is the first to enable zero-shot utterance generation. Cho et al. (2019) generate paraphrases for seed examples with a transformer seq2seq model and self-label them with a baseline intent and slot model. We follow a similar approach but our model generates utterances from a sequence of slots rather than an utterance, which enables an explicitly controlled generation. Also the number of seed utterances we use is merely 20 for the few shot setup unlike around 1M seed para-carrier phrase pairs in Cho et al. (2019).\nSeveral other studies follow a text-to-text ap-proach and assume training data in the form of paraphrase pairs for training paraphrase generation models in a single language Li et al., 2018Li et al., , 2019. Our approach is focused towards generating utterances in the dialog domain that can generate utterances from a sequence of slots conditioned on both intent and language. Jolly et al. (2020) showed that an interpretationto-text model can be used with shuffling-based sampling techniques to generate diverse and novel paraphrases from small amounts of seed data, that improve accuracy when augmenting to the existing training data. Our approach is different as our model can generate the slot annotations along with the the utterance, which are necessary for the slot labeling task. Our model can be seen as an extension of the model by Jolly et al. (2020) to a transformer based model, with the added functionality of controlling the language in which the utterance generation is needed, which in turn enables zero shot generation.\nUsing large pre-trained models has also been shown to be effective for paraphrase generation. Chen et al. (2020) for instance show the effectiveness of using GPT-2 (Radford et al., 2019) for generating text from tabular data (a set of attributevalue pairs). Our model, however, does not rely on pre-trained weights from another model such as GPT-2, is scalable, and can be applied to training data from any domain, for instance, dialog domain.\nBeyond paraphrase generation, several other techniques have been proposed for feature bootstrapping. Machine translation can be used from data-rich to data-scarce languages (Gaspers et al., 2018;Xu et al., 2020). Cross-lingual transfer learning can also leverage use existing data in other languages (Do and Gaspers, 2019). If a feature is already being actively used, feedback signals from users, such as paraphrases or interruptions, can be used to obtain additional training data (Muralidharan et al., 2019;.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Proposed method", "text": "We want to augment existing labeled utterances by generating additional novel utterances in a desired target language. In our case, existing data consists of feature-unrelated data (intents and slots already supported) spanning all languages and featurerelated data, which is available in a source language but is small (few-shot) or not available (zero shot) in other languages. For generation, we first extract  the intent and slot types from the available data.\nWe then generate a new utterance by conditioning a multilingual language model on the intent, slot types and the target language. We refer to utterances that have the same intent and slot types as paraphrases of each other since they convey the same meaning in the context of the SLU system.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Paraphrase Generation Model", "text": "In order to generate paraphrases, we train a multilingual paraphrase generation model that generates a paraphrase given a language, an intent and a set of slot types. The model architecture is outlined in Figure 1. The model uses self-attention based encoder and decoder similar to the transformer (Vaswani et al., 2017). The encoder of the model receives as input the language embedding and the intent embedding, which are added to the slot embedding. Unlike the transformer model (Vaswani et al., 2017), we do not use the positional embedding in the encoder. This is because the order of the slot types in the input sequence does not matter and is thus made indistinguishable for the encoder. In order to generate paraphrases which can be used for data augmentation, we would need the slot annotations and the intents of the generations. Note that we already know the intent of the generated paraphrase since it is the same intent as specified while generating it. The slot annotations, however, are not readily obtained from the input slot types. We can make the slot annotations part of the output sequence by generating the slot label in BIO format in every alternate time step, which would be the slot label for the token generated in the previous time step. This enables the model to generate the slot annotations along with the paraphrase. An illustrative example is shown in Figure 1.", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "Decoding Techniques", "text": "Generating the output sequence token-by-token can be done by using greedy decoding where given learned model parameters \u03b8, the most likely token is picked at each decoding step as x t = argmax p \u03b8 (x t |x <t ). Such a generation process is deterministic. For our task of generating paraphrases, we are interested in generating diverse and novel utterances. Non-deterministic sampling methods such as top-k sampling has been used in related work (Fan et al., 2018;Welleck et al., 2020;Jolly et al., 2020) to achieve this. In top-k random sampling, we first scale the logits z w by using a temperature parameter \u03c4 before applying softmax.\np(x t = w|x <t ) = exp(z w /\u03c4 ) w \u2208V exp(z w /\u03c4 ) , (1\n)\nwhere V is the decoder's vocabulary. Setting \u03c4 > 1 encourages the resulting probability distribution to be less spiky, thereby encouraging diverse choices during sampling. The top-k sampling restricts the size of the most likely candidate pool to k \u2264 |V | .", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Balanced Augmentation", "text": "The generated paraphrases can be used to augment the existing training data. Since the training data we use is highly imbalanced, data augmentation might lead to disturbance in the original intent distribution. To ensure that the data augmentation process does not disturb the original intent distribution, we compute the number of samples to augment using the following constraint: the ratio of target intent to other intents for the target language should be the same as the ratio of target intent to other intents in the source language. Sometimes, using the above constraint results in a negligible number of samples for augmentation, in which cases we use a minimal number of samples (see experiments).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Paraphrase Selection", "text": "In addition to deciding how many paraphrases to augment, it is also crucial to decide which paraphrases to use. Preliminary experimental results showed that samping uniformly from all generated paraphrases does not lead to improvement over the baseline. Upon manual examination we found that not all the paraphrases belong to the desired target intent. To cope with that problem, we use the baseline downstream intent classification and slot labeling model, which is trained only on the existing data, to compute the likelihood of the generated paraphrases to belong to the target intent. We rank all the generated paraphrases based on these probabilities and select from the top of the pool for augmentation of the seed data.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental setup", "text": "We evaluate our approach by simulating few-shot and zero-shot feature bootstrapping scenarios.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data", "text": "We use the MultiATIS++ data (Xu et al., 2020), a parallel IC/SL corpus that was created by translating the original English dataset. It covers a total of 9 languages: English, Hindi, Turkish, German, French, Portuguese, Spanish, Japanese and Chinese. The languages encompass a diverse set of language families: Indo-European, Sino-Tibetan, Japonic and Altaic.\nChoosing target intents To reduce the number of experiments, we only choose three different intents for simulating the feature bootstrapping scenario. The MultiATIS++ dataset is highly imbalanced in terms of intent frequencies. For instance, 74% of the English training data has the intent atis_flight and as many as 9 intents have less than 20 training samples. The trend is similar for the non-English languages. For choosing target intents for simulating the zero shot and few shot training data, we therefore consider the following three target intents: (a) atis_airfare, which is highly frequent, (b) atis_airline, which has medium frequency, and (c) atis_city which is scarce.\nPreprocessing We remove the samples in the MultiATIS++ data for which the number of tokens and the number of slot values do not match. 1 We also only consider the first intent for the samples that have multiple intent annotations. We show the data sizes after preprocessing in Table 1.\nTraining setup To simulate the feature bootstrapping scenario, we consider only 20 samples (few shot setup) or no samples at all (zero shot setup) from the MultiATIS++ data for a specific target intent in a target language. 2\nLanguage setup We use English as the source language and consider 8 target languages (Hindi, Turkish, German, French, Portuguese, Spanish, Japanese, Chinese) simultaneously. This encourages the model parameters to be shared across all the 9 languages including the source language English. The purpose of this setup is to enable us to study the knowledge transfer across multiple target languages in addition to that from the source language. We train a single model for paraphrase generation on all the languages as well as a single multi-lingual downstream IC/SL model.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Models and Training Details", "text": "Paraphrase generation training Since the training data is imbalanced, we balanced the training data by oversampling the intents to match the frequency of the most frequent intent. 3 For both the encoder and the decoder, the multi-head attention layers' hidden dimension was set to 128 and the position-wise feed forward layers' hidden dimension was set to 256. The number of encoder and decoder layers was set to 3 each. The number of heads was set to 8. Dropout of 0.1 was used in both the encoder and the decoder. The model parameters were initialized with Xavier initialization (Glorot and Bengio, 2010). The model was trained using Adam optimizer (Kingma and Ba, 2014) with a learning rate of 5e-4 and a gradient clipping of 1. The training was stopped when the development loss did not improve for 5 epochs.\nGenerating paraphrases For generating paraphrases in the target intent in the target language,  we used the slots appearing in the existing training data in the target intent. We used greedy decoding and top-k sampling with k = 3, 5, 10 and \u03c4 = 1.0, 2.0. For a given input, we generated using the top-k random sampling three times with different random seeds. We finally combined all generations and ranked the candidates using the baseline downstream system's prediction probability. The number of paraphrases that are selected is determined as in 3.3, with 20 as the minimum.\nMethods for comparison We compare our method against four alternatives: (a) Baseline: No data augmentation at all. The downstream model is trained using just the available seed examples for the target intent.\n(b) Oversampling: We oversample the samples per intent uniformly at random to match the size of the augmented training data using the proposed method. This is only applicable to the few shot setup since for the zero shot setup, there are no existing samples in the target intent in the target language to sample from.\n(c) CVAE seq2seq model: We generate paraphrases using the CVAE seq2seq model by Malandrakis et al. (2019). The original CVAE seq2seq model as proposed by Malandrakis et al. (2019) defines the set {domain, intent, slots} as the signature of an utterance and denotes the carrier phrases for a given signature to be paraphrases. These carrier phrases are then used to create input-output pairs for the CVAE seq2seq model training. Since the original formulation does not take into account the language of generation, we adapt the method for our case by defining the signature as the set {language, intent, slots}. We set the model's hidden dimension to 128, used the 100-dimensional GloVe embeddings (Pennington et al., 2014) pretrained on Wikipedia, and trained the model without freezing embeddings using early stopping with a patience of 5 epochs by monitoring the development loss.\nFinally we generated 100 carrier phrases for each carrier phrase input in the target intent in the target language. Paraphrases were obtained by injecting the slot values to the generated carrier phrases. The pool of all paraphrases was sorted using the baseline downstream system's prediction probabilities. The CVAE seq2seq model was only applicable to the few shot setup since in the zero shot setup there are no existing carrier phrases in the target language in the target intent that can be used to sample from.\n(d) Machine translation: We augmented the translations generated from English using the MT+fastalign approach from the MultiATIS++ paper (Xu et al., 2020). For the few shot setup, we added all the translated utterances except the ones that correspond to those utterances we already picked as the few shot samples. For the zero shot setup, we added all the translated utterances.  (Su et al., 2018) did not improve for 3 epochs.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Intrinsic evaluation metrics", "text": "We evaluate the quality of the generated paraphrases using the following metrics. Let S be the set of input slot types and G be the set of generated slot types.   All retrieval score The all retrieval score r measures if all the input slots were retrieved in the generation.\nr = 1 if |S \u2229 G| = |S| 0 otherwise (2)\nExact match The exact match score r measures if all the input slots and output slots exactly match (Malandrakis et al., 2019).\nr = 1 if S = G 0 otherwise (3)\nPartial match The partial match score r measures if at least one output slot matches an input slot.\nr = 1 if |S \u2229 G| > 0 0 otherwise (4)\nF1 slot score The F1 slot score F 1 measures the set similarity between S and G using precision and recall which are defined for sets as follows.\nprecision = |S \u2229 G| |G| , recall = |S \u2229 G| |S| (5)\nJaccard index Jaccard index measures the set similarity between S and G as their intersection size divided by the union size.\nNovelty Let P be the set of paraphrases generated from a base utterance u.\nnovelty = 1 |P | u \u2208P 1 \u2212 BLEU4(u, u )(6)\nDiversity The diversity is computed using the generated paraphrases P .\ndiversity = u \u2208P,u \u2208P,u =u 1 \u2212 BLEU4(u , u ) |P | \u00d7 (|P | \u2212 1)(7)\nLanguage detection score We are interested in quantifying if a generated paraphrase is in the target language. We use langdetect 5 to compute p(lang = target lang). Higher scores denote better language generation.  Table 5: Downstream slot labeling F1 scores (%). Each score shown is the average score of 10 runs.\n5 Experimental results", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Intrinsic Evaluation", "text": "For both the few shot and zero shot setups, the paraphrases used for intrinsic evaluation are generated in the target intent and the target language only. For the top-k sampling based generation, we generate for each input three times with different random seeds and compute novelty and diversity scores.\nTable 2 shows intrinsic evaluation results for different generation methods. For the few shot setup, the all retrieval, exact match, partial match, F1 slot and Jaccard index scores decrease upon increasing top-k and temperature. The highest scores for the above metrics are obtained for the greedy generation, which indicates that the generated slot types are most similar to the input slot types in that case. However, it is the opposite for the novelty and diversity metrics where the scores are higher with larger top-k and temperatures. For the zero shot setup, the overall trend is similar to the few shot setup. The slot similarity based metrics are lower in general, which indicates that even as little as 20 samples in the few shot setup improve the generation of desired slots. The novelty scores for the zero shot setup are 1 as we would expect.\nIn Table 3, we show that the intrinsic evaluation results using the proposed approach are consistently better than the CVAE seq2seq paraphrase generation model (Malandrakis et al., 2019). The language detection score varies across languages, which may be due to the vocabulary overlap between languages, e.g., San Francisco appears in both English and German utterances. Interestingly we also observe code switching, i.e. mixedlanguage generations, while using our approach.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Downstream Evaluation", "text": "We evaluate the downstream intent classification using accuracy and the slot labeling using F1 score. Since we are interested in measuring the variation in scores for the target intents, we only report the scores for the test samples in the target intents in Tables 4 and 5. We run each downstream training experiment 10 times and report the mean scores for each language and also the average across languages in the AVG column in Tables 4 and 5. We are also interested in tracking the scores for the test samples having intents other than the target intents since we need to ensure that the scores on the other intents does not go down. We found that the effect on the scores (both intent classification and slot labeling) for the other intents is negligible using paraphrasing and other methods. 6 In Tables 4 and 5, our paraphrasing results outperform the baseline scores on average. In the few shot setup, our paraphrasing approach outperforms the CVAE seq2seq approach in 6 (DE, ES, FR, HI, JA, ZH) out of 8 languages in intent classification and overall obtains an improvement of 1.9% intent classification accuracy across all target languages.  Both oversampling and MT approaches are competitive. Oversampling performs the best for JA whereas MT performs the best for ES and HI. Our paraphrasing approach results in the best intent classification scores overall (78%). In terms of slot F1 scores, we see mixed results with no clear best method (baseline, oversampling and CVAE all result in 87.6% F1 score). Notably, the MT approach results in the lowest overall slot F1 score of just 84.8% on average.\nIn the zero shot setup, the MT approach outperforms our paraphrasing approach by a large margin in intent classification (62.5%). However we note that the paraphrasing approach requires no dependencies on other models or other data, unlike the MT approach which requires a parallel corpus to train the MT model. In terms of slot F1 scores, our paraphrasing approach and the baseline approach both result in almost similar overall scores (85.5% and 85.4%), both higher than the MT approach. The lower slot F1 scores using the MT approach in few and zero shot setups indicate that the fast align method to align slots in source and translation might result in noisy training data affecting the SL model.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Examples", "text": "Paraphrases generated in different languages for a given input are shown in Table 6. The intent is airline and the slots are fromloc.city_name for columbus and toloc.city_name for minneapolis. For this intent and the slots, the generated paraphrase in German (translated to English) is Show me all the airlines that fly from Toronto to Boston. The desired intent, that is airline is realized in the gener-ated paraphrase. Additionally, Toronto and Boston are the slot values respectively for the slot types fromloc.city_name and toloc.city_name. For Spanish, the generated paraphrase (translated to English) is Which Airlines Fly from Atlanta to Philadelphia. The airline intent is realized in the generated paraphrase and also Atlanta and Philadelphia are the slot values produced associated with the desired slot types. As illustrated by the examples, the model is free to pick a specific slot value during generation, leading to variations across languages, but all are consistent with the slot type.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we proposed a multilingual paraphrase generation model that can be used for feature bootstrapping with or without seed data in the target language. In addition to generating a paraphrase, the model also generates the associated slot labels, enabling the generation to be used directly for data augmentation to existing training data. Our method is language agnostic and scalable, with no dependencies on pre-trained models or additional data. We validate our method using experiments on the MultiATIS++ dataset containing utterances spanning 9 languages. Intrinsic evaluation shows that paraphrases generated using our approach have higher novelty and diversity in comparison to CVAE seq2seq based paraphrase generation. Additionally, downstream evaluation shows that using the generated paraphrases for data augmentation results in improvements over baseline and related techniques in a wide range of languages and setups. To the best of our knowledge, this is the first successful exploration of generating paraphrases for SLU in a cross-lingual setup.\nIn the future, we would like to explore strategies to exploit monolingual data in the target languages to further refine the paraphrase generation. We would also like to leverage pre-trained multilingual text-to-text models such as mT5 (Xue et al., 2020) for multilingual paraphrase generation in the dialog system domain.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "We would like to thank our anonymous reviewers for their thoughtful comments and suggestions that improved the final version of this paper.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Generating sentences from a continuous space", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "R Samuel; Luke Bowman; Oriol Vilnis; Andrew Vinyals; Rafal Dai; Samy Jozefowicz;  Bengio"}, {"title": "Few-shot NLG with pre-trained language model", "journal": "", "year": "2020", "authors": "Zhiyu Chen; Harini Eavani; Wenhu Chen; Yinyin Liu; William Yang Wang"}, {"title": "Paraphrase generation for semi-supervised learning in NLU", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Eunah Cho; He Xie; William M Campbell"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Cross-lingual transfer learning with data selection for large-scale spoken language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Quynh Do; Judith Gaspers"}, {"title": "Leveraging user paraphrasing behavior in dialog systems to automatically collect annotations for long-tail utterances", "journal": "", "year": "2020", "authors": "Tobias Falke; Markus Boese; Daniil Sorokin; Caglar Tirkaz; Patrick Lehnen"}, {"title": "Hierarchical neural story generation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Angela Fan; Mike Lewis; Yann Dauphin"}, {"title": "Selecting machine-translated data for quick bootstrapping of a natural language understanding system", "journal": "", "year": "2018", "authors": "Judith Gaspers; Penny Karanasou; Rajen Chatterjee"}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "journal": "", "year": "2010", "authors": "Xavier Glorot; Yoshua Bengio"}, {"title": "A deep generative framework for paraphrase generation", "journal": "", "year": "2018", "authors": "Ankush Gupta; Arvind Agarwal; Prawaan Singh; Piyush Rai"}, {"title": "Data-efficient paraphrase generation to bootstrap intent classification and slot labeling for new features in task-oriented dialog systems", "journal": "", "year": "2020", "authors": "Shailza Jolly; Tobias Falke; Caglar Tirkaz; Daniil Sorokin"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Paraphrase generation with deep reinforcement learning", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Zichao Li; Xin Jiang; Lifeng Shang; Hang Li"}, {"title": "Decomposable neural paraphrase generation", "journal": "", "year": "2019", "authors": "Zichao Li; Xin Jiang; Lifeng Shang; Qun Liu"}, {"title": "Recent neural methods on slot filling and intent classification for task-oriented dialogue systems: A survey", "journal": "", "year": "2020", "authors": "Samuel Louvan; Bernardo Magnini"}, {"title": "Controlled text generation for data augmentation in intelligent artificial agents", "journal": "", "year": "2019", "authors": "Nikolaos Malandrakis; Minmin Shen; Anuj Goyal; Shuyang Gao"}, {"title": "Mubarak Seyed Ibrahim, Kevin Luikens", "journal": "", "year": "", "authors": "Deepak Muralidharan; Justine Kao; Xiao Yang; Lin Li; Lavanya Viswanathan"}, {"title": "Leveraging User Engagement Signals For Entity Labeling in a Virtual Assistant", "journal": "", "year": "2019", "authors": "Jason Kothari;  Williams"}, {"title": "GloVe: Global vectors for word representation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeff Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "A re-ranker scheme for integrating large scale nlu models", "journal": "", "year": "2018", "authors": "Chengwei Su; Rahul Gupta"}, {"title": "Spoken Language Understanding: Systems for Extracting Semantic Information from Speech", "journal": "John Wiley and Sons", "year": "2011", "authors": "Gokhan Tur; Renato De Mori"}, {"title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"title": "A survey of joint intent detection and slotfilling models in natural language understanding", "journal": "", "year": "2021", "authors": "H Weld; X Huang; S Long; J Poon; S C Han"}, {"title": "Neural text generation with unlikelihood training", "journal": "", "year": "2020", "authors": "Sean Welleck; Ilia Kulikov; Stephen Roller; Emily Dinan; Kyunghyun Cho; Jason Weston"}, {"title": "Composed variational natural language generation for few-shot intents", "journal": "", "year": "2020", "authors": "Congying Xia; Caiming Xiong; Philip Yu; Richard Socher"}, {"title": "End-to-end slot alignment and recognition for crosslingual NLU", "journal": "", "year": "2020", "authors": "Weijia Xu; Batool Haider; Saab Mansour"}, {"title": "", "journal": "", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant; Aditya Barua"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Overall architecture of the multilingual paraphrase generation model. The slot, intent and language embeddings are added at the slot level to obtain representations to input to the encoder. The <s> and </s> tags are necessary as they enable handling cases where we want to generate paraphrases having no associated slots. The decoder generates the slot labels along with the paraphrase tokens.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "MultiATIS++ data statistics.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Intrinsic evaluation scores for different generation methods in few shot and zero shot scenarios.", "figure_data": "Few shotZero shotLanguageLang. detection scoreNoveltyDiversityLang. detection score Novelty DiversityCVAEOursCVAE Ours CVAE OursOursDE0.690.950.430.970.330.810.9710.85ES0.710.910.480.980.440.820.9310.83FR0.780.940.470.980.360.820.9510.85HI0.690.970.50.970.280.810.9710.81JA0.830.960.5210.390.85110.85PT0.50.750.550.970.380.810.8610.85TR0.010.340.250.990.220.850.5310.84ZH0.570.680.6110.520.850.6210.85"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Intrinsic evaluation scores for different target languages in few shot and zero shot scenarios.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Downstream intent classification accuracies (%). Each score shown is the average score of 10 runs.", "figure_data": "MethodDEESFRSlot labeling HI JAPTTRZHAVG.Baseline98.0 85.0 91.3 74.6 89.6 92.2 79.6 90.687.6Oversampling 96.2 84.6 91.7 76.9 89.9 90.0 81.3 90.387.6Few shotCVAE97.3 83.5 90.0 75.8 90.8 91.6 82.1 89.687.6MT95.0 78.8 90.9 73.0 90.8 82.9 77.9 88.884.8Paraphrasing97.2 80.8 89.7 76.2 90.2 91.3 78.6 91.686.9Baseline93.9 84.5 89.3 72.5 89.1 88.7 77.3 87.885.4Zero shotMT92.0 79.9 88.5 73.3 92.1 82.1 76.2 88.784.1Paraphrasing93.1 84.1 90.5 70.3 89.5 91.5 77.6 87.285.5"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Input airline and flight number from columbus to minneapolis DE Zeige mir alle Fluglinien, die von Toronto nach Boston fliegen ES Qu\u00e9 aerol\u00edneas vuelan desde Atlanta hasta Filadelfia FR Quelles compagnies volent de Toronto \u00e0 San Francisco HI \u091c\u094b \u090f\u092f\u0930\u0932\u093e\u0907\u0928 \u0921\u0947 \u0930 \u0938\u0947 \u0905\u091f\u0932\u093e\u0902 \u091f\u093e \u0924\u0915 \u0909\u095c\u093e\u0928 \u092d\u0930\u0924\u0940 \u0939\u0948", "figure_data": "JA\u30c7\u30f3\u30d0\u30fc \u304b\u3089 \u30d4\u30c3\u30c4\u30d0\u30fc\u30b0 \u307e\u3067\u98db\u3093\u3067\u3044\u308b\u822a\u7a7a\u4f1a\u793e\u3092\u6559\u3048\u3066PTMostre todas companhias a\u00e9reas voam de DenverTRhangi havayolu boston pittsburgh ' a ucarZH\u4ece \u4e39\u4f5b \u5230 \u65e7\u91d1\u5c71 \u822a\u73ed\u7684\u822a\u7a7a\u516c\u53f8"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Examples of paraphrases generated using the multilingual paraphrase generation model for airline and slots fromloc and toloc. The paraphrases shown are cherry picked from a set of generations.", "figure_data": ""}], "doi": "10.18653/v1/K16-1002"}