{"authors": "Vivek Gupta; Shuo Zhang; Alakananda Vempala; Yujie He; Temma Choji; Vivek Srikumar", "pub_date": "", "title": "Right for the Right Reason: Evidence Extraction for Trustworthy Tabular Reasoning", "abstract": "When pre-trained contextualized embeddingbased models developed for unstructured data are adapted for structured tabular data, they perform admirably. However, recent probing studies show that these models use spurious correlations, and often predict inference labels by focusing on false evidence or ignoring it altogether. To study this issue, we introduce the task of Trustworthy Tabular Reasoning, where a model needs to extract evidence to be used for reasoning, in addition to predicting the label. As a case study, we propose a twostage sequential prediction approach, which includes an evidence extraction and an inference stage. First, we crowdsource evidence row labels and develop several unsupervised and supervised evidence extraction strategies for INFOTABS, a tabular NLI benchmark. Our evidence extraction strategy outperforms earlier baselines. On the downstream tabular inference task, using only the automatically extracted evidence as the premise, our approach outperforms prior benchmarks.", "sections": [{"heading": "Introduction", "text": "Reasoning on tabular or semi-structured knowledge is a fundamental challenge for today's natural language processing (NLP) systems. Two recently created tabular Natural language Inference (NLI) datasets, TabFact (Chen et al., 2020b) on Wikipedia relational tables and INFOTABS  on Wikipedia Infoboxes help study the question of inferential reasoning over semi-structured tables. Today's state-of-the-art for NLI over unstructured text uses contextualized embeddings (e.g., Devlin et al., 2019;Liu et al., 2019b). When adapted for tabular NLI by flattening tables into synthetic sentences using heuristics, these models achieve remarkable performance on the datasets.\nHowever, a recent study  demonstrates that these models fail to reason prop- * Work done during an internship at Bloomberg Peter Henderson, Supertramp 1 H1 H1: Supertramp produced 1 an album that was less than an hour long 2 . H2: Most of Breakfast in America was recorded 3 in the last month of 1978 3 . H3: Breakfast in America was released 4 the same month recording ended 4 .\nFigure 1: A semi-structured premise (the table 'Breakfast in America') example from . Hypotheses H1 are entailed by it, H2 is neither entailed nor contradictory, and H3 is a contradiction. The Relevant column shows the hypotheses that use the corresponding row. The colored text (and superscripts) in the table and hypothesis highlights relevance token level alignment.\nerly on the semi-structured inputs in many cases. For example, they can ignore relevant rows, and (a) focus on the irrelevant rows (Neeraja et al., 2021), (b) use only the hypothesis sentence (Poliak et al., 2018;Gururangan et al., 2018), or (c) knowledge acquired during pre-training (Jain et al., 2021; . In essence, they use spurious correlations between irrelevant rows, the hypothesis, and the inference label to predict labels. This paper argues that existing NLI systems optimized solely for label prediction cannot be trusted. It is not sufficient for a model to be merely Right but also Right for the Right Reasons. In particular, at least identifying the relevant elements of inputs as the 'Right Reasons' is essential for trustworthy reasoning 1 . We address this issue by introducing the task of Trustworthy Tabular Inference, where the goal is to extract relevant rows as evidence and predict inference labels.\nTo illustrate this task, consider an example from the INFOTABS dataset in Figure 1, which shows a premise table and three hypotheses. The figure also marks the rows needed to make decisions about each hypothesis, and also indicates the relevant tokens for each hypothesis. For trustworthy tabular reasoning, in addition to predicting the label ENTAIL for H1, CONTRADICT for H2 and NEU-TRAL for H3, the model should also identify the evidence rows-namely, the rows Producer and Length for hypothesis H1, Recorded for hypothesis H2, Released and Recorded for hypothesis H3.\nAs a first step, we propose a two-stage sequential prediction approach for the task, comprising of an evidence extraction stage, followed by an inference stage. In the evidence extraction stage, the model extracts the necessary information needed for the second stage. In the inference stage, the NLI model uses only the extracted evidence as the premise for the label prediction task.\nWe explore several unsupervised evidence extraction approaches for INFOTABS. Our best unsupervised evidence extraction method outperforms a previously developed baseline by 4.3%, 2.5% and 5.4% absolute score on the three test sets. For supervised evidence extraction, we annotate the IN-FOTABS training set (17K table-hypothesis pairs with 1740 unique tables) with relevant rows following the methodology of , and then train a RoBERTa LARGE classifier. The supervised model improves the evidence extraction performance by 8.7%, 10.8%, and 4.2% absolute scores on the three test sets over the unsupervised approach. Finally, for the full inference task, we demonstrate that our two-stage approach with best extraction, outperforms the earlier baseline by 1.6%, 3.8%, and 4.2% on the three test sets.\nIn summary, our contributions are as follows 2 :\n\u2022 We introduce the problem of trustworthy tabular reasoning and study a two-stage prediction approach that first extracts evidence and then predicts the NLI label. \u2022 We investigate a variety of unsupervised evidence extraction techniques. Our unsupervised approach for evidence extraction outperforms the previous methods.\n\u2022 We enrich the INFOTABS training set with evidence rows, and develop a supervised extractor that has near-human performance. \u2022 We demonstrate that our two-stage technique with best extraction outperforms all the prior benchmarks on the downstream NLI task.", "n_publication_ref": 8, "n_figure_ref": 2}, {"heading": "Task Formulation", "text": "We begin by introducing the task and the datasets we use.\nTabular Inference is a reasoning task that, like conventional NLI (Dagan et al., 2013;Bowman et al., 2015;Williams et al., 2018), asks whether a natural language hypothesis can be inferred from a tabular premise. Concretely, given a premise table T with m rows {r 1 , r 2 , . . . , r m }, and a hypothesis sentence H, the task maps them to ENTAIL (E), CONTRADICT (C) or NEUTRAL (N ). We can denote the mapping as\nf (T, H) \u2192 y (1)\nwhere, y \u2208 {E, N, C}. For example, for the tabular premise in Figure 1, the model should predict E, C, and N for the hypotheses H1, H2, and H3, respectively.\nTrustworthy Tabular Inference is a table reasoning problem that seeks not just the NLI label, but also relevant evidence from the input table that supports the label prediction. We use T R , a subset of T, to denote the relevant rows or evidence. Then, the task is defined as follows.\nf (T, H) \u2192 {T R , y}(2)\nIn our example table, this task will also indicate the evidence rows T R of Producer and Length for hypothesis H1, Recorded for hypothesis H2, and Released and Recorded for hypothesis H3.\nWhile the notion of evidence is well-defined for the ENTAIL and CONTRADICT labels, the NEU-TRAL label requires explanation. To decide on the NEUTRAL label, one must first search for relevant rows (if any), i.e., identify evidence in the premise tables. In fact, this is a causally correct sequential approach. Indeed, INFOTABS has multiple neutral hypotheses that are partly entailed by the table; if any part of a hypothesis contradicts the table, then the inference label should be CONTRADICT. For example, in our example table, the premise table indicates that the album was recorded in 1978, emphasizing the importance of the Recorded row for the hypothesis H2. For NEUTRAL examples, we refer to any such pertinent rows as evidence.\nDataset Details. There are several datasets for tabular NLI: TabFact, INFOTABS, and the Se-mEval'21 Task 9 (Wang et al., 2021b) and the FEVEROUS'21 shared task (Aly et al., 2021) datasets. We use the INFOTABS data in this work. It contains finer-grained annotation (e.g., TabFact lacks NEUTRAL hypotheses) and more complex reasoning than the others 3 .\nThe dataset consists of 23, 738 premisehypothesis pairs collected via crowdsourcing on Amazon MTurk. The tabular premises are based on 2, 540 Wikipedia Infoboxes representing twelve diverse domains, and the hypotheses are short statements paired with NLI labels. All tables contain a title followed by two columns (cf. Figure 1); the left columns are keys and the right ones are values).\nIn addition to the train and development sets, the data includes multiple test sets, some of which are adversarial: \u03b1 1 represents a standard test set that is both topically and lexically similar to the training data; \u03b1 2 hypotheses are designed to be lexically adversarial 4 ; and \u03b1 3 tables are drawn from topics unavailable in the training set. The dev and test set, comprising of 7200 table-hypothesis pairs, were recently extended with crowdsourced evidence rows . As one of our contributions, we describe the evidence rows annotation for the training set in the next Section 3.", "n_publication_ref": 5, "n_figure_ref": 2}, {"heading": "Crowdsource Evidence Extraction", "text": "This section describes the process of using Amazon MTurk to annotate evidence rows for the 16, 538 premise-hypothesis pairs that make the training set of INFOTABS. We followed the protocol of : one table and three distinct hypotheses formed a HIT. For each of the hypotheses, five annotators would select the evidence rows. We divide the tasks equally into 110 batches, each batch having 51 HITs each having three examples. To reduce bias induced by a link between the NLI label and row selection, we do not reveal the labels to the annotators. The quality control details are provided in the Appendix \u00a7B.\nIn total, we collected 81,282 annotations from 3 As per , 33% of examples in INFOTABS involve multiple rows. The dataset covers all the reasoning types present in the Glue  and SuperGlue    Choice of Semi-structured Data. The rows of an Infobox table are semantically distinct, though all connected to the title entity. Each row can be considered a separate and uniquely distinct source of information about the title entity. Because of this property, the problem of evidence extraction is well-formed as relevant row selection. The same is not valid for unstructured text, whose units of information may be tokens, phrases, sentences or entire paragraphs, and is typically unavailable (Ribeiro et al., 2020;Goel et al., 2021;Mishra et al., 2021;Yin et al., 2021).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Trustworthy Tabular Inference", "text": "Trustworthy inference has an intrinsic sequential causal structure: extract evidence first, then predict the inference label using the extracted evidence data, knowledge/common sense, and perhaps formal reasoning (Herzig et al., 2021;Paranjape et al., 2020) 7 . To operationalize this intuition, we chose a two-stage sequential approach which consists of an evidence extraction followed by the NLI classi- fication, as shown in Figure 2.\nNotation. The function f in Eq. 2 can be rewritten with functions g and h, f (.) = g(.), h \u2022 g(.), as\nf (T, H) = {g(T, H) , h (g(T, H), H)} (3)\nHere, g extracts the evidence rows T R subset of T, and h uses the extracted evidence T R and the hypothesis H to predict the inference label y, as\ng(T, H) \u2192 T R h(T R , H) \u2192 y (4)\nTo obtain f , we need to define the functions g and h, and a flexible representation of a semi-structured  . We explore unsupervised ($4.1) and supervised ($4.2) methods for the evidence row extractor g.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Unsupervised Evidence Extraction", "text": "The unsupervised approaches extract Top-K rows are based on relevance scores, where K is a hyperparameter. We use the cosine similarity between the row and the hypothesis sentence representations to score rows. We study three ways to define relevance described next.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Using Static Embeddings", "text": "Inspired by the Distracting Row Removal (DRR) heuristic of Neeraja et al. (2021), we propose DRR (Re-Rank + Top-S \u03c4 ), which uses fastText (Joulin et al., 2016;Mikolov et al., 2018) based static embeddings to measure sentence similarity. We employ three modifications to improve DRR.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Re-Rank (\u03b4):", "text": "We observed that the raw similarity scores (i.e., using only fastText) for some valid evidence rows could be low, despite exact wordlevel lexical matching with the row's key and values. We augmented the scores by \u03b4 for each exact match to incentivize precise matches.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Sparse Extraction (S):", "text": "For most instances, the number of relevant rows (K) is much lower than the total number of rows (m); most examples have only one or two relevant rows. We constrained the sparsity in the extraction by capping the value of K to S m.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dynamic Selection (\u03c4 ):", "text": "We use a threshold \u03c4 to select rows dynamically Top-K \u03c4 based on the hypothesis, rather than always selecting fixed K rows. We only select rows whose similarity (after Re-Ranking) to the hypothesis sentence representations is greater than a threshold \u03c4 . We adopt this strategy because (a) the number of rows in the premise table can vary across examples, and (b) different hypotheses may require a differing number of evidence rows.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Using Word Alignments", "text": "This approach consists of two parts (a) aligning rows and hypothesis words, and (b) then computing cosine similarity between the aligned words. Specifically, we use the SimAlign (Jalili Sabet et al., 2020) method for word-level alignment. SimAlign uses static and contextualized embeddings without parallel training data to get word alignments. Among the approaches explored by SimAlign, we use the Match (mwmf) method, which uses maximum-weight maximal matching in the bipartite weighted network formed by the word level similarity matrix. Our choice of this approach over the other greedy methods (Itermax and Argmax) is motivated by the fact that it finds the global optimum matching, while the other two do not. After alignment, we normalize the sum of cosine similarities of RoBERTa LARGE token embeddings 8 to derive the relevance score. Furthermore, because all rows use the same title, we assign title matching terms zero weight. This paper refers to this method as SimAlign (Match (mwmf)).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Using Contextualised Embeddings", "text": "The approach we saw in $4.1.2 defines rowhypothesis similarity using word alignments. As an alternative, we can directly compute similarities between the contextualised sentence embeddings of rows and the hypothesis. We explore two options here.\nSentence Transformer: We use Sentence-BERT (Reimers and Gurevych, 2019) and its variants (Reimers and Gurevych, 2020;Thakur et al., 2021;Wang et al., 2021a), which use Siamese neural networks (Koch et al., 2015;Chicco, 2021). We explore several pre-trained sentence transformers models 9 for sentence representation. These models differ in (a) the data used for pre-training, (b) the main model type and it size, and (c) the maximum sequence length.\nSimCSE: SimCSE (Gao et al., 2021) uses a contrastive learning to train sentence embeddings in both unsupervised and supervised settings. The former is trained to take an input sentence and reconstruct it using standard dropout as noise. The latter uses example pairs from the MNLI dataset (Williams et al., 2018) with entailments serving as positive examples and contradiction serving as hard negatives for contrastive learning.\nWe give the row sentences directly to SimCSE to get their embeddings. To avoid misleading matches between the hypothesis tokens and those in the premise title, we swap the hypothesis title tokens with a single token title from another randomly selected table of the same category. We then use the cosine similarity between SimCSE sentence embeddings to compute the final relevance score. We again use the sparsity and dynamic selection as earlier. In the study, we refer to this method as SimCSE (Hypo-Title-Swap + Re-rank + Top-K \u03c4 ).", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Supervised Evidence Extraction", "text": "The supervised evidence extraction procedure consists of three aspects: Dataset Construction. We use the annotated relevant row data ($3) to construct a supervised extraction training dataset. Every row in the table, paired with the hypothesis, is associated with a binary label signifying whether the row is relevant or not. As before, we use the sentences from Better Paragraph Representation (BPR) (Neeraja et al., 2021) to represent each row.\nLabel Balancing. Our annotation, and the perturbation probing analysis of   10 , show that the number of irrelevant rows can be much larger than the relevant ones for a tablehypothesis pair. Therefore, if we use all irrelevant rows from tables as negative examples, the resulting training set would be imbalanced, with about 6\u00d7 more irrelevant rows than relevant rows.\nWe investigate several label balancing strategies by sub-sampling irrelevant rows for training. We explore the following schemes: (a) taking all irrelevant rows from the table without sub-sampling (on average 6\u00d7 more irrelevant rows) referred to as Without Sample(6\u00d7), (b) randomly sampling unrelated rowsin the same proportion as relevant rows, referred to as Random Negative(1\u00d7), (c) using the unsupervised DRR (Re-Rank + Top-S \u03c4 ) method to pick the irrelevant rows that are most similar to the hypothesis, in equal proportion as the relevant rows, referred to as Hard Negative(1\u00d7), and (d) same as (c), except picking three times as many irrelevant rows, referred to as Hard Negative(3\u00d7) 11 .\nClassifier Training. We train a relevant-vsirrelevant row classifier using RoBERTa LARGE 's two sentence classifier. We use RoBERTa LARGE because of its superior performance over other models in preliminary experiments, and also the fact that it is also used for the NLI classifier.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Natural Language Inference", "text": "For the downstream NLI task, the function h is a two-sentence classifier whose inputs are T R (the rows selected by g) and the hypothesis H. We use BPR to represent T R as we did for the full table T. Since |T R | |T|, the extraction benefits larger tables (especially in \u03b1 3 set) which exceed the model's token limit.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Evaluation", "text": "Our experiments assess the efficacy of evidence extraction ($4) and its impact on the downstream NLI task by studying the following questions:\n\u2022 RQ1: What is the efficacy of unsupervised approaches for evidence extraction? ($5.2)  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "First, we briefly summarize the models used in our experiments. We investigate both unsupervised ($4.1) and supervised ($4.2) evidence extraction methods. We use only the extracted evidence as the premise for the tabular inference task ($4.3). We compare both tasks against human performance. As baselines, we use the Word Mover Distance (WMD) of  and the original DRR (Neeraja et al., 2021) with Top-4 extracted evidence rows. For DRR (Re-Rank + Top-S \u03c4 ), which uses static embeddings, we set the sparsity parameter S = 2, and the dynamic row selection parameter \u03c4 = 1.0. Our choice of S is based on the observation that in INFOTABS most (92%) instances have only one (54%) or two (38%) relevant rows. We set \u03b4 to 0.5 for all experiments.\nFor the Sentence Transformer, we used the paraphrase-mpnet-base v2 model (Reimers and Gurevych, 2019) which is a pre-trained with the mpnet-base architecture using several existing paraphrase datasets. This choice is based on performance on the development set.\nBoth the supervised and unsupervised SimCSE models use the same parameters as DRR (Re-Rank + Top-K \u03c4 ). We refer to the supervised and unsupervised variants as SimCSE-Supervised and SimCSE-Unsupervised respectively.\nFor the NLI task, we use the BPR representation over extracted evidence T R with the RoBERTa LARGE two sentence classification model. We compare the following settings: (a) WMD Top-3 from , (b) No extraction i.e. using the full premise table with the \"para\" representation from , (c) DRR Top-4, (d) DRR (Re-Rank + Top-2 (\u03c4 =1) ) for training, de-velopment and test sets, (e) training a supervised classifier with a human oracle i.e. annotated evidence extraction as discussed in $3, and using the best extraction model, i.e. supervised evidence extraction with Hard Negative (3\u00d7) for the test sets, and (f) the human oracle across the training, development, and test sets.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Results of Evidence Extraction", "text": "Unsupervised evidence extraction. For RQ1, Table 2 shows the performance of unsupervised methods. We see that the contextual embedding method, SimCSE-Supervised (Hypo-Title-Swap + Re-Rank + Top-2 (\u03c4 =1) ), performs the best. Among the static embedding cases, DRR (Re-Rank + Top-2 (\u03c4 =1) ) sees substantial performance improvement over the original DRR baseline. The alignment based approach using SimAlign underperforms, especially on the \u03b1 1 and \u03b1 2 test sets. However, its performance on the \u03b1 3 data, with out of domain and longer tables, is competitive to other methods.\nOverall, the idea of using Top-S \u03c4 , i.e., using the dynamic number of rows prediction and Re-Rank (exact-match based re-ranking) is beneficial. Previously used approaches such as DRR and WMD have low F1-score, because of poor precision. Using Re-Rank based on exact match improves the evidence extraction recall. Furthermore, introducing sparsity with Top-S \u03c4 , i.e. considering only the Top-2 rows (S=2) and dynamic row selection (\u03c4 = 1) substantially enhances evidence extraction precision. Furthermore, the zero weighting of title matches using the Hypo-Title-Swap heuristic, benefits contextualized embedding models such as SimCSE 12 .\nSimCSE-supervised (Hypo-Title-Swap + Re-Rank + Top-2 (\u03c4 =1) ) outperforms DRR (Re-Rank + Top-2 (\u03c4 =1) ) by 4.3% (\u03b1 1 ), 2.5% (\u03b1 2 ) and 5.4% (\u03b1 3 ) absolute score. Since the table domains and the NLI reasoning involved for \u03b1 1 and \u03b1 2 are sim- ilar, so is their evidence extraction performance. However, the performance of \u03b1 3 , which contains out-of-domain and longer tables (an average of thirteen rows, versus nine rows in \u03b1 1 and \u03b1 2 ) is relatively worse. The unsupervised approaches are still 12.69% (\u03b1 1 ), 13.49% (\u03b1 2 ), and 19.81% (\u03b1 3 ) behind the human performance, highlighting the challenges of the task.\nSupervised evidence extraction. For RQ2, Table 4 shows the performance of the supervised relevant row extraction approaches that use binary classifiers trained with several sampling techniques for irrelevant rows. Overall, adding supervision is advantageous 13 . Furthermore, we observe that using the unsupervised DRR technique to extract challenging irrelevant rows, i.e., Hard Negative, is more effective than random sampling. Indeed, using random negative examples as the irrelevant rows performs the worst. Not sampling (6\u00d7) or using only one irrelevant row, namely Hard Negative (1\u00d7), also underperforms. We see that employing moderate sampling, i.e., Hard Negative (3\u00d7), performs best across all test sets.\nThe best supervised model with Hard Negative (3\u00d7) sampling improves evidence extraction performance by 8.7% (\u03b1 1 ), 10.8% (\u03b1 2 ), and 4.2% (\u03b1 3 ) absolute score over the best unsupervised model, namely SimCSE-Supervised (Hypo-Title-Swap + Re-Rank + Top-2 (\u03c4 =1) ). 14 The human oracle outperforms the best supervised model by 4.13% (\u03b1 1 ) and 2.65% (\u03b1 2 ) absolute scores-a smaller gap than the best unsupervised approach. We also observe that the supervision does not benefit the \u03b1 3 set much, where the performance gap to humans is still about 15.95% (only 3.80% improvement over unsupervised approach). We suspect this is because of the distributional changes in \u03b1 3 set noted earlier. 13 We investigate \"How much supervision is adequate?\" in Appendix A. 14 Although \u03b12 is adversarial owing to label flipping, rendering the NLI task more difficult, both \u03b11 and \u03b12 have instances with the same domain tables and hypotheses with similar reasoning types, making the relevant row extraction task equally challenging. This highlights directions for future improvement via domain adaptation.  ", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Results of Natural Language Inference", "text": "For RQ3, we investigate how using only extracted evidence as a premise impacts the performance of the tabular NLI task. Table 3 shows the results. Compared to the baseline DRR, our unsupervised DRR (Re-Rank + Top-2 (\u03c4 =1) ) performs similarly for \u03b1 2 , worse by 1.12% on \u03b1 1 , and outperforms by 0.95% on \u03b1 3 .\nUsing evidence extraction with the best supervised model, Hard Negative (3\u00d7), trained on human-extracted (Oracle) rows results in 2.68% (\u03b1 1 ), 3.93% (\u03b1 2 ), and 4.04% (\u03b1 3 ) improvements against DRR. Furthermore, using human extracted (Oracle) rows for both training and testing sets outperforms all models-based extraction methods. The human oracle based evidence extraction leads to largest performance improvements of 3.05% (\u03b1 1 ), 4.39% (\u03b1 2 ), and 6.67% (\u03b1 3 ) over DRR. Overall, these findings indicate that extracting evidence is beneficial for reasoning in tabular inference task. Despite using human extracted (Oracle) rows for both training and testing, the NLI model still falls far behind human reasoning (Human NLI) . This gap exists because, in addition to extracting evidence, the INFOTABS hypotheses require inference with the evidence involving common-sense and knowledge, which the NLI component does not adequately perform.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Evidence Extraction: Human versus Model", "text": "We perform an error analysis of how well our proposed supervised extraction model (Hard Negative(3x)) performs compared to the human annotators. The model makes two types of errors: a Type I error occurs when an evidence row is marked as irrelevant, whereas Type II error occurs when an irrelevant row is marked as evidence. A Type I error will reduce the model's precision for the extraction model, whereas a Type II error will decrease the model's recall. Type I errors are especially concerning for the downstream NLI task. Since mislabeled evidence rows will be absent from the extracted premise, necessary evidence will be omitted, leading to inaccurate entailment labels. On the other hand, with Type II errors, when an irrelevant row is labeled as evidence, the model has to deal with from extra noise in the premise. However, all the required evidence remains.\nTable 5 shows a comparison of the supervised extraction (Hard Negative (3x)) approach with the ground truth human labels on all the three test sets for both error types. On the \u03b1 3 set, Type-I and Type-II errors are substantially higher than \u03b1 1 and \u03b1 2 . This highlights the fact that on the \u03b1 3 set, the model disagrees with with humans the most. Furthermore, the ratio of Type-II over Type-I errors is much higher for \u03b1 3 . This indicates that the super-  vised extraction model marks many irrelevant rows as evidence (Type-II error) for \u03b1 3 set. The out-ofdomain origin of \u03b1 3 tables, as well as their larger size, might be one explanation for this poor performance. Appendix \u00a7C provides several examples of both types of errors.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Discussion", "text": "Why Sequential Prediction? Our choice of the sequential paradigm is motivated by the observation that it enforces a causal structure. Of course, a joint or a multi-task model may make better predictions. However, these models ignore the causal relationship between evidence selection and label prediction (Herzig et al., 2021;Paranjape et al., 2020). Ideally, each row is independent and, its relevance to the hypothesis can be determined on its own. In a joint or a multi-task model that exploits correlations across rows and the final label, irrelevant rows and the NLI label, can erroneously influence row selection.\nFuture Directions. Based on the observations and discussions, we identify the future directions as follows.\n(1) Joint Causal Model. To build a joint or a multi-task model that follows the causal reasoning structure, significant changes in model architecture are required. Such a model would first identify important rows and then use them for NLI predictions, but without risking spurious correlations.\n(2) How much Supervision is Needed? As evident from our experiments, relevant row supervision improves the evidence extraction, especially on \u03b1 1 and \u03b1 2 sets compared to unsupervised extraction. But do we need full supervision for all examples? Is there any lower limit to supervision?\nWe partially answered this question in the affirmative by training the evidence extraction model with limited supervision (semi-supervised setting), but a deeper analysis is needed to understand the limits. See Appendix A for details.\n(3) Improving Zero-shot Domain Performance. As evident from \u00a75.2, the evidence extraction performance of outof-domain tables in \u03b1 3 needs further improvements, setting up a domain adaptation research question as future work. (4) Finally, inspired by Neeraja et al. (2021), we may be able to add explicit knowledge to improve evidence extraction.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Comparison with Related Work", "text": "Tabular Reasoning Many recent studies investigate various NLP tasks on semi-structured tabular data, including tabular NLI and fact verification (Chen et al., 2020b;, various question answering and semantic parsing tasks (Zhang and Balog, 2020;Pasupat and Liang, 2015;Krishnamurthy et al., 2017;Abbas et al., 2016;Sun et al., 2016;Chen et al., 2020c;Lin et al., 2020;Zayats et al., 2021;Oguz et al., 2020;Chen et al., 2021, inter alia), andtable-to-text generation (e.g., Parikh et al., 2020;Nan et al., 2021;Yoran et al., 2021;Chen et al., 2020a). Several strategies for representing Wikipedia relational tables are proposed, such as Ta-ble2vec (Deng et al., 2019), TAPAS (Herzig et al., 2020), TaBERT (Yin et al., 2020), TabStruc (Zhang et al., 2020a), TABBIE (Iida et al., 2021), TabGCN (Pramanick and Bhattacharya, 2021) and RCI (Glass et al., 2021). Yu et al. (2018;  and Neeraja et al. (2021) study pre-training for improving tabular inference.\nInterpretability and Explainability Model interpretability can either be through explanations or by identifying the evidence for the predictions (Feng et al., 2018;Serrano and Smith, 2019;Jain and Wallace, 2019;Wiegreffe and Pinter, 2019;DeYoung et al., 2020;Paranjape et al., 2020). Additionally, NLI models (e.g. Ribeiro et al., 2016Ribeiro et al., , 2018aZhao et al., 2018;Glockner et al., 2018;Naik et al., 2018;McCoy et al., 2019;Nie et al., 2019;Liu et al., 2019a) must be subjected to numerous test sets with adversarial settings. These settings can focus on various aspects of reasoning, such as perturbed premises for evidence selection , zeroshot transferability (\u03b1 3 ), counterfactual premises (Jain et al., 2021), and contrasting hypotheses \u03b1 2 . Recently, Kumar and Talukdar (2020) introduced Natural-language Inference over Label-specific Explanations (NILE), an NLI approach for generating labels and accompanying faithful explanations using auto-generated label-specific natural language explanations. Our work focuses on the extraction of label-independent evidence for correct inference, rather than on the generation of abstractive explanations for a given label.\nComparison with Shared Tasks The Se-mEval'21 Task 9 (Wang et al., 2021b) and FEVEROUS'21 shared task (Aly et al., 2021) are conceptually close to this work.\nThe SemEval task focuses on statement verification and evidence extraction using relational tables from scientific articles. In this work, we focus on item evidence extraction for non-scientific Wikipedia Infobox entity tables, proposed a twostage sequential approach, and used the INFOTABS dataset which has complex reasoning and multiple adversarial tests for robust evaluation.\nThe FEVEROUS'21 shared task focuses on verifying information using unstructured and structured evidence from open-domain Wikipedia. Our approach concerns evidence extraction from a single table rather than open-domain document, table or paragraph retrieval. Furthermore, we are only concerned with entity tables rather than relational tables or unstructured text, while the FEVEROUS data has relational tables, unstructured text, and fewer entity tables.", "n_publication_ref": 42, "n_figure_ref": 0}, {"heading": "Conclusion and Future Work", "text": "In this paper, we introduced the problem of Trustworthy Tabular Inference, where a reasoning model both extracts evidence from a table and predicts an inference label. We studied a two-stage approach, comprising an evidence extraction and an inference stage. We explored several unsupervised and supervised strategies for evidence extraction, several of which outperformed prior benchmarks. Finally, we showed that by using only extracted evidence as the premise, our approach outperforms previous baselines on the downstream tabular inference task.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A How Much Supervision is Enough for Evidence Extraction?", "text": "To investigate this, we use Hard Negative (3x) with RoBERTa LARGE model as our evidence extraction classifier, which is similar to the full supervision method. To simulate semi-supervision settings, we randomly sample 10%, 20%, 30%, 40%, and 50% example instances of the train set in an incremental fashion for model training, where we repeat the random samplings three times. Figure 3, 4, and 5 compare the average F1-score over three runs on the three test sets \u03b1 1 , \u03b1 2 and \u03b1 3 respectively.  We discovered that adding some supervision had advantages over not having any supervision. However, we also find that 20% supervision is adequate for reasonably good evidence extraction with only < 5% F1-score gap with full supervision. One key issue we observe is the lack of a visible trend due to significant variation produced by random data sub-sampling. It would be worthwhile to explore if this volatility could be reduced by strategic sampling using an unsupervised extraction model, an active learning framework, and strategic diversity maximizing sampling, which is left as future work.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "B Human Annotation Quality Control", "text": "Since many hypothesis sentences (especially those with neutral labels) require out-of-table information for inference, we introduced the option to choose out-of-table (OOT) pseudo rows, which are highlighted only when the hypothesis requires information that is not common (i.e. common sense) and missing from the table. To reduce any possible bias due to unintended associations between the NLI label and the row selections (e.g., using OOT for neutral examples), we avoid showing inference labels to the annotators 15 .\nTo assess an annotator, we compare their annotations with the majority consensus of other annotators' (four) annotations. We perform this comparison at two levels: (a) local-consensus-score on the most recent batch, and (b) cumulative-consensusscore on all batches annotated thus far.\nWe use these consensus scores to temporarily (local-consensus-score) or permanently (cumulative score) block the poor annotators from the task. We also review the annotations manually and provide feedback with more detailed instructions and personalized examples for annotators who were making mistakes due to ambiguity in the task. We give incentives to annotators who received high consensus scores. As in previous work, we removed certain annotators' annotations that have a poor consensus score (cumulative score) and published a second validation HIT to double-check each data point if necessary.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C Human vs Models Qualitative Examples", "text": "We manually inspect the Type I and Type II error instances for the supervised model and human annotation for the development set. Below, we show some of these examples where models conflict with ground-truth human annotation. We also provide a possible reason behind the model mistakes. Example III Row: The trainer of Caveat is Woody\nStephens.\nHypothesis: Caveat won more in winnings than it took to raise and train him.\nModel Prediction: Relevant Evidence Human Ground Truth: Not Relevant.\nPossible Reason: Model connects the 'raise and train' term with the trainer name which is unrelated and has no connection to overall, winning races money vs spending for animal.\nDiscussion Based on the observation from the above examples as also stated in $5.3, the model fails on many examples due to its lack of knowledge and common-sense reasoning ability. One possible solution to mitigate this is by the addition of implicit and explicit knowledge on-the-fly for evidence extraction, as done for inference task by Neeraja et al. (2021).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "D Implicit Relevance Indication", "text": "We manually examine the human-annotated evidence in the development set. We discovered the existence of several relevant phrases/tokens which implicitly indicate the presence of evidence rows. E.g. The existence of tokens such as married, husband, lesbian, and wife in hypothesis (H) is very suggestive of the row Spouse being the relevant evidence. Learning such implicit relevance-based phrases and tokens connection is easy for humans and large pre-trained supervision models. It is a challenging task for similarity-based unsupervised extraction methods. Below, we show implicit relevance, indicating token and the corresponding relevant evidence rows.\nRelevance Indicating Phrase (H) \u2192 Relevant Evidence Rows Key(T)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "The authors thank Bloomberg's AI Engineering team, especially Ketevan Tsereteli, Anju Kambadur, and Amanda Stent for helpful feedback and directions. We also appreciate the useful feedback provided by Ellen Riloff and the Utah NLP group. Additionally, we appreciate the helpful inputs provided by Atreya Ghosal, Riyaz A. Bhat, Manish Srivastava, and Maneesh Singh. Vivek Gupta acknowledges support from Bloomberg's Data Science Ph.D. Fellowship. This work was supported in part by National Science Foundation grants #1801446 (SaTC) and #1822877 (Cyberlearning). Finally, we would like to express our gratitude to the reviewing team for their insightful comments.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "", "journal": "", "year": "2020", "authors": " Wmd (gupta"}, {"title": "2020) No Extraction", "journal": "", "year": "2020", "authors": " Gupta"}, {"title": "Unsupervised DRR (Re-Rank + Top-2 (\u03c4 =1) )", "journal": "", "year": "", "authors": ""}, {"title": "DRR (Re-Rank + Top-2 (\u03c4 =1) )", "journal": "", "year": "", "authors": ""}, {"title": "Supervised Oracle Supervised (3\u00d7 Hard Negative)", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "2021", "authors": "; Human Oracle Oracle;  Gupta"}, {"title": "", "journal": "", "year": "2020", "authors": "Nli ( Human Human;  Gupta"}, {"title": "Wikiqa -a question answering system on wikipedia using freebase, dbpedia and infobox", "journal": "", "year": "2016", "authors": "M K References Faheem Abbas; M Malik; Rizwan Rashid;  Zafar"}, {"title": "Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. The fact extraction and VERification over unstructured and structured information (FEVEROUS) shared task", "journal": "Association for Computational Linguistics", "year": "", "authors": "Rami Aly; Zhijiang Guo; Michael Sejr Schlichtkrull; James Thorne"}, {"title": "A Large Annotated Corpus for Learning Natural Language Inference", "journal": "", "year": "2015", "authors": "R Samuel; Gabor Bowman; Christopher Angeli; Christopher D Potts;  Manning"}, {"title": "Open question answering over tables and text", "journal": "", "year": "2021", "authors": "Wenhu Chen; Ming-Wei Chang; Eva Schlinger; William Yang Wang; William W Cohen"}, {"title": "Logical natural language generation from open-domain tables", "journal": "", "year": "2020", "authors": "Wenhu Chen; Jianshu Chen; Yu Su; Zhiyu Chen; William Yang Wang"}, {"title": "Tabfact: A large-scale dataset for table-based fact verification", "journal": "", "year": "2020", "authors": "Wenhu Chen; Hongmin Wang; Jianshu Chen; Yunkai Zhang; Hong Wang; Shiyang Li; Xiyou Zhou; William Yang Wang"}, {"title": "HybridQA: A dataset of multi-hop question answering over tabular and textual data", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Wenhu Chen; Hanwen Zha; Zhiyu Chen; Wenhan Xiong; Hong Wang; William Yang Wang"}, {"title": "Siamese Neural Networks: An Overview", "journal": "Springer US", "year": "2021", "authors": "Davide Chicco"}, {"title": "Recognizing Textual Entailment: Models and Applications", "journal": "Synthesis Lectures on Human Language Technologies", "year": "2013", "authors": "Dan Ido Dagan; Mark Roth; Fabio Massimo Sammons;  Zanzotto"}, {"title": "Ta-ble2vec: Neural word and entity embeddings for table population and retrieval", "journal": "ACM", "year": "2019", "authors": "Li Deng; Shuo Zhang; Krisztian Balog"}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "ERASER: A benchmark to evaluate rationalized NLP models", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Jay Deyoung; Sarthak Jain; Nazneen Fatema Rajani; Eric Lehman; Caiming Xiong; Richard Socher; Byron C Wallace"}, {"title": "Understanding tables with intermediate pre-training", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Julian Eisenschlos; Syrine Krichene; Thomas M\u00fcller"}, {"title": "Pathologies of neural models make interpretations difficult", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Eric Shi Feng; Alvin Wallace; I I Grissom; Mohit Iyyer; Pedro Rodriguez; Jordan Boyd-Graber"}, {"title": "SimCSE: Simple contrastive learning of sentence embeddings", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Tianyu Gao; Xingcheng Yao; Danqi Chen"}, {"title": "Samarth Bharadwaj, and Nicolas Rodolfo Fauceglia. 2021. Capturing row and column semantics in transformer based question answering over tables", "journal": "", "year": "", "authors": "Michael Glass; Mustafa Canim; Alfio Gliozzo; Saneem Chemmengath; Vishwajeet Kumar; Rishav Chakravarti; Avi Sil; Feifei Pan"}, {"title": "Breaking NLI systems with sentences that require simple lexical inferences", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Max Glockner; Vered Shwartz; Yoav Goldberg"}, {"title": "Robustness gym: Unifying the NLP evaluation landscape", "journal": "", "year": "2021", "authors": "Karan Goel; Jesse Nazneen Fatema Rajani; Zachary Vig; Mohit Taschdjian; Christopher Bansal;  R\u00e9"}, {"title": "Is my model using the right evidence? systematic probes for examining evidence-based tabular reasoning", "journal": "CoRR", "year": "2021", "authors": "Vivek Gupta; Riyaz A Bhat; Atreya Ghosal; Manish Srivastava; Maneesh Singh; Vivek Srikumar"}, {"title": "INFOTABS: Inference on tables as semi-structured data", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Vivek Gupta; Maitrey Mehta; Pegah Nokhiz; Vivek Srikumar"}, {"title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel Schwartz; Noah A Bowman;  Smith"}, {"title": "Open domain question answering over tables via dense retrieval", "journal": "", "year": "2021", "authors": "Jonathan Herzig; Thomas M\u00fcller; Syrine Krichene; Julian Eisenschlos"}, {"title": "TaPas: Weakly supervised table parsing via pre-training", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Jonathan Herzig; Krzysztof Nowak; Thomas M\u00fcller; Francesco Piccinno; Julian Eisenschlos"}, {"title": "TABBIE: Pretrained representations of tabular data", "journal": "", "year": "2021", "authors": "Hiroshi Iida; Dung Thai; Varun Manjunatha; Mohit Iyyer"}, {"title": "Adversarial example generation with syntactically controlled paraphrase networks", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Mohit Iyyer; John Wieting; Kevin Gimpel; Luke Zettlemoyer"}, {"title": "TabPert : An effective platform for tabular perturbation", "journal": "Association for Computational Linguistics", "year": "2021-03", "authors": "Nupur Jain; Vivek Gupta; Anshul Rai; Gaurav "}, {"title": "Attention is not Explanation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Sarthak Jain; Byron C Wallace"}, {"title": "SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Masoud Jalili Sabet; Philipp Dufter; Fran\u00e7ois Yvon; Hinrich Sch\u00fctze"}, {"title": "Fasttext.zip: Compressing text classification models", "journal": "", "year": "2016", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Matthijs Douze; H\u00e9rve J\u00e9gou; Tomas Mikolov"}, {"title": "Siamese neural networks for one-shot image recognition", "journal": "", "year": "2015", "authors": "Gregory Koch; Richard Zemel; Ruslan Salakhutdinov"}, {"title": "Neural semantic parsing with type constraints for semi-structured tables", "journal": "", "year": "2017", "authors": "Jayant Krishnamurthy; Pradeep Dasigi; Matt Gardner"}, {"title": "NILE : Natural language inference with faithful natural language explanations", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Sawan Kumar; Partha Talukdar"}, {"title": "TWT: Table with written text for controlled data-to-text generation", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Tongliang Li; Lei Fang; Jian-Guang Lou; Zhoujun Li"}, {"title": "Bridging textual and tabular data for crossdomain text-to-SQL semantic parsing", "journal": "", "year": "2020", "authors": "Richard Xi Victoria Lin; Caiming Socher;  Xiong"}, {"title": "Inoculation by fine-tuning: A method for analyzing challenge datasets", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nelson F Liu; Roy Schwartz; Noah A Smith"}, {"title": "Roberta: A Robustly Optimized BERT Pretraining Approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"title": "Advances in pre-training distributed word representations", "journal": "", "year": "2018", "authors": "Tomas Mikolov; Edouard Grave; Piotr Bojanowski; Christian Puhrsch; Armand Joulin"}, {"title": "Looking beyond sentencelevel natural language inference for question answering and text summarization", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Anshuman Mishra; Dhruvesh Patel; Aparna Vijayakumar; Lorraine Xiang; Pavan Li; Kartik Kapanipathi;  Talamadupula"}, {"title": "Stress test evaluation for natural language inference", "journal": "", "year": "2018", "authors": "Aakanksha Naik; Abhilasha Ravichander; Norman Sadeh; Carolyn Rose; Graham Neubig"}, {"title": "DART: Open-domain structured data record to text generation", "journal": "", "year": "2021", "authors": "Linyong Nan; Dragomir Radev; Rui Zhang; Amrit Rau; Abhinand Sivaprasad; Chiachun Hsieh; Xiangru Tang; Aadit Vyas; Neha Verma; Pranav Krishna; Yangxiaokang Liu; Nadia Irwanto; Jessica Pan; Faiaz Rahman; Ahmad Zaidi; Mutethia Mutuma; Yasin Tarabar; Ankit Gupta; Tao Yu; Yi Chern Tan; Xi Victoria Lin; Caiming Xiong; Richard Socher; Nazneen Fatema Rajani"}, {"title": "Incorporating external knowledge to enhance tabular reasoning", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "J Neeraja; Vivek Gupta; Vivek Srikumar"}, {"title": "Analyzing compositionality-sensitivity of nli models", "journal": "", "year": "2019", "authors": "Yixin Nie; Yicheng Wang; Mohit Bansal"}, {"title": "Yashar Mehdad, and Scott Yih. 2020. Unified open-domain question answering with structured and unstructured knowledge", "journal": "", "year": "", "authors": "Barlas Oguz; Xilun Chen; Vladimir Karpukhin; Stan Peshterliev; Dmytro Okhonko; Michael Schlichtkrull; Sonal Gupta"}, {"title": "An information bottleneck approach for controlling conciseness in rationale extraction", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Mandar Bhargavi Paranjape; John Joshi; Hannaneh Thickstun; Luke Hajishirzi;  Zettlemoyer"}, {"title": "ToTTo: A controlled table-totext generation dataset", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Ankur Parikh; Xuezhi Wang; Sebastian Gehrmann; Manaal Faruqui; Bhuwan Dhingra; Diyi Yang; Dipanjan Das"}, {"title": "Compositional semantic parsing on semi-structured tables", "journal": "Long Papers", "year": "2015", "authors": "Panupong Pasupat; Percy Liang"}, {"title": "Hypothesis only baselines in natural language inference", "journal": "", "year": "2018", "authors": "Adam Poliak; Jason Naradowsky; Aparajita Haldar; Rachel Rudinger; Benjamin Van Durme"}, {"title": "Joint learning of representations for web-tables, entities and types using graph convolutional network", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Aniket Pramanick; Indrajit Bhattacharya"}, {"title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"title": "Making monolingual sentence embeddings multilingual using knowledge distillation", "journal": "", "year": "2020", "authors": "Nils Reimers; Iryna Gurevych"}, {"title": "why should i trust you?\": Explaining the predictions of any classifier", "journal": "Association for Computing Machinery", "year": "2016", "authors": "Sameer Marco Tulio Ribeiro; Carlos Singh;  Guestrin"}, {"title": "Anchors: High-precision modelagnostic explanations", "journal": "", "year": "2018", "authors": "Sameer Marco Tulio Ribeiro; Carlos Singh;  Guestrin"}, {"title": "Semantically equivalent adversarial rules for debugging NLP models", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sameer Marco Tulio Ribeiro; Carlos Singh;  Guestrin"}, {"title": "Beyond accuracy: Behavioral testing of NLP models with CheckList", "journal": "", "year": "2020", "authors": "Tongshuang Marco Tulio Ribeiro; Carlos Wu; Sameer Guestrin;  Singh"}, {"title": "Is attention interpretable?", "journal": "", "year": "2019", "authors": "Sofia Serrano; Noah A Smith"}, {"title": "Table cell search for question answering", "journal": "", "year": "2016", "authors": "Huan Sun; Hao Ma; Xiaodong He; Scott Wen-Tau Yih; Yu Su; Xifeng Yan"}, {"title": "Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks", "journal": "", "year": "2021", "authors": "Nandan Thakur; Nils Reimers; Johannes Daxenberger; Iryna Gurevych"}, {"title": "Superglue: A stickier benchmark for general-purpose language understanding systems", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Alex Wang; Yada Pruksachatkun; Nikita Nangia; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"title": "Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning", "journal": "", "year": "2021", "authors": "Kexin Wang; Nils Reimers; Iryna Gurevych"}, {"title": "SemEval-2021 task 9: Fact verification and evidence finding for tabular data in scientific documents (SEM-TAB-FACTS)", "journal": "", "year": "2021", "authors": "X R Nancy; Diwakar Wang; Marina Mahajan; Sara Danilevsky;  Rosenthal"}, {"title": "Attention is not not explanation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Sarah Wiegreffe; Yuval Pinter"}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Long Papers", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"title": "TaBERT: Pretraining for joint understanding of textual and tabular data", "journal": "", "year": "2020", "authors": "Pengcheng Yin; Graham Neubig; Yih Wen-Tau; Sebastian Riedel"}, {"title": "DocNLI: A large-scale dataset for documentlevel natural language inference", "journal": "", "year": "2021", "authors": "Wenpeng Yin; Dragomir Radev; Caiming Xiong"}, {"title": "Turning tables: Generating examples from semi-structured tables for endowing language models with reasoning skills", "journal": "", "year": "2021", "authors": "Alon Ori Yoran; Jonathan Talmor;  Berant"}, {"title": "Gra{pp}a: Grammar-augmented pre-training for table semantic parsing", "journal": "", "year": "2021", "authors": "Tao Yu; Chien-Sheng Wu; Xi Victoria Lin; Yi Chern Tan; Xinyi Yang; Dragomir Radev; Socher Richard; Caiming Xiong"}, {"title": "Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-SQL task", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Tao Yu; Rui Zhang; Kai Yang; Michihiro Yasunaga; Dongxu Wang; Zifan Li; James Ma; Irene Li; Qingning Yao; Shanelle Roman; Zilin Zhang; Dragomir Radev"}, {"title": "Representations for question answering from documents with tables and text", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Vicky Zayats; Kristina Toutanova; Mari Ostendorf"}, {"title": "Table fact verification with structure-aware transformer", "journal": "", "year": "2020", "authors": "Hongzhi Zhang; Yingyao Wang; Sirui Wang; Xuezhi Cao; Fuzheng Zhang; Zhongyuan Wang"}, {"title": "Autocompletion for data cells in relational tables", "journal": "ACM", "year": "2019", "authors": "Shuo Zhang; Krisztian Balog"}, {"title": "Web table extraction, retrieval, and augmentation: A survey", "journal": "ACM Trans. Intell. Syst. Technol", "year": "2020", "authors": "Shuo Zhang; Krisztian Balog"}, {"title": "Summarizing and exploring tabular data in conversational search", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Shuo Zhang; Zhuyun Dai; Krisztian Balog; Jamie Callan"}, {"title": "Generating natural adversarial examples", "journal": "", "year": "2018", "authors": "Zhengli Zhao; Dheeru Dua; Sameer Singh"}, {"title": "Hypothesis: International Fund for Animal Welfareifaw is a national organization focused on only North America. Model Prediction: Not Relevant Human Ground Truth: Relevant Evidence. Possible Reason: Model wasn't able to connect the clue ('worldwide') in the table row with the phrase", "journal": "", "year": "", "authors": ""}, {"title": "Hypothesis: Combined driving is a horse racing event style. Model Prediction: Not Relevant Human Ground Truth: Relevant Evidence. Possible Reason: Model wasn't able to connect the horse related equipment i.e. 'horse carriage", "journal": "", "year": "", "authors": "Iii Example;  Row"}, {"title": "Type II. Below, we show Type II error examples", "journal": "", "year": "", "authors": ""}, {"title": "Hypothesis: Dazed and Confused was directed in 1993. Model Prediction: Relevant Evidence Human Ground Truth: Not Relevant. Possible Reason: Model focuses on lexical match token 'directed' instead using entity type where premise refer for 'Person' who", "journal": "", "year": "", "authors": "I Example;  Row"}, {"title": "The spouse(s) of Celine Dion (CC OQ ChLD) is Ren\u00e9 Ang\u00e9lil, ( m", "journal": "", "year": "1994", "authors": "I I Example;  Row"}, {"title": "first started' \u2192 year active age related term, 'were of <age>', 'after <age>', 'fall', 'spring','birthday' \u2192 born 'several years', 'one month', century art \u2192 years 'co-wrote', 'written', 'writer', 'original written' \u2192 written by (novel and book) 'married', 'husband', 'lesbian', 'wives' \u2192 Spouse 'no-reward', 'monetary value', 'prize' \u2192 rewards 'earlier', 'debut', '21st century', 'early 90s', 'recording','product of years' \u2192 recorded 'lost', 'won', 'races','competition' \u2192 records (horse races, car races etc) 'sea level' \u2192 'lowest elevation', 'highest elevation', 'elevation' multi-lingual, multi-faith \u2192 'regional languages', 'official languages', 'religion', ','race or faith' 'acting', 'rapping', 'politics' \u2192 occupation 'over an', 'shortest', 'longest', 'run-time' \u2192 length 'is form <country>', 'originate', 'are an <nationality>', 'formed on <location>', 'moved to <Country>', 'descended from' \u2192 origin, descendant, parenthood etc 'city' with 'x' peoples \u2192 'metropolitan municipality' or 'metro' 'was painted with', 'mosaic', 'oil', 'water' \u2192 medium 'hung in' , 'museum', 'is stored in/at', 'wall', 'mural' \u2192 'location' 'was discontinued', 'awards' \u2192 'last awarded' 'playing bass' \u2192 'instruments' 'served', 'term', 'current charge' , 'in-charge' \u2192 'in office' 'is controlled by', 'under control' \u2192 'government' 'classical', 'pop', 'rock', 'hip-hop', 'sufi' \u2192 genre 'won more', 'in winning (race)', 'earned more than' \u2192 earnings 'Register of', 'Cultural Properties' \u2192 designated 'urban area', 'less dense' -> urban density, density 'founded by', 'has been around', 'years' \u2192 founded , introduce 'was started', 'century', 'was formed", "journal": "", "year": "1994", "authors": ""}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: High level flowchart showing our approach for evidence extraction and trustworthy tabular inference.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "(a) Dataset construction, (b) Label balancing, and (c) Classifier training.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: Extraction performance with limited supervision for \u03b13. All results are average of three random splits runs.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": Examples (%) for each Fleiss' Kappa score bucket.90 distinct annotators. Overall, twenty five annota-tors completed over 1000 tasks, corresponding to87.75 % of the examples, indicating a tail distribu-tion with the annotations. Overall, 16,248 trainingset table-hypothesis pairs were successfully labeledwith the evidence rows 5 . On average, we obtain89.49% F1-score with equal precision and recallfor annotation agreement when compared with ma-jority vote. Furthermore, 85% examples have anF1-score of >80 %, and 62% examples have anF1-score of >90 %. Around 60% examples haveeither perfect (100%) precision or recall, and 42%have both. Table 1 reports the Fleiss' Kappa score"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": " 29.42 30.13 28.23 DRR(Neeraja et al., 2021) 33.36 35.72 33.38 Static Embedding DRR (Re-Rank + Top-2 (\u03c4 =1) ) 71.49 73.28 63.41 Alignment SimAlign (Match (mwmf)) 58.98 61.53 66.33 Sentence-Transformer (paraphrase-mpnet-base-v2) 67.37 69.88 63.36 Contextualised SimCSE-Unsupervised (Hypo-Title-Swap + Re-Rank + Top-2 (\u03c4 =1) ) 72.93 70.88 66.33", "figure_data": "Unsupervised Methods\u03b11\u03b12\u03b13Baseline WMD (Embedding SimCSE-Supervised (Hypo-Title-Swap + Re-Rank + Top-2 (\u03c4 =1) )75.79 75.74 68.81HumanOracle (Gupta et al., 2021)88.62 89.23 88.56"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "F1-scores of the unsupervised evidence extraction methods.", "figure_data": "\u2022 RQ2: Is supervision beneficial? Is it help-ful to use hard negatives from unsupervisedapproaches for supervised training? ($5.2).\u2022 RQ3: Does evidence extraction enhance thedownstream tabular inference task? ($5.3)"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Tabular NLI performance with the extracted relevant rows as the premise.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "F1-scores of supervised evidence extractors.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Type-I and Type-II error of best supervised evidence extraction model.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "All results are average of three random splits runs. All results are average of three random splits runs.", "figure_data": "Semi-Supervised \u03b12 PerformanceSemi-SupervisedSupervised (100%)UnsupervisedHuman Baseline9085F1-Score807510%20%30%40%50%Percentage (%) of Annotated Data 20% 30% 40% Supervised (100%) Unsupervised Human Baseline Semi-Supervised \u03b11 Performance Figure 3: Extraction performance with limited supervision F1-Score 70 75 80 85 90 10% 50% Semi-Supervised for \u03b11. Percentage (%) of Annotated Data Figure 4: Extraction performance with limited supervision F1-Score 60 70 80 90 10% 20% 30% 40% 50% Semi-Supervised Supervised (100%) Unsupervised Human Baseline for \u03b12. Percentage (%) of Annotated Data Semi-Supervised \u03b13 Performance"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Type I. Below, we show Type I error examples. Model wasn't able to connect the concept of elevation with the perfect high elevation training ground requirement of endurance athletes. Requires common sense and knowledge.", "figure_data": "Example IRow: Colorado Springs, Colorado is a poor traininglocation for endurance athletes.Hypothesis: The elevation of Colorado Springs,Colorado is 6,035 ft (1,839 m).Model Prediction: Not RelevantHuman Ground Truth: Relevant Evidence.Possible Reason:"}], "doi": "10.18653/v1/2021.fever-1.1"}