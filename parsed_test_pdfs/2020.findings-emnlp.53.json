{"authors": "Zhao Jinman; Shawn Zhong; Xiaomin Zhang; Yingyu Liang", "pub_date": "", "title": "PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding", "abstract": "We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the spellings of words and propose a model, along with an efficient algorithm, that simultaneously models subword segmentation and computes subword-based compositional word embedding. We call the model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords for all possible segmentations based on their likelihood. Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge. Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword-level models in the quality of generated word embeddings across languages.", "sections": [{"heading": "Introduction", "text": "Word embeddings pre-trained over large texts have demonstrated benefits for many NLP tasks, especially when the task is label-deprived. However, many popular pre-trained sets of word embeddings assume fixed finite-size vocabularies 1, 2 , which hinders their ability to provide useful word representations for out-of-vocabulary (OOV) words.\nWe look into the task of generalizing word embeddings: extrapolating a set of pre-trained word embeddings to words out of its fixed vocabulary, without extra access to contextual information (e.g. example sentences or text corpus). In contrast, the more common task of learning word embeddings, or often just word embedding, is to obtain distributed representations of words directly from large unlabeled text. The motivation here is to extend the usefulness of pre-trained embeddings without expensive retraining over large text.\nThere have been works showing that contextual information can also help generalize word embeddings (for example, Khodak et al., 2018;Schick and Sch\u00fctze, 2019a,b). We here, however, focus more on the research question of how much one can achieve from just word compositions. In addition, our proposed way of utilizing word composition information can be combined with the contextual embedding algorithms to further improve the performance of generalized embeddings.\nThe hidden assumption here is that words are made of meaningful parts (cf. morphemes) and that the meaning of a word is related to the meaning of their parts. This way, humans are often able to guess the meaning of a word or term they have never seen before. For example, \"postEMNLP\" probably means \"after EMNLP\".\nDifferent models have been proposed for that task of generalizing word embeddings using word compositions, usually under the name of subword(level) models. Stratos (2017); Pinter et al. (2017); Kim et al. (2018b) model words at the character level. However, they have been surpassed by later subword-level models, probably because of putting too much burden on the models to form and discover meaningful subwords from characters. Bag-of-subwords (BoS) is a simple yet effective model for learning  and generalizing (Zhao et al., 2018) word embeddings. BoS composes a word embedding vector by taking the sum or average of the vectors of the subwords (character n-grams) that appear in the given word. However, it ignores the importance of different subwords since all of them are given the same weight. Intuitively, \"farm\" and \"land\" should be more relevant in composing representation for word \"farmland\" than some random subwords like \"armla\".\nEven more favorable would be a model's ability to discover meaningful subword segmentations on its own. Cotterell et al. (2016) bases their model over morphemes but needs help from an external morphological analyzer such as Morfessor (Virpioja et al., 2013). Sasaki et al. (2019) use trainable self-attention to combine subword vectors. While the attention implicitly facilitates interactions among subwords, there has been no explicit enforcement of mutual exclusiveness from subword segmentation, making it sometimes difficult to rule out less relevant subwords. For example, \"her\" is itself a likely subword, but is unlikely to be relevant for \"higher\" as the remaining \"hig\" is unlikely.\nWe propose the probabilistic bag-of-subwords (PBoS) model for generalizing word embedding. PBoS simultaneously models subword segmentation and composition of word representations out of subword representations. The subword segmentation part is a probabilistic model capable of handling ambiguity of subword boundaries and ranking possible segmentations based on their overall likelihood. For each segmentation, we compose a word vector as the sum of all subwords that appear in the segmentation. The final embedding vector is the expectation of the word vectors from all possible segmentations. An alternative view is that the model assigns word-specific weights to subwords based on how likely they appear as meaningful segments for the given word. Coupled with an efficient algorithm, our model is able to compose better word embedding vectors with little computational overhead compared to BoS.\nManual inspections show that PBoS is able to produce subword segmentations and subword weights that align with human intuition. Affix prediction experiment quantitatively shows that the subword weights given by PBoS are able to recover most eminent affixes of words with good accuracy.\nTo assess the quality of generated word embeddings, we evaluate with the intrinsic task of word similarity which relates to the semantics; as well as the extrinsic task of part-of-speech (POS) tagging which requires rich information to determine each word's role in a sentence. English word similarity experiment shows that PBoS improves the correlation scores over previous best models under vari-ous settings and is the only model that consistently improves over the target pre-trained embeddings. POS tagging experiment over 23 languages shows that PBoS improves accuracy compared in all but one language to the previous best models, often by a big margin.\nWe summarize our contributions as follows:\n\u2022 We propose PBoS, a subword-level word embedding model that is based on probabilistic segmentation of words into subwords, the first of its kind (Section 2). \u2022 We propose an efficient algorithm that leads to an efficient implementation 3 of PBoS with little overhead over previous much simpler BoS. (Section 3). \u2022 Manual inspection and affix prediction experiment show that PBoS is able to give reasonable subword segmentations and subword weights (Section 4.1 and 4.2).", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "\u2022 Word similarity and POS tagging experiments", "text": "show that word vectors generated by PBoS have better quality compared to previously proposed models across languages (Section 4.3 and 4.4).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "PBoS Model", "text": "Following the above intuition, in this section we describe the PBoS model in detail.\nWe first develop a model that segments a word into subword and associates each subword segmentation with a likelihood based on the meaningfulness of each subword segment. We then apply BoS over each segmentation to compose a \"segmentation vector\". The final word embedding vector is then the probabilistic expectation of all the segmentation vectors. The subword segmentation and likelihood association part require no explicit source of morphological knowledge and are tightly integrated with the word vector composition part, which in turn gives rise to an efficient algorithm that considers all possible segmentations simultaneously (Section 3). The model can be trained by fitting a set of pre-trained word embeddings.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Terminology", "text": "For a given language, let \u0393 be its alphabet. A word w of length l = |w| is a string made of l letters in \u0393, i.e. w = c 1 c 2 . . . c l \u2208 \u0393 l where w[i] = c i is the i-th letter. Let p w \u2208 [0, 1] be the probability that w appears in the language. Empirically, this is proportional to the unigram frequency of word w observed in large text in that language.\nNote that we do not assume a vocabulary. That is, we do not distinguish words from arbitrary strings made out of the alphabet. The implicit assumption here is that a \"word\" in common sense is just a string associated with high probability. In this sense, p w can also be seen as the likelihood of string w being a \"legit word\". This blurs the boundary between words and non-words, and automatically enables us to handle unseen words, alternative spellings, typos, and nonce words as normal cases.\nWe say a string s \u2208 \u0393 + is a subword of word w, denoted as s \u2286 w, if s = w[i : j] = c i . . . c j for some 1 \u2264 i \u2264 j \u2264 |w|, i.e. s is a substring of w. The probability that subword s appears in the language can then be defined as\np s \u221d w\u2208\u0393 + p w 1\u2264i\u2264j\u2264|w| 1(s = w[i : j]) (1)\nwhere 1(pred) gives 1 and otherwise 0 only if pred holds. Note that a subword s may occur more than once in the same word w. For example, subword \"ana\" occurs twice in the word \"banana\".\nA subword segmentation g of word w of length k = |g| is a tuple (s 1 , s 2 , . . . , s k ) of subwords of w, so that w is the concatenation of s 1 , . . . , s k .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Probabilistic Subword Segmentation", "text": "A subword transition graph for word w is a directed acyclic graph G w = (N w , E w ). Let l = |w|. The vertices N w = {0, . . . , l} correspond to the positions between w[i] and w[i + 1] for all i \u2208 [l \u2212 1], as well as to the beginning (vertiex 0) and the end (vertex l) of w. Each edge (i, j) \u2208 E w = {(i, j) : 0 \u2264 i < j \u2264 l} corresponds to subword w[i : j]. We use G w as a useful image for developing our model. Proposition 1. Paths from 0 to |w| in G w are in one-to-one correspondence to segmentations of w.\nProposition 2. There are 2 |w|\u22121 different possible segmentations for word w.\nEach edge (i, j) is associated with a weight p w[i:j] -how likely w[i : j] itself is a meaningful subword. We model the likelihood of segmentation g being a segmentation of w as being proportional to the product of all its subword likelihood -the\n0 1 2 3 4 5 6 h p \"h\" i p \"i\" g p \"g\" h p \"h\" e\np \"e\" r p \"r\" hi p \"hi\" gher p \"gher\" gh p \"gh\" her p \"her\" high p \"high\" er p \"er\" Figure 1: Diagram of probabilistic subwords transitions for word \"higher\". Some edges are omitted to reduce clutter. Each edge is labeled by a subword s of the word, associated with ps. Bold edges constituent a path from node 0 to 6, corresponding to the segmentation of the word into \"high\" and \"er\". transition along a path from 0 to |w| in G w :\np g|w \u221d s\u2208g p s .\n(2)\nExample. Figure 1 illustrates G w for word w = \"higher\" of length 6. Bold edges (0, 4) and (4, 6) form a path from 0 to 6, which corresponds to the segmentation (\"high\", \"er\"). The likelihood p (\"high\",\"er\")|w of this particular segmentation is proportional to p \"high\" p \"er\" -the product of weights along the path.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Probabilistic Bag-of-Subwords", "text": "Based on the above modeling of subword segmentations, we propose the Probabilistic Bag-of-Subword (PBoS) model for composing word embeddings.\nThe embedding vector w for word w is the expectation of all its segmentation-based word embedding:\nw = g\u2208Segw p g|w g (3\n)\nwhere g is the embedding for segmentation g. Given a subword segmentation g, we adopt the Bag-of-Subwords (BoS) model Zhao et al., 2018) for composing word embedding from subwords. Specifically, we apply BoS 4 over the subword segments in g:\ng = s\u2208g s, (4\n)\nwhere s is the vector representation for subword s, as if the current segmentation g is the \"golden\" segmentation of the word. In such case, we assume the meaning of the word is the combination of the meaning of all its subword segments. We maintain a look-up table S : \u0393 + \u2192 R d for all subword vectors (i.e. s = S(s)) as trainable parameters of the model, where d is the embedding dimension. Combining Eq. ( 3) and ( 4), we can compose vector representation for any word w \u2208 \u0393 + as w = g\u2208Segw p g|w s\u2208g s.\n(5)\nGiven a set of target pre-trained word vectors w * defined for words within a finite vocabulary W , our model can be trained by minimizing the mean square loss:\nminimize S 1 |W | w\u2208W w \u2212 w * 2 2 .(6)\n3 Efficient Algorithm\nPBoS simultaneously considers all possible subword segmentations and their contributions in composing word representations. However, summing over embeddings of all possible segmentations can be awfully inefficient, as simply enumerating all possible segmentations of w takes number of steps exponential to the length of w (Proposition 2). We therefore need an efficient way to compute Eq. (5).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Alternative View: Weighted Subwords", "text": "Exchanging the order of summations in Eq. ( 5) from segmentation first to subword first, we get\nw = s\u2286w a s|w s(7)\nwhere\na s|w \u221d g\u2208Segw, g s p g|w (8)\nis the weight accumulated over subword s, summing over all segmentations of w that contain s. 5 Eq. ( 7) provides an alternative view of the word vector composed by our model: a weighted sum of all the word's subword vectors. Comparing to BoS, we assign different importance a s|w , instead of a uniform weight, to each subword. a s|w can be viewed as the likelihood of subword s being a meaningful segment of the particular word w, considering both the likelihood of s itself being meaningful, and at the same time how likely the rest of the word can still be segmented into meaningful subwords.\nExample. Consider the contribution of subword s = \"gher\" in word w = \"higher\". Possible contributions only come from segmentations that contain \"higher\": g 1 = (\"h\", \"i\", \"gher\") and g 2 = (\"hi\", \"gher\"). Each segmentation g adds weight p g|w to a s|w . In this case, a \"gher\"|w will be smaller than a \"er\"|w because both p g 1 |w and p g 2 |w would be rather small.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Computing Subword Weights", "text": "Now we can efficiently compute Eq. ( 7) if we can efficiently compute a s|w . Here we present an algorithm that computes a s|w for all s \u2286 w in O(|w| 2 ) time.\nThe specific structure of the subword transition graph means that edges only go from left to right. Thus, we can split every path going through e into three parts: edges left to e, e itself and edges right to e. In terms of subwords, that is, for s = w[i : j], l = |w|, each segmentation g that contains s can be divided into three parts: segmentation g w[1 \n:i\u22121] over w[1 : i \u2212 1],\n= p s b 1,i\u22121 b j+1,l ,(10)\nwhere b i ,j = g \u2208Seg w[i :j ] s \u2208g p s . Now we can efficiently compute a s|w if we can efficiently compute b 1,i\u22121 and b j+1,l for all 1 \u2264 i, j \u2264 l. Fortunately, we can do so for b 1,i using the following recursive relation\nb 1,i = i\u22121 k=0 b 1,k p w[k+1:i](11)\nfor i = 1, . . . , l with b 1,0 = 1. Similar formulas hold for b j,l , j = 1, . . . , l with b l+1,l = 1.\nBased on this, we devise Algorithm 1 for computing a s|w for all s \u2286 w. Here we take the alternative view of our model as a weighted average of all possible subwords (thus the normalization in Line 12), and an extension to the unweighted averaging of subwords as used in Zhao et al. (2018).\nAlgorithm 1 Computing a s|w . 1: Input: Word w, p s for all s \u2286 w. l = |w|.\n2: b 1,0 \u2190 1; b l+1,l \u2190 1; 3: for i \u2190 1 . . . l do 4: b 1,i \u2190 i\u22121 k=0 p w[k+1:i] b 1,k 5: b l\u2212i+1,l \u2190 l k=l\u2212i+1 p w[l\u2212i+1:k] b k+1,l6\n: end for 7:\u00e3 s|w \u2190 0 for all s \u2286 w 8: for i \u2190 1 . . . l , j \u2190 i . . . l do 9:\u00e3 \u2190 p w[i:j] b 1,i\u22121 b j+1,l 10:\u00e3 w[i:j]|w \u2190\u00e3 w[i:j]|w +\u00e3 11: end for 12: a s|w \u2190\u00e3 s|w / s \u2286w\u00e3 s |w for all s \u2286 w 13: return a \u2022|w Time complexity As we only access each subword once in each for-statement, the number of multiplications and additions involved is bounded by the number of subword locations of w. Each of Line 4 and Line 5 take i multiplications and i \u2212 1 additions respectively. So Line 3 to Line 6 in total takes 2l 2 computations. Line 8 to Line 11 takes 3l(l+1) 2 computations. Thus, the time complexity of Algorithm 1 is O(l 2 ). Given a word of length 20, O(l 2 ) (20 2 = 400) is much better than enumerating all O(2 l ) (2 20 = 1, 048, 576) segmentations.\nUsing the setting in Section 4.3, PBoS only takes 30% more time (590 \u00b5s vs 454 \u00b5s) in average than BoS (by disabling a s|w computation) to compose a 300-dimensional word embedding vector.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We design experiments to answer two questions: Do the segmentation likelihood and subword weights computed by PBoS align with their meaningfulness? Are the word embedding vectors generated by PBoS of good quality?\nFor the former, we inspect segmentation results and subword weights (Section 4.1), and see how good they are at predicting word affixes (Section 4.2). For the latter, we evaluate the word embeddings composed by PBoS at word similarity task (Section 4.3) and part-of-speech (POS) tagging task (Section 4.4).\nDue to the page limit, we only report the most relevant settings and results in this section. Other details, including hardware, running time and detailed list of hyperparameters, can be found in Appendix A.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Subword Segmentation", "text": "In this subsection, we provide anecdotal evidence that PBoS is able to assign meaningful segmentation likelihood and subword weights.\nTable 1 shows top subword segmentations and subsequent top subwords calculated by PBoS for some example word, ranked by their likelihood and weights respectively. The calculation is based on the word frequency derived from the Google Web Trillion Word Corpus 6 . We use the same list for word probability p w throughout our experiments if not otherwise mentioned. All other settings are the same as described for PBoS in Section 4.3.\nWe can see the segmentation likelihood and subword weight favors the whole words as subword segments if the word appears in the word list, e.g. \"higher\", \"farmland\". This allows the model to closely mimic the word embeddings for frequent words that are probably part of the target vectors.\nSecond to the whole-word segmentation, or when the word is rare, e.g. \"penpineanpplepie\", \"paradichlorobenzene\", we see that PBoS gives higher likelihood to meaningful segmentations such as \"high/er\", \"farm/land\", \"pen/pineapple/pie\" and \"para/dichlorobenzene\"against other possible segmentations. 7 Subsequently, respective subword segments get higher weights among all possible subwords for the word, often by a good amount. This behavior would help PBoS to focus on meaningful subwords when composing word embedding. The fact that this can be achieved without any explicit source of morphological knowledge is itself interesting.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Affix Prediction", "text": "We quantitatively evaluate the quality of subword segmentations and subsequent subword weights by testing if our PBoS model is able to discover the most eminent word affixes. Note this has nothing to do with embeddings, so no training is involved in this experiment.\nThe affix prediction task is to predict the most eminent affix for a given word. For example, \"-able\" for \"replaceable\" and \"re-\" for \"rename\".\nModels We get affix prediction from our PBoS by taking the top-ranked subword that is one of the possible affixes. To show our advantage, we Word w\nTop segmentation g (and their p g|w ) Top subword s (and their a s|w ) higher higher (0.924), high/er (0.030), highe/r (0.027), h/igher (0.007), hig/her (0.004).\nhigher (0.852), high (0.031), er (0.029), r (0.029), highe (0.025). farmland farmland (0.971), farmlan/d (0.010), farm/land (0.006), f/armland (0.005).\nfarmland (0.941), d (0.010), farmlan (0.009), farm (0.008), land (0.007). penpineapplepie pen/pineapple/pie (0.359), pen/pineapple/pi/e (0.157), pen/pineapple/p/ie (0.101). pineapple (0.238), pen (0.186), pie (0.131), p (0.101), e (0.099). paradichlorobenzene para/dichlorobenzene (0.611), par/a/dichlorobenzene (0.110), paradi/chlorobenzene (0.083). dichlorobenzene (0.344), para (0.283), a (0.061), par (0.054), ichlorobenzene (0.042).  compare it with a BoS-style baseline affix predictor.\nBecause BoS gives same weight to all subwords in a given word, we randomly choose one of the possible affixes that appear as subword of the word.\nBenchmark We use the derivational morphology dataset 8 from Lazaridou et al. (2013). The dataset contains 7449 English words in total along with their most eminent affixes. Because no training is needed in this experiment, we use all the words for evaluation. To make the task more challenging, we drop trivial instances where there is only one possible affix appears as a subword in the given word. For example, \"rename\" is dropped because only prefix \"re-\" is present; on the other hand, \"replaceable\" is kept because both \"re-\" and \"-able\" are present. Besides excluding the trivial cases described above, we also exclude instances labeled with suffix \"-y\", because it is always included by \"-ly\" and \"-ity\". Altogether, we acquire 3546 words with 17 possible affixes for this evaluation.\nResults Affix prediction results in terms of macro precision, recall, and F1 score are shown in Table 2. We can see a definite advantage of PBoS at predicting most word affixes, where all the metrics boost about 0.4 and F1 almost doubles compared to BoS, providing evidence that PBoS is able to assign meaningful subword weights.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Word Similarity", "text": "Given that PBoS is able to produce sensible segmentation likelihood and subword weights, we now turn our focus onto the quality of the generated 8 http://marcobaroni.org/PublicData/ affix_complete_set.txt.gz word embeddings. In this section, we evaluate the word vectors' ability to capture word senses using the intrinsic task of word similarity.\nWord similarity aims to test how well word embeddings capture words' semantic similarity. The task is given as pairs of words, along with their similarity scores labeled by language speakers. Given a set of word embeddings, we compute the similarity scores induced by the cosine distance between the embedding vectors of each pair of words. The performance is then measured in Spearman's correlation \u03c1 for all pairs. Benchmarks We use WordSim353 (WS) from Finkelstein et al. (2001) which mainly consists of common words. To better access models' ability to generalize word embeddings towards OOV words, we include the rare word datasets RareWord (RW) from Luong et al. (2013) and the newer Card-660 (Card) from Pilehvar et al. (2018).\nModel Setup PBoS composes word embeddings out of subword vectors exactly as described in Section 3. Unlike some of previous models, we do not add special characters to indicate word boundaries and do not set any constraint on subword lengths. PBoS is trained 50 epochs using vanilla SGD with initial learning rate 1 and inverse square root decay.\nFor baselines, we compare against the bag-ofsubword model (BoS) from Zhao et al. (2018), and the best attention-based model (KVQ-FH) from Sasaki et al. (2019). For BoS, we use our implementation by disabling subword weight computation. For KVQ-FH, we use the implementation given in the paper. All the hyperparameters are set the same as described in the original papers. We choose to not include the character-RNN model (MIMICK) from Pinter et al. (2017), as it has been shown clearly outperformed by the two.   KVQ-FH, PBoS can often match and sometimes surpass it even though PBoS is a much simpler model with better explainability. Compared to the scores by using just the target embeddings (Table 3, All pairs), PBoS is the only model that demonstrates improvement across all cases.\nThe only case where PBoS is not doing well is with Polyglot vectors and RW benchmark. After many manual inspections, we conjecture that it may be related to the vector norm. Sometimes the vector of a relevant subword can be of a small norm, prone to be overwhelmed by less relevant subword vectors. To counter this, we tried to normalize subword vectors before summing them up into a word vector (PBoS-n). PBoS-n showed good improvement for the Polyglot RW case (25 to 32), matching the performance of the other two.\nOne may argue that PBoS has an advantage for using the most number of parameters. However, this is largely because we do not constrain the length of subwords as in BoS or use hashing as in KVQ-FH. In fact, restricting subword length and using hashing helped them for the word similarity task. We found that PBoS is insensitive to subword length constraints and decide to keep the setting simple. Despite being an interesting direction, we decide to not involve hashing in this work to focus on the effect of our unique weighting scheme.\nFaxtText Comparison Albeit targeted for a different task (training word embedding) which have access to contextual information, the popular fast-Text ) also uses a subwordlevel model. We train fastText 12 over the same English corpus on which the Polyglot target vectors are trained, in order to understand the quantitative impact of contextual information. To ensure a fair comparison, we restrict the vocabulary sizes and embedding dimensions to match those of Polyglot vectors. The word similarity scores we get for the trained fastText model are 65/40/14 for WS/RW/Card. We note the great gain for WS and RW, suggesting the helpfulness of contextual information in learning and generalizing word embeddings in the setting of small to moderate OOV rates. Surprisingly, we find that for the case of extremely high OOV rate (Card), PBoS slightly surpasses fastText, suggesting PBoS' effectiveness in generalizing embeddings to OOV words even without any help from contexts.\nMultilingual Results To evaluate and compare the effectiveness of PBoS across languages, we further train the models targeting multilingual Wikipedia2Vec vectors (Yamada et al., 2020) and evaluate them on multilingual WordSim353 and SemLex999 from Leviant and Reichart (2015) which are available in English, German, Italian and Russian. To better access the models' ability to generalize, we only take the top 10k words from the target vectors for training, which yields decent OOV rates, ranging from 23% to 84%. Detailed results can be found in Appendix Section A.3. In summary, we find 1) that PBoS surpasses KVQ-FH for English and German and is comparable to KVQ-FH for Italian; 2) that PBoS and KVQ-FH surpasses BoS for English, German and Italian; and 3) no definitive trend among the three models for Russian.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "POS Tagging", "text": "We further assess the quality of generated word embedding via the extrinsic task of POS tagging. The task is to categorize each word in a given context into a particular part of speech, e.g. noun, verb, and adjective.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "POS Tagging Model", "text": "We follow the evaluation protocol for sequential labeling used by Kiros et al. (2015) and Li et al. (2017), and use logistic regression classifier 13 as the model for POS tagging. When predicting the tag for the i-th word w i in a sentence, the input to the classifier is the concatenation of the vectors w i\u22122 , w i\u22121 , w i , w i+1 , w i+2 for the word itself and the words in its context. This setup allows a more direct evaluation of the quality of word vectors themselves, and thus gives better discriminative power. 14 Dataset We train and evaluate the performance of generated word embeddings over 23 languages at the intersection of the Polyglot (Al-Rfou ' et al., 2013) pre-trained embedding vectors 15 and the Universal Dependency (UD, v1.4 16 ) dataset. Polyglot vectors contain 64-dimensional vectors over 13 https://scikit-learn.org/0.19/ modules/generated/sklearn.linear_model. LogisticRegression.html\n14 As a side note, in our early trials, we tried to evaluate using an LSTM model following Pinter et al. (2017) and Zhao et al. (2018), but found the numbers rather similar across embedding models. One possible explanation is that LSTMs are so good at picking up contextual features that the impact of mild deviations of a single word vector is marginal.  an 100k vocabulary for each language and are used as target vectors for each of the subword-level embedding models in this experiment. For PBoS, we use the Polyglot word counts for each language as the base for subword segmentation and subword weights calculation. UD is used as the POS tagging dataset to train and test the POS tagging model. We use the default partition of training and testing set. Statistics vary from language to language. See Appendix A.4 for more details.\nResults Table 5 shows the POS tagging accuracy over the 23 languages that appear in both Polyglot and UD. All the subword-level embedding models follow the same hyperparameters as in Section 4.3. Following Sasaki et al. (2019), we tune the regularization term of the logistic regression model when evaluating KVQ-FH. Even with that, PBoS is able to achieve the best POS tagging accuracy in all but one language regardless of morphological types, OOV rates, and the number of training instances (Appendix Table 12). Particularly, PBoS improvement accuracy by greater than 0.1 for 9 languages. For the one language (Tamil) where PBoS is not the most accurate, the difference to the best is small (0.003). KVQ-FH gives no significantly more accurate predictions than BoS despite it is more complex and is the only one tuned with hyperparameters.\nOverall, Table 5 shows that the word embeddings composed by our PBoS is effective at predicting POS tags for a wide range of languages.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Popular word embedding methods, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), often assume finite-size vocabularies, giving rise to the problem of OOV words.\nFastText  attempted to alleviate the problem using subword-level model, and was followed by interests of using subword information to improve word embedding (Wieting et al., 2016;Cao and Lu, 2017;Li et al., 2017;Athiwaratkun et al., 2018;Li et al., 2018;Salle and Villavicencio, 2018;Xu et al., 2019;Zhu et al., 2019). Among them are Charagram by Wieting et al. (2016) which, albeit trained on specific downstream tasks, is similar to BoS followed by a non-linear activation, and the systematic evaluation by Zhu et al. (2019) over various choices of word composition functions and subword segmentation methods. However, all works above either pay little attention to the interaction among subwords inside a given word, or treat subword segmentation and composing word representation as separate problems.\nAnother interesting thread of works (Oshikiri, 2017;Kim et al., 2018aKim et al., , 2019 attempted to model language solely at the subword level and learn subword embeddings directly from text, providing evidence to the power of subword-level models, especially as the notion of word is thought doubtful by some linguistics (Haspelmath, 2011).\nBesides the recent interest in subwords, there have been long efforts of using morphology to improve word embedding (Luong et al., 2013;Cotterell and Sch\u00fctze, 2015;Cui et al., 2015;Soricut and Och, 2015;Bhatia et al., 2016;Cao and Rei, 2016;Xu et al., 2018;\u00dcst\u00fcn et al., 2018;Edmiston and Stratos, 2018;Chaudhary et al., 2018;Park and Shin, 2018). However, most of them require an external oracle, such as Morfessor (Creutz and Lagus, 2002;Virpioja et al., 2013), for the morphological segmentations of input words, limiting their power to the quality and availability of such segmenters. The only exception is the character LSTM model by Cao and Rei (2016), which has shown some ability to recover the morphological boundary as a byproduct of learning word embedding.\nThe most related works in generalizing pretrained word embeddings have been discussed in Section 1 and compared throughout the paper.", "n_publication_ref": 30, "n_figure_ref": 0}, {"heading": "Conclusion and Future Work", "text": "We propose PBoS model for generalizing pretrained word embeddings without contextual information. PBoS simultaneously considers all possible subword segmentations of a word and derives meaningful subword weights that lead to better composed word embeddings. Experiments on segmentation results, affix prediction, word similarity, and POS tagging over 23 languages support the claim.\nIn the future, it would be interesting to see if PBoS can also help with the task of learning word embedding, and how hashing would impact the quality of composed embedding while facilitating a more compact model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Experimental Details", "text": "Here we list the details of our experiments that are omitted in the main paper due to space constraints.\nWe run all our experiments on a machine with an 8-core Intel i7-6700 CPU @ 3.40GHz, 32GB Memory, and GeForce GTX 970 GPU.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Hyperparameters", "text": "The meaning of hyperparameters shown in Table 6, Table 7 and Table 8 as explained as follows.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Subwords", "text": "\u2022 min len: The minimum length for a subword to be considered. \u2022 max len: The maximum length for a subword to be considered. \u2022 word boundary: Whether to add special characters to annotate word boundaries.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Training", "text": "\u2022 epochs: The number of training epochs.\n\u2022 lr: Learning rate.\n\u2022 lr decay: Whether to set learning rate to be inversely proportional to the square root of the epoch number.\n\u2022 normalize semb: Whether to normalize subword embeddings before composing word embeddings. \u2022 prob eps: Default likelihood for unknown characters.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Evaluation", "text": "\u2022 C: The inverse regularization term used by the logistic regression classifier.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.2 Word Similarity", "text": "Table 6 and Table 8 show the hyperparameter values used in the word similarity experiment (Section 4.3). We transform all words in the benchmarks into lowercase, following the convention in FastText , BoS (Zhao et al., 2018), and KVQ-FH (Sasaki et al., 2019).\nDuring the evaluation, we use 0 as the similarity score for a pair of words if we cannot get word vector for one of the words, or the magnitude of the word vector is too small. This is especially the case when we evaluate the target vectors, where OOV rates can be significant.\nTable 9 lists experimental result for word similarity in greater detail.\nRegarding the training epoch time, note that KVQ-FH uses GPU and is implemented using a deep learning library 17 with underlying optimized C code, whereas our PBoS is implemented using pure Python and uses only single thread CPU. We omit the prediction time for KVQ-FH, as we found it hard to separate the actual inference time from time used for other processes such as batching and data transfer between CPU and GPU. However, we believe the overall trend should be similar as for the training time.\nOne may notice that the prediction time for BoS in Table 9 is different from what was reported at the end of Section 3. This is largely because the BoS in Table 9 has a different (smaller) set of possible subwords to consider due to the subword length limits. In Section 3, to fairly access the impact of subword weights computation, we ensure that BoS and PBoS work with the same set of possible subwords (that used by PBoS in Section 4.3), and thus observe a slight longer prediction time for BoS.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A.3 Multilingual Word Similarity", "text": "We use Wikipedia2Vec (Yamada et al., 2020) as target vectors, and keep the most frequent 10k words to get decent OOV rates. The OOV rates and word similarity scores can be found in Table 10.\nWe do not clean or filter words as we did for the English word similarity, because we found it difficult to have a consistent way of pre-processing words across languages. For PBoS, we use the word frequencies from Polyglot for subword segmentation and subword weight calculation as the same for the multilingual POS tagging experiment (Section 4.4).\nWe evaluate all the models on multilingual Word-Sim353 (mWS) and SemLex999 (mSL) from Leviant and Reichart (2015), which is available for English, German, Italian and Russian. The dataset also contains the relatedness (rel) and similarity (sim) benchmarks derived from mWS.\nWe list the results for multilingual word similarity in Table 11.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A.4 POS Tagging", "text": "Table 7 and Table 8 show the hyperparameter values used in the POS tagging experiment (Section 4.4). For the prediction model, we use the logistic regression classifier from scikit-learn 0.19.1 with the default settings.\nFollowing the observation in Sasaki et al. (2019), we tune the regularization parameter C for KVQ-FH for all values a \u00d7 10 b where a = 1, . . . , 9 and b = \u22121, 0, . . . , 4. We use the POS tagging accuracy for English as criterion, and choose C = 70.\nTable 12 lists some statistics of the datasets used in the POS tagging experiment. PBoS is able to achieve better accuracy over BoS and KVQ-FH in all languages regardless of their morphological type, OOV rate and number of training instances for POS tagging.   ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "The authors would like to thank anonymous reviewers of EMNLP for their comments. ZJ would like to thank Xuezhou Zhang, Sidharth Mudgal, Matt Du and Harit Vishwakarma for their helpful discussions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "Ntrain is the number of training instances for the POS tagging model. OOV % is the percentage of the words in the POS tagging testing set that is out of the vocabulary of the Polyglot vectors in that language. Experimental results are included for convenience.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Polyglot: Distributed word representations for multilingual NLP", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Rami Al-Rfou; ' ; Bryan Perozzi; Steven Skiena"}, {"title": "Probabilistic FastText for multi-sense word embeddings", "journal": "Long Papers", "year": "2018", "authors": "Ben Athiwaratkun; Andrew Wilson; Anima Anandkumar"}, {"title": "Morphological priors for probabilistic neural word embeddings", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Parminder Bhatia; Robert Guthrie; Jacob Eisenstein"}, {"title": "Enriching word vectors with subword information", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov"}, {"title": "A joint model for word embedding and word morphology", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Kris Cao; Marek Rei"}, {"title": "Improving word embeddings with convolutional feature learning and subword information", "journal": "", "year": "2017", "authors": "Shaosheng Cao; Wei Lu"}, {"title": "Adapting word embeddings to new languages with morphological and phonological subword representations", "journal": "", "year": "2018", "authors": "Aditi Chaudhary; Chunting Zhou; Lori Levin; Graham Neubig; David R Mortensen; Jaime Carbonell"}, {"title": "Morphological word-embeddings", "journal": "", "year": "2015", "authors": "Ryan Cotterell; Hinrich Sch\u00fctze"}, {"title": "Morphological smoothing and extrapolation of word embeddings", "journal": "Long Papers", "year": "2016", "authors": "Ryan Cotterell; Hinrich Sch\u00fctze; Jason Eisner"}, {"title": "Unsupervised discovery of morphemes", "journal": "", "year": "2002", "authors": "Mathias Creutz; Krista Lagus"}, {"title": "KNET: A general framework for learning word embedding using morphological knowledge", "journal": "ACM Trans. Inf. Syst", "year": "2015", "authors": "Qing Cui; Bin Gao; Jiang Bian; Siyu Qiu; Hanjun Dai; Tie-Yan Liu"}, {"title": "Compositional morpheme embeddings with affixes as functions and stems as arguments", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Daniel Edmiston; Karl Stratos"}, {"title": "Placing search in context: The concept revisited", "journal": "Association for Computing Machinery", "year": "2001", "authors": "Lev Finkelstein; Evgeniy Gabrilovich; Yossi Matias; Ehud Rivlin; Zach Solan; Gadi Wolfman; Eytan Ruppin"}, {"title": "The indeterminacy of word segmentation and the nature of morphology and syntax. Folia linguistica", "journal": "", "year": "2011", "authors": "Martin Haspelmath"}, {"title": "Bag of tricks for efficient text classification", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Tomas Mikolov"}, {"title": "A la carte embedding: Cheap but effective induction of semantic feature vectors", "journal": "", "year": "2018", "authors": "Mikhail Khodak; Nikunj Saunshi; Yingyu Liang; Tengyu Ma; Brandon Stewart; Sanjeev Arora"}, {"title": "Word-like character n-gram embedding", "journal": "", "year": "2018", "authors": "Geewook Kim; Kazuki Fukui; Hidetoshi Shimodaira"}, {"title": "Segmentation-free compositional ngram embedding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Geewook Kim; Kazuki Fukui; Hidetoshi Shimodaira"}, {"title": "Learning to generate word representations using subword information", "journal": "", "year": "2018", "authors": "Yeachan Kim; Kang-Min Kim; Ji-Min Lee; Sangkeun Lee"}, {"title": "Skip-thought vectors", "journal": "Curran Associates, Inc", "year": "2015", "authors": "Ryan Kiros; Yukun Zhu; R Russ; Richard Salakhutdinov; Raquel Zemel; Antonio Urtasun; Sanja Torralba;  Fidler"}, {"title": "Compositional-ly derived representations of morphologically complex words in distributional semantics", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Angeliki Lazaridou; Marco Marelli; Roberto Zamparelli; Marco Baroni"}, {"title": "Separated by an un-common language: Towards judgment language informed vector space modeling", "journal": "", "year": "2015", "authors": "Ira Leviant; Roi Reichart"}, {"title": "Subword-level composition functions for learning word embeddings", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Bofang Li; Aleksandr Drozd; Tao Liu; Xiaoyong Du"}, {"title": "Investigating different syntactic context types and context representations for learning word embeddings", "journal": "", "year": "2017", "authors": "Bofang Li; Tao Liu; Zhe Zhao; Buzhou Tang; Aleksandr Drozd; Anna Rogers; Xiaoyong Du"}, {"title": "Better word representations with recursive neural networks for morphology", "journal": "", "year": "2013", "authors": "Thang Luong; Richard Socher; Christopher Manning"}, {"title": "Distributed representations of words and phrases and their compositionality", "journal": "Curran Associates, Inc", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"title": "Segmentation-free word embedding for unsegmented languages", "journal": "", "year": "2017", "authors": "Takamasa Oshikiri"}, {"title": "Grapheme-level awareness in word embeddings for morphologically rich languages", "journal": "", "year": "2018", "authors": "Suzi Park; Hyopil Shin"}, {"title": "Glove: Global vectors for word representation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"title": "Card-660: Cambridge rare word dataset -a reliable benchmark for infrequent word representation models", "journal": "", "year": "2018", "authors": "Dimitri Mohammad Taher Pilehvar; Victor Kartsaklis; Nigel Prokhorov;  Collier"}, {"title": "Mimicking word embeddings using subword RNNs", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Yuval Pinter; Robert Guthrie; Jacob Eisenstein"}, {"title": "Incorporating subword information into matrix factorization word embeddings", "journal": "", "year": "2018", "authors": "Alexandre Salle; Aline Villavicencio"}, {"title": "Subword-based Compact Reconstruction of Word Embeddings", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Shota Sasaki; Jun Suzuki; Kentaro Inui"}, {"title": "Attentive mimicking: Better word embeddings by attending to informative contexts", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"title": "Learning semantic representations for novel words: Leveraging both form and context", "journal": "", "year": "2019", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"title": "Unsupervised morphology induction using word embeddings", "journal": "", "year": "2015", "authors": "Radu Soricut; Franz Och"}, {"title": "Reconstruction of word embeddings from sub-word parameters", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Karl Stratos"}, {"title": "Characters or morphemes: How to represent words?", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Murathan Ahmet\u00fcst\u00fcn; Burcu Kurfal\u0131;  Can"}, {"title": "Morfessor 2.0: Python implementation and extensions for morfessor baseline. D4 julkaistu kehitt\u00e4mis-tai tutkimusraportti taiselvitys", "journal": "", "year": "2013", "authors": "Sami Virpioja; Peter Smit; Stig-Arne Gr\u00f6nroos; Mikko Kurimo"}, {"title": "Charagram: Embedding words and sentences via character n-grams", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "John Wieting; Mohit Bansal; Kevin Gimpel; Karen Livescu"}, {"title": "Incorporating latent meanings of morphological compositions to enhance word embeddings", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yang Xu; Jiawei Liu; Wei Yang; Liusheng Huang"}, {"title": "Treat the word as a whole or look inside? subword embeddings model language change and typology", "journal": "", "year": "2019", "authors": "Yang Xu; Jiasheng Zhang; David Reitter"}, {"title": "Wikipedia2vec: An efficient toolkit for learning and visualizing the embeddings of words and entities from wikipedia", "journal": "", "year": "2020", "authors": "Ikuya Yamada; Akari Asai; Jin Sakuma; Hiroyuki Shindo; Hideaki Takeda; Yoshiyasu Takefuji; Yuji Matsumoto"}, {"title": "Generalizing word embeddings using bag of subwords", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jinman Zhao; Sidharth Mudgal; Yingyu Liang"}, {"title": "A systematic study of leveraging subword information for learning word representations", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yi Zhu; Ivan Vuli\u0107; Anna Korhonen"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "subword s itself, and segmentation g w[j+1:l] over w[j + 1 : l]. Based on this, we can rewrite Eq. (8) as a s|w \u221d", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Top segmentations and subword weights by PBoS for some example words", "figure_data": "Model Precision RecallF1BoS0.4930.465 0.425PBoS0.8610.874 0.829"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Affix prediction results based on subword weights.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Target vectors statistics and word similarity performance measured in Spearman's \u03c1 \u00d7 100.", "figure_data": "Model# Param WS RW CardTarget: PolyglotBoS29.8M34346KVQ-FH7.8M313212PBoS37.8M412515Target: Google NewsBoS162.7M614811KVQ-FH36.2M644921PBoS315.7M684925"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "Dimension ofword vectors, number of words in target vectors aresummarized in Table 3, along with their word sim-ilarity scores and OOV rate over the benchmarks.As we can see, both pre-trained embeddings yielddecent correlations with human-labeled word sim-ilarity. However, the scores drop significantly asthe OOV rate goes up. Polyglot vectors yield lowerscores probably due to their smaller dimension andsmaller token coverage.Results Word similarity results of the threesubword-level models are summarized in Ta-ble 4. 11 PBoS achieves scores better than or at leastcomparable to BoS and KVQ-FH in all but one ofthe six combinations of target vectors and wordsimilarity benchmarks. Viewed as an extension toBoS, PBoS is in majority cases better than BoS, of-ten by a good margin, suggesting the effectivenessof the subword weighting scheme. Compared to9 https://polyglot.readthedocs.io/en/latest/Download.html10 https://code.google.com/archive/p/word2vec/11 We regard training and prediction time as less of a concernhere as all the three models are able to finish a training epochin under a minute. Details and discussions can be found inAppendix A.2."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "POS tagging accuracy over 23 languages. In parentheses are the gains to the best of BoS and KVQ-FH.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "mWS mWS-rel mWS-sim mSL English: 10k tokens \u00d7 300 dim", "figure_data": "IV pairs65567126All pairs2936247OOV27%23%30% 36%Germen: 10k tokens \u00d7 300 dimIV pairs58506035All pairs81477OOV54%52%55% 67%Italian: 10k tokens \u00d7 300 dimIV pairs52505424All pairs112082OOV48%45%50% 54%Russian: 10k tokens \u00d7 300 dimIV pairs47324812All pairs1429OOV73%69%75% 84%"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Multilingual target vectors statistics and word similarity performance measured in Spearman's \u03c1 \u00d7 100.", "figure_data": "Model# Param mWSmWS mWS mSL rel simEnglishBoS20.2M32293423KVQ-FH36.0M36413413PBoS30.4M53446122GermenBoS21.3M32243713KVQ-FH36.0M18191914PBoS45.8M38303812ItalianBoS18.8M8-21725KVQ-FH36.0M1922219PBoS35.7M25162713RussianBoS20.0M20152114KVQ-FH36.0M1911249PBoS35.6M18122212"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Multilingual word similarity performance of subword-level models measured in Spearman's \u03c1 \u00d7 100.", "figure_data": ""}], "doi": "10.18653/v1/P18-1001"}