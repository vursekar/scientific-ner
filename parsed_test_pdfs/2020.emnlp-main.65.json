{"authors": "Hyunwoo Kim; Byeongchang Kim; Gunhee Kim", "pub_date": "", "title": "Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness", "abstract": "We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the bestperforming persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al.,  2018)  dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve contextconsistency beyond persona in dialogues.", "sections": [{"heading": "Introduction", "text": "In the study of dialogue agents, consistency has been a long-standing issue. To resolve this, much research has been conducted to endow dialogue agents with personas. Li et al. (2016) propose to encode persona in embeddings and Zhang et al. (2018) introduce a persona-conditioned dialogue dataset. On top of these works, many efforts have been made to improve consistency.\nIn spite of such recent significant progress, there is much room for improving persona-based dialogue agents. We observe that even the best performing persona-based generative models (See et al., 2019;Wolf et al., 2019b; I like to stay at home.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Interlocutor", "text": "Literal Agent: !", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "[Inconsistent]", "text": "I like going outside.\nInterlocutor Self-Conscious Agent: \"", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "[Consistent]", "text": "I like going outside. I love Disneyland! I go there every week.\n'Will I sound like me?' Figure 1: Illustration of the consistency issue in dialogue. While a literal dialogue agent (S 0 ) fails to deliver a consistent persona, our self-conscious agent (S 1 ) does so, by modeling an imaginary listener. Icons are designed by Nhor Phai and Vincent Le Moign.\nare highly insensitive to contradictory words, and thus fail to deliver consistent persona to the interlocutor (Figure 1). Also, extra modules other than the generative model is often required for improving consistency. Recent works on consistency in persona-based dialogue actively adopt the NLIbased approach (Welleck et al., 2019;Song et al., 2019;Li et al., 2020;Song et al., 2020), which have the following prerequisites. First, they require labeled pairs of persona sentences and dialogue utterances with three categories: entailment, neutral, and contradiction. Next, methods with NLI models for rating the agent's consistency also need to train them separately with those labels.\nIn this work, we step back from this NLI-based supervised approach and ponder: how do humans maintain consistency? We humans never learn how to be consistent. Instead, we have an innate drive for consistency to hold our beliefs and behavior in harmony (Festinger, 1962). If so, how do we know we are consistent or not? We do not ask others. We ask ourselves by predicting how we are perceived by others. Public self-consciousness is this awareness of the self as a social object that can be observed and evaluated by others (Fenigstein et al., 1975). We particularly emphasize that public self-consciousness is not equivalent to the philosophical self-consciousness (or self-awareness) 1 . Simply put, public self-consciousness is the concern about how oneself will be perceived by others, as opposed to the philosophical state of being conscious of self-existence.\nAccording to Doherty and Schlenker (1991), people with high public self-consciousness tend to act more consistent with known information about themselves. They care deeply about how others will evaluate them and have a strong tendency to avoid negative evaluations (Fenigstein et al., 1975). Since inconsistency is condemned by others, one who has high public self-consciousness will try more to maintain consistency. In order to predict how we are perceived, we rely on abstract models of others (Gopnik and Wellman, 1992) and simulate others' reactions based on imagination (Hassabis et al., 2013). Inspired by this, our intuition is that self-consciousness through an imaginary listener will let dialogue agents better maintain consistency.\nModeling a listener has been one of the main topics in computational pragmatics. Our work extends this long line of work in cognitive science by making use of the Bayesian Rational Speech Acts framework (Frank and Goodman, 2012), which has been originally applied to improving informativeness of referring expressions. Since personas ought to express who we are, we adopt this framework for dialogue agents by regarding personas as targets that should be conveyed to the interlocutor. As the agent tries to generate tokens that help the imaginary listener identify the agent's persona, it can lastly generate more consistent utterances.\nIn summary, we take inspiration from social cognition and pragmatics to endow generative agents with self-consciousness, which makes them imagine the listener's reaction and incorporate it to the generation process for improving consistency. Our major contributions can be outlined as follows:\n(1) We propose an orthogonally applicable approach for any persona-based generative agents to improve consistency without the use of additional consistency labels and training. Moreover, it is even generalizable to improve context-consistency beyond persona in dialogue.\n(2) We extend the Rational Speech Acts framework (Frank and Goodman, 2012) with two new technical features: (i) a learning method for distractor selection (e.g. other samples different from the given target (Andreas and Klein, 2016)), which has been usually done manually or randomly, and (ii) a different update for the listener's world prior that better preserves information of previous states.\n(3) Our approach improves consistency of three recent generative agents (See et al., 2019;Wolf et al., 2019b; over Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018). Along with large reduction in contradiction, the utterance accuracy significantly increases too.", "n_publication_ref": 18, "n_figure_ref": 2}, {"heading": "Related Work", "text": "Persona & Consistency in Dialogue. Li et al. (2016) learn personas in embeddings. Zhang et al. (2018) release the PersonaChat dataset, a chitchat dialogue set involving two interlocutors each playing their given persona. Madotto et al. (2019) use meta-learning to adapt to new personas with few dialogue samples.  use reinforcement learning to enhance mutual persona perception.\nRecent works use extra modules or NLI labels to improve consistency. Shum et al. (2019) fill generated templates, and rank with a language model.  use self-supervised feature extractors for generation. Welleck et al. (2019) annotate NLI labels to the PersonaChat dataset. They train an NLI model and run pairwise comparison between candidates and persona to compute contradiction scores. The NLI approach is applied for coherence evaluation (Dziri et al., 2019), rewards to reinforcement learning agents (Song et al., 2019), finding inconsistent words (Song et al., 2020), and unlikelihood training (Li et al., 2020). They require NLI labels on the target dialogue dataset; otherwise, sharp decrease in performance is observed, due to mismatch of data distribution (Welleck et al., 2019). Such dataset-specific NLI annotations and training NLI models can be costly and time-consuming.\nCompared to previous methods, the novelty of our approach is to improve consistency without NLI labels and extra modules.\nPragmatics. Our approach belongs to the general family of Bayesian Rational Speech Acts   (Andreas and Klein, 2016), image captioning (Mao et al., 2016;Vedantam et al., 2017;Cohn-Gordon et al., 2018), instruction following (Fried et al., 2017), navigating (Fried et al., 2018), translation (Cohn-Gordon and Goodman, 2019), summarization (Shen et al., 2019) and referring expression generation (Zarrie\u00df and Schlangen, 2019). However, its application to the dialogue domain remains understudied. In this work, we explore how the RSA framework can be adopted in dialogue agents to alleviate the inconsistency problem. Also, we further extend the framework by making the distractor selection as a learnable process.  (Zhang et al., 2018). They collect entailing and contradictory utterances to the given persona, and release an evaluation set comprised of dialogues each with 31 utterance candidates: 10 entailing, 10 neutral, and 10 contradictory utterances with 1 ground-truth (GT) utterance. On this evaluation set, we run three recent models (See et al., 2019;Wolf et al., 2019b  et al., 2020) that achieve the best performance on PersonaChat. We report four ranking metrics following Welleck et al. (2019): Hits@1, Entail@1, Neutral@1 and Contradict@1. Each metric is the proportion of GT, entailing, neutral and contradictory utterances in the top-1 candidates returned by the model, respectively. The models rank the candidates by perplexity scores.\nFigure 2 shows that all three models select contradictory candidates much more often than the GT utterances (see further results in Table 3). Though models are conditioned on a given persona, they are highly insensitive to contradictions.", "n_publication_ref": 23, "n_figure_ref": 1}, {"heading": "Analysis of Contradict@1 Utterances", "text": "To investigate why insensitivity to contradiction prevails in the state-of-the-art models, we further analyze the contradictory utterances returned by the models (Contradict@1-Utt), comparing with the GT utterances and the top-ranked entailing candidates (Top Entail-Utt). Table 1 reports language metrics between the selected candidates and the given persona sentences using SPICE (Anderson et al., 2016) and ROUGE (Lin, 2004). SPICE metric measures semantic similarity and ROUGE metric measures n-gram overlaps between two sentences. Contradict@1-Utt shows lower SPICE scores and higher ROUGE scores than other utterances, implying that it may be different in semantics but similar in syntax to the given persona.\nTo take a closer look, we extract the contradicting words from Contradict@1-Utt and their counterparts from GT utterances to compare their average perplexity scores. In the Dialogue NLI dataset, every utterance is labeled with a triple (entity 1 , relation, entity 2 ), such as \"I just like to listen to rock music\" with (i, like music, rock).\nBy construction, Contradict@1-Utt must contain words that are contradictory to the GT utterance and the given persona. The perplexity scores of contradictory words (106.7) were considerably lower than those of the counterparts in GT utterances (280.1). Table 2 shows an example of such dialogue instance with perplexity per word. If properly conditioned with the given persona, models should show lower perplexity for the words in the persona. However, their perplexity scores are significantly higher than those of contradictory words. It reveals that models behave more as a plain language model rather than as a persona-conditioned model. Thus, guarantee of consistency for each word generation step is required for persona-based dialogue agents to resolve such issue.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Approach", "text": "We introduce how to endow dialogue agents with public self-consciousness, which helps them keep consistency in mind at each generation step by reflecting an imaginary listener's distribution over personas. Since the imaginary listener arises from the plain dialogue-agent, separate training is not needed. Figure 3 illustrates its overall structure.\nWe present how to model public selfconsciousness using the Rational Speech Acts (RSA) framework (Frank and Goodman, 2012) in Section 4.1. We then discuss learning of distractor selection as our major novelty for the RSA in Section 4.2.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Modeling the Public Self-Consciousness", "text": "We seek to build a dialogue agent who is selfconscious about its consistency without the need for training on NLI labels or rating consistency with NLI models. Given that modeling the interactions between listener and speaker is a main topic in pragmatics, we take advantage of the RSA framework (Frank and Goodman, 2012). It treats language use as a recursive process where probabilistic speaker and listener reason about each other's intentions in a Bayesian fashion. To apply the framework to sequence generation for dialogues, we extend the incremental approach proposed for image captioning (Cohn-Gordon et al., 2018).\nTo generate an utterance, the agent computes the distribution of every next token u t at timestep t in Bayesian fashion as follows.\nBase Speaker S 0 . We first assume persona i is given to the base speaker, along with the dialogue \n\u221d # \" \u210e, $\" , \" % \u00d7 # \" \" , \u210e, &\" ) \"'! ( ) Imaginary Listener: # \" ( | $\" , \u210e, \" ) Base Speaker: # \" \" , \u210e, &\" )\nFigure 3: The proposed self-conscious agent S 1 consists of base speaker S 0 and imaginary listener L 0 . It recursively generates the next token u t at every time t.\nhistory h and partial utterance u <t , as shown in Figure 3. The base speaker S t 0 returns a distribution over the next token at timestep t: S t 0 (u t |i, h, u <t ). Any conditional dialogue agent can be used as a base speaker. See the details in Section 5.2.\nImaginary Listener L 0 . While the base speaker generates each token one at a time, the imaginary listener reasons about the speaker's persona. The imaginary listener L t 0 is the posterior distribution of the speaker's persona in terms of the base speaker and the world prior p t (i) over personas as follows,\nL t 0 (i|h, u \u2264t , p t ) \u221d S t 0 (u t |i, h, u <t ) \u03b2 \u00d7 p t (i) i \u2208I S t 0 (u t |i , h, u <t ) \u03b2 \u00d7 p t (i )\n.\n(1)\nwhere \u03b2 on S t 0 is the listener rationality coefficient that controls the amount of information from the current timestep compared to the cumulative prior p t (i). L 0 returns a probability distribution over the personas in world I, which is a finite set (|I| = 3) comprising the given persona i and distractor personas. The distractors are different personas from other dialogue instances in the dataset. We decide world I per dialogue instance through learning, which will be elaborated in Section 4.2.\nSelf-Conscious Speaker S 1 . With S t 0 and L t 0 , the self-conscious speaker S t 1 is defined as\nS t 1 (u t |i, h, u <t ) \u221d L t 0 (i|h, u \u2264t , p t ) \u03b1 \u00d7 S t 0 (u t |i, h, u <t ), (2\n)\nwhere \u03b1 is the speaker rationality coefficient that determines how much the likelihood is considered. By taking the listener's distribution into account, the speaker is now self-conscious about what persona it sounds like. Especially, the agent seeks to be perceived as the given persona i rather than some other persona i . The likelihood of each token being identified as the persona i acts as a bonus added to the base speaker's token scores. Hence, tokens that are consistent to the given persona are preferred to others. The token with the highest probability is added to the partial utterance, becoming the next input u <t+1 for the speaker.\nUpdating the world prior with L 0 . Starting from a uniform distribution as the initial prior p 0 (i), we update the world prior p t+1 (i) according to S 1 's output u t at every time step:\np t+1 (i) = L t 0 (i|h, u \u2264t , p t ).(3)\nHence, p t (i) represents the cumulative state of the partial utterance up to t. Cohn-Gordon et al. (2018) report the prior update with L 1 \u221d S t 0 (u t |i, h, u <t ) \u00d7 L t 0 (i|h, u \u2264t , p t ) makes little practical effect compared to a uniform prior. We find that updating the prior with Eq. (3) instead is effective. See the results in Section 5.6.", "n_publication_ref": 3, "n_figure_ref": 2}, {"heading": "Learning to Select Distractors", "text": "Distractors (Andreas and Klein, 2016) are samples (e.g. other personas in the dataset) which are different from the given target. In previous works of RSA, the distractors to be included in world I are selected manually or randomly from the dataset. However, we find that performance variance is large according to the selected distractors. We thus propose to learn distractor selection, especially based on the life-long memory network (Kaiser et al., 2017). The life-long memory network is capable of implicitly clustering similar dialogue contexts into a few slots with associated persona. Therefore, it can efficiently memorize and retrieve distractor personas for each context. In Appendix, we experiment that our approach outperforms other models including BERT-based algorithms.\nTo better select useful distractor personas, supervised learning is desirable. However, there is no explicit label indicating which distractors are helpful for each dialogue. We select the persona that have the best Hits@1 as the distractor label per training dialogue. The Hits@1 is the score for favoring the ground-truth next utterance (consistent and context-relevant) over other candidate utterances which are just being consistent (i.e. entailing) or contradictory to the given persona. In other words, the score represents consistency and also appropriateness at the same time. Thus, such distractors can help the self-conscious agent to generate responses which are context-relevant and allow the imaginary listener to identify the speaker's persona. Each training datapoint comprises a given persona, a distractor persona and dialogue context.\nMemory Structure. The memory consists of three types of information: M = (K, v, a). K \u2208 R m\u00d7d is a key matrix, where m is the number of memory slots and d is the dimension of the key vectors, which are the embedding of datapoints. The value vector v \u2208 R m stores the index of a persona. a \u2208 R m is an age vector, which is used for memory update. We set m = 16, 000 and d = 768.\nMemory Addressing. We construct the query vector q for each datapoint with the BERT-Uncased-Base (Devlin et al., 2019) model. We use the output embedding of BERT's [CLS] token, and normalize it to a unit length to build q \u2208 R d .\nUsing the cosine similarity between q and each memory key, we can find the k nearest neighbors:\n(n 1 , n 2 , ..., n k ) = N N k (q, K).(4)\nMemory Loss. Suppose that the query datapoint has a distractor label l. Among (n 1 , ..., n k ), we denote the positive neighbor n p as the one with v[n p ] = l and the negative neighbor n b with v[n b ] = l. If there are multiple positive neighbors, we pick the one with the smallest memory index. If no positive neighbor is found, we select a random key whose value is l. For the negative neighbor, we select one randomly from (n 1 , ..., n k ). We set k = 2048. Then, the loss is computed as\nL = max(q \u2022 K[n b ] \u2212 q \u2022 K[n p ] + \u03b1, 0), (5)\nwhere \u03b1 is a positive margin, which we set as 0.2. This loss maximizes the cosine similarity between the query q and the positive key K[n p ], while minimizing the similarity to the negative key K[n b ]. We finetune the query network BERT with this loss.\nMemory Update. After computing the loss, memory M is updated differently for two cases.\n(1) If the top-1 neighbor's value (i.e. persona) is correct (v[n 1 ] = l), the key vector is updated as:\nK[n 1 ] \u2190 q + K[n 1 ] q + K[n 1 ] .(6)\n(2) Otherwise (v[n 1 ] = l), we make a slot for the query; we find the oldest memory slot n according to the age vector a and write\nK[n ] \u2190 q, v[n ] \u2190 l, a[n ] \u2190 0. (7)\nTraining & Inference. In our Distractor Memory network, training corresponds to updating the memory and the parameters of the query network.\nAt inference, given a test example, we obtain the query by encoding the dialogue context and the persona using BERT. We find n nearest keys from the memory, and use their values (i.e. persona indices) as the distractor personas. We set n = 2.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We show that our self-conscious framework can significantly improve consistency and accuracy of state-of-the-art persona-based agents on two benchmark datasets. We prove its effectiveness using both automatic and human evaluations. We also show our framework can be generalized to improve consistency of dialogue context beyond persona.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": "Dialogue NLI Evaluation Set (Welleck et al., 2019). This dataset is based on PersonaChat with additional NLI annotations. Its main task is to rank next-utterance candidates given previous context. For each dialogue, they collect 31 next-utterance candidates in respect to the given persona: 10 entailing, 10 neutral and 10 contradicting candidates with 1 ground-truth utterance. In total, the evaluation set includes 542 instances.\nPersonaChat dialogue (Zhang et al., 2018). This dataset involves two interlocutors who are each given a persona and asked to get to know each other while playing their roles. This task was the subject of the ConvAI2 competition (Dinan et al., 2019) at NeurIPS 2018. The competition version contains 17,878 chitchat conversations conditioned on 1,155 personas for training and 1,000 conversations conditioned on 100 personas for validation.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Experimental Setting", "text": "Base Speakers. We experiment on three pretrained models including ControlSeq2Seq (See et al., 2019), TransferTransfo (Wolf et al., 2019b), and Blender  as base speakers (S 0 ) for our self-conscious agents (S 1 ). The ControlSeq2Seq is a Seq2Seq model with attention trained on Twitter dataset (Miller et al., 2017) and finetuned on PersonaChat. TranferTransfo based on GPT (Radford et al., 2018) is the winner of the ConvAI2 competition in automatic evaluation. Blender, a recently released generative dialogue model, is the state-of-the-art open-domain chatbot. Our approach improves these base speakers by   (Welleck et al., 2019). +DM is the Distractor Memory. High scores in Hits@1, Entail@1 and low scores in Contradict@1 imply better consistency.\ngranting them the sense of self-consciousness. We defer implementation details to Appendix. Evaluation Metrics. For Dialogue NLI, we report three ranking metrics introduced in the original paper: Hits@1, Entail@1, and Contradict@1. Each metric is the proportion of GT, entailing, and contradictory utterances in the top-1 candidates returned by the model, respectively. High scores in Entail@1 and low scores in Contradict@1 indicate better consistency with the persona.\nFor PersonaChat, we report Hits@1, standard F1 score, perplexity and C score, following the Con-vAI2 protocol. Hits@1 is the accuracy of choosing the ground-truth next-utterance among 20 candidates as the models rank the candidates by perplexity. The C score is a metric for dialogue consistency, introduced in Madotto et al. (2019). It computes pairwise comparison between utterance u and persona sentence p j with a pretrained NLI model. The NLI model returns 1, 0, -1 for entailment, neutrality, and contradiction, respectively. We sum the NLI scores across persona sentences per dialogue instance: C(u) = j NLI(u, p j ).", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Quantitative Results", "text": "Results on Dialogue NLI. Table 3 compares the performance of dialogue agents on the Dialogue NLI evaluation set. Our self-conscious agent S 1 significantly reduces Contradict@1 scores and increases the Entail@1 along with the Hits@1 accuracy of the literal agents S 0 . We remind that each entailing candidate shares the same annotated triple as the GT utterance. In other words, they have similar semantics to the GT utterance and follow the   (Zhang et al., 2018). C is the consistency score evaluated by a pretrained NLI model (Madotto et al., 2019). For TransferTransfo, we use the generative version to calculate Hits@1.\ngiven persona. Thus, Entail@1 is a lenient version of Hits@1 (Welleck et al., 2019). The Distractor Memory (DM) is better than random distractor selection for S 1 across all metrics. It concludes that learned distractors are more effective than random distractors for pragmatic agents.\nResults on PersonaChat. Table 4 compares the performance of different dialogue agents on the PersonaChat dataset. Our model S 1 outperforms all other generative dialogue agents in terms of consistency related metrics, i.e. Hits@1 and C score. Since the posterior update of our self-conscious agent revises the distribution learned by the base speaker, the increase in perplexity is natural due to the effect of regularization. Nevertheless, our approach improves the F1 score for TransferTransfo and Blender. Thus, being consistent to the given persona can also help improve the generation performance of dialogue agents.\nComparison with agents that use NLI model. We also test agents with pretrained NLI models attached (Welleck et al., 2019), denoted by +NLI in Table 5. The NLI model computes contradiction scores of each candidate utterances, and penalize its rank accordingly. Compared to base agents with no self-consciousness, our agents improve consistency in all three metrics even further when using additional NLI models. Another notable result is that our agents without NLI (S 1 +DM in Table 3) for ControlSeq2Seq and TransferTransfo even outperform the base agents with NLI (S 0 +NLI) on Hits@1. That is, our self-conscious agents achieve   better GT accuracy even without the help of an NLI model trained on consistency labels.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Human Evaluation", "text": "We perform human evaluation via Amazon Mechanical Turk. We random sample 250 test examples, each is rated by three unique human judges in terms of (i) Consistency and (ii) Engagingness.\nTurkers are shown a given persona, a dialogue context, and the model's generated utterance. For consistency, we follow Madotto et al. (2019) and ask judges to assign 1, 0, \u22121 to the utterance for consistency, neutrality, and contradiction, respectively. Following See et al. (2019), we evaluate the engagingness of the utterance in a 4-point scale, where higher scores are better. To alleviate annotator bias and inter-annotator variability, we apply Bayesian calibration (Kulikov et al., 2019) to the scores. Table 6 summarizes the human evaluation results. The agent with our self-consciousness method S 1 is rated as more consistent than the base agent S 0 while maintaining a similar level of engagingness. While it can be trivial to increase consistency at the cost of engagingness (e.g. perfect consistency can by generating boring utterances with very little variance), it is not the case for our agent. Since  our agent seeks to be heard as the given persona to the listener, self-distinctive words tend to meld into generated responses (see Figure 6). Thus, the responses from self-conscious agents have their own color, which can help improving engagingness.\nFigure 4 displays selected examples of utterance generation. Each example is comprised of dialogue history, human response, and utterances generated by our method and baselines.", "n_publication_ref": 3, "n_figure_ref": 2}, {"heading": "Consistency for Dialogue Context", "text": "We demonstrate that our self-conscious agent can be generalized to generate context-consistent utterances beyond persona. We condition the agent with its previous responses in the dialogue history; that is, i in Eq. (2) is the agent's past responses instead of persona sentences. Hence, tokens that are inconsistent to the agent's past response would be less favored by the model. Table 7 reports the results of context conditioned self-conscious agents. The EmpatheticDialogue (Rashkin et al., 2019) is an open-domain dialogue dataset where a speaker describes a past emotional experience and the listener responds accordingly. Since the speaker's descriptions should be consistent to the experience and previous utterances, it is a suitable benchmark for consistency. We model the speaker's utterances and measure its consistency.\nOur S 1 agent outperforms other literal agents on all three datasets in terms of consistency. Thus, our approach can also be applied to help agents stay more consistent to its context.  (Zhang et al., 2018). We compare it with the base speaker (S 0 ) of TransferTransfo (Wolf et al., 2019b) and the human response (Human).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Controlling the Self-Conscious Agent", "text": "To further analyze our self-conscious agent, we conduct experiments by controlling three features of our agent: world prior updates p t (i), listener rationality \u03b2 and speaker rationality \u03b1. World Prior Update. In the self-conscious agent, the world prior acts as a cumulative state over personas. We remind that we propose to update the world prior with L t 0 instead of L t 1 in Eq. (3). As reported in Cohn-Gordon et al. (2018), our experiments on the Dialogue NLI dataset confirm the prior update with L t 1 makes little difference in performance compared with using a uniform distribution. However, our approach with L t 0 makes significant difference, as shown in Figure 5. The reason is that the pragmatic listener L t 1 \u221d S t 0 (u t |i, h, u <t ) \u00d7 L t 0 (i|h, u \u2264t , p t ) reflects the current S t 0 twice (i.e. in L t 0 and in itself) per time step. Hence, the update with L t 1 becomes more of an instantaneous prior rather than a cumulative one. On the other hand, L t 0 moderately combines the information from both S t 0 and p t (i), preserving better cumulative information.\nListener Rationality \u03b2. We add \u03b2 in L t 0 to control the amount of information incorporated to the world prior p t (i). Figure 5 depicts that when \u03b2 is large, the Hits@1 scores (i.e. the GT accuracy) drop. With a big \u03b2, the information S t 0 at current time step overrides the cumulative prior p t (i). That is, the utterance state evolves shortsightedly, ignoring the context information from the previous steps. Therefore, setting of \u03b2 \u2264 1 is advantageous for the self-conscious agent to incrementally decode.\nSpeaker Rationality \u03b1. Figure 6 shows an example of how generated responses vary according to the intensity of speaker rationality \u03b1. As \u03b1 increases, the self-conscious agent reflects the listener's distribution (i.e. the likelihood) more into the posterior. When \u03b1 is too large, the posterior distribution is overwhelmed by the likelihood of the persona. Then, the language model degenerates to favor uttering fragments of the given persona while even ignoring the syntax. Hence, \u03b1 can control the degree of copying the given condition text. An appropriate \u03b1 value allows the given persona condition to blend smoothly in the utterance.", "n_publication_ref": 1, "n_figure_ref": 3}, {"heading": "Conclusion", "text": "This work investigated how modeling public selfconsciousness can help dialogue agents improve persona-consistency. We showed existing dialogue agents are highly insensitive to contradiction, and introduced an orthogonally applicable method using the RSA framework (Frank and Goodman, 2012) to alleviate the issue. We also designed a 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.  learning method for distractor selection, named Distractor Memory and proposed a better update for the listener's world prior. Furthermore, we demonstrated how our approach can be generalized to improve dialogue context-consistency. Our self-conscious agents improved the base agents on the Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset, without consistency labels and NLI models. An important future direction will be generating the distractors and learning the rationality coefficients.\nA Results on Variants of Distractor Selection (Section 4.2)   (Welleck et al., 2019).\nWe compare our proposed Distractor Memory (DM) with three heuristic methods, and two variants of the pretrained BERT model (Devlin et al., 2019). As a straightforward baseline, we randomly select k personas from training set and directly use it as distractors. Second, we test the k-nearest search by speaker's persona, denoted by Nearest; for a given persona descriptions, we find its closest training persona embedding using cosine similarity on average pooled BERT features. The third baseline denoted by Farthest is to find the k-farthest persona among the training personas.\nWe also compare with two variants of the BERT model. The first variant is BERT-Classifier, which takes dialogue context as input and returns the index of persona from training set as output. The second variant is bi-encoder ranking model of Miller et al. (2017), denoted by BERT-Ranker. It encodes dialogue context and candidate persona with separate BERT encoders measuring its ranking with cosine similarity. For both methods, we use top-k ranked personas as distractors and set k = 4 for all the methods. We use Adam optimizer (Kingma and Ba, 2015) with learning rate 2e-5 and finetune BERT-Uncased-Base up to 3 epochs.\nTable 8 compares the performance of different distractor selecting methods on the Dialogue NLI evaluation set (Welleck et al., 2019). We set \u03b1 = 8, \u03b2 = 0.5, and |I| = 5. The DM model outperforms all the baselines across all metrics. The Farthest shows better performance than the Nearest.It can be understood that dissimilar distractors are more effective in the Rational Speech Acts framework (Frank and Goodman, 2012). The BERT-Ranker performs the best among baselines, but not as good as ours, which validates that memorization capability is effective for selecting useful distractors.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "B Implementation Details", "text": "Base Codes and Datasets. We use the ParlAI framework 2 (Miller et al., 2017) and Hugging-Face's Transformers 3 (Wolf et al., 2019a) to implement our models and baselines. We use Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) datasets from the ParlAI framework as is. We use the default preprocessing in ParlAI.\nTraining. Our self-consciousness approach improves consistency for any pretrained dialogueagents without additional consistency labels and pretrained NLI models. Since it post-processes the output probability of pretrained dialogue-agents in a Bayesian fashion, no additional model parameters are added to the dialogue agents. Thus, it does not require any training. In the case of using the Distractor Memory (DM), first we initialize BERT-Uncased-Base with pretrained weights and finetune it up to 3 epochs with Adam optimizer with learning rate 2e-5. Then we find the best distractor persona for each model and use those labels to train our DM. We train our DM on one NVIDIA TITAN Xp GPU up to 7 epochs.\nHyperparameters. For Dialogue NLI evaluation, we set the speaker rationality \u03b1 = 8.0, the listener rationality \u03b2 = 1.0, and the cardinality of the world I to 3. In PersonaChat evaluation, we set \u03b1 = 2.0, \u03b2 = 0.3 for ControlSeq2Seq (See et al., 2019), \u03b1 = 2, \u03b2 = 0.9 for TransferTransfo (Wolf et al., 2019b), and \u03b1 = 2.0, \u03b2 = 0.5 for Blender 90M . We also set |I| = 3. We experiment \u03b1 = {1.0, 2.0, 4.0, 8.0, 16.0}, \u03b2 = {0.3, 0.5, 0.9, 1.0, 2.0, 4.0}, and |I| = {2, 3, 5}. We choose the hyper-parameter configuration showing the best performance in Hits@1 for Dialogue NLI and F1 score for PersonaChat. The posterior distribution of our self-conscious agents are computed deterministically. For our Distractor Memory, we set the memory key matrix as K \u2208 R m\u00d7d , where m = 16000 and d = 768. We set the number of nearest neighbor k = 2048.\nInference. We use greedy decoding for all methods. The average runtime for our self-conscious approach is dependent on the base dialogue agents and the cardinality of world I which can be run in parallel like beam search.\nEvaluation. We follow the evaluation of the Par-lAI framework. Following Madotto et al. (2019), 2 https://parl.ai/ 3 https://huggingface.co/transformers/ we use the finetuned BERT-based NLI model 4 to compute the C score.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "C Dialogue Examples", "text": "Figure 7 shows selected examples of generated responses. In each set, we show given persona, dialogue context, human responses, and generated responses by our self-conscious agent and the base speaker. We use TransferTransfo (Wolf et al., 2019b) as a base speaker.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Acknowledgements", "text": "We would like to thank Reuben Cohn-Gordon, Sean Welleck, Junhyug Noh and Jiwan Chung for their valuable comments. We also thank the anonymous reviewers for their thoughtful suggestions on this work. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dialogue History", "text": "[P1] I really enjoy shopping and my dream is to one day own a Rolls Royce ghost.\n[P2] Wow. I enjoy running over driving.\n[P1] Running is also quite lovely. Breathing in the lovely outside air.\n[P2] Yes it is. It clears my head when I need to as well.\n(S 1 +DM) shopping is a great way to clear my head. (S 0 ) i love to shop and watch movies. (Human) yes , and it also helps with depression i have found.\n\u2022 \u2022 \u2022 Figure 7: Examples of generated responses by our self-conscious agent with Distractor Memory (S 1 +DM) on the PersonaChat dataset (Zhang et al., 2018). We compare it with the base speaker (S 0 ) of TransferTransfo (Wolf et al., 2019b) and the human response (Human).", "n_publication_ref": 2, "n_figure_ref": 1}], "references": [{"title": "SPICE: Semantic Propositional Image Caption Evaluation", "journal": "Springer", "year": "2016", "authors": "Peter Anderson; Basura Fernando; Mark Johnson; Stephen Gould"}, {"title": "Reasoning about Pragmatics with Neural Listeners and Speakers", "journal": "", "year": "2016", "authors": "Jacob Andreas; Dan Klein"}, {"title": "Lost in Machine Translation: A Method to Reduce Meaning Loss", "journal": "", "year": "2019", "authors": "Reuben Cohn; - Gordon; Noah Goodman"}, {"title": "Pragmatically Informative Image Captioning With Character-level Inference", "journal": "", "year": "2018", "authors": "Reuben Cohn-Gordon; Noah Goodman; Christopher Potts"}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "The Second Conversational Intelligence Challenge (ConvAI2)", "journal": "", "year": "2019", "authors": "Emily Dinan; Varvara Logacheva; Valentin Malykh; Alexander Miller; Kurt Shuster; Jack Urbanek; Douwe Kiela; Arthur Szlam; Iulian Serban; Ryan Lowe"}, {"title": "Self-Consciousness and Strategic Self-Presentation", "journal": "Journal of Personality", "year": "1991", "authors": "Kevin Doherty; R Barry;  Schlenker"}, {"title": "Evaluating Coherence in Dialogue Systems Using Entailment", "journal": "", "year": "2019", "authors": "Nouha Dziri; Ehsan Kamalloo; Kory W Mathewson; Osmar Zaiane"}, {"title": "Public and Private Self-Consciousness: Assessment and Theory", "journal": "Journal of Consulting and Clinical Psychology", "year": "1975", "authors": "Allan Fenigstein; F Michael; Arnold H Scheier;  Buss"}, {"title": "A Theory of Cognitive Dissonance", "journal": "Stanford University Press", "year": "1962", "authors": "Leon Festinger"}, {"title": "Predicting Pragmatic Reasoning in Language Games", "journal": "Science", "year": "2012", "authors": "C Michael;  Frank;  Noah D Goodman"}, {"title": "Unified Pragmatic Models for Generating and Following Instructions", "journal": "", "year": "2017", "authors": "Daniel Fried; Jacob Andreas; Dan Klein"}, {"title": "Speaker-follower models for vision-and-language navigation", "journal": "", "year": "2018", "authors": "Daniel Fried; Ronghang Hu; Volkan Cirik; Anna Rohrbach; Jacob Andreas; Louis-Philippe Morency; Taylor Berg-Kirkpatrick; Kate Saenko; Dan Klein; Trevor Darrell"}, {"title": "Why the Child's Theory of Mind Really is a Theory", "journal": "Mind & Language", "year": "1992", "authors": "Alison Gopnik;  Henry M Wellman"}, {"title": "Imagine All the People: How the Brain Creates and Uses Personality Models to Predict Behavior", "journal": "Cerebral Cortex", "year": "2013", "authors": "Demis Hassabis; Nathan Spreng; Andrei A Rusu; Clifford A Robbins; Raymond A Mar; Daniel L Schacter"}, {"title": "Learning to Remember Rare Events", "journal": "", "year": "2017", "authors": "\u0141ukasz Kaiser; Ofir Nachum; Aurko Roy; Samy Bengio"}, {"title": "Adam: A Method for Stochastic Optimization", "journal": "", "year": "2015", "authors": "Diederik Kingma; Jimmy Ba"}, {"title": "Importance of Search and Evaluation Strategies in Neural Dialogue Modeling", "journal": "", "year": "2019", "authors": "Ilia Kulikov; Alexander Miller; Kyunghyun Cho; Jason Weston"}, {"title": "A Persona-Based Neural Conversation Model", "journal": "", "year": "2016", "authors": "Jiwei Li; Michel Galley; Chris Brockett; P Georgios; Jianfeng Spithourakis; Bill Gao;  Dolan"}, {"title": "Don't Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training", "journal": "", "year": "2020", "authors": "Margaret Li; Stephen Roller; Ilia Kulikov; Sean Welleck; Y-Lan Boureau; Kyunghyun Cho; Jason Weston"}, {"title": "Rouge: A Package for Automatic Evaluation of Summaries", "journal": "", "year": "2004", "authors": "Chin-Yew Lin"}, {"title": "You Impress Me: Dialogue Generation via Mutual Persona Perception", "journal": "", "year": "2020", "authors": "Qian Liu; Yihong Chen; Bei Chen; Jian-Guang Lou; Zixuan Chen; Bin Zhou; Dongmei Zhang"}, {"title": "Personalizing Dialogue Agents via Meta-Learning", "journal": "", "year": "2019", "authors": "Andrea Madotto; Zhaojiang Lin; Chien-Sheng Wu; Pascale Fung"}, {"title": "Generation and Comprehension of Unambiguous Object Descriptions", "journal": "", "year": "2016", "authors": "Junhua Mao; Jonathan Huang; Alexander Toshev; Oana Camburu; Alan L Yuille; Kevin Murphy"}, {"title": "ParlAI: A Dialog Research Software Platform", "journal": "", "year": "2017", "authors": "A H Miller; W Feng; A Fisch; J Lu; D Batra; A Bordes; D Parikh; J Weston"}, {"title": "Improving Language Understanding with Unsupervised Learning", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"title": "Towards Empathetic Opendomain Conversation Models: A New Benchmark and Dataset", "journal": "", "year": "2019", "authors": "Eric Michael Hannah Rashkin; Margaret Smith; Y-Lan Li;  Boureau"}, {"title": "Recipes for Building an Open-Domain Chatbot", "journal": "", "year": "2020", "authors": "Stephen Roller; Emily Dinan; Naman Goyal; Da Ju; Mary Williamson; Yinhan Liu; Jing Xu; Myle Ott; Kurt Shuster; Eric M Smith"}, {"title": "What Makes a Good Conversation? How Controllable Attributes Affect Human Judgments", "journal": "", "year": "2019", "authors": "Abigail See; Stephen Roller; Douwe Kiela; Jason Weston"}, {"title": "Pragmatically Informative Text Generation", "journal": "", "year": "2019", "authors": "Sheng Shen; Daniel Fried; Jacob Andreas; Dan Klein"}, {"title": "Sketch-Fill-AR: A Persona-Grounded Chit-Chat Generation Framework", "journal": "", "year": "2019", "authors": "Michael Shum; Stephan Zheng; Wojciech Kry\u015bci\u0144ski; Caiming Xiong; Richard Socher"}, {"title": "2020. Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation", "journal": "", "year": "", "authors": "Haoyu Song; Yan Wang; Wei-Nan Zhang; Xiaojiang Liu; Ting Liu"}, {"title": "Generating Persona Consistent Dialogues by Exploiting Natural Language Inference", "journal": "", "year": "2019", "authors": "Haoyu Song; Wei-Nan Zhang; Jingwen Hu; Tiu Liu"}, {"title": "Context-Aware Captions from Context-Agnostic Supervision", "journal": "", "year": "2017", "authors": "Ramakrishna Vedantam; Samy Bengio; Kevin Murphy; Devi Parikh; Gal Chechik"}, {"title": "Dialogue Natural Language Inference", "journal": "", "year": "2019", "authors": "Sean Welleck; Jason Weston; Arthur Szlam; Kyunghyun Cho"}, {"title": "Transformers: State-of-the-art Natural Language Processing", "journal": "", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz"}, {"title": "TransferTransfo: A Transfer Learning Approach for Neural Network based Conversational Agents", "journal": "", "year": "2019", "authors": "Thomas Wolf; Victor Sanh; Julien Chaumond; Clement Delangue"}, {"title": "Know What You Don't Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories", "journal": "", "year": "2019", "authors": "Sina Zarrie\u00df; David Schlangen"}, {"title": "Personalizing Dialogue Agents: I Have a Dog, Do You Have Pets Too", "journal": "", "year": "2018", "authors": "Saizheng Zhang; Emily Dinan; Jack Urbanek; Arthur Szlam; Douwe Kiela; Jason Weston"}, {"title": "Consistent Dialogue Generation with Selfsupervised Feature Learning", "journal": "", "year": "2019", "authors": "Yizhe Zhang; Xiang Gao; Sungjin Lee; Chris Brockett; Michel Galley; Jianfeng Gao; Bill Dolan"}], "figures": [{"figure_label": "4", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 4 :4Figure4: Examples of generated responses by our selfconscious agent with Distractor Memory (S 1 +DM) on the PersonaChat dataset(Zhang et al., 2018). We compare it with the base speaker (S 0 ) of TransferTransfo(Wolf et al., 2019b) and the human response (Human).", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 5 :5Figure5: Performance variation of the self-conscious agent for TransferTransfo (left) and Blender (right) according to \u03b2. We compare different methods of updating the world prior p t (i) with L 0 (Ours), L 1 and a uniform prior. The dashed line is the base speaker S 0 .", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 6 :6Figure 6: An example of utterance changes by controlling the speaker rationality \u03b1 on the PersonaChat.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Comparison between ground-truth utterances,top-ranked entailing candidates and Contradict@1 ut-terances in ROUGE and SPICE scores.(RSA) frameworks (Frank and Goodman, 2012)in pragmatics. It has improved informativeness ina number of NLP tasks, including reference games"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Example of a contradictory utterance returnedby the model and its GT utterance with perplexity pertoken. The words of entailment and contradiction tothe persona are shown in blue and red, respectively."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Comparison of our approach (S 1 ) with base speakers (S 0 ) on the Dialogue NLI evaluation set", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Comparison of our approach (S 1 ) with base speakers (S 0 ) on PersonaChat", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Comparison of our approach (S 1 ) with base speakers (S 0 ) on the Dialogue NLI evaluation set(Welleck et al., 2019) with pretrained NLI model attached.", "figure_data": "RawCalibratedModel Consistent Engaging Consistent EngagingTransferTransfo (Wolf et al., 2019b)S00.53 (0.02) 2.48 (0.03) 0.44 (0.01) 2.48 (0.01)S1+DM 0.61 (0.02) 2.55 (0.03) 0.52 (0.01) 2.52 (0.01)"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Human evaluation results comparing the consistency and engagingness of the base speaker (S 0 ) and our self-conscious agent (S 1 ). Numbers in parentheses are the standard errors.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": ": Comparison of our approach (S 1 ) with basespeaker Blender (S 0 ) when conditioned on dialoguecontext in three datasets. We compute the consistencyscore C respect to the dialogue context."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Quantitative results of the proposed Distractor Memory (DM) and other distractor selection methods on the Dialogue NLI evaluation set", "figure_data": ""}], "doi": ""}