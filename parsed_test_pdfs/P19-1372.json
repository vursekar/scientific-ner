{"authors": "Lisong Qiu; Juntao Li; Wei Bi; Dongyan Zhao; Rui Yan", "pub_date": "", "title": "Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References", "abstract": "Due to its potential applications, open-domain dialogue generation has become popular and achieved remarkable progress in recent years, but sometimes suffers from generic responses. Previous models are generally trained based on 1-to-1 mapping from an input query to its response, which actually ignores the nature of 1-to-n mapping in dialogue that there may exist multiple valid responses corresponding to the same query. In this paper, we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture. The first generation phase extracts the common features of different responses which, combined with distinctive features obtained in the second phase, can generate multiple diverse and appropriate responses. Experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations.", "sections": [{"heading": "Introduction", "text": "In recent years, open-domain dialogue generation has become a research hotspot in Natural Language Processing due to its broad application prospect, including chatbots, virtual personal assistants, etc. Though plenty of systems have been proposed to improve the quality of generated responses from various aspects such as topic , persona modeling  and emotion controlling (Zhou et al., 2018b), most of these recent approaches are primarily built upon the sequence-to-sequence architecture Shang et al., 2015) which suffers from the \"safe\" response problem (Li et al., 2016a;Sato et al., 2017). This can be ascribed to modeling the response generation process as 1to-1 mapping, which ignores the nature of 1-to- n mapping of dialogue that multiple possible responses can correspond to the same query.\nTo deal with the generic response problem, various methods have been proposed, including diversity-promoting objective function (Li et al., 2016a), enhanced beam search (Shao et al., 2016), latent dialogue mechanism (Zhou et al., , 2018a, Variational Autoencoders (VAEs) based models Serban et al., 2017), etc. However, these methods still view multiple responses as independent ones and fail to model multiple responses jointly. Recently, Zhang et al. (2018a) introduce a maximum likelihood strategy that given an input query, the most likely response is approximated rather than all possible responses, which is further implemented by Rajendran et al. (2018) with reinforcement learning for task-oriented dialogue. Although capable of generating the most likely response, these methods fail to model other possible responses and ignore the correlation of different responses.\nIn this paper, we propose a novel response generation model for open-domain conversation, which learns to generate multiple diverse responses with multiple references by considering the correlation of different responses. Our motivation lies in two aspects: 1) multiple responses for a query are likely correlated, which can facilitate building the dialogue system. 2) it is easier to model each response based on other responses than from scratch every time. As shown in Figure 1, given an input query, different responses may share some common features e.g. positive attitudes or something else, but vary in discourses or expressions which we refer to as distinct features. Accordingly, the system can benefit from modeling these features respectively rather than learning each query-response mapping from scratch.\nInspired by this idea, we propose a two-step dialogue generation architecture as follows. We jointly view the multiple possible responses to the same query as a response bag. In the first generation phase, the common feature of different valid responses is extracted, serving as a base from which each specific response in the bag is further approximated. The system then, in the second generation phase, learns to model the distinctive feature of each individual response which, combined with the common feature, can generate multiple diverse responses simultaneously.\nExperimental results show that our method can outperform existing competitive neural models under both automatic and human evaluation metrics, which demonstrates the effectiveness of the overall approach. We also provide ablation analyses to validate each component of our model. To summarize, our contributions are threefold:\n\u2022 We propose to model multiple responses to a query jointly by considering the correlations of responses with multi-reference learning.\n\u2022 We consider the common and distinctive features of the response bag and propose a novel two-step dialogue generation architecture.\n\u2022 Experiments show that the proposed method can generate multiple diverse responses and outperform existing competitive models on both automatic and human evaluations.", "n_publication_ref": 11, "n_figure_ref": 1}, {"heading": "Related Work", "text": "Along with the flourishing development of neural networks, the sequence-to-sequence framework has been widely used for conversation response generation (Shang et al., 2015;Sordoni et al., 2015) where the mapping from a query x to a reply y is learned with the negative log likelihood. However, these models suffer from the \"safe\" response problem. To address this problem, various methods have been proposed. Li et al. (2016a) propose a diversity-promoting objective function to encourage diverse responses during decoding. Zhou et al. ( , 2018a introduce a responding mechanism between the encoder and decoder to generate various responses.  incorporate topic information to generate informative responses. However, these models suffer from the deterministic structure when generating multiple diverse responses. Besides, during the training of these models, response utterances are only used in the loss function and ignored when forward computing, which can confuse the model for pursuing multiple objectives simultaneously.\nA few works explore to change the deterministic structure of sequence-to-sequence models by introducing stochastic latent variables. VAE is one of the most popular methods (Bowman et al., 2016;Serban et al., 2017;Cao and Clark, 2017), where the discourse-level diversity is modeled by a Gaussian distribution. However, it is observed that in the CVAE with a fixed Gaussian prior, the learned conditional posteriors tend to collapse to a single mode, resulting in a relatively simple scope (Wang et al., 2017). To tackle this, WAE (Gu et al., 2018) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD (Du et al., 2018) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed. Although these models overcome the deterministic structure of sequence-to-sequence model, they still ignore the correlation of multiple valid responses and each case is trained separately.\nTo consider the multiple responses jointly, the maximum likelihood strategy is explored. Zhang et al. (2018a) propose the maximum generated likelihood criteria which model a query with its multiple responses as a bag of instances and proposes to optimize the model towards the most likely answer rather than all possible responses. Similarly, Rajendran et al. (2018) propose to reward the dialogue system if any valid answer is produced in the reinforcement learning phase. Though considering multiple responses jointly, the maximum likelihood strategy fails to utilize all the references during training with some cases ig-Figure 2: The overall architecture of our proposed dialogue system where the two generation steps and testing process are illustrated. Given an input query x, the model aims to approximate the multiple responses in a bag {y} simultaneously with the continuous common and distinctive features, i.e., the latent variables c and z obtained from the two generation phases respectively. nored. In our approach, we consider multiple responses jointly and model each specific response separately by a two-step generation architecture.", "n_publication_ref": 12, "n_figure_ref": 1}, {"heading": "Approach", "text": "In this paper, we propose a novel response generation model for short-text conversation, which models multiple valid responses for a given query jointly. We posit that a dialogue system can benefit from multi-reference learning by considering the correlation of multiple responses. Figure 2 demonstrates the whole architecture of our model. We now describe the details as follows.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Problem Formulation and Model Overview", "text": "Training samples {(x, {y}) i } i=N i=1 consist of each query x and the set of its valid responses {y}, where N denotes the number of training samples. For a dialogue generation model, it aims to map from the input query x to the output response y \u2208 {y}. To achieve this, different from conventional methods which view the multiple responses as independent ones, we propose to consider the correlation of multiple responses with a novel twostep generation architecture, where the response bag {y} and each response y \u2208 {y} are modeled by two separate features which are obtained in each generation phase respectively. Specifically, we assume a variable c \u2208 R n representing the common feature of different responses and an unobserved latent variable z \u2208 Z corresponding to the distinct feature for each y in the bag. The com-mon feature c is generated in the first stage given x and the distinctive feature z is sampled from the latent space Z in the second stage given the query x and common feature c. The final responses are then generated conditioned on both the common feature c and distinct feature z simultaneously.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Common Feature of the Response Bag", "text": "In the first generation step, we aim to map from the input query x to the common feature c of the response bag {y}. Inspired by multi-instance learning (Zhou, 2004), we start from the simple intuition that it is much easier for the model to fit multiple instances from their mid-point than a random start-point, as illustrated in Figure 1.\nTo obtain this, we model the common feature of the response bag as the mid-point of embeddings of multiple responses. In practice, we first encode the input x with a bidirectional gated recurrent units (GRU)  to obtain an input representation h x . Then, the common feature c is computed by a mapping network which is implemented by a feed-forward neural network whose trainable parameter is denoted as \u03b8. The feature c is then fed into the response decoder to obtain the intermediate response y c which is considered to approximate all valid responses. Mathematically, the objective function is defined as:\nL avg = 1 |{y}| y\u2208{y} log p \u03c8 (y|c) (1)\nwhere |{y}| is the cardinality of the response bag {y} and p \u03c8 represents the response decoder. Besides, to measure how well the intermediate response y c approximates the mid-point response, we set up an individual discriminator and derive the mapping function to produce better results. As to the discriminator, we first project each utterance to an embedding space with fixed dimensionality via convolutional neural networks (CNNs) with different kernels as the process shown in Figure 3. Then, the cosine similarity of the query and response embeddings is computed, denoted as D \u03b8 (x, y), where \u03b8 represents trainable parameter in the discriminator. For the response bag {y}, the average response embedding is used to compute the matching score. The objective of intermediate response y c is then to minimize the difference between D \u03b8 (x, y c ) and D \u03b8 (x, {y}):\nL disc = E x,{y},y c [D \u03b8 (x, y c ) \u2212 D \u03b8 (x, {y})] (2)\nwhere y c denotes the utterance produced by the decoder conditioned on the variable c.\nTo overcome the discrete and non-differentiable problem, which breaks down gradient propagation from the discriminator, we adopt a \"soft\" continuous approximation (Hu et al., 2017):\ny ct \u223c softmax(o t /\u03c4 ) (3)\nwhere o t is the logit vector as the inputs to the softmax function at time-step t and the temperature \u03c4 is set to \u03c4 \u2192 0 as training proceeds for increasingly peaked distributions. The whole loss for the step-one generation is then\nL f irst = L avg + L disc (4)\nwhich is optimized by a minimax game with adversarial training (Goodfellow et al., 2014).", "n_publication_ref": 3, "n_figure_ref": 2}, {"heading": "Response Specific Generation", "text": "The second generation phase aims to model each specific response in a response bag respectively. In practice, we adopt the CVAE  architecture, while two prominent modifications remain. Firstly, rather than modeling each response with the latent variable z from scratch, our model approximates each response based on the bag representation c with only the distinctive feature of each specific response remaining to be captured. Secondly, the prior common feature c can provide extra information for the sampling network which is supposed to decrease the latent searching space. Specifically, similar to the CVAE architecture, the overall objective for our model in the second generation phase is as below:\nL cvae =E q \u03c6 (z|x,y,c)p \u03b8 (c|x) [log p \u03c8 (y|c, z)] \u2212 D[q \u03c6 (z|x, y, c)||p \u03d5 (z|x, c)] (5\n)\nwhere q \u03c6 represents the recognition network and p \u03d5 is the prior network with \u03c6 and \u03d5 as the trainable parameters; D(\u2022||\u2022) is the regularization term which measures the distance between the two distributions. In practice, the recognition networks are implemented with a feed-forward network that\n\u00b5 log \u03c3 2 = W q \uf8ee \uf8f0 h x h y c \uf8f9 \uf8fb + b q (6)\nwhere h x and h y are the utterance representations of query and response got by GRU respectively, and the latent variable z \u223c N (\u00b5, \u03c3 2 I). For the prior networks, we consider two kinds of implements. One is the vanilla CVAE model  where the prior p \u03d5 (z|x, c) is modeled by a another feed-forward network conditioned on the representations h x and c as follows,\n\u00b5 log \u03c3 2 = W p h x c + b p (7)\nand the distance D(\u2022||\u2022) here is measured by the KL divergence. For the other, we adopt the WAE model (Gu et al., 2018) in which the prior p \u03d5 (z|x, c) is modeled by a mixture of Gaussian distributions GMM(\u03c0 k , \u00b5 k , \u03c3 k 2 I) K k=1 , where K is the number of Gaussian distributions and \u03c0 k is the mixture coefficient of the k-th component of the GMM module as computed:\n\u03c0 k = exp(e k ) K i=1 exp(e i )(8)\nand\n\uf8ee \uf8f0 e k \u00b5 k log \u03c3 2 k \uf8f9 \uf8fb = W p,k h x c + b p,k(9)\nTo sample an instance, Gumble-Softmax reparametrization trick (Kusner and Hern\u00e1ndez-Lobato, 2016) is utilized to normalize the coefficients. The distance here is measured by the Wasserstein distance which is implemented with an adversarial discriminator .\nRecap that in the second generation phase the latent variable z is considered to only capture the distinctive feature of each specific response. Hence to distinguish the latent variable z for each separate response, we further introduce a multireference bag-of-word loss (MBOW) which requires the network to predict the current response y against the response bag:\nL mbow = E q \u03c6 (z|x,y,c) [log p(y bow |x, z) + \u03bb log(1 \u2212 p({\u0233} bow |x, z))](10)\nwhere the probability is computed by a feedforward network f as the vanilla bag-of-word loss  does; {\u0233} is the complementary response bag of y and its probability is computed as the average probability of responses in the bag; and \u03bb is a scaling factor accounting for the difference in magnitude. As it shows, the MBOW loss penalizes the recognition networks if other complementary responses can be predicted from the distinctive variable z. Besides, since the probability of the complementary term may approach zero which makes it difficult to optimize, we actually adopt its lower bound in practice:\nlog(1 \u2212 p(y bow |x, z)) = log(1 \u2212 |y| t=1 e fy t |V | j e f j ) \u2265 log( |y| t=1 (1 \u2212 e fy t |V | j e f j\n))\n(11) where |V | is vocabulary size.\nTotally, the whole loss for the step-two generation is then:\nL second = L cvae + L mbow (12\n)\nwhich can be optimized in an end-to-end way.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Optimization and Testing", "text": "Our whole model can be trained in an end-to-end fashion. To train the model, we first pre-train the word embedding using Glove ( (Pennington et al., 2014)) 1 . Then modules of the model are jointly trained by optimizing the losses L f irst and L second of the two generation phases respectively. To overcome the vanishing latent variable problem (Wang et al., 2017) of CVAE, we adopt the KL annealing strategy (Bowman et al., 2016), where the weight of the KL term is gradually increased during training. The other technique employed is the MBOW loss which is able to sharpen the distribution of latent variable z for each specific response and alleviate the vanishing problem at the same time.\nDuring testing, diverse responses can be obtained by the two generation phases described above, where the distinctive latent variable z corresponding to each specific response is sampled from the prior probability network. This process is illustrated in Figure 2. Capable of capturing the common feature of the response bag, the variable c is obtained from the mapping network and no intermediate utterance is required, which facilitates reducing the complexity of decoding.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Experimental Setup", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dataset", "text": "Focusing on open-domain dialogue, we perform experiments on a large-scale single-turn conversation dataset Weibo (Shang et al., 2015), where each input post is generally associated with multiple response utterances 2 . Concretely, the Weibo dataset consists of short-text online chit-chat dialogues in Chinese, which is crawled from Sina Weibo 3 . Totally, there are 4,423,160 queryresponse pairs for training set and 10000 pairs for the validation and testing, where there are around 200k unique query in the training set and each query used in testing correlates with four responses respectively. For preprocessing, we follow the conventional settings (Shang et al., 2015).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Baselines", "text": "We compare our model with representative dialogue generation approaches as listed below:   S2S: the vanilla sequence-to-sequence model with attention mechanism  where standard beam search is applied in testing to generate multiple different responses.\nMethod Multi-BLEU EMBEDDING Intra-Dist Inter-Dist BLEU-1 BLEU-2 G A E Dist-1 Dist-2\nS2S+DB: the vanilla sequence-to-sequence model with the modified diversity-promoting beam search method (Li et al., 2016b) where a fixed diversity rate 0.5 is used.\nMMS: the modified multiple responding mechanisms enhanced dialogue model proposed by Zhou et al. (2018a) which introduces responding mechanism embeddings  for diverse response generation.\nCVAE: the vanilla CVAE model  with and without BOW (bag-of-word) loss (CVAE+BOW and CVAE).\nWAE: the conditional Wasserstein autoencoder model for dialogue generation (Gu et al., 2018) which models the distribution of data by training a GAN within the latent variable space.\nOurs: we explore our model Ours and conduct various ablation studies: the model with only the second stage generation (Ours-First), the model without the discriminator (Ours-Disc) and multireference BOW loss (Ours-MBOW), and the model with GMM prior networks (Ours+GMP).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Evaluation Metrics", "text": "To comprehensively evaluate the quality of generated response utterances, we adopt both automatic and human evaluation metrics: BLEU: In dialogue generation, BLEU is widely used in previous studies (Yao et al., 2017;Shang et al., 2018). Since multiple valid responses exist in this paper, we adopt multi-reference BLEU where the evaluated utterance is compared to provided multiple references simultaneously.\nDistinctness: To distinguish safe and commonplace responses, the distinctness score (Li et al., 2016a) is designed to measure word-level diversity by counting the ratio of distinctive [1,2]-grams.\nIn our experiments, we adopt both Intra-Dist: the distinctness scores of multiple responses for a given query and Inter-Dist: the distinctness scores of generated responses of the whole testing set.\nEmbedding Similarity: Embedding-based metrics compute the cosine similarity between the sentence embedding of a ground-truth response and that of the generated one. There are various ways to obtain the sentence-level embedding from the constituent word embeddings. In our experiments, we apply three most commonly used strategies: Greedy matches each word of the reference with the most similar word in the evaluated sentence; Average uses the average of word embed-\nInput \u706b\u5c71\u55b7\u53d1\u77ac\u95f4\u7684\u4e00\u4e9b\u58ee\u89c2\u666f\u8c61\u3002 \u518d\u8fc7\u5341\u5206\u949f\u5c31\u8fdb\u5165win8\u65f6\u4ee3\uff0c\u6211\u662f\u7cfb\u7edf\u5347\u7ea7\u63a7\u3002 Query\nThese are some magnificent sights at the moment of the volcanic eruption.\nThere remain ten minutes before we entering the era of win8. I am a geek of system updating. What application is this. \u5982\u6b64\u8fd9\u822c\u8fd9\u822c\u6dfc\u5c0f\u3002 \u6211\u89c9\u5f97\u8fd9\u6837\u7684\u754c\u9762\u66f4\u50cfwindows8\u3002 It is so so imperceptible.\nI think interface like this looks more like windows8.\nTable 3: Case study for the generated responses from the testing set of Weibo, where the Chinese utterances are translated into English for the sake of readability. For each input query, we show four responses generated by each method and an additional intermediate utterance (marked with underline) for our model.\ndings; and Extreme takes the most extreme value among all words for each dimension of word embeddings in a sentence. Since multiple references exist, for each utterance to be evaluated, we compute its score with the most similar reference.\nHuman Evaluation with Case Analysis: As automatic evaluation metrics lose sight of the overall quality of a response (Tao et al., 2018), we also adopt human evaluation on 100 random samples to assess the generation quality with three independent aspects considered: relevance (whether the reply is relevant to the query), diversity (whether the reply narrates with diverse words) and readability (whether the utterance is grammatically formed). Each property is assessed with a score from 1 (worst) to 5 (best) by three annotators. The evaluation is conducted in a blind process with the utterance belonging unknown to the reviewers.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Implementation Details", "text": "All models are trained with the following hyperparameters: both encoder and decoder are set to one layer with GRU  cells, where the hidden state size of GRU is 256; the utterance length is limited to 50; the vocabulary size is 50,000 and the word embedding dimension is 256; the word embeddings are shared by the encoder and decoder; all trainable parameters are initialized from a uniform distribution [-0.08, 0.08]; we employ the Adam (Kingma and Ba, 2014) for optimization with a mini-batch size 128 and initialized learning rate 0.001; the gradient clipping strategy is utilized to avoid gradient explosion, where the gradient clipping value is set to be 5. For the latent variable, we adopt dimensional size 256 and the component number of the mixture Gaussian for prior networks in WAE is set to 5. As to the discriminator, we set the initialized learning rate as 0.0002 and use 128 different kernels for each kernel size in {2, 3, 4}. The size of the response bag is limited to 10 where the instances inside are randomly sampled for each mini-batch. All the models are implemented with Pytorch 0.4.1 4 .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results and Analysis", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Comparison against Baselines", "text": "Table 1 shows our main experimental results, with baselines shown in the top and our models at the bottom. The results show that our model (Ours) outperforms competitive baselines on various evaluation metrics. The Seq2seq based models (S2S, S2S-DB and MMS) tend to generate fluent utterances and can share some overlapped words with the references, as the high BLEU-2 scores show. However, the distinctness scores illustrate that these models fail to generate multiple diverse responses in spite of the diversitypromoting objective and responding mechanisms used. We attribute this to that these models fail to consider multiple references for the same query, which may confuse the models and lead to a commonplace utterance. As to the CVAE and WAE models, with the latent variable to control the discourse-level diversity, diverse responses can be obtained. Compared against these previous methods, our model can achieve the best or second best performances on different automatic evaluation metrics where the improvements are most consistent on BLEU-1 and embedding-based metrics, which demonstrates the overall effectiveness of our proposed architecture.\nIn order to better study the quality of generated responses, we also report the human evaluation results in Table 2. As results show, although there remains a huge gap between existing methods and human performance (the Gold), our model gains promising promotions over previous methods on generating appropriate responses with diverse expressions. With both obvious superiority (readability for S2S and diversity for CVAE) and inferiority (diversity for S2S and relevance for CVAE), the baselines show limited overall performances, in contrast to which our method can output more diverse utterances while maintaining the relevance to the input query and achieve a high overall score.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ablation Study", "text": "To better understand the effectiveness of each component in our model, we further conduct the ablation studies with results shown at the bottom of Table 1. Above all, to validate the effectiveness of the common feature, we remove the first generation stage and get the Ours-First model. As the results of BLEU and embedding-based metrics show, the system can benefit from the common feature for better relevance to the query.\nMoreover, pairwise comparisons Ours-Disc vs. Ours and Ours-MBOW vs. Ours validate the effects of the discriminator and modified multireference bag-of-word loss (MBOW). As results show, the discriminator facilitates extracting the common feature and yields more relevant responses to the input query afterward. The MBOW loss, similar to the effects of BOW loss in the CVAE, can lead to a more unique latent variable for each response and improve the final distinctness scores of generated utterances. In the experiments, we also observed the KL vanishing problem when training our model and we overcame it with the KL weight annealing strategy and the MBOW loss described above.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Case Study and Discussion", "text": "Table 3 illustrates two examples of generated replies to the input query got from the testing set. Comparing the CVAE and Ours, we can find that although the CVAE model can generate diverse utterances, its responses tend to be irrelevant to the query and sometimes not grammatically formed, e.g. the words \"glowworm\" and \"robot\" in the sentences. In contrast, responses generated by our model show better quality, achieving both high relevance and diversity. This demonstrates the ability of the two-step generation architecture. For better insight into the procedure, we present the intermediately generated utterances which show that the feature extracted in the first stage can focus on some common and key aspects of the query and its possible responses, such as the \"amazing\" and \"software\". With the distinctive features sampled in the second generation phase, the model further revises the response and outputs multiple responses with diverse contents and expressions.\nRecap that the common feature is expected to capture the correlations of different responses and serve as the base of a response bag from which different responses are further generated, as shown in Figure 1. To investigate the actual performances achieved by our model, we compute the distance between the input query/intermediate utterance and gold references/generated responses and present the results in Figure 4. As shown, intermediate utterances obtained in the first generation phase tend to approximate multiple responses with similar distances at the same time. Comparing the generated responses and the references, we find that generated responses show both high relevant and irrelevant ratios, as the values near 0.00 and 1.00 show. This actually agrees well with our observation that the model may sometimes rely heavily on or ignore the prior common feature information. From a further comparison between the input query and the mid, we also observe that the intermediate utterance is more similar to final responses than the input query, which correlates well with our original intention shown in Figure 1.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "Conclusion and future work", "text": "In this paper, we tackle the one-to-many queryresponse mapping problem in open-domain conversation and propose a novel two-step generation architecture with the correlation of multiple valid responses considered. Jointly viewing the multiple responses as a response bag, the model extracts the common and distinct features of different responses in two generation phases respectively to output multiple diverse responses. Experimental results illustrate the superior performance of the proposed model in generating diverse and appropriate responses compared to previous representative approaches. However, the modeling of the common and distinct features of responses in our method is currently implicit and coarse-grained. Directions of future work may be pursuing betterdefined features and easier training strategies.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their constructive comments. This work was supported by the National Key Research and Development Program of China (No. 2017YFC0804001), the National Science Foundation of China (NSFC No. 61672058; NSFC No. 61876196). Rui Yan was sponsored by CCF-Tencent Open Research Fund and Alibaba Innovative Research (AIR) Fund.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2014", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Generating sentences from a continuous space", "journal": "", "year": "2016", "authors": "Luke Samuel R Bowman; Oriol Vilnis; Andrew Vinyals; Rafal Dai; Samy Jozefowicz;  Bengio"}, {"title": "Latent variable dialogue models and their diversity", "journal": "Short Papers", "year": "2017", "authors": "Kris Cao; Stephen Clark"}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "journal": "", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"title": "Variational autoregressive decoder for neural response generation", "journal": "", "year": "2018", "authors": "Jiachen Du; Wenjie Li; Yulan He; Ruifeng Xu; Lidong Bing; Xuan Wang"}, {"title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"title": "Dialogwae: Multimodal response generation with conditional wasserstein auto-encoder", "journal": "", "year": "2018", "authors": "Xiaodong Gu; Kyunghyun Cho; Jungwoo Ha; Sunghun Kim"}, {"title": "Toward controlled generation of text", "journal": "", "year": "2017", "authors": "Zhiting Hu; Zichao Yang; Xiaodan Liang; Ruslan Salakhutdinov; Eric P Xing"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Gans for sequences of discrete elements with the gumbel-softmax distribution", "journal": "", "year": "2016", "authors": "J Matt; Jos\u00e9 Miguel Hern\u00e1ndez-Lobato Kusner"}, {"title": "A diversity-promoting objective function for neural conversation models", "journal": "", "year": "2016", "authors": "Jiwei Li; Michel Galley; Chris Brockett; Jianfeng Gao; Bill Dolan"}, {"title": "A simple, fast diverse decoding algorithm for neural generation", "journal": "", "year": "2016", "authors": "Jiwei Li; Will Monroe; Dan Jurafsky"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"title": "Learning endto-end goal-oriented dialog with multiple answers", "journal": "", "year": "2018", "authors": "Janarthanan Rajendran; Jatin Ganhotra; Satinder Singh; Lazaros Polymenakos"}, {"title": "Modeling situations in neural chat bots", "journal": "", "year": "2017", "authors": "Shoetsu Sato; Naoki Yoshinaga; Masashi Toyoda; Masaru Kitsuregawa"}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "journal": "", "year": "2017", "authors": "Iulian Vlad Serban; Alessandro Sordoni; Ryan Lowe; Laurent Charlin; Joelle Pineau; C Aaron; Yoshua Courville;  Bengio"}, {"title": "Neural responding machine for short-text conversation", "journal": "", "year": "2015", "authors": "Lifeng Shang; Zhengdong Lu; Hang Li"}, {"title": "Learning to converse with noisy data: Generation with calibration", "journal": "", "year": "2018", "authors": "Mingyue Shang; Zhenxin Fu; Nanyun Peng; Yansong Feng; Dongyan Zhao; Rui Yan"}, {"title": "Generating long and diverse responses with neural conversation models", "journal": "", "year": "2016", "authors": "Louis Shao; Stephan Gouws; Denny Britz; Anna Goldie; Brian Strope; Ray Kurzweil"}, {"title": "Learning structured output representation using deep conditional generative models", "journal": "", "year": "2015", "authors": "Kihyuk Sohn; Honglak Lee; Xinchen Yan"}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "journal": "", "year": "2015", "authors": "Alessandro Sordoni; Michel Galley; Michael Auli; Chris Brockett; Yangfeng Ji; Margaret Mitchell; Jian-Yun Nie; Jianfeng Gao; Bill Dolan"}, {"title": "Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems", "journal": "", "year": "2018", "authors": "Chongyang Tao; Lili Mou; Dongyan Zhao; Rui Yan"}, {"title": "Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space", "journal": "", "year": "2017", "authors": "Liwei Wang; Alexander Schwing; Svetlana Lazebnik"}, {"title": "Topic aware neural response generation", "journal": "", "year": "2017", "authors": "Chen Xing; Wei Wu; Yu Wu; Jie Liu; Yalou Huang; Ming Zhou; Wei-Ying Ma"}, {"title": "Conditional image generation from visual attributes", "journal": "", "year": "2015", "authors": "Xinchen Yan; Jimei Yang; Kihyuk Sohn; Honglak Lee"}, {"title": "Towards implicit contentintroducing for generative short-text conversation systems", "journal": "", "year": "2017", "authors": "Lili Yao; Yaoyuan Zhang; Yansong Feng; Dongyan Zhao; Rui Yan"}, {"title": "Tailored sequence to sequence models to different conversation scenarios", "journal": "", "year": "2018", "authors": "Hainan Zhang; Yanyan Lan; Jiafeng Guo; Jun Xu; Xueqi Cheng"}, {"title": "Personalizing dialogue agents: I have a dog", "journal": "", "year": "2018", "authors": "Saizheng Zhang; Emily Dinan; Jack Urbanek; Arthur Szlam; Douwe Kiela; Jason Weston"}, {"title": "Adversarially regularized autoencoders", "journal": "", "year": "2018", "authors": "Junbo Zhao; Yoon Kim; Kelly Zhang; Alexander Rush; Yann Lecun"}, {"title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders", "journal": "Long Papers", "year": "2017", "authors": "Tiancheng Zhao; Ran Zhao; Maxine Eskenazi"}, {"title": "Mechanism-aware neural machine for dialogue response generation", "journal": "", "year": "2017", "authors": "Ganbin Zhou; Ping Luo; Rongyu Cao; Fen Lin; Bo Chen; Qing He"}, {"title": "Elastic responding machine for dialog generation with dynamically mechanism selecting", "journal": "AAAI", "year": "2018", "authors": "Ganbin Zhou; Ping Luo; Yijun Xiao; Fen Lin; Bo Chen; Qing He"}, {"title": "Emotional chatting machine: Emotional conversation generation with internal and external memory", "journal": "", "year": "2018", "authors": "Hao Zhou; Minlie Huang; Tianyang Zhang; Xiaoyan Zhu; Bing Liu"}, {"title": "Multi-instance learning: A survey", "journal": "", "year": "2004", "authors": "Zhi-Hua Zhou"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An illustration of the two-step generation architecture. Different from the conventional methods (shown in green color) which model each response from scratch every time, our method first builds a common feature of multiple responses and models each response based on it afterward.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: The sentence embedding function of the discriminator in the first generation phase.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: The statistics of distances between the input query/intermediate utterance and gold references/generated responses, where the distance is measured by the cosine similarity of sentence embeddings.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Automatic evaluation results of different models where the best results are bold. The G, A and E of Embedding represent Greedy, Average, Extreme embedding-based metrics, repsectively.", "figure_data": "Method Rela. Divt. Red. OverallGold3.90 4.22 3.793.97S2S3.10 2.77 3.243.07CVAE2.98 3.12 3.103.07Ours3.22 3.19 3.233.21"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Human evaluation results of different models. Rela., Divt. and Red. represent Relevance, Diversity and Readability, respectively. The Kappa score among different human evaluators is 0.4412, which indicates moderate human agreements.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Gold \u5927\u81ea\u7136\u624d\u662f\u4eba\u7c7b\u7684\u6700\u7ec8boss\u3002 \u95ee\u4e2a\u767d\u75f4\u95ee\u9898\u5fc5\u987b\u6b63\u7248\u624d\u80fd\u5347\u7ea7\u5427\uff1f Nature is the final boss of human. May I ask an idiot problem. Does the update require a license? \u771f\u5e05\uff0c12\u6708\u4efd\u7684\u65f6\u5019\u5c31\u80fd\u4eb2\u773c\u770b\u5230\u4e86\uff0c\u597d\u5f00\u5fc3\u554a\u3002 \u4e0d\u662f\u7ed9\u5e73\u677f\u7535\u8111\u7528\u7684\u7cfb\u7edf\u5417\uff1f So cool! I am so happy to see it by myself in December. Isn't this system for PAD? \u88ab\u60ca\u8273\u9707\u64bc\u5230\u4e86\u3002 \u5df2\u7ecf\u7528\u4e86\u4e00\u4e2a\u591a\u6708\u4e86\uff0c\u4e0d\u8fc7\u8fd8\u662f\u4e0d\u559c\u6b228 I am deeply surprised and shocked. I have used it for a month but I still don't like it 8 \u9707\u64bc\u4e86\uff0c\u5c0f\u5c0f\u4eba\u7c7b\u4ef0\u89c6\u9020\u7269\u4e3b\u7684\u5f3a\u5927\u3002 \u597d\u4e45\u6728\u7528\u7535\u8111\u4e86\uff0c\u60f3\u5ff5\u3002 Shocked! The imperceptible humanity looks up to the power of the creator.Having not used the computer for a long time, I miss it.", "figure_data": "CVAE\u5927\u534a\u591c\u7684\u4e0d\u5149\u662f\u767d\u5929\u3002\u8fd9\u662f\u8981\u7528\u624b\u673a\u5417\uff1fIt's midnight, not only daytime.Do you want to use the phone?\u4e00\u5929\u4e00\u5929\u5c31\u80fd\u770b\u5230\u4e86\u3002\u6211\u662f\u5347\u7ea7\u4e86\u5347\u7ea7\u7248\u4e86\u3002We can see it day after day.I have updated to the upgrade.\u5929\u5730\u4e4b\u95f4\u7684\u98ce\u666f\u6709\u5982\u6b64\u4e4b\u7f8e\u3002\u6211\u8fd8\u4ee5\u4e3a\u662f\u6211\u7684\u7535\u8111\u3002How could there exist such amazing sights.I thought it was my computer.\u706b\u5c71\u55b7\u53d1\u77ac\u95f4\u7684\u8424\u706b\u866b\u3002\u5347\u7ea7\u7248\u7684\u673a\u5668\u4eba\u3002The glowworm at the moment of volcanic eruption.The upgraded robot.Ours\u597d\u7f8e\uff0c\u8fd9\u662f\u54ea\u91cc\u5440\uff1f\u8fd9\u662f\u4ec0\u4e48\u8f6f\u4ef6\u554a\uff0c\u6c42\u89e3\u3002So amazing! Where is this?I am wondering what software it is.\u597d\u58ee\u89c2\u554a\u4e00\u5b9a\u8981\u4fdd\u5b58\u4e0b\u6765\u3002\u6211\u89c9\u5f97\u5fae\u8f6f\u7684ui\u8fd8\u4e0d\u9519\u3002It's so magnificent that it should be preserved.I think the ui of Microsoft is not bad.\u5927\u767d\u5929\u7684\u4e0d\u80fd\u770b\u5230\u3002\u73b0\u5728\u7684\u4ea7\u54c1\u5df2\u7ecf\u4e0d\u662f\u65b0\u4ea7\u54c1\u4e86\u3002It can't be seen during the day.The current product is not the new.\u5982\u679c\u6709\u673a\u4f1a\u4eb2\u773c\u6240\u89c1\u8fc7\u3002\u8fd9\u4e2a\u662f\u4ec0\u4e48\u5e94\u7528\u554a\u3002If you have chance to see it yourself."}], "doi": ""}