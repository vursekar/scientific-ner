{"authors": "Minki Kang; Jinheon Baek; Sung Ju Hwang", "pub_date": "", "title": "KALA: Knowledge-Augmented Language Model Adaptation", "abstract": "Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM's performance on the downstream task by causing catastrophic forgetting of its general knowledge. To overcome such limitations of adaptive pre-training for PLM adaption, we propose a novel domain adaption framework for PLMs coined as Knowledge-Augmented Language model Adaptation (KALA), which modulates the intermediate hidden representations of PLMs with domain knowledge, consisting of entities and their relational facts. We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains. The results show that, despite being computationally efficient, our KALA largely outperforms adaptive pre-training. Code is available at: https://github.com/Nardien/KALA.", "sections": [{"heading": "Introduction", "text": "Pre-trained Language Models (PLMs) (Devlin et al., 2019;Brown et al., 2020) have shown to be effective on various Natural Language Understanding (NLU) tasks. Although PLMs aim to address diverse downstream tasks from various data sources, there have been considerable efforts to adapt the PLMs to specific domains -distributions over the language characterizing a given topic or genre (Gururangan et al., 2020)-for which the acquisition of domain knowledge is required to accurately solve the downstream tasks (e.g., Biomedical Named Entity Recognition (Dogan et al., 2014)). This problem, known as Language Model Adaptation, can be viewed as a transfer learning problem (Yosinski et al., 2014;Ruder, 2019) under domain shift, where the model is pre-trained on the general domain and the labeled distribution is available for the target domain-specific task. The most prevalent approach to this problem is adaptive pre-training (Figure 2a) which further updates all parameters of the PLM on a large domain-specific or curated task-specific corpus, with the same pretraining strategy (e.g., masked language modeling) before fine-tuning it on the downstream task (Beltagy et al., 2019;Gururangan et al., 2020). This continual pre-training of a PLM on the target domain corpus allows it to learn the distribution of the target domain, resulting in improved performance on domain-specific tasks (Howard and Ruder, 2018;Han and Eisenstein, 2019).\nWhile it has shown to be effective, adaptive pretraining has obvious drawbacks. First, it is computationally inefficient. Although a PLM becomes more powerful with the increasing amount of pretraining data (Gururangan et al., 2020), further pre-training on the additional data requires larger memory and computational cost as the dataset size grows (Bai et al., 2021). Besides, it is difficult to adapt the PLM to a new domain without forgetting the general knowledge it obtained from the initial pretraining step, since all pre-trained parameters are continually updated to fit the domain-specific corpus during adaptive pre-training . This catastrophic forgetting of the task- Our KALA framework embeds the unseen entities on the embedding space of seen entities by representing them with their relational knowledge over the graph, while the strong DAPT baseline (Gururangan et al., 2020) cannot appropriately handle unseen entities that are not given for task fine-tuning.\ngeneral knowledge may lead to the performance degradation on the downstream tasks. In Figure 1, we show that adaptive pre-training with more training steps could lead to performance degeneration. Thus, it would be preferable if we could adapt the PLM to the domain-specific task without costly adaptive pre-training. To this end, we aim to integrate the domain-specific knowledge into the PLM directly during the task-specific fine-tuning step, as shown in Figure 2b, eliminating the adaptive pre-training stage. Specifically, we first note that entities and relations are core building blocks of the domain-specific knowledge that are required to solve for the domain-specific downstream tasks. Clinical domain experts, for example, are familiar with medical terminologies and their complex relations. Then, to represent the domain knowledge consisting of entities and relations, we introduce the Entity Memory, which is the source of entity embeddings but independent of the PLM parameters (See Entity Memory in Figure 2b). Then, we further exploit the relational structures of the entities by utilizing a Knowledge Graph (KG), which denotes the factual relationships between entities, as shown in Knowledge Graph of Figure 2b.\nThe remaining step is how to integrate the knowledge into the PLM during fine-tuning. To this end, we propose a novel layer named Knowledgeconditioned Feature Modulation (KFM, \u00a73.2), which scales and shifts the intermediate hidden representations of PLMs by conditioning them with retrieved knowledge representations. This knowledge integration scheme has several advantages. First, it does not modify the original PLM architecture, and thus could be integrated into any PLMs regardless of their architectures. Also, it only re-quires marginal computational and memory overhead, while eliminating the need of excessive further pre-training (Figure 1). Finally, it can effectively handle unseen entities with relational knowledge from the KG, which are suboptimally embedded by adaptive pre-training. For example, as shown in Figure 2, an entity restenosis does not appear in the training dataset for fine-tuning, thus adaptive pre-training only implicitly infers them within the context from the broad domain corpus. However, we can explicitly represent the unknown entity by aggregating the representations of known entities in the entity memory (i.e., in Figure 2, neighboring entities, such as asthma and pethidine, are used to represent the unseen entity restenosis).\nWe combine all the previously described components into a novel language model adaptation framework, coined as Knowledge-Augmented Language model Adaptation (KALA) (Figure 3). We empirically verify that KALA improves the performance of the PLM over adaptive pre-training on various domains with two knowledge-intensive tasks: Question Answering (QA) and Named Entity Recognition (NER). Our contribution is threefold:\n\u2022 We propose a novel LM adaptation framework, which augments PLMs with entities and their relations from the target domain, during fine-tuning without any further pre-training. To our knowledge, this is the first work that utilizes the structured knowledge for language model adaptation.\n\u2022 To reflect structural knowledge into the PLM, we introduce a novel layer which scales and shifts the intermediate PLM representations with the entity representations contextualized by their related entities according to the KG.\n\u2022 We show that our KALA significantly enhances the model's performance on domain-specific QA and NER tasks, while being significantly more efficient over existing LM adaptation methods.", "n_publication_ref": 13, "n_figure_ref": 9}, {"heading": "Related Work", "text": "Language Model Adaptation Nowadays, transfer learning (Howard and Ruder, 2018) is a dominant approach for solving Natural Language Understanding (NLU) tasks. This strategy first pretrains a Language Model (LM) on a large and unlabeled corpus, then fine-tunes it on downstream tasks with labeled data (Devlin et al., 2019). While this scheme alone achieves impressive performance on various NLU tasks, adaptive pre-training of the PLM on a domain-specific corpus helps the PLM achieve better performance on the domain-specific tasks. For example,  demonstrated that a further pre-trained LM on biomedical documents outperforms the original LM on biomedical NLU tasks. Also, Gururangan et al. (2020) showed that adaptive pre-training of the PLM on the corpus of a target domain (Domain-adaptive Pre-training; DAPT) or a target task (Task-adaptive Pre-training; TAPT) improves its performance on domain-specific tasks. However, above approaches generally require a large amount of computational costs for pre-training.\nKnowledge-aware LM Accompanied with increasing sources of knowledge (Vrandecic and Kr\u00f6tzsch, 2014), some prior works have proposed to integrate external knowledge into PLMs, to enhance their performance on tasks that require structured knowledge. For instance, ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019) incorporate entities as additional inputs in the pretraining stage to obtain a knowledge-aware LM, wherein a pre-trained knowledge graph embedding from Wikidata (Vrandecic and Kr\u00f6tzsch, 2014) is used to represent entities. Entity-as-Experts (F\u00e9vry et al., 2020) and LUKE (Yamada et al., 2020) use the entity memory that is pre-trained along with the LMs from scratch. ERICA (Qin et al., 2021) further uses the fact consisting of entities and their relations in the pre-training stage of LMs from scratch. Previous works aim to integrate external knowledge into the LMs during the pre-training step to obtain a universal knowledge-aware LM that requires additional parameters for millions of entities. In contrast to this, our framework aims to efficiently modify a general PLM for the domain-specific task with a linear modulation layer scheme discussed in Section 3.2, during fine-tuning.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Method", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Problem Statement", "text": "Our goal is to solve Natural Language Understanding (NLU) tasks for a specific domain, with a knowledge-augmented Language Model (LM). We first introduce the NLU tasks we target, followed by the descriptions of the proposed knowledgeaugmented LM. After that, we formally define the ingredients for structured knowledge integration.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "NLU tasks", "text": "The goal of an NLU task is to predict the label y of the given input instance x, where the input x contains the sequence of tokens (Devlin et al., 2019):\nx = [w 1 , w 2 , . . . , w |x| ]. Then, given a training dataset D = {(x (i) , y (i) )} N i=1\n, the objective is to maximize the log-likelihood as follows:\nmax \u03b8 L(\u03b8) := max \u03b8 (x,y)\u223cD log p(y|x; \u03b8), p(y|x; \u03b8) = g(H; \u03b8 g ), H = f (x; \u03b8 f ),\nwhere f is an encoder of the PLM which outputs contextualized representation H from x, and g is a decoder which models the probability distribution p of the label y, with trainable parameters \u03b8 = (\u03b8 f , \u03b8 g ). If the LM is composed of L-layers of transformer blocks (Devlin et al., 2019), the function f is decomposed to multiple functions f = [f 0 , . . . , f L ], where each block gets the output of the previous block as the input:\nH l = f l (H l\u22121 ). 1\nKnowledge-Augmented Language Model The conventional learning objective defined above might be sufficient for understanding the texts if the tasks require only the general knowledge stored in PLMs. However, it is suboptimal for tackling domain-specific tasks since the general knowledge captured by the parameters \u03b8 f may not include the knowledge required for solving the domain-specific tasks. Thus, contextualizing the texts by the domain knowledge, captured by the domain-specific entities and their relations, is more appropriate for handling such domain-specific problems.\nTo this end, we propose a function h(\u2022; \u03c6) which augments PLMs conditioned on the domain knowledge. Formally, the objective for a NLU task with our knowledge-augmented LM is given as follows:\nmax \u03b8,\u03c6 L(\u03b8, \u03c6) := max \u03b8,\u03c6 (x,y)\u223cD log p(y|x; \u03b8, \u03c6), p(y|x; \u03b8, \u03c6) = g(H; \u03b8 g ), H l = f l (H l\u22121 , h l (H l\u22121 , E, M, G; \u03c6); \u03b8 f l ),\nwhere \u03c6 is parameters for the function h, E is the set of entities, M is the set of corresponding mentions, and G is a knowledge graph. In the following, we will describe the definition of the knowledgerelated inputs E, M, G, and the details of h(\u2022, \u03c6).\nDefinition 1 (Entity and Mention). Given a sequence of tokens x = [w 1 , . . . , w |x| ], let E be a set of entities in x. Then an entity e \u2208 E is composed of one or multiple adjacent tokens within the input text: [w m \u03b1 , . . . , w m \u03c9 ] x 2 . Here, m = (m \u03b1 , m \u03c9 ) is a mention that denotes the start and end locations for the entity within the input tokens x, which term is commonly used for defining entities (F\u00e9vry et al., 2020). Consequently, for each given input x (i) , there are a set of entities E (i) = {e 1 , . . . , e K } and their corresponding mentions M (i) = {m 1 , . . . , m K }. For example, given an input x = [New, York, is, a, city], we have two entities E = {New_York, city} and their associated mentions M = {(1, 2), (4, 4)}.\nWe further construct the entity vocabulary E train = N i=1 E (i) , which consists of all entities appearing in the training dataset. However, at test time, we may encounter unseen entities that are not in E train . To tackle this, we regard unknown entities as the null entity e \u2205 , so that \u2200e \u2208 E train \u222a {e \u2205 }.\nDefinition 2 (Entity Memory). Given a set of all entities E train \u222a {e \u2205 }, we represent them in the continuous vector (feature) space to learn meaningful entity embeddings. In order to implement this, we define the entity memory E \u2208 R (|E train |+1)\u00d7d that comprises of an entity e \u2208 R as a key and its embedding e \u2208 R d as its value. Also, to access the value in the entity memory, we define the point-wise memory access function EntEmbed which takes an entity as an input. For instance, e = EntEmbed(New_York) returns the embedding of the New_York entity, and e = EntEmbed(e \u2205 ) returns the zero embedding. This entity memory E is the part of the parameter \u03c6 used in function h. Definition 3 (Knowledge Graph). Since the entity memory alone cannot represent relational information between entities, we further define a Knowledge Graph (KG) G that consists of a set of factual triplets {(h, r, t)}, where the head and the tail entities, h and t, are the elements of E, and a relation r is an element of a set of relations R: h, t \u2208 E and r \u2208 R. We assume that a preconstructed KG G (i) is given for each input x (i) , and provide the details of the KGs and how to construct them in Appendix A.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Knowledge-conditioned Feature Modulation on Transformer", "text": "The remaining problem is how to augment a PLM by conditioning it on the domain-specific knowledge, through the function h. An effective approach to do so without stacking additional layers on top of the LM is to interleave the knowledge from h with the pre-trained parameters of the language model (Devlin et al., 2019) consisting of transformer layers (Vaswani et al., 2017). Before describing our interleaving method in detail, we first describe the Transformer architecture.\nTransformer Given |x| token representations\nH l\u22121 = [h l\u22121 1 , . . . , h l\u22121 |x| ] \u2208 R |x|\u00d7d from the layer l \u2212 1\nwhere d is the embedding size, each transformer block outputs the contextualized representations for all tokens. In detail, the l-th block consists of the multi-head self-attention (Attn) layer and the residual feed-forward (FF) layer as follows:\nH l = LN (H l\u22121 + Attn(H l\u22121 )) F F (\u0124 l ) = \u03c3(\u0124 l \u2022 W 1 ) \u2022 W 2 , H l = LN (\u0124 l + F F (\u0124 l )),\nwhere LN is a layer normalization (Ba et al., 2016), \u03c3 is an activation function (Hendrycks and Gimpel, 2016), W 2 \u2208 R d \u00d7d and W 1 \u2208 R d\u00d7d are weight matrices, and d is an intermediate hidden size. We omit the bias term for brevity.\nLinear Modulation on Transformer An effective yet efficient way to fuse knowledge from different sources without modifying the original model architecture is to scale and shift the features of one source with respect to the data from another source . This scheme of feature-wise affine transformation is effective on various tasks, such as language-conditioned image reasoning  or style-transfer in image generation (Huang and Belongie, 2017  Motivated by them, we propose to linearly transform the intermediate features after the layer normalization of the transformer-based PLM, conditioned on the knowledge sources E, M, G. We term this method as the Knowledge-conditioned Feature Modulation (KFM), described as follows:\n\u0393, B,\u0393,B = h l (H l\u22121 , E, M, G; \u03c6), H l = \u0393 \u2022 LN (H l\u22121 + Attn(H l\u22121 )) + B, F F (\u0124 l ) = \u03c3(\u0124 l \u2022 W 1 ) \u2022 W 2 , H l =\u0393 \u2022 LN (\u0124 l + F F (\u0124 l )) +B,(1)\nwhere H l\u22121 \u2208 R |x|\u00d7d is the matrix of hidden representations from the previous layer, \u2022 denotes the hadamard (element-wise) product, and\n\u0393 = [\u03b3 1 , . . . , \u03b3 |x| ] \u2208 R |x|\u00d7d , B = [\u03b2 1 , . . . , \u03b2 |x| ] \u2208 R |x|\u00d7d .\n\u0393 and B are learnable modulation parameters from the function h, which are conditioned by the entity representation. For instance, in Figure 3, \u03b3 and \u03b2 for token 'New' are conditioned on the corresponding entity New_York. However, if tokens are not part of any entity (e.g., 'is'), \u03b3 and \u03b2 for such tokens are fixed to 1 and 0, respectively. One notable advantage of our KFM is that multiple tokens associated to the identical entity are affected by the same modulation (e.g., 'New' and 'York' in Figure 3), which allows the PLM to know which adjacent tokens are in the same entity. This is important for representing the tokens of the domain entity (e.g., 'cod' and 'on'), since the original PLM might regard them as separate, unrelated tokens (See analysis in \u00a75.5 with Figure 5). However, with our KFM, the PLM can identify associated tokens and embed them to be close to each other.\nThen, how can we design such functional operations in h? The easiest way is to retrieve the entity embedding of e, associated to the typical to-ken, from the entity memory E, and then use the retrieved entity embedding as the input to obtain \u03b3 and \u03b2 for every entity (See Figure 3). Formally, for each entity e \u2208 E and its mention (m \u03b1 , m\n\u03c9 ) \u2208 M, v = EntEmbed(e)\n(2)\n\u03b3 j = 1 + h 1 (v), \u03b2 j = h 2 (v), \u03b3 j = 1 + h 3 (v),\u03b2 j = h 4 (v), m \u03b1 \u2264 j \u2264 m \u03c9 ,\nwhere v is the retrieved entity embedding from the entity memory, h 1 , h 2 , h 3 , and h 4 are mutually independent Multi-Layer Perceptrons (MLPs) which return a zero vector 0 if e = e \u2205 .", "n_publication_ref": 5, "n_figure_ref": 4}, {"heading": "Relational Retrieval from Entity Memory", "text": "Although the simple access to the entity memory can retrieve the necessary entity embeddings for the modulation, this approach has obvious drawbacks as it not only fails to reflect the relations with other entities, but also regards unseen entities as the same null entity e \u2205 . If so, all unseen entities are inevitably modulated by the same parameters even if they have essentially different meaning.\nTo tackle these limitations, we further consider the relational information between two entities that are linked with a particular relation. For example, the entity New_York alone will not give meaningful information. However, with two associated facts (New_York, instance of, city) and (New_York, country, USA), it is clear that New_York is a city in the USA. Motivated by this observation, we propose Relational Retrieval which leverages a KG G to retrieve entity embeddings from the memory, according to the relations defined in the given KG (See Figure 3, right).\nMore specifically, our goal is to effectively utilize the relations among entities in G, to improve the EntEmbed function in equation 2. We tackle this objective by utilizing a Graph Neural Network (GNN) which learns feature representations of each node using a neighborhood aggregation scheme (Hamilton et al., 2017), as follows:\nv = UPDATE(EntEmbed(e), AGG({EntEmbed(\u00ea) : \u2200\u00ea \u2208 N (e; G)})),\nwhere N (e; G) is a set of neighboring entities of the entity e, AGG is the function that aggregates embeddings of neighboring entities of e, and UPDATE is the function that updates the representation of e with the aggregated messages from AGG. However, simple aggregation (e.g., mean) cannot reflect the relative importance on neighboring nodes, thus we consider the attentive scheme (Velickovic et al., 2018;Brody et al., 2021) for neighborhood aggregation, to allocate weights to the target entity's neighbors by their importance. This scheme is helpful in filtering out less useful relations. Formally, we first define a scoring function \u03c8 that calculates a score for every triplet (e i , r ij , e j ), which is then used to weigh each node during aggregation: e i = EntEmbed(e i ), e j = EntEmbed(e j ), e * = [e i r ij e j h e i ],\n\u03c8(e i , r ij , e j , h e i ) = a \u03c3(W \u2022 e * ),\nwhere \u03c3 is a nonlinear activation, e * \u2208 R 4d is concatenated vector where denotes the concatenation, a \u2208 R d and W \u2208 R d\u00d74d are learnable parameters, r ij \u2208 R d is a embedding of the relation, and h e i \u2208 R d is a context representation of the entity e i obtained from the intermediate hidden states of the LM 3 .\nThe scores obtained from \u03c8 are normalized across all neighbors e j \u2208 N (e i ; G) with softmax:\n\u03b1 ij = softmax(\u03c8(e i , r ij , e j )) = exp(\u03c8(e i , r ij , e j ))\ne j \u2208N (e i ;G) exp(\u03c8(e i , r ij , e j ))\n.\nThen, we update the entity embedding with a weighted average of the neighboring nodes with \u03b1 as an attention coefficient, denoted as follows:\nv = UPDATE e j \u2208N (e i ;G) \u03b1 ij \u2022 e j . (3\n) 1 m \u03c9 \u2212m \u03b1 +1 m \u03c9 i=m \u03b1 h l\u22121 i\nBy replacing the EntEmbed function in equation 2 with the above GNN in equation 3, we now represent each entity with its relational information in KG. This relational retrieval has several advantages over simple retrieval of a single entity from the entity memory. First, the relational retrieval with KG can consider richer interactions among entities, as described in Figure 3.\nIn addition, we can naturally represent an unseen entity -which is not seen during training but appears at test time -through neighboring aggregation, which is impossible only with the entity memory. In Figure 2, we provide an illustrative example of the unseen entity representation, where the unseen entity restenosis is represented with a weighted sum of representations of its neighboring entities myocardial_infarction, asthma, and pethidine, which is beneficial when the set of entities for training and test datasets have small overlaps.", "n_publication_ref": 3, "n_figure_ref": 3}, {"heading": "Experiment", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Tasks and Datasets", "text": "We evaluate our model on two NLU tasks: Question Answering (QA) and Named Entity Recognition (NER). For QA, we use three domain-specific datasets: NewsQA (News, Trischler et al., 2017) and two subsets (Relation, Medication) of EMRQA (Clinical, Pampari et al., 2018). We use the Exact-Match (EM) and the F1 score as evaluation metrics. For NER, we use three datasets from different domains, namely CoNLL-2003 (News, Sang andMeulder, 2003), WNUT-17 (Social Media, Derczynski et al., 2017) and NCBI-Disease (Biomedical, Dogan et al., 2014). We use the F1 score as the evaluation metric. We report statistics and detailed descriptions of each dataset in Appendix B.2.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Baselines", "text": "A direct baseline of our KALA is the adaptive pre-training, which is commonly used to adapt the PLM independent to the choice of a domain and task. Also, to compare ours against a more powerful baseline, we modify a recent method  that alleviates forgetting of PLM during fine-tuning. Details for each baseline we use are described as follows:\n1. Vanilla Fine-Tuning (FT): A baseline that directly fine-tunes the LM on downstream tasks.\n2. Fine-Tuning + more params: A baseline with one more transformer layer at the end of the means and standard deviations of performances over five different runs with Exact Match / F1 score as a metric. The numbers in bold fonts denote the best score. \u2020 indicates the method under an extremely high computational resource setting (See Figure 1).  LM. We use this baseline to show that the performance gain of our model does not come from the use of additional parameters. 6. KALA (pointwise): A variant of KALA that only uses the entity memory and does not use the knowledge graphs.\n7. KALA (relational): Our full model that uses KGs to perform relational retrieval from the entity memory.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Experimental Setup", "text": "We use the uncased BERT-base (Devlin et al., 2019) as the base PLM for all our experiments on QA and NER tasks. For more details on training and implementation, please see the Appendix B.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experimental Results", "text": "Performance on QA and NER tasks On both extractive QA and NER tasks, our KALA outperforms all baselines, including TAPT and TAPT+RedcAdam (Gururangan et al., 2020;, as shown in Table 1 and 2. These results show that our KALA is highly effective for the language model adaptation task. KALA also largely outperforms DAPT (Gururangan et al., 2020) which is trained with extra data and requires a significantly higher computational cost compare to KALA (See Figure 1 for the plot of efficiency, discussed in Section 5.3).\nEffect of Using more Parameters One may suspect whether the performance of our KALA comes from the increment of parameters. However, the experimental results in Table 1 and 2 show that increasing the parameters for PLM during fine-tuning (+ more params) yields marginal performance improvements over naive fine-tuning. This result confirms that the performance improvement of KALA is not due to the increased number of parameters.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Importance of Relational Retrieval", "text": "The performance gap between KALA (relational) and KALA (point-wise) shows the effectiveness of relational retrieval for language model adaptation, which allows us to incorporate relational knowledge into the PLM. The relational retrieval also helps address unseen entities, as discussed in Section 5.4.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Analysis and Discussion", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ablation Studies", "text": "We perform an ablation study to see how much each component contributes to the performance gain.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "KFM Parameters", "text": "We first analyze the effect of feature modulation parameters (i.e., gamma and beta) in transformers by ablating a subset of them in Table 3, in which we observe that using both    gamma and beta after both layer normalization on a transformer layer obtains the best performance.\nArchitectural Variants We now examine the effectiveness of the proposed knowledge conditioning scheme in our KALA framework. To this end, we use or adapt the knowledge integration methods from previous literature, to compare their effectiveness. Specifically, we couple the following five components with KALA: Entity-as-Experts (F\u00e9vry et al., 2020), Adapter (Houlsby et al., 2019), KT-Net (Yang et al., 2019), ERNIE (Zhang et al., 2019), and ERICA (Qin et al., 2021). Note that, most of them were proposed for improving pre-training from scratch, while we adapt them for fine-tuning under our KALA framework (The details are given in Appendix B.4). As shown in Table 4, our KFM used in KALA outperforms all variants, demonstrating the effectiveness of feature modulation in the middle of transformer layers for fine-tuning.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Robustness to Other PLMs", "text": "Although we believe our experimental results on  more params on NewsQA). Thus, we believe that our KALA would be useful to any PLMs, not depending on specific PLMs.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Efficiency", "text": "Figure 1 illustrates the performance and training FLOPs of KALA against baselines on the NewsQA dataset. We observe that the performance of TAPT decreases with the increased number of iterations, which could be due to forgetting of the knowledge from the PLM. On the other hand, DAPT, while not suffering from performance loss, requires huge computational costs as it trains on 112 times larger data for further pre-training (See Appendix B.3 for detailed explanations on training data). On the other hand, our KALA outperforms DAPT without using external data, while requiring 17 times fewer computational costs, which shows that KALA is not only effective but also highly efficient.\nTo further compare the efficiency in various aspects, we report GPU memory, training wall time, and training FLOPs for baselines and ours in Table 6. Through this, we verify that our KALA is more efficient to train for language model adaptation settings than baselines. Note that the resource requirement of KALA could be further reduced by adjusting the size of the entity memory (e.g., removing less frequent entities). Therefore, to show the flexibility of our KALA on the typical resource constraint, we provide the experimental results on two different settings (i.e., tuning the number of entities in the entity memory) -KALA with memory size of 200 and 62.8k (full memory) in Appendix C.6.   ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Effectiveness on Unseen Entities", "text": "One remarkable advantage of our KALA is its ability to represent an unseen entity by aggregating features of its neighbors from a given KG. To analyze this, we first divide all contexts into one of Seen and Unseen, where Seen denotes the context with less than 3 unseen entities, and then measure the performance on the two subsets. As shown in Figure 4, we observe that the performance gain of KALA over the baselines is much larger on the Unseen subset, which demonstrates the effectiveness of KALA's relational retrieval scheme to represent unseen entities. DAPT also largely outperforms fine-tuning and TAPT as it is trained on an extremely large external corpus for adaptive pre-training. However, KALA even outperforms DAPT in most cases, verifying that our knowledgeaugmentation method is more effective for tackling domain-specific tasks. The visualization of embeddings of seen and unseen entities in Figure 2 shows that KALA embeds the unseen entities more closely to the seen entities 4 , which explains KALA's good performance on the Unseen subset.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Case Study", "text": "To better see how our KFM ( \u00a73.2) works, we show the context and its fact, and then visualize representations from the PLM modulated by the KFM.\nAs shown in Figure 5 right, the token '##on' is not aligned with their corresponding tokens, such as 'ex' (for exon) and 'cod' (for codon), in the baseline. However, with our feature modulation that transforms multiple tokens associated with the single entity equally, the two tokens (e.g., ('ex', '##on')), composing one entity, are closely embedded. Also, while the baseline cannot handle the unseen entity consisting of three tokens: 're', '##tina', and '##l', KALA embeds them closely by representing the unseen retinal from the representation of its neighborhood gene derived by the domain knowledge -(retinal, instance of, gene).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Extension to Generative Model", "text": "Our KALA framework is also applicable to encoder-decoder PLMs by applying the KFM to the encoder. Therefore, we further validate KALA's effectiveness on the encoder-decoder PLMs on the generative QA task (Lee et al., 2021) with T5small (Raffel et al., 2020). Table 7 shows that KALA largely outperforms baselines even with such a generative PLM.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we introduced KALA, a novel framework for language model adaptation, which modulates the intermediate representations of a PLM by conditioning it with the entity memory and the relational facts from KGs. We validated KALA on various domains of QA and NER tasks, on which KALA significantly outperforms relevant baselines while being computationally efficient. We demonstrate that the success of KALA comes from both KFM and relational retrieval, allowing the PLM to recognize entities but also handle unseen ones that might frequently appear in domain-specific tasks.\nThere are many other avenues for future work, including the application of KALA on pre-training of knowledge-augmented PLMs from scratch.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ethical Statements", "text": "Enhancing the domain converge of pre-traind language models (PLMs) with external knowledge is increasingly important, since the PLMs cannot observe all the data during training and cannot memorize all the necessary knowledge for solving down-stream tasks. Our KALA contributes to this problem by augmenting domain knowledge graphs for PLMs. However, we have to still consider the accurateness of knowledge, i.e., the fact in the knowledge graph may not be correct, which affects the model to generate incorrect answers. Also, the model's prediction performance is still far from optimal. Thus, we should be aware of model's failure from errors in knowledge and prediction, especially on high-risk domains (e.g., biomedicine). Fine-tuned model \"text\": \"Arvane Rezai\", \"start\": 30, \"end\": 43, \"id\": 228998 \"h\": 11578, \"r\": \"P3373\", \"t\": 228998  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Details on KG Construction", "text": "In this work, we propose to use the Knowledge Graph (KG) that can define the relational information among entities that only appear in each dataset. However, unfortunately, most of the task datasets do not contain such relational facts on its context, thus we need to construct them manually to obtain the knowledge graph. In this section, we explain the way of constructing the knowledge graph that we used, consisting of facts of entities for each context in the task dataset. Relation extraction is the way how we obtain the factual knowledge from the text of the target dataset. To do so, we first need to extract entities and their corresponding mentions from the text, and then link it to the existing entities in wikidata (Vrandecic and Kr\u00f6tzsch, 2014). In order to do this, we use the existing library named as spaCy 5 , and opensourced implementation of Entity Linker 6 . To sum up, in our work, a set of entities E (i) and corresponding mentions M (i) for the given input x (i) are obtained through this step. Regarding a concrete example, please see format (a) in Figure 6. In the example, \"Text\" indicates the entity mention within the input x, the \"start\" and \"end\" indicates its mention position denoted as (m \u03b1 , m \u03c9 ), and \"id\" indicates the wikidata id for the entity identification used in the next step.\nTo extract the relation among entities that we obtained above, we use the scheme of Relation Extraction (RE). In other words, we use the trained 5 https://spacy.io/ 6 https://github.com/egerber/spaCy-entity-linker RE model to build our own knowledge base (KB) instead of using the existing KG directly from the existing general-domain KB 7 . Specifically, we first fine-tune the BERT-base model (Devlin et al., 2019) for 2 epochs with 600k distantly supervised data used in Qin et al. (2021), where the Wikipedia document and the Wikidata triplets are aligned. Then, we use the fine-tuned BERT model to extract the relations between entity pairs in the text. We use the model with a simple bilinear layer on top of it, which is widely used scheme in the relation extraction literature (Yao et al., 2019). For an example of the extracted fact, please see format (b) in Figure 6. In the example, \"h\" denotes the wikidata id of the head entity, \"r\" denotes the wikidata id of the extracted relation, and \"t\" denotes the wikidata id of the tail entity. In the relation extraction, the model returns the categorical distribution over the top 100 frequent relations. In general, the relation of top-1 probability is used as the relation for the corresponding entity pair. However, this approach sometimes results in predicting no_relation on most entity pairs. Thus, to obtain more relations, we further use the relation of top-2 probability in the case where no_relation has a top-1  probability but the top-2 probability is larger than a certain threshold (e.g., > 0.1). In Figure 6, we summarize our KG construction pipeline. In Table 8, we report the hyperparameters related to our KG construction.", "n_publication_ref": 5, "n_figure_ref": 2}, {"heading": "B Experimental Setup", "text": "In this section, we introduce the detailed setups for our models and baselines used in Table 1, 2, and 4.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.1 Implementation Details", "text": "We use the Pytorch (Paszke et al., 2019) for the implementation of all models. Also, to easily implement the language model, we use the huggingface library (Wolf et al., 2020) containing various transformer-based pre-trained language models (PLMs) and their checkpoints.\nDetails for KALA In this paragraph, we describe the implementation details of the components, such as four linear layers in the proposed KFM, architectural specifications in the attentionbased GNN, and initialization of both the entity memory and relational embeddings, in the following. In terms of the functions h 1 , h 2 , h 3 , and h 4 in the KFM of Equation 2, we use two linear layers with the ReLU (Nair and Hinton, 2010) activation function, where the dimension is set to 768.\nFor relational retrieval, we implement the novel GNN model based on GATv2 (Brody et al., 2021) provided by the torch-geometric package (Fey and Lenssen, 2019). Specifically, we stack two GNN layers with the RELU activation function and also use the dropout with a probability of 0.1. For attention in our GNN, we mask the nodes of the null entity, so that the attention score becomes zero for them. Moreover, to obtain the context representation of the entity (See Footnote 3 in the main paper) used in the GNN attention, we use the scatter operation 8 for reduced computational cost.\nFor Entity Memory, we experimentally found that initializing the embeddings of the entity memory with the contextualized features obtained from 8 https://github.com/rusty1s/pytorch_scatter the pre-trained language model could be helpful. Therefore, the dimension of the entity embedding is set to the same as the language model d = 768. For relation embeddings, we randomly initialize them, where the dimension size is set to 128.\nLocation of KLM in the PLM Note that, the number and location of the KFM layers inside the PLM are hyperparameters. However, we empirically found that inserting one to three KFM layers at the end of the PLM (i.e., after the 9th -11th layers of the BERT-base language model) is beneficial to the performance (See Appendix C.4 for experiments on diverse layer locations).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "B.2 Dataset Details", "text": "Here we describe the dataset details with its statistics for two different tasks: extractive question answering (QA) and named entity recognition (NER).\nQuestion Answering We evaluate models on three domain-specific datasets: NewsQA, Relation, and Medication. Notably, NewsQA (Trischler et al., 2017) is curated from CNN news articles. Relation and Medication are originally part of the emrQA (Pampari et al., 2018), which is an automatically constructed question answering dataset based on the electrical medical record from n2c2 challenges 9 . However, Yue et al. (2020) extract two major subsets by dividing the entire dataset into Relation and Medication and suggest the usage of sampled questions from the original em-rQA dataset. Following the suggestion of Yue et al. (2020), we use only 1% of generated questions of Relation for training, validation, and testing. Also, we only use 1% of generated questions of Medication for training and use 5% of generated questions of Medication for validation and testing. Since the original emrQA is automatically generated based on templates, the quality is poor -it means that the original emrQA dataset was inappropriate to evaluate the ability of the model to reason over the clinical text since the most of questions can be  answered by the simple text matching. To overcome this limitation, Yue et al. (2020) suggests two ways to make the task more difficult. First, they divide the question templates into easy and hard versions and then use the hard question only. Second, they suggest replacing medical terminologies in the question of the test set into synonyms to avoid the trivial question which can be solvable with a simple text matching. We use both methods to Relation and Medication datasets to report the performance of every model. For more details on Relation and Medication datasets, please refer to the original paper (Yue et al., 2020). The statistics of training, validation, and test sets on all QA datasets are provided in Table 9.\nNamed Entity Recognition We use three different domain-specific datasets for evaluating our KALA on NER tasks: CoNLL-2003 (Sang andMeulder, 2003) (News), WNUT-17 (Derczynski et al., 2017) ", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "B.3 Training details", "text": "All experiments are constrained to be done with a single 12GB Geforce RTX 2080 Ti GPU for fairness in terms of memory and the availability on the academic budget, except for the DAPT and generative QA which use a single 48GB Quadro 8000 GPU. KALA training needs 3 hours in wall time with a single GPU. For all experiments, we select the best checkpoint on the validation set. For the summary of training setups, please see Table 10  and 12. Fine-tuning Setup In the following three paragraphs, we explain the setting of fine-tuning for QA, NER, and generative QA tasks. For all experiments on extractive QA tasks, we fine-tune the Pre-trained Language Model (PLM) for 2 epochs with the weight decay of 0.01, learning rate of 3e-5, maximum sequence length of 384, batch size of 12, linear learning rate decay of 0.06 warmup rate, and half precision (Micikevicius et al., 2018).\nFor all experiments on NER tasks, we finetune the PLM for 20 epochs, where the learning rate is set to 5e-5, maximum sequence length is set to 128, and batch size is set to 32. We use AdamW (Loshchilov and Hutter, 2019) as an optimizer using BERT-base as the PLM.\nFor the generative QA task in Table 7, we finetune the T5-small (Raffel et al., 2020) for 4 epochs with the learning rate of 1e-4, maximum sequence length of 512, and batch size of 64. We also use the Adafactor (Shazeer and Stern, 2018) optimizer. Instead of training with the same optimizer as in BERT for QA and NER, we instead use the independent AdamW optimizer with the learning rate of 1e-4 and weight decay of 0.01 to train the KALA module with T5.\nAdaptive Pre-training Setup In this paragraph, we describe the experimental settings of adaptive pre-training baselines, namely TAPT, TAPT (+ RecAdam), and DAPT. For QA tasks, we further pre-train the PLM for {1,3,5,10} epochs and then report the best performance among them. Specifically, reported TAPT result on NewsQA, Relation, and Medication are obtained by 1 epoch of further pre-training. We use the weight decay of 0.01, learning rate of 5e-5, maximum sequence length of 384, batch size of 12, and linear learning rate decay of 0.06 warmup rate, with a half-precision. Also, the masking ratio for the pre-training objective is set to 0.15, following the existing strategy introduced in the original BERT paper (Devlin et al., 2019).\nFor NER tasks, we further pre-train the PLM for 3 epochs across all datasets. In particular, the learning rate is set to 5e-5, batch size is set to 32, and the maximum sequence length is set to 128. We also use AdamW (Loshchilov and Hutter, 2019) as the optimizer for all experiments.\nIn the case of T5-small for generative QA in Table 7, we further pre-train the PLM for 4 epochs with the learning rate of 0.001, batch size of 64, maximum sequence length of 384, and Adafac-tor (Shazeer and Stern, 2018) optimizer.\nRegarding the setting of TAPT (+ RecAdam) on all tasks, we follow the best setting in the original paper  -sigmoid as an annealing function with annealing parameters: k = 0.5, t 0 = 250, and the pretraining coefficient of 5000.\nFor training with DAPT, we need an external corpus having a large amount of data for adaptive pre-training. Thus, we first choose the datasets of two domains -News and Medical. Specifically, as the source of corpus for the News domain, we use the sampled set of 10 million News from the RealNews dataset used in Gururangan et al. (2021). As the source of corpus for the Medical domain, we use the set of approximately 100k passages from the Medical textbook provided in Jin et al. (2020). The size of pre-training data used in DAPT is much larger than TAPT. In other words, for experiments on NewsQA, TAPT only uses fine-tuning contexts containing 5.8 million words from the NewsQA training dataset, while DAPT uses more than a hundred times larger data -enormous contexts containing about 618 million words from the RealNews database. For both News and Medical domains, we further pre-train the BERT-base model for 50 epochs with the batch size of 64, to match the similar computational cost used in Gururangan et al. (2020). Other experimental details are the same as TAPT described above.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "B.4 Architectural Variant Details", "text": "In this subsection, we describe the details of architectural variants reported in Section 5.1. For all variants, we use the same KGs used in KALA.\nEntity-as-Experts (F\u00e9vry et al. (2020); EaE) utilizes the entity memory similar to our work, but they use the parametric dense retrieval more like the memory neural network (Sukhbaatar et al., 2015). Similar to F\u00e9vry et al. (2020); Verga et al. (2021), we change the formulation of query and memory retrieval by using the mention representation of the entity from the intermediate hidden states of PLMs, which is formally defined as follows:\nh e = 1 m \u03c9 \u2212 m \u03b1 + 1 m \u03c9 i=m \u03b1 h l\u22121 i ,(4)\nv = softmax(h e \u2022 E ) \u2022 E,\nwhere h e represents the average of token representations of the entity mention m = (m \u03b1 , m \u03c9 ). We also give the supervised retrieval loss (ELLoss in F\u00e9vry et al. ( 2020)), when training the EaE model. With this retrieval, EaE also can represent the unseen entity e / \u2208 E train if we know the mention boundary of the given entity on the context. We believe it is expected to work well, if the entity memory is pre-trained on the enormous text along with the pre-training of the language model from the scratch. However, it might underperform for the language model adaptation scenario, since it can fall into the problem of circular reasoning -the PLM does not properly represent the unseen entity, but it should predict which entity it is similar from the representation. Regarding the integration of the knowledge from the entity memory into the PLM, the retrieved entity representation v is simply added (Peters et al., 2019) to the hidden representations H after the transformer block as follows:\nH l = H l + h(v) (5\n)\nwhere h is Multi-Layer Perceptrons (MLPs).\nAdapter (Houlsby et al., 2019) is introduced to fine-tune the PLM only with a few trainable parameters, instead of fine-tuning the whole parameters of the PLM. To adapt this original implementation into our KALA framework, we replace our Knowledge-conditioned Feature Modulation with it, where the Adapter is used as the knowledge integration module. We interleave the layer of Adapter after the feed-forward layer (F F ) and before the residual connection of the transformer block. Also, instead of only providing the LM hidden states as an input, we concatenate the knowledge representation in Equation 3 to the LM hidden states. Note that we fine-tune the whole parameters following our KALA setting, unlike fine-tuning the parameters of only Adapter layers in Houlsby et al. (2019).\nERNIE (Zhang et al., 2019) is a notable PLM model that utilizes the external KB as an input for the language model. The key feature of ERNIE can be summarized into two folds. First, they use the multi-head self-attention scheme (Vaswani et al., 2017) to contextualize the input entities. Second, ERNIE fuses the entity representation at the end of the PLM by adding it to the corresponding language representation. We assume that those two features are important points of ERNIE. Therefore, instead of using a Graph Neural Network (GNN) layer, we use a multi-head self-attention layer to contextualize the entity embeddings. Then, we add it to a representation of the entity from the PLM, which is the same as the design in equation 5. KT-Net (Yang et al., 2019) uses knowledge as an external input in the fine-tuning stage for extractive QA. Since they have a typical layer for integrating existing KB (Miller, 1995;Carlson et al., 2010) with the PLM, we only adopt the self-matching layer as the architecture variant of the KFM layer used in our KALA framework. The computation of the self-matching matrix in KT-Net is costly, i.e., it requires a large computational cost that is approximately 12 times larger than KALA.\nERICA (Qin et al., 2021) uses contrastive learning in LM pre-training to reflect the relational knowledge into the language model. We use the Entity Discrimination task from ERICA on the primary task of fine-tuning. We would like to note that, as reported in Section 5 of the original paper (Qin et al., 2021), the use of ERICA on fine-tuning has no effect, since the size and diversity of entities and relations in downstream training data are limited. Such limited information rather harms the performance, as it can hinder the generalization. In other words, contrastive learning cannot reflect the entity and relation in the test dataset.", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "B.5 FLOPs Computation", "text": "In this subsection, we give detailed descriptions of how the FLOPs in Figure 1 are measured. We majorly follow the script from the ELECTRA (Clark et al., 2020) repository to compute the approximated FLOPs for all models including ours. For FLOPs computation of our KALA, we additionally include the FLOPs of the entity embedding layer, linear layers for h 1 , h 2 , h 3 , h 4 , and GNN layer. Since the GNN layer is implemented based on the sparse implementation, we first calculate the FLOPs of the message propagation over one edge, and then multiply it to the average number of edges per node. Also, in terms of the computation on mentions, we consider the maximum sequence length of the context rather than the average number of mentions, to set the upper bound of FLOPs for our KALA. Note that, in NewsQA training data, the average number of nodes is 57, the average number of edges for each node is 0.64, and the average number of mentions in the context is 92.68.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "C Additional Experimental Results", "text": "In this section, we provide the analyses on the forgetting of TAPT, entity memory, number of entities and facts, location of the KLM layer, and values of Gamma and Beta.    ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C.1 Analysis on forgetting of TAPT", "text": "In Figure 1, we observe that the performance of TAPT decreases as the number of training steps increases. To get a concrete intuition on this particular phenomenon, we analysis what happens in the Pre-trained Language Model (PLM), when we further pre-train it on the task-specific corpus. Specifically, in Figure 7 ", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "C.2 Effects of the Size of Entity Memory", "text": "In this subsection, we analyze how the size of entity memory affects the performance of our KALA. In Figure 8, we plot the performance of KALA on the NewsQA dataset by varying the number of entity elements in the memory. Note that, we reduce the size of the entity memory by eliminating the entity appearing fewer times. Thus, the results are obtained by only considering the entities that appear more than [1000, 100, 10, 5, 0] times, e.g., 0 means the model with full entity memory. As shown in Figure 8, we observe that the size of the  entity memory is larger, the performance of our KALA is better in general. However, interestingly, we also observe that the smallest size of the entity memory shows decent performance, which might be due to the fact that some parameters in the entity memory are stale. For more discussions on it including visualization, please refer to Appendix D.2. Finally, we would like to note that, in Figure 1, we report the performance of our KALA in the case of [1000, 5, 0] (i.e., considering entities appearing more than [1000, 5, 0] times).", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "C.3 Effects of the Number of Entity and Fact", "text": "In this subsection, we aim to analyze which numbers of entities and facts per context are appropriate to achieve good performance in NER tasks. Specifically, we first collect the contexts having more than or equal to the k number of entities (or facts), and then calculate the performance difference from our KALA to the fine-tuning baseline. As shown in Figure 9, while there are no obvious patterns, performance improvements from the baseline are consistent across a varying number of entities and facts. This result suggests that our KALA is indeed beneficial when entities and facts are given to the model, whereas the appropriate number of entities and facts to obtain the best performance against the baseline is different across datasets.   ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "C.4 Effects of the Location of KFM", "text": "In the main paper and Appendix B.1, we describe that the location of the KFM layer inside the PLM architecture is the hyperparameter. However, someone might wonder which location of KFM yields the best performance, and what is the reason for this. Therefore, in this section, we analyze where we obtain the best performance in various locations of the KFM layer on the NewsQA dataset. Specifically, in Figure 10, we show the performance of our KALA with varying the location of the KFM layer insider the BERT-base model. The results demonstrate that the model with the KFM on the last layer of the BERT-base outperforms all the other choices. This might be because, as the final layer of the PLM is generally considered as the most task-specific layer, our KFM interleaved in the latest layer of BERT expressively injects the task-specific information from the entity memory and KGs, to such a task-specific layer.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "C.5 Analysis on Values of Gamma and Beta", "text": "To see how much amount of value on gamma and beta is used to shift and scale the intermediate hidden representations in transformer layers, we visualize the modulation values, namely gamma and beta, in Figure 11. We first observe that, as shown in Figure 11, the distribution of values of gamma and beta approximately follow the Gaussian dis-tribution, with zero mean for beta and one mean for gamma. Also, we notice that the scale of values remain nearly around the mean point, which suggests that the small amount of shifting to intermediate hidden representations on transformer layers is enough to contribute to the performance gain, as we can see in the main results of Table 1, 2.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "C.6 Detailed Efficiency Comparison", "text": "While we provide the efficiency on FLOPs in Figure 1, we further provide the efficiency on GPU memory, wall time, and FLOPs for training each method in Table 6. Specifically, we measure the computational cost on the NewsQA dataset with BERT-base, where we use the single Geforce RTX 2080 Ti GPU on the same machine. For our KALA, as we can flexibly manage the cost of GPU memory by reducing the number of entities in entity memory (See Figure 8 with Appendix C.2 for more analysis on the effects of the size of entity memory), we provide the experimental results on two settings -KALA with memory size 0.2k and 62.8k (full memory). As shown in Table 6, we confirm that the computational cost of our KALA with the full memory is similar to the cost of the more params baseline that uses one additional transformer layer on top of BERT-base. However, by reducing the number of entities in the memory, we can achieve better efficiency than more params in terms of GPU memory and FLOPs. Also, we observe that the training cost (i.e., Wall Time and FLOPs) of TAPT and DAPT is high, especially on DAPT, thus we verify that our KALA is more efficient to train for domain adaptation settings.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "D Additional Visualization Results", "text": "Here we provide the frequency distribution of entities, additional case studies, and more illustrations of textual examples and embedding spaces.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D.1 Additional Representation Visualization", "text": "While we already show the contextualized representations of seen and unseen entities in the latent 5163 space in Figure 2 right, we further visualize them including the missing baselines of Figure 2, such as Fine-tuning or TAPT, in Figure 12 on the NCBI-Disease dataset. Similar to Figure 2, we observe that all baselines fail to closely embed the unseen entities in the representation space of seen entities. While this visualization result does not give a strong evidence of why our KALA outperforms other baselines, we clearly observe that KALA is beneficial to represent unseen entities in the feature space of seen entities, which suggests that such an advantage of our KALA helps the PLM to generalize over the test dataset, where the context contains unseen entities.", "n_publication_ref": 0, "n_figure_ref": 4}, {"heading": "D.2 Entity Frequency Distribution", "text": "We visualize the frequency of entities in Figure 13 and 14. The entity frequency denotes the number of mentions of their associated entities within the entire text corpus of the training dataset. As shown in Figure 13 and 14 of QA and NER datasets, the entity frequency follows the long-tail distribution, where most entities appear a few times. For instance, in the NewsQA dataset, more than 20k entities among entire 60k entities appear only once in the training dataset, whereas one entity (CNN 10 ) appears approximately 20k times. This observation suggests that most of the elements in the entity memory are not utilized frequently. In other words, only few entities are accurately trained with many training instances, whereas there exists the stale embeddings which are rarely updated. This observation raises an interesting research question on the efficient usage of the entity memory, as we can see in Figure 8 that the small size of entity memory could result in the better performance (See Appendix C.2). We leave the more in-depth analysis on the entity memory as the future work. where almost all entities appear less than 10 times, while an extremely few numbers of entities appear very frequently.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "D.3 Additional Case Study", "text": "In addition to the case study in Figure 5, we further show the case on the question answering task in Figure 15, like in Section 5.5, With this example, we explain how the factual knowledge in KGs could be utilized to solve the task via our KALA. The question in the example is \"who was kidnapped because of her neighbor\". We observe that DAPT answers this question as Araceli Valencia. This prediction may come from matching the word 'her' in the question to the feminine name 'Araceli Valencia' in the context. In contrast, our KALA predicts the Jaime Andrade as an answer, which is the ground truth. We suspect that this might be because of the fact \"(Jaime Andrade, spouse, Valencia)\" in the knowledge graph, which relates the 'Valencia' to the 'Jaime Andrade'. Although it is not clear how it directly affects the model's performance, we can reason that KALA can successfully answer the question by utilizing the existing facts.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "D.4 Additional Data Visualization", "text": "In Figure 16 and 17, we visualize the examples of the context with its seen and unseen entities and its relational facts. We first confirm that the quality of facts is moderate to use. For instance, in the first example of Figure 16, the fact in the context that Omar_bin_Laden is son of Osama_bin_Laden, is also appeared in the knowledge graph. In addition, we observe that there are facts that link unseen entities to the seen entities in both Figure 16 and 17. Thus, while some of the facts in the knowledge graph are not accurate, we can represent the unseen entities with their relation to the seen entities. We expect that there is a still room to improve in terms of the quality of KGs, allowing our KALA to modulate the entity representation more accurately. We leave the study on this as the future work.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "Acknowledgement", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Context", "text": "The adenomatous polyposis coli ( APC ) tumour -suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK -3beta ), axin / conductin and betacatenin.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Facts (Sampled)", "text": "(complex, subclass of, protein) (GSK, instance of, protein) (glycogen, instance of, protein) (APC, instance of, protein)\nContext HLA typing for HLA -B27, HLA -B60, and HLA -DR1 was performed by polymerase chain reaction with sequence -specific primers, and zygosity was assessed using microsatellite markers.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Facts (Sampled)", "text": "(microsatellite, subclass of, primers) (DR1, instance of, microsatellite) (microsatellite, subclass of, typing)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Context", "text": "We identified four germline mutations in three breast cancer families and in one breast -ovarian cancer family. among these were one frameshift mutation, one nonsense mutation, one novel splice site mutation, and one missense mutation.  ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "", "journal": "", "year": "", "authors": "Jimmy Lei; Jamie Ryan Ba; Geoffrey E Kiros"}, {"title": "Pre-train or annotate? domain adaptation with a constrained budget", "journal": "", "year": "2021-11-11", "authors": "Fan Bai; Alan Ritter; Wei Xu"}, {"title": "Scibert: A pretrained language model for scientific text", "journal": "", "year": "2019-11-03", "authors": "Iz Beltagy; Kyle Lo; Arman Cohan"}, {"title": "How attentive are graph attention networks? arXiv preprint", "journal": "", "year": "2021", "authors": "Shaked Brody; Uri Alon; Eran Yahav"}, {"title": "Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"title": "", "journal": "", "year": "", "authors": "Andrew Carlson; Justin Betteridge; Bryan Kisiel; Burr Settles; R Estevam; Tom M Hruschka"}, {"title": "Toward an architecture for neverending language learning", "journal": "AAAI Press", "year": "2010-07-11", "authors": " Mitchell"}, {"title": "Recall and learn: Fine-tuning deep pretrained language models with less forgetting", "journal": "", "year": "2020-11-16", "authors": "Sanyuan Chen; Yutai Hou; Yiming Cui; Wanxiang Che; Ting Liu; Xiangzhan Yu"}, {"title": "ELECTRA: pretraining text encoders as discriminators rather than generators", "journal": "", "year": "2020-04-26", "authors": "Kevin Clark; Minh-Thang Luong; Quoc V Le; Christopher D Manning"}, {"title": "Results of the WNUT2017 shared task on novel and emerging entity recognition", "journal": "Association for Computational Linguistics", "year": "2017-09-07", "authors": "Leon Derczynski; Eric Nichols; Marieke Van Erp; Nut Limsopatham"}, {"title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019-06-02", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "NCBI disease corpus: A resource for disease name recognition and concept normalization", "journal": "J. Biomed. Informatics", "year": "2014", "authors": "Robert Rezarta Islamaj Dogan; Zhiyong Leaman;  Lu"}, {"title": "Harm de Vries, Aaron Courville, and Yoshua Bengio", "journal": "Distill", "year": "2018", "authors": "Ethan Vincent Dumoulin; Nathan Perez; Florian Schucher;  Strub"}, {"title": "Parameter-efficient transfer learning for NLP", "journal": "", "year": "2019-06", "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly"}, {"title": "Universal language model fine-tuning for text classification", "journal": "Long Papers", "year": "2018-07-15", "authors": "Jeremy Howard; Sebastian Ruder"}, {"title": "Arbitrary style transfer in real-time with adaptive instance normalization", "journal": "", "year": "2017-10-22", "authors": "Xun Huang; Serge J Belongie"}, {"title": "What disease does this patient have? A large-scale open domain question answering dataset from medical exams", "journal": "", "year": "2020", "authors": "Di Jin; Eileen Pan; Nassim Oufattole; Wei-Hung Weng; Hanyi Fang; Peter Szolovits"}, {"title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining", "journal": "Bioinform", "year": "2020", "authors": "Jinhyuk Lee; Wonjin Yoon; Sungdong Kim; Donghyeon Kim; Sunkyu Kim; Chan Ho So; Jaewoo Kang"}, {"title": "Learning to perturb word embeddings for out-of-distribution QA", "journal": "Long Papers", "year": "2021-08-01", "authors": "Seanie Lee; Minki Kang; Juho Lee; Sung Ju Hwang"}, {"title": "Roberta: A robustly optimized BERT pretraining approach. arXiv preprint", "journal": "", "year": "2020", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Decoupled weight decay regularization", "journal": "", "year": "2019-05-06", "authors": "Ilya Loshchilov; Frank Hutter"}, {"title": "Mixed precision training", "journal": "", "year": "2018-04-30", "authors": "Paulius Micikevicius; Sharan Narang; Jonah Alben; Gregory F Diamos; Erich Elsen; David Garc\u00eda; Boris Ginsburg; Michael Houston; Oleksii Kuchaiev; Ganesh Venkatesh; Hao Wu"}, {"title": "Wordnet: A lexical database for english", "journal": "Commun. ACM", "year": "1995", "authors": "George A Miller"}, {"title": "Rectified linear units improve restricted boltzmann machines", "journal": "", "year": "2010-06-21", "authors": "Vinod Nair; Geoffrey E Hinton"}, {"title": "emrqa: A large corpus for question answering on electronic medical records", "journal": "", "year": "2018-10-31", "authors": "Anusri Pampari; Preethi Raghavan; Jennifer J Liang; Jian Peng"}, {"title": "", "journal": "", "year": "", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga"}, {"title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "", "year": "2019-12-08", "authors": "Zachary Yang; Martin Devito; Alykhan Raison; Sasank Tejani; Benoit Chilamkurthy; Lu Steiner; Junjie Fang; Soumith Bai;  Chintala"}, {"title": "Film: Visual reasoning with a general conditioning layer", "journal": "", "year": "2018-02-02", "authors": "Ethan Perez; Florian Strub; Vincent Harm De Vries; Aaron C Dumoulin;  Courville"}, {"title": "Knowledge enhanced contextual word representations", "journal": "", "year": "2019-11-03", "authors": "Matthew E Peters; Mark Neumann; Robert L Logan; I V ; Roy Schwartz; Vidur Joshi; Sameer Singh; Noah A Smith"}, {"title": "ERICA: improving entity and relation understanding for pre-trained language models via contrastive learning", "journal": "Long Papers", "year": "2021-08-01", "authors": "Yujia Qin; Yankai Lin; Ryuichi Takanobu; Zhiyuan Liu; Peng Li; Heng Ji; Minlie Huang; Maosong Sun; Jie Zhou"}, {"title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"title": "Neural Transfer Learning for Natural Language Processing", "journal": "", "year": "2019", "authors": "Sebastian Ruder"}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "journal": "ACL", "year": "2003-05-31", "authors": "Erik F Tjong; Kim Sang; Fien De Meulder"}, {"title": "Adafactor: Adaptive learning rates with sublinear memory cost", "journal": "", "year": "2018-07-10", "authors": "Noam Shazeer; Mitchell Stern"}, {"title": "End-to-end memory networks", "journal": "", "year": "2015-12-07", "authors": "Sainbayar Sukhbaatar; Arthur Szlam; Jason Weston; Rob Fergus"}, {"title": "Newsqa: A machine comprehension dataset", "journal": "", "year": "2017-08-03", "authors": "Adam Trischler; Tong Wang; Xingdi Yuan; Justin Harris; Alessandro Sordoni; Philip Bachman; Kaheer Suleman"}, {"title": "Attention is all you need", "journal": "", "year": "2017-12-04", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Graph attention networks", "journal": "", "year": "2018-04-30", "authors": "Petar Velickovic; Guillem Cucurull; Arantxa Casanova; Adriana Romero; Pietro Li\u00f2; Yoshua Bengio"}, {"title": "Adaptable and interpretable neural memoryover symbolic knowledge", "journal": "", "year": "2021-06-06", "authors": "Pat Verga; Haitian Sun; Baldini Livio; William W Soares;  Cohen"}, {"title": "Wikidata: a free collaborative knowledgebase", "journal": "Commun. ACM", "year": "2014", "authors": "Denny Vrandecic; Markus Kr\u00f6tzsch"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020-11-16", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"title": "LUKE: deep contextualized entity representations with entityaware self-attention", "journal": "", "year": "2020-11-16", "authors": "Ikuya Yamada; Akari Asai; Hiroyuki Shindo; Hideaki Takeda; Yuji Matsumoto"}, {"title": "Enhancing pre-trained language representations with 5155 rich knowledge for machine reading comprehension", "journal": "Long Papers", "year": "2019-07-28", "authors": "An Yang; Quan Wang; Jing Liu; Kai Liu; Yajuan Lyu; Hua Wu; Qiaoqiao She; Sujian Li"}, {"title": "Docred: A large-scale document-level relation extraction dataset", "journal": "", "year": "2019-07-28", "authors": "Yuan Yao; Deming Ye; Peng Li; Xu Han; Yankai Lin; Zhenghao Liu; Zhiyuan Liu; Lixin Huang; Jie Zhou; Maosong Sun"}, {"title": "How transferable are features in deep neural networks?", "journal": "", "year": "2014-12-08", "authors": "Jason Yosinski; Jeff Clune; Yoshua Bengio; Hod Lipson"}, {"title": "Clinical reading comprehension: A thorough analysis of the emrqa dataset", "journal": "", "year": "2020-07-05", "authors": "Xiang Yue; Jimenez Bernal; Huan Gutierrez;  Sun"}, {"title": "ERNIE: enhanced language representation with informative entities", "journal": "Long Papers", "year": "2019-07-28", "authors": "Zhengyan Zhang; Xu Han; Zhiyuan Liu; Xin Jiang; Maosong Sun; Qun Liu"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: F1 Score and Training FLOPs for different methods on Question Answering (NewsQA). Note that DAPT uses about 112 times larger data for adaptation. Details are in \u00a75.3", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Concepts (Left). (a) Adaptive Pre-training updates whole parameters of the PLM through further pre-training on the domain corpus. (b) Our method KALA integrates the external knowledge so that the PLM adapts to the target domain only with fine-tuning, which is realized by the affine transformation on the intermediate feature. Visualization of the contextualized representation from the PLM for seen and unseen entities (Right). Our KALA framework embeds the unseen entities on the embedding space of seen entities by representing them with their relational knowledge over the graph, while the strong DAPT baseline (Gururangan et al., 2020) cannot appropriately handle unseen entities that are not given for task fine-tuning.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Framework Overview. (Left) The architecture of a knowledge-augmented LM with our method. Some of the input tokens are annotated as entities with their mentions. (Middle) Inside the transformer block, KFM ( \u00a73.2) is applied after the layer normalization as in equation 1, to modulate the hidden representations of tokens in entity mentions. (Right) The retrieved embedding of an entity New_York is composed by the weighted aggregation of neighbors through the knowledge graph ( \u00a73.3).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "3.Task-Adaptive Pre-training (TAPT): A baseline that further pre-trains the PLM on taskspecific corpus as in Gururangan et al. (2020). 4. TAPT + RecAdam: A baseline that uses RecAdam (Chen et al., 2020) during further pre-training of PLMs (i.e., TAPT), to alleviate catastrophic forgetting of the learned general knowledge in PLMs from adaptive pre-training. 5. Domain-Adaptive Pre-training (DAPT): A strong baseline that uses a large-scale domain corpus outside the training set during further pretraining (Gururangan et al., 2020), and requires extra data and large computational overhead.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Results on seen and unseen, where Seen denotes the context having less than 3 unseen entities, otherwise Unseen.Note that DAPT uses extra datasets in addition to the training dataset, thus the Unseen for other models could be considered as the Seen for DAPT.", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Masked Language Model loss from Task-Adaptive Pre-Training on the domain-specific training dataset (Relation) and the general domain test dataset (Sampled wikipedia).", "figure_data": ""}, {"figure_label": "8", "figure_type": "", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: The performance (F1 score and Exact Match) and the GPU memory usage on NewsQA dataset with varying the size of elements in the entity memory.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_9", "figure_caption": ", we visualize the Masked Language Model (MLM) loss of TAPT on both domain-specific corpus from the Relation dataset and general corpus from the sampled Wikipedia documents during the adaptive pre-traing. As Figure 7 shows, the test MLM loss increases while the training MLM loss persistently increases as the training step increases. This result indicates that TAPT on domain-specific corpus may yield the catastrophic forgetting of the general knowledge in the PLM.", "figure_data": ""}, {"figure_label": "9", "figure_type": "", "figure_id": "fig_10", "figure_caption": "Figure 9 :9Figure 9: Performance improvements of our KALA from simple fine-tuning, with varying the number of entities and facts in the context on Named Entity Recognition tasks.", "figure_data": ""}, {"figure_label": "10", "figure_type": "", "figure_id": "fig_11", "figure_caption": "Figure 10 :10Figure 10: The performance of our KALA with varying the location of the KFM layer inside the BERT-base model. yaxis denotes the F1 score on NewsQA and x-axis denotes the location of the KFM layer. For instance, 11 means the case where the KFM layer is appended in the 11th transformer layer of BERT-base.", "figure_data": ""}, {"figure_label": "11", "figure_type": "", "figure_id": "fig_13", "figure_caption": "Figure 11 :11Figure 11: Histogram of values of gamma and beta on the CoNLL-2003 dataset.Fine-tuning seen unseen", "figure_data": ""}, {"figure_label": "12", "figure_type": "", "figure_id": "fig_14", "figure_caption": "Figure 12 :12Figure 12: Visualization of contextual representations for seen and unseen entities on the NCBI-Disease dataset.", "figure_data": ""}, {"figure_label": "13", "figure_type": "", "figure_id": "fig_15", "figure_caption": "Figure 13 :13Figure13: Distribution of frequency of entities on QA datasets: NewsQA, Relation, and Medication, where almost all entities appear less than 10 times, while an extremely few numbers of entities appear very frequently.", "figure_data": ""}, {"figure_label": "14", "figure_type": "", "figure_id": "fig_16", "figure_caption": "Figure 14 :14Figure 14: Distribution of frequency of entities on NER datasets: CoNLL-2003, WNUT-17, and NCBI-Disease, where almost all entities appear less than 10 times, while an extremely few numbers of entities appear very frequently.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ").", "figure_data": "Transformer LayersKFM\ufffd, \ufffdEntity MemoryLayerNormFeed-Forward(City)CityKFM,(New_York)New_YorkLayerNormTransformer LayersMulti-Head Self-AttentionWeighted Aggregation(USA)USA . . .[New York]isa[city] of [United States]Knowledge-conditioned Feature ModulationState"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Tuning 53.06 \u00b1 0.63 | 67.20 \u00b1 0.19 54.01 \u00b1 1.14 | 61.43 \u00b1 1.18 12.50 \u00b1 0.28 | 43.31 \u00b1 0.67 + more params 53.59 \u00b1 0.99 | 67.79 \u00b1 0.67 54.06 \u00b1 1.35 | 62.07 \u00b1 1.44 12.46 \u00b1 0.25 | 42.74 \u00b1 0.91 TAPT 53.47 \u00b1 1.69 | 67.59 \u00b1 1.44 53.57 \u00b1 2.05 | 60.87 \u00b1 2.52 12.58 \u00b1 0.42 | 43.82 \u00b1 1.10 + RecAdam 53.95 \u00b1 1.02 | 67.89 \u00b1 0.75 54.88 \u00b1 1.94 | 62.54 \u00b1 2.14 12.63 \u00b1 0.30 | 43.86 \u00b1 0.87 DAPT \u2020 53.68 \u00b1 0.94 | 67.76 \u00b1 0.61 55.29 \u00b1 1.74 | 62.25 \u00b1 1.80 12.67 \u00b1 0.27 | 43.26 \u00b1 0.88 KALA (point-wise) 53.41 \u00b1 0.74 | 67.30 \u00b1 0.45 56.13 \u00b1 0.85 | 64.69 \u00b1 0.92 12.01 \u00b1 0.47 | 42.97 \u00b1 0.70 KALA (relational) 54.25 \u00b1 0.63 | 68.27 \u00b1 0.63 55.96 \u00b1 1.37 | 64.22 \u00b1 1.15 12.75 \u00b1 0.61 | 44.19 \u00b1 0.46 Table 1: Experimental results of the extractive QA task on three different datasets with the BERT-base. The reported results are", "figure_data": "MethodNewsQARelationMedicationFine-"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Experimental results of the NER task on three different datasets with the BERT-base. The reported results are means and standard deviations over five different runs with an F1 score as a metric. The numbers in bold fonts denote the best score. \u2020 indicates the baseline under an extremely high computational resource setting (See Figure1).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Tuning 57.21 \u00b1 0.56 | 71.91 \u00b1 0.35 46.61 \u00b1 2.75 | 53.89 \u00b1 2.92 55.00 \u00b1 1.66 86.91 \u00b1 1.08 + more params 58.07 \u00b1 1.19 | 72.38 \u00b1 1.04 45.12 \u00b1 0.86 | 53.22 \u00b1 1.27 56.62 \u00b1 0.26 87.21 \u00b1 0.26 TAPT 57.24 \u00b1 0.53 | 71.77 \u00b1 0.34 45.66 \u00b1 2.20 | 53.23 \u00b1 2.38 55.46 \u00b1 1.90 86.24 \u00b1 0.76 KALA (relational) 58.01 \u00b1 0.57 | 72.70 \u00b1 0.25 47.40 \u00b1 1.67 | 55.13 \u00b1 1.26 56.96 \u00b1 0.27 87.72 \u00b1 0.27", "figure_data": "MethodNewsQARelationWNUT-17NCBI-DiseaseFine-"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Experimental results of the extractive QA and NER tasks on four different datasets -NewsQA, Relation, WNUT-17 and NCBI-Disease -with the RoBERTa-base. The reported results are means and standard deviations over five different runs. We use Exact Match and F1 score as a metric for QA, and F1 score for NER. The numbers in bold fonts denote the best score.", "figure_data": "KFM ( \u00a73.2) ComponentsNewsQA EM F1None (Fine-tuning) 53.06 67.20+ \u0393,\u0393 (gamma only) 54.10 67.98 + B,B (beta only) 53.74 67.69 + \u0393, B (first only) 53.77 67.88+\u0393,B (second only) 53.89 67.49+ \u0393, B,\u0393,B (final) 54.25 68.27"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "An ablation study of the KFM parameters \u0393, B,\u0393,B. We report the average results over five different runs.", "figure_data": "ArchitectureNewsQAVariants ( \u00a75.2) EM F1ERNIE53.35 67.49Adapter53.32 67.38KT-Net53.15 67.01EaE53.00 67.40ERICA51.99 66.40KALA (ours) 54.25 68.27"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": Experimental re-sults on knowledge integra-tion architecture variants, averaged over five runs."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_8", "figure_caption": ", 2 with BERT(Devlin et al., 2019) are enough to show the effectiveness of KALA across different pre-trained language models (PLMs), one might be curious that KALA can work on even", "figure_data": "91 92 93CoNLL-2003 Fine-tuning DAPT KALA (point-wise) KALA (relational) TAPT46 48 50WNUT-1780 85 90 NCBI-Disease90447589SeenUnseen42SeenUnseen70SeenUnseen"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Context: A nonsense mutation in exon 17 ( codon 556 ) of the RB1 gene was found to be present homozygously in both the retinal and the pineal tumours.Figure 5: A case study on one context of the NCBI-Disease dataset. A left table shows the context and its fact, and a right figure shows a visualization of token representations. Text in blue and red denote the seen and unseen entities, respectively.", "figure_data": "Fine-TuningKALA (Ours)Fact: (retinal, instance of, gene)renonsense mutation ex ##on cod ##on gene ##tina ##lcod ##on re ##tina ##lnonsense mutation ##on ex geneT5-small Fine-tuning TAPT + RecAdam KALA (ours)NewsQA EM F1 48.96 64.24 48.66 64.30 48.37 63.41 51.78 66.88"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ": Experimental results on generative question answer-ing with T5-small as a PLM and NewsQA as a dataset."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Thibault F\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 4937-4951.", "figure_data": "DataEntity ExtractionData w/ extracted entitiesRelation ExtractionData w/ extracted factsFormat (a):Format (b):Matthias Fey and Jan E. Lenssen. 2019. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds.Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. 2021. Demix layers: Disentangling domains for modular lan-guage modeling. arXiv preprint, arXiv:2108.05036.Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8342-8360.William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in Neural Information Process-ing Systems 30: Annual Conference on Neural In-formation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 1024-1034.Xiaochuang Han and Jacob Eisenstein. 2019. Unsu-pervised domain adaptation of contextualized em-beddings for sequence labeling. In Proceedings of the 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, Novem-ber 3-7, 2019, pages 4237-4247.Dan Hendrycks and Kevin Gimpel. 2016. Bridg-ing nonlinearities and stochastic regularizers with gaussian error linear units. arXiv preprint, arXiv:1606.08415."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Figure6: Visual diagram of the KG construction pipeline used in this work. The entity format is composed of its corresponding text in the data, its character-level mention boundary, and its wikidata id. The fact format is composed of the head, relation, and tail, where head and tail entities are represented with their wikidata ids following the entity format.", "figure_data": "HyperparametersNewsQA Relation Medication CoNLL-2003 WNUT-17 NCBI-DiseaseLM for Relation ExtractionBERT-base-uncasedThreshold on Relation Extraction0.1Size of Entity Memory6282357244635102881013502The location of KFM11111189, 118, 10"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "QA dataset statistics. We report the number of contexts and questions (i.e., # Context and # Question), with the average length of contexts (i.e., C. Length) where the length is measured as the number of tokens after wordpiece tokenization.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Hyperparamters for Fine-tuning (Top) and TAPT (Bottom) on six datasets (+ generative QA) we used for reporting the performances in the main paper. Note that the Fine-tuning setup is applied to all methods including KALA.", "figure_data": "DatasetTraining # Context C. Length # Context C. Length # Context C. Length Validation TestCoNLL-2003 WNUT-17 NCBI-Disease14,041 3,394 5,43319.95 31.32 34.363,250 1,009 92421.36 19.28 35.003,453 1,287 94118.77 30.58 35.50"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "NER dataset statistics. We report the number of contexts (i.e., # Context), with the average length of them (i.e., C. Length) on training, validation, and test sets.", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Hyperparamters for DAPT on two domains we used for reporting the performances in the main paper.", "figure_data": "bels. The NCBI-Disease dataset consists of the793 PubMed articles from the biomedical domain,which contains 6,892 disease mentions and 790disease concepts, and also has 3 class labels. Thestatistics of training, validation, and test sets areprovided in Table 11."}], "doi": "10.18653/v1/D19-1371"}