Transfer Learning for Related Languages : Submissions to the WMT20 Similar Language Translation Task
In this paper , we describe IIT Delhi 's submissions to the WMT 2020 task on Similar Language Translation for four language directions : Hindi ↔ Marathi and Spanish ↔ Portuguese . We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models . For our best submissions , we fine - tune the mBART model ( Liu et al , 2020 ) on the parallel data provided for the task . The pre - training is done using self - supervised objectives on a large amount of monolingual data for many languages . Overall , our models are ranked in the top four of all systems for the submitted language pairs , with first rank in Spanish Portuguese .
Machine Translation ( MT ) is currently tackled using rule - based methods ( RBMT ) ( Charoenpornsawat et al , 2002 ) , phrase - based statistical methods ( SMT ) ( Koehn et al , 2003 ) and neural methods ( NMT ) ( Cho et al , 2014 ; Sutskever et al , 2014 ; Bahdanau et al , 2015 ; Vaswani et al , 2017 ) . NMT has achieved high translation quality for several language pairs ( Bojar et al , 2018 ; Barrault et al , 2019 ) , but this level of performance usually requires large amounts of aligned data in the order of millions of sentence pairs . For low and medium resource languages , SMT performs better than NMT ( Koehn and Knowles , 2017 ; Sennrich and Zhang , 2019 ) . SMT also shows better performance when there is a domain mismatch between the train and test datasets , which is typical of low and medium resource language pairs . In these settings , NMT performance can be boosted by leveraging additional monolingual data to enforce various types of constraints or increasing the training data using back - translation . These methods can be particularly helpful if the source and target languages in MT are closely related and share language structure and alphabet . Recently , pre - training methods for sequence - to - sequence ( seq2seq ) models have been introduced like MASS ( Song et al , 2019a ) , XLM ( Conneau and Lample , 2019 ) , BART ( Lewis et al , 2019 ) , and mBART ( Liu et al , 2020 ) . These methods show significant gains in downstream tasks like NMT , summarization , natural language inference ( NLI ) , etc . In this paper , we focus on the transfer learning capabilities in NMT for the task of translation between related languages where parallel data is scarce . IIT Delhi participated in the WMT 2020 Shared task on Similar Language Translation for four language directions : Hindi ( hi ) ↔ Marathi ( mr ) and Spanish ( es ) ↔ Portuguese ( pt ) . The first language pair is low resource and second is medium resource in terms of the parallel data available for the task . Refer to Table 2 for the classification . We fine - tuned the pre - trained mBART model ( Liu et al , 2020 ) on the parallel data provided for the task . mBART gives better performance than SMT models even when the parallel data is very limited . mBART is pre - trained on 25 languages , which contain Hindi and Spanish , but not Marathi and Portuguese . mBART is able to leverage transfer learning capabilities even for those languages that are originally not present during the pre - training phase . The fine - tuned mBART architecture forms our best submissions for both language pairs : hi ↔ mr and es ↔ pt . The rankings obtained by us in each of the language directions are listed in Table 1 The results and analysis are detailed in Section 5 . We finally conclude in Section 6 .
SMT is tackled by building a phrase table from the aligned parallel data . The target side translation is then generated by matching the most appropriate phrases in the source sentence conditioned on the target side language model along with a reordering model ( Koehn et al , 2003 ) . NMT is modeled using Encoder - Decoder models ( Cho et al , 2014 ; Sutskever et al , 2014 ; Bahdanau et al , 2015 ) , with the Transformer model ( Vaswani et al , 2017 ) achieving state - of - the - art on many MT problems . But these models ' reliance on large aligned parallel data for the source and target languages makes them unsuitable for low / medium resource language pairs ( Koehn and Knowles , 2017 ) . Some of the previous works in these settings to improve NMT performance are described below :
Instead of using only two languages ( source and target ) for training an NMT model , using multiple languages has been shown to help in low resource scenarios . For example , it might be the case that a certain pair of languages have very little parallel data between them , but there exists a third language with abundant parallel data with the original two languages . This third language acts as a pivot and helps in improving NMT between the two languages ( Aharoni et al , 2019 ; Gu et al , 2018 ; Liu et al , 2020 ; Zhang et al , 2020 ) .
Back - Translation Hoang et al , 2018 ) increases the amount of training data by using monolingual corpus along with partially - trained NMT models on the limited parallel data . Pseudo - parallel corpus for each direction is first obtained by generating the translations of the monolingual data for each language using the partially - trained MT models on the limited parallel data . Using these pseudoparallel corpora , the partially - trained NMT models are then trained further for some number of steps . In this way , millions of pseudo - parallel sentence pairs can be generated to improve NMT models because of the abundance of monolingual data . Another version of using back - translation is the copying mechanism . Currey et al ( 2017 ) proposes to copy the target side monolingual data on the source side to create additional data without modifying the training regimen for NMT . This helps the model to generate fluent translations .
For NMT , the first step is the random initialization of model weights in both the encoder and decoder . Instead of random initialization , NMT models can be initialized by pre - training parts of the model ( Conneau and Lample , 2019 ; Edunov et al , 2019 ) , or pre - training the complete seq2seq model ( Ramachandran et al , 2017 ; Song et al , 2019b ; Liu et al , 2020 ) . These pre - training methods leverage different kinds of masking techniques and the pretraining objective is to predict these masked tokens , similar to BERT ( Devlin et al , 2019 ) . Denoising auto - encoding can also be used where a sentence is corrupted by various noising techniques and the pre - training objective is to generate the original uncorrupted sentence as in BART ( Lewis et al , 2019 ) and mBART ( Liu et al , 2020 ) .
There also have been works to improve low / medium resource NMT by adding linguistic information either using data augmentation ( Currey and Heafield , 2019 ) , subword embedding augmentation , or architectural changes ( Eriguchi et al , 2017 ) . This helps the model to not only learn the alignment between source and target language spaces , but also syntax structure like dependency parse , part of speech , etc . This helps in making the target side translations more fluent and conforming to the structure of the language . We do not explore this direction in this paper .
We experimented with three different settings for hi ↔ mr as listed below . SMT This phrase - based system leverages both monolingual and parallel data provided for the task . We use Moses ( Koehn et al , 2007 ) for training the SMT systems . NMT ( Transformer ) For this , we used the standard Transformer large architecture from Vaswani et al ( 2017 ) for training on the parallel data provided for the task . NMT ( mBART ) mBART ( Liu et al , 2020 ) is a large Transformer pre - trained on monolingual data for 25 languages . The pre - training objective for mBART is seq2seq de - noising for natural text as in BART ( Lewis et al , 2019 ) . mBART provides a general - purpose pre - trained Transformer for any downstream task . It has been shown to give significant improvements over the random initialization for NMT and is the current state - of - the - art for many low resource language pairs . Implementation Details mBART uses a shared subword vocabulary of 250 K tokens for all the 25 languages present in the pre - training . We use the same vocabulary for Marathi and Portuguese also , even though they were not used during the pre - training phase . Marathi shares its subword vocabulary with languages like Hindi and Nepali in mBART , and Portuguese shares with Spanish , Italian and other European languages present in mBART . The percentage of unknown tokens [ UNK ] in Marathi and Portuguese parallel datasets is less than 0.003 % when using the shared mBART vocabulary . Additionally , the mBART architecture requires language specific token at the end of each input sequence to provide the language specific context for the decoder . Since Marathi and Portuguese were not present during the pre - training phase , we use the token corresponding to the second most related language present in mBART pre - training for specifying the context at the time of decoding in each case . For Marathi , we used the Nepali language token and for Portuguese , we used the Italian language token . We could not use Spanish language token for Portuguese because we are doing translations to and from Spanish .
We use hi ↔ mr and es ↔ pt language pairs for our experiments .
Because of the constrained nature of the shared task , we only use the parallel data provided for this task . We removed the empty instances for both language pairs ( < 2000 instances ) . For es ↔ pt , we do not use ' WikiTitles v2 ' part of the parallel data for training because of very short sentences in the dataset . The cleaned parallel dataset statistics are provided in Table 2 . Preprocessing We use sentence piece tokenization ( Kudo and Richardson , 2018 ) for generating the source and target sequences for the NMT architectures . For the standard Transformer , we train a sentence piece model using 40 K subword tokens for hi ↔ mr . For mBART , we use Liu et al ( 2020 ) 's pre - trained 1 sentence piece model comprising of 250 K subword tokens as the vocabulary . For the SMT model on hi ↔ mr , we also use the monolingual data provided for this task . We extract 5 Million monolingual sentences each for Hindi and Marathi after deduplication and use this set for training the language models . We use Moses ( Koehn et al , 2007 ) for all tokenization / detokenization scripts . Lample et al ( 2018 ) . We used Moses ( Koehn et al , 2007 ) and Giza++ with standard settings to train the SMT model in both directions .
We use the large Transformer from Vaswani et al ( 2017 ) with 8 encoder and decoder layers and replicate all the parameters from . The number of parameters in the model are approximately 248 Million and it takes ∼26 hours on 4 Nvidia V100 ( 32 GB ) GPUs . NMT ( mBART ) For this , we use 12 Transformer encoder and decoder layers , with total number of model parameters ∼611 Million . We use the pretrained mBART for initializing the model weights . We follow the recommendations of Liu et al ( 2020 ) for the hyperparameter settings . We stop the training after 25 K gradient updates for the model . These updates take ∼35 hours on 4 Nvidia V100 ( 32 GB ) GPUs .
We use case - insensitive BLEU scores ( Papineni et al , 2002 ) calculated using sacreBLEU 2 ( Post , 2018 ) . These scores are calculated on the validation set to decide our primary and contrastive submissions . For evaluating performance on the test set , the organizers use BLEU , TER ( Snover et al , 2006 ) , and RIBES ( Isozaki et al , 2010 ) .
Results Table 3 shows our results on the test set for our primary and contrastive submissions . We observed the performance of our three model settings on the validation set , and we selected the mBART model as our primary submission and SMT model as the contrastive submission for hi ↔ mr . Similarly , the mBART model forms our primary submission for es ↔ pt . Table 4 lists our final results on this shared task . We also list the BLEU scores for the submission that got first rank in each of the language directions . Since the test sets were hidden at the time of submission , we do not report our numbers on the standard Transformer architecture . Analysis Even though Marathi and Portuguese are not present during the pre - training phase of mBART , fine - tuning on these languages provides significant boosts over SMT and standard Transformer . This shows that some level of language independent multilingual embeddings are present in the pre - trained model weights which can be exploited for the transfer task .
We have participated in the Similar Language Translation task on four language directions . We have shown that pre - trained models can help in low and medium resource NMT . Our best system uses the pre - trained mBART model ( Liu et al , 2020 ) and fine - tunes on the parallel data provided for the specific translation task . Our results demonstrate that pre - training can help even when the language used for fine - tuning is not present during pre - training . One direction of future work is to add linguistic information during the pre - training phase to get more fluent translations . When this information is not available directly ( especially for low resource languages ) , pre - training on a related high resource language with syntax information can help low resource languages also . by the DARPA Explainable Artificial Intelligence ( XAI ) Program with number N66001 - 17 - 2 - 4032 , Visvesvaraya Young Faculty Fellowships by Govt . of India and IBM SUR awards . Any opinions , findings , conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies , either expressed or implied , of the funding agencies .
We thank the IIT Delhi HPC facility 3 for the computational resources . We are also thankful to Ganesh Ramakrishnan and Pawan Goyal for initial discussions on the project . Parag Singla is supported
