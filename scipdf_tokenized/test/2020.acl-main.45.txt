Dice Loss for Data - imbalanced NLP Tasks
Many NLP tasks such as tagging and machine reading comprehension ( MRC ) are faced with the severe data imbalance issue : negative examples significantly outnumber positive ones , and the huge number of easy - negative examples overwhelms training . The most commonly used cross entropy criteria is actually accuracy - oriented , which creates a discrepancy between training and test . At training time , each training instance contributes equally to the objective function , while at test time F1 score concerns more about positive examples .
In this paper , we propose to use dice loss in replacement of the standard cross - entropy objective for data - imbalanced NLP tasks . Dice loss is based on the Sørensen - Dice coefficient ( Sorensen , 1948 ) or Tversky index ( Tversky , 1977 ) , which attaches similar importance to false positives and false negatives , and is more immune to the data - imbalance issue . To further alleviate the dominating influence from easy - negative examples in training , we propose to associate training examples with dynamically adjusted weights to deemphasize easy - negative examples . Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training . With the proposed training objective , we observe significant performance boosts over a wide range of data imbalanced NLP tasks . Notably , we are able to achieve SOTA results on CTB5 , CTB6 and UD1.4 for the part of speech tagging task , and competitive or even better results on CoNLL03 , OntoNotes5.0 , MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks . The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP . ( Rajpurkar et al , 2016 ) 10.3 M 175 K 55.9 SQuAD 2.0 ( Rajpurkar et al , 2018 ) 15.4 M 188 K 82.0 QUOREF ( Dasigi et al , 2019 ) 6.52 M 38.6 K 169
Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension . Table 1 gives concrete examples : for the Named Entity Recognition ( NER ) task Nadeau and Sekine , 2007 ) , most tokens are backgrounds with tagging class O. Specifically , the number of tokens with tagging class O is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset ; Dataimbalanced issue is more severe for MRC tasks ( Rajpurkar et al , 2016 ; Nguyen et al , 2016 ; Rajpurkar et al , 2018 ; Kočiskỳ et al , 2018 ; Dasigi et al , 2019 ) with the value of negative - positive ratio being 50 - 200 , which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context , and given a chunk of text of an arbitrary length , only two tokens are positive ( or of interest ) with all the rest being background . Data imbalance results in the following two issues : ( 1 ) the training - test discrepancy : Without balancing the labels , the learning process tends to converge to a point that strongly biases towards class with the majority label . ( Lample et al , 2016 ; Devlin et al , 2018 ; Yu et al , 2018a ; McCann et al , 2018 ; Ma and Hovy , 2016 ; , handles neither of the issues . To handle the first issue , we propose to replace CE or MLE with losses based on the Sørensen - Dice coefficient ( Sorensen , 1948 ) or Tversky index ( Tversky , 1977 ) . The Sørensen - Dice coefficient , dice loss for short , is the harmonic mean of precision and recall . It attaches equal importance to false positives ( FPs ) and false negatives ( FNs ) and is thus more immune to data - imbalanced datasets . Tversky index extends dice loss by using a weight that trades precision and recall , which can be thought as the approximation of the F β score , and thus comes with more flexibility . Therefore , we use dice loss or Tversky index to replace CE loss to address the first issue . Only using dice loss or Tversky index is not enough since they are unable to address the dominating influence of easy - negative examples . This is intrinsically because dice loss is actually a soft version of the F1 score . Taking the binary classification task as an example , at test time , an example will be classified as negative as long as its probability is smaller than 0.5 , but training will push the value to 0 as much as possible . The rest of this paper is organized as follows : related work is presented in Section 2 . We describe different proposed losses in Section 3 . Experimental results are presented in Section 4 . We perform ablation studies in Section 5 , followed by a brief conclusion in Section 6 . 2 Related Work
The idea of weighting training examples has a long history . Importance sampling ( Kahn and Marshall , 1953 ) assigns weights to different samples and changes the data distribution . Boosting algorithms such as AdaBoost ( Kanduri et al , 2018 ) select harder examples to train subsequent classifiers . Similarly , hard example mining ( Malisiewicz et al , 2011 ) downsamples the majority class and exploits the most difficult examples . Oversampling ( Chen et al , 2010 ; Chawla et al , 2002 ) ( Jiang et al , 2017 ; Fan et al , 2018 ) proposed to learn a separate network to predict sample weights .
The background - object label imbalance issue is severe and thus well studied in the field of object detection ( Li et al , 2015 ; Girshick , 2015 ; Girshick et al , 2013 ; . The idea of hard negative mining ( HNM ) ( Girshick et al , 2013 ) has gained much attention recently . Pang et al ( 2019 ) proposed a novel method called IoU - balanced sampling and designed a ranking model to replace the conventional classification task with an average - precision loss to alleviate the class imbalance issue . The efforts made on object detection have greatly inspired us to solve the data imbalance issue in NLP . Sudre et al ( 2017 ) addressed the severe class imbalance issue for the image segmentation task . They proposed to use the class re - balancing property of the Generalized Dice Loss as the training objective for unbalanced tasks . Shen et al ( 2018 ) investigated the influence of Dice - based loss for multi - class organ segmentation using a dataset of abdominal CT volumes . Kodym et al ( 2018 ) 3 Losses
For illustration purposes , we use the binary classification task to demonstrate how different losses work . The mechanism can be easily extended to multi - class classification . Let X denote a set of training instances and each instance x i X is associated with a golden binary label y i = [ y i0 , y i1 ] denoting the ground - truth class x i belongs to , and p i = [ p i0 , p i1 ] is the predicted probabilities of the two classes respectively , where y i0 , y i1 { 0 , 1 } , p i0 , p i1 [ 0 , 1 ] and p i1 + p i0 = 1 .
The vanilla cross entropy ( CE ) loss is given by : CE = − 1 N i j { 0 , 1 } y ij log p ij ( 1 ) As can be seen from Eq.1 , each x i contributes equally to the final objective . Two strategies are normally used to address the the case where we wish that not all x i are treated equally : associating different classes with different weighting factor α or resampling the datasets . For the former , Eq.1 is adjusted as follows : Weighted CE = − 1 N i α i j { 0 , 1 } y ij log p ij ( 2 ) where α i [ 0 , 1 ] may be set by the inverse class frequency or treated as a hyperparameter to set by cross validation . In this work , we use lg ( n−nt nt + K ) to calculate the coefficient α , where n t is the number of samples with class t and n is the total number of samples in the training set . K is a hyperparameter to tune . Intuitively , this equation assigns less weight to the majority class and more weight to the minority class . The data resampling strategy constructs a new dataset by sampling training examples from the original dataset based on human - designed criteria , e.g. extracting equal training samples from each class . Both strategies are equivalent to changing the data distribution during training and thus are of the same nature . Empirically , these two methods are not widely used due to the trickiness of selecting α especially for multi - class classification tasks and that inappropriate selection can easily bias towards rare classes ( Valverde et al , 2017 ) .
Sørensen - Dice coefficient ( Sorensen , 1948 ; Dice , 1945 ) , dice coefficient ( DSC ) for short , is an F1oriented statistic used to gauge the similarity of two sets . Given two sets A and B , the vanilla dice coefficient between them is given as follows : DSC ( A , B ) = 2 | A ∩ B | | A | + | B | ( 3 ) In our case , A is the set that contains all positive examples predicted by a specific model , and B is the set of all golden positive examples in the dataset . When applied to boolean data with the definition of true positive ( TP ) , false positive ( FP ) , and false negative ( FN ) , it can be then written as follows : DSC = 2TP 2TP + FN + FP = 2 TP TP+FN TP TP+FP TP TP+FN + TP TP+FP = 2Pre × Rec Pre+Rec = F 1 ( 4 ) For an individual example x i , its corresponding dice coefficient is given as follows : DSC ( x i ) = 2p i1 y i1 p i1 + y i1 ( 5 ) As can be seen , a negative example ( y i1 = 0 ) does not contribute to the objective . For smoothing purposes , it is common to add a γ factor to both the nominator and the denominator , making the form to be as follows ( we simply set γ = 1 in the rest of Loss Formula ( one sample x i ) CE − j { 0 , 1 } y ij log p ij WCE −α i j { 0 , 1 } y ij log p ij DL 1 − 2p i1 y i1 + γ p 2 i1 + y 2 i1 + γ TL 1 − p i1 y i1 + γ p i1 y i1 + α p i1 y i0 + β p i0 y i1 + γ DSC 1 − 2 ( 1−p i1 ) p i1 y i1 + γ ( 1−p i1 ) p i1 + y i1 + γ FL −α i j { 0 , 1 } ( 1 − p ij ) γ log p ij DSC ( x i ) = 2p i1 y i1 + γ p i1 + y i1 + γ ( 6 ) As can be seen , negative examples whose DSC is γ p i1 + γ , also contribute to the training . Additionally , Milletari et al ( 2016 ) proposed to change the denominator to the square form for faster convergence , which leads to the following dice loss ( DL ) : DL = 1 N i 1 − 2p i1 y i1 + γ p 2 i1 + y 2 i1 + γ ( 7 ) Another version of DL is to directly compute setlevel dice coefficient instead of the sum of individual dice coefficient , which is easier for optimization : DL = 1 − 2 i p i1 y i1 + γ i p 2 i1 + i y 2 i1 + γ ( 8 ) Tversky index ( TI ) , which can be thought as the approximation of the F β score , extends dice coefficient to a more general case . Given two sets A and B , tversky index is computed as follows : TI = | A ∩ B | | A ∩ B | + α | A\B | + β | B\A | ( 9 ) Tversky index offers the flexibility in controlling the tradeoff between false - negatives and falsepositives . It degenerates to DSC if α = β = 0.5 . The Tversky loss ( TL ) is thus given as follows : TL = 1 N i 1 − pi1yi1 + γ pi1yi1 + α pi1yi0 + β pi0yi1 + γ ( 10 )
Consider a simple case where the dataset consists of only one example x i , which is classified as positive as long as p i1 is larger than 0.5 . The computation of F 1 score is actually as follows : The derivative of DSC approaches zero right after p exceeds 0.5 , and for the other losses , the derivatives reach 0 only if the probability is exactly 1 , which means they will push p to 1 as much as possible . F1 ( x i ) = 2 I ( p i1 > 0.5 ) y i1 I ( p i1 > 0.5 ) + y i1 ( 11 ) Comparing Eq.5 with Eq.11 , we can see that Eq.5 is actually a soft form of F 1 , using a continuous p rather than the binary I ( p i1 > 0 . To address this issue , we propose to multiply the soft probability p with a decaying factor ( 1 − p ) , changing Eq.11 to the following adaptive variant of DSC : DSC ( x i ) = 2 ( 1 − p i1 ) p i1 y i1 + γ ( 1 − p i1 ) p i1 + y i1 + γ ( 12 ) One can think ( 1 − p i1 ) as a weight associated with each example , which changes as training proceeds . The intuition of changing p i1 to ( 1 − p i1 ) p i1 is to push down the weight of easy examples . For easy examples whose probability are approaching 0 or 1 , ( 1 − p i1 ) p i1 makes the model attach significantly less focus to them . A close look at Eq.12 reveals that it actually mimics the idea of focal loss ( FL for short ) ( Lin et al , 2017 ) In Table 2 , we summarize all the aforementioned losses . Figure 1 gives an explanation from the perspective in derivative : The derivative of DSC approaches zero right after p exceeds 0.5 , which suggests the model attends less to examples once they are correctly classified . But for the other losses , the derivatives reach 0 only if the probability is exactly 1 , which means they will push p to 1 as much as possible .
We evaluated the proposed method on four NLP tasks , part - of - speech tagging , named entity recognition , machine reading comprehension and paraphrase identification . Hyperparameters are tuned on the corresponding development set of each dataset . More experiment details including datasets and hyperparameters are shown in supplementary material .
Settings Part - of - speech tagging ( POS ) is the task of assigning a part - of - speech label ( e.g. , noun , verb , adjective ) to each word in a given text . In this paper , we choose BERT ( Devlin et al , 2018 ) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank ( Xue et al , 2005 ) 5.0/6.0 and UD1.4 and English datasets including Wall Street Journal ( WSJ ) and the dataset proposed by Ritter et al ( 2011 ) . We report the span - level micro - averaged precision , recall and F1 for evaluation . Baselines We used the following baselines : Results
Settings Named entity recognition ( NER ) is the task of detecting the span and semantic category of entities within a chunk of text . Our implementation uses the current state - of - the - art model proposed by as the backbone , and changes the MLE loss to DSC loss . Datasets that we use include OntoNotes4.0 ( Pradhan et al , 2011 ) , MSRA ( Levow , 2006 ) , CoNLL2003 ( Sang and Meulder , 2003 and OntoNotes5.0 ( Pradhan et al , 2013 ) . We report span - level micro - averaged precision , recall and F1 . Baselines We use the following baselines : ELMo : a tagging model with pretraining from Peters et al ( 2018 ) . Lattice - LSTM : Zhang and Yang ( 2018 ) Results
Settings The task of machine reading comprehension ( MRC ) ( Seo et al , 2016 ; Wang and Jiang , 2016 ; Shen et al , 2017 ; predicts the answer span in the passage given a question and the passage . We followed the standard protocols in Seo et al ( 2016 ) , in which the start and end indexes of answer are predicted . We report Extract Match ( EM ) as well as F1 score on validation set . We use three datasets on this task : SQuAD v1.1 , SQuAD v2.0 ( Rajpurkar et al , 2016 ( Rajpurkar et al , , 2018 and Quoref ( Dasigi et al , 2019 ) . Baselines We used the following baselines : enables learning bidirectional contexts . Results Table 6 shows the experimental results for MRC task . With either BERT or XLNet , our proposed DSC loss obtains significant performance boost on both EM and F1 . For SQuADv1.1 , our proposed method outperforms XLNet by +1.25 in terms of F1 score and +0.84 in terms of EM . For SQuAD v2.0 , the proposed method achieves 87.65 on EM and 89.51 on F1 . On QuoRef , the proposed method surpasses XLNet by +1.46 on EM and +1.41 on F1 .
Settings Paraphrase identification ( PI ) is the task of identifying whether two sentences have the same meaning or not . We conduct experiments on the two widely - used datasets : MRPC ( Dolan and Brockett , 2005 ) and QQP . F1 score is reported for comparison . We use BERT ( Devlin et al , 2018 ) and XLNet ( Yang et al , 2019 ) as baselines . Results Table 7 shows the results . We find that replacing the training objective with DSC introduces performance boost for both settings , +0.58 for MRPC and +0.73 for QQP .
It is interesting to see how differently the proposed objectives affect datasets imbalanced to different extents . We use the paraphrase identification dataset QQP ( 37 % positive and 63 % negative ) for studies . To construct datasets with different imbalance degrees , we used the original QQP dataset to construct synthetic training sets with different positive - negative ratios . Models are trained on these different synthetic sets and then test on the same original test set . Results are shown in Table 8 . We first look at the first line , with all results obtained using the MLE objective . We can see that + positive outperforms original , and + negative underperforms original . This is in line with our expectation since + positive creates a balanced dataset while + negative creates a more imbalanced dataset . Despite the fact that - negative creates a balanced dataset , the number of training data decreases , resulting in inferior performances . DSC achieves the highest F1 score across all datasets . Specially , for + positive , DSC achieves minor improvements ( +0.05 F1 ) over DL . In contrast , it significantly outperforms DL for + negative dataset . This is in line with our expectation since DSC helps more on more imbalanced datasets . The performance of FL and DL are not consistent across different datasets , while DSC consistently performs the best on all datasets .
We argue that the cross - entropy objective is actually accuracy - oriented , whereas the proposed losses perform as a soft version of F1 score . To These results verify that the proposed dice loss is not accuracy - oriented , and should not be used for accuracy - oriented tasks .
As mentioned in Section 3.3 , Tversky index ( TI ) offers the flexibility in controlling the tradeoff between false - negatives and false - positives . In this subsection , we explore the effect of hyperparameters ( i.e. , α and β ) in TI to test how they manipulate the tradeoff . We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset . Experimental results are shown in Table 10 . The highest F1 on Chinese OntoNotes4.0 is 84.67 when α is set to 0.6 while for QuoRef , the highest F1 is 68.44 when α is set to 0.4 . In addition , we can observe that the performance varies a lot as α changes in distinct datasets , which shows that the hyperparameters α , β acturally play an important role in TI .
In Table 10 : The effect of hyperparameters in Tversky Index . We set β = 1 − α and thus we only list α here . to achieve significant performance boost without changing model architectures . annotation of grammar ( parts of speech , morphological features , and syntactic dependencies ) across different human languages . In this work , we use UD1.4 for Chinese POS tagging .
Datasets For the NER task , we consider both Chinese datasets , i.e. , OntoNotes4.0 5 and MSRA 6 , and English datasets , i.e. , CoNLL2003 7 and OntoNotes5.0 8 . CoNLL2003 is an English dataset with 4 entity types : Location , Organization , Person and Miscellaneous . We followed data processing protocols in ( Ma and Hovy , 2016 ) . English OntoNotes5.0 consists of texts from a wide variety of sources and contains 18 entity types . We use the standard train / dev / test split of CoNLL2012 shared task . Chinese MSRA performs as a Chinese benchmark dataset containing 3 entity types . Data in MSRA is collected from news domain . Since the development set is not provided in the original MSRA dataset , we randomly split the training set into training and development splits by 9:1 . We use the official test set for evaluation . Chinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain , which has 18 entity types . In this paper , we take the same data split as did .
Datasets For MRC task , we use three datasets : SQuADv1.1 / v2.0 9 and Queref 10 datasets . SQuAD v1.1 and SQuAD v2.0 are the most widely used QA benchmarks . SQuAD1.1 is a collection of 100 K crowdsourced question - answer pairs , and SQuAD2.0 extends SQuAD1.1 allowing no short answer exists in the provided passage . MRPC is a corpus of sentence pairs automatically extracted from online news sources , with human annotations of whether the sentence pairs are semantically equivalent . The MRPC dataset has imbalanced classes ( 6800 pairs in total , and 68 % for positive , 32 % for negative ) . QQP is a collection of question pairs from the community question - answering website Quora . The class distribution in QQP is also unbalanced ( over 400 , 000 question pairs in total , and 37 % for positive , 63 % for negative ) .
We thank all anonymous reviewers , as well as Qinghong Han , Wei Wu and Jiawei Wu for their comments and suggestions . The work is supported by the National Natural Science Foundation of China ( NSFC No . 61625107 and 61751209 ) .
