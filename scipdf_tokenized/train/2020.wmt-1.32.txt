IIE 's Neural Machine Translation Systems for WMT20
In this paper we introduce the systems IIE submitted for the WMT20 shared task on German↔French news translation .
Our systems are based on the Transformer architecture with some effective improvements .
Multiscale collaborative deep architecture , data selection , back translation , knowledge distillation , domain adaptation , model ensemble and re - ranking are employed and proven effective in our experiments .
Our German French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .
We participate in the WMT20 shared news translation task in one language pair and two language directions , German French and French German .
Our methods are based on techniques and approaches used in submissions from past years ( Deng et al , 2018 ; Ng et al , 2019 ; Sun et al , 2019 ; Li et al , 2019 ; Xia et al , 2019 ) , including the use of subword models ( Sennrich et al , 2016 ) , iterative back - translation , knowledge distillation , model ensembling and several techniques we proposed recently ( Wei et al , 2020b , a ) . 
For our submissions of two language directions , we adopt the deep transformer architectures ( 48layer ) based on multiscale collaboration mechanism ( Wei et al , 2020b ) as our baseline , which outperformed the standard Transformer - Big as well as shallower models significantly in terms of translation quality .
We also use an iterative back - translation approach with the controllable sampling to extend the back translation method by jointly training source - to - target and target - to - source NMT models .
Moreover , the knowledge distillation ( Freitag et al , 2017 ) is employed to leverage the source - side monolingual data .
For our final models , we apply a domainspecific fine - tuning process and model ensembling , and decode using noisy channel model re - ranking . 
The paper is structured as follows : Section 2 describes the techniques we used , then section 3 shows the experimental settings and results .
Finally , we conclude our work in Section 4 .
The structure of NMT models has evolved quickly , such as RNN - based ( Wu et al , 2016 ) , CNN - based ( Gehring et al , 2017 ) and attentionbased ( Vaswani et al , 2017 ) systems .
Deep neural networks have revolutionized the state - of - the - art in various communities , from computer vision to natural language processing .
We adopt the deep transformer model proposed by our work ( Wei et al , 2020b ) .
Instead of relying on the whole encoder stack to directly learn a desired representation , we let each encoder block learn a fine - grained representation and enhance it by encoding spatial dependencies using a bottom - up network .
For coordination , we attend each block of the decoder to both the corresponding representation of the encoder and the contextual representation with spatial dependencies .
This not only shortens the path of error propagation , but also helps to prevent the lower level information from being forgotten or diluted .
In this section we describe the details ( as illustrated in figure 1 ) of our deep architectures as below : Block - Scale Collaboration .
An intuitive extension of naive stacking of layers is to group few stacked layers into a block .
We suppose that the encoder and decoder of our model have the same number of blocks ( i.e. , N ) .
Each block of the encoder has M n ( n { 1 , 2 , ... , N } )
identical layers , while each decoder block contains one layer .
Thus , we can adjust the value of each M n flexibly to increase the depth of the encoder .
Formally , for the n - th block of the encoder : B n e = BLOCK e ( B n−1 e ) , ( 1 ) where BLOCK e ( ) is the block function , in which the layer function F ( ) is iterated M n times , i.e. where l { 1 , 2 , ... , M n } , H n , l e and Θ n , l e are the representation and parameters of the l - th layer in the n - th block , respectively .
The decoder works in a similar way but the layer function G ( ) is iterated only once in each block , B n d =
BLOCK d ( B n−1 d , B n e )
= G ( B n−1 d , B n e ; Θ n d ) + B n−1 d .
( 3 ) Each block of the decoder attends to the corresponding encoder block . 
Contextual Collaboration .
To model long - term spatial dependencies and reuse global representations , we define a GRU cell Q ( c , x ) , which maps a hidden state c and an additional inputx into a new hidden state : C n = Q
( C n−1 , B n e ) ,
n [ 1 , N ] C 0
= E e , ( 4 ) where E e is the embedding matrix of the source input x.
The new state C n can be fused with each layer of the subsequent blocks in both the encoder and the decoder .
Formally , B n e in Eq .
( 1 ) can be re - calculated in the following way : B
n e = H
n , Mn e , H n ,
l e = F
( H n , l−1 e , C n−1 ; Θ n , l e )
+ H n , l−1 e , H n , 0 e = B n−1 e .
( 5 ) Similarly , for decoder , we have B n
d = BLOCK d ( B n−1 d , B n e )
= G ( B n−1 d , B n e , C n ; Θ n d ) + B n−1 d .
( 6 )
Back - translation ( BT ) is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system .
Back - translation first trains an intermediate targetto - source system that is used to translate monolingual target data into additional synthetic parallel data .
This data is used in conjunction with human translated bitext data to train the desired source - totarget system . 
In our work , we use an iterative back - translation approach to jointly train source - to - target and targetto - source NMT models .
The process can be summarized as below : step 1 : we train both a source - to - target model ( M 0 x y ) and a target - to - source model ( M 0 y x ) using the human translated data . step 2 : we use M t
x y to translate source - side monolingual data to target language , and use M t
y x to translate target - side monolingual data to source language , where t starts from 0 . step 3 : we combine both the human translated data and pseudo data synthesized in step 2 to further optimize the two NMT models respectively .
Repeat steps 2 - 3 until the models converge . 
In practice , we repeat 3 times for steps 2 - 3 .
We apply the controllable sampling strategy ( Wei et al , 2020a ) to synthesize reasonable sentences which are at both high quality and diversity .
The early adoption of knowledge distillation ( KD ) ( Kim and Rush , 2016 ) is for model compression .
We use the same method as in Sun et al ( 2019 ) that adopts hybrid heterogeneous teacher : base transformer , deep transformer , big transformer and RNMT+ .
For each individual model , we use the other two models as the teacher model to further improve the performance . 
In addition , model ensemble is also used to boost the performance by combining the predictions of above four models at each decoding step .
Fine - tuning with domain - specific data is a common and effective method to improve translation quality for a downstream task .
After completing training on the bitext and back - translated data , we train for an additional epoch on a smaller in - domain corpus . 
We first select 100 K sentence - pairs from the bilingual as well as pseudo - generated data according to the filter method in Deng et al ( 2018 ) and continue to train the model on the filtered data .
N - best reranking is a method of improving translation quality by scoring and selecting a candidate hypothesis from a list of n - best hypotheses generated by a source - to - target model .
For our submissions , we rerank the n - best hypotheses using two aspects as follows : log p ( y
| x )
+
λ 1 log p
( x | y )
+
λ 2 log p ( y ) ( 7 ) 
The weights λ 1 and λ 2 are determined by tuning them with a random search on a validation set and selecting the weights that give the best performance .
We submit constrained systems to both German to French and French to German translations , with the same techniques .
We use all available bilingual datasets and select 10 M bilingual data from WMT'20 corpora using the script filter interactive.py 1 .
We share a vocabulary for the two languages and apply BPE for word segmentation with 32 K merge operations .
For monolingual data , we use 18 M German sentences and 18 M French sentences from Newscrawl , and pre - process them in the same way as bilingual data .
We split 9k sentences from the " dev08 - 14 " as the validation set and use newstest 2019 as the test set .
We use the PyTorch implementation of Transformer 2 .
We choose the Transformer base setting , in which the encoder and decoder are of 48 and 6 layers , respectively .
The dropout rate is fixed as 0.1 .
We set the batch size as 4096 and the parameter - - update - freq as 16 .
Results and ablations for De Fr Fr De are shown in Table 1 and 2 , respectively .
We report case - sensitive SacreBLEU scores using Sacre - BLEU ( Post , 2018 ) 3 , using international tokenization for German↔French . 
German French For De Fr , iterative BT improves our baseline performance on newstest 2019 by about 2.5 BLEU .
The addition of KD and model ensemble improves single model performance by 0.8 BLEU , but combining this with fine - tuning and reranking gives us a total of 2 BLEU .
Our final submission for WMT20 achieves 35.0 BLEU points for German French translation ( ranked in the second place ) . 
French German For Fr De , we see similar improvements with iterative BT by about 2.3 BLEU .
KD , ensembling , and fine - tuning add an additional 1.4 BLEU , with reranking contributing 0.9 BLEU .
Our final submission for WMT20 achieves 36.6 BLEU points for French German translation ( ranked in the fourth among anonymous submissions ) .
This paper describes CAS IIE 's submission to the WMT20 German↔French news translation task .
We investigate extremely deep models ( with 48 layers ) and exploit effective strategies to better utilize parallel data as well as monolingual data .
Finally , our German French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .
