Semi - Supervised Iterative Approach for Domain - Specific Complaint Detection in Social Media
In this paper , we present a semi - supervised bootstrapping approach to detect product or service related complaints in social media .
Our approach begins with a small collection of annotated samples which are used to identify a preliminary set of linguistic indicators pertinent to complaints .
These indicators are then used to expand the dataset .
The expanded dataset is again used to extract more indicators .
This process is applied for several iterations until we can no longer find any new indicators .
We evaluated this approach on a Twitter corpus specifically to detect complaints about transportation services .
We started with an annotated set of 326 samples of transportation complaints , and after four iterations of the approach , we collected 2 , 840 indicators and over 3 , 700 tweets .
We annotated a random sample of 700 tweets from the final dataset and observed that nearly half the samples were actual transportation complaints .
Lastly , we also studied how different features based on semantics , orthographic properties , and sentiment contribute towards the prediction of complaints .
Social media has lately become one of the primary venues where users express their opinions about various products and services .
These opinions are extremely useful in understanding the user 's perceptions and sentiment about these services .
They are also useful in identifying potential defects ( Abrahams et al , 2012 ) and thus critical to the execution of downstream customer service responses .
Therefore , automatic detection of user complaints on social media could prove beneficial to both the clients and the service providers .
To build such detection systems , we could employ supervised approaches that would typically require a large corpus of labeled training samples .
However , labeling social media posts that capture complaints about a particular service is challenging because of their low prevalence and also the vast amounts of inevitable noise ( Kietzmann et al , 2011 ; Lee , 2018 ) .
Additionally , social media platforms are also likely to be plagued with redundancy , where the posts are rephrased or structurally morphed before being re - posted ( Ellison et al , 2011 ; Harrigan et al , 2012 ) . 
Prior work in event detection ( Ritter et al , 2012 ) has demonstrated that simple linguistic indicators ( phrases or n - grams ) can be useful in the accurate discovery of events in social media .
Though user complaints are not the same as events , more of a speech act ( Preotiuc - Pietro et al , 2019 ) , we posit that similar indicators can be used in complaint detection .
To pursue this hypothesis , we propose a semi - supervised iterative approach to identify social media posts that complain about a specific service . 
In our approach , we first begin with a small , manually curated dataset containing samples of social media posts complaining about a service .
We then identify linguistic indicators ( phrases or n - grams ) that serve as strong evidence of this phenomenon .
These indicators are then used to extract more posts from the unannotated corpus .
This newly obtained data is then used to create a new set of indicators .
This process is repeated until it reaches a certain convergence point .
Since the set of indicators is growing after each iteration , they are re - evaluated continuously in terms of their relevance .
This process is similar to the mutual bootstrapping approach for information extraction proposed in ( Riloff et al , 2003 ) . 
We employ this approach to the problem of complaint detection for transportation services on Twitter .
Transportation and its related logistic services are critical aspects of every economy as they account for nearly 40 % of the value of international trade ( Rodrigue , 2007 ) .
As with most businesses ( Gallaugher and Ransbotham , 2010 ; Gottipati et al , 2018 ) , transportation also often relies on social media to ascertain feedback and initiate appropriate responses ( Stelzer et al , 2016 ( Stelzer et al , , 2014 .
In our experimental work , we started with an annotated set of 326 samples of transportation complaints , and after four iterations of the approach , we collected 2 , 840 indicators and over 3 , 700 tweets .
We annotated a random sample of 700 tweets from the final dataset and observed that over 47 % of the samples were actual transportation complaints .
We also characterize the performance of basic classification algorithms on this dataset .
In doing so , we also study how different linguistic features contribute to the performance of a supervised model in this domain . 
The main contributions of this paper are as follows : We propose a semi - supervised iterative approach to collect user complaints about a service from social media platforms .
We evaluate the proposed approach for the problem of complaint detection for transportation services on Twitter .
We annotate a random sample of the resulting dataset to establish that nearly half the tweets were actual complaints .
We release a curated dataset for the task of traffic - related complaint detection in social media 1 . 
Lastly , we characterize the performance of basic classification algorithms on the dataset .
Complaints are often considered dialogue acts used to express a mismatch between the expectation and reality ( Olshtain and Weinbach , 1985 ) .
The problem of complaint detection is of great interest to the marketing and research teams of various service providers .
Previous works on complaint identification have applied text mining with LDA and sentiment analysis on user - generated content Duan et al , 2013 ) .
Prior works have also focused on leveraging data streamed from social media platforms for outage and complaint detection as they are publicly available ( Augustine et al , 2012 ; Kursar and Gopinath , 2013 ) .
( Yang et al , 2019 ) inspected customer support dialogue for support .
Different complaint expressions have been explored by analyzing variations across cultures ( Cohen and Olshtain , 1993 ) , sociodemographic traits ( Boxer , 1993 ) and temporal representations ( Raghavan , 2014 ) .
However , mentioned works on user - generated content have focused on static data repositories only .
These have not been robust to linguistic variations ( Shah and Zimmermann , 2017 ) and morphological changes ( Abdul - Mageed and Korayem , 2010 ) .
Our pipeline builds on linguistic identifiers to expand on lexical cues in order to identify complaint relevant posts . 
Researches have proposed many semisupervised architectures for identification of events pertaining to societal and civil unrest ( Hua et al , 2013 ) , using speech modality ( Serizel et al , 2018 ; Wu et al , 2014 ; Zhang et al , 2017 ) and Hidden Markov Models ( Zhang , 2005 ) .
These have been documented to give better performance as compared against their counterparts ( Lee et al , 2017 ; Zheng et al , 2017 ) with minimal intervention ( Rahimi et al , 2018 ) .
For our analysis , the semi - supervised approach has been preferred as opposed to supervised ones because : ( a ) usage of supervised approach relies on carefully choosing the training set making it cumbersome and less attractive for practical use ( Watanabe , 2018 ) and ( b ) imbalance between the subjective and objective classes lead to poor performance ( Yu et al , 2015 ) .
Our proposed approach begins with a large corpus of transport - related tweets and a small set of annotated complaints .
We use this labeled data to create a set of seed indicators that drive the rest of our iterative complaint detection process .
We focused our experimentation over the period of November 2018 to December 2018 .
Our first step towards creating a corpus of transportrelated tweets is to identify linguistic markers related to the transport domain .
To this end , we scraped random posts from transport - related web forums 2 .
These forums involve users discussing their grievances and raising awareness about a wide array of transportation - related issues .
We then processed this data to extract words and phrases ( unigrams , bigrams , and trigrams ) with high tf - idf scores .
We then had human annotators prune them further to remove duplicates and irrelevant items .
This resulted in a lexicon of 75 unique phrases .
Some examples include cabs , discount , tickets , underground , luggage , transit , parking , neighborhood , downtown , traffic , Uber . 
We used Twitter 's public streaming API to query for tweets that contained any of the 75 phrases over the chosen time range .
We then excluded non - English tweets and any tweets with less than two tokens .
This resulted in a collection of 19 , 300 tweets .
We will refer to this collection as corpus C.
We chose a random sample of 1 , 500 tweets from this collection for human annotation .
We employed two human annotators to identify traffic - related complaints from these 1 , 500 tweets .
Following are some high - level details of the annotation task . 
We instructed the annotators to identify any tweets that contain first - hand accounts of a complaint or a grievance related to a public / private mode of transport .
Following is a sample tweet from this instruction : " @ [ UserHandle ] can you please make sure that compartment A - 6 is at least clean before public use . "
We also instructed them to identify tweets that provide verifiable sources of information ( news ) about transport - related services .
Sample tweet : " 4 hour jam in [ place ] area due to rain and poor management of traffic police . " .
Lastly , we also explicitly asked them to exclude tweets that contain announcements or advertisements about transportation services .
Sample tweet : " Please use [ name ] cabs , you will get 60 % discount on your first 3 rides .
" The two annotators worked independently , and when we finally tallied their responses , we observed that they had an inter - annotator agreement rate of κ = 0.81
( Cohen kappa ) .
In cases where the annotators disagreed , the labels were resolved through a discussion .
After the disagreements were resolved , the final seed dataset had 326 samples of traffic - related complaints .
We will refer to this as T s .
Table 1 shows some examples of tweets that were annotated as complaints .
Our proposed iterative approach is summarized in Algorithm 1 .
First , we use the seed data T s to build a set of linguistic indicators I for complaints . 
We then use these indicators to get potential new complaints T l from the corpus C.
We merge T s and T l to build our new dataset .
We then use this new dataset to extract a new set of indicators I l .
The indicators are combined with the original indicators I to extract the next version of T l .
This process is repeated until we can no longer find any new indicators .
As shown in Algorithm 1 , extracting linguistic indicators ( n - grams ) is one of the most important steps in the process .
These indicators are critical to identifying tweets that are most likely domainspecific complaints .
We employ two different approaches for extracting these indicators .
For seed data , T s , which is annotated , we just select n - grams with the highest tf - idf scores .
In our experimental work , T s had 326 annotated tweets .
We identified 50 n - grams with the highest tf - idf scores to initialize I. Some examples include : problem , station , services , toll - fee , reply , fault , provide information , driver , district , passenger .
In subsequent iterations , when we are handling unannotated samples , we use a more advanced domain relevance criterion for extracting the indicators .
When extracting indicators from T l , which is not annotated , it is possible that there could be frequently occurring phrases that are not necessarily indicative of complaints .
These phrases could lead to a concept drift in subsequent iterations .
To avoid these digressions , we use a measure of domain relevance when selecting indicators .
This is defined as the ratio of the frequency of an n - gram in T l to that of in T r .
T r is a collection of randomly chosen tweets that do not intersect with C. In our experimental work , we defined T r as a random sample of 5 , 000 tweets from a different time range than that of C. We also wanted to quantitatively en - Samples of transport - related complaints .
1 . No metro fares will be reduced , but proper fare structure needs to be introduced .... right ? .
2 . It takes [ name ] govt .
longer to refund charges , but it took them a few mins to remove that bus stop .
You ca n't erase the problem [ name ] .
3 . I tried to lodge a complaint on [ url ] but see the results .
Sir if 8 A.C 's are not working in this coach , why have you attached that coach .
 [ name ] Is that for when people ca n't travel due to your staff having to strike to keep everyone safe ?
Or perhaps short formed trains that you ca nt get on .
sure that the lexicon in T r is different from that of C. Namely , we calculated the cosine similarity between the two datasets in the tf - idf space .
The cosine similarity at a value of 0.028 was statistically significant with a Pearson correlation coefficient value 0.012 ( p - value 0.0034 )
( Schober et al , 2018 ) .
Given a set of indicators I , the process of selecting tweets from corpus C is fairly straightforward .
It only requires to identify all the tweet that contains any of the indicators .
The only caveat here is to reduce the redundancy in the dataset .
For this , we just filtered out tweets that have a cosine similarity of more than 0.85 with any other tweet in the tf - idf space ( Albakour et al , 2013 ) .
This process also helped remove tweets , which are exact matches , sub - strings , or differing by some punctuation .
Removal of these redundant tweets also helps in diversifying the lexicon for subsequent iterations .
Our iterative approach converged in four rounds , after which it did not extract any new indicators .
Figure 1 shows the counts of indicators and the number of tweets after each iteration .
After four iterations , this approach chose 3 , 732 tweets and generated 2 , 840 unique indicators .
We also manually inspected the indicators chosen during the process .
We observed that only indicators with a domain relevance score of ≥ 2.5 were chosen for subsequent iterations .
Table 2 provides a few examples of strong and weak indicators acquired after the first iteration .
In this figure , strong indicators are those with a domain relevance score ≥ 2.5 . 
We chose a random set of 700 tweets from the final complaints dataset T and annotated them manually to help understand the quality .
We used the same guidelines as discussed in section 3.1 and also employed the same annotators as before .
The anno - tators once again obtained a high agreement score of κ = 0.83 .
After resolving the disagreements , we observed that 332 tweets were labeled as complaints .
This accounts for 47.4 % of the sampled 700 tweets .
This demonstrates that nearly half the tweets selected by our semi - supervised approach were traffic - related complaints .
This is a significantly higher proportion in the original seed data T s , where only 21.7 % were actual complaints .
We conducted a series of experiments to understand if we can automatically build simple machine learning models to detect complaints .
These experiments also helped us evaluate the quality of the final dataset .
Additionally , this experimental work also studies how different types of linguistic features contribute to the detection of social media complaints .
For these experiments , we used the annotated sample of 700 posts as a test dataset .
We built our training dataset by selecting another 2 , 000 posts from the original corpus C , and anno - tated them once again per guidelines discussed in section 3.1 .
In this sample , we observed that the annotators had similar agreements scores of κ = 0.79 , and there were 702 instances of complaints .
We also wanted to understand the predictive power of different types of linguistic features towards the detection of complaints .
These features can be broadly broken down into four groups .
( i )
The first group of features are based on simple semantic properties such as n - grams , word embeddings , and part of speech tags .
( ii ) The second group of features are based on pre - trained sentiment models or lexicons .
( iii ) The third group of features use orthographic information such as hashtags , user mentions , and intensifiers .
( iv ) The last group of features again use pre - trained models or lexicons associated with request , which is a closely related speech act ( Švárová , 2008 ) .
We experimented with four different semantic features : Unigrams : Each tweet ( Wallach , 2006 ) is represented as sparse vector of tf - idf values correspond - ing to the constituent tokens . 
Word2Vec Clusters :
We follow the same approach as in ( Preoţiuc - Pietro et al , 2015 ) , where words are clustered using pair - wise similarities in Word2Vec space ( Mikolov et al , 2013 ) .
Each tweet is then represented as a distribution over these clusters ; the values are proportional to the number of tokens belonging to a cluster .
These clusters have previously been demonstrated to have great interpretability ( Preoţiuc - Pietro et
al , 2015Zou et al , 2016 ) . 
POS Tags :
We used the Stanford POS Tagger ( Manning et al , 2014 ) to represent tweets as a dense frequency vector over five main POS tags : nouns , adjectives , adverbs , verbs , pronouns . 
Pronoun Types :
Pronouns are often used in complaints and suggestions to reveal personal involvement or to add intensity to an opinion ( Claridge , 2007 ; Meinl , 2013 ) .
We identify various pronoun types ( first person , second person , third person , demonstrative , indefinite ) using dictionaries and use their counts as features .
We expect sentiment to contribute strongly towards the prediction of complaints .
We experiment with two pre - trained models : Stanford Sentiment ( Socher et al , 2013 ) and VADER ( Hutto and Gilbert , 2014 ) .
Namely , we use the scores predicted by these models as representations of tweets .
Likewise , we also experiment with two sentiment lexicons : MPQA ( Wilson et al , 2005 ) , NRC ( Mohammad et al , 2013 ) for assigning sentiment scores to tweets .
Our first set of orthographic feature uses counts of URLs , hashtags , user mentions , and special symbols used in the post .
The second set of orthographic features try to identify potential intensifiers such as capitalization and repeated use of exclamation or question marks .
These types of intensifiers are often used to express anger or strong opinions ( Meinl , 2013 ) .
A request is a speech act very closely related to complaints .
Often , the main motivation behind a complaint on a social media platform is to get a correction or reparation from the service providers ( Blum - Kulka and Olshtain , 1984 ) .
We use the model presented in ( Danescu - Niculescu - Mizil et al , 2013 ) to detect if a given tweet is a request .
Requests might also often include polite phrases in expectation of better service .
They are coded using various dictionaries e.g , downgraders ( little ) , down - toners ( just ) , hedges ( somewhat ) .
Apology markers have the same effect as politeness markers , they may include greetings at the start ( Good Morning ) , direct start ( e.g so ) , subjunctive phrases ( could you ) ( Švárová , 2008 ) .
We utilize pre - defined dictionaries to determine the presence of politeness identifiers along with the politeness score of the tweet based on the model in ( Danescu - Niculescu - Mizil et al , 2013 ) .
We trained a logistic regression model for complaint detection using each one of the features described in section 4.1 .
Table 3 summarizes the results in terms of accuracy and macro averaged F1 - score .
The best performing model is based on unigrams , with an accuracy of 75.3 % .
There is not a significant difference in the performance of different sentiment models .
It is also interesting to observe that simple features like the counts of different pronoun types and counts of intensifiers have strong predictive ability .
Overall , we observe that most of the features studied here have some ability to predict complaints .
In this paper , we presented a semi - supervised iterative approach for the detection of complaints in social media platforms .
The process begins with a small sample of annotated examples , and then iteratively builds more linguistic identifiers to expand the dataset .
We evaluated this approach on the domain of transportation on Twitter , starting with a sample of 326 annotated tweets .
After four iterations , we were able to construct a corpus with over 3 , 700 tweets .
Annotation of random samples established that nearly half the tweets were actual complaints .
We evaluated the predictive power based on semantic , orthographic , and sentiment features .
We observed that complaint is a complex speech act , which is related to many other linguistic properties . 
Automatic detection of complaints is not only useful to service providers as feedback ; it could also prove helpful in improving service providers ' operations and in downstream applications such as developing chat - bots .
Additionally , it could also be of interest to linguists in understanding how humans express grievances and criticism . 
This proposed methodology could be applied to many other products or services to detect complaints .
This would only additionally require some lexicons and a small annotated dataset .
We also expect it would be fairly straightforward to adapt this technique to many other types of speech acts .
Further investigation is necessary to understand how this method compares against supervised or completely unsupervised techniques .
