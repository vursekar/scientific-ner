Dialogue - act - driven Conversation Model : An Experimental Study
The utility of additional semantic information for the task of next utterance selection in an automated dialogue system is the focus of study in this paper .
In particular , we show that additional information available in the form of dialogue acts - when used along with context given in the form of dialogue history - improves the performance irrespective of the underlying model being generative or discriminative .
In order to show the model agnostic behavior of dialogue acts , we experiment with several well - known models such as sequence - to - sequence encoder - decoder model , hierarchical encoder - decoder model , and Siamese - based models with and without hierarchy ; and show that in all models , incorporating dialogue acts improves the performance by a significant margin .
We , furthermore , propose a novel way of encoding dialogue act information , and use it along with hierarchical encoder to build a model that can use the sequential dialogue act information in a natural way .
Our proposed model achieves an MRR of about 84.8 % for the task of next utterance selection on a newly introduced DailyDialog dataset , and outperform the baseline models .
We also provide a detailed analysis of results including key insights that explain the improvement in MRR because of dialogue act information .
In the last decade , natural language processing and machine learning - in particular deep learning - have come a long way towards building an automated dialogue system .
In a fully automated dialogue system , the goal is to predict an appropriate response given the dialogue history .
This problem of response prediction can be formulated in two ways .
One is purely generative , where the task is to generate a text response , i.e. generating a sentence or utterance from scratch , whereas the other is Next Utterance Selection , where the task is to select an appropriate response from a set of given candidates .
Despite significant research in text generation , a pure generative model capable of generating syntactically and semantically correct text still remains a distant reality .
There have been several efforts such as ( Vinyals and Le , 2015 ; Serban et al , 2016a ; Serban et al , 2016b ; Serban et al , 2017b ) for the task of dialogue generation , however these models still do not seem to work in practice ( Liu et al , 2016 ) .
This is particularly true for open domain dialogue systems .
Dialogue generation in a task - oriented oriented dialogue system , such as flight - booking and troubleshooting , is much easier than in a non - task oriented dialogue system .
This level of difficulty arises because a non - task - oriented dialogue system has no predefined goal ( or domain ) , and the vocabulary and possibilities of the dialogues could be endless .
Given these challenges , researchers have defined a simpler problem for conversation modeling based on retrieval , i.e. next utterance selection .
In this paper we use this second formulation of the problem , and show that using additional information available in the form of dialogue acts help in improving the performance of the underlying model . 
Dialogue acts ( DA ) are higher level semantic abstractions assigned to utterances in a conversation .
An example of a dialogue act for an utterance i 'll give you a call tonight is Inform since speaker is providing information .
In a traditional dialogue system , where dialogues are formulated by first sentence planning and then by surface realization , the first step is to understand the dialogue act of the utterance that needs to be generated , and then plan and realize the dialogue accordingly .
To better understand the importance of dialogue acts , consider an example of a simple conversation , where if the previous utterance is of type Question then the next utterance is most likely going to be of the type , i.e. Inform , providing information to that question .
Knowing that the next utterance is of type Inform , a conversation system with support of dialogue act information can filter a set of candidate responses , and select the most appropriate one .
Driven by this intuition , we hypothesize that understanding dialogue acts and using them in the task of next utterance selection should improve the performance irrespective of the underlying model . 
Driven by this intuition , we hypothesize that understanding dialogue acts and using them in the task of next utterance selection should improve the performance irrespective of the underlying model . 
Most of the existing literature for the task of next utterance selection can be classified into two categories .
First is based on Sequence - to - sequence models ( generative models ) ( Serban et al , 2016a ; Serban et al , 2017a ; Vinyals and Le , 2015 ) , where a model is trained to generate a response given context ; and the other is Siamese models ( discriminative models )
( Lowe et al , 2017 ) , where a model is trained to discriminate between positive and negative responses for a similar context .
In both types of models , at test time , a set of candidate responses is provided consisting of one correct response and several incorrect responses , and the model is evaluated on its ability to assign a higher rank to the true response . 
In this paper , through the experimentation with both generative and discriminative types of models , we validate the hypothesis that additional information available in the form of dialogue act significantly improves the performance irrespective of the underlying model .
In addition to showing the utility of dialogue acts , we propose a novel model that can use the sequential dialogue act information in a natural way .
More specifically , we propose a dialogue - act - driven hierarchical Siamese model .
Hierarchical models have shown to perform better than non - hierarchical models for the task of dialogue generation , whereas Siamese models have been shown to outperform the encoder - decoder based models for the task of next utterance selection .
In this paper , we combine both of these models , and further enhance them with a dialogue act encoder .
The proposed model has a hierarchical encoder which encodes the past utterances , and combine them with the representation of additional contextual information , obtained from the dialogue acts associated with the past utterances , to discriminate the correct response from the incorrect ones .
Our proposed model provides us the best of both worlds and outperforms the baseline models by a significant margin .
Among others , a key contribution of this paper is that we do a deeper analysis of the reasons for the performance improvement due to inclusion of dialogue act and draw several important key insights such as , dialogue acts induce uniformity in the data , they aid in learning the right patterns .
We believe that these insights would inspire new research in this field and push the boundary even further .
The main contributions of this paper are as follows : 1 .
For the task of next utterance selection , we validate the hypothesis that additional information available in the form of dialogue acts improves the performance irrespective of the underlying models . 
2 . We propose a novel model that combines the strength of Siamese network with strengths of hierarchical structure inherent in the conversations and dialogue act information .
The model gives us the best of all , and outperforms the baseline models by a significant margin on the DailyDialog Dataset . 
3 . We perform a deeper analysis of the utility of the dialogue act information and draw three key insights : models learn dominant dialogue act patterns ; dialogue acts induce uniformity ; dialogue acts reinforce correct dialogue act patterns . 
4 . We modify the DailyDialog ( Li et al , 2017b ) dataset for the task of next utterance selection , and release it publicly along with the code - base of the proposed model 1 .
We believe that this dataset will work as a benchmark dataset for further research on this problem .
Similar benchmark datasets have been released earlier , however they do not come with dialogue act information .
In this section , we provide details of several existing models that we will use to validate our hypothesis .
These models include generative models ( such as encoder - decoder model and its hierarchical version i.e. , hierarchical encoder - decoder ) and discriminative model ( Siamese - based model ) .
Next , we provide details of the proposed model that adds the hierarchical structure to the Siamese model along with the dialogue act information .
To set the notations , we are given a set D of N conversations , i.e. D = ( C 1 , C 2 , . . .
C N ) , with each conversation C i being a sequence of R i utterances , 
C
i = ( u 1 , u 2 , . . .
u R i ) . 
Each utterance u j in turn is itself a sequence of S j words , i.e. u j = ( w 1 , w 2 , . . .
w S j ) .
Generative models are the most widely used models for conversation modeling .
These models include encoder - decoder model and hierarchical encoder - decoder model .
An encoder - decoder is a generative model that works on the idea of obtaining a representation of an input and use it for generating an output .
It has two main components , encoder and decoder .
The encoder encodes the first K utterances , and the decoder uses that encoding to generate the next K + 1 th utterance .
In a conversation , all words in first K utterances can be stringed together to form a single long chain and passed to an RNN encoder as following : e k = f 1 embed ( w k ) ∀k 1 , 2 , . . .
h e k = f 1 rnn ( h e k−1 , e k ) ∀k 1 , 2 , . . .
( 1 ) where , f 1 embed represents the embedding layer , whereas f 1 rnn is the encoder ( RNN ) .
Let v be the final output of the encoder which is considered as a representation of the entire context , and used to initialize the decoder ( another RNN ) .
Mathematically , the sequence of operations at the decoder are as follows : h
d 0
=
v h
d k = f 2 rnn ( h d k−1 , f 2 embed ( w k ) ) ∀k
1 , 2 , . . .
n − 1 P k = Logistic ( h d k ) .
( 2 ) Here , f 2 embed represents the embedding layer .
Logistic is the final layer , which outputs the probability distribution over the vocabulary .
Encoder - decoder models are trained to maximize the likelihood of generating the next utterance , however , for the task of next utterance selection , they are tested based on the probability of generating the candidate utterances .
A simple encoder - decoder treats the first K utterances as a single long chain of words , and therefore fails to leverage the hierarchical structure , which is an inherent part of a conversation .
Hierarchy is important for conversation modeling since it captures the natural dependency among utterances .
Several researchers ( Sordoni et al , 2015 ; Serban et al , 2016b ; Serban et al , 2017b ; Dehghani et al , 2017 ; Kumar et al , 2017 ) have shown that hierarchical models outperform standard non - hierarchical models .
Hierarchical models use two encoders to capture the hierarchical structure .
The first encoder , referred as utterance encoder , operates at the utterance level , encoding each word in each utterance .
The second encoder , referred as conversation encoder , operates at the conversation level , encoding each utterance in the conversation , based on the representations of the previous encoder .
These two encoders make sure that the output of the conversation encoder captures the dependencies among utterances .
For a given conversation , each word w k of each utterance u j is processed by an embedding layer , followed by an RNN which serves as the utterance encoder .
Similar to the encoder in equation ( 1 ) , an utterance encoder gives us a sequence of representations v 1 , v 2 , . . .
v K , corresponding to the first K utterances u 1 , u 2 , . . .
u K in a conversation .
These representations are passed on to the conversation encoder , another RNN , which transforms v j to another representation g j .
The representation obtained from the last time - step of the conversation - level encoder i.e. g K is considered as the representation of the entire conversation and used to initialize the decoder which works in the same way as Equation 2 .
A decoder in the encoder - decoder model generates the next word given the context , and though it has several valid and reasonable choices , it is burdened with the task of generating exactly a particular choice that matches the ground truth .
For example , for a context I am enjoying the day , it is warm and sunny , if decoder generates yes , it is .
and the ground truth dictates yes , indeed , it is a lovely day , the decoder has failed , though it is a valid response .
Due to these challenges with generative models , discriminative models are trained directly to discriminate between positive and negative utterances .
A typical discriminative model , or in particular Siamese model , consists of two encoders , one encoder encoding the context , while another encoding the candidate utterance , i.e utterance K
+
1 .
These two representations are passed to a final layer that computes the probability of candidate being a valid response given the context .
Let h ( 1 ) and h ( 2 ) be the representations obtained from the first encoder and second encoder , respectively , then the probability of their association can be computed using the following expression . 
p ( s | h ( 1 ) , h ( 2 ) )
= σ h ( 1 ) T Ah ( 2 ) + b ( 3 ) where , the bias b and matrix A are learned model parameters .
Dialogue acts are higher level abstractions assigned to utterances .
In our problem setting , we are given a list of dialogue acts da 1 , da 2 , . . .
da K , corresponding to first K utterances in the conversation .
These dialogue acts are treated as an additional sequence of signals that can aid in the learning process , and are passed through an encoder , denoted as Dialog - Act encoder ( DA - encoder ) .
The DA - encoder works on the same principle as the utterance encoder .
It builds a dialogue act vocabulary and uses that to learn dialogue act embeddings .
Similar to the utterance encoder , the input to the DA - encoder are one hot encodings of the dialogue acts , which are then passed through an embedding layer to learn DA embeddings .
These DA embeddings are sent to an RNN to learn dialogue act representations .
The sequence of operations for the DA - encoder are as follows : e da k = f 3 embed ( da k ) ∀k 1 , 2 , . . .
K h da k = f 4 rnn ( h da k−1 , e da k ) ∀k 1 , 2 , . . .
K. q
K = h da K ( 4 ) 
The output of the DA - encoder at the last time step ( q K ) gives us the representation of the entire DA sequence which is then used in the further modeling process in generative and discriminative models .
In generative models , it is used in the decoder by concatenating g K and q K , whereas in discriminative models , it is used along with encoder 's output by combining g K with q K through a linear combination .
Our proposed model , i.e. Dialog - act - driven Hierarchical Siamese Model ( HSiamese - DA ) , uses the following three components : a hierarchical encoder to obtain a representation that captures the dependencies among K utterances ; an utterance encoder to obtain a representation of the candidate response , ( K + 1 ) th utterance ; a DA - encoder ( Equation 4 ) that captures the dependencies among the dialogue acts of the first K utterances .
Let the representation obtained from the hierarchical encoder , DA - encoder and utterance encoder be g K , q K and v K+1 , respectively .
The two representations , g K and q K , are linearly combined to obtained a compositional representation of the context , which is then used along with candidate representation to compute the probability of associating the candidate response with the context using following expression : 
d K = α *
g K + ( 1 − α )
*
q K p ( s |
d K , v K+1 )
= σ ( d T K
A v K+1 + b ) ( 5 ) 
The model is trained by minimizing the cross - entropy of all labeled conversations including positive and negative examples .
At the test time , each conversation has K utterances followed by a set of 10 candidates responses .
The system is tested in its ability to assign a higher rank to the true response .
In this section , we describe the details of the experiments , i.e. dataset and its preparation , baseline models , experimental setup , and analysis of results .
In our problem setting , we require a dataset that is of reasonable size 2 and has utterances annotated with the corresponding dialogue acts .
Although there are several available datasets , such as SwDA ( Switchboard Dialogue Act Corpus ( Jurafsky , 1997 ) ) , MRDA ( Meeting Recorder Dialogue Act corpus ( Janin et al , 2003 ) ) , Ubuntu , OpenSubtitles ( Tiedemann , 2009 ) , etc . , they are not really suitable for our problem setting .
Most of these datasets do not come with dialogue acts , and the ones which do ( i.e. SWDA and MRDA ) are small in size .
Note that the SwDA and MRDA datasets contain 1003 and 51 conversations , respectively .
To the best of our knowledge , a recently released dataset , DailyDialog ( Li et al , 2017b ) , is the only dataset that has utterances annotated with dialogue acts and is large enough for conversation modeling methods to work .
Furthermore , in this dataset , conversations are non - task oriented , and each conversation focuses on one topic .
Each utterance is annotated with four dialogue acts as described in Table 1 .
The dataset has train , validation , and test splits of 11118 , 1000 , and 1000 conversations , respectively .
We evaluate and report our results on the DailyDialog dataset . 
In this paper , we hypothesize that dialogue acts improve conversation modeling .
However , it is not always possible that such dialogue acts are available in practice , and it would be ideal to predict dialogue acts first ( Kumar et al , 2017 ) , and then use them for next utterance generation / retrieval ; having a model where both tasks , i.e. prediction and generation , are performed simultaneously may not be ideal for validating the hypothesis .
Note that the error from the dialogue act prediction may propagate to the next utterance generation / retrieval .
Therefore , we intentionally did not use the predicted dialogue acts ( rather used the available dialogue acts ) to make sure that the insights about the usefulness of the dialogue acts are not corrupted due to the error in the upstream prediction model .
A speaker is providing information by means of a question or statement
Question A speaker intends to obtain information by asking a question Directive A speaker is requesting , accept / reject offer , or making a suggestion Comissive A speaker accept / reject a request or suggestion Table 1 : Dialogue Acts and their description available in the DailyDialog Dataset .
The DailyDialog dataset in its original form is not directly useful for the task of next utterance selection , and hence requires preparation .
The dataset has the dialogues from both the speakers .
Owing to the different conversational style of human and conversation agent , our objective is to build a model that is specific to the agent , i.e. bot .
Therefore , we need to modify the dataset in such a way that we only consider those turns where we need to predict the bot 's utterance .
To clarify further , consider the example conversation given in Table 2 .
The conversation has 8 utterances , and each utterances is marked with the speaker , i.e. human ( H ) and bot ( B ) .
Since we are only interested in building bot - specific model , we only pick those subsequences from this conversation where the last utterance is " B " .
This gives us three subsequences : 1 , 2 , 3 , 4 ; 3 , 4 , 5 , 6 ; 5 , 6 , 7 , 8 for a context of size 3 .
In each of these sub - conversations , the first three utterances constitute the context , while the last utterance is the true response .
Our training data consists of such subsequences made up of 4 utterances .
In the test data , each subsequence , in addition to these 4 utterances , has 9 more utterances selected randomly from the test pool , therefore a total of 13 utterances .
These 9 utterances along with the 4 th response ( i.
Here we list the baseline models , their modified version enhanced with dialogue act information , and the proposed model .
 ED - It is a vanilla sequence to sequence model that uses an utterance encoder to obtain a representation of first K utterances which is then used in a decoder to generate next utterance .
HRED - An extension of sequence to sequence model that uses a hierarchical encoder to obtain a representation of first K utterances , which is then used in decoder to generate next utterance . 
ED - DA - An extension of the ED model which uses dialogue act information .
It has a conditional decoder , that conditions the generation of each word on the dialogue acts representation .
HRED - DA - An extension of the HRED model which uses dialogue act information .
Similar to ED - DA , it also has a conditional decoder that conditions the generation of each word on the dialogue acts representation .
 Siamese - Also known as Dual - Encoder , it uses two encoders ( both utterance encoders ) with shared weights , to produce the representation for the K utterances and the ( K + 1 ) utterance . 
HSiamese - A Hierarchical version of the Siamese model that uses a hierarchical encoder to produce a representation for the K utterances , and a plain encoder ( utterance encoder ) to produce a representation for the ( K + 1 ) utterance .
Siamese - DA - An extension of Siamese model that uses the additional dialogue act information obtained through DA - encoder .
The representation obtained from the DA - encoder is linearly combined with the representation of the K utterances obtained from an utterance encoder .
HSiamese - DA - The proposed model uses a Hierarchical Encoder and a DA - Encoder .
The representation obtained from the DA - Encoder is linearly combined with the representation obtained from the hierarchical encoder .
In our experiments , the parameters are tuned on validation set while the results are reported on test set .
Each utterance in a mini - batch was padded to the maximum length for that batch .
The maximum batch size allowed was 32 .
The word vectors were initialized with the 300 - dimensional Glove embeddings ( Pennington et al , 2014 ) , and were also updated during training .
For the generative models , the utterance encoder , conversation encoder , DA - Encoder and decoder are all GRUs with rnn size set to 1000 ( optimized over 100 to 1200 in steps of 100 ) .
For the discriminative model , the utterance encoder , conversation encoder , and DA - Encoder are all GRUs with rnn size set to 300 ( optimized over 100 to 500 in steps of 100 ) .
Dropout of 0.1 ( optimized over 0.0 to 0.7 in steps of 0.1 ) was applied to embeddings obtained from the output of conversation encoder .
Note that , dropout was not used in the discriminative model and its variations .
Models were trained to minimize cross entropy using Adam optimizer with learning rate of 0.0003 ( optimized over 0.0001 , 0.0003 , 0.0005 , 0.0007 , 0.001 ) .
We found that a higher learning rate up - to 0.0005 helps the model to learn quickly , whereas learning rate greater than 0.0005 leads to oscillations .
In this section , we present results of our experimental study , followed by its analysis .
Since our problem formulation is retrieval based , we use standard IR metrics such as Mean Reciprocal Rank ( MRR ) and Recall@k as our evaluation metrics .
MRR is calculated as the mean of the reciprocal rank of the true candidate response among other candidate responses .
Recall@k measures whether the true candidate response appears in a ranked list of k responses . 
In this work , our hypothesis is that additional information about utterances available in the form of dialogue acts helps irrespective of the underlying model , i.e. generative or discriminative .
Results in Table 3 support our hypothesis .
These results clearly indicate that the MRR of the true candidate response improves when dialogue acts of previous utterances are provided .
From these tables we see that for all underlying models , the dialogue act version performs better than non dialogue act version .
These results furthermore indicate that hierarchical version performs better than non - hierarchical version for both generative and discriminative models .
In the generative case , the plain ED has an MRR of 0.474 , whereas the same model , when conditioned with DA - Encoder , has an MRR of 0.54 , an improvement of 13.9 % .
The hierarchical encoder - decoder HRED and HRED - DA has an MRR of 0.523 and 0.583 , respectively , an improvement of 11.4 % .
Generative models are sequence - to - sequence models and rather complex in nature , so it is interesting to note that even a much simpler discriminative model , i.e. plain Siamese model , without any dialogue act information , has an MRR of 0.8 compared to 0.58 of the best performing generative model , i.e. HRED - DA .
This observation demonstrates the strength of the discriminative models , and therefore is a motivation behind the proposed model .
The proposed model improves these baseline numbers by incorporating hierarchy and dialogue act information , and pushes the MRR to 0.848 .
While we have shown that using dialogue act information does help in the next utterance selection task , in this section , we dig deeper and understand reasons for it .
In order to do that , we analyze the dialogue act distribution of the test data and model outputs .
Although all K dialogue acts corresponding to K utterances in the context might play a role in ranking candidate utterances , the following analysis only uses the pairs of dialogue acts , i.e. dialogue acts of K th
and ( K + 1 ) th utterances .
Tables 4 ( a ) , 4 ( b ) and 4 ( c ) show the distribution of such dialogue act pairs for test data , HSiamese , and HSiamese - DA models respectively .
Here , rows indicate the dialogue act of K th utterance , whereas columns indicate the dialogue act of ( K + 1 ) th utterance .
A cell value indicates the count of utterance pairs with the respective dialogue act combinations where ( K + 1 ) th utterance was ranked 1 .
Note that in the test data , ( K + 1 ) th utterance is the true candidate response and always have the rank 1 .
For instance , there are 742 utterance pairs in the test data , where K th
and ( K + 1 ) th utterances have dialogue acts Q
and
I , respectively , however , out of those 742 instances , HSiamese ranked only 605 as 1 while HSiamese - DA ranked 638 as 1 .
From these tables , we draw following observations .
Models Learn Dominant Patterns : The first is that there are certain dominant communication patterns that we observe in both , test data and model outputs ( See Table 4 ) , suggesting that models are able to learn these patterns and retain them in their outputs .
We observe that a Question is often followed by an Information , whereas an Information can be followed by another Information or a Question .
A Directive tends to be followed by Commissive .
These communication patterns not only make sense intuitively but they are also in agreement with previous studies ( Li et al , 2017b ; Ribeiro et al , 2015 ) . 
Dialogue Acts Bring Uniformity : The second and a rather more important observation is that dialogue acts help the most for the dialogue act class ( DA - class ) when the utterances belonging to that class are non - uniform in their linguistic construct .
In order to better exlpain this , we first compute the break - up of recall@1 according to the dialogue act classes .
A DA - class of a conversation in the test data is defined based on the dialogue act of the last utterance ( K th utterance ) in the context .
These numbers are shown in the last column of Tables 4 ( b ) and 4 ( c ) for the respective models .
In Table 4 ( b ) , first row in recall@1 column is 0.65 , which indicates that out of the total number of test conversations where dialogue act of the last utterance of context was I , 65 % of true candidate responses were ranked 1 by the HSiamese model .
Such a DA - class wise breakup of the recall@1 numbers helps us do an analysis with respect to individual DA - classes .
From this break - up , it is clear that for the HSiamese model , Question DA - class has the best performance of 78 % whereas Directive has the worst performance of 63 % .
This difference can be attributed to the fact that all utterances with dialogue act as Question have rather uniform construct .
Some examples of Question utterances are , ' Q : Do you have a fever ? '
and ' Q : Why do you want to work for our company ? ' , while the examples of Directive utterances are , ' D : when we have the final results , we will call you . '
and ' D : we will take the trip .
could you give us a pamphlet ? ' .
From these examples , we observe that utterances belonging to DA - class Question have rather uniform construct in terms of linguistic features , whereas utterances belonging to DA - class Directive are ambiguous - some of the utterances of type Directive can be easily confused for Question .
This uniformity makes the learning task easier for Question class , and thereby giving us better results in the next utterance selection task , even for the model that does not use the dialogue act information .
This performance difference reduces when we provide the dialogue act information along with the textual content ( See 5 , we show the relative improvement of HSiamese - DA model over HSiamese .
From this table , we observe that there are a total of 228 conversations where the proposed model was able to improve the ranking of true candidate response to 1 .
We further observe that the biggest improvement is in I I , I Q , Q I , and D C , which make sense intuitively .
These are dominant patterns observed in the training data which should be preserved in the model output as well , however these patterns will only be preserved when model is able to capture the correct dialogue act information .
Since in many cases D and Q have similar construct , without explicit dialogue act information , a model may get confused and may learn patterns not observed in the training data .
For example , Q I and D C are the dominant and right patterns in the training data , however in the absence of explicit dialogue act information , the model may get confused between D and Q and may learn D
I and Q C instead of the dominant patterns i.e. Q I and D C.
With the explicit dialogue act information , this ambiguity is alleviated and model learns the right patterns as demonstrated by Table 5 .
Similar observations are true for other two constructs , i.e. Information and Commisive .
Both are rather similar in construct , ' I : No , thank you ' , ' I :
It does n't matter .
it happens to everyone . '
and ' C : I knew you 'd see it my way . '
, ' C : Ok , i am ready to think of other things . ' , and there is no obvious distinguishing factor .
However , providing explicit DA information helps in disambiguation , and learn the patterns that are observed in the training data such as I I , I Q.
In conversation modeling , the most basic problem is to generate a response given a context .
Several efforts have been made towards solving the problem of dialogue generation ( Vinyals and Le , 2015 ; Liu et al , 2016 ; Li et al , 2015 ) , however , due to the inherent difficulty of the problem , these efforts have only had limited success and are known to have issues like generating repetitive and generalized responses such as I do n't know or Ok . 
For the task of Next Utterance Selection , which is a relatively simpler problem than generation , though existing generative models can be easily adopted , their counterpart discriminative models have shown to have better performance .
In generative models , the most notable work is from ( Vinyals and Le , 2015 ) , however this work considers the context as a flat long string of words and ignores the hierarchical structure .
Researchers have proposed hierarchical model ( Serban et al , 2016b ) and their variations ( Serban et al , 2017b ; Serban et al , 2017a ; Li et al , 2017a ) but none of these models take into account the dialogue act information .
In Discriminative models , such as Siamese , a very notable work by ( Kannan et al , 2016 ) , smart reply , retrieves the most likely response from a set of candidate response clusters .
( Lowe et al , 2017 ) has used a retrieval based Siamese model and shown its results on the Ubuntu corpus .
Our proposed model builds upon the strengths of generative and discriminative models , and uses hierarchy along with the dialogue act information to achieve the best performance .
A recent work by ( Zhao et al , 2017 ) has used dialogue acts for the task of dialogue generation .
Our work complements their findings , and further show that dialogue acts improve the model performance across the board irrespective of underlying model ( i.e. generative or discriminative models ) and for the task of next utterance selection .
For the task of next utterance selection , we show that dialogue acts helps achieve better performance irrespective of the underlying model , be it generative or discriminative .
We also propose a novel discriminative model that leverages the hierarchical structure in a conversation and dialogue act information to produce much improved results , an MRR of 0.848 .
Our results not only show the improvement in performance , but we also present key reasons for it by doing a detailed analysis and drawing key insights that the inclusion of dialogue act information induces uniformity and removes ambiguity .
