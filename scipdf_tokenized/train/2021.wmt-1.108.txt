NoahNMT at WMT 2021 :
Dual Transfer for Very Low Resource Supervised Machine Translation
This paper describes the NoahNMT system submitted to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .
The system is a standard Transformer model equipped with our recent technique of dual transfer .
It also employs widely used techniques that are known to be helpful for neural machine translation , including iterative backtranslation , selected finetuning , and ensemble .
The final submission achieves the top BLEU for three translation directions .
In this paper , we describe the NoahNMT system submitted to one of the WMT 2021 shared tasks .
The shared task features both unsupervised machine translation and very low resource supervised machine translation .
As our core technique is mainly suitable for low resource supervised machine translation , we participated in four translation directions between Chuvash - Russian ( chv - ru ) and Upper Sorbian - German ( hsb - de ) . 
Our core technique is called dual transfer ( Zhang et al , 2021 ) , which belongs to the family of transfer learning .
It transfers from both high resource neural machine translation model and pretrained language model to improve the quality of low resource machine translation .
During the preparation for the shared task , we conducted additional experiments that supplement the original paper , including the choice of parent language , the validation of Transformer big model , and the usage of dual transfer along with iterative back - translation . 
In addition , we also applied proven techniques to strengthen the quality of our system , including selected finetuning and ensemble .
Our final submission achieves the top BLEU on the blind test sets for three translation directions : chv ru , ru chv , and hsb de .
In this section , we describe the techniques used in our system .
Interested readers are encouraged to check out the original papers for further details .
We reproduced the illustration of dual transfer from the original paper ( Zhang et al , 2021 ) , as shown in Figure 1 .
This illustration shows the case of general transfer , where the high resource translation direction is A B , and the low resource translation direction is P Q.
As discussed in the original paper , in many cases , it is possible to use shared target transfer ( B = Q ) or shared source transfer ( A = P ) .
Taking chv ru as an example , we can choose en ru as the high resource translation direction , resulting in an instance of shared target transfer .
In this shared task , when training the high resource translation model , we always initialize the shared language side with the pretrained language model BERT ( Devlin et al , 2019 ) .
Iterative back - translation ( Hoang et al , 2018 ) is an extension of back - translation ( Sennrich et al , 2016a ) .
It can exploit both sides of monolingual data of a language pair , and produces translation models for both directions , which is suitable for this shared task . 
The initial models for generating synthetic parallel data are produced by using dual transfer with low resource authentic parallel data .
In each iteration of iterative back - translation , we use the latest model to greedily decode a disjoint subset of 4 m monolingual sentences 1 to generate synthetic parallel data .
Then a new model is trained on a mixture of authentic and synthetic parallel data .
With the use of dual transfer , model training can start from [ A ] PLM emb . 
[ A ] PLM body A and B mono . 
( 1 ) 
[ P ] PLM emb . 
[ A ] PLM body P and Q mono . 
( 2 ) 
[ A ] NMT encoder emb . 
[ A ] NMT encoder body [ B ] NMT decoder emb .
 ( 3 ) 
[ P ] NMT encoder emb .
Selected finetuning aims to deal with the domain difference that may exist between the test set and the training set .
Given the source side of the test set , we try to select similar source sentences from the training set , and then finetune the translation model on the selected subset of training sentence pairs .
We use BM25 ( Robertson and Zaragoza , 2009 ) to calculate the similarity between two sentences for retrieval .
The BM25 score between a query sentence Q and a sentence D in the corpus for parent language chv ru BLEU kk 18.47 en 18.61 retrieval C is given by s ( D , Q )
=
L Q i=1 IDF ( q i )
( k + 1 ) TF ( q i , D )
k 1 − b + b L D Lavg + TF
( q i , D ) , where the query sentence Q is a sequence of L Q subwords { q i } L Q i=1 , IDF ( q i ) is the Inverse Docu - ment Frequency for q
i in the corpus C , TF ( q i , D ) is the Term Frequency for q
i in the sentence D , L D is the length of the sentence D , L avg is the average length of the corpus C , k and b are hyperparameters , which are set as 1.5 and 0.75 , respectively . 
Based on the BM25 score , we calculate the similarity between a source test sentence ( as the query sentence ) and the source sentences in the training set to obtain the top 500 sentences .
After performing the selection for all the source test sentences , we merge them and remove duplicates to obtain the set for finetuning .
3 Experimental Setup
We collected allowed data for the involved languages and followed the same preprocessing pipeline of punctuation normalization and tokenization , using scripts from Moses 2 .
The English monolingual data came from the English original side of ru - en back - translated news 3 , but its automatic translation to Russian was discarded .
The provided Chuvash - Russian dictionary was not used .
Each language was encoded with byte pair encoding ( BPE ) ( Sennrich et al , 2016b ) .
The BPE codes and vocabularies were learned on each language 's monolingual data , and then used to segment parallel data .
We used 32k merge operations for all languages .
After BPE segmentation , we discarded sentences with more than 128 subwords , and cleaned parallel data with length ratio 1.5 .
Training data statistics is provided in Table 1 .
Note that we experimented with Kazakh ( kk ) data ( Section 4.1 ) , but did not use it for our final submission .
Evaluation on test sets is given by SacreBLEU 4 ( Post , 2018 ) , after BPE removal and detokenization .
We use Transformer ( Vaswani et al , 2017 ) as our translation model , but with slight modifications that follow the implementation of BERT 5 .
The absolute position embeddings are also learned as in BERT .
The encoder and decoder embeddings are independent because each language manages its own vocabulary , but we tie the decoder input and output embeddings ( Press and Wolf , 2017 ) .
We apply dropout with probability 0.1 .
We use LazyAdam as the optimizer .
Learning rate warms up for 16 , 000 steps and then follows inverse square root decay . 
The peak learning rate is 5 × 10 −4 for parent translation models , and 1 × 10 −4 for child translation models .
Early stopping occurs when the validation BLEU does not improve for 10 checkpoints .
We set checkpoint frequency to 2 , 000 updates for parent translation models and 1 , 000 updates for child translation models .
The batch size is 6 , 144 tokens per GPU and 8 NVIDIA V100 GPUs are used .
Hyperparameters for BERT are the same as in the original paper ( Zhang et al , 2021 ) . 
For selected finetuning , we use stochastic gradient descent as the optimizer , and the learning rate is 1 × 10 −5 .
We finetune for 10 , 000 updates , and save a checkpoint every 100 updates .
The checkpoint with the highest validation BLEU is kept .
In our preliminary experiments , we found it beneficial to use a closely related language as the parent language .
It is clear that there are several factors that should be taken into account , such as the degree of closeness , and the amount of resource for training the parent model .
For Upper Sorbian , Czech ( cs ) is closely related to it , and Czech - German has a good amount of parallel data , so we directly choose Czech as the parent language . 
Chuvash , however , is a rather isolated language in the Turkic family .
The closest language with usable data is Kazakh ( kk ) , but the amount of parallel data for Kazakh - Russian is relatively small , and we found it to be quite noisy .
Therefore , we considered using English ( en ) as the parent language of Chuvash .
Even though English is unrelated to Chuvash and they use different scripts , English - Russian has more parallel data that can guarantee the quality of the parent model .
We conducted an experiment with Transformer base .
Results in Table 2 indicate that English can serve as an eligible parent for Chuvash .
Considering that we plan to use Transformer big for which data amount is likely to play a more important role , we decided to use English as the parent language for Chuvash .
The original paper ( Zhang et al , 2021 ) evaluated dual transfer only with Transformer base .
In this shared task , we scale up to Transformer big .
We also face a more realistic setting where the monolingual data for the low resource languages ( chv and hsb ) are quite scarce .
Therefore it is worth testing the effect of scaling up .
Results in Table 3 show that Transformer big brings consistent improvements .
We also report the runtime of each step in dual transfer for NMT chv ru with Transformer big in Table 4 for reference , but the numbers can vary depending on implementation and data size .
In the following experiments and our final submission , we use Transformer big models .
We ran five iterations of iterative back - translation .
Results are shown in Table 5 .
The best BLEU scores are attained with two or three iterations .
Another observation is that iterative back - translation brings larger improvements for chv ru and hsb de than ru chv and de hsb .
This is probably because the monolingual data for chv and hsb are small in quantity .
We only use selected finetuning for the chv - ru pair because parallel data for hsb - de is scarce .
In order to test the effect of selected finetuning , we start from the models of Iteration 2 in Table 5 .
Results in Table 6 indicate that selected finetuning gives modest improvements .
We validate the effectiveness of ensemble on hsb de and de hsb , by performing ensemble decoding from the five models from iterative back - translation .
Results in Table 7 demonstrate that ensemble gives BLEU improvements of about 0.8 .
For chv ru and ru chv , we perform selected finetuning starting from the best models from iterative back - translation ( Iteration 2 for chv ru , Iteration 3 for ru chv ) .
Note that the selected training subsets are different from those in Section 4.4 because the selection is based on the source side of the blind test sets .
We finetune five times with different random seeds for model ensemble .
For hsb de and de hsb , we ensemble the five models from iterative back - translation .
In this paper , we describe a series of experiments that contribute to our submission to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .
These experiments , as well as the good results of the final submission , show that dual transfer can work in synergy with several widely used techniques in realistic scenarios .
