Building a De - identification System for Real Swedish Clinical Text Using Pseudonymised Clinical Text
This article presents experiments with pseudonymised Swedish clinical text used as training data to de - identify real clinical text with the future aim to transfer non - sensitive training data to other hospitals . Conditional Random Fields ( CFR ) and Long Short - Term Memory ( LSTM ) machine learning algorithms were used to train deidentification models . The two models were trained on pseudonymised data and evaluated on real data . For benchmarking , models were also trained on real data , and evaluated on real data as well as trained on pseudonymised data and evaluated on pseudonymised data . CRF showed better performance for some PHI information like Date Part , First Name and Last Name ; consistent with some reports in the literature . In contrast , poor performances on Location and Health Care Unit information were noted , partially due to the constrained vocabulary in the pseudonymised training data . It is concluded that it is possible to train transferable models based on pseudonymised Swedish clinical data , but even small narrative and distributional variation could negatively impact performance .
Electronic health records ( EHR ) are produced in a steady stream , with the potential of advancing future medical care . Research on EHR data holds the potential to improve our understanding of patient care , care processes , and disease characteristics and progression . However , much of the data ⇤ Hercules Dalianis is also guest professor at the Norwegian Centre for E - health Research is sensitive , containing Protected Health Information ( PHI ) such as personal names , addresses , phone numbers , that can identify particular individuals and thus can not be available to the public for general scientific inquiry . Although good progress has been made in the general sub - field of de - identifying clinical text , the problem is still not fully resolved ( Meystre et al , 2010 ; Yogarajan et al , 2018 ) . This study examines the use of pseudonymised health records as training data for de - identification tasks . Several ethical and scientific issues arise regarding the balance between maintaining patient confidentiality and the need for wider application of trained models . How will a de - identification system be constructed and used in a cross hospital setting without risking the privacy of patients ? Is it possible to obscuring the training data by pseudonymising it and then use it for the training of a machine learning system ? De - identification and pseudonymisation are two related concepts . In this paper de - identification is used as a more general term to describe the process of finding personal health information to be able to conceal identifying information . A pseudonymised text is a text where the personal health information has been identified either manually or automatically and then replaced with realistic surrogates . The research question in this study is whether it is possible to use de - identified and pseudonymised clinical text in Swedish as training data for deidentifying real clinical text , and hence make it possible to transfer the system cross hospital . We highlight whether learning from the exist - ing , non - sensitive , pseudonymised Swedish clinical text can be useful in a new and different context ; considering the normal variations in the distribution and nature of PHI information , and potential effects of scrubbing ( Berman , 2003 ) , that is , removing and modifying PHIs that was carried out to patient records during the de - identification process .
The identification of PHI is a type of named entity recognition task where sensitive named entities specifically are identified . The first study with CRF - based de - identification for Swedish was on the gold standard Stockholm EPR PHI Corpus . The distribution of PHIs is shown in Table 1 . In this instance , manual annotation with expert consensus was used to create the gold standard ( Dalianis and Velupillai , 2010 ) . De - identification tasks based on the CRF machine learning algorithm has been carried out on this data set previously with precision scores ranging between 85 % and 95 % , recalls ranging between 71 % and 87 % and F1 - scores between 0.76 and 0.91 ( Dalianis and Velupillai , 2010 ; Berg and Dalianis , 2019 ) . One approach previously used for concealing the training set 's sensitive data was carried out by , using the Stockholm EPR PHI Corpus . In the study , the textual part of the data were used to create 14 different features and part of speech tags . The textual part was then removed , and only the features and part of speech tags were used for training a Random Forest model . Fairly high precision of 89.1 % was obtained , but with a recall of 54.3 % and F1 - score of 64.8 . In contrast to using only the sensitive EHR data for training , McMurry et al ( 2013 ) integrated both publicly available scientific , medical publications and private sensitive clinical notes to develop a de - identification system . While considering the term frequencies and part of speech tags between the two data sources , they used both rule lists and decision trees for their system . This was an interesting approach since it raised the prospect of using non - sensitive data in building useful deidentification models . However , it is not clear whether medical journals have significant advantages over any other public text , like news corpora , for detecting PHI . A study similar to Mc - Murry et al ( 2013 ) , by Berg and Dalianis ( 2019 ) , showed few benefits of combining non - medical public text and sensitive clinical notes to build a de - identification system for medical records . More recently , deep learning approaches using recurrent neural networks seem to yield significant improvements over traditional rules - based methods or statistical machine learning ( Dernoncourt et al , 2017 ) . Still , recent studies indicate that combining several approaches will yield the best results . For instance , the best system in a recent de - identification shared task was a combination of bidirectional LSTM , CRF and a rule - based subsystem ( Liu et al , 2017 ) . Significant domain variation , such as a different language , is an important factor that was not considered in the discussed shared task . Domain differences were cited as the reason for poor performance on psychiatric notes de - identification ( Stubbs et al , 2017 ) , compared with the previous de - identification task on general clinical narratives ( Stubbs et al , 2015 ) . Within the same language and similar clinical settings , the change of domain is likely not substantial . While in future research it may be worth considering domain adaption techniques to work towards a system meant to be used between hospitals , they were not considered in this study , beyond the use of non - sensitive dictionaries for names and location .
In this study , machine learning approaches are used since the best de - identification systems appear to be machine learning - based ( Kushida et al , 2012 ) . While rule - based methods such as using dictionaries and pattern - matching were previously more prevalent than machine learning methods for solving text - based de - identification problems ( Meystre et al , 2010 ) , today it is more typical to have both approaches used , since rule - based methods still yield better results for some PHI information ( Neamatullah et al , 2008b ) . Dictionaries and patterns were therefore used as features within one of the models .
Two different data sets for de - identification were used : Stockholm EPR PHI Psuedo Corpus ( Pseudo ) as well as the Stockholm EPR PHI Cor - The Stockholm EPR PHI Pseudo Corpus was produced from the Stockholm EPR PHI Corpus by automatically pseudonymising all PHIs . This process is described by Dalianis ( 2019 ) . The Stockholm EPR PHI Corpus is described by Dalianis and Velupillai ( 2010 ) . An example is shown in Figure 1 ( Dalianis et al , 2015 ) . The number of entities and types of entities in both the Stockholm EPR PHI Psuedo Corpus and the Stockholm EPR PHI Corpus is shown in Table 2 . From Table 2 , it can be observed that the distribution of PHI instances between the two data sets is somewhat similar , but there is a significant difference when it comes to unique instances between the two data sets . In total , the Real data set contains proportionally more unique instances than the Pseudo data set . The entities in the Real data set al o tend to have more tokens .
Using the de - identified and pseudonymised data set , two models were trained based on two machine learning algorithms ; CRF and the deep learning algorithm LSTM . The two algorithms were chosen since both have been shown to produce state of the art performance , and applying the two on Swedish clinical data sets makes for an informative comparison . The two models were evaluated on both the real data set that is annotated for PHI , but not pseudonymised , ' Pseudo - Real ' , as well as on the pseudonymised data set , ' Pseudo - Pseudo ' . For additional comparison basis models trained on the real data set were evaluated on test sets from the same data set , ' Real - Real ' .
In this study , the CRF algorithm implemented in CRFSuite ( Okazaki , 2007 ) is used with the sklearn - crfsuite wrapper 2 and the LSTM architecture described by Lample et al ( 2016 ) , based on an open - source implementation with Tensorflow 3 is used . The linear - chain Conditional Random Fields model , implemented with sklearn - CRFSuite 4 , Figure 1 : Example of a pseudonymised record . The original Swedish pseudonymised record is to the right and the translated version is to the left . The underlined words are the surrogates , where real data has been replaced with pseudonyms . uses lexical , orthographic , syntactic and dictionary features . The CRF is based on trial - and - error experiments with feature sets described by Berg and Dalianis ( 2019 ) , and uses the same features except for section features .
The long short - term memory ( LSTM ) needs word embeddings as features for the training . Word2vec 5 was used to produce word embeddings using shallow neural networks , based on two corpora ; a clinical corpus and medical journals . For the training using real clinical data , word embeddings were produced using a clinical corpus of 200 million tokens that produced 300 , 824 vectors with a dimension of 300 . For the training with pseudo clinical data , word embeddings were produced using Läkartidningen corpus ( The Swedish scientific medical journals from 1996 to 2005 ) containing 21 million tokens that produced 118 , 662 vectors with a dimension of 300 . The reason for using Läkartidningen is that the corpus does not contain sensitive data and hence is also more easily usable for transferable cross hospital training .
The results of the experimental work are summarised in Figure 2 . As can be observed in the figure , the CRF algorithm seems to generally outperform the LSTM algorithm on all metrics ; precision , recall and F1 measure . This result is not consistent with repeated reports in the literature , where deep learning apsklearn - crfsuite.readthedocs.io/en/ latest/ 5 word2vec , https://github.com/tmikolov/ word2vec proaches such as LSTM have been shown to out - perform most other methods , including CRF . Since deep learning approaches normally require very large amounts of data , one explanation for this result could be that the word embeddings used in this study did not contain sufficient context variations required for more robust performance or an insufficient training set of annotated data . The ability to identify date part and age entities are similar when training on pseudonymised data and real data for the CRF . In contrast , Location , Health Care Unit and Full Date were negatively affected when using pseudonymised training data regardless of using a CRF or LSTM model .
Experimental results of the CRF algorithm are shown in Table 3 . Not presented in the table is the combination of training on real data and evaluation of pseudo data ( Real - Pseudo ) , but the results of this combination gave a precision of 86.37 and recall of 77.80 % and an F1 - score of 81.86 .
The experimental results of the LSTM algorithm are shown in Table 4 and again , not presented in the table is the combination of training on real data and evaluation of pseudo data ( Real - Pseudo ) . The result of this combination is a precision of 65.83 % and recall of 74.79 % and F1 - score of 70.03 .
The training set used in this study has a substantially constrained vocabulary compared to the evaluation set , which may partially explain the overall performance achieved when evaluating on real data ( Pseudo - Real version of the data has less PHI tokens and the entities are more often single tokens . The Full Date structure yyyyddmm - yyyyddmm is commonly occurring in the pseudo data , and the dash between the dates , " - " , is often incorrectly identified . For example , using the CRF algorithm on real - data training and pseudo - data testing ( Real - Pseudo ) , of the 159 instances not identified as full dates tokens , sixty contain ' - ' . The pseudo data uses the structure yyyyddmm while the real data uses yyddmm , which leads to errors . For these kinds of errors on standard data formats such as dates , it is easy to see how rule - based approaches using regular expressions could significantly improve the overall performance of the system . The weakest performance area was for location information . There is a large variety of locations in the pseudo - data . These are also fairly specific and unlikely to occur in the real data , for example , locations with very few inhabitants . These uncommon rural places have names similar to residential homes ( äldreboenden ) . There are multiple instances of the suffix ' gården ' ( yard ) in the location pseudo - PHI , whereas , in the real data , the same suffix is common for care units . In the pseudo - data , the care units are more general than in the real data , often too general to be annotated in the real data set . Infirmaries are fairly common in the real data but non - existent in the pseudo data . This lack of variation in the pseudo is partially responsible for the drop in performance . There are at least two ways to think about mitigating this poor performance . First , location and care unit could be combined as one entity type since they are conceptually very similar , and sometimes have interchangeable entity names . Secondly , using more detailed municipality street and location mapping databases as dictionaries could be considered .
There is one similar study to ours but for English by Yeniterzi et al ( 2010 ) , where the authors train their de - identification system with all combinations of pseudonymised textual data ( or what they call resynthesized records ) and real data and their results are in line with ours . However , there are some studies on cross - domain adaptation . In cross - domain adaption there is , however , a substantial domain change between the training and testing data , unlike in this study . Martinez et al ( 2014 ) used models trained in one hospital on pathology reports in another hospital . Their system only required minor feature normalisation , and the reported results were comparable across the hospitals . Although this demonstrates feasibility , it is important to note that the pathology reports were from the same medical sub - speciality with only some narrative differences . In this study , in addition to narrative differences between the training data and the target evaluation data , the number of care units and locations involved , as well as personal names , are widely varied . With large amounts of out of vocabulary variation , training on limited data will likely yield poor results . In practice , these data types exist in other non - sensitive sources such as city and rural location and street mapping data . Except for location and care unit , evaluation on pseudo - data ( Pseudo - Pseudo ) produced better outcomes compared to performance on real - data ( Pseudo - Real ) , which can be expected . What was a bit unexpected was the lower performance of the LSTM algorithm . The algorithm 's results would potentially have been improved by larger vector data or more labelled data ( Dernoncourt et al , 2017 ) . While clinical notes have unique linguistic structures and grammatical peculiarities , nonclinical data sources could still provide important contextual information for constructing a useful vector space . Additional sources using nonsensitive data , such as public corpora in the general domain , hold a potential to improve performance on the de - identification task , therefore this line of inquiry will be followed up on in future work . In the same vein , factoring in part of speech tags from other sources of clinical data could be useful in this case . For instance , there are deidentification databases of clinical text , such as MIMIC ( Neamatullah et al , 2008a ; Goldberger et al , 2000 ) , which could be used as additional information for training purposes , and using only the part of speech tags reduces security risks . Current results are calculated as exact matches , and the partial match is not factored in , which may affect the result . As mentioned in the analysis the CRF algorithm rarely classifies the ' - ' in between dates as a part of the dates , and these are therefore not counted as matches despite the most identifying parts of the entity being identified . To improve the general performance , a combination of both the LSTM and CRF algorithms could be performed instead of testing them independently . Combining high - performance algorithms and the use of ensemble methods seem to produce the best results as reported in the literature ( Dernoncourt et al , 2017 ; Liu et al , 2017 ) , and these techniques will be investigated in future work on the data sets .
The results of this study suggest that although it is possible to train models on pseudonymised data for use in different contexts , there is severe deterioration in performance for some PHI information . Even small narrative and distributional variation could negatively impact performance . Transferring a system from one set of clinical text to a different set could result in the performance of the system deteriorating ; in this study the Pseudo - Real case . This problem , what we call The cross pseudo - real text adaptation problem , is an issue that could happen due to the pseudonymisation / de - identification processes on the training data due to the narrative and distributional variation as well as other differences in the nature of the PHI between the training data and the target . In the future , we will try to improve the pseudonymisation module described in Dalianis ( 2019 ) to produce a larger variation in the vocabulary as the lack of variation may affect the current result negatively . We will also apply the learned models to other Nordic languages such as Norwegian clinical text and use the system as a pre - annotation system to assist the manual annotators in their work to create a Norwegian gold standard .
We are grateful to the DataLEASH project and Helse Nord for funding this research work .
