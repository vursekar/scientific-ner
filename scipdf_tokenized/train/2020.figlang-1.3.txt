A Report on the 2020 VUA and TOEFL Metaphor Detection Shared Task
In this paper , we report on the shared task on metaphor identification on VU Amsterdam Metaphor Corpus and on a subset of the TOEFL Native Language Identification Corpus .
The shared task was conducted as apart of the ACL 2020 Workshop on Processing Figurative Language .
Metaphor use in everyday language is a way to relate our physical and familiar social experiences to a multitude of other subjects and contexts ( Lakoff and Johnson , 2008 ) ; it is a fundamental way to structure our understanding of the world even without our conscious realization of its presence as we speak and write .
It highlights the unknown using the known , explains the complex using the simple , and helps us to emphasize the relevant aspects of meaning resulting in effective communication . 
Metaphor has been studied in the context of political communication , marketing , mental health , teaching , assessment of English proficiency , among others Gutierrez et al , 2017 ;
Littlemore et al , 2013 ; Thibodeau and Boroditsky , 2011 ; Kaviani and Hamedi , 2011 ; Kathpalia and Carmel , 2011 ; Landau et al , 2009 ; Beigman Klebanov et al , 2008 ; Zaltman and Zaltman , 2008 ; Littlemore and Low , 2006 ; Cameron , 2003 ; Lakoff , 2010 ; Billow et al , 1997 ; Bosman , 1987 ) ; see chapter 7 in Veale et al ( 2016 ) for a recent review . 
We report on the second shared task on automatic metaphor detection , following up on the first shared task held in 2018 .
We present the shared task and provide a brief description of each of the participating systems , a comparative evaluation of the systems , and our observations about trends in designs and performance of the systems that participated in the shared task .
Over the last decade , automated detection of metaphor has become an popular topic , which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied .
In terms of methods , approaches based on feature - engineering in a supervised machine learning paradigm explored features based on concreteness and imageability , semantic classification using WordNet , FrameNet , VerbNet , SUMO ontology , property norms , and distributional semantic models , syntactic dependency patterns , sensorial and vision - based features Köper and i m Walde , 2017 ; Tekiroglu et al , 2015 ; Tsvetkov et al , 2014 ; Beigman Klebanov
et al , 2014 ; Dunn , 2013 ; Neuman et al , 2013 ; Mohler et al , 2013 ; Hovy et al , 2013 ; Tsvetkov et al , 2013 ; Turney et al , 2011 ; Shutova et al , 2010 ; Gedigian et al , 2006 ) ; see and Veale et al ( 2016 ) for reviews of supervised as well as semi - supervised and unsupervised approaches .
Recently , deep learning methods have been explored for token - level metaphor detection ( Mao et al , 2019 ;
Dankers et al , 2019 ; Gao et al , 2018 ; Wu et al , 2018 ; Rei et al , 2017 ; Gutierrez et al , 2017 ; Do Dinh and Gurevych , 2016 ) . 
In terms of data , researchers used specially constructed or selected sets , such as adjective noun pairs Tsvetkov et al , 2014 ) , WordNet synsets and glosses ( Mohammad et al , 2016 ) , annotated lexical items ( from a range of word classes ) in sentences sampled from corpora ( Özbal et al , 2016 ; Jang et al , 2015 ;
Hovy et al , 2013 ; Birke and Sarkar , 2006 ) , all the way to annotation of all words in running text for metaphoricity Steen et al , 2010 ) ; Veale et al ( 2016 ) review various annotated datasets . 
The goal of this shared task is to detect , at the word level , all content word metaphors in a given text .
We are using two datasets - VUA and TOEFL , to be described shortly .
There are two tracks for each dataset , for a total of four tracks : VUA All POS , VUA Verbs , TOEFL All POS , and TOEFL Verbs .
The AllPOS track is concerned with the detection of all content words , i.e. , nouns , verbs , adverbs and adjectives that are labeled as metaphorical while the Verbs track is concerned only with verbs that are metaphorical .
We excluded all forms of be , do , and have for both tracks .
For each dataset , each participating individual or team can elect to compete in the All POS track , Verbs track , or both .
The competition is organized into two phases : training and testing .
We use the VU Amsterdam Metaphor Corpus ( VUA ) ( Steen et al , 2010 ) .
The dataset consists of 117 fragments sampled across four genres from the British National Corpus : Academic , News , Conversation , and Fiction .
The data is annotated using the MIPVU procedure with a strong interannotator reliability of κ > 0.8 ( Steen et al , 2010 ) .
The VUA dataset and annotations is the same as the one used in the first shared task on metaphor detection , where the reader is referred for further details .
This data labeled for metaphor was sampled from the publicly available ETS Corpus of Non - Native Written English 1 and was first introduced by ( Beigman .
The annotated data comprises essay responses to eight persuasive / argumentative prompts , for three native languages of the writer ( Japanese , Italian , Arabic ) , and for two proficiency levels - medium and high .
The data was annotated using the protocol in Beigman Klebanov and Flor ( 2013 ) , that emphasized argumentation - relevant metaphors : " Argumentation - relevant metaphors are , briefly , those that help the author advance her argument .
For example , if you are arguing against some action because it would drain resources , drain 1 https://catalog.ldc.upenn.edu/LDC2014T06 is a metaphor that helps you advance your argument , because it presents the expenditure in a very negative way , suggesting that resources would disappear very quickly and without control . "
Beigman Klebanov and Flor ( 2013 ) Average inter - annotator agreement was κ = 0.56 - 0.62 , for multiple passes of the annotation ( see for more details ) .
We use the data partition from Beigman , with 180 essays as training data and 60 essays as testing data . 
Tables 1 and 2 show some descriptive characteristics of the data : the number of texts , sentences , tokens , and class distribution information for Verbs and AllPOS tracks for the two datasets .
To facilitate the use of the datasets and evaluation scripts beyond this shared task in future research , the complete set of task instructions and scripts are published on Github 2 .
We also provide a set of features used to construct one of the baseline classification models for prediction of metaphor / non - metaphor classes at the word level , and instructions on how to replicate that baseline .
In this first phase , data is released for training and/or development of metaphor detection models .
Participants can elect to perform crossvalidation on the training data , or partition the training data further to have a held - out set for preliminary evaluations , and/or set apart a subset of the data for development / tuning of hyperparameters .
However the training data is used , the goal is to have N final systems ( or versions of a system ) ready for evaluation when the test data is released .
In this phase , instances for evaluation are released .
3 Each participating system generated predictions for the test instances , for up to N models .
4 Predictions are submitted to CodaLab 5 and evaluated automatically against the goldstandard labels .
Submissions were anonymized .
The only statistics displayed were the highest score of all systems per day .
The total allowable number of system submissions per day was limited to 5 per team per track .
The metric used for evaluation is the F1 score ( least frequent class / label , which is " metaphor " ) with Precision and Recall also available via the detailed results link in CodaLab . 
The shared task started on January 12 , 2020 when the training data was made available to registered participants .
On February 14 , 2020 , the testing data was released .
Submissions were accepted until April 17 , 2020 .
Table 3 shows the submission statistics for systems with a system paper .
Generally , there were more participants in the VUA tracks than in TOEFL tracks , and in All POS tracks than in Verbs tracks .
In total , 13 system papers were submitted describing methods for generating metaphor / non - metaphor predictions .
We first describe the baseline systems .
Next , we briefly describe the general approach taken by every team .
Interested readers can refer to the teams ' papers for more details .
We make available to shared task participants a number of features from prior published work on metaphor detection , including unigram features , features based on WordNet , VerbNet , and those derived from a distributional semantic model , POS - based , concreteness and difference in concreteness , as well as topic models .
We adopted three informed baselines from prior work .
As Baseline 1 : UL + WordNet + CCDB , we use the best system from Beigman .
The features are : lemmatized unigrams , generalized WordNet semantic classes , and difference in concreteness ratings between verbs / adjectives and nouns ( UL + WN + CCDB ) .
6 Baseline 2 : bot.zen is one of the top - ranked systems in the first metaphor shared task in 2018 by Stemle and Onysko ( 2018 ) that uses a bi - directional recursive neural network architecture with long - term short - term memory ( LSTM BiRNN ) and implements a flat sequenceto - sequence neural network with one hidden layer using TensorFlow and Keras in Python .
The system uses fastText word embeddings from different corpora , including learner corpus and BNC data .
Finally , Baseline 3 : BERT is constructed by finetuning the BERT model ( Devlin et al , 2018 ) in a standard token classification task : After obtaining the contextualized embeddings of a sentence , we apply a linear layer followed by softmax on each token to predict whether it is metaphorical or not .
gives more details about the architecture of this baseline .
For Verbs tracks , we tune the system on All POS data and test on Verbs , as this produced better results during preliminary experimentation than training on Verbs only .
Zenith :
Character embeddings + Similarity Networks + Bi - LSTM + Transformer Kumar and Sharma ( 2020 ) added lexical and orthographic information via character embeddings in addition to GloVe and ELMo embeddings for an enriched input representation .
The authors also constructed a similarity metric between the literal and contextual representations of a word as another input component .
A Bi - LSTM network and Transformer network are trained independently and combined in an ensemble .
Eventually , adding both character - based information and similarity network are the most helpful , as evidenced by results obtained using cross - validation on the training datasets . 
rowanhm : Static and contextual embeddings + concreteness + Multi - layer Perceptron Maudslay et
al ( 2020 ) created a system that combines the concreteness of a word , its static embedding and its contextual embedding before providing them as inputs into a deep Multi - layer Perceptron network which predicts word metaphoricity .
Specifically , the concreteness value of a word is formulated as a linear interpolation between two reference vectors ( concrete and abstract ) which were randomly initialized and learned from data . iiegn : LSTM BiRNN + metadata ; combine TOEFL and VUA data Stemle and Onysko ( 2020 ) used an LSTM BiRNN classifier to study the relationship between the metadata in the TOEFL corpus ( proficiency , L1 of the author , and the prompt to which the essay is responding ) and classifier performance .
The system is an extension of the authors ' system for the 2018 shared task ( Stemle and Onysko , 2018 ) that served as one of the baseline in the current shared task ( see section 4.1 ) .
Analyzing the training data , the authors observed that essays written by more proficient users had significantly more metaphors , and that essays responding to some of the prompts had significantly more metaphors than other prompts ; however , using proficiency and prompt metadata explicitly in the classifier did not improve performance .
The authors also experimented with combining VUA and TOEFL data . 
Duke Data Science : BERT , XNET language models + POS tags as features for a Bi - LSTM classifier Liu et al ( 2020 ) use pre - trained BERT and XLNet language models to create contextualized embeddings , which are combined with POS tags to generate features for a Bi - LSTM for token - level metaphor classification .
For the testing phase , the authros used an ensemble strategy , training four copies of the Bi - LSTM with different initializations and averaging their predictions .
To increase the likelihood of prediction of a metaphor label , a token is declared a metaphor if : ( 1 ) its predicted probability is higher than the threshold , or ( 2 ) if its probability is three orders of magnitude higher than the median predicted probability for that word in the evaluation set .
chasingkangaroos : RNN + BiLSTM + Attention + Ensemble Brooks and Youssef ( 2020 ) use an ensemble of RNN models with Bi - LSTMs and bidirectional attention mechanisms .
Each word was represented by an 11 - gram and appeared at the center of the 11 - gram ; each word in the 11 - gram was represented by a 1 , 324 dimensional word embedding ( concatenation of ELMo and GloVe embeddings ) .
The authors experimented with ensembles of models that implement somewhat different architecture ( in terms of attention ) and models trained on all POS and on a specific POS .
baseline system ( also one of the shared task baselines , see section 4.1 ) uses BERT - after obtaining the contextualized embeddings of a sentence , a linear layer is applied followed by softmax on each token to predict whether it is metaphorical or not .
The authors spell - correct the TOEFL data , which improves performance .
present two multi - task settings :
In the first , metaphor detection on out - of - domain data is treated as an auxiliary task ; in the second , idiom detection on in - domain data is the auxiliary task .
Performance on TOEFL is helped by the first multi - task setting ; performance on VUA is helped by the second .
UoB team : Bi - LSTM + GloVe embeddings + concreteness Alnafesah et al ( 2020 ) explore ways of using concreteness information in a neural metaphor detection context .
GloVe embeddings are used as features to an SVM classifier to learn concreteness values , training it using human labels of concreteness .
Then , for metaphor detection , every input word is represented as a 304 - dimensional vector - 300 dimensions are GloVe pre - trained embeddings , plus probabilities for the four concreteness classes .
These representations of words are given as input to a Bi - LSTM which outputs a sequence of labels .
Results suggest that explicit concreteness information helps improve metaphor detection , relative to a baseline that uses GloVe embeddings only . 
zhengchang :
ALBERT + BiLSTM Li et al ( 2020 ) use a sequence labeling model based on ALBERT - LSTM - Softmax .
Embeddings produced by BERT serve as input to BiLSTM , as well as to the final softmax layer .
The authors report on experiments with inputs to BERT ( single - sentence vs pairs ; variants using BERT tokenization ) , spellcorrection of the TOEFL data , and CRF vs softmax at the classification layer . 
PolyU - LLT : Sensorimotor and embodiment features + embeddings + n - grams + logistic regression classifier Wan et al ( 2020 ) use sensorimotor and embodiment features .
They use the Lancaster Sensorimotor norms ( Lynott et al , 2019 ) that include measures of sensorimotor strength for about 40 K English words across six perceptual modalities ( e.g. , touch , hearing , smell ) , and five action effectors ( mouth / throat , hand / arm , etc ) , and embodiment norms from Sidhu et al ( 2014 ) .
The authors also use word , lemma , and POS n - grams ; word2vec and GloVe word embeddings , as well as cosine distance measurements using the embeddings .
The different features are combined using logistic regression and other classifiers .
Table 4 present the results for All POS and Verbs tracks for VUA data .
Table 5 present the results for All POS and Verbs tracks for TOEFL data .
The clearest trend in the 2020 submissions is the use of deep learning architectures based on BERT ( Devlin et al , 2018 ) - more than half of the participating systems used BERT or its variant .
The usefulness of BERT for metaphor detection has been shown by Mao et al ( 2019 ) , where a BERT - based system posted F1 = 0.717 on VUA AllPOS , hence our use of a BERT - based system as Baseline 3 . 
Beyond explorations of neural architectures , we also observe usage of new lexical , grammatical , and morphological information , such as finegrained POS , spell - corrected variants of words ( for TOEFL data ) , sub - word level information ( e.g. , character embeddings ) , idioms , sensorimotor and embodiment - related information .
Since the same VUA dataset was used in 2020 shared task as in the 2018 shared task , we can directly compare the performance of the best systems to observe the extent of the improvement .
The best system in 2018 performed at F1 = 0.651 ; the best performance in 2020 is more than 10 points better - F1 = 0.769 .
Indeed , the 2018 best performing system would have earned the rank of 11 in the 2020 All POS track , suggesting that the field has generally moved to more effective models than those proposed for the 2018 competitions . 
The best results posted for the 2020 shared task are the new state - of - the - art for both VUA 7 and TOEFL corpora .
7 While a number of recent systems were evaluted on VUA data ( Le et al , 2020 ; Dankers et al , 2019 ; Mao et al , 2019 ; Gao et al , 2018 ) , their results are not directly comparable to the shared task , since they evaluated on all parts of speech , including function words .
See Dankers et al ( 2020 ) for a discussion .
Table 6 shows performance by genre for the VUA data All POS track .
The patterns are highly consistent across systems , and replicate those observed for the 2018 shared task - Academic and News genres are substantially easier to handle than Fiction and Conversation .
The gap between the best and worst performance across genres for the same system remains wide - between 11.4 F1 points and 24.3 F1 points .
Somewhat encouragingly , the gap is narrower for the better performing systems - the top 6 systems show the smallest gaps between best and worst genres ( 11.4 - 14.0 ) .
Table 7 shows performance and ranks of the best systems for teams that participated in both VUA and TOEFL AllPOS tracks , along with baselines .
Overall , the relative performance rankings are consistent - F1 scores are correlated at r = .92 and team ranks are correlated at r = 0.95 across the two datasets .
All teams posted better performance on the VUA data than on the TOEFL data ; the difference ( see column 4 in Table 6 : VUA Dataset : Performance ( F1 - score ) of the best systems submitted to All - POS track by genre subsets of the test data .
In parentheses , we show the rank of the given genre within all genres for the system .
The last column shows the overall drop in performance from best genre ( ranked 1 ) to worst ( ranked 4 ) .
The top three performances for a given genre are boldfaced .
F1 point ( zhengchang ) to 5 F1 points ( DeepMet , umd bilstm , Zenith ) .
The BERT baseline posted a relatively large difference of 9 F1 points ; this could be because BNC data is more similar to the data on which BERT has been pre - trained than TOEFL data .
We note , however , that participating systems that used BERT showed a smaller performance gap between VUA and TOEFL data ; in zhengchang the gap is all but eliminated .
This suggests that a BERT - based system with parameters optimized for performance on TOEFL data can close this gap . 
Considering TOEFL data as an additional genre , along with the four genres represented in VUA , we observe that it is generally harder than Academic and News , and is commensurate with Fiction in terms of performance , for the three systems with best VUA All POS performance ( Deep - Met : 0.72 both , Go Figure ! : 0.69 both , illiniMet : 0.69 for VUA Fiction , .70 for TOEFL ) ; a caveat to this observation is that the difference between VUA and TOEFL is not only in genre but in the metaphor annotation guidelines as well .
Table 8 shows performance for All POS track on the TOEFL data by the writer 's proficiency level - high or medium .
We note that the quality of the human annotations does not appear to differ substantially by proficiency :
The average inter - annotator agreement for the high proficiency essays was κ = 0.619 , while it was κ = 0.613 for the medium proficiency essays .
We observe that generally systems tend to perform better on the higher proficiency essays , although two of the 12 systems posted better performance on the medium proficiency data .
However , even though the medium proficiency essays might have deficiencies in grammar , spelling , coherence and other properties of the essay that could interfere with metaphor detection , we generally observe relatively small differences in performance by proficiency - up to 3.5 F1 points , with a few ex - ceptions ( zhengchang , Go Figure ! ) .
Interestingly , automatic correction of spelling errors does not seem to guarantee a smaller gap in performance ( see , Go Figure ! ) .
In parentheses , we show the rank of the given proficiency level within all levels for the system .
The last column shows the overall drop in performance from best proficiency level ( ranked 1 ) to worst ( ranked 4 ) .
The top three performances for a given genre are boldfaced .
Table 9 shows the performance of the systems submitted to the All POS tracks for VUA and TOEFL data broken down by part of speech ( Verbs , Nouns , Adjectives , Adverbs ) .
As can be observed both from the All POS vs Verbs tracks ( Tables 4 and 5 ) and from Table 9 , performance on Verbs is generally better than on All POS .
8
For VUA data , all but one systems perform best on Verbs , followed by Adjectives and Nouns , with the worst performance generally observed for Adverbs .
These results replicate the findings from the 2018 shared task and follow the proportions of metaphors in the respective parts of speech , led by Verbs ( 30 % ) , Adjectives ( 18 % ) , Nouns ( 13 % ) , Adverbs ( 8 % ) .
The average gap between best and worst POS performance has also stayed similar - 11 F1 points ( it was 9 % in 2018 ) . 
For the TOEFL data , the situation is quite different .
Adjectives lead the scoreboard for all but 3 systems , with Adverbs and Verbs coming next , while Nouns proved to be the most challenging category for all participating systems .
Furthermore , the gap between best and worst POS performance is large - 17 F1 points on average , ranging between 11 and 22 points .
The best performance on Nouns is only F1 = 0.641 ; it would have ranked 10th out of 12 on Adjectives .
The proportions of metaphorically used Verbs ( 13 % ) , Adjectives ( 8 % ) , Nouns ( 4 % ) , and Adverbs ( 3 % ) ( based on training data ) perhaps offer some explanation of the difficulty with nouns , since nominal metaphors seem to be quite rare .
Stemle and Onysko ( 2020 ) observed that metaphors occur more frequently in responses to some essay prompts that to others among the 8 prompts covered in the TOEFL dataset ; moreover , for some prompts , a metaphor is suggested in the prompt itself and occurs frequently in responses ( e.g. whether broad knowledge is better than specialized knowledge ) .
It is possible that prompt - based patterns interact with POS patterns in ways that affect relative ease or difficulty of POS for metaphor identification .
9 : VUA and TOEFL Datasets by POS : Performance ( F1 - score ) of the best systems submitted to All - POS track by POS subsets of the test data .
In parentheses , we show the rank of the given POS within all POS for the system .
The last column shows the overall drop in performance from best POS ( ranked 1 ) to worst ( ranked 4 ) .
As organizers of the shared task , we would like to thank all the teams for their interest and participation .
We would also like to thank Ton Veale , Eyal Sagi , Debanjan Ghosh , Xinhao Wang , and Keelan Evanini for their helpful comments on the paper , and Verna Dankers for pointing out an error in the original paper that has since been fixed .
