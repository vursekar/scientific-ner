Adobe AMPS 's Submission for Very Low Resource Supervised Translation Task at WMT20
In this paper , we describe our systems submitted to the very low resource supervised translation task at WMT20 . We participate in both translation directions for Upper Sorbian - German language pair . Our primary submission is a subword - level Transformer - based neural machine translation model trained on original training bitext . We also conduct several experiments with backtranslation using limited monolingual data in our postsubmission work and include our results for the same . In one such experiment , we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic , backtranslated corpus followed by fine - tuning on the original parallel training data .
This paper describes our submissions to the shared task on Very Low Resource Supervised Machine Translation at WMT 2020 . The task involved a single language pair : Upper Sorbian - German . We submit supervised neural machine translation ( NMT ) systems for both translation directions , Upper Sorbian German and German Upper Sorbian . NMT models ( Sutskever et al , 2014 ; Bahdanau et al , 2015 ; Cho et al , 2014a ) have achieved stateof - the - art performance on benchmark datasets for multiple language pairs . A big advantage of such systems over phrase - based statistical machine translation ( PBSMT ) ( Koehn et al , 2003 ) models is that they can be trained end - to - end . The bulk of the development , however , has been limited to a handful of high - resource language pairs . The primary reason is that training a well - performing NMT system requires a large amount of parallel training data , which means a lot of equivalent investment in terms of resources . Koehn and Knowles ( 2017 ) show that when compared to PBSMT approaches , NMT models need more training data to achieve the same level of performance . 1 One of the most popular ways to increase the amount of parallel training data for supervised training is backtranslation ( Sennrich et al , 2016a ) . We utilize this approach to improve upon the performance of our baseline models . All of our systems follow the Transformer architecture ( Vaswani et al , 2017 ) . Our primary system is a supervised NMT model trained on the original training bitext . We also report our results on experiments with backtranslation , which were completed post the shared task and hence not a part of our primary submissions . We use the backtranslated data in two distinct ways - as a standalone parallel corpus , and to create a combined parallel corpus by mixing in a 1:1 ratio with the provided training data . We also report the performance of fine - tuned models originally trained only on the backtranslated data . In the following sections , we begin by briefly describing the Transformer architecture and backtranslation . We then discuss our experimental setup as well as our experiments with backtranslation . We conclude with a discussion of our results and possible future work .
The Transformer model is the dominant architecture within current NMT models due to its superior performance on several language pairs . While still a sequence - to - sequence ( Sutskever et al , 2014 ) model composed of an encoder and a decoder , Transformer models are highly parallelizable thanks to being composed purely of feedforward and self - attention layers rather than recurrent layers ( Hochreiter and Schmidhuber , 1997 ; Cho et al , 2014b ) . The reader is encouraged to read the original paper ( Vaswani et al , 2017 ) to gain a deeper understanding of the model . We adopt the Transformer base architecture available under the fairseq 2 ( Ott et al , 2019 ) library for all our models . However , NMT models are known to be datahungry ( Koehn and Knowles , 2017 ) ; their performance improves sharply with the availability of more parallel training data . Except for a few language pairs ( e.g. English - German ) , most have little to no such data available . On the other hand , a far greater number of languages have a decent amount of monolingual data available online ( e.g. Wikipedia ) . To address this issue of lack of parallel data , Sennrich et al ( 2016a ) introduced the concept of backtranslation . It involves creating a synthetic parallel corpus by translating sentences from the target - side monolingual data to the source language and making corresponding pairs . A baseline target source model ( PBSMT or NMT ) , trained with limited data , is generally used for this purpose . It enables the use of large corpora of monolingual data for several languages , the size of which is typically orders of magnitude larger than any corresponding bitext available . What is notable is that only the sourceside data is synthetic in such a scenario and the target - side still corresponds to original monolingual data . Some studies ( Poncelas et al , 2018 ; Popel , 2018 ) have investigated the effects of varying the amount of backtranslated data as a proportion of the total training corpus , including training only on the synthetic dataset as a standalone corpus . We follow some of the related experiments conducted by Kocmi and Bojar ( 2019 ) on Gujarati - English ( another low - resource pair ) with a few exceptions . Besides , we also report performance when pretraining solely on the synthetic corpus following by finetuning on either original or mixed data . While not quite the same , one could think of this approach as having some similarities with transfer learning ( Zoph et al , 2016 ) as well as domain adaptation ( Luong and Manning , 2015 ; Freitag and Al - Onaizan , 2016 ) for machine translation . There has also been work on using sampling ( Edunov et al , 2018 ) for generating backtranslations , but we stick to using beam search in this work .
We used the complete parallel training corpus for our primary systems . In addition , we also made use of monolingual data from each language for 2 https://github.com/pytorch/fairseq two purposes - learning Byte Pair Encodings ( BPE ) ( Sennrich et al , 2016b ) and backtranslation . For Upper Sorbian ( hsb ) , we used the monolingual corpora provided by the Sorbian Institute and by the Witaj Sprachzentrum . To control the quality of the backtranslated data , we chose not to use the data scraped from the web . For the German ( de ) side , we made use of the News Crawl 3 2009 dataset , as it is large enough to satisfy the requirements for our experiments .
No . of sentences hsb - de , bitext 58 , 389 hsb , monolingual 540 , 994 de , monolingual 2 , 000 , 000 Moses toolkit ( Koehn et al , 2007 ) was used for tokenization and punctuation normalization for all data . Before doing any additional preprocessing , we learned separate truecaser models using the toolkit . For this purpose , we took first 500 K sentences from each of the monolingual corpora and aggregated them with the corresponding portion from the training bitext . After tokenizing and truecasing , we joined the parallel training corpus with the same monolingual data . We learned joint BPE 4 with 32 K merge operations over this corpus and applied them to the parallel training data to get vocabularies for each language . Additionally , we used the clean - corpus - n.perl script within Moses to filter out sentences from the parallel corpus with more than 250 subwords as well as sentence length ratio over 1.5 in either direction . Final corpus statistics are presented in Table 1 .
Our primary system is a Transformer base model , trained on the parallel training corpus for both translation directions till 60 epochs . We keep most of the hyperparameters to their default values in fairseq . More precisely , we chose Adam ( Kingma and Ba , 2015 ) as the optimizer and Adam betas were set to 0.9 and 0.98 , respectively . The maximum number of tokens in each batch was set to 4096 . Learning rate was set to 0.0005 , with an inverse squared root decay schedule and 4000 steps of warmup updates . Label smoothing was set to 0.1 and dropout to 0.3 . Label - smoothed cross - entropy was used as the training criterion . We trained all our models for a fixed number of epochs , determined separately for each system , and chose the last checkpoint for reporting BLEU ( Papineni et al , 2002 ) scores on the test sets . All training was done using a single NVIDIA P100 GPU . Due to the small amount of parallel training data , each epoch of training took about 90 seconds on average for the primary system .
In this section , we report our post - submission work on using monolingual data for backtranslation . We took the raw monolingual data that we describe in Section 3.1 and backtranslated with our primary submission models for the respective translation directions , i.e. , hsb de for Upper Sorbian data and de hsb for German data . We used fairseq - generate function with a beam size of 5 for this purpose . Once again , we limited the number of subwords in each sentence to 250 . Finally , we took all sentence pairs for backtranslated Upper Sorbian corpus and the first two million sentence pairs for the German corpus . Table 1 indicates the size of the backtranslated corpora by original language . For further experiments , we name the datasets as follows : auth : Processed original training data . synth : Backtranslated de hsb and hsb de corpora . mixed : Augmented training data obtained by mixing auth with a portion of synth in 1:1 ratio , providing a total of 116 , 778 sentence pairs . We define the following systems for making use of the backtranslated data . Note that the first system only differs from the primary system in the number of training epochs completed . auth - from - scratch : This system has the same settings as the primary system . It was trained on the auth corpus till 80 epochs ( as opposed to 60 for primary ) . mixed - from - scratch : We trained models on mixed data from scratch for 40 epochs . 5 synth - from - scratch : Models were trained only on the synth datasets . To adjust for the difference in the size of the respective backtranslated corpora , we trained hsb de system for 10 epochs and de hsb system for 30 epochs . synth - auth - finetune : We took the models trained via the previous system and fine - tuned them on auth data for 20 epochs in each translation direction . synth - mixed - finetune : Same as the last model , except that fine - tuning was done on mixed data . Fine - tuning was carried out by loading pretrained checkpoints and adding extra training flags in reset - optimizer and reset - lr - scheduler .
The systems were evaluated on the blind test set ( newstest2020 ) using automated metrics ; no human evaluation was done . Table 2 shows cased BLEU scores for various systems . Our primary systems achieved a BLEU score of 47.6 for Upper Sorbian German and 45.2 for German Upper Sorbian translation . We achieved an improvement of 0.3 and 0.4 BLEU points , respectively , by training further till 80 epochs in each direction . We also evaluated a third system , synth - auth - finetune , as described in Section 4 , which provided a jump of 2.6 points in BLEU score over the primary system for Upper Sorbian German and 2.5 for German Upper Sorbian . In addition to evaluating on blind test sets , we also report BLEU scores on the development test set in the same table . Two outcomes are worth highlighting : Model trained only on synth data for German Upper Sorbian translation matched the performance of a similar model trained on the authentic bitext . Best results were obtained by fine - tuning a model trained on synth data with either auth or mixed . The second result is notable since the regime of pretraining followed by fine - tuning improves the BLEU scores by up to 4 points on this test set when compared to training only on the original bitext . Moreover , while the model trained on synth was not able to match the performance of that trained on auth for Upper Sorbian German , it still provides the same benefits as German Upper Sorbian model when fine - tuned further . Looking at the small improvements achieved by using only the mixed corpus for training , increasing its size by combining upsampled auth data with more synth data might lead to even further jumps in the BLEU scores .
In this paper , we described our Transformer model for supervised machine translation for Upper Sorbian - German language pair . We take note of relatively high BLEU scores achieved by our primary systems ( and those of other participants ) on this low - resource language pair , which could relate to the high quality of the training corpus . We also report results and takeaways from several experiments with backtranslated data completed post the shared task . A key result is matching the performance of a system trained on the original bitext with one trained on a limited amount of synthetic , backtranslated data . Domain mismatch and a difference in the quality of monolingual corpus might have prevented the system from achieving a similar result in the other direction . We notice big improvements in performance over the primary systems by following a " pretraining then fine - tuning " regime . An interesting future work would be to measure the applicability of this approach to other lowresource language pairs . Additional systems could be added as well . For instance , models trained on mixed data and fine - tuned on auth data might provide a meaningful comparison . Prior work ( Ding et al , 2019 ) has shown that the number of BPE merge operations has a significant effect on the performance of NMT systems . This work was pointed out during the review process and should be an avenue for further improvement of the model performance .
The author would like to thank his manager for supporting this project , and the anonymous reviewers for their thoughtful comments which helped improve the presentation of this work .
