Interpreting the Robustness of Neural NLP Models to Textual Perturbations
Modern Natural Language Processing ( NLP ) models are known to be sensitive to input perturbations and their performance can decrease when applied to real - world , noisy data . However , it is still unclear why models are less robust to some perturbations than others . In this work , we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation ( robustness ) can be explained by the learnability of the perturbation ( defined as how well the model learns to identify the perturbation with a small amount of evidence ) . We further give a causal justification for the learnability metric . We conduct extensive experiments with four prominent NLP models - TextRNN , BERT , RoBERTa and XLNetover eight types of textual perturbations on three datasets . We show that a model which is better at identifying a perturbation ( higher learnability ) becomes worse at ignoring such a perturbation at test time ( lower robustness ) , providing empirical support for our hypothesis .
Despite the success of deep neural models on many Natural Language Processing ( NLP ) tasks ( Liu et al , 2016 ; Devlin et al , 2019 ; Liu et al , 2019b ) , recent work has discovered that these models are not robust to noisy input from the real world and thus their performance will decrease ( Prabhakaran et al , 2019 ; Niu et al , 2020 ; Ribeiro et al , 2020 ; Moradi and Samwald , 2021 ) . A reliable NLP system should not be easily fooled by slight noise in the text . Although a wide range of evaluation approaches for robust NLP models have been proposed ( Ribeiro et al , 2020 ; Morris et al , 2020 ; Goel et al , 2021 ; , few attempts have been made to understand these benchmark results . Given the difference of robustness between models and perturbations , it is a natural question why models are more sensitive to some perturbations than others . It is crucial to avoid over - sensitivity to input perturbations , and understanding why it happens is useful for revealing the weaknesses of current models and designing more robust training methods . To the best of our knowledge , a quantitative measure to interpret the robustness of NLP models to textual perturbations has yet to be proposed . To improve the robustness under perturbation , it is common practice to leverage data augmentation ( Li and Specia , 2019 ; Min et al , 2020 ; Tan and Joty , 2021 ) . Similarly , how much data augmentation through the perturbation improves model robustness varies between models and perturbations . In this work , we aim to investigate two Research Questions ( RQ ) : RQ1 : Why are NLP models less robust to some perturbations than others ? RQ2 : Why does data augmentation work better at improving the model robustness to some perturbations than others ? We test a hypothesis for RQ1 that the extent to which a model is affected by an unseen textual perturbation ( robustness ) can be explained by the learnability of the perturbation ( defined as how well the model learns to identify the perturbation with a small amount of evidence ) . We also validate another hypothesis for RQ2 that the learnability metric is predictive of the improvement on robust performance brought by data augmentation along a perturbation . Our proposed learnability is inspired by the concepts of Randomized Controlled Trial ( RCT ) and Average Treatment Effect ( ATE ) from Causal Inference ( Rubin , 1974 ; Holland , 1986 ) . Estimation of perturbation learnability for a model consists of three steps : ① randomly labelling a dataset , ② perturbing examples of a particular pseudo class with probabilities , and ③ using ATE to measure the ease with which the model learns the perturbation . The core intuition for our method is to frame an RCT as a perturbation identification task and formalize the notion of learnability Exp No . Measurement
Perturbation Training Examples Test Examples 0 Standard original l ( x i , 0 ) , ( x j , 1 ) ( x i , 0 ) , ( x j , 1 ) 1 Robustness original l { 0 , 1 } ( x i , 0 ) , ( x j , 1 ) ( x * i , 0 ) , ( x * j , 1 ) 2 Data Augmentation original l { 0 , 1 } ( x i , 0 ) , ( x j , 1 ) ( x * i , 0 ) , ( x * j , 1 ) ( x * i , 0 ) , ( x * j , 1 ) 3 Learnability random l ′ { 1 ′ } ( x j , 0 ′ ) , ( x * i , 1 ′ ) ( x * i , 1 ′ ) 4 random l ′ { 1 ′ } ( x j , 0 ′ ) , ( x * i , 1 ′ ) ( x i , 1 ′ ) Table 1 : Example experiment settings for measuring learnability , robustness and improvement by data augmentation . We perturb an example if its label falls in the set of label ( s ) in " Perturbation " column . means no perturbation at all . Training / test examples are the expected input data , assuming we have only one negative ( x i , 0 ) and positive ( x j , 1 ) example in our original training / test set . l ′ is a random label and x * is a perturbed example . as a causal estimand based on ATE . We conduct extensive experiments on four neural NLP models with eight different perturbations across three datasets and find strong evidence for our two hypotheses . Combining these two findings , we further show that data augmentation is only more effective at improving robustness against perturbations that a model is more sensitive to , contributing to the interpretation of robustness and data augmentation . Learnability provides a clean setup for analysis of the model behaviour under perturbation , which contributes better model interpretation as well . Contribution . This work provides an empirical explanation for why NLP models are less robust to some perturbations than others . The key to this question is perturbation learnability , which is grounded in the causality framework . We show a statistically significant inverse correlation between learnability and robustness .
As a pilot study , we consider the task of binary text classification . The training set is denoted as D train = { ( x 1 , l 1 ) , ... , ( x n , l n ) } , where x i is the i - th example and l i { 0 , 1 } is the corresponding label . We fit a model f ∶ ( x ; θ ) ↦ { 0 , 1 } with parameters θ on the training data . A textual perturbation is a transformation g ∶ ( x ; β ) x * that injects a specific type of noise into an example x with parameters β and the resulting perturbed example is x * . We design several experiment settings ( Table 1 ) to answer our research questions . Experiment 0 in Table 1 is the standard learning setup , where we train and evaluate a model on the original dataset . Below we detail other experiment settings .
Robustness . We apply the perturbations to test examples and measure the robustness of model to said perturbations as the decrease in accuracy . In Table 1 , Experiment 1 is related to robustness measurement , where we train a model on unperturbed dataset and test it on perturbed examples . We denote the test accuracy of a model f ( ⋅ ) on examples perturbed by g ( ⋅ ) in Experiment 1 as A 1 ( f , g , D * test ) . Similarly , the test accuracy in Experiment 0 is A 0 ( f , D test ) . Consequently , the robustness is calculated as the difference of test accuracies : robustness ( f , g , D ) = A 1 ( f , g , D * test ) −A 0 ( f , D test ) . ( 1 ) Models usually suffer a performance drop when encountering perturbations , therefore the robustness is usually negative , where lower values indicate decreased robustness . Improvement by Data Augmentation ( Post Augmentation ∆ ) . To improve robust accuracy ( Tu et al , 2020 ) ( i.e. , accuracy on the perturbed test set ) , it is a common practice to leverage data augmentation ( Li and Specia , 2019 ; Min et al , 2020 ; Tan and Joty , 2021 ) . We simulate the data augmentation process by appending perturbed data to the training set ( Experiment 2 of Table 1 ) . We calculate the improvement on performance after data augmentation as the difference of test accuracies : ∆ post_aug ( f , g , D ) = A 2 ( f , g , D * test ) −A 1 ( f , g , D * test ) . ( ) 2 where A 2 ( f , g , D * test ) denotes the test accuracy of Experiment 2 . ∆ post_aug is the higher the better . Learnability . We want to compare perturbations in terms of how well the model learns to identify them with a small amount of evidence . We cast learnability estimation as a perturbation classification task , where a model is trained to identify the perturbation in an example . We define that the learnability estimation consists of three steps , namely ① assigning random labels , ② perturbing with probabilities , and ③ estimating model performance . Below we introduce the procedure and intuition for each step . This estimation framework is further grounded in concepts from the causality literature in Section 3 , which justifies our motivations . We summarize our estimation approach formally in Algorithm 1 ( Appendix A ) . ① Assigning Random Labels . We randomly assign pseudo labels to each training example regardless of its original label . Each data point has equal probability of being assigned to positive ( l ′ = 1 ) or negative ( l ′ = 0 ) pseudo label . This results in a randomly labeled dataset D ′ train = { ( x 1 ; l ′ 1 ) , ... , ( x n , l ′ n ) } , where L ′ ∼ Bernoulli ( 1 , 0.5 ) . In this way , we ensure that there is no difference between the two pseudo groups since the data are randomly split . ② Perturbing with Probabilities . We apply the perturbation g ( ⋅ ) to each training example in one of the pseudo groups ( e.g. , l ′ = 1 in Algorithm 1 ) 1 . In this way , we create a correlation between the existence of perturbation and label ( i.e. , the perturbation occurrence is predictive of the label ) . We control the perturbation probability p [ 0 , 1 ] , i.e. , an example has a specific probability p of being perturbed . This results in a perturbed training set D ′ * train = { ( x * 1 , l ′ 1 ) , ... , ( x * n , l ′ n ) } , where the perturbed example x * i is : Z ∼ U ( 0 , 1 ) , ∀i { 1 , 2 , ... , n } x * i = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ g ( x i ) l ′ i = 1 z < p , x i otherwise . ( ) 3 Here Z is a random variable drawn from a uniform distribution U ( 0 , 1 ) . Due to randomization in the formal step , now the only difference between the two pseudo groups is the occurrence of perturbation . ③ Estimating Model Performance . We train a model on the randomly labeled dataset with per - 1 Because the training data is randomly split into two pseudo groups , applying perturbations to any one of the groups should yield same result . We assume that we always perturb into the first group ( l ′ = 1 ) hereafter . turbed examples . Since the only difference between the two pseudo groups is the existence of the perturbation , the model is trained to identify the perturbation . The original test examples D test are also assigned random labels and become D ′ test . We perturb all of the test examples in one pseudo group ( e.g. , l ′ = 1 , as in step 2.1 ) to produce a perturbed test set D ′ * test . Finally , the perturbation learnability is calculated as the difference of accuracies on D ′ * test and D ′ test , which indicates how much the model learns from the perturbation 's co - occurrence with pseudo label : learnability ( f , g , p , D ) = A 3 ( f , g , p , D ′ * test ) −A 4 ( f , g , p , D ′ test ) . ( 4 ) A 4 ( f , g , p , D ′ * test ) and A 3 ( f , g , p , D ′ test ) are accuracies measured by Experiment 4 and 3 of Table 1 , respectively . We observe that the learnability depends on perturbation probability p. For each modelperturbation pair , we obtain multiple learnability estimates by varying the perturbation probability ( Figure 3 ) . However , we expect that learnability of the perturbation ( as a concept ) should be independent of perturbation probability . To this end , we use the log AU C ( area under the curve in log scale ) of the p − learnability curve ( Figure 3 ) , termed as " average learnability " , which summarizes the overall learnability across different perturbation probabilities p 1 , ... , p t : avg_learnability ( f , g , D ) ∶= log AU C ( { ( p i , learnability ( f , g , p i , D ) ) | i { 1 , 2 , ... , t } } ) . ( 5 ) We use log AU C rather than AU C because we empirically find that the learnability varies substantially between perturbations when p is small , and a log scale can better capture this nuance . We also introduce learnability at a specific perturbation probability ( Learnability @ p ) as an alternate summary metric and provide a comparison of this metric against log AU C in Appendix D.
With the above - defined terminologies , we propose hypotheses for RQ1 and RQ2 in Section 1 , respectively . Hypothesis 1 ( H1 ) : A model for which a perturbation is more learnable is less robust against the same perturbation at the test time . This is not obvious because the model encounters this perturbation during training in learnability estimation while they do not in robustness measurement . Hypothesis 2 ( H2 ) : A model for which a perturbation is more learnable experiences bigger robustness gains with data augmentation along such a perturbation . We validate both Hypotheses 1 and 2 with experiments on several perturbations and models described in Section 4.1 and 4.2 .
In Section 2.1 , we introduce the term " learnability " in an intuitive way . Now we map it to a formal , quantitative measure in standard statistical frameworks . Learnability is actually motivated by concepts from the causality literature . We provide a brief introduction to basic concepts of causal inference in Appendix B. In fact , learnability is the causal effect of perturbation on models , which is often difficult to measure due to the confounding latent features . In the language of causality , this is " correlation is not causation " . Causality provides insight on how to fully decouple the effect of perturbation and other latent features . We introduce the causal motivations for step 2.1 and 2.1 of learnability estimation in the following Section 3.1 and 3.2 , respectively .
Natural noise ( simulated by perturbations in this work ) usually co - occurs with latent features in an example . If we did not assign random labels and simply perturbed one of the original groups , there would be confounding latent features that would prevent us from estimating the causal effect of the perturbation . Figure 1a illustrates this scenario . Both perturbation P and latent feature T may affect the outcome Y , 2 while the latent feature is predictive of label L. Since we make the perturbation P on examples with the same label , P is decided by L. It therefore follows that T is a confounder of the effect of P on Y , resulting in non - causal association flowing along the path P L T Y . However , if we do randomize the labels , P no longer has any causal parents ( i.e. , incoming edges ) ( Figure 1b ) . This is because perturbation is purely 2 Y is later defined in Section 3.2 the language of causality , this is " correlation is not causation " . Causality provides insight on how to fully decouple the effect of perturbation and other latent features . We introduce the causal motivations for step 1 and 3 of learnability estimation in the following Section 3.1 and 3.2 respectively .
Natural noise ( simulated by perturbations in this work ) usually co - occurs with latent features in an example . If we did not assign random labels and simply perturbed one of the original groups , there would be confounding latent features that would prevent us from estimating the causal effect of the perturbation . Figure 4a illustrates this scenario . Both perturbation P and latent feature T may affect the outcome Y , 3 while the latent feature is predictive of label L. Since we make perturbation P on examples with the same label , P is decided by L. It therefore follows that T is a confounder of the effect of P on Y , resulting in non - causal association flowing along the path P L T Y . However , if we do randomize the labels , P no longer has any causal parents ( i.e. , incoming edges ) ( Figure 4b ) . This is because perturbation is purely random . Without the path represented by P L , all of the association that flows from P to Y is causal . As a result , we can directly calculate the causal effect from the observed outcomes ( Section 3.2 ) . Our randomization experiments allow us to dis - of the association that flows from P to Y is causal .
As a result , we can directly calculate the causal 315 effect from the observed outcomes ( Section 3.2 ) .
Our randomization experiments allow us to dis - Figure 1 : Causal graph explanation for decoupling perturbation and latent feature with randomization . P is the perturbation and T is the latent feature . L is the original label and Y is the correctness of the predicted label . random . Without the path represented by P L , all of the association that flows from P to Y is causal . As a result , we can directly calculate the causal effect from the observed outcomes .
We identify learnability as a causal estimand . In causality , the term " identification " refers to the process of moving from a causal estimand ( Average Treatment Effect , ATE ) to an equivalent statistical estimand . We show that the difference of accuracies on D ′ * test and D ′ test is actually a causal estimand . We define the outcome Y of a test example x i as the correctness of the predicted label : Y i ( 0 ) ∶= 1 { f ( x i ) = l ′ i } . ( 6 ) where 1 { ⋅ } is the indicator function . Similarly , the outcome Y of a perturbed test example x * i is : Y i ( 1 ) ∶= 1 { f ( x * i ) = l ′ i } . ( 7 ) According to the definition of Individual Treatment Effect ( ITE , see Equation 9of Appendix B ) , we have IT E i = 1 { f ( x * i ) = l ′ i } −1 { f ( x i ) = l ′ i } . We then take the average over all the perturbed test examples ( half of the test set ) 3 . This is our Average Treatment Effect ( ATE ) : AT E = E [ Y ( 1 ) ] − E [ Y ( 0 ) ] = E [ 1 { f ( x * ) = l ′ } ] − E [ 1 { f ( x ) = l ′ } ] = P ( f ( x * ) = l ′ ) − P ( f ( x ) = l ′ ) = A ( f , g , p , D ′ * test ) − A ( f , g , p , D ′ test ) . ( 8 ) Perturbation Example Sentence None His quiet and straightforward demeanor was rare then and would be today . duplicate_punctuations His quiet and straightforward demeanor was rare then and would be today .. butter_fingers_perturbation His quiet and straightforward demeanor was rarw then and would be today . shuffle_word quiet would and was be and straightforward then demeanor His today . rare random_upper_transformation His quiEt and straightForwARd Demeanor was rare TheN and would be today . insert_abbreviation His quiet and straightforward demeanor wuz rare then and would b today . whitespace_perturbation His quiet and straightforward demean or wa s rare thenand would be today . visual_attack_letters Hiṩ qủiẽt ầռd strḁighṭḟorwẳrȡ dԑmeanoŕ wȃṣ rȧre tḫen and wouᶅd ϸә tອḏầȳ . leet_letters His qui3 t and strai9htfor3ard d3m3an0r 3as rar3 t43n and 30uld 63 t0da4 . where A ( f , g , p , D ) is the accuracy of model f ( ⋅ ) trained with perturbation g ( ⋅ ) at perturbation probability p on test set D. Therefore , we show that ATE is exactly the difference of accuracy on the perturbed and unperturbed test sets with random labels . And the difference is learnability according to Equation 4 . We discuss another means of identification of ATE in Appendix C , based on the prediction probability . We compare between the probability - based and accuracy - based metrics there . We find that our accuracy - based metric yields better resolution , so we report this metric in the main text of this paper .
Criteria for Perturbations . We select various character - level and word - level perturbation methods in existing literature that simulate different types of noise an NLP model may encounter in real - world situations . These perturbations are nonadversarial , label - consistent , and can be automatically generated at scale . We note that our perturbations do not require access to the model internal structure . We also assume that the feature of perturbation does not exist in the original data . Not all perturbations in the existing literature are suitable for our task . For example , a perturbation that swaps gender words ( i.e. , female male , male female ) is not suitable for our experiments since we can not distinguish the perturbed text from an unperturbed one . In other words , the perturbation function g ( ⋅ ) should be asymmetric , such that g ( g ( x ) ) ≠ x. Figure 2 shows an example sentence with different perturbations . Perturbation of " dupli - cate_punctuation " doubles the punctuation by appending a duplicate after each punctuation , e.g. , " , " " " " ; " butter_fingers_perturbation " misspells some words with noise erupting from keyboard typos ; " shuffle_word " randomly changes the order of word in the text ( Moradi and Samwald , 2021 ) ; " random_upper_transformation " randomly adds upper cased letters ( Wei and Zou , 2019 ) ; " in - sert_abbreviation " implements a rule system that encodes word sequences associated with the replaced abbreviations ; " whitespace_perturbation " randomly removes or adds whitespaces to text ; " vi - sual_attack_letters " replaces letters with visually similar , but different , letters ( Eger et al , 2019 ) ; " leet_letters " replaces letters with leet , a common encoding used in gaming ( Eger et al , 2019 ) .
To test the learnability , robustness and improvement by data augmentation with different NLP models and perturbations , we experiment with four modern and representative neural NLP models : TextRNN ( Liu et al , 2016 ) , BERT ( Devlin et al , 2019 ) , RoBERTa ( Liu et al , 2019b ) and XLNet ( Yang et al , 2019 ) . For TextRNN , we use the implementation by an open - source text classification toolkit NeuralClassifier ( Liu et al , 2019a ) . For the other three pretrained models , we use the bert - base - cased , roberta - base , xlnet - base - cased versions from Hugging Face ( Wolf et al , 2020 ) , respectively . These two platforms support most of the common NLP models , thus facilitating extension studies of more models in future . We use three common binary text classification datasets - IMDB movie reviews ( IMDB ) ( Pang and Lee , 2005 ) , Yelp polarity reviews ( YELP ) ( Zhang et al , 2015 ) , Quora Question Pair ( QQP ) ( Iyer et al , 2017 ) - as our testbeds . IMDB and YELP datasets present the task of sentiment analysis , where each sentence is labelled as positive or negative sentiment . QQP is a paraphrase detection task , where each pair of sentences is marked as semantically equivalent or not . To control the effect of dataset size and imbalanced classes , all datasets are randomly subsampled to the same size as IMDB ( 50k ) with balanced classes . The training steps for all experiments are the same as well . We implement perturbations g ( ⋅ ) with two self - designed ones and six selected ones from the NL - Augmenter library ( Dhole et al , 2021 ) . For perturbation probabilities , we choose 0.001 , 0.005 , 0.01 , 0.02 , 0.05 , 0.10 , 0.50 , 1.00 . We run all experiments across three random seeds and report the average results .
Figure 3 shows learnability as a function of perturbation probability . Learnability @ p generally increases as we increase the perturbation probability , and when we perturb all the examples ( i.e. , p = 1.0 ) , every model can easily identify it well , resulting in the maximum learnability of 1.0 . This shows that neural NLP models master these perturbations eventually . At lower perturbation probabilities , some models still learn that perturbation alone predicts the label . In fact , the major difference between different p − learnability curves is the area of lower perturbation probabilities and this provides motivation for using log AU C instead of AU C as the summarization of learnability at different p ( Section 2.1 ) . Table 2 shows the average learnability over all perturbation probabilities of each modelperturbation pair on IMDB dataset in Figure 3 . 4 It reveals the most learnable perturbation for each model . For example , the learnability of " vi - sual_attack_letters " and " leet_letters " are very high for all four models , likely due to their strong effects on the tokenization process ( Salesky et al , 2021 ) . Perturbations like " white_space_perturbation " and " duplicate_punctuations " are less learnable for pretrained models , probably because they have weaker effects on the subword level tokenization , or they may have encountered similar noise in the pretraining corpora . We observe that " dupli - cate_punctuations " already exists in the original text of YELP dataset ( e.g. , " The burgers are awesome ! ! " ) , thus violating our assumptions for perturbations in Section 4.1 . As a result , the curve for 4 Please refer to Appendix E for benchmark results on YELP ( Table 5 ) and QQP ( this perturbation substantially deviates from others in Figure 3 . We do not count this perturbation on YELP dataset in the following analysis . The perturbation learnability experiments provide a clean setup for NLP practitioners to analyze the effect of textual perturbations on models .
We observe a negative correlation between learnability ( Equation 4 ) and robustness ( Equation 1 ) across all three datasets in Table 2 , validating Hypothesis 1 . Table 2 also quantifies the trend that data augmentation with a perturbation the model is less robust to has more improvement on robustness ( Hypothesis 2 ) . We plot the correlations on IMDB dataset in Figure 4a and 4b . 5 Both the correlations between 1 ) learnability vs. robustness and 2 ) learnability vs. improvement by data augmentation are strong ( Spearman | ρ | > 0.6 ) and highly significant ( p - value < 0.001 ) , which firmly supports our hypotheses . Our findings provide insight about when the model is less robust and when data augmentation works better for improving robustness . Figure 4c shows that the more learnable a perturbation is for a model , the greater the likelihood that its robustness can be improved through data augmentation along this perturbation . We argue that this is not simply because there is more room for improvement by data augmentation . From a causal perspective , learnability acts as a common cause ( confounder ) for both robustness and improvement by data augmentation . This indicates a potential limitation of using data augmentation for improving robustness to perturbations : data augmentation is only more effective at improving robustness against perturbations more learnable for a model .
Potential Impacts . Our findings seem intuitive but are non - trivial . The NLP models were not trained on perturbed examples when measuring robustness , but still they display a strong correlation with perturbation learnability . Understanding these findings are important for a more principled evaluation of and control over NLP models . Specifically , the learnability metric complements to the evaluation of newly designed perturbations by revealing model weaknesses in a clean setup . Reducing perturbation learnability is promising for improving robustness of models . Contrastive learning ( Gao et al , 2021 ; Yan et al , 2021 ) that pulls the representations of the original and perturbed text together , makes it difficult for the model to identify the perturbation ( reducing learnability ) and thus may help improve robustness . Perturbation can also be viewed as injecting spurious feature into the examples , so the learnability metric also helps to interpret robustness to spurious correlation ( Sagawa et al , 2020 ) . Moreover , learnability may facilitate the development of model architectures with explicit inductive biases ( Warstadt and Bowman , 2020 ; to avoid sensitivity to noisy perturbations . Grounding the learnability within the causality framework inspires future researchers to incorporate the causal perspective into model design ( Zhang et al , 2020 ) , and make the model robust to different types of perturbations . Limitations . In this work , we focus on the robust accuracy ( Section 2.1 ) , which is accuracy on the perturbed test set . We do not assume that the test accuracy of the original test set , a.k.a in - distribution accuracy , is invariant invariant against training with augmentation or not . It would be interesting to investigate the trade - off between robust accuracy and in - distribution accuracy in the future . We also note that this work has not established that the relationship between learnability and robustness is causal . This could be explored with other approaches in causal inference for deconfounding besides simulation on randomized control trial , such as working with real data but stratifying it ( Frangakis and Rubin , 2002 ) , to bring the learnability experiment closer to more naturalistic settings . Although we restrict to balanced , binary classification for simplicity in this pilot study , our framework can also be extended to imbalanced , multi - class classification . We are aware that computing average learnability is expensive for large models and datasets , which is further discussed in Section 8 . We provide a greener solution in Appendix D. We could further verify our assumptions for perturbations with a user study ( Moradi and Samwald , 2021 ) which investigates how understandable the perturbed texts are to humans .
Robustness of NLP Models to Perturbations . The performance of NLP models can decrease when encountering noisy data in the real world . Recent works ( Prabhakaran et al , 2019 ; Ribeiro et al , 2020 ; Niu et al , 2020 ; Moradi and Samwald , 2021 ) present comprehensive evaluations of the robustness of NLP models to different types of perturbations , including typos , changed entities , negation , etc . Their results reveal the phenomenon that NLP models can handle some specific types of perturbation more effectively than others . However , they do not go into a deeper analysis of the reason behind the difference of robustness between models and perturbations . Interpretation of Data Augmentation . Although data augmentation has been widely used in CV ( Sato et al , 2015 ; DeVries and Taylor , 2017 ; Dwibedi et al , 2017 ) and NLP ( Wang and Yang , 2015 ; Kobayashi , 2018 ; Wei and Zou , 2019 ) , the underlying mechanism of its effectiveness remains under - researched . Recent studies aim to quantify intuitions of how data augmentation improves model generalization . Gontijo - Lopes et al ( 2020 ) introduce affinity and diversity , and find a correlation between the two metrics and augmentation performance in image classification . In NLP , Kashefi and Hwa ( 2020 ) propose a KL - divergence - based metric to predict augmentation performance . Our proposed learnability metric implies when data augmentation works better and thus acts as a complement to this line of research .
This work targets at an open question in NLP : why models are less robust to some textual perturbations than others ? We find that learnability , which causally quantifies how well a model learns to identify a perturbation , is predictive of the model robustness to the perturbation . In future work , we will investigate whether these findings can generalize to other domains , including computer vision .
Computing average learnability requires training a model for multiple times at different perturbation probabilities , which can be computationally intensive if the sizes of the datasets and models are large . This can be a non - trivial problem for NLP practitioners with limited computational resources . We hope that our benchmark results of typical perturbations for NLP models work as a reference for potential users . Collaboratively sharing the results of such metrics on popular models and perturbations in public fora can also help reduce duplicate investigation and coordinate efforts across teams . To alleviate the computational efficiency issue of average learnability estimation , using learnability at selected perturbation probabilities may help at the cost of reduced precision ( Appendix D ) . We are not alone in facing this issue : two similar metrics for interpreting model inductive bias , extractability and s - only error ) also require training the model repeatedly over the whole dataset . Therefore , finding an efficient proxy for average learnability is promising for more practical use of learnability in model interpretation .
Algorithm 1 Learnability Estimation Input : training set D train = { ( x 1 , l 1 ) , ... , ( x n , l n ) } , test set D test = { ( x n+1 , l n+1 ) , ... , ( x n+m , l n+m ) } , D = D train ∪ D test , model f ∶ ( x ; θ ) ↦ { 0 , 1 } , perturbation g ∶ ( x ; β ) x * , perturbation probability p Output : learnability ( f , g , p , D ) 1 : // ① assigning random labels 2 : Initialize an empty dataset D ′ 3 : for i in { 1 , 2 , ... , n + m } do 4 : l ′ i randint [ 0 , 1 ] 5 : D ′ D ′ ∪ { ( x i , l ′ i ) } 6 : end for 7 : // ② perturbing with probabilities 8 : Initialize an empty dataset D ′ * 9 : for i in { 1 , 2 , ... , n + m } do 10 : z rand ( 0 , 1 ) 11 : x * i x i 12 : if l ′ i = 1 z < p then 13 : x * i g ( x i ) 14 : end if 15 : D ′ * D ′ * ∪ { ( x * i , l ′ i ) D ′ train , D ′ test D ′ [ 1 ∶ n ] , D ′ [ n + 1 ∶ n + m ] 19 : D ′ * train , D ′ * test D ′ * [ 1 ∶ n ] , D ′ * [ n+1 ∶ n+m ] 20 : fit the model f ( ⋅ ) on D ′ * train 21 : A ( f , g , p , D ′ * test ) f ( ⋅ ) accuracy on D ′ * test 22 : A ( f , g , p , D ′ test ) f ( ⋅ ) accuracy on D ′ test 23 : return A ( f , g , p , D ′ * test ) − A ( f , g , p , D ′ test )
The aim of causal inference is to investigate how a treatment T affects the outcome Y . Confounder X refers to a variable that influences both treatment T and outcome Y . For example , sleeping with shoes on ( T ) is strongly associated with waking up with a headache ( Y ) , but they both have a common cause : drinking the night before ( X ) ( Neal , 2020 ) . In our work , we aim to study how a perturbation ( treatment ) affects the model 's prediction ( outcome ) . However , the latent features and other noise usually act as confounders . Causality offers solutions for two questions : 1 ) how to eliminate the spurious association and isolate the treatment 's causal effect ; and 2 ) how varying T affects Y , given both variables are causallyrelated . We leverage both of these properties in our proposed method . Let us now introduce Randomized Controlled Trial and Average Treatment Effect as key concepts in answering the above two questions , respectively . Randomized Controlled Trial ( RCT ) . In an RCT , each participant is randomly assigned to either the treatment group or the non - treatment group . In this way , the only difference between the two groups is the treatment they receive . Randomized experiments ideally guarantee that there is no confounding factor , and thus any observed association is actually causal . We operationalize RCT as a perturbation classification task in Section 3.1 . Average Treatment Effect ( ATE ) . In Section 3.2 , we apply ATE ( Holland , 1986 ) as a measure of learnability . ATE is based on Individual Treatment Effect ( ITE , Equation 9 ) , which is the difference of the outcome with and without treatment . IT E i = Y i ( 1 ) − Y i ( 0 ) . ( 9 ) Here , Y i ( 1 ) is the outcome Y of individual i that receives treatment ( T = 1 ) , while Y i ( 0 ) is the opposite . In the above example , waking up with a headache ( Y = 1 ) with shoes on ( T = 1 ) means Y i ( 1 ) = 1 . We calculate the Average Treatment Effect ( ATE ) by taking an average over ITEs : AT E = E [ Y ( 1 ) ] − E [ Y ( 0 ) ] . ( 10 ) ATE quantifies how the outcome Y is expected to change if we modify the treatment T from 0 to 1 . We provide specific definitions of ITE and ATE in Section 3.2 .
In Section 3.2 , we propose an accuracy - based identification of ATE . Now we discuss another probability - based identification and compare between them . We can also define the outcome Y of a test example x i as the predicted probability of ( pseudo ) true label given by the trained model f ( ⋅ ) : Y i ( 0 ) ∶= P f ( L ′ = l ′ i | X = x i ) ( 0 , 1 ) . ( 11 ) Similarly , the performance outcome Y of a perturbed test data point x * i is : Y i ( 1 ) ∶= P f ( L ′ = l ′ i | X = x * i ) ( 0 , 1 ) . ( 12 ) For example , for a test example ( x i , l ′ i ) which receives treatment ( l ′ i = 1 ) , the trained model f ( ⋅ ) predicts its label as 1 with only a small probability 0.1 before treatment ( it has not been perturbed yet ) , and 0.9 after treatment . So the Individual Treatment Effect ( ITE , see Equation 9 ) of this example is calculated as IT E i = Y i ( 1 ) − Y i ( 0 ) = 0.9 − 0.1 = 0.8 . We then take an average over all the perturbed test examples ( half of the test set ) as Average Treatment Effect ( ATE , see Equation 10 ) , which is exactly the learnability of a perturbation for a model . To clarify , the two operands in Equation 10 are defined as follows : E [ Y ( 1 ) ] ∶= P ( f , g , p , D ′ * test ) . ( 13 ) It means the average predicted probability of ( pseudo ) true label given by the trained model f ( ⋅ ) on the perturbed test set D ′ * test . E [ Y ( 0 ) ] ∶= P ( f , g , p , D ′ test ) . ( 14 ) Similarly , this is the average predicted probability on the randomly labeled test set D ′ test . Notice that the accuracy - based definition of outcome Y ( Equation 6 ) can also be written in a similar form to the probability - based one ( Equation 11 ) : Y i ( 0 ) ∶= 1 { f ( x i ) = l ′ i } = 1 { P f ( L ′ = l ′ i | X = x i ) > 0.5 } { 0 , 1 } . ( 15 ) because the correctness of the prediction is equal to whether the predicted probability of true ( pseudo ) label exceeds a certain threshold ( i.e. , 0.5 ) . The major difference is that , accuracy - based IT E is a discrete variable falling in { −1 , 0 , 1 } , while probability - based IT E is a continuous one ranging from - 1 to 1 . For example , if a model learns to identify a perturbation and thus changes its prediction from wrong ( before perturbation ) to correct ( after perturbation ) , accuracy - based IT E will be 1 − 0 = 1 while probability - based IT E will be less than 1 . That is to say , accuracy - based AT E tends to vary more drastically than probability - based if inconsistent predictions occur more often , and thus can better capture the nuance of perturbation learnability . Empirically , we find that accuracy - based average learnability varies greatly ( σ = 0.375 , Table 4 ) and thus can better distinguish between different model - perturbation pairs than probabilitybased one ( σ = 0.288 , Table 4 ) . As a result , we choose accuracy - based ATE as the primary measurement of learnability in this paper .
Inspired by Precision @ K in Information Retrieval ( IR ) , we propose a similar metric dubbed Learnability @ p , which is the learnability of a perturbation for a model at a specific perturbation probability p. We are primarily interested in whether a selected p can represent the learnability over different perturbation probabilities and correlates well with robustness and post data augmentation ∆. We calculate the standard deviation ( σ ) of Learnability @ p and average learnability ( log AU C ) over all model - perturbation pairs to measure how well it can distinguish between different models and perturbations . Table 4 shows that average learnability is more diversified than all Learnability @ p and diversity ( σ ) peaks at p = 0.01 for accuracybased / probability - based measurement . Accuracybased Learnability @ p is generally more diversified across models and perturbations than its counterpart . To investigate the strength of the correlations , we also calculate Spearman ρ between accuracy - based / probability - based learnability @ p vs. average learnability / robustness / post data augmentation ∆ over all model - perturbation pairs . Table 4 shows that generally average learnability has stronger correlation than Learnability @ p. Correlations with both robustness and post data augmentation ∆ peak at p = 0.02 for accuracybased / probability - based measurements , and the correlations with average learnability ( 0.816*/0.886 * ) are also strong at these perturbation probabilities . Overall , Learnability @ p with higher standard deviation correlates better with average learnability , robustness and post data augmentation ∆. Our analysis shows that if p is carefully selected by σ , Learnability @ p is also a promising metric , though not as accurate as average learnability . One advantage of Learnability @ p over average learnability is that it costs less time to obtain learnability at a single perturbation probability . 3 ) of each model - perturbation pair on QQP dataset . Rows are sorted by average values over all models . The perturbation for which a model is most learnable is highlighted in bold while the following one is underlined .
This research is supported by the National Research Foundation , Singapore under its International Research Centres in Singapore Funding Initiative . Any opinions , findings and conclusions or recommendations expressed in this material are those of the author ( s ) and do not reflect the views of National Research Foundation , Singapore . We acknowledge the support of NVIDIA Corporation for their donation of the GeForce RTX 3090 GPU that facilitated this research .
