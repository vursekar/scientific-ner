DAMO - NLP at SemEval - 2022 Task 11 : A Knowledge - based System for Multilingual Named Entity Recognition
The MultiCoNER shared task aims at detecting semantically ambiguous and complex named entities in short and low - context settings for multiple languages .
The lack of contexts makes the recognition of ambiguous named entities challenging .
To alleviate this issue , our team DAMO - NLP proposes a knowledge - based system , where we build a multilingual knowledge base based on Wikipedia to provide related context information to the named entity recognition ( NER ) model .
Given an input sentence , our system effectively retrieves related contexts from the knowledge base .
The original input sentences are then augmented with such context information , allowing significantly better contextualized token representations to be captured .
Our system wins 10 out of 13 tracks in the MultiCoNER shared task .
1 * : project lead .
⋆ : equal contributions .
The MultiCoNER shared task ( Malmasi et al , 2022b ) aims at building Named Entity Recognition ( NER ) systems for 11 languages , including English , Spanish , Dutch , Russian , Turkish , Korean , Farsi , German , Chinese , Hindi , and Bangla .
The task has three kinds of tracks including one multilingual track , 11 monolingual tracks and one code - mixed track .
The multilingual track requires training multilingual NER models that are able to handle all languages .
The monolingual tracks require training individual monolingual models where each model works for only one language .
The code - mixed track requires handling code - mixed samples ( sentences that may involve multiple languages ) .
The datasets mainly contain sentences from three domains : Wikipedia , web questions and user queries , köpings is rate which are usually short and low - context sentences .
Moreover , these short sentences usually contain semantically ambiguous and complex entities , which makes the problem more difficult .
In practice , professional annotators usually use their domain knowledge to disambiguate such kinds of entities .
They may retrieve the related documents from a knowledge base ( KB ) or from a search engine to better guide them the annotation of ambiguous named entities ( Wang et al , 2019 ) .
Therefore , we believe retrieving related knowledge can help the NER model to disambiguate hard samples in the shared task as well .
A motivating example is shown in Figure 1 , which shows how the retrieval results could help to improve the prediction in practice . 
In this paper , we propose a general knowledgebased system for the MultiCoNER shared task .
We propose to retrieve the related documents of the input sentence so that the recognition of difficult entities can be significantly eased .
Based on Wikipedia of the 11 languages , we build a multilingual KB to search for the related documents of the input sentence .
We then feed the input sentence and the related documents into the NER model .
Moreover , we propose an iterative retrieval approach to i m - prove the retrieval quality .
During training , we propose multi - stage fine - tuning .
We first train a multilingual model so that the NER model can learn from all annotations .
Next , we train the monolingual models ( one for each language ) and a code - mixed model by using the fine - tuned XLM - RoBERTa ( XLM - R ) ( Conneau et al , 2020 ) embeddings in the multilingual model as initialization to further boost model performance on monolingual and code - mixed tracks .
For each track , we train multiple models with different random seeds and use majority voting to form the final predictions . 
Besides the system description , we make the following observations based on our experiments : 1 . Knowledge - based systems can significantly improve both in - and out - of - domain performance compared with system without knowledge inputs .
2 . Our multi - stage fine - tuning approach can help improve model performance in all the monolingual and code - mixed tracks .
The approach can also reduce the training time to speed up our system building at different stages .
3 . Our iterative retrieval strategy can further improve the retrieval quality and result in significant improvement on the performance of codemixed track .
4 . Searching over Wikipedia KB performs better than using online search engines on the Multi - CoNER datasets .
5 .
Comparing with other model variants we have tried , our NER model enjoys a good balance between model performance and speed .
NER ( Sundheim , 1995 ) is a fundamental task in natural language processing .
The task has a lot of applications in various domains such as social media ( Derczynski et al , 2017 ) , news ( Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ) , Ecommerce ( Fetahu et al , 2021 ;
Wang et al , 2021b ) , and medical domains ( Dogan et al , 2014 ; Li et al , 2016 ) .
Recently , pretrained contextual embeddings such as BERT ( Devlin et al , 2019 ) , XLM - R and LUKE ( Yamada et al , 2020 ) have significantly improved the NER performance .
The embeddings are trained on large - scale unlabeled data such as Wikipedia , which can significantly improve the contextual representations of named entities .
Recent efforts ( Peters et al , 2018 ; Akbik et al , 2018 ; Straková et al , 2019 ) concatenate different kinds of pretrained embeddings to form stronger token representations .
Moreover , the embeddings are trained over long documents , which allows the model to easily model long - range dependencies to disambiguate complex named entities in the sentence .
Recently , a lot of work shows that utilizing the document - level contexts in the CoNLL NER datasets can significantly improve token representations and achieves state - of - the - art performance ( Yu et al , 2020 ; Luoma and Pyysalo , 2020 ; Yamada et al , 2020 ; Wang et al , 2021a ) .
However , the lack of context in the MultiCoNER datasets means the embeddings can not take advantage of long - range dependencies for entity disambiguation .
Recently , Wang et al ( 2021b ) use Google search to retrieve external contexts of the input sentence and successfully achieve state - of - the - art performance across multiple domains .
We adopt this idea so that the embeddings can utilize the related knowledge by taking the advantage of long - range dependencies to form stronger token representations .
Comparing with Wang et al ( 2021b ) , we build the local KB based on Wikipedia because the KB matches the indomain data of the shared task and is fast enough to meet the time requirement in the test phase 2 .
Fine - tuning pretrained contextual embeddings is a useful and effective approach to many NLP tasks .
Recently , some of the research efforts propose to further train the fine - tuned embeddings with specific training data or in a larger model architecture to improve model performance .
Shi and Lee ( 2021 ) proposed two - stage fine - tuning , which first trains a general multilingual Enhanced Universal Dependency ( Bouma et al , 2021 ) parser and then finetunes on each specific language separately .
Wang et al ( 2021a ) proposed to train models through concatenating fine - tuned embeddings .
We extend these ideas as multi - stage fine - tuning , which improves the accuracy of monolingual models that use finetuned multilingual embeddings as initialization in training .
Moreover , multi - stage fine - tuning can accelerate the training process in system building .
We introduce how our knowledge - based NER system works in this section .
Given a sentence of n tokens x = { x 1 , , x n } , the sentence is fed into our knowledge retrieval module .
The knowledge retrieval module takes the sentence as the query and retrieves top - k related paragraphs in KB .
The system then concatenates the input sentence and the related paragraphs together and feeds the concatenated sequence into the embeddings .
The output token representations of the input sentence are fed into a linear - chain conditional random field ( CRF ) ( Lafferty et al , 2001 ) layer and the CRF layer produces the label predictions .
Given the label predictions of multiple NER models with different random seeds , the ensemble module uses a voting strategy to decide the final predictionŝ y = { ŷ 1 , , ŷ n } of the sentence .
The architecture of our framework is shown in Figure 2 .
Retrieval - augmented context is effective for named entity recognition tasks ( Wang et al , 2021b ) , as external relevant contexts can provide auxiliary information for disambiguating complex named entities .
We construct multilingual KBs based on Wikipedia pages of the 11 languages , and then retrieve relevant documents by using the input sentence as a query .
These retrieved documents act as contexts and are fed into the NER module .
To enhance the retrieval quality , we further designed an iterative retrieval approach , which incorporates predicted entities of NER models into the search query . 
Knowledge Base Building Wikipedia is an evolving source of knowledge that can facilitate many NLP tasks ( Chen et al , 2017 ; Verlinden et al , 2021 ) .
Wikipedia provides a rich collection of mention hyperlinks ( referred to as wiki anchors ) .
For example , in the sentence " Steve Jobs founded Apple " , entities " Steve Jobs " and " Apple " are linked to the wiki entries Steve_Jobs and Apple_Inc respectively .
For the NER task , these anchors provide useful clues on where the entities are to the model .
Based on Wikipedia we can build local Wikipedia search engines to retrieve the relevant context of the input sentences for each language . 
We download the latest ( 2021 - 12 - 20 ) version of the Wikipedia dump from Wikimedia 3 and convert it to plain texts .
Then we use ElasticSearch ( ES ) 4 to index them .
ElasticSearch is document - oriented , and the document is the least searchable unit .
We define the document in our local Wikipedia search engines with three fields : sentence , paragraph and title .
We create inverted indexes on both the sentence field and the title field .
The former is used as a sentence - level full - text retrieval field , while the latter indicates the core entity described by the wiki page and can be used as an entity - level retrieval field .
The paragraph field stores the contexts of the sentence .
To take advantage of the rich wiki anchors in Wikipedia paragraphs , we marked them with special markers .
For example , to incorporate the hyperlinks [ Apple Apple Inc ] and [ Steve Jobs Steve Jobs ] to the paragraph , we transformed " Steve Jobs founded Apple " into " < e : Steve Jobs > Steve Jobs</e > founded < e : Apple_inc > Apple</e > " 5 . 
Sentence Retrieval Retrieval at the sentence level takes the input sentence as a query and retrieves the top - k documents on the sentence field .
Given an input sentence , we select the corresponding search engine according to the language of the sentence . 
Iterative Entity Retrieval
The core of the NER task lies in the entities , while retrieval at the sentence level overlooks the key entities in the sentences .
For this reason , we consider the relevance of the entities in the sentence to the title field in the documents during retrieval .
We concatenate the entities in the sentences with " | " and then retrieve them on the title field .
On the training and development sets , we utilize the ground - truth entities directly .
On the test set , we first perform the sentence retrieval and then use the entity mentions 6 predicted by the model for entity retrieval .
This bootstrapping manner can be applied for T turns . 
Context Processing After top - k results from the KB are retrieved , the system post - processes the retrieved documents into the contexts of the input sentence .
There are three options of utilizing the texts in the documents , which are : 1 ) use the matched paragraph ; 2 ) use the matched sentence ; 3 ) use the matched sentence but remove the wiki anchors .
We compare the performance of each option in section 5.4 .
In each retrieved document , we concatenate the title and texts together to form the contextx i .
The results are then concatenated into { x 1 , , x k } based on the retrieval ranking .
In our system , we use XLM - R large as the embedding for all the tracks .
It is a multilingual model and is applicable to all tracks .
Given the input sentence x and the retrieved contexts { x 1 , , x k } , we add the separator token ( i.e. , " < /s > " in XLM - R ) between them and concatenated them together to form the inputx of the NER module .
We chunk retrieved texts to avoid the amount of subtoken in the sequence exceeding the maximum subtoken length in XLM - R ( i.e. , 512 in XLM - R ) . 
Our system regards the NER task as a sequence labeling problem .
The embedding layer in the NER module encode the concatenated sequencẽ x and output the corresponding token representa - tions { v 1 , ,
v n , } . 
The module then feeds the token representations { v 1 , , v n } of the input sentence into a linear - chain CRF layer to obtain the conditional probability p θ ( y | x ) : ψ ( y ′ , y , v i )
= exp ( W T
y
v i
+ b y ′ , y )
( 1 ) p θ ( y | x ) =
n i=1 ψ
( y i−1 , y i ,
v i )
y ′
Y ( x ) n i=1 ψ
( y ′ i−1 , y ′
i , v i ) where θ represents the model parameters and Y ( x ) denotes the set of all possible label sequences given x.
In the potential function ψ
( y ′ , y , v i ) , W T
y
v i is the emission score and b y ′ , y is the transition score , where W T R t×d and b R t×t are parameters and the subscripts y ′
and y are the indices of the matrices .
During training , the negative log - likelihood loss L NLL ( θ ) = − log p
θ ( y * | x ) for the concatenated input sequence with gold labels
y * is used .
During inference , the model predictionŷ θ is given by Viterbi decoding .
Given predictions { ŷ θ 1 , , ŷ θm } from m models with different random seeds , we use majority voting to generate the final predictionŷ .
We convert the label sequences into entity spans to perform majority voting .
Following Yamada et al ( 2020 ) , the module ranks all spans in the predictions by the number of votes in descending order and selects the spans with more than 50 % votes into the final prediction .
The spans with more votes are kept if the selected spans have overlaps and the longer spans are kept if the spans have the same votes .
( Nguyen et al , 2016 ) containing a lot of natural language questions ; ORCAS ( Search Query NER ) contains user queries from Microsoft Bing ( Craswell et al , 2020 ) .
The MSQ and ORCAS samples are taken as out - ofdomain data in the shared task .
The training and development sets only contain a small collection of samples of these two domains and mainly contain data from the LOWNER domain .
The test set , however , contains much more MSQ and ORCAS samples to assess the out - of - domain performance . 
The results of the shared task are evaluated with the entity - level macro F1 scores , which treat all the labels equally .
In comparison , most of the publicly available NER datasets ( e.g. , CoNLL 2002CoNLL , 2003 are evaluated with the entity - level micro F1 scores , which emphasize common labels .
NER Model Training Before building the final system , we compare a lot of variants of the system .
We train these variant models on the training set for 3 times each with different random seeds and compare the averaged performance of the models .
According to the dataset sizes , we train the models for 5 epochs , 10 epochs and 100 epochs for multilingual , monolingual and code - mixed models respectively .
Our final NER models are trained on the combined dataset including both the training and development sets on each track to fully utilize the labeled data .
For models trained on the training set , we use the best macro F1 on the development set during training to select the best model checkpoint .
For models trained on the combined dataset , Continue Pretraining To make XLM - R learn the data distribution of the shared task , we combine the training and development sets on the monolingual tracks to build a corpus to continue pretrain
XLM - R.
Specifically , we collocate all sentences according to their languages , then cut the text into chunks of fixed length , and train the model on these text chunks using the Masked Language Modeling objective .
We continue pretrain XLM - R for 5 epochs .
We use the continue pretrained XLM - R model as the initialization of the multilingual 7 Please refer to Appendix A for detailed settings .
models during training .
In this section , we use language codes 8 to represent languages , and use MULTI and MIX to represent multilingual and code - mixed tracks respectively 9 .
There are 55 teams that participated in the shared task .
Due to limited space , we only compare our system with the systems from teams USTC - NELSLIP , RACAI and Sliced 10 .
In the postevaluation phase , we evaluate a baseline system without using the knowledge retrieval module to further show the effectiveness of our knowledgebased system .
The official results and the results of our baseline system are shown in Table 1 .
Our system performs the best on 10 out of 13 tracks and is competitive on the other 3 tracks .
Moreover , our system outperforms our baseline by 14.39 F1 on average , which shows the knowledge retrieval module is extremely helpful for disambiguating complex entities leading to significant improvement on model performance .
To further show the effectiveness of our knowledgebased system , we show the relative improvements of our system over our baseline system on each domain in Table 2 .
We observe that in most of the cases , the two out - of - domain test sets have more relative improvements than the in - domain test set .
This observation shows that the knowledge from Wikipedia can not only improve the performance of the LOWNER domain which is the same domain as the KB , but also has very strong cross - domain Table 2 : Per - domain macro F1 score on the test set of our system and our baseline system for each language .
∆ represents the relative improvements of our system over the baseline system . transferability to other domains such as web questions and user queries .
According to the baseline performance over the three domains , the ORCAS domain has the lowest score , which shows the challenges in recognizing named entities in user queries .
However , our retrieved documents in KB can significantly ease the challenges in this domain and results in the highest improvement out of the three domains .
To evaluate the relevance of the retrieval results to the query , we define a character - level relevance metric , which calculates the Intersectionover - Union ( IoU ) between the characters of query and result .
Assuming that the character sets 11 of query and retrieval result are A and B respectively , then the character - level IoU is A∩B A∪B .
We calculate the character - level IoU of the sentence and its top - 1 retrieval result on all tracks , and plot its distribution on the training , development and test set in Figure 3 .
We have the following observations : 1 ) the IoU values are concentrated around 1.0 on the training and development sets of EN , ES , NL , RU , TR , KO , FA , which indicates that most of the samples were derived from Wikipedia .
Therefore , by retrieving , we can obtain the original documents for these samples .
2 ) the distribution of data on the test set is consistent with the training and development sets for most languages , except for TR .
On TR , the character - level IoU values of the samples and query results cluster at around 0.5 .
We hypothesize that this is because the source of the test set for TR is different from the training set .
However , the model still performs strongly on this language , suggesting that the model can mitigate the difficulties caused 11
The sets take repeat characters as different characters . 
by inconsistent data distribution by retrieving the context from Wikipedia .
We compare several types of KBs and contexts during our system building . 
Online Search Engine In the early stage , we tried to use the knowledge retrieved from Google Search , which can retrieve related knowledge from a large scale of webs and is believed to be a strong multilingual search engine .
As we mentioned in Section 3.1 , there are three context processing options , which are : 1 ) use the matched paragraph ; 2 ) use the matched sentence ; 3 ) use the matched sentence but remove the wiki anchors .
We denote the three options as PARA , SENT and SENT - LINK respectively . 
Entity Retrieval with Gold Entities We use gold entities on the development set to see whether the model performance can be improved .
This can be seen as the most ideal scenario for iterative retrieval .
We denote this process as ITER G and use PARA for the context type . 
In Table 3 , we can observe that : 1 )
For the three context options , PARA is the best option for EN , ES , NL , RU , TR , KO , FA , MIX and MULTI .
SENT - LINK is the best option for HI and BN .
For DE and ZH , SENT and SENT - LINK are competitive .
As a result , we choose SENT for the two languages since we believe the wiki anchors from the Wikipedia can help model performance ; 2 ) Comparing with the baseline , the knowledge from Google Search can improve model performance .
Based on the best context option of each track , the knowledge from Wikipedia is better than the online search engine ; 3 ) For ITER G , we can find that the context can further Iterative Entity Retrieval with Predicted Entities Based on the results in Table 3 , we further analyze how the predicted entity mentions can improve the retrieval quality .
We denote the iterative entity retrieval with predicted mentions as ITER P . 
In the experiment , we set T = 2 . 12
We extract the predicted mentions of the development sets from the models based on the best context option for each track .
We conduct the experiments over HI , BN and MIX which have significant improvement with ITER G .
In Table 4 , we also list the performance of ITER G for reference , which can be seen as using the predicted mentions with 100 % accuracy .
From the results , we observe that only MIX can be improved .
Since iterative entity retrieval uses predicted mentions as a part of retrieval query , the performance of mention detection directly affects the retrieval quality .
To further analyze the observation in Table 4 , we evaluate the mention F1 score of the NER models with sentence retrieval .
For comparison with mention detection performance of NER models , we additionally train mention detection models by discarding the entity labels during training .
From the results in Table 5 , we suspect the low mention F1 introduces noises in the knowledge retrieval module for BN and HI , which lead to the decline of performance as shown in Table 4 .
Moreover , the mention F1 of mention detection models ( second row of Table 5 ) only outperform that of the NER models ( first row of Table 5 ) in a moderate scale .
Therefore , we train the ITER models only for the code - mixed track and use the NER models with sentence retrieval to predict mentions .
Table 6 shows the speed of each module in our system .
In the table , we also show that the retrieval speed of our local KB is significantly faster than that of Google Search .
The bottleneck of the system speed is the NER module rather than the knowledge retrieval module .
The main reason for the slow speed of the NER module is that the input length of the knowledge - based system is significantly longer than the original input .
Taking the EN test set as an example , there are on average 10 tokens for each input sentence in the original test set while there are 218 tokens for the input of our knowledge - based system .
The longer inputs slow down the encoding at XLM - R embeddings .
We compare with some variants of our system that we designed but did not use in the test phase .
 ) CE is one of the usual approaches to NER , which concatenates different kinds of embeddings to improve the token representations .
In the early stage of our system building , we compare CE with only using the XLM - R embeddings based on the knowledge retrieved from the Google Search .
Results in Table 7 show that CE models are stronger than the models using XLM - R embeddings only in all the cases , which show the effectiveness of CE . 
ACE ( Automated Concatenation of Embeddings ) ACE ( Wang et al , 2021a ) is an improved version of CE which automatically selects a better concatenation of the embeddings .
We use the same embedding types as CE and the knowledge are from our Wikipedia KB .
We experiment on EN , ES , NL , RU , TR , KO and FA , which are strong with PARA contexts .
In Table 9 , we further compare ACE with ensemble XLM - R models .
Results show ACE can improve the model performance and even outperform the ensemble models 13 . 
The results in Table 7 and 9 show the advantage of the embedding concatenation .
However , as we have shown in Section 5.5 , the prediction speed is quite slow with the single XLM - R embeddings .
The CE models further slow down the prediction speed since the models contain more embeddings .
The ACE models usually have faster prediction speed than the CE models .
However , training the ACE models is quite slow .
It takes about four days to train a single ACE model .
Moreover , the ACE models can not use the development set to train the model since they use development score as the reward to select the embedding concatenations .
Therefore , due to the time constraints , we did not use these two variants in our submission during the shared task period .
In Table 8 , we show the effectiveness of multistage fine - tuning on the development set for our baseline system .
The result shows that multi - stage fine - tuning can significantly improve the model performance for all the tracks .
In this paper , we describe our knowledge - based system for the MultiCoNER shared task , which wins 10 out of 13 tracks in the shared task .
We construct multilingual KBs and retrieve the related documents from KBs to enhance the token representations of input text .
We show that the NER models can use the retrieved knowledge to facilitate complex entity prediction , significantly improving both the in - domain and out - of - domain performance .
Multi - stage fine - tuning can help the monolingual models learn from the training data of all the languages and improve the model performance and training efficiency .
We also show that the system presents a good balance between the model performance and prediction efficiency to meet the time requirement in the test phase .
We believe this system can be widely applied to other domains for the task of NER .
For future work , we plan to improve the retrieval quality and adopt the system to support other kinds of entity - related tasks .
For the knowledge retrieval module , we retrieve top - 10 related results from the KB .
For iterative entity retrieval , we set T = 2 .
In masked language model pretraining , we use a learning rate of 5 × 10 −5 .
For the NER module , we use a learning rate of 5 × 10 −6 for fine - tuning the XLM - R embeddings and use a learning rate of 0.05 to update the parameters in the CRF layer following Wang et al ( 2021b ) .
Each NER model built by our system can be trained and evaluated on a single Tesla V100 GPU with 16 GB memory .
For the ensemble module , we train about 10 models for each track . 
A. ( Akbik et al , 2018 ) , ELMo embeddings ( Peters et al , 2018 ; Che et al , 2018 ) , XLM - R embeddings fine - tuned on the whole training data and XLM - R embeddings fine - tuned on the language data by multi - stage finetuning .
We only feed the knowledge - based input into XLM - R embeddings and feed the original input into other embeddings because it is hard for the other embeddings ( especially for LSTM - based embeddings such as Flair and ELMo ) to encode such a long input .
We use Bi - LSTM encoder to encode the concatenated embeddings with a hidden state of 1 , 000 and then feed the output token representations into the CRF layer .
Following most of the previous efforts , we use SGD optimizer with a learning rate of 0.01 .
For ACE , we search the embedding concatenation for 30 episodes .
As we state in Section 3.3 , we use majority voting as the ensemble algorithm in our system .
We show an experiment about how the voting threshold affect the ensemble model performance during our system building on the development set .
We ensemble the models on DE , ZH , HI , BN , MIX with PARA since these five tracks have relatively lower performance than the other 7 tracks .
In Figure 4 , we show how the threshold of the majority voting affects the model performance .
From the figure , we can see that the best threshold varies over the language .
Therefore , we simply choose 0.5 as there is no best threshold value .
Moreover , we compare the majority voting ensemble and CRF level ensemble in Table 12 .
The CRF level ensemble averages the emission and transition scores in the Eq . 1 predicted by the candidate models and uses the Viterbi algorithm to get the prediction .
The results show that CRF level ensemble performs inferior to the majority voting ensemble .
The possible reason is that training with different random seeds may lead to different emission transition scores at different Avg . scales .
As a result , the models with larger scales have higher weights in the ensemble .
Performance ? In the multilingual test set , we can find 304 , 905 sentences in the other monolingual test sets while there are 167 , 006 sentences that can not be found .
For these sentences , we can either search on the whole KB of all languages or first detect the language of the input sentence and then search in the specific language KB 14 .
Moreover , as we discussed in Section 5.4 , using different kinds of retrieved knowledge affects the model performance .
As a result , we train two types of multilingual models .
One is only using the PARA contexts for all language and another is using the best option for each language based on Table 3 .
From the results in Table 13 , we can observe that : 1 ) searching over the language specific KB performs better than searching the whole KB , 2 ) using the language specific context option can not improve the model performance .
Therefore , we ensemble both types of the model for the final submission .
This work was supported by Alibaba Group through Alibaba Innovative Research Program .
The detailed statistics of the MultiCoNER dataset are listed in Table 10 and the statistics of our KBs ares shown in Table 11 .
