IST - Unbabel 2021 Submission for the Quality Estimation Shared Task
We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation . Our team participated on two tasks : Direct Assessment and Post - Editing Effort , encompassing a total of 35 submissions . For all submissions , our efforts focused on training multilingual models on top of OpenKiwi predictor - estimator architecture , using pre - trained multilingual encoders combined with adapters . We further experiment with and uncertainty - related objectives and features as well as training on out - ofdomain direct assessment data .
Quality estimation ( QE ) is the task of evaluating a translation system 's quality without access to reference translations ( Blatz et al , 2004 ; Specia et al , 2018 ) . This paper describes the joint contribution of Instituto Superior Técnico ( IST ) and Unbabel to the WMT21 Quality Estimation shared task ( Specia et al , 2021 ) , where systems were submitted to two tasks : 1 ) sentence - level direct assessment ; 2 ) word - and sentence - level post - editing effort . This year 's submission combines several ideas built on top of the OpenKiwi framework . Motivated by the mixture of blind and seen language pairs in the test sets , we experimented with extensions that would allow us to train multilingual models that maintain good generalization ability and are robust to the presence of epistemic and aleatoric uncertainty . For both tasks we trained and submitted an ensemble of multilingual models . All submitted models follow the predictor - estimator architecture ( Kim and Lee , 2016 ; Kim et al , 2017 ) and use pretrained models for feature extraction . Also , we fine - tune all models on the provided QE data using stacked adapter layers ( Pfeiffer et al , 2020 ) . * The first three authors have equal contribution . We show that we can thus achieve comparable performance across language pairs while minimising the number of trainable parameters ( see Table 1 ) . Furthermore , we experimented with different types of uncertainty - related information to leverage it 's benefits , improving performance and robustness of the submitted systems ( see 3.1.1 ) . All related code extensions will be publicly available . Our main contributions are : We build on our OpenKiwi architecture by exploring adapter layers ( Houlsby et al , 2019 ; Pfeiffer et al , 2020 ) for quality estimation as these demonstrated to be less amenable to overfitting while presenting the same or superior quality performance than fine - tuning the whole base pre - trained model for different NLP tasks ( He et al , 2021 ) . We incorporate different types of uncertainty into our architectures . We make use of the glass - box features ( Fomicheva et al , 2020 ) extracted from the NMT models , the aleatoric ( data ) uncertainty derived from the human annotations and the epistemic ( model ) uncertainty ( Hora , 1996 ; Kiureghian and Ditlevsen , 2009 ; Huellermeier and Waegeman , 2021 ) that originates from the QE model . We show that training the QE models on additional out - of - domain direct assessment ( DA ) data gives considerable gains in performance for the new language pairs from the blind test sets .
In this year 's shared task edition we submitted models for the first two tasks : 1 . Task 1 : sentence - level direct assessment 2 . Task 2 : word - and sentence level post - editing effort , comprising of two subtasks : a ) predicting the HTER score of the translated sentence ( hypothesis ) ; and b ) predicting OK / BAD tags for the words and gaps ( both in source and translation ) We note that this year , both tasks 1 and 2 provided additional blind test sets with language pairs that were not included in the data made available for training / development , providing an interesting challenge and motivating multilingual and generalisable approaches . 3 Implemented Systems
For Task 1 our final submission consisted of an ensemble of two different multilingual models , that differ in the way they process the input source ( original sentence ) and hypothesis ( machine translation ) . Both models are based on the predictor - estimator architecture , using different pre - trained models to extract features and different training approaches to optimise for the QE task . The key idea explored with our first model ( denoted by M1 variations in the experiments ) , revolved around pursuing highly generalisable multilingual models , robust to overfitting . To this end , we train a cross - lingual transformer ( XLM - RoBERTa ( Conneau et al , 2020 ) ) on large , multilingual data with direct assessments and then use adapters ( Houlsby et al , 2019 ; Pfeiffer et al , 2020 ) to adapt to the domain specific data of the QE task with minimal training effort . In line with our efforts for good generalisation , we use only task - specific adapters and refrain from using specific adapters for each language pair . For these experiments we build on the OpenKiwi architecture ( Kepler et al , 2019 ) , using a pre - trained xlm - roberta - large encoder as a feature predictor . The source and hypothesis sentences are jointly encoded with hypothesis first . Then , source and hypothesis features are generated using average pooling over the hypothesis embeddings and forwarded to the estimator module which corresponds to a feed - forward layer . Figure 1 provides the general architecture 1 The model was first trained on the direct assessment data provided in the Metrics shared tasks ( Mathur et al , 2020 ) , as described in 3.1.2 . Upon training , the XML - R encoder is frozen and the the model is fine - tuned on sentence regression with the task - specific data , using stacked adapters . We hence manage to maintain a low number of trainable parameters during fine - tuning and minimize training time while learning to predict task - specific sentence scores . For the second model ( denoted by M2 - KL - G - MCD ) we aimed to explore the potential of a large pre - trained multilingual model ( trained with MT objectives ) . We use the mBART ( Liu et al , 2020 ) encoder - decoder architecture to encode the source and force - decode the hypothesis . We specifically use the mBART50 model ( Tang et al , 2020 ) which is trained with multilingual finetuning on 50 languages , including all languages of interest for the QE 2021 task . We obtain the features by averaging the decoder embeddings and concatenating with the < eos > token of the sequence . The estimator part of the model consists of a bottleneck feed - forward layer that reduces the dimensionality of the decoder output , and is concatenated with a vector with additional glass - box features from the NMT models ( see 3.1.1 ) . The combined vector is then forwarded to a feed - forward estimator and the full model is fine - tuned on the task specific QE data . Apart from the glass - box features we experimented further with methods that allow the model to be more robust towards the underlying uncertainty of its predictions . We elaborate that in the next section . Figure 2 provides a general architecture of the M2 model variations .
Multiple neural models are involved in the process of obtaining and scoring machine translations , which naturally leads to several sources of uncertainty . These sources can be very informative and useful for MT evaluation . In this work we try to consider three types of uncertainty : ( 1 ) uncertainty of the NMT models used to obtain the hypotheses , ( 2 ) data ( aleatoric ) uncertainty for which we use the inter - annotator disagreement as a proxy , and ( 3 ) uncertainty of the MT evaluation model itself . NMT model uncertainty The idea of extracting uncertainty - related features from the MT systems in order to estimate the quality of their predictions , was originally introduced by Fomicheva et al ( 2020 ) . This glass - box approach to QE is mostly focusing on capturing epistemic uncertainty , and the proposed features are extracted either using Monte Carlo ( MC ) dropout on the NMT or using the output probability distributions obtained from a standard deterministic MT system . In our last year 's submission ( Moura et al , 2020 ) the integration of such features proved to be effective , thus we decided to incorporate it into our new model as well . We list the extracted features below : TP sentence average of word translation probability - of MT output generated in different stochastic passes .
The noise and complexity of the training data is a source of predictive uncertainty in itself , referred to as data or aleatoric uncertainty ( Kiureghian and Ditlevsen , 2009 ) . This uncertainty is often reflected in the disagreement between human annotations for the same sourcehypothesis segment ( Cohn and Specia , 2013 ; Fornaciari et al , 2021 ) . We hypothesize that the direct assessments can be better modelled as normally distributed scores rather than a single score , and that a model trained to predict this distribution ( mean and standard deviation ) could provide better quality estimates 2 . We formalise this as a KL divergence objective , using the closed form solution to estimate the KL divergence between the target distribution p ( x ) = N ( µ 1 , σ 1 ) and the predicted distribution q ( x ) = N ( µ 2 , σ 2 ) , as shown in Eq . 1 . KL ( p | | q ) = log σ 2 σ 1 + σ 2 1 + ( µ 1 − µ 2 ) 2 2σ 2 2 − 1 2 ( 1 ) where we take the mean and standard deviation ( std ) of the direct assessment z_scores as the target ( ground truth proxy ) values p. This way , we account for the annotator disagreement ( reflected in the std value ) during learning . QE epistemic uncertainty We use MC dropout ( Gal and Ghahramani , 2016 ) to account for the uncertainty of the QE model . Specifically , we enable dropout during inference and run multiple forward runs over each test instance . Thus we obtain a distribution of quality predictions for each instance instead of a single point estimate . We use the estimated mean of the distribution as our predicted quality estimate . MC dropout has been shown to improve predictive accuracy and perform on par or even better compared to deep ensembles for MT evaluation tasks ( Glushkova et al , 2021 ) . It thus allows us to simulate ensembling in a cheap and effective way , without the need to train multiple checkpoints .
The QE data is relatively limited , making it harder to train multilingual models with a large number of parameters without over - fitting . Thus , as explained in 3.1 we aimed to investigate whether we could obtain models that generalise better and are more robust to noise and out - of - distribution data by training the XLM - RoBERTa model first on a larger - yet noisier and out - of - domain dataset . To that end we leverage the data provided for the past Metrics shared tasks , which covers the language pairs used in this year 's QE task , including the blind tests for which we had no in - domain data available . Altogether , it encompasses 30 language pairs from the news domain ( versus 7 in the QE dataset ) . We provide more detailed statistics for each language pair of the Metrics data in Appendix C. We refer to experiments using the model initially trained on the Metrics data as M1 M - . We also show that using the trained XLM - RoBERTa encoder from the M1 M model can prove beneficial for the predictions on post - edited data of Task 2 ( see Table 3 ) .
For Task 2 we submitted an ensemble of two variations of the first model ( M1 - ADAPT and M1 M - ADAPT ) presented for Task 1 ( see 3.1 ) . In both cases , we use multi - task training and a feedforward for each output types : hypothesis word tags , hypothesis gap tags , source word tags , and sentence regression ( on HTER scores ) . Both variations use a pre - trained XLM - RoBERTa ( large ) encoder to extract features as described for Task 1 , but differ in the training of the encoder . In the first case we use the pre - trained model 3 and finetune on the QE data using stacked adapters . In the second variation we swap the original pre - trained model with the XLM - RoBERTa model that has been trained on the Metrics data as described in 3.1.2 . We note that the two variations favor different language pairs , hence we combine multiple checkpoints from each variation ( ranging training steps ) . We use the test - 20 split of the data to optimise the hyper - parameters and following this approach we use the estimated top - 3 checkpoints from each variation using the combined dataset 4 and the top checkpoint for the non - augmented model trained exclusively on the train set , resulting in total 7 checkpoints in our final ensemble .
We present the performance of the implemented models on the test - 20 dataset .
The results can be seen in Tables 1 and 2 . In line with the shared task guidelines we treat Pearson r as the primary performance metric and select the submitted models accordingly . We can observe , that while on average the M1 model and its variations outperform the M2 model , their performance is comparable , and M2 - KL - G - MCD can even outperform M1 M - ADAPT for specific language pairs , hence it made sense to combine them in the final ensemble . We can also see that fine - tuning the M1 model on the Metrics data , results in performance gains for the majority of the language pairs . Specifically , even applying the M1 M directly , without further fine - tuning on QE data , achieves competitive performance for most pairs , which further improves upon fine - tuning . It helps in increasing the performance on the blind sets ( denoted as zeroshot in the Appendix B assessments for each segment . Thus , the difference in target score range and distribution could affect the magnitude of predicted scores and the distance to the ground truth values , which is reflected in the MAE and RMSE metrics . These findings , further supported by the results on Task 2 , is a first step in exploring the underlying connection and bridging the gap between the Metrics and Quality Estimation shared tasks .
The results can be seen in Table 3 . Similarly to Task 1 , the primary evaluation metric for the sentence level sub - task of Task 2 is the Pearson r coefficient , 2 : Results for Task 1 with the M2 predictorestimator ( mBART ) and different uncertainty handling additions . " KL " signifies the incorporation of KL loss , " G"the incorporation of glass - box features and MCD the addition of MC dropout . ML stands for MULTILIN - GUAL , showing the performance averaged over all language pairs . Underlined numbers indicate the best result for each language pair and evaluation metric . Bold systems were selected for the final ensemble . while the word level sub - task is evaluated using the Matthews correlation coefficient ( MCC , ( Matthews , 1975 ) ) as the primary performance indicator . We can see that while HTER scores do not always correlate highly with DAs ( see Table 4 ) , the use of the M1 M model encoder that was trained on large data with direct assessments can still prove useful . Indeed , when fine - tuning on the Task2 data , the model using the M1 M encoder ( M1 M - ADAPT in the table 3 ) provides a performance boost for the Pearson correlation in most language pairs , and competitive performance for the rest . Based on these results , we deem it worthwhile to include checkpoints trained with this configuration in the ensemble estimating that they will contribute in higher performance , especially on the blind test sets . This can be further confirmed when
We presented a joint contribution of IST and Unbabel to the WMT 2021 QE shared task . Our submissions are ensembles of multilingual checkpoints extending the OpenKiwi framework . We found adapter - tuning to be suitable for fine - tuning OpenKiwi on the QE tasks data and less prone to overfitting . We showed that pre - training on large , out - of - domain annotated data can prove beneficial both for the direct assessment and the postediting QE tasks . We also demonstrated that handling uncertainty - related sources of information improves the performance when integrated into the QE system . For Task 2 we do multi - task training based on the models from the previous task and use multiple checkpoints to create the submitted ensemble .
In
In Table 6 is an excerpt of the training configuration used for training the M2 models using the mBART encoder - decoder :
We present the performance of the submitted ensembles on the TEST - 21 dataset as calculated in the official QE results 6 for each task and sub - task . We also provide the comparison with the organisers ' baseline . The results for Task1 on TEST - 21 are presented in Table 7 .
The results for Task2 on TEST - 21TEST - 21 are presented in Table 8 , showing the performance for the sentence level , HTER score predictions .
The results for Task2 on TEST - 21 are presented in Table 9 , showing the performance for the word tag predictions .
We present below ( Tables 10 and 11 ) the statistics on the Metrics data used to train the M1 M model on direct assessments .
We are grateful to Alon Lavie and Craig Stewart for their valuable feedback and discussions . This work was supported by the P2020 programs MAIA ( contract 045909 ) and Unbabel4EU ( contract 042671 ) , by the European Research Council ( ERC StG Deep - SPIN 758969 ) , and by the Fundação para a Ciência e Tecnologia through contract UIDB/50008/2020 .
