Partner Personas Generation for Dialogue Response Generation
Incorporating personas information allows diverse and engaging responses in dialogue response generation .
Unfortunately , prior works have primarily focused on self personas and have overlooked the value of partner personas .
Moreover , in practical applications , the availability of the gold partner personas is often not the case .
This paper attempts to tackle these issues by offering a novel framework that leverages automatic partner personas generation to enhance the succeeding dialogue response generation .
Our framework employs reinforcement learning with a dedicatedly designed critic network for reward judgement .
Experimental results from automatic and human evaluations indicate that our framework is capable of generating relevant , interesting , coherent and informative partner personas , even compared to the ground truth partner personas .
This enhances the succeeding dialogue response generation , which surpasses our competitive baselines that condition on the ground truth partner personas .
Building informative and engaging dialogue agents Roller et al , 2021 ) is a popular research direction within the area of natural language processing .
For the sake of engagement , diverse and consistent responses ( Song et al , 2020 ( Song et al , , 2021 are important factors , and personas information gives rise to both .
There are two types of personas , namely self persona and partner persona .
The former refers to a self profile consisting of several sentences representing the dialogue agents .
Such a persona allows producing consistent responses rather than solely relying on the personas that are randomly learned and embedded in the model parameters ( Kim et al , 2020 ) .
The latter refers to a profile that represents the users .
Leveraging such partner personas has been empirically shown to be helpful for dialogue response selection ( Gu et al , 2021 ) . 
Unfortunately , the existence of partner personas suffers from the cold start ( Schein et al , 2002 ;
Zhang et al , 2014 ; at the beginning of the conversation .
Most of the works , if not all , ( Li et al , 2016b ; Mazaré et al , 2018 ; Gu et al , 2019 ; Zhao et al , 2019 ; Madotto et al , 2019 ; Majumder et al , 2020 ; Wu et al , 2020a ; Song et al , 2020 ) have been either overlooking partner personas or simply focusing on the impractical situation where partner personas guarantee to exist .
In contrast , our work does not suffer from the practical issue when partner personas are missing during inference , and our proposed framework surpasses the baseline that conditions on the ground truth partner personas . 
To our knowledge , this is the first attempt to formulate partner personas generation for improved performance on the downstream dialogue response generation .
Our work is motivated by the underlying hypothesis that partner personas generation is plausible given the self personas and dialogue context .
Automatic and human evaluation results support the hypothesis and indicate that generated personas are even more interesting than the ground truth , which improves the downstream dialogue response generation .
This paper thus paves the way to exploit partner personas generation ( PPG ) for dialogue response generation ( DRG ) . 
We propose a novel framework composed of three major components , namely a personas generator , a dialogue response generator and a critic network .
The personas generator generates partner personas , which the dialogue response generator conditions on .
We employ reinforcement learning with a critic network that propagates the reward back to the generators for joint training . 
Prior works have investigated partner persona retrieval .
The human - constructed ground truth personas serve as the upper bound for such retrieval - based systems , and we argue that the ground truth is not coherent and diverse enough .
Interestingly , we observe that the generative counterpart proposed in our framework generates relevant , informative and coherent partner personas , which further improves the succeeding dialogue response generation .
It follows another advantage that our framework does not need an external database to retrieve from ( Madotto et al , 2020 ; . 
One close work to ours is a multi - task framework for meta - learning ( Lee et al , 2021 ) that uses personas reconstruction as an auxiliary task to improve response consistency .
The differences are that theirs does not differentiate between self personas and partner personas , while ours does .
Theirs indicates an improvement over personality consistency , while ours report improvements for the overall quality .
We conduct an empirical comparison with their model by reconstructing the partner personas .
Experimental results indicate that such a multi - task model does not work well in our problem setting .
Very recently , formulates personas generation as a Seq2Seq task for improved downstream response generation via multi - task learning .
In contrast , our work leverages reinforcement learning to jointly train the partner personas generator and the response generator . 
Automatic and human evaluation results indicate that our framework can generate partner personas that are more diverse and interesting than the ground truth partner personas and generate more diverse and engaging responses than the baseline conditioned on ground truth partner personas .
1 2 Related Work
Conditioning on personas helps to produce informative and engaging responses .
The most wellknown multi - turn dialogue dataset conditioned on personal profiles is PERSONACHAT , in which two crowdsourcers converse and find more about each other .
The community has proposed many methods to better utilize self personas .
Mazaré et al ( 2018 ) employs a pre - training stage based on dedicatedly extracted large - scale persona - based dialogues .
Zhao et al ( 2019 ) fuses information in personas and dialogue context into individual contextualized representations by attending to different parts of both .
Gu et al ( 2019 ) exploits the interaction between personas , dialogue context and response to improve retrieval - based dialogue agents .
Madotto et al ( 2019 ) leverages meta - learning with several dialogues of the current speakers to enhance response personality .
Welleck et al ( 2019 ) releases a dataset for measuring dialogue consistency .
Song et al ( 2020 ) employs a multi - stage pipeline to improve response personality by response rewriting .
Lee et al ( 2021 ) uses multi - task learning for improved personality consistency in the meta - learning scenario .
Gu et al ( 2021 ) employs four different strategies for personas fusing to leverage both self persona .
However , most of these works focus on exploiting self personas rather than partner personas , and they assume the existance of the gold partner personas .
Li et al ( 2014 ) leverages distant supervision to classify the spouse , education and job information from user twitters .
Wu et al ( 2020b ) proposes a twostaged profile extractor that extracts attributes before extracting the underlying relationship .
proposes to categorize the profile extraction task into two different difficulties , namely ' extraction ' and ' inference ' , and they leverage a GPT - based generator to extract user profiles .
These works have formulated user profile extraction as a classification task that conditions on an input sentence , and they aim at better profile extraction .
In contrast , we propose to formulate personas generation to be conditioned dialogue input to be jointly trained with response generation .
While ground truth personas serve as the upper bound for these user profile extractors , we empirically demonstrate that our reinforcement learning algorithm surpasses the response model conditioned on the ground truth partner personas .
As supported by our human evaluation , we believe the underlying reason is that our model can leverage pre - trained generators to generate coherent and relevant partner personas .
Reinforcement learning ( RL ) , or specifically , policy gradient methods ( Williams , 1992 ) , have been frequently adopted to both task - oriented dialogue agents ( Roman Roman et al , 2020 ; Deng et al , 2021 ) or open - domain chitchat agents ( Li et al , 2016c ; Saleh et al , 2020 ) .
It can either propagate non - differentiable loss ( Cai et al , 2019a ) or optimize an expert reward such as ease of answering ( Li et al , 2016c ) .
It also adopts a scenario where a user simulator and a dialogue agent interact , and an Figure 1 : An example of the inference flow that shows the generated partner personas and the incorporation of partner personas generation into response generation . 
Figure 2 : The illustrated reinforcement learning strategy that directly backpropagates the response - related rewards from the critic network to the partner personas generator and the dialogue response generator .
expert reward function can be defined to assign the goodness to each response generated ( Roman Roman et al , 2020 ) .
We propose a novel framework composed of three major components , namely a partner personas generator , a dialogue response generator and a reinforcement learning component with a critic network .
Figure 1 depicts the inference flow of our setting .
The input dialogue context with self persona is first fed into the partner personas generator . 
The generated partner personas output is then concatenated with the dialogue context and the self personas as the input into the dialogue response generator .
In the beginning , we train our partner personas generator and dialogue response generator under supervised learning .
In the training stage , we use the ground truth partner personas to train the dialogue response generator , and we replace it with generated partner personas in the inference stage .
After the supervised learning stage , the second stage is a reinforcement learning stage which jointly optimizes both partner personas generator and dialogue response generator as depicted in Figure 2 to train the partner personas generator under the reward signal that is relevant to dialogue response generation as well as fine - tuning dialogue response generator trained on the generated partner personas .
2
Particularly , we employ a dedicatedly designed critic network that receives generated partner personas and generated dialogue responses as the input and output a reward that measures the relevance between the generated personas and responses and propagates back to the generators .
A Seq2Seq neural network ( Sutskever et al , 2014 ) is adopted as our partner personas generator for the task of partner personas generation ( PPG ) .
The concatenation of dialogue context c and self personas s is fed as an input into the partner personas generator .
The personas generator then outputs an approximated partner personasp conditioned on the input that maximises the following likelihood : P ( p | s , c )
= T t=1 P ( p t | p 1 , ... , p t−1 , s , c ) , where T represents the length of the generated partner personas andp t represents the word at the position t that has been inferenced . 
For training , the ground truth partner personas p is used and we train our generator to maximise the likelihood P ( p | s , c ) .
We generate the complete partner personas profiles in an one - off shot for all the dialogue samples .
We also adopt a Seq2Seq neural network for the task of dialogue response generation ( DRG ) .
During inference , the concatenation of dialogue context c , self personas s , and generated partner personasp is fed as an input into the dialogue response generator .
The response generator then outputs a dialogue responser conditioned on the input , which maximises the conditional likelihood : P ( r | s , p , c ) . 
For training , the ground truth partner personas p and the ground truth dialogue responses r are used .
We employ a critic network to compute the reinforcement learning rewards for our generators .
We use a binary classifier as critic by extracting training instances ( s , r , L = 1 ) , 3 ( s A , r A , L = 1 )
and ( s B , r B , L = 1 ) .
Then we can derive two negative samples as : ( s A , r B , L = 0 ) and ( s B , r A , L = 0 ) .
Thereafter , we fine - tune on a binary classifier to be used as our critic in RL on the training partition by minimizing the binary cross - entropy loss : −Llog ( P ( L | s , r ) )
− ( 1−L ) log ( 1 − P ( L | s , r ) ) , where the binary label L indicates whether the response is relevant to the personas . 
We then use this classifier acting as a critic network that outputsL , conditioned on the generated partner personasp and generated responser .
The predicted binary labelL is then converted to a reward R. R is a positive reward whenL = 1 , and R is a negative reward whenL = 0 .
We empirically set the reward R for RL to { 1 , - 1 } for both PPG and DRG .
We then update our RL agents with the following gradients : 
∆θ PPG = −R ▽ θ PPG log P
( p | s , c ) 
for the partner personas generator ( PPG ) , and for the dialogue response generator ( DRG ) : ∆θ DRG = −R ▽ θ DRG log P ( r | s , p , c ) 
By formulating a reward that measures the relevance between generated partner personas and generated dialogue response , we are motivated by the following objectives : Further fine - tune the partner personas generator to generate personas that benefits the downstream dialogue response generation .
Further fine - tune the dialogue response generator trained with ground - truth partner personas to adapt to noisy partner personas generated by the partner personas generator . 
As mentioned in Section 3.1 , the first motivation is that we are generating the complete personas profile .
However , some of them can be irrelevant and unhelpful for the next - turn dialogue response generation .
It could be challenging for the partner personas generator alone to identify which personas could be helpful .
Therefore , we design such a reward to train the personas generator to learn to generate a set of personas that is more helpful for the downstream dialogue response generation . 
Our second motivation is that the dialogue response generator has not been exposed to the generated partner personas .
We would like to fine - tune the response generator to mitigate the potential traininginference discrepancy .
Experimental results indicate that our design empirically works well .
The previous work from Cai et al ( 2019a ) employed critic network for RL loss backpropagation .
The major difference is that their critic is trained in an adversarial manner ( Li et al , 2018 ) to pick up the gold response among other negative candidates .
Also , their critic network conditions only on the dialogue response but not on the generated skeleton .
In contrast , we aim for improved response generation with a classifier conditioning on both the generated personas and the generated response .
For both PPG and DRG , perplexity ( PPL ) is reported to measure the intrinsic performance with the ground truth output ( Roller et al , 2021 ) .
We adopt well - known sequence evaluation metrics weighted BLEU ( Papineni et al , 2002 ) and Fmeasure for ROUGE - L ( Lin , 2004 ) as the extrinsic evaluations .
For PPG , we also report Distinct - N with N= { 1 , 2 } to measure the response diversity ( Li et al , 2016a ;
Cai et al , 2019b ; Gao et al , 2019 ) with the ratio of distinct unigrams / bigrams against total number of unigrams / bigrams generated .
We conduct experiments on the PERSONACHAT , the most well - known multiturn dialogue dataset conditioned on personas .
We follow the train / valid / test split from the PARLAI platform ( Miller et al , 2017 ) that contains about 65 , 000/7 , 800/7 , 500 instances respectively .
Each instance contains about 8 utterances on average and about 4 traits for each of the self and partner personas .
We denote the dataset with this original personas as PERSONACHAT - ORI .
Later the original personas have been manually scrutinized by rephrasing , generalizing or specializing , which we denote as PERSONACHAT - REV .
We apply the same preprocessing operation to both datasets .
To train the critic for RL , we collected about 130 , 000 instances from the train split with equally distributed positive and negative samples .
During early experiments , we found that feeding all traits yields lower performance .
Retrieving Top - 3 relevant partner personas using BM25 ( Robertson and Walker , 1994 ) yields the best performance on the original personas . GPT - 2
This is a comparison model fine - tuned on GPT - 2 ( Radford et al , 2019 ) .
We build the same three E2E systems described above , and the best model is selected , the third one . 
TRANSFERTRANSFO
A comparison model built with a Transformer - based model pre - trained on gen - eral domain corpus , which is then fine - tuned on PERSONACHAT . 
PERCVAE
This is a comparison model that employs a memory - augmented architecture incorporated with conditional variational autoencoder that exploits persona information . 
PAML
This is a comparison model that leverages several dialogues collected from the same speaker to enhance response personality via metalearning ( Madotto et al , 2019 ) .
As the authors did not conduct experiments on the PERSONACHAT - REV and no preprocessing scripts are provided for the revised personas , we only report the results of their model on the PERSONACHAT - ORI only . 
MTL w/ Personas Reconstruction
This is a multi - task learning ( MTL ) comparison model ( Lee et al , 2021 ) trained to maximise the objective : αL PPG + ( 1 − α ) L DRG , where L PPG represents the auxiliary PPG likelihood , and L DRG represents the DRG likelihood .
α is weight tuned over the validation set , and both tasks condition on dialogue context and self personas and share the same model parameters .
We build our baselines , the partner personas generator and the dialogue response generator based on a state - of - the - art pre - trained dialogue model DIALOGPT for parameters Table 2 : Case studies that compare our framework against the baseline with the complete partner personas as well as the human response .
We present the preceding partner utterance as dialogue context , and we give the most salient ground truth partner personas ( Gold Partner ) and generated partner personas ( Generated Partner ) for clarity . initialization .
More implementation details can be found in Appendx B. The dialogue response generation results are presented in Table 1 .
Our framework with reinforcement learning attains the best over all the metrics on both PERSONACHAT - ORI and PERSONACHAT - REV .
This supports the usefulness of our framework , which generates reasonable personas and effectively enhances the succeeding dialogue response generation , through the use of RL . 
Although TRANSFERTRANSFO attains a better score on the PPL than the fine - tuned GPT - 2 , GPT - 2 have better extrinsic scores than TRANSFER - TRANSFO .
GPT - 2 also has better overall scores than the E2E baselines without the complete partner personas .
However , it is surpassed by the E2E baseline with the complete partner personas during training and inference . 
The E2E baseline with the complete ground truth partner personas attains better scores on all of the metrics than our remaining baselines .
Our framework with RL succeeds the performance of such a competitive baseline for both PERSONACHAT - ORI and PERSONACHAT - REV , indicating our proposed framework 's robustness against paraphrasal . 
The multi - task learning comparison model ( Lee et al , 2021 ) produces less promising results .
Concretely , we postulate that the nature of PPG and DRG largely differs .
The textual format of partner personas always initiates with first - person sentence starters , while dialogue responses are more general , ranging from greetings to goodbyes .
Therefore , it could be hard to capture both in a single model .
Cold start is a common problem in recommender systems ( Schein et al , 2002 ;
Zhang et al , 2014 ; .
This also applies to dialogue systems , as the partner personas are commonly missing in early turns .
We conduct an analysis on the baselines and our framework when N turns are available where N= { 1 , 2 , 3 } , using PERSONACHAT - ORI .
As demonstrated in Figure 3 , all the methods attain a better PPL when N increases , which indicates the existence of the cold start .
This is also the case for the baseline with ground truth personas , and we postulate that it fails to learn how to use partner personas during cold start due to the lack of clues .
Our framework effectively mitigates the cold start problem and attains the best among them for all N. our framework successfully recognizes that the partner is asking specifically for metallica .
It then conditions on the generated personas to generate a much more entailed response than the baseline .
The human response expresses negatively and thus seems less engaging .
In the second case , our framework recognizes that the partner has a garden .
It then talks about the garden rather than the irrelevant response from the baseline that we postulate is misled by the ' large ' adjective in the dialogue context .
The human response is potentially sarcastic if the partner is not joking , while our generation does not have such issue .
For the third case , the baseline produces a response that could be potentially offensive , which could be biased by the word ' violent ' in the dialogue context .
In contrast , our framework recognizes the identity of the partner to generate a response without such an issue .
The human response tends to raise a new topic and is less relevant .
For the fourth case , we observe that the annotator sometimes converses based on the partner profile rather than his own traits .
In this case , the annotator ( Dialogue Context ) said that he has many pets , which is not in his own traits ( Gold Partner ) .
Rather , his conversation partner expressed his passion for animals in previous dialogue contexts .
We postulate that the annotator attempted to engage the conversation by conditioning his partner personas and telling a relevant joke .
Our PPG can recognize this , which further tweaks the model output to talk about dogs and cats rather than the dog only .
These cases validate that leveraging partner personas is beneficial , and our framework can generate reasonable partner personas , which is not even in the ground truth .
Table 4 presents the quality measurements of the generated partner personas from our PPG with no RL .
We observe that our models have much higher Distinct - N scores as the number of unique words in the generated output is much higher than the ground truth test personas .
Compared to the ground truth personas that are limited sets of traits , our generator can leverage the power of pre - trained models for better diversity .
The remaining metrics also report reasonable scores , suggesting the plausbility to formulate personas generation as a Seq2Seq task .
Table 3 presents generated partner personas using PERSONACHAT - ORI .
As depicted , our PPG can generate reasonable partner personas which are relevant to the ground truth partner personas .
It sometimes gives a reasonable generation which is even not in the ground truth partner personas .
In the first case , the generator successfully identifies the partner as being an army ranger .
It then becomes rather positive than a violent person as given in the ground truth personas .
Conditioning on such positive contents can give a positive response .
In the second case , it recognizes the partner as a gym person , and imagines that the partner drinks protein and life weights , which is not in the ground truth personas .
In the third case , the generator generates coherent personas , saying that the partner would drink beer and eat food while watching football , which is also not in the ground truth .
We postulate that personas could be semantically closer to each other when they frequently co - occur in the training set .
Our PPG then tends to generate more coherent personas by learning such semantical relationship .
Since our generated personas are relevant and coherent , we postulate it as the underlying reason why our method gives a better generalization to DRG .
In contrast , as demonstrated by Table 3 , ground truth personas tend to be more like discrete collections of traits .
This could be the reason why some of our generated partner personas could beat the ground truth , which is also supported by our human evaluation in Section 5.6 .
This is a potential benefit of our approach compared to sentence - level user profile extraction ( Li et al , 2014 ; Wu et al , 2020b ; that is upper bounded by the discrete ground truth .
We present more examples in Table 8 in the Appendix .
We hired experienced annotators who have degrees relevant to English Linguistics to conduct evaluation on PERSONACHAT - ORI .
For both DRG and PPG , we present a questionnaire composed of 800 questions with randomly sampled 200 test instances to three annotators who compare model outputs under A / B testing .
As in Zou et al ( 2021 ) and ACUTE - Evals ( Li et al , 2020 ) , annotators follow the criteria which we present in Appendix D. trained under RL surpasses the E2E model that leverages both training and inference ground truth partner personas from all the aspects .
Table 6 presents the human evaluation results on PPG .
We observe that our PPG generates personas that are more coherent and interesting than the ground truth , which align with the facts observed in Section 5.4 and Section 5.5 indicating that our generated partner personas are more coherent and diverse .
We conduct an ablation study on PERSONACHAT - ORI as reported in Table 7 to present the performance of our framework when one of the components is frozen during RL .
The result indicates that our proposed framework yields the best result when both of the components are actively trained under RL .
We also notice that scaling the RL reward for either PPG or DRG by 10 leads to minor decrease in the performance .
Further scaling deteriorates the quality of response generation .
Our novel framework incorporates partner personas generation into dialogue response generation .
It effectively mitigates the problem that partner personas are not available in practical applications as well as the cold start problem during early conversation .
The experimental results with both automatic and human evaluation demonstrate that our framework generates coherent , diverse , interesting and engaging partner personas , even compared to the ground truth partner personas .
We employ reinforcement learning with a dedicatedly designed critic network that boosts the response generation by conditioning on the generated personas .
Automatic and human evaluation results indicate that our response generator surpasses our competitive baselines that condition on the ground truth partner personas .
Extensive case studies demonstrate that our framework can generate satisfying dialogue responses and partner personas .
The PERSONACHAT dataset used in this work is well - known and widely used .
In our view , there is no known ethical issue with its usage .
Large - scale pre - trained models are also employed , but they are widely known to be subject to potential problems such as generating offensiveness context .
With its use , our partner personas generator could generate unseen personas , which are also subject to potential offensive generation .
An offensiveness check can be incorporated to alleviate this problem for actual usage ( Baheti et al , 2021 ) .
For supervised phase , we set Adam ( Kingma and Ba , 2015 ) as our optimizer , with hyperparameters η = 5e−4 , β 1 = 0.9 , β 2 = 0.999 , ϵ = 1e−8 .
The models are fine - tuned for 2 epochs .
For RL phase , we set Adam as our optimizer , with η = 5e−6 , β 1 = 0.9 , β 2 = 0.999 , ϵ = 1e−8 .
We update the model parameters every 20 training instances and validate the model performance every 50 updates .
DistilBERT is used to initialize the model parameters for the critic network .
We set Adam as our optimizer , with hyperparameters η
= 5e−6 , β 1 = 0.9 , β 2 = 0.999 , ϵ = 1e−8 .
We fine - tune the critic for 1 epoch , and we freeze it empirically during RL .
All the experiments are conducted based on the TRANSFORMERS library from HUGGINGFACE ( Wolf et al , 2020 ) .
We present the progressive change of the testing perplexity for DRG and PPG on PERSONACHAT - ORI in Figure 4 . 4
We observe that they improve D Human Evaluation Criteria ( Appropriateness ) : " Who is more appropriate given the previous dialogue context ? " ( Informativeness ) : " Who is more diverse instead of null answers such as I do not know ? " ( Engagingness ) : " Who would you prefer to talk with for a long conversation ? " ( Human - likeness ) : " Which speaker do you think sounds more like a real person ? " ( Coherence ) : " Which persona contains traits that are more coherent to each other ? " ( Interestingness ) : " Which persona is more interesting and diverse ? " 
The first four are from the existing work Zou et
al , 2021 ) and we propose the last two for evaluating PPG .
We report the first four for DRG , and we report the last four for PPG .
Our work uses an off - the - shelf persona - based conversational dataset PERSONACHAT , which is collected and built by crowdsourcing to converse based on a fake set of discrete traits . 
There is no personal information and hence no ethics concern , but this might result in limited usefulness as there could be discrepancies between the collected samples and real - life conversation .
It is also more expensive to collect real data .
However , PERSONACHAT has been widely used by the community as a standard dataset .
Many well - known persona - based datasets suffer from the same problem ( Urbanek et al , 2019 ) as widely known . 
Although Mazaré et al ( 2018 ) proposed a useful method to collect large - scale persona - based dialogue datasets by extracting persona from user comments with classifiers trained on revised personas from PERSONACHAT which can improve the model performance on PERSONACHAT .
For legal reasons , they did not release this dataset at the time of writing .
Similarly , Zheng et al ( 2019 ) proposed a persona - based dialogue dataset with diversified traits , but it is not currently online readily available .
Zhong et al ( 2020 ) has followed the approach suggested by Mazaré et al ( 2018 ) to build an empathetic conversation dataset based on personas .
8 : More generated personas .
We highlight in pink for informativeness and in yellow for coherence .
However , their main focus is to investigate the impact of personas on empathetic dialogue generation .
Therefore , we choose to follow the community to investigate our method on the most well - known dataset , PERSONACHAT .
We run all our experiments on a single NVIDIA TI - TAN RTX with 24 GB GPU memory .
Fine - tuning the generators for 2 epochs as we have done on our preprocessed PERSONACHAT train split consumes about 3 - 4 hours .
Fine - tuning our critic classifier for 1 epoch consumes about 1 hour .
Our RL phase consumes about 15 hours to achieve the best validation loss before being early stopped .
We report averaged results from 3 runs for our dialogue response generation and partner personas generation results reported in Table 1 , Table 4 and Table 7 .
This research / paper was supported by the Center for Perceptual and Interactive Intelligence ( CPII ) Ltd under the Innovation and Technology Commission 's InnoHK scheme .
