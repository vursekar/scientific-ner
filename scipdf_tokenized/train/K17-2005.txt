Morphological Inflection Generation with Multi - space Variational Encoder - Decoders
This paper describes the CMU submission to shared task 1 of SIGMORPHON 2017 . The system is based on the multi - space variational encoder - decoder ( MSVED ) method of Zhou and Neubig ( 2017 ) , which employs both continuous and discrete latent variables for the variational encoder - decoder and is trained in a semi - supervised fashion . We discuss some language - specific errors and present result analysis .
In morphologically rich languages , different affixes ( i.e. prefixes , infixes , suffixes ) can be combined with the lemma to reflect various syntactic and semantic features of a word . In many areas of natural language processing ( NLP ) it is important that systems are able to correctly analyze and generate different morphological forms , including previously unseen forms . The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation ( Chahuneau et al , 2013 ) and information retrieval ( Darwish and Oard , 2007 ) . Accordingly , learning morphological reinflection patterns from labeled data is an important challenge . The Universal Morphological Reinflection task at SIGMORPHON 2017 ( Cotterell and Schütze , 2017 ) is an evaluation campaign aimed at systems that tackle the task of morphological inflection . It extends the SIGMORPHON 2016 Morphological Reinflection by conducting tasks in 52 languages instead of 10 Cotterell et al ( 2016 ) . In our system submission , we utilize multispace variational encoder - decoders ( MSVEDs ) , which are a varitional encoder - decoder with both continuous and discrete latent variables ( Zhou and Neubig , 2017 ) . The continuous latent variable is expected to reflect the lemma form of a word and the discrete variables are used to induce the desired labels of the inflected word . The whole model is trained in a semi - supervised fashion . For the supervised part we are reducing the reconstruction error of generating the inflected word given the lemma and corresponding tags . For the unsupervised part , we introduce the discrete latent variables representing the morphological tags , and train an auto - encoder over unlabeled corpora . Thus , the training objective includes both the variational lower bound on the marginal log likelihood of the observed parallel training data and the monolingual data . There are two tasks in SIGMORPHON 2017 , which are morphology inflection ( task 1 ) and paradigm completion ( task 2 ) respectively . We participated in task 1 , inflection generation , in which the goal is to output the inflected form of a lemma given a set of desired morphological tags . 1 Experimental results found that our model works relatively well on the shared task 1 without extensive tuning of hyper - parameters and languagespecific features .
In this section we will detail the multi - space variational encoder - decoder model .
In morphological reinflection , the source sequence x ( s ) consists of the characters in an inflected word ( e.g. , " played " ) , while the associated labels y ( t ) describe some linguistic features ( e.g. , y ( t ) pos = Verb , y ( t ) tense = Past ) that we hope to realize in the target . The target sequence x ( t ) is therefore the characters of the re - inflected form of the source word ( e.g. , " played " ) that satisfy the linguistic features specified by y ( t ) . For this task , each discrete variable y ( t ) k has a set of possible labels ( e.g. pos = V , pos = ADJ , etc ) and follows a multinomial distribution .
The variational autoencoder ( Kingma and Welling , 2014 ) is an efficient way to handle ( continuous ) latent variables in neural models . We describe it briefly here , and interested readers can refer to Doersch ( 2016 ) for details . The VAE learns a generative model of the probability p ( x ) of observed data x. The generative process consists of first generating a continuous latent variable z conditioned on the observed data x , which is termed as the recognition model q ( z | x ) ( encoder ) and then use this latent variable to reconstruct the observation x known as the reconstruction ( decoder ) model p ( x | z ) . VAE uses the variational inference to approximate the intractable posterior by learning a parametric posterior distribution for all observations . Th learning objective function is the variational lower bound on the marginal log likelihood of data : log p θ ( x ) ≥ E z∼q φ ( z | x ) [ log p θ ( x | z ) ] − KL ( q φ ( z | x ) | | p ( z ) ) ( 1 ) To optimize the parameters with gradient descent , Kingma and Welling ( 2014 ) introduce a reparameterization trick that allows for training using simple backpropagation w.r.t . the Gaussian latent variables z.
There are two cases to discuss when employing the variational encoder - decoder framework for labeled sequence transduction . First , when the labels of the inflected words are known as is the format of the training data in the shared task , we do n't need to bother introduction the discrete latent variables for the inflected labels . We maximize the variational lower bound on the conditional log likelihood of observing x ( t ) and y ( t ) as follows : log p θ ( x ( t ) , y ( t ) | x ( s ) ) ≥ E z∼q φ ( z | x ( s ) ) log p θ ( x ( t ) , y ( t ) , z | x ( s ) ) q φ ( z | x ( s ) ) = E z∼q φ ( z | x ( s ) ) [ log p θ ( x ( t ) | y ( t ) , z ) + log p π ( y ( t ) ) ] − KL ( q φ ( z | x ( s ) ) | | p ( z ) ) = L l ( x ( t ) , y ( t ) | x ( s ) ) ( 2 ) which is a simple extension to the vanilla variational auto - enocders . Second , in the case of unsupervised learning or when the labels of the inflected word is not observed , we only observe a word or a pair of words and we would like to maximize the log likelihood of the observed data by marginalizing over possible morphological labels , which is consisted to the supervised case above . In this scenario , we can introduce the discrete latent variables for the inflected labels which are used to infer the labels for the target word . Then when decoding the word , we condition both on the continuous and discrete latent variables . For the variational encoder - decoder ( MSVED ) , the variational lower bound on the conditional log likelihood is affected by the recognition model , and thus is computed as : log p θ ( x ( t ) | x ( s ) ) ≥E ( y ( t ) , z ) ∼q φ ( y ( t ) , z | x ( s ) , x ( t ) ) log p θ ( x ( t ) , y ( t ) , z | x ( s ) ) q φ ( y ( t ) , z | x ( s ) , x ( t ) ) = E y ( t ) ∼q φ ( y ( t ) | x ( t ) ) [ E z∼q φ ( z | x ( s ) ) [ log p θ ( x ( t ) | y ( t ) , z ) ] − KL ( q φ ( z | x ( s ) ) | | p ( z ) ) + log p π ( y ( t ) ) − log q φ ( y ( t ) | x ( t ) ) ] = L u ( x ( t ) | x ( s ) ) ( 3 ) While the unsupervised objective is trained by maximizing the following variational lower bound U ( x ) on the objective for unlabeled data : log p θ ( x ) ≥ E ( y , z ) ∼q φ ( y , z | x ) log p θ ( x , y , z ) q φ ( y , z | x ) = E y∼q φ ( y | x ) [ E z∼q φ ( z | x ) [ log p θ ( x | z , y ) ] − KL ( q φ ( z | x ) | | p ( z ) ) + log p π ( y ) − log q φ ( y | x ) ] = U ( x ) ( 4 ) Note that when labels are not observed , the inference model q φ ( y | x ) has the form of a discriminative classifier , thus we can use observed labels as the supervision signal to learn a better classifier . In this case we also minimize the following cross entropy as the classification loss : D ( x , y ) = E ( x , y ) ∼p l ( x , y ) [ − log q φ ( y | x ) ] ( 5 ) where p l ( x , y ) is the distribution of labeled data . To sum up , the semi - supervised model ( Semisup ) is trained to maximize the variational lower bounds and minimize the classification crossentropy error of 5 . L ( x ( s ) , x ( t ) , y ( t ) , x ) = α U ( x ) + L u ( x ( s ) | x ( t ) ) + L l ( x ( t ) , y ( t ) | x ( s ) ) − D ( x ( t ) , y ( t ) ) ( 6 ) The weight α controls the relative weight between the loss from unlabeled data and labeled data . 3 Learning MSVED
One challenge in training our model is that discrete random variables in a stochastic computation graph prevent the gradient from being backpropagated due to their non - differentiability , and marginalizing over all label combinations is also infeasible in our case . To alleviate this problem , we use the recently proposed Gumbel - Softmax trick ( Maddison et al , 2014 ; Gumbel and Lieblein , 1954 ) to create a differentiable estimator for categorical variables . In experiments , we start with a relatively large temperature and decrease it gradually .
We observe that with the vanilla implementation the KL cost quickly decreases to near zero , setting q φ ( z | x ) equal to standard normal distribution . In this case , the RNN decoder can easily degenerate into an RNN language model . Hence , the latent variables are ignored by the decoder and can not encode any useful information . The latent variable z learns an undesirable distribution that coincides with the imposed prior distribution but has no contribution to the decoder . To force the decoder to use the latent variables , we take the following two approaches which are similar to Bowman et al ( 2016 ) . KL - Divergence Annealing : We add a coefficient λ to the KL cost and gradually anneal it from zero to a predefined threshold λ m . At the early stage of training , we set λ to be zero and let the model first figure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost . This technique can also be seen in ( Kočiskỳ et al , 2016 ; Miao and Blunsom , 2016 ) . Input Dropout in the Decoder : Besides annealing the KL cost , we also randomly drop out the input token with a probability of β at each time step of the decoder during learning . The previous ground - truth token embedding is replaced with a zero vector when dropped . In this way , the RNN decoder could not fully rely on the ground - truth previous token , which ensures that the decoder uses information encoded in the latent variables . k a l b ⌃ ( x ) µ ( x ) ✏ ⇠ N ( 0 , 1 ) z < w > k k ä + y T 1 y T 2 y T 3 y T 4 .... ...... k a l b ⌃ ( x ) µ ( x ) ✏ ⇠ N ( 0 , 1 ) z < w > k k a Multinomial Sampling + ...... y12
The overall model architecture is shown in Fig . 1 . Each character and each label is associated with a continuous vector . We employ Gated Recurrent Units ( GRUs ) for the encoder and decoder . We use only single directional GRUs as the encoder for the input word x ( s ) . u is the hidden representation of x ( s ) which is the last hidden state of GRUs . and is used as the input for the inference model on z. We represent µ ( u ) and σ 2 ( u ) as MLPs and sample z from N ( µ ( u ) , diag ( σ 2 ( u ) ) ) , using z = µ + σ , where ∼ N ( 0 , I ) . Similarly , we can obtain the hidden representation of ( t ) and use this as input to the inference model on each label y x ( t ) i , which is also an MLP following a softmax layer to generate the categorical probabilities of target labels . Other experimental setups : We apply temperature annealing in the Gumble - Softmax with the scheme max ( 0.5 , exp ( −3e − 5 t ) ) every 2000 updates where t is the update steps . We observe that our model is not sensitive to the temperature in this task . All hyperparameters are tuned on the validation set , and include the following : For KL cost annealing , λ m is set to be 0.2 for all language settings . For character drop - out at the decoder , we empirically set β to be 0.4 for all languages . We set the dimension of character embeddings to be 300 , tag label embeddings to be 200 , RNN hidden state to be 256 , and latent variable z to be 150 or 100 . We set α the weight for the unsupervised loss to be 0.8 . We train the model with Adadelta ( Zeiler , 2012 ) and use early - stop with a patience of 5 . Our system is an ensemble of five models and the probability vector at each time step is obtained by averaging the output probabilities from each model 5 Experiments
Creating morphosyntactic tag maps : In our model , we treat the inference model on discrete labels in the form of discriminator , thus we need to know which label belongs to which morphosyntactic dimension . For example , V is a label of Part - of - speech - tagging . To obtain such mapping from a specific label to the morphosyntactic dimension , we leverage the Universal Morphological Feature Schema ( Sylak - Glassman , 2016 ) and also add the missing schema from the training data to create the key - value pairs of morphosysntactic dimension and label . Then we reformat the labels provided in the data set into the key - value pairs to train a classifier for each morphosyntactic dimension . Data Augmentation : We augment the data set in the similar way as Kann and Schütze ( 2016 ) . By doing so , the training data is not limited to the form of lemma to inflected word but can also be any word pairs that share the same lemma . This helps our model generalize better and learn the latent continuous representations more effectively . The size of training data set after augmentation scales with a factor of 2 to 20 times compared with the original one .
We process the Wikipedia corpus provided by the shared task organizer as our unsupervised training data together with words in the training data . For each language , we first get the character vocabulary of the corresponding training data and only keep words in the Wiki corpus for which characters are all in the character set we obtained . All words that occur less than 20 times are eliminated . We also limit the number of words used during training to be the 50000 most frequent words .
The results on the dev and test data of the 52 languages are presented in 1 . We obtain a generation accuracy above 80 % over more than 25 % languages and an average of 87.2 % for both dev and test data . The generation accuracy is almost consistent on the dev and test data except that the test data accuracy of Scottish - Gaelic drops by near 21 % . We find that only a medium volume of training data is provided for Scottish - Gaelic . This may be the reason why the model trained for Scottish - Gaelic can not generalize as well as other languages . We do not tune the hyper - parameters for each language manually . However , we test on different dimensions for the continuous latent variables . The dimension size we have used included 100 and 150 . And we observe significant improvement by using a larger dimension size of latent variables over a portion of languages including Faroese , Lithuanian , Navajo , Scottish - gaelic , Northern - sami , Slovene , Sorani , Slovak . However , we also observe that for some languages including Finnish , German , French , etc , the performance drops signficantly after increasing the size of continuous latent variable dimension . This indicates that for different languages , the continuous space required to encode the lemma and inflected information varies from language to language . We will further investigate this in the future work .
Wiki Data While our performance was reasonable , it was not as good as that presented in our previous work ( Zhou and Neubig , 2017 ) , nor was it competitive with the highest - scoring models on the shared task . In order to examine the reason for this , we performed several ablations , the results of which are presented in Tab . 3 First , we first examined the effects of data augmentation and Wiki Data for semi - supervised learning on the performance of our model . By removing the augmented data from the training set , we observe a large gain in the generation accuracy . Besides , we find that Wiki Data for semisupervised learning does n't help much to increase the model 's performance . The reasons for this will be examined further in the following section . We additionally reimplemented a vanilla encoder - decoder model with attention that concatenates the input characters and target word tags together with a special token in the middle as the new input sequence to the encoder ( Kann and Schütze , 2016 ) . The results show that the vanilla encoder - decoder works better than our model in some cases . We suspect that since task 1 is purely an inflection task and because semi - supervised learning did not provide a particularly large benefit , a simpler model that utilizes attention may be sufficient . This is in contrast to our previous findings , where semi - supervised learning was highly effective , and the proposed model out - performed the simpler attention - based baseline .
One potential reason for the lack of effectiveness of semi - supervised training is that the semi - supervised data that we used for training was not appropriate for the task at hand , or that we were not able to use it in the most effective way . In order to do so , we analyze the distribution of linguistic tags for words from the training data in the shared task and the Wiki Data provided by the organizer , with the hypothesis that if the distribution of tags for the Wiki Data is very different from the training and test data for the shared task , our predictions may be biased away from the testing distribution by incorporating the unsupervised Wiki data . To perform this examination , we use the tag classifier trained in our model to predict the labels for each word in the Wiki Data . The percentages of each label within each morphosyntactic dimension for Arabic and Persian are listed in Tab . 4 and Tab . 5 . We found that the distribution of the linguistic tags for the Wiki Data and the training data in the shared task are not always consistent . For example , in Arabic , the distributions of predicted tags with respect to case , possession , part - of - speech , and several other classes differ significantly from the original training data . Such difference suggests that either the words in the unlabeled Wiki Data have very different characteristics than our training set , or our tag classifier is not functioning properly to identify the tags . Either case would be detrimental to semi - supervised learning . The problem is even more stark for Persian : in Persian the only labeled words in the training data are verbs , so all nonverb words in the Wiki Data will receive an incorrect analysis , which is obviously not conducive to learning anything useful . As a recommendation for the future , when performing semi - supervised learning for morphology where the labeled data only represents a subset of the phenomena in the language , it is likely necessary to first identify which of the available unlabeled data is appropriate for semi - supervised learning before applying such methods .
In Tab . 1 , we notice that the performance on Latin is relatively poor compared with other languages . Latin is a highly inflected languages with three distinct genders , seven noun cases , four verb conjugations , four verb principal parts , six tenses , three persons , three moods , two voices , two aspects and two numbers . In addition to this , we found that the data set size after augmentation was only enlarged 2 times . We examine some errors made by our system on two worst performed languages Latin and Icelandic in Tab . 2 . As shown in the table , we found that the inflections of Latin and Icelandic have more suffix variations from the lemma . We guess our model still lacks the ability to capture more complicated inflections for such languages . We might consider adding the dependencies between different inflections for multiple target labels in our future work .
In this work , we further examine the method proposed in ( Zhou and Neubig , 2017 ) for the shared task of SIGMORPHON 2017 on 52 languages and demonstrate the effectiveness of this approach . We will further improve our model 's sophistication by investigating strategies for choosing appropriate semi - supervised data , and examining the model 's performance on languages with a high inflection level .
This work has been supported in part by an Amazon Academic Research Award . We thank Matthew Honnibal for pointing out that the data distribution of Wikipedia corpus might be biased .
