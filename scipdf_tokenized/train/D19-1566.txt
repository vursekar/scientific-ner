Context - aware Interactive Attention for Multi - modal Sentiment and Emotion Analysis
In recent times , multi - modal analysis has been an emerging and highly sought - after field at the intersection of natural language processing , computer vision , and speech processing . The prime objective of such studies is to leverage the diversified information , ( e.g. , textual , acoustic and visual ) , for learning a model . The effective interaction among these modalities often leads to a better system in terms of performance . In this paper , we introduce a recurrent neural network based approach for the multi - modal sentiment and emotion analysis . The proposed model learns the inter - modal interaction among the participating modalities through an auto - encoder mechanism . We employ a context - aware attention module to exploit the correspondence among the neighboring utterances . We evaluate our proposed approach for five standard multi - modal affect analysis datasets . Experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state - of - the - art systems .
In recent past , the world has witnessed tremendous growth of various social media platforms , e.g. , YouTube , Instagram , Twitter , Facebook , etc . People treat these platforms as a communication medium and freely express themselves with the help of a diverse set of input sources , e.g. videos , images , audio , text etc . The amount of information produced daily through these mediums are enormous , and hence , the research on multi - modal information processing has attracted attention to the researchers and developers . A video is a multimodal input which provides visual , acoustic , and textual information . The motivation of multi - modal sentiment and emotion analysis lies in fact to leverage the varieties of ( often distinct ) information from multiple sources for building more efficient systems . For some cases , text can provide a better clue for the prediction , whereas for the others , acoustic or visual sources can be more informative . Similarly , in some situations , a combination of two or more information sources together ensures better and unambiguous classification decision . For example , only text " shut up " can not decide the mood of a person but acoustic ( tone of a person ) and visual ( expression of a person ) can reveal the exact mood . Similarly , for some instances visual features such as gesture , postures , facial expression etc . have important roles to play in determining the correctness of the system . However , effectively combining this information is a nontrivial task that researchers often have to face ( Poria et al , 2016 ; Ranganathan et al , 2016 ; Lee et al , 2018 ) . Traditionally , ' text ' has been the key factor in any Natural Language Processing ( NLP ) tasks , including sentiment and emotion analysis . However , with the recent emergence of social media platforms , an interdisciplinary study involving text , visual and acoustic features have drawn a great interest among the research community . Expressing the feelings and emotions through a video is much convenient than the text for a user , and it is the best source to extract all multi - modal information . Not only the visual , it also provides other information such as acoustic and textual representation of spoken language . Additionally , a single video can have multiple utterances based on a speaker 's pause ( speech bounded by breaths ) with different sentiments and emotions . The sentiments and emotions of an utterance often have interdependence on the other contextual utterances . Independently classifying such an utterance poses several challenges to the underlying problem . In contrast , multi - modal sentiment and emotion analysis take inputs from more than one sources e.g. text , visual , acoustic for the analysis . Effectively fusing this diverse information is non - trivial and poses several challenges to the underlying problem . In our current work , we propose an end - to - end Context - aware Interactive Attention ( CIA ) based recurrent neural network for sentiment and emotion analysis . We aim to leverage the interaction between the modalities to increase the confidence of individual task in prediction . The main contributions of our current research are as follows : ( 1 ) We propose an Inter - modal Interactive Module ( IIM ) that aims to learn the interaction among the diverse and distinct features of the input modalities , i.e. , text , acoustic and visual ; ( 2 ) We employ a Context - aware Attention Module ( CAM ) that identifies and assigns the weights to the neighboring utterances based on their contributing features . It exploits the interactive representations of pairwise modalities to learn the attention weights , and ( 3 ) We present new state - of - the - arts for five benchmark datasets for both sentiment and emotion predictions .
Different reviews in ( Arevalo et al , 2017 ; Poria et al , 2016Poria et al , , 2017bGhosal et al , 2018 ; Morency et al , 2011a ; Zadeh et al , 2018a ; Mihalcea , 2012 ; Lee et al , 2018 ; Tsai et al , 2018 ) suggest that multi - modal sentiment and emotion analysis are relatively new areas as compared to uni - modal analysis . Feature selection ( fusion ) is a challenging and important task for any multi - modal analysis . Poria et al ( 2016 ) proposed a multi - kernel learning based feature selection method for multimodal sentiment and emotion recognition . A convolutional deep belief network ( CDBN ) is proposed in ( Ranganathan et al , 2016 ) to learn salient multi - modal features of low - intensity expressions of emotions , whereas Lee et al ( 2018 ) introduced a convolutional attention network to learn multimodal feature representation between speech and text data for multi - modal emotion recognition . A feature level fusion vector was built , and then a Support Vector Machine ( SVM ) classifier was used to detect the emotional duality and mixed emotional experience in ( Patwardhan , 2017 ) . Similar work on feature - level fusion based on self - attention mechanism is reported in ( Hazarika et al , 2018 ) . Fu et al ( 2017 ) introduced an enhanced sparse local discriminative canoni - cal correlation analysis approach ( En - SLDCCA ) to learn the multi - modal shared feature representation . Tzirakis et al ( 2017 ) introduced a Long Short Term Memory ( LSTM ) based end - to - end multi - modal emotion recognition system in which convolutional neural network ( CNN ) and a deep residual network are used to capture the emotional content for various styles of speaking , robust features . Poria et al ( 2017a ) presented a literature survey on various affect dimensions e.g. , sentiment analysis , emotion analysis , etc . , for the multi - modal analysis . A multi - modal fusion - based approach is proposed in ( Blanchard et al , 2018 ) for sentiment classification . The author used exclusively high - level fusion of visual and acoustic features to classify the sentiment . Zadeh et al ( 2016 ) presented the multi - modal dictionary - based technique to capture the interaction between spoken words and facial expression better when expressing the sentiment . In another work , proposed a Tensor Fusion Network ( TFN ) to capture the inter - modality and intra - modality dynamics between the multi - modalities ( i.e. , text , visual , and acoustic ) . These works did not take contextual information into account . Poria et al ( 2017b ) introduced an Long Short Term Memory ( LSTM ) based framework for sentiment classification which uses contextual information to capture interrelationships between the utterances . In another work , Poria et al ( 2017c ) proposed a user opinion based framework to combine all the multi - modal inputs ( i.e. , visual , acoustic , and textual ) by applying a multi - kernel learning - based approach . Contextual inter - modal attention mechanism was not explored in much details until recently . Zadeh et al ( 2018a ) introduced a multi - attention blocks based model for multi - modal sentiment classification but did not account for contextual information , whereas Ghosal et al ( 2018 ) proposed a contextual inter - modal attention based framework for multi - modal sentiment classification . Recently , Zadeh et al ( 2018c ) introduced the largest multimodal dataset namely CMU - MOSEI for sentiment and emotion analysis . Author effectively fused the multi - modality inputs i.e. , text , visual , and acoustic through a dynamic fusion graph and reported competitive performance w.r.t . various state - ofthe - art systems for both sentiment and emotion analysis . Very recently , Akhtar et al ( 2019 ) in - troduced an attention based multi - task learning framework for sentiment and emotion classification on the CMU - MOSEI dataset . In comparison to the existing systems , our proposed approach aims to exploits the interaction between the input modalities through an autoencoder based inter - modal interactive module . The interactive module learns the joint representation for the participating modalities , which are further utilized to capture the contributing contextual utterances in a context - aware attention module . 3 Context - aware Interactive Attention ( CIA ) Affect Analysis In this section , we describe our proposed approach for the effective fusion of multi - modal input sources . We propose an end - to - end Contextaware Interactive Attention ( CIA ) based recurrent neural network for sentiment and emotion analysis . As discussed earlier , one of the main challenges for multi - modal information analysis is to exploit the interaction among the input modalities . Therefore , we introduce an Inter - modal Interactive Module ( IIM ) that aims to learn the interaction between any two modalities through an auto - encoder like structure . For the text - acoustic pair of modalities , we aim to decode the acoustic representation through the encoded textual representation . After training of IIM , we extract the encoded representation for further processing . We argue that the encoded representation learns the interaction between the text and acoustic modalities . Similarly , we compute the interaction among all the other pairs ( i.e. , acoustic - text , text - visual , visualtext , acoustic - visual , and visual - acoustic ) . Next , we extract the sequential pattern of the utterances through a Bi - directional Gated Recurrent Unit ( Bi - GRU ) ( Cho et al , 2014 ) ) . For each pair of modalities , the two representations denoting the interactions between them are combined through a mean operation . For an instance , we compute the mean of the text - acoustic and acoustic - text representations for text and acoustic modalities . The mean operation ensures that the network utilizes the two distinct representations by keeping the minimal dimension . In our network , we , additionally , learn the interaction among the modalities through a feedforward network . At first , all the three modalities are passed through a separate Bi - GRU . Then , pair - Algorithm 1 Inter - modal Interactive Module for Multi - modal Sentiment and Emotion Recognition ( IIM - MMSE ) procedure IIM - MMSE ( t , v , a ) for i 1 , ... , K do K = # modalities for j 1 , ... , K do ∀x , y [ T , V , A ] , x = y and i ≤ j C x i y j IIM ( x i , y j ) C x i y j biGRU ( C x i y j ) C x i biGRU ( x i ) for i , j 1 , ... , K do ∀x , y [ T , V , A ] , and x = y M x i , y j M ean ( C x i y j , C y i x j ) cat x i , y j Concatenate ( C x i , C y j ) BI x i , y j F ullyConnected ( cat x i , y j ) A x i , y j CAM ( M x i , y j , BI x i , y j ) Rep [ A T V , A T A , A AV ] polarity Sent ( Rep ) /Emo ( Rep ) return polarity Algorithm 2 Inter - Modal Interactive Module ( IIM ) procedure IIM ( X , Y ) C XY IIM Encoder ( X , Y ) Y IIM Decoder ( C XY ) loss cross entropy ( Y , Y ) Backpropagation to update the weights return C XY Algorithm 3 Context - aware Attention Module ( CAM ) procedure CAM ( M , BI ) P M.BI T Cross product for i , j 1 , ... , u do u = # utterances N ( i , j ) e P ( i , j ) u k=1 e P ( i , k ) O N.BI A O M Multiplicative gating . return A wise concatenation is performed over the output of Bi - GRU and passed through a fully - connected layer to extract the bi - modal interaction ( BI ) . Further , we employ a Context - aware Attention Module ( CAM ) to exploit the correspondence among the neighboring utterances . The inputs to the CAM are the two representations for each pair of modalities , e.g. , mean representation M T A and bi - modal interaction BI T A for the text - acoustic pair . The attention module assists the network in attending the contributing features by putting weights to the current and the neighboring utterances in a video . In the end , the pair - wise ( i.e. , text - acoustic , text - visual , and acoustic - visual ) attended representations are concatenated and fed to an output layer for the prediction . We depict and summarize the proposed approach in Figure 1 and Algorithm 1 , 2 , and 3 . The source code is available at http://www.iitp . ac.in/˜ai - nlp - ml / resources.html .
Since the utterances in a video are the split units of the break / pause of the speech , their emotions ( or sentiments ) often have relations with their neighboring utterances . Therefore , knowledge of the emotions ( or , sentiments ) of the neighboring utterances is an important piece of information and has the capability to derive the prediction of an utterance , if the available inputs are insufficient for the correct prediction . Our proposed context - aware attention module leverages the contextual information . For each utterance in a video , we compute the attention weights of all the neighboring utterances based on their contributions in predicting the current utterance . It ensures that the network properly utilizes the local contextual information of an utterance as well as the global contextual information of a video together . The aim is to compute the interactive attention weights utilizing a softmax activation for each utterance in the video . Next , we apply a multiplicative gating mechanism following the work of Dhingra et al ( 2016 ) . The attentive representation is , then , forwarded to the upper layers for further processing . We summarize the process of CAM in Algorithm3 .
One of the key objectives of the multi - modal analysis is to fuse the available input modalities effectively . In general , different modalities represent distinct features despite serving a common goal . For example , in multi - modal sentiment analysis all the three modalities , i.e. , text , acoustic , and visual , aim to predict the expressed polarity of an utterance . The distinctive features in isolation might create an ambiguous scenario for a network to learn effectively . Therefore , we introduce an auto - encoder based inter - modal interactive module whose objective is to learn the interaction between two distinct modalities to serve a common goal . The IIM encodes the feature representation of one modality ( say , text ) , and aims to decode it into the feature representation of another modality ( say , acoustic ) . Similar to an auto - encoder where the input and output are conceptually the same ( or closely related ) , in our case the input and output feature representations of two modalities also intuitively serve a common goal . After training of IIM , the encoded vector signifies a joint representation of the two modalities , which can be further utilized in the network . As the proposed architecture in Figure 1 depicts , our proposed model is an end - to - end system , which takes multi - modal raw features for each utterance in a video and predicts an output . We also train our proposed IIM in the combined framework . For any pair of modalities , e.g. , text - visual , the encoded vector in IIM receives two gradients of errors , i.e. , one error from the IIM output ( visual ) l 1 and another from the task - specific label l 2 . We aggregate the errors ( l 1 + l 2 ) at the encoded vector and backpropagate it to the input ( text ) . Thus , the weights in the encoder part will adjust according to the desired task - specific label as well . However , in contrast , the decoder part does not have such information . Therefore , we employ another IIM to capture the interaction between the visual - text . This time , the visual features are aware of the desired label during the interaction with textual features . A conceptual diagram , depicting the gradient flow in IIM for the text and visual modalities , is shown in Figure 2 .
In this section , we present our experimental results along with necessary analysis . We also compare our obtained results with several state - of - the - art systems .
For the evaluation of our proposed approach , we employ five multi - modal benchmark datasets 1 covering two affect analysis tasks , i.e. , sentiment and emotion . Table 1 : Results of sentiment and emotion analysis for the proposed approach . T : Text , V : Visual , A : Acoustic . Weighted accuracy as a metric is chosen due to unbalanced samples across various emotions and it is also in line with the other existing works ( Zadeh et al , 2018c
The above datasets offer different dimension of sentiment analysis . We define the following setups for our experiments . Two - class ( pos and neg ) classification : MO - SEI , MOSI , ICT - MMMO , and MOUD . Three - class ( pos , neu , and neg ) classification : YouTube . Five - class ( strong pos , weak pos , neu , weak neg , and strong neg ) classification : MOSEI . Seven - class ( strong pos , moderate pos , weak pos , neu , weak neg , moderate neg , and strong neg ) classification : MOSEI and MOSI . Intensity prediction : MOSEI and MOSI .
We implement our proposed model on the Pythonbased Keras deep learning library . As the evaluation metric , we employ accuracy ( weighted accu - racy ( Tong et al , 2017 ) ) and F1 - score for the classification problems , while for the intensity prediction task , we compute Pearson correlation scores and mean - absolute - error ( MAE ) . We evaluate our proposed CIA model on five benchmark datasets i.e. , MOUD , MOSI , YouTube , ICT - MMMO , and MOSEI . For all the datasets , we perform grid search to find the optimal hyperparameters ( c.f . Table 4 ) . Though we push for a generic hyper - parameter configuration for all datasets , in some cases , a different choice of the parameter has a significant effect . Therefore , we choose different parameters for different datasets for our experiments . Details of hyper - parameters for different datasets are depicted in Table 4 . We use different activation functions for the various modules in our model . We use tanh as the activation function for the inter - modal interactive module ( IIM ) , while we employ ReLu for the context - aware attention module . For each dataset , we use Adam as optimizer . In this paper , we address three multi - modal affective analysis problems , namely i.e. , sentiment classification ( S C ) , sentiment intensity ( S I ) and emotion classification ( E C ) . We use softmax as a classifier for sentiment classification , while optimizing the categorical cross - entropy as a loss function . In comparison , we use sigmoid for prediction and binary cross - entropy as the loss function for the emotion classification . As the emotions in the dataset are multi - labeled , we apply a threshold over the predicted sigmoid outputs for each emotion and consider all the emotions as present whose respective values are above the threshold . We cross - validate and optimize both We evaluate our proposed approach for all the possible input combinations i.e. , uni - modal ( T , A , V ) , bi - modal ( T+V , T+A , A+V ) and tri - modal ( T+V+A ) . We depict our obtained results in Table 1 . For MOSEI dataset , with tri - modal inputs , our proposed system reports 79.02 % F1 - score and 62.97 % weighted - accuracy for emotion classification . For sentiment classification , we obtain 78.23 % , 80.37 % , 49.15 % and 50.14 % as F1 - score for two - class , five - class and seven - class , respectively . For sentiment intensity prediction task , our proposed system yields MAE and Pearson score of 0.683 and 0.594 , respectively . We also observe that the proposed approach yields better performance for the tri - modal inputs than the bi - modal and uni - modal input combinations . This improvement implies that our proposed CIA architecture utilizes the interaction among the input modalities very effectively . Furthermore , for the other datasets , i.e. , MOSI , ICT - MMMO , YouTube , and MOUD , we also observe a similar phenomenon as well ( c.f . Table 1 ) . To show that our proposed IIM module , indeed , learns the interaction among the distinct modalities , we also perform an ablation study of the proposed CIA architecture . Consequently , we omit the IIM module from our architecture and compute the self - attention on the pair - wise fully - connected representations for the prediction . We observe that , for all the datasets , the performance of this modified architecture ( i.e. , CIA - IIM ) is constantly inferior ( with 1 % to 7 % F - score points ) to the proposed CIA architecture . This performance degradation suggests that the IIM module is , indeed , an important component of our proposed architecture . In Table 3 , we depict the evaluation results for both - with and without IIM .
In this section , we present our comparative studies against several existing and recent state - ofthe - art systems . For each dataset , we report three best systems for the comparisons 2 . In particular , we compare with the following systems : Bag of Feature - Multimodal Sentiment Analysis ( BoF - MSA ) ( Blanchard et al , 2018 ) , Memory Fusion Network ( MFN ) ( Zadeh et al , 2018b ) ( Zadeh et al , 2018c ) , Tensor Fusion Network ( TFN ) , Random Forest ( RF ) ( Breiman , 2001 ) , Support Vector Machine ( Zadeh et al , 2016 ) , Multi - Attention Recurrent Network ( MARN ) ( Zadeh et al , 2018a ) , Dynamic Fusion Graph ( DFG ) ( Zadeh et al , 2018c ) , Multi Modal Multi Utterance - Bimodal Attention ( MMMU - BA ) ( Ghosal et al , 2018 ) , Bi - directional Contextual LSTM ( BC - LSTM ) ( Poria et al , 2017b ) and Multimodal Factorization Model ( MFM ) ( Tsai et al , 2018 ) . We show the comparative results in Table 5a and Table 5b for emotion and sentiment analysis , respectively . We observe that the proposed CIA framework yields better performance against the state - of - the - art for all the cases . For emotion classification , our proposed approach achieves approximately 3 and 0.6 percentage higher F1 - ( Zadeh et al , 2018c ) . † Values are taken from ( Tsai et al , 2018 ) . Significance T - test ( < 0.05 ) signifies that the obtained results are statistically significant over the existing systems with 95 % confidence score . score and weighted accuracy , respectively , than the state - of - the - art DFG ( Zadeh et al , 2018c ) system . Furthermore , we also see improvements for most of the individual emotion classes as well . In sentiment analysis ( c.f . Table 5b ) , for all the five datasets and different experimental setups , the proposed CIA framework obtains the improved accuracies for the classification tasks . For intensity prediction , our proposed framework yields lesser mean - absolute - error with high Pearson correlation scores . On average , we observe 1 to 5 % improvement in accuracy values in comparison to the next best systems . Similarly , for the intensity prediction task , we report approximately 0.03 and 0.04 points improvement in mean - absolute - error and Pearson score , respectively . We perform statistical significance test ( paired T - test ) on the obtained results and observe that performance improvement in the proposed model over the state - of - the - art is significant with 95 % confidence ( i.e. , p - value < 0.05 ) .
We analyze our proposed CIA model to understand the importance of the baseline framework CIA - IIM . We study the predictions of both the models and observe that the proposed CIA framework improves the predictions of the baseline CIA - IIM model . It indicates that the CIA framework , indeed , learns the interaction among the input modalities , and the model effectively exploits this interaction for better judgment . In Table 6 , we list the utterances of a CMU - MOSEI video along with their correct and predicted labels for both the proposed and baseline systems . The video in Table 6 has 4 utterances , out of which the correct sentiments of three utterances ( i.e. , u 1 , u 3 , and u 4 ) are positive , while one utterance ( i.e. , u 2 ) is negative . We observe that our proposed CIA model predicts all the 4 utterances correctly , while the CIA - IIM mis - classify the sentiments of the utterances , u 2 and u 3 . We also analyze the context - aware attention module ( CAM ) with the help of heatmaps of the attention weights . The heatmaps , as depicted in Figure 3 , represent the contributing utterances in the neighbourhood for the classification of each utterance . Figures 3a , 3b and 3c show the heatmaps of the pair - wise modality interaction of the proposed model CIA . In Figure 3a , each cell ( i , j ) of the heatmap signifies the weights of utterance ' j ' for the classification of utterance ' i ' . For the utterance u 4 , the model puts more attention weights on the u 2 and u 3 of the text - visual interactions , while for the text - acoustic interaction the model assigns higher weights to the u 4 utterance itself . Similarly , the model assigns the least weight to the u 1 utterance , whereas the utterance u 3 gets the highest weights . We argue that the proposed CAM module captures the diversity in the input modalities of the contextual utterances for the correct prediction . For emotion prediction , the CIA model captures all the emotions correctly , while the CIA - IIM framework fails to predict the correct emotions of the utterances , u 2 and u 3 . For the same video , we also show the attention heatmaps for emotion in Figure 3 . For the utterance u 2 , our proposed model ( CIA ) captures the emotion class ' sad ' as the CAM module assigns higher attention weights on the utterances u 2 and u 3 in Figure 3d , u 4 in Figure 3e , and u 2 in Figure 3f . Since the system finds the contributing neighbours as utterances u 2 , u 3 and u 4 for various combinations , we argue that it utilizes the information of these utterances - which all express the ' sad ' emotion - for the correct prediction of utterance u 2 as ' sad ' .
In this paper , we have proposed a Context - aware Interactive Attention framework that aims to capture the interaction between the input modalities for the multi - modal sentiment and emotion prediction . We employed a contextual attention module to learn the contributing utterances in the neighborhood by exploiting the interaction among the input modalities . We evaluate our proposed approach on five standard multi - modal datasets . Experiments suggest the effectiveness of the proposed model over various existing systems , for both sentiment and emotion analysis , as we obtained new state - of - the - art for all five datasets . In current work , we undertook the problem of sentiment and emotion analysis for a single - party utterances . In future , we would like to extend our work towards the multi - party dialogue . 6 Acknowledgment
The research reported here is partially supported by SkyMap Global India Private Limited . Asif Ekbal acknowledges the Young Faculty Research Fellowship ( YFRF ) , supported by Visvesvaraya PhD scheme for Electronics and IT , Ministry of Electronics and Information Technology ( MeitY ) , Government of India , being implemented by Digital India Corporation ( formerly Media Lab Asia ) .
