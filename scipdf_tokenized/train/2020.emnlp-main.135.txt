Tired of Topic Models ? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too !
Topic models are a useful analysis tool to uncover the underlying themes within document collections . The dominant approach is to use probabilistic topic models that posit a generative story , but in this paper we propose an alternative way to obtain topics : clustering pretrained word embeddings while incorporating document information for weighted clustering and reranking top words . We provide benchmarks for the combination of different word embeddings and clustering algorithms , and analyse their performance under dimensionality reduction with PCA . The best performing combination for our approach performs as well as classical topic models , but with lower runtime and computational complexity .
Topic models are the standard approach for exploratory document analysis ( Boyd - Graber et al , 2017 ) , which aims to uncover main themes and underlying narratives within a corpus . But in times of distributed and even contextualized embeddings , are they the only option ? This work explores an alternative to topic modeling by casting ' key themes ' or ' topics ' as clusters of word types under the modern distributed representation learning paradigm : unsupervised pre - trained word embeddings provide a representation for each word type as a vector , allowing us to cluster them based on their distance in high - dimensional space . The goal of this work is not to strictly outperform , but rather to benchmark standard clustering of modern embedding methods against the classical approach of Latent Dirichlet Allocation ( LDA ; Blei et al , 2003 ) . We restrict our study to influential embedding methods and focus on centroid - based clustering algorithms as they provide a natural way to obtain the top words in each cluster based on distance from the cluster center . 1 Aside from reporting the best performing combination of word embeddings and clustering algorithm , we are also interested in whether there are consistent patterns : embeddings which perform consistently well across clustering algorithms might be good representations for unsupervised document analysis , clustering algorithms that perform consistently well are more likely to generalize to future word embedding methods . To make our approach reliably work as well as LDA , we incorporate corpus frequency statistics directly into the clustering algorithm , and quantify the effects of two key methods , 1 ) weighting terms during clustering and 2 ) reranking terms for obtaining the top J representative words . Our contributions are as follows : We systematically apply centroid - based clustering algorithms on top of a variety of pretrained word embeddings and embedding methods for document analysis . Through weighted clustering and reranking of top words we obtain sensible topics ; the best performing combination is comparable with LDA , but with smaller time complexity and empirical runtime . We show that further speedups are possible by reducing the embedding dimensions by up to 80 % using PCA .
Analyzing documents by clustering word embeddings is a natural idea - clustering has been used for readability assessment ( Cha et al , 2017 ) , argument mining ( Reimers et al , 2019 ) , document classification and document clustering ( Sano et al , 2017 ) , inter alia . So far , however , clustering word embeddings has not seen much success for the purposes of topic modeling . While many modern efforts have attempted to incorporate word embeddings into the probabilistic LDA framework ( Liu et al , 2015 ; Nguyen et al , 2015 ; Das et al , 2015 ; Batmanghelich et al , 2016 ; Xun et al , 2017 ; Dieng et al , 2019 ) , relatively little work has examined the feasibility of clustering embeddings directly . Xie and Xing ( 2013 ) and Viegas et al ( 2019 ) first cluster documents and subsequently find words within each cluster for document analysis . Sridhar ( 2015 ) targets short texts where LDA performs poorly in particular , fitting GMMs to learned word2vec representations . De Miranda et al ( 2019 ) cluster using self - organising maps , but provide only qualitative results . In contrast , our proposed approach is straightforward to implement , feasible for regular length documents , requires no retraining of embeddings , and yields qualitatively and quantitatively convincing results . We focus on centroid based k - means ( KM ) , Spherical k - means ( SK ) , and k - medoids ( KD ) for hard clustering , and von Mises - Fisher Models ( VMFM ) and Gaussian Mixture Models ( GMM ) for soft clustering ; as pre - trained embeddings we consider word2vec ( Mikolov et al , 2013 ) , GloVe ( Pennington et al , 2014 ) , FastText ( Bojanowski et al , 2017 , Spherical ( Meng et al , 2019 ) , ELMo ( Peters et al , 2018 ) , andBERT ( Devlin et al , 2018 ) .
After preprocessing and extracting the vocabulary from our training documents , each word type is converted to its embedding representation ( averaging all of its tokens for contextualized embeddings ; details in 5.3 ) . Following this we apply the various clustering algorithms on the entire training corpus vocabulary to obtain k clusters , using weighted ( 3.2 ) or unweighted word types . After the clustering algorithm has converged , we obtain the top J words ( 3.1 ) from each cluster for evaluation . Note that one potential shortcoming of our approach is the possibility of outliers forming their own cluster , which we leave to future work . Figure 1 : The figure on the left shows the cluster center ( ) without weighting , while the figure on the right shows that after weighting ( larger points have higher weight ) a hopefully more representative cluster center is found . Note that top words based on distance from the cluster center could still very well be low frequency word types , motivating reranking ( 3.3 ) .
In traditional topic modeling ( LDA ) , the top J words are those with highest probability under each topic - word distribution . For centroid based clustering algorithms , the top words of some cluster i are naturally those closest to the cluster center c ( i ) , or with highest probability under the cluster parameters . Formally , this means choosing the set of types J as argmin J : | J | = 10 j J     c ( i ) − x j 2 2 for KM / KD , cos ( c ( i ) , x j ) for SK , f ( x j | c ( i ) , Σ i ) for GMM / VMFM . Our results in 6 focus on KM and GMM , as we observe that k - medoids , spherical KM and von Mises - Fisher tend to perform worse than KM and GMM ( see App . A , App . B ) . Note that it is possible to extend this approach to obtain the top topics given a document : compute similarity scores between learned topic cluster centers and all word embeddings from that particular document , and normalize them using softmax to obtain a ( non - calibrated ) probability distribution . Crucial to our method is the incorporation of corpus statistics on top of vanilla clustering algorithms , which we will describe in the remainder of this section .
The intuition of weighted clustering is based on the formulation of classical LDA which models the probability of the word type t belonging to a topic i as N t , i + βt t N t i + β t , where N t , i refers to the number of times word type t has been assigned to topic i , and β is a parameter of the Dirichlet prior on the pertopic word distribution . In our case , illustrated by the schematic in Fig . 1 , weighting is a natural way to account for the frequency effects of vocabulary terms during clustering .
When obtaining the top - J words that make up a cluster 's topic , we also consider reranking terms , as there is no guarantee that words closest to cluster centers are important word types . We will show in Table 2 that without reranking , clustering yields " sensible " topics but low NPMI scores .
To incorporate corpus statistics into the clustering algorithm , we examine three different schemes 2 to assign weights to word types , where n t is the count of word type t in corpus D , and d is a document : tf = n t t n t ( 1 ) tf - df = tf | { d D | t d } | | D | ( 2 ) tf - idf = tf log | D | | { d D | t d } | + 1 ( 3 ) These scores can now be used for weighting word types when clustering ( w ) , reranking top 100 words ( r ) after , both ( w r ) , or neither ( simply ) . We find that simply using tf outperforms the other weighting schemes ( App . C ) . Our results and subsequent analysis in 6 uses tf for weighting and reranking .
The complexity of KM is O ( tknm ) , and of GMM is O ( tknm 3 ) , for t iterations , 3 k clusters ( topics ) , n word types ( unique vocabulary ) , and m embedding dimensions . Weighted variants have a oneoff cost of weight initialization , and contribute a constant factor when recalulculating the centroid during clustering . Reranking has an additional O ( n log ( n k ) ) factor , where n k is the average number of elements in a cluster . In contrast , LDA via collapsed Gibbs sampling has a complexity of O ( tkN ) , where N is the number of all tokens , so when N n , clustering methods can potentially achieve better performance - complexity tradeoffs . Note that running ELMo and BERT over documents also requires iterating over all tokens , but only once , and not for every topic and iteration .
For readily available pretrained word embeddings such as word2vec , FastText , GloVe and Spherical , the embeddings can be considered as ' given ' as the practioner does not need to generate these embeddings from scratch . However for contextual embeddings such as ELMo and BERT , there is additional computational cost in obtaining these embeddings before clustering , which requires passing through RNN and transformer layers respectively . This can be trivially parallelised by batching the context window ( usually a sentence ) . We use standard pretrained ELMo and BERT models in our experiments and therefore do not consider the runtime of training these models from scratch .
Our implementation is freely available online . 4
We use the 20 newsgroup dataset ( 20NG ) which contains around 18000 documents and 20 categories , 5 and a subset of Reuters21578 6 which contains around 10000 documents .
We adopt a standard 60 - 40 train - test split for 20NG and 70 - 30 for Reuters . The top 10 words ( 3.1 ) were evaluated using normalized pointwise mutual information ( NPMI ; Bouma , 2009 ) which has been shown to correlate with human judgements ( Lau et al , 2014 ) . NPMI ranges from [ −1 , 1 ] with 1 indicating perfect association . The train split is used to obtain the top topic words in an unsupervised fashion ( we do not use any document labels ) , and the test split is used to evaluate the " topic coherence " of these top words . NPMI scores are averaged across all topics . For both datasets we use 20 topics ; which gives best NPMI out of 20 , 50 , 100 topics for Reuters , and is the ground truth number for 20NG . The NPMI scores presented in Table 1 are averaged across cluster centers initialized using 5 random seeds .
We lowercase tokens , remove stopwords , punctuation and digits , and exclude words that appear in less than 5 documents and appear in long sentences of more than 50 words , removing email artifacts and noisy token sequences which are not valid sentences . An analysis on the effect of rare word removal can be found in 6.2 . For contextualized word embeddings ( BERT and ELMo ) , sentences served as the context window to obtain the token representations . Subword representations were averaged for BERT , which performs better than just using the first subword .
Our main results are shown in Table 1 .
Running LDA with MALLET ( McCallum , 2002 ) takes a minute , but performs no better than KM w r , which takes little more than 10 seconds on CPU using sklearn ( Pedregosa et al , 2011 ) , and 3 - 4 seconds using a simple implementation using JAX ( Bradbury et al , 2018 ) on GPU .
From Table 1 , we see that reranking and weighting greatly improves clustering performance across different embeddings . As a first step to uncover why , we investigate how sensitive our methods are to restricting the clustering to only frequently appearing word types . Visualized in Fig . 3 , we find that as we vary the cutoff term frequency , thus changing the vocabulary size and allowing more rare words on the x - axis , NPMI is more affected for the models without reweighting . This suggests that reweighting using term frequency is effective for clustering without the need for ad - hoc restriction of infrequent terms - without it , all combinations perform poorly compared to LDA . In general , GMM outperforms KM for both weighted and unweighted variants averaged across all embedding methods ( p < 0.05 ) . 7
For KM , extracted topics before reranking results in reasonable looking themes , but scores poorly on NPMI . Reranking strongly improves KM on average ( p < 0.02 ) for both Reuters and 20NG . Examples before and after reranking are provided in Table 2 . This indicates that while cluster centers are centered around valid themes , they are surrounded by low frequency word types . We observe that when applying reranking to GMM w the gains are much less pronounced than KM w . The top topic words before and after reranking for BERT - GMM w have an average Jaccard similarity score of 0.910 , indicating that the cluster centers learned by weighted GMMs are already centered at word types of high frequency in the training corpus .
Spherical embeddings and BERT perform consistently well across both datasets . For 20NG , KM w r Spherical and LDA both achieve 0.26 NPMI . For Reuters , GMM w r BERT achieves the top NPMI score of 0.15 compared to 0.12 of LDA . Word2vec and ELMo ( using only the last layer 8 ) perform poorly compared to the other embeddings . Fast - Text and GloVe can achieve similar performance to BERT on 20NG but are slightly inferior on Reuters . Training or fine - tuning embeddings on the given data prior to clustering could potentially achieve better performance , but we leave this to future work .
We find that our approach yields a greater diversity within topics as compared to LDA while achieving comparable coherence scores ( App . D ) . Such topics are arguably more valuable for exploratory analysis .
We apply PCA to the word embeddings before clustering to investigate the amount of redundancy in the dimensions of large embeddings , which impact clustering complexity ( 4 ) . With reranking , the dimensions of all embeddings can be reduced by more than 80 % ( Fig . 2 ) . We observe that KM w r can consistently reduce the number of dimensions across different embedding types without loss of performance . Although GMM w does not require reranking for good performance , it 's cubic complexity indicates that KM w r might be preferred in practical settings .
We outlined a methodology for clustering word embeddings for unsupervised document analysis , and presented a systematic comparison of various influential embedding methods and clustering algorithms . Our experiments suggest that pretrained word embeddings ( both contextualized and non - contextualized ) , combined with tf - weighted k - means and tf - based reranking , provide a viable alternative to traditional topic modeling at lower complexity and runtime .
To further understand the effect of other centroid based algorithms on topic coherence , we also applied the k - medoids ( KD ) clustering algorithm . KD is a hard clustering algorithm similar to KM but less sensitive to outliers . As we can see in Table 3 , in all cases KD usually did as well or worse than KM . KD also did relatively poorly after frequency reranking . Where KD did do better than KM , the difference is not very striking and the NPMI scores were still quite below the other top performing models .
Mises - Fisher Mixture
We present the results for using different reranking schemes for KM ( Table 5 ) and Weighted KM for Frequency ( Table 6 ) . We can see that compared to the TF results in the main paper , other schemes for reranking such as aggregated TF - IDF and TF - DF improve over the original hard clustering , but fare worse in comparison with reranking with TF . of topics due to the greater diversity of words over all the topics . Top 10 Word for Each Topic NPMI dollar rate rates exchange currency market dealers central interest point 0.369 year growth rise government economic economy expected domestic inflation report 0.355 gold reserves year tons company production exploration ounces feet mine 0.290 billion year rose dlrs fell marks earlier figures surplus rise - 0.005 year tonnes crop production week grain sugar estimated expected area 0.239 dlrs company sale agreement unit acquisition assets agreed subsidiary sell - 0.043 bank billion banks money interest market funds credit debt loans 0.239 tonnes wheat export sugar tonne exports sources shipment sales week 0.218 plan bill industry farm proposed government administration told proposal change 0.212 prices production price crude output barrels barrel increase demand industry 0.339 group company investment stake firm told companies capital chairman president 0.191 trade countries foreign officials told official world government imports agreement 0.298 offer company shares share dlrs merger board stock tender shareholders 0.074 shares stock share common dividend company split shareholders record outstanding 0.277 dlrs year quarter earnings company share sales reported expects results - 0.037 market analysts time added long analyst term noted high back 0.316 coffee meeting stock producers prices export buffer quotas market price 0.170 loss dlrs profit shrs includes year gain share mths excludes - 0.427 spokesman today government strike union state yesterday workers officials told 0.201 program corn dlrs prior futures price loan contract contracts cents - 0.287 Top 10 Word for Each Topic NPMI rise increase growth fall change decline drop gains cuts rising 0.238 president chairman minister house baker administration secretary executive chief washington 0.111 make continue result include reduce open support work raise remain 0.101 january march february april december june september october july friday 0.043 year quarter week month earlier months years time period term 0.146 rose fell compared reported increased estimated revised adjusted unchanged raised 0.196 today major made announced recent full previously strong final additional 0.125 share stock shares dividend common cash stake shareholders outstanding preferred 0.281 dlrs billion tonnes marks francs barrels cents tonne barrel tons - 0.364 sales earnings business operations companies products markets assets industries operating 0.115 sale acquisition merger sell split sold owned purchase acquire held 0.003 board meeting report general commission annual bill committee association council 0.106 loss profit revs record note oper prior shrs gain includes 0.221 company corp group unit firm management subsidiary trust pacific holdings 0.058 prices price current total lower higher surplus system high average 0.198 offer agreement agreed talks tender plan terms program proposed issue 0.138 bank trade market rate exchange dollar foreign interest rates banks 0.327 told official added department analysts officials spokesman sources statement reuters 0.181 production export exports industry wheat sugar imports output crude domestic 0.262 japan government international world countries american japanese national states united 0.251
We thank Aaron Mueller , Pamela Shapiro , Li Ke , Adam Poliak , Kevin Duh and the anonymous reviewers for their feedback .
We present the different topics generated using LDA ( Table 7 ) and topics generated using BERT KM w r for the Reuters dataset (
