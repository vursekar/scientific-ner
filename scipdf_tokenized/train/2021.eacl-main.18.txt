Non - Autoregressive Text Generation with Pre - trained Language Models
Non - autoregressive generation ( NAG ) has recently attracted great attention due to its fast inference speed .
However , the generation quality of existing NAG models still lags behind their autoregressive counterparts .
In this work , we show that BERT can be employed as the backbone of a NAG model to greatly improve performance .
Additionally , we devise mechanisms to alleviate the two common problems of vanilla NAG models : the inflexibility of prefixed output length and the conditional independence of individual token predictions .
Lastly , to further increase the speed advantage of the proposed model , we propose a new decoding strategy , ratio - first , for applications where the output lengths can be approximately estimated beforehand .
For a comprehensive evaluation , we test the proposed model on three text generation tasks , including text summarization , sentence compression and machine translation .
Experimental results show that our model significantly outperforms existing non - autoregressive baselines and achieves competitive performance with many strong autoregressive models .
In addition , we also conduct extensive analysis experiments to reveal the effect of each proposed component .
1
Autoregressive generation ( AG ) models achieve state - of - the - art performance on a wide range of text generation tasks , such as machine translation ( Vaswani et al , 2017 ) and text summarization ( Rush et al , 2015 ) .
Such models generate a token sequence in a left - to - right , token - by - token fashion .
The prediction for the next token is conditioned on all previously generated tokens .
This characteristic makes it impossible to parallelize the computational overhead for token predictions in different positions , which leads to a relatively high latency in inference .
On the other hand , non - autoregressive generation ( NAG ) models ( Gu et al , 2018 ) have emerged as a promising alternative due to their fast inference speed .
NAG models omit the sequential dependencies within the output - side sequence and predict tokens in all positions simultaneously once the output length has been determined beforehand .
While NAG models enjoy full parallelism and faster inference , the generation quality of NAG models often lags behind their autoregressive counterparts . 
In this work , we explore the potential of largescale pre - trained language models for improving the performance of non - autoregressive generation .
Specifically , we utilize BERT ( Devlin et al , 2019 ) as the backbone for NAG modelling and extend the architecture of BERT with a CRF output layer ( Lafferty et al , 2001 ; for better capturing the output - side dependencies . 
In addition , we analyze two significant limitations that NAG models currently suffer from : ( 1 ) the inflexibility of prefixed output length , and ( 2 ) the conditional independence of individual token predictions .
Accordingly , we devise two solutions to these two problems . 
First , prior NAG models require the output length to be determined before token generation , thus an extra module for output length prediction is always required .
Nevertheless , the most likely length from the prediction module is not necessarily the best - suited one for the token generation model .
To this end , previous works ( Gu et al , 2018 ; Ma et al , 2019 ) usually rely on length - parallel decoding ( LPD ) for performance enhancement ; that is , generating and re - ranking the results from different output length candidates .
In this work , we propose a simple and elegant decoding mechanism that lets the model determine the output length on - the - fly .
Specifically , our model dynamically adjusts the output sequence length via emitting an [ eos ] token at any output position to indicate the ending of the generated sequence .
Therefore , we can avoid the additional efforts of output length prediction and results re - ranking . 
Second , most existing NAG models assume the token predictions in different positions are conditionally independent .
As a consequence , they often tend to generate results that are ungrammatical with repetitions ( Wang et al , 2019b ) .
To alleviate this problem , we propose a context - aware learning objective which impels the model to output different tokens at adjacent positions , thereby reducing the possibility of repetitive generation . 
Furthermore , for tasks like text summarization , the output sequence ( summary ) is known to be shorter than the source sequence ( article ) .
In such cases , to further improve the model 's inference efficiency , we introduce a new ratio - first decoding strategy .
Specifically , instead of performing inference on all source - side hidden states , ratio - first generates the result only based on a subset of source hidden states .
The subset size is jointly determined by the source length T and a predefined ratio α that is set based on our prior knowledge from the data statistics .
In the experiments , we show that ratio - first can significantly improve the inference speed while maintaining the generation quality . 
We evaluate the proposed model on three typical text generation tasks , including text summarization , sentence compression and machine translation .
Experimental results show that our model significantly outperforms many strong non - autoregressive baselines , and even performs competitively with several strong autoregressive models .
In addition , we conduct extensive analysis experiments to study the effect of individual proposed components . 
In summary , our contributions are : ( 1 ) We propose a novel framework that utilizes BERT for text generation under the non - autoregressive generation paradigm ; ( 2 ) We propose a decoding mechanism that allows the model to dynamically determine the output length , and a new context - aware learning objective that reduces errors stemming from the output - side conditional independence assumption ; ( 3 ) We introduce a ratio - first decoding strategy that further improve the model 's inference efficiency .
Autoregressive generation ( AG ) models generate sequences based on a left - to - right factorization .
As shown in Figure 1 , given the source sequence X , the target sequence Y with length T is generated via a chain of conditional probabilities based on the left - to - right sequential dependencies as : p ( Y | X )
= T i=1
p ( y
i |
y
< i , X ) , ( 1 ) where y < i denotes the tokens before the i - th step .
This property of autoregressive factorization makes the generation process hard to be parallelized as the result is generated token by token .
Unlike AG models , non - autoregressive ( NAG ) models generate sequences without modelling the output - side dependencies .
As shown in Figure 1 , given the prespecified output length T , the probability of the target sequence Y is then modelled as : p ( Y | X )
= T i=1
p ( y
i |
X , i , T ) . 
( 2 ) 
With this conditional independence assumption , NAG models can fully parallelize their generation process , which significantly improves the inference speed .
However , it has been shown that , the choice of the prespecified output length has a notable impact on the model 's generation quality ( Gu et al , 2018 ) .
In addition , the removal of output - side sequential dependency also causes the generation quality of NAG models to be inferior to their autoregressive counterparts ( Wang et al , 2019b ) .
In this section , we give a detailed explanation of the proposed model .
First , we describe how to utilize BERT as a non - autoregressive generation model .
Then we discuss the decoding mechanism which allows the model to determine the output length dynamically .
Finally , we introduce the new ratiofirst decoding strategy which further improves the model 's decoding efficiency .
The architecture of the proposed model is presented in Figure 2 , in which the embedding layer and the stack of transformer layers are initialized with BERT ( Devlin et al , 2019 ) . 
Input Representation Following the setup of BERT , we first append a [ cls ] and a [ sep ] token on both sides of the source sequence .
Then we attach a number of [ pad ] tokens at the end of source sequence to make its length equal to the predefined maximum size ( e.g. , 256 ) .
Thus we can make sure the source length is longer than or equal to the output length .
As a special case , for tasks like text summarization where the source is known to be longer than the target , we do not attach the [ pad ] tokens when constructing the input . 
Transformer Layers Given the source sequence X , it is processed by a stack of N transformer ( Vaswani et al , 2017 ) layers .
Formally , the Multi - Head Attention is defined as MultiHead ( Q , K , V ) , where Q , K , V denotes the query , key and value respectively .
The computation of the first transformer layer is then defined as : V ( 1 ) = MultiHead ( E ( X ) , E ( X ) , E ( X ) ) , ( 3 ) O ( 1 )
= FFN ( V ( 1 ) ) , ( 4 ) FFN ( x )
= max ( 0 , xW 1 +
b 1 ) W 2 + b 2 , ( 5 ) where E ( X )
= T E ( X ) + P E ( X ) in which T E ( ) denotes the token embedding and P E ( ) denotes the position embedding .
For other layers : V ( n ) = MultiHead ( O ( n−1 ) , O ( n−1 ) , O ( n−1 ) ) , ( 6 ) O ( n ) = FFN ( V ( n ) ) , ( 7 ) where n = 2 , ... , N and N is the total number of transformer layers .
The final sequence representation H R T ×d model is the output states of BERT from the last layer , where T is the source sequence length and d model is the model size . 
CRF Layer
Then , H is passed through a linearchain CRF ( Lafferty et al , 2001 ) .
Under the CRF framework , the likelihood of the target sequence Y with length T is then modelled as : P CRF ( Y | X )
= e S ( X , Y )
Y e S ( X , Y )
= 1 Z ( X ) exp ( T i=1 Φ
y i ( h i )
+ T i=2 t ( y i−1 , y i ) ) , ( 8 ) where Z ( X ) is the normalizing factor and Φ
y i ( h i ) denotes the label score of y
i at position i.
In practice , Φ is parameterized by a neural network that maps the BERT output state
h i into the label ( vocabulary ) space .
The t ( y i−1 , y i )
= T y i−1 ,
y i denotes the transition score from label y i−1 to y
i where T R | V | × | V | is the transition matrix . 
Approximation
In the context of text generation , the size of the label space ( vocabulary size ) | V | is typically large , e.g. , 32k .
Therefore , it is intractable to directly model the transition matrix T and the normalizing factor Z ( X ) .
To this end , we adopt the techniques proposed by to approximate these two terms .
Specifically , the full transition matrix is approximated by the product of two low - rank matrices T = E 1 E T 2 , where E 1 , E 2 R | V | ×d and d is much smaller than | V | .
To compute the normalizing factor Z ( X ) , at each time step , instead of searching through all possible paths , the number of candidates is heuristically truncated to a predefined beam size k.
We refer readers to the original paper for further details .
In this section , we describe how to let the model determine the output sequence length by itself .
Our basic idea is that we want the model to dynamically stop generation via emitting a special [ eos ] token .
To achieve this , during training , we manually append two consecutive [ eos ] tokens to the end of the target sequence , as shown in the top left part of Figure 2 .
In this way , the model can learn a deterministic transition behaviour between two [ eos ] states , meaning that t ( [ eos ] , [ eos ] )
= max v V t ( [ eos ] , v ) .
This is because , during training , the model never sees a transition ( [ eos ] , v ) , where v = [ eos ] . 
During inference , the resultỸ is acquired as Y = arg max Y S ( X , Y ) , where the CRF scoring function S ( X , Y ) in Equation ( 8 ) can be decomposed as : S ( X , Y )
= T i=1 Φ
y i ( h i )
+ T i=2 t ( y i−1 , y i )
= Φ y 1
( h 1 ) initial state + T i=2 { label score Φ
y
i ( h i )
+ transition score t ( y i−1 , y i ) state transition } .
( 9 ) Once the decoded trajectory enters the [ eos ] state , the state transition term in S ( X , Y ) will be dominated by the transition score term t ( [ eos ] , [ eos ] ) .
As a result , the model will keep transitioning to [ eos ] in the remaining steps .
An example is provided in the right part of Figure 2 , from which we can see that , at step 5 , the decoded trajectory enters the [ eos ] state and remains at it in the rest of the generation process .
In this way , our model can dynamically control the length of output sequence by entering the [ eos ] state during the generation process .
After the entire generation process is completed , the final output sequence can be obtained by removing all generated [ eos ] tokens .
We note that the outputs of BERT can be divided into two subsets .
The first subset ranges from the beginning to the position where the first [ eos ] is emitted , and the second subset is the rest .
For example , in Figure 2 , the first subset are those corresponding to the output sequence " y ( 1 ) y ( 2 ) y ( 3 ) y ( 4 ) [ eos ] " .
As for the second part , we can see that it has little effect on the final output and removing it should not change the result .
This indicates that it suffices to only consider the beginning part of BERT outputs for improving the inference speed .
Especially , for tasks like summarization where the target is known to be shorter than the source sequence , we are safe to only use the first [ α T ] outputs of BERT to perform inference .
Here T denotes the source length , α ( 0.0 , 1.0 ) is set based on the data statistics and [ ] is the integer rounding operation .
Formally , given the source sequence X , the ratio - first decoding is defined as Y =
arg max Y F ( X , Y , α ) , = arg max Y { [ α T ] i=1 Φ
y i ( h i )
+ [ α T ] i=2 t ( y i−1 , y i ) } .
( 10 ) When α = 1.0 , ratio - first degenerates to the standard decoding strategy in CRF - based models . 
It should be noted that , [ α T ] only constrains the maximum length of the generated result , and the actual output length ( after removing the generated [ eos ] tokens ) is still decided by the model itself .
In the experiment section , we demonstrate that ratio - first can notably improve the inference speed whilst maintaining the generation quality .
Due to the conditional independence approximation on output tokens , NAG models often tend to generate repeated tokens ( Wang et al , 2019b ) .
One way to alleviate this problem is to introduce implicit dependencies on the output side .
In this work , we propose to use the unlikelihood formulation of Welleck et al ( 2020 ) in the context of NAG , where we define the set of negative candidate as the surrounding tokens within a predefined context window c. Formally , given the source sequence X and the target sequence Y with length T , the proposed context - aware objective is defined as : L CA ( Y | X )
= − T i=1 { log p θ ( y
i |
h i ; X ) +
l CA ( i ) } , l CA ( i ) =
j = i+c j = i−c , y
j = y i log ( 1.0 − p θ ( y j | h i ; X ) ) , ( 11 ) where h i is the model output state at position i.
At position i , the proposed objective maximizes the probability of token
y
i while minimizing the probabilities of the surrounding tokens .
In this way , it discourages the model from generating repetitive tokens at different time steps . 
The overall learning objective is then defined as L CRF
= − log P CRF ( Y | X ) , L = L CRF + λ L CA , ( 12 ) where λ controls the importance of different loss terms and P CRF ( Y | X ) is described in Equation ( 8 ) .
Non - Autoregressive generation was first introduced by Gu et al ( 2018 ) to reduce the inference latency in machine translation .
Recent works in this area have investigated ways to mitigate the tradeoff between the decoding speed and generation quality .
Gu et al ( 2018 ) utilized fertility as latent variables for better translation performance .
Wang et al ( 2019b ) proposed two auxiliary objectives for better modelling the output states and solving the under - translation problem .
To better model the intermediate alignments between source and target sides , Ma et al ( 2019 ) proposed a model based on the generative flow framework .
Ghazvininejad et al ( 2019 ) proposed to use a masked language objective to train the NAG model .
During inference , starting from a fully masked sequence , the output is generated in an iterative refinement manner .
Recently , proposed to incorporate a conditional random field into the decoder of a NAG model for better modelling the outputside dependencies .
Our work is different from prior works in two aspects : ( 1 ) we directly utilize a pretrained language model ( BERT ) to perform nonautoregressive generation ; ( 2 ) our model can dynamically generate the output sequence without the need of prespecified output length .
We evaluate the proposed model on three typical text generation tasks : ( 1 ) text summarization ; ( 2 ) sentence compression and ( 3 ) machine translation .
We implement the proposed model with PyTorch ( Paszke et al , 2017 ) .
The BERT model we use is the Huggingface implementation ( Wolf et al , 2019 ) ( bert - base - uncased ) .
To approximate the transition matrix in the CRF layer , we set the dimension d of matrices E 1 and E 2 as 32 .
For the normalizing factor Z ( X ) , we set the predefined beam size k as 256 .
As for the overall learning objective , we set the window size c as 3 and λ as 1.0 .
In training , we use Adam optimizer ( Kingma and Ba , 2015 ) .
To measure the relative speedup , we follow the standard setup which runs inference for each individual example separately .
The model 's inference speed is computed by averaging the results of test cases .
For a fair comparison , we measure the inference speed of all models on the same platform .
Text summarization aims to automatically generate a compact summary that retains the most important content of the original text document ( Nenkova and McKeown , 2012 ) .
In this experiment , we use the Gigawords dataset ( Rush et al , 2015 ) as our benchmark .
For evaluation , standard metrics including ROUGE - 1 ( R - 1 ) , ROUGE - 2 ( R - 2 ) and ROUGE - L ( R - L ) ( Lin , 2004 ) are reported .
We compare our model with several representative and the latest NAG models , including NAG - NMT ( Gu et al , 2018 ) , NAR - REG ( Wang et al , 2019b ) and NAG - CRF .
Following previous works , during training , we train a length predictor to predict the output length .
During inference , for each NAG baseline , we adopt the length - parallel decoding strategy ( LPD - k ) , that is , generating k results using the top - k possible output length predictions from the length predictor .
The results are then re - ranked by a transformer model to get the final ouput .
In the experiment , we report the results of different NAG baselines using LPD - 9 decoding .
In addition , to better examine the effect of using BERT in NAG models , we add a BNAG - CRF baseline which adopts the same structure of the NAG - CRF model but using BERT as the encoder .
We also compare our model with several strong autoregressive models , which are Luong - NMT ( Luong et al , 2015 ) , Pointer - Generator ( See et al , 2017 ) , DRGD ( Li et al , 2017 ) and Concept Pointer ( Wang et al , 2019a ) .
To measure the relative inference speedup , we include transformer as a baseline model . 
The results are shown in Table 1 , from which we can see that , by using length - parallel decoding , the performance of all NAG baselines can be notably improved .
However , such procedure significantly increases the inference latency .
In contrast , our model can self - determine the output length without any re - ranking process .
As shown in the results , our model outperforms the best NAG baseline ( with LPD ) and achieves performances that are comparable with several strong AG models . 
Comparing the results of BNAG - CRF and NAG - CRF , we can see that incorporating BERT as encoder helps to improve the model performance .
Nonetheless , our model still outperforms BNAG - CRF with LPD - 9 decoding .
This is because the dynamic length decoding mechanism allows our model to generate results with optimal length , leading to stronger model performances . 
Finally , we analyze the proposed ratio - first decoding .
From the results , we observe a moderate performance drop when using ratio - first ( α = 0.3 ) .
It comes from the fact that , for some input documents with length T , the reference summary is longer than [ α T ] .
In such cases , ratio - first fails to generate the complete reference summary , leading to the drop of performance .
On the other hand , we can see that , ratio - first can notably improve the inference speedup .
With α = 0.3 , our model achieves the highest inference speedup while still outperforms all compared NAG models .
Sentence compression aims at compressing a long sentence into a short one by deleting redundant words .
In this experiment , we use the Google sentence compression dataset ( Filippova and Altun , 2013 ) as our benchmark .
For evaluation , we use the standard token - kept - F1 ( F1 ) score .
In addition , We also report the results of other standard metrics including ROUGE - 1 , ROUGE - 2 and ROUGE - L. Models F1
R - 1 R - 2 R - L We compare the proposed model with the same NAG baselines as in the previous experiment .
We also compare our model with several strong autoregressive models , including Bi - LSTM - Dep ( Filippova et al , 2015 ) , Tagger and Tagger+ILP , HiSAN - Dep and HiSAN ( Kamigaito et al , 2018 ) .
To measure the inference speedup , we include transformer as a baseline model . 
The results are presented in Table 2 , from which we see that our model outperforms the best reported NAG baseline ( with LPD ) in terms of both the generation quality and inference speed .
Comparing with the strong autoregressive models , our model can achieve competitive performance with a over 8.42× inference speed up .
We also report the results of our model using the ratio - first decoding strategy .
By setting α as 0.7 , it achieves a 10.00× inference speedup while still outperforming other compared NAG baselines .
Machine translation aims at translating text from the source language to the target language .
In this task , we use the IWSLT14 German - to - English ( DE - EN ) dataset as our benchmark .
Following previous works , we use the sequence - level knowledge distillation ( Gu et al , 2018 ) during training .
For evaluation , we report results in BLEU scores ( Papineni et al , 2002 ) .
In this experiment , we use the BERT model in German language . 
We compare our model with a range of strong NAG models , including NAG - NMT ( Gu et al , 2018 ) , ENAG - E and ENAG - P
( Guo et al , 2019 ) , NAG - REG ( Wang et al , 2019b ) , NAG - CRF and BNAG - CRF .
For each NAG baseline , we also report the results using LPD - 9 decoding .
In addition , we compare our model with several strong autoregressive models , including LSTM - based ( Wu et al , 2016 ) , CNN - based ( Gehring et al , 2017 ) and transformer model .
The results are shown in Table 3 , from which we see that our model outperforms the best NAG baseline ( with LPD ) in terms of both the generation quality and inference speedup .
Additionally , we also report the results using the ratio - first decoding .
By setting α as 0.8 , the inference speedup can be further boosted to 13.92× while the generation quality is still higher than the best NAG baseline .
In this section , we present further discussions and empirical analysis of the proposed model .
nents , the overall performance decreases .
By removing BERT from the model , we observe notable drop across all metrics .
This shows that the knowledge of BERT is an important factor of the model 's strong performance .
Comparing with results in Table 1 , it still outperforms vanilla NAG - CRF and performs comparably with NAG - CRF using LPD decoding , which demonstrates the merit of the proposed dynamic length decoding mechanism .
Another interesting finding is that , by only removing the CRF layer , the most notable drop is observed on the bigram - level metric .
This shows that the bigram - level dependencies on the output side are mainly captured by the CRF module .
In addition , by removing both BERT and CRF , all metrics further decrease .
This confirms that each of these two components positively contributes to the model 's overall performance .
Context - Aware Objective
In this part , we study the effect of the context - aware objective .
As described in Equation ( 11 ) , it aims at alleviating the problem of repetitive generation .
To give a quantitative analysis , we use the measurement of sentencelevel repetition ( Welleck et al , 2020 ) to compute the ratio of duplicate n - grams ( rep - n ) in the generated result .
This metric is defined as rep - n ( Y ) = 100 × ( 1.0 − | unique n - grams ( Y )
| | n - grams ( Y ) | ) . 
( 13 ) For each generated result , rep - n is 0.0 when it has no repeating n - grams .
The final result is computed by averaging over the entire evaluation set . 
We conduct experiments on Gigawords dataset to evaluate the n - gram repetitions ranging from uni - gram to 4 - gram .
The results are shown in Table 5 , where w/o CA means the model is trained without using context - aware objective and
R - L denotes the model 's ROUGE - L score .
Additionally , we also show the results from transformer model for a direct comparison .
Comparing the two variants of our model , we see that training with context - aware objective leads to a 42 % drop on rep - 3 metric ( 0.427 vs 0.741 ) and a 64 % drop on rep - 4 metric ( 0.106 vs 0.295 ) .
The ROUGE - L results also indicate that the reduction in token repetition can effectively improve the model generation quality . 
Dynamic Length Determination Next , we examine the importance of the model 's ability to dynamically determine the length of the generated output .
To this end , we train another model variant by removing the two [ eos ] tokens from the target sequence .
In this way , the model is not able to self - determine the output length throughout the generation process .
To perform inference , we use length - parallel decoding ( LPD ) with different number of length candidates .
Formally , for each length candidate l , the model generates the resultỸ as Y = arg max Y { l i=1 Φ
y i ( h i ) + l i=2 t ( y i−1 , y i ) } .
( 14 ) 
The final result is acquired by re - ranking the generated results with a transformer model . 
We conduct experiments on the IWSLT14 DE - EN dataset in which we try a different number of length candidates , including top - 1 , top - 5 and top - 10 .
The results are shown in Table 6 , from which we can see , as the number of length candidates increases , the model performance increases as well .
The reason is that a larger candidates set is more likely to contain the best - suited length for the generation model , leading to better performance .
However , such decoding procedure inevitably increases the required computation overhead .
We can see that , when setting k as 10 , the inference speedup decreases from 11.84× to 6.01×.
In contrast , our proposed model is able to determine the optimal output length by itself .
Without any re - ranking process , it outperforms the model with LPD - 10 decoding and achieves the inference speedup that is comparable with the model using LPD - 1 decoding .
We are also interested in the effect of the ratio - first decoding strategy .
To provide a quantitative analysis , we perform inference on the Gigawords dataset using ratio - first with different α .
The experimental results with different α are presented in Figure 3 .
It can be observed that , when α reaches 0.3 , the model approximately achieves its optimal performance .
At the same time , a notable improvement can be observed in terms of the inference speedup ( 6.72× 9.31× ) .
Now we illustrate why the near optimal performance can be achieved when α reaches 0.3 .
In Figure 4 , we present the distribution of the target / source length ratio of every data instance in the Gigawords dataset .
We can see that , for most cases , the ratio between the target length T and source length T is less than 0.3 .
Recall the definition of ratio - first decoding in Equation ( 10 ) , the [ α T ] constrains the maximum length of the generated result .
Therefore , once we have a prior knowledge on the data statistic , we can easily choose a proper α that both improves the inference speed whilst maintaining the generation quality .
In this case , a proper α could be 0.3 which is demonstrated by the results in Figure 3 and 4 .
By setting different α , ratio - first provides us an explicit way to control the balance between the inference speed and the generation quality .
This property of ratio - first is especially favorable in real - life scenarios where the inference speed is the highest concern .
In this work , we explored the potential of BERT in various text generation tasks under the NAG framework .
To address problems from NAG models previously having a prefixed output length , we devised a decoding mechanism which enables the model to determine the output length dynamically .
To reduce errors stemming from the assumption of conditional independence of output tokens , we proposed a context - aware objective as well as using a CRF decoding .
Furthermore , to maximize the inference speed advantage of our model , we introduced a ratio - first decoding strategy .
We evaluated our model on three benchmark datasets and the results show that our model significantly outperforms many strong NAG baselines and performs comparably to many strong AG models .
The authors wish to thank Jialu Xu , Guanlin Li , Xing Wang for their insightful discussions and support .
Many thanks to our anonymous reviewers for their suggestions and comments .
